/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_25_rnnt_bias_3word_finetune_seperate.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/ddp_init
2023-02-25 15:26:38,275 INFO training on multiple gpus, this gpu 7
2023-02-25 15:26:38,291 INFO training on multiple gpus, this gpu 6
2023-02-25 15:26:38,317 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-25 15:26:41,377 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-25 15:26:41,423 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-02-25 15:26:41,471 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-02-25 15:26:44,227 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/10.pt for GPU
2023-02-25 15:26:51,179 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/10.pt for GPU
2023-02-25 15:27:08,101 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-25 15:27:08,103 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-25 15:27:08,118 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-25 15:27:08,119 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-25 15:28:59,409 DEBUG TRAIN Batch 11/0 loss 313.826965 loss_att 80.543594 loss_ctc 1123.228882 loss_rnnt 249.683594 hw_loss 5.399605 lr 0.00052195 rank 0
2023-02-25 15:28:59,415 DEBUG TRAIN Batch 11/0 loss 727.433044 loss_att 83.319290 loss_ctc 1019.740906 loss_rnnt 814.107056 hw_loss 5.951736 lr 0.00052195 rank 1
2023-02-25 15:29:51,578 DEBUG TRAIN Batch 11/100 loss 311.594696 loss_att 297.660278 loss_ctc 392.713013 loss_rnnt 303.407776 hw_loss 0.296269 lr 0.00052167 rank 1
2023-02-25 15:29:51,598 DEBUG TRAIN Batch 11/100 loss 298.861786 loss_att 286.271118 loss_ctc 372.250488 loss_rnnt 291.429504 hw_loss 0.309828 lr 0.00052167 rank 0
2023-02-25 15:30:41,649 DEBUG TRAIN Batch 11/200 loss 236.863861 loss_att 301.276794 loss_ctc 302.500244 loss_rnnt 215.131836 hw_loss 0.183569 lr 0.00052139 rank 1
2023-02-25 15:30:41,649 DEBUG TRAIN Batch 11/200 loss 221.774231 loss_att 294.594055 loss_ctc 280.662781 loss_rnnt 199.134140 hw_loss 0.420627 lr 0.00052139 rank 0
2023-02-25 15:31:32,124 DEBUG TRAIN Batch 11/300 loss 188.935715 loss_att 304.674652 loss_ctc 192.145584 loss_rnnt 165.133606 hw_loss 0.424402 lr 0.00052110 rank 1
2023-02-25 15:31:32,141 DEBUG TRAIN Batch 11/300 loss 236.446381 loss_att 333.762207 loss_ctc 255.826233 loss_rnnt 214.296616 hw_loss 0.192395 lr 0.00052110 rank 0
2023-02-25 15:33:14,633 DEBUG TRAIN Batch 11/400 loss 161.347641 loss_att 269.122040 loss_ctc 160.092590 loss_rnnt 139.825653 hw_loss 0.252081 lr 0.00052082 rank 0
2023-02-25 15:33:14,657 DEBUG TRAIN Batch 11/400 loss 179.314270 loss_att 263.765289 loss_ctc 179.054550 loss_rnnt 162.272034 hw_loss 0.349950 lr 0.00052082 rank 1
2023-02-25 15:34:05,444 DEBUG TRAIN Batch 11/500 loss 145.021545 loss_att 255.070129 loss_ctc 128.917892 loss_rnnt 124.895561 hw_loss 0.493946 lr 0.00052054 rank 0
2023-02-25 15:34:05,454 DEBUG TRAIN Batch 11/500 loss 124.482529 loss_att 225.009476 loss_ctc 118.500473 loss_rnnt 105.008873 hw_loss 0.311009 lr 0.00052054 rank 1
2023-02-25 15:34:55,477 DEBUG TRAIN Batch 11/600 loss 78.292473 loss_att 140.280823 loss_ctc 74.892982 loss_rnnt 66.185692 hw_loss 0.304460 lr 0.00052026 rank 0
2023-02-25 15:34:55,482 DEBUG TRAIN Batch 11/600 loss 100.787804 loss_att 180.160126 loss_ctc 89.649124 loss_rnnt 86.228386 hw_loss 0.318956 lr 0.00052026 rank 1
2023-02-25 15:35:46,279 DEBUG TRAIN Batch 11/700 loss 116.836182 loss_att 254.293549 loss_ctc 96.008850 loss_rnnt 92.031296 hw_loss 0.169491 lr 0.00051997 rank 1
2023-02-25 15:35:46,299 DEBUG TRAIN Batch 11/700 loss 181.014679 loss_att 320.550049 loss_ctc 159.441483 loss_rnnt 155.898972 hw_loss 0.159476 lr 0.00051997 rank 0
2023-02-25 15:37:40,130 DEBUG TRAIN Batch 11/800 loss 98.950821 loss_att 249.790161 loss_ctc 81.135880 loss_rnnt 71.042358 hw_loss 0.217369 lr 0.00051969 rank 1
2023-02-25 15:37:40,131 DEBUG TRAIN Batch 11/800 loss 111.406082 loss_att 258.013458 loss_ctc 90.261383 loss_rnnt 84.801849 hw_loss 0.191343 lr 0.00051969 rank 0
2023-02-25 15:38:30,132 DEBUG TRAIN Batch 11/900 loss 137.983826 loss_att 308.252686 loss_ctc 112.448479 loss_rnnt 107.200455 hw_loss 0.251804 lr 0.00051941 rank 0
2023-02-25 15:38:30,135 DEBUG TRAIN Batch 11/900 loss 112.495583 loss_att 232.616516 loss_ctc 106.591667 loss_rnnt 89.139084 hw_loss 0.224079 lr 0.00051941 rank 1
2023-02-25 15:39:19,765 DEBUG TRAIN Batch 11/1000 loss 106.943695 loss_att 250.975021 loss_ctc 96.287415 loss_rnnt 79.394600 hw_loss 0.306882 lr 0.00051913 rank 0
2023-02-25 15:39:19,769 DEBUG TRAIN Batch 11/1000 loss 95.778427 loss_att 195.801544 loss_ctc 84.913971 loss_rnnt 77.115639 hw_loss 0.200168 lr 0.00051913 rank 1
2023-02-25 15:41:01,009 DEBUG TRAIN Batch 11/1100 loss 97.258881 loss_att 217.191498 loss_ctc 95.451202 loss_rnnt 73.406387 hw_loss 0.200597 lr 0.00051885 rank 0
2023-02-25 15:41:01,023 DEBUG TRAIN Batch 11/1100 loss 124.189667 loss_att 237.071167 loss_ctc 125.966042 loss_rnnt 101.218040 hw_loss 0.297146 lr 0.00051885 rank 1
2023-02-25 15:41:51,044 DEBUG TRAIN Batch 11/1200 loss 64.254425 loss_att 164.849182 loss_ctc 48.309963 loss_rnnt 46.158813 hw_loss 0.192344 lr 0.00051857 rank 1
2023-02-25 15:41:51,061 DEBUG TRAIN Batch 11/1200 loss 107.433197 loss_att 191.209122 loss_ctc 101.512627 loss_rnnt 91.315086 hw_loss 0.285639 lr 0.00051857 rank 0
2023-02-25 15:42:41,597 DEBUG TRAIN Batch 11/1300 loss 54.730587 loss_att 113.920181 loss_ctc 46.411118 loss_rnnt 43.828560 hw_loss 0.325056 lr 0.00051829 rank 1
2023-02-25 15:42:41,616 DEBUG TRAIN Batch 11/1300 loss 127.801239 loss_att 300.528564 loss_ctc 89.856522 loss_rnnt 98.158379 hw_loss 0.293806 lr 0.00051829 rank 0
2023-02-25 15:43:32,084 DEBUG TRAIN Batch 11/1400 loss 127.000099 loss_att 273.450684 loss_ctc 121.771935 loss_rnnt 98.335564 hw_loss 0.134066 lr 0.00051802 rank 1
2023-02-25 15:43:32,119 DEBUG TRAIN Batch 11/1400 loss 97.991371 loss_att 223.714752 loss_ctc 99.423790 loss_rnnt 72.553528 hw_loss 0.191581 lr 0.00051802 rank 0
2023-02-25 15:45:10,376 DEBUG TRAIN Batch 11/1500 loss 66.526711 loss_att 152.443237 loss_ctc 50.287498 loss_rnnt 51.404999 hw_loss 0.194302 lr 0.00051774 rank 0
2023-02-25 15:45:10,391 DEBUG TRAIN Batch 11/1500 loss 92.534531 loss_att 214.620636 loss_ctc 76.833771 loss_rnnt 70.125603 hw_loss 0.159651 lr 0.00051774 rank 1
2023-02-25 15:46:00,475 DEBUG TRAIN Batch 11/1600 loss 89.491653 loss_att 154.791000 loss_ctc 96.649307 loss_rnnt 75.399208 hw_loss 0.146663 lr 0.00051746 rank 0
2023-02-25 15:46:00,494 DEBUG TRAIN Batch 11/1600 loss 69.662941 loss_att 150.959091 loss_ctc 63.704597 loss_rnnt 54.118401 hw_loss 0.149556 lr 0.00051746 rank 1
2023-02-25 15:46:48,805 DEBUG TRAIN Batch 11/1700 loss 86.522331 loss_att 174.687363 loss_ctc 80.924919 loss_rnnt 69.474152 hw_loss 0.302800 lr 0.00051718 rank 0
2023-02-25 15:46:48,808 DEBUG TRAIN Batch 11/1700 loss 74.783119 loss_att 177.727020 loss_ctc 68.793747 loss_rnnt 54.845100 hw_loss 0.277156 lr 0.00051718 rank 1
2023-02-25 15:48:34,799 DEBUG TRAIN Batch 11/1800 loss 67.974419 loss_att 134.731445 loss_ctc 73.031654 loss_rnnt 53.772675 hw_loss 0.330076 lr 0.00051691 rank 0
2023-02-25 15:48:34,801 DEBUG TRAIN Batch 11/1800 loss 74.030296 loss_att 138.383514 loss_ctc 65.743050 loss_rnnt 62.053001 hw_loss 0.396792 lr 0.00051691 rank 1
2023-02-25 15:49:23,886 DEBUG TRAIN Batch 11/1900 loss 74.656822 loss_att 121.716660 loss_ctc 68.018387 loss_rnnt 66.027969 hw_loss 0.191278 lr 0.00051663 rank 1
2023-02-25 15:49:23,893 DEBUG TRAIN Batch 11/1900 loss 55.264015 loss_att 105.319908 loss_ctc 47.208664 loss_rnnt 46.189342 hw_loss 0.257896 lr 0.00051663 rank 0
2023-02-25 15:50:13,333 DEBUG TRAIN Batch 11/2000 loss 49.276333 loss_att 117.956207 loss_ctc 43.756729 loss_rnnt 36.218796 hw_loss 0.107835 lr 0.00051636 rank 1
2023-02-25 15:50:13,360 DEBUG TRAIN Batch 11/2000 loss 86.174515 loss_att 162.496246 loss_ctc 81.441429 loss_rnnt 71.474396 hw_loss 0.125344 lr 0.00051636 rank 0
2023-02-25 15:51:02,748 DEBUG TRAIN Batch 11/2100 loss 90.258743 loss_att 142.629486 loss_ctc 103.953003 loss_rnnt 77.817184 hw_loss 0.265338 lr 0.00051608 rank 1
2023-02-25 15:51:02,756 DEBUG TRAIN Batch 11/2100 loss 66.386627 loss_att 145.001038 loss_ctc 60.500275 loss_rnnt 51.240047 hw_loss 0.391027 lr 0.00051608 rank 0
2023-02-25 15:52:41,406 DEBUG TRAIN Batch 11/2200 loss 64.730659 loss_att 135.357391 loss_ctc 57.240921 loss_rnnt 51.529236 hw_loss 0.140073 lr 0.00051581 rank 1
2023-02-25 15:52:41,413 DEBUG TRAIN Batch 11/2200 loss 57.242558 loss_att 114.789749 loss_ctc 54.242737 loss_rnnt 45.982086 hw_loss 0.283140 lr 0.00051581 rank 0
2023-02-25 15:53:29,476 DEBUG TRAIN Batch 11/2300 loss 73.512329 loss_att 128.442093 loss_ctc 71.456482 loss_rnnt 62.627872 hw_loss 0.323632 lr 0.00051553 rank 1
2023-02-25 15:53:29,477 DEBUG TRAIN Batch 11/2300 loss 62.633606 loss_att 106.621109 loss_ctc 59.705616 loss_rnnt 54.079250 hw_loss 0.276099 lr 0.00051553 rank 0
2023-02-25 15:54:18,950 DEBUG TRAIN Batch 11/2400 loss 55.178474 loss_att 103.878021 loss_ctc 58.541634 loss_rnnt 44.874233 hw_loss 0.217332 lr 0.00051526 rank 1
2023-02-25 15:54:18,951 DEBUG TRAIN Batch 11/2400 loss 49.533173 loss_att 93.479599 loss_ctc 48.134068 loss_rnnt 40.768162 hw_loss 0.304263 lr 0.00051526 rank 0
2023-02-25 15:55:56,707 DEBUG TRAIN Batch 11/2500 loss 60.840218 loss_att 92.971992 loss_ctc 61.443188 loss_rnnt 54.138943 hw_loss 0.364729 lr 0.00051499 rank 0
2023-02-25 15:55:56,724 DEBUG TRAIN Batch 11/2500 loss 43.034458 loss_att 84.662056 loss_ctc 44.056133 loss_rnnt 34.358776 hw_loss 0.401136 lr 0.00051499 rank 1
2023-02-25 15:56:45,981 DEBUG TRAIN Batch 11/2600 loss 20.589710 loss_att 27.725084 loss_ctc 20.800291 loss_rnnt 18.889469 hw_loss 0.459540 lr 0.00051471 rank 0
2023-02-25 15:56:45,986 DEBUG TRAIN Batch 11/2600 loss 90.879456 loss_att 139.112396 loss_ctc 90.828178 loss_rnnt 81.120026 hw_loss 0.224396 lr 0.00051471 rank 1
2023-02-25 15:57:35,257 DEBUG TRAIN Batch 11/2700 loss 65.762924 loss_att 112.358673 loss_ctc 54.298676 loss_rnnt 57.881664 hw_loss 0.170020 lr 0.00051444 rank 1
2023-02-25 15:57:35,261 DEBUG TRAIN Batch 11/2700 loss 55.423344 loss_att 91.864304 loss_ctc 59.473461 loss_rnnt 47.447472 hw_loss 0.276880 lr 0.00051444 rank 0
2023-02-25 15:58:24,859 DEBUG TRAIN Batch 11/2800 loss 43.951633 loss_att 83.335175 loss_ctc 42.789948 loss_rnnt 36.174454 hw_loss 0.103810 lr 0.00051417 rank 1
2023-02-25 15:58:24,861 DEBUG TRAIN Batch 11/2800 loss 62.936855 loss_att 106.691826 loss_ctc 71.605736 loss_rnnt 52.914833 hw_loss 0.215961 lr 0.00051417 rank 0
2023-02-25 16:00:01,904 DEBUG TRAIN Batch 11/2900 loss 47.141518 loss_att 73.104843 loss_ctc 45.661087 loss_rnnt 41.926376 hw_loss 0.412250 lr 0.00051390 rank 1
2023-02-25 16:00:01,925 DEBUG TRAIN Batch 11/2900 loss 38.969852 loss_att 58.890942 loss_ctc 42.842049 loss_rnnt 34.364937 hw_loss 0.195761 lr 0.00051390 rank 0
2023-02-25 16:00:50,868 DEBUG TRAIN Batch 11/3000 loss 45.829838 loss_att 67.514427 loss_ctc 49.171143 loss_rnnt 40.729389 hw_loss 0.596286 lr 0.00051362 rank 0
2023-02-25 16:00:50,871 DEBUG TRAIN Batch 11/3000 loss 72.379097 loss_att 110.947304 loss_ctc 73.240944 loss_rnnt 64.434845 hw_loss 0.216944 lr 0.00051362 rank 1
2023-02-25 16:01:39,947 DEBUG TRAIN Batch 11/3100 loss 37.453121 loss_att 66.828766 loss_ctc 38.353172 loss_rnnt 31.333561 hw_loss 0.233295 lr 0.00051335 rank 0
2023-02-25 16:01:39,964 DEBUG TRAIN Batch 11/3100 loss 32.478718 loss_att 58.519009 loss_ctc 33.091702 loss_rnnt 27.014158 hw_loss 0.327694 lr 0.00051335 rank 1
2023-02-25 16:03:15,140 DEBUG TRAIN Batch 11/3200 loss 25.554689 loss_att 27.440598 loss_ctc 29.203356 loss_rnnt 24.490152 hw_loss 0.376619 lr 0.00051308 rank 0
2023-02-25 16:03:15,153 DEBUG TRAIN Batch 11/3200 loss 32.009377 loss_att 43.386074 loss_ctc 34.857723 loss_rnnt 29.150991 hw_loss 0.381120 lr 0.00051308 rank 1
2023-02-25 16:04:04,087 DEBUG TRAIN Batch 11/3300 loss 48.787308 loss_att 65.423019 loss_ctc 53.821224 loss_rnnt 44.679787 hw_loss 0.204729 lr 0.00051281 rank 0
2023-02-25 16:04:04,101 DEBUG TRAIN Batch 11/3300 loss 42.398849 loss_att 63.339138 loss_ctc 42.097435 loss_rnnt 38.125053 hw_loss 0.236113 lr 0.00051281 rank 1
2023-02-25 16:04:52,547 DEBUG TRAIN Batch 11/3400 loss 34.206295 loss_att 54.044071 loss_ctc 33.970913 loss_rnnt 30.226608 hw_loss 0.081590 lr 0.00051254 rank 1
2023-02-25 16:04:52,556 DEBUG TRAIN Batch 11/3400 loss 38.179737 loss_att 68.434296 loss_ctc 33.899986 loss_rnnt 32.484116 hw_loss 0.403766 lr 0.00051254 rank 0
2023-02-25 16:05:41,478 DEBUG TRAIN Batch 11/3500 loss 36.764069 loss_att 55.142151 loss_ctc 36.175144 loss_rnnt 33.036720 hw_loss 0.244225 lr 0.00051228 rank 0
2023-02-25 16:05:41,486 DEBUG TRAIN Batch 11/3500 loss 36.992249 loss_att 65.464012 loss_ctc 38.229469 loss_rnnt 30.934246 hw_loss 0.372542 lr 0.00051228 rank 1
2023-02-25 16:07:18,026 DEBUG TRAIN Batch 11/3600 loss 40.358322 loss_att 69.728653 loss_ctc 43.705582 loss_rnnt 33.887100 hw_loss 0.282852 lr 0.00051201 rank 1
2023-02-25 16:07:18,045 DEBUG TRAIN Batch 11/3600 loss 51.431667 loss_att 73.692902 loss_ctc 62.303375 loss_rnnt 45.216476 hw_loss 0.587584 lr 0.00051201 rank 0
2023-02-25 16:08:07,586 DEBUG TRAIN Batch 11/3700 loss 33.058029 loss_att 49.220554 loss_ctc 33.915363 loss_rnnt 29.567005 hw_loss 0.270381 lr 0.00051174 rank 1
2023-02-25 16:08:07,604 DEBUG TRAIN Batch 11/3700 loss 39.293751 loss_att 52.792007 loss_ctc 38.744526 loss_rnnt 36.474377 hw_loss 0.361785 lr 0.00051174 rank 0
2023-02-25 16:08:56,238 DEBUG TRAIN Batch 11/3800 loss 44.748028 loss_att 65.939629 loss_ctc 51.046204 loss_rnnt 39.442162 hw_loss 0.427107 lr 0.00051147 rank 1
2023-02-25 16:08:56,239 DEBUG TRAIN Batch 11/3800 loss 36.440022 loss_att 43.401997 loss_ctc 37.377529 loss_rnnt 34.754990 hw_loss 0.314320 lr 0.00051147 rank 0
2023-02-25 16:09:46,217 DEBUG TRAIN Batch 11/3900 loss 38.489136 loss_att 60.426426 loss_ctc 39.537243 loss_rnnt 33.836395 hw_loss 0.235378 lr 0.00051120 rank 0
2023-02-25 16:09:46,219 DEBUG TRAIN Batch 11/3900 loss 53.160931 loss_att 90.458359 loss_ctc 55.756508 loss_rnnt 45.193916 hw_loss 0.302724 lr 0.00051120 rank 1
2023-02-25 16:11:21,209 DEBUG TRAIN Batch 11/4000 loss 19.959530 loss_att 33.119194 loss_ctc 18.508299 loss_rnnt 17.352818 hw_loss 0.315519 lr 0.00051094 rank 0
2023-02-25 16:11:21,212 DEBUG TRAIN Batch 11/4000 loss 37.479118 loss_att 57.621140 loss_ctc 35.306301 loss_rnnt 33.547119 hw_loss 0.362440 lr 0.00051094 rank 1
2023-02-25 16:12:09,963 DEBUG TRAIN Batch 11/4100 loss 34.134594 loss_att 52.851662 loss_ctc 29.327358 loss_rnnt 30.863382 hw_loss 0.316431 lr 0.00051067 rank 0
2023-02-25 16:12:09,973 DEBUG TRAIN Batch 11/4100 loss 32.212578 loss_att 44.752171 loss_ctc 33.092903 loss_rnnt 29.482559 hw_loss 0.196364 lr 0.00051067 rank 1
2023-02-25 16:12:58,904 DEBUG TRAIN Batch 11/4200 loss 41.364925 loss_att 63.398899 loss_ctc 40.737209 loss_rnnt 36.963478 hw_loss 0.146902 lr 0.00051040 rank 1
2023-02-25 16:12:58,909 DEBUG TRAIN Batch 11/4200 loss 56.418434 loss_att 92.332191 loss_ctc 59.853352 loss_rnnt 48.598820 hw_loss 0.335379 lr 0.00051040 rank 0
2023-02-25 16:14:33,280 DEBUG TRAIN Batch 11/4300 loss 35.902336 loss_att 55.347775 loss_ctc 43.868118 loss_rnnt 30.793167 hw_loss 0.296197 lr 0.00051014 rank 0
2023-02-25 16:14:33,301 DEBUG TRAIN Batch 11/4300 loss 21.829868 loss_att 34.822678 loss_ctc 21.893496 loss_rnnt 19.040949 hw_loss 0.341009 lr 0.00051014 rank 1
2023-02-25 16:15:22,890 DEBUG TRAIN Batch 11/4400 loss 52.352695 loss_att 73.972862 loss_ctc 51.272121 loss_rnnt 48.057411 hw_loss 0.216224 lr 0.00050987 rank 0
2023-02-25 16:15:22,891 DEBUG TRAIN Batch 11/4400 loss 41.121563 loss_att 49.360840 loss_ctc 39.540951 loss_rnnt 39.495255 hw_loss 0.354762 lr 0.00050987 rank 1
2023-02-25 16:16:12,263 DEBUG TRAIN Batch 11/4500 loss 30.420874 loss_att 34.083710 loss_ctc 29.637199 loss_rnnt 29.582712 hw_loss 0.393907 lr 0.00050961 rank 1
2023-02-25 16:16:12,268 DEBUG TRAIN Batch 11/4500 loss 53.860680 loss_att 84.992401 loss_ctc 51.312756 loss_rnnt 47.859116 hw_loss 0.215524 lr 0.00050961 rank 0
2023-02-25 16:17:01,242 DEBUG TRAIN Batch 11/4600 loss 63.919518 loss_att 88.530739 loss_ctc 58.169640 loss_rnnt 59.705376 hw_loss 0.109789 lr 0.00050934 rank 1
2023-02-25 16:17:01,264 DEBUG TRAIN Batch 11/4600 loss 44.706440 loss_att 61.826080 loss_ctc 37.201599 loss_rnnt 42.118496 hw_loss 0.308732 lr 0.00050934 rank 0
2023-02-25 16:18:36,625 DEBUG TRAIN Batch 11/4700 loss 30.136261 loss_att 46.334599 loss_ctc 21.826109 loss_rnnt 27.885666 hw_loss 0.223027 lr 0.00050908 rank 0
2023-02-25 16:18:36,630 DEBUG TRAIN Batch 11/4700 loss 38.414936 loss_att 60.541298 loss_ctc 37.709408 loss_rnnt 33.961380 hw_loss 0.229416 lr 0.00050908 rank 1
2023-02-25 16:19:25,961 DEBUG TRAIN Batch 11/4800 loss 29.007338 loss_att 51.615494 loss_ctc 29.021069 loss_rnnt 24.364040 hw_loss 0.224689 lr 0.00050882 rank 1
2023-02-25 16:19:25,983 DEBUG TRAIN Batch 11/4800 loss 33.485092 loss_att 49.439091 loss_ctc 32.538895 loss_rnnt 30.271547 hw_loss 0.279197 lr 0.00050882 rank 0
2023-02-25 16:20:14,968 DEBUG TRAIN Batch 11/4900 loss 26.455868 loss_att 39.971035 loss_ctc 29.271246 loss_rnnt 23.282576 hw_loss 0.177892 lr 0.00050855 rank 1
2023-02-25 16:20:14,969 DEBUG TRAIN Batch 11/4900 loss 46.378887 loss_att 58.821007 loss_ctc 51.434856 loss_rnnt 42.950783 hw_loss 0.497901 lr 0.00050855 rank 0
2023-02-25 16:21:52,483 DEBUG TRAIN Batch 11/5000 loss 32.410103 loss_att 41.230343 loss_ctc 33.576965 loss_rnnt 30.359053 hw_loss 0.246406 lr 0.00050829 rank 0
2023-02-25 16:21:52,493 DEBUG TRAIN Batch 11/5000 loss 24.421186 loss_att 35.195969 loss_ctc 29.266674 loss_rnnt 21.527117 hw_loss 0.174461 lr 0.00050829 rank 1
2023-02-25 16:22:42,226 DEBUG TRAIN Batch 11/5100 loss 17.846802 loss_att 18.220089 loss_ctc 19.763632 loss_rnnt 17.323441 hw_loss 0.362109 lr 0.00050803 rank 0
2023-02-25 16:22:42,243 DEBUG TRAIN Batch 11/5100 loss 23.230328 loss_att 29.298103 loss_ctc 25.191994 loss_rnnt 21.621609 hw_loss 0.250517 lr 0.00050803 rank 1
2023-02-25 16:23:30,814 DEBUG TRAIN Batch 11/5200 loss 43.181606 loss_att 59.732567 loss_ctc 45.235027 loss_rnnt 39.502998 hw_loss 0.177421 lr 0.00050776 rank 0
2023-02-25 16:23:30,819 DEBUG TRAIN Batch 11/5200 loss 31.457432 loss_att 42.618080 loss_ctc 32.509689 loss_rnnt 28.992964 hw_loss 0.172571 lr 0.00050776 rank 1
2023-02-25 16:24:20,096 DEBUG TRAIN Batch 11/5300 loss 17.571915 loss_att 29.119719 loss_ctc 17.315996 loss_rnnt 15.157257 hw_loss 0.261035 lr 0.00050750 rank 1
2023-02-25 16:24:20,122 DEBUG TRAIN Batch 11/5300 loss 39.538948 loss_att 57.006580 loss_ctc 40.550720 loss_rnnt 35.807991 hw_loss 0.192241 lr 0.00050750 rank 0
2023-02-25 16:26:06,529 DEBUG TRAIN Batch 11/5400 loss 53.925098 loss_att 76.982254 loss_ctc 59.692398 loss_rnnt 48.420219 hw_loss 0.233396 lr 0.00050724 rank 1
2023-02-25 16:26:06,539 DEBUG TRAIN Batch 11/5400 loss 51.557438 loss_att 66.105347 loss_ctc 55.024559 loss_rnnt 47.995853 hw_loss 0.355725 lr 0.00050724 rank 0
2023-02-25 16:26:55,049 DEBUG TRAIN Batch 11/5500 loss 34.882446 loss_att 49.223522 loss_ctc 38.390072 loss_rnnt 31.434664 hw_loss 0.209776 lr 0.00050698 rank 1
2023-02-25 16:26:55,061 DEBUG TRAIN Batch 11/5500 loss 37.580135 loss_att 50.298794 loss_ctc 42.633011 loss_rnnt 34.283806 hw_loss 0.147897 lr 0.00050698 rank 0
2023-02-25 16:27:43,846 DEBUG TRAIN Batch 11/5600 loss 29.152046 loss_att 35.089943 loss_ctc 29.351467 loss_rnnt 27.811176 hw_loss 0.237560 lr 0.00050672 rank 1
2023-02-25 16:27:43,849 DEBUG TRAIN Batch 11/5600 loss 24.148052 loss_att 36.443069 loss_ctc 26.367432 loss_rnnt 21.291723 hw_loss 0.190140 lr 0.00050672 rank 0
2023-02-25 16:29:17,734 DEBUG TRAIN Batch 11/5700 loss 11.717640 loss_att 18.095228 loss_ctc 13.316138 loss_rnnt 10.051260 hw_loss 0.333243 lr 0.00050646 rank 0
2023-02-25 16:29:17,750 DEBUG TRAIN Batch 11/5700 loss 53.006348 loss_att 62.673672 loss_ctc 52.467598 loss_rnnt 50.957348 hw_loss 0.351310 lr 0.00050646 rank 1
2023-02-25 16:30:06,201 DEBUG TRAIN Batch 11/5800 loss 37.811085 loss_att 52.411411 loss_ctc 42.219902 loss_rnnt 34.184132 hw_loss 0.223204 lr 0.00050620 rank 1
2023-02-25 16:30:06,204 DEBUG TRAIN Batch 11/5800 loss 25.318579 loss_att 35.023125 loss_ctc 26.399826 loss_rnnt 23.088112 hw_loss 0.272609 lr 0.00050620 rank 0
2023-02-25 16:30:54,667 DEBUG TRAIN Batch 11/5900 loss 16.368452 loss_att 27.112343 loss_ctc 14.559950 loss_rnnt 14.322256 hw_loss 0.259785 lr 0.00050594 rank 1
2023-02-25 16:30:54,673 DEBUG TRAIN Batch 11/5900 loss 18.424154 loss_att 32.383331 loss_ctc 13.295782 loss_rnnt 16.233253 hw_loss 0.155341 lr 0.00050594 rank 0
2023-02-25 16:31:44,182 DEBUG TRAIN Batch 11/6000 loss 43.486130 loss_att 54.892803 loss_ctc 47.199978 loss_rnnt 40.503429 hw_loss 0.386603 lr 0.00050568 rank 1
2023-02-25 16:31:44,204 DEBUG TRAIN Batch 11/6000 loss 26.715935 loss_att 38.727730 loss_ctc 31.222723 loss_rnnt 23.614864 hw_loss 0.183386 lr 0.00050568 rank 0
2023-02-25 16:33:19,010 DEBUG TRAIN Batch 11/6100 loss 32.732014 loss_att 43.323898 loss_ctc 31.558935 loss_rnnt 30.662350 hw_loss 0.201930 lr 0.00050542 rank 1
2023-02-25 16:33:19,013 DEBUG TRAIN Batch 11/6100 loss 37.357693 loss_att 51.839088 loss_ctc 48.425117 loss_rnnt 32.872353 hw_loss 0.212635 lr 0.00050542 rank 0
2023-02-25 16:34:08,566 DEBUG TRAIN Batch 11/6200 loss 35.589947 loss_att 41.091255 loss_ctc 32.507778 loss_rnnt 34.754654 hw_loss 0.273721 lr 0.00050517 rank 0
2023-02-25 16:34:08,580 DEBUG TRAIN Batch 11/6200 loss 47.866844 loss_att 51.205917 loss_ctc 53.428726 loss_rnnt 46.357567 hw_loss 0.187277 lr 0.00050517 rank 1
2023-02-25 16:34:58,469 DEBUG TRAIN Batch 11/6300 loss 33.425377 loss_att 41.962605 loss_ctc 37.947441 loss_rnnt 30.991377 hw_loss 0.231768 lr 0.00050491 rank 1
2023-02-25 16:34:58,470 DEBUG TRAIN Batch 11/6300 loss 29.905846 loss_att 36.143562 loss_ctc 33.049030 loss_rnnt 28.096027 hw_loss 0.268468 lr 0.00050491 rank 0
2023-02-25 16:36:42,226 DEBUG TRAIN Batch 11/6400 loss 28.435219 loss_att 29.562002 loss_ctc 31.857241 loss_rnnt 27.514565 hw_loss 0.448181 lr 0.00050465 rank 0
2023-02-25 16:36:42,254 DEBUG TRAIN Batch 11/6400 loss 43.021645 loss_att 54.365852 loss_ctc 44.028301 loss_rnnt 40.544025 hw_loss 0.139793 lr 0.00050465 rank 1
2023-02-25 16:37:32,054 DEBUG TRAIN Batch 11/6500 loss 30.797934 loss_att 43.741875 loss_ctc 28.917072 loss_rnnt 28.385731 hw_loss 0.139118 lr 0.00050439 rank 1
2023-02-25 16:37:32,076 DEBUG TRAIN Batch 11/6500 loss 36.210548 loss_att 53.612534 loss_ctc 39.780571 loss_rnnt 32.171028 hw_loss 0.155849 lr 0.00050439 rank 0
2023-02-25 16:38:21,031 DEBUG TRAIN Batch 11/6600 loss 32.306679 loss_att 38.611023 loss_ctc 37.248791 loss_rnnt 30.197821 hw_loss 0.354454 lr 0.00050414 rank 1
2023-02-25 16:38:21,034 DEBUG TRAIN Batch 11/6600 loss 31.929420 loss_att 46.769650 loss_ctc 26.050306 loss_rnnt 29.618216 hw_loss 0.238206 lr 0.00050414 rank 0
2023-02-25 16:39:10,677 DEBUG TRAIN Batch 11/6700 loss 40.176571 loss_att 49.555328 loss_ctc 39.893204 loss_rnnt 38.220356 hw_loss 0.221713 lr 0.00050388 rank 0
2023-02-25 16:39:10,680 DEBUG TRAIN Batch 11/6700 loss 20.882402 loss_att 32.038815 loss_ctc 22.083946 loss_rnnt 18.295822 hw_loss 0.365795 lr 0.00050388 rank 1
2023-02-25 16:40:50,714 DEBUG TRAIN Batch 11/6800 loss 39.578880 loss_att 45.390087 loss_ctc 42.497307 loss_rnnt 37.911617 hw_loss 0.217312 lr 0.00050363 rank 1
2023-02-25 16:40:50,721 DEBUG TRAIN Batch 11/6800 loss 25.922146 loss_att 36.278091 loss_ctc 29.297977 loss_rnnt 23.268105 hw_loss 0.248892 lr 0.00050363 rank 0
2023-02-25 16:41:40,374 DEBUG TRAIN Batch 11/6900 loss 40.780266 loss_att 52.975109 loss_ctc 45.917004 loss_rnnt 37.410110 hw_loss 0.461789 lr 0.00050337 rank 0
2023-02-25 16:41:40,390 DEBUG TRAIN Batch 11/6900 loss 34.124821 loss_att 34.620476 loss_ctc 30.918171 loss_rnnt 34.348503 hw_loss 0.196389 lr 0.00050337 rank 1
2023-02-25 16:42:30,186 DEBUG TRAIN Batch 11/7000 loss 30.010296 loss_att 34.780853 loss_ctc 35.594116 loss_rnnt 28.137753 hw_loss 0.326104 lr 0.00050312 rank 1
2023-02-25 16:42:30,207 DEBUG TRAIN Batch 11/7000 loss 30.262665 loss_att 34.660732 loss_ctc 34.886822 loss_rnnt 28.600630 hw_loss 0.311002 lr 0.00050312 rank 0
2023-02-25 16:43:19,807 DEBUG TRAIN Batch 11/7100 loss 36.692459 loss_att 50.718700 loss_ctc 36.013493 loss_rnnt 33.831730 hw_loss 0.273759 lr 0.00050286 rank 1
2023-02-25 16:43:19,811 DEBUG TRAIN Batch 11/7100 loss 24.331434 loss_att 40.482182 loss_ctc 24.795269 loss_rnnt 20.831732 hw_loss 0.389459 lr 0.00050286 rank 0
2023-02-25 16:45:05,922 DEBUG TRAIN Batch 11/7200 loss 57.535633 loss_att 61.949265 loss_ctc 61.087357 loss_rnnt 56.106590 hw_loss 0.136422 lr 0.00050261 rank 1
2023-02-25 16:45:05,926 DEBUG TRAIN Batch 11/7200 loss 30.112326 loss_att 33.013065 loss_ctc 27.726986 loss_rnnt 29.687878 hw_loss 0.304397 lr 0.00050261 rank 0
2023-02-25 16:45:55,028 DEBUG TRAIN Batch 11/7300 loss 31.481564 loss_att 38.282097 loss_ctc 33.786289 loss_rnnt 29.696735 hw_loss 0.220166 lr 0.00050235 rank 1
2023-02-25 16:45:55,038 DEBUG TRAIN Batch 11/7300 loss 31.887688 loss_att 36.503456 loss_ctc 25.700607 loss_rnnt 31.630110 hw_loss 0.298816 lr 0.00050235 rank 0
2023-02-25 16:46:44,644 DEBUG TRAIN Batch 11/7400 loss 30.029392 loss_att 41.554436 loss_ctc 28.538687 loss_rnnt 27.833817 hw_loss 0.167487 lr 0.00050210 rank 0
2023-02-25 16:46:44,646 DEBUG TRAIN Batch 11/7400 loss 38.117298 loss_att 51.994354 loss_ctc 39.017361 loss_rnnt 35.062668 hw_loss 0.298521 lr 0.00050210 rank 1
2023-02-25 16:48:24,919 DEBUG TRAIN Batch 11/7500 loss 23.319996 loss_att 26.909702 loss_ctc 21.318659 loss_rnnt 22.674477 hw_loss 0.364540 lr 0.00050185 rank 1
2023-02-25 16:48:24,938 DEBUG TRAIN Batch 11/7500 loss 30.837914 loss_att 35.970238 loss_ctc 38.791317 loss_rnnt 28.585676 hw_loss 0.309972 lr 0.00050185 rank 0
2023-02-25 16:49:13,863 DEBUG TRAIN Batch 11/7600 loss 37.905418 loss_att 40.278191 loss_ctc 42.397480 loss_rnnt 36.718170 hw_loss 0.213283 lr 0.00050160 rank 1
2023-02-25 16:49:13,863 DEBUG TRAIN Batch 11/7600 loss 24.281517 loss_att 30.303028 loss_ctc 25.631628 loss_rnnt 22.772549 hw_loss 0.233721 lr 0.00050160 rank 0
2023-02-25 16:50:03,394 DEBUG TRAIN Batch 11/7700 loss 17.656178 loss_att 16.900959 loss_ctc 19.611721 loss_rnnt 17.338026 hw_loss 0.390855 lr 0.00050134 rank 0
2023-02-25 16:50:03,406 DEBUG TRAIN Batch 11/7700 loss 19.835955 loss_att 21.228020 loss_ctc 21.116140 loss_rnnt 19.225187 hw_loss 0.303122 lr 0.00050134 rank 1
2023-02-25 16:50:52,087 DEBUG TRAIN Batch 11/7800 loss 26.562578 loss_att 33.237106 loss_ctc 29.704611 loss_rnnt 24.716293 hw_loss 0.173325 lr 0.00050109 rank 0
2023-02-25 16:50:52,090 DEBUG TRAIN Batch 11/7800 loss 32.913391 loss_att 41.234623 loss_ctc 33.289467 loss_rnnt 31.025427 hw_loss 0.325449 lr 0.00050109 rank 1
2023-02-25 16:52:26,819 DEBUG TRAIN Batch 11/7900 loss 21.472443 loss_att 34.104424 loss_ctc 20.373623 loss_rnnt 19.001041 hw_loss 0.171592 lr 0.00050084 rank 0
2023-02-25 16:52:26,836 DEBUG TRAIN Batch 11/7900 loss 19.558989 loss_att 29.168285 loss_ctc 24.512411 loss_rnnt 16.829849 hw_loss 0.275291 lr 0.00050084 rank 1
2023-02-25 16:53:15,716 DEBUG TRAIN Batch 11/8000 loss 35.000706 loss_att 37.148899 loss_ctc 39.119118 loss_rnnt 33.890457 hw_loss 0.246543 lr 0.00050059 rank 1
2023-02-25 16:53:15,741 DEBUG TRAIN Batch 11/8000 loss 32.750118 loss_att 38.162182 loss_ctc 32.395348 loss_rnnt 31.647861 hw_loss 0.125893 lr 0.00050059 rank 0
2023-02-25 16:54:04,942 DEBUG TRAIN Batch 11/8100 loss 29.576033 loss_att 41.809536 loss_ctc 36.581676 loss_rnnt 26.077620 hw_loss 0.220549 lr 0.00050034 rank 1
2023-02-25 16:54:04,950 DEBUG TRAIN Batch 11/8100 loss 14.579967 loss_att 20.895782 loss_ctc 19.453627 loss_rnnt 12.432465 hw_loss 0.439718 lr 0.00050034 rank 0
2023-02-25 16:55:41,049 DEBUG TRAIN Batch 11/8200 loss 29.065161 loss_att 34.095970 loss_ctc 29.480198 loss_rnnt 27.901676 hw_loss 0.191220 lr 0.00050009 rank 1
2023-02-25 16:55:41,053 DEBUG TRAIN Batch 11/8200 loss 21.339716 loss_att 26.610619 loss_ctc 17.510433 loss_rnnt 20.667572 hw_loss 0.241004 lr 0.00050009 rank 0
2023-02-25 16:56:29,956 DEBUG TRAIN Batch 11/8300 loss 17.634325 loss_att 19.133720 loss_ctc 18.236130 loss_rnnt 17.052183 hw_loss 0.378797 lr 0.00049984 rank 1
2023-02-25 16:56:29,962 DEBUG TRAIN Batch 11/8300 loss 21.152105 loss_att 22.162413 loss_ctc 21.610218 loss_rnnt 20.687464 hw_loss 0.377805 lr 0.00049984 rank 0
2023-02-25 16:57:19,039 DEBUG TRAIN Batch 11/8400 loss 40.356449 loss_att 50.727440 loss_ctc 39.804409 loss_rnnt 38.247414 hw_loss 0.203334 lr 0.00049959 rank 1
2023-02-25 16:57:19,042 DEBUG TRAIN Batch 11/8400 loss 18.255938 loss_att 28.754501 loss_ctc 19.903728 loss_rnnt 15.770247 hw_loss 0.311760 lr 0.00049959 rank 0
2023-02-25 16:58:08,023 DEBUG TRAIN Batch 11/8500 loss 22.865442 loss_att 32.850975 loss_ctc 23.895512 loss_rnnt 20.587591 hw_loss 0.268880 lr 0.00049934 rank 1
2023-02-25 16:58:08,029 DEBUG TRAIN Batch 11/8500 loss 30.622919 loss_att 36.642822 loss_ctc 30.772018 loss_rnnt 29.238991 hw_loss 0.300124 lr 0.00049934 rank 0
2023-02-25 16:59:50,704 DEBUG TRAIN Batch 11/8600 loss 31.518343 loss_att 47.929287 loss_ctc 34.854279 loss_rnnt 27.664249 hw_loss 0.238331 lr 0.00049909 rank 0
2023-02-25 16:59:50,706 DEBUG TRAIN Batch 11/8600 loss 21.679308 loss_att 32.283287 loss_ctc 24.091408 loss_rnnt 19.083433 hw_loss 0.287747 lr 0.00049909 rank 1
2023-02-25 17:00:39,131 DEBUG TRAIN Batch 11/8700 loss 18.954868 loss_att 29.300362 loss_ctc 19.070980 loss_rnnt 16.783165 hw_loss 0.163355 lr 0.00049884 rank 1
2023-02-25 17:00:39,148 DEBUG TRAIN Batch 11/8700 loss 28.856356 loss_att 40.343212 loss_ctc 28.304081 loss_rnnt 26.575935 hw_loss 0.106282 lr 0.00049884 rank 0
2023-02-25 17:01:28,127 DEBUG TRAIN Batch 11/8800 loss 34.603676 loss_att 39.426117 loss_ctc 41.507507 loss_rnnt 32.599522 hw_loss 0.223405 lr 0.00049859 rank 0
2023-02-25 17:01:28,130 DEBUG TRAIN Batch 11/8800 loss 29.928442 loss_att 37.590382 loss_ctc 27.257208 loss_rnnt 28.600155 hw_loss 0.285120 lr 0.00049859 rank 1
2023-02-25 17:03:34,943 DEBUG TRAIN Batch 11/8900 loss 17.269653 loss_att 18.891644 loss_ctc 20.293282 loss_rnnt 16.363310 hw_loss 0.335239 lr 0.00049835 rank 1
2023-02-25 17:03:34,953 DEBUG TRAIN Batch 11/8900 loss 47.048275 loss_att 47.260502 loss_ctc 58.449242 loss_rnnt 45.345314 hw_loss 0.263218 lr 0.00049835 rank 0
2023-02-25 17:04:24,169 DEBUG TRAIN Batch 11/9000 loss 20.510656 loss_att 29.282284 loss_ctc 29.076042 loss_rnnt 17.441931 hw_loss 0.323159 lr 0.00049810 rank 0
2023-02-25 17:04:24,193 DEBUG TRAIN Batch 11/9000 loss 22.917345 loss_att 25.461666 loss_ctc 25.685072 loss_rnnt 21.865387 hw_loss 0.326366 lr 0.00049810 rank 1
2023-02-25 17:05:12,170 DEBUG TRAIN Batch 11/9100 loss 28.884989 loss_att 30.448338 loss_ctc 30.809021 loss_rnnt 28.210028 hw_loss 0.198289 lr 0.00049785 rank 0
2023-02-25 17:05:12,171 DEBUG TRAIN Batch 11/9100 loss 20.044302 loss_att 27.193470 loss_ctc 23.986938 loss_rnnt 17.974365 hw_loss 0.214534 lr 0.00049785 rank 1
2023-02-25 17:06:01,269 DEBUG TRAIN Batch 11/9200 loss 33.248211 loss_att 43.134834 loss_ctc 35.816696 loss_rnnt 30.805304 hw_loss 0.230849 lr 0.00049760 rank 0
2023-02-25 17:06:01,317 DEBUG TRAIN Batch 11/9200 loss 19.812141 loss_att 20.573000 loss_ctc 20.435682 loss_rnnt 19.360907 hw_loss 0.404860 lr 0.00049760 rank 1
2023-02-25 17:07:46,741 DEBUG TRAIN Batch 11/9300 loss 24.860830 loss_att 35.688545 loss_ctc 26.739082 loss_rnnt 22.313850 hw_loss 0.245635 lr 0.00049736 rank 1
2023-02-25 17:07:46,764 DEBUG TRAIN Batch 11/9300 loss 21.603447 loss_att 29.584782 loss_ctc 22.419518 loss_rnnt 19.756226 hw_loss 0.266523 lr 0.00049736 rank 0
2023-02-25 17:08:35,254 DEBUG TRAIN Batch 11/9400 loss 32.603371 loss_att 42.690300 loss_ctc 32.567097 loss_rnnt 30.431530 hw_loss 0.298675 lr 0.00049711 rank 1
2023-02-25 17:08:35,296 DEBUG TRAIN Batch 11/9400 loss 43.189335 loss_att 49.522083 loss_ctc 47.606621 loss_rnnt 41.208908 hw_loss 0.234189 lr 0.00049711 rank 0
2023-02-25 17:09:24,494 DEBUG TRAIN Batch 11/9500 loss 31.213284 loss_att 38.386082 loss_ctc 36.144852 loss_rnnt 28.978609 hw_loss 0.267325 lr 0.00049687 rank 0
2023-02-25 17:09:24,499 DEBUG TRAIN Batch 11/9500 loss 26.419643 loss_att 27.389263 loss_ctc 28.767029 loss_rnnt 25.793013 hw_loss 0.224475 lr 0.00049687 rank 1
2023-02-25 17:11:00,519 DEBUG TRAIN Batch 11/9600 loss 17.262766 loss_att 17.749218 loss_ctc 17.949347 loss_rnnt 16.731752 hw_loss 0.641586 lr 0.00049662 rank 0
2023-02-25 17:11:00,524 DEBUG TRAIN Batch 11/9600 loss 41.154903 loss_att 54.458435 loss_ctc 43.464409 loss_rnnt 38.021942 hw_loss 0.308102 lr 0.00049662 rank 1
2023-02-25 17:11:49,107 DEBUG TRAIN Batch 11/9700 loss 36.023834 loss_att 46.692829 loss_ctc 38.160259 loss_rnnt 33.528336 hw_loss 0.144080 lr 0.00049638 rank 0
2023-02-25 17:11:49,110 DEBUG TRAIN Batch 11/9700 loss 36.076492 loss_att 47.825413 loss_ctc 42.388004 loss_rnnt 32.786987 hw_loss 0.184099 lr 0.00049638 rank 1
2023-02-25 17:12:37,800 DEBUG TRAIN Batch 11/9800 loss 40.530975 loss_att 41.280243 loss_ctc 37.468327 loss_rnnt 40.704662 hw_loss 0.159025 lr 0.00049613 rank 1
2023-02-25 17:12:37,820 DEBUG TRAIN Batch 11/9800 loss 38.940163 loss_att 48.083145 loss_ctc 42.620480 loss_rnnt 36.531113 hw_loss 0.168262 lr 0.00049613 rank 0
2023-02-25 17:13:26,970 DEBUG TRAIN Batch 11/9900 loss 26.105762 loss_att 33.699001 loss_ctc 28.997314 loss_rnnt 24.067728 hw_loss 0.250961 lr 0.00049589 rank 0
2023-02-25 17:13:26,972 DEBUG TRAIN Batch 11/9900 loss 19.051355 loss_att 33.178860 loss_ctc 22.082558 loss_rnnt 15.714050 hw_loss 0.201829 lr 0.00049589 rank 1
2023-02-25 17:15:09,421 DEBUG TRAIN Batch 11/10000 loss 19.168205 loss_att 29.348192 loss_ctc 22.482716 loss_rnnt 16.545990 hw_loss 0.270527 lr 0.00049565 rank 0
2023-02-25 17:15:09,426 DEBUG TRAIN Batch 11/10000 loss 27.339891 loss_att 30.552940 loss_ctc 25.553532 loss_rnnt 26.759819 hw_loss 0.329331 lr 0.00049565 rank 1
2023-02-25 17:15:58,090 DEBUG TRAIN Batch 11/10100 loss 20.913780 loss_att 26.915028 loss_ctc 21.081608 loss_rnnt 19.556822 hw_loss 0.251874 lr 0.00049540 rank 1
2023-02-25 17:15:58,123 DEBUG TRAIN Batch 11/10100 loss 38.649975 loss_att 41.612331 loss_ctc 49.571854 loss_rnnt 36.481335 hw_loss 0.224849 lr 0.00049540 rank 0
2023-02-25 17:16:47,257 DEBUG TRAIN Batch 11/10200 loss 18.353319 loss_att 20.919346 loss_ctc 20.412237 loss_rnnt 17.448462 hw_loss 0.219620 lr 0.00049516 rank 1
2023-02-25 17:16:47,271 DEBUG TRAIN Batch 11/10200 loss 18.597382 loss_att 23.075388 loss_ctc 20.628000 loss_rnnt 17.300098 hw_loss 0.245498 lr 0.00049516 rank 0
2023-02-25 17:17:36,770 DEBUG TRAIN Batch 11/10300 loss 21.241953 loss_att 25.601974 loss_ctc 19.292900 loss_rnnt 20.558109 hw_loss 0.134465 lr 0.00049492 rank 1
2023-02-25 17:17:36,771 DEBUG TRAIN Batch 11/10300 loss 25.254738 loss_att 31.956982 loss_ctc 20.126005 loss_rnnt 24.498638 hw_loss 0.186531 lr 0.00049492 rank 0
2023-02-25 17:19:14,452 DEBUG TRAIN Batch 11/10400 loss 25.962093 loss_att 33.025024 loss_ctc 25.189505 loss_rnnt 24.528973 hw_loss 0.231649 lr 0.00049467 rank 0
2023-02-25 17:19:14,463 DEBUG TRAIN Batch 11/10400 loss 29.082130 loss_att 31.274370 loss_ctc 30.748816 loss_rnnt 28.294022 hw_loss 0.238937 lr 0.00049467 rank 1
2023-02-25 17:20:03,276 DEBUG TRAIN Batch 11/10500 loss 23.495388 loss_att 28.175865 loss_ctc 23.846542 loss_rnnt 22.353489 hw_loss 0.298091 lr 0.00049443 rank 0
2023-02-25 17:20:03,291 DEBUG TRAIN Batch 11/10500 loss 26.435234 loss_att 28.303801 loss_ctc 32.238323 loss_rnnt 25.139639 hw_loss 0.277758 lr 0.00049443 rank 1
2023-02-25 17:20:52,638 DEBUG TRAIN Batch 11/10600 loss 42.076096 loss_att 46.247070 loss_ctc 53.620007 loss_rnnt 39.564491 hw_loss 0.259160 lr 0.00049419 rank 1
2023-02-25 17:20:52,644 DEBUG TRAIN Batch 11/10600 loss 37.735954 loss_att 60.208405 loss_ctc 43.621513 loss_rnnt 32.322029 hw_loss 0.252555 lr 0.00049419 rank 0
2023-02-25 17:22:34,522 DEBUG TRAIN Batch 11/10700 loss 27.400459 loss_att 31.632610 loss_ctc 32.634331 loss_rnnt 25.805023 hw_loss 0.095915 lr 0.00049395 rank 1
2023-02-25 17:22:34,541 DEBUG TRAIN Batch 11/10700 loss 24.883991 loss_att 30.290922 loss_ctc 32.737701 loss_rnnt 22.593332 hw_loss 0.303959 lr 0.00049395 rank 0
2023-02-25 17:23:24,264 DEBUG TRAIN Batch 11/10800 loss 21.681454 loss_att 24.982296 loss_ctc 25.896408 loss_rnnt 20.266508 hw_loss 0.361467 lr 0.00049371 rank 0
2023-02-25 17:23:24,271 DEBUG TRAIN Batch 11/10800 loss 37.844368 loss_att 40.090469 loss_ctc 38.037643 loss_rnnt 37.214249 hw_loss 0.290866 lr 0.00049371 rank 1
2023-02-25 17:24:13,748 DEBUG TRAIN Batch 11/10900 loss 36.025475 loss_att 51.488560 loss_ctc 39.991207 loss_rnnt 32.275719 hw_loss 0.240700 lr 0.00049347 rank 1
2023-02-25 17:24:13,753 DEBUG TRAIN Batch 11/10900 loss 29.676720 loss_att 35.934952 loss_ctc 34.268425 loss_rnnt 27.692730 hw_loss 0.225218 lr 0.00049347 rank 0
2023-02-25 17:25:02,631 DEBUG TRAIN Batch 11/11000 loss 50.312424 loss_att 63.991165 loss_ctc 50.708755 loss_rnnt 47.413841 hw_loss 0.206232 lr 0.00049323 rank 0
2023-02-25 17:25:02,657 DEBUG TRAIN Batch 11/11000 loss 37.247997 loss_att 46.252464 loss_ctc 41.495411 loss_rnnt 34.761063 hw_loss 0.224469 lr 0.00049323 rank 1
2023-02-25 17:26:59,895 DEBUG TRAIN Batch 11/11100 loss 30.523043 loss_att 28.796688 loss_ctc 30.099579 loss_rnnt 30.832235 hw_loss 0.173509 lr 0.00049299 rank 1
2023-02-25 17:26:59,924 DEBUG TRAIN Batch 11/11100 loss 20.065403 loss_att 30.031719 loss_ctc 21.322420 loss_rnnt 17.760902 hw_loss 0.269319 lr 0.00049299 rank 0
2023-02-25 17:27:47,821 DEBUG TRAIN Batch 11/11200 loss 37.950829 loss_att 50.947296 loss_ctc 34.466015 loss_rnnt 35.743462 hw_loss 0.136333 lr 0.00049275 rank 1
2023-02-25 17:27:47,835 DEBUG TRAIN Batch 11/11200 loss 21.825768 loss_att 28.218399 loss_ctc 26.384075 loss_rnnt 19.842436 hw_loss 0.181938 lr 0.00049275 rank 0
2023-02-25 17:28:35,840 DEBUG TRAIN Batch 11/11300 loss 35.986092 loss_att 39.935501 loss_ctc 43.522968 loss_rnnt 34.053974 hw_loss 0.257475 lr 0.00049251 rank 0
2023-02-25 17:28:35,846 DEBUG TRAIN Batch 11/11300 loss 42.799175 loss_att 48.842873 loss_ctc 49.555695 loss_rnnt 40.586609 hw_loss 0.193044 lr 0.00049251 rank 1
2023-02-25 17:30:11,386 DEBUG TRAIN Batch 11/11400 loss 19.517004 loss_att 21.686106 loss_ctc 21.188406 loss_rnnt 18.681616 hw_loss 0.335088 lr 0.00049227 rank 1
2023-02-25 17:30:11,394 DEBUG TRAIN Batch 11/11400 loss 26.778938 loss_att 35.791836 loss_ctc 31.261234 loss_rnnt 24.229404 hw_loss 0.279962 lr 0.00049227 rank 0
2023-02-25 17:31:00,400 DEBUG TRAIN Batch 11/11500 loss 24.607889 loss_att 25.596020 loss_ctc 27.284330 loss_rnnt 23.924706 hw_loss 0.241309 lr 0.00049203 rank 1
2023-02-25 17:31:00,406 DEBUG TRAIN Batch 11/11500 loss 19.170158 loss_att 16.852888 loss_ctc 20.968624 loss_rnnt 19.224592 hw_loss 0.317295 lr 0.00049203 rank 0
2023-02-25 17:31:49,228 DEBUG TRAIN Batch 11/11600 loss 22.701895 loss_att 30.648176 loss_ctc 25.968548 loss_rnnt 20.518120 hw_loss 0.298054 lr 0.00049179 rank 1
2023-02-25 17:31:49,277 DEBUG TRAIN Batch 11/11600 loss 19.474512 loss_att 21.055166 loss_ctc 19.977741 loss_rnnt 19.002865 hw_loss 0.165780 lr 0.00049179 rank 0
2023-02-25 17:32:38,053 DEBUG TRAIN Batch 11/11700 loss 20.278877 loss_att 26.838634 loss_ctc 21.299099 loss_rnnt 18.788475 hw_loss 0.079541 lr 0.00049156 rank 1
2023-02-25 17:32:38,078 DEBUG TRAIN Batch 11/11700 loss 30.719900 loss_att 38.992615 loss_ctc 34.328545 loss_rnnt 28.409439 hw_loss 0.327689 lr 0.00049156 rank 0
2023-02-25 17:34:10,803 DEBUG TRAIN Batch 11/11800 loss 39.241997 loss_att 39.279457 loss_ctc 43.523796 loss_rnnt 38.470963 hw_loss 0.361199 lr 0.00049132 rank 1
2023-02-25 17:34:10,807 DEBUG TRAIN Batch 11/11800 loss 25.141750 loss_att 31.284489 loss_ctc 25.564009 loss_rnnt 23.689980 hw_loss 0.312975 lr 0.00049132 rank 0
2023-02-25 17:34:59,669 DEBUG TRAIN Batch 11/11900 loss 25.977560 loss_att 29.301811 loss_ctc 28.843401 loss_rnnt 24.757465 hw_loss 0.324623 lr 0.00049108 rank 1
2023-02-25 17:34:59,674 DEBUG TRAIN Batch 11/11900 loss 25.539696 loss_att 35.216988 loss_ctc 28.907093 loss_rnnt 22.879375 hw_loss 0.517268 lr 0.00049108 rank 0
2023-02-25 17:35:49,122 DEBUG TRAIN Batch 11/12000 loss 17.593189 loss_att 29.424324 loss_ctc 24.769882 loss_rnnt 14.159186 hw_loss 0.207906 lr 0.00049085 rank 0
2023-02-25 17:35:49,154 DEBUG TRAIN Batch 11/12000 loss 17.707479 loss_att 27.253044 loss_ctc 18.277651 loss_rnnt 15.620327 hw_loss 0.191277 lr 0.00049085 rank 1
2023-02-25 17:37:26,441 DEBUG TRAIN Batch 11/12100 loss 32.942101 loss_att 29.590883 loss_ctc 32.768135 loss_rnnt 33.502605 hw_loss 0.249256 lr 0.00049061 rank 0
2023-02-25 17:37:26,449 DEBUG TRAIN Batch 11/12100 loss 16.040850 loss_att 15.707877 loss_ctc 15.615852 loss_rnnt 15.892409 hw_loss 0.509436 lr 0.00049061 rank 1
2023-02-25 17:38:15,742 DEBUG TRAIN Batch 11/12200 loss 20.521700 loss_att 22.401955 loss_ctc 22.729294 loss_rnnt 19.688223 hw_loss 0.305772 lr 0.00049037 rank 0
2023-02-25 17:38:15,759 DEBUG TRAIN Batch 11/12200 loss 8.555082 loss_att 12.393776 loss_ctc 7.545104 loss_rnnt 7.744623 hw_loss 0.332597 lr 0.00049037 rank 1
2023-02-25 17:39:05,039 DEBUG TRAIN Batch 11/12300 loss 43.952827 loss_att 52.013283 loss_ctc 52.520248 loss_rnnt 41.033810 hw_loss 0.308631 lr 0.00049014 rank 1
2023-02-25 17:39:05,056 DEBUG TRAIN Batch 11/12300 loss 31.938278 loss_att 33.878365 loss_ctc 37.826477 loss_rnnt 30.606655 hw_loss 0.297213 lr 0.00049014 rank 0
2023-02-25 17:39:54,710 DEBUG TRAIN Batch 11/12400 loss 29.075468 loss_att 33.883724 loss_ctc 23.818863 loss_rnnt 28.703873 hw_loss 0.207800 lr 0.00048990 rank 0
2023-02-25 17:39:54,713 DEBUG TRAIN Batch 11/12400 loss 24.100880 loss_att 34.324783 loss_ctc 22.400833 loss_rnnt 22.108881 hw_loss 0.326040 lr 0.00048990 rank 1
2023-02-25 17:41:46,335 DEBUG TRAIN Batch 11/12500 loss 27.963266 loss_att 35.779762 loss_ctc 32.503895 loss_rnnt 25.656454 hw_loss 0.258930 lr 0.00048967 rank 1
2023-02-25 17:41:46,374 DEBUG TRAIN Batch 11/12500 loss 20.480934 loss_att 32.665939 loss_ctc 23.111521 loss_rnnt 17.571344 hw_loss 0.228451 lr 0.00048967 rank 0
2023-02-25 17:42:35,730 DEBUG TRAIN Batch 11/12600 loss 19.536285 loss_att 27.181564 loss_ctc 20.864006 loss_rnnt 17.666790 hw_loss 0.306390 lr 0.00048943 rank 0
2023-02-25 17:42:35,734 DEBUG TRAIN Batch 11/12600 loss 58.456757 loss_att 57.877277 loss_ctc 62.571335 loss_rnnt 57.910908 hw_loss 0.212129 lr 0.00048943 rank 1
2023-02-25 17:43:26,109 DEBUG TRAIN Batch 11/12700 loss 16.304556 loss_att 23.112755 loss_ctc 21.245731 loss_rnnt 14.166143 hw_loss 0.221156 lr 0.00048920 rank 0
2023-02-25 17:43:26,111 DEBUG TRAIN Batch 11/12700 loss 26.477236 loss_att 28.320526 loss_ctc 34.324474 loss_rnnt 24.868509 hw_loss 0.363318 lr 0.00048920 rank 1
2023-02-25 17:45:03,914 DEBUG TRAIN Batch 11/12800 loss 20.385271 loss_att 29.388660 loss_ctc 17.194834 loss_rnnt 18.901999 hw_loss 0.202473 lr 0.00048896 rank 1
2023-02-25 17:45:03,924 DEBUG TRAIN Batch 11/12800 loss 18.774227 loss_att 20.640415 loss_ctc 19.066742 loss_rnnt 18.177317 hw_loss 0.346256 lr 0.00048896 rank 0
2023-02-25 17:46:05,631 DEBUG TRAIN Batch 11/12900 loss 13.052813 loss_att 15.813988 loss_ctc 13.368446 loss_rnnt 12.396304 hw_loss 0.116606 lr 0.00048873 rank 0
2023-02-25 17:46:05,640 DEBUG TRAIN Batch 11/12900 loss 28.815536 loss_att 36.359619 loss_ctc 35.424419 loss_rnnt 26.339506 hw_loss 0.161306 lr 0.00048873 rank 1
2023-02-25 17:46:57,473 DEBUG TRAIN Batch 11/13000 loss 12.835109 loss_att 24.800444 loss_ctc 15.221907 loss_rnnt 9.951155 hw_loss 0.323713 lr 0.00048850 rank 0
2023-02-25 17:46:57,476 DEBUG TRAIN Batch 11/13000 loss 21.806826 loss_att 23.921164 loss_ctc 26.207708 loss_rnnt 20.634922 hw_loss 0.304220 lr 0.00048850 rank 1
2023-02-25 17:47:48,956 DEBUG TRAIN Batch 11/13100 loss 18.042376 loss_att 27.357893 loss_ctc 21.651878 loss_rnnt 15.589231 hw_loss 0.203952 lr 0.00048826 rank 1
2023-02-25 17:47:48,969 DEBUG TRAIN Batch 11/13100 loss 25.926640 loss_att 30.090900 loss_ctc 29.382559 loss_rnnt 24.466063 hw_loss 0.313006 lr 0.00048826 rank 0
2023-02-25 17:49:23,859 DEBUG TRAIN Batch 11/13200 loss 32.173241 loss_att 43.318623 loss_ctc 36.607986 loss_rnnt 29.235788 hw_loss 0.219519 lr 0.00048803 rank 0
2023-02-25 17:49:23,869 DEBUG TRAIN Batch 11/13200 loss 16.259708 loss_att 22.237629 loss_ctc 19.693953 loss_rnnt 14.554934 hw_loss 0.096166 lr 0.00048803 rank 1
2023-02-25 17:50:17,713 DEBUG TRAIN Batch 11/13300 loss 36.735279 loss_att 39.573666 loss_ctc 44.707069 loss_rnnt 35.036301 hw_loss 0.128239 lr 0.00048780 rank 0
2023-02-25 17:50:17,724 DEBUG TRAIN Batch 11/13300 loss 27.224409 loss_att 32.242081 loss_ctc 31.539410 loss_rnnt 25.566208 hw_loss 0.148749 lr 0.00048780 rank 1
2023-02-25 17:51:11,145 DEBUG TRAIN Batch 11/13400 loss 19.191700 loss_att 21.058355 loss_ctc 21.154787 loss_rnnt 18.442877 hw_loss 0.213278 lr 0.00048757 rank 0
2023-02-25 17:51:11,151 DEBUG TRAIN Batch 11/13400 loss 13.753207 loss_att 13.234947 loss_ctc 14.573748 loss_rnnt 13.478848 hw_loss 0.503637 lr 0.00048757 rank 1
2023-02-25 17:52:51,352 DEBUG TRAIN Batch 11/13500 loss 43.419476 loss_att 45.930580 loss_ctc 42.998878 loss_rnnt 42.845013 hw_loss 0.240601 lr 0.00048734 rank 1
2023-02-25 17:52:51,375 DEBUG TRAIN Batch 11/13500 loss 21.544130 loss_att 28.845161 loss_ctc 23.324120 loss_rnnt 19.744175 hw_loss 0.192032 lr 0.00048734 rank 0
2023-02-25 17:53:46,132 DEBUG TRAIN Batch 11/13600 loss 15.197109 loss_att 18.279270 loss_ctc 14.764242 loss_rnnt 14.553198 hw_loss 0.159741 lr 0.00048710 rank 1
2023-02-25 17:53:46,156 DEBUG TRAIN Batch 11/13600 loss 26.039335 loss_att 24.982285 loss_ctc 29.035513 loss_rnnt 25.607033 hw_loss 0.457915 lr 0.00048710 rank 0
2023-02-25 17:54:40,454 DEBUG TRAIN Batch 11/13700 loss 20.578407 loss_att 20.379028 loss_ctc 18.638462 loss_rnnt 20.706234 hw_loss 0.320077 lr 0.00048687 rank 0
2023-02-25 17:54:40,469 DEBUG TRAIN Batch 11/13700 loss 24.720409 loss_att 26.063957 loss_ctc 26.397392 loss_rnnt 24.125963 hw_loss 0.191508 lr 0.00048687 rank 1
2023-02-25 17:55:33,997 DEBUG TRAIN Batch 11/13800 loss 26.583050 loss_att 28.577606 loss_ctc 36.492180 loss_rnnt 24.778400 hw_loss 0.158471 lr 0.00048664 rank 1
2023-02-25 17:55:34,020 DEBUG TRAIN Batch 11/13800 loss 16.857826 loss_att 25.386429 loss_ctc 18.797993 loss_rnnt 14.821323 hw_loss 0.135172 lr 0.00048664 rank 0
2023-02-25 17:57:07,544 DEBUG TRAIN Batch 11/13900 loss 40.965508 loss_att 46.848507 loss_ctc 50.861816 loss_rnnt 38.332848 hw_loss 0.256040 lr 0.00048641 rank 0
2023-02-25 17:57:07,557 DEBUG TRAIN Batch 11/13900 loss 22.613882 loss_att 27.533644 loss_ctc 28.356310 loss_rnnt 20.747246 hw_loss 0.219427 lr 0.00048641 rank 1
2023-02-25 17:58:01,313 DEBUG TRAIN Batch 11/14000 loss 27.574848 loss_att 26.689667 loss_ctc 31.204042 loss_rnnt 27.105543 hw_loss 0.304591 lr 0.00048618 rank 1
2023-02-25 17:58:01,321 DEBUG TRAIN Batch 11/14000 loss 31.880610 loss_att 32.735443 loss_ctc 39.973808 loss_rnnt 30.476814 hw_loss 0.288254 lr 0.00048618 rank 0
2023-02-25 17:58:56,044 DEBUG TRAIN Batch 11/14100 loss 18.219086 loss_att 24.961718 loss_ctc 22.519562 loss_rnnt 16.154709 hw_loss 0.267099 lr 0.00048595 rank 1
2023-02-25 17:58:56,093 DEBUG TRAIN Batch 11/14100 loss 23.783880 loss_att 26.487423 loss_ctc 26.290640 loss_rnnt 22.745968 hw_loss 0.305567 lr 0.00048595 rank 0
2023-02-25 17:59:50,173 DEBUG TRAIN Batch 11/14200 loss 8.762260 loss_att 14.676693 loss_ctc 9.765039 loss_rnnt 7.254150 hw_loss 0.359098 lr 0.00048572 rank 1
2023-02-25 17:59:50,194 DEBUG TRAIN Batch 11/14200 loss 22.973402 loss_att 35.162857 loss_ctc 24.407309 loss_rnnt 20.235729 hw_loss 0.203616 lr 0.00048572 rank 0
2023-02-25 18:01:23,816 DEBUG TRAIN Batch 11/14300 loss 27.936897 loss_att 35.713352 loss_ctc 29.101400 loss_rnnt 26.129736 hw_loss 0.181129 lr 0.00048549 rank 1
2023-02-25 18:01:23,832 DEBUG TRAIN Batch 11/14300 loss 20.510197 loss_att 26.450352 loss_ctc 21.273815 loss_rnnt 19.103958 hw_loss 0.218235 lr 0.00048549 rank 0
2023-02-25 18:02:16,916 DEBUG TRAIN Batch 11/14400 loss 22.070292 loss_att 28.632160 loss_ctc 27.942554 loss_rnnt 19.830400 hw_loss 0.271023 lr 0.00048527 rank 0
2023-02-25 18:02:16,939 DEBUG TRAIN Batch 11/14400 loss 20.710020 loss_att 28.754620 loss_ctc 27.361242 loss_rnnt 18.089638 hw_loss 0.233683 lr 0.00048527 rank 1
2023-02-25 18:03:10,204 DEBUG TRAIN Batch 11/14500 loss 32.022030 loss_att 38.574024 loss_ctc 31.597733 loss_rnnt 30.648117 hw_loss 0.225164 lr 0.00048504 rank 1
2023-02-25 18:03:10,206 DEBUG TRAIN Batch 11/14500 loss 19.260738 loss_att 22.121225 loss_ctc 21.051088 loss_rnnt 18.335663 hw_loss 0.214241 lr 0.00048504 rank 0
2023-02-25 18:05:07,735 DEBUG TRAIN Batch 11/14600 loss 17.580940 loss_att 25.101501 loss_ctc 20.653332 loss_rnnt 15.507875 hw_loss 0.298687 lr 0.00048481 rank 0
2023-02-25 18:05:07,739 DEBUG TRAIN Batch 11/14600 loss 24.504129 loss_att 32.536369 loss_ctc 25.932783 loss_rnnt 22.572933 hw_loss 0.251742 lr 0.00048481 rank 1
2023-02-25 18:06:01,029 DEBUG TRAIN Batch 11/14700 loss 21.075201 loss_att 20.646837 loss_ctc 24.901743 loss_rnnt 20.409229 hw_loss 0.452701 lr 0.00048458 rank 0
2023-02-25 18:06:01,044 DEBUG TRAIN Batch 11/14700 loss 14.157255 loss_att 14.175817 loss_ctc 15.559502 loss_rnnt 13.851313 hw_loss 0.216118 lr 0.00048458 rank 1
2023-02-25 18:06:54,621 DEBUG TRAIN Batch 11/14800 loss 27.307299 loss_att 40.680466 loss_ctc 29.324850 loss_rnnt 24.206196 hw_loss 0.295240 lr 0.00048435 rank 1
2023-02-25 18:06:54,625 DEBUG TRAIN Batch 11/14800 loss 21.770958 loss_att 31.272505 loss_ctc 27.552542 loss_rnnt 18.908453 hw_loss 0.358722 lr 0.00048435 rank 0
2023-02-25 18:07:48,528 DEBUG TRAIN Batch 11/14900 loss 33.308083 loss_att 39.846653 loss_ctc 40.584061 loss_rnnt 30.967773 hw_loss 0.117122 lr 0.00048413 rank 1
2023-02-25 18:07:48,531 DEBUG TRAIN Batch 11/14900 loss 15.918097 loss_att 25.381275 loss_ctc 21.543171 loss_rnnt 13.164721 hw_loss 0.207619 lr 0.00048413 rank 0
2023-02-25 18:09:21,317 DEBUG TRAIN Batch 11/15000 loss 24.663303 loss_att 31.176003 loss_ctc 27.294210 loss_rnnt 22.913843 hw_loss 0.180243 lr 0.00048390 rank 1
2023-02-25 18:09:21,325 DEBUG TRAIN Batch 11/15000 loss 9.804796 loss_att 14.328035 loss_ctc 9.809550 loss_rnnt 8.795145 hw_loss 0.195691 lr 0.00048390 rank 0
2023-02-25 18:10:14,144 DEBUG TRAIN Batch 11/15100 loss 45.537663 loss_att 51.512341 loss_ctc 46.123142 loss_rnnt 44.110584 hw_loss 0.288898 lr 0.00048367 rank 1
2023-02-25 18:10:14,145 DEBUG TRAIN Batch 11/15100 loss 16.621124 loss_att 24.872372 loss_ctc 22.881830 loss_rnnt 14.051657 hw_loss 0.158358 lr 0.00048367 rank 0
2023-02-25 18:11:06,828 DEBUG TRAIN Batch 11/15200 loss 26.641390 loss_att 29.232651 loss_ctc 28.523834 loss_rnnt 25.681511 hw_loss 0.357440 lr 0.00048345 rank 1
2023-02-25 18:11:06,837 DEBUG TRAIN Batch 11/15200 loss 16.974344 loss_att 19.198509 loss_ctc 19.168182 loss_rnnt 16.128502 hw_loss 0.203433 lr 0.00048345 rank 0
2023-02-25 18:12:41,867 DEBUG TRAIN Batch 11/15300 loss 23.579521 loss_att 26.855089 loss_ctc 24.003080 loss_rnnt 22.691317 hw_loss 0.331153 lr 0.00048322 rank 1
2023-02-25 18:12:41,873 DEBUG TRAIN Batch 11/15300 loss 25.441267 loss_att 27.689594 loss_ctc 29.177292 loss_rnnt 24.355427 hw_loss 0.258821 lr 0.00048322 rank 0
2023-02-25 18:13:35,104 DEBUG TRAIN Batch 11/15400 loss 16.294346 loss_att 29.053270 loss_ctc 14.821094 loss_rnnt 13.780242 hw_loss 0.297661 lr 0.00048300 rank 0
2023-02-25 18:13:35,123 DEBUG TRAIN Batch 11/15400 loss 13.176035 loss_att 11.751665 loss_ctc 14.545861 loss_rnnt 12.930099 hw_loss 0.652813 lr 0.00048300 rank 1
2023-02-25 18:14:28,010 DEBUG TRAIN Batch 11/15500 loss 19.479057 loss_att 25.620066 loss_ctc 23.438040 loss_rnnt 17.598776 hw_loss 0.232905 lr 0.00048277 rank 1
2023-02-25 18:14:28,019 DEBUG TRAIN Batch 11/15500 loss 14.168973 loss_att 20.195095 loss_ctc 16.062771 loss_rnnt 12.641195 hw_loss 0.131338 lr 0.00048277 rank 0
2023-02-25 18:15:21,604 DEBUG TRAIN Batch 11/15600 loss 19.557743 loss_att 22.296318 loss_ctc 22.842932 loss_rnnt 18.439762 hw_loss 0.247948 lr 0.00048255 rank 0
2023-02-25 18:15:21,607 DEBUG TRAIN Batch 11/15600 loss 20.604229 loss_att 29.053282 loss_ctc 21.555265 loss_rnnt 18.553116 hw_loss 0.439680 lr 0.00048255 rank 1
2023-02-25 18:17:12,990 DEBUG TRAIN Batch 11/15700 loss 37.434631 loss_att 40.052261 loss_ctc 39.686386 loss_rnnt 36.464001 hw_loss 0.275388 lr 0.00048232 rank 1
2023-02-25 18:17:12,994 DEBUG TRAIN Batch 11/15700 loss 23.416481 loss_att 26.633711 loss_ctc 34.397079 loss_rnnt 21.245758 hw_loss 0.118493 lr 0.00048232 rank 0
2023-02-25 18:18:05,393 DEBUG TRAIN Batch 11/15800 loss 13.785902 loss_att 17.331116 loss_ctc 16.067142 loss_rnnt 12.560283 hw_loss 0.398270 lr 0.00048210 rank 1
2023-02-25 18:18:05,397 DEBUG TRAIN Batch 11/15800 loss 12.920076 loss_att 17.903139 loss_ctc 14.508059 loss_rnnt 11.506055 hw_loss 0.385647 lr 0.00048210 rank 0
2023-02-25 18:18:58,507 DEBUG TRAIN Batch 11/15900 loss 35.920162 loss_att 38.387596 loss_ctc 42.561726 loss_rnnt 34.382816 hw_loss 0.296841 lr 0.00048187 rank 1
2023-02-25 18:18:58,513 DEBUG TRAIN Batch 11/15900 loss 24.652264 loss_att 26.280651 loss_ctc 24.152889 loss_rnnt 24.217152 hw_loss 0.330028 lr 0.00048187 rank 0
2023-02-25 18:20:32,976 DEBUG TRAIN Batch 11/16000 loss 27.830389 loss_att 27.703178 loss_ctc 25.861389 loss_rnnt 27.889608 hw_loss 0.428917 lr 0.00048165 rank 1
2023-02-25 18:20:33,013 DEBUG TRAIN Batch 11/16000 loss 13.483868 loss_att 18.674856 loss_ctc 15.181376 loss_rnnt 12.101515 hw_loss 0.220913 lr 0.00048165 rank 0
2023-02-25 18:21:26,373 DEBUG TRAIN Batch 11/16100 loss 25.418146 loss_att 28.858389 loss_ctc 25.675779 loss_rnnt 24.630951 hw_loss 0.121495 lr 0.00048143 rank 1
2023-02-25 18:21:26,376 DEBUG TRAIN Batch 11/16100 loss 25.568844 loss_att 30.886936 loss_ctc 30.232925 loss_rnnt 23.765736 hw_loss 0.220518 lr 0.00048143 rank 0
2023-02-25 18:22:18,766 DEBUG TRAIN Batch 11/16200 loss 16.704487 loss_att 21.418522 loss_ctc 17.620180 loss_rnnt 15.465018 hw_loss 0.327315 lr 0.00048120 rank 1
2023-02-25 18:22:18,774 DEBUG TRAIN Batch 11/16200 loss 23.747820 loss_att 31.488850 loss_ctc 24.094841 loss_rnnt 22.076752 hw_loss 0.143610 lr 0.00048120 rank 0
2023-02-25 18:23:11,699 DEBUG TRAIN Batch 11/16300 loss 19.516430 loss_att 19.853203 loss_ctc 20.697041 loss_rnnt 19.156263 hw_loss 0.253867 lr 0.00048098 rank 1
2023-02-25 18:23:11,703 DEBUG TRAIN Batch 11/16300 loss 15.849246 loss_att 19.098682 loss_ctc 13.140745 loss_rnnt 15.475313 hw_loss 0.159710 lr 0.00048098 rank 0
2023-02-25 18:24:54,775 DEBUG TRAIN Batch 11/16400 loss 17.179985 loss_att 18.944004 loss_ctc 16.364910 loss_rnnt 16.826153 hw_loss 0.205699 lr 0.00048076 rank 0
2023-02-25 18:24:54,796 DEBUG TRAIN Batch 11/16400 loss 18.959751 loss_att 22.821829 loss_ctc 21.805382 loss_rnnt 17.676125 hw_loss 0.247113 lr 0.00048076 rank 1
2023-02-25 18:25:48,156 DEBUG TRAIN Batch 11/16500 loss 11.624451 loss_att 15.158879 loss_ctc 12.848139 loss_rnnt 10.538427 hw_loss 0.404959 lr 0.00048054 rank 0
2023-02-25 18:25:48,173 DEBUG TRAIN Batch 11/16500 loss 26.244925 loss_att 31.787472 loss_ctc 28.513386 loss_rnnt 24.682444 hw_loss 0.284074 lr 0.00048054 rank 1
2023-02-25 18:26:41,329 DEBUG TRAIN Batch 11/16600 loss 21.627188 loss_att 24.270237 loss_ctc 26.333492 loss_rnnt 20.285517 hw_loss 0.347916 lr 0.00048031 rank 0
2023-02-25 18:26:41,334 DEBUG TRAIN Batch 11/16600 loss 14.693892 loss_att 18.965570 loss_ctc 14.823188 loss_rnnt 13.690017 hw_loss 0.248062 lr 0.00048031 rank 1
2023-02-25 18:27:35,261 DEBUG TRAIN Batch 11/16700 loss 29.699743 loss_att 38.768875 loss_ctc 35.757309 loss_rnnt 26.925442 hw_loss 0.286498 lr 0.00048009 rank 1
2023-02-25 18:27:35,273 DEBUG TRAIN Batch 11/16700 loss 33.937107 loss_att 41.856102 loss_ctc 34.804520 loss_rnnt 32.087688 hw_loss 0.281192 lr 0.00048009 rank 0
2023-02-25 18:29:07,702 DEBUG TRAIN Batch 11/16800 loss 32.838760 loss_att 27.598953 loss_ctc 30.332809 loss_rnnt 34.071968 hw_loss 0.279147 lr 0.00047987 rank 1
2023-02-25 18:29:07,715 DEBUG TRAIN Batch 11/16800 loss 20.109926 loss_att 24.794214 loss_ctc 20.749058 loss_rnnt 18.891460 hw_loss 0.368234 lr 0.00047987 rank 0
2023-02-25 18:30:00,556 DEBUG TRAIN Batch 11/16900 loss 18.634201 loss_att 23.978500 loss_ctc 25.250793 loss_rnnt 16.565718 hw_loss 0.220148 lr 0.00047965 rank 0
2023-02-25 18:30:00,560 DEBUG TRAIN Batch 11/16900 loss 40.975334 loss_att 40.749943 loss_ctc 41.778259 loss_rnnt 40.762856 hw_loss 0.282182 lr 0.00047965 rank 1
2023-02-25 18:30:53,131 DEBUG TRAIN Batch 11/17000 loss 23.899624 loss_att 27.033062 loss_ctc 28.320311 loss_rnnt 22.576031 hw_loss 0.201528 lr 0.00047943 rank 0
2023-02-25 18:30:53,144 DEBUG TRAIN Batch 11/17000 loss 29.772633 loss_att 35.654106 loss_ctc 31.371929 loss_rnnt 28.235723 hw_loss 0.276328 lr 0.00047943 rank 1
2023-02-25 18:32:32,286 DEBUG TRAIN Batch 11/17100 loss 18.618437 loss_att 22.022532 loss_ctc 26.758686 loss_rnnt 16.644276 hw_loss 0.389952 lr 0.00047921 rank 1
2023-02-25 18:32:32,298 DEBUG TRAIN Batch 11/17100 loss 35.998123 loss_att 39.996277 loss_ctc 38.407791 loss_rnnt 34.740051 hw_loss 0.257158 lr 0.00047921 rank 0
2023-02-25 18:33:25,803 DEBUG TRAIN Batch 11/17200 loss 13.449852 loss_att 18.523510 loss_ctc 15.507174 loss_rnnt 12.025682 hw_loss 0.253365 lr 0.00047899 rank 1
2023-02-25 18:33:25,828 DEBUG TRAIN Batch 11/17200 loss 25.920023 loss_att 25.963505 loss_ctc 33.801514 loss_rnnt 24.677343 hw_loss 0.343349 lr 0.00047899 rank 0
2023-02-25 18:34:19,507 DEBUG TRAIN Batch 11/17300 loss 23.419863 loss_att 29.352602 loss_ctc 22.024982 loss_rnnt 22.346329 hw_loss 0.136821 lr 0.00047877 rank 0
2023-02-25 18:34:19,509 DEBUG TRAIN Batch 11/17300 loss 20.112770 loss_att 20.006413 loss_ctc 22.136557 loss_rnnt 19.649736 hw_loss 0.402124 lr 0.00047877 rank 1
2023-02-25 18:35:13,824 DEBUG TRAIN Batch 11/17400 loss 33.982574 loss_att 35.698059 loss_ctc 41.576134 loss_rnnt 32.490685 hw_loss 0.255593 lr 0.00047855 rank 1
2023-02-25 18:35:13,840 DEBUG TRAIN Batch 11/17400 loss 27.725454 loss_att 31.412930 loss_ctc 31.023312 loss_rnnt 26.479496 hw_loss 0.128902 lr 0.00047855 rank 0
2023-02-25 18:37:29,151 DEBUG TRAIN Batch 11/17500 loss 20.831697 loss_att 24.606825 loss_ctc 26.819601 loss_rnnt 19.181356 hw_loss 0.181738 lr 0.00047833 rank 1
2023-02-25 18:37:29,164 DEBUG TRAIN Batch 11/17500 loss 25.770447 loss_att 33.200447 loss_ctc 32.596039 loss_rnnt 23.228989 hw_loss 0.272588 lr 0.00047833 rank 0
2023-02-25 18:38:22,239 DEBUG TRAIN Batch 11/17600 loss 39.470997 loss_att 42.920055 loss_ctc 42.315308 loss_rnnt 38.290497 hw_loss 0.208965 lr 0.00047811 rank 1
2023-02-25 18:38:22,251 DEBUG TRAIN Batch 11/17600 loss 11.517137 loss_att 17.518988 loss_ctc 15.474112 loss_rnnt 9.682870 hw_loss 0.199310 lr 0.00047811 rank 0
2023-02-25 18:39:15,643 DEBUG TRAIN Batch 11/17700 loss 20.655468 loss_att 24.593498 loss_ctc 24.456038 loss_rnnt 19.288715 hw_loss 0.135757 lr 0.00047789 rank 1
2023-02-25 18:39:15,679 DEBUG TRAIN Batch 11/17700 loss 24.361418 loss_att 24.607536 loss_ctc 23.317825 loss_rnnt 24.346487 hw_loss 0.196597 lr 0.00047789 rank 0
2023-02-25 18:40:50,752 DEBUG TRAIN Batch 11/17800 loss 20.931383 loss_att 21.771805 loss_ctc 23.160189 loss_rnnt 20.289055 hw_loss 0.332010 lr 0.00047768 rank 1
2023-02-25 18:40:50,783 DEBUG TRAIN Batch 11/17800 loss 23.932966 loss_att 26.645767 loss_ctc 31.893888 loss_rnnt 22.202246 hw_loss 0.237576 lr 0.00047768 rank 0
2023-02-25 18:41:43,812 DEBUG TRAIN Batch 11/17900 loss 22.309349 loss_att 26.932503 loss_ctc 25.303488 loss_rnnt 20.859335 hw_loss 0.236558 lr 0.00047746 rank 1
2023-02-25 18:41:43,849 DEBUG TRAIN Batch 11/17900 loss 22.491320 loss_att 32.482124 loss_ctc 22.639755 loss_rnnt 20.360022 hw_loss 0.212525 lr 0.00047746 rank 0
2023-02-25 18:42:35,664 DEBUG TRAIN Batch 11/18000 loss 18.530725 loss_att 28.378193 loss_ctc 24.918419 loss_rnnt 15.624426 hw_loss 0.159590 lr 0.00047724 rank 0
2023-02-25 18:42:35,689 DEBUG TRAIN Batch 11/18000 loss 32.816044 loss_att 34.967850 loss_ctc 39.941227 loss_rnnt 31.302435 hw_loss 0.249793 lr 0.00047724 rank 1
2023-02-25 18:43:28,386 DEBUG TRAIN Batch 11/18100 loss 10.912421 loss_att 19.308849 loss_ctc 11.974789 loss_rnnt 8.947927 hw_loss 0.269173 lr 0.00047702 rank 1
2023-02-25 18:43:28,399 DEBUG TRAIN Batch 11/18100 loss 21.659039 loss_att 23.937269 loss_ctc 24.569151 loss_rnnt 20.701490 hw_loss 0.213537 lr 0.00047702 rank 0
2023-02-25 18:45:07,893 DEBUG TRAIN Batch 11/18200 loss 19.395798 loss_att 29.680260 loss_ctc 25.813198 loss_rnnt 16.315832 hw_loss 0.313910 lr 0.00047681 rank 1
2023-02-25 18:45:07,919 DEBUG TRAIN Batch 11/18200 loss 21.932270 loss_att 24.912254 loss_ctc 21.411551 loss_rnnt 21.314007 hw_loss 0.171930 lr 0.00047681 rank 0
2023-02-25 18:46:01,943 DEBUG TRAIN Batch 11/18300 loss 30.958584 loss_att 31.674665 loss_ctc 36.594143 loss_rnnt 29.957161 hw_loss 0.200247 lr 0.00047659 rank 0
2023-02-25 18:46:01,963 DEBUG TRAIN Batch 11/18300 loss 19.228292 loss_att 21.779840 loss_ctc 21.478004 loss_rnnt 18.259096 hw_loss 0.297987 lr 0.00047659 rank 1
2023-02-25 18:46:56,112 DEBUG TRAIN Batch 11/18400 loss 24.055634 loss_att 25.719511 loss_ctc 30.618145 loss_rnnt 22.669022 hw_loss 0.335312 lr 0.00047637 rank 1
2023-02-25 18:46:56,137 DEBUG TRAIN Batch 11/18400 loss 14.695124 loss_att 17.707127 loss_ctc 17.788832 loss_rnnt 13.525192 hw_loss 0.290695 lr 0.00047637 rank 0
2023-02-25 18:48:30,808 DEBUG TRAIN Batch 11/18500 loss 20.289448 loss_att 20.361513 loss_ctc 21.158943 loss_rnnt 19.920893 hw_loss 0.446641 lr 0.00047616 rank 0
2023-02-25 18:48:30,821 DEBUG TRAIN Batch 11/18500 loss 23.933044 loss_att 26.962997 loss_ctc 21.957186 loss_rnnt 23.485128 hw_loss 0.197573 lr 0.00047616 rank 1
2023-02-25 18:49:23,349 DEBUG TRAIN Batch 11/18600 loss 15.867887 loss_att 15.593996 loss_ctc 16.712345 loss_rnnt 15.552169 hw_loss 0.483569 lr 0.00047594 rank 1
2023-02-25 18:49:23,366 DEBUG TRAIN Batch 11/18600 loss 25.084526 loss_att 33.613892 loss_ctc 30.534801 loss_rnnt 22.521236 hw_loss 0.245090 lr 0.00047594 rank 0
2023-02-25 18:50:17,028 DEBUG TRAIN Batch 11/18700 loss 19.789110 loss_att 23.233915 loss_ctc 19.409706 loss_rnnt 19.058487 hw_loss 0.172963 lr 0.00047573 rank 0
2023-02-25 18:50:17,050 DEBUG TRAIN Batch 11/18700 loss 14.007144 loss_att 19.129261 loss_ctc 13.567762 loss_rnnt 12.865063 hw_loss 0.330453 lr 0.00047573 rank 1
2023-02-25 18:51:10,783 DEBUG TRAIN Batch 11/18800 loss 13.838297 loss_att 19.023781 loss_ctc 15.310179 loss_rnnt 12.441809 hw_loss 0.305890 lr 0.00047551 rank 1
2023-02-25 18:51:10,798 DEBUG TRAIN Batch 11/18800 loss 20.741873 loss_att 22.315075 loss_ctc 25.538549 loss_rnnt 19.611153 hw_loss 0.330981 lr 0.00047551 rank 0
2023-02-25 18:52:47,718 DEBUG TRAIN Batch 11/18900 loss 9.519953 loss_att 15.551817 loss_ctc 9.126293 loss_rnnt 8.251272 hw_loss 0.215242 lr 0.00047530 rank 1
2023-02-25 18:52:47,733 DEBUG TRAIN Batch 11/18900 loss 42.878868 loss_att 45.847584 loss_ctc 41.133488 loss_rnnt 42.448498 hw_loss 0.130022 lr 0.00047530 rank 0
2023-02-25 18:53:41,047 DEBUG TRAIN Batch 11/19000 loss 20.743435 loss_att 24.120485 loss_ctc 23.501450 loss_rnnt 19.605068 hw_loss 0.178542 lr 0.00047508 rank 1
2023-02-25 18:53:41,061 DEBUG TRAIN Batch 11/19000 loss 15.982154 loss_att 18.737814 loss_ctc 18.276890 loss_rnnt 15.000390 hw_loss 0.233749 lr 0.00047508 rank 0
2023-02-25 18:54:34,236 DEBUG TRAIN Batch 11/19100 loss 21.811670 loss_att 22.308043 loss_ctc 23.159767 loss_rnnt 21.352421 hw_loss 0.337924 lr 0.00047487 rank 0
2023-02-25 18:54:34,240 DEBUG TRAIN Batch 11/19100 loss 25.668919 loss_att 31.284504 loss_ctc 28.656109 loss_rnnt 23.986732 hw_loss 0.301455 lr 0.00047487 rank 1
2023-02-25 18:56:35,840 DEBUG TRAIN Batch 11/19200 loss 32.082695 loss_att 33.593990 loss_ctc 39.541431 loss_rnnt 30.660589 hw_loss 0.235030 lr 0.00047465 rank 1
2023-02-25 18:56:35,859 DEBUG TRAIN Batch 11/19200 loss 22.523933 loss_att 33.534973 loss_ctc 24.621168 loss_rnnt 19.852039 hw_loss 0.356351 lr 0.00047465 rank 0
2023-02-25 18:57:29,458 DEBUG TRAIN Batch 11/19300 loss 48.697235 loss_att 50.432041 loss_ctc 48.745049 loss_rnnt 48.170937 hw_loss 0.324301 lr 0.00047444 rank 1
2023-02-25 18:57:29,459 DEBUG TRAIN Batch 11/19300 loss 39.107609 loss_att 52.205894 loss_ctc 48.916069 loss_rnnt 34.998150 hw_loss 0.341261 lr 0.00047444 rank 0
2023-02-25 18:58:22,104 DEBUG TRAIN Batch 11/19400 loss 34.003105 loss_att 39.303528 loss_ctc 33.985443 loss_rnnt 32.824474 hw_loss 0.226691 lr 0.00047423 rank 1
2023-02-25 18:58:22,109 DEBUG TRAIN Batch 11/19400 loss 26.453480 loss_att 30.477497 loss_ctc 28.975338 loss_rnnt 25.219765 hw_loss 0.173744 lr 0.00047423 rank 0
2023-02-25 18:59:15,652 DEBUG TRAIN Batch 11/19500 loss 16.648104 loss_att 22.154470 loss_ctc 18.309603 loss_rnnt 15.234144 hw_loss 0.170910 lr 0.00047401 rank 1
2023-02-25 18:59:15,664 DEBUG TRAIN Batch 11/19500 loss 25.075832 loss_att 30.034050 loss_ctc 32.040092 loss_rnnt 23.014214 hw_loss 0.265137 lr 0.00047401 rank 0
2023-02-25 19:00:52,775 DEBUG TRAIN Batch 11/19600 loss 19.377766 loss_att 23.071379 loss_ctc 23.831867 loss_rnnt 17.887789 hw_loss 0.295078 lr 0.00047380 rank 1
2023-02-25 19:00:52,788 DEBUG TRAIN Batch 11/19600 loss 11.769749 loss_att 14.755264 loss_ctc 11.271984 loss_rnnt 11.071876 hw_loss 0.313388 lr 0.00047380 rank 0
2023-02-25 19:01:47,247 DEBUG TRAIN Batch 11/19700 loss 20.132858 loss_att 28.120651 loss_ctc 24.539864 loss_rnnt 17.772930 hw_loss 0.327693 lr 0.00047359 rank 1
2023-02-25 19:01:47,246 DEBUG TRAIN Batch 11/19700 loss 21.790173 loss_att 26.034258 loss_ctc 20.607878 loss_rnnt 21.014631 hw_loss 0.158188 lr 0.00047359 rank 0
2023-02-25 19:02:42,695 DEBUG TRAIN Batch 11/19800 loss 10.280940 loss_att 15.587421 loss_ctc 14.033207 loss_rnnt 8.655903 hw_loss 0.118946 lr 0.00047338 rank 0
2023-02-25 19:02:42,698 DEBUG TRAIN Batch 11/19800 loss 20.321007 loss_att 20.052692 loss_ctc 23.575825 loss_rnnt 19.792643 hw_loss 0.277594 lr 0.00047338 rank 1
2023-02-25 19:03:39,036 DEBUG TRAIN Batch 11/19900 loss 38.905758 loss_att 47.342037 loss_ctc 47.843941 loss_rnnt 35.915977 hw_loss 0.207680 lr 0.00047316 rank 1
2023-02-25 19:03:39,044 DEBUG TRAIN Batch 11/19900 loss 31.257673 loss_att 35.868599 loss_ctc 35.305325 loss_rnnt 29.696537 hw_loss 0.186117 lr 0.00047316 rank 0
2023-02-25 19:05:14,419 DEBUG TRAIN Batch 11/20000 loss 12.345521 loss_att 21.476309 loss_ctc 13.028450 loss_rnnt 10.302423 hw_loss 0.236026 lr 0.00047295 rank 1
2023-02-25 19:05:14,425 DEBUG TRAIN Batch 11/20000 loss 9.352324 loss_att 13.541672 loss_ctc 12.520796 loss_rnnt 7.984813 hw_loss 0.200959 lr 0.00047295 rank 0
2023-02-25 19:06:08,773 DEBUG TRAIN Batch 11/20100 loss 27.235476 loss_att 30.355009 loss_ctc 32.080742 loss_rnnt 25.827774 hw_loss 0.258297 lr 0.00047274 rank 0
2023-02-25 19:06:08,774 DEBUG TRAIN Batch 11/20100 loss 12.683145 loss_att 17.086323 loss_ctc 23.116261 loss_rnnt 10.259934 hw_loss 0.284046 lr 0.00047274 rank 1
2023-02-25 19:07:04,488 DEBUG TRAIN Batch 11/20200 loss 33.105133 loss_att 41.819374 loss_ctc 35.965504 loss_rnnt 30.922422 hw_loss 0.109653 lr 0.00047253 rank 0
2023-02-25 19:07:04,501 DEBUG TRAIN Batch 11/20200 loss 24.474007 loss_att 29.625046 loss_ctc 25.559032 loss_rnnt 23.228289 hw_loss 0.132828 lr 0.00047253 rank 1
2023-02-25 19:08:42,315 DEBUG TRAIN Batch 11/20300 loss 15.915353 loss_att 22.115120 loss_ctc 17.882675 loss_rnnt 14.339066 hw_loss 0.138796 lr 0.00047232 rank 0
2023-02-25 19:08:42,334 DEBUG TRAIN Batch 11/20300 loss 17.995836 loss_att 20.657408 loss_ctc 22.287241 loss_rnnt 16.719650 hw_loss 0.321906 lr 0.00047232 rank 1
2023-02-25 19:09:38,681 DEBUG TRAIN Batch 11/20400 loss 22.494478 loss_att 21.940142 loss_ctc 25.305918 loss_rnnt 22.098516 hw_loss 0.247447 lr 0.00047211 rank 0
2023-02-25 19:09:38,683 DEBUG TRAIN Batch 11/20400 loss 25.053762 loss_att 23.997494 loss_ctc 26.579054 loss_rnnt 24.913467 hw_loss 0.277830 lr 0.00047211 rank 1
2023-02-25 19:10:34,505 DEBUG TRAIN Batch 11/20500 loss 14.332623 loss_att 24.036083 loss_ctc 17.513372 loss_rnnt 11.891676 hw_loss 0.142791 lr 0.00047190 rank 1
2023-02-25 19:10:34,524 DEBUG TRAIN Batch 11/20500 loss 13.995960 loss_att 15.464347 loss_ctc 13.969749 loss_rnnt 13.621811 hw_loss 0.157436 lr 0.00047190 rank 0
2023-02-25 19:11:30,042 DEBUG TRAIN Batch 11/20600 loss 22.210632 loss_att 20.821100 loss_ctc 23.196651 loss_rnnt 22.230963 hw_loss 0.236451 lr 0.00047169 rank 1
2023-02-25 19:11:30,060 DEBUG TRAIN Batch 11/20600 loss 18.000328 loss_att 23.758179 loss_ctc 21.731962 loss_rnnt 16.189154 hw_loss 0.303852 lr 0.00047169 rank 0
2023-02-25 19:13:04,912 DEBUG TRAIN Batch 11/20700 loss 6.218009 loss_att 9.950126 loss_ctc 7.464582 loss_rnnt 5.131252 hw_loss 0.326481 lr 0.00047148 rank 0
2023-02-25 19:13:04,931 DEBUG TRAIN Batch 11/20700 loss 19.357857 loss_att 25.275455 loss_ctc 22.054119 loss_rnnt 17.746750 hw_loss 0.127658 lr 0.00047148 rank 1
2023-02-25 19:13:59,609 DEBUG TRAIN Batch 11/20800 loss 18.676250 loss_att 28.640745 loss_ctc 20.890232 loss_rnnt 16.248259 hw_loss 0.262305 lr 0.00047127 rank 0
2023-02-25 19:13:59,614 DEBUG TRAIN Batch 11/20800 loss 42.571033 loss_att 43.668499 loss_ctc 45.088444 loss_rnnt 41.876938 hw_loss 0.260527 lr 0.00047127 rank 1
2023-02-25 19:14:54,755 DEBUG TRAIN Batch 11/20900 loss 29.087915 loss_att 35.181004 loss_ctc 32.658890 loss_rnnt 27.233833 hw_loss 0.298751 lr 0.00047106 rank 1
2023-02-25 19:14:54,764 DEBUG TRAIN Batch 11/20900 loss 17.173153 loss_att 20.727247 loss_ctc 21.676708 loss_rnnt 15.780674 hw_loss 0.152221 lr 0.00047106 rank 0
2023-02-25 19:16:32,018 DEBUG TRAIN Batch 11/21000 loss 18.536861 loss_att 20.551790 loss_ctc 22.527344 loss_rnnt 17.501394 hw_loss 0.188285 lr 0.00047085 rank 0
2023-02-25 19:16:32,021 DEBUG TRAIN Batch 11/21000 loss 21.991989 loss_att 26.283981 loss_ctc 21.549416 loss_rnnt 21.078278 hw_loss 0.214355 lr 0.00047085 rank 1
2023-02-25 19:17:27,341 DEBUG TRAIN Batch 11/21100 loss 17.843197 loss_att 18.694988 loss_ctc 20.768169 loss_rnnt 17.114815 hw_loss 0.315051 lr 0.00047064 rank 1
2023-02-25 19:17:27,352 DEBUG TRAIN Batch 11/21100 loss 13.486525 loss_att 12.425652 loss_ctc 14.957047 loss_rnnt 13.278608 hw_loss 0.420042 lr 0.00047064 rank 0
2023-02-25 19:18:22,685 DEBUG TRAIN Batch 11/21200 loss 18.417980 loss_att 25.992439 loss_ctc 21.937597 loss_rnnt 16.286369 hw_loss 0.276444 lr 0.00047043 rank 1
2023-02-25 19:18:22,687 DEBUG TRAIN Batch 11/21200 loss 14.783317 loss_att 18.889128 loss_ctc 13.813770 loss_rnnt 13.943969 hw_loss 0.276485 lr 0.00047043 rank 0
2023-02-25 19:19:17,959 DEBUG TRAIN Batch 11/21300 loss 19.535435 loss_att 26.101725 loss_ctc 18.058626 loss_rnnt 18.220776 hw_loss 0.371830 lr 0.00047023 rank 1
2023-02-25 19:19:17,960 DEBUG TRAIN Batch 11/21300 loss 19.688215 loss_att 20.094900 loss_ctc 20.172157 loss_rnnt 19.419489 hw_loss 0.230371 lr 0.00047023 rank 0
2023-02-25 19:21:05,205 DEBUG TRAIN Batch 11/21400 loss 19.420586 loss_att 23.570570 loss_ctc 26.351013 loss_rnnt 17.519381 hw_loss 0.275907 lr 0.00047002 rank 1
2023-02-25 19:21:05,210 DEBUG TRAIN Batch 11/21400 loss 18.416470 loss_att 27.898739 loss_ctc 25.913294 loss_rnnt 15.414473 hw_loss 0.198687 lr 0.00047002 rank 0
2023-02-25 19:21:59,365 DEBUG TRAIN Batch 11/21500 loss 23.161522 loss_att 27.401014 loss_ctc 28.479158 loss_rnnt 21.432804 hw_loss 0.322124 lr 0.00046981 rank 1
2023-02-25 19:21:59,411 DEBUG TRAIN Batch 11/21500 loss 22.429287 loss_att 25.835083 loss_ctc 24.548388 loss_rnnt 21.326557 hw_loss 0.260668 lr 0.00046981 rank 0
2023-02-25 19:22:54,541 DEBUG TRAIN Batch 11/21600 loss 18.768997 loss_att 23.754814 loss_ctc 24.216663 loss_rnnt 16.926149 hw_loss 0.223744 lr 0.00046960 rank 1
2023-02-25 19:22:54,573 DEBUG TRAIN Batch 11/21600 loss 17.322989 loss_att 19.564575 loss_ctc 20.471884 loss_rnnt 16.298483 hw_loss 0.293132 lr 0.00046960 rank 0
2023-02-25 19:24:33,772 DEBUG TRAIN Batch 11/21700 loss 25.302258 loss_att 28.662683 loss_ctc 28.954901 loss_rnnt 24.110012 hw_loss 0.062141 lr 0.00046940 rank 0
2023-02-25 19:24:33,798 DEBUG TRAIN Batch 11/21700 loss 20.797041 loss_att 18.672119 loss_ctc 22.849537 loss_rnnt 20.799906 hw_loss 0.278351 lr 0.00046940 rank 1
2023-02-25 19:25:28,711 DEBUG TRAIN Batch 11/21800 loss 16.081060 loss_att 25.900877 loss_ctc 19.391378 loss_rnnt 13.483238 hw_loss 0.360904 lr 0.00046919 rank 0
2023-02-25 19:25:28,732 DEBUG TRAIN Batch 11/21800 loss 37.196491 loss_att 39.248348 loss_ctc 48.410030 loss_rnnt 35.167488 hw_loss 0.231552 lr 0.00046919 rank 1
2023-02-25 19:26:24,444 DEBUG TRAIN Batch 11/21900 loss 9.277343 loss_att 13.332043 loss_ctc 8.616270 loss_rnnt 8.337935 hw_loss 0.406143 lr 0.00046898 rank 1
2023-02-25 19:26:24,495 DEBUG TRAIN Batch 11/21900 loss 23.247629 loss_att 26.029526 loss_ctc 24.499962 loss_rnnt 22.436253 hw_loss 0.165033 lr 0.00046898 rank 0
2023-02-25 19:27:20,992 DEBUG TRAIN Batch 11/22000 loss 36.535336 loss_att 41.760101 loss_ctc 45.433571 loss_rnnt 34.127602 hw_loss 0.330653 lr 0.00046878 rank 1
2023-02-25 19:27:20,999 DEBUG TRAIN Batch 11/22000 loss 17.497906 loss_att 23.159260 loss_ctc 19.353750 loss_rnnt 15.948202 hw_loss 0.318725 lr 0.00046878 rank 0
2023-02-25 19:28:54,582 DEBUG TRAIN Batch 11/22100 loss 17.058529 loss_att 20.899944 loss_ctc 19.944820 loss_rnnt 15.597996 hw_loss 0.576395 lr 0.00046857 rank 1
2023-02-25 19:28:54,609 DEBUG TRAIN Batch 11/22100 loss 9.970250 loss_att 14.446823 loss_ctc 11.765972 loss_rnnt 8.599718 hw_loss 0.442102 lr 0.00046857 rank 0
2023-02-25 19:29:49,354 DEBUG TRAIN Batch 11/22200 loss 16.882330 loss_att 16.536175 loss_ctc 17.013731 loss_rnnt 16.845089 hw_loss 0.166785 lr 0.00046836 rank 1
2023-02-25 19:29:49,359 DEBUG TRAIN Batch 11/22200 loss 35.455711 loss_att 37.154072 loss_ctc 41.977131 loss_rnnt 34.144215 hw_loss 0.191814 lr 0.00046836 rank 0
2023-02-25 19:30:44,335 DEBUG TRAIN Batch 11/22300 loss 13.661680 loss_att 14.805348 loss_ctc 16.444502 loss_rnnt 12.937096 hw_loss 0.234015 lr 0.00046816 rank 0
2023-02-25 19:30:44,338 DEBUG TRAIN Batch 11/22300 loss 24.933519 loss_att 24.871952 loss_ctc 27.177282 loss_rnnt 24.528597 hw_loss 0.221378 lr 0.00046816 rank 1
2023-02-25 19:32:44,242 DEBUG TRAIN Batch 11/22400 loss 17.280769 loss_att 28.620935 loss_ctc 21.094566 loss_rnnt 14.389805 hw_loss 0.214547 lr 0.00046795 rank 0
2023-02-25 19:32:44,260 DEBUG TRAIN Batch 11/22400 loss 12.806769 loss_att 11.593126 loss_ctc 14.040640 loss_rnnt 12.579616 hw_loss 0.572563 lr 0.00046795 rank 1
2023-02-25 19:33:46,218 DEBUG TRAIN Batch 11/22500 loss 11.181346 loss_att 15.506981 loss_ctc 16.726204 loss_rnnt 9.400612 hw_loss 0.330548 lr 0.00046775 rank 1
2023-02-25 19:33:46,259 DEBUG TRAIN Batch 11/22500 loss 8.447677 loss_att 11.686106 loss_ctc 9.917414 loss_rnnt 7.498498 hw_loss 0.197864 lr 0.00046775 rank 0
2023-02-25 19:34:41,275 DEBUG TRAIN Batch 11/22600 loss 23.341015 loss_att 28.809998 loss_ctc 30.501696 loss_rnnt 21.187002 hw_loss 0.197735 lr 0.00046755 rank 0
2023-02-25 19:34:41,304 DEBUG TRAIN Batch 11/22600 loss 40.290947 loss_att 48.624428 loss_ctc 43.774670 loss_rnnt 38.078926 hw_loss 0.151561 lr 0.00046755 rank 1
2023-02-25 19:35:36,338 DEBUG TRAIN Batch 11/22700 loss 14.676922 loss_att 21.022655 loss_ctc 14.722260 loss_rnnt 13.342301 hw_loss 0.111428 lr 0.00046734 rank 1
2023-02-25 19:35:36,383 DEBUG TRAIN Batch 11/22700 loss 28.708143 loss_att 36.297009 loss_ctc 35.326710 loss_rnnt 26.174297 hw_loss 0.250496 lr 0.00046734 rank 0
2023-02-25 19:37:12,512 DEBUG TRAIN Batch 11/22800 loss 14.262250 loss_att 16.394917 loss_ctc 14.871389 loss_rnnt 13.668243 hw_loss 0.161726 lr 0.00046714 rank 0
2023-02-25 19:37:12,515 DEBUG TRAIN Batch 11/22800 loss 18.500473 loss_att 20.236887 loss_ctc 21.357239 loss_rnnt 17.696503 hw_loss 0.142097 lr 0.00046714 rank 1
2023-02-25 19:38:07,407 DEBUG TRAIN Batch 11/22900 loss 21.745077 loss_att 21.379076 loss_ctc 25.093147 loss_rnnt 21.272579 hw_loss 0.186168 lr 0.00046693 rank 1
2023-02-25 19:38:07,420 DEBUG TRAIN Batch 11/22900 loss 27.752956 loss_att 33.567242 loss_ctc 33.642590 loss_rnnt 25.642162 hw_loss 0.304977 lr 0.00046693 rank 0
2023-02-25 19:39:02,946 DEBUG TRAIN Batch 11/23000 loss 19.009766 loss_att 19.764492 loss_ctc 21.103565 loss_rnnt 18.388506 hw_loss 0.358392 lr 0.00046673 rank 1
2023-02-25 19:39:02,951 DEBUG TRAIN Batch 11/23000 loss 16.783718 loss_att 24.224932 loss_ctc 18.697208 loss_rnnt 14.954852 hw_loss 0.160298 lr 0.00046673 rank 0
2023-02-25 19:39:58,913 DEBUG TRAIN Batch 11/23100 loss 19.843309 loss_att 20.339314 loss_ctc 29.870003 loss_rnnt 18.313316 hw_loss 0.176065 lr 0.00046653 rank 0
2023-02-25 19:39:58,925 DEBUG TRAIN Batch 11/23100 loss 18.343237 loss_att 25.285519 loss_ctc 22.199858 loss_rnnt 16.264753 hw_loss 0.329648 lr 0.00046653 rank 1
2023-02-25 19:41:35,117 DEBUG TRAIN Batch 11/23200 loss 12.271049 loss_att 15.132375 loss_ctc 17.158033 loss_rnnt 10.943756 hw_loss 0.193933 lr 0.00046632 rank 1
2023-02-25 19:41:35,161 DEBUG TRAIN Batch 11/23200 loss 14.156791 loss_att 19.868584 loss_ctc 18.444838 loss_rnnt 12.295343 hw_loss 0.276280 lr 0.00046632 rank 0
2023-02-25 19:42:30,820 DEBUG TRAIN Batch 11/23300 loss 12.571327 loss_att 16.788107 loss_ctc 12.593714 loss_rnnt 11.635271 hw_loss 0.168214 lr 0.00046612 rank 0
2023-02-25 19:42:30,841 DEBUG TRAIN Batch 11/23300 loss 12.294013 loss_att 15.194927 loss_ctc 13.826011 loss_rnnt 11.452473 hw_loss 0.107047 lr 0.00046612 rank 1
2023-02-25 19:43:25,317 DEBUG TRAIN Batch 11/23400 loss 12.646054 loss_att 15.671105 loss_ctc 12.756886 loss_rnnt 11.911534 hw_loss 0.215122 lr 0.00046592 rank 1
2023-02-25 19:43:25,322 DEBUG TRAIN Batch 11/23400 loss 16.507042 loss_att 17.403101 loss_ctc 19.437429 loss_rnnt 15.841043 hw_loss 0.180128 lr 0.00046592 rank 0
2023-02-25 19:45:02,009 DEBUG TRAIN Batch 11/23500 loss 22.119171 loss_att 25.832386 loss_ctc 24.221035 loss_rnnt 21.000542 hw_loss 0.179506 lr 0.00046572 rank 1
2023-02-25 19:45:02,013 DEBUG TRAIN Batch 11/23500 loss 29.257223 loss_att 35.724457 loss_ctc 32.713444 loss_rnnt 27.333809 hw_loss 0.317130 lr 0.00046572 rank 0
2023-02-25 19:45:57,583 DEBUG TRAIN Batch 11/23600 loss 21.336700 loss_att 25.534122 loss_ctc 26.172171 loss_rnnt 19.640375 hw_loss 0.397704 lr 0.00046551 rank 1
2023-02-25 19:45:57,606 DEBUG TRAIN Batch 11/23600 loss 39.133835 loss_att 44.557911 loss_ctc 42.612671 loss_rnnt 37.468433 hw_loss 0.218887 lr 0.00046551 rank 0
2023-02-25 19:46:53,255 DEBUG TRAIN Batch 11/23700 loss 18.689604 loss_att 18.667150 loss_ctc 22.918268 loss_rnnt 17.920061 hw_loss 0.394144 lr 0.00046531 rank 1
2023-02-25 19:46:53,263 DEBUG TRAIN Batch 11/23700 loss 10.949423 loss_att 13.532972 loss_ctc 10.703703 loss_rnnt 10.349345 hw_loss 0.217744 lr 0.00046531 rank 0
2023-02-25 19:47:50,077 DEBUG TRAIN Batch 11/23800 loss 26.085966 loss_att 27.056465 loss_ctc 30.277660 loss_rnnt 25.233618 hw_loss 0.186296 lr 0.00046511 rank 1
2023-02-25 19:47:50,078 DEBUG TRAIN Batch 11/23800 loss 21.891548 loss_att 25.552837 loss_ctc 19.081541 loss_rnnt 21.414570 hw_loss 0.223853 lr 0.00046511 rank 0
2023-02-25 19:49:24,202 DEBUG TRAIN Batch 11/23900 loss 22.206114 loss_att 29.671383 loss_ctc 27.576870 loss_rnnt 19.902931 hw_loss 0.176299 lr 0.00046491 rank 0
2023-02-25 19:49:24,213 DEBUG TRAIN Batch 11/23900 loss 12.975553 loss_att 15.794912 loss_ctc 20.705526 loss_rnnt 11.169088 hw_loss 0.397368 lr 0.00046491 rank 1
2023-02-25 19:50:19,970 DEBUG TRAIN Batch 11/24000 loss 21.059757 loss_att 25.390709 loss_ctc 27.304729 loss_rnnt 19.280743 hw_loss 0.150298 lr 0.00046471 rank 1
2023-02-25 19:50:19,972 DEBUG TRAIN Batch 11/24000 loss 28.127277 loss_att 32.328556 loss_ctc 29.685030 loss_rnnt 26.830605 hw_loss 0.466343 lr 0.00046471 rank 0
2023-02-25 19:51:15,699 DEBUG TRAIN Batch 11/24100 loss 16.482786 loss_att 24.127193 loss_ctc 20.490963 loss_rnnt 14.323706 hw_loss 0.179582 lr 0.00046451 rank 1
2023-02-25 19:51:15,710 DEBUG TRAIN Batch 11/24100 loss 27.812910 loss_att 31.623259 loss_ctc 37.194229 loss_rnnt 25.654127 hw_loss 0.273509 lr 0.00046451 rank 0
2023-02-25 19:53:22,701 DEBUG TRAIN Batch 11/24200 loss 12.876333 loss_att 15.239014 loss_ctc 17.155191 loss_rnnt 11.775761 hw_loss 0.107853 lr 0.00046431 rank 1
2023-02-25 19:53:22,732 DEBUG TRAIN Batch 11/24200 loss 12.193432 loss_att 11.914877 loss_ctc 13.456604 loss_rnnt 11.772268 hw_loss 0.578349 lr 0.00046431 rank 0
2023-02-25 19:54:18,148 DEBUG TRAIN Batch 11/24300 loss 17.242750 loss_att 21.353567 loss_ctc 22.064217 loss_rnnt 15.618438 hw_loss 0.298664 lr 0.00046411 rank 0
2023-02-25 19:54:18,160 DEBUG TRAIN Batch 11/24300 loss 11.621063 loss_att 14.773279 loss_ctc 12.380931 loss_rnnt 10.756195 hw_loss 0.249578 lr 0.00046411 rank 1
2023-02-25 19:55:13,427 DEBUG TRAIN Batch 11/24400 loss 25.446867 loss_att 28.644552 loss_ctc 34.643978 loss_rnnt 23.496323 hw_loss 0.158863 lr 0.00046391 rank 0
2023-02-25 19:55:13,432 DEBUG TRAIN Batch 11/24400 loss 24.657125 loss_att 27.154020 loss_ctc 27.575455 loss_rnnt 23.661297 hw_loss 0.201262 lr 0.00046391 rank 1
2023-02-25 19:56:10,526 DEBUG TRAIN Batch 11/24500 loss 8.983309 loss_att 14.441948 loss_ctc 13.132538 loss_rnnt 7.226712 hw_loss 0.209322 lr 0.00046371 rank 1
2023-02-25 19:56:10,532 DEBUG TRAIN Batch 11/24500 loss 17.593754 loss_att 17.707706 loss_ctc 16.359480 loss_rnnt 17.583048 hw_loss 0.285908 lr 0.00046371 rank 0
2023-02-25 19:57:57,958 DEBUG TRAIN Batch 11/24600 loss 20.089357 loss_att 23.482601 loss_ctc 24.736446 loss_rnnt 18.648706 hw_loss 0.266980 lr 0.00046351 rank 1
2023-02-25 19:57:57,960 DEBUG TRAIN Batch 11/24600 loss 13.586229 loss_att 18.186953 loss_ctc 17.358269 loss_rnnt 12.063076 hw_loss 0.187632 lr 0.00046351 rank 0
run_2_25_rnnt_bias_3word_finetune_seperate.sh: line 167: 43732 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
run_2_25_rnnt_bias_3word_finetune_seperate.sh: line 167: 43731 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
