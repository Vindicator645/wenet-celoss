/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_27_rnnt_bias_both_finetune_100head.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head_correct/ddp_init
2023-03-01 04:29:54,408 INFO training on multiple gpus, this gpu 1
2023-03-01 04:29:54,409 INFO training on multiple gpus, this gpu 0
2023-03-01 04:29:54,441 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-01 04:29:54,442 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-01 04:29:54,446 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 04:29:54,446 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 04:29:55,946 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head_correct/43.pt for GPU
2023-03-01 04:29:57,210 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head_correct/43.pt for GPU
2023-03-01 04:30:26,466 INFO Epoch 44 TRAIN info lr 4e-08
2023-03-01 04:30:26,467 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 04:30:26,468 INFO Epoch 44 TRAIN info lr 4e-08
2023-03-01 04:30:26,469 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-03-01 04:30:59,807 DEBUG TRAIN Batch 44/0 loss 5.519252 loss_att 5.575342 loss_ctc 7.948673 loss_rnnt 4.754099 hw_loss 0.806272 lr 0.00029907 rank 1
2023-03-01 04:30:59,838 DEBUG TRAIN Batch 44/0 loss 8.764514 loss_att 9.442068 loss_ctc 12.574373 loss_rnnt 7.684795 hw_loss 0.817925 lr 0.00029907 rank 0
2023-03-01 04:31:21,183 DEBUG TRAIN Batch 44/100 loss 5.426251 loss_att 9.560884 loss_ctc 9.667831 loss_rnnt 3.934745 hw_loss 0.185691 lr 0.00029906 rank 1
2023-03-01 04:31:21,183 DEBUG TRAIN Batch 44/100 loss 11.762045 loss_att 10.215568 loss_ctc 11.535167 loss_rnnt 11.877254 hw_loss 0.420633 lr 0.00029906 rank 0
2023-03-01 04:31:43,099 DEBUG TRAIN Batch 44/200 loss 3.583617 loss_att 7.775059 loss_ctc 9.836103 loss_rnnt 1.806607 hw_loss 0.196980 lr 0.00029905 rank 1
2023-03-01 04:31:43,099 DEBUG TRAIN Batch 44/200 loss 8.915258 loss_att 12.659010 loss_ctc 13.690089 loss_rnnt 7.422813 hw_loss 0.200720 lr 0.00029905 rank 0
2023-03-01 04:32:04,512 DEBUG TRAIN Batch 44/300 loss 9.223157 loss_att 9.150137 loss_ctc 14.632457 loss_rnnt 8.334775 hw_loss 0.340771 lr 0.00029903 rank 1
2023-03-01 04:32:04,512 DEBUG TRAIN Batch 44/300 loss 5.517839 loss_att 8.336729 loss_ctc 9.013368 loss_rnnt 4.293961 hw_loss 0.363808 lr 0.00029903 rank 0
2023-03-01 04:32:37,575 DEBUG TRAIN Batch 44/400 loss 3.774633 loss_att 5.726997 loss_ctc 3.986997 loss_rnnt 3.208405 hw_loss 0.276448 lr 0.00029902 rank 1
2023-03-01 04:32:37,576 DEBUG TRAIN Batch 44/400 loss 7.815626 loss_att 10.255069 loss_ctc 10.417912 loss_rnnt 6.862232 hw_loss 0.222250 lr 0.00029902 rank 0
2023-03-01 04:32:58,928 DEBUG TRAIN Batch 44/500 loss 5.796443 loss_att 6.615059 loss_ctc 8.253294 loss_rnnt 5.132277 hw_loss 0.324117 lr 0.00029901 rank 0
2023-03-01 04:32:58,928 DEBUG TRAIN Batch 44/500 loss 6.765679 loss_att 9.259119 loss_ctc 12.344403 loss_rnnt 5.462402 hw_loss 0.113923 lr 0.00029901 rank 1
2023-03-01 04:33:21,025 DEBUG TRAIN Batch 44/600 loss 6.055680 loss_att 7.647959 loss_ctc 10.880631 loss_rnnt 4.869888 hw_loss 0.420017 lr 0.00029899 rank 0
2023-03-01 04:33:21,026 DEBUG TRAIN Batch 44/600 loss 10.353838 loss_att 11.268116 loss_ctc 16.457008 loss_rnnt 9.194646 hw_loss 0.304837 lr 0.00029899 rank 1
2023-03-01 04:33:42,856 DEBUG TRAIN Batch 44/700 loss 5.377088 loss_att 7.924000 loss_ctc 7.694330 loss_rnnt 4.449377 hw_loss 0.205054 lr 0.00029898 rank 1
2023-03-01 04:33:42,857 DEBUG TRAIN Batch 44/700 loss 2.113768 loss_att 5.650906 loss_ctc 4.138266 loss_rnnt 1.033130 hw_loss 0.193646 lr 0.00029898 rank 0
2023-03-01 04:34:14,833 DEBUG TRAIN Batch 44/800 loss 8.982388 loss_att 11.348499 loss_ctc 15.064891 loss_rnnt 7.563116 hw_loss 0.253216 lr 0.00029897 rank 1
2023-03-01 04:34:14,834 DEBUG TRAIN Batch 44/800 loss 4.788457 loss_att 10.214670 loss_ctc 7.000722 loss_rnnt 3.232041 hw_loss 0.330384 lr 0.00029897 rank 0
2023-03-01 04:34:36,132 DEBUG TRAIN Batch 44/900 loss 2.115114 loss_att 6.020091 loss_ctc 3.155533 loss_rnnt 1.038808 hw_loss 0.293602 lr 0.00029895 rank 1
2023-03-01 04:34:36,132 DEBUG TRAIN Batch 44/900 loss 2.550315 loss_att 4.630743 loss_ctc 6.428477 loss_rnnt 1.465768 hw_loss 0.283824 lr 0.00029895 rank 0
2023-03-01 04:34:57,193 DEBUG TRAIN Batch 44/1000 loss 2.287448 loss_att 5.710016 loss_ctc 6.388536 loss_rnnt 0.920305 hw_loss 0.254658 lr 0.00029894 rank 0
2023-03-01 04:34:57,195 DEBUG TRAIN Batch 44/1000 loss 10.012471 loss_att 14.356758 loss_ctc 15.783002 loss_rnnt 8.206568 hw_loss 0.314328 lr 0.00029894 rank 1
2023-03-01 04:35:30,745 DEBUG TRAIN Batch 44/1100 loss 2.938904 loss_att 5.522001 loss_ctc 5.348202 loss_rnnt 1.963161 hw_loss 0.258531 lr 0.00029893 rank 1
2023-03-01 04:35:30,747 DEBUG TRAIN Batch 44/1100 loss 9.651374 loss_att 10.870760 loss_ctc 15.096002 loss_rnnt 8.510133 hw_loss 0.321400 lr 0.00029893 rank 0
2023-03-01 04:35:52,656 DEBUG TRAIN Batch 44/1200 loss 6.701803 loss_att 7.589803 loss_ctc 11.524435 loss_rnnt 5.692736 hw_loss 0.353342 lr 0.00029891 rank 1
2023-03-01 04:35:52,658 DEBUG TRAIN Batch 44/1200 loss 7.806049 loss_att 10.578619 loss_ctc 20.966618 loss_rnnt 5.297128 hw_loss 0.374372 lr 0.00029891 rank 0
2023-03-01 04:36:14,979 DEBUG TRAIN Batch 44/1300 loss 6.197973 loss_att 6.601723 loss_ctc 10.023554 loss_rnnt 5.340040 hw_loss 0.500822 lr 0.00029890 rank 0
2023-03-01 04:36:14,979 DEBUG TRAIN Batch 44/1300 loss 7.781857 loss_att 10.874006 loss_ctc 12.878764 loss_rnnt 6.400114 hw_loss 0.156984 lr 0.00029890 rank 1
2023-03-01 04:36:36,651 DEBUG TRAIN Batch 44/1400 loss 5.530818 loss_att 7.764255 loss_ctc 12.581316 loss_rnnt 4.024296 hw_loss 0.224567 lr 0.00029889 rank 1
2023-03-01 04:36:36,652 DEBUG TRAIN Batch 44/1400 loss 5.670593 loss_att 6.621948 loss_ctc 5.956785 loss_rnnt 5.299420 hw_loss 0.267644 lr 0.00029889 rank 0
2023-03-01 04:37:09,112 DEBUG TRAIN Batch 44/1500 loss 4.498084 loss_att 7.900787 loss_ctc 12.800659 loss_rnnt 2.520617 hw_loss 0.356092 lr 0.00029887 rank 1
2023-03-01 04:37:09,114 DEBUG TRAIN Batch 44/1500 loss 7.212164 loss_att 10.787247 loss_ctc 15.099631 loss_rnnt 5.356037 hw_loss 0.167715 lr 0.00029887 rank 0
2023-03-01 04:37:30,587 DEBUG TRAIN Batch 44/1600 loss 7.503184 loss_att 10.475481 loss_ctc 14.178517 loss_rnnt 5.941155 hw_loss 0.145357 lr 0.00029886 rank 1
2023-03-01 04:37:30,588 DEBUG TRAIN Batch 44/1600 loss 3.599752 loss_att 8.678818 loss_ctc 10.104376 loss_rnnt 1.601944 hw_loss 0.215083 lr 0.00029886 rank 0
2023-03-01 04:37:52,300 DEBUG TRAIN Batch 44/1700 loss 5.574809 loss_att 7.383935 loss_ctc 8.125175 loss_rnnt 4.807243 hw_loss 0.123172 lr 0.00029885 rank 1
2023-03-01 04:37:52,302 DEBUG TRAIN Batch 44/1700 loss 7.606165 loss_att 11.229929 loss_ctc 13.171688 loss_rnnt 6.056767 hw_loss 0.154831 lr 0.00029885 rank 0
2023-03-01 04:38:26,967 DEBUG TRAIN Batch 44/1800 loss 12.104826 loss_att 12.853491 loss_ctc 19.089691 loss_rnnt 10.805669 hw_loss 0.408955 lr 0.00029883 rank 1
2023-03-01 04:38:26,968 DEBUG TRAIN Batch 44/1800 loss 6.451576 loss_att 10.660240 loss_ctc 15.487763 loss_rnnt 4.243414 hw_loss 0.303009 lr 0.00029883 rank 0
2023-03-01 04:38:49,201 DEBUG TRAIN Batch 44/1900 loss 7.560452 loss_att 10.879825 loss_ctc 10.855952 loss_rnnt 6.300490 hw_loss 0.293788 lr 0.00029882 rank 1
2023-03-01 04:38:49,202 DEBUG TRAIN Batch 44/1900 loss 6.117765 loss_att 7.043332 loss_ctc 9.184140 loss_rnnt 5.323369 hw_loss 0.375812 lr 0.00029882 rank 0
2023-03-01 04:39:10,623 DEBUG TRAIN Batch 44/2000 loss 5.054867 loss_att 11.387453 loss_ctc 10.318559 loss_rnnt 3.040763 hw_loss 0.085803 lr 0.00029881 rank 1
2023-03-01 04:39:10,623 DEBUG TRAIN Batch 44/2000 loss 9.660164 loss_att 11.007848 loss_ctc 13.941859 loss_rnnt 8.671685 hw_loss 0.277591 lr 0.00029881 rank 0
2023-03-01 04:39:33,070 DEBUG TRAIN Batch 44/2100 loss 4.443826 loss_att 8.024236 loss_ctc 9.877150 loss_rnnt 2.813906 hw_loss 0.355116 lr 0.00029879 rank 1
2023-03-01 04:39:33,071 DEBUG TRAIN Batch 44/2100 loss 5.516572 loss_att 9.055986 loss_ctc 8.699467 loss_rnnt 4.176971 hw_loss 0.388749 lr 0.00029879 rank 0
2023-03-01 04:40:06,227 DEBUG TRAIN Batch 44/2200 loss 5.174652 loss_att 7.941712 loss_ctc 9.478682 loss_rnnt 3.913509 hw_loss 0.250987 lr 0.00029878 rank 1
2023-03-01 04:40:06,227 DEBUG TRAIN Batch 44/2200 loss 7.111193 loss_att 10.627717 loss_ctc 10.072062 loss_rnnt 5.932005 hw_loss 0.152063 lr 0.00029878 rank 0
2023-03-01 04:40:27,189 DEBUG TRAIN Batch 44/2300 loss 7.949844 loss_att 12.227287 loss_ctc 13.101069 loss_rnnt 6.230896 hw_loss 0.331181 lr 0.00029877 rank 1
2023-03-01 04:40:27,192 DEBUG TRAIN Batch 44/2300 loss 11.578001 loss_att 14.237307 loss_ctc 19.800056 loss_rnnt 9.850340 hw_loss 0.186610 lr 0.00029877 rank 0
2023-03-01 04:40:49,276 DEBUG TRAIN Batch 44/2400 loss 6.649823 loss_att 9.645998 loss_ctc 10.116027 loss_rnnt 5.405242 hw_loss 0.343472 lr 0.00029875 rank 1
2023-03-01 04:40:49,277 DEBUG TRAIN Batch 44/2400 loss 6.948120 loss_att 8.096166 loss_ctc 7.771575 loss_rnnt 6.487935 hw_loss 0.226465 lr 0.00029875 rank 0
2023-03-01 04:41:24,011 DEBUG TRAIN Batch 44/2500 loss 5.437335 loss_att 6.124296 loss_ctc 7.659524 loss_rnnt 4.799301 hw_loss 0.383157 lr 0.00029874 rank 1
2023-03-01 04:41:24,011 DEBUG TRAIN Batch 44/2500 loss 8.242160 loss_att 9.103598 loss_ctc 12.252605 loss_rnnt 7.363112 hw_loss 0.322563 lr 0.00029874 rank 0
2023-03-01 04:41:44,848 DEBUG TRAIN Batch 44/2600 loss 6.918736 loss_att 11.500320 loss_ctc 10.590973 loss_rnnt 5.377027 hw_loss 0.254552 lr 0.00029873 rank 1
2023-03-01 04:41:44,849 DEBUG TRAIN Batch 44/2600 loss 2.177218 loss_att 4.719326 loss_ctc 5.196193 loss_rnnt 1.111981 hw_loss 0.289284 lr 0.00029873 rank 0
2023-03-01 04:42:06,796 DEBUG TRAIN Batch 44/2700 loss 4.481649 loss_att 6.941319 loss_ctc 6.519918 loss_rnnt 3.604254 hw_loss 0.213171 lr 0.00029871 rank 1
2023-03-01 04:42:06,797 DEBUG TRAIN Batch 44/2700 loss 4.430678 loss_att 8.092955 loss_ctc 8.216845 loss_rnnt 3.121778 hw_loss 0.134292 lr 0.00029871 rank 0
2023-03-01 04:42:28,953 DEBUG TRAIN Batch 44/2800 loss 5.883512 loss_att 7.566824 loss_ctc 8.841320 loss_rnnt 5.072715 hw_loss 0.149552 lr 0.00029870 rank 1
2023-03-01 04:42:28,954 DEBUG TRAIN Batch 44/2800 loss 4.772299 loss_att 6.533751 loss_ctc 6.370553 loss_rnnt 4.033301 hw_loss 0.325513 lr 0.00029870 rank 0
2023-03-01 04:43:02,568 DEBUG TRAIN Batch 44/2900 loss 11.621700 loss_att 16.262369 loss_ctc 19.083374 loss_rnnt 9.509393 hw_loss 0.354905 lr 0.00029869 rank 0
2023-03-01 04:43:02,568 DEBUG TRAIN Batch 44/2900 loss 6.264521 loss_att 10.648508 loss_ctc 12.599395 loss_rnnt 4.361609 hw_loss 0.340245 lr 0.00029869 rank 1
2023-03-01 04:43:24,177 DEBUG TRAIN Batch 44/3000 loss 5.524937 loss_att 7.548044 loss_ctc 7.613173 loss_rnnt 4.696066 hw_loss 0.273406 lr 0.00029867 rank 0
2023-03-01 04:43:24,177 DEBUG TRAIN Batch 44/3000 loss 19.705965 loss_att 20.263601 loss_ctc 33.310814 loss_rnnt 17.617573 hw_loss 0.305410 lr 0.00029867 rank 1
2023-03-01 04:43:45,796 DEBUG TRAIN Batch 44/3100 loss 1.992192 loss_att 2.801741 loss_ctc 3.324108 loss_rnnt 1.402958 hw_loss 0.468256 lr 0.00029866 rank 1
2023-03-01 04:43:45,798 DEBUG TRAIN Batch 44/3100 loss 5.299160 loss_att 6.082660 loss_ctc 8.826476 loss_rnnt 4.503639 hw_loss 0.315962 lr 0.00029866 rank 0
2023-03-01 04:44:07,655 DEBUG TRAIN Batch 44/3200 loss 8.714045 loss_att 8.894379 loss_ctc 21.626156 loss_rnnt 6.896471 hw_loss 0.112297 lr 0.00029865 rank 1
2023-03-01 04:44:07,656 DEBUG TRAIN Batch 44/3200 loss 8.694305 loss_att 15.361845 loss_ctc 13.209732 loss_rnnt 6.596975 hw_loss 0.303312 lr 0.00029865 rank 0
2023-03-01 04:44:40,038 DEBUG TRAIN Batch 44/3300 loss 9.076353 loss_att 13.872499 loss_ctc 10.415882 loss_rnnt 7.898747 hw_loss 0.074573 lr 0.00029863 rank 1
2023-03-01 04:44:40,039 DEBUG TRAIN Batch 44/3300 loss 6.944432 loss_att 8.820179 loss_ctc 13.747156 loss_rnnt 5.649633 hw_loss 0.023661 lr 0.00029863 rank 0
2023-03-01 04:45:01,029 DEBUG TRAIN Batch 44/3400 loss 15.553436 loss_att 18.547407 loss_ctc 24.345661 loss_rnnt 13.745562 hw_loss 0.068966 lr 0.00029862 rank 1
2023-03-01 04:45:01,030 DEBUG TRAIN Batch 44/3400 loss 7.004896 loss_att 10.618172 loss_ctc 13.226945 loss_rnnt 5.255551 hw_loss 0.369529 lr 0.00029862 rank 0
2023-03-01 04:45:22,249 DEBUG TRAIN Batch 44/3500 loss 9.878036 loss_att 13.695667 loss_ctc 19.245155 loss_rnnt 7.763008 hw_loss 0.192286 lr 0.00029861 rank 1
2023-03-01 04:45:22,250 DEBUG TRAIN Batch 44/3500 loss 8.055142 loss_att 9.182211 loss_ctc 11.113235 loss_rnnt 7.267880 hw_loss 0.288941 lr 0.00029861 rank 0
2023-03-01 04:45:54,754 DEBUG TRAIN Batch 44/3600 loss 4.312989 loss_att 7.188442 loss_ctc 7.354831 loss_rnnt 3.236926 hw_loss 0.178864 lr 0.00029859 rank 1
2023-03-01 04:45:54,756 DEBUG TRAIN Batch 44/3600 loss 7.312565 loss_att 10.271287 loss_ctc 11.531914 loss_rnnt 6.079162 hw_loss 0.148271 lr 0.00029859 rank 0
2023-03-01 04:46:17,452 DEBUG TRAIN Batch 44/3700 loss 3.221104 loss_att 5.733040 loss_ctc 8.207766 loss_rnnt 1.879963 hw_loss 0.325999 lr 0.00029858 rank 1
2023-03-01 04:46:17,454 DEBUG TRAIN Batch 44/3700 loss 8.296730 loss_att 8.975358 loss_ctc 16.439276 loss_rnnt 6.996225 hw_loss 0.148325 lr 0.00029858 rank 0
2023-03-01 04:46:39,995 DEBUG TRAIN Batch 44/3800 loss 3.971605 loss_att 10.535990 loss_ctc 9.470725 loss_rnnt 1.861246 hw_loss 0.120499 lr 0.00029857 rank 0
2023-03-01 04:46:39,996 DEBUG TRAIN Batch 44/3800 loss 7.461629 loss_att 9.765610 loss_ctc 11.795585 loss_rnnt 6.251624 hw_loss 0.321279 lr 0.00029857 rank 1
2023-03-01 04:47:01,470 DEBUG TRAIN Batch 44/3900 loss 7.514627 loss_att 10.727003 loss_ctc 11.362791 loss_rnnt 6.273921 hw_loss 0.159640 lr 0.00029855 rank 1
2023-03-01 04:47:01,470 DEBUG TRAIN Batch 44/3900 loss 4.583141 loss_att 8.341752 loss_ctc 6.341080 loss_rnnt 3.449334 hw_loss 0.276926 lr 0.00029855 rank 0
2023-03-01 04:47:36,270 DEBUG TRAIN Batch 44/4000 loss 3.800134 loss_att 6.939744 loss_ctc 7.228195 loss_rnnt 2.660389 hw_loss 0.102651 lr 0.00029854 rank 1
2023-03-01 04:47:36,272 DEBUG TRAIN Batch 44/4000 loss 2.654726 loss_att 5.335781 loss_ctc 5.572312 loss_rnnt 1.588817 hw_loss 0.263787 lr 0.00029854 rank 0
2023-03-01 04:47:57,762 DEBUG TRAIN Batch 44/4100 loss 6.512792 loss_att 8.190362 loss_ctc 8.702848 loss_rnnt 5.655520 hw_loss 0.430781 lr 0.00029853 rank 1
2023-03-01 04:47:57,765 DEBUG TRAIN Batch 44/4100 loss 3.640053 loss_att 6.230995 loss_ctc 5.936274 loss_rnnt 2.759618 hw_loss 0.105158 lr 0.00029853 rank 0
2023-03-01 04:48:19,463 DEBUG TRAIN Batch 44/4200 loss 5.919408 loss_att 8.287859 loss_ctc 7.658421 loss_rnnt 5.103203 hw_loss 0.207462 lr 0.00029851 rank 1
2023-03-01 04:48:19,464 DEBUG TRAIN Batch 44/4200 loss 3.550844 loss_att 5.736069 loss_ctc 4.896950 loss_rnnt 2.778591 hw_loss 0.291989 lr 0.00029851 rank 0
2023-03-01 04:48:51,981 DEBUG TRAIN Batch 44/4300 loss 10.313511 loss_att 13.521145 loss_ctc 16.304962 loss_rnnt 8.693751 hw_loss 0.336326 lr 0.00029850 rank 1
2023-03-01 04:48:51,984 DEBUG TRAIN Batch 44/4300 loss 14.565639 loss_att 18.092066 loss_ctc 27.629261 loss_rnnt 12.002802 hw_loss 0.217005 lr 0.00029850 rank 0
2023-03-01 04:49:13,877 DEBUG TRAIN Batch 44/4400 loss 7.501509 loss_att 8.161602 loss_ctc 12.612244 loss_rnnt 6.476247 hw_loss 0.397147 lr 0.00029849 rank 1
2023-03-01 04:49:13,880 DEBUG TRAIN Batch 44/4400 loss 7.964288 loss_att 9.459173 loss_ctc 11.032421 loss_rnnt 7.113474 hw_loss 0.267659 lr 0.00029849 rank 0
2023-03-01 04:49:35,122 DEBUG TRAIN Batch 44/4500 loss 11.197248 loss_att 17.357019 loss_ctc 20.908354 loss_rnnt 8.482393 hw_loss 0.352663 lr 0.00029847 rank 1
2023-03-01 04:49:35,124 DEBUG TRAIN Batch 44/4500 loss 4.525211 loss_att 10.381733 loss_ctc 7.936609 loss_rnnt 2.813926 hw_loss 0.159614 lr 0.00029847 rank 0
2023-03-01 04:49:56,997 DEBUG TRAIN Batch 44/4600 loss 8.564516 loss_att 12.820427 loss_ctc 15.679220 loss_rnnt 6.595342 hw_loss 0.317560 lr 0.00029846 rank 0
2023-03-01 04:49:56,998 DEBUG TRAIN Batch 44/4600 loss 4.102128 loss_att 7.775212 loss_ctc 9.039194 loss_rnnt 2.576702 hw_loss 0.248500 lr 0.00029846 rank 1
2023-03-01 04:50:28,379 DEBUG TRAIN Batch 44/4700 loss 5.041145 loss_att 8.729607 loss_ctc 7.875226 loss_rnnt 3.855785 hw_loss 0.130855 lr 0.00029845 rank 0
2023-03-01 04:50:28,379 DEBUG TRAIN Batch 44/4700 loss 8.380721 loss_att 10.812799 loss_ctc 15.066467 loss_rnnt 6.862991 hw_loss 0.262276 lr 0.00029845 rank 1
2023-03-01 04:50:49,186 DEBUG TRAIN Batch 44/4800 loss 5.513260 loss_att 7.356554 loss_ctc 6.678934 loss_rnnt 4.890206 hw_loss 0.185572 lr 0.00029843 rank 0
2023-03-01 04:50:49,186 DEBUG TRAIN Batch 44/4800 loss 5.855841 loss_att 7.637643 loss_ctc 9.768659 loss_rnnt 4.791244 hw_loss 0.349740 lr 0.00029843 rank 1
2023-03-01 04:51:10,747 DEBUG TRAIN Batch 44/4900 loss 12.705671 loss_att 14.010851 loss_ctc 19.752100 loss_rnnt 11.400005 hw_loss 0.197072 lr 0.00029842 rank 0
2023-03-01 04:51:10,747 DEBUG TRAIN Batch 44/4900 loss 4.909962 loss_att 6.747437 loss_ctc 8.691225 loss_rnnt 3.936425 hw_loss 0.191014 lr 0.00029842 rank 1
2023-03-01 04:51:44,670 DEBUG TRAIN Batch 44/5000 loss 4.194855 loss_att 5.611975 loss_ctc 6.368505 loss_rnnt 3.534550 hw_loss 0.163240 lr 0.00029841 rank 1
2023-03-01 04:51:44,671 DEBUG TRAIN Batch 44/5000 loss 7.188121 loss_att 9.071786 loss_ctc 10.042480 loss_rnnt 6.267372 hw_loss 0.306442 lr 0.00029841 rank 0
2023-03-01 04:52:06,897 DEBUG TRAIN Batch 44/5100 loss 7.687832 loss_att 10.990267 loss_ctc 13.739566 loss_rnnt 6.146989 hw_loss 0.137733 lr 0.00029840 rank 1
2023-03-01 04:52:06,898 DEBUG TRAIN Batch 44/5100 loss 2.969957 loss_att 6.980082 loss_ctc 5.466621 loss_rnnt 1.627508 hw_loss 0.389129 lr 0.00029840 rank 0
2023-03-01 04:52:28,257 DEBUG TRAIN Batch 44/5200 loss 3.045269 loss_att 7.835464 loss_ctc 7.007446 loss_rnnt 1.377263 hw_loss 0.340644 lr 0.00029838 rank 0
2023-03-01 04:52:28,257 DEBUG TRAIN Batch 44/5200 loss 6.233814 loss_att 10.147243 loss_ctc 8.417046 loss_rnnt 5.004467 hw_loss 0.291682 lr 0.00029838 rank 1
2023-03-01 04:52:50,461 DEBUG TRAIN Batch 44/5300 loss 10.603036 loss_att 12.904287 loss_ctc 22.460232 loss_rnnt 8.435146 hw_loss 0.237525 lr 0.00029837 rank 0
2023-03-01 04:52:50,461 DEBUG TRAIN Batch 44/5300 loss 4.242072 loss_att 8.125217 loss_ctc 7.163117 loss_rnnt 3.007380 hw_loss 0.128607 lr 0.00029837 rank 1
2023-03-01 04:53:23,068 DEBUG TRAIN Batch 44/5400 loss 4.638147 loss_att 8.261055 loss_ctc 10.205751 loss_rnnt 2.988072 hw_loss 0.343400 lr 0.00029836 rank 0
2023-03-01 04:53:23,069 DEBUG TRAIN Batch 44/5400 loss 11.399261 loss_att 13.496062 loss_ctc 17.941809 loss_rnnt 9.948416 hw_loss 0.298397 lr 0.00029836 rank 1
2023-03-01 04:53:43,814 DEBUG TRAIN Batch 44/5500 loss 1.717610 loss_att 3.987481 loss_ctc 2.808733 loss_rnnt 0.888094 hw_loss 0.431360 lr 0.00029834 rank 1
2023-03-01 04:53:43,816 DEBUG TRAIN Batch 44/5500 loss 10.537273 loss_att 11.774146 loss_ctc 15.630171 loss_rnnt 9.557067 hw_loss 0.100836 lr 0.00029834 rank 0
2023-03-01 04:54:05,259 DEBUG TRAIN Batch 44/5600 loss 7.764593 loss_att 9.731590 loss_ctc 12.659886 loss_rnnt 6.516380 hw_loss 0.378951 lr 0.00029833 rank 0
2023-03-01 04:54:05,260 DEBUG TRAIN Batch 44/5600 loss 6.737462 loss_att 9.020682 loss_ctc 9.053252 loss_rnnt 5.924508 hw_loss 0.089131 lr 0.00029833 rank 1
2023-03-01 04:54:38,871 DEBUG TRAIN Batch 44/5700 loss 5.224714 loss_att 5.506783 loss_ctc 8.196395 loss_rnnt 4.643952 hw_loss 0.240233 lr 0.00029832 rank 1
2023-03-01 04:54:38,872 DEBUG TRAIN Batch 44/5700 loss 3.543272 loss_att 8.698499 loss_ctc 6.360351 loss_rnnt 2.041997 hw_loss 0.177412 lr 0.00029832 rank 0
2023-03-01 04:55:00,329 DEBUG TRAIN Batch 44/5800 loss 11.626727 loss_att 12.911636 loss_ctc 23.921659 loss_rnnt 9.613624 hw_loss 0.218995 lr 0.00029830 rank 1
2023-03-01 04:55:00,330 DEBUG TRAIN Batch 44/5800 loss 3.182284 loss_att 6.759048 loss_ctc 6.858329 loss_rnnt 1.835192 hw_loss 0.265499 lr 0.00029830 rank 0
2023-03-01 04:55:21,962 DEBUG TRAIN Batch 44/5900 loss 4.037763 loss_att 7.098151 loss_ctc 6.230025 loss_rnnt 2.939475 hw_loss 0.363578 lr 0.00029829 rank 1
2023-03-01 04:55:21,964 DEBUG TRAIN Batch 44/5900 loss 1.390210 loss_att 3.695101 loss_ctc 3.105942 loss_rnnt 0.586691 hw_loss 0.213330 lr 0.00029829 rank 0
2023-03-01 04:55:43,421 DEBUG TRAIN Batch 44/6000 loss 6.140792 loss_att 8.423702 loss_ctc 9.119121 loss_rnnt 5.179121 hw_loss 0.202459 lr 0.00029828 rank 0
2023-03-01 04:55:43,422 DEBUG TRAIN Batch 44/6000 loss 5.606595 loss_att 8.090017 loss_ctc 8.343818 loss_rnnt 4.533130 hw_loss 0.397157 lr 0.00029828 rank 1
2023-03-01 04:56:15,285 DEBUG TRAIN Batch 44/6100 loss 12.764880 loss_att 17.982271 loss_ctc 17.068890 loss_rnnt 11.055611 hw_loss 0.172355 lr 0.00029826 rank 1
2023-03-01 04:56:15,288 DEBUG TRAIN Batch 44/6100 loss 6.898735 loss_att 11.585381 loss_ctc 14.164223 loss_rnnt 4.837295 hw_loss 0.291336 lr 0.00029826 rank 0
2023-03-01 04:56:36,961 DEBUG TRAIN Batch 44/6200 loss 6.969524 loss_att 9.665834 loss_ctc 11.558130 loss_rnnt 5.612223 hw_loss 0.386669 lr 0.00029825 rank 0
2023-03-01 04:56:36,961 DEBUG TRAIN Batch 44/6200 loss 6.310406 loss_att 7.799052 loss_ctc 9.756592 loss_rnnt 5.402127 hw_loss 0.283232 lr 0.00029825 rank 1
2023-03-01 04:56:58,621 DEBUG TRAIN Batch 44/6300 loss 9.875553 loss_att 11.444003 loss_ctc 14.950450 loss_rnnt 8.628149 hw_loss 0.481990 lr 0.00029824 rank 1
2023-03-01 04:56:58,623 DEBUG TRAIN Batch 44/6300 loss 8.421396 loss_att 9.320580 loss_ctc 16.348742 loss_rnnt 6.993078 hw_loss 0.359065 lr 0.00029824 rank 0
2023-03-01 04:57:20,659 DEBUG TRAIN Batch 44/6400 loss 7.184230 loss_att 11.762629 loss_ctc 11.726611 loss_rnnt 5.575863 hw_loss 0.163193 lr 0.00029822 rank 1
2023-03-01 04:57:20,660 DEBUG TRAIN Batch 44/6400 loss 1.763999 loss_att 5.476900 loss_ctc 1.858661 loss_rnnt 0.848463 hw_loss 0.300627 lr 0.00029822 rank 0
2023-03-01 04:57:52,510 DEBUG TRAIN Batch 44/6500 loss 5.121175 loss_att 10.042690 loss_ctc 13.828991 loss_rnnt 2.858255 hw_loss 0.220453 lr 0.00029821 rank 1
2023-03-01 04:57:52,512 DEBUG TRAIN Batch 44/6500 loss 8.760563 loss_att 9.633558 loss_ctc 17.137268 loss_rnnt 7.301929 hw_loss 0.313386 lr 0.00029821 rank 0
2023-03-01 04:58:14,475 DEBUG TRAIN Batch 44/6600 loss 1.493721 loss_att 4.115655 loss_ctc 2.394941 loss_rnnt 0.702542 hw_loss 0.274930 lr 0.00029820 rank 1
2023-03-01 04:58:14,475 DEBUG TRAIN Batch 44/6600 loss 8.047994 loss_att 11.630365 loss_ctc 13.643789 loss_rnnt 6.476745 hw_loss 0.203753 lr 0.00029820 rank 0
2023-03-01 04:58:35,597 DEBUG TRAIN Batch 44/6700 loss 4.021068 loss_att 6.352015 loss_ctc 7.729689 loss_rnnt 2.915246 hw_loss 0.272154 lr 0.00029818 rank 1
2023-03-01 04:58:35,598 DEBUG TRAIN Batch 44/6700 loss 12.633212 loss_att 14.523050 loss_ctc 24.228762 loss_rnnt 10.529712 hw_loss 0.336487 lr 0.00029818 rank 0
2023-03-01 04:59:08,224 DEBUG TRAIN Batch 44/6800 loss 13.302200 loss_att 15.410269 loss_ctc 18.565432 loss_rnnt 12.008926 hw_loss 0.318554 lr 0.00029817 rank 1
2023-03-01 04:59:08,225 DEBUG TRAIN Batch 44/6800 loss 11.155882 loss_att 14.381323 loss_ctc 19.593655 loss_rnnt 9.183835 hw_loss 0.378605 lr 0.00029817 rank 0
2023-03-01 04:59:30,187 DEBUG TRAIN Batch 44/6900 loss 5.005053 loss_att 7.253698 loss_ctc 8.676436 loss_rnnt 4.019372 hw_loss 0.087063 lr 0.00029816 rank 1
2023-03-01 04:59:30,188 DEBUG TRAIN Batch 44/6900 loss 4.810475 loss_att 8.785454 loss_ctc 9.346728 loss_rnnt 3.259732 hw_loss 0.282963 lr 0.00029816 rank 0
2023-03-01 04:59:52,000 DEBUG TRAIN Batch 44/7000 loss 5.881948 loss_att 7.340466 loss_ctc 6.108912 loss_rnnt 5.456678 hw_loss 0.193695 lr 0.00029814 rank 1
2023-03-01 04:59:52,000 DEBUG TRAIN Batch 44/7000 loss 4.070866 loss_att 7.450976 loss_ctc 9.495467 loss_rnnt 2.585433 hw_loss 0.161493 lr 0.00029814 rank 0
2023-03-01 05:00:13,338 DEBUG TRAIN Batch 44/7100 loss 2.138357 loss_att 5.377633 loss_ctc 4.004647 loss_rnnt 1.022546 hw_loss 0.410845 lr 0.00029813 rank 1
2023-03-01 05:00:13,338 DEBUG TRAIN Batch 44/7100 loss 0.791074 loss_att 2.500768 loss_ctc 0.823786 loss_rnnt 0.187666 hw_loss 0.482075 lr 0.00029813 rank 0
2023-03-01 05:00:45,529 DEBUG TRAIN Batch 44/7200 loss 10.579309 loss_att 12.234375 loss_ctc 21.757915 loss_rnnt 8.637367 hw_loss 0.225837 lr 0.00029812 rank 0
2023-03-01 05:00:45,529 DEBUG TRAIN Batch 44/7200 loss 9.708103 loss_att 16.655903 loss_ctc 18.638531 loss_rnnt 7.024187 hw_loss 0.194311 lr 0.00029812 rank 1
2023-03-01 05:01:06,450 DEBUG TRAIN Batch 44/7300 loss 4.946093 loss_att 6.587798 loss_ctc 5.790774 loss_rnnt 4.280477 hw_loss 0.421218 lr 0.00029810 rank 1
2023-03-01 05:01:06,451 DEBUG TRAIN Batch 44/7300 loss 9.704163 loss_att 12.479331 loss_ctc 17.272753 loss_rnnt 7.999750 hw_loss 0.262937 lr 0.00029810 rank 0
2023-03-01 05:01:27,978 DEBUG TRAIN Batch 44/7400 loss 11.306255 loss_att 13.534817 loss_ctc 15.333571 loss_rnnt 10.183883 hw_loss 0.261909 lr 0.00029809 rank 0
2023-03-01 05:01:27,978 DEBUG TRAIN Batch 44/7400 loss 7.540050 loss_att 10.963860 loss_ctc 12.971878 loss_rnnt 5.904437 hw_loss 0.424889 lr 0.00029809 rank 1
2023-03-01 05:02:01,519 DEBUG TRAIN Batch 44/7500 loss 11.678985 loss_att 13.796794 loss_ctc 17.599415 loss_rnnt 10.367698 hw_loss 0.184379 lr 0.00029808 rank 1
2023-03-01 05:02:01,521 DEBUG TRAIN Batch 44/7500 loss 4.178914 loss_att 5.731391 loss_ctc 5.569857 loss_rnnt 3.598496 hw_loss 0.158369 lr 0.00029808 rank 0
2023-03-01 05:02:23,557 DEBUG TRAIN Batch 44/7600 loss 11.738189 loss_att 12.584175 loss_ctc 17.270107 loss_rnnt 10.569757 hw_loss 0.490585 lr 0.00029806 rank 1
2023-03-01 05:02:23,560 DEBUG TRAIN Batch 44/7600 loss 2.408758 loss_att 6.186245 loss_ctc 3.329159 loss_rnnt 1.409934 hw_loss 0.226138 lr 0.00029806 rank 0
2023-03-01 05:02:44,341 DEBUG TRAIN Batch 44/7700 loss 3.266548 loss_att 8.157257 loss_ctc 8.638223 loss_rnnt 1.477624 hw_loss 0.177299 lr 0.00029805 rank 1
2023-03-01 05:02:44,341 DEBUG TRAIN Batch 44/7700 loss 5.043785 loss_att 7.431665 loss_ctc 8.407417 loss_rnnt 3.948191 hw_loss 0.317875 lr 0.00029805 rank 0
2023-03-01 05:03:06,584 DEBUG TRAIN Batch 44/7800 loss 12.041155 loss_att 14.674508 loss_ctc 21.976345 loss_rnnt 10.133863 hw_loss 0.104867 lr 0.00029804 rank 0
2023-03-01 05:03:06,584 DEBUG TRAIN Batch 44/7800 loss 10.069860 loss_att 12.700411 loss_ctc 18.339508 loss_rnnt 8.387513 hw_loss 0.100529 lr 0.00029804 rank 1
2023-03-01 05:03:39,129 DEBUG TRAIN Batch 44/7900 loss 4.432868 loss_att 7.601789 loss_ctc 6.794135 loss_rnnt 3.272748 hw_loss 0.396561 lr 0.00029802 rank 1
2023-03-01 05:03:39,129 DEBUG TRAIN Batch 44/7900 loss 2.731916 loss_att 4.631510 loss_ctc 4.964225 loss_rnnt 1.889106 hw_loss 0.309843 lr 0.00029802 rank 0
2023-03-01 05:04:01,349 DEBUG TRAIN Batch 44/8000 loss 7.836828 loss_att 11.414489 loss_ctc 12.980408 loss_rnnt 6.324724 hw_loss 0.207677 lr 0.00029801 rank 1
2023-03-01 05:04:01,349 DEBUG TRAIN Batch 44/8000 loss 10.443517 loss_att 14.309797 loss_ctc 20.993956 loss_rnnt 8.069899 hw_loss 0.363071 lr 0.00029801 rank 0
2023-03-01 05:04:23,822 DEBUG TRAIN Batch 44/8100 loss 7.086770 loss_att 8.662909 loss_ctc 8.228763 loss_rnnt 6.478499 hw_loss 0.263957 lr 0.00029800 rank 1
2023-03-01 05:04:23,824 DEBUG TRAIN Batch 44/8100 loss 11.859941 loss_att 13.575899 loss_ctc 23.831583 loss_rnnt 9.682313 hw_loss 0.446655 lr 0.00029800 rank 0
2023-03-01 05:04:58,717 DEBUG TRAIN Batch 44/8200 loss 5.837832 loss_att 5.758481 loss_ctc 8.554914 loss_rnnt 5.240082 hw_loss 0.471268 lr 0.00029798 rank 0
2023-03-01 05:04:58,719 DEBUG TRAIN Batch 44/8200 loss 9.456878 loss_att 12.733141 loss_ctc 19.854715 loss_rnnt 7.258148 hw_loss 0.294559 lr 0.00029798 rank 1
2023-03-01 05:05:20,748 DEBUG TRAIN Batch 44/8300 loss 1.819062 loss_att 4.439466 loss_ctc 3.461924 loss_rnnt 0.948199 hw_loss 0.239500 lr 0.00029797 rank 1
2023-03-01 05:05:20,749 DEBUG TRAIN Batch 44/8300 loss 2.482344 loss_att 5.727608 loss_ctc 8.471920 loss_rnnt 0.912642 hw_loss 0.228824 lr 0.00029797 rank 0
2023-03-01 05:05:42,872 DEBUG TRAIN Batch 44/8400 loss 5.622287 loss_att 7.165478 loss_ctc 13.263109 loss_rnnt 4.241821 hw_loss 0.099471 lr 0.00029796 rank 1
2023-03-01 05:05:42,873 DEBUG TRAIN Batch 44/8400 loss 8.187013 loss_att 12.492761 loss_ctc 12.574557 loss_rnnt 6.632758 hw_loss 0.202687 lr 0.00029796 rank 0
2023-03-01 05:06:05,158 DEBUG TRAIN Batch 44/8500 loss 5.325051 loss_att 7.272497 loss_ctc 8.164391 loss_rnnt 4.399937 hw_loss 0.294461 lr 0.00029794 rank 1
2023-03-01 05:06:05,160 DEBUG TRAIN Batch 44/8500 loss 6.765326 loss_att 9.943083 loss_ctc 8.623615 loss_rnnt 5.805695 hw_loss 0.143076 lr 0.00029794 rank 0
2023-03-01 05:06:37,839 DEBUG TRAIN Batch 44/8600 loss 9.073386 loss_att 13.837615 loss_ctc 14.077118 loss_rnnt 7.291493 hw_loss 0.303531 lr 0.00029793 rank 1
2023-03-01 05:06:37,840 DEBUG TRAIN Batch 44/8600 loss 8.216146 loss_att 10.533670 loss_ctc 17.721380 loss_rnnt 6.348045 hw_loss 0.257310 lr 0.00029793 rank 0
2023-03-01 05:07:00,145 DEBUG TRAIN Batch 44/8700 loss 4.931762 loss_att 7.040277 loss_ctc 7.843911 loss_rnnt 4.008888 hw_loss 0.211657 lr 0.00029792 rank 1
2023-03-01 05:07:00,146 DEBUG TRAIN Batch 44/8700 loss 3.256003 loss_att 5.546744 loss_ctc 5.874372 loss_rnnt 2.331037 hw_loss 0.220690 lr 0.00029792 rank 0
2023-03-01 05:07:22,830 DEBUG TRAIN Batch 44/8800 loss 10.902276 loss_att 14.047447 loss_ctc 18.892052 loss_rnnt 8.947960 hw_loss 0.487459 lr 0.00029790 rank 1
2023-03-01 05:07:22,831 DEBUG TRAIN Batch 44/8800 loss 5.376710 loss_att 6.592463 loss_ctc 8.272402 loss_rnnt 4.656229 hw_loss 0.171073 lr 0.00029790 rank 0
2023-03-01 05:07:55,736 DEBUG TRAIN Batch 44/8900 loss 4.801831 loss_att 5.298123 loss_ctc 6.869488 loss_rnnt 4.254603 hw_loss 0.323028 lr 0.00029789 rank 1
2023-03-01 05:07:55,738 DEBUG TRAIN Batch 44/8900 loss 4.260610 loss_att 6.467505 loss_ctc 7.516386 loss_rnnt 3.282722 hw_loss 0.192011 lr 0.00029789 rank 0
2023-03-01 05:08:17,683 DEBUG TRAIN Batch 44/9000 loss 2.098341 loss_att 4.357106 loss_ctc 2.617535 loss_rnnt 1.404471 hw_loss 0.324172 lr 0.00029788 rank 1
2023-03-01 05:08:17,683 DEBUG TRAIN Batch 44/9000 loss 4.956341 loss_att 8.472024 loss_ctc 8.222558 loss_rnnt 3.728425 hw_loss 0.167407 lr 0.00029788 rank 0
2023-03-01 05:08:40,578 DEBUG TRAIN Batch 44/9100 loss 12.085622 loss_att 17.483450 loss_ctc 20.051949 loss_rnnt 9.861759 hw_loss 0.153976 lr 0.00029787 rank 1
2023-03-01 05:08:40,578 DEBUG TRAIN Batch 44/9100 loss 1.005942 loss_att 3.045093 loss_ctc 1.498880 loss_rnnt 0.460396 hw_loss 0.134982 lr 0.00029787 rank 0
2023-03-01 05:09:02,747 DEBUG TRAIN Batch 44/9200 loss 5.816901 loss_att 10.713307 loss_ctc 16.893320 loss_rnnt 3.150222 hw_loss 0.394767 lr 0.00029785 rank 0
2023-03-01 05:09:02,747 DEBUG TRAIN Batch 44/9200 loss 5.274880 loss_att 7.651018 loss_ctc 13.919724 loss_rnnt 3.566431 hw_loss 0.151080 lr 0.00029785 rank 1
2023-03-01 05:09:35,475 DEBUG TRAIN Batch 44/9300 loss 6.855144 loss_att 9.442113 loss_ctc 10.528862 loss_rnnt 5.675901 hw_loss 0.322535 lr 0.00029784 rank 1
2023-03-01 05:09:35,475 DEBUG TRAIN Batch 44/9300 loss 10.520013 loss_att 13.363993 loss_ctc 17.629522 loss_rnnt 8.875545 hw_loss 0.239508 lr 0.00029784 rank 0
2023-03-01 05:09:58,346 DEBUG TRAIN Batch 44/9400 loss 3.815136 loss_att 5.449884 loss_ctc 4.651752 loss_rnnt 3.202285 hw_loss 0.326911 lr 0.00029783 rank 1
2023-03-01 05:09:58,348 DEBUG TRAIN Batch 44/9400 loss 6.150010 loss_att 7.709928 loss_ctc 9.356606 loss_rnnt 5.255680 hw_loss 0.290252 lr 0.00029783 rank 0
2023-03-01 05:10:21,458 DEBUG TRAIN Batch 44/9500 loss 3.622339 loss_att 6.382749 loss_ctc 7.161319 loss_rnnt 2.484404 hw_loss 0.213730 lr 0.00029781 rank 0
2023-03-01 05:10:21,460 DEBUG TRAIN Batch 44/9500 loss 1.781869 loss_att 3.519659 loss_ctc 3.309612 loss_rnnt 0.967851 hw_loss 0.492677 lr 0.00029781 rank 1
2023-03-01 05:10:44,034 DEBUG TRAIN Batch 44/9600 loss 5.169080 loss_att 9.950394 loss_ctc 6.906619 loss_rnnt 3.862339 hw_loss 0.222762 lr 0.00029780 rank 0
2023-03-01 05:10:44,034 DEBUG TRAIN Batch 44/9600 loss 1.982631 loss_att 4.814329 loss_ctc 4.201261 loss_rnnt 1.051211 hw_loss 0.129868 lr 0.00029780 rank 1
2023-03-01 05:11:16,250 DEBUG TRAIN Batch 44/9700 loss 11.472935 loss_att 14.104032 loss_ctc 27.537384 loss_rnnt 8.630911 hw_loss 0.326024 lr 0.00029779 rank 1
2023-03-01 05:11:16,252 DEBUG TRAIN Batch 44/9700 loss 5.897117 loss_att 9.717402 loss_ctc 6.893406 loss_rnnt 4.948949 hw_loss 0.096135 lr 0.00029779 rank 0
2023-03-01 05:11:38,214 DEBUG TRAIN Batch 44/9800 loss 6.529795 loss_att 10.906363 loss_ctc 13.317101 loss_rnnt 4.638921 hw_loss 0.207347 lr 0.00029777 rank 0
2023-03-01 05:11:38,215 DEBUG TRAIN Batch 44/9800 loss 3.741500 loss_att 6.321926 loss_ctc 5.898481 loss_rnnt 2.739581 hw_loss 0.371693 lr 0.00029777 rank 1
2023-03-01 05:12:00,883 DEBUG TRAIN Batch 44/9900 loss 11.010572 loss_att 14.493328 loss_ctc 18.033009 loss_rnnt 9.284254 hw_loss 0.175205 lr 0.00029776 rank 1
2023-03-01 05:12:00,884 DEBUG TRAIN Batch 44/9900 loss 6.461233 loss_att 7.249274 loss_ctc 11.658621 loss_rnnt 5.473766 hw_loss 0.256639 lr 0.00029776 rank 0
2023-03-01 05:12:33,627 DEBUG TRAIN Batch 44/10000 loss 3.683851 loss_att 5.779143 loss_ctc 4.676001 loss_rnnt 3.032593 hw_loss 0.187337 lr 0.00029775 rank 1
2023-03-01 05:12:33,627 DEBUG TRAIN Batch 44/10000 loss 4.806082 loss_att 6.852392 loss_ctc 7.155385 loss_rnnt 3.991095 hw_loss 0.173410 lr 0.00029775 rank 0
2023-03-01 05:12:56,865 DEBUG TRAIN Batch 44/10100 loss 6.679710 loss_att 9.385880 loss_ctc 11.346334 loss_rnnt 5.346987 hw_loss 0.317386 lr 0.00029773 rank 1
2023-03-01 05:12:56,866 DEBUG TRAIN Batch 44/10100 loss 8.820115 loss_att 9.695644 loss_ctc 14.013775 loss_rnnt 7.783984 hw_loss 0.316008 lr 0.00029773 rank 0
2023-03-01 05:13:19,224 DEBUG TRAIN Batch 44/10200 loss 8.133396 loss_att 11.344303 loss_ctc 15.338748 loss_rnnt 6.363358 hw_loss 0.313391 lr 0.00029772 rank 0
2023-03-01 05:13:19,225 DEBUG TRAIN Batch 44/10200 loss 7.612777 loss_att 8.682878 loss_ctc 11.380501 loss_rnnt 6.666421 hw_loss 0.431198 lr 0.00029772 rank 1
2023-03-01 05:13:42,640 DEBUG TRAIN Batch 44/10300 loss 6.532660 loss_att 9.766087 loss_ctc 10.433140 loss_rnnt 5.256645 hw_loss 0.204876 lr 0.00029771 rank 0
2023-03-01 05:13:42,640 DEBUG TRAIN Batch 44/10300 loss 7.550460 loss_att 10.104486 loss_ctc 13.489032 loss_rnnt 6.148454 hw_loss 0.186358 lr 0.00029771 rank 1
2023-03-01 05:14:14,582 DEBUG TRAIN Batch 44/10400 loss 4.706207 loss_att 7.510988 loss_ctc 10.520984 loss_rnnt 3.242677 hw_loss 0.238631 lr 0.00029769 rank 1
2023-03-01 05:14:14,584 DEBUG TRAIN Batch 44/10400 loss 3.519544 loss_att 5.000299 loss_ctc 3.830523 loss_rnnt 3.013239 hw_loss 0.316294 lr 0.00029769 rank 0
2023-03-01 05:14:36,719 DEBUG TRAIN Batch 44/10500 loss 3.472253 loss_att 5.715078 loss_ctc 5.597741 loss_rnnt 2.673133 hw_loss 0.125920 lr 0.00029768 rank 1
2023-03-01 05:14:36,720 DEBUG TRAIN Batch 44/10500 loss 3.454737 loss_att 6.076688 loss_ctc 9.706105 loss_rnnt 2.014361 hw_loss 0.154633 lr 0.00029768 rank 0
2023-03-01 05:14:59,167 DEBUG TRAIN Batch 44/10600 loss 7.088091 loss_att 10.753864 loss_ctc 9.724685 loss_rnnt 5.910861 hw_loss 0.173494 lr 0.00029767 rank 1
2023-03-01 05:14:59,168 DEBUG TRAIN Batch 44/10600 loss 6.728691 loss_att 9.794786 loss_ctc 13.683659 loss_rnnt 4.934401 hw_loss 0.475766 lr 0.00029767 rank 0
2023-03-01 05:15:32,025 DEBUG TRAIN Batch 44/10700 loss 6.905967 loss_att 9.446959 loss_ctc 11.660486 loss_rnnt 5.604462 hw_loss 0.298822 lr 0.00029765 rank 1
2023-03-01 05:15:32,026 DEBUG TRAIN Batch 44/10700 loss 6.160896 loss_att 6.624368 loss_ctc 9.709311 loss_rnnt 5.388902 hw_loss 0.386583 lr 0.00029765 rank 0
2023-03-01 05:15:54,514 DEBUG TRAIN Batch 44/10800 loss 5.959870 loss_att 7.964149 loss_ctc 11.507685 loss_rnnt 4.688793 hw_loss 0.244710 lr 0.00029764 rank 1
2023-03-01 05:15:54,515 DEBUG TRAIN Batch 44/10800 loss 11.470490 loss_att 11.398678 loss_ctc 18.110332 loss_rnnt 10.464927 hw_loss 0.252401 lr 0.00029764 rank 0
2023-03-01 05:16:17,804 DEBUG TRAIN Batch 44/10900 loss 8.624267 loss_att 11.480419 loss_ctc 14.709124 loss_rnnt 7.133375 hw_loss 0.203152 lr 0.00029763 rank 0
2023-03-01 05:16:17,805 DEBUG TRAIN Batch 44/10900 loss 8.400674 loss_att 8.577403 loss_ctc 11.835541 loss_rnnt 7.708822 hw_loss 0.372231 lr 0.00029763 rank 1
2023-03-01 05:16:40,590 DEBUG TRAIN Batch 44/11000 loss 5.690800 loss_att 8.693206 loss_ctc 9.957413 loss_rnnt 4.453436 hw_loss 0.127503 lr 0.00029761 rank 1
2023-03-01 05:16:40,593 DEBUG TRAIN Batch 44/11000 loss 14.308435 loss_att 17.435452 loss_ctc 24.763611 loss_rnnt 12.206437 hw_loss 0.154820 lr 0.00029761 rank 0
2023-03-01 05:17:12,724 DEBUG TRAIN Batch 44/11100 loss 5.812398 loss_att 8.164642 loss_ctc 10.645599 loss_rnnt 4.635227 hw_loss 0.116805 lr 0.00029760 rank 0
2023-03-01 05:17:12,725 DEBUG TRAIN Batch 44/11100 loss 13.070589 loss_att 20.053225 loss_ctc 24.231289 loss_rnnt 10.038149 hw_loss 0.277162 lr 0.00029760 rank 1
2023-03-01 05:17:35,154 DEBUG TRAIN Batch 44/11200 loss 3.311080 loss_att 6.058399 loss_ctc 5.121649 loss_rnnt 2.440220 hw_loss 0.149975 lr 0.00029759 rank 1
2023-03-01 05:17:35,156 DEBUG TRAIN Batch 44/11200 loss 7.852912 loss_att 11.563447 loss_ctc 11.851507 loss_rnnt 6.542487 hw_loss 0.065949 lr 0.00029759 rank 0
2023-03-01 05:17:57,393 DEBUG TRAIN Batch 44/11300 loss 6.793079 loss_att 8.249909 loss_ctc 7.395650 loss_rnnt 6.329649 hw_loss 0.171977 lr 0.00029757 rank 1
2023-03-01 05:17:57,394 DEBUG TRAIN Batch 44/11300 loss 10.301166 loss_att 11.111987 loss_ctc 16.861927 loss_rnnt 9.099647 hw_loss 0.308602 lr 0.00029757 rank 0
2023-03-01 05:18:29,335 DEBUG TRAIN Batch 44/11400 loss 4.550429 loss_att 8.006481 loss_ctc 10.615173 loss_rnnt 2.919852 hw_loss 0.245127 lr 0.00029756 rank 1
2023-03-01 05:18:29,336 DEBUG TRAIN Batch 44/11400 loss 6.000120 loss_att 9.470577 loss_ctc 12.613089 loss_rnnt 4.280052 hw_loss 0.270465 lr 0.00029756 rank 0
2023-03-01 05:18:52,674 DEBUG TRAIN Batch 44/11500 loss 7.497586 loss_att 9.240046 loss_ctc 12.764458 loss_rnnt 6.240098 hw_loss 0.387649 lr 0.00029755 rank 1
2023-03-01 05:18:52,676 DEBUG TRAIN Batch 44/11500 loss 5.170857 loss_att 7.671674 loss_ctc 8.693967 loss_rnnt 3.981678 hw_loss 0.411128 lr 0.00029755 rank 0
2023-03-01 05:19:15,655 DEBUG TRAIN Batch 44/11600 loss 13.614567 loss_att 14.890675 loss_ctc 19.596985 loss_rnnt 12.434161 hw_loss 0.239115 lr 0.00029754 rank 1
2023-03-01 05:19:15,656 DEBUG TRAIN Batch 44/11600 loss 7.453376 loss_att 10.673991 loss_ctc 11.793928 loss_rnnt 6.119925 hw_loss 0.207351 lr 0.00029754 rank 0
2023-03-01 05:19:38,035 DEBUG TRAIN Batch 44/11700 loss 10.343246 loss_att 13.710791 loss_ctc 17.541574 loss_rnnt 8.708261 hw_loss 0.003184 lr 0.00029752 rank 1
2023-03-01 05:19:38,036 DEBUG TRAIN Batch 44/11700 loss 5.187453 loss_att 8.941740 loss_ctc 10.541370 loss_rnnt 3.553109 hw_loss 0.318056 lr 0.00029752 rank 0
2023-03-01 05:20:11,123 DEBUG TRAIN Batch 44/11800 loss 7.924930 loss_att 10.097987 loss_ctc 10.152245 loss_rnnt 7.122005 hw_loss 0.133758 lr 0.00029751 rank 1
2023-03-01 05:20:11,124 DEBUG TRAIN Batch 44/11800 loss 3.888487 loss_att 6.271581 loss_ctc 6.691786 loss_rnnt 2.898377 hw_loss 0.261971 lr 0.00029751 rank 0
2023-03-01 05:20:33,268 DEBUG TRAIN Batch 44/11900 loss 6.772380 loss_att 11.296120 loss_ctc 12.951458 loss_rnnt 4.880052 hw_loss 0.306945 lr 0.00029750 rank 1
2023-03-01 05:20:33,271 DEBUG TRAIN Batch 44/11900 loss 7.432851 loss_att 9.028809 loss_ctc 13.797782 loss_rnnt 6.128928 hw_loss 0.255137 lr 0.00029750 rank 0
2023-03-01 05:20:55,982 DEBUG TRAIN Batch 44/12000 loss 8.786366 loss_att 12.785289 loss_ctc 16.263147 loss_rnnt 6.811800 hw_loss 0.333521 lr 0.00029748 rank 1
2023-03-01 05:20:55,983 DEBUG TRAIN Batch 44/12000 loss 4.277262 loss_att 5.241577 loss_ctc 7.537446 loss_rnnt 3.488575 hw_loss 0.302125 lr 0.00029748 rank 0
2023-03-01 05:21:28,044 DEBUG TRAIN Batch 44/12100 loss 7.643549 loss_att 9.330105 loss_ctc 14.202288 loss_rnnt 6.307775 hw_loss 0.232433 lr 0.00029747 rank 1
2023-03-01 05:21:28,044 DEBUG TRAIN Batch 44/12100 loss 6.870555 loss_att 10.946301 loss_ctc 11.313605 loss_rnnt 5.433822 hw_loss 0.054706 lr 0.00029747 rank 0
2023-03-01 05:21:51,923 DEBUG TRAIN Batch 44/12200 loss 6.764006 loss_att 9.295007 loss_ctc 15.278780 loss_rnnt 4.995730 hw_loss 0.237698 lr 0.00029746 rank 0
2023-03-01 05:21:51,924 DEBUG TRAIN Batch 44/12200 loss 8.896000 loss_att 9.572309 loss_ctc 13.210363 loss_rnnt 7.999719 hw_loss 0.348321 lr 0.00029746 rank 1
2023-03-01 05:22:13,907 DEBUG TRAIN Batch 44/12300 loss 1.207844 loss_att 3.265243 loss_ctc 0.971271 loss_rnnt 0.702457 hw_loss 0.235219 lr 0.00029744 rank 1
2023-03-01 05:22:13,909 DEBUG TRAIN Batch 44/12300 loss 2.040803 loss_att 4.487366 loss_ctc 3.562386 loss_rnnt 1.217327 hw_loss 0.246161 lr 0.00029744 rank 0
2023-03-01 05:22:36,739 DEBUG TRAIN Batch 44/12400 loss 7.993414 loss_att 11.613724 loss_ctc 15.330680 loss_rnnt 6.145882 hw_loss 0.272189 lr 0.00029743 rank 0
2023-03-01 05:22:36,739 DEBUG TRAIN Batch 44/12400 loss 8.111055 loss_att 10.325134 loss_ctc 8.682829 loss_rnnt 7.479241 hw_loss 0.211426 lr 0.00029743 rank 1
2023-03-01 05:23:12,212 DEBUG TRAIN Batch 44/12500 loss 5.368856 loss_att 9.485554 loss_ctc 10.483068 loss_rnnt 3.733986 hw_loss 0.243065 lr 0.00029742 rank 1
2023-03-01 05:23:12,213 DEBUG TRAIN Batch 44/12500 loss 3.770902 loss_att 6.469532 loss_ctc 5.697310 loss_rnnt 2.823702 hw_loss 0.282413 lr 0.00029742 rank 0
2023-03-01 05:23:34,360 DEBUG TRAIN Batch 44/12600 loss 4.247200 loss_att 7.949487 loss_ctc 10.776409 loss_rnnt 2.534209 hw_loss 0.191197 lr 0.00029740 rank 1
2023-03-01 05:23:34,361 DEBUG TRAIN Batch 44/12600 loss 5.761954 loss_att 6.503168 loss_ctc 8.542913 loss_rnnt 5.027696 hw_loss 0.403539 lr 0.00029740 rank 0
2023-03-01 05:23:57,025 DEBUG TRAIN Batch 44/12700 loss 6.105833 loss_att 9.277289 loss_ctc 9.699146 loss_rnnt 4.882677 hw_loss 0.205793 lr 0.00029739 rank 0
2023-03-01 05:23:57,025 DEBUG TRAIN Batch 44/12700 loss 4.040713 loss_att 5.484198 loss_ctc 4.158191 loss_rnnt 3.630541 hw_loss 0.198398 lr 0.00029739 rank 1
2023-03-01 05:24:32,314 DEBUG TRAIN Batch 44/12800 loss 9.297182 loss_att 10.981234 loss_ctc 13.537220 loss_rnnt 8.255287 hw_loss 0.262025 lr 0.00029738 rank 1
2023-03-01 05:24:32,314 DEBUG TRAIN Batch 44/12800 loss 6.688511 loss_att 9.538254 loss_ctc 11.269870 loss_rnnt 5.295614 hw_loss 0.397691 lr 0.00029738 rank 0
2023-03-01 05:24:55,170 DEBUG TRAIN Batch 44/12900 loss 5.494873 loss_att 6.313866 loss_ctc 5.788093 loss_rnnt 5.128042 hw_loss 0.307380 lr 0.00029736 rank 1
2023-03-01 05:24:55,171 DEBUG TRAIN Batch 44/12900 loss 6.069094 loss_att 7.560817 loss_ctc 7.536849 loss_rnnt 5.537506 hw_loss 0.070392 lr 0.00029736 rank 0
2023-03-01 05:25:17,777 DEBUG TRAIN Batch 44/13000 loss 4.266430 loss_att 7.634861 loss_ctc 6.472585 loss_rnnt 3.214555 hw_loss 0.157567 lr 0.00029735 rank 1
2023-03-01 05:25:17,778 DEBUG TRAIN Batch 44/13000 loss 5.038956 loss_att 10.672848 loss_ctc 9.449995 loss_rnnt 3.146879 hw_loss 0.332175 lr 0.00029735 rank 0
2023-03-01 05:25:40,732 DEBUG TRAIN Batch 44/13100 loss 9.624438 loss_att 11.610241 loss_ctc 13.803321 loss_rnnt 8.455339 hw_loss 0.402662 lr 0.00029734 rank 0
2023-03-01 05:25:40,732 DEBUG TRAIN Batch 44/13100 loss 7.041819 loss_att 11.322821 loss_ctc 15.391731 loss_rnnt 4.985846 hw_loss 0.162097 lr 0.00029734 rank 1
2023-03-01 05:26:15,584 DEBUG TRAIN Batch 44/13200 loss 12.181453 loss_att 16.650324 loss_ctc 17.611237 loss_rnnt 10.451697 hw_loss 0.210020 lr 0.00029732 rank 1
2023-03-01 05:26:15,585 DEBUG TRAIN Batch 44/13200 loss 1.656652 loss_att 3.628247 loss_ctc 3.586396 loss_rnnt 0.847832 hw_loss 0.294752 lr 0.00029732 rank 0
2023-03-01 05:26:38,437 DEBUG TRAIN Batch 44/13300 loss 16.410629 loss_att 19.540474 loss_ctc 23.620449 loss_rnnt 14.673889 hw_loss 0.280237 lr 0.00029731 rank 1
2023-03-01 05:26:38,437 DEBUG TRAIN Batch 44/13300 loss 7.585963 loss_att 9.076846 loss_ctc 10.538314 loss_rnnt 6.835458 hw_loss 0.110027 lr 0.00029731 rank 0
2023-03-01 05:27:01,041 DEBUG TRAIN Batch 44/13400 loss 6.189741 loss_att 11.188211 loss_ctc 11.350940 loss_rnnt 4.329908 hw_loss 0.322460 lr 0.00029730 rank 0
2023-03-01 05:27:01,042 DEBUG TRAIN Batch 44/13400 loss 6.800629 loss_att 9.222277 loss_ctc 10.122005 loss_rnnt 5.740966 hw_loss 0.248403 lr 0.00029730 rank 1
2023-03-01 05:27:24,677 DEBUG TRAIN Batch 44/13500 loss 3.461147 loss_att 7.279315 loss_ctc 8.583075 loss_rnnt 1.952872 hw_loss 0.115721 lr 0.00029729 rank 1
2023-03-01 05:27:24,680 DEBUG TRAIN Batch 44/13500 loss 7.901792 loss_att 14.411296 loss_ctc 15.010118 loss_rnnt 5.480640 hw_loss 0.321513 lr 0.00029729 rank 0
2023-03-01 05:28:00,330 DEBUG TRAIN Batch 44/13600 loss 4.707260 loss_att 7.430448 loss_ctc 7.512014 loss_rnnt 3.621539 hw_loss 0.313343 lr 0.00029727 rank 0
2023-03-01 05:28:00,330 DEBUG TRAIN Batch 44/13600 loss 16.589296 loss_att 18.461231 loss_ctc 28.735260 loss_rnnt 14.445103 hw_loss 0.281896 lr 0.00029727 rank 1
2023-03-01 05:28:23,111 DEBUG TRAIN Batch 44/13700 loss 6.728271 loss_att 9.757750 loss_ctc 9.500250 loss_rnnt 5.612943 hw_loss 0.262190 lr 0.00029726 rank 1
2023-03-01 05:28:23,113 DEBUG TRAIN Batch 44/13700 loss 4.453101 loss_att 6.136550 loss_ctc 7.868218 loss_rnnt 3.499609 hw_loss 0.302725 lr 0.00029726 rank 0
2023-03-01 05:28:46,044 DEBUG TRAIN Batch 44/13800 loss 10.335485 loss_att 14.273943 loss_ctc 18.866222 loss_rnnt 8.265376 hw_loss 0.271851 lr 0.00029725 rank 1
2023-03-01 05:28:46,044 DEBUG TRAIN Batch 44/13800 loss 7.169212 loss_att 8.453639 loss_ctc 9.274851 loss_rnnt 6.452931 hw_loss 0.334957 lr 0.00029725 rank 0
2023-03-01 05:29:22,819 DEBUG TRAIN Batch 44/13900 loss 8.228196 loss_att 8.229127 loss_ctc 8.966753 loss_rnnt 7.863552 hw_loss 0.498719 lr 0.00029723 rank 0
2023-03-01 05:29:22,819 DEBUG TRAIN Batch 44/13900 loss 6.325387 loss_att 9.451082 loss_ctc 8.451160 loss_rnnt 5.296723 hw_loss 0.225163 lr 0.00029723 rank 1
2023-03-01 05:29:45,612 DEBUG TRAIN Batch 44/14000 loss 5.543902 loss_att 6.721791 loss_ctc 8.492773 loss_rnnt 4.777181 hw_loss 0.258675 lr 0.00029722 rank 1
2023-03-01 05:29:45,614 DEBUG TRAIN Batch 44/14000 loss 3.691234 loss_att 7.208629 loss_ctc 5.225174 loss_rnnt 2.618465 hw_loss 0.308934 lr 0.00029722 rank 0
2023-03-01 05:30:08,355 DEBUG TRAIN Batch 44/14100 loss 3.669663 loss_att 6.502244 loss_ctc 7.782388 loss_rnnt 2.397363 hw_loss 0.295163 lr 0.00029721 rank 0
2023-03-01 05:30:08,355 DEBUG TRAIN Batch 44/14100 loss 8.886529 loss_att 11.676478 loss_ctc 16.644644 loss_rnnt 7.177845 hw_loss 0.218022 lr 0.00029721 rank 1
2023-03-01 05:30:30,991 DEBUG TRAIN Batch 44/14200 loss 10.324935 loss_att 15.669910 loss_ctc 13.657412 loss_rnnt 8.705865 hw_loss 0.198272 lr 0.00029719 rank 0
2023-03-01 05:30:30,991 DEBUG TRAIN Batch 44/14200 loss 12.942212 loss_att 17.547104 loss_ctc 19.899487 loss_rnnt 10.977768 hw_loss 0.217179 lr 0.00029719 rank 1
2023-03-01 05:31:06,603 DEBUG TRAIN Batch 44/14300 loss 1.391468 loss_att 4.915924 loss_ctc 1.725879 loss_rnnt 0.534082 hw_loss 0.202324 lr 0.00029718 rank 1
2023-03-01 05:31:06,604 DEBUG TRAIN Batch 44/14300 loss 7.604160 loss_att 8.273397 loss_ctc 12.656191 loss_rnnt 6.656137 hw_loss 0.263571 lr 0.00029718 rank 0
2023-03-01 05:31:29,140 DEBUG TRAIN Batch 44/14400 loss 10.816044 loss_att 14.097452 loss_ctc 21.741692 loss_rnnt 8.550037 hw_loss 0.286822 lr 0.00029717 rank 1
2023-03-01 05:31:29,140 DEBUG TRAIN Batch 44/14400 loss 8.092915 loss_att 12.403828 loss_ctc 13.342435 loss_rnnt 6.412488 hw_loss 0.221825 lr 0.00029717 rank 0
2023-03-01 05:31:52,258 DEBUG TRAIN Batch 44/14500 loss 7.683187 loss_att 7.561203 loss_ctc 9.887213 loss_rnnt 7.197721 hw_loss 0.404988 lr 0.00029715 rank 0
2023-03-01 05:31:52,258 DEBUG TRAIN Batch 44/14500 loss 6.428126 loss_att 8.923368 loss_ctc 15.111901 loss_rnnt 4.631550 hw_loss 0.261921 lr 0.00029715 rank 1
2023-03-01 05:32:27,558 DEBUG TRAIN Batch 44/14600 loss 4.247838 loss_att 6.798448 loss_ctc 4.737949 loss_rnnt 3.528205 hw_loss 0.270305 lr 0.00029714 rank 0
2023-03-01 05:32:27,558 DEBUG TRAIN Batch 44/14600 loss 3.254505 loss_att 5.651754 loss_ctc 4.406445 loss_rnnt 2.470581 hw_loss 0.282905 lr 0.00029714 rank 1
2023-03-01 05:32:50,534 DEBUG TRAIN Batch 44/14700 loss 9.172146 loss_att 12.983362 loss_ctc 17.859875 loss_rnnt 7.181125 hw_loss 0.132028 lr 0.00029713 rank 1
2023-03-01 05:32:50,534 DEBUG TRAIN Batch 44/14700 loss 8.610949 loss_att 11.551399 loss_ctc 20.260700 loss_rnnt 6.279242 hw_loss 0.356844 lr 0.00029713 rank 0
2023-03-01 05:33:12,930 DEBUG TRAIN Batch 44/14800 loss 18.364983 loss_att 19.909184 loss_ctc 29.973166 loss_rnnt 16.420067 hw_loss 0.165597 lr 0.00029711 rank 1
2023-03-01 05:33:12,932 DEBUG TRAIN Batch 44/14800 loss 9.738923 loss_att 10.840370 loss_ctc 11.226873 loss_rnnt 9.210217 hw_loss 0.206291 lr 0.00029711 rank 0
2023-03-01 05:33:36,179 DEBUG TRAIN Batch 44/14900 loss 5.290164 loss_att 7.079401 loss_ctc 9.917950 loss_rnnt 4.188416 hw_loss 0.237867 lr 0.00029710 rank 0
2023-03-01 05:33:36,179 DEBUG TRAIN Batch 44/14900 loss 4.211907 loss_att 6.596991 loss_ctc 6.246154 loss_rnnt 3.293484 hw_loss 0.319075 lr 0.00029710 rank 1
2023-03-01 05:34:11,326 DEBUG TRAIN Batch 44/15000 loss 3.509600 loss_att 7.111979 loss_ctc 6.002943 loss_rnnt 2.345125 hw_loss 0.209163 lr 0.00029709 rank 1
2023-03-01 05:34:11,327 DEBUG TRAIN Batch 44/15000 loss 5.644676 loss_att 6.701221 loss_ctc 8.302101 loss_rnnt 4.885490 hw_loss 0.362911 lr 0.00029709 rank 0
2023-03-01 05:34:34,255 DEBUG TRAIN Batch 44/15100 loss 3.837221 loss_att 6.033097 loss_ctc 5.002389 loss_rnnt 3.085873 hw_loss 0.294032 lr 0.00029708 rank 1
2023-03-01 05:34:34,256 DEBUG TRAIN Batch 44/15100 loss 5.671315 loss_att 7.707715 loss_ctc 10.052945 loss_rnnt 4.554831 hw_loss 0.234348 lr 0.00029708 rank 0
2023-03-01 05:34:57,188 DEBUG TRAIN Batch 44/15200 loss 3.392384 loss_att 5.736005 loss_ctc 7.451880 loss_rnnt 2.253708 hw_loss 0.241285 lr 0.00029706 rank 1
2023-03-01 05:34:57,189 DEBUG TRAIN Batch 44/15200 loss 2.350810 loss_att 5.234420 loss_ctc 3.614831 loss_rnnt 1.502992 hw_loss 0.192300 lr 0.00029706 rank 0
2023-03-01 05:35:32,617 DEBUG TRAIN Batch 44/15300 loss 4.725476 loss_att 7.386325 loss_ctc 9.291582 loss_rnnt 3.449240 hw_loss 0.253597 lr 0.00029705 rank 0
2023-03-01 05:35:32,616 DEBUG TRAIN Batch 44/15300 loss 7.885669 loss_att 7.739209 loss_ctc 10.674810 loss_rnnt 7.343930 hw_loss 0.373398 lr 0.00029705 rank 1
2023-03-01 05:35:55,602 DEBUG TRAIN Batch 44/15400 loss 8.572529 loss_att 15.698324 loss_ctc 17.108454 loss_rnnt 5.898769 hw_loss 0.207146 lr 0.00029704 rank 1
2023-03-01 05:35:55,604 DEBUG TRAIN Batch 44/15400 loss 12.521701 loss_att 17.486889 loss_ctc 24.209805 loss_rnnt 9.937535 hw_loss 0.061337 lr 0.00029704 rank 0
2023-03-01 05:36:17,510 DEBUG TRAIN Batch 44/15500 loss 6.850288 loss_att 8.632558 loss_ctc 10.815326 loss_rnnt 5.879525 hw_loss 0.160570 lr 0.00029702 rank 1
2023-03-01 05:36:17,511 DEBUG TRAIN Batch 44/15500 loss 8.334718 loss_att 11.409951 loss_ctc 14.889427 loss_rnnt 6.774234 hw_loss 0.134017 lr 0.00029702 rank 0
2023-03-01 05:36:40,283 DEBUG TRAIN Batch 44/15600 loss 5.608360 loss_att 9.342610 loss_ctc 12.351009 loss_rnnt 3.788753 hw_loss 0.325757 lr 0.00029701 rank 1
2023-03-01 05:36:40,283 DEBUG TRAIN Batch 44/15600 loss 1.670130 loss_att 4.277715 loss_ctc 3.166753 loss_rnnt 0.862542 hw_loss 0.162228 lr 0.00029701 rank 0
2023-03-01 05:37:15,609 DEBUG TRAIN Batch 44/15700 loss 14.894364 loss_att 17.576042 loss_ctc 26.156775 loss_rnnt 12.773188 hw_loss 0.155972 lr 0.00029700 rank 1
2023-03-01 05:37:15,610 DEBUG TRAIN Batch 44/15700 loss 1.652128 loss_att 4.251731 loss_ctc 3.977638 loss_rnnt 0.680202 hw_loss 0.266131 lr 0.00029700 rank 0
2023-03-01 05:37:38,866 DEBUG TRAIN Batch 44/15800 loss 7.915044 loss_att 10.699420 loss_ctc 13.841591 loss_rnnt 6.433828 hw_loss 0.251502 lr 0.00029698 rank 1
2023-03-01 05:37:38,867 DEBUG TRAIN Batch 44/15800 loss 9.550517 loss_att 11.454388 loss_ctc 17.695755 loss_rnnt 8.003203 hw_loss 0.150953 lr 0.00029698 rank 0
2023-03-01 05:38:01,357 DEBUG TRAIN Batch 44/15900 loss 7.716059 loss_att 11.871321 loss_ctc 13.386614 loss_rnnt 5.858064 hw_loss 0.507879 lr 0.00029697 rank 1
2023-03-01 05:38:01,357 DEBUG TRAIN Batch 44/15900 loss 7.873488 loss_att 11.161088 loss_ctc 15.123886 loss_rnnt 6.060040 hw_loss 0.354766 lr 0.00029697 rank 0
2023-03-01 05:38:24,499 DEBUG TRAIN Batch 44/16000 loss 3.774462 loss_att 6.143203 loss_ctc 5.813424 loss_rnnt 2.918855 hw_loss 0.206246 lr 0.00029696 rank 1
2023-03-01 05:38:24,500 DEBUG TRAIN Batch 44/16000 loss 5.944877 loss_att 8.990964 loss_ctc 14.013960 loss_rnnt 4.174697 hw_loss 0.159533 lr 0.00029696 rank 0
2023-03-01 05:38:58,592 DEBUG TRAIN Batch 44/16100 loss 11.016528 loss_att 14.588354 loss_ctc 19.807037 loss_rnnt 9.081512 hw_loss 0.091092 lr 0.00029694 rank 0
2023-03-01 05:38:58,592 DEBUG TRAIN Batch 44/16100 loss 5.260131 loss_att 7.485703 loss_ctc 10.489119 loss_rnnt 3.927771 hw_loss 0.356338 lr 0.00029694 rank 1
2023-03-01 05:39:21,311 DEBUG TRAIN Batch 44/16200 loss 3.516993 loss_att 7.386847 loss_ctc 6.501285 loss_rnnt 2.226947 hw_loss 0.221567 lr 0.00029693 rank 1
2023-03-01 05:39:21,312 DEBUG TRAIN Batch 44/16200 loss 10.319672 loss_att 14.778341 loss_ctc 27.117123 loss_rnnt 7.026246 hw_loss 0.303811 lr 0.00029693 rank 0
2023-03-01 05:39:44,154 DEBUG TRAIN Batch 44/16300 loss 4.227295 loss_att 6.899025 loss_ctc 6.359822 loss_rnnt 3.246645 hw_loss 0.303689 lr 0.00029692 rank 0
2023-03-01 05:39:44,155 DEBUG TRAIN Batch 44/16300 loss 10.667888 loss_att 14.371577 loss_ctc 16.006224 loss_rnnt 9.070780 hw_loss 0.271108 lr 0.00029692 rank 1
2023-03-01 05:40:20,057 DEBUG TRAIN Batch 44/16400 loss 9.871423 loss_att 11.794587 loss_ctc 19.410774 loss_rnnt 8.148804 hw_loss 0.123887 lr 0.00029691 rank 1
2023-03-01 05:40:20,057 DEBUG TRAIN Batch 44/16400 loss 4.105990 loss_att 6.055004 loss_ctc 5.867072 loss_rnnt 3.346411 hw_loss 0.253059 lr 0.00029691 rank 0
2023-03-01 05:40:43,279 DEBUG TRAIN Batch 44/16500 loss 3.132071 loss_att 5.815186 loss_ctc 5.341050 loss_rnnt 2.196773 hw_loss 0.195271 lr 0.00029689 rank 0
2023-03-01 05:40:43,280 DEBUG TRAIN Batch 44/16500 loss 8.512338 loss_att 11.980048 loss_ctc 13.733866 loss_rnnt 6.984904 hw_loss 0.258164 lr 0.00029689 rank 1
2023-03-01 05:41:05,937 DEBUG TRAIN Batch 44/16600 loss 5.826819 loss_att 6.546109 loss_ctc 8.457238 loss_rnnt 5.157612 hw_loss 0.327425 lr 0.00029688 rank 1
2023-03-01 05:41:05,938 DEBUG TRAIN Batch 44/16600 loss 5.543461 loss_att 8.423040 loss_ctc 9.751960 loss_rnnt 4.327812 hw_loss 0.147373 lr 0.00029688 rank 0
2023-03-01 05:41:28,873 DEBUG TRAIN Batch 44/16700 loss 8.453561 loss_att 14.506255 loss_ctc 15.231242 loss_rnnt 6.255486 hw_loss 0.157210 lr 0.00029687 rank 0
2023-03-01 05:41:28,874 DEBUG TRAIN Batch 44/16700 loss 7.926154 loss_att 12.387417 loss_ctc 14.133049 loss_rnnt 6.028690 hw_loss 0.333048 lr 0.00029687 rank 1
2023-03-01 05:42:04,172 DEBUG TRAIN Batch 44/16800 loss 7.600415 loss_att 9.834426 loss_ctc 9.264064 loss_rnnt 6.771754 hw_loss 0.300073 lr 0.00029685 rank 0
2023-03-01 05:42:04,172 DEBUG TRAIN Batch 44/16800 loss 6.225690 loss_att 8.551256 loss_ctc 9.244286 loss_rnnt 5.258508 hw_loss 0.186729 lr 0.00029685 rank 1
2023-03-01 05:42:26,129 DEBUG TRAIN Batch 44/16900 loss 6.722184 loss_att 9.245316 loss_ctc 11.296692 loss_rnnt 5.450225 hw_loss 0.295122 lr 0.00029684 rank 1
2023-03-01 05:42:26,129 DEBUG TRAIN Batch 44/16900 loss 2.153774 loss_att 4.276014 loss_ctc 4.150318 loss_rnnt 1.231306 hw_loss 0.434650 lr 0.00029684 rank 0
2023-03-01 05:42:48,500 DEBUG TRAIN Batch 44/17000 loss 7.625906 loss_att 9.647128 loss_ctc 9.395088 loss_rnnt 6.836055 hw_loss 0.280717 lr 0.00029683 rank 0
2023-03-01 05:42:48,500 DEBUG TRAIN Batch 44/17000 loss 11.246649 loss_att 17.478226 loss_ctc 20.827820 loss_rnnt 8.644620 hw_loss 0.146671 lr 0.00029683 rank 1
2023-03-01 05:43:24,039 DEBUG TRAIN Batch 44/17100 loss 3.926787 loss_att 7.095469 loss_ctc 5.840414 loss_rnnt 2.867629 hw_loss 0.319259 lr 0.00029681 rank 1
2023-03-01 05:43:24,039 DEBUG TRAIN Batch 44/17100 loss 11.878349 loss_att 13.358269 loss_ctc 20.761316 loss_rnnt 10.300463 hw_loss 0.182827 lr 0.00029681 rank 0
2023-03-01 05:43:46,303 DEBUG TRAIN Batch 44/17200 loss 8.875895 loss_att 12.787870 loss_ctc 21.669249 loss_rnnt 6.303084 hw_loss 0.158689 lr 0.00029680 rank 1
2023-03-01 05:43:46,303 DEBUG TRAIN Batch 44/17200 loss 2.897864 loss_att 5.261224 loss_ctc 2.845337 loss_rnnt 2.217612 hw_loss 0.402346 lr 0.00029680 rank 0
2023-03-01 05:44:07,947 DEBUG TRAIN Batch 44/17300 loss 3.242098 loss_att 6.625609 loss_ctc 10.029911 loss_rnnt 1.475929 hw_loss 0.345796 lr 0.00029679 rank 0
2023-03-01 05:44:07,948 DEBUG TRAIN Batch 44/17300 loss 5.206287 loss_att 7.348455 loss_ctc 8.657493 loss_rnnt 4.220862 hw_loss 0.181558 lr 0.00029679 rank 1
2023-03-01 05:44:30,636 DEBUG TRAIN Batch 44/17400 loss 4.983872 loss_att 6.225894 loss_ctc 5.841329 loss_rnnt 4.472111 hw_loss 0.279430 lr 0.00029677 rank 1
2023-03-01 05:44:30,637 DEBUG TRAIN Batch 44/17400 loss 6.802236 loss_att 8.879547 loss_ctc 11.915706 loss_rnnt 5.607304 hw_loss 0.183138 lr 0.00029677 rank 0
2023-03-01 05:45:06,030 DEBUG TRAIN Batch 44/17500 loss 4.748910 loss_att 8.986869 loss_ctc 11.936512 loss_rnnt 2.808520 hw_loss 0.252096 lr 0.00029676 rank 1
2023-03-01 05:45:06,031 DEBUG TRAIN Batch 44/17500 loss 3.797653 loss_att 5.782943 loss_ctc 6.398339 loss_rnnt 3.019791 hw_loss 0.063835 lr 0.00029676 rank 0
2023-03-01 05:45:28,435 DEBUG TRAIN Batch 44/17600 loss 7.676808 loss_att 10.734856 loss_ctc 13.441637 loss_rnnt 6.141236 hw_loss 0.291221 lr 0.00029675 rank 1
2023-03-01 05:45:28,436 DEBUG TRAIN Batch 44/17600 loss 8.479400 loss_att 10.421363 loss_ctc 14.994934 loss_rnnt 7.102608 hw_loss 0.224364 lr 0.00029675 rank 0
2023-03-01 05:45:50,923 DEBUG TRAIN Batch 44/17700 loss 4.992977 loss_att 7.867266 loss_ctc 9.177002 loss_rnnt 3.735285 hw_loss 0.234307 lr 0.00029674 rank 1
2023-03-01 05:45:50,925 DEBUG TRAIN Batch 44/17700 loss 10.974698 loss_att 14.582735 loss_ctc 19.987400 loss_rnnt 8.923531 hw_loss 0.239748 lr 0.00029674 rank 0
2023-03-01 05:46:25,914 DEBUG TRAIN Batch 44/17800 loss 4.278355 loss_att 7.802012 loss_ctc 9.048114 loss_rnnt 2.816151 hw_loss 0.227819 lr 0.00029672 rank 0
2023-03-01 05:46:25,915 DEBUG TRAIN Batch 44/17800 loss 10.215566 loss_att 12.477413 loss_ctc 18.758865 loss_rnnt 8.470191 hw_loss 0.288560 lr 0.00029672 rank 1
2023-03-01 05:46:48,477 DEBUG TRAIN Batch 44/17900 loss 3.891711 loss_att 7.218486 loss_ctc 5.658016 loss_rnnt 2.919296 hw_loss 0.134162 lr 0.00029671 rank 1
2023-03-01 05:46:48,479 DEBUG TRAIN Batch 44/17900 loss 4.218153 loss_att 7.704379 loss_ctc 7.516712 loss_rnnt 2.986758 hw_loss 0.176891 lr 0.00029671 rank 0
2023-03-01 05:47:11,311 DEBUG TRAIN Batch 44/18000 loss 8.556721 loss_att 10.674376 loss_ctc 12.363513 loss_rnnt 7.542868 hw_loss 0.155156 lr 0.00029670 rank 1
2023-03-01 05:47:11,312 DEBUG TRAIN Batch 44/18000 loss 10.033824 loss_att 14.103262 loss_ctc 17.096077 loss_rnnt 8.175976 hw_loss 0.191863 lr 0.00029670 rank 0
2023-03-01 05:47:33,844 DEBUG TRAIN Batch 44/18100 loss 10.344543 loss_att 12.654011 loss_ctc 18.234133 loss_rnnt 8.728122 hw_loss 0.192343 lr 0.00029668 rank 1
2023-03-01 05:47:33,845 DEBUG TRAIN Batch 44/18100 loss 10.746525 loss_att 18.011450 loss_ctc 24.009209 loss_rnnt 7.414798 hw_loss 0.206972 lr 0.00029668 rank 0
2023-03-01 05:48:08,636 DEBUG TRAIN Batch 44/18200 loss 9.873955 loss_att 12.276138 loss_ctc 19.255173 loss_rnnt 8.105712 hw_loss 0.069333 lr 0.00029667 rank 0
2023-03-01 05:48:08,636 DEBUG TRAIN Batch 44/18200 loss 11.104769 loss_att 13.631316 loss_ctc 13.224964 loss_rnnt 10.160151 hw_loss 0.293654 lr 0.00029667 rank 1
2023-03-01 05:48:31,421 DEBUG TRAIN Batch 44/18300 loss 18.838211 loss_att 21.937305 loss_ctc 29.139181 loss_rnnt 16.793638 hw_loss 0.096167 lr 0.00029666 rank 0
2023-03-01 05:48:31,421 DEBUG TRAIN Batch 44/18300 loss 5.204065 loss_att 9.095378 loss_ctc 9.933286 loss_rnnt 3.640571 hw_loss 0.290005 lr 0.00029666 rank 1
2023-03-01 05:48:54,782 DEBUG TRAIN Batch 44/18400 loss 9.844184 loss_att 12.671720 loss_ctc 15.787323 loss_rnnt 8.402262 hw_loss 0.157493 lr 0.00029664 rank 0
2023-03-01 05:48:54,784 DEBUG TRAIN Batch 44/18400 loss 6.803806 loss_att 9.350790 loss_ctc 9.259336 loss_rnnt 5.832599 hw_loss 0.252015 lr 0.00029664 rank 1
2023-03-01 05:49:30,583 DEBUG TRAIN Batch 44/18500 loss 6.233893 loss_att 11.054935 loss_ctc 18.163328 loss_rnnt 3.618099 hw_loss 0.114365 lr 0.00029663 rank 0
2023-03-01 05:49:30,583 DEBUG TRAIN Batch 44/18500 loss 6.266437 loss_att 7.713623 loss_ctc 11.717978 loss_rnnt 5.012218 hw_loss 0.446079 lr 0.00029663 rank 1
2023-03-01 05:49:53,384 DEBUG TRAIN Batch 44/18600 loss 6.536430 loss_att 8.406940 loss_ctc 8.957410 loss_rnnt 5.637897 hw_loss 0.378063 lr 0.00029662 rank 1
2023-03-01 05:49:53,384 DEBUG TRAIN Batch 44/18600 loss 4.239874 loss_att 8.582940 loss_ctc 7.778127 loss_rnnt 2.840559 hw_loss 0.110503 lr 0.00029662 rank 0
2023-03-01 05:50:16,144 DEBUG TRAIN Batch 44/18700 loss 3.984927 loss_att 7.046767 loss_ctc 4.485992 loss_rnnt 3.221756 hw_loss 0.157491 lr 0.00029660 rank 1
2023-03-01 05:50:16,146 DEBUG TRAIN Batch 44/18700 loss 5.407029 loss_att 9.640882 loss_ctc 9.309167 loss_rnnt 3.989001 hw_loss 0.095572 lr 0.00029660 rank 0
2023-03-01 05:50:38,608 DEBUG TRAIN Batch 44/18800 loss 6.454438 loss_att 8.074242 loss_ctc 10.697727 loss_rnnt 5.448570 hw_loss 0.217753 lr 0.00029659 rank 1
2023-03-01 05:50:38,610 DEBUG TRAIN Batch 44/18800 loss 13.710199 loss_att 17.608059 loss_ctc 23.741581 loss_rnnt 11.385695 hw_loss 0.388903 lr 0.00029659 rank 0
2023-03-01 05:51:14,298 DEBUG TRAIN Batch 44/18900 loss 10.428733 loss_att 12.868866 loss_ctc 13.854784 loss_rnnt 9.371368 hw_loss 0.210997 lr 0.00029658 rank 1
2023-03-01 05:51:14,299 DEBUG TRAIN Batch 44/18900 loss 8.655790 loss_att 12.064606 loss_ctc 15.896105 loss_rnnt 6.860843 hw_loss 0.277142 lr 0.00029658 rank 0
2023-03-01 05:51:37,177 DEBUG TRAIN Batch 44/19000 loss 10.882286 loss_att 13.058313 loss_ctc 16.698204 loss_rnnt 9.495161 hw_loss 0.330871 lr 0.00029657 rank 1
2023-03-01 05:51:37,177 DEBUG TRAIN Batch 44/19000 loss 4.374849 loss_att 5.702784 loss_ctc 6.868714 loss_rnnt 3.525129 hw_loss 0.471784 lr 0.00029657 rank 0
2023-03-01 05:51:59,963 DEBUG TRAIN Batch 44/19100 loss 4.013395 loss_att 6.716700 loss_ctc 5.999963 loss_rnnt 3.094000 hw_loss 0.213482 lr 0.00029655 rank 1
2023-03-01 05:51:59,963 DEBUG TRAIN Batch 44/19100 loss 3.039012 loss_att 6.907765 loss_ctc 5.811819 loss_rnnt 1.721723 hw_loss 0.325933 lr 0.00029655 rank 0
2023-03-01 05:52:23,410 DEBUG TRAIN Batch 44/19200 loss 2.763258 loss_att 6.794807 loss_ctc 4.312973 loss_rnnt 1.686943 hw_loss 0.118831 lr 0.00029654 rank 1
2023-03-01 05:52:23,411 DEBUG TRAIN Batch 44/19200 loss 16.797688 loss_att 18.434891 loss_ctc 22.907354 loss_rnnt 15.490435 hw_loss 0.309728 lr 0.00029654 rank 0
2023-03-01 05:52:57,522 DEBUG TRAIN Batch 44/19300 loss 4.792469 loss_att 10.594655 loss_ctc 8.623087 loss_rnnt 3.022167 hw_loss 0.185842 lr 0.00029653 rank 1
2023-03-01 05:52:57,523 DEBUG TRAIN Batch 44/19300 loss 7.985743 loss_att 9.791743 loss_ctc 12.234675 loss_rnnt 6.921406 hw_loss 0.256148 lr 0.00029653 rank 0
2023-03-01 05:53:19,832 DEBUG TRAIN Batch 44/19400 loss 9.338456 loss_att 11.731273 loss_ctc 12.401049 loss_rnnt 8.332894 hw_loss 0.222473 lr 0.00029651 rank 1
2023-03-01 05:53:19,833 DEBUG TRAIN Batch 44/19400 loss 18.220814 loss_att 22.128649 loss_ctc 30.316895 loss_rnnt 15.696439 hw_loss 0.243746 lr 0.00029651 rank 0
2023-03-01 05:53:42,075 DEBUG TRAIN Batch 44/19500 loss 5.792445 loss_att 8.250460 loss_ctc 6.990344 loss_rnnt 4.962755 hw_loss 0.334439 lr 0.00029650 rank 1
2023-03-01 05:53:42,075 DEBUG TRAIN Batch 44/19500 loss 8.504964 loss_att 12.926702 loss_ctc 14.430431 loss_rnnt 6.674827 hw_loss 0.291987 lr 0.00029650 rank 0
2023-03-01 05:54:18,502 DEBUG TRAIN Batch 44/19600 loss 10.041433 loss_att 13.274057 loss_ctc 13.114332 loss_rnnt 8.874046 hw_loss 0.208391 lr 0.00029649 rank 1
2023-03-01 05:54:18,503 DEBUG TRAIN Batch 44/19600 loss 7.092859 loss_att 8.972991 loss_ctc 12.367370 loss_rnnt 5.858202 hw_loss 0.291302 lr 0.00029649 rank 0
2023-03-01 05:54:41,837 DEBUG TRAIN Batch 44/19700 loss 8.586268 loss_att 10.439754 loss_ctc 13.336637 loss_rnnt 7.437801 hw_loss 0.270726 lr 0.00029647 rank 1
2023-03-01 05:54:41,838 DEBUG TRAIN Batch 44/19700 loss 7.395710 loss_att 7.054545 loss_ctc 10.477725 loss_rnnt 6.896737 hw_loss 0.293010 lr 0.00029647 rank 0
2023-03-01 05:55:04,435 DEBUG TRAIN Batch 44/19800 loss 7.264682 loss_att 6.967967 loss_ctc 10.791630 loss_rnnt 6.683183 hw_loss 0.319842 lr 0.00029646 rank 1
2023-03-01 05:55:04,436 DEBUG TRAIN Batch 44/19800 loss 10.644674 loss_att 11.676983 loss_ctc 12.549750 loss_rnnt 10.084938 hw_loss 0.186119 lr 0.00029646 rank 0
2023-03-01 05:55:27,597 DEBUG TRAIN Batch 44/19900 loss 17.220295 loss_att 18.040464 loss_ctc 23.919128 loss_rnnt 16.102493 hw_loss 0.113606 lr 0.00029645 rank 1
2023-03-01 05:55:27,598 DEBUG TRAIN Batch 44/19900 loss 3.096247 loss_att 7.468640 loss_ctc 6.638887 loss_rnnt 1.599551 hw_loss 0.280999 lr 0.00029645 rank 0
2023-03-01 05:56:03,123 DEBUG TRAIN Batch 44/20000 loss 3.959981 loss_att 8.883528 loss_ctc 7.678401 loss_rnnt 2.389077 hw_loss 0.169511 lr 0.00029644 rank 0
2023-03-01 05:56:03,123 DEBUG TRAIN Batch 44/20000 loss 8.080529 loss_att 8.908850 loss_ctc 9.554008 loss_rnnt 7.583473 hw_loss 0.252990 lr 0.00029644 rank 1
2023-03-01 05:56:25,425 DEBUG TRAIN Batch 44/20100 loss 17.860199 loss_att 20.205175 loss_ctc 34.388683 loss_rnnt 15.054096 hw_loss 0.249952 lr 0.00029642 rank 1
2023-03-01 05:56:25,426 DEBUG TRAIN Batch 44/20100 loss 3.219654 loss_att 5.901978 loss_ctc 6.496174 loss_rnnt 2.135140 hw_loss 0.208463 lr 0.00029642 rank 0
2023-03-01 05:56:48,338 DEBUG TRAIN Batch 44/20200 loss 5.019296 loss_att 7.568067 loss_ctc 8.766315 loss_rnnt 3.897231 hw_loss 0.211328 lr 0.00029641 rank 0
2023-03-01 05:56:48,338 DEBUG TRAIN Batch 44/20200 loss 7.188158 loss_att 11.441754 loss_ctc 10.602924 loss_rnnt 5.785247 hw_loss 0.181666 lr 0.00029641 rank 1
2023-03-01 05:57:24,401 DEBUG TRAIN Batch 44/20300 loss 9.574961 loss_att 12.018618 loss_ctc 16.324499 loss_rnnt 8.061136 hw_loss 0.234664 lr 0.00029640 rank 1
2023-03-01 05:57:24,401 DEBUG TRAIN Batch 44/20300 loss 2.430663 loss_att 3.722255 loss_ctc 4.208727 loss_rnnt 1.732498 hw_loss 0.380199 lr 0.00029640 rank 0
2023-03-01 05:57:47,026 DEBUG TRAIN Batch 44/20400 loss 10.412700 loss_att 11.666789 loss_ctc 16.181240 loss_rnnt 9.249188 hw_loss 0.269166 lr 0.00029638 rank 1
2023-03-01 05:57:47,028 DEBUG TRAIN Batch 44/20400 loss 2.699555 loss_att 6.567687 loss_ctc 7.519807 loss_rnnt 1.126529 hw_loss 0.293813 lr 0.00029638 rank 0
2023-03-01 05:58:09,043 DEBUG TRAIN Batch 44/20500 loss 7.532153 loss_att 10.793415 loss_ctc 15.207853 loss_rnnt 5.704990 hw_loss 0.284030 lr 0.00029637 rank 0
2023-03-01 05:58:09,043 DEBUG TRAIN Batch 44/20500 loss 3.041276 loss_att 6.804834 loss_ctc 6.857428 loss_rnnt 1.685841 hw_loss 0.176068 lr 0.00029637 rank 1
2023-03-01 05:58:31,697 DEBUG TRAIN Batch 44/20600 loss 5.069649 loss_att 9.842656 loss_ctc 12.334393 loss_rnnt 2.983655 hw_loss 0.305174 lr 0.00029636 rank 1
2023-03-01 05:58:31,698 DEBUG TRAIN Batch 44/20600 loss 6.120551 loss_att 10.001767 loss_ctc 9.231678 loss_rnnt 4.831027 hw_loss 0.184620 lr 0.00029636 rank 0
2023-03-01 05:59:06,590 DEBUG TRAIN Batch 44/20700 loss 3.457455 loss_att 5.693371 loss_ctc 4.984969 loss_rnnt 2.701174 hw_loss 0.197678 lr 0.00029634 rank 1
2023-03-01 05:59:06,591 DEBUG TRAIN Batch 44/20700 loss 4.721154 loss_att 7.469399 loss_ctc 6.299542 loss_rnnt 3.805387 hw_loss 0.291874 lr 0.00029634 rank 0
2023-03-01 05:59:28,914 DEBUG TRAIN Batch 44/20800 loss 6.472799 loss_att 9.837935 loss_ctc 8.826224 loss_rnnt 5.347287 hw_loss 0.260051 lr 0.00029633 rank 1
2023-03-01 05:59:28,916 DEBUG TRAIN Batch 44/20800 loss 5.387275 loss_att 8.365705 loss_ctc 11.612997 loss_rnnt 3.804711 hw_loss 0.293967 lr 0.00029633 rank 0
2023-03-01 05:59:51,902 DEBUG TRAIN Batch 44/20900 loss 5.909580 loss_att 8.196583 loss_ctc 8.016438 loss_rnnt 5.031196 hw_loss 0.262629 lr 0.00029632 rank 1
2023-03-01 05:59:51,904 DEBUG TRAIN Batch 44/20900 loss 9.867665 loss_att 12.869949 loss_ctc 16.469696 loss_rnnt 8.199471 hw_loss 0.351500 lr 0.00029632 rank 0
2023-03-01 06:00:28,083 DEBUG TRAIN Batch 44/21000 loss 6.485909 loss_att 10.844305 loss_ctc 14.325447 loss_rnnt 4.464251 hw_loss 0.196326 lr 0.00029630 rank 0
2023-03-01 06:00:28,084 DEBUG TRAIN Batch 44/21000 loss 6.964313 loss_att 10.589275 loss_ctc 9.904929 loss_rnnt 5.650035 hw_loss 0.369756 lr 0.00029630 rank 1
2023-03-01 06:00:50,514 DEBUG TRAIN Batch 44/21100 loss 4.114794 loss_att 6.223267 loss_ctc 5.372116 loss_rnnt 3.409099 hw_loss 0.218170 lr 0.00029629 rank 1
2023-03-01 06:00:50,515 DEBUG TRAIN Batch 44/21100 loss 10.257554 loss_att 17.329895 loss_ctc 19.931194 loss_rnnt 7.460531 hw_loss 0.173879 lr 0.00029629 rank 0
2023-03-01 06:01:13,469 DEBUG TRAIN Batch 44/21200 loss 9.097008 loss_att 15.405324 loss_ctc 18.440403 loss_rnnt 6.457367 hw_loss 0.247860 lr 0.00029628 rank 1
2023-03-01 06:01:13,471 DEBUG TRAIN Batch 44/21200 loss 6.058291 loss_att 8.408825 loss_ctc 9.137607 loss_rnnt 5.123277 hw_loss 0.101871 lr 0.00029628 rank 0
2023-03-01 06:01:36,102 DEBUG TRAIN Batch 44/21300 loss 4.074590 loss_att 8.004301 loss_ctc 9.261691 loss_rnnt 2.505960 hw_loss 0.170765 lr 0.00029627 rank 1
2023-03-01 06:01:36,103 DEBUG TRAIN Batch 44/21300 loss 2.910716 loss_att 5.544249 loss_ctc 3.985872 loss_rnnt 2.128971 hw_loss 0.209409 lr 0.00029627 rank 0
2023-03-01 06:02:10,572 DEBUG TRAIN Batch 44/21400 loss 9.923843 loss_att 11.468211 loss_ctc 16.830944 loss_rnnt 8.524328 hw_loss 0.318176 lr 0.00029625 rank 1
2023-03-01 06:02:10,572 DEBUG TRAIN Batch 44/21400 loss 7.715189 loss_att 11.321823 loss_ctc 16.270603 loss_rnnt 5.733726 hw_loss 0.223903 lr 0.00029625 rank 0
2023-03-01 06:02:33,457 DEBUG TRAIN Batch 44/21500 loss 6.441674 loss_att 10.052503 loss_ctc 13.827471 loss_rnnt 4.649994 hw_loss 0.158888 lr 0.00029624 rank 1
2023-03-01 06:02:33,458 DEBUG TRAIN Batch 44/21500 loss 15.188451 loss_att 16.174248 loss_ctc 23.433409 loss_rnnt 13.734055 hw_loss 0.296079 lr 0.00029624 rank 0
2023-03-01 06:02:57,225 DEBUG TRAIN Batch 44/21600 loss 4.863942 loss_att 6.629865 loss_ctc 4.668541 loss_rnnt 4.504809 hw_loss 0.060004 lr 0.00029623 rank 1
2023-03-01 06:02:57,226 DEBUG TRAIN Batch 44/21600 loss 5.090777 loss_att 9.147163 loss_ctc 11.188616 loss_rnnt 3.290202 hw_loss 0.330475 lr 0.00029623 rank 0
2023-03-01 06:03:33,244 DEBUG TRAIN Batch 44/21700 loss 6.932813 loss_att 7.088452 loss_ctc 9.455414 loss_rnnt 6.384563 hw_loss 0.338953 lr 0.00029621 rank 1
2023-03-01 06:03:33,246 DEBUG TRAIN Batch 44/21700 loss 4.698965 loss_att 7.595151 loss_ctc 8.140833 loss_rnnt 3.535465 hw_loss 0.235025 lr 0.00029621 rank 0
2023-03-01 06:03:55,323 DEBUG TRAIN Batch 44/21800 loss 4.606063 loss_att 7.876390 loss_ctc 7.716512 loss_rnnt 3.360893 hw_loss 0.330708 lr 0.00029620 rank 0
2023-03-01 06:03:55,324 DEBUG TRAIN Batch 44/21800 loss 5.076226 loss_att 7.282550 loss_ctc 11.725660 loss_rnnt 3.650066 hw_loss 0.184319 lr 0.00029620 rank 1
2023-03-01 06:04:18,044 DEBUG TRAIN Batch 44/21900 loss 2.619617 loss_att 5.381228 loss_ctc 5.055865 loss_rnnt 1.582691 hw_loss 0.299568 lr 0.00029619 rank 1
2023-03-01 06:04:18,046 DEBUG TRAIN Batch 44/21900 loss 6.639830 loss_att 8.636930 loss_ctc 11.853298 loss_rnnt 5.379963 hw_loss 0.309970 lr 0.00029619 rank 0
2023-03-01 06:04:40,551 DEBUG TRAIN Batch 44/22000 loss 6.274049 loss_att 7.454897 loss_ctc 7.998861 loss_rnnt 5.648829 hw_loss 0.298267 lr 0.00029617 rank 0
2023-03-01 06:04:40,552 DEBUG TRAIN Batch 44/22000 loss 5.262753 loss_att 6.675668 loss_ctc 7.100369 loss_rnnt 4.576556 hw_loss 0.297371 lr 0.00029617 rank 1
2023-03-01 06:05:15,323 DEBUG TRAIN Batch 44/22100 loss 4.787606 loss_att 7.924637 loss_ctc 7.361425 loss_rnnt 3.642315 hw_loss 0.327580 lr 0.00029616 rank 1
2023-03-01 06:05:15,325 DEBUG TRAIN Batch 44/22100 loss 6.883432 loss_att 10.156761 loss_ctc 8.991594 loss_rnnt 5.795815 hw_loss 0.284743 lr 0.00029616 rank 0
2023-03-01 06:05:38,444 DEBUG TRAIN Batch 44/22200 loss 11.899345 loss_att 12.482800 loss_ctc 14.169664 loss_rnnt 11.284143 hw_loss 0.367128 lr 0.00029615 rank 0
2023-03-01 06:05:38,444 DEBUG TRAIN Batch 44/22200 loss 7.594992 loss_att 9.459145 loss_ctc 12.286694 loss_rnnt 6.478718 hw_loss 0.221031 lr 0.00029615 rank 1
2023-03-01 06:06:01,461 DEBUG TRAIN Batch 44/22300 loss 6.542258 loss_att 7.774693 loss_ctc 8.689959 loss_rnnt 5.893718 hw_loss 0.216925 lr 0.00029614 rank 1
2023-03-01 06:06:01,462 DEBUG TRAIN Batch 44/22300 loss 8.293296 loss_att 10.043713 loss_ctc 16.907799 loss_rnnt 6.750840 hw_loss 0.082071 lr 0.00029614 rank 0
2023-03-01 06:06:24,384 DEBUG TRAIN Batch 44/22400 loss 6.097909 loss_att 9.133093 loss_ctc 11.684032 loss_rnnt 4.543223 hw_loss 0.380313 lr 0.00029612 rank 0
2023-03-01 06:06:24,384 DEBUG TRAIN Batch 44/22400 loss 2.806586 loss_att 6.139137 loss_ctc 5.794059 loss_rnnt 1.591048 hw_loss 0.282558 lr 0.00029612 rank 1
2023-03-01 06:07:00,229 DEBUG TRAIN Batch 44/22500 loss 9.612241 loss_att 12.418934 loss_ctc 14.784678 loss_rnnt 8.344567 hw_loss 0.031269 lr 0.00029611 rank 1
2023-03-01 06:07:00,230 DEBUG TRAIN Batch 44/22500 loss 10.364919 loss_att 13.073854 loss_ctc 17.055847 loss_rnnt 8.831141 hw_loss 0.187249 lr 0.00029611 rank 0
2023-03-01 06:07:22,326 DEBUG TRAIN Batch 44/22600 loss 4.073621 loss_att 6.438707 loss_ctc 5.386696 loss_rnnt 3.236587 hw_loss 0.354263 lr 0.00029610 rank 1
2023-03-01 06:07:22,328 DEBUG TRAIN Batch 44/22600 loss 1.996204 loss_att 4.924863 loss_ctc 4.911694 loss_rnnt 0.915422 hw_loss 0.199347 lr 0.00029610 rank 0
2023-03-01 06:07:44,306 DEBUG TRAIN Batch 44/22700 loss 5.874878 loss_att 9.081594 loss_ctc 8.919012 loss_rnnt 4.722980 hw_loss 0.196258 lr 0.00029608 rank 1
2023-03-01 06:07:44,307 DEBUG TRAIN Batch 44/22700 loss 5.942490 loss_att 9.082952 loss_ctc 12.626226 loss_rnnt 4.260459 hw_loss 0.305200 lr 0.00029608 rank 0
2023-03-01 06:08:19,722 DEBUG TRAIN Batch 44/22800 loss 7.303228 loss_att 9.181438 loss_ctc 9.546373 loss_rnnt 6.466012 hw_loss 0.304665 lr 0.00029607 rank 1
2023-03-01 06:08:19,724 DEBUG TRAIN Batch 44/22800 loss 5.567389 loss_att 9.074955 loss_ctc 8.327708 loss_rnnt 4.332933 hw_loss 0.309187 lr 0.00029607 rank 0
2023-03-01 06:08:42,884 DEBUG TRAIN Batch 44/22900 loss 8.103728 loss_att 11.001583 loss_ctc 14.331160 loss_rnnt 6.511743 hw_loss 0.341418 lr 0.00029606 rank 0
2023-03-01 06:08:42,884 DEBUG TRAIN Batch 44/22900 loss 8.214088 loss_att 12.512167 loss_ctc 14.983348 loss_rnnt 6.311218 hw_loss 0.263788 lr 0.00029606 rank 1
2023-03-01 06:09:05,108 DEBUG TRAIN Batch 44/23000 loss 6.107064 loss_att 9.785071 loss_ctc 9.445623 loss_rnnt 4.762617 hw_loss 0.306945 lr 0.00029605 rank 1
2023-03-01 06:09:05,109 DEBUG TRAIN Batch 44/23000 loss 4.506745 loss_att 7.618927 loss_ctc 10.998617 loss_rnnt 2.882665 hw_loss 0.255113 lr 0.00029605 rank 0
2023-03-01 06:09:27,395 DEBUG TRAIN Batch 44/23100 loss 4.861717 loss_att 6.847795 loss_ctc 6.299774 loss_rnnt 4.069458 hw_loss 0.381191 lr 0.00029603 rank 1
2023-03-01 06:09:27,396 DEBUG TRAIN Batch 44/23100 loss 12.660409 loss_att 14.223887 loss_ctc 22.802755 loss_rnnt 10.837179 hw_loss 0.296666 lr 0.00029603 rank 0
2023-03-01 06:10:03,383 DEBUG TRAIN Batch 44/23200 loss 15.500064 loss_att 21.070194 loss_ctc 27.983559 loss_rnnt 12.597349 hw_loss 0.232917 lr 0.00029602 rank 1
2023-03-01 06:10:03,388 DEBUG TRAIN Batch 44/23200 loss 6.286354 loss_att 9.787551 loss_ctc 11.989570 loss_rnnt 4.732428 hw_loss 0.174858 lr 0.00029602 rank 0
2023-03-01 06:10:25,725 DEBUG TRAIN Batch 44/23300 loss 4.061011 loss_att 7.519809 loss_ctc 8.800640 loss_rnnt 2.568230 hw_loss 0.317008 lr 0.00029601 rank 1
2023-03-01 06:10:25,726 DEBUG TRAIN Batch 44/23300 loss 4.067115 loss_att 5.927542 loss_ctc 6.135983 loss_rnnt 3.281142 hw_loss 0.258824 lr 0.00029601 rank 0
2023-03-01 06:10:48,723 DEBUG TRAIN Batch 44/23400 loss 6.645834 loss_att 9.003988 loss_ctc 14.684590 loss_rnnt 4.959551 hw_loss 0.267784 lr 0.00029599 rank 1
2023-03-01 06:10:48,724 DEBUG TRAIN Batch 44/23400 loss 12.832115 loss_att 15.147264 loss_ctc 23.663475 loss_rnnt 10.819997 hw_loss 0.196699 lr 0.00029599 rank 0
2023-03-01 06:11:23,281 DEBUG TRAIN Batch 44/23500 loss 6.961773 loss_att 10.771184 loss_ctc 10.426455 loss_rnnt 5.569438 hw_loss 0.315928 lr 0.00029598 rank 1
2023-03-01 06:11:23,281 DEBUG TRAIN Batch 44/23500 loss 3.922183 loss_att 5.203089 loss_ctc 3.876203 loss_rnnt 3.509363 hw_loss 0.305193 lr 0.00029598 rank 0
2023-03-01 06:11:46,258 DEBUG TRAIN Batch 44/23600 loss 6.854383 loss_att 8.067806 loss_ctc 12.006839 loss_rnnt 5.761683 hw_loss 0.305663 lr 0.00029597 rank 1
2023-03-01 06:11:46,259 DEBUG TRAIN Batch 44/23600 loss 7.270114 loss_att 11.158970 loss_ctc 16.543325 loss_rnnt 5.138014 hw_loss 0.221064 lr 0.00029597 rank 0
2023-03-01 06:12:08,924 DEBUG TRAIN Batch 44/23700 loss 3.852482 loss_att 6.605738 loss_ctc 4.873833 loss_rnnt 3.003561 hw_loss 0.303916 lr 0.00029595 rank 1
2023-03-01 06:12:08,924 DEBUG TRAIN Batch 44/23700 loss 5.616015 loss_att 6.467131 loss_ctc 9.924223 loss_rnnt 4.785175 hw_loss 0.161608 lr 0.00029595 rank 0
2023-03-01 06:12:31,351 DEBUG TRAIN Batch 44/23800 loss 6.085896 loss_att 11.923489 loss_ctc 10.643421 loss_rnnt 4.221183 hw_loss 0.167858 lr 0.00029594 rank 1
2023-03-01 06:12:31,352 DEBUG TRAIN Batch 44/23800 loss 10.636214 loss_att 11.982946 loss_ctc 17.477848 loss_rnnt 9.361842 hw_loss 0.174016 lr 0.00029594 rank 0
2023-03-01 06:13:05,890 DEBUG TRAIN Batch 44/23900 loss 9.313404 loss_att 10.660291 loss_ctc 16.359886 loss_rnnt 8.009340 hw_loss 0.178416 lr 0.00029593 rank 1
2023-03-01 06:13:05,891 DEBUG TRAIN Batch 44/23900 loss 5.264135 loss_att 9.299318 loss_ctc 11.547028 loss_rnnt 3.468059 hw_loss 0.283727 lr 0.00029593 rank 0
2023-03-01 06:13:28,036 DEBUG TRAIN Batch 44/24000 loss 3.285748 loss_att 5.694136 loss_ctc 5.758294 loss_rnnt 2.314593 hw_loss 0.299633 lr 0.00029592 rank 1
2023-03-01 06:13:28,038 DEBUG TRAIN Batch 44/24000 loss 9.198645 loss_att 10.993612 loss_ctc 14.265179 loss_rnnt 8.055831 hw_loss 0.203030 lr 0.00029592 rank 0
2023-03-01 06:13:50,697 DEBUG TRAIN Batch 44/24100 loss 8.318707 loss_att 9.837890 loss_ctc 11.224402 loss_rnnt 7.440552 hw_loss 0.350426 lr 0.00029590 rank 1
2023-03-01 06:13:50,697 DEBUG TRAIN Batch 44/24100 loss 5.762434 loss_att 5.650652 loss_ctc 7.635953 loss_rnnt 5.319953 hw_loss 0.403191 lr 0.00029590 rank 0
2023-03-01 06:14:25,662 DEBUG TRAIN Batch 44/24200 loss 3.199306 loss_att 7.428379 loss_ctc 2.854345 loss_rnnt 2.322057 hw_loss 0.145179 lr 0.00029589 rank 0
2023-03-01 06:14:25,661 DEBUG TRAIN Batch 44/24200 loss 6.249338 loss_att 9.344144 loss_ctc 9.744648 loss_rnnt 5.090099 hw_loss 0.139193 lr 0.00029589 rank 1
2023-03-01 06:14:48,852 DEBUG TRAIN Batch 44/24300 loss 8.995815 loss_att 12.476582 loss_ctc 13.258524 loss_rnnt 7.648950 hw_loss 0.154406 lr 0.00029588 rank 1
2023-03-01 06:14:48,853 DEBUG TRAIN Batch 44/24300 loss 2.086738 loss_att 4.614773 loss_ctc 2.933682 loss_rnnt 1.378884 hw_loss 0.167477 lr 0.00029588 rank 0
2023-03-01 06:15:11,254 DEBUG TRAIN Batch 44/24400 loss 6.869746 loss_att 9.642930 loss_ctc 13.718478 loss_rnnt 5.256202 hw_loss 0.273268 lr 0.00029586 rank 1
2023-03-01 06:15:11,256 DEBUG TRAIN Batch 44/24400 loss 4.852452 loss_att 7.901288 loss_ctc 6.278290 loss_rnnt 3.939153 hw_loss 0.212664 lr 0.00029586 rank 0
2023-03-01 06:15:34,146 DEBUG TRAIN Batch 44/24500 loss 11.638977 loss_att 13.983803 loss_ctc 20.914066 loss_rnnt 9.795876 hw_loss 0.257732 lr 0.00029585 rank 1
2023-03-01 06:15:34,147 DEBUG TRAIN Batch 44/24500 loss 4.399400 loss_att 6.245507 loss_ctc 6.376811 loss_rnnt 3.571447 hw_loss 0.365768 lr 0.00029585 rank 0
2023-03-01 06:16:08,342 DEBUG TRAIN Batch 44/24600 loss 3.961272 loss_att 6.399298 loss_ctc 7.080347 loss_rnnt 2.971402 hw_loss 0.161976 lr 0.00029584 rank 0
2023-03-01 06:16:08,342 DEBUG TRAIN Batch 44/24600 loss 9.899121 loss_att 13.426773 loss_ctc 15.705167 loss_rnnt 8.241428 hw_loss 0.333792 lr 0.00029584 rank 1
2023-03-01 06:16:31,275 DEBUG TRAIN Batch 44/24700 loss 7.606846 loss_att 9.720604 loss_ctc 15.220785 loss_rnnt 6.026820 hw_loss 0.266404 lr 0.00029582 rank 1
2023-03-01 06:16:31,275 DEBUG TRAIN Batch 44/24700 loss 5.132808 loss_att 6.165360 loss_ctc 7.358466 loss_rnnt 4.440292 hw_loss 0.354846 lr 0.00029582 rank 0
2023-03-01 06:16:54,075 DEBUG TRAIN Batch 44/24800 loss 3.951291 loss_att 8.231808 loss_ctc 5.145175 loss_rnnt 2.868962 hw_loss 0.125703 lr 0.00029581 rank 0
2023-03-01 06:16:54,075 DEBUG TRAIN Batch 44/24800 loss 7.207688 loss_att 11.606931 loss_ctc 12.432945 loss_rnnt 5.422468 hw_loss 0.391257 lr 0.00029581 rank 1
2023-03-01 06:17:29,761 DEBUG TRAIN Batch 44/24900 loss 5.027205 loss_att 7.006357 loss_ctc 9.594704 loss_rnnt 3.856558 hw_loss 0.310907 lr 0.00029580 rank 0
2023-03-01 06:17:29,762 DEBUG TRAIN Batch 44/24900 loss 6.396650 loss_att 5.881769 loss_ctc 8.818888 loss_rnnt 5.970028 hw_loss 0.387436 lr 0.00029580 rank 1
2023-03-01 06:17:52,194 DEBUG TRAIN Batch 44/25000 loss 13.080950 loss_att 17.537647 loss_ctc 17.442520 loss_rnnt 11.527271 hw_loss 0.151492 lr 0.00029579 rank 0
2023-03-01 06:17:52,195 DEBUG TRAIN Batch 44/25000 loss 5.853329 loss_att 6.067105 loss_ctc 5.616714 loss_rnnt 5.693048 hw_loss 0.279514 lr 0.00029579 rank 1
2023-03-01 06:18:15,014 DEBUG TRAIN Batch 44/25100 loss 6.454049 loss_att 9.934487 loss_ctc 10.129885 loss_rnnt 5.114528 hw_loss 0.287478 lr 0.00029577 rank 1
2023-03-01 06:18:15,016 DEBUG TRAIN Batch 44/25100 loss 12.642906 loss_att 17.155912 loss_ctc 22.832073 loss_rnnt 10.254987 hw_loss 0.237679 lr 0.00029577 rank 0
2023-03-01 06:18:37,948 DEBUG TRAIN Batch 44/25200 loss 3.055936 loss_att 6.549148 loss_ctc 5.455090 loss_rnnt 1.970768 hw_loss 0.124946 lr 0.00029576 rank 1
2023-03-01 06:18:37,949 DEBUG TRAIN Batch 44/25200 loss 11.209900 loss_att 12.287405 loss_ctc 16.818693 loss_rnnt 10.079123 hw_loss 0.313944 lr 0.00029576 rank 0
2023-03-01 06:19:14,777 DEBUG TRAIN Batch 44/25300 loss 8.163390 loss_att 12.207520 loss_ctc 13.797431 loss_rnnt 6.509367 hw_loss 0.176237 lr 0.00029575 rank 0
2023-03-01 06:19:14,777 DEBUG TRAIN Batch 44/25300 loss 5.415999 loss_att 8.883839 loss_ctc 8.369975 loss_rnnt 4.175290 hw_loss 0.287397 lr 0.00029575 rank 1
2023-03-01 06:19:38,047 DEBUG TRAIN Batch 44/25400 loss 3.387306 loss_att 6.947879 loss_ctc 4.305082 loss_rnnt 2.410294 hw_loss 0.267238 lr 0.00029573 rank 1
2023-03-01 06:19:38,048 DEBUG TRAIN Batch 44/25400 loss 5.383913 loss_att 5.300204 loss_ctc 7.548859 loss_rnnt 4.869983 hw_loss 0.453773 lr 0.00029573 rank 0
2023-03-01 06:20:00,717 DEBUG TRAIN Batch 44/25500 loss 7.227795 loss_att 8.851326 loss_ctc 15.136379 loss_rnnt 5.773485 hw_loss 0.140861 lr 0.00029572 rank 1
2023-03-01 06:20:00,718 DEBUG TRAIN Batch 44/25500 loss 2.932575 loss_att 6.391794 loss_ctc 4.148584 loss_rnnt 1.925470 hw_loss 0.287111 lr 0.00029572 rank 0
2023-03-01 06:20:23,943 DEBUG TRAIN Batch 44/25600 loss 5.607099 loss_att 10.500463 loss_ctc 7.937507 loss_rnnt 4.276698 hw_loss 0.076889 lr 0.00029571 rank 1
2023-03-01 06:20:23,944 DEBUG TRAIN Batch 44/25600 loss 5.953818 loss_att 7.983643 loss_ctc 10.118254 loss_rnnt 4.892065 hw_loss 0.188493 lr 0.00029571 rank 0
2023-03-01 06:20:58,505 DEBUG TRAIN Batch 44/25700 loss 8.040209 loss_att 11.374194 loss_ctc 15.315227 loss_rnnt 6.292287 hw_loss 0.208353 lr 0.00029570 rank 1
2023-03-01 06:20:58,506 DEBUG TRAIN Batch 44/25700 loss 6.405944 loss_att 8.554797 loss_ctc 12.242753 loss_rnnt 5.098661 hw_loss 0.186133 lr 0.00029570 rank 0
2023-03-01 06:21:20,723 DEBUG TRAIN Batch 44/25800 loss 6.246921 loss_att 10.513222 loss_ctc 13.984547 loss_rnnt 4.250234 hw_loss 0.209519 lr 0.00029568 rank 1
2023-03-01 06:21:20,726 DEBUG TRAIN Batch 44/25800 loss 14.331938 loss_att 14.739344 loss_ctc 21.750158 loss_rnnt 13.131816 hw_loss 0.242896 lr 0.00029568 rank 0
2023-03-01 06:21:42,841 DEBUG TRAIN Batch 44/25900 loss 5.336237 loss_att 7.186048 loss_ctc 7.198130 loss_rnnt 4.564090 hw_loss 0.288623 lr 0.00029567 rank 1
2023-03-01 06:21:42,842 DEBUG TRAIN Batch 44/25900 loss 6.986504 loss_att 9.088470 loss_ctc 9.472563 loss_rnnt 6.110849 hw_loss 0.232099 lr 0.00029567 rank 0
2023-03-01 06:22:19,267 DEBUG TRAIN Batch 44/26000 loss 6.940686 loss_att 8.891099 loss_ctc 13.611540 loss_rnnt 5.499475 hw_loss 0.303152 lr 0.00029566 rank 1
2023-03-01 06:22:19,269 DEBUG TRAIN Batch 44/26000 loss 4.283636 loss_att 7.889674 loss_ctc 8.727298 loss_rnnt 2.785282 hw_loss 0.346234 lr 0.00029566 rank 0
2023-03-01 06:22:42,132 DEBUG TRAIN Batch 44/26100 loss 5.398277 loss_att 6.176169 loss_ctc 7.473113 loss_rnnt 4.768388 hw_loss 0.370624 lr 0.00029564 rank 1
2023-03-01 06:22:42,133 DEBUG TRAIN Batch 44/26100 loss 6.040906 loss_att 11.839355 loss_ctc 9.793581 loss_rnnt 4.305279 hw_loss 0.141715 lr 0.00029564 rank 0
2023-03-01 06:23:04,594 DEBUG TRAIN Batch 44/26200 loss 3.687450 loss_att 6.342272 loss_ctc 5.745655 loss_rnnt 2.714308 hw_loss 0.314531 lr 0.00029563 rank 1
2023-03-01 06:23:04,594 DEBUG TRAIN Batch 44/26200 loss 11.500606 loss_att 12.884343 loss_ctc 14.796896 loss_rnnt 10.622473 hw_loss 0.303524 lr 0.00029563 rank 0
2023-03-01 06:23:26,524 DEBUG TRAIN Batch 44/26300 loss 7.959193 loss_att 11.691022 loss_ctc 11.625395 loss_rnnt 6.598773 hw_loss 0.234801 lr 0.00029562 rank 1
2023-03-01 06:23:26,525 DEBUG TRAIN Batch 44/26300 loss 2.543995 loss_att 4.866730 loss_ctc 3.610369 loss_rnnt 1.774441 hw_loss 0.305296 lr 0.00029562 rank 0
2023-03-01 06:24:00,427 DEBUG TRAIN Batch 44/26400 loss 8.316943 loss_att 10.195889 loss_ctc 9.761583 loss_rnnt 7.654259 hw_loss 0.176768 lr 0.00029561 rank 0
2023-03-01 06:24:00,427 DEBUG TRAIN Batch 44/26400 loss 1.978504 loss_att 4.626729 loss_ctc 4.704666 loss_rnnt 0.987949 hw_loss 0.182666 lr 0.00029561 rank 1
2023-03-01 06:24:21,991 DEBUG TRAIN Batch 44/26500 loss 9.379941 loss_att 14.766613 loss_ctc 22.610573 loss_rnnt 6.376469 hw_loss 0.303847 lr 0.00029559 rank 1
2023-03-01 06:24:21,992 DEBUG TRAIN Batch 44/26500 loss 13.069680 loss_att 16.616543 loss_ctc 24.590008 loss_rnnt 10.681861 hw_loss 0.267007 lr 0.00029559 rank 0
2023-03-01 06:24:44,520 DEBUG TRAIN Batch 44/26600 loss 7.784883 loss_att 11.221691 loss_ctc 15.103218 loss_rnnt 5.953822 hw_loss 0.314851 lr 0.00029558 rank 1
2023-03-01 06:24:44,521 DEBUG TRAIN Batch 44/26600 loss 12.198298 loss_att 15.641095 loss_ctc 23.711040 loss_rnnt 9.844324 hw_loss 0.244468 lr 0.00029558 rank 0
2023-03-01 06:25:17,851 DEBUG TRAIN Batch 44/26700 loss 7.465246 loss_att 10.594326 loss_ctc 13.747196 loss_rnnt 5.873081 hw_loss 0.241417 lr 0.00029557 rank 0
2023-03-01 06:25:17,851 DEBUG TRAIN Batch 44/26700 loss 9.200236 loss_att 11.695976 loss_ctc 14.202011 loss_rnnt 7.877841 hw_loss 0.293142 lr 0.00029557 rank 1
2023-03-01 06:25:40,287 DEBUG TRAIN Batch 44/26800 loss 4.921257 loss_att 8.466948 loss_ctc 8.305708 loss_rnnt 3.704747 hw_loss 0.105210 lr 0.00029555 rank 0
2023-03-01 06:25:40,288 DEBUG TRAIN Batch 44/26800 loss 8.206583 loss_att 12.474271 loss_ctc 11.532664 loss_rnnt 6.859775 hw_loss 0.093362 lr 0.00029555 rank 1
2023-03-01 06:26:02,096 DEBUG TRAIN Batch 44/26900 loss 5.569325 loss_att 8.652939 loss_ctc 7.223228 loss_rnnt 4.580598 hw_loss 0.284033 lr 0.00029554 rank 0
2023-03-01 06:26:02,096 DEBUG TRAIN Batch 44/26900 loss 9.168350 loss_att 10.439482 loss_ctc 16.718544 loss_rnnt 7.705822 hw_loss 0.378017 lr 0.00029554 rank 1
2023-03-01 06:26:24,273 DEBUG TRAIN Batch 44/27000 loss 1.833190 loss_att 4.954512 loss_ctc 2.656276 loss_rnnt 0.989592 hw_loss 0.205478 lr 0.00029553 rank 1
2023-03-01 06:26:24,275 DEBUG TRAIN Batch 44/27000 loss 10.491618 loss_att 11.203579 loss_ctc 14.099516 loss_rnnt 9.641431 hw_loss 0.425141 lr 0.00029553 rank 0
2023-03-01 06:26:55,766 DEBUG TRAIN Batch 44/27100 loss 10.071646 loss_att 12.102579 loss_ctc 14.794374 loss_rnnt 8.906458 hw_loss 0.242445 lr 0.00029551 rank 1
2023-03-01 06:26:55,767 DEBUG TRAIN Batch 44/27100 loss 6.252625 loss_att 8.904320 loss_ctc 8.272079 loss_rnnt 5.247217 hw_loss 0.385890 lr 0.00029551 rank 0
2023-03-01 06:27:16,668 DEBUG TRAIN Batch 44/27200 loss 4.882663 loss_att 7.485038 loss_ctc 10.677523 loss_rnnt 3.440220 hw_loss 0.279976 lr 0.00029550 rank 1
2023-03-01 06:27:16,669 DEBUG TRAIN Batch 44/27200 loss 12.191614 loss_att 15.451283 loss_ctc 20.328163 loss_rnnt 10.319035 hw_loss 0.254571 lr 0.00029550 rank 0
2023-03-01 06:27:38,125 DEBUG TRAIN Batch 44/27300 loss 9.174003 loss_att 10.204138 loss_ctc 14.791483 loss_rnnt 8.018622 hw_loss 0.375666 lr 0.00029549 rank 1
2023-03-01 06:27:38,127 DEBUG TRAIN Batch 44/27300 loss 5.665816 loss_att 8.596356 loss_ctc 10.137819 loss_rnnt 4.384793 hw_loss 0.184963 lr 0.00029549 rank 0
2023-03-01 06:28:12,538 DEBUG TRAIN Batch 44/27400 loss 7.464628 loss_att 8.066149 loss_ctc 10.230453 loss_rnnt 6.803525 hw_loss 0.322541 lr 0.00029548 rank 1
2023-03-01 06:28:12,538 DEBUG TRAIN Batch 44/27400 loss 9.851872 loss_att 9.968153 loss_ctc 15.180135 loss_rnnt 8.967911 hw_loss 0.281759 lr 0.00029548 rank 0
2023-03-01 06:28:33,533 DEBUG TRAIN Batch 44/27500 loss 4.963199 loss_att 7.824208 loss_ctc 10.465436 loss_rnnt 3.412659 hw_loss 0.458824 lr 0.00029546 rank 0
2023-03-01 06:28:33,533 DEBUG TRAIN Batch 44/27500 loss 8.623601 loss_att 11.641256 loss_ctc 12.076838 loss_rnnt 7.480276 hw_loss 0.148805 lr 0.00029546 rank 1
2023-03-01 06:28:54,868 DEBUG TRAIN Batch 44/27600 loss 7.621758 loss_att 9.986094 loss_ctc 13.925803 loss_rnnt 6.207096 hw_loss 0.189853 lr 0.00029545 rank 1
2023-03-01 06:28:54,869 DEBUG TRAIN Batch 44/27600 loss 13.232767 loss_att 14.541240 loss_ctc 19.069920 loss_rnnt 12.108763 hw_loss 0.157543 lr 0.00029545 rank 0
2023-03-01 06:29:16,574 DEBUG TRAIN Batch 44/27700 loss 5.559010 loss_att 7.224510 loss_ctc 11.194828 loss_rnnt 4.378929 hw_loss 0.179133 lr 0.00029544 rank 0
2023-03-01 06:29:16,574 DEBUG TRAIN Batch 44/27700 loss 9.340133 loss_att 11.518442 loss_ctc 12.624803 loss_rnnt 8.359264 hw_loss 0.201094 lr 0.00029544 rank 1
2023-03-01 06:29:48,360 DEBUG TRAIN Batch 44/27800 loss 8.611976 loss_att 11.558455 loss_ctc 10.377653 loss_rnnt 7.661714 hw_loss 0.235390 lr 0.00029542 rank 1
2023-03-01 06:29:48,361 DEBUG TRAIN Batch 44/27800 loss 10.955454 loss_att 13.786778 loss_ctc 13.582171 loss_rnnt 9.966077 hw_loss 0.136655 lr 0.00029542 rank 0
2023-03-01 06:30:09,898 DEBUG TRAIN Batch 44/27900 loss 8.047411 loss_att 10.024038 loss_ctc 14.890244 loss_rnnt 6.641575 hw_loss 0.184001 lr 0.00029541 rank 1
2023-03-01 06:30:09,899 DEBUG TRAIN Batch 44/27900 loss 12.445231 loss_att 16.739964 loss_ctc 22.233269 loss_rnnt 10.190292 hw_loss 0.170476 lr 0.00029541 rank 0
2023-03-01 06:30:32,062 DEBUG TRAIN Batch 44/28000 loss 13.214820 loss_att 20.265913 loss_ctc 22.046537 loss_rnnt 10.438631 hw_loss 0.353264 lr 0.00029540 rank 1
2023-03-01 06:30:32,063 DEBUG TRAIN Batch 44/28000 loss 9.909451 loss_att 11.343590 loss_ctc 16.853680 loss_rnnt 8.509046 hw_loss 0.351904 lr 0.00029540 rank 0
2023-03-01 06:30:54,696 DEBUG TRAIN Batch 44/28100 loss 10.175960 loss_att 14.257719 loss_ctc 15.509344 loss_rnnt 8.518575 hw_loss 0.243591 lr 0.00029539 rank 0
2023-03-01 06:30:54,697 DEBUG TRAIN Batch 44/28100 loss 8.650486 loss_att 10.643861 loss_ctc 13.718925 loss_rnnt 7.446110 hw_loss 0.243580 lr 0.00029539 rank 1
2023-03-01 06:31:27,761 DEBUG TRAIN Batch 44/28200 loss 5.994613 loss_att 9.041938 loss_ctc 9.135838 loss_rnnt 4.805563 hw_loss 0.301413 lr 0.00029537 rank 0
2023-03-01 06:31:27,761 DEBUG TRAIN Batch 44/28200 loss 7.438367 loss_att 9.641137 loss_ctc 12.875556 loss_rnnt 6.082143 hw_loss 0.357584 lr 0.00029537 rank 1
2023-03-01 06:31:49,754 DEBUG TRAIN Batch 44/28300 loss 9.515454 loss_att 10.834842 loss_ctc 15.164152 loss_rnnt 8.317914 hw_loss 0.338441 lr 0.00029536 rank 1
2023-03-01 06:31:49,754 DEBUG TRAIN Batch 44/28300 loss 5.936870 loss_att 10.473161 loss_ctc 11.513050 loss_rnnt 4.229957 hw_loss 0.105307 lr 0.00029536 rank 0
2023-03-01 06:32:11,595 DEBUG TRAIN Batch 44/28400 loss 3.062912 loss_att 4.573310 loss_ctc 4.829912 loss_rnnt 2.388026 hw_loss 0.257261 lr 0.00029535 rank 0
2023-03-01 06:32:11,596 DEBUG TRAIN Batch 44/28400 loss 6.336908 loss_att 8.514248 loss_ctc 11.365407 loss_rnnt 5.098230 hw_loss 0.248893 lr 0.00029535 rank 1
2023-03-01 06:32:44,691 DEBUG TRAIN Batch 44/28500 loss 7.571736 loss_att 11.061017 loss_ctc 12.707800 loss_rnnt 6.091314 hw_loss 0.183295 lr 0.00029533 rank 1
2023-03-01 06:32:44,692 DEBUG TRAIN Batch 44/28500 loss 7.323911 loss_att 9.047764 loss_ctc 11.310186 loss_rnnt 6.307583 hw_loss 0.262599 lr 0.00029533 rank 0
2023-03-01 06:33:07,477 DEBUG TRAIN Batch 44/28600 loss 8.272689 loss_att 10.786793 loss_ctc 14.652773 loss_rnnt 6.738082 hw_loss 0.339580 lr 0.00029532 rank 1
2023-03-01 06:33:07,477 DEBUG TRAIN Batch 44/28600 loss 14.510071 loss_att 15.467326 loss_ctc 23.038603 loss_rnnt 13.053270 hw_loss 0.240394 lr 0.00029532 rank 0
2023-03-01 06:33:30,186 DEBUG TRAIN Batch 44/28700 loss 7.147120 loss_att 9.148270 loss_ctc 11.820474 loss_rnnt 5.964738 hw_loss 0.298198 lr 0.00029531 rank 1
2023-03-01 06:33:30,188 DEBUG TRAIN Batch 44/28700 loss 3.494262 loss_att 8.182398 loss_ctc 10.470737 loss_rnnt 1.486362 hw_loss 0.262642 lr 0.00029531 rank 0
2023-03-01 06:33:52,473 DEBUG TRAIN Batch 44/28800 loss 6.621315 loss_att 12.265042 loss_ctc 9.623535 loss_rnnt 4.968608 hw_loss 0.231871 lr 0.00029530 rank 1
2023-03-01 06:33:52,474 DEBUG TRAIN Batch 44/28800 loss 8.621588 loss_att 10.884454 loss_ctc 15.662299 loss_rnnt 7.061708 hw_loss 0.316021 lr 0.00029530 rank 0
2023-03-01 06:34:26,000 DEBUG TRAIN Batch 44/28900 loss 4.418117 loss_att 7.203614 loss_ctc 4.836196 loss_rnnt 3.636829 hw_loss 0.315834 lr 0.00029528 rank 0
2023-03-01 06:34:26,000 DEBUG TRAIN Batch 44/28900 loss 10.931409 loss_att 12.765151 loss_ctc 14.525558 loss_rnnt 9.999209 hw_loss 0.161684 lr 0.00029528 rank 1
2023-03-01 06:34:47,426 DEBUG TRAIN Batch 44/29000 loss 4.405322 loss_att 6.989650 loss_ctc 8.211140 loss_rnnt 3.253604 hw_loss 0.238894 lr 0.00029527 rank 0
2023-03-01 06:34:47,426 DEBUG TRAIN Batch 44/29000 loss 5.912268 loss_att 8.796129 loss_ctc 8.499897 loss_rnnt 4.897124 hw_loss 0.175039 lr 0.00029527 rank 1
2023-03-01 06:35:09,077 DEBUG TRAIN Batch 44/29100 loss 9.835404 loss_att 14.490543 loss_ctc 17.015608 loss_rnnt 7.884072 hw_loss 0.118020 lr 0.00029526 rank 1
2023-03-01 06:35:09,078 DEBUG TRAIN Batch 44/29100 loss 11.153878 loss_att 13.808043 loss_ctc 15.174829 loss_rnnt 9.998385 hw_loss 0.165999 lr 0.00029526 rank 0
2023-03-01 06:35:41,874 DEBUG TRAIN Batch 44/29200 loss 5.950312 loss_att 8.082378 loss_ctc 8.302553 loss_rnnt 5.054470 hw_loss 0.292118 lr 0.00029524 rank 0
2023-03-01 06:35:41,874 DEBUG TRAIN Batch 44/29200 loss 8.098865 loss_att 9.501151 loss_ctc 13.446266 loss_rnnt 6.997413 hw_loss 0.202516 lr 0.00029524 rank 1
2023-03-01 06:36:04,330 DEBUG TRAIN Batch 44/29300 loss 8.226853 loss_att 10.385713 loss_ctc 16.322786 loss_rnnt 6.568563 hw_loss 0.275740 lr 0.00029523 rank 1
2023-03-01 06:36:04,331 DEBUG TRAIN Batch 44/29300 loss 7.001851 loss_att 8.411510 loss_ctc 11.664878 loss_rnnt 5.955211 hw_loss 0.268072 lr 0.00029523 rank 0
2023-03-01 06:36:26,722 DEBUG TRAIN Batch 44/29400 loss 5.228588 loss_att 7.168624 loss_ctc 10.628170 loss_rnnt 4.003710 hw_loss 0.219235 lr 0.00029522 rank 1
2023-03-01 06:36:26,723 DEBUG TRAIN Batch 44/29400 loss 16.774105 loss_att 22.157475 loss_ctc 23.878651 loss_rnnt 14.630362 hw_loss 0.224617 lr 0.00029522 rank 0
2023-03-01 06:36:48,411 DEBUG TRAIN Batch 44/29500 loss 7.078830 loss_att 8.556742 loss_ctc 8.097892 loss_rnnt 6.505481 hw_loss 0.266047 lr 0.00029521 rank 0
2023-03-01 06:36:48,411 DEBUG TRAIN Batch 44/29500 loss 3.418565 loss_att 4.997408 loss_ctc 6.381387 loss_rnnt 2.581933 hw_loss 0.235912 lr 0.00029521 rank 1
2023-03-01 06:37:22,252 DEBUG TRAIN Batch 44/29600 loss 8.968440 loss_att 12.490352 loss_ctc 12.722153 loss_rnnt 7.578843 hw_loss 0.346349 lr 0.00029519 rank 1
2023-03-01 06:37:22,254 DEBUG TRAIN Batch 44/29600 loss 7.768829 loss_att 10.843497 loss_ctc 13.886932 loss_rnnt 6.168552 hw_loss 0.317992 lr 0.00029519 rank 0
2023-03-01 06:37:43,899 DEBUG TRAIN Batch 44/29700 loss 8.242557 loss_att 12.710505 loss_ctc 13.176128 loss_rnnt 6.548881 hw_loss 0.266766 lr 0.00029518 rank 1
2023-03-01 06:37:43,900 DEBUG TRAIN Batch 44/29700 loss 3.820062 loss_att 6.831303 loss_ctc 9.146084 loss_rnnt 2.349075 hw_loss 0.297379 lr 0.00029518 rank 0
2023-03-01 06:38:05,699 DEBUG TRAIN Batch 44/29800 loss 5.651295 loss_att 7.449545 loss_ctc 7.733184 loss_rnnt 4.908894 hw_loss 0.197188 lr 0.00029517 rank 1
2023-03-01 06:38:05,700 DEBUG TRAIN Batch 44/29800 loss 6.963614 loss_att 8.584223 loss_ctc 11.526331 loss_rnnt 5.944067 hw_loss 0.163245 lr 0.00029517 rank 0
2023-03-01 06:38:38,976 DEBUG TRAIN Batch 44/29900 loss 7.241899 loss_att 9.009863 loss_ctc 11.535059 loss_rnnt 6.214736 hw_loss 0.189652 lr 0.00029515 rank 1
2023-03-01 06:38:38,977 DEBUG TRAIN Batch 44/29900 loss 8.952515 loss_att 10.781264 loss_ctc 13.774539 loss_rnnt 7.845369 hw_loss 0.184612 lr 0.00029515 rank 0
2023-03-01 06:39:01,137 DEBUG TRAIN Batch 44/30000 loss 5.099168 loss_att 6.210762 loss_ctc 7.782478 loss_rnnt 4.337757 hw_loss 0.339969 lr 0.00029514 rank 1
2023-03-01 06:39:01,138 DEBUG TRAIN Batch 44/30000 loss 4.480893 loss_att 7.579881 loss_ctc 11.910168 loss_rnnt 2.749787 hw_loss 0.226386 lr 0.00029514 rank 0
2023-03-01 06:39:22,734 DEBUG TRAIN Batch 44/30100 loss 7.874002 loss_att 8.330173 loss_ctc 11.827125 loss_rnnt 7.165210 hw_loss 0.169641 lr 0.00029513 rank 1
2023-03-01 06:39:22,735 DEBUG TRAIN Batch 44/30100 loss 10.304706 loss_att 13.875238 loss_ctc 20.128078 loss_rnnt 8.205475 hw_loss 0.141266 lr 0.00029513 rank 0
2023-03-01 06:39:45,654 DEBUG TRAIN Batch 44/30200 loss 3.642434 loss_att 6.385057 loss_ctc 7.508843 loss_rnnt 2.436915 hw_loss 0.265263 lr 0.00029512 rank 1
2023-03-01 06:39:45,655 DEBUG TRAIN Batch 44/30200 loss 3.560127 loss_att 6.085322 loss_ctc 5.892784 loss_rnnt 2.600633 hw_loss 0.268939 lr 0.00029512 rank 0
2023-03-01 06:40:18,916 DEBUG TRAIN Batch 44/30300 loss 7.641840 loss_att 10.452862 loss_ctc 18.316645 loss_rnnt 5.528362 hw_loss 0.239935 lr 0.00029510 rank 1
2023-03-01 06:40:18,917 DEBUG TRAIN Batch 44/30300 loss 8.502339 loss_att 10.813000 loss_ctc 18.181561 loss_rnnt 6.620440 hw_loss 0.242257 lr 0.00029510 rank 0
2023-03-01 06:40:40,444 DEBUG TRAIN Batch 44/30400 loss 5.522576 loss_att 7.272042 loss_ctc 9.301363 loss_rnnt 4.525629 hw_loss 0.268528 lr 0.00029509 rank 0
2023-03-01 06:40:40,445 DEBUG TRAIN Batch 44/30400 loss 6.168347 loss_att 12.737942 loss_ctc 13.134674 loss_rnnt 3.738751 hw_loss 0.350312 lr 0.00029509 rank 1
2023-03-01 06:41:02,276 DEBUG TRAIN Batch 44/30500 loss 9.849930 loss_att 11.998264 loss_ctc 10.690470 loss_rnnt 9.145649 hw_loss 0.304766 lr 0.00029508 rank 1
2023-03-01 06:41:02,276 DEBUG TRAIN Batch 44/30500 loss 10.975001 loss_att 15.084566 loss_ctc 22.298328 loss_rnnt 8.471725 hw_loss 0.321726 lr 0.00029508 rank 0
2023-03-01 06:41:34,866 DEBUG TRAIN Batch 44/30600 loss 11.655313 loss_att 15.512430 loss_ctc 18.360929 loss_rnnt 9.918831 hw_loss 0.133082 lr 0.00029506 rank 0
2023-03-01 06:41:34,867 DEBUG TRAIN Batch 44/30600 loss 4.449327 loss_att 5.175345 loss_ctc 7.326519 loss_rnnt 3.760094 hw_loss 0.300758 lr 0.00029506 rank 1
2023-03-01 06:41:56,626 DEBUG TRAIN Batch 44/30700 loss 6.548827 loss_att 8.277519 loss_ctc 8.867039 loss_rnnt 5.809678 hw_loss 0.158089 lr 0.00029505 rank 1
2023-03-01 06:41:56,627 DEBUG TRAIN Batch 44/30700 loss 6.518813 loss_att 12.611396 loss_ctc 11.159021 loss_rnnt 4.526608 hw_loss 0.290614 lr 0.00029505 rank 0
2023-03-01 06:42:17,673 DEBUG TRAIN Batch 44/30800 loss 15.352970 loss_att 24.176594 loss_ctc 34.260807 loss_rnnt 10.977253 hw_loss 0.168652 lr 0.00029504 rank 1
2023-03-01 06:42:17,674 DEBUG TRAIN Batch 44/30800 loss 8.229634 loss_att 9.390580 loss_ctc 12.759947 loss_rnnt 7.294019 hw_loss 0.186344 lr 0.00029504 rank 0
2023-03-01 06:42:39,154 DEBUG TRAIN Batch 44/30900 loss 6.696360 loss_att 9.023705 loss_ctc 11.550589 loss_rnnt 5.434502 hw_loss 0.279673 lr 0.00029503 rank 0
2023-03-01 06:42:39,154 DEBUG TRAIN Batch 44/30900 loss 2.407184 loss_att 7.967830 loss_ctc 3.465565 loss_rnnt 1.153165 hw_loss 0.001448 lr 0.00029503 rank 1
2023-03-01 06:43:11,481 DEBUG TRAIN Batch 44/31000 loss 8.725113 loss_att 10.960029 loss_ctc 15.618377 loss_rnnt 7.175797 hw_loss 0.343558 lr 0.00029501 rank 1
2023-03-01 06:43:11,482 DEBUG TRAIN Batch 44/31000 loss 13.076482 loss_att 15.192302 loss_ctc 22.221428 loss_rnnt 11.255220 hw_loss 0.335196 lr 0.00029501 rank 0
2023-03-01 06:43:32,818 DEBUG TRAIN Batch 44/31100 loss 8.896915 loss_att 11.126822 loss_ctc 14.092941 loss_rnnt 7.566124 hw_loss 0.360011 lr 0.00029500 rank 1
2023-03-01 06:43:32,820 DEBUG TRAIN Batch 44/31100 loss 8.475513 loss_att 11.330803 loss_ctc 15.057579 loss_rnnt 6.917147 hw_loss 0.205687 lr 0.00029500 rank 0
2023-03-01 06:43:54,868 DEBUG TRAIN Batch 44/31200 loss 6.520086 loss_att 9.026642 loss_ctc 8.699254 loss_rnnt 5.639165 hw_loss 0.166976 lr 0.00029499 rank 0
2023-03-01 06:43:54,868 DEBUG TRAIN Batch 44/31200 loss 7.206955 loss_att 10.684094 loss_ctc 12.572014 loss_rnnt 5.708564 hw_loss 0.164291 lr 0.00029499 rank 1
2023-03-01 06:44:28,551 DEBUG TRAIN Batch 44/31300 loss 3.495096 loss_att 8.758664 loss_ctc 5.685683 loss_rnnt 2.093973 hw_loss 0.105620 lr 0.00029497 rank 0
2023-03-01 06:44:28,559 DEBUG TRAIN Batch 44/31300 loss 4.057182 loss_att 5.653973 loss_ctc 10.609299 loss_rnnt 2.752492 hw_loss 0.209469 lr 0.00029497 rank 1
2023-03-01 06:44:49,359 DEBUG TRAIN Batch 44/31400 loss 2.720909 loss_att 5.974840 loss_ctc 5.009464 loss_rnnt 1.697384 hw_loss 0.126747 lr 0.00029496 rank 1
2023-03-01 06:44:49,360 DEBUG TRAIN Batch 44/31400 loss 12.126583 loss_att 15.383879 loss_ctc 21.129545 loss_rnnt 10.173952 hw_loss 0.188957 lr 0.00029496 rank 0
2023-03-01 06:45:10,535 DEBUG TRAIN Batch 44/31500 loss 6.253965 loss_att 9.918802 loss_ctc 10.499855 loss_rnnt 4.906690 hw_loss 0.090354 lr 0.00029495 rank 1
2023-03-01 06:45:10,537 DEBUG TRAIN Batch 44/31500 loss 13.087416 loss_att 14.752392 loss_ctc 14.282372 loss_rnnt 12.406679 hw_loss 0.353274 lr 0.00029495 rank 0
2023-03-01 06:45:32,271 DEBUG TRAIN Batch 44/31600 loss 5.765463 loss_att 8.454628 loss_ctc 9.582502 loss_rnnt 4.633225 hw_loss 0.160249 lr 0.00029494 rank 1
2023-03-01 06:45:32,271 DEBUG TRAIN Batch 44/31600 loss 14.778209 loss_att 17.048635 loss_ctc 22.140434 loss_rnnt 13.214176 hw_loss 0.240595 lr 0.00029494 rank 0
2023-03-01 06:46:04,221 DEBUG TRAIN Batch 44/31700 loss 7.919298 loss_att 9.652094 loss_ctc 15.053222 loss_rnnt 6.485617 hw_loss 0.254874 lr 0.00029492 rank 1
2023-03-01 06:46:04,222 DEBUG TRAIN Batch 44/31700 loss 7.707992 loss_att 9.844915 loss_ctc 15.549874 loss_rnnt 6.143775 hw_loss 0.171090 lr 0.00029492 rank 0
2023-03-01 06:46:26,148 DEBUG TRAIN Batch 44/31800 loss 10.128879 loss_att 10.897946 loss_ctc 14.602440 loss_rnnt 9.229782 hw_loss 0.279013 lr 0.00029491 rank 1
2023-03-01 06:46:26,149 DEBUG TRAIN Batch 44/31800 loss 8.863987 loss_att 10.428868 loss_ctc 11.519482 loss_rnnt 8.021643 hw_loss 0.328691 lr 0.00029491 rank 0
2023-03-01 06:46:48,040 DEBUG TRAIN Batch 44/31900 loss 9.125954 loss_att 10.288016 loss_ctc 14.868509 loss_rnnt 7.960362 hw_loss 0.314071 lr 0.00029490 rank 1
2023-03-01 06:46:48,041 DEBUG TRAIN Batch 44/31900 loss 4.606452 loss_att 4.742236 loss_ctc 6.287281 loss_rnnt 4.167889 hw_loss 0.351179 lr 0.00029490 rank 0
2023-03-01 06:47:10,041 DEBUG TRAIN Batch 44/32000 loss 5.998925 loss_att 10.485374 loss_ctc 11.696735 loss_rnnt 4.192086 hw_loss 0.280952 lr 0.00029488 rank 1
2023-03-01 06:47:10,041 DEBUG TRAIN Batch 44/32000 loss 3.416825 loss_att 7.192286 loss_ctc 7.614948 loss_rnnt 2.023155 hw_loss 0.147801 lr 0.00029488 rank 0
2023-03-01 06:47:42,479 DEBUG TRAIN Batch 44/32100 loss 7.530122 loss_att 10.620704 loss_ctc 13.660858 loss_rnnt 5.989941 hw_loss 0.196187 lr 0.00029487 rank 1
2023-03-01 06:47:42,480 DEBUG TRAIN Batch 44/32100 loss 3.786682 loss_att 6.249348 loss_ctc 6.658629 loss_rnnt 2.781668 hw_loss 0.242915 lr 0.00029487 rank 0
2023-03-01 06:48:04,256 DEBUG TRAIN Batch 44/32200 loss 7.525813 loss_att 11.875708 loss_ctc 18.392462 loss_rnnt 5.085553 hw_loss 0.227614 lr 0.00029486 rank 1
2023-03-01 06:48:04,258 DEBUG TRAIN Batch 44/32200 loss 3.571243 loss_att 7.276933 loss_ctc 9.733528 loss_rnnt 1.826979 hw_loss 0.340290 lr 0.00029486 rank 0
2023-03-01 06:48:25,650 DEBUG TRAIN Batch 44/32300 loss 5.886681 loss_att 9.904497 loss_ctc 9.916966 loss_rnnt 4.403056 hw_loss 0.267545 lr 0.00029485 rank 1
2023-03-01 06:48:25,652 DEBUG TRAIN Batch 44/32300 loss 3.579674 loss_att 4.629087 loss_ctc 4.366057 loss_rnnt 3.147992 hw_loss 0.219277 lr 0.00029485 rank 0
2023-03-01 06:48:57,441 DEBUG TRAIN Batch 44/32400 loss 8.988003 loss_att 11.018829 loss_ctc 15.145655 loss_rnnt 7.717949 hw_loss 0.080379 lr 0.00029483 rank 1
2023-03-01 06:48:57,441 DEBUG TRAIN Batch 44/32400 loss 5.293365 loss_att 8.103571 loss_ctc 7.838237 loss_rnnt 4.238912 hw_loss 0.287053 lr 0.00029483 rank 0
2023-03-01 06:49:19,216 DEBUG TRAIN Batch 44/32500 loss 6.414472 loss_att 8.342871 loss_ctc 10.074162 loss_rnnt 5.398109 hw_loss 0.267607 lr 0.00029482 rank 0
2023-03-01 06:49:19,216 DEBUG TRAIN Batch 44/32500 loss 4.767445 loss_att 7.124842 loss_ctc 10.190182 loss_rnnt 3.379494 hw_loss 0.362700 lr 0.00029482 rank 1
2023-03-01 06:49:41,288 DEBUG TRAIN Batch 44/32600 loss 9.059171 loss_att 11.793289 loss_ctc 15.449442 loss_rnnt 7.404190 hw_loss 0.480226 lr 0.00029481 rank 0
2023-03-01 06:49:41,288 DEBUG TRAIN Batch 44/32600 loss 9.612544 loss_att 11.804293 loss_ctc 17.134008 loss_rnnt 7.943029 hw_loss 0.428069 lr 0.00029481 rank 1
2023-03-01 06:50:02,777 DEBUG TRAIN Batch 44/32700 loss 5.407992 loss_att 9.477960 loss_ctc 7.070470 loss_rnnt 4.298835 hw_loss 0.137814 lr 0.00029479 rank 1
2023-03-01 06:50:02,778 DEBUG TRAIN Batch 44/32700 loss 7.252665 loss_att 10.176435 loss_ctc 11.175102 loss_rnnt 6.027837 hw_loss 0.219528 lr 0.00029479 rank 0
2023-03-01 06:50:36,231 DEBUG TRAIN Batch 44/32800 loss 4.818357 loss_att 9.301145 loss_ctc 9.088694 loss_rnnt 3.248893 hw_loss 0.194115 lr 0.00029478 rank 1
2023-03-01 06:50:36,232 DEBUG TRAIN Batch 44/32800 loss 5.812185 loss_att 9.071930 loss_ctc 8.745004 loss_rnnt 4.669333 hw_loss 0.187239 lr 0.00029478 rank 0
2023-03-01 06:50:57,837 DEBUG TRAIN Batch 44/32900 loss 8.560909 loss_att 13.666537 loss_ctc 19.250717 loss_rnnt 6.072229 hw_loss 0.079213 lr 0.00029477 rank 1
2023-03-01 06:50:57,838 DEBUG TRAIN Batch 44/32900 loss 10.751992 loss_att 14.750883 loss_ctc 20.015965 loss_rnnt 8.570787 hw_loss 0.274180 lr 0.00029477 rank 0
2023-03-01 06:51:19,181 DEBUG TRAIN Batch 44/33000 loss 7.954329 loss_att 13.251387 loss_ctc 15.295058 loss_rnnt 5.790247 hw_loss 0.236073 lr 0.00029476 rank 1
2023-03-01 06:51:19,182 DEBUG TRAIN Batch 44/33000 loss 5.880283 loss_att 9.558604 loss_ctc 9.755903 loss_rnnt 4.504357 hw_loss 0.231585 lr 0.00029476 rank 0
2023-03-01 06:51:43,716 DEBUG TRAIN Batch 44/33100 loss 9.374877 loss_att 13.369588 loss_ctc 18.672194 loss_rnnt 7.179421 hw_loss 0.294132 lr 0.00029474 rank 1
2023-03-01 06:51:43,717 DEBUG TRAIN Batch 44/33100 loss 5.872124 loss_att 8.333716 loss_ctc 10.240928 loss_rnnt 4.687659 hw_loss 0.205575 lr 0.00029474 rank 0
2023-03-01 06:52:05,553 DEBUG TRAIN Batch 44/33200 loss 7.139542 loss_att 7.873606 loss_ctc 10.954282 loss_rnnt 6.257369 hw_loss 0.425116 lr 0.00029473 rank 0
2023-03-01 06:52:05,567 DEBUG TRAIN Batch 44/33200 loss 5.497298 loss_att 5.714288 loss_ctc 8.422265 loss_rnnt 4.846160 hw_loss 0.408269 lr 0.00029473 rank 1
2023-03-01 06:52:26,713 DEBUG TRAIN Batch 44/33300 loss 3.072025 loss_att 4.693919 loss_ctc 8.120059 loss_rnnt 1.947096 hw_loss 0.239023 lr 0.00029472 rank 1
2023-03-01 06:52:26,716 DEBUG TRAIN Batch 44/33300 loss 2.522788 loss_att 4.739468 loss_ctc 4.526986 loss_rnnt 1.601225 hw_loss 0.395627 lr 0.00029472 rank 0
2023-03-01 06:52:39,879 DEBUG CV Batch 44/0 loss 1.220134 loss_att 1.049964 loss_ctc 2.195359 loss_rnnt 0.919337 hw_loss 0.384001 history loss 1.174944 rank 0
2023-03-01 06:52:39,883 DEBUG CV Batch 44/0 loss 1.220134 loss_att 1.049964 loss_ctc 2.195359 loss_rnnt 0.919337 hw_loss 0.384001 history loss 1.174944 rank 1
2023-03-01 06:52:47,461 DEBUG CV Batch 44/100 loss 3.675184 loss_att 4.982674 loss_ctc 8.331108 loss_rnnt 2.661162 hw_loss 0.247001 history loss 3.006639 rank 0
2023-03-01 06:52:47,655 DEBUG CV Batch 44/100 loss 3.675184 loss_att 4.982674 loss_ctc 8.331108 loss_rnnt 2.661162 hw_loss 0.247001 history loss 3.006639 rank 1
2023-03-01 06:52:57,246 DEBUG CV Batch 44/200 loss 5.462955 loss_att 8.141899 loss_ctc 7.668537 loss_rnnt 4.585129 hw_loss 0.089926 history loss 3.530393 rank 0
2023-03-01 06:52:57,423 DEBUG CV Batch 44/200 loss 5.462955 loss_att 8.141899 loss_ctc 7.668537 loss_rnnt 4.585129 hw_loss 0.089926 history loss 3.530393 rank 1
2023-03-01 06:53:05,833 DEBUG CV Batch 44/300 loss 3.613651 loss_att 4.382339 loss_ctc 6.474351 loss_rnnt 2.917595 hw_loss 0.301671 history loss 3.703356 rank 1
2023-03-01 06:53:05,991 DEBUG CV Batch 44/300 loss 3.613651 loss_att 4.382339 loss_ctc 6.474351 loss_rnnt 2.917595 hw_loss 0.301671 history loss 3.703356 rank 0
2023-03-01 06:53:14,232 DEBUG CV Batch 44/400 loss 17.096825 loss_att 69.397125 loss_ctc 9.359360 loss_rnnt 7.615479 hw_loss 0.099277 history loss 4.564930 rank 1
2023-03-01 06:53:14,453 DEBUG CV Batch 44/400 loss 17.096825 loss_att 69.397125 loss_ctc 9.359360 loss_rnnt 7.615479 hw_loss 0.099277 history loss 4.564930 rank 0
2023-03-01 06:53:21,021 DEBUG CV Batch 44/500 loss 4.607132 loss_att 4.986826 loss_ctc 5.836164 loss_rnnt 4.214056 hw_loss 0.287373 history loss 5.186301 rank 1
2023-03-01 06:53:21,321 DEBUG CV Batch 44/500 loss 4.607132 loss_att 4.986826 loss_ctc 5.836164 loss_rnnt 4.214056 hw_loss 0.287373 history loss 5.186301 rank 0
2023-03-01 06:53:29,475 DEBUG CV Batch 44/600 loss 7.084784 loss_att 6.371981 loss_ctc 9.761873 loss_rnnt 6.632505 hw_loss 0.446052 history loss 6.059815 rank 1
2023-03-01 06:53:29,838 DEBUG CV Batch 44/600 loss 7.084784 loss_att 6.371981 loss_ctc 9.761873 loss_rnnt 6.632505 hw_loss 0.446052 history loss 6.059815 rank 0
2023-03-01 06:53:37,197 DEBUG CV Batch 44/700 loss 10.112924 loss_att 20.874485 loss_ctc 14.574965 loss_rnnt 7.364994 hw_loss 0.001272 history loss 6.576474 rank 1
2023-03-01 06:53:37,624 DEBUG CV Batch 44/700 loss 10.112924 loss_att 20.874485 loss_ctc 14.574965 loss_rnnt 7.364994 hw_loss 0.001272 history loss 6.576474 rank 0
2023-03-01 06:53:44,652 DEBUG CV Batch 44/800 loss 6.022512 loss_att 6.996973 loss_ctc 12.799854 loss_rnnt 4.763259 hw_loss 0.301341 history loss 6.103181 rank 1
2023-03-01 06:53:45,184 DEBUG CV Batch 44/800 loss 6.022512 loss_att 6.996973 loss_ctc 12.799854 loss_rnnt 4.763259 hw_loss 0.301341 history loss 6.103181 rank 0
2023-03-01 06:53:54,358 DEBUG CV Batch 44/900 loss 7.980972 loss_att 10.130877 loss_ctc 15.735536 loss_rnnt 6.393839 hw_loss 0.231018 history loss 5.931159 rank 1
2023-03-01 06:53:54,994 DEBUG CV Batch 44/900 loss 7.980972 loss_att 10.130877 loss_ctc 15.735536 loss_rnnt 6.393839 hw_loss 0.231018 history loss 5.931159 rank 0
2023-03-01 06:54:02,898 DEBUG CV Batch 44/1000 loss 3.268054 loss_att 4.608129 loss_ctc 4.627900 loss_rnnt 2.638914 hw_loss 0.337148 history loss 5.744597 rank 1
2023-03-01 06:54:03,671 DEBUG CV Batch 44/1000 loss 3.268054 loss_att 4.608129 loss_ctc 4.627900 loss_rnnt 2.638914 hw_loss 0.337148 history loss 5.744597 rank 0
2023-03-01 06:54:11,192 DEBUG CV Batch 44/1100 loss 4.711618 loss_att 4.670808 loss_ctc 7.550691 loss_rnnt 4.138379 hw_loss 0.380359 history loss 5.719345 rank 1
2023-03-01 06:54:12,042 DEBUG CV Batch 44/1100 loss 4.711618 loss_att 4.670808 loss_ctc 7.550691 loss_rnnt 4.138379 hw_loss 0.380359 history loss 5.719345 rank 0
2023-03-01 06:54:18,011 DEBUG CV Batch 44/1200 loss 6.531282 loss_att 6.800958 loss_ctc 8.207892 loss_rnnt 6.142821 hw_loss 0.208083 history loss 6.004245 rank 1
2023-03-01 06:54:18,958 DEBUG CV Batch 44/1200 loss 6.531282 loss_att 6.800958 loss_ctc 8.207892 loss_rnnt 6.142821 hw_loss 0.208083 history loss 6.004245 rank 0
2023-03-01 06:54:26,338 DEBUG CV Batch 44/1300 loss 4.834076 loss_att 4.605671 loss_ctc 6.814595 loss_rnnt 4.435444 hw_loss 0.337958 history loss 6.296925 rank 1
2023-03-01 06:54:27,352 DEBUG CV Batch 44/1300 loss 4.834076 loss_att 4.605671 loss_ctc 6.814595 loss_rnnt 4.435444 hw_loss 0.337958 history loss 6.296925 rank 0
2023-03-01 06:54:33,874 DEBUG CV Batch 44/1400 loss 3.907872 loss_att 14.180786 loss_ctc 5.825475 loss_rnnt 1.596930 hw_loss 0.001272 history loss 6.556531 rank 1
2023-03-01 06:54:34,987 DEBUG CV Batch 44/1400 loss 3.907872 loss_att 14.180786 loss_ctc 5.825475 loss_rnnt 1.596930 hw_loss 0.001272 history loss 6.556531 rank 0
2023-03-01 06:54:41,521 DEBUG CV Batch 44/1500 loss 7.050069 loss_att 7.592520 loss_ctc 7.459674 loss_rnnt 6.713874 hw_loss 0.324544 history loss 6.427256 rank 1
2023-03-01 06:54:42,733 DEBUG CV Batch 44/1500 loss 7.050069 loss_att 7.592520 loss_ctc 7.459674 loss_rnnt 6.713874 hw_loss 0.324545 history loss 6.427256 rank 0
2023-03-01 06:54:50,936 DEBUG CV Batch 44/1600 loss 11.389887 loss_att 14.335804 loss_ctc 14.148859 loss_rnnt 10.286444 hw_loss 0.274494 history loss 6.387877 rank 1
2023-03-01 06:54:52,250 DEBUG CV Batch 44/1600 loss 11.389887 loss_att 14.335804 loss_ctc 14.148859 loss_rnnt 10.286444 hw_loss 0.274494 history loss 6.387877 rank 0
2023-03-01 06:54:59,819 DEBUG CV Batch 44/1700 loss 6.393665 loss_att 6.134247 loss_ctc 11.971035 loss_rnnt 5.478262 hw_loss 0.419320 history loss 6.326539 rank 1
2023-03-01 06:55:01,192 DEBUG CV Batch 44/1700 loss 6.393665 loss_att 6.134247 loss_ctc 11.971035 loss_rnnt 5.478262 hw_loss 0.419320 history loss 6.326539 rank 0
2023-03-01 06:55:06,465 INFO Epoch 44 CV info cv_loss 6.311289106702025
2023-03-01 06:55:06,465 INFO Epoch 45 TRAIN info lr 0.00029471174842738535
2023-03-01 06:55:06,466 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 06:55:07,886 INFO Epoch 44 CV info cv_loss 6.311289106577114
2023-03-01 06:55:07,887 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head_correct/44.pt
2023-03-01 06:55:08,293 INFO Epoch 45 TRAIN info lr 0.00029471226037258393
2023-03-01 06:55:08,295 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 06:55:41,043 DEBUG TRAIN Batch 45/0 loss 6.918471 loss_att 6.983129 loss_ctc 11.466039 loss_rnnt 6.139001 hw_loss 0.300366 lr 0.00029471 rank 1
2023-03-01 06:55:41,050 DEBUG TRAIN Batch 45/0 loss 7.024840 loss_att 7.742870 loss_ctc 10.124181 loss_rnnt 6.240403 hw_loss 0.426722 lr 0.00029471 rank 0
2023-03-01 06:56:01,885 DEBUG TRAIN Batch 45/100 loss 9.208805 loss_att 11.743473 loss_ctc 13.007408 loss_rnnt 8.071909 hw_loss 0.231529 lr 0.00029470 rank 1
2023-03-01 06:56:01,886 DEBUG TRAIN Batch 45/100 loss 7.651170 loss_att 12.618061 loss_ctc 16.821486 loss_rnnt 5.412485 hw_loss 0.042372 lr 0.00029470 rank 0
2023-03-01 06:56:23,626 DEBUG TRAIN Batch 45/200 loss 2.208718 loss_att 5.993378 loss_ctc 5.600862 loss_rnnt 0.857043 hw_loss 0.267108 lr 0.00029469 rank 1
2023-03-01 06:56:23,628 DEBUG TRAIN Batch 45/200 loss 4.965791 loss_att 7.084053 loss_ctc 7.726961 loss_rnnt 3.968342 hw_loss 0.385575 lr 0.00029469 rank 0
2023-03-01 06:56:44,928 DEBUG TRAIN Batch 45/300 loss 8.356898 loss_att 11.370924 loss_ctc 16.289181 loss_rnnt 6.482166 hw_loss 0.401792 lr 0.00029467 rank 0
2023-03-01 06:56:44,929 DEBUG TRAIN Batch 45/300 loss 11.077670 loss_att 11.671488 loss_ctc 15.935657 loss_rnnt 10.143208 hw_loss 0.314939 lr 0.00029467 rank 1
2023-03-01 06:57:16,467 DEBUG TRAIN Batch 45/400 loss 5.393770 loss_att 6.840877 loss_ctc 6.135381 loss_rnnt 4.742919 hw_loss 0.492277 lr 0.00029466 rank 1
2023-03-01 06:57:16,469 DEBUG TRAIN Batch 45/400 loss 4.695992 loss_att 6.329726 loss_ctc 6.966826 loss_rnnt 3.846922 hw_loss 0.411646 lr 0.00029466 rank 0
2023-03-01 06:57:38,036 DEBUG TRAIN Batch 45/500 loss 5.909066 loss_att 7.461513 loss_ctc 7.038282 loss_rnnt 5.376972 hw_loss 0.133205 lr 0.00029465 rank 0
2023-03-01 06:57:38,037 DEBUG TRAIN Batch 45/500 loss 3.132256 loss_att 6.022027 loss_ctc 4.616009 loss_rnnt 2.355724 hw_loss 0.001394 lr 0.00029465 rank 1
2023-03-01 06:57:59,777 DEBUG TRAIN Batch 45/600 loss 6.364289 loss_att 7.793295 loss_ctc 11.189142 loss_rnnt 5.292487 hw_loss 0.267537 lr 0.00029463 rank 1
2023-03-01 06:57:59,778 DEBUG TRAIN Batch 45/600 loss 8.000702 loss_att 9.830638 loss_ctc 16.137148 loss_rnnt 6.474232 hw_loss 0.141794 lr 0.00029463 rank 0
2023-03-01 06:58:21,715 DEBUG TRAIN Batch 45/700 loss 7.037795 loss_att 11.823563 loss_ctc 10.229177 loss_rnnt 5.454990 hw_loss 0.375251 lr 0.00029462 rank 0
2023-03-01 06:58:21,716 DEBUG TRAIN Batch 45/700 loss 5.512893 loss_att 7.754696 loss_ctc 8.116752 loss_rnnt 4.601200 hw_loss 0.217785 lr 0.00029462 rank 1
2023-03-01 06:58:53,155 DEBUG TRAIN Batch 45/800 loss 6.195204 loss_att 10.936270 loss_ctc 11.373485 loss_rnnt 4.453358 hw_loss 0.193490 lr 0.00029461 rank 1
2023-03-01 06:58:53,156 DEBUG TRAIN Batch 45/800 loss 6.287383 loss_att 8.789681 loss_ctc 12.261241 loss_rnnt 4.930123 hw_loss 0.113036 lr 0.00029461 rank 0
2023-03-01 06:59:14,806 DEBUG TRAIN Batch 45/900 loss 4.834756 loss_att 7.567525 loss_ctc 5.186824 loss_rnnt 4.147807 hw_loss 0.175224 lr 0.00029460 rank 0
2023-03-01 06:59:14,805 DEBUG TRAIN Batch 45/900 loss 17.467251 loss_att 21.474789 loss_ctc 27.339275 loss_rnnt 15.198538 hw_loss 0.283002 lr 0.00029460 rank 1
2023-03-01 06:59:35,887 DEBUG TRAIN Batch 45/1000 loss 13.503182 loss_att 14.882551 loss_ctc 20.050131 loss_rnnt 12.238789 hw_loss 0.216737 lr 0.00029458 rank 0
2023-03-01 06:59:35,887 DEBUG TRAIN Batch 45/1000 loss 4.217996 loss_att 7.345304 loss_ctc 6.225945 loss_rnnt 3.192559 hw_loss 0.247965 lr 0.00029458 rank 1
2023-03-01 07:00:07,922 DEBUG TRAIN Batch 45/1100 loss 4.465322 loss_att 8.401503 loss_ctc 7.550745 loss_rnnt 3.186760 hw_loss 0.149880 lr 0.00029457 rank 1
2023-03-01 07:00:07,922 DEBUG TRAIN Batch 45/1100 loss 6.469203 loss_att 6.969317 loss_ctc 8.927636 loss_rnnt 5.888176 hw_loss 0.287274 lr 0.00029457 rank 0
2023-03-01 07:00:30,168 DEBUG TRAIN Batch 45/1200 loss 14.301958 loss_att 17.482384 loss_ctc 19.020281 loss_rnnt 12.979526 hw_loss 0.107323 lr 0.00029456 rank 0
2023-03-01 07:00:30,168 DEBUG TRAIN Batch 45/1200 loss 5.567646 loss_att 9.732404 loss_ctc 13.875472 loss_rnnt 3.502393 hw_loss 0.233609 lr 0.00029456 rank 1
2023-03-01 07:00:52,651 DEBUG TRAIN Batch 45/1300 loss 1.662814 loss_att 3.068996 loss_ctc 3.707715 loss_rnnt 1.038681 hw_loss 0.131706 lr 0.00029454 rank 1
2023-03-01 07:00:52,651 DEBUG TRAIN Batch 45/1300 loss 7.071676 loss_att 9.627474 loss_ctc 11.196655 loss_rnnt 5.853439 hw_loss 0.294526 lr 0.00029455 rank 0
2023-03-01 07:01:14,205 DEBUG TRAIN Batch 45/1400 loss 5.408208 loss_att 9.918218 loss_ctc 13.056212 loss_rnnt 3.387130 hw_loss 0.186266 lr 0.00029453 rank 0
2023-03-01 07:01:14,204 DEBUG TRAIN Batch 45/1400 loss 5.087023 loss_att 8.225351 loss_ctc 8.029228 loss_rnnt 3.995087 hw_loss 0.134956 lr 0.00029453 rank 1
2023-03-01 07:01:46,557 DEBUG TRAIN Batch 45/1500 loss 12.585107 loss_att 15.982430 loss_ctc 20.342230 loss_rnnt 10.710854 hw_loss 0.300945 lr 0.00029452 rank 0
2023-03-01 07:01:46,557 DEBUG TRAIN Batch 45/1500 loss 4.685154 loss_att 9.344892 loss_ctc 11.325815 loss_rnnt 2.718122 hw_loss 0.280620 lr 0.00029452 rank 1
2023-03-01 07:02:08,136 DEBUG TRAIN Batch 45/1600 loss 8.916833 loss_att 10.928751 loss_ctc 21.512815 loss_rnnt 6.724743 hw_loss 0.206703 lr 0.00029451 rank 0
2023-03-01 07:02:08,136 DEBUG TRAIN Batch 45/1600 loss 3.974540 loss_att 6.249110 loss_ctc 6.258447 loss_rnnt 3.125461 hw_loss 0.168083 lr 0.00029451 rank 1
2023-03-01 07:02:29,906 DEBUG TRAIN Batch 45/1700 loss 5.992671 loss_att 10.409964 loss_ctc 11.153391 loss_rnnt 4.301505 hw_loss 0.224272 lr 0.00029449 rank 0
2023-03-01 07:02:29,906 DEBUG TRAIN Batch 45/1700 loss 5.170251 loss_att 7.619219 loss_ctc 7.071889 loss_rnnt 4.313658 hw_loss 0.212339 lr 0.00029449 rank 1
2023-03-01 07:03:02,888 DEBUG TRAIN Batch 45/1800 loss 5.869918 loss_att 7.482095 loss_ctc 8.873466 loss_rnnt 5.027233 hw_loss 0.224582 lr 0.00029448 rank 1
2023-03-01 07:03:02,888 DEBUG TRAIN Batch 45/1800 loss 6.420631 loss_att 8.440454 loss_ctc 10.118159 loss_rnnt 5.382687 hw_loss 0.264329 lr 0.00029448 rank 0
2023-03-01 07:03:25,187 DEBUG TRAIN Batch 45/1900 loss 8.511716 loss_att 9.532187 loss_ctc 13.533813 loss_rnnt 7.430939 hw_loss 0.388257 lr 0.00029447 rank 1
2023-03-01 07:03:25,188 DEBUG TRAIN Batch 45/1900 loss 7.810678 loss_att 14.749312 loss_ctc 18.698917 loss_rnnt 4.844961 hw_loss 0.236674 lr 0.00029447 rank 0
2023-03-01 07:03:46,886 DEBUG TRAIN Batch 45/2000 loss 5.845439 loss_att 8.250538 loss_ctc 9.489298 loss_rnnt 4.770411 hw_loss 0.202801 lr 0.00029446 rank 1
2023-03-01 07:03:46,887 DEBUG TRAIN Batch 45/2000 loss 5.567814 loss_att 7.798563 loss_ctc 7.907947 loss_rnnt 4.654140 hw_loss 0.291576 lr 0.00029446 rank 0
2023-03-01 07:04:09,301 DEBUG TRAIN Batch 45/2100 loss 10.782645 loss_att 15.453754 loss_ctc 15.669978 loss_rnnt 9.068896 hw_loss 0.239779 lr 0.00029444 rank 1
2023-03-01 07:04:09,302 DEBUG TRAIN Batch 45/2100 loss 2.248667 loss_att 5.144221 loss_ctc 2.812906 loss_rnnt 1.481951 hw_loss 0.210701 lr 0.00029444 rank 0
2023-03-01 07:04:40,958 DEBUG TRAIN Batch 45/2200 loss 6.663280 loss_att 10.739552 loss_ctc 11.349668 loss_rnnt 5.116062 hw_loss 0.200837 lr 0.00029443 rank 1
2023-03-01 07:04:40,960 DEBUG TRAIN Batch 45/2200 loss 7.902461 loss_att 10.007567 loss_ctc 11.198919 loss_rnnt 6.930952 hw_loss 0.208049 lr 0.00029443 rank 0
2023-03-01 07:05:02,815 DEBUG TRAIN Batch 45/2300 loss 11.165738 loss_att 12.651073 loss_ctc 11.379530 loss_rnnt 10.688290 hw_loss 0.284767 lr 0.00029442 rank 1
2023-03-01 07:05:02,815 DEBUG TRAIN Batch 45/2300 loss 5.647095 loss_att 7.084436 loss_ctc 7.889278 loss_rnnt 4.871749 hw_loss 0.354225 lr 0.00029442 rank 0
2023-03-01 07:05:24,779 DEBUG TRAIN Batch 45/2400 loss 4.330025 loss_att 6.855319 loss_ctc 7.288032 loss_rnnt 3.264628 hw_loss 0.311131 lr 0.00029440 rank 1
2023-03-01 07:05:24,779 DEBUG TRAIN Batch 45/2400 loss 7.557332 loss_att 9.826778 loss_ctc 12.168947 loss_rnnt 6.279735 hw_loss 0.391549 lr 0.00029441 rank 0
2023-03-01 07:05:57,144 DEBUG TRAIN Batch 45/2500 loss 7.401840 loss_att 9.774007 loss_ctc 13.427360 loss_rnnt 5.939452 hw_loss 0.346035 lr 0.00029439 rank 1
2023-03-01 07:05:57,144 DEBUG TRAIN Batch 45/2500 loss 8.997764 loss_att 9.454597 loss_ctc 13.595254 loss_rnnt 8.041258 hw_loss 0.472762 lr 0.00029439 rank 0
Terminated
Terminated
