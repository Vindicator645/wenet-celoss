/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_17_rnnt_bias_1word.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_17_rnnt_bias_loss_2_class_1word/ddp_init
2023-02-17 12:44:15,304 INFO training on multiple gpus, this gpu 5
2023-02-17 12:44:15,305 INFO training on multiple gpus, this gpu 1
2023-02-17 12:44:15,307 INFO training on multiple gpus, this gpu 0
2023-02-17 12:44:15,308 INFO training on multiple gpus, this gpu 7
2023-02-17 12:44:15,309 INFO training on multiple gpus, this gpu 2
2023-02-17 12:44:15,309 INFO training on multiple gpus, this gpu 3
2023-02-17 12:44:15,351 INFO training on multiple gpus, this gpu 4
2023-02-17 12:44:15,355 INFO training on multiple gpus, this gpu 6
2023-02-17 12:44:26,470 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-17 12:44:26,482 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-17 12:44:26,485 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-17 12:44:26,504 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-17 12:44:26,505 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-17 12:44:26,515 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-17 12:44:27,503 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-17 12:44:27,536 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-17 12:44:27,538 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:27,539 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:27,547 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:27,635 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:27,727 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:28,079 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:29,520 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:29,524 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 12:44:46,378 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word/init.pt
2023-02-17 12:44:46,381 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:46,383 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:44:46,386 INFO Epoch 0 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,388 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,411 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:46,413 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,424 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:46,426 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:44:46,426 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:46,428 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,431 INFO Epoch 0 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,433 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:46,455 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:46,458 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:44:47,027 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 12:44:47,030 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 12:45:48,287 DEBUG TRAIN Batch 0/0 loss 558.226501 loss_att 75.628586 loss_ctc 536.186340 loss_rnnt 657.275757 hw_loss 0.766901 lr 0.00000004 rank 6
2023-02-17 12:45:48,291 DEBUG TRAIN Batch 0/0 loss 545.630737 loss_att 83.787941 loss_ctc 504.185028 loss_rnnt 642.977112 hw_loss 1.028030 lr 0.00000004 rank 0
2023-02-17 12:45:48,300 DEBUG TRAIN Batch 0/0 loss 547.724792 loss_att 68.157715 loss_ctc 522.774719 loss_rnnt 646.329407 hw_loss 1.191520 lr 0.00000004 rank 4
2023-02-17 12:45:48,301 DEBUG TRAIN Batch 0/0 loss 522.464722 loss_att 74.558525 loss_ctc 494.863617 loss_rnnt 615.152710 hw_loss 1.075163 lr 0.00000004 rank 3
2023-02-17 12:45:48,304 DEBUG TRAIN Batch 0/0 loss 542.467102 loss_att 74.409958 loss_ctc 525.720520 loss_rnnt 637.832336 hw_loss 0.898276 lr 0.00000004 rank 5
2023-02-17 12:45:48,305 DEBUG TRAIN Batch 0/0 loss 587.991394 loss_att 77.099792 loss_ctc 570.017151 loss_rnnt 692.063049 hw_loss 0.943638 lr 0.00000004 rank 1
2023-02-17 12:45:48,364 DEBUG TRAIN Batch 0/0 loss 557.660889 loss_att 77.514160 loss_ctc 517.883057 loss_rnnt 658.400757 hw_loss 1.112283 lr 0.00000004 rank 2
2023-02-17 12:45:48,607 DEBUG TRAIN Batch 0/0 loss 501.545776 loss_att 74.212875 loss_ctc 478.954590 loss_rnnt 589.471802 hw_loss 1.036349 lr 0.00000004 rank 7
2023-02-17 12:47:03,879 DEBUG TRAIN Batch 0/100 loss 2143.705566 loss_att 349.064392 loss_ctc 3065.100342 loss_rnnt 2379.487793 hw_loss 0.549692 lr 0.00000404 rank 4
2023-02-17 12:47:03,880 DEBUG TRAIN Batch 0/100 loss 2113.695801 loss_att 431.982941 loss_ctc 3013.942383 loss_rnnt 2329.617188 hw_loss 0.728356 lr 0.00000404 rank 0
2023-02-17 12:47:03,881 DEBUG TRAIN Batch 0/100 loss 2027.200195 loss_att 362.973236 loss_ctc 3119.628662 loss_rnnt 2214.057861 hw_loss 0.619403 lr 0.00000404 rank 1
2023-02-17 12:47:03,883 DEBUG TRAIN Batch 0/100 loss 2089.345215 loss_att 384.945709 loss_ctc 3123.118896 loss_rnnt 2291.973145 hw_loss 0.778891 lr 0.00000404 rank 6
2023-02-17 12:47:03,884 DEBUG TRAIN Batch 0/100 loss 2194.183838 loss_att 459.011688 loss_ctc 3069.947754 loss_rnnt 2424.223145 hw_loss 0.424757 lr 0.00000404 rank 7
2023-02-17 12:47:03,889 DEBUG TRAIN Batch 0/100 loss 2179.938477 loss_att 384.766937 loss_ctc 3163.220703 loss_rnnt 2407.584717 hw_loss 0.532118 lr 0.00000404 rank 3
2023-02-17 12:47:03,893 DEBUG TRAIN Batch 0/100 loss 2061.052002 loss_att 357.861328 loss_ctc 2983.726074 loss_rnnt 2278.170654 hw_loss 0.930511 lr 0.00000404 rank 2
2023-02-17 12:47:03,897 DEBUG TRAIN Batch 0/100 loss 2065.616455 loss_att 393.462097 loss_ctc 2928.718750 loss_rnnt 2284.727051 hw_loss 0.449686 lr 0.00000404 rank 5
2023-02-17 12:48:17,776 DEBUG TRAIN Batch 0/200 loss 1027.386963 loss_att 380.020233 loss_ctc 2535.966309 loss_rnnt 955.538269 hw_loss 0.334116 lr 0.00000804 rank 6
2023-02-17 12:48:17,779 DEBUG TRAIN Batch 0/200 loss 885.098816 loss_att 389.635773 loss_ctc 2445.293945 loss_rnnt 776.087769 hw_loss 0.145512 lr 0.00000804 rank 1
2023-02-17 12:48:17,780 DEBUG TRAIN Batch 0/200 loss 916.192139 loss_att 439.072998 loss_ctc 2543.417969 loss_rnnt 794.584961 hw_loss 0.126604 lr 0.00000804 rank 3
2023-02-17 12:48:17,781 DEBUG TRAIN Batch 0/200 loss 921.975281 loss_att 421.310608 loss_ctc 2545.645020 loss_rnnt 805.521240 hw_loss 0.183163 lr 0.00000804 rank 2
2023-02-17 12:48:17,785 DEBUG TRAIN Batch 0/200 loss 973.263794 loss_att 456.081451 loss_ctc 2551.510742 loss_rnnt 866.081421 hw_loss 0.348589 lr 0.00000804 rank 7
2023-02-17 12:48:17,791 DEBUG TRAIN Batch 0/200 loss 867.590393 loss_att 348.207611 loss_ctc 2548.999512 loss_rnnt 747.135925 hw_loss 0.268434 lr 0.00000804 rank 5
2023-02-17 12:48:17,808 DEBUG TRAIN Batch 0/200 loss 888.634521 loss_att 357.440247 loss_ctc 2497.874023 loss_rnnt 780.171143 hw_loss 0.256865 lr 0.00000804 rank 4
2023-02-17 12:48:17,816 DEBUG TRAIN Batch 0/200 loss 904.903320 loss_att 349.422424 loss_ctc 2503.023438 loss_rnnt 802.814880 hw_loss 0.191061 lr 0.00000804 rank 0
2023-02-17 12:49:32,317 DEBUG TRAIN Batch 0/300 loss 425.046417 loss_att 372.981903 loss_ctc 691.741882 loss_rnnt 399.851074 hw_loss 0.091558 lr 0.00001204 rank 2
2023-02-17 12:49:32,324 DEBUG TRAIN Batch 0/300 loss 492.472351 loss_att 392.135895 loss_ctc 1178.259155 loss_rnnt 421.026001 hw_loss 0.141397 lr 0.00001204 rank 7
2023-02-17 12:49:32,324 DEBUG TRAIN Batch 0/300 loss 419.623596 loss_att 332.557983 loss_ctc 1051.258545 loss_rnnt 352.755615 hw_loss 0.118298 lr 0.00001204 rank 1
2023-02-17 12:49:32,326 DEBUG TRAIN Batch 0/300 loss 437.933990 loss_att 387.884033 loss_ctc 708.073853 loss_rnnt 411.895935 hw_loss 0.055181 lr 0.00001204 rank 6
2023-02-17 12:49:32,327 DEBUG TRAIN Batch 0/300 loss 473.123535 loss_att 420.761292 loss_ctc 741.019836 loss_rnnt 447.805542 hw_loss 0.132969 lr 0.00001204 rank 4
2023-02-17 12:49:32,328 DEBUG TRAIN Batch 0/300 loss 444.895508 loss_att 352.217712 loss_ctc 1085.935547 loss_rnnt 377.847656 hw_loss 0.208784 lr 0.00001204 rank 3
2023-02-17 12:49:32,331 DEBUG TRAIN Batch 0/300 loss 404.760223 loss_att 308.144531 loss_ctc 1047.620850 loss_rnnt 338.264069 hw_loss 0.196032 lr 0.00001204 rank 0
2023-02-17 12:49:32,376 DEBUG TRAIN Batch 0/300 loss 491.679810 loss_att 385.231201 loss_ctc 1230.078613 loss_rnnt 414.421936 hw_loss 0.177039 lr 0.00001204 rank 5
2023-02-17 12:50:47,171 DEBUG TRAIN Batch 0/400 loss 321.051208 loss_att 292.159363 loss_ctc 450.550049 loss_rnnt 309.479767 hw_loss 0.156194 lr 0.00001604 rank 6
2023-02-17 12:50:47,172 DEBUG TRAIN Batch 0/400 loss 361.888641 loss_att 288.224365 loss_ctc 863.399170 loss_rnnt 309.690247 hw_loss 0.118522 lr 0.00001604 rank 4
2023-02-17 12:50:47,173 DEBUG TRAIN Batch 0/400 loss 393.840698 loss_att 319.722076 loss_ctc 907.333008 loss_rnnt 340.084015 hw_loss 0.215124 lr 0.00001604 rank 0
2023-02-17 12:50:47,179 DEBUG TRAIN Batch 0/400 loss 377.453827 loss_att 253.658813 loss_ctc 1404.059082 loss_rnnt 265.253723 hw_loss 0.146988 lr 0.00001604 rank 5
2023-02-17 12:50:47,179 DEBUG TRAIN Batch 0/400 loss 373.797180 loss_att 299.937531 loss_ctc 923.573853 loss_rnnt 315.165924 hw_loss 0.186778 lr 0.00001604 rank 2
2023-02-17 12:50:47,182 DEBUG TRAIN Batch 0/400 loss 295.538757 loss_att 267.557098 loss_ctc 418.402069 loss_rnnt 284.718475 hw_loss 0.065315 lr 0.00001604 rank 1
2023-02-17 12:50:47,183 DEBUG TRAIN Batch 0/400 loss 297.033020 loss_att 267.579468 loss_ctc 416.992920 loss_rnnt 286.858032 hw_loss 0.133175 lr 0.00001604 rank 7
2023-02-17 12:50:47,184 DEBUG TRAIN Batch 0/400 loss 338.160339 loss_att 309.062592 loss_ctc 482.887939 loss_rnnt 324.548615 hw_loss 0.251780 lr 0.00001604 rank 3
2023-02-17 12:52:00,830 DEBUG TRAIN Batch 0/500 loss 311.253754 loss_att 240.043671 loss_ctc 744.721069 loss_rnnt 267.606567 hw_loss 0.175366 lr 0.00002004 rank 2
2023-02-17 12:52:00,833 DEBUG TRAIN Batch 0/500 loss 255.388138 loss_att 221.107376 loss_ctc 350.293488 loss_rnnt 249.461273 hw_loss 0.241811 lr 0.00002004 rank 6
2023-02-17 12:52:00,833 DEBUG TRAIN Batch 0/500 loss 301.444458 loss_att 265.066467 loss_ctc 403.083008 loss_rnnt 295.089844 hw_loss 0.147046 lr 0.00002004 rank 5
2023-02-17 12:52:00,833 DEBUG TRAIN Batch 0/500 loss 292.078033 loss_att 236.149933 loss_ctc 586.193054 loss_rnnt 263.907196 hw_loss 0.264699 lr 0.00002004 rank 7
2023-02-17 12:52:00,834 DEBUG TRAIN Batch 0/500 loss 261.900909 loss_att 229.308228 loss_ctc 346.665588 loss_rnnt 257.031586 hw_loss 0.161077 lr 0.00002004 rank 0
2023-02-17 12:52:00,835 DEBUG TRAIN Batch 0/500 loss 277.722778 loss_att 220.709045 loss_ctc 577.893738 loss_rnnt 248.927444 hw_loss 0.328700 lr 0.00002004 rank 4
2023-02-17 12:52:00,841 DEBUG TRAIN Batch 0/500 loss 305.935272 loss_att 251.670547 loss_ctc 569.813843 loss_rnnt 281.509705 hw_loss 0.177648 lr 0.00002004 rank 3
2023-02-17 12:52:00,842 DEBUG TRAIN Batch 0/500 loss 284.382721 loss_att 235.429260 loss_ctc 511.626770 loss_rnnt 263.801239 hw_loss 0.136806 lr 0.00002004 rank 1
2023-02-17 12:53:14,619 DEBUG TRAIN Batch 0/600 loss 216.740524 loss_att 163.505005 loss_ctc 516.187500 loss_rnnt 187.381561 hw_loss 0.149662 lr 0.00002404 rank 5
2023-02-17 12:53:14,631 DEBUG TRAIN Batch 0/600 loss 118.898941 loss_att 92.229820 loss_ctc 264.881378 loss_rnnt 104.559464 hw_loss 0.391834 lr 0.00002404 rank 2
2023-02-17 12:53:14,637 DEBUG TRAIN Batch 0/600 loss 98.948586 loss_att 69.912491 loss_ctc 296.565247 loss_rnnt 78.286682 hw_loss 0.225428 lr 0.00002404 rank 4
2023-02-17 12:53:14,637 DEBUG TRAIN Batch 0/600 loss 209.025146 loss_att 179.935669 loss_ctc 277.848328 loss_rnnt 205.633316 hw_loss 0.062394 lr 0.00002404 rank 0
2023-02-17 12:53:14,638 DEBUG TRAIN Batch 0/600 loss 257.572205 loss_att 195.712799 loss_ctc 610.291687 loss_rnnt 222.811813 hw_loss 0.193097 lr 0.00002404 rank 6
2023-02-17 12:53:14,638 DEBUG TRAIN Batch 0/600 loss 155.875000 loss_att 122.696190 loss_ctc 327.725098 loss_rnnt 139.483948 hw_loss 0.212763 lr 0.00002404 rank 1
2023-02-17 12:53:14,639 DEBUG TRAIN Batch 0/600 loss 135.919250 loss_att 102.811928 loss_ctc 327.303589 loss_rnnt 116.930244 hw_loss 0.173568 lr 0.00002404 rank 7
2023-02-17 12:53:14,644 DEBUG TRAIN Batch 0/600 loss 215.382812 loss_att 178.283661 loss_ctc 358.443176 loss_rnnt 203.615814 hw_loss 0.210245 lr 0.00002404 rank 3
2023-02-17 12:54:29,883 DEBUG TRAIN Batch 0/700 loss 364.045746 loss_att 314.482666 loss_ctc 456.907715 loss_rnnt 361.574707 hw_loss 0.003902 lr 0.00002804 rank 5
2023-02-17 12:54:29,884 DEBUG TRAIN Batch 0/700 loss 327.316498 loss_att 283.760345 loss_ctc 414.765320 loss_rnnt 324.332703 hw_loss 0.065894 lr 0.00002804 rank 4
2023-02-17 12:54:29,889 DEBUG TRAIN Batch 0/700 loss 307.509827 loss_att 264.944855 loss_ctc 382.413910 loss_rnnt 306.000397 hw_loss 0.066000 lr 0.00002804 rank 0
2023-02-17 12:54:29,891 DEBUG TRAIN Batch 0/700 loss 365.993713 loss_att 317.200684 loss_ctc 436.155823 loss_rnnt 366.377045 hw_loss 0.038170 lr 0.00002804 rank 2
2023-02-17 12:54:29,892 DEBUG TRAIN Batch 0/700 loss 381.610626 loss_att 332.697449 loss_ctc 457.240601 loss_rnnt 381.221527 hw_loss 0.164495 lr 0.00002804 rank 6
2023-02-17 12:54:29,910 DEBUG TRAIN Batch 0/700 loss 323.461914 loss_att 277.062958 loss_ctc 403.920654 loss_rnnt 322.011780 hw_loss 0.003902 lr 0.00002804 rank 3
2023-02-17 12:54:29,915 DEBUG TRAIN Batch 0/700 loss 286.721161 loss_att 205.435654 loss_ctc 782.657959 loss_rnnt 236.851288 hw_loss 0.003901 lr 0.00002804 rank 7
2023-02-17 12:54:29,937 DEBUG TRAIN Batch 0/700 loss 367.020844 loss_att 277.871185 loss_ctc 859.712158 loss_rnnt 319.093750 hw_loss 0.121540 lr 0.00002804 rank 1
2023-02-17 12:55:43,283 DEBUG TRAIN Batch 0/800 loss 361.332153 loss_att 283.850281 loss_ctc 720.755249 loss_rnnt 328.841949 hw_loss 0.118994 lr 0.00003204 rank 4
2023-02-17 12:55:43,285 DEBUG TRAIN Batch 0/800 loss 315.029419 loss_att 270.769104 loss_ctc 406.424255 loss_rnnt 311.657806 hw_loss 0.070739 lr 0.00003204 rank 0
2023-02-17 12:55:43,286 DEBUG TRAIN Batch 0/800 loss 332.412415 loss_att 254.679550 loss_ctc 734.692139 loss_rnnt 294.282379 hw_loss 0.073674 lr 0.00003204 rank 5
2023-02-17 12:55:43,288 DEBUG TRAIN Batch 0/800 loss 396.151733 loss_att 312.422119 loss_ctc 786.947754 loss_rnnt 360.718323 hw_loss 0.137235 lr 0.00003204 rank 2
2023-02-17 12:55:43,290 DEBUG TRAIN Batch 0/800 loss 291.855682 loss_att 252.582214 loss_ctc 354.772095 loss_rnnt 291.225433 hw_loss 0.180221 lr 0.00003204 rank 1
2023-02-17 12:55:43,291 DEBUG TRAIN Batch 0/800 loss 305.820435 loss_att 229.914490 loss_ctc 717.602783 loss_rnnt 265.997040 hw_loss 0.188018 lr 0.00003204 rank 6
2023-02-17 12:55:43,303 DEBUG TRAIN Batch 0/800 loss 374.958038 loss_att 299.304596 loss_ctc 730.646973 loss_rnnt 342.559906 hw_loss 0.194325 lr 0.00003204 rank 3
2023-02-17 12:55:43,340 DEBUG TRAIN Batch 0/800 loss 280.415039 loss_att 243.440735 loss_ctc 333.534149 loss_rnnt 280.600800 hw_loss 0.237275 lr 0.00003204 rank 7
2023-02-17 12:56:56,836 DEBUG TRAIN Batch 0/900 loss 343.991150 loss_att 301.514984 loss_ctc 351.100433 loss_rnnt 351.466980 hw_loss 0.134138 lr 0.00003604 rank 6
2023-02-17 12:56:56,841 DEBUG TRAIN Batch 0/900 loss 290.285217 loss_att 255.725677 loss_ctc 299.475555 loss_rnnt 295.886108 hw_loss 0.160608 lr 0.00003604 rank 1
2023-02-17 12:56:56,842 DEBUG TRAIN Batch 0/900 loss 317.990723 loss_att 279.497803 loss_ctc 327.248901 loss_rnnt 324.274597 hw_loss 0.337949 lr 0.00003604 rank 0
2023-02-17 12:56:56,843 DEBUG TRAIN Batch 0/900 loss 366.547577 loss_att 321.347565 loss_ctc 376.194794 loss_rnnt 374.252991 hw_loss 0.090484 lr 0.00003604 rank 2
2023-02-17 12:56:56,845 DEBUG TRAIN Batch 0/900 loss 376.486084 loss_att 330.905609 loss_ctc 389.285858 loss_rnnt 383.835419 hw_loss 0.112750 lr 0.00003604 rank 4
2023-02-17 12:56:56,845 DEBUG TRAIN Batch 0/900 loss 296.521149 loss_att 260.628845 loss_ctc 305.379822 loss_rnnt 302.413696 hw_loss 0.196358 lr 0.00003604 rank 7
2023-02-17 12:56:56,851 DEBUG TRAIN Batch 0/900 loss 339.723175 loss_att 299.609558 loss_ctc 350.503723 loss_rnnt 346.263062 hw_loss 0.085179 lr 0.00003604 rank 3
2023-02-17 12:56:56,854 DEBUG TRAIN Batch 0/900 loss 337.461853 loss_att 298.218231 loss_ctc 348.280853 loss_rnnt 343.769897 hw_loss 0.184055 lr 0.00003604 rank 5
2023-02-17 12:58:10,441 DEBUG TRAIN Batch 0/1000 loss 297.608582 loss_att 261.889191 loss_ctc 306.067627 loss_rnnt 303.517151 hw_loss 0.201391 lr 0.00004004 rank 2
2023-02-17 12:58:10,444 DEBUG TRAIN Batch 0/1000 loss 358.166626 loss_att 315.970093 loss_ctc 369.166687 loss_rnnt 365.050659 hw_loss 0.166171 lr 0.00004004 rank 3
2023-02-17 12:58:10,447 DEBUG TRAIN Batch 0/1000 loss 340.244568 loss_att 298.679718 loss_ctc 351.175476 loss_rnnt 347.063660 hw_loss 0.068251 lr 0.00004004 rank 4
2023-02-17 12:58:10,449 DEBUG TRAIN Batch 0/1000 loss 293.579315 loss_att 259.369812 loss_ctc 302.699890 loss_rnnt 299.134857 hw_loss 0.131779 lr 0.00004004 rank 6
2023-02-17 12:58:10,449 DEBUG TRAIN Batch 0/1000 loss 282.237000 loss_att 250.390503 loss_ctc 291.597351 loss_rnnt 287.268555 hw_loss 0.168118 lr 0.00004004 rank 0
2023-02-17 12:58:10,455 DEBUG TRAIN Batch 0/1000 loss 301.276611 loss_att 264.864838 loss_ctc 309.543213 loss_rnnt 307.424255 hw_loss 0.060889 lr 0.00004004 rank 1
2023-02-17 12:58:10,488 DEBUG TRAIN Batch 0/1000 loss 350.533112 loss_att 307.625183 loss_ctc 362.142273 loss_rnnt 357.500061 hw_loss 0.125150 lr 0.00004004 rank 7
2023-02-17 12:58:10,501 DEBUG TRAIN Batch 0/1000 loss 318.625305 loss_att 278.546082 loss_ctc 328.828461 loss_rnnt 325.237213 hw_loss 0.081574 lr 0.00004004 rank 5
2023-02-17 12:59:25,717 DEBUG TRAIN Batch 0/1100 loss 282.174988 loss_att 250.678116 loss_ctc 290.940491 loss_rnnt 287.191101 hw_loss 0.214711 lr 0.00004404 rank 4
2023-02-17 12:59:25,723 DEBUG TRAIN Batch 0/1100 loss 279.147919 loss_att 247.958984 loss_ctc 288.244873 loss_rnnt 284.171570 hw_loss 0.002200 lr 0.00004404 rank 1
2023-02-17 12:59:25,724 DEBUG TRAIN Batch 0/1100 loss 302.006409 loss_att 265.414124 loss_ctc 311.845703 loss_rnnt 307.936646 hw_loss 0.143134 lr 0.00004404 rank 2
2023-02-17 12:59:25,727 DEBUG TRAIN Batch 0/1100 loss 300.591339 loss_att 266.711273 loss_ctc 314.421783 loss_rnnt 305.457703 hw_loss 0.123000 lr 0.00004404 rank 6
2023-02-17 12:59:25,730 DEBUG TRAIN Batch 0/1100 loss 258.893066 loss_att 227.749664 loss_ctc 268.128052 loss_rnnt 263.855896 hw_loss 0.064661 lr 0.00004404 rank 7
2023-02-17 12:59:25,730 DEBUG TRAIN Batch 0/1100 loss 280.418335 loss_att 247.804581 loss_ctc 288.510315 loss_rnnt 285.809570 hw_loss 0.098602 lr 0.00004404 rank 0
2023-02-17 12:59:25,738 DEBUG TRAIN Batch 0/1100 loss 276.754059 loss_att 246.143036 loss_ctc 287.265686 loss_rnnt 281.397614 hw_loss 0.144610 lr 0.00004404 rank 5
2023-02-17 12:59:25,773 DEBUG TRAIN Batch 0/1100 loss 286.514313 loss_att 253.277695 loss_ctc 301.230042 loss_rnnt 291.125397 hw_loss 0.139039 lr 0.00004404 rank 3
2023-02-17 13:00:38,305 DEBUG TRAIN Batch 0/1200 loss 235.410522 loss_att 207.949814 loss_ctc 244.452438 loss_rnnt 239.641357 hw_loss 0.104451 lr 0.00004804 rank 4
2023-02-17 13:00:38,310 DEBUG TRAIN Batch 0/1200 loss 273.187653 loss_att 242.597290 loss_ctc 287.078186 loss_rnnt 277.425262 hw_loss 0.053236 lr 0.00004804 rank 3
2023-02-17 13:00:38,313 DEBUG TRAIN Batch 0/1200 loss 224.906967 loss_att 200.339890 loss_ctc 236.466171 loss_rnnt 228.148392 hw_loss 0.245187 lr 0.00004804 rank 0
2023-02-17 13:00:38,315 DEBUG TRAIN Batch 0/1200 loss 194.986633 loss_att 173.522049 loss_ctc 204.192627 loss_rnnt 197.987427 hw_loss 0.121215 lr 0.00004804 rank 1
2023-02-17 13:00:38,315 DEBUG TRAIN Batch 0/1200 loss 236.337402 loss_att 209.838303 loss_ctc 245.907837 loss_rnnt 240.307571 hw_loss 0.100481 lr 0.00004804 rank 6
2023-02-17 13:00:38,316 DEBUG TRAIN Batch 0/1200 loss 172.245590 loss_att 152.825974 loss_ctc 179.491135 loss_rnnt 175.012863 hw_loss 0.282344 lr 0.00004804 rank 7
2023-02-17 13:00:38,316 DEBUG TRAIN Batch 0/1200 loss 274.888367 loss_att 243.073105 loss_ctc 288.534546 loss_rnnt 279.346252 hw_loss 0.160671 lr 0.00004804 rank 5
2023-02-17 13:00:38,317 DEBUG TRAIN Batch 0/1200 loss 151.118561 loss_att 133.843918 loss_ctc 159.708786 loss_rnnt 153.331726 hw_loss 0.180763 lr 0.00004804 rank 2
2023-02-17 13:01:51,725 DEBUG TRAIN Batch 0/1300 loss 375.324829 loss_att 331.775330 loss_ctc 396.300964 loss_rnnt 381.185425 hw_loss 0.098431 lr 0.00005204 rank 3
2023-02-17 13:01:51,740 DEBUG TRAIN Batch 0/1300 loss 312.938110 loss_att 278.888733 loss_ctc 327.611877 loss_rnnt 317.734833 hw_loss 0.106232 lr 0.00005204 rank 2
2023-02-17 13:01:51,741 DEBUG TRAIN Batch 0/1300 loss 76.988174 loss_att 68.538513 loss_ctc 81.371796 loss_rnnt 78.006226 hw_loss 0.163886 lr 0.00005204 rank 6
2023-02-17 13:01:51,746 DEBUG TRAIN Batch 0/1300 loss 326.561615 loss_att 289.072693 loss_ctc 341.144592 loss_rnnt 332.057617 hw_loss 0.107591 lr 0.00005204 rank 4
2023-02-17 13:01:51,747 DEBUG TRAIN Batch 0/1300 loss 153.875046 loss_att 136.124710 loss_ctc 163.706482 loss_rnnt 155.998413 hw_loss 0.217223 lr 0.00005204 rank 5
2023-02-17 13:01:51,747 DEBUG TRAIN Batch 0/1300 loss 372.294312 loss_att 328.154480 loss_ctc 393.612152 loss_rnnt 378.264252 hw_loss 0.029409 lr 0.00005204 rank 0
2023-02-17 13:01:51,748 DEBUG TRAIN Batch 0/1300 loss 303.098267 loss_att 269.996948 loss_ctc 323.120728 loss_rnnt 306.983643 hw_loss 0.122318 lr 0.00005204 rank 1
2023-02-17 13:01:51,795 DEBUG TRAIN Batch 0/1300 loss 316.873077 loss_att 281.202271 loss_ctc 330.256165 loss_rnnt 322.163422 hw_loss 0.111477 lr 0.00005204 rank 7
2023-02-17 13:03:06,023 DEBUG TRAIN Batch 0/1400 loss 310.558228 loss_att 276.000000 loss_ctc 326.861877 loss_rnnt 315.211395 hw_loss 0.158760 lr 0.00005604 rank 0
2023-02-17 13:03:06,027 DEBUG TRAIN Batch 0/1400 loss 342.433167 loss_att 305.525269 loss_ctc 362.370911 loss_rnnt 347.088989 hw_loss 0.126332 lr 0.00005604 rank 5
2023-02-17 13:03:06,032 DEBUG TRAIN Batch 0/1400 loss 302.098297 loss_att 268.506073 loss_ctc 317.931702 loss_rnnt 306.655090 hw_loss 0.094792 lr 0.00005604 rank 4
2023-02-17 13:03:06,035 DEBUG TRAIN Batch 0/1400 loss 280.504242 loss_att 249.219788 loss_ctc 294.632660 loss_rnnt 284.857910 hw_loss 0.036395 lr 0.00005604 rank 3
2023-02-17 13:03:06,035 DEBUG TRAIN Batch 0/1400 loss 297.340302 loss_att 265.815460 loss_ctc 313.581299 loss_rnnt 301.396912 hw_loss 0.155387 lr 0.00005604 rank 1
2023-02-17 13:03:06,037 DEBUG TRAIN Batch 0/1400 loss 344.150116 loss_att 307.907227 loss_ctc 365.985352 loss_rnnt 348.445892 hw_loss 0.077701 lr 0.00005604 rank 2
2023-02-17 13:03:06,047 DEBUG TRAIN Batch 0/1400 loss 316.548523 loss_att 278.845032 loss_ctc 334.128296 loss_rnnt 321.697113 hw_loss 0.090268 lr 0.00005604 rank 7
2023-02-17 13:03:06,088 DEBUG TRAIN Batch 0/1400 loss 352.757019 loss_att 315.386536 loss_ctc 372.804932 loss_rnnt 357.473358 hw_loss 0.158854 lr 0.00005604 rank 6
2023-02-17 13:04:19,582 DEBUG TRAIN Batch 0/1500 loss 275.338074 loss_att 244.509277 loss_ctc 298.247040 loss_rnnt 278.448212 hw_loss 0.002085 lr 0.00006004 rank 4
2023-02-17 13:04:19,587 DEBUG TRAIN Batch 0/1500 loss 329.831329 loss_att 295.992493 loss_ctc 354.208679 loss_rnnt 333.347717 hw_loss 0.002085 lr 0.00006004 rank 6
2023-02-17 13:04:19,590 DEBUG TRAIN Batch 0/1500 loss 323.432556 loss_att 287.486603 loss_ctc 344.702637 loss_rnnt 327.715820 hw_loss 0.131109 lr 0.00006004 rank 5
2023-02-17 13:04:19,591 DEBUG TRAIN Batch 0/1500 loss 304.068634 loss_att 271.424011 loss_ctc 324.281372 loss_rnnt 307.860474 hw_loss 0.078892 lr 0.00006004 rank 2
2023-02-17 13:04:19,592 DEBUG TRAIN Batch 0/1500 loss 284.454437 loss_att 255.872894 loss_ctc 304.831726 loss_rnnt 287.452698 hw_loss 0.002085 lr 0.00006004 rank 1
2023-02-17 13:04:19,593 DEBUG TRAIN Batch 0/1500 loss 322.727203 loss_att 285.180756 loss_ctc 352.645142 loss_rnnt 326.181580 hw_loss 0.123516 lr 0.00006004 rank 0
2023-02-17 13:04:19,598 DEBUG TRAIN Batch 0/1500 loss 289.020630 loss_att 256.812256 loss_ctc 306.054382 loss_rnnt 293.162231 hw_loss 0.054193 lr 0.00006004 rank 3
2023-02-17 13:04:19,637 DEBUG TRAIN Batch 0/1500 loss 288.517548 loss_att 254.776779 loss_ctc 305.464539 loss_rnnt 292.951416 hw_loss 0.102542 lr 0.00006004 rank 7
2023-02-17 13:05:32,266 DEBUG TRAIN Batch 0/1600 loss 319.174622 loss_att 281.202881 loss_ctc 342.407288 loss_rnnt 323.586487 hw_loss 0.159021 lr 0.00006404 rank 6
2023-02-17 13:05:32,268 DEBUG TRAIN Batch 0/1600 loss 300.430908 loss_att 271.035889 loss_ctc 328.453918 loss_rnnt 302.542236 hw_loss 0.058628 lr 0.00006404 rank 5
2023-02-17 13:05:32,271 DEBUG TRAIN Batch 0/1600 loss 264.290192 loss_att 234.361176 loss_ctc 285.907593 loss_rnnt 267.316772 hw_loss 0.144225 lr 0.00006404 rank 0
2023-02-17 13:05:32,271 DEBUG TRAIN Batch 0/1600 loss 295.116882 loss_att 264.798401 loss_ctc 317.816528 loss_rnnt 298.129395 hw_loss 0.046116 lr 0.00006404 rank 7
2023-02-17 13:05:32,271 DEBUG TRAIN Batch 0/1600 loss 294.700653 loss_att 260.482025 loss_ctc 315.243713 loss_rnnt 298.730194 hw_loss 0.140841 lr 0.00006404 rank 3
2023-02-17 13:05:32,272 DEBUG TRAIN Batch 0/1600 loss 295.150909 loss_att 263.194397 loss_ctc 311.967163 loss_rnnt 299.146423 hw_loss 0.288029 lr 0.00006404 rank 2
2023-02-17 13:05:32,273 DEBUG TRAIN Batch 0/1600 loss 316.113556 loss_att 282.849213 loss_ctc 338.005188 loss_rnnt 319.846649 hw_loss 0.001724 lr 0.00006404 rank 1
2023-02-17 13:05:32,273 DEBUG TRAIN Batch 0/1600 loss 278.729523 loss_att 249.024826 loss_ctc 299.142639 loss_rnnt 281.874664 hw_loss 0.138821 lr 0.00006404 rank 4
2023-02-17 13:06:45,083 DEBUG TRAIN Batch 0/1700 loss 270.411011 loss_att 243.149734 loss_ctc 298.192200 loss_rnnt 272.097015 hw_loss 0.116390 lr 0.00006804 rank 2
2023-02-17 13:06:45,088 DEBUG TRAIN Batch 0/1700 loss 309.540771 loss_att 278.042633 loss_ctc 337.608917 loss_rnnt 311.996368 hw_loss 0.190537 lr 0.00006804 rank 5
2023-02-17 13:06:45,089 DEBUG TRAIN Batch 0/1700 loss 271.012085 loss_att 241.142654 loss_ctc 292.177246 loss_rnnt 274.082642 hw_loss 0.152440 lr 0.00006804 rank 0
2023-02-17 13:06:45,092 DEBUG TRAIN Batch 0/1700 loss 331.922546 loss_att 297.140442 loss_ctc 354.534485 loss_rnnt 335.841614 hw_loss 0.042057 lr 0.00006804 rank 4
2023-02-17 13:06:45,094 DEBUG TRAIN Batch 0/1700 loss 264.416931 loss_att 237.050766 loss_ctc 288.920105 loss_rnnt 266.587036 hw_loss 0.067585 lr 0.00006804 rank 7
2023-02-17 13:06:45,096 DEBUG TRAIN Batch 0/1700 loss 292.785126 loss_att 258.688721 loss_ctc 315.925049 loss_rnnt 296.479034 hw_loss 0.075074 lr 0.00006804 rank 6
2023-02-17 13:06:45,100 DEBUG TRAIN Batch 0/1700 loss 246.621719 loss_att 219.224670 loss_ctc 262.036560 loss_rnnt 249.957809 hw_loss 0.165041 lr 0.00006804 rank 1
2023-02-17 13:06:45,147 DEBUG TRAIN Batch 0/1700 loss 290.339478 loss_att 257.381134 loss_ctc 309.218872 loss_rnnt 294.346039 hw_loss 0.127246 lr 0.00006804 rank 3
2023-02-17 13:08:00,222 DEBUG TRAIN Batch 0/1800 loss 281.470154 loss_att 252.254456 loss_ctc 303.719055 loss_rnnt 284.289642 hw_loss 0.107099 lr 0.00007204 rank 3
2023-02-17 13:08:00,230 DEBUG TRAIN Batch 0/1800 loss 229.740875 loss_att 206.446518 loss_ctc 245.884033 loss_rnnt 232.246582 hw_loss 0.001392 lr 0.00007204 rank 6
2023-02-17 13:08:00,233 DEBUG TRAIN Batch 0/1800 loss 249.441467 loss_att 219.919662 loss_ctc 265.099701 loss_rnnt 253.187866 hw_loss 0.131650 lr 0.00007204 rank 4
2023-02-17 13:08:00,237 DEBUG TRAIN Batch 0/1800 loss 266.596558 loss_att 237.655670 loss_ctc 290.790924 loss_rnnt 269.090057 hw_loss 0.128961 lr 0.00007204 rank 0
2023-02-17 13:08:00,237 DEBUG TRAIN Batch 0/1800 loss 214.858444 loss_att 191.926117 loss_ctc 237.162430 loss_rnnt 216.357132 hw_loss 0.213542 lr 0.00007204 rank 7
2023-02-17 13:08:00,238 DEBUG TRAIN Batch 0/1800 loss 222.688904 loss_att 198.644287 loss_ctc 242.586700 loss_rnnt 224.826874 hw_loss 0.033593 lr 0.00007204 rank 5
2023-02-17 13:08:00,239 DEBUG TRAIN Batch 0/1800 loss 258.779877 loss_att 229.162354 loss_ctc 283.918396 loss_rnnt 261.267578 hw_loss 0.157499 lr 0.00007204 rank 2
2023-02-17 13:08:00,285 DEBUG TRAIN Batch 0/1800 loss 141.529327 loss_att 127.498856 loss_ctc 154.291061 loss_rnnt 142.477432 hw_loss 0.293276 lr 0.00007204 rank 1
2023-02-17 13:09:13,156 DEBUG TRAIN Batch 0/1900 loss 133.954605 loss_att 120.126923 loss_ctc 142.308258 loss_rnnt 135.517120 hw_loss 0.167231 lr 0.00007604 rank 2
2023-02-17 13:09:13,166 DEBUG TRAIN Batch 0/1900 loss 224.398712 loss_att 201.636978 loss_ctc 241.132690 loss_rnnt 226.633957 hw_loss 0.161113 lr 0.00007604 rank 5
2023-02-17 13:09:13,170 DEBUG TRAIN Batch 0/1900 loss 105.479904 loss_att 95.353401 loss_ctc 113.368103 loss_rnnt 106.309441 hw_loss 0.270014 lr 0.00007604 rank 6
2023-02-17 13:09:13,170 DEBUG TRAIN Batch 0/1900 loss 119.383904 loss_att 106.087738 loss_ctc 129.134308 loss_rnnt 120.650818 hw_loss 0.172994 lr 0.00007604 rank 4
2023-02-17 13:09:13,172 DEBUG TRAIN Batch 0/1900 loss 352.142639 loss_att 313.953705 loss_ctc 383.219849 loss_rnnt 355.562256 hw_loss 0.139753 lr 0.00007604 rank 1
2023-02-17 13:09:13,173 DEBUG TRAIN Batch 0/1900 loss 98.405746 loss_att 88.399689 loss_ctc 108.426506 loss_rnnt 98.944794 hw_loss 0.236354 lr 0.00007604 rank 0
2023-02-17 13:09:13,174 DEBUG TRAIN Batch 0/1900 loss 159.057846 loss_att 142.372955 loss_ctc 171.854355 loss_rnnt 160.647659 hw_loss 0.076798 lr 0.00007604 rank 7
2023-02-17 13:09:13,214 DEBUG TRAIN Batch 0/1900 loss 178.964386 loss_att 160.062988 loss_ctc 195.971970 loss_rnnt 180.426941 hw_loss 0.093835 lr 0.00007604 rank 3
2023-02-17 13:10:25,941 DEBUG TRAIN Batch 0/2000 loss 353.221527 loss_att 316.211121 loss_ctc 374.666199 loss_rnnt 357.710815 hw_loss 0.100289 lr 0.00008004 rank 4
2023-02-17 13:10:25,947 DEBUG TRAIN Batch 0/2000 loss 321.871002 loss_att 287.734497 loss_ctc 356.574921 loss_rnnt 323.994354 hw_loss 0.143871 lr 0.00008004 rank 0
2023-02-17 13:10:25,948 DEBUG TRAIN Batch 0/2000 loss 242.654739 loss_att 218.968445 loss_ctc 265.750763 loss_rnnt 244.271454 hw_loss 0.077009 lr 0.00008004 rank 7
2023-02-17 13:10:25,948 DEBUG TRAIN Batch 0/2000 loss 319.587402 loss_att 284.100372 loss_ctc 353.229187 loss_rnnt 322.158813 hw_loss 0.075825 lr 0.00008004 rank 2
2023-02-17 13:10:25,949 DEBUG TRAIN Batch 0/2000 loss 332.800842 loss_att 298.791748 loss_ctc 359.186218 loss_rnnt 336.056519 hw_loss 0.052719 lr 0.00008004 rank 1
2023-02-17 13:10:25,950 DEBUG TRAIN Batch 0/2000 loss 345.616180 loss_att 306.422760 loss_ctc 377.882446 loss_rnnt 349.127533 hw_loss 0.047259 lr 0.00008004 rank 5
2023-02-17 13:10:25,950 DEBUG TRAIN Batch 0/2000 loss 303.461365 loss_att 277.164215 loss_ctc 339.904633 loss_rnnt 303.775330 hw_loss 0.161898 lr 0.00008004 rank 6
2023-02-17 13:10:25,950 DEBUG TRAIN Batch 0/2000 loss 291.680298 loss_att 262.361603 loss_ctc 321.905548 loss_rnnt 293.442230 hw_loss 0.134553 lr 0.00008004 rank 3
2023-02-17 13:11:40,045 DEBUG TRAIN Batch 0/2100 loss 346.433167 loss_att 309.973145 loss_ctc 384.041718 loss_rnnt 348.680603 hw_loss 0.056478 lr 0.00008404 rank 2
2023-02-17 13:11:40,050 DEBUG TRAIN Batch 0/2100 loss 282.332062 loss_att 253.073624 loss_ctc 307.240082 loss_rnnt 284.778870 hw_loss 0.157081 lr 0.00008404 rank 0
2023-02-17 13:11:40,054 DEBUG TRAIN Batch 0/2100 loss 305.001434 loss_att 274.041931 loss_ctc 335.758270 loss_rnnt 306.987793 hw_loss 0.196177 lr 0.00008404 rank 4
2023-02-17 13:11:40,055 DEBUG TRAIN Batch 0/2100 loss 307.477325 loss_att 273.352173 loss_ctc 334.020996 loss_rnnt 310.721680 hw_loss 0.077844 lr 0.00008404 rank 6
2023-02-17 13:11:40,056 DEBUG TRAIN Batch 0/2100 loss 284.042206 loss_att 255.774231 loss_ctc 304.495087 loss_rnnt 286.967651 hw_loss 0.002058 lr 0.00008404 rank 7
2023-02-17 13:11:40,059 DEBUG TRAIN Batch 0/2100 loss 296.982727 loss_att 263.050232 loss_ctc 321.974976 loss_rnnt 300.415955 hw_loss 0.039394 lr 0.00008404 rank 3
2023-02-17 13:11:40,077 DEBUG TRAIN Batch 0/2100 loss 316.779236 loss_att 280.599091 loss_ctc 346.809906 loss_rnnt 319.941986 hw_loss 0.129736 lr 0.00008404 rank 5
2023-02-17 13:11:40,104 DEBUG TRAIN Batch 0/2100 loss 309.506165 loss_att 278.684692 loss_ctc 338.899353 loss_rnnt 311.725281 hw_loss 0.048880 lr 0.00008404 rank 1
2023-02-17 13:12:53,400 DEBUG TRAIN Batch 0/2200 loss 254.460281 loss_att 226.111816 loss_ctc 281.120972 loss_rnnt 256.523132 hw_loss 0.097658 lr 0.00008804 rank 2
2023-02-17 13:12:53,401 DEBUG TRAIN Batch 0/2200 loss 321.194061 loss_att 288.971985 loss_ctc 349.940338 loss_rnnt 323.751007 hw_loss 0.102355 lr 0.00008804 rank 4
2023-02-17 13:12:53,402 DEBUG TRAIN Batch 0/2200 loss 284.164886 loss_att 250.323349 loss_ctc 315.905029 loss_rnnt 286.666565 hw_loss 0.064927 lr 0.00008804 rank 0
2023-02-17 13:12:53,408 DEBUG TRAIN Batch 0/2200 loss 292.843994 loss_att 264.080414 loss_ctc 325.684265 loss_rnnt 294.150818 hw_loss 0.125985 lr 0.00008804 rank 3
2023-02-17 13:12:53,407 DEBUG TRAIN Batch 0/2200 loss 326.006897 loss_att 292.739594 loss_ctc 357.655273 loss_rnnt 328.377686 hw_loss 0.117837 lr 0.00008804 rank 1
2023-02-17 13:12:53,408 DEBUG TRAIN Batch 0/2200 loss 250.472107 loss_att 225.851028 loss_ctc 276.073303 loss_rnnt 251.912216 hw_loss 0.132421 lr 0.00008804 rank 6
2023-02-17 13:12:53,414 DEBUG TRAIN Batch 0/2200 loss 319.168793 loss_att 290.564697 loss_ctc 356.032959 loss_rnnt 319.958313 hw_loss 0.030131 lr 0.00008804 rank 7
2023-02-17 13:12:53,456 DEBUG TRAIN Batch 0/2200 loss 263.990173 loss_att 236.824615 loss_ctc 290.102600 loss_rnnt 265.880798 hw_loss 0.114078 lr 0.00008804 rank 5
2023-02-17 13:14:06,224 DEBUG TRAIN Batch 0/2300 loss 317.199890 loss_att 278.661499 loss_ctc 346.958923 loss_rnnt 320.845154 hw_loss 0.177257 lr 0.00009204 rank 7
2023-02-17 13:14:06,231 DEBUG TRAIN Batch 0/2300 loss 293.312714 loss_att 262.253662 loss_ctc 316.006042 loss_rnnt 296.475647 hw_loss 0.043319 lr 0.00009204 rank 4
2023-02-17 13:14:06,233 DEBUG TRAIN Batch 0/2300 loss 276.745667 loss_att 247.438019 loss_ctc 308.556366 loss_rnnt 278.301422 hw_loss 0.120599 lr 0.00009204 rank 6
2023-02-17 13:14:06,237 DEBUG TRAIN Batch 0/2300 loss 223.548370 loss_att 200.896835 loss_ctc 254.531815 loss_rnnt 223.893814 hw_loss 0.100755 lr 0.00009204 rank 2
2023-02-17 13:14:06,239 DEBUG TRAIN Batch 0/2300 loss 271.267303 loss_att 242.390076 loss_ctc 296.500793 loss_rnnt 273.646484 hw_loss 0.059568 lr 0.00009204 rank 0
2023-02-17 13:14:06,241 DEBUG TRAIN Batch 0/2300 loss 301.101501 loss_att 270.988525 loss_ctc 337.707031 loss_rnnt 302.194885 hw_loss 0.090863 lr 0.00009204 rank 3
2023-02-17 13:14:06,241 DEBUG TRAIN Batch 0/2300 loss 198.399780 loss_att 176.290436 loss_ctc 221.039215 loss_rnnt 199.701355 hw_loss 0.190660 lr 0.00009204 rank 1
2023-02-17 13:14:06,249 DEBUG TRAIN Batch 0/2300 loss 351.447113 loss_att 313.324127 loss_ctc 377.479553 loss_rnnt 355.544678 hw_loss 0.105068 lr 0.00009204 rank 5
2023-02-17 13:15:19,202 DEBUG TRAIN Batch 0/2400 loss 275.500244 loss_att 247.925873 loss_ctc 312.399109 loss_rnnt 276.050232 hw_loss 0.084490 lr 0.00009604 rank 7
2023-02-17 13:15:19,212 DEBUG TRAIN Batch 0/2400 loss 259.876862 loss_att 233.067734 loss_ctc 285.196960 loss_rnnt 261.793579 hw_loss 0.129557 lr 0.00009604 rank 5
2023-02-17 13:15:19,213 DEBUG TRAIN Batch 0/2400 loss 285.963440 loss_att 255.580627 loss_ctc 314.481323 loss_rnnt 288.236938 hw_loss 0.001246 lr 0.00009604 rank 4
2023-02-17 13:15:19,213 DEBUG TRAIN Batch 0/2400 loss 228.993393 loss_att 201.099823 loss_ctc 248.677109 loss_rnnt 231.897461 hw_loss 0.094030 lr 0.00009604 rank 1
2023-02-17 13:15:19,214 DEBUG TRAIN Batch 0/2400 loss 289.461151 loss_att 259.923370 loss_ctc 322.076599 loss_rnnt 290.984985 hw_loss 0.065587 lr 0.00009604 rank 6
2023-02-17 13:15:19,216 DEBUG TRAIN Batch 0/2400 loss 231.288635 loss_att 207.184006 loss_ctc 257.660217 loss_rnnt 232.528107 hw_loss 0.122297 lr 0.00009604 rank 3
2023-02-17 13:15:19,216 DEBUG TRAIN Batch 0/2400 loss 284.482483 loss_att 252.630585 loss_ctc 313.647034 loss_rnnt 286.935364 hw_loss 0.054175 lr 0.00009604 rank 0
2023-02-17 13:15:19,218 DEBUG TRAIN Batch 0/2400 loss 290.469849 loss_att 262.534241 loss_ctc 321.689270 loss_rnnt 291.808899 hw_loss 0.160287 lr 0.00009604 rank 2
2023-02-17 13:16:34,244 DEBUG TRAIN Batch 0/2500 loss 228.567520 loss_att 204.484451 loss_ctc 252.364746 loss_rnnt 230.175980 hw_loss 0.065947 lr 0.00010004 rank 7
2023-02-17 13:16:34,251 DEBUG TRAIN Batch 0/2500 loss 105.299423 loss_att 95.262817 loss_ctc 116.655373 loss_rnnt 105.702225 hw_loss 0.169485 lr 0.00010004 rank 6
2023-02-17 13:16:34,251 DEBUG TRAIN Batch 0/2500 loss 223.791367 loss_att 200.230759 loss_ctc 249.227493 loss_rnnt 225.046341 hw_loss 0.123092 lr 0.00010004 rank 4
2023-02-17 13:16:34,251 DEBUG TRAIN Batch 0/2500 loss 218.370056 loss_att 196.910553 loss_ctc 248.019928 loss_rnnt 218.614365 hw_loss 0.176769 lr 0.00010004 rank 3
2023-02-17 13:16:34,254 DEBUG TRAIN Batch 0/2500 loss 113.516426 loss_att 102.108124 loss_ctc 126.368980 loss_rnnt 113.969727 hw_loss 0.215007 lr 0.00010004 rank 0
2023-02-17 13:16:34,254 DEBUG TRAIN Batch 0/2500 loss 319.676849 loss_att 284.506378 loss_ctc 362.828339 loss_rnnt 320.829956 hw_loss 0.238968 lr 0.00010004 rank 1
2023-02-17 13:16:34,274 DEBUG TRAIN Batch 0/2500 loss 144.155090 loss_att 129.148407 loss_ctc 154.075836 loss_rnnt 145.774536 hw_loss 0.110863 lr 0.00010004 rank 2
2023-02-17 13:16:34,306 DEBUG TRAIN Batch 0/2500 loss 246.188507 loss_att 217.723297 loss_ctc 272.361145 loss_rnnt 248.391449 hw_loss 0.000806 lr 0.00010004 rank 5
2023-02-17 13:17:47,281 DEBUG TRAIN Batch 0/2600 loss 327.537903 loss_att 292.164459 loss_ctc 359.645630 loss_rnnt 330.315155 hw_loss 0.030741 lr 0.00010404 rank 4
2023-02-17 13:17:47,284 DEBUG TRAIN Batch 0/2600 loss 304.245575 loss_att 274.421539 loss_ctc 333.064331 loss_rnnt 306.303406 hw_loss 0.120865 lr 0.00010404 rank 2
2023-02-17 13:17:47,286 DEBUG TRAIN Batch 0/2600 loss 349.074310 loss_att 311.464081 loss_ctc 385.573456 loss_rnnt 351.691986 hw_loss 0.070961 lr 0.00010404 rank 6
2023-02-17 13:17:47,288 DEBUG TRAIN Batch 0/2600 loss 277.454437 loss_att 245.191589 loss_ctc 299.076721 loss_rnnt 280.925385 hw_loss 0.185024 lr 0.00010404 rank 0
2023-02-17 13:17:47,288 DEBUG TRAIN Batch 0/2600 loss 51.324013 loss_att 48.204483 loss_ctc 55.426735 loss_rnnt 51.318138 hw_loss 0.155164 lr 0.00010404 rank 5
2023-02-17 13:17:47,290 DEBUG TRAIN Batch 0/2600 loss 256.743988 loss_att 229.849777 loss_ctc 288.244263 loss_rnnt 257.856781 hw_loss 0.123764 lr 0.00010404 rank 1
2023-02-17 13:17:47,293 DEBUG TRAIN Batch 0/2600 loss 360.406586 loss_att 319.822327 loss_ctc 391.330261 loss_rnnt 364.383759 hw_loss 0.030958 lr 0.00010404 rank 3
2023-02-17 13:17:47,338 DEBUG TRAIN Batch 0/2600 loss 65.136475 loss_att 59.663628 loss_ctc 72.949806 loss_rnnt 65.031769 hw_loss 0.295318 lr 0.00010404 rank 7
2023-02-17 13:19:00,888 DEBUG TRAIN Batch 0/2700 loss 291.525543 loss_att 263.604797 loss_ctc 320.672119 loss_rnnt 293.187469 hw_loss 0.067517 lr 0.00010804 rank 5
2023-02-17 13:19:00,891 DEBUG TRAIN Batch 0/2700 loss 267.651459 loss_att 239.458893 loss_ctc 295.826416 loss_rnnt 269.459686 hw_loss 0.138073 lr 0.00010804 rank 4
2023-02-17 13:19:00,902 DEBUG TRAIN Batch 0/2700 loss 326.186493 loss_att 293.228088 loss_ctc 359.084717 loss_rnnt 328.368713 hw_loss 0.043162 lr 0.00010804 rank 6
2023-02-17 13:19:00,907 DEBUG TRAIN Batch 0/2700 loss 271.939178 loss_att 247.787369 loss_ctc 301.098694 loss_rnnt 272.785980 hw_loss 0.179391 lr 0.00010804 rank 2
2023-02-17 13:19:00,908 DEBUG TRAIN Batch 0/2700 loss 344.545227 loss_att 311.450623 loss_ctc 381.955872 loss_rnnt 346.175629 hw_loss 0.000872 lr 0.00010804 rank 0
2023-02-17 13:19:00,910 DEBUG TRAIN Batch 0/2700 loss 278.942902 loss_att 250.840820 loss_ctc 309.963470 loss_rnnt 280.393005 hw_loss 0.064162 lr 0.00010804 rank 1
2023-02-17 13:19:00,914 DEBUG TRAIN Batch 0/2700 loss 286.795685 loss_att 256.025024 loss_ctc 318.996460 loss_rnnt 288.647308 hw_loss 0.017049 lr 0.00010804 rank 7
2023-02-17 13:19:00,964 DEBUG TRAIN Batch 0/2700 loss 301.115631 loss_att 275.168854 loss_ctc 330.375092 loss_rnnt 302.338226 hw_loss 0.122859 lr 0.00010804 rank 3
2023-02-17 13:20:15,846 DEBUG TRAIN Batch 0/2800 loss 253.798004 loss_att 232.150726 loss_ctc 277.732056 loss_rnnt 254.852097 hw_loss 0.157790 lr 0.00011204 rank 7
2023-02-17 13:20:15,849 DEBUG TRAIN Batch 0/2800 loss 260.766907 loss_att 236.058258 loss_ctc 282.246643 loss_rnnt 262.823090 hw_loss 0.040434 lr 0.00011204 rank 4
2023-02-17 13:20:15,851 DEBUG TRAIN Batch 0/2800 loss 289.798370 loss_att 259.010101 loss_ctc 313.468018 loss_rnnt 292.794067 hw_loss 0.011225 lr 0.00011204 rank 6
2023-02-17 13:20:15,854 DEBUG TRAIN Batch 0/2800 loss 284.662262 loss_att 258.890625 loss_ctc 325.558136 loss_rnnt 284.313416 hw_loss 0.094420 lr 0.00011204 rank 1
2023-02-17 13:20:15,857 DEBUG TRAIN Batch 0/2800 loss 266.229370 loss_att 242.088913 loss_ctc 297.802612 loss_rnnt 266.811646 hw_loss 0.067601 lr 0.00011204 rank 2
2023-02-17 13:20:15,859 DEBUG TRAIN Batch 0/2800 loss 275.621857 loss_att 252.772461 loss_ctc 308.203217 loss_rnnt 275.794128 hw_loss 0.100202 lr 0.00011204 rank 5
2023-02-17 13:20:15,880 DEBUG TRAIN Batch 0/2800 loss 256.399292 loss_att 235.340332 loss_ctc 285.749878 loss_rnnt 256.647644 hw_loss 0.093843 lr 0.00011204 rank 3
2023-02-17 13:20:15,885 DEBUG TRAIN Batch 0/2800 loss 314.874603 loss_att 284.323669 loss_ctc 342.689697 loss_rnnt 317.206329 hw_loss 0.130782 lr 0.00011204 rank 0
2023-02-17 13:21:29,259 DEBUG TRAIN Batch 0/2900 loss 271.995453 loss_att 246.495972 loss_ctc 295.921539 loss_rnnt 273.788513 hw_loss 0.218830 lr 0.00011604 rank 2
2023-02-17 13:21:29,274 DEBUG TRAIN Batch 0/2900 loss 243.254715 loss_att 222.169189 loss_ctc 271.704742 loss_rnnt 243.651276 hw_loss 0.051007 lr 0.00011604 rank 0
2023-02-17 13:21:29,275 DEBUG TRAIN Batch 0/2900 loss 256.774567 loss_att 231.473175 loss_ctc 280.411560 loss_rnnt 258.602722 hw_loss 0.150967 lr 0.00011604 rank 4
2023-02-17 13:21:29,276 DEBUG TRAIN Batch 0/2900 loss 289.414978 loss_att 262.808533 loss_ctc 319.220184 loss_rnnt 290.730408 hw_loss 0.059678 lr 0.00011604 rank 6
2023-02-17 13:21:29,277 DEBUG TRAIN Batch 0/2900 loss 253.354828 loss_att 232.073257 loss_ctc 270.066803 loss_rnnt 255.332855 hw_loss 0.093821 lr 0.00011604 rank 7
2023-02-17 13:21:29,281 DEBUG TRAIN Batch 0/2900 loss 290.886078 loss_att 264.719635 loss_ctc 316.032288 loss_rnnt 292.708984 hw_loss 0.107934 lr 0.00011604 rank 3
2023-02-17 13:21:29,281 DEBUG TRAIN Batch 0/2900 loss 320.107941 loss_att 286.549866 loss_ctc 351.377136 loss_rnnt 322.584106 hw_loss 0.124106 lr 0.00011604 rank 5
2023-02-17 13:21:29,324 DEBUG TRAIN Batch 0/2900 loss 231.225708 loss_att 212.066238 loss_ctc 253.183228 loss_rnnt 232.055725 hw_loss 0.139120 lr 0.00011604 rank 1
2023-02-17 13:22:42,117 DEBUG TRAIN Batch 0/3000 loss 260.599152 loss_att 242.208359 loss_ctc 292.953552 loss_rnnt 259.916626 hw_loss 0.087631 lr 0.00012004 rank 5
2023-02-17 13:22:42,118 DEBUG TRAIN Batch 0/3000 loss 247.475113 loss_att 223.588531 loss_ctc 268.813477 loss_rnnt 249.345001 hw_loss 0.116788 lr 0.00012004 rank 4
2023-02-17 13:22:42,118 DEBUG TRAIN Batch 0/3000 loss 249.000641 loss_att 230.608643 loss_ctc 276.102966 loss_rnnt 249.008118 hw_loss 0.107428 lr 0.00012004 rank 6
2023-02-17 13:22:42,120 DEBUG TRAIN Batch 0/3000 loss 243.643387 loss_att 228.289062 loss_ctc 277.020691 loss_rnnt 242.243408 hw_loss 0.038499 lr 0.00012004 rank 3
2023-02-17 13:22:42,123 DEBUG TRAIN Batch 0/3000 loss 245.260788 loss_att 228.028610 loss_ctc 271.735901 loss_rnnt 245.144287 hw_loss 0.061768 lr 0.00012004 rank 0
2023-02-17 13:22:42,123 DEBUG TRAIN Batch 0/3000 loss 218.833893 loss_att 200.286438 loss_ctc 244.350052 loss_rnnt 219.034897 hw_loss 0.199361 lr 0.00012004 rank 1
2023-02-17 13:22:42,126 DEBUG TRAIN Batch 0/3000 loss 267.218781 loss_att 242.194962 loss_ctc 286.617432 loss_rnnt 269.631775 hw_loss 0.009935 lr 0.00012004 rank 7
2023-02-17 13:22:42,175 DEBUG TRAIN Batch 0/3000 loss 245.011322 loss_att 224.724243 loss_ctc 268.300964 loss_rnnt 245.905167 hw_loss 0.109306 lr 0.00012004 rank 2
2023-02-17 13:23:55,462 DEBUG TRAIN Batch 0/3100 loss 230.316895 loss_att 214.838120 loss_ctc 246.279892 loss_rnnt 231.256073 hw_loss 0.052835 lr 0.00012404 rank 7
2023-02-17 13:23:55,464 DEBUG TRAIN Batch 0/3100 loss 225.783127 loss_att 205.663361 loss_ctc 243.582184 loss_rnnt 227.354172 hw_loss 0.149457 lr 0.00012404 rank 3
2023-02-17 13:23:55,468 DEBUG TRAIN Batch 0/3100 loss 254.335327 loss_att 229.953522 loss_ctc 282.390717 loss_rnnt 255.414246 hw_loss 0.106345 lr 0.00012404 rank 4
2023-02-17 13:23:55,469 DEBUG TRAIN Batch 0/3100 loss 139.320663 loss_att 131.793167 loss_ctc 151.033356 loss_rnnt 139.134338 hw_loss 0.243975 lr 0.00012404 rank 6
2023-02-17 13:23:55,473 DEBUG TRAIN Batch 0/3100 loss 283.989197 loss_att 263.452393 loss_ctc 304.494141 loss_rnnt 285.293762 hw_loss 0.129014 lr 0.00012404 rank 1
2023-02-17 13:23:55,475 DEBUG TRAIN Batch 0/3100 loss 220.446762 loss_att 204.363266 loss_ctc 239.413071 loss_rnnt 221.053024 hw_loss 0.152959 lr 0.00012404 rank 5
2023-02-17 13:23:55,476 DEBUG TRAIN Batch 0/3100 loss 166.783539 loss_att 153.709152 loss_ctc 179.155685 loss_rnnt 167.671616 hw_loss 0.144749 lr 0.00012404 rank 0
2023-02-17 13:23:55,480 DEBUG TRAIN Batch 0/3100 loss 202.443008 loss_att 187.565231 loss_ctc 225.655502 loss_rnnt 202.229599 hw_loss 0.176216 lr 0.00012404 rank 2
2023-02-17 13:25:11,284 DEBUG TRAIN Batch 0/3200 loss 234.087479 loss_att 222.214447 loss_ctc 254.968262 loss_rnnt 233.644501 hw_loss 0.062796 lr 0.00012804 rank 1
2023-02-17 13:25:11,289 DEBUG TRAIN Batch 0/3200 loss 63.024471 loss_att 60.752007 loss_ctc 69.702820 loss_rnnt 62.487007 hw_loss 0.190327 lr 0.00012804 rank 4
2023-02-17 13:25:11,289 DEBUG TRAIN Batch 0/3200 loss 267.831421 loss_att 251.119720 loss_ctc 290.377167 loss_rnnt 268.104431 hw_loss 0.118589 lr 0.00012804 rank 6
2023-02-17 13:25:11,295 DEBUG TRAIN Batch 0/3200 loss 110.535065 loss_att 104.940628 loss_ctc 119.695534 loss_rnnt 110.309296 hw_loss 0.231115 lr 0.00012804 rank 5
2023-02-17 13:25:11,295 DEBUG TRAIN Batch 0/3200 loss 265.280823 loss_att 251.041077 loss_ctc 286.915009 loss_rnnt 265.178497 hw_loss 0.123179 lr 0.00012804 rank 0
2023-02-17 13:25:11,297 DEBUG TRAIN Batch 0/3200 loss 257.263794 loss_att 241.996552 loss_ctc 287.697021 loss_rnnt 256.201080 hw_loss 0.109487 lr 0.00012804 rank 2
2023-02-17 13:25:11,298 DEBUG TRAIN Batch 0/3200 loss 104.074799 loss_att 97.673790 loss_ctc 112.683464 loss_rnnt 104.114990 hw_loss 0.172845 lr 0.00012804 rank 3
2023-02-17 13:25:11,308 DEBUG TRAIN Batch 0/3200 loss 143.325119 loss_att 135.425781 loss_ctc 156.412521 loss_rnnt 143.070984 hw_loss 0.166882 lr 0.00012804 rank 7
2023-02-17 13:26:23,129 DEBUG TRAIN Batch 0/3300 loss 250.804123 loss_att 241.056946 loss_ctc 262.498383 loss_rnnt 251.121552 hw_loss 0.136495 lr 0.00013204 rank 1
2023-02-17 13:26:23,132 DEBUG TRAIN Batch 0/3300 loss 278.802429 loss_att 266.145477 loss_ctc 308.772095 loss_rnnt 277.337677 hw_loss 0.000273 lr 0.00013204 rank 4
2023-02-17 13:26:23,134 DEBUG TRAIN Batch 0/3300 loss 312.947937 loss_att 294.593231 loss_ctc 334.982880 loss_rnnt 313.617493 hw_loss 0.118858 lr 0.00013204 rank 7
2023-02-17 13:26:23,136 DEBUG TRAIN Batch 0/3300 loss 314.292816 loss_att 295.852539 loss_ctc 339.898102 loss_rnnt 314.563354 hw_loss 0.006516 lr 0.00013204 rank 5
2023-02-17 13:26:23,137 DEBUG TRAIN Batch 0/3300 loss 260.308868 loss_att 248.597351 loss_ctc 277.986328 loss_rnnt 260.274597 hw_loss 0.036726 lr 0.00013204 rank 2
2023-02-17 13:26:23,137 DEBUG TRAIN Batch 0/3300 loss 300.439972 loss_att 288.495789 loss_ctc 322.692200 loss_rnnt 299.821259 hw_loss 0.076044 lr 0.00013204 rank 6
2023-02-17 13:26:23,140 DEBUG TRAIN Batch 0/3300 loss 257.407349 loss_att 251.478851 loss_ctc 274.542023 loss_rnnt 256.236877 hw_loss 0.134111 lr 0.00013204 rank 0
2023-02-17 13:26:23,142 DEBUG TRAIN Batch 0/3300 loss 296.354858 loss_att 283.289429 loss_ctc 320.569641 loss_rnnt 295.724945 hw_loss 0.026922 lr 0.00013204 rank 3
2023-02-17 13:27:36,560 DEBUG TRAIN Batch 0/3400 loss 250.180588 loss_att 242.326950 loss_ctc 258.779907 loss_rnnt 250.560593 hw_loss 0.082778 lr 0.00013604 rank 4
2023-02-17 13:27:36,562 DEBUG TRAIN Batch 0/3400 loss 279.006042 loss_att 278.546082 loss_ctc 300.549805 loss_rnnt 276.182068 hw_loss 0.081475 lr 0.00013604 rank 7
2023-02-17 13:27:36,562 DEBUG TRAIN Batch 0/3400 loss 365.473267 loss_att 352.894562 loss_ctc 399.688324 loss_rnnt 363.382324 hw_loss 0.083740 lr 0.00013604 rank 5
2023-02-17 13:27:36,563 DEBUG TRAIN Batch 0/3400 loss 250.965195 loss_att 251.938751 loss_ctc 267.415955 loss_rnnt 248.502197 hw_loss 0.140361 lr 0.00013604 rank 3
2023-02-17 13:27:36,564 DEBUG TRAIN Batch 0/3400 loss 253.852982 loss_att 246.634888 loss_ctc 267.304413 loss_rnnt 253.457703 hw_loss 0.085078 lr 0.00013604 rank 0
2023-02-17 13:27:36,564 DEBUG TRAIN Batch 0/3400 loss 277.008301 loss_att 269.385376 loss_ctc 292.786987 loss_rnnt 276.362061 hw_loss 0.125551 lr 0.00013604 rank 6
2023-02-17 13:27:36,566 DEBUG TRAIN Batch 0/3400 loss 237.845108 loss_att 234.665176 loss_ctc 264.346191 loss_rnnt 234.926422 hw_loss 0.039714 lr 0.00013604 rank 2
2023-02-17 13:27:36,615 DEBUG TRAIN Batch 0/3400 loss 285.464661 loss_att 279.801147 loss_ctc 304.097046 loss_rnnt 284.054077 hw_loss 0.110548 lr 0.00013604 rank 1
2023-02-17 13:28:50,301 DEBUG TRAIN Batch 0/3500 loss 217.580734 loss_att 216.636841 loss_ctc 234.867126 loss_rnnt 215.437347 hw_loss 0.051234 lr 0.00014004 rank 2
2023-02-17 13:28:50,313 DEBUG TRAIN Batch 0/3500 loss 238.234329 loss_att 235.954468 loss_ctc 244.444183 loss_rnnt 237.835938 hw_loss 0.049489 lr 0.00014004 rank 7
2023-02-17 13:28:50,314 DEBUG TRAIN Batch 0/3500 loss 225.643982 loss_att 224.723969 loss_ctc 232.447739 loss_rnnt 224.893311 hw_loss 0.051579 lr 0.00014004 rank 6
2023-02-17 13:28:50,315 DEBUG TRAIN Batch 0/3500 loss 245.376404 loss_att 245.823944 loss_ctc 255.678101 loss_rnnt 243.891724 hw_loss 0.040501 lr 0.00014004 rank 1
2023-02-17 13:28:50,315 DEBUG TRAIN Batch 0/3500 loss 271.021271 loss_att 269.654388 loss_ctc 287.126221 loss_rnnt 269.132812 hw_loss 0.027265 lr 0.00014004 rank 3
2023-02-17 13:28:50,318 DEBUG TRAIN Batch 0/3500 loss 221.066055 loss_att 222.666718 loss_ctc 227.232681 loss_rnnt 219.854752 hw_loss 0.129286 lr 0.00014004 rank 0
2023-02-17 13:28:50,326 DEBUG TRAIN Batch 0/3500 loss 260.433929 loss_att 251.194122 loss_ctc 274.413147 loss_rnnt 260.380646 hw_loss 0.070033 lr 0.00014004 rank 4
2023-02-17 13:28:50,329 DEBUG TRAIN Batch 0/3500 loss 266.299652 loss_att 269.605225 loss_ctc 276.124146 loss_rnnt 264.243958 hw_loss 0.158656 lr 0.00014004 rank 5
2023-02-17 13:30:04,937 DEBUG TRAIN Batch 0/3600 loss 192.456696 loss_att 195.191193 loss_ctc 201.100906 loss_rnnt 190.708191 hw_loss 0.091947 lr 0.00014404 rank 4
2023-02-17 13:30:04,940 DEBUG TRAIN Batch 0/3600 loss 263.450439 loss_att 266.603821 loss_ctc 268.082092 loss_rnnt 262.135315 hw_loss 0.125457 lr 0.00014404 rank 5
2023-02-17 13:30:04,940 DEBUG TRAIN Batch 0/3600 loss 247.128296 loss_att 254.190887 loss_ctc 261.264832 loss_rnnt 243.830826 hw_loss 0.000161 lr 0.00014404 rank 6
2023-02-17 13:30:04,943 DEBUG TRAIN Batch 0/3600 loss 283.363708 loss_att 283.094177 loss_ctc 295.868530 loss_rnnt 281.719238 hw_loss 0.058287 lr 0.00014404 rank 7
2023-02-17 13:30:04,946 DEBUG TRAIN Batch 0/3600 loss 180.491135 loss_att 188.839783 loss_ctc 187.461227 loss_rnnt 177.789444 hw_loss 0.192369 lr 0.00014404 rank 1
2023-02-17 13:30:04,946 DEBUG TRAIN Batch 0/3600 loss 234.538635 loss_att 239.862579 loss_ctc 239.220520 loss_rnnt 232.827393 hw_loss 0.041626 lr 0.00014404 rank 3
2023-02-17 13:30:04,948 DEBUG TRAIN Batch 0/3600 loss 191.786240 loss_att 199.443756 loss_ctc 197.699081 loss_rnnt 189.466278 hw_loss 0.000161 lr 0.00014404 rank 2
2023-02-17 13:30:04,949 DEBUG TRAIN Batch 0/3600 loss 248.405121 loss_att 243.753143 loss_ctc 257.445587 loss_rnnt 248.107239 hw_loss 0.042905 lr 0.00014404 rank 0
2023-02-17 13:31:19,965 DEBUG TRAIN Batch 0/3700 loss 204.426453 loss_att 203.453232 loss_ctc 220.985016 loss_rnnt 202.359177 hw_loss 0.101480 lr 0.00014804 rank 0
2023-02-17 13:31:19,967 DEBUG TRAIN Batch 0/3700 loss 147.945023 loss_att 151.396423 loss_ctc 156.345627 loss_rnnt 146.089478 hw_loss 0.084721 lr 0.00014804 rank 6
2023-02-17 13:31:19,967 DEBUG TRAIN Batch 0/3700 loss 66.781212 loss_att 68.084106 loss_ctc 70.462440 loss_rnnt 65.871872 hw_loss 0.296113 lr 0.00014804 rank 1
2023-02-17 13:31:19,968 DEBUG TRAIN Batch 0/3700 loss 217.605865 loss_att 229.785187 loss_ctc 218.491501 loss_rnnt 214.984863 hw_loss 0.125737 lr 0.00014804 rank 4
2023-02-17 13:31:19,970 DEBUG TRAIN Batch 0/3700 loss 141.281815 loss_att 149.682526 loss_ctc 144.315277 loss_rnnt 139.106232 hw_loss 0.170612 lr 0.00014804 rank 2
2023-02-17 13:31:19,974 DEBUG TRAIN Batch 0/3700 loss 202.787933 loss_att 210.625183 loss_ctc 196.030273 loss_rnnt 202.044556 hw_loss 0.144285 lr 0.00014804 rank 7
2023-02-17 13:31:19,974 DEBUG TRAIN Batch 0/3700 loss 193.922928 loss_att 203.323654 loss_ctc 200.893463 loss_rnnt 191.011154 hw_loss 0.191681 lr 0.00014804 rank 3
2023-02-17 13:31:20,017 DEBUG TRAIN Batch 0/3700 loss 232.101608 loss_att 239.721268 loss_ctc 245.134201 loss_rnnt 228.799149 hw_loss 0.076563 lr 0.00014804 rank 5
2023-02-17 13:32:33,247 DEBUG TRAIN Batch 0/3800 loss 149.832977 loss_att 162.509766 loss_ctc 147.993683 loss_rnnt 147.437103 hw_loss 0.198301 lr 0.00015204 rank 4
2023-02-17 13:32:33,250 DEBUG TRAIN Batch 0/3800 loss 258.071838 loss_att 270.561157 loss_ctc 263.051483 loss_rnnt 254.910004 hw_loss 0.000110 lr 0.00015204 rank 0
2023-02-17 13:32:33,251 DEBUG TRAIN Batch 0/3800 loss 223.552277 loss_att 237.797958 loss_ctc 217.359421 loss_rnnt 221.486877 hw_loss 0.078689 lr 0.00015204 rank 1
2023-02-17 13:32:33,251 DEBUG TRAIN Batch 0/3800 loss 280.673767 loss_att 302.450043 loss_ctc 290.915131 loss_rnnt 274.929688 hw_loss 0.043718 lr 0.00015204 rank 2
2023-02-17 13:32:33,253 DEBUG TRAIN Batch 0/3800 loss 143.966080 loss_att 152.528564 loss_ctc 145.129974 loss_rnnt 141.976273 hw_loss 0.228976 lr 0.00015204 rank 7
2023-02-17 13:32:33,255 DEBUG TRAIN Batch 0/3800 loss 349.561676 loss_att 368.941864 loss_ctc 361.144653 loss_rnnt 344.141174 hw_loss 0.000110 lr 0.00015204 rank 5
2023-02-17 13:32:33,256 DEBUG TRAIN Batch 0/3800 loss 246.406479 loss_att 265.464203 loss_ctc 243.523499 loss_rnnt 242.888412 hw_loss 0.170493 lr 0.00015204 rank 6
2023-02-17 13:32:33,258 DEBUG TRAIN Batch 0/3800 loss 152.392822 loss_att 166.476227 loss_ctc 153.934113 loss_rnnt 149.289886 hw_loss 0.151400 lr 0.00015204 rank 3
2023-02-17 13:33:48,717 DEBUG TRAIN Batch 0/3900 loss 254.543167 loss_att 286.242432 loss_ctc 256.261963 loss_rnnt 247.945282 hw_loss 0.054141 lr 0.00015604 rank 4
2023-02-17 13:33:48,720 DEBUG TRAIN Batch 0/3900 loss 250.192642 loss_att 283.160370 loss_ctc 253.850555 loss_rnnt 243.090179 hw_loss 0.039753 lr 0.00015604 rank 6
2023-02-17 13:33:48,722 DEBUG TRAIN Batch 0/3900 loss 257.959015 loss_att 284.429535 loss_ctc 257.128479 loss_rnnt 252.683090 hw_loss 0.173604 lr 0.00015604 rank 3
2023-02-17 13:33:48,722 DEBUG TRAIN Batch 0/3900 loss 176.967529 loss_att 206.269394 loss_ctc 184.665268 loss_rnnt 170.028076 hw_loss 0.098840 lr 0.00015604 rank 2
2023-02-17 13:33:48,723 DEBUG TRAIN Batch 0/3900 loss 206.128448 loss_att 237.712952 loss_ctc 200.531342 loss_rnnt 200.557709 hw_loss 0.000215 lr 0.00015604 rank 7
2023-02-17 13:33:48,723 DEBUG TRAIN Batch 0/3900 loss 255.550842 loss_att 272.273865 loss_ctc 263.750580 loss_rnnt 251.038498 hw_loss 0.139581 lr 0.00015604 rank 0
2023-02-17 13:33:48,735 DEBUG TRAIN Batch 0/3900 loss 261.125061 loss_att 278.100952 loss_ctc 269.048767 loss_rnnt 256.639160 hw_loss 0.064180 lr 0.00015604 rank 5
2023-02-17 13:33:48,768 DEBUG TRAIN Batch 0/3900 loss 229.418961 loss_att 258.303345 loss_ctc 226.005417 loss_rnnt 224.021698 hw_loss 0.141627 lr 0.00015604 rank 1
2023-02-17 13:35:01,421 DEBUG TRAIN Batch 0/4000 loss 233.348053 loss_att 272.442810 loss_ctc 232.019699 loss_rnnt 225.680099 hw_loss 0.048979 lr 0.00016004 rank 6
2023-02-17 13:35:01,425 DEBUG TRAIN Batch 0/4000 loss 263.876801 loss_att 292.559753 loss_ctc 250.414398 loss_rnnt 259.901123 hw_loss 0.063848 lr 0.00016004 rank 7
2023-02-17 13:35:01,425 DEBUG TRAIN Batch 0/4000 loss 302.295441 loss_att 317.729675 loss_ctc 299.392883 loss_rnnt 299.497345 hw_loss 0.184238 lr 0.00016004 rank 0
2023-02-17 13:35:01,426 DEBUG TRAIN Batch 0/4000 loss 254.322235 loss_att 295.311188 loss_ctc 248.965057 loss_rnnt 246.792526 hw_loss 0.086669 lr 0.00016004 rank 4
2023-02-17 13:35:01,430 DEBUG TRAIN Batch 0/4000 loss 199.595535 loss_att 233.581512 loss_ctc 193.374756 loss_rnnt 193.602448 hw_loss 0.047452 lr 0.00016004 rank 2
2023-02-17 13:35:01,431 DEBUG TRAIN Batch 0/4000 loss 208.444244 loss_att 244.493393 loss_ctc 207.863770 loss_rnnt 201.286713 hw_loss 0.047071 lr 0.00016004 rank 5
2023-02-17 13:35:01,433 DEBUG TRAIN Batch 0/4000 loss 202.772018 loss_att 231.097443 loss_ctc 204.758957 loss_rnnt 196.732574 hw_loss 0.205175 lr 0.00016004 rank 1
2023-02-17 13:35:01,435 DEBUG TRAIN Batch 0/4000 loss 205.551361 loss_att 219.468246 loss_ctc 213.314667 loss_rnnt 201.698410 hw_loss 0.064639 lr 0.00016004 rank 3
2023-02-17 13:36:13,219 DEBUG TRAIN Batch 0/4100 loss 234.097336 loss_att 263.896790 loss_ctc 228.669312 loss_rnnt 228.798874 hw_loss 0.116826 lr 0.00016404 rank 4
2023-02-17 13:36:13,224 DEBUG TRAIN Batch 0/4100 loss 214.272110 loss_att 236.200165 loss_ctc 206.729996 loss_rnnt 210.821594 hw_loss 0.132208 lr 0.00016404 rank 2
2023-02-17 13:36:13,224 DEBUG TRAIN Batch 0/4100 loss 190.658813 loss_att 226.384491 loss_ctc 185.014618 loss_rnnt 184.240677 hw_loss 0.047912 lr 0.00016404 rank 6
2023-02-17 13:36:13,227 DEBUG TRAIN Batch 0/4100 loss 177.991180 loss_att 213.343094 loss_ctc 173.009460 loss_rnnt 171.533981 hw_loss 0.095701 lr 0.00016404 rank 0
2023-02-17 13:36:13,227 DEBUG TRAIN Batch 0/4100 loss 206.046646 loss_att 239.545410 loss_ctc 205.128860 loss_rnnt 199.464233 hw_loss 0.009478 lr 0.00016404 rank 1
2023-02-17 13:36:13,228 DEBUG TRAIN Batch 0/4100 loss 210.403793 loss_att 255.354462 loss_ctc 205.718719 loss_rnnt 201.963013 hw_loss 0.141240 lr 0.00016404 rank 3
2023-02-17 13:36:13,230 DEBUG TRAIN Batch 0/4100 loss 192.982864 loss_att 231.820862 loss_ctc 193.804199 loss_rnnt 185.073166 hw_loss 0.061102 lr 0.00016404 rank 5
2023-02-17 13:36:13,230 DEBUG TRAIN Batch 0/4100 loss 191.229431 loss_att 239.449829 loss_ctc 184.453125 loss_rnnt 182.452728 hw_loss 0.067732 lr 0.00016404 rank 7
2023-02-17 13:37:26,188 DEBUG TRAIN Batch 0/4200 loss 187.855515 loss_att 211.938995 loss_ctc 184.726562 loss_rnnt 183.384369 hw_loss 0.134361 lr 0.00016804 rank 2
2023-02-17 13:37:26,189 DEBUG TRAIN Batch 0/4200 loss 221.744308 loss_att 249.733627 loss_ctc 224.857224 loss_rnnt 215.693634 hw_loss 0.070746 lr 0.00016804 rank 5
2023-02-17 13:37:26,198 DEBUG TRAIN Batch 0/4200 loss 241.115677 loss_att 289.261169 loss_ctc 254.138641 loss_rnnt 229.698730 hw_loss 0.096491 lr 0.00016804 rank 4
2023-02-17 13:37:26,204 DEBUG TRAIN Batch 0/4200 loss 237.002258 loss_att 266.351135 loss_ctc 247.560242 loss_rnnt 229.667114 hw_loss 0.108069 lr 0.00016804 rank 6
2023-02-17 13:37:26,204 DEBUG TRAIN Batch 0/4200 loss 215.051987 loss_att 250.750916 loss_ctc 221.793304 loss_rnnt 206.950439 hw_loss 0.117948 lr 0.00016804 rank 7
2023-02-17 13:37:26,207 DEBUG TRAIN Batch 0/4200 loss 214.128098 loss_att 262.225739 loss_ctc 209.717499 loss_rnnt 205.049286 hw_loss 0.088801 lr 0.00016804 rank 0
2023-02-17 13:37:26,241 DEBUG TRAIN Batch 0/4200 loss 197.460312 loss_att 249.776031 loss_ctc 196.493347 loss_rnnt 187.047379 hw_loss 0.147595 lr 0.00016804 rank 3
2023-02-17 13:37:26,245 DEBUG TRAIN Batch 0/4200 loss 179.125519 loss_att 221.308105 loss_ctc 169.177216 loss_rnnt 171.972504 hw_loss 0.080537 lr 0.00016804 rank 1
2023-02-17 13:38:40,452 DEBUG TRAIN Batch 0/4300 loss 145.090698 loss_att 186.326004 loss_ctc 149.383331 loss_rnnt 136.271225 hw_loss 0.000109 lr 0.00017204 rank 4
2023-02-17 13:38:40,454 DEBUG TRAIN Batch 0/4300 loss 155.004517 loss_att 178.740021 loss_ctc 155.084549 loss_rnnt 150.114288 hw_loss 0.248330 lr 0.00017204 rank 6
2023-02-17 13:38:40,455 DEBUG TRAIN Batch 0/4300 loss 111.415230 loss_att 136.227905 loss_ctc 116.355507 loss_rnnt 105.743637 hw_loss 0.094407 lr 0.00017204 rank 1
2023-02-17 13:38:40,456 DEBUG TRAIN Batch 0/4300 loss 173.015335 loss_att 195.531860 loss_ctc 170.247711 loss_rnnt 168.817123 hw_loss 0.119886 lr 0.00017204 rank 2
2023-02-17 13:38:40,458 DEBUG TRAIN Batch 0/4300 loss 143.870209 loss_att 176.968475 loss_ctc 145.482910 loss_rnnt 137.016464 hw_loss 0.035732 lr 0.00017204 rank 0
2023-02-17 13:38:40,461 DEBUG TRAIN Batch 0/4300 loss 195.498245 loss_att 239.010986 loss_ctc 200.395416 loss_rnnt 186.028809 hw_loss 0.213609 lr 0.00017204 rank 7
2023-02-17 13:38:40,462 DEBUG TRAIN Batch 0/4300 loss 204.062851 loss_att 258.011719 loss_ctc 201.222046 loss_rnnt 193.651764 hw_loss 0.000109 lr 0.00017204 rank 3
2023-02-17 13:38:40,466 DEBUG TRAIN Batch 0/4300 loss 160.621246 loss_att 213.155746 loss_ctc 159.282516 loss_rnnt 150.193954 hw_loss 0.185428 lr 0.00017204 rank 5
2023-02-17 13:39:53,710 DEBUG TRAIN Batch 0/4400 loss 199.296936 loss_att 273.369141 loss_ctc 193.648865 loss_rnnt 185.204865 hw_loss 0.057572 lr 0.00017604 rank 2
2023-02-17 13:39:53,726 DEBUG TRAIN Batch 0/4400 loss 140.471344 loss_att 193.834488 loss_ctc 136.650406 loss_rnnt 130.204147 hw_loss 0.195030 lr 0.00017604 rank 4
2023-02-17 13:39:53,726 DEBUG TRAIN Batch 0/4400 loss 44.188553 loss_att 52.937057 loss_ctc 46.072300 loss_rnnt 41.980808 hw_loss 0.387884 lr 0.00017604 rank 6
2023-02-17 13:39:53,726 DEBUG TRAIN Batch 0/4400 loss 183.818405 loss_att 231.043427 loss_ctc 189.384674 loss_rnnt 173.551376 hw_loss 0.149722 lr 0.00017604 rank 1
2023-02-17 13:39:53,730 DEBUG TRAIN Batch 0/4400 loss 68.467224 loss_att 85.524422 loss_ctc 68.193871 loss_rnnt 64.959396 hw_loss 0.249068 lr 0.00017604 rank 0
2023-02-17 13:39:53,731 DEBUG TRAIN Batch 0/4400 loss 39.724323 loss_att 47.430996 loss_ctc 41.194237 loss_rnnt 37.888500 hw_loss 0.184686 lr 0.00017604 rank 5
2023-02-17 13:39:53,732 DEBUG TRAIN Batch 0/4400 loss 172.727676 loss_att 208.382156 loss_ctc 172.350174 loss_rnnt 165.593796 hw_loss 0.099999 lr 0.00017604 rank 3
2023-02-17 13:39:53,736 DEBUG TRAIN Batch 0/4400 loss 166.643372 loss_att 195.537369 loss_ctc 173.487915 loss_rnnt 159.920944 hw_loss 0.058183 lr 0.00017604 rank 7
2023-02-17 13:41:06,812 DEBUG TRAIN Batch 0/4500 loss 195.888382 loss_att 255.761566 loss_ctc 201.254623 loss_rnnt 183.155884 hw_loss 0.079462 lr 0.00018004 rank 6
2023-02-17 13:41:06,818 DEBUG TRAIN Batch 0/4500 loss 78.758018 loss_att 103.298454 loss_ctc 82.544060 loss_rnnt 73.249146 hw_loss 0.179966 lr 0.00018004 rank 4
2023-02-17 13:41:06,818 DEBUG TRAIN Batch 0/4500 loss 223.337967 loss_att 278.846741 loss_ctc 226.424744 loss_rnnt 211.725952 hw_loss 0.185036 lr 0.00018004 rank 7
2023-02-17 13:41:06,820 DEBUG TRAIN Batch 0/4500 loss 85.726952 loss_att 107.089561 loss_ctc 89.103050 loss_rnnt 80.981148 hw_loss 0.043389 lr 0.00018004 rank 3
2023-02-17 13:41:06,822 DEBUG TRAIN Batch 0/4500 loss 175.428604 loss_att 257.486084 loss_ctc 167.497040 loss_rnnt 160.040924 hw_loss 0.063244 lr 0.00018004 rank 0
2023-02-17 13:41:06,824 DEBUG TRAIN Batch 0/4500 loss 172.383896 loss_att 226.226807 loss_ctc 167.817780 loss_rnnt 162.224060 hw_loss 0.000107 lr 0.00018004 rank 2
2023-02-17 13:41:06,825 DEBUG TRAIN Batch 0/4500 loss 151.316299 loss_att 227.018158 loss_ctc 148.454910 loss_rnnt 136.557373 hw_loss 0.000107 lr 0.00018004 rank 5
2023-02-17 13:41:06,871 DEBUG TRAIN Batch 0/4500 loss 210.208481 loss_att 246.437469 loss_ctc 216.147873 loss_rnnt 202.170700 hw_loss 0.000107 lr 0.00018004 rank 1
2023-02-17 13:42:21,599 DEBUG TRAIN Batch 0/4600 loss 177.227051 loss_att 243.404739 loss_ctc 176.155746 loss_rnnt 164.122406 hw_loss 0.022399 lr 0.00018404 rank 3
2023-02-17 13:42:21,604 DEBUG TRAIN Batch 0/4600 loss 153.146881 loss_att 213.604614 loss_ctc 157.578613 loss_rnnt 140.398407 hw_loss 0.123841 lr 0.00018404 rank 7
2023-02-17 13:42:21,607 DEBUG TRAIN Batch 0/4600 loss 221.502472 loss_att 284.253082 loss_ctc 230.430237 loss_rnnt 207.748322 hw_loss 0.025568 lr 0.00018404 rank 5
2023-02-17 13:42:21,606 DEBUG TRAIN Batch 0/4600 loss 191.664917 loss_att 277.778442 loss_ctc 184.315170 loss_rnnt 175.359467 hw_loss 0.117604 lr 0.00018404 rank 4
2023-02-17 13:42:21,608 DEBUG TRAIN Batch 0/4600 loss 201.187790 loss_att 254.901154 loss_ctc 205.602905 loss_rnnt 189.775787 hw_loss 0.151190 lr 0.00018404 rank 2
2023-02-17 13:42:21,610 DEBUG TRAIN Batch 0/4600 loss 178.056366 loss_att 235.297211 loss_ctc 169.049255 loss_rnnt 167.734436 hw_loss 0.140079 lr 0.00018404 rank 1
2023-02-17 13:42:21,611 DEBUG TRAIN Batch 0/4600 loss 215.980591 loss_att 282.986572 loss_ctc 217.482346 loss_rnnt 202.345093 hw_loss 0.063899 lr 0.00018404 rank 6
2023-02-17 13:42:21,612 DEBUG TRAIN Batch 0/4600 loss 169.243362 loss_att 245.613388 loss_ctc 156.375717 loss_rnnt 155.665237 hw_loss 0.037126 lr 0.00018404 rank 0
2023-02-17 13:43:35,540 DEBUG TRAIN Batch 0/4700 loss 237.556381 loss_att 319.377991 loss_ctc 223.218018 loss_rnnt 223.052505 hw_loss 0.096261 lr 0.00018804 rank 5
2023-02-17 13:43:35,554 DEBUG TRAIN Batch 0/4700 loss 163.764923 loss_att 220.238892 loss_ctc 169.345062 loss_rnnt 151.660461 hw_loss 0.123051 lr 0.00018804 rank 6
2023-02-17 13:43:35,555 DEBUG TRAIN Batch 0/4700 loss 192.608353 loss_att 268.554688 loss_ctc 194.504211 loss_rnnt 177.131500 hw_loss 0.065249 lr 0.00018804 rank 2
2023-02-17 13:43:35,556 DEBUG TRAIN Batch 0/4700 loss 168.735947 loss_att 222.723373 loss_ctc 157.090820 loss_rnnt 159.425568 hw_loss 0.122941 lr 0.00018804 rank 4
2023-02-17 13:43:35,557 DEBUG TRAIN Batch 0/4700 loss 188.938736 loss_att 240.514862 loss_ctc 197.318909 loss_rnnt 177.454666 hw_loss 0.096558 lr 0.00018804 rank 1
2023-02-17 13:43:35,564 DEBUG TRAIN Batch 0/4700 loss 153.346298 loss_att 222.854401 loss_ctc 145.337051 loss_rnnt 140.436218 hw_loss 0.143153 lr 0.00018804 rank 0
2023-02-17 13:43:35,565 DEBUG TRAIN Batch 0/4700 loss 182.957825 loss_att 255.814774 loss_ctc 188.256653 loss_rnnt 167.593842 hw_loss 0.161390 lr 0.00018804 rank 7
2023-02-17 13:43:35,607 DEBUG TRAIN Batch 0/4700 loss 160.239838 loss_att 229.417938 loss_ctc 150.970261 loss_rnnt 147.583710 hw_loss 0.105855 lr 0.00018804 rank 3
2023-02-17 13:44:48,388 DEBUG TRAIN Batch 0/4800 loss 178.544724 loss_att 267.199860 loss_ctc 176.427460 loss_rnnt 161.095917 hw_loss 0.000106 lr 0.00019204 rank 4
2023-02-17 13:44:48,390 DEBUG TRAIN Batch 0/4800 loss 175.266144 loss_att 245.523560 loss_ctc 168.164154 loss_rnnt 162.098083 hw_loss 0.119052 lr 0.00019204 rank 2
2023-02-17 13:44:48,392 DEBUG TRAIN Batch 0/4800 loss 111.224274 loss_att 183.943176 loss_ctc 106.415550 loss_rnnt 97.254456 hw_loss 0.126006 lr 0.00019204 rank 0
2023-02-17 13:44:48,395 DEBUG TRAIN Batch 0/4800 loss 179.231689 loss_att 253.476654 loss_ctc 179.075348 loss_rnnt 164.319153 hw_loss 0.158237 lr 0.00019204 rank 5
2023-02-17 13:44:48,397 DEBUG TRAIN Batch 0/4800 loss 159.542847 loss_att 233.475510 loss_ctc 150.566284 loss_rnnt 145.938553 hw_loss 0.027422 lr 0.00019204 rank 3
2023-02-17 13:44:48,399 DEBUG TRAIN Batch 0/4800 loss 159.661453 loss_att 231.992645 loss_ctc 154.039810 loss_rnnt 145.929184 hw_loss 0.029203 lr 0.00019204 rank 6
2023-02-17 13:44:48,398 DEBUG TRAIN Batch 0/4800 loss 164.859940 loss_att 218.060089 loss_ctc 168.538391 loss_rnnt 153.670135 hw_loss 0.111194 lr 0.00019204 rank 1
2023-02-17 13:44:48,441 DEBUG TRAIN Batch 0/4800 loss 165.401703 loss_att 227.819336 loss_ctc 158.871735 loss_rnnt 153.689697 hw_loss 0.185900 lr 0.00019204 rank 7
2023-02-17 13:46:01,998 DEBUG TRAIN Batch 0/4900 loss 155.577896 loss_att 229.654022 loss_ctc 157.023514 loss_rnnt 140.538315 hw_loss 0.059259 lr 0.00019604 rank 3
2023-02-17 13:46:02,014 DEBUG TRAIN Batch 0/4900 loss 153.011749 loss_att 211.573212 loss_ctc 151.304672 loss_rnnt 141.488190 hw_loss 0.072895 lr 0.00019604 rank 6
2023-02-17 13:46:02,015 DEBUG TRAIN Batch 0/4900 loss 140.252014 loss_att 200.264389 loss_ctc 137.560455 loss_rnnt 128.586060 hw_loss 0.041915 lr 0.00019604 rank 0
2023-02-17 13:46:02,015 DEBUG TRAIN Batch 0/4900 loss 168.817200 loss_att 225.640732 loss_ctc 167.177475 loss_rnnt 157.633118 hw_loss 0.071264 lr 0.00019604 rank 7
2023-02-17 13:46:02,017 DEBUG TRAIN Batch 0/4900 loss 161.400314 loss_att 222.852310 loss_ctc 159.090500 loss_rnnt 149.381653 hw_loss 0.067921 lr 0.00019604 rank 4
2023-02-17 13:46:02,031 DEBUG TRAIN Batch 0/4900 loss 105.410873 loss_att 142.338425 loss_ctc 108.352013 loss_rnnt 97.522217 hw_loss 0.208091 lr 0.00019604 rank 1
2023-02-17 13:46:02,049 DEBUG TRAIN Batch 0/4900 loss 151.640030 loss_att 213.717194 loss_ctc 154.357407 loss_rnnt 138.835495 hw_loss 0.050244 lr 0.00019604 rank 5
2023-02-17 13:46:02,069 DEBUG TRAIN Batch 0/4900 loss 180.379166 loss_att 245.080444 loss_ctc 182.298508 loss_rnnt 167.104416 hw_loss 0.147319 lr 0.00019604 rank 2
2023-02-17 13:47:17,376 DEBUG TRAIN Batch 0/5000 loss 139.988266 loss_att 207.590729 loss_ctc 139.392914 loss_rnnt 126.481155 hw_loss 0.123713 lr 0.00020004 rank 4
2023-02-17 13:47:17,380 DEBUG TRAIN Batch 0/5000 loss 79.998436 loss_att 104.837341 loss_ctc 83.067131 loss_rnnt 74.541496 hw_loss 0.150010 lr 0.00020004 rank 2
2023-02-17 13:47:17,382 DEBUG TRAIN Batch 0/5000 loss 155.618179 loss_att 226.871140 loss_ctc 153.556091 loss_rnnt 141.621384 hw_loss 0.039692 lr 0.00020004 rank 3
2023-02-17 13:47:17,382 DEBUG TRAIN Batch 0/5000 loss 143.377090 loss_att 207.706863 loss_ctc 143.808350 loss_rnnt 130.446243 hw_loss 0.013853 lr 0.00020004 rank 0
2023-02-17 13:47:17,383 DEBUG TRAIN Batch 0/5000 loss 69.835709 loss_att 100.389679 loss_ctc 71.793861 loss_rnnt 63.342583 hw_loss 0.227338 lr 0.00020004 rank 6
2023-02-17 13:47:17,383 DEBUG TRAIN Batch 0/5000 loss 168.789963 loss_att 250.296143 loss_ctc 160.444214 loss_rnnt 153.560532 hw_loss 0.076827 lr 0.00020004 rank 1
2023-02-17 13:47:17,388 DEBUG TRAIN Batch 0/5000 loss 120.986656 loss_att 174.893951 loss_ctc 120.256615 loss_rnnt 110.271683 hw_loss 0.057824 lr 0.00020004 rank 7
2023-02-17 13:47:17,437 DEBUG TRAIN Batch 0/5000 loss 81.206863 loss_att 105.908035 loss_ctc 78.573959 loss_rnnt 76.452248 hw_loss 0.310182 lr 0.00020004 rank 5
2023-02-17 13:48:30,292 DEBUG TRAIN Batch 0/5100 loss 160.367050 loss_att 255.454391 loss_ctc 155.918259 loss_rnnt 141.916443 hw_loss 0.049311 lr 0.00020404 rank 2
2023-02-17 13:48:30,296 DEBUG TRAIN Batch 0/5100 loss 146.991989 loss_att 188.842239 loss_ctc 149.391602 loss_rnnt 138.261765 hw_loss 0.075425 lr 0.00020404 rank 3
2023-02-17 13:48:30,302 DEBUG TRAIN Batch 0/5100 loss 169.045105 loss_att 257.891754 loss_ctc 169.528961 loss_rnnt 151.177185 hw_loss 0.063910 lr 0.00020404 rank 7
2023-02-17 13:48:30,306 DEBUG TRAIN Batch 0/5100 loss 156.820862 loss_att 238.282486 loss_ctc 150.802917 loss_rnnt 141.241455 hw_loss 0.167785 lr 0.00020404 rank 6
2023-02-17 13:48:30,307 DEBUG TRAIN Batch 0/5100 loss 83.606674 loss_att 123.093788 loss_ctc 79.783463 loss_rnnt 76.098999 hw_loss 0.225029 lr 0.00020404 rank 4
2023-02-17 13:48:30,311 DEBUG TRAIN Batch 0/5100 loss 200.277496 loss_att 293.847473 loss_ctc 200.685471 loss_rnnt 181.449936 hw_loss 0.110955 lr 0.00020404 rank 0
2023-02-17 13:48:30,311 DEBUG TRAIN Batch 0/5100 loss 182.622116 loss_att 269.109314 loss_ctc 179.518768 loss_rnnt 165.709366 hw_loss 0.054550 lr 0.00020404 rank 5
2023-02-17 13:48:30,319 DEBUG TRAIN Batch 0/5100 loss 183.448853 loss_att 271.893311 loss_ctc 185.070526 loss_rnnt 165.543701 hw_loss 0.000083 lr 0.00020404 rank 1
2023-02-17 13:49:43,179 DEBUG TRAIN Batch 0/5200 loss 152.803772 loss_att 233.908020 loss_ctc 154.265167 loss_rnnt 136.387985 hw_loss 0.000171 lr 0.00020804 rank 4
2023-02-17 13:49:43,183 DEBUG TRAIN Batch 0/5200 loss 158.358688 loss_att 255.838806 loss_ctc 146.120895 loss_rnnt 140.491867 hw_loss 0.004685 lr 0.00020804 rank 6
2023-02-17 13:49:43,185 DEBUG TRAIN Batch 0/5200 loss 137.985992 loss_att 217.140793 loss_ctc 129.132202 loss_rnnt 123.335449 hw_loss 0.000171 lr 0.00020804 rank 0
2023-02-17 13:49:43,187 DEBUG TRAIN Batch 0/5200 loss 168.496033 loss_att 257.523590 loss_ctc 159.538803 loss_rnnt 151.820969 hw_loss 0.119718 lr 0.00020804 rank 2
2023-02-17 13:49:43,188 DEBUG TRAIN Batch 0/5200 loss 145.737808 loss_att 223.936462 loss_ctc 140.338730 loss_rnnt 130.756256 hw_loss 0.115712 lr 0.00020804 rank 1
2023-02-17 13:49:43,189 DEBUG TRAIN Batch 0/5200 loss 116.270912 loss_att 192.814255 loss_ctc 101.344627 loss_rnnt 102.927505 hw_loss 0.046693 lr 0.00020804 rank 3
2023-02-17 13:49:43,189 DEBUG TRAIN Batch 0/5200 loss 224.518112 loss_att 308.089783 loss_ctc 226.895142 loss_rnnt 207.433716 hw_loss 0.099609 lr 0.00020804 rank 5
2023-02-17 13:49:43,192 DEBUG TRAIN Batch 0/5200 loss 176.441422 loss_att 270.943176 loss_ctc 175.231232 loss_rnnt 157.627167 hw_loss 0.141098 lr 0.00020804 rank 7
2023-02-17 13:50:56,668 DEBUG TRAIN Batch 0/5300 loss 180.314972 loss_att 285.036896 loss_ctc 187.241501 loss_rnnt 158.379562 hw_loss 0.126534 lr 0.00021204 rank 3
2023-02-17 13:50:56,671 DEBUG TRAIN Batch 0/5300 loss 174.290909 loss_att 253.757111 loss_ctc 189.392242 loss_rnnt 156.384094 hw_loss 0.000130 lr 0.00021204 rank 4
2023-02-17 13:50:56,672 DEBUG TRAIN Batch 0/5300 loss 154.352127 loss_att 255.307846 loss_ctc 156.497589 loss_rnnt 133.796371 hw_loss 0.147281 lr 0.00021204 rank 7
2023-02-17 13:50:56,680 DEBUG TRAIN Batch 0/5300 loss 138.002258 loss_att 242.064362 loss_ctc 137.698822 loss_rnnt 117.200684 hw_loss 0.055546 lr 0.00021204 rank 1
2023-02-17 13:50:56,680 DEBUG TRAIN Batch 0/5300 loss 159.472534 loss_att 247.361267 loss_ctc 159.157181 loss_rnnt 141.848358 hw_loss 0.165914 lr 0.00021204 rank 6
2023-02-17 13:50:56,681 DEBUG TRAIN Batch 0/5300 loss 163.911926 loss_att 251.375793 loss_ctc 166.034653 loss_rnnt 146.116531 hw_loss 0.036704 lr 0.00021204 rank 0
2023-02-17 13:50:56,712 DEBUG TRAIN Batch 0/5300 loss 147.476547 loss_att 245.401581 loss_ctc 148.363251 loss_rnnt 127.697815 hw_loss 0.141544 lr 0.00021204 rank 5
2023-02-17 13:50:56,731 DEBUG TRAIN Batch 0/5300 loss 95.216316 loss_att 206.781189 loss_ctc 82.214462 loss_rnnt 74.621124 hw_loss 0.029618 lr 0.00021204 rank 2
2023-02-17 13:52:09,925 DEBUG TRAIN Batch 0/5400 loss 159.017426 loss_att 246.928955 loss_ctc 158.715363 loss_rnnt 141.422333 hw_loss 0.099471 lr 0.00021604 rank 6
2023-02-17 13:52:09,927 DEBUG TRAIN Batch 0/5400 loss 131.961731 loss_att 223.552582 loss_ctc 127.354172 loss_rnnt 114.228477 hw_loss 0.055163 lr 0.00021604 rank 4
2023-02-17 13:52:09,933 DEBUG TRAIN Batch 0/5400 loss 144.025894 loss_att 247.262238 loss_ctc 139.020782 loss_rnnt 124.018806 hw_loss 0.050920 lr 0.00021604 rank 2
2023-02-17 13:52:09,934 DEBUG TRAIN Batch 0/5400 loss 115.992012 loss_att 210.513168 loss_ctc 116.202141 loss_rnnt 96.991623 hw_loss 0.127779 lr 0.00021604 rank 0
2023-02-17 13:52:09,934 DEBUG TRAIN Batch 0/5400 loss 134.183136 loss_att 214.606415 loss_ctc 133.881470 loss_rnnt 118.087799 hw_loss 0.095446 lr 0.00021604 rank 7
2023-02-17 13:52:09,935 DEBUG TRAIN Batch 0/5400 loss 154.036697 loss_att 248.247986 loss_ctc 155.751129 loss_rnnt 134.903625 hw_loss 0.116683 lr 0.00021604 rank 3
2023-02-17 13:52:09,936 DEBUG TRAIN Batch 0/5400 loss 165.677002 loss_att 239.662292 loss_ctc 163.147003 loss_rnnt 151.170624 hw_loss 0.087472 lr 0.00021604 rank 5
2023-02-17 13:52:09,987 DEBUG TRAIN Batch 0/5400 loss 144.029999 loss_att 229.470016 loss_ctc 143.151459 loss_rnnt 126.976753 hw_loss 0.154460 lr 0.00021604 rank 1
2023-02-17 13:53:22,737 DEBUG TRAIN Batch 0/5500 loss 129.379166 loss_att 229.565216 loss_ctc 118.046516 loss_rnnt 110.830597 hw_loss 0.041948 lr 0.00022004 rank 4
2023-02-17 13:53:22,742 DEBUG TRAIN Batch 0/5500 loss 134.566895 loss_att 222.245880 loss_ctc 136.597076 loss_rnnt 116.706924 hw_loss 0.100241 lr 0.00022004 rank 6
2023-02-17 13:53:22,743 DEBUG TRAIN Batch 0/5500 loss 121.071922 loss_att 205.681641 loss_ctc 113.638390 loss_rnnt 105.107635 hw_loss 0.062769 lr 0.00022004 rank 5
2023-02-17 13:53:22,744 DEBUG TRAIN Batch 0/5500 loss 146.768143 loss_att 238.211533 loss_ctc 153.288086 loss_rnnt 127.552841 hw_loss 0.107407 lr 0.00022004 rank 3
2023-02-17 13:53:22,747 DEBUG TRAIN Batch 0/5500 loss 126.112213 loss_att 211.218475 loss_ctc 128.718521 loss_rnnt 108.660271 hw_loss 0.155975 lr 0.00022004 rank 0
2023-02-17 13:53:22,747 DEBUG TRAIN Batch 0/5500 loss 134.282471 loss_att 210.828659 loss_ctc 136.004242 loss_rnnt 118.743591 hw_loss 0.000121 lr 0.00022004 rank 2
2023-02-17 13:53:22,748 DEBUG TRAIN Batch 0/5500 loss 114.018700 loss_att 190.834793 loss_ctc 104.439774 loss_rnnt 99.851891 hw_loss 0.151466 lr 0.00022004 rank 1
2023-02-17 13:53:22,749 DEBUG TRAIN Batch 0/5500 loss 156.982788 loss_att 244.407166 loss_ctc 154.886932 loss_rnnt 139.753830 hw_loss 0.044147 lr 0.00022004 rank 7
2023-02-17 13:54:35,745 DEBUG TRAIN Batch 0/5600 loss 133.903656 loss_att 209.346191 loss_ctc 132.657089 loss_rnnt 118.936630 hw_loss 0.083889 lr 0.00022404 rank 4
2023-02-17 13:54:35,749 DEBUG TRAIN Batch 0/5600 loss 146.629303 loss_att 253.118744 loss_ctc 143.722229 loss_rnnt 125.660484 hw_loss 0.109793 lr 0.00022404 rank 1
2023-02-17 13:54:35,751 DEBUG TRAIN Batch 0/5600 loss 91.082809 loss_att 149.250473 loss_ctc 90.754814 loss_rnnt 79.474899 hw_loss 0.033957 lr 0.00022404 rank 6
2023-02-17 13:54:35,752 DEBUG TRAIN Batch 0/5600 loss 118.955368 loss_att 192.755981 loss_ctc 122.509277 loss_rnnt 103.591591 hw_loss 0.243360 lr 0.00022404 rank 2
2023-02-17 13:54:35,756 DEBUG TRAIN Batch 0/5600 loss 130.368866 loss_att 206.070465 loss_ctc 134.312668 loss_rnnt 114.631187 hw_loss 0.134078 lr 0.00022404 rank 7
2023-02-17 13:54:35,756 DEBUG TRAIN Batch 0/5600 loss 139.046982 loss_att 227.013367 loss_ctc 135.495285 loss_rnnt 121.879059 hw_loss 0.090378 lr 0.00022404 rank 3
2023-02-17 13:54:35,760 DEBUG TRAIN Batch 0/5600 loss 96.963867 loss_att 165.067719 loss_ctc 95.058495 loss_rnnt 83.562622 hw_loss 0.064731 lr 0.00022404 rank 0
2023-02-17 13:54:35,805 DEBUG TRAIN Batch 0/5600 loss 100.686409 loss_att 158.617752 loss_ctc 103.055222 loss_rnnt 88.694519 hw_loss 0.168336 lr 0.00022404 rank 5
2023-02-17 13:55:51,368 DEBUG TRAIN Batch 0/5700 loss 152.309555 loss_att 257.076599 loss_ctc 152.514801 loss_rnnt 131.242844 hw_loss 0.161135 lr 0.00022804 rank 1
2023-02-17 13:55:51,383 DEBUG TRAIN Batch 0/5700 loss 124.742752 loss_att 232.641144 loss_ctc 117.998680 loss_rnnt 104.044373 hw_loss 0.033596 lr 0.00022804 rank 6
2023-02-17 13:55:51,383 DEBUG TRAIN Batch 0/5700 loss 78.256317 loss_att 153.596909 loss_ctc 73.361473 loss_rnnt 63.776356 hw_loss 0.120903 lr 0.00022804 rank 4
2023-02-17 13:55:51,386 DEBUG TRAIN Batch 0/5700 loss 157.554886 loss_att 279.841095 loss_ctc 153.846664 loss_rnnt 133.556320 hw_loss 0.067017 lr 0.00022804 rank 5
2023-02-17 13:55:51,386 DEBUG TRAIN Batch 0/5700 loss 129.053238 loss_att 212.543304 loss_ctc 127.512527 loss_rnnt 112.525269 hw_loss 0.066351 lr 0.00022804 rank 3
2023-02-17 13:55:51,387 DEBUG TRAIN Batch 0/5700 loss 57.364624 loss_att 84.921196 loss_ctc 60.559174 loss_rnnt 51.335102 hw_loss 0.172994 lr 0.00022804 rank 0
2023-02-17 13:55:51,392 DEBUG TRAIN Batch 0/5700 loss 63.216518 loss_att 96.680435 loss_ctc 64.974640 loss_rnnt 56.208241 hw_loss 0.152023 lr 0.00022804 rank 7
2023-02-17 13:55:51,393 DEBUG TRAIN Batch 0/5700 loss 175.370071 loss_att 277.926697 loss_ctc 178.158737 loss_rnnt 154.485382 hw_loss 0.002849 lr 0.00022804 rank 2
2023-02-17 13:57:04,296 DEBUG TRAIN Batch 0/5800 loss 141.885315 loss_att 253.343964 loss_ctc 141.111389 loss_rnnt 119.696701 hw_loss 0.000158 lr 0.00023204 rank 4
2023-02-17 13:57:04,299 DEBUG TRAIN Batch 0/5800 loss 120.714790 loss_att 217.465668 loss_ctc 122.917412 loss_rnnt 101.021477 hw_loss 0.092730 lr 0.00023204 rank 0
2023-02-17 13:57:04,302 DEBUG TRAIN Batch 0/5800 loss 57.358845 loss_att 86.675171 loss_ctc 58.168060 loss_rnnt 51.290443 hw_loss 0.182310 lr 0.00023204 rank 3
2023-02-17 13:57:04,302 DEBUG TRAIN Batch 0/5800 loss 98.451180 loss_att 214.367065 loss_ctc 92.488762 loss_rnnt 76.062912 hw_loss 0.000158 lr 0.00023204 rank 2
2023-02-17 13:57:04,303 DEBUG TRAIN Batch 0/5800 loss 145.336838 loss_att 236.474091 loss_ctc 143.010590 loss_rnnt 127.395767 hw_loss 0.044612 lr 0.00023204 rank 6
2023-02-17 13:57:04,303 DEBUG TRAIN Batch 0/5800 loss 162.205093 loss_att 268.078491 loss_ctc 153.604797 loss_rnnt 142.098373 hw_loss 0.147663 lr 0.00023204 rank 5
2023-02-17 13:57:04,304 DEBUG TRAIN Batch 0/5800 loss 97.347847 loss_att 183.883926 loss_ctc 89.421593 loss_rnnt 81.066788 hw_loss 0.057526 lr 0.00023204 rank 7
2023-02-17 13:57:04,345 DEBUG TRAIN Batch 0/5800 loss 135.077667 loss_att 239.313553 loss_ctc 134.954361 loss_rnnt 114.208549 hw_loss 0.071936 lr 0.00023204 rank 1
2023-02-17 13:58:16,729 DEBUG TRAIN Batch 0/5900 loss 116.123894 loss_att 214.261536 loss_ctc 116.060028 loss_rnnt 96.504761 hw_loss 0.000211 lr 0.00023604 rank 2
2023-02-17 13:58:16,730 DEBUG TRAIN Batch 0/5900 loss 148.990875 loss_att 255.390564 loss_ctc 149.869324 loss_rnnt 127.542961 hw_loss 0.095374 lr 0.00023604 rank 6
2023-02-17 13:58:16,730 DEBUG TRAIN Batch 0/5900 loss 118.942101 loss_att 225.617111 loss_ctc 108.816101 loss_rnnt 98.882439 hw_loss 0.140255 lr 0.00023604 rank 5
2023-02-17 13:58:16,730 DEBUG TRAIN Batch 0/5900 loss 172.560349 loss_att 272.306946 loss_ctc 172.665100 loss_rnnt 152.528168 hw_loss 0.129216 lr 0.00023604 rank 0
2023-02-17 13:58:16,731 DEBUG TRAIN Batch 0/5900 loss 153.081543 loss_att 257.095032 loss_ctc 148.915268 loss_rnnt 132.803558 hw_loss 0.057740 lr 0.00023604 rank 4
2023-02-17 13:58:16,731 DEBUG TRAIN Batch 0/5900 loss 127.651764 loss_att 220.182495 loss_ctc 128.561996 loss_rnnt 108.984268 hw_loss 0.074955 lr 0.00023604 rank 7
2023-02-17 13:58:16,734 DEBUG TRAIN Batch 0/5900 loss 132.890488 loss_att 248.716705 loss_ctc 136.217300 loss_rnnt 109.233299 hw_loss 0.090729 lr 0.00023604 rank 1
2023-02-17 13:58:16,736 DEBUG TRAIN Batch 0/5900 loss 107.715210 loss_att 227.912048 loss_ctc 110.178612 loss_rnnt 83.262085 hw_loss 0.159950 lr 0.00023604 rank 3
2023-02-17 13:59:30,406 DEBUG TRAIN Batch 0/6000 loss 152.301804 loss_att 266.075378 loss_ctc 150.057709 loss_rnnt 129.802048 hw_loss 0.082961 lr 0.00024004 rank 4
2023-02-17 13:59:30,407 DEBUG TRAIN Batch 0/6000 loss 110.026749 loss_att 225.872437 loss_ctc 112.559128 loss_rnnt 86.489677 hw_loss 0.056789 lr 0.00024004 rank 0
2023-02-17 13:59:30,409 DEBUG TRAIN Batch 0/6000 loss 114.075279 loss_att 204.162872 loss_ctc 112.985611 loss_rnnt 96.162224 hw_loss 0.076549 lr 0.00024004 rank 5
2023-02-17 13:59:30,409 DEBUG TRAIN Batch 0/6000 loss 122.120117 loss_att 218.466644 loss_ctc 120.672897 loss_rnnt 103.043686 hw_loss 0.000167 lr 0.00024004 rank 3
2023-02-17 13:59:30,411 DEBUG TRAIN Batch 0/6000 loss 107.755836 loss_att 208.600555 loss_ctc 103.348633 loss_rnnt 88.126358 hw_loss 0.090297 lr 0.00024004 rank 7
2023-02-17 13:59:30,415 DEBUG TRAIN Batch 0/6000 loss 133.903336 loss_att 231.713516 loss_ctc 133.807831 loss_rnnt 114.301559 hw_loss 0.098376 lr 0.00024004 rank 6
2023-02-17 13:59:30,426 DEBUG TRAIN Batch 0/6000 loss 120.303719 loss_att 240.893707 loss_ctc 114.573654 loss_rnnt 96.857437 hw_loss 0.173032 lr 0.00024004 rank 2
2023-02-17 13:59:30,434 DEBUG TRAIN Batch 0/6000 loss 107.796875 loss_att 214.722961 loss_ctc 105.584557 loss_rnnt 86.693184 hw_loss 0.025208 lr 0.00024004 rank 1
2023-02-17 14:00:44,176 DEBUG TRAIN Batch 0/6100 loss 119.204384 loss_att 222.757111 loss_ctc 113.521454 loss_rnnt 99.169876 hw_loss 0.153163 lr 0.00024404 rank 6
2023-02-17 14:00:44,178 DEBUG TRAIN Batch 0/6100 loss 138.624649 loss_att 215.854553 loss_ctc 147.939896 loss_rnnt 121.911217 hw_loss 0.047664 lr 0.00024404 rank 1
2023-02-17 14:00:44,178 DEBUG TRAIN Batch 0/6100 loss 157.462097 loss_att 250.051575 loss_ctc 157.034256 loss_rnnt 138.960693 hw_loss 0.076058 lr 0.00024404 rank 4
2023-02-17 14:00:44,179 DEBUG TRAIN Batch 0/6100 loss 120.258492 loss_att 225.508240 loss_ctc 113.662933 loss_rnnt 100.059189 hw_loss 0.053917 lr 0.00024404 rank 0
2023-02-17 14:00:44,179 DEBUG TRAIN Batch 0/6100 loss 113.880402 loss_att 213.748291 loss_ctc 113.921494 loss_rnnt 93.845474 hw_loss 0.104753 lr 0.00024404 rank 2
2023-02-17 14:00:44,180 DEBUG TRAIN Batch 0/6100 loss 134.996597 loss_att 256.833557 loss_ctc 133.044464 loss_rnnt 110.817810 hw_loss 0.134384 lr 0.00024404 rank 3
2023-02-17 14:00:44,182 DEBUG TRAIN Batch 0/6100 loss 103.481766 loss_att 208.681717 loss_ctc 100.958702 loss_rnnt 82.728958 hw_loss 0.092302 lr 0.00024404 rank 7
2023-02-17 14:00:44,186 DEBUG TRAIN Batch 0/6100 loss 135.602753 loss_att 216.418152 loss_ctc 138.133606 loss_rnnt 119.021057 hw_loss 0.152212 lr 0.00024404 rank 5
2023-02-17 14:01:56,997 DEBUG TRAIN Batch 0/6200 loss 126.353157 loss_att 216.889862 loss_ctc 135.456589 loss_rnnt 106.965332 hw_loss 0.125030 lr 0.00024804 rank 6
2023-02-17 14:01:56,998 DEBUG TRAIN Batch 0/6200 loss 144.610718 loss_att 236.446640 loss_ctc 155.409195 loss_rnnt 124.770554 hw_loss 0.062202 lr 0.00024804 rank 4
2023-02-17 14:01:56,999 DEBUG TRAIN Batch 0/6200 loss 104.464600 loss_att 197.824554 loss_ctc 103.042679 loss_rnnt 85.921989 hw_loss 0.112888 lr 0.00024804 rank 0
2023-02-17 14:01:57,000 DEBUG TRAIN Batch 0/6200 loss 146.406067 loss_att 254.169662 loss_ctc 152.015656 loss_rnnt 124.034744 hw_loss 0.132451 lr 0.00024804 rank 3
2023-02-17 14:01:57,004 DEBUG TRAIN Batch 0/6200 loss 94.509178 loss_att 192.197952 loss_ctc 86.567215 loss_rnnt 75.916916 hw_loss 0.212690 lr 0.00024804 rank 7
2023-02-17 14:01:57,008 DEBUG TRAIN Batch 0/6200 loss 98.704704 loss_att 180.856522 loss_ctc 93.387970 loss_rnnt 82.938301 hw_loss 0.084253 lr 0.00024804 rank 2
2023-02-17 14:01:57,014 DEBUG TRAIN Batch 0/6200 loss 58.899380 loss_att 103.083717 loss_ctc 58.480515 loss_rnnt 50.057228 hw_loss 0.114615 lr 0.00024804 rank 1
2023-02-17 14:01:57,032 DEBUG TRAIN Batch 0/6200 loss 119.773544 loss_att 204.112366 loss_ctc 123.476952 loss_rnnt 102.346069 hw_loss 0.123604 lr 0.00024804 rank 5
2023-02-17 14:03:09,897 DEBUG TRAIN Batch 0/6300 loss 76.601692 loss_att 137.691559 loss_ctc 80.256897 loss_rnnt 63.837494 hw_loss 0.110376 lr 0.00025204 rank 2
2023-02-17 14:03:09,899 DEBUG TRAIN Batch 0/6300 loss 105.867378 loss_att 201.347610 loss_ctc 101.289650 loss_rnnt 87.360451 hw_loss 0.039826 lr 0.00025204 rank 4
2023-02-17 14:03:09,899 DEBUG TRAIN Batch 0/6300 loss 100.733467 loss_att 145.621994 loss_ctc 105.621605 loss_rnnt 91.027496 hw_loss 0.143474 lr 0.00025204 rank 0
2023-02-17 14:03:09,901 DEBUG TRAIN Batch 0/6300 loss 151.695435 loss_att 289.660217 loss_ctc 154.005341 loss_rnnt 123.763367 hw_loss 0.058370 lr 0.00025204 rank 6
2023-02-17 14:03:09,901 DEBUG TRAIN Batch 0/6300 loss 102.008385 loss_att 192.111465 loss_ctc 102.959305 loss_rnnt 83.818542 hw_loss 0.079577 lr 0.00025204 rank 3
2023-02-17 14:03:09,902 DEBUG TRAIN Batch 0/6300 loss 94.173981 loss_att 167.827087 loss_ctc 89.435654 loss_rnnt 80.075066 hw_loss 0.000130 lr 0.00025204 rank 7
2023-02-17 14:03:09,902 DEBUG TRAIN Batch 0/6300 loss 59.542252 loss_att 104.782959 loss_ctc 62.000744 loss_rnnt 50.058529 hw_loss 0.202096 lr 0.00025204 rank 5
2023-02-17 14:03:09,903 DEBUG TRAIN Batch 0/6300 loss 135.071426 loss_att 240.024124 loss_ctc 129.616150 loss_rnnt 114.663597 hw_loss 0.271207 lr 0.00025204 rank 1
2023-02-17 14:04:25,024 DEBUG TRAIN Batch 0/6400 loss 129.286499 loss_att 237.137299 loss_ctc 130.635498 loss_rnnt 107.487076 hw_loss 0.092633 lr 0.00025604 rank 1
2023-02-17 14:04:25,035 DEBUG TRAIN Batch 0/6400 loss 115.763489 loss_att 238.398712 loss_ctc 105.954758 loss_rnnt 92.514297 hw_loss 0.056192 lr 0.00025604 rank 0
2023-02-17 14:04:25,038 DEBUG TRAIN Batch 0/6400 loss 110.789436 loss_att 242.089508 loss_ctc 99.623375 loss_rnnt 86.018158 hw_loss 0.000134 lr 0.00025604 rank 5
2023-02-17 14:04:25,038 DEBUG TRAIN Batch 0/6400 loss 118.763832 loss_att 227.664215 loss_ctc 119.669701 loss_rnnt 96.758591 hw_loss 0.195721 lr 0.00025604 rank 6
2023-02-17 14:04:25,040 DEBUG TRAIN Batch 0/6400 loss 75.142113 loss_att 116.361877 loss_ctc 83.214157 loss_rnnt 65.770630 hw_loss 0.096111 lr 0.00025604 rank 4
2023-02-17 14:04:25,049 DEBUG TRAIN Batch 0/6400 loss 105.753235 loss_att 213.371338 loss_ctc 107.233078 loss_rnnt 84.013184 hw_loss 0.035856 lr 0.00025604 rank 7
2023-02-17 14:04:25,050 DEBUG TRAIN Batch 0/6400 loss 93.133377 loss_att 149.432907 loss_ctc 95.156693 loss_rnnt 81.482452 hw_loss 0.227314 lr 0.00025604 rank 3
2023-02-17 14:04:25,083 DEBUG TRAIN Batch 0/6400 loss 131.821579 loss_att 232.841064 loss_ctc 125.654358 loss_rnnt 112.345604 hw_loss 0.176945 lr 0.00025604 rank 2
2023-02-17 14:05:37,945 DEBUG TRAIN Batch 0/6500 loss 131.530502 loss_att 254.920563 loss_ctc 134.699417 loss_rnnt 106.390656 hw_loss 0.073687 lr 0.00026004 rank 4
2023-02-17 14:05:37,947 DEBUG TRAIN Batch 0/6500 loss 141.855499 loss_att 298.014099 loss_ctc 128.762787 loss_rnnt 112.326202 hw_loss 0.081117 lr 0.00026004 rank 3
2023-02-17 14:05:37,949 DEBUG TRAIN Batch 0/6500 loss 109.101036 loss_att 236.248962 loss_ctc 101.634911 loss_rnnt 84.588043 hw_loss 0.147908 lr 0.00026004 rank 7
2023-02-17 14:05:37,950 DEBUG TRAIN Batch 0/6500 loss 96.291420 loss_att 218.890106 loss_ctc 85.793114 loss_rnnt 73.134628 hw_loss 0.069053 lr 0.00026004 rank 6
2023-02-17 14:05:37,949 DEBUG TRAIN Batch 0/6500 loss 110.151039 loss_att 219.242676 loss_ctc 103.662476 loss_rnnt 89.159019 hw_loss 0.072813 lr 0.00026004 rank 0
2023-02-17 14:05:37,951 DEBUG TRAIN Batch 0/6500 loss 120.116844 loss_att 225.378967 loss_ctc 122.443039 loss_rnnt 98.733078 hw_loss 0.039734 lr 0.00026004 rank 5
2023-02-17 14:05:37,956 DEBUG TRAIN Batch 0/6500 loss 111.621033 loss_att 241.485260 loss_ctc 110.273201 loss_rnnt 85.767670 hw_loss 0.112913 lr 0.00026004 rank 1
2023-02-17 14:05:37,997 DEBUG TRAIN Batch 0/6500 loss 135.551666 loss_att 236.154175 loss_ctc 135.304749 loss_rnnt 115.455444 hw_loss 0.016230 lr 0.00026004 rank 2
2023-02-17 14:06:51,016 DEBUG TRAIN Batch 0/6600 loss 120.349350 loss_att 232.575287 loss_ctc 121.599449 loss_rnnt 97.648422 hw_loss 0.167007 lr 0.00026404 rank 4
2023-02-17 14:06:51,022 DEBUG TRAIN Batch 0/6600 loss 82.073013 loss_att 205.588013 loss_ctc 77.526810 loss_rnnt 57.918407 hw_loss 0.108293 lr 0.00026404 rank 7
2023-02-17 14:06:51,024 DEBUG TRAIN Batch 0/6600 loss 112.550247 loss_att 213.949463 loss_ctc 116.943138 loss_rnnt 91.670761 hw_loss 0.026103 lr 0.00026404 rank 1
2023-02-17 14:06:51,025 DEBUG TRAIN Batch 0/6600 loss 145.729828 loss_att 238.889160 loss_ctc 152.514618 loss_rnnt 126.172638 hw_loss 0.038759 lr 0.00026404 rank 6
2023-02-17 14:06:51,026 DEBUG TRAIN Batch 0/6600 loss 102.394020 loss_att 224.340256 loss_ctc 95.727455 loss_rnnt 78.846664 hw_loss 0.088092 lr 0.00026404 rank 5
2023-02-17 14:06:51,027 DEBUG TRAIN Batch 0/6600 loss 132.875824 loss_att 220.032593 loss_ctc 131.462463 loss_rnnt 115.566513 hw_loss 0.124528 lr 0.00026404 rank 3
2023-02-17 14:06:51,029 DEBUG TRAIN Batch 0/6600 loss 131.418182 loss_att 230.134186 loss_ctc 134.455490 loss_rnnt 111.196060 hw_loss 0.138644 lr 0.00026404 rank 0
2023-02-17 14:06:51,077 DEBUG TRAIN Batch 0/6600 loss 89.640991 loss_att 199.009048 loss_ctc 82.525543 loss_rnnt 68.666183 hw_loss 0.093586 lr 0.00026404 rank 2
2023-02-17 14:08:04,106 DEBUG TRAIN Batch 0/6700 loss 119.565842 loss_att 229.302429 loss_ctc 120.950089 loss_rnnt 97.402786 hw_loss 0.058442 lr 0.00026804 rank 4
2023-02-17 14:08:04,107 DEBUG TRAIN Batch 0/6700 loss 123.628166 loss_att 222.379242 loss_ctc 131.342896 loss_rnnt 102.753555 hw_loss 0.179547 lr 0.00026804 rank 6
2023-02-17 14:08:04,109 DEBUG TRAIN Batch 0/6700 loss 97.836128 loss_att 203.282578 loss_ctc 97.956818 loss_rnnt 76.655716 hw_loss 0.140693 lr 0.00026804 rank 1
2023-02-17 14:08:04,110 DEBUG TRAIN Batch 0/6700 loss 91.991478 loss_att 196.517761 loss_ctc 86.554962 loss_rnnt 71.793419 hw_loss 0.033140 lr 0.00026804 rank 7
2023-02-17 14:08:04,109 DEBUG TRAIN Batch 0/6700 loss 112.927765 loss_att 220.460358 loss_ctc 114.489235 loss_rnnt 91.180260 hw_loss 0.061500 lr 0.00026804 rank 0
2023-02-17 14:08:04,110 DEBUG TRAIN Batch 0/6700 loss 133.075668 loss_att 275.596680 loss_ctc 130.534592 loss_rnnt 104.882942 hw_loss 0.051235 lr 0.00026804 rank 2
2023-02-17 14:08:04,114 DEBUG TRAIN Batch 0/6700 loss 87.930611 loss_att 208.560089 loss_ctc 76.041985 loss_rnnt 65.329971 hw_loss 0.112301 lr 0.00026804 rank 5
2023-02-17 14:08:04,156 DEBUG TRAIN Batch 0/6700 loss 123.197426 loss_att 247.162231 loss_ctc 115.475792 loss_rnnt 99.412155 hw_loss 0.040992 lr 0.00026804 rank 3
2023-02-17 14:09:18,458 DEBUG TRAIN Batch 0/6800 loss 91.291161 loss_att 204.275421 loss_ctc 91.532288 loss_rnnt 68.580528 hw_loss 0.153070 lr 0.00027204 rank 0
2023-02-17 14:09:18,461 DEBUG TRAIN Batch 0/6800 loss 121.270676 loss_att 223.480545 loss_ctc 117.863159 loss_rnnt 101.201492 hw_loss 0.152889 lr 0.00027204 rank 4
2023-02-17 14:09:18,461 DEBUG TRAIN Batch 0/6800 loss 89.014282 loss_att 205.507797 loss_ctc 84.140442 loss_rnnt 66.365356 hw_loss 0.000126 lr 0.00027204 rank 6
2023-02-17 14:09:18,462 DEBUG TRAIN Batch 0/6800 loss 76.269394 loss_att 148.844345 loss_ctc 73.150482 loss_rnnt 62.104156 hw_loss 0.123942 lr 0.00027204 rank 1
2023-02-17 14:09:18,464 DEBUG TRAIN Batch 0/6800 loss 100.925972 loss_att 197.771423 loss_ctc 97.668961 loss_rnnt 81.970688 hw_loss 0.038359 lr 0.00027204 rank 7
2023-02-17 14:09:18,466 DEBUG TRAIN Batch 0/6800 loss 86.847275 loss_att 187.949982 loss_ctc 80.464584 loss_rnnt 67.441925 hw_loss 0.067177 lr 0.00027204 rank 2
2023-02-17 14:09:18,471 DEBUG TRAIN Batch 0/6800 loss 104.189262 loss_att 213.887360 loss_ctc 103.702446 loss_rnnt 82.222565 hw_loss 0.172471 lr 0.00027204 rank 5
2023-02-17 14:09:18,470 DEBUG TRAIN Batch 0/6800 loss 171.215134 loss_att 291.783936 loss_ctc 164.869476 loss_rnnt 147.919830 hw_loss 0.051798 lr 0.00027204 rank 3
2023-02-17 14:10:30,923 DEBUG TRAIN Batch 0/6900 loss 102.878578 loss_att 184.071014 loss_ctc 101.131729 loss_rnnt 86.832413 hw_loss 0.076088 lr 0.00027604 rank 4
2023-02-17 14:10:30,926 DEBUG TRAIN Batch 0/6900 loss 93.549126 loss_att 158.128082 loss_ctc 94.342285 loss_rnnt 80.458908 hw_loss 0.128747 lr 0.00027604 rank 6
2023-02-17 14:10:30,927 DEBUG TRAIN Batch 0/6900 loss 70.781502 loss_att 166.481323 loss_ctc 67.448196 loss_rnnt 52.027649 hw_loss 0.109375 lr 0.00027604 rank 5
2023-02-17 14:10:30,927 DEBUG TRAIN Batch 0/6900 loss 119.768936 loss_att 237.365448 loss_ctc 118.194382 loss_rnnt 96.364807 hw_loss 0.177692 lr 0.00027604 rank 1
2023-02-17 14:10:30,930 DEBUG TRAIN Batch 0/6900 loss 89.917206 loss_att 188.024429 loss_ctc 90.638962 loss_rnnt 70.154900 hw_loss 0.083674 lr 0.00027604 rank 0
2023-02-17 14:10:30,933 DEBUG TRAIN Batch 0/6900 loss 105.808395 loss_att 224.988617 loss_ctc 102.751083 loss_rnnt 82.348198 hw_loss 0.059628 lr 0.00027604 rank 3
2023-02-17 14:10:30,938 DEBUG TRAIN Batch 0/6900 loss 93.446053 loss_att 191.664871 loss_ctc 88.217049 loss_rnnt 74.408478 hw_loss 0.170628 lr 0.00027604 rank 7
2023-02-17 14:10:30,974 DEBUG TRAIN Batch 0/6900 loss 89.385231 loss_att 174.700745 loss_ctc 102.329887 loss_rnnt 70.567146 hw_loss 0.054421 lr 0.00027604 rank 2
2023-02-17 14:11:44,472 DEBUG TRAIN Batch 0/7000 loss 94.254799 loss_att 170.626938 loss_ctc 98.973015 loss_rnnt 78.298927 hw_loss 0.098154 lr 0.00028004 rank 3
2023-02-17 14:11:44,474 DEBUG TRAIN Batch 0/7000 loss 104.853401 loss_att 253.984116 loss_ctc 88.829521 loss_rnnt 77.163727 hw_loss 0.000088 lr 0.00028004 rank 2
2023-02-17 14:11:44,482 DEBUG TRAIN Batch 0/7000 loss 40.410961 loss_att 68.305954 loss_ctc 41.921570 loss_rnnt 34.551121 hw_loss 0.148918 lr 0.00028004 rank 0
2023-02-17 14:11:44,483 DEBUG TRAIN Batch 0/7000 loss 68.437157 loss_att 150.980255 loss_ctc 73.024147 loss_rnnt 51.243141 hw_loss 0.138367 lr 0.00028004 rank 4
2023-02-17 14:11:44,487 DEBUG TRAIN Batch 0/7000 loss 90.344055 loss_att 224.246368 loss_ctc 83.603348 loss_rnnt 64.356857 hw_loss 0.197786 lr 0.00028004 rank 6
2023-02-17 14:11:44,489 DEBUG TRAIN Batch 0/7000 loss 137.735550 loss_att 291.181213 loss_ctc 138.125122 loss_rnnt 106.933708 hw_loss 0.113944 lr 0.00028004 rank 7
2023-02-17 14:11:44,493 DEBUG TRAIN Batch 0/7000 loss 150.238480 loss_att 260.714966 loss_ctc 140.046677 loss_rnnt 129.454102 hw_loss 0.090005 lr 0.00028004 rank 5
2023-02-17 14:11:44,529 DEBUG TRAIN Batch 0/7000 loss 82.529495 loss_att 195.464188 loss_ctc 84.158615 loss_rnnt 59.709732 hw_loss 0.029265 lr 0.00028004 rank 1
2023-02-17 14:12:59,084 DEBUG TRAIN Batch 0/7100 loss 95.241806 loss_att 216.577362 loss_ctc 90.595413 loss_rnnt 71.594170 hw_loss 0.000076 lr 0.00028404 rank 0
2023-02-17 14:12:59,084 DEBUG TRAIN Batch 0/7100 loss 67.917892 loss_att 169.452377 loss_ctc 66.862747 loss_rnnt 47.649147 hw_loss 0.192237 lr 0.00028404 rank 2
2023-02-17 14:12:59,084 DEBUG TRAIN Batch 0/7100 loss 127.593758 loss_att 249.792038 loss_ctc 133.575256 loss_rnnt 102.231400 hw_loss 0.234669 lr 0.00028404 rank 4
2023-02-17 14:12:59,085 DEBUG TRAIN Batch 0/7100 loss 114.498611 loss_att 242.093323 loss_ctc 106.982498 loss_rnnt 89.981766 hw_loss 0.000076 lr 0.00028404 rank 3
2023-02-17 14:12:59,086 DEBUG TRAIN Batch 0/7100 loss 113.138130 loss_att 244.618240 loss_ctc 109.050400 loss_rnnt 87.335693 hw_loss 0.096458 lr 0.00028404 rank 6
2023-02-17 14:12:59,090 DEBUG TRAIN Batch 0/7100 loss 93.011902 loss_att 203.972168 loss_ctc 96.534447 loss_rnnt 70.277458 hw_loss 0.136334 lr 0.00028404 rank 7
2023-02-17 14:12:59,109 DEBUG TRAIN Batch 0/7100 loss 110.930504 loss_att 226.185425 loss_ctc 119.032944 loss_rnnt 86.761841 hw_loss 0.070023 lr 0.00028404 rank 1
2023-02-17 14:12:59,135 DEBUG TRAIN Batch 0/7100 loss 102.270569 loss_att 238.307739 loss_ctc 94.222656 loss_rnnt 76.112793 hw_loss 0.043877 lr 0.00028404 rank 5
2023-02-17 14:14:12,147 DEBUG TRAIN Batch 0/7200 loss 101.605362 loss_att 216.439941 loss_ctc 97.784775 loss_rnnt 79.140327 hw_loss 0.014115 lr 0.00028804 rank 2
2023-02-17 14:14:12,151 DEBUG TRAIN Batch 0/7200 loss 93.826103 loss_att 190.888596 loss_ctc 92.914551 loss_rnnt 74.520294 hw_loss 0.027829 lr 0.00028804 rank 1
2023-02-17 14:14:12,153 DEBUG TRAIN Batch 0/7200 loss 111.158615 loss_att 218.997513 loss_ctc 103.366455 loss_rnnt 90.571152 hw_loss 0.109931 lr 0.00028804 rank 4
2023-02-17 14:14:12,154 DEBUG TRAIN Batch 0/7200 loss 148.082291 loss_att 273.278717 loss_ctc 158.115723 loss_rnnt 121.663963 hw_loss 0.077360 lr 0.00028804 rank 0
2023-02-17 14:14:12,155 DEBUG TRAIN Batch 0/7200 loss 81.763969 loss_att 181.450272 loss_ctc 79.915527 loss_rnnt 62.013107 hw_loss 0.112606 lr 0.00028804 rank 6
2023-02-17 14:14:12,158 DEBUG TRAIN Batch 0/7200 loss 100.559868 loss_att 197.475067 loss_ctc 104.387192 loss_rnnt 80.657974 hw_loss 0.016030 lr 0.00028804 rank 5
2023-02-17 14:14:12,161 DEBUG TRAIN Batch 0/7200 loss 91.481110 loss_att 220.980408 loss_ctc 80.084282 loss_rnnt 67.017403 hw_loss 0.156399 lr 0.00028804 rank 7
2023-02-17 14:14:12,206 DEBUG TRAIN Batch 0/7200 loss 83.341995 loss_att 198.934082 loss_ctc 73.752708 loss_rnnt 61.462704 hw_loss 0.073974 lr 0.00028804 rank 3
2023-02-17 14:15:25,419 DEBUG TRAIN Batch 0/7300 loss 108.507263 loss_att 225.150421 loss_ctc 110.143089 loss_rnnt 84.939880 hw_loss 0.038688 lr 0.00029204 rank 4
2023-02-17 14:15:25,420 DEBUG TRAIN Batch 0/7300 loss 120.206993 loss_att 237.172852 loss_ctc 118.566757 loss_rnnt 97.004570 hw_loss 0.052409 lr 0.00029204 rank 2
2023-02-17 14:15:25,423 DEBUG TRAIN Batch 0/7300 loss 97.255119 loss_att 208.700211 loss_ctc 96.924728 loss_rnnt 74.948730 hw_loss 0.115157 lr 0.00029204 rank 0
2023-02-17 14:15:25,428 DEBUG TRAIN Batch 0/7300 loss 94.846397 loss_att 210.232422 loss_ctc 95.353447 loss_rnnt 71.666862 hw_loss 0.065102 lr 0.00029204 rank 3
2023-02-17 14:15:25,429 DEBUG TRAIN Batch 0/7300 loss 76.625542 loss_att 188.101639 loss_ctc 73.392914 loss_rnnt 54.700272 hw_loss 0.114496 lr 0.00029204 rank 1
2023-02-17 14:15:25,429 DEBUG TRAIN Batch 0/7300 loss 106.591965 loss_att 224.720703 loss_ctc 105.517822 loss_rnnt 83.086838 hw_loss 0.042365 lr 0.00029204 rank 6
2023-02-17 14:15:25,431 DEBUG TRAIN Batch 0/7300 loss 98.485985 loss_att 198.895126 loss_ctc 105.228874 loss_rnnt 77.490662 hw_loss 0.027066 lr 0.00029204 rank 7
2023-02-17 14:15:25,433 DEBUG TRAIN Batch 0/7300 loss 105.868202 loss_att 214.910553 loss_ctc 114.798248 loss_rnnt 82.815247 hw_loss 0.100883 lr 0.00029204 rank 5
2023-02-17 14:16:38,801 DEBUG TRAIN Batch 0/7400 loss 102.121857 loss_att 235.037750 loss_ctc 100.980331 loss_rnnt 75.640663 hw_loss 0.094154 lr 0.00029604 rank 3
2023-02-17 14:16:38,801 DEBUG TRAIN Batch 0/7400 loss 120.934135 loss_att 217.866104 loss_ctc 128.749619 loss_rnnt 100.469193 hw_loss 0.068391 lr 0.00029604 rank 5
2023-02-17 14:16:38,804 DEBUG TRAIN Batch 0/7400 loss 115.412888 loss_att 219.750931 loss_ctc 120.503319 loss_rnnt 93.805573 hw_loss 0.114321 lr 0.00029604 rank 6
2023-02-17 14:16:38,806 DEBUG TRAIN Batch 0/7400 loss 128.688446 loss_att 259.375092 loss_ctc 124.284721 loss_rnnt 103.092323 hw_loss 0.086153 lr 0.00029604 rank 4
2023-02-17 14:16:38,808 DEBUG TRAIN Batch 0/7400 loss 72.676254 loss_att 162.434921 loss_ctc 67.189804 loss_rnnt 55.406506 hw_loss 0.092888 lr 0.00029604 rank 1
2023-02-17 14:16:38,808 DEBUG TRAIN Batch 0/7400 loss 122.655556 loss_att 201.568588 loss_ctc 119.553200 loss_rnnt 107.220619 hw_loss 0.123695 lr 0.00029604 rank 7
2023-02-17 14:16:38,811 DEBUG TRAIN Batch 0/7400 loss 111.716507 loss_att 225.162964 loss_ctc 114.734253 loss_rnnt 88.624329 hw_loss 0.000963 lr 0.00029604 rank 0
2023-02-17 14:16:38,855 DEBUG TRAIN Batch 0/7400 loss 130.817795 loss_att 239.726166 loss_ctc 126.970459 loss_rnnt 109.477798 hw_loss 0.133649 lr 0.00029604 rank 2
2023-02-17 14:17:53,734 DEBUG TRAIN Batch 0/7500 loss 74.508751 loss_att 163.287277 loss_ctc 73.374916 loss_rnnt 56.853333 hw_loss 0.095431 lr 0.00030004 rank 6
2023-02-17 14:17:53,736 DEBUG TRAIN Batch 0/7500 loss 114.285675 loss_att 219.395966 loss_ctc 110.687637 loss_rnnt 93.674423 hw_loss 0.129250 lr 0.00030004 rank 0
2023-02-17 14:17:53,737 DEBUG TRAIN Batch 0/7500 loss 84.703384 loss_att 155.302612 loss_ctc 85.691795 loss_rnnt 70.380959 hw_loss 0.132731 lr 0.00030004 rank 7
2023-02-17 14:17:53,737 DEBUG TRAIN Batch 0/7500 loss 76.220398 loss_att 180.425735 loss_ctc 76.811737 loss_rnnt 55.254906 hw_loss 0.085449 lr 0.00030004 rank 2
2023-02-17 14:17:53,738 DEBUG TRAIN Batch 0/7500 loss 91.489967 loss_att 210.251892 loss_ctc 89.689445 loss_rnnt 67.947075 hw_loss 0.057327 lr 0.00030004 rank 4
2023-02-17 14:17:53,739 DEBUG TRAIN Batch 0/7500 loss 116.349380 loss_att 235.823578 loss_ctc 111.759163 loss_rnnt 93.035637 hw_loss 0.057983 lr 0.00030004 rank 3
2023-02-17 14:17:53,739 DEBUG TRAIN Batch 0/7500 loss 111.769485 loss_att 241.400253 loss_ctc 119.209015 loss_rnnt 84.851341 hw_loss 0.000116 lr 0.00030004 rank 1
2023-02-17 14:17:53,743 DEBUG TRAIN Batch 0/7500 loss 63.829838 loss_att 139.308945 loss_ctc 65.555542 loss_rnnt 48.425541 hw_loss 0.146960 lr 0.00030004 rank 5
2023-02-17 14:19:08,736 DEBUG TRAIN Batch 0/7600 loss 77.932251 loss_att 189.631897 loss_ctc 71.307312 loss_rnnt 56.436241 hw_loss 0.073884 lr 0.00030404 rank 4
2023-02-17 14:19:08,738 DEBUG TRAIN Batch 0/7600 loss 61.934566 loss_att 100.506226 loss_ctc 62.738613 loss_rnnt 54.017612 hw_loss 0.178903 lr 0.00030404 rank 2
2023-02-17 14:19:08,742 DEBUG TRAIN Batch 0/7600 loss 104.248283 loss_att 199.076599 loss_ctc 106.073540 loss_rnnt 84.994576 hw_loss 0.083767 lr 0.00030404 rank 3
2023-02-17 14:19:08,744 DEBUG TRAIN Batch 0/7600 loss 81.613495 loss_att 139.572662 loss_ctc 83.540405 loss_rnnt 69.687981 hw_loss 0.143927 lr 0.00030404 rank 5
2023-02-17 14:19:08,744 DEBUG TRAIN Batch 0/7600 loss 38.553692 loss_att 55.877876 loss_ctc 42.030319 loss_rnnt 34.554321 hw_loss 0.133093 lr 0.00030404 rank 6
2023-02-17 14:19:08,744 DEBUG TRAIN Batch 0/7600 loss 58.282837 loss_att 159.260254 loss_ctc 57.806847 loss_rnnt 38.108047 hw_loss 0.080190 lr 0.00030404 rank 1
2023-02-17 14:19:08,747 DEBUG TRAIN Batch 0/7600 loss 64.262375 loss_att 119.928169 loss_ctc 63.411797 loss_rnnt 53.155033 hw_loss 0.164239 lr 0.00030404 rank 0
2023-02-17 14:19:08,747 DEBUG TRAIN Batch 0/7600 loss 70.738251 loss_att 124.318825 loss_ctc 75.527748 loss_rnnt 59.293015 hw_loss 0.169717 lr 0.00030404 rank 7
2023-02-17 14:20:21,314 DEBUG TRAIN Batch 0/7700 loss 102.801155 loss_att 216.043732 loss_ctc 101.169128 loss_rnnt 80.370163 hw_loss 0.000154 lr 0.00030804 rank 1
2023-02-17 14:20:21,314 DEBUG TRAIN Batch 0/7700 loss 46.544773 loss_att 87.743973 loss_ctc 46.683750 loss_rnnt 38.244118 hw_loss 0.079278 lr 0.00030804 rank 4
2023-02-17 14:20:21,316 DEBUG TRAIN Batch 0/7700 loss 84.058388 loss_att 195.595001 loss_ctc 84.763947 loss_rnnt 61.631325 hw_loss 0.048126 lr 0.00030804 rank 6
2023-02-17 14:20:21,316 DEBUG TRAIN Batch 0/7700 loss 75.883965 loss_att 181.159698 loss_ctc 68.825714 loss_rnnt 55.745861 hw_loss 0.045102 lr 0.00030804 rank 5
2023-02-17 14:20:21,319 DEBUG TRAIN Batch 0/7700 loss 89.287796 loss_att 199.066269 loss_ctc 84.838737 loss_rnnt 67.905418 hw_loss 0.037282 lr 0.00030804 rank 2
2023-02-17 14:20:21,322 DEBUG TRAIN Batch 0/7700 loss 123.505547 loss_att 245.806976 loss_ctc 128.225708 loss_rnnt 98.342407 hw_loss 0.137807 lr 0.00030804 rank 0
2023-02-17 14:20:21,322 DEBUG TRAIN Batch 0/7700 loss 120.414696 loss_att 241.131195 loss_ctc 123.171043 loss_rnnt 95.855797 hw_loss 0.090167 lr 0.00030804 rank 7
2023-02-17 14:20:21,327 DEBUG TRAIN Batch 0/7700 loss 50.731922 loss_att 89.570503 loss_ctc 53.605583 loss_rnnt 42.509201 hw_loss 0.134714 lr 0.00030804 rank 3
2023-02-17 14:21:35,583 DEBUG TRAIN Batch 0/7800 loss 94.956879 loss_att 179.702866 loss_ctc 103.128807 loss_rnnt 76.917549 hw_loss 0.001013 lr 0.00031204 rank 2
2023-02-17 14:21:35,589 DEBUG TRAIN Batch 0/7800 loss 92.607689 loss_att 217.077118 loss_ctc 86.214500 loss_rnnt 68.550392 hw_loss 0.029674 lr 0.00031204 rank 3
2023-02-17 14:21:35,589 DEBUG TRAIN Batch 0/7800 loss 126.261536 loss_att 244.726532 loss_ctc 134.635910 loss_rnnt 101.440613 hw_loss 0.021249 lr 0.00031204 rank 1
2023-02-17 14:21:35,589 DEBUG TRAIN Batch 0/7800 loss 102.628365 loss_att 246.388931 loss_ctc 95.916611 loss_rnnt 74.755386 hw_loss 0.029546 lr 0.00031204 rank 0
2023-02-17 14:21:35,590 DEBUG TRAIN Batch 0/7800 loss 75.295654 loss_att 180.925064 loss_ctc 80.270950 loss_rnnt 53.406158 hw_loss 0.187946 lr 0.00031204 rank 6
2023-02-17 14:21:35,598 DEBUG TRAIN Batch 0/7800 loss 78.934303 loss_att 179.183243 loss_ctc 76.012222 loss_rnnt 59.223049 hw_loss 0.095765 lr 0.00031204 rank 7
2023-02-17 14:21:35,614 DEBUG TRAIN Batch 0/7800 loss 100.706070 loss_att 191.798676 loss_ctc 96.657257 loss_rnnt 82.931274 hw_loss 0.180201 lr 0.00031204 rank 5
2023-02-17 14:21:35,625 DEBUG TRAIN Batch 0/7800 loss 82.002739 loss_att 196.876526 loss_ctc 81.608032 loss_rnnt 59.078655 hw_loss 0.003654 lr 0.00031204 rank 4
2023-02-17 14:22:49,460 DEBUG TRAIN Batch 0/7900 loss 114.550644 loss_att 232.834396 loss_ctc 121.851273 loss_rnnt 89.875748 hw_loss 0.083852 lr 0.00031604 rank 2
2023-02-17 14:22:49,466 DEBUG TRAIN Batch 0/7900 loss 100.194054 loss_att 209.090668 loss_ctc 103.251701 loss_rnnt 77.949570 hw_loss 0.107762 lr 0.00031604 rank 6
2023-02-17 14:22:49,466 DEBUG TRAIN Batch 0/7900 loss 76.364998 loss_att 185.976654 loss_ctc 80.224419 loss_rnnt 53.862366 hw_loss 0.123193 lr 0.00031604 rank 0
2023-02-17 14:22:49,466 DEBUG TRAIN Batch 0/7900 loss 84.945457 loss_att 191.099609 loss_ctc 85.349052 loss_rnnt 63.637604 hw_loss 0.043527 lr 0.00031604 rank 5
2023-02-17 14:22:49,467 DEBUG TRAIN Batch 0/7900 loss 75.471947 loss_att 189.632385 loss_ctc 70.621307 loss_rnnt 53.246830 hw_loss 0.074593 lr 0.00031604 rank 1
2023-02-17 14:22:49,469 DEBUG TRAIN Batch 0/7900 loss 73.819893 loss_att 180.229401 loss_ctc 81.084557 loss_rnnt 51.569275 hw_loss 0.000165 lr 0.00031604 rank 4
2023-02-17 14:22:49,470 DEBUG TRAIN Batch 0/7900 loss 141.442017 loss_att 252.906769 loss_ctc 142.473694 loss_rnnt 118.950500 hw_loss 0.114403 lr 0.00031604 rank 3
2023-02-17 14:22:49,515 DEBUG TRAIN Batch 0/7900 loss 87.802963 loss_att 201.047974 loss_ctc 88.335258 loss_rnnt 65.047211 hw_loss 0.067090 lr 0.00031604 rank 7
2023-02-17 14:24:02,120 DEBUG TRAIN Batch 0/8000 loss 80.952835 loss_att 191.807007 loss_ctc 74.404327 loss_rnnt 59.586288 hw_loss 0.129095 lr 0.00032004 rank 2
2023-02-17 14:24:02,124 DEBUG TRAIN Batch 0/8000 loss 63.440235 loss_att 151.226181 loss_ctc 67.431152 loss_rnnt 45.321655 hw_loss 0.054887 lr 0.00032004 rank 7
2023-02-17 14:24:02,128 DEBUG TRAIN Batch 0/8000 loss 111.625702 loss_att 222.196396 loss_ctc 113.618195 loss_rnnt 89.235001 hw_loss 0.020432 lr 0.00032004 rank 4
2023-02-17 14:24:02,134 DEBUG TRAIN Batch 0/8000 loss 94.408760 loss_att 212.776855 loss_ctc 96.157639 loss_rnnt 70.461243 hw_loss 0.076341 lr 0.00032004 rank 6
2023-02-17 14:24:02,135 DEBUG TRAIN Batch 0/8000 loss 102.439125 loss_att 219.757248 loss_ctc 105.845779 loss_rnnt 78.419006 hw_loss 0.191745 lr 0.00032004 rank 0
2023-02-17 14:24:02,136 DEBUG TRAIN Batch 0/8000 loss 97.754807 loss_att 210.579361 loss_ctc 103.082474 loss_rnnt 74.479424 hw_loss 0.000215 lr 0.00032004 rank 3
2023-02-17 14:24:02,139 DEBUG TRAIN Batch 0/8000 loss 79.620956 loss_att 178.768661 loss_ctc 77.774483 loss_rnnt 60.001755 hw_loss 0.067236 lr 0.00032004 rank 5
2023-02-17 14:24:02,182 DEBUG TRAIN Batch 0/8000 loss 74.835915 loss_att 141.422195 loss_ctc 84.026962 loss_rnnt 60.223671 hw_loss 0.130326 lr 0.00032004 rank 1
2023-02-17 14:25:15,085 DEBUG TRAIN Batch 0/8100 loss 84.802216 loss_att 173.027206 loss_ctc 85.660172 loss_rnnt 66.989639 hw_loss 0.099722 lr 0.00032404 rank 6
2023-02-17 14:25:15,086 DEBUG TRAIN Batch 0/8100 loss 91.358223 loss_att 204.989609 loss_ctc 93.466331 loss_rnnt 68.323296 hw_loss 0.051683 lr 0.00032404 rank 1
2023-02-17 14:25:15,087 DEBUG TRAIN Batch 0/8100 loss 82.775841 loss_att 180.158295 loss_ctc 85.676788 loss_rnnt 62.863197 hw_loss 0.092543 lr 0.00032404 rank 7
2023-02-17 14:25:15,087 DEBUG TRAIN Batch 0/8100 loss 67.281929 loss_att 145.955475 loss_ctc 73.409622 loss_rnnt 50.673836 hw_loss 0.105659 lr 0.00032404 rank 0
2023-02-17 14:25:15,091 DEBUG TRAIN Batch 0/8100 loss 77.026802 loss_att 172.493698 loss_ctc 77.260231 loss_rnnt 57.876587 hw_loss 0.048201 lr 0.00032404 rank 5
2023-02-17 14:25:15,092 DEBUG TRAIN Batch 0/8100 loss 97.246384 loss_att 213.428680 loss_ctc 105.988297 loss_rnnt 72.819740 hw_loss 0.046101 lr 0.00032404 rank 4
2023-02-17 14:25:15,110 DEBUG TRAIN Batch 0/8100 loss 67.019218 loss_att 167.609665 loss_ctc 67.609764 loss_rnnt 46.802528 hw_loss 0.037225 lr 0.00032404 rank 2
2023-02-17 14:25:15,135 DEBUG TRAIN Batch 0/8100 loss 78.227913 loss_att 158.214142 loss_ctc 74.380905 loss_rnnt 62.728722 hw_loss 0.027914 lr 0.00032404 rank 3
2023-02-17 14:26:28,503 DEBUG TRAIN Batch 0/8200 loss 82.400047 loss_att 143.863937 loss_ctc 86.941032 loss_rnnt 69.410973 hw_loss 0.170299 lr 0.00032804 rank 6
2023-02-17 14:26:28,503 DEBUG TRAIN Batch 0/8200 loss 120.250702 loss_att 219.726471 loss_ctc 118.851463 loss_rnnt 100.495422 hw_loss 0.087543 lr 0.00032804 rank 4
2023-02-17 14:26:28,504 DEBUG TRAIN Batch 0/8200 loss 85.796638 loss_att 145.398911 loss_ctc 100.739212 loss_rnnt 71.775360 hw_loss 0.203391 lr 0.00032804 rank 0
2023-02-17 14:26:28,505 DEBUG TRAIN Batch 0/8200 loss 90.921783 loss_att 183.699783 loss_ctc 94.737999 loss_rnnt 71.838760 hw_loss 0.034870 lr 0.00032804 rank 3
2023-02-17 14:26:28,506 DEBUG TRAIN Batch 0/8200 loss 53.939194 loss_att 134.485336 loss_ctc 50.261723 loss_rnnt 38.268295 hw_loss 0.097490 lr 0.00032804 rank 5
2023-02-17 14:26:28,508 DEBUG TRAIN Batch 0/8200 loss 87.786308 loss_att 186.439911 loss_ctc 87.845062 loss_rnnt 67.988480 hw_loss 0.111135 lr 0.00032804 rank 1
2023-02-17 14:26:28,510 DEBUG TRAIN Batch 0/8200 loss 54.073017 loss_att 109.663193 loss_ctc 58.190071 loss_rnnt 42.384338 hw_loss 0.040691 lr 0.00032804 rank 7
2023-02-17 14:26:28,556 DEBUG TRAIN Batch 0/8200 loss 59.559906 loss_att 107.486786 loss_ctc 61.106342 loss_rnnt 49.710220 hw_loss 0.108969 lr 0.00032804 rank 2
2023-02-17 14:27:40,761 DEBUG TRAIN Batch 0/8300 loss 81.422104 loss_att 186.225388 loss_ctc 80.534409 loss_rnnt 60.558605 hw_loss 0.039741 lr 0.00033204 rank 0
2023-02-17 14:27:40,774 DEBUG TRAIN Batch 0/8300 loss 105.606300 loss_att 199.360321 loss_ctc 112.276978 loss_rnnt 85.900520 hw_loss 0.122889 lr 0.00033204 rank 4
2023-02-17 14:27:40,779 DEBUG TRAIN Batch 0/8300 loss 108.116776 loss_att 222.177673 loss_ctc 111.794182 loss_rnnt 84.814194 hw_loss 0.000154 lr 0.00033204 rank 1
2023-02-17 14:27:40,781 DEBUG TRAIN Batch 0/8300 loss 82.764366 loss_att 183.702240 loss_ctc 85.723717 loss_rnnt 62.144356 hw_loss 0.070980 lr 0.00033204 rank 6
2023-02-17 14:27:40,786 DEBUG TRAIN Batch 0/8300 loss 85.060211 loss_att 182.740051 loss_ctc 83.718964 loss_rnnt 65.669281 hw_loss 0.063365 lr 0.00033204 rank 2
2023-02-17 14:27:40,787 DEBUG TRAIN Batch 0/8300 loss 77.715088 loss_att 161.227020 loss_ctc 87.839165 loss_rnnt 59.629883 hw_loss 0.061761 lr 0.00033204 rank 3
2023-02-17 14:27:40,795 DEBUG TRAIN Batch 0/8300 loss 88.725288 loss_att 189.443588 loss_ctc 80.322891 loss_rnnt 69.649597 hw_loss 0.098158 lr 0.00033204 rank 7
2023-02-17 14:27:40,806 DEBUG TRAIN Batch 0/8300 loss 73.481796 loss_att 177.807037 loss_ctc 69.881828 loss_rnnt 53.093517 hw_loss 0.006046 lr 0.00033204 rank 5
2023-02-17 14:28:42,348 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 4
2023-02-17 14:28:42,353 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 6
2023-02-17 14:28:42,359 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 7
2023-02-17 14:28:42,361 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 5
2023-02-17 14:28:42,362 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 3
2023-02-17 14:28:42,369 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 2
2023-02-17 14:28:42,370 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 0
2023-02-17 14:28:42,382 DEBUG CV Batch 0/0 loss 14.997763 loss_att 24.241177 loss_ctc 17.112810 loss_rnnt 12.698989 hw_loss 0.315161 history loss 14.442290 rank 1
2023-02-17 14:28:53,587 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 4
2023-02-17 14:28:53,607 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 0
2023-02-17 14:28:53,708 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 6
2023-02-17 14:28:54,087 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 2
2023-02-17 14:28:54,107 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 1
2023-02-17 14:28:54,175 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 3
2023-02-17 14:28:54,177 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 7
2023-02-17 14:28:54,230 DEBUG CV Batch 0/100 loss 68.073570 loss_att 125.430969 loss_ctc 73.754211 loss_rnnt 55.775906 hw_loss 0.128941 history loss 39.004920 rank 5
2023-02-17 14:29:06,970 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 4
2023-02-17 14:29:07,008 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 0
2023-02-17 14:29:07,052 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 6
2023-02-17 14:29:07,626 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 1
2023-02-17 14:29:07,841 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 3
2023-02-17 14:29:07,985 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 2
2023-02-17 14:29:08,066 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 7
2023-02-17 14:29:08,165 DEBUG CV Batch 0/200 loss 149.295181 loss_att 343.883698 loss_ctc 139.644287 loss_rnnt 111.632927 hw_loss 0.058738 history loss 44.060432 rank 5
2023-02-17 14:29:18,955 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 4
2023-02-17 14:29:19,098 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 6
2023-02-17 14:29:19,185 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 0
2023-02-17 14:29:19,939 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 1
2023-02-17 14:29:20,282 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 3
2023-02-17 14:29:20,347 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 2
2023-02-17 14:29:20,659 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 5
2023-02-17 14:29:20,761 DEBUG CV Batch 0/300 loss 48.595188 loss_att 91.946327 loss_ctc 52.503498 loss_rnnt 39.403793 hw_loss 0.000101 history loss 43.241889 rank 7
2023-02-17 14:29:30,925 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 4
2023-02-17 14:29:31,045 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 6
2023-02-17 14:29:31,275 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 0
2023-02-17 14:29:32,004 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 1
2023-02-17 14:29:32,475 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 3
2023-02-17 14:29:32,555 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 2
2023-02-17 14:29:32,947 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 7
2023-02-17 14:29:33,376 DEBUG CV Batch 0/400 loss 203.503952 loss_att 494.606506 loss_ctc 172.521667 loss_rnnt 149.414352 hw_loss 0.000101 history loss 45.963091 rank 5
2023-02-17 14:29:41,621 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 4
2023-02-17 14:29:41,692 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 6
2023-02-17 14:29:42,028 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 0
2023-02-17 14:29:42,763 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 1
2023-02-17 14:29:43,409 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 3
2023-02-17 14:29:43,579 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 2
2023-02-17 14:29:44,046 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 7
2023-02-17 14:29:44,336 DEBUG CV Batch 0/500 loss 71.614861 loss_att 129.605560 loss_ctc 77.755898 loss_rnnt 59.186108 hw_loss 0.022146 history loss 46.278322 rank 5
2023-02-17 14:29:53,789 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 6
2023-02-17 14:29:53,864 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 4
2023-02-17 14:29:54,263 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 0
2023-02-17 14:29:55,277 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 1
2023-02-17 14:29:55,644 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 3
2023-02-17 14:29:56,234 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 2
2023-02-17 14:29:56,372 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 7
2023-02-17 14:29:56,696 DEBUG CV Batch 0/600 loss 34.165672 loss_att 51.891426 loss_ctc 39.329113 loss_rnnt 29.796535 hw_loss 0.254118 history loss 47.977779 rank 5
2023-02-17 14:30:05,097 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 6
2023-02-17 14:30:05,150 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 4
2023-02-17 14:30:05,786 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 0
2023-02-17 14:30:06,707 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 1
2023-02-17 14:30:07,193 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 3
2023-02-17 14:30:08,035 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 2
2023-02-17 14:30:08,205 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 7
2023-02-17 14:30:08,319 DEBUG CV Batch 0/700 loss 191.350647 loss_att 421.207489 loss_ctc 201.801926 loss_rnnt 143.985718 hw_loss 0.000101 history loss 48.879397 rank 5
2023-02-17 14:30:16,319 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 4
2023-02-17 14:30:16,337 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 6
2023-02-17 14:30:17,171 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 0
2023-02-17 14:30:18,094 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 1
2023-02-17 14:30:18,749 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 3
2023-02-17 14:30:19,781 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 5
2023-02-17 14:30:19,888 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 7
2023-02-17 14:30:19,896 DEBUG CV Batch 0/800 loss 68.177368 loss_att 125.463264 loss_ctc 75.373436 loss_rnnt 55.643963 hw_loss 0.218906 history loss 47.434154 rank 2
2023-02-17 14:30:29,476 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 4
2023-02-17 14:30:29,614 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 6
2023-02-17 14:30:30,598 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 0
2023-02-17 14:30:31,564 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 1
2023-02-17 14:30:32,234 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 3
2023-02-17 14:30:33,300 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 5
2023-02-17 14:30:33,322 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 7
2023-02-17 14:30:33,699 DEBUG CV Batch 0/900 loss 116.139671 loss_att 293.636047 loss_ctc 119.083214 loss_rnnt 80.172348 hw_loss 0.141689 history loss 47.710978 rank 2
2023-02-17 14:30:41,544 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 4
2023-02-17 14:30:41,722 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 6
2023-02-17 14:30:43,003 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 0
2023-02-17 14:30:43,969 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 1
2023-02-17 14:30:45,011 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 3
2023-02-17 14:30:45,739 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 5
2023-02-17 14:30:46,086 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 7
2023-02-17 14:30:46,380 DEBUG CV Batch 0/1000 loss 37.790825 loss_att 78.395988 loss_ctc 39.007164 loss_rnnt 29.425156 hw_loss 0.154614 history loss 47.187696 rank 2
2023-02-17 14:30:53,350 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 4
2023-02-17 14:30:53,578 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 6
2023-02-17 14:30:55,058 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 0
2023-02-17 14:30:55,880 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 1
2023-02-17 14:30:57,255 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 3
2023-02-17 14:30:58,145 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 7
2023-02-17 14:30:58,498 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 5
2023-02-17 14:30:58,810 DEBUG CV Batch 0/1100 loss 19.104052 loss_att 27.174290 loss_ctc 22.784697 loss_rnnt 16.837156 hw_loss 0.303927 history loss 47.332246 rank 2
2023-02-17 14:31:04,002 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 4
2023-02-17 14:31:04,331 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 6
2023-02-17 14:31:05,934 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 0
2023-02-17 14:31:06,563 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 1
2023-02-17 14:31:08,193 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 3
2023-02-17 14:31:09,330 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 7
2023-02-17 14:31:09,462 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 5
2023-02-17 14:31:10,002 DEBUG CV Batch 0/1200 loss 77.301781 loss_att 141.640381 loss_ctc 83.870232 loss_rnnt 63.469238 hw_loss 0.166929 history loss 47.733208 rank 2
2023-02-17 14:31:15,796 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 4
2023-02-17 14:31:16,257 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 6
2023-02-17 14:31:18,029 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 0
2023-02-17 14:31:18,648 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 1
2023-02-17 14:31:20,516 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 3
2023-02-17 14:31:21,622 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 7
2023-02-17 14:31:21,727 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 5
2023-02-17 14:31:22,754 DEBUG CV Batch 0/1300 loss 32.227440 loss_att 49.808563 loss_ctc 37.918526 loss_rnnt 27.832666 hw_loss 0.224500 history loss 48.236844 rank 2
2023-02-17 14:31:26,895 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 4
2023-02-17 14:31:27,508 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 6
2023-02-17 14:31:29,440 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 0
2023-02-17 14:31:29,861 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 1
2023-02-17 14:31:31,874 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 3
2023-02-17 14:31:33,252 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 5
2023-02-17 14:31:33,648 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 7
2023-02-17 14:31:34,639 DEBUG CV Batch 0/1400 loss 150.080338 loss_att 331.218750 loss_ctc 145.561813 loss_rnnt 114.455063 hw_loss 0.000101 history loss 48.772727 rank 2
2023-02-17 14:31:38,387 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 4
2023-02-17 14:31:39,083 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 6
2023-02-17 14:31:41,234 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 0
2023-02-17 14:31:41,470 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 1
2023-02-17 14:31:43,733 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 3
2023-02-17 14:31:45,174 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 5
2023-02-17 14:31:45,424 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 7
2023-02-17 14:31:46,653 DEBUG CV Batch 0/1500 loss 66.022285 loss_att 131.444855 loss_ctc 66.867691 loss_rnnt 52.773129 hw_loss 0.097361 history loss 48.080547 rank 2
2023-02-17 14:31:51,273 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 4
2023-02-17 14:31:52,102 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 6
2023-02-17 14:31:54,421 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 0
2023-02-17 14:31:54,512 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 1
2023-02-17 14:31:57,081 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 3
2023-02-17 14:31:58,486 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 5
2023-02-17 14:31:58,668 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 7
2023-02-17 14:32:00,302 DEBUG CV Batch 0/1600 loss 112.258766 loss_att 306.406464 loss_ctc 105.598442 loss_rnnt 74.317215 hw_loss 0.000101 history loss 48.262645 rank 2
2023-02-17 14:32:03,538 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 4
2023-02-17 14:32:04,389 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 6
2023-02-17 14:32:06,961 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 0
2023-02-17 14:32:06,965 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 1
2023-02-17 14:32:09,602 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 3
2023-02-17 14:32:10,816 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 5
2023-02-17 14:32:11,114 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 7
2023-02-17 14:32:12,530 INFO Epoch 0 CV info cv_loss 48.11881301002982
2023-02-17 14:32:12,530 INFO Epoch 1 TRAIN info lr 0.00033304
2023-02-17 14:32:12,532 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:13,153 DEBUG CV Batch 0/1700 loss 57.081017 loss_att 100.246758 loss_ctc 64.425064 loss_rnnt 47.404037 hw_loss 0.121179 history loss 47.944729 rank 2
2023-02-17 14:32:13,495 INFO Epoch 0 CV info cv_loss 48.1188130067218
2023-02-17 14:32:13,495 INFO Epoch 1 TRAIN info lr 0.00033383999999999996
2023-02-17 14:32:13,500 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:16,037 INFO Epoch 0 CV info cv_loss 48.1188130067218
2023-02-17 14:32:16,038 INFO Epoch 1 TRAIN info lr 0.00033516000000000004
2023-02-17 14:32:16,043 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:16,121 INFO Epoch 0 CV info cv_loss 48.11881300658397
2023-02-17 14:32:16,122 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word/0.pt
2023-02-17 14:32:16,708 INFO Epoch 1 TRAIN info lr 0.00033371999999999997
2023-02-17 14:32:16,712 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:18,815 INFO Epoch 0 CV info cv_loss 48.11881300816906
2023-02-17 14:32:18,816 INFO Epoch 1 TRAIN info lr 0.00033312
2023-02-17 14:32:18,821 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:19,962 INFO Epoch 0 CV info cv_loss 48.11881301306217
2023-02-17 14:32:19,962 INFO Epoch 1 TRAIN info lr 0.00033348
2023-02-17 14:32:19,964 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:20,298 INFO Epoch 0 CV info cv_loss 48.1188130038962
2023-02-17 14:32:20,299 INFO Epoch 1 TRAIN info lr 0.00033392000000000003
2023-02-17 14:32:20,301 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:32:22,809 INFO Epoch 0 CV info cv_loss 48.11881300837581
2023-02-17 14:32:22,810 INFO Epoch 1 TRAIN info lr 0.00033395999999999995
2023-02-17 14:32:22,813 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:33:24,074 DEBUG TRAIN Batch 1/0 loss 27.224705 loss_att 35.610081 loss_ctc 30.679390 loss_rnnt 24.953356 hw_loss 0.250590 lr 0.00033520 rank 1
2023-02-17 14:33:24,076 DEBUG TRAIN Batch 1/0 loss 27.249168 loss_att 39.570671 loss_ctc 29.020187 loss_rnnt 24.431137 hw_loss 0.220488 lr 0.00033376 rank 0
2023-02-17 14:33:24,077 DEBUG TRAIN Batch 1/0 loss 26.192438 loss_att 36.966530 loss_ctc 26.550785 loss_rnnt 23.831947 hw_loss 0.296048 lr 0.00033316 rank 3
2023-02-17 14:33:24,077 DEBUG TRAIN Batch 1/0 loss 29.968941 loss_att 39.613659 loss_ctc 33.132236 loss_rnnt 27.531170 hw_loss 0.163231 lr 0.00033388 rank 6
2023-02-17 14:33:24,079 DEBUG TRAIN Batch 1/0 loss 29.872814 loss_att 38.294426 loss_ctc 32.250038 loss_rnnt 27.749369 hw_loss 0.229049 lr 0.00033308 rank 4
2023-02-17 14:33:24,095 DEBUG TRAIN Batch 1/0 loss 33.397793 loss_att 44.406178 loss_ctc 38.237530 loss_rnnt 30.436153 hw_loss 0.214992 lr 0.00033352 rank 5
2023-02-17 14:33:24,102 DEBUG TRAIN Batch 1/0 loss 27.534994 loss_att 36.727551 loss_ctc 31.359478 loss_rnnt 25.085115 hw_loss 0.190189 lr 0.00033396 rank 7
2023-02-17 14:33:24,127 DEBUG TRAIN Batch 1/0 loss 28.217684 loss_att 37.908165 loss_ctc 30.684860 loss_rnnt 25.832277 hw_loss 0.221915 lr 0.00033400 rank 2
2023-02-17 14:34:35,471 DEBUG TRAIN Batch 1/100 loss 99.400711 loss_att 216.564758 loss_ctc 101.987968 loss_rnnt 75.551529 hw_loss 0.133900 lr 0.00033800 rank 2
2023-02-17 14:34:35,472 DEBUG TRAIN Batch 1/100 loss 93.496788 loss_att 206.761322 loss_ctc 89.617264 loss_rnnt 71.301849 hw_loss 0.111185 lr 0.00033776 rank 0
2023-02-17 14:34:35,472 DEBUG TRAIN Batch 1/100 loss 92.005867 loss_att 189.656174 loss_ctc 92.531723 loss_rnnt 72.389580 hw_loss 0.030215 lr 0.00033708 rank 4
2023-02-17 14:34:35,474 DEBUG TRAIN Batch 1/100 loss 89.679619 loss_att 178.795853 loss_ctc 83.939941 loss_rnnt 72.597374 hw_loss 0.045553 lr 0.00033796 rank 7
2023-02-17 14:34:35,474 DEBUG TRAIN Batch 1/100 loss 91.197861 loss_att 190.617554 loss_ctc 102.629257 loss_rnnt 69.729515 hw_loss 0.112922 lr 0.00033788 rank 6
2023-02-17 14:34:35,482 DEBUG TRAIN Batch 1/100 loss 112.946648 loss_att 225.427124 loss_ctc 111.419029 loss_rnnt 90.624847 hw_loss 0.055094 lr 0.00033752 rank 5
2023-02-17 14:34:35,484 DEBUG TRAIN Batch 1/100 loss 76.553101 loss_att 165.233200 loss_ctc 79.114029 loss_rnnt 58.457088 hw_loss 0.034750 lr 0.00033920 rank 1
2023-02-17 14:34:35,525 DEBUG TRAIN Batch 1/100 loss 68.042137 loss_att 165.160812 loss_ctc 61.494133 loss_rnnt 49.413139 hw_loss 0.146857 lr 0.00033716 rank 3
2023-02-17 14:35:47,933 DEBUG TRAIN Batch 1/200 loss 100.579971 loss_att 193.715286 loss_ctc 120.852356 loss_rnnt 79.236412 hw_loss 0.025329 lr 0.00034188 rank 6
2023-02-17 14:35:47,939 DEBUG TRAIN Batch 1/200 loss 89.127121 loss_att 198.477356 loss_ctc 102.409752 loss_rnnt 65.446075 hw_loss 0.074951 lr 0.00034176 rank 0
2023-02-17 14:35:47,939 DEBUG TRAIN Batch 1/200 loss 71.812096 loss_att 146.924866 loss_ctc 72.630058 loss_rnnt 56.678764 hw_loss 0.003232 lr 0.00034108 rank 4
2023-02-17 14:35:47,941 DEBUG TRAIN Batch 1/200 loss 80.414925 loss_att 194.242249 loss_ctc 94.954422 loss_rnnt 55.646286 hw_loss 0.121083 lr 0.00034116 rank 3
2023-02-17 14:35:47,942 DEBUG TRAIN Batch 1/200 loss 58.202850 loss_att 158.634964 loss_ctc 54.060661 loss_rnnt 38.668427 hw_loss 0.000547 lr 0.00034200 rank 2
2023-02-17 14:35:47,944 DEBUG TRAIN Batch 1/200 loss 112.884476 loss_att 221.757599 loss_ctc 129.284866 loss_rnnt 88.868362 hw_loss 0.102670 lr 0.00034152 rank 5
2023-02-17 14:35:47,945 DEBUG TRAIN Batch 1/200 loss 100.655983 loss_att 192.557877 loss_ctc 104.498650 loss_rnnt 81.749146 hw_loss 0.026426 lr 0.00034320 rank 1
2023-02-17 14:35:47,985 DEBUG TRAIN Batch 1/200 loss 100.588890 loss_att 198.786621 loss_ctc 107.904099 loss_rnnt 79.973679 hw_loss 0.000581 lr 0.00034196 rank 7
2023-02-17 14:37:01,543 DEBUG TRAIN Batch 1/300 loss 70.083755 loss_att 181.567642 loss_ctc 73.168938 loss_rnnt 47.342667 hw_loss 0.061770 lr 0.00034596 rank 7
2023-02-17 14:37:01,544 DEBUG TRAIN Batch 1/300 loss 86.171165 loss_att 173.978683 loss_ctc 94.739349 loss_rnnt 67.420837 hw_loss 0.086999 lr 0.00034516 rank 3
2023-02-17 14:37:01,552 DEBUG TRAIN Batch 1/300 loss 99.973579 loss_att 216.940628 loss_ctc 99.646027 loss_rnnt 76.577499 hw_loss 0.086875 lr 0.00034720 rank 1
2023-02-17 14:37:01,559 DEBUG TRAIN Batch 1/300 loss 76.233276 loss_att 156.350815 loss_ctc 82.607803 loss_rnnt 59.327042 hw_loss 0.061478 lr 0.00034552 rank 5
2023-02-17 14:37:01,560 DEBUG TRAIN Batch 1/300 loss 83.434402 loss_att 192.841827 loss_ctc 86.558365 loss_rnnt 61.116184 hw_loss 0.037875 lr 0.00034600 rank 2
2023-02-17 14:37:01,561 DEBUG TRAIN Batch 1/300 loss 89.309998 loss_att 190.780899 loss_ctc 88.759064 loss_rnnt 69.057259 hw_loss 0.060018 lr 0.00034508 rank 4
2023-02-17 14:37:01,567 DEBUG TRAIN Batch 1/300 loss 77.464493 loss_att 163.061188 loss_ctc 70.696365 loss_rnnt 61.203339 hw_loss 0.082932 lr 0.00034576 rank 0
2023-02-17 14:37:01,584 DEBUG TRAIN Batch 1/300 loss 79.237000 loss_att 165.395905 loss_ctc 83.388428 loss_rnnt 61.401054 hw_loss 0.094950 lr 0.00034588 rank 6
2023-02-17 14:38:15,327 DEBUG TRAIN Batch 1/400 loss 96.473831 loss_att 172.534515 loss_ctc 108.583298 loss_rnnt 79.619621 hw_loss 0.051515 lr 0.00034908 rank 4
2023-02-17 14:38:15,331 DEBUG TRAIN Batch 1/400 loss 79.943581 loss_att 171.152969 loss_ctc 79.176041 loss_rnnt 61.741489 hw_loss 0.117276 lr 0.00034976 rank 0
2023-02-17 14:38:15,332 DEBUG TRAIN Batch 1/400 loss 73.880493 loss_att 162.591797 loss_ctc 76.890495 loss_rnnt 55.684303 hw_loss 0.098606 lr 0.00034988 rank 6
2023-02-17 14:38:15,336 DEBUG TRAIN Batch 1/400 loss 74.073715 loss_att 181.367294 loss_ctc 71.710403 loss_rnnt 52.873184 hw_loss 0.106744 lr 0.00035000 rank 2
2023-02-17 14:38:15,338 DEBUG TRAIN Batch 1/400 loss 103.439636 loss_att 215.046341 loss_ctc 113.019836 loss_rnnt 79.824707 hw_loss 0.030403 lr 0.00034996 rank 7
2023-02-17 14:38:15,339 DEBUG TRAIN Batch 1/400 loss 90.088264 loss_att 179.709808 loss_ctc 98.481247 loss_rnnt 71.001160 hw_loss 0.081997 lr 0.00034916 rank 3
2023-02-17 14:38:15,342 DEBUG TRAIN Batch 1/400 loss 108.679321 loss_att 195.833511 loss_ctc 111.060638 loss_rnnt 90.877274 hw_loss 0.100669 lr 0.00035120 rank 1
2023-02-17 14:38:15,342 DEBUG TRAIN Batch 1/400 loss 56.379456 loss_att 137.326569 loss_ctc 50.894943 loss_rnnt 40.886559 hw_loss 0.065140 lr 0.00034952 rank 5
2023-02-17 14:39:27,904 DEBUG TRAIN Batch 1/500 loss 55.041935 loss_att 145.362473 loss_ctc 54.365337 loss_rnnt 37.041924 hw_loss 0.048970 lr 0.00035376 rank 0
2023-02-17 14:39:27,905 DEBUG TRAIN Batch 1/500 loss 78.559868 loss_att 175.729172 loss_ctc 89.492828 loss_rnnt 57.611427 hw_loss 0.106577 lr 0.00035308 rank 4
2023-02-17 14:39:27,907 DEBUG TRAIN Batch 1/500 loss 90.624863 loss_att 171.644882 loss_ctc 99.797562 loss_rnnt 73.131783 hw_loss 0.123819 lr 0.00035396 rank 7
2023-02-17 14:39:27,910 DEBUG TRAIN Batch 1/500 loss 64.807182 loss_att 158.067856 loss_ctc 62.656887 loss_rnnt 46.381741 hw_loss 0.112507 lr 0.00035388 rank 6
2023-02-17 14:39:27,912 DEBUG TRAIN Batch 1/500 loss 93.116394 loss_att 179.060120 loss_ctc 100.799850 loss_rnnt 74.847839 hw_loss 0.103771 lr 0.00035316 rank 3
2023-02-17 14:39:27,913 DEBUG TRAIN Batch 1/500 loss 76.294495 loss_att 148.026352 loss_ctc 87.500809 loss_rnnt 60.371326 hw_loss 0.154906 lr 0.00035352 rank 5
2023-02-17 14:39:27,916 DEBUG TRAIN Batch 1/500 loss 59.316875 loss_att 142.002609 loss_ctc 61.987316 loss_rnnt 42.363892 hw_loss 0.112086 lr 0.00035520 rank 1
2023-02-17 14:39:27,963 DEBUG TRAIN Batch 1/500 loss 69.498329 loss_att 156.157669 loss_ctc 67.536041 loss_rnnt 52.346939 hw_loss 0.152162 lr 0.00035400 rank 2
2023-02-17 14:40:41,577 DEBUG TRAIN Batch 1/600 loss 98.782951 loss_att 235.186661 loss_ctc 100.810814 loss_rnnt 71.178429 hw_loss 0.100123 lr 0.00035788 rank 6
2023-02-17 14:40:41,577 DEBUG TRAIN Batch 1/600 loss 60.422710 loss_att 109.746323 loss_ctc 70.007690 loss_rnnt 49.230690 hw_loss 0.092426 lr 0.00035776 rank 0
2023-02-17 14:40:41,579 DEBUG TRAIN Batch 1/600 loss 50.135254 loss_att 76.238480 loss_ctc 57.574272 loss_rnnt 43.854546 hw_loss 0.127867 lr 0.00035800 rank 2
2023-02-17 14:40:41,580 DEBUG TRAIN Batch 1/600 loss 55.438889 loss_att 98.680511 loss_ctc 61.476784 loss_rnnt 45.957687 hw_loss 0.052170 lr 0.00035708 rank 4
2023-02-17 14:40:41,582 DEBUG TRAIN Batch 1/600 loss 60.972610 loss_att 109.555573 loss_ctc 64.999649 loss_rnnt 50.619400 hw_loss 0.186898 lr 0.00035920 rank 1
2023-02-17 14:40:41,583 DEBUG TRAIN Batch 1/600 loss 57.897980 loss_att 99.526001 loss_ctc 66.639565 loss_rnnt 48.354824 hw_loss 0.097517 lr 0.00035716 rank 3
2023-02-17 14:40:41,589 DEBUG TRAIN Batch 1/600 loss 49.467117 loss_att 97.430542 loss_ctc 55.711449 loss_rnnt 39.024734 hw_loss 0.032102 lr 0.00035752 rank 5
2023-02-17 14:40:41,589 DEBUG TRAIN Batch 1/600 loss 66.708733 loss_att 112.808075 loss_ctc 77.072769 loss_rnnt 56.062542 hw_loss 0.083358 lr 0.00035796 rank 7
2023-02-17 14:41:57,255 DEBUG TRAIN Batch 1/700 loss 66.251411 loss_att 175.017380 loss_ctc 64.297958 loss_rnnt 44.733620 hw_loss 0.046980 lr 0.00036188 rank 6
2023-02-17 14:41:57,257 DEBUG TRAIN Batch 1/700 loss 82.874596 loss_att 195.966064 loss_ctc 87.834641 loss_rnnt 59.488060 hw_loss 0.200447 lr 0.00036176 rank 0
2023-02-17 14:41:57,259 DEBUG TRAIN Batch 1/700 loss 76.222252 loss_att 190.627167 loss_ctc 88.987633 loss_rnnt 51.608887 hw_loss 0.056853 lr 0.00036196 rank 7
2023-02-17 14:41:57,259 DEBUG TRAIN Batch 1/700 loss 83.983887 loss_att 186.572983 loss_ctc 90.840866 loss_rnnt 62.518780 hw_loss 0.061910 lr 0.00036108 rank 4
2023-02-17 14:41:57,265 DEBUG TRAIN Batch 1/700 loss 53.364716 loss_att 131.947311 loss_ctc 58.379997 loss_rnnt 36.979362 hw_loss 0.000243 lr 0.00036200 rank 2
2023-02-17 14:41:57,274 DEBUG TRAIN Batch 1/700 loss 105.263474 loss_att 213.824677 loss_ctc 112.185486 loss_rnnt 82.628113 hw_loss 0.000344 lr 0.00036116 rank 3
2023-02-17 14:41:57,286 DEBUG TRAIN Batch 1/700 loss 84.415627 loss_att 186.076035 loss_ctc 97.726379 loss_rnnt 62.231777 hw_loss 0.144364 lr 0.00036320 rank 1
2023-02-17 14:41:57,310 DEBUG TRAIN Batch 1/700 loss 68.521255 loss_att 183.334061 loss_ctc 73.622261 loss_rnnt 44.869194 hw_loss 0.017563 lr 0.00036152 rank 5
2023-02-17 14:43:10,425 DEBUG TRAIN Batch 1/800 loss 92.839272 loss_att 186.838852 loss_ctc 103.641830 loss_rnnt 72.564667 hw_loss 0.064404 lr 0.00036600 rank 2
2023-02-17 14:43:10,434 DEBUG TRAIN Batch 1/800 loss 94.137459 loss_att 205.692596 loss_ctc 99.436607 loss_rnnt 71.118980 hw_loss 0.001675 lr 0.00036508 rank 4
2023-02-17 14:43:10,434 DEBUG TRAIN Batch 1/800 loss 54.282814 loss_att 129.412384 loss_ctc 52.992386 loss_rnnt 39.360752 hw_loss 0.127877 lr 0.00036588 rank 6
2023-02-17 14:43:10,434 DEBUG TRAIN Batch 1/800 loss 68.697578 loss_att 154.145813 loss_ctc 67.152374 loss_rnnt 51.757141 hw_loss 0.106522 lr 0.00036596 rank 7
2023-02-17 14:43:10,435 DEBUG TRAIN Batch 1/800 loss 69.875664 loss_att 148.570221 loss_ctc 72.349060 loss_rnnt 53.742577 hw_loss 0.120726 lr 0.00036576 rank 0
2023-02-17 14:43:10,437 DEBUG TRAIN Batch 1/800 loss 90.350296 loss_att 190.388123 loss_ctc 97.065567 loss_rnnt 69.364136 hw_loss 0.156033 lr 0.00036552 rank 5
2023-02-17 14:43:10,441 DEBUG TRAIN Batch 1/800 loss 67.812210 loss_att 164.995895 loss_ctc 71.196526 loss_rnnt 47.819103 hw_loss 0.197112 lr 0.00036720 rank 1
2023-02-17 14:43:10,483 DEBUG TRAIN Batch 1/800 loss 111.574394 loss_att 213.291870 loss_ctc 120.823013 loss_rnnt 89.895386 hw_loss 0.191942 lr 0.00036516 rank 3
2023-02-17 14:44:22,428 DEBUG TRAIN Batch 1/900 loss 70.274124 loss_att 154.946426 loss_ctc 81.093285 loss_rnnt 51.854935 hw_loss 0.079078 lr 0.00037000 rank 2
2023-02-17 14:44:22,431 DEBUG TRAIN Batch 1/900 loss 86.512924 loss_att 175.874313 loss_ctc 91.770866 loss_rnnt 67.875214 hw_loss 0.120697 lr 0.00036988 rank 6
2023-02-17 14:44:22,437 DEBUG TRAIN Batch 1/900 loss 81.359673 loss_att 171.531189 loss_ctc 89.405693 loss_rnnt 62.219616 hw_loss 0.061779 lr 0.00036976 rank 0
2023-02-17 14:44:22,438 DEBUG TRAIN Batch 1/900 loss 56.451900 loss_att 137.834839 loss_ctc 60.325981 loss_rnnt 39.561478 hw_loss 0.182427 lr 0.00036916 rank 3
2023-02-17 14:44:22,438 DEBUG TRAIN Batch 1/900 loss 81.178589 loss_att 171.544388 loss_ctc 82.179153 loss_rnnt 62.896236 hw_loss 0.142089 lr 0.00036952 rank 5
2023-02-17 14:44:22,440 DEBUG TRAIN Batch 1/900 loss 109.751778 loss_att 217.801025 loss_ctc 120.319305 loss_rnnt 86.649895 hw_loss 0.155699 lr 0.00036908 rank 4
2023-02-17 14:44:22,442 DEBUG TRAIN Batch 1/900 loss 82.755089 loss_att 160.003723 loss_ctc 94.935425 loss_rnnt 65.681160 hw_loss 0.000279 lr 0.00037120 rank 1
2023-02-17 14:44:22,490 DEBUG TRAIN Batch 1/900 loss 97.562721 loss_att 193.506683 loss_ctc 100.073479 loss_rnnt 77.919029 hw_loss 0.225259 lr 0.00036996 rank 7
2023-02-17 14:45:36,007 DEBUG TRAIN Batch 1/1000 loss 89.113571 loss_att 166.259476 loss_ctc 100.147873 loss_rnnt 72.143753 hw_loss 0.130112 lr 0.00037308 rank 4
2023-02-17 14:45:36,009 DEBUG TRAIN Batch 1/1000 loss 65.914261 loss_att 139.565811 loss_ctc 73.274101 loss_rnnt 50.194077 hw_loss 0.016055 lr 0.00037388 rank 6
2023-02-17 14:45:36,011 DEBUG TRAIN Batch 1/1000 loss 103.540421 loss_att 190.363113 loss_ctc 118.368050 loss_rnnt 84.133530 hw_loss 0.122518 lr 0.00037316 rank 3
2023-02-17 14:45:36,012 DEBUG TRAIN Batch 1/1000 loss 64.868744 loss_att 151.538071 loss_ctc 70.339127 loss_rnnt 46.700500 hw_loss 0.196866 lr 0.00037352 rank 5
2023-02-17 14:45:36,012 DEBUG TRAIN Batch 1/1000 loss 119.947945 loss_att 225.426666 loss_ctc 127.460434 loss_rnnt 97.823364 hw_loss 0.050950 lr 0.00037400 rank 2
2023-02-17 14:45:36,014 DEBUG TRAIN Batch 1/1000 loss 88.343689 loss_att 183.424515 loss_ctc 92.452179 loss_rnnt 68.736267 hw_loss 0.081481 lr 0.00037520 rank 1
2023-02-17 14:45:36,013 DEBUG TRAIN Batch 1/1000 loss 98.409546 loss_att 195.602264 loss_ctc 106.684708 loss_rnnt 77.808205 hw_loss 0.111444 lr 0.00037376 rank 0
2023-02-17 14:45:36,058 DEBUG TRAIN Batch 1/1000 loss 42.307182 loss_att 111.590164 loss_ctc 37.716412 loss_rnnt 29.062336 hw_loss 0.000655 lr 0.00037396 rank 7
2023-02-17 14:46:50,312 DEBUG TRAIN Batch 1/1100 loss 67.173035 loss_att 148.591980 loss_ctc 78.456490 loss_rnnt 49.357391 hw_loss 0.051363 lr 0.00037708 rank 4
2023-02-17 14:46:50,315 DEBUG TRAIN Batch 1/1100 loss 73.617966 loss_att 142.829971 loss_ctc 76.144356 loss_rnnt 59.438622 hw_loss 0.000170 lr 0.00037920 rank 1
2023-02-17 14:46:50,316 DEBUG TRAIN Batch 1/1100 loss 75.264755 loss_att 137.738968 loss_ctc 84.817589 loss_rnnt 61.439556 hw_loss 0.106196 lr 0.00037800 rank 2
2023-02-17 14:46:50,316 DEBUG TRAIN Batch 1/1100 loss 69.881577 loss_att 140.214813 loss_ctc 79.260437 loss_rnnt 54.535751 hw_loss 0.053750 lr 0.00037788 rank 6
2023-02-17 14:46:50,321 DEBUG TRAIN Batch 1/1100 loss 65.704117 loss_att 138.822723 loss_ctc 62.218361 loss_rnnt 51.529667 hw_loss 0.029055 lr 0.00037796 rank 7
2023-02-17 14:46:50,322 DEBUG TRAIN Batch 1/1100 loss 67.436104 loss_att 142.253723 loss_ctc 69.318275 loss_rnnt 52.138302 hw_loss 0.156231 lr 0.00037776 rank 0
2023-02-17 14:46:50,328 DEBUG TRAIN Batch 1/1100 loss 92.989326 loss_att 170.719879 loss_ctc 99.210442 loss_rnnt 76.596230 hw_loss 0.032829 lr 0.00037752 rank 5
2023-02-17 14:46:50,368 DEBUG TRAIN Batch 1/1100 loss 63.101879 loss_att 143.803741 loss_ctc 70.611786 loss_rnnt 45.912556 hw_loss 0.089291 lr 0.00037716 rank 3
2023-02-17 14:48:02,615 DEBUG TRAIN Batch 1/1200 loss 54.605415 loss_att 110.281738 loss_ctc 59.111916 loss_rnnt 42.789490 hw_loss 0.149611 lr 0.00038108 rank 4
2023-02-17 14:48:02,621 DEBUG TRAIN Batch 1/1200 loss 71.588539 loss_att 125.435883 loss_ctc 77.873642 loss_rnnt 59.937817 hw_loss 0.081081 lr 0.00038176 rank 0
2023-02-17 14:48:02,622 DEBUG TRAIN Batch 1/1200 loss 35.584736 loss_att 60.814919 loss_ctc 38.019081 loss_rnnt 30.152704 hw_loss 0.115159 lr 0.00038200 rank 2
2023-02-17 14:48:02,622 DEBUG TRAIN Batch 1/1200 loss 40.966991 loss_att 81.513824 loss_ctc 44.720123 loss_rnnt 32.312798 hw_loss 0.083264 lr 0.00038188 rank 6
2023-02-17 14:48:02,624 DEBUG TRAIN Batch 1/1200 loss 55.461109 loss_att 103.219086 loss_ctc 63.094124 loss_rnnt 44.866783 hw_loss 0.046868 lr 0.00038116 rank 3
2023-02-17 14:48:02,625 DEBUG TRAIN Batch 1/1200 loss 82.136734 loss_att 136.353912 loss_ctc 88.881096 loss_rnnt 70.338615 hw_loss 0.103946 lr 0.00038196 rank 7
2023-02-17 14:48:02,654 DEBUG TRAIN Batch 1/1200 loss 60.479752 loss_att 135.813858 loss_ctc 69.550293 loss_rnnt 44.157207 hw_loss 0.086841 lr 0.00038152 rank 5
2023-02-17 14:48:02,663 DEBUG TRAIN Batch 1/1200 loss 61.412590 loss_att 127.798592 loss_ctc 67.667458 loss_rnnt 47.247074 hw_loss 0.101867 lr 0.00038320 rank 1
2023-02-17 14:49:16,088 DEBUG TRAIN Batch 1/1300 loss 88.907898 loss_att 211.195801 loss_ctc 99.951324 loss_rnnt 62.922005 hw_loss 0.104720 lr 0.00038508 rank 4
2023-02-17 14:49:16,094 DEBUG TRAIN Batch 1/1300 loss 46.286724 loss_att 135.886185 loss_ctc 42.193810 loss_rnnt 28.884850 hw_loss 0.051946 lr 0.00038588 rank 6
2023-02-17 14:49:16,103 DEBUG TRAIN Batch 1/1300 loss 77.573616 loss_att 155.360840 loss_ctc 80.119370 loss_rnnt 61.629837 hw_loss 0.087950 lr 0.00038516 rank 3
2023-02-17 14:49:16,108 DEBUG TRAIN Batch 1/1300 loss 26.606146 loss_att 39.496128 loss_ctc 30.303072 loss_rnnt 23.411640 hw_loss 0.231719 lr 0.00038720 rank 1
2023-02-17 14:49:16,110 DEBUG TRAIN Batch 1/1300 loss 31.815985 loss_att 53.822281 loss_ctc 38.694153 loss_rnnt 26.441568 hw_loss 0.105129 lr 0.00038596 rank 7
2023-02-17 14:49:16,112 DEBUG TRAIN Batch 1/1300 loss 76.012260 loss_att 176.946472 loss_ctc 72.696594 loss_rnnt 56.222984 hw_loss 0.083476 lr 0.00038576 rank 0
2023-02-17 14:49:16,116 DEBUG TRAIN Batch 1/1300 loss 35.117973 loss_att 62.958557 loss_ctc 39.512642 loss_rnnt 28.904861 hw_loss 0.110705 lr 0.00038552 rank 5
2023-02-17 14:49:16,118 DEBUG TRAIN Batch 1/1300 loss 64.779823 loss_att 154.529449 loss_ctc 74.656250 loss_rnnt 45.467117 hw_loss 0.086103 lr 0.00038600 rank 2
2023-02-17 14:50:30,256 DEBUG TRAIN Batch 1/1400 loss 84.269539 loss_att 159.091675 loss_ctc 92.643463 loss_rnnt 68.168182 hw_loss 0.038267 lr 0.00038988 rank 6
2023-02-17 14:50:30,267 DEBUG TRAIN Batch 1/1400 loss 83.215645 loss_att 165.938904 loss_ctc 93.916290 loss_rnnt 65.244125 hw_loss 0.000219 lr 0.00039120 rank 1
2023-02-17 14:50:30,268 DEBUG TRAIN Batch 1/1400 loss 65.522873 loss_att 157.230438 loss_ctc 64.937737 loss_rnnt 47.224113 hw_loss 0.066126 lr 0.00038976 rank 0
2023-02-17 14:50:30,269 DEBUG TRAIN Batch 1/1400 loss 110.831345 loss_att 190.294296 loss_ctc 117.030685 loss_rnnt 94.085968 hw_loss 0.049127 lr 0.00038908 rank 4
2023-02-17 14:50:30,269 DEBUG TRAIN Batch 1/1400 loss 96.115723 loss_att 193.533081 loss_ctc 112.989700 loss_rnnt 74.349602 hw_loss 0.061472 lr 0.00038916 rank 3
2023-02-17 14:50:30,271 DEBUG TRAIN Batch 1/1400 loss 47.061474 loss_att 131.088501 loss_ctc 44.784081 loss_rnnt 30.559530 hw_loss 0.000349 lr 0.00039000 rank 2
2023-02-17 14:50:30,273 DEBUG TRAIN Batch 1/1400 loss 82.220787 loss_att 194.556549 loss_ctc 74.714096 loss_rnnt 60.675438 hw_loss 0.148299 lr 0.00038996 rank 7
2023-02-17 14:50:30,280 DEBUG TRAIN Batch 1/1400 loss 93.115227 loss_att 180.386139 loss_ctc 106.233238 loss_rnnt 73.866486 hw_loss 0.085303 lr 0.00038952 rank 5
2023-02-17 14:51:42,841 DEBUG TRAIN Batch 1/1500 loss 60.644482 loss_att 145.631546 loss_ctc 62.259079 loss_rnnt 43.391937 hw_loss 0.074722 lr 0.00039308 rank 4
2023-02-17 14:51:42,846 DEBUG TRAIN Batch 1/1500 loss 54.601227 loss_att 153.150238 loss_ctc 58.854107 loss_rnnt 34.307888 hw_loss 0.030922 lr 0.00039352 rank 5
2023-02-17 14:51:42,848 DEBUG TRAIN Batch 1/1500 loss 77.451797 loss_att 151.984146 loss_ctc 87.727348 loss_rnnt 61.103760 hw_loss 0.134051 lr 0.00039400 rank 2
2023-02-17 14:51:42,847 DEBUG TRAIN Batch 1/1500 loss 79.446228 loss_att 168.426834 loss_ctc 88.861320 loss_rnnt 60.361099 hw_loss 0.063124 lr 0.00039388 rank 6
2023-02-17 14:51:42,850 DEBUG TRAIN Batch 1/1500 loss 62.784813 loss_att 160.546204 loss_ctc 70.420204 loss_rnnt 42.163879 hw_loss 0.094882 lr 0.00039396 rank 7
2023-02-17 14:51:42,851 DEBUG TRAIN Batch 1/1500 loss 73.302246 loss_att 160.830490 loss_ctc 89.083900 loss_rnnt 53.646744 hw_loss 0.085558 lr 0.00039376 rank 0
2023-02-17 14:51:42,853 DEBUG TRAIN Batch 1/1500 loss 62.258770 loss_att 147.428680 loss_ctc 66.767426 loss_rnnt 44.601646 hw_loss 0.041225 lr 0.00039520 rank 1
2023-02-17 14:51:42,898 DEBUG TRAIN Batch 1/1500 loss 84.259750 loss_att 164.227158 loss_ctc 90.505005 loss_rnnt 67.418564 hw_loss 0.028121 lr 0.00039316 rank 3
2023-02-17 14:52:55,436 DEBUG TRAIN Batch 1/1600 loss 59.321426 loss_att 128.641998 loss_ctc 57.781670 loss_rnnt 45.631905 hw_loss 0.057567 lr 0.00039800 rank 2
2023-02-17 14:52:55,437 DEBUG TRAIN Batch 1/1600 loss 75.090691 loss_att 154.268341 loss_ctc 80.654709 loss_rnnt 58.410446 hw_loss 0.192824 lr 0.00039708 rank 4
2023-02-17 14:52:55,437 DEBUG TRAIN Batch 1/1600 loss 61.037277 loss_att 142.874008 loss_ctc 71.475998 loss_rnnt 43.198364 hw_loss 0.149510 lr 0.00039752 rank 5
2023-02-17 14:52:55,441 DEBUG TRAIN Batch 1/1600 loss 52.688499 loss_att 115.407951 loss_ctc 60.481808 loss_rnnt 38.996346 hw_loss 0.204657 lr 0.00039920 rank 1
2023-02-17 14:52:55,441 DEBUG TRAIN Batch 1/1600 loss 56.432602 loss_att 136.008957 loss_ctc 60.043335 loss_rnnt 39.951077 hw_loss 0.159044 lr 0.00039788 rank 6
2023-02-17 14:52:55,445 DEBUG TRAIN Batch 1/1600 loss 37.963650 loss_att 97.758484 loss_ctc 41.420799 loss_rnnt 25.539543 hw_loss 0.007845 lr 0.00039776 rank 0
2023-02-17 14:52:55,448 DEBUG TRAIN Batch 1/1600 loss 57.645184 loss_att 121.872879 loss_ctc 66.076965 loss_rnnt 43.591805 hw_loss 0.156750 lr 0.00039716 rank 3
2023-02-17 14:52:55,495 DEBUG TRAIN Batch 1/1600 loss 90.346703 loss_att 165.697144 loss_ctc 98.567940 loss_rnnt 74.151764 hw_loss 0.053759 lr 0.00039796 rank 7
2023-02-17 14:54:09,129 DEBUG TRAIN Batch 1/1700 loss 61.356014 loss_att 122.479630 loss_ctc 75.103714 loss_rnnt 47.269585 hw_loss 0.053779 lr 0.00040188 rank 6
2023-02-17 14:54:09,138 DEBUG TRAIN Batch 1/1700 loss 78.480446 loss_att 141.979614 loss_ctc 88.867065 loss_rnnt 64.358444 hw_loss 0.069899 lr 0.00040108 rank 4
2023-02-17 14:54:09,142 DEBUG TRAIN Batch 1/1700 loss 91.519112 loss_att 162.752884 loss_ctc 99.697014 loss_rnnt 76.124908 hw_loss 0.106983 lr 0.00040152 rank 5
2023-02-17 14:54:09,143 DEBUG TRAIN Batch 1/1700 loss 64.902687 loss_att 137.317413 loss_ctc 72.825867 loss_rnnt 49.339073 hw_loss 0.045480 lr 0.00040176 rank 0
2023-02-17 14:54:09,145 DEBUG TRAIN Batch 1/1700 loss 53.381966 loss_att 120.628647 loss_ctc 57.445015 loss_rnnt 39.369587 hw_loss 0.039937 lr 0.00040320 rank 1
2023-02-17 14:54:09,145 DEBUG TRAIN Batch 1/1700 loss 53.421837 loss_att 125.421555 loss_ctc 55.027756 loss_rnnt 38.716309 hw_loss 0.171492 lr 0.00040196 rank 7
2023-02-17 14:54:09,149 DEBUG TRAIN Batch 1/1700 loss 60.566303 loss_att 129.927872 loss_ctc 69.162521 loss_rnnt 45.483334 hw_loss 0.120930 lr 0.00040200 rank 2
2023-02-17 14:54:09,151 DEBUG TRAIN Batch 1/1700 loss 65.613350 loss_att 141.722900 loss_ctc 72.255539 loss_rnnt 49.466034 hw_loss 0.074584 lr 0.00040116 rank 3
2023-02-17 14:55:23,766 DEBUG TRAIN Batch 1/1800 loss 44.152901 loss_att 87.615746 loss_ctc 54.798611 loss_rnnt 33.967388 hw_loss 0.137847 lr 0.00040576 rank 0
2023-02-17 14:55:23,774 DEBUG TRAIN Batch 1/1800 loss 83.319832 loss_att 147.951294 loss_ctc 102.166046 loss_rnnt 67.849884 hw_loss 0.057800 lr 0.00040508 rank 4
2023-02-17 14:55:23,775 DEBUG TRAIN Batch 1/1800 loss 62.140785 loss_att 104.356400 loss_ctc 65.779327 loss_rnnt 53.161102 hw_loss 0.096405 lr 0.00040600 rank 2
2023-02-17 14:55:23,775 DEBUG TRAIN Batch 1/1800 loss 68.214928 loss_att 111.773056 loss_ctc 91.235527 loss_rnnt 56.332901 hw_loss 0.189336 lr 0.00040516 rank 3
2023-02-17 14:55:23,775 DEBUG TRAIN Batch 1/1800 loss 51.965023 loss_att 100.355133 loss_ctc 57.676838 loss_rnnt 41.479462 hw_loss 0.086181 lr 0.00040588 rank 6
2023-02-17 14:55:23,781 DEBUG TRAIN Batch 1/1800 loss 85.361855 loss_att 144.101685 loss_ctc 99.231728 loss_rnnt 71.705322 hw_loss 0.111097 lr 0.00040552 rank 5
2023-02-17 14:55:23,786 DEBUG TRAIN Batch 1/1800 loss 81.942657 loss_att 156.836700 loss_ctc 95.311989 loss_rnnt 65.091492 hw_loss 0.168343 lr 0.00040596 rank 7
2023-02-17 14:55:23,823 DEBUG TRAIN Batch 1/1800 loss 88.926056 loss_att 159.425995 loss_ctc 105.931000 loss_rnnt 72.522385 hw_loss 0.068169 lr 0.00040720 rank 1
2023-02-17 14:56:36,557 DEBUG TRAIN Batch 1/1900 loss 64.080505 loss_att 146.138611 loss_ctc 67.401535 loss_rnnt 47.225079 hw_loss 0.001868 lr 0.00040976 rank 0
2023-02-17 14:56:36,568 DEBUG TRAIN Batch 1/1900 loss 71.545219 loss_att 159.008804 loss_ctc 72.065361 loss_rnnt 53.919415 hw_loss 0.119496 lr 0.00040988 rank 6
2023-02-17 14:56:36,570 DEBUG TRAIN Batch 1/1900 loss 40.119057 loss_att 58.247719 loss_ctc 45.525970 loss_rnnt 35.633228 hw_loss 0.260946 lr 0.00040916 rank 3
2023-02-17 14:56:36,571 DEBUG TRAIN Batch 1/1900 loss 33.747055 loss_att 65.990921 loss_ctc 35.982903 loss_rnnt 26.951582 hw_loss 0.091101 lr 0.00040908 rank 4
2023-02-17 14:56:36,574 DEBUG TRAIN Batch 1/1900 loss 98.143951 loss_att 195.324692 loss_ctc 118.296356 loss_rnnt 75.979019 hw_loss 0.078347 lr 0.00041000 rank 2
2023-02-17 14:56:36,574 DEBUG TRAIN Batch 1/1900 loss 64.673767 loss_att 110.282143 loss_ctc 71.362305 loss_rnnt 54.615417 hw_loss 0.084126 lr 0.00040996 rank 7
2023-02-17 14:56:36,579 DEBUG TRAIN Batch 1/1900 loss 67.343628 loss_att 119.377686 loss_ctc 76.064224 loss_rnnt 55.724926 hw_loss 0.092142 lr 0.00041120 rank 1
2023-02-17 14:56:36,622 DEBUG TRAIN Batch 1/1900 loss 52.089478 loss_att 93.078064 loss_ctc 55.722301 loss_rnnt 43.369457 hw_loss 0.071103 lr 0.00040952 rank 5
2023-02-17 14:57:48,517 DEBUG TRAIN Batch 1/2000 loss 65.938362 loss_att 148.219131 loss_ctc 68.570389 loss_rnnt 49.122154 hw_loss 0.017087 lr 0.00041388 rank 6
2023-02-17 14:57:48,519 DEBUG TRAIN Batch 1/2000 loss 93.103165 loss_att 166.210999 loss_ctc 109.800194 loss_rnnt 76.227257 hw_loss 0.052652 lr 0.00041308 rank 4
2023-02-17 14:57:48,523 DEBUG TRAIN Batch 1/2000 loss 50.593536 loss_att 129.044708 loss_ctc 55.772442 loss_rnnt 34.184395 hw_loss 0.053224 lr 0.00041316 rank 3
2023-02-17 14:57:48,524 DEBUG TRAIN Batch 1/2000 loss 65.454964 loss_att 133.807617 loss_ctc 78.162216 loss_rnnt 50.086575 hw_loss 0.006661 lr 0.00041376 rank 0
2023-02-17 14:57:48,533 DEBUG TRAIN Batch 1/2000 loss 94.381676 loss_att 183.633942 loss_ctc 109.927429 loss_rnnt 74.448975 hw_loss 0.017782 lr 0.00041396 rank 7
2023-02-17 14:57:48,547 DEBUG TRAIN Batch 1/2000 loss 49.674343 loss_att 123.367004 loss_ctc 52.944237 loss_rnnt 34.451611 hw_loss 0.090400 lr 0.00041352 rank 5
2023-02-17 14:57:48,551 DEBUG TRAIN Batch 1/2000 loss 99.800781 loss_att 183.660385 loss_ctc 107.713913 loss_rnnt 81.911095 hw_loss 0.117537 lr 0.00041520 rank 1
2023-02-17 14:57:48,572 DEBUG TRAIN Batch 1/2000 loss 90.734093 loss_att 158.862427 loss_ctc 105.372887 loss_rnnt 75.139679 hw_loss 0.031716 lr 0.00041400 rank 2
2023-02-17 14:59:02,655 DEBUG TRAIN Batch 1/2100 loss 67.273598 loss_att 146.221039 loss_ctc 74.287842 loss_rnnt 50.520321 hw_loss 0.053544 lr 0.00041708 rank 4
2023-02-17 14:59:02,656 DEBUG TRAIN Batch 1/2100 loss 77.171883 loss_att 163.950394 loss_ctc 91.346375 loss_rnnt 57.885036 hw_loss 0.077258 lr 0.00041796 rank 7
2023-02-17 14:59:02,656 DEBUG TRAIN Batch 1/2100 loss 85.071419 loss_att 154.563644 loss_ctc 94.793701 loss_rnnt 69.851311 hw_loss 0.047546 lr 0.00041920 rank 1
2023-02-17 14:59:02,657 DEBUG TRAIN Batch 1/2100 loss 73.577477 loss_att 141.230835 loss_ctc 80.606934 loss_rnnt 59.066742 hw_loss 0.080258 lr 0.00041800 rank 2
2023-02-17 14:59:02,659 DEBUG TRAIN Batch 1/2100 loss 68.250244 loss_att 136.155823 loss_ctc 75.919350 loss_rnnt 53.606503 hw_loss 0.075144 lr 0.00041776 rank 0
2023-02-17 14:59:02,659 DEBUG TRAIN Batch 1/2100 loss 53.087029 loss_att 116.476395 loss_ctc 66.979706 loss_rnnt 38.497078 hw_loss 0.111984 lr 0.00041788 rank 6
2023-02-17 14:59:02,662 DEBUG TRAIN Batch 1/2100 loss 81.219551 loss_att 166.079620 loss_ctc 98.692368 loss_rnnt 61.917645 hw_loss 0.000339 lr 0.00041752 rank 5
2023-02-17 14:59:02,668 DEBUG TRAIN Batch 1/2100 loss 67.642441 loss_att 147.861038 loss_ctc 79.676147 loss_rnnt 49.968201 hw_loss 0.048792 lr 0.00041716 rank 3
2023-02-17 15:00:15,726 DEBUG TRAIN Batch 1/2200 loss 49.338001 loss_att 110.264801 loss_ctc 61.670147 loss_rnnt 35.415436 hw_loss 0.174228 lr 0.00042196 rank 7
2023-02-17 15:00:15,730 DEBUG TRAIN Batch 1/2200 loss 71.316658 loss_att 160.476151 loss_ctc 77.911514 loss_rnnt 52.571880 hw_loss 0.062921 lr 0.00042116 rank 3
2023-02-17 15:00:15,743 DEBUG TRAIN Batch 1/2200 loss 27.663599 loss_att 75.756134 loss_ctc 32.293957 loss_rnnt 17.411215 hw_loss 0.030935 lr 0.00042108 rank 4
2023-02-17 15:00:15,745 DEBUG TRAIN Batch 1/2200 loss 57.211086 loss_att 130.364136 loss_ctc 67.350639 loss_rnnt 41.151031 hw_loss 0.145310 lr 0.00042320 rank 1
2023-02-17 15:00:15,747 DEBUG TRAIN Batch 1/2200 loss 45.089088 loss_att 107.823402 loss_ctc 50.089596 loss_rnnt 31.856747 hw_loss 0.035145 lr 0.00042176 rank 0
2023-02-17 15:00:15,748 DEBUG TRAIN Batch 1/2200 loss 51.745426 loss_att 120.503059 loss_ctc 58.124413 loss_rnnt 37.085350 hw_loss 0.108785 lr 0.00042188 rank 6
2023-02-17 15:00:15,749 DEBUG TRAIN Batch 1/2200 loss 65.323051 loss_att 137.886063 loss_ctc 78.989120 loss_rnnt 48.950031 hw_loss 0.071773 lr 0.00042152 rank 5
2023-02-17 15:00:15,794 DEBUG TRAIN Batch 1/2200 loss 69.614220 loss_att 143.776367 loss_ctc 80.545258 loss_rnnt 53.323826 hw_loss 0.000919 lr 0.00042200 rank 2
2023-02-17 15:01:28,111 DEBUG TRAIN Batch 1/2300 loss 68.635025 loss_att 135.215424 loss_ctc 84.041000 loss_rnnt 53.221809 hw_loss 0.080645 lr 0.00042508 rank 4
2023-02-17 15:01:28,114 DEBUG TRAIN Batch 1/2300 loss 76.892212 loss_att 139.678284 loss_ctc 89.178215 loss_rnnt 62.608124 hw_loss 0.166375 lr 0.00042576 rank 0
2023-02-17 15:01:28,116 DEBUG TRAIN Batch 1/2300 loss 65.402863 loss_att 125.764297 loss_ctc 71.451096 loss_rnnt 52.492672 hw_loss 0.059020 lr 0.00042588 rank 6
2023-02-17 15:01:28,119 DEBUG TRAIN Batch 1/2300 loss 82.312607 loss_att 156.669312 loss_ctc 92.763214 loss_rnnt 66.038681 hw_loss 0.017202 lr 0.00042600 rank 2
2023-02-17 15:01:28,119 DEBUG TRAIN Batch 1/2300 loss 84.821663 loss_att 141.525238 loss_ctc 99.334541 loss_rnnt 71.502876 hw_loss 0.080667 lr 0.00042720 rank 1
2023-02-17 15:01:28,120 DEBUG TRAIN Batch 1/2300 loss 38.493225 loss_att 91.343689 loss_ctc 52.161583 loss_rnnt 26.046021 hw_loss 0.102489 lr 0.00042596 rank 7
2023-02-17 15:01:28,121 DEBUG TRAIN Batch 1/2300 loss 53.820026 loss_att 124.058716 loss_ctc 54.752747 loss_rnnt 39.567085 hw_loss 0.151563 lr 0.00042516 rank 3
2023-02-17 15:01:28,126 DEBUG TRAIN Batch 1/2300 loss 77.208305 loss_att 136.380096 loss_ctc 90.742493 loss_rnnt 63.534882 hw_loss 0.064695 lr 0.00042552 rank 5
2023-02-17 15:02:40,484 DEBUG TRAIN Batch 1/2400 loss 64.047081 loss_att 134.827713 loss_ctc 83.022247 loss_rnnt 47.337566 hw_loss 0.043803 lr 0.00042996 rank 7
2023-02-17 15:02:40,488 DEBUG TRAIN Batch 1/2400 loss 44.308327 loss_att 97.311996 loss_ctc 55.283463 loss_rnnt 32.211460 hw_loss 0.061452 lr 0.00042916 rank 3
2023-02-17 15:02:40,491 DEBUG TRAIN Batch 1/2400 loss 64.310081 loss_att 119.287231 loss_ctc 76.412971 loss_rnnt 51.643436 hw_loss 0.107809 lr 0.00042908 rank 4
2023-02-17 15:02:40,491 DEBUG TRAIN Batch 1/2400 loss 58.852467 loss_att 113.966156 loss_ctc 65.481133 loss_rnnt 46.933716 hw_loss 0.022846 lr 0.00042988 rank 6
2023-02-17 15:02:40,493 DEBUG TRAIN Batch 1/2400 loss 61.968243 loss_att 119.714516 loss_ctc 77.111595 loss_rnnt 48.306278 hw_loss 0.175487 lr 0.00043120 rank 1
2023-02-17 15:02:40,493 DEBUG TRAIN Batch 1/2400 loss 72.050591 loss_att 129.116699 loss_ctc 79.687401 loss_rnnt 59.566017 hw_loss 0.099590 lr 0.00042976 rank 0
2023-02-17 15:02:40,498 DEBUG TRAIN Batch 1/2400 loss 59.205517 loss_att 129.105896 loss_ctc 66.201080 loss_rnnt 44.252781 hw_loss 0.074836 lr 0.00042952 rank 5
2023-02-17 15:02:40,503 DEBUG TRAIN Batch 1/2400 loss 53.363880 loss_att 115.866333 loss_ctc 62.637691 loss_rnnt 39.552368 hw_loss 0.139706 lr 0.00043000 rank 2
2023-02-17 15:03:57,062 DEBUG TRAIN Batch 1/2500 loss 43.251003 loss_att 99.499062 loss_ctc 52.814892 loss_rnnt 30.691772 hw_loss 0.064553 lr 0.00043308 rank 4
2023-02-17 15:03:57,063 DEBUG TRAIN Batch 1/2500 loss 61.179096 loss_att 140.553284 loss_ctc 71.453064 loss_rnnt 43.846985 hw_loss 0.163897 lr 0.00043376 rank 0
2023-02-17 15:03:57,065 DEBUG TRAIN Batch 1/2500 loss 42.027058 loss_att 75.698288 loss_ctc 45.969513 loss_rnnt 34.693336 hw_loss 0.138394 lr 0.00043352 rank 5
2023-02-17 15:03:57,066 DEBUG TRAIN Batch 1/2500 loss 48.314369 loss_att 100.840866 loss_ctc 53.456814 loss_rnnt 37.039436 hw_loss 0.157449 lr 0.00043396 rank 7
2023-02-17 15:03:57,066 DEBUG TRAIN Batch 1/2500 loss 26.981913 loss_att 50.670853 loss_ctc 30.731552 loss_rnnt 21.626266 hw_loss 0.221076 lr 0.00043400 rank 2
2023-02-17 15:03:57,068 DEBUG TRAIN Batch 1/2500 loss 47.060356 loss_att 70.144943 loss_ctc 54.121731 loss_rnnt 41.422832 hw_loss 0.148297 lr 0.00043316 rank 3
2023-02-17 15:03:57,068 DEBUG TRAIN Batch 1/2500 loss 25.954748 loss_att 36.520485 loss_ctc 31.557959 loss_rnnt 23.027014 hw_loss 0.126549 lr 0.00043388 rank 6
2023-02-17 15:03:57,070 DEBUG TRAIN Batch 1/2500 loss 66.588486 loss_att 113.043877 loss_ctc 75.007195 loss_rnnt 56.092995 hw_loss 0.153588 lr 0.00043520 rank 1
2023-02-17 15:05:09,351 DEBUG TRAIN Batch 1/2600 loss 73.239471 loss_att 139.139008 loss_ctc 78.098839 loss_rnnt 59.411499 hw_loss 0.000286 lr 0.00043716 rank 3
2023-02-17 15:05:09,365 DEBUG TRAIN Batch 1/2600 loss 45.286041 loss_att 113.466461 loss_ctc 52.715862 loss_rnnt 30.659185 hw_loss 0.000238 lr 0.00043776 rank 0
2023-02-17 15:05:09,367 DEBUG TRAIN Batch 1/2600 loss 47.493656 loss_att 107.863701 loss_ctc 50.960323 loss_rnnt 34.927635 hw_loss 0.055856 lr 0.00043800 rank 2
2023-02-17 15:05:09,368 DEBUG TRAIN Batch 1/2600 loss 50.945786 loss_att 116.069511 loss_ctc 56.560722 loss_rnnt 37.172207 hw_loss 0.000337 lr 0.00043788 rank 6
2023-02-17 15:05:09,369 DEBUG TRAIN Batch 1/2600 loss 87.244682 loss_att 169.600830 loss_ctc 99.884674 loss_rnnt 69.067070 hw_loss 0.039467 lr 0.00043708 rank 4
2023-02-17 15:05:09,372 DEBUG TRAIN Batch 1/2600 loss 56.541798 loss_att 109.785614 loss_ctc 76.522675 loss_rnnt 43.173035 hw_loss 0.104785 lr 0.00043920 rank 1
2023-02-17 15:05:09,372 DEBUG TRAIN Batch 1/2600 loss 60.814453 loss_att 136.834274 loss_ctc 63.341660 loss_rnnt 45.273369 hw_loss 0.000298 lr 0.00043752 rank 5
2023-02-17 15:05:09,376 DEBUG TRAIN Batch 1/2600 loss 29.633860 loss_att 46.613121 loss_ctc 37.880398 loss_rnnt 25.102695 hw_loss 0.067072 lr 0.00043796 rank 7
2023-02-17 15:06:22,275 DEBUG TRAIN Batch 1/2700 loss 62.533539 loss_att 134.573776 loss_ctc 71.209198 loss_rnnt 46.904476 hw_loss 0.120491 lr 0.00044108 rank 4
2023-02-17 15:06:22,278 DEBUG TRAIN Batch 1/2700 loss 84.694626 loss_att 139.968658 loss_ctc 102.467705 loss_rnnt 71.139053 hw_loss 0.245658 lr 0.00044176 rank 0
2023-02-17 15:06:22,279 DEBUG TRAIN Batch 1/2700 loss 78.232933 loss_att 145.867950 loss_ctc 99.101868 loss_rnnt 61.857597 hw_loss 0.123380 lr 0.00044116 rank 3
2023-02-17 15:06:22,281 DEBUG TRAIN Batch 1/2700 loss 57.784485 loss_att 124.885315 loss_ctc 61.215485 loss_rnnt 43.847607 hw_loss 0.111085 lr 0.00044188 rank 6
2023-02-17 15:06:22,282 DEBUG TRAIN Batch 1/2700 loss 35.439766 loss_att 101.569901 loss_ctc 35.756458 loss_rnnt 22.086746 hw_loss 0.158937 lr 0.00044320 rank 1
2023-02-17 15:06:22,282 DEBUG TRAIN Batch 1/2700 loss 50.427921 loss_att 123.753151 loss_ctc 56.553726 loss_rnnt 34.898922 hw_loss 0.088464 lr 0.00044196 rank 7
2023-02-17 15:06:22,290 DEBUG TRAIN Batch 1/2700 loss 68.018272 loss_att 138.012695 loss_ctc 74.387131 loss_rnnt 53.129738 hw_loss 0.075867 lr 0.00044200 rank 2
2023-02-17 15:06:22,292 DEBUG TRAIN Batch 1/2700 loss 67.928200 loss_att 139.632126 loss_ctc 78.562622 loss_rnnt 52.056999 hw_loss 0.210942 lr 0.00044152 rank 5
2023-02-17 15:07:36,812 DEBUG TRAIN Batch 1/2800 loss 63.270145 loss_att 117.923637 loss_ctc 66.860207 loss_rnnt 51.804646 hw_loss 0.105236 lr 0.00044588 rank 6
2023-02-17 15:07:36,815 DEBUG TRAIN Batch 1/2800 loss 53.906948 loss_att 98.852348 loss_ctc 69.127350 loss_rnnt 42.828163 hw_loss 0.113086 lr 0.00044600 rank 2
2023-02-17 15:07:36,815 DEBUG TRAIN Batch 1/2800 loss 110.097168 loss_att 170.243835 loss_ctc 127.676353 loss_rnnt 95.636292 hw_loss 0.164329 lr 0.00044720 rank 1
2023-02-17 15:07:36,816 DEBUG TRAIN Batch 1/2800 loss 72.691170 loss_att 129.930511 loss_ctc 89.422768 loss_rnnt 58.964931 hw_loss 0.089057 lr 0.00044576 rank 0
2023-02-17 15:07:36,818 DEBUG TRAIN Batch 1/2800 loss 75.327980 loss_att 150.231186 loss_ctc 90.701111 loss_rnnt 58.247253 hw_loss 0.094371 lr 0.00044552 rank 5
2023-02-17 15:07:36,818 DEBUG TRAIN Batch 1/2800 loss 76.194710 loss_att 142.623535 loss_ctc 90.655365 loss_rnnt 60.958099 hw_loss 0.042672 lr 0.00044516 rank 3
2023-02-17 15:07:36,835 DEBUG TRAIN Batch 1/2800 loss 55.474606 loss_att 112.294418 loss_ctc 64.924316 loss_rnnt 42.793549 hw_loss 0.107124 lr 0.00044596 rank 7
2023-02-17 15:07:36,838 DEBUG TRAIN Batch 1/2800 loss 47.510601 loss_att 107.326889 loss_ctc 57.499054 loss_rnnt 34.179153 hw_loss 0.068247 lr 0.00044508 rank 4
2023-02-17 15:08:49,674 DEBUG TRAIN Batch 1/2900 loss 59.213654 loss_att 100.268860 loss_ctc 70.585281 loss_rnnt 49.408585 hw_loss 0.145896 lr 0.00044988 rank 6
2023-02-17 15:08:49,676 DEBUG TRAIN Batch 1/2900 loss 51.468716 loss_att 102.525482 loss_ctc 57.492931 loss_rnnt 40.428070 hw_loss 0.048865 lr 0.00044908 rank 4
2023-02-17 15:08:49,676 DEBUG TRAIN Batch 1/2900 loss 64.422333 loss_att 127.603500 loss_ctc 72.059105 loss_rnnt 50.729771 hw_loss 0.071408 lr 0.00045000 rank 2
2023-02-17 15:08:49,676 DEBUG TRAIN Batch 1/2900 loss 68.063843 loss_att 117.254333 loss_ctc 80.367737 loss_rnnt 56.556602 hw_loss 0.053677 lr 0.00045120 rank 1
2023-02-17 15:08:49,679 DEBUG TRAIN Batch 1/2900 loss 52.703671 loss_att 101.020355 loss_ctc 67.991287 loss_rnnt 40.928272 hw_loss 0.138208 lr 0.00044976 rank 0
2023-02-17 15:08:49,681 DEBUG TRAIN Batch 1/2900 loss 78.784584 loss_att 141.318985 loss_ctc 102.255219 loss_rnnt 63.129616 hw_loss 0.035007 lr 0.00044916 rank 3
2023-02-17 15:08:49,682 DEBUG TRAIN Batch 1/2900 loss 77.437691 loss_att 152.960175 loss_ctc 98.074753 loss_rnnt 59.562843 hw_loss 0.035135 lr 0.00044952 rank 5
2023-02-17 15:08:49,683 DEBUG TRAIN Batch 1/2900 loss 56.513531 loss_att 110.868027 loss_ctc 61.890640 loss_rnnt 44.892925 hw_loss 0.061428 lr 0.00044996 rank 7
2023-02-17 15:10:01,989 DEBUG TRAIN Batch 1/3000 loss 58.720970 loss_att 119.840179 loss_ctc 68.761490 loss_rnnt 45.112640 hw_loss 0.085780 lr 0.00045388 rank 6
2023-02-17 15:10:01,995 DEBUG TRAIN Batch 1/3000 loss 43.887829 loss_att 84.826538 loss_ctc 54.585976 loss_rnnt 34.224834 hw_loss 0.091558 lr 0.00045400 rank 2
2023-02-17 15:10:01,995 DEBUG TRAIN Batch 1/3000 loss 64.527519 loss_att 113.974518 loss_ctc 75.326920 loss_rnnt 53.177746 hw_loss 0.038357 lr 0.00045376 rank 0
2023-02-17 15:10:01,997 DEBUG TRAIN Batch 1/3000 loss 73.572601 loss_att 108.035812 loss_ctc 84.241257 loss_rnnt 65.216316 hw_loss 0.077164 lr 0.00045316 rank 3
2023-02-17 15:10:01,997 DEBUG TRAIN Batch 1/3000 loss 51.591965 loss_att 105.791168 loss_ctc 65.296173 loss_rnnt 38.911106 hw_loss 0.025854 lr 0.00045308 rank 4
2023-02-17 15:10:01,998 DEBUG TRAIN Batch 1/3000 loss 76.842407 loss_att 142.101440 loss_ctc 89.092728 loss_rnnt 62.086815 hw_loss 0.132018 lr 0.00045520 rank 1
2023-02-17 15:10:02,000 DEBUG TRAIN Batch 1/3000 loss 71.418442 loss_att 126.867134 loss_ctc 84.522903 loss_rnnt 58.543724 hw_loss 0.070725 lr 0.00045352 rank 5
2023-02-17 15:10:02,004 DEBUG TRAIN Batch 1/3000 loss 66.844688 loss_att 118.695816 loss_ctc 81.513336 loss_rnnt 54.484200 hw_loss 0.064580 lr 0.00045396 rank 7
2023-02-17 15:11:16,255 DEBUG TRAIN Batch 1/3100 loss 43.536289 loss_att 77.660873 loss_ctc 52.211021 loss_rnnt 35.508667 hw_loss 0.086398 lr 0.00045788 rank 6
2023-02-17 15:11:16,257 DEBUG TRAIN Batch 1/3100 loss 55.140949 loss_att 109.554001 loss_ctc 69.122665 loss_rnnt 42.394058 hw_loss 0.000097 lr 0.00045776 rank 0
2023-02-17 15:11:16,258 DEBUG TRAIN Batch 1/3100 loss 50.135403 loss_att 99.104713 loss_ctc 62.658573 loss_rnnt 38.647087 hw_loss 0.046307 lr 0.00045920 rank 1
2023-02-17 15:11:16,259 DEBUG TRAIN Batch 1/3100 loss 51.739437 loss_att 92.433609 loss_ctc 63.582218 loss_rnnt 42.000881 hw_loss 0.038782 lr 0.00045708 rank 4
2023-02-17 15:11:16,261 DEBUG TRAIN Batch 1/3100 loss 32.668488 loss_att 78.976135 loss_ctc 38.926102 loss_rnnt 22.516035 hw_loss 0.106079 lr 0.00045752 rank 5
2023-02-17 15:11:16,262 DEBUG TRAIN Batch 1/3100 loss 33.587513 loss_att 56.022503 loss_ctc 37.424622 loss_rnnt 28.503870 hw_loss 0.159424 lr 0.00045800 rank 2
2023-02-17 15:11:16,267 DEBUG TRAIN Batch 1/3100 loss 53.851696 loss_att 102.096634 loss_ctc 59.797173 loss_rnnt 43.358372 hw_loss 0.096755 lr 0.00045796 rank 7
2023-02-17 15:11:16,666 DEBUG TRAIN Batch 1/3100 loss 56.556526 loss_att 85.757019 loss_ctc 66.298248 loss_rnnt 49.397549 hw_loss 0.037474 lr 0.00045716 rank 3
2023-02-17 15:12:33,870 DEBUG TRAIN Batch 1/3200 loss 69.000801 loss_att 129.760178 loss_ctc 81.291748 loss_rnnt 55.133377 hw_loss 0.143900 lr 0.00046176 rank 0
2023-02-17 15:12:33,875 DEBUG TRAIN Batch 1/3200 loss 28.116711 loss_att 61.142403 loss_ctc 32.699017 loss_rnnt 20.899448 hw_loss 0.002154 lr 0.00046196 rank 7
2023-02-17 15:12:33,876 DEBUG TRAIN Batch 1/3200 loss 41.229286 loss_att 65.951340 loss_ctc 51.500195 loss_rnnt 34.838196 hw_loss 0.144793 lr 0.00046320 rank 1
2023-02-17 15:12:33,876 DEBUG TRAIN Batch 1/3200 loss 34.820190 loss_att 57.593765 loss_ctc 40.333271 loss_rnnt 29.452520 hw_loss 0.146028 lr 0.00046108 rank 4
2023-02-17 15:12:33,877 DEBUG TRAIN Batch 1/3200 loss 52.168041 loss_att 88.212006 loss_ctc 62.209148 loss_rnnt 43.565948 hw_loss 0.102159 lr 0.00046152 rank 5
2023-02-17 15:12:33,878 DEBUG TRAIN Batch 1/3200 loss 47.729008 loss_att 112.076630 loss_ctc 53.377342 loss_rnnt 34.069649 hw_loss 0.068857 lr 0.00046188 rank 6
2023-02-17 15:12:33,882 DEBUG TRAIN Batch 1/3200 loss 59.955376 loss_att 125.293152 loss_ctc 70.015106 loss_rnnt 45.546429 hw_loss 0.000169 lr 0.00046200 rank 2
2023-02-17 15:12:33,889 DEBUG TRAIN Batch 1/3200 loss 45.487988 loss_att 106.098999 loss_ctc 50.294426 loss_rnnt 32.655075 hw_loss 0.130966 lr 0.00046116 rank 3
2023-02-17 15:13:46,527 DEBUG TRAIN Batch 1/3300 loss 74.843132 loss_att 132.428497 loss_ctc 89.634125 loss_rnnt 61.307884 hw_loss 0.086333 lr 0.00046508 rank 4
2023-02-17 15:13:46,531 DEBUG TRAIN Batch 1/3300 loss 68.524170 loss_att 134.542938 loss_ctc 83.285606 loss_rnnt 53.329449 hw_loss 0.042712 lr 0.00046600 rank 2
2023-02-17 15:13:46,533 DEBUG TRAIN Batch 1/3300 loss 87.034279 loss_att 151.426651 loss_ctc 90.939926 loss_rnnt 73.595474 hw_loss 0.074215 lr 0.00046720 rank 1
2023-02-17 15:13:46,533 DEBUG TRAIN Batch 1/3300 loss 78.116646 loss_att 132.016418 loss_ctc 101.163147 loss_rnnt 64.214264 hw_loss 0.092928 lr 0.00046588 rank 6
2023-02-17 15:13:46,534 DEBUG TRAIN Batch 1/3300 loss 76.703369 loss_att 139.659744 loss_ctc 84.060730 loss_rnnt 63.102581 hw_loss 0.053483 lr 0.00046552 rank 5
2023-02-17 15:13:46,534 DEBUG TRAIN Batch 1/3300 loss 82.683731 loss_att 140.821777 loss_ctc 108.996384 loss_rnnt 67.547623 hw_loss 0.000259 lr 0.00046596 rank 7
2023-02-17 15:13:46,536 DEBUG TRAIN Batch 1/3300 loss 57.148705 loss_att 109.682655 loss_ctc 68.108917 loss_rnnt 45.107967 hw_loss 0.136112 lr 0.00046576 rank 0
2023-02-17 15:13:46,538 DEBUG TRAIN Batch 1/3300 loss 50.785549 loss_att 105.793045 loss_ctc 54.720299 loss_rnnt 39.181541 hw_loss 0.146015 lr 0.00046516 rank 3
2023-02-17 15:14:59,315 DEBUG TRAIN Batch 1/3400 loss 54.985100 loss_att 110.358688 loss_ctc 66.308876 loss_rnnt 42.366425 hw_loss 0.063973 lr 0.00046976 rank 0
2023-02-17 15:14:59,316 DEBUG TRAIN Batch 1/3400 loss 43.723175 loss_att 101.719467 loss_ctc 59.637779 loss_rnnt 29.943848 hw_loss 0.108979 lr 0.00046908 rank 4
2023-02-17 15:14:59,318 DEBUG TRAIN Batch 1/3400 loss 86.540443 loss_att 138.564545 loss_ctc 103.945755 loss_rnnt 73.779610 hw_loss 0.066180 lr 0.00046988 rank 6
2023-02-17 15:14:59,318 DEBUG TRAIN Batch 1/3400 loss 78.231133 loss_att 151.284943 loss_ctc 98.077484 loss_rnnt 60.919807 hw_loss 0.101972 lr 0.00047120 rank 1
2023-02-17 15:14:59,321 DEBUG TRAIN Batch 1/3400 loss 106.969894 loss_att 169.583344 loss_ctc 140.905609 loss_rnnt 89.862228 hw_loss 0.112923 lr 0.00047000 rank 2
2023-02-17 15:14:59,323 DEBUG TRAIN Batch 1/3400 loss 64.915901 loss_att 109.451485 loss_ctc 77.038666 loss_rnnt 54.299236 hw_loss 0.174711 lr 0.00046916 rank 3
2023-02-17 15:14:59,324 DEBUG TRAIN Batch 1/3400 loss 81.502304 loss_att 145.031235 loss_ctc 103.229790 loss_rnnt 65.854889 hw_loss 0.083669 lr 0.00046952 rank 5
2023-02-17 15:14:59,371 DEBUG TRAIN Batch 1/3400 loss 64.383827 loss_att 115.160339 loss_ctc 86.461197 loss_rnnt 51.157074 hw_loss 0.239629 lr 0.00046996 rank 7
2023-02-17 15:16:12,671 DEBUG TRAIN Batch 1/3500 loss 60.459301 loss_att 117.513550 loss_ctc 73.467529 loss_rnnt 47.313805 hw_loss 0.000408 lr 0.00047388 rank 6
2023-02-17 15:16:12,683 DEBUG TRAIN Batch 1/3500 loss 49.472664 loss_att 100.659538 loss_ctc 62.450008 loss_rnnt 37.503525 hw_loss 0.002728 lr 0.00047400 rank 2
2023-02-17 15:16:12,688 DEBUG TRAIN Batch 1/3500 loss 30.839033 loss_att 90.412590 loss_ctc 32.745728 loss_rnnt 18.649954 hw_loss 0.037769 lr 0.00047520 rank 1
2023-02-17 15:16:12,690 DEBUG TRAIN Batch 1/3500 loss 54.176685 loss_att 109.642700 loss_ctc 63.433880 loss_rnnt 41.834187 hw_loss 0.028124 lr 0.00047352 rank 5
2023-02-17 15:16:12,690 DEBUG TRAIN Batch 1/3500 loss 56.975182 loss_att 111.111816 loss_ctc 72.374184 loss_rnnt 44.044472 hw_loss 0.094089 lr 0.00047376 rank 0
2023-02-17 15:16:12,694 DEBUG TRAIN Batch 1/3500 loss 33.422134 loss_att 79.181725 loss_ctc 39.878704 loss_rnnt 23.367886 hw_loss 0.077725 lr 0.00047308 rank 4
2023-02-17 15:16:12,698 DEBUG TRAIN Batch 1/3500 loss 79.324051 loss_att 122.639854 loss_ctc 90.730431 loss_rnnt 69.139771 hw_loss 0.000495 lr 0.00047316 rank 3
2023-02-17 15:16:12,744 DEBUG TRAIN Batch 1/3500 loss 50.462162 loss_att 117.176727 loss_ctc 56.959663 loss_rnnt 36.232552 hw_loss 0.038171 lr 0.00047396 rank 7
2023-02-17 15:17:26,800 DEBUG TRAIN Batch 1/3600 loss 39.169102 loss_att 81.481316 loss_ctc 46.012276 loss_rnnt 29.723801 hw_loss 0.132061 lr 0.00047788 rank 6
2023-02-17 15:17:26,800 DEBUG TRAIN Batch 1/3600 loss 47.274200 loss_att 95.539642 loss_ctc 58.563812 loss_rnnt 36.101112 hw_loss 0.027594 lr 0.00047752 rank 5
2023-02-17 15:17:26,800 DEBUG TRAIN Batch 1/3600 loss 59.585934 loss_att 117.535210 loss_ctc 73.546906 loss_rnnt 46.103848 hw_loss 0.057684 lr 0.00047920 rank 1
2023-02-17 15:17:26,805 DEBUG TRAIN Batch 1/3600 loss 56.641949 loss_att 104.793182 loss_ctc 65.962387 loss_rnnt 45.749264 hw_loss 0.036963 lr 0.00047796 rank 7
2023-02-17 15:17:26,806 DEBUG TRAIN Batch 1/3600 loss 76.188126 loss_att 125.131111 loss_ctc 96.949005 loss_rnnt 63.617069 hw_loss 0.026893 lr 0.00047800 rank 2
2023-02-17 15:17:26,805 DEBUG TRAIN Batch 1/3600 loss 46.088577 loss_att 86.722984 loss_ctc 58.724926 loss_rnnt 36.149002 hw_loss 0.239704 lr 0.00047776 rank 0
2023-02-17 15:17:26,806 DEBUG TRAIN Batch 1/3600 loss 65.991829 loss_att 122.026947 loss_ctc 76.692093 loss_rnnt 53.309078 hw_loss 0.091912 lr 0.00047708 rank 4
2023-02-17 15:17:26,809 DEBUG TRAIN Batch 1/3600 loss 66.382301 loss_att 116.481079 loss_ctc 80.842537 loss_rnnt 54.401127 hw_loss 0.062601 lr 0.00047716 rank 3
2023-02-17 15:18:39,392 DEBUG TRAIN Batch 1/3700 loss 61.237446 loss_att 113.757156 loss_ctc 82.961700 loss_rnnt 47.759270 hw_loss 0.145632 lr 0.00048320 rank 1
2023-02-17 15:18:39,393 DEBUG TRAIN Batch 1/3700 loss 20.644602 loss_att 25.315372 loss_ctc 24.682880 loss_rnnt 19.021011 hw_loss 0.283124 lr 0.00048176 rank 0
2023-02-17 15:18:39,394 DEBUG TRAIN Batch 1/3700 loss 55.735626 loss_att 79.789574 loss_ctc 66.686615 loss_rnnt 49.400890 hw_loss 0.119651 lr 0.00048188 rank 6
2023-02-17 15:18:39,394 DEBUG TRAIN Batch 1/3700 loss 48.506821 loss_att 93.977249 loss_ctc 57.792824 loss_rnnt 38.146740 hw_loss 0.052238 lr 0.00048108 rank 4
2023-02-17 15:18:39,394 DEBUG TRAIN Batch 1/3700 loss 43.765503 loss_att 79.301895 loss_ctc 55.238041 loss_rnnt 35.104446 hw_loss 0.045205 lr 0.00048116 rank 3
2023-02-17 15:18:39,396 DEBUG TRAIN Batch 1/3700 loss 45.539631 loss_att 85.483376 loss_ctc 55.696957 loss_rnnt 36.148281 hw_loss 0.090554 lr 0.00048196 rank 7
2023-02-17 15:18:39,430 DEBUG TRAIN Batch 1/3700 loss 40.393124 loss_att 71.965530 loss_ctc 47.478977 loss_rnnt 33.049580 hw_loss 0.158023 lr 0.00048200 rank 2
2023-02-17 15:18:39,433 DEBUG TRAIN Batch 1/3700 loss 44.907295 loss_att 94.215637 loss_ctc 53.348526 loss_rnnt 33.868706 hw_loss 0.096421 lr 0.00048152 rank 5
2023-02-17 15:19:52,439 DEBUG TRAIN Batch 1/3800 loss 57.455170 loss_att 91.285599 loss_ctc 71.579849 loss_rnnt 48.793762 hw_loss 0.022551 lr 0.00048508 rank 4
2023-02-17 15:19:52,440 DEBUG TRAIN Batch 1/3800 loss 47.871422 loss_att 92.386642 loss_ctc 57.708855 loss_rnnt 37.595760 hw_loss 0.114296 lr 0.00048576 rank 0
2023-02-17 15:19:52,444 DEBUG TRAIN Batch 1/3800 loss 55.333641 loss_att 110.603363 loss_ctc 65.214981 loss_rnnt 42.935867 hw_loss 0.049344 lr 0.00048588 rank 6
2023-02-17 15:19:52,448 DEBUG TRAIN Batch 1/3800 loss 42.845539 loss_att 72.307983 loss_ctc 53.803772 loss_rnnt 35.425533 hw_loss 0.124536 lr 0.00048720 rank 1
2023-02-17 15:19:52,453 DEBUG TRAIN Batch 1/3800 loss 44.171097 loss_att 87.842041 loss_ctc 47.236103 loss_rnnt 34.872063 hw_loss 0.292839 lr 0.00048600 rank 2
2023-02-17 15:19:52,454 DEBUG TRAIN Batch 1/3800 loss 40.852936 loss_att 62.963326 loss_ctc 49.265705 loss_rnnt 35.290810 hw_loss 0.034401 lr 0.00048516 rank 3
2023-02-17 15:19:52,464 DEBUG TRAIN Batch 1/3800 loss 44.646271 loss_att 80.884804 loss_ctc 50.979786 loss_rnnt 36.508499 hw_loss 0.085488 lr 0.00048552 rank 5
2023-02-17 15:19:52,492 DEBUG TRAIN Batch 1/3800 loss 50.239925 loss_att 100.693138 loss_ctc 59.917755 loss_rnnt 38.842911 hw_loss 0.029997 lr 0.00048596 rank 7
2023-02-17 15:21:07,687 DEBUG TRAIN Batch 1/3900 loss 43.815941 loss_att 87.635475 loss_ctc 53.658615 loss_rnnt 33.672211 hw_loss 0.126500 lr 0.00049000 rank 2
2023-02-17 15:21:07,687 DEBUG TRAIN Batch 1/3900 loss 46.139427 loss_att 94.616272 loss_ctc 57.170380 loss_rnnt 34.965519 hw_loss 0.014527 lr 0.00048988 rank 6
2023-02-17 15:21:07,688 DEBUG TRAIN Batch 1/3900 loss 71.075073 loss_att 149.064575 loss_ctc 87.323021 loss_rnnt 53.299725 hw_loss 0.020709 lr 0.00048952 rank 5
2023-02-17 15:21:07,690 DEBUG TRAIN Batch 1/3900 loss 52.975872 loss_att 116.564087 loss_ctc 66.154526 loss_rnnt 38.386227 hw_loss 0.215348 lr 0.00048976 rank 0
2023-02-17 15:21:07,691 DEBUG TRAIN Batch 1/3900 loss 60.517174 loss_att 113.915962 loss_ctc 69.363007 loss_rnnt 48.635643 hw_loss 0.041862 lr 0.00048908 rank 4
2023-02-17 15:21:07,691 DEBUG TRAIN Batch 1/3900 loss 41.888634 loss_att 60.189316 loss_ctc 48.603874 loss_rnnt 37.274067 hw_loss 0.110746 lr 0.00048996 rank 7
2023-02-17 15:21:07,691 DEBUG TRAIN Batch 1/3900 loss 27.128017 loss_att 38.407375 loss_ctc 32.697639 loss_rnnt 24.047327 hw_loss 0.154127 lr 0.00049120 rank 1
2023-02-17 15:21:07,738 DEBUG TRAIN Batch 1/3900 loss 59.693829 loss_att 108.458595 loss_ctc 77.565346 loss_rnnt 47.519997 hw_loss 0.071267 lr 0.00048916 rank 3
2023-02-17 15:22:21,107 DEBUG TRAIN Batch 1/4000 loss 39.024433 loss_att 88.369598 loss_ctc 47.938107 loss_rnnt 27.940481 hw_loss 0.049555 lr 0.00049376 rank 0
2023-02-17 15:22:21,107 DEBUG TRAIN Batch 1/4000 loss 60.289658 loss_att 106.092743 loss_ctc 74.696411 loss_rnnt 49.203548 hw_loss 0.008607 lr 0.00049400 rank 2
2023-02-17 15:22:21,112 DEBUG TRAIN Batch 1/4000 loss 68.962761 loss_att 120.965286 loss_ctc 80.571823 loss_rnnt 57.012875 hw_loss 0.002825 lr 0.00049520 rank 1
2023-02-17 15:22:21,113 DEBUG TRAIN Batch 1/4000 loss 49.734768 loss_att 100.914848 loss_ctc 60.448792 loss_rnnt 37.973618 hw_loss 0.181115 lr 0.00049316 rank 3
2023-02-17 15:22:21,114 DEBUG TRAIN Batch 1/4000 loss 52.520996 loss_att 106.427956 loss_ctc 67.535423 loss_rnnt 39.693356 hw_loss 0.083101 lr 0.00049308 rank 4
2023-02-17 15:22:21,116 DEBUG TRAIN Batch 1/4000 loss 73.934540 loss_att 118.897095 loss_ctc 88.502731 loss_rnnt 62.961185 hw_loss 0.072018 lr 0.00049388 rank 6
2023-02-17 15:22:21,117 DEBUG TRAIN Batch 1/4000 loss 42.787724 loss_att 95.279312 loss_ctc 45.921814 loss_rnnt 31.871357 hw_loss 0.000312 lr 0.00049352 rank 5
2023-02-17 15:22:21,168 DEBUG TRAIN Batch 1/4000 loss 58.454266 loss_att 121.851700 loss_ctc 65.611519 loss_rnnt 44.786591 hw_loss 0.063530 lr 0.00049396 rank 7
2023-02-17 15:23:33,875 DEBUG TRAIN Batch 1/4100 loss 66.233597 loss_att 109.439522 loss_ctc 81.744286 loss_rnnt 55.524090 hw_loss 0.000426 lr 0.00049752 rank 5
2023-02-17 15:23:33,878 DEBUG TRAIN Batch 1/4100 loss 64.036674 loss_att 109.352043 loss_ctc 78.045998 loss_rnnt 53.078716 hw_loss 0.050583 lr 0.00049920 rank 1
2023-02-17 15:23:33,878 DEBUG TRAIN Batch 1/4100 loss 55.077087 loss_att 98.269997 loss_ctc 62.146435 loss_rnnt 45.448799 hw_loss 0.088345 lr 0.00049708 rank 4
2023-02-17 15:23:33,881 DEBUG TRAIN Batch 1/4100 loss 43.574997 loss_att 83.468216 loss_ctc 50.904690 loss_rnnt 34.618443 hw_loss 0.001154 lr 0.00049776 rank 0
2023-02-17 15:23:33,882 DEBUG TRAIN Batch 1/4100 loss 58.999584 loss_att 101.413406 loss_ctc 65.348221 loss_rnnt 49.596142 hw_loss 0.139109 lr 0.00049800 rank 2
2023-02-17 15:23:33,882 DEBUG TRAIN Batch 1/4100 loss 56.900101 loss_att 112.079742 loss_ctc 70.555328 loss_rnnt 43.973900 hw_loss 0.130449 lr 0.00049788 rank 6
2023-02-17 15:23:33,887 DEBUG TRAIN Batch 1/4100 loss 39.525875 loss_att 77.214851 loss_ctc 47.753662 loss_rnnt 30.869360 hw_loss 0.040651 lr 0.00049796 rank 7
2023-02-17 15:23:33,930 DEBUG TRAIN Batch 1/4100 loss 36.195354 loss_att 83.610779 loss_ctc 44.435406 loss_rnnt 25.591619 hw_loss 0.041210 lr 0.00049716 rank 3
2023-02-17 15:24:46,662 DEBUG TRAIN Batch 1/4200 loss 48.501068 loss_att 90.987144 loss_ctc 55.997681 loss_rnnt 38.951454 hw_loss 0.099097 lr 0.00050196 rank 7
2023-02-17 15:24:46,667 DEBUG TRAIN Batch 1/4200 loss 60.357761 loss_att 104.448601 loss_ctc 67.694397 loss_rnnt 50.553200 hw_loss 0.015321 lr 0.00050320 rank 1
2023-02-17 15:24:46,671 DEBUG TRAIN Batch 1/4200 loss 41.991291 loss_att 86.215630 loss_ctc 55.433517 loss_rnnt 31.248398 hw_loss 0.198241 lr 0.00050108 rank 4
2023-02-17 15:24:46,673 DEBUG TRAIN Batch 1/4200 loss 66.876732 loss_att 111.560104 loss_ctc 83.845444 loss_rnnt 55.593407 hw_loss 0.157797 lr 0.00050176 rank 0
2023-02-17 15:24:46,672 DEBUG TRAIN Batch 1/4200 loss 52.350273 loss_att 99.538284 loss_ctc 73.262009 loss_rnnt 40.101612 hw_loss 0.042796 lr 0.00050152 rank 5
2023-02-17 15:24:46,674 DEBUG TRAIN Batch 1/4200 loss 41.197464 loss_att 73.796974 loss_ctc 52.346237 loss_rnnt 33.117203 hw_loss 0.138483 lr 0.00050188 rank 6
2023-02-17 15:24:46,675 DEBUG TRAIN Batch 1/4200 loss 50.980404 loss_att 95.964142 loss_ctc 66.561737 loss_rnnt 39.872845 hw_loss 0.062436 lr 0.00050116 rank 3
2023-02-17 15:24:46,686 DEBUG TRAIN Batch 1/4200 loss 46.700352 loss_att 79.834641 loss_ctc 63.153782 loss_rnnt 37.798481 hw_loss 0.152294 lr 0.00050200 rank 2
2023-02-17 15:26:02,237 DEBUG TRAIN Batch 1/4300 loss 42.579868 loss_att 71.631134 loss_ctc 54.971428 loss_rnnt 35.026310 hw_loss 0.170807 lr 0.00050508 rank 4
2023-02-17 15:26:02,243 DEBUG TRAIN Batch 1/4300 loss 43.851223 loss_att 64.450417 loss_ctc 53.173794 loss_rnnt 38.477364 hw_loss 0.020646 lr 0.00050600 rank 2
2023-02-17 15:26:02,245 DEBUG TRAIN Batch 1/4300 loss 92.541565 loss_att 129.275375 loss_ctc 101.070297 loss_rnnt 84.042007 hw_loss 0.029299 lr 0.00050516 rank 3
2023-02-17 15:26:02,246 DEBUG TRAIN Batch 1/4300 loss 44.115852 loss_att 68.050705 loss_ctc 54.615429 loss_rnnt 37.898724 hw_loss 0.056648 lr 0.00050588 rank 6
2023-02-17 15:26:02,247 DEBUG TRAIN Batch 1/4300 loss 43.997429 loss_att 58.611687 loss_ctc 52.253124 loss_rnnt 39.925346 hw_loss 0.090889 lr 0.00050576 rank 0
2023-02-17 15:26:02,249 DEBUG TRAIN Batch 1/4300 loss 38.104359 loss_att 70.332520 loss_ctc 42.608479 loss_rnnt 31.024790 hw_loss 0.062598 lr 0.00050720 rank 1
2023-02-17 15:26:02,254 DEBUG TRAIN Batch 1/4300 loss 34.691383 loss_att 69.540459 loss_ctc 42.112850 loss_rnnt 26.683531 hw_loss 0.090949 lr 0.00050596 rank 7
2023-02-17 15:26:02,295 DEBUG TRAIN Batch 1/4300 loss 54.624615 loss_att 103.026642 loss_ctc 80.392654 loss_rnnt 41.447235 hw_loss 0.114810 lr 0.00050552 rank 5
2023-02-17 15:27:15,288 DEBUG TRAIN Batch 1/4400 loss 43.697346 loss_att 60.759232 loss_ctc 56.473766 loss_rnnt 38.509563 hw_loss 0.134779 lr 0.00050908 rank 4
2023-02-17 15:27:15,292 DEBUG TRAIN Batch 1/4400 loss 62.073299 loss_att 118.112991 loss_ctc 75.611176 loss_rnnt 48.962296 hw_loss 0.183773 lr 0.00050988 rank 6
2023-02-17 15:27:15,293 DEBUG TRAIN Batch 1/4400 loss 39.205612 loss_att 65.811203 loss_ctc 51.279079 loss_rnnt 32.224083 hw_loss 0.094904 lr 0.00050996 rank 7
2023-02-17 15:27:15,296 DEBUG TRAIN Batch 1/4400 loss 83.353821 loss_att 129.253693 loss_ctc 97.599991 loss_rnnt 72.253784 hw_loss 0.038563 lr 0.00050976 rank 0
2023-02-17 15:27:15,294 DEBUG TRAIN Batch 1/4400 loss 23.041622 loss_att 29.006382 loss_ctc 31.830708 loss_rnnt 20.597750 hw_loss 0.148200 lr 0.00051000 rank 2
2023-02-17 15:27:15,296 DEBUG TRAIN Batch 1/4400 loss 54.552864 loss_att 89.364532 loss_ctc 70.898056 loss_rnnt 45.387711 hw_loss 0.043988 lr 0.00051120 rank 1
2023-02-17 15:27:15,297 DEBUG TRAIN Batch 1/4400 loss 56.931698 loss_att 94.120041 loss_ctc 72.019218 loss_rnnt 47.430462 hw_loss 0.097301 lr 0.00050952 rank 5
2023-02-17 15:27:15,300 DEBUG TRAIN Batch 1/4400 loss 43.886669 loss_att 66.412094 loss_ctc 57.310818 loss_rnnt 37.515133 hw_loss 0.143559 lr 0.00050916 rank 3
2023-02-17 15:28:27,857 DEBUG TRAIN Batch 1/4500 loss 69.089668 loss_att 116.200333 loss_ctc 80.175491 loss_rnnt 58.105530 hw_loss 0.157299 lr 0.00051376 rank 0
2023-02-17 15:28:27,858 DEBUG TRAIN Batch 1/4500 loss 42.883945 loss_att 89.542931 loss_ctc 66.188126 loss_rnnt 30.386414 hw_loss 0.109705 lr 0.00051388 rank 6
2023-02-17 15:28:27,860 DEBUG TRAIN Batch 1/4500 loss 24.003157 loss_att 26.939449 loss_ctc 29.095064 loss_rnnt 22.645233 hw_loss 0.172018 lr 0.00051308 rank 4
2023-02-17 15:28:27,863 DEBUG TRAIN Batch 1/4500 loss 74.982231 loss_att 118.718742 loss_ctc 87.222130 loss_rnnt 64.572739 hw_loss 0.056624 lr 0.00051400 rank 2
2023-02-17 15:28:27,864 DEBUG TRAIN Batch 1/4500 loss 49.636650 loss_att 83.474998 loss_ctc 56.464184 loss_rnnt 41.885418 hw_loss 0.137295 lr 0.00051520 rank 1
2023-02-17 15:28:27,865 DEBUG TRAIN Batch 1/4500 loss 86.399185 loss_att 167.391266 loss_ctc 99.301620 loss_rnnt 68.398804 hw_loss 0.153075 lr 0.00051316 rank 3
2023-02-17 15:28:27,869 DEBUG TRAIN Batch 1/4500 loss 50.310799 loss_att 70.334122 loss_ctc 58.880836 loss_rnnt 45.147057 hw_loss 0.030767 lr 0.00051396 rank 7
2023-02-17 15:28:27,870 DEBUG TRAIN Batch 1/4500 loss 30.288542 loss_att 41.273529 loss_ctc 40.275455 loss_rnnt 26.699768 hw_loss 0.112851 lr 0.00051352 rank 5
2023-02-17 15:29:43,493 DEBUG TRAIN Batch 1/4600 loss 56.609230 loss_att 94.838699 loss_ctc 73.295372 loss_rnnt 46.738384 hw_loss 0.000246 lr 0.00051716 rank 3
2023-02-17 15:29:43,501 DEBUG TRAIN Batch 1/4600 loss 44.343147 loss_att 77.461342 loss_ctc 59.075500 loss_rnnt 35.689743 hw_loss 0.122717 lr 0.00051800 rank 2
2023-02-17 15:29:43,507 DEBUG TRAIN Batch 1/4600 loss 66.304314 loss_att 102.478897 loss_ctc 78.192711 loss_rnnt 57.452682 hw_loss 0.059250 lr 0.00051776 rank 0
2023-02-17 15:29:43,508 DEBUG TRAIN Batch 1/4600 loss 61.711746 loss_att 107.298264 loss_ctc 70.480751 loss_rnnt 51.402813 hw_loss 0.042059 lr 0.00051796 rank 7
2023-02-17 15:29:43,509 DEBUG TRAIN Batch 1/4600 loss 30.855682 loss_att 41.068226 loss_ctc 40.744755 loss_rnnt 27.425552 hw_loss 0.129518 lr 0.00051920 rank 1
2023-02-17 15:29:43,509 DEBUG TRAIN Batch 1/4600 loss 63.543861 loss_att 110.276520 loss_ctc 83.912102 loss_rnnt 51.390026 hw_loss 0.171621 lr 0.00051788 rank 6
2023-02-17 15:29:43,509 DEBUG TRAIN Batch 1/4600 loss 58.758904 loss_att 107.735771 loss_ctc 76.655884 loss_rnnt 46.515495 hw_loss 0.115828 lr 0.00051752 rank 5
2023-02-17 15:29:43,510 DEBUG TRAIN Batch 1/4600 loss 52.703617 loss_att 97.124817 loss_ctc 67.153183 loss_rnnt 41.838272 hw_loss 0.102183 lr 0.00051708 rank 4
2023-02-17 15:30:56,252 DEBUG TRAIN Batch 1/4700 loss 32.885372 loss_att 62.957245 loss_ctc 38.975037 loss_rnnt 25.986813 hw_loss 0.135427 lr 0.00052108 rank 4
2023-02-17 15:30:56,253 DEBUG TRAIN Batch 1/4700 loss 58.756683 loss_att 89.576729 loss_ctc 69.622543 loss_rnnt 51.063370 hw_loss 0.150990 lr 0.00052188 rank 6
2023-02-17 15:30:56,258 DEBUG TRAIN Batch 1/4700 loss 72.492828 loss_att 115.697624 loss_ctc 88.100410 loss_rnnt 61.741554 hw_loss 0.054931 lr 0.00052200 rank 2
2023-02-17 15:30:56,258 DEBUG TRAIN Batch 1/4700 loss 39.288811 loss_att 75.613945 loss_ctc 52.026165 loss_rnnt 30.302017 hw_loss 0.043964 lr 0.00052152 rank 5
2023-02-17 15:30:56,259 DEBUG TRAIN Batch 1/4700 loss 53.101688 loss_att 92.322762 loss_ctc 56.959667 loss_rnnt 44.742828 hw_loss 0.000456 lr 0.00052196 rank 7
2023-02-17 15:30:56,262 DEBUG TRAIN Batch 1/4700 loss 36.380989 loss_att 86.631157 loss_ctc 50.542252 loss_rnnt 24.442627 hw_loss 0.000298 lr 0.00052320 rank 1
2023-02-17 15:30:56,262 DEBUG TRAIN Batch 1/4700 loss 73.064934 loss_att 110.389954 loss_ctc 81.051453 loss_rnnt 64.478729 hw_loss 0.105608 lr 0.00052116 rank 3
2023-02-17 15:30:56,263 DEBUG TRAIN Batch 1/4700 loss 47.733620 loss_att 91.241631 loss_ctc 63.121487 loss_rnnt 36.901390 hw_loss 0.147959 lr 0.00052176 rank 0
2023-02-17 15:32:08,643 DEBUG TRAIN Batch 1/4800 loss 73.680946 loss_att 116.052650 loss_ctc 90.302444 loss_rnnt 62.979431 hw_loss 0.020583 lr 0.00052508 rank 4
2023-02-17 15:32:08,645 DEBUG TRAIN Batch 1/4800 loss 47.047497 loss_att 95.278961 loss_ctc 64.199890 loss_rnnt 35.066761 hw_loss 0.088982 lr 0.00052588 rank 6
2023-02-17 15:32:08,645 DEBUG TRAIN Batch 1/4800 loss 42.049622 loss_att 77.125328 loss_ctc 54.212250 loss_rnnt 33.371235 hw_loss 0.077924 lr 0.00052576 rank 0
2023-02-17 15:32:08,648 DEBUG TRAIN Batch 1/4800 loss 43.658485 loss_att 80.827782 loss_ctc 50.749290 loss_rnnt 35.257622 hw_loss 0.040422 lr 0.00052600 rank 2
2023-02-17 15:32:08,651 DEBUG TRAIN Batch 1/4800 loss 41.372696 loss_att 72.662537 loss_ctc 51.517059 loss_rnnt 33.686886 hw_loss 0.141119 lr 0.00052720 rank 1
2023-02-17 15:32:08,651 DEBUG TRAIN Batch 1/4800 loss 58.967831 loss_att 100.700836 loss_ctc 84.921677 loss_rnnt 47.131557 hw_loss 0.054676 lr 0.00052596 rank 7
2023-02-17 15:32:08,652 DEBUG TRAIN Batch 1/4800 loss 47.198349 loss_att 91.463348 loss_ctc 57.617199 loss_rnnt 36.926949 hw_loss 0.054787 lr 0.00052552 rank 5
2023-02-17 15:32:08,658 DEBUG TRAIN Batch 1/4800 loss 56.125320 loss_att 99.644928 loss_ctc 71.528473 loss_rnnt 45.289772 hw_loss 0.146007 lr 0.00052516 rank 3
2023-02-17 15:33:21,727 DEBUG TRAIN Batch 1/4900 loss 44.609386 loss_att 69.589867 loss_ctc 57.165005 loss_rnnt 37.937794 hw_loss 0.002652 lr 0.00052976 rank 0
2023-02-17 15:33:21,729 DEBUG TRAIN Batch 1/4900 loss 41.633389 loss_att 77.694633 loss_ctc 50.519917 loss_rnnt 33.185822 hw_loss 0.094587 lr 0.00052988 rank 6
2023-02-17 15:33:21,729 DEBUG TRAIN Batch 1/4900 loss 46.089451 loss_att 76.400330 loss_ctc 57.755585 loss_rnnt 38.430561 hw_loss 0.077306 lr 0.00052996 rank 7
2023-02-17 15:33:21,730 DEBUG TRAIN Batch 1/4900 loss 50.454819 loss_att 94.978714 loss_ctc 65.364899 loss_rnnt 39.519474 hw_loss 0.079781 lr 0.00052908 rank 4
2023-02-17 15:33:21,730 DEBUG TRAIN Batch 1/4900 loss 35.854984 loss_att 68.063812 loss_ctc 50.253479 loss_rnnt 27.468273 hw_loss 0.047152 lr 0.00052916 rank 3
2023-02-17 15:33:21,732 DEBUG TRAIN Batch 1/4900 loss 32.024399 loss_att 57.617378 loss_ctc 43.035439 loss_rnnt 25.423634 hw_loss 0.026301 lr 0.00053000 rank 2
2023-02-17 15:33:21,733 DEBUG TRAIN Batch 1/4900 loss 43.505390 loss_att 80.805458 loss_ctc 47.954918 loss_rnnt 35.402611 hw_loss 0.092801 lr 0.00052952 rank 5
2023-02-17 15:33:21,746 DEBUG TRAIN Batch 1/4900 loss 69.623146 loss_att 117.093636 loss_ctc 88.518478 loss_rnnt 57.586948 hw_loss 0.042599 lr 0.00053120 rank 1
2023-02-17 15:34:36,233 DEBUG TRAIN Batch 1/5000 loss 34.708080 loss_att 56.901741 loss_ctc 43.274773 loss_rnnt 29.075081 hw_loss 0.097570 lr 0.00053400 rank 2
2023-02-17 15:34:36,249 DEBUG TRAIN Batch 1/5000 loss 46.068043 loss_att 73.270851 loss_ctc 63.368675 loss_rnnt 38.295792 hw_loss 0.046759 lr 0.00053352 rank 5
2023-02-17 15:34:36,253 DEBUG TRAIN Batch 1/5000 loss 43.137814 loss_att 80.314819 loss_ctc 54.393394 loss_rnnt 34.157288 hw_loss 0.083214 lr 0.00053520 rank 1
2023-02-17 15:34:36,253 DEBUG TRAIN Batch 1/5000 loss 33.664036 loss_att 61.089226 loss_ctc 44.160927 loss_rnnt 26.730999 hw_loss 0.090776 lr 0.00053308 rank 4
2023-02-17 15:34:36,254 DEBUG TRAIN Batch 1/5000 loss 54.044216 loss_att 77.932831 loss_ctc 65.232170 loss_rnnt 47.737156 hw_loss 0.070515 lr 0.00053316 rank 3
2023-02-17 15:34:36,254 DEBUG TRAIN Batch 1/5000 loss 32.610569 loss_att 45.163689 loss_ctc 40.354218 loss_rnnt 29.015629 hw_loss 0.097179 lr 0.00053388 rank 6
2023-02-17 15:34:36,258 DEBUG TRAIN Batch 1/5000 loss 60.710781 loss_att 92.565262 loss_ctc 75.413200 loss_rnnt 52.286949 hw_loss 0.173650 lr 0.00053396 rank 7
2023-02-17 15:34:36,260 DEBUG TRAIN Batch 1/5000 loss 56.810448 loss_att 100.697586 loss_ctc 76.094925 loss_rnnt 45.461548 hw_loss 0.000395 lr 0.00053376 rank 0
2023-02-17 15:35:48,812 DEBUG TRAIN Batch 1/5100 loss 37.020218 loss_att 62.930458 loss_ctc 51.049740 loss_rnnt 29.948410 hw_loss 0.035919 lr 0.00053716 rank 3
2023-02-17 15:35:48,819 DEBUG TRAIN Batch 1/5100 loss 31.507051 loss_att 38.066887 loss_ctc 40.529163 loss_rnnt 28.889215 hw_loss 0.192971 lr 0.00053708 rank 4
2023-02-17 15:35:48,819 DEBUG TRAIN Batch 1/5100 loss 36.420887 loss_att 75.047417 loss_ctc 54.497955 loss_rnnt 26.185177 hw_loss 0.187752 lr 0.00053800 rank 2
2023-02-17 15:35:48,821 DEBUG TRAIN Batch 1/5100 loss 43.155331 loss_att 61.510059 loss_ctc 52.909904 loss_rnnt 38.123077 hw_loss 0.113811 lr 0.00053752 rank 5
2023-02-17 15:35:48,822 DEBUG TRAIN Batch 1/5100 loss 46.197716 loss_att 68.965485 loss_ctc 59.627136 loss_rnnt 39.824688 hw_loss 0.054156 lr 0.00053796 rank 7
2023-02-17 15:35:48,826 DEBUG TRAIN Batch 1/5100 loss 53.811142 loss_att 86.519623 loss_ctc 72.288521 loss_rnnt 44.805553 hw_loss 0.000456 lr 0.00053776 rank 0
2023-02-17 15:35:48,827 DEBUG TRAIN Batch 1/5100 loss 68.259178 loss_att 110.645981 loss_ctc 87.208176 loss_rnnt 57.185158 hw_loss 0.131464 lr 0.00053788 rank 6
2023-02-17 15:35:48,870 DEBUG TRAIN Batch 1/5100 loss 51.864491 loss_att 74.620949 loss_ctc 67.228455 loss_rnnt 45.244331 hw_loss 0.038132 lr 0.00053920 rank 1
2023-02-17 15:37:01,794 DEBUG TRAIN Batch 1/5200 loss 61.581741 loss_att 107.604019 loss_ctc 77.936798 loss_rnnt 50.184719 hw_loss 0.022296 lr 0.00054176 rank 0
2023-02-17 15:37:01,795 DEBUG TRAIN Batch 1/5200 loss 53.156052 loss_att 91.118385 loss_ctc 67.409454 loss_rnnt 43.655792 hw_loss 0.013761 lr 0.00054188 rank 6
2023-02-17 15:37:01,796 DEBUG TRAIN Batch 1/5200 loss 65.176025 loss_att 106.467056 loss_ctc 83.810074 loss_rnnt 54.424099 hw_loss 0.017198 lr 0.00054200 rank 2
2023-02-17 15:37:01,797 DEBUG TRAIN Batch 1/5200 loss 52.844486 loss_att 94.479889 loss_ctc 58.745235 loss_rnnt 43.707699 hw_loss 0.043017 lr 0.00054108 rank 4
2023-02-17 15:37:01,798 DEBUG TRAIN Batch 1/5200 loss 62.550251 loss_att 118.588898 loss_ctc 79.533073 loss_rnnt 49.066051 hw_loss 0.022670 lr 0.00054152 rank 5
2023-02-17 15:37:01,799 DEBUG TRAIN Batch 1/5200 loss 45.991852 loss_att 90.624420 loss_ctc 56.531326 loss_rnnt 35.614914 hw_loss 0.084674 lr 0.00054196 rank 7
2023-02-17 15:37:01,799 DEBUG TRAIN Batch 1/5200 loss 38.254524 loss_att 56.079781 loss_ctc 57.096191 loss_rnnt 32.162472 hw_loss 0.027711 lr 0.00054320 rank 1
2023-02-17 15:37:01,805 DEBUG TRAIN Batch 1/5200 loss 55.656971 loss_att 92.040886 loss_ctc 68.066116 loss_rnnt 46.701336 hw_loss 0.045562 lr 0.00054116 rank 3
2023-02-17 15:38:15,772 DEBUG TRAIN Batch 1/5300 loss 51.146004 loss_att 94.696785 loss_ctc 70.673920 loss_rnnt 39.792793 hw_loss 0.073749 lr 0.00054600 rank 2
2023-02-17 15:38:15,772 DEBUG TRAIN Batch 1/5300 loss 47.716846 loss_att 76.267326 loss_ctc 58.017727 loss_rnnt 40.620018 hw_loss 0.024897 lr 0.00054588 rank 6
2023-02-17 15:38:15,773 DEBUG TRAIN Batch 1/5300 loss 41.765438 loss_att 84.184006 loss_ctc 60.504375 loss_rnnt 30.745243 hw_loss 0.071165 lr 0.00054508 rank 4
2023-02-17 15:38:15,773 DEBUG TRAIN Batch 1/5300 loss 62.905216 loss_att 104.088577 loss_ctc 90.875671 loss_rnnt 50.917797 hw_loss 0.040043 lr 0.00054596 rank 7
2023-02-17 15:38:15,776 DEBUG TRAIN Batch 1/5300 loss 57.114773 loss_att 92.576523 loss_ctc 75.044243 loss_rnnt 47.629143 hw_loss 0.005038 lr 0.00054552 rank 5
2023-02-17 15:38:15,777 DEBUG TRAIN Batch 1/5300 loss 66.044586 loss_att 105.170197 loss_ctc 87.871040 loss_rnnt 55.287315 hw_loss 0.041169 lr 0.00054576 rank 0
2023-02-17 15:38:15,780 DEBUG TRAIN Batch 1/5300 loss 156.270111 loss_att 204.574341 loss_ctc 188.872070 loss_rnnt 142.233734 hw_loss 0.053633 lr 0.00054516 rank 3
2023-02-17 15:38:15,822 DEBUG TRAIN Batch 1/5300 loss 41.519268 loss_att 82.839165 loss_ctc 63.638153 loss_rnnt 30.305908 hw_loss 0.000360 lr 0.00054720 rank 1
2023-02-17 15:39:29,046 DEBUG TRAIN Batch 1/5400 loss 48.327538 loss_att 73.537621 loss_ctc 59.721809 loss_rnnt 41.713665 hw_loss 0.098663 lr 0.00054908 rank 4
2023-02-17 15:39:29,050 DEBUG TRAIN Batch 1/5400 loss 54.824043 loss_att 94.067375 loss_ctc 67.239670 loss_rnnt 45.297844 hw_loss 0.041468 lr 0.00055120 rank 1
2023-02-17 15:39:29,051 DEBUG TRAIN Batch 1/5400 loss 45.950012 loss_att 68.679199 loss_ctc 58.715054 loss_rnnt 39.630959 hw_loss 0.133525 lr 0.00054976 rank 0
2023-02-17 15:39:29,053 DEBUG TRAIN Batch 1/5400 loss 40.135567 loss_att 74.704948 loss_ctc 52.289566 loss_rnnt 31.591677 hw_loss 0.017778 lr 0.00054916 rank 3
2023-02-17 15:39:29,053 DEBUG TRAIN Batch 1/5400 loss 42.228436 loss_att 68.889160 loss_ctc 60.940453 loss_rnnt 34.345406 hw_loss 0.104902 lr 0.00054988 rank 6
2023-02-17 15:39:29,056 DEBUG TRAIN Batch 1/5400 loss 80.403046 loss_att 125.729126 loss_ctc 97.012840 loss_rnnt 69.048141 hw_loss 0.140717 lr 0.00054952 rank 5
2023-02-17 15:39:29,059 DEBUG TRAIN Batch 1/5400 loss 61.414608 loss_att 92.538765 loss_ctc 85.505699 loss_rnnt 51.961437 hw_loss 0.030355 lr 0.00054996 rank 7
2023-02-17 15:39:29,060 DEBUG TRAIN Batch 1/5400 loss 37.976582 loss_att 75.621521 loss_ctc 46.266739 loss_rnnt 29.341887 hw_loss 0.000660 lr 0.00055000 rank 2
2023-02-17 15:40:41,743 DEBUG TRAIN Batch 1/5500 loss 72.248199 loss_att 99.126198 loss_ctc 87.623924 loss_rnnt 64.801773 hw_loss 0.038875 lr 0.00055352 rank 5
2023-02-17 15:40:41,758 DEBUG TRAIN Batch 1/5500 loss 52.008442 loss_att 87.788795 loss_ctc 64.773178 loss_rnnt 43.111526 hw_loss 0.072899 lr 0.00055308 rank 4
2023-02-17 15:40:41,759 DEBUG TRAIN Batch 1/5500 loss 39.714916 loss_att 65.230736 loss_ctc 54.336674 loss_rnnt 32.609745 hw_loss 0.098317 lr 0.00055388 rank 6
2023-02-17 15:40:41,764 DEBUG TRAIN Batch 1/5500 loss 60.999954 loss_att 103.354675 loss_ctc 78.332108 loss_rnnt 50.180672 hw_loss 0.070093 lr 0.00055396 rank 7
2023-02-17 15:40:41,764 DEBUG TRAIN Batch 1/5500 loss 67.116920 loss_att 92.043129 loss_ctc 91.068405 loss_rnnt 58.938046 hw_loss 0.000195 lr 0.00055520 rank 1
2023-02-17 15:40:41,765 DEBUG TRAIN Batch 1/5500 loss 29.486893 loss_att 52.953087 loss_ctc 41.487785 loss_rnnt 23.157312 hw_loss 0.067916 lr 0.00055400 rank 2
2023-02-17 15:40:41,766 DEBUG TRAIN Batch 1/5500 loss 63.319046 loss_att 95.359764 loss_ctc 87.946838 loss_rnnt 53.605419 hw_loss 0.040825 lr 0.00055376 rank 0
2023-02-17 15:40:41,813 DEBUG TRAIN Batch 1/5500 loss 45.928944 loss_att 76.064102 loss_ctc 58.533089 loss_rnnt 38.187035 hw_loss 0.064360 lr 0.00055316 rank 3
2023-02-17 15:41:54,596 DEBUG TRAIN Batch 1/5600 loss 51.646690 loss_att 74.904312 loss_ctc 68.142883 loss_rnnt 44.771278 hw_loss 0.045741 lr 0.00055716 rank 3
2023-02-17 15:41:54,605 DEBUG TRAIN Batch 1/5600 loss 32.622864 loss_att 42.335297 loss_ctc 44.054619 loss_rnnt 29.106638 hw_loss 0.092817 lr 0.00055776 rank 0
2023-02-17 15:41:54,605 DEBUG TRAIN Batch 1/5600 loss 51.620758 loss_att 78.840111 loss_ctc 60.064884 loss_rnnt 44.978802 hw_loss 0.135381 lr 0.00055920 rank 1
2023-02-17 15:41:54,609 DEBUG TRAIN Batch 1/5600 loss 48.231461 loss_att 78.041756 loss_ctc 62.809929 loss_rnnt 40.249115 hw_loss 0.143415 lr 0.00055752 rank 5
2023-02-17 15:41:54,613 DEBUG TRAIN Batch 1/5600 loss 46.762150 loss_att 70.880608 loss_ctc 65.504646 loss_rnnt 39.381802 hw_loss 0.108111 lr 0.00055800 rank 2
2023-02-17 15:41:54,621 DEBUG TRAIN Batch 1/5600 loss 53.483982 loss_att 92.172394 loss_ctc 68.904160 loss_rnnt 43.651276 hw_loss 0.073128 lr 0.00055796 rank 7
2023-02-17 15:41:54,631 DEBUG TRAIN Batch 1/5600 loss 62.311817 loss_att 86.350479 loss_ctc 79.619179 loss_rnnt 55.140076 hw_loss 0.105678 lr 0.00055788 rank 6
2023-02-17 15:41:54,636 DEBUG TRAIN Batch 1/5600 loss 49.261509 loss_att 77.560463 loss_ctc 63.678688 loss_rnnt 41.620384 hw_loss 0.110714 lr 0.00055708 rank 4
2023-02-17 15:43:09,324 DEBUG TRAIN Batch 1/5700 loss 67.561684 loss_att 108.708557 loss_ctc 81.556999 loss_rnnt 57.414772 hw_loss 0.096542 lr 0.00056176 rank 0
2023-02-17 15:43:09,324 DEBUG TRAIN Batch 1/5700 loss 46.577965 loss_att 64.633949 loss_ctc 59.442513 loss_rnnt 41.210133 hw_loss 0.077555 lr 0.00056108 rank 4
2023-02-17 15:43:09,327 DEBUG TRAIN Batch 1/5700 loss 24.683189 loss_att 47.908363 loss_ctc 32.847462 loss_rnnt 18.910898 hw_loss 0.072537 lr 0.00056196 rank 7
2023-02-17 15:43:09,329 DEBUG TRAIN Batch 1/5700 loss 48.663254 loss_att 76.145325 loss_ctc 56.710949 loss_rnnt 42.050610 hw_loss 0.081006 lr 0.00056188 rank 6
2023-02-17 15:43:09,334 DEBUG TRAIN Batch 1/5700 loss 23.910210 loss_att 30.538408 loss_ctc 33.167667 loss_rnnt 21.312767 hw_loss 0.070266 lr 0.00056116 rank 3
2023-02-17 15:43:09,338 DEBUG TRAIN Batch 1/5700 loss 46.257576 loss_att 67.898582 loss_ctc 61.378395 loss_rnnt 39.844765 hw_loss 0.128444 lr 0.00056152 rank 5
2023-02-17 15:43:09,338 DEBUG TRAIN Batch 1/5700 loss 78.980370 loss_att 104.633911 loss_ctc 101.279861 loss_rnnt 70.827972 hw_loss 0.090787 lr 0.00056320 rank 1
2023-02-17 15:43:09,348 DEBUG TRAIN Batch 1/5700 loss 59.303783 loss_att 96.124008 loss_ctc 78.887848 loss_rnnt 49.314987 hw_loss 0.025390 lr 0.00056200 rank 2
2023-02-17 15:44:22,607 DEBUG TRAIN Batch 1/5800 loss 36.451740 loss_att 60.180763 loss_ctc 52.822781 loss_rnnt 29.515701 hw_loss 0.013930 lr 0.00056576 rank 0
2023-02-17 15:44:22,618 DEBUG TRAIN Batch 1/5800 loss 49.075466 loss_att 83.236862 loss_ctc 61.193466 loss_rnnt 40.593830 hw_loss 0.063040 lr 0.00056600 rank 2
2023-02-17 15:44:22,619 DEBUG TRAIN Batch 1/5800 loss 54.027802 loss_att 77.032677 loss_ctc 68.625809 loss_rnnt 47.401642 hw_loss 0.147724 lr 0.00056508 rank 4
2023-02-17 15:44:22,619 DEBUG TRAIN Batch 1/5800 loss 37.272282 loss_att 50.304108 loss_ctc 49.560280 loss_rnnt 32.984169 hw_loss 0.081277 lr 0.00056720 rank 1
2023-02-17 15:44:22,619 DEBUG TRAIN Batch 1/5800 loss 56.894180 loss_att 87.354256 loss_ctc 68.369461 loss_rnnt 49.254436 hw_loss 0.033175 lr 0.00056516 rank 3
2023-02-17 15:44:22,619 DEBUG TRAIN Batch 1/5800 loss 49.121670 loss_att 78.099998 loss_ctc 62.571819 loss_rnnt 41.532112 hw_loss 0.001008 lr 0.00056588 rank 6
2023-02-17 15:44:22,620 DEBUG TRAIN Batch 1/5800 loss 27.651611 loss_att 32.978630 loss_ctc 35.034801 loss_rnnt 25.458725 hw_loss 0.268231 lr 0.00056596 rank 7
2023-02-17 15:44:22,622 DEBUG TRAIN Batch 1/5800 loss 20.830214 loss_att 27.180435 loss_ctc 26.066555 loss_rnnt 18.745781 hw_loss 0.217892 lr 0.00056552 rank 5
2023-02-17 15:45:34,961 DEBUG TRAIN Batch 1/5900 loss 63.191032 loss_att 94.174942 loss_ctc 85.011459 loss_rnnt 54.084656 hw_loss 0.000379 lr 0.00056908 rank 4
2023-02-17 15:45:34,963 DEBUG TRAIN Batch 1/5900 loss 35.400421 loss_att 61.634926 loss_ctc 42.358215 loss_rnnt 29.203537 hw_loss 0.041772 lr 0.00057000 rank 2
2023-02-17 15:45:34,963 DEBUG TRAIN Batch 1/5900 loss 38.604801 loss_att 61.399918 loss_ctc 52.349030 loss_rnnt 32.212963 hw_loss 0.000469 lr 0.00056988 rank 6
2023-02-17 15:45:34,965 DEBUG TRAIN Batch 1/5900 loss 44.680733 loss_att 72.413193 loss_ctc 61.936504 loss_rnnt 36.797375 hw_loss 0.067678 lr 0.00056952 rank 5
2023-02-17 15:45:34,964 DEBUG TRAIN Batch 1/5900 loss 36.895397 loss_att 66.943878 loss_ctc 46.872612 loss_rnnt 29.555239 hw_loss 0.000311 lr 0.00056976 rank 0
2023-02-17 15:45:34,968 DEBUG TRAIN Batch 1/5900 loss 55.781742 loss_att 95.255409 loss_ctc 68.624329 loss_rnnt 46.174419 hw_loss 0.000458 lr 0.00056996 rank 7
2023-02-17 15:45:34,973 DEBUG TRAIN Batch 1/5900 loss 26.608936 loss_att 55.567581 loss_ctc 40.898849 loss_rnnt 18.911518 hw_loss 0.000687 lr 0.00056916 rank 3
2023-02-17 15:45:34,973 DEBUG TRAIN Batch 1/5900 loss 71.016365 loss_att 100.461975 loss_ctc 95.565430 loss_rnnt 61.838940 hw_loss 0.028315 lr 0.00057120 rank 1
2023-02-17 15:46:48,379 DEBUG TRAIN Batch 1/6000 loss 57.867641 loss_att 80.016090 loss_ctc 76.337982 loss_rnnt 50.947762 hw_loss 0.051514 lr 0.00057388 rank 6
2023-02-17 15:46:48,380 DEBUG TRAIN Batch 1/6000 loss 40.453407 loss_att 69.870285 loss_ctc 53.573711 loss_rnnt 32.800655 hw_loss 0.037502 lr 0.00057308 rank 4
2023-02-17 15:46:48,381 DEBUG TRAIN Batch 1/6000 loss 63.387966 loss_att 94.469284 loss_ctc 96.629967 loss_rnnt 52.738899 hw_loss 0.001004 lr 0.00057352 rank 5
2023-02-17 15:46:48,381 DEBUG TRAIN Batch 1/6000 loss 53.094105 loss_att 82.998909 loss_ctc 79.062637 loss_rnnt 43.549431 hw_loss 0.189829 lr 0.00057520 rank 1
2023-02-17 15:46:48,386 DEBUG TRAIN Batch 1/6000 loss 42.868362 loss_att 62.661476 loss_ctc 63.276157 loss_rnnt 36.142208 hw_loss 0.087166 lr 0.00057376 rank 0
2023-02-17 15:46:48,391 DEBUG TRAIN Batch 1/6000 loss 31.711525 loss_att 52.805847 loss_ctc 45.249802 loss_rnnt 25.661200 hw_loss 0.049421 lr 0.00057316 rank 3
2023-02-17 15:46:48,413 DEBUG TRAIN Batch 1/6000 loss 50.418808 loss_att 82.716721 loss_ctc 72.267792 loss_rnnt 41.023903 hw_loss 0.041488 lr 0.00057400 rank 2
2023-02-17 15:46:48,448 DEBUG TRAIN Batch 1/6000 loss 37.742813 loss_att 57.548996 loss_ctc 50.517273 loss_rnnt 32.077637 hw_loss 0.001267 lr 0.00057396 rank 7
2023-02-17 15:48:02,247 DEBUG TRAIN Batch 1/6100 loss 55.784958 loss_att 78.024773 loss_ctc 70.970428 loss_rnnt 49.287216 hw_loss 0.046972 lr 0.00057708 rank 4
2023-02-17 15:48:02,254 DEBUG TRAIN Batch 1/6100 loss 43.950191 loss_att 63.885841 loss_ctc 64.924583 loss_rnnt 37.151649 hw_loss 0.027804 lr 0.00057788 rank 6
2023-02-17 15:48:02,257 DEBUG TRAIN Batch 1/6100 loss 49.954578 loss_att 76.938560 loss_ctc 75.673126 loss_rnnt 41.053917 hw_loss 0.140102 lr 0.00057716 rank 3
2023-02-17 15:48:02,259 DEBUG TRAIN Batch 1/6100 loss 36.948193 loss_att 61.497864 loss_ctc 43.614288 loss_rnnt 31.096796 hw_loss 0.098716 lr 0.00057800 rank 2
2023-02-17 15:48:02,259 DEBUG TRAIN Batch 1/6100 loss 47.779278 loss_att 71.002182 loss_ctc 77.747047 loss_rnnt 39.138874 hw_loss 0.000215 lr 0.00057776 rank 0
2023-02-17 15:48:02,260 DEBUG TRAIN Batch 1/6100 loss 42.295216 loss_att 63.202091 loss_ctc 51.840515 loss_rnnt 36.820053 hw_loss 0.039532 lr 0.00057796 rank 7
2023-02-17 15:48:02,261 DEBUG TRAIN Batch 1/6100 loss 53.007095 loss_att 73.966606 loss_ctc 76.538452 loss_rnnt 45.662617 hw_loss 0.028226 lr 0.00057920 rank 1
2023-02-17 15:48:02,265 DEBUG TRAIN Batch 1/6100 loss 35.447037 loss_att 51.687744 loss_ctc 49.956566 loss_rnnt 30.239347 hw_loss 0.046764 lr 0.00057752 rank 5
2023-02-17 15:49:14,879 DEBUG TRAIN Batch 1/6200 loss 38.817146 loss_att 65.531700 loss_ctc 53.892197 loss_rnnt 31.447554 hw_loss 0.031262 lr 0.00058108 rank 4
2023-02-17 15:49:14,886 DEBUG TRAIN Batch 1/6200 loss 41.858078 loss_att 68.142738 loss_ctc 55.645348 loss_rnnt 34.696510 hw_loss 0.124382 lr 0.00058320 rank 1
2023-02-17 15:49:14,888 DEBUG TRAIN Batch 1/6200 loss 41.338699 loss_att 61.271034 loss_ctc 59.330429 loss_rnnt 34.952969 hw_loss 0.000686 lr 0.00058116 rank 3
2023-02-17 15:49:14,889 DEBUG TRAIN Batch 1/6200 loss 64.363892 loss_att 83.141609 loss_ctc 89.039322 loss_rnnt 57.275482 hw_loss 0.080253 lr 0.00058200 rank 2
2023-02-17 15:49:14,888 DEBUG TRAIN Batch 1/6200 loss 51.800377 loss_att 66.447189 loss_ctc 64.130150 loss_rnnt 47.203854 hw_loss 0.043487 lr 0.00058188 rank 6
2023-02-17 15:49:14,889 DEBUG TRAIN Batch 1/6200 loss 55.065830 loss_att 80.113777 loss_ctc 81.221802 loss_rnnt 46.515553 hw_loss 0.099789 lr 0.00058196 rank 7
2023-02-17 15:49:14,891 DEBUG TRAIN Batch 1/6200 loss 29.819582 loss_att 47.216614 loss_ctc 41.431107 loss_rnnt 24.774118 hw_loss 0.033475 lr 0.00058176 rank 0
2023-02-17 15:49:14,943 DEBUG TRAIN Batch 1/6200 loss 63.622204 loss_att 82.548363 loss_ctc 83.106178 loss_rnnt 57.219585 hw_loss 0.036606 lr 0.00058152 rank 5
2023-02-17 15:50:28,485 DEBUG TRAIN Batch 1/6300 loss 46.020115 loss_att 69.018364 loss_ctc 67.591049 loss_rnnt 38.489574 hw_loss 0.102686 lr 0.00058720 rank 1
2023-02-17 15:50:28,488 DEBUG TRAIN Batch 1/6300 loss 51.764194 loss_att 68.915504 loss_ctc 74.756844 loss_rnnt 45.234322 hw_loss 0.063601 lr 0.00058516 rank 3
2023-02-17 15:50:28,495 DEBUG TRAIN Batch 1/6300 loss 21.717129 loss_att 23.840513 loss_ctc 28.066101 loss_rnnt 20.363276 hw_loss 0.154961 lr 0.00058588 rank 6
2023-02-17 15:50:28,499 DEBUG TRAIN Batch 1/6300 loss 31.506138 loss_att 37.002712 loss_ctc 38.178646 loss_rnnt 29.467825 hw_loss 0.092491 lr 0.00058576 rank 0
2023-02-17 15:50:28,499 DEBUG TRAIN Batch 1/6300 loss 54.254208 loss_att 72.107285 loss_ctc 75.473724 loss_rnnt 47.790646 hw_loss 0.119398 lr 0.00058552 rank 5
2023-02-17 15:50:28,501 DEBUG TRAIN Batch 1/6300 loss 25.218523 loss_att 40.316246 loss_ctc 32.242504 loss_rnnt 21.262224 hw_loss 0.000413 lr 0.00058596 rank 7
2023-02-17 15:50:28,502 DEBUG TRAIN Batch 1/6300 loss 38.829075 loss_att 55.967087 loss_ctc 49.930035 loss_rnnt 33.921024 hw_loss 0.000602 lr 0.00058508 rank 4
2023-02-17 15:50:28,504 DEBUG TRAIN Batch 1/6300 loss 27.576313 loss_att 33.145279 loss_ctc 35.868870 loss_rnnt 25.338779 hw_loss 0.033872 lr 0.00058600 rank 2
2023-02-17 15:51:43,664 DEBUG TRAIN Batch 1/6400 loss 42.223160 loss_att 56.816833 loss_ctc 58.845028 loss_rnnt 37.022781 hw_loss 0.122611 lr 0.00059120 rank 1
2023-02-17 15:51:43,677 DEBUG TRAIN Batch 1/6400 loss 39.622345 loss_att 59.726479 loss_ctc 47.064507 loss_rnnt 34.567577 hw_loss 0.078097 lr 0.00058908 rank 4
2023-02-17 15:51:43,677 DEBUG TRAIN Batch 1/6400 loss 37.766106 loss_att 71.429016 loss_ctc 56.400646 loss_rnnt 28.530424 hw_loss 0.034673 lr 0.00059000 rank 2
2023-02-17 15:51:43,677 DEBUG TRAIN Batch 1/6400 loss 28.356987 loss_att 45.998871 loss_ctc 40.490505 loss_rnnt 23.149361 hw_loss 0.115214 lr 0.00058988 rank 6
2023-02-17 15:51:43,679 DEBUG TRAIN Batch 1/6400 loss 27.715111 loss_att 44.327667 loss_ctc 33.908646 loss_rnnt 23.501823 hw_loss 0.121820 lr 0.00058996 rank 7
2023-02-17 15:51:43,686 DEBUG TRAIN Batch 1/6400 loss 36.359913 loss_att 47.886131 loss_ctc 46.319973 loss_rnnt 32.671593 hw_loss 0.103250 lr 0.00058952 rank 5
2023-02-17 15:51:43,714 DEBUG TRAIN Batch 1/6400 loss 64.143478 loss_att 90.922928 loss_ctc 90.952385 loss_rnnt 55.123589 hw_loss 0.167768 lr 0.00058976 rank 0
2023-02-17 15:51:43,766 DEBUG TRAIN Batch 1/6400 loss 51.453163 loss_att 87.242294 loss_ctc 66.752335 loss_rnnt 42.255238 hw_loss 0.000396 lr 0.00058916 rank 3
2023-02-17 15:52:55,924 DEBUG TRAIN Batch 1/6500 loss 65.829506 loss_att 97.250153 loss_ctc 90.232841 loss_rnnt 56.277702 hw_loss 0.026057 lr 0.00059388 rank 6
2023-02-17 15:52:55,929 DEBUG TRAIN Batch 1/6500 loss 30.870583 loss_att 56.612747 loss_ctc 42.940826 loss_rnnt 24.062260 hw_loss 0.094727 lr 0.00059308 rank 4
2023-02-17 15:52:55,930 DEBUG TRAIN Batch 1/6500 loss 38.787189 loss_att 64.218155 loss_ctc 54.447048 loss_rnnt 31.546988 hw_loss 0.123797 lr 0.00059520 rank 1
2023-02-17 15:52:55,930 DEBUG TRAIN Batch 1/6500 loss 57.996254 loss_att 83.379066 loss_ctc 81.536911 loss_rnnt 49.738266 hw_loss 0.080010 lr 0.00059400 rank 2
2023-02-17 15:52:55,932 DEBUG TRAIN Batch 1/6500 loss 63.367371 loss_att 92.052155 loss_ctc 80.467178 loss_rnnt 55.341690 hw_loss 0.016399 lr 0.00059352 rank 5
2023-02-17 15:52:55,934 DEBUG TRAIN Batch 1/6500 loss 45.784241 loss_att 68.464081 loss_ctc 63.486412 loss_rnnt 38.844360 hw_loss 0.081796 lr 0.00059376 rank 0
2023-02-17 15:52:55,939 DEBUG TRAIN Batch 1/6500 loss 78.431320 loss_att 109.591568 loss_ctc 103.258080 loss_rnnt 68.833900 hw_loss 0.103374 lr 0.00059396 rank 7
2023-02-17 15:52:55,940 DEBUG TRAIN Batch 1/6500 loss 51.232552 loss_att 69.908028 loss_ctc 69.475487 loss_rnnt 45.064568 hw_loss 0.000935 lr 0.00059316 rank 3
2023-02-17 15:54:09,195 DEBUG TRAIN Batch 1/6600 loss 54.650211 loss_att 89.192223 loss_ctc 73.997063 loss_rnnt 45.162071 hw_loss 0.000298 lr 0.00059708 rank 4
2023-02-17 15:54:09,200 DEBUG TRAIN Batch 1/6600 loss 60.048912 loss_att 75.580269 loss_ctc 81.089813 loss_rnnt 54.086010 hw_loss 0.095962 lr 0.00059800 rank 2
2023-02-17 15:54:09,201 DEBUG TRAIN Batch 1/6600 loss 44.468052 loss_att 67.406158 loss_ctc 60.725773 loss_rnnt 37.629219 hw_loss 0.156591 lr 0.00059788 rank 6
2023-02-17 15:54:09,201 DEBUG TRAIN Batch 1/6600 loss 50.559696 loss_att 72.944084 loss_ctc 65.778168 loss_rnnt 44.045837 hw_loss 0.014726 lr 0.00059796 rank 7
2023-02-17 15:54:09,202 DEBUG TRAIN Batch 1/6600 loss 36.224823 loss_att 59.992462 loss_ctc 56.668846 loss_rnnt 28.722549 hw_loss 0.042888 lr 0.00059920 rank 1
2023-02-17 15:54:09,202 DEBUG TRAIN Batch 1/6600 loss 45.191586 loss_att 70.910599 loss_ctc 62.202007 loss_rnnt 37.734924 hw_loss 0.084012 lr 0.00059776 rank 0
2023-02-17 15:54:09,208 DEBUG TRAIN Batch 1/6600 loss 27.812145 loss_att 46.781776 loss_ctc 41.617905 loss_rnnt 22.089100 hw_loss 0.165655 lr 0.00059716 rank 3
2023-02-17 15:54:09,209 DEBUG TRAIN Batch 1/6600 loss 41.544022 loss_att 62.846954 loss_ctc 54.864357 loss_rnnt 35.507244 hw_loss 0.000279 lr 0.00059752 rank 5
2023-02-17 15:55:22,523 DEBUG TRAIN Batch 1/6700 loss 45.890656 loss_att 60.733665 loss_ctc 61.854748 loss_rnnt 40.738392 hw_loss 0.103342 lr 0.00060188 rank 6
2023-02-17 15:55:22,531 DEBUG TRAIN Batch 1/6700 loss 61.070789 loss_att 83.336243 loss_ctc 82.602463 loss_rnnt 53.725571 hw_loss 0.039817 lr 0.00060176 rank 0
2023-02-17 15:55:22,533 DEBUG TRAIN Batch 1/6700 loss 61.347912 loss_att 80.513092 loss_ctc 86.307014 loss_rnnt 54.172672 hw_loss 0.026851 lr 0.00060200 rank 2
2023-02-17 15:55:22,534 DEBUG TRAIN Batch 1/6700 loss 61.080109 loss_att 90.520256 loss_ctc 75.688034 loss_rnnt 53.201744 hw_loss 0.079900 lr 0.00060152 rank 5
2023-02-17 15:55:22,534 DEBUG TRAIN Batch 1/6700 loss 40.853851 loss_att 58.197182 loss_ctc 62.675079 loss_rnnt 34.452953 hw_loss 0.042625 lr 0.00060320 rank 1
2023-02-17 15:55:22,534 DEBUG TRAIN Batch 1/6700 loss 28.161505 loss_att 45.020790 loss_ctc 39.287853 loss_rnnt 23.276894 hw_loss 0.054825 lr 0.00060108 rank 4
2023-02-17 15:55:22,537 DEBUG TRAIN Batch 1/6700 loss 71.669136 loss_att 94.466629 loss_ctc 97.447372 loss_rnnt 63.576885 hw_loss 0.179352 lr 0.00060196 rank 7
2023-02-17 15:55:22,536 DEBUG TRAIN Batch 1/6700 loss 48.541809 loss_att 70.086823 loss_ctc 59.747940 loss_rnnt 42.678299 hw_loss 0.113172 lr 0.00060116 rank 3
2023-02-17 15:56:36,921 DEBUG TRAIN Batch 1/6800 loss 50.211582 loss_att 73.271889 loss_ctc 70.522423 loss_rnnt 42.833992 hw_loss 0.107653 lr 0.00060588 rank 6
2023-02-17 15:56:36,922 DEBUG TRAIN Batch 1/6800 loss 39.389561 loss_att 59.610790 loss_ctc 54.929138 loss_rnnt 33.208073 hw_loss 0.122430 lr 0.00060508 rank 4
2023-02-17 15:56:36,921 DEBUG TRAIN Batch 1/6800 loss 57.647854 loss_att 72.890366 loss_ctc 70.425140 loss_rnnt 52.895485 hw_loss 0.000424 lr 0.00060576 rank 0
2023-02-17 15:56:36,923 DEBUG TRAIN Batch 1/6800 loss 35.988323 loss_att 54.137970 loss_ctc 46.962200 loss_rnnt 30.881250 hw_loss 0.026172 lr 0.00060720 rank 1
2023-02-17 15:56:36,924 DEBUG TRAIN Batch 1/6800 loss 33.025990 loss_att 46.647324 loss_ctc 44.311562 loss_rnnt 28.718851 hw_loss 0.146492 lr 0.00060600 rank 2
2023-02-17 15:56:36,925 DEBUG TRAIN Batch 1/6800 loss 52.764214 loss_att 76.190781 loss_ctc 68.756699 loss_rnnt 45.853268 hw_loss 0.174940 lr 0.00060552 rank 5
2023-02-17 15:56:36,927 DEBUG TRAIN Batch 1/6800 loss 26.845058 loss_att 44.473763 loss_ctc 35.414680 loss_rnnt 22.134007 hw_loss 0.080052 lr 0.00060516 rank 3
2023-02-17 15:56:36,933 DEBUG TRAIN Batch 1/6800 loss 48.133091 loss_att 75.892883 loss_ctc 67.382561 loss_rnnt 39.985092 hw_loss 0.055205 lr 0.00060596 rank 7
2023-02-17 15:57:49,429 DEBUG TRAIN Batch 1/6900 loss 45.520840 loss_att 66.291313 loss_ctc 61.528362 loss_rnnt 39.175434 hw_loss 0.106821 lr 0.00060916 rank 3
2023-02-17 15:57:49,437 DEBUG TRAIN Batch 1/6900 loss 24.557682 loss_att 30.227486 loss_ctc 30.669678 loss_rnnt 22.570921 hw_loss 0.071007 lr 0.00060988 rank 6
2023-02-17 15:57:49,437 DEBUG TRAIN Batch 1/6900 loss 83.350410 loss_att 109.012512 loss_ctc 112.855843 loss_rnnt 74.201782 hw_loss 0.154030 lr 0.00060908 rank 4
2023-02-17 15:57:49,437 DEBUG TRAIN Batch 1/6900 loss 29.300997 loss_att 42.256977 loss_ctc 42.069519 loss_rnnt 24.901659 hw_loss 0.198128 lr 0.00060952 rank 5
2023-02-17 15:57:49,440 DEBUG TRAIN Batch 1/6900 loss 22.592064 loss_att 30.023478 loss_ctc 33.180367 loss_rnnt 19.687351 hw_loss 0.012483 lr 0.00060976 rank 0
2023-02-17 15:57:49,443 DEBUG TRAIN Batch 1/6900 loss 36.505226 loss_att 53.778202 loss_ctc 48.203453 loss_rnnt 31.469242 hw_loss 0.040551 lr 0.00060996 rank 7
2023-02-17 15:57:49,444 DEBUG TRAIN Batch 1/6900 loss 37.027802 loss_att 48.608326 loss_ctc 50.568508 loss_rnnt 32.876232 hw_loss 0.056325 lr 0.00061000 rank 2
2023-02-17 15:57:49,489 DEBUG TRAIN Batch 1/6900 loss 36.187828 loss_att 55.535221 loss_ctc 43.973492 loss_rnnt 31.251080 hw_loss 0.054713 lr 0.00061120 rank 1
2023-02-17 15:59:03,715 DEBUG TRAIN Batch 1/7000 loss 65.413162 loss_att 75.713310 loss_ctc 85.616310 loss_rnnt 60.649830 hw_loss 0.017913 lr 0.00061352 rank 5
2023-02-17 15:59:03,726 DEBUG TRAIN Batch 1/7000 loss 31.021917 loss_att 44.228874 loss_ctc 36.829006 loss_rnnt 27.554131 hw_loss 0.097719 lr 0.00061308 rank 4
2023-02-17 15:59:03,727 DEBUG TRAIN Batch 1/7000 loss 58.679482 loss_att 84.248871 loss_ctc 80.297775 loss_rnnt 50.682991 hw_loss 0.000321 lr 0.00061388 rank 6
2023-02-17 15:59:03,727 DEBUG TRAIN Batch 1/7000 loss 71.438324 loss_att 107.277878 loss_ctc 111.458130 loss_rnnt 58.883125 hw_loss 0.096202 lr 0.00061376 rank 0
2023-02-17 15:59:03,730 DEBUG TRAIN Batch 1/7000 loss 36.544632 loss_att 50.882217 loss_ctc 51.398682 loss_rnnt 31.685099 hw_loss 0.021510 lr 0.00061520 rank 1
2023-02-17 15:59:03,730 DEBUG TRAIN Batch 1/7000 loss 22.634836 loss_att 38.965839 loss_ctc 35.566170 loss_rnnt 17.601294 hw_loss 0.080933 lr 0.00061396 rank 7
2023-02-17 15:59:03,731 DEBUG TRAIN Batch 1/7000 loss 70.798492 loss_att 118.017677 loss_ctc 105.150528 loss_rnnt 56.774227 hw_loss 0.000298 lr 0.00061400 rank 2
2023-02-17 15:59:03,776 DEBUG TRAIN Batch 1/7000 loss 67.144020 loss_att 91.632019 loss_ctc 93.932297 loss_rnnt 58.674519 hw_loss 0.000252 lr 0.00061316 rank 3
2023-02-17 16:00:18,878 DEBUG TRAIN Batch 1/7100 loss 64.430405 loss_att 89.122978 loss_ctc 85.250671 loss_rnnt 56.687115 hw_loss 0.053878 lr 0.00061776 rank 0
2023-02-17 16:00:18,888 DEBUG TRAIN Batch 1/7100 loss 48.017990 loss_att 62.469444 loss_ctc 63.929882 loss_rnnt 42.968155 hw_loss 0.071182 lr 0.00061708 rank 4
2023-02-17 16:00:18,888 DEBUG TRAIN Batch 1/7100 loss 40.282650 loss_att 58.166817 loss_ctc 49.046547 loss_rnnt 35.537014 hw_loss 0.000529 lr 0.00061716 rank 3
2023-02-17 16:00:18,891 DEBUG TRAIN Batch 1/7100 loss 44.402142 loss_att 74.763458 loss_ctc 64.784935 loss_rnnt 35.578362 hw_loss 0.063393 lr 0.00061788 rank 6
2023-02-17 16:00:18,892 DEBUG TRAIN Batch 1/7100 loss 45.449135 loss_att 70.771942 loss_ctc 57.436520 loss_rnnt 38.786015 hw_loss 0.000446 lr 0.00061920 rank 1
2023-02-17 16:00:18,892 DEBUG TRAIN Batch 1/7100 loss 45.363621 loss_att 77.912399 loss_ctc 62.196236 loss_rnnt 36.584999 hw_loss 0.045981 lr 0.00061752 rank 5
2023-02-17 16:00:18,892 DEBUG TRAIN Batch 1/7100 loss 29.546066 loss_att 39.140598 loss_ctc 41.047626 loss_rnnt 26.021725 hw_loss 0.134804 lr 0.00061796 rank 7
2023-02-17 16:00:18,893 DEBUG TRAIN Batch 1/7100 loss 45.788639 loss_att 68.367668 loss_ctc 61.165192 loss_rnnt 39.207352 hw_loss 0.028635 lr 0.00061800 rank 2
2023-02-17 16:01:31,378 DEBUG TRAIN Batch 1/7200 loss 47.765034 loss_att 67.462646 loss_ctc 65.868225 loss_rnnt 41.395981 hw_loss 0.029574 lr 0.00062176 rank 0
2023-02-17 16:01:31,385 DEBUG TRAIN Batch 1/7200 loss 48.261826 loss_att 66.153114 loss_ctc 68.510025 loss_rnnt 41.983475 hw_loss 0.000626 lr 0.00062116 rank 3
2023-02-17 16:01:31,385 DEBUG TRAIN Batch 1/7200 loss 66.559578 loss_att 93.489510 loss_ctc 84.429657 loss_rnnt 58.731018 hw_loss 0.112297 lr 0.00062108 rank 4
2023-02-17 16:01:31,385 DEBUG TRAIN Batch 1/7200 loss 57.316547 loss_att 88.639481 loss_ctc 83.399719 loss_rnnt 47.508759 hw_loss 0.122709 lr 0.00062152 rank 5
2023-02-17 16:01:31,386 DEBUG TRAIN Batch 1/7200 loss 49.295750 loss_att 71.228073 loss_ctc 67.902451 loss_rnnt 42.368332 hw_loss 0.112612 lr 0.00062196 rank 7
2023-02-17 16:01:31,390 DEBUG TRAIN Batch 1/7200 loss 37.889332 loss_att 57.488884 loss_ctc 63.242981 loss_rnnt 30.588539 hw_loss 0.000736 lr 0.00062200 rank 2
2023-02-17 16:01:31,391 DEBUG TRAIN Batch 1/7200 loss 56.023815 loss_att 76.621170 loss_ctc 69.419983 loss_rnnt 50.084148 hw_loss 0.063827 lr 0.00062188 rank 6
2023-02-17 16:01:31,435 DEBUG TRAIN Batch 1/7200 loss 31.080036 loss_att 51.691730 loss_ctc 40.213120 loss_rnnt 25.739666 hw_loss 0.000539 lr 0.00062320 rank 1
2023-02-17 16:02:44,813 DEBUG TRAIN Batch 1/7300 loss 27.441528 loss_att 47.807693 loss_ctc 37.733437 loss_rnnt 21.944534 hw_loss 0.096573 lr 0.00062508 rank 4
2023-02-17 16:02:44,815 DEBUG TRAIN Batch 1/7300 loss 45.997154 loss_att 64.871544 loss_ctc 66.685501 loss_rnnt 39.393375 hw_loss 0.132101 lr 0.00062576 rank 0
2023-02-17 16:02:44,819 DEBUG TRAIN Batch 1/7300 loss 60.809261 loss_att 80.992752 loss_ctc 79.286453 loss_rnnt 54.288643 hw_loss 0.038046 lr 0.00062588 rank 6
2023-02-17 16:02:44,821 DEBUG TRAIN Batch 1/7300 loss 31.641630 loss_att 69.471451 loss_ctc 47.963913 loss_rnnt 21.885162 hw_loss 0.026623 lr 0.00062552 rank 5
2023-02-17 16:02:44,822 DEBUG TRAIN Batch 1/7300 loss 34.632099 loss_att 54.198814 loss_ctc 46.281891 loss_rnnt 29.084763 hw_loss 0.151286 lr 0.00062600 rank 2
2023-02-17 16:02:44,825 DEBUG TRAIN Batch 1/7300 loss 39.908337 loss_att 64.395615 loss_ctc 54.484356 loss_rnnt 33.011108 hw_loss 0.105565 lr 0.00062720 rank 1
2023-02-17 16:02:44,826 DEBUG TRAIN Batch 1/7300 loss 59.492924 loss_att 88.286194 loss_ctc 80.766960 loss_rnnt 50.881664 hw_loss 0.030128 lr 0.00062516 rank 3
2023-02-17 16:02:44,871 DEBUG TRAIN Batch 1/7300 loss 53.096043 loss_att 75.687561 loss_ctc 78.621826 loss_rnnt 45.149742 hw_loss 0.046048 lr 0.00062596 rank 7
2023-02-17 16:03:57,652 DEBUG TRAIN Batch 1/7400 loss 40.193344 loss_att 68.033897 loss_ctc 58.288467 loss_rnnt 32.185398 hw_loss 0.050907 lr 0.00062952 rank 5
2023-02-17 16:03:57,658 DEBUG TRAIN Batch 1/7400 loss 34.223667 loss_att 55.606018 loss_ctc 55.444374 loss_rnnt 27.117437 hw_loss 0.000617 lr 0.00062996 rank 7
2023-02-17 16:03:57,660 DEBUG TRAIN Batch 1/7400 loss 70.755692 loss_att 87.530998 loss_ctc 97.707779 loss_rnnt 63.753296 hw_loss 0.100728 lr 0.00062988 rank 6
2023-02-17 16:03:57,667 DEBUG TRAIN Batch 1/7400 loss 60.407215 loss_att 78.407341 loss_ctc 76.433594 loss_rnnt 54.640369 hw_loss 0.056193 lr 0.00063120 rank 1
2023-02-17 16:03:57,672 DEBUG TRAIN Batch 1/7400 loss 26.311773 loss_att 44.618423 loss_ctc 39.070648 loss_rnnt 20.892113 hw_loss 0.107152 lr 0.00062908 rank 4
2023-02-17 16:03:57,676 DEBUG TRAIN Batch 1/7400 loss 24.208420 loss_att 40.870071 loss_ctc 41.114182 loss_rnnt 18.605473 hw_loss 0.030965 lr 0.00062976 rank 0
2023-02-17 16:03:57,678 DEBUG TRAIN Batch 1/7400 loss 39.173615 loss_att 62.119232 loss_ctc 54.581066 loss_rnnt 32.466888 hw_loss 0.118632 lr 0.00063000 rank 2
2023-02-17 16:03:57,677 DEBUG TRAIN Batch 1/7400 loss 48.417160 loss_att 73.019897 loss_ctc 77.073433 loss_rnnt 39.618252 hw_loss 0.107864 lr 0.00062916 rank 3
2023-02-17 16:05:12,176 DEBUG TRAIN Batch 1/7500 loss 46.155056 loss_att 68.464188 loss_ctc 63.942432 loss_rnnt 39.260509 hw_loss 0.114510 lr 0.00063400 rank 2
2023-02-17 16:05:12,178 DEBUG TRAIN Batch 1/7500 loss 33.103962 loss_att 53.623356 loss_ctc 47.440968 loss_rnnt 27.017639 hw_loss 0.132829 lr 0.00063388 rank 6
2023-02-17 16:05:12,178 DEBUG TRAIN Batch 1/7500 loss 42.305157 loss_att 49.123230 loss_ctc 55.334820 loss_rnnt 39.118408 hw_loss 0.160962 lr 0.00063376 rank 0
2023-02-17 16:05:12,180 DEBUG TRAIN Batch 1/7500 loss 33.769348 loss_att 42.359497 loss_ctc 46.818588 loss_rnnt 30.250389 hw_loss 0.114431 lr 0.00063308 rank 4
2023-02-17 16:05:12,182 DEBUG TRAIN Batch 1/7500 loss 41.443687 loss_att 58.578083 loss_ctc 51.210724 loss_rnnt 36.646667 hw_loss 0.127262 lr 0.00063520 rank 1
2023-02-17 16:05:12,186 DEBUG TRAIN Batch 1/7500 loss 48.835876 loss_att 65.303474 loss_ctc 67.145950 loss_rnnt 43.071983 hw_loss 0.054437 lr 0.00063396 rank 7
2023-02-17 16:05:12,185 DEBUG TRAIN Batch 1/7500 loss 27.088213 loss_att 35.279247 loss_ctc 32.539227 loss_rnnt 24.673277 hw_loss 0.093611 lr 0.00063316 rank 3
2023-02-17 16:05:12,233 DEBUG TRAIN Batch 1/7500 loss 34.560474 loss_att 53.835464 loss_ctc 55.107269 loss_rnnt 27.892563 hw_loss 0.137506 lr 0.00063352 rank 5
2023-02-17 16:06:24,818 DEBUG TRAIN Batch 1/7600 loss 41.319515 loss_att 54.473343 loss_ctc 53.265629 loss_rnnt 37.069984 hw_loss 0.048657 lr 0.00063788 rank 6
2023-02-17 16:06:24,819 DEBUG TRAIN Batch 1/7600 loss 47.840233 loss_att 63.215385 loss_ctc 68.363083 loss_rnnt 42.028599 hw_loss 0.000419 lr 0.00063920 rank 1
2023-02-17 16:06:24,819 DEBUG TRAIN Batch 1/7600 loss 34.269249 loss_att 53.708160 loss_ctc 45.204681 loss_rnnt 28.915415 hw_loss 0.014996 lr 0.00063708 rank 4
2023-02-17 16:06:24,821 DEBUG TRAIN Batch 1/7600 loss 29.298044 loss_att 40.968636 loss_ctc 40.065495 loss_rnnt 25.527935 hw_loss 0.000622 lr 0.00063796 rank 7
2023-02-17 16:06:24,821 DEBUG TRAIN Batch 1/7600 loss 41.487106 loss_att 57.696136 loss_ctc 51.082577 loss_rnnt 36.867882 hw_loss 0.183789 lr 0.00063752 rank 5
2023-02-17 16:06:24,825 DEBUG TRAIN Batch 1/7600 loss 40.750248 loss_att 68.234283 loss_ctc 63.699158 loss_rnnt 32.191166 hw_loss 0.004537 lr 0.00063776 rank 0
2023-02-17 16:06:24,826 DEBUG TRAIN Batch 1/7600 loss 36.934673 loss_att 57.834969 loss_ctc 46.323864 loss_rnnt 31.462742 hw_loss 0.074956 lr 0.00063716 rank 3
2023-02-17 16:06:24,871 DEBUG TRAIN Batch 1/7600 loss 47.573074 loss_att 75.224945 loss_ctc 68.956596 loss_rnnt 39.122051 hw_loss 0.130338 lr 0.00063800 rank 2
2023-02-17 16:07:38,463 DEBUG TRAIN Batch 1/7700 loss 27.580488 loss_att 42.455513 loss_ctc 40.599457 loss_rnnt 22.829105 hw_loss 0.075967 lr 0.00064200 rank 2
2023-02-17 16:07:38,480 DEBUG TRAIN Batch 1/7700 loss 20.218845 loss_att 26.744640 loss_ctc 31.099030 loss_rnnt 17.357624 hw_loss 0.197571 lr 0.00064108 rank 4
2023-02-17 16:07:38,484 DEBUG TRAIN Batch 1/7700 loss 21.319450 loss_att 23.365761 loss_ctc 26.800941 loss_rnnt 20.115505 hw_loss 0.119657 lr 0.00064320 rank 1
2023-02-17 16:07:38,486 DEBUG TRAIN Batch 1/7700 loss 29.219898 loss_att 44.756287 loss_ctc 40.595085 loss_rnnt 24.550476 hw_loss 0.085218 lr 0.00064188 rank 6
2023-02-17 16:07:38,488 DEBUG TRAIN Batch 1/7700 loss 37.136745 loss_att 48.925583 loss_ctc 48.140182 loss_rnnt 33.220764 hw_loss 0.170794 lr 0.00064152 rank 5
2023-02-17 16:07:38,489 DEBUG TRAIN Batch 1/7700 loss 59.501503 loss_att 85.958107 loss_ctc 82.074295 loss_rnnt 51.122948 hw_loss 0.145374 lr 0.00064176 rank 0
2023-02-17 16:07:38,490 DEBUG TRAIN Batch 1/7700 loss 53.515781 loss_att 86.606171 loss_ctc 78.072563 loss_rnnt 43.598053 hw_loss 0.047655 lr 0.00064116 rank 3
2023-02-17 16:07:38,497 DEBUG TRAIN Batch 1/7700 loss 20.309216 loss_att 24.991299 loss_ctc 27.377119 loss_rnnt 18.382029 hw_loss 0.090715 lr 0.00064196 rank 7
2023-02-17 16:08:53,532 DEBUG TRAIN Batch 1/7800 loss 42.149330 loss_att 59.275330 loss_ctc 63.465740 loss_rnnt 35.880901 hw_loss 0.001951 lr 0.00064508 rank 4
2023-02-17 16:08:53,535 DEBUG TRAIN Batch 1/7800 loss 35.449734 loss_att 50.152512 loss_ctc 52.878311 loss_rnnt 30.100597 hw_loss 0.158948 lr 0.00064516 rank 3
2023-02-17 16:08:53,536 DEBUG TRAIN Batch 1/7800 loss 43.338631 loss_att 69.381210 loss_ctc 62.493500 loss_rnnt 35.553223 hw_loss 0.042953 lr 0.00064588 rank 6
2023-02-17 16:08:53,540 DEBUG TRAIN Batch 1/7800 loss 36.488049 loss_att 62.491318 loss_ctc 59.020683 loss_rnnt 28.206442 hw_loss 0.143623 lr 0.00064600 rank 2
2023-02-17 16:08:53,540 DEBUG TRAIN Batch 1/7800 loss 49.546013 loss_att 68.120407 loss_ctc 62.977600 loss_rnnt 43.968292 hw_loss 0.134927 lr 0.00064596 rank 7
2023-02-17 16:08:53,541 DEBUG TRAIN Batch 1/7800 loss 61.827370 loss_att 78.552094 loss_ctc 74.809746 loss_rnnt 56.722870 hw_loss 0.053568 lr 0.00064576 rank 0
2023-02-17 16:08:53,598 DEBUG TRAIN Batch 1/7800 loss 37.294205 loss_att 57.733154 loss_ctc 54.382652 loss_rnnt 30.892349 hw_loss 0.066755 lr 0.00064720 rank 1
2023-02-17 16:08:53,612 DEBUG TRAIN Batch 1/7800 loss 57.409599 loss_att 84.655518 loss_ctc 77.987137 loss_rnnt 49.170471 hw_loss 0.086764 lr 0.00064552 rank 5
2023-02-17 16:10:07,619 DEBUG TRAIN Batch 1/7900 loss 51.699646 loss_att 86.393478 loss_ctc 71.050064 loss_rnnt 42.172935 hw_loss 0.014787 lr 0.00064988 rank 6
2023-02-17 16:10:07,628 DEBUG TRAIN Batch 1/7900 loss 45.488045 loss_att 60.700493 loss_ctc 63.177818 loss_rnnt 40.086685 hw_loss 0.000441 lr 0.00064916 rank 3
2023-02-17 16:10:07,631 DEBUG TRAIN Batch 1/7900 loss 40.788017 loss_att 58.219326 loss_ctc 68.869843 loss_rnnt 33.540115 hw_loss 0.032611 lr 0.00064976 rank 0
2023-02-17 16:10:07,633 DEBUG TRAIN Batch 1/7900 loss 60.072075 loss_att 82.335983 loss_ctc 81.483994 loss_rnnt 52.764000 hw_loss 0.000692 lr 0.00065000 rank 2
2023-02-17 16:10:07,633 DEBUG TRAIN Batch 1/7900 loss 62.696339 loss_att 91.873108 loss_ctc 80.571930 loss_rnnt 54.408287 hw_loss 0.129908 lr 0.00064908 rank 4
2023-02-17 16:10:07,636 DEBUG TRAIN Batch 1/7900 loss 36.978329 loss_att 55.396992 loss_ctc 64.707718 loss_rnnt 29.565788 hw_loss 0.059163 lr 0.00064996 rank 7
2023-02-17 16:10:07,637 DEBUG TRAIN Batch 1/7900 loss 39.600796 loss_att 53.228035 loss_ctc 53.151390 loss_rnnt 35.022507 hw_loss 0.086422 lr 0.00064952 rank 5
2023-02-17 16:10:07,678 DEBUG TRAIN Batch 1/7900 loss 29.144621 loss_att 48.691498 loss_ctc 41.060131 loss_rnnt 23.646103 hw_loss 0.000764 lr 0.00065120 rank 1
2023-02-17 16:11:20,383 DEBUG TRAIN Batch 1/8000 loss 45.320419 loss_att 64.827812 loss_ctc 69.012001 loss_rnnt 38.247429 hw_loss 0.023686 lr 0.00065308 rank 4
2023-02-17 16:11:20,386 DEBUG TRAIN Batch 1/8000 loss 32.651463 loss_att 54.791672 loss_ctc 50.146492 loss_rnnt 25.821974 hw_loss 0.128952 lr 0.00065376 rank 0
2023-02-17 16:11:20,391 DEBUG TRAIN Batch 1/8000 loss 35.756714 loss_att 50.378273 loss_ctc 61.803810 loss_rnnt 29.294312 hw_loss 0.122143 lr 0.00065388 rank 6
2023-02-17 16:11:20,393 DEBUG TRAIN Batch 1/8000 loss 26.499811 loss_att 46.403091 loss_ctc 37.579044 loss_rnnt 20.997589 hw_loss 0.083129 lr 0.00065400 rank 2
2023-02-17 16:11:20,395 DEBUG TRAIN Batch 1/8000 loss 49.519650 loss_att 67.747269 loss_ctc 70.316544 loss_rnnt 43.090302 hw_loss 0.020445 lr 0.00065316 rank 3
2023-02-17 16:11:20,394 DEBUG TRAIN Batch 1/8000 loss 67.679665 loss_att 87.120155 loss_ctc 92.216072 loss_rnnt 60.494514 hw_loss 0.047852 lr 0.00065352 rank 5
2023-02-17 16:11:20,395 DEBUG TRAIN Batch 1/8000 loss 48.275463 loss_att 70.209198 loss_ctc 73.355354 loss_rnnt 40.502678 hw_loss 0.078847 lr 0.00065396 rank 7
2023-02-17 16:11:20,437 DEBUG TRAIN Batch 1/8000 loss 50.656662 loss_att 66.460732 loss_ctc 67.004684 loss_rnnt 45.307339 hw_loss 0.016440 lr 0.00065520 rank 1
2023-02-17 16:12:33,481 DEBUG TRAIN Batch 1/8100 loss 29.905293 loss_att 41.765434 loss_ctc 43.174362 loss_rnnt 25.750076 hw_loss 0.026208 lr 0.00065800 rank 2
2023-02-17 16:12:33,485 DEBUG TRAIN Batch 1/8100 loss 37.014175 loss_att 56.773918 loss_ctc 48.692135 loss_rnnt 31.444553 hw_loss 0.113639 lr 0.00065796 rank 7
2023-02-17 16:12:33,492 DEBUG TRAIN Batch 1/8100 loss 56.630756 loss_att 71.331375 loss_ctc 69.195145 loss_rnnt 51.921818 hw_loss 0.175430 lr 0.00065708 rank 4
2023-02-17 16:12:33,495 DEBUG TRAIN Batch 1/8100 loss 43.372658 loss_att 61.780823 loss_ctc 61.145607 loss_rnnt 37.277859 hw_loss 0.081457 lr 0.00065920 rank 1
2023-02-17 16:12:33,495 DEBUG TRAIN Batch 1/8100 loss 42.572388 loss_att 57.022240 loss_ctc 62.297813 loss_rnnt 37.030235 hw_loss 0.041487 lr 0.00065752 rank 5
2023-02-17 16:12:33,498 DEBUG TRAIN Batch 1/8100 loss 26.020412 loss_att 41.398422 loss_ctc 44.297859 loss_rnnt 20.482048 hw_loss 0.048313 lr 0.00065788 rank 6
2023-02-17 16:12:33,501 DEBUG TRAIN Batch 1/8100 loss 38.233124 loss_att 54.180252 loss_ctc 51.495636 loss_rnnt 33.274261 hw_loss 0.002066 lr 0.00065776 rank 0
2023-02-17 16:12:33,551 DEBUG TRAIN Batch 1/8100 loss 36.182201 loss_att 49.704067 loss_ctc 47.108055 loss_rnnt 32.015263 hw_loss 0.010840 lr 0.00065716 rank 3
2023-02-17 16:13:47,204 DEBUG TRAIN Batch 1/8200 loss 37.955379 loss_att 53.341999 loss_ctc 54.502514 loss_rnnt 32.613720 hw_loss 0.108848 lr 0.00066152 rank 5
2023-02-17 16:13:47,205 DEBUG TRAIN Batch 1/8200 loss 36.539696 loss_att 50.666779 loss_ctc 54.451069 loss_rnnt 31.257746 hw_loss 0.128155 lr 0.00066188 rank 6
2023-02-17 16:13:47,205 DEBUG TRAIN Batch 1/8200 loss 56.872257 loss_att 80.088654 loss_ctc 78.363335 loss_rnnt 49.305981 hw_loss 0.107851 lr 0.00066176 rank 0
2023-02-17 16:13:47,207 DEBUG TRAIN Batch 1/8200 loss 41.740620 loss_att 56.693710 loss_ctc 51.091839 loss_rnnt 37.485783 hw_loss 0.032607 lr 0.00066320 rank 1
2023-02-17 16:13:47,208 DEBUG TRAIN Batch 1/8200 loss 34.618214 loss_att 44.510761 loss_ctc 43.585812 loss_rnnt 31.364372 hw_loss 0.149342 lr 0.00066108 rank 4
2023-02-17 16:13:47,209 DEBUG TRAIN Batch 1/8200 loss 42.648952 loss_att 51.009861 loss_ctc 57.419201 loss_rnnt 38.927383 hw_loss 0.150042 lr 0.00066196 rank 7
2023-02-17 16:13:47,209 DEBUG TRAIN Batch 1/8200 loss 39.203106 loss_att 54.631874 loss_ctc 50.508381 loss_rnnt 34.557579 hw_loss 0.098255 lr 0.00066116 rank 3
2023-02-17 16:13:47,253 DEBUG TRAIN Batch 1/8200 loss 31.758530 loss_att 34.681770 loss_ctc 38.009251 loss_rnnt 30.225357 hw_loss 0.215806 lr 0.00066200 rank 2
2023-02-17 16:14:59,693 DEBUG TRAIN Batch 1/8300 loss 50.557079 loss_att 63.882832 loss_ctc 71.430771 loss_rnnt 45.072304 hw_loss 0.068369 lr 0.00066508 rank 4
2023-02-17 16:14:59,693 DEBUG TRAIN Batch 1/8300 loss 52.667473 loss_att 61.400196 loss_ctc 60.933205 loss_rnnt 49.787987 hw_loss 0.057839 lr 0.00066552 rank 5
2023-02-17 16:14:59,695 DEBUG TRAIN Batch 1/8300 loss 36.314087 loss_att 50.703163 loss_ctc 53.483440 loss_rnnt 31.091072 hw_loss 0.104900 lr 0.00066516 rank 3
2023-02-17 16:14:59,698 DEBUG TRAIN Batch 1/8300 loss 35.473778 loss_att 43.728279 loss_ctc 49.652584 loss_rnnt 31.872959 hw_loss 0.111387 lr 0.00066720 rank 1
2023-02-17 16:14:59,700 DEBUG TRAIN Batch 1/8300 loss 55.580189 loss_att 85.437759 loss_ctc 72.142334 loss_rnnt 47.400223 hw_loss 0.000315 lr 0.00066600 rank 2
2023-02-17 16:14:59,700 DEBUG TRAIN Batch 1/8300 loss 44.795429 loss_att 63.393364 loss_ctc 62.258553 loss_rnnt 38.698170 hw_loss 0.092345 lr 0.00066596 rank 7
2023-02-17 16:14:59,712 DEBUG TRAIN Batch 1/8300 loss 61.178017 loss_att 84.081329 loss_ctc 86.781738 loss_rnnt 53.120842 hw_loss 0.117527 lr 0.00066576 rank 0
2023-02-17 16:14:59,717 DEBUG TRAIN Batch 1/8300 loss 44.852200 loss_att 51.925575 loss_ctc 56.843159 loss_rnnt 41.787018 hw_loss 0.096963 lr 0.00066588 rank 6
2023-02-17 16:15:43,367 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 0
2023-02-17 16:15:43,367 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 5
2023-02-17 16:15:43,367 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 4
2023-02-17 16:15:43,370 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 1
2023-02-17 16:15:43,371 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 2
2023-02-17 16:15:43,372 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 3
2023-02-17 16:15:43,376 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 6
2023-02-17 16:15:43,388 DEBUG CV Batch 1/0 loss 6.920739 loss_att 7.487376 loss_ctc 10.368159 loss_rnnt 6.175671 hw_loss 0.322656 history loss 6.664415 rank 7
2023-02-17 16:15:54,394 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 1
2023-02-17 16:15:54,434 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 7
2023-02-17 16:15:54,476 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 5
2023-02-17 16:15:54,713 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 6
2023-02-17 16:15:54,727 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 3
2023-02-17 16:15:54,813 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 2
2023-02-17 16:15:54,906 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 0
2023-02-17 16:15:54,973 DEBUG CV Batch 1/100 loss 33.273438 loss_att 40.347191 loss_ctc 49.695625 loss_rnnt 29.636642 hw_loss 0.060786 history loss 14.561726 rank 4
2023-02-17 16:16:07,562 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 7
2023-02-17 16:16:07,779 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 5
2023-02-17 16:16:07,892 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 3
2023-02-17 16:16:08,025 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 2
2023-02-17 16:16:08,078 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 1
2023-02-17 16:16:08,515 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 6
2023-02-17 16:16:08,701 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 4
2023-02-17 16:16:08,824 DEBUG CV Batch 1/200 loss 36.754925 loss_att 74.133072 loss_ctc 56.790344 loss_rnnt 26.607693 hw_loss 0.000405 history loss 16.115664 rank 0
2023-02-17 16:16:19,578 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 7
2023-02-17 16:16:19,914 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 5
2023-02-17 16:16:19,926 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 3
2023-02-17 16:16:20,248 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 1
2023-02-17 16:16:20,718 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 2
2023-02-17 16:16:21,056 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 6
2023-02-17 16:16:21,205 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 4
2023-02-17 16:16:21,431 DEBUG CV Batch 1/300 loss 18.114981 loss_att 22.818291 loss_ctc 30.477776 loss_rnnt 15.520672 hw_loss 0.009883 history loss 16.131094 rank 0
2023-02-17 16:16:31,342 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 7
2023-02-17 16:16:31,787 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 5
2023-02-17 16:16:31,798 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 3
2023-02-17 16:16:32,027 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 1
2023-02-17 16:16:32,630 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 2
2023-02-17 16:16:33,498 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 6
2023-02-17 16:16:33,592 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 4
2023-02-17 16:16:34,131 DEBUG CV Batch 1/400 loss 79.576469 loss_att 241.064178 loss_ctc 98.376511 loss_rnnt 44.772041 hw_loss 0.000405 history loss 17.901458 rank 0
2023-02-17 16:16:41,878 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 7
2023-02-17 16:16:42,333 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 5
2023-02-17 16:16:42,420 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 1
2023-02-17 16:16:42,434 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 3
2023-02-17 16:16:43,395 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 2
2023-02-17 16:16:44,503 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 6
2023-02-17 16:16:44,583 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 4
2023-02-17 16:16:45,406 DEBUG CV Batch 1/500 loss 25.249609 loss_att 28.899769 loss_ctc 40.967640 loss_rnnt 22.327745 hw_loss 0.180174 history loss 19.275984 rank 0
2023-02-17 16:16:53,923 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 7
2023-02-17 16:16:54,391 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 1
2023-02-17 16:16:54,601 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 3
2023-02-17 16:16:54,695 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 5
2023-02-17 16:16:55,491 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 2
2023-02-17 16:16:56,871 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 6
2023-02-17 16:16:57,035 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 4
2023-02-17 16:16:57,979 DEBUG CV Batch 1/600 loss 18.395317 loss_att 20.450251 loss_ctc 25.612074 loss_rnnt 16.911098 hw_loss 0.208122 history loss 20.767322 rank 0
2023-02-17 16:17:05,255 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 7
2023-02-17 16:17:05,526 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 1
2023-02-17 16:17:05,896 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 3
2023-02-17 16:17:05,996 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 5
2023-02-17 16:17:06,744 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 2
2023-02-17 16:17:08,591 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 6
2023-02-17 16:17:08,784 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 4
2023-02-17 16:17:09,896 DEBUG CV Batch 1/700 loss 66.255402 loss_att 156.394318 loss_ctc 82.257339 loss_rnnt 46.093811 hw_loss 0.000405 history loss 21.913845 rank 0
2023-02-17 16:17:16,381 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 7
2023-02-17 16:17:16,784 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 1
2023-02-17 16:17:17,197 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 3
2023-02-17 16:17:17,470 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 5
2023-02-17 16:17:17,975 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 2
2023-02-17 16:17:20,321 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 6
2023-02-17 16:17:20,489 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 4
2023-02-17 16:17:21,661 DEBUG CV Batch 1/800 loss 33.357353 loss_att 39.477028 loss_ctc 51.117088 loss_rnnt 29.658428 hw_loss 0.200670 history loss 20.900443 rank 0
2023-02-17 16:17:29,553 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 7
2023-02-17 16:17:30,216 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 1
2023-02-17 16:17:30,452 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 3
2023-02-17 16:17:30,820 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 5
2023-02-17 16:17:31,370 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 2
2023-02-17 16:17:33,873 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 6
2023-02-17 16:17:34,125 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 4
2023-02-17 16:17:35,436 DEBUG CV Batch 1/900 loss 42.731125 loss_att 84.794724 loss_ctc 63.785339 loss_rnnt 31.462719 hw_loss 0.090855 history loss 20.649582 rank 0
2023-02-17 16:17:41,790 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 7
2023-02-17 16:17:42,304 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 1
2023-02-17 16:17:42,830 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 3
2023-02-17 16:17:43,075 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 5
2023-02-17 16:17:43,744 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 2
2023-02-17 16:17:46,499 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 6
2023-02-17 16:17:46,739 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 4
2023-02-17 16:17:48,174 DEBUG CV Batch 1/1000 loss 15.049076 loss_att 18.625456 loss_ctc 20.816231 loss_rnnt 13.519510 hw_loss 0.085004 history loss 20.220289 rank 0
2023-02-17 16:17:53,676 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 7
2023-02-17 16:17:54,267 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 1
2023-02-17 16:17:54,644 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 3
2023-02-17 16:17:55,035 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 5
2023-02-17 16:17:55,564 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 2
2023-02-17 16:17:58,742 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 6
2023-02-17 16:17:58,900 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 4
2023-02-17 16:18:00,565 DEBUG CV Batch 1/1100 loss 12.917270 loss_att 13.031832 loss_ctc 18.684511 loss_rnnt 11.953215 hw_loss 0.322834 history loss 20.234437 rank 0
2023-02-17 16:18:04,231 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 7
2023-02-17 16:18:04,814 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 1
2023-02-17 16:18:05,407 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 3
2023-02-17 16:18:05,746 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 5
2023-02-17 16:18:06,313 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 2
2023-02-17 16:18:09,895 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 6
2023-02-17 16:18:09,912 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 4
2023-02-17 16:18:11,763 DEBUG CV Batch 1/1200 loss 36.098080 loss_att 43.432751 loss_ctc 51.313515 loss_rnnt 32.531834 hw_loss 0.132350 history loss 20.794690 rank 0
2023-02-17 16:18:16,133 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 7
2023-02-17 16:18:16,793 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 1
2023-02-17 16:18:17,817 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 3
2023-02-17 16:18:17,874 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 5
2023-02-17 16:18:18,542 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 2
2023-02-17 16:18:22,170 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 4
2023-02-17 16:18:22,231 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 6
2023-02-17 16:18:24,334 DEBUG CV Batch 1/1300 loss 18.799633 loss_att 20.122686 loss_ctc 26.727396 loss_rnnt 17.363934 hw_loss 0.213852 history loss 21.256097 rank 0
2023-02-17 16:18:27,210 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 7
2023-02-17 16:18:27,817 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 1
2023-02-17 16:18:29,147 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 5
2023-02-17 16:18:29,545 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 3
2023-02-17 16:18:29,696 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 2
2023-02-17 16:18:33,667 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 4
2023-02-17 16:18:33,736 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 6
2023-02-17 16:18:36,182 DEBUG CV Batch 1/1400 loss 60.170929 loss_att 159.434708 loss_ctc 74.296867 loss_rnnt 38.434502 hw_loss 0.000405 history loss 21.829848 rank 0
2023-02-17 16:18:38,722 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 7
2023-02-17 16:18:39,206 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 1
2023-02-17 16:18:40,767 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 5
2023-02-17 16:18:41,306 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 2
2023-02-17 16:18:41,324 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 3
2023-02-17 16:18:45,675 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 6
2023-02-17 16:18:45,717 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 4
2023-02-17 16:18:48,260 DEBUG CV Batch 1/1500 loss 24.469339 loss_att 31.723705 loss_ctc 31.742931 loss_rnnt 21.997190 hw_loss 0.096491 history loss 21.418141 rank 0
2023-02-17 16:18:51,672 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 7
2023-02-17 16:18:52,214 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 1
2023-02-17 16:18:53,870 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 5
2023-02-17 16:18:54,346 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 2
2023-02-17 16:18:54,404 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 3
2023-02-17 16:18:59,015 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 6
2023-02-17 16:18:59,050 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 4
2023-02-17 16:19:01,761 DEBUG CV Batch 1/1600 loss 36.321785 loss_att 76.520035 loss_ctc 50.984367 loss_rnnt 26.292076 hw_loss 0.065720 history loss 21.294154 rank 0
2023-02-17 16:19:04,083 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 7
2023-02-17 16:19:04,554 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 1
2023-02-17 16:19:06,566 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 5
2023-02-17 16:19:06,835 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 2
2023-02-17 16:19:07,044 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 3
2023-02-17 16:19:11,275 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 4
2023-02-17 16:19:11,383 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 6
2023-02-17 16:19:13,191 INFO Epoch 1 CV info cv_loss 21.002981455723386
2023-02-17 16:19:13,191 INFO Epoch 2 TRAIN info lr 0.00066668
2023-02-17 16:19:13,193 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:13,715 INFO Epoch 1 CV info cv_loss 21.002981458962488
2023-02-17 16:19:13,715 INFO Epoch 2 TRAIN info lr 0.00066844
2023-02-17 16:19:13,720 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:14,231 DEBUG CV Batch 1/1700 loss 25.075270 loss_att 29.496670 loss_ctc 38.309429 loss_rnnt 22.331837 hw_loss 0.177371 history loss 21.051587 rank 0
2023-02-17 16:19:16,008 INFO Epoch 1 CV info cv_loss 21.0029814521397
2023-02-17 16:19:16,008 INFO Epoch 2 TRAIN info lr 0.0006664
2023-02-17 16:19:16,013 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:16,028 INFO Epoch 1 CV info cv_loss 21.002981455103132
2023-02-17 16:19:16,028 INFO Epoch 2 TRAIN info lr 0.00066808
2023-02-17 16:19:16,030 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:16,221 INFO Epoch 1 CV info cv_loss 21.00298145710173
2023-02-17 16:19:16,221 INFO Epoch 2 TRAIN info lr 0.0006667600000000001
2023-02-17 16:19:16,224 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:20,332 INFO Epoch 1 CV info cv_loss 21.002981455999056
2023-02-17 16:19:20,333 INFO Epoch 2 TRAIN info lr 0.0006661999999999999
2023-02-17 16:19:20,336 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:20,561 INFO Epoch 1 CV info cv_loss 21.00298145661931
2023-02-17 16:19:20,562 INFO Epoch 2 TRAIN info lr 0.0006672799999999999
2023-02-17 16:19:20,567 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:19:23,800 INFO Epoch 1 CV info cv_loss 21.002981455999056
2023-02-17 16:19:23,801 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word/1.pt
2023-02-17 16:19:24,386 INFO Epoch 2 TRAIN info lr 0.0006678400000000001
2023-02-17 16:19:24,390 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:20:22,087 DEBUG TRAIN Batch 2/0 loss 21.317394 loss_att 21.371721 loss_ctc 25.748249 loss_rnnt 20.602146 hw_loss 0.213001 lr 0.00066624 rank 4
2023-02-17 16:20:22,088 DEBUG TRAIN Batch 2/0 loss 26.287323 loss_att 27.711044 loss_ctc 33.625481 loss_rnnt 24.947668 hw_loss 0.143417 lr 0.00066672 rank 7
2023-02-17 16:20:22,090 DEBUG TRAIN Batch 2/0 loss 21.724716 loss_att 22.423819 loss_ctc 26.537197 loss_rnnt 20.848713 hw_loss 0.177221 lr 0.00066848 rank 1
2023-02-17 16:20:22,092 DEBUG TRAIN Batch 2/0 loss 25.196405 loss_att 25.598320 loss_ctc 32.358658 loss_rnnt 24.033983 hw_loss 0.238255 lr 0.00066680 rank 3
2023-02-17 16:20:22,096 DEBUG TRAIN Batch 2/0 loss 24.819857 loss_att 24.417160 loss_ctc 30.574202 loss_rnnt 24.072432 hw_loss 0.113847 lr 0.00066812 rank 2
2023-02-17 16:20:22,097 DEBUG TRAIN Batch 2/0 loss 23.945196 loss_att 24.549843 loss_ctc 28.653618 loss_rnnt 23.075169 hw_loss 0.227451 lr 0.00066644 rank 5
2023-02-17 16:20:22,101 DEBUG TRAIN Batch 2/0 loss 20.194399 loss_att 24.376884 loss_ctc 28.030279 loss_rnnt 18.182617 hw_loss 0.244691 lr 0.00066732 rank 6
2023-02-17 16:20:22,133 DEBUG TRAIN Batch 2/0 loss 19.154848 loss_att 20.680470 loss_ctc 23.562250 loss_rnnt 18.136461 hw_loss 0.235515 lr 0.00066788 rank 0
2023-02-17 16:21:34,476 DEBUG TRAIN Batch 2/100 loss 70.489014 loss_att 100.979233 loss_ctc 98.723877 loss_rnnt 60.589745 hw_loss 0.068584 lr 0.00067044 rank 5
2023-02-17 16:21:34,491 DEBUG TRAIN Batch 2/100 loss 42.255451 loss_att 73.469040 loss_ctc 47.499783 loss_rnnt 35.252815 hw_loss 0.113759 lr 0.00067188 rank 0
2023-02-17 16:21:34,491 DEBUG TRAIN Batch 2/100 loss 56.416683 loss_att 76.639755 loss_ctc 75.451569 loss_rnnt 49.747768 hw_loss 0.161842 lr 0.00067212 rank 2
2023-02-17 16:21:34,493 DEBUG TRAIN Batch 2/100 loss 27.961790 loss_att 47.205624 loss_ctc 48.322853 loss_rnnt 21.341003 hw_loss 0.107273 lr 0.00067024 rank 4
2023-02-17 16:21:34,494 DEBUG TRAIN Batch 2/100 loss 31.029268 loss_att 55.045273 loss_ctc 42.146038 loss_rnnt 24.704237 hw_loss 0.074238 lr 0.00067080 rank 3
2023-02-17 16:21:34,494 DEBUG TRAIN Batch 2/100 loss 40.215206 loss_att 70.694534 loss_ctc 52.628521 loss_rnnt 32.449333 hw_loss 0.027936 lr 0.00067132 rank 6
2023-02-17 16:21:34,494 DEBUG TRAIN Batch 2/100 loss 33.620041 loss_att 48.664532 loss_ctc 53.184124 loss_rnnt 27.946020 hw_loss 0.106083 lr 0.00067248 rank 1
2023-02-17 16:21:34,544 DEBUG TRAIN Batch 2/100 loss 31.903589 loss_att 54.259937 loss_ctc 44.326931 loss_rnnt 25.753819 hw_loss 0.041346 lr 0.00067072 rank 7
2023-02-17 16:22:46,802 DEBUG TRAIN Batch 2/200 loss 36.171169 loss_att 47.824093 loss_ctc 49.100662 loss_rnnt 32.116394 hw_loss 0.000484 lr 0.00067424 rank 4
2023-02-17 16:22:46,805 DEBUG TRAIN Batch 2/200 loss 48.026382 loss_att 65.340439 loss_ctc 74.565361 loss_rnnt 40.981941 hw_loss 0.080809 lr 0.00067648 rank 1
2023-02-17 16:22:46,805 DEBUG TRAIN Batch 2/200 loss 56.889294 loss_att 70.555855 loss_ctc 69.633514 loss_rnnt 52.443981 hw_loss 0.023947 lr 0.00067612 rank 2
2023-02-17 16:22:46,807 DEBUG TRAIN Batch 2/200 loss 52.601803 loss_att 69.030334 loss_ctc 73.939911 loss_rnnt 46.450756 hw_loss 0.037990 lr 0.00067532 rank 6
2023-02-17 16:22:46,808 DEBUG TRAIN Batch 2/200 loss 43.602547 loss_att 59.439079 loss_ctc 56.249340 loss_rnnt 38.683960 hw_loss 0.121944 lr 0.00067444 rank 5
2023-02-17 16:22:46,811 DEBUG TRAIN Batch 2/200 loss 64.677353 loss_att 83.940140 loss_ctc 88.342300 loss_rnnt 57.590893 hw_loss 0.147347 lr 0.00067588 rank 0
2023-02-17 16:22:46,813 DEBUG TRAIN Batch 2/200 loss 47.849476 loss_att 59.627319 loss_ctc 63.912563 loss_rnnt 43.301895 hw_loss 0.094249 lr 0.00067480 rank 3
2023-02-17 16:22:46,814 DEBUG TRAIN Batch 2/200 loss 47.126781 loss_att 66.937111 loss_ctc 64.984695 loss_rnnt 40.781364 hw_loss 0.004306 lr 0.00067472 rank 7
2023-02-17 16:24:00,796 DEBUG TRAIN Batch 2/300 loss 37.376873 loss_att 55.594330 loss_ctc 52.915741 loss_rnnt 31.618790 hw_loss 0.080133 lr 0.00067844 rank 5
2023-02-17 16:24:00,808 DEBUG TRAIN Batch 2/300 loss 28.739540 loss_att 43.988754 loss_ctc 40.389565 loss_rnnt 24.098919 hw_loss 0.070209 lr 0.00067824 rank 4
2023-02-17 16:24:00,809 DEBUG TRAIN Batch 2/300 loss 34.992676 loss_att 55.439247 loss_ctc 50.741875 loss_rnnt 28.774988 hw_loss 0.053393 lr 0.00068048 rank 1
2023-02-17 16:24:00,809 DEBUG TRAIN Batch 2/300 loss 27.635645 loss_att 43.277721 loss_ctc 41.161587 loss_rnnt 22.677572 hw_loss 0.049118 lr 0.00068012 rank 2
2023-02-17 16:24:00,811 DEBUG TRAIN Batch 2/300 loss 51.016087 loss_att 68.396317 loss_ctc 73.008858 loss_rnnt 44.591724 hw_loss 0.029895 lr 0.00067932 rank 6
2023-02-17 16:24:00,812 DEBUG TRAIN Batch 2/300 loss 40.908184 loss_att 52.407761 loss_ctc 51.849697 loss_rnnt 37.149307 hw_loss 0.000174 lr 0.00067988 rank 0
2023-02-17 16:24:00,820 DEBUG TRAIN Batch 2/300 loss 48.132240 loss_att 66.874435 loss_ctc 65.176437 loss_rnnt 42.057484 hw_loss 0.100795 lr 0.00067880 rank 3
2023-02-17 16:24:00,856 DEBUG TRAIN Batch 2/300 loss 46.313606 loss_att 75.826820 loss_ctc 67.786400 loss_rnnt 37.518684 hw_loss 0.054828 lr 0.00067872 rank 7
2023-02-17 16:25:14,080 DEBUG TRAIN Batch 2/400 loss 65.306938 loss_att 79.925583 loss_ctc 91.416374 loss_rnnt 58.866699 hw_loss 0.066091 lr 0.00068224 rank 4
2023-02-17 16:25:14,094 DEBUG TRAIN Batch 2/400 loss 49.747387 loss_att 58.928932 loss_ctc 64.566864 loss_rnnt 45.907852 hw_loss 0.051171 lr 0.00068412 rank 2
2023-02-17 16:25:14,098 DEBUG TRAIN Batch 2/400 loss 37.246948 loss_att 51.699982 loss_ctc 53.455418 loss_rnnt 32.146912 hw_loss 0.090562 lr 0.00068332 rank 6
2023-02-17 16:25:14,100 DEBUG TRAIN Batch 2/400 loss 45.519978 loss_att 62.496861 loss_ctc 70.220375 loss_rnnt 38.811813 hw_loss 0.036373 lr 0.00068448 rank 1
2023-02-17 16:25:14,100 DEBUG TRAIN Batch 2/400 loss 68.309128 loss_att 90.203064 loss_ctc 86.574997 loss_rnnt 61.461109 hw_loss 0.063325 lr 0.00068280 rank 3
2023-02-17 16:25:14,102 DEBUG TRAIN Batch 2/400 loss 25.968452 loss_att 41.739113 loss_ctc 43.785774 loss_rnnt 20.320496 hw_loss 0.221584 lr 0.00068244 rank 5
2023-02-17 16:25:14,102 DEBUG TRAIN Batch 2/400 loss 33.423420 loss_att 44.812435 loss_ctc 44.092117 loss_rnnt 29.703533 hw_loss 0.036736 lr 0.00068272 rank 7
2023-02-17 16:25:14,103 DEBUG TRAIN Batch 2/400 loss 45.524834 loss_att 59.896042 loss_ctc 62.005863 loss_rnnt 40.452271 hw_loss 0.001599 lr 0.00068388 rank 0
2023-02-17 16:26:26,785 DEBUG TRAIN Batch 2/500 loss 42.726730 loss_att 53.255554 loss_ctc 60.027649 loss_rnnt 38.290401 hw_loss 0.044579 lr 0.00068812 rank 2
2023-02-17 16:26:26,788 DEBUG TRAIN Batch 2/500 loss 29.731901 loss_att 42.286888 loss_ctc 43.068310 loss_rnnt 25.441900 hw_loss 0.001532 lr 0.00068624 rank 4
2023-02-17 16:26:26,790 DEBUG TRAIN Batch 2/500 loss 40.861881 loss_att 56.554527 loss_ctc 53.733528 loss_rnnt 35.985512 hw_loss 0.040538 lr 0.00068732 rank 6
2023-02-17 16:26:26,790 DEBUG TRAIN Batch 2/500 loss 41.916935 loss_att 60.409561 loss_ctc 54.186085 loss_rnnt 36.516159 hw_loss 0.124425 lr 0.00068644 rank 5
2023-02-17 16:26:26,791 DEBUG TRAIN Batch 2/500 loss 55.565277 loss_att 65.387428 loss_ctc 68.001175 loss_rnnt 51.886475 hw_loss 0.105475 lr 0.00068788 rank 0
2023-02-17 16:26:26,793 DEBUG TRAIN Batch 2/500 loss 39.996967 loss_att 60.301872 loss_ctc 55.814552 loss_rnnt 33.738838 hw_loss 0.165255 lr 0.00068672 rank 7
2023-02-17 16:26:26,794 DEBUG TRAIN Batch 2/500 loss 49.900131 loss_att 60.575272 loss_ctc 69.479935 loss_rnnt 45.138535 hw_loss 0.029861 lr 0.00068680 rank 3
2023-02-17 16:26:26,843 DEBUG TRAIN Batch 2/500 loss 37.451542 loss_att 56.039215 loss_ctc 48.862221 loss_rnnt 32.165653 hw_loss 0.087986 lr 0.00068848 rank 1
2023-02-17 16:27:39,420 DEBUG TRAIN Batch 2/600 loss 26.749249 loss_att 36.522335 loss_ctc 33.329041 loss_rnnt 23.842592 hw_loss 0.140123 lr 0.00069188 rank 0
2023-02-17 16:27:39,425 DEBUG TRAIN Batch 2/600 loss 40.670135 loss_att 45.160210 loss_ctc 50.571312 loss_rnnt 38.437946 hw_loss 0.026273 lr 0.00069072 rank 7
2023-02-17 16:27:39,427 DEBUG TRAIN Batch 2/600 loss 25.336929 loss_att 31.929491 loss_ctc 34.166222 loss_rnnt 22.728039 hw_loss 0.212135 lr 0.00069132 rank 6
2023-02-17 16:27:39,429 DEBUG TRAIN Batch 2/600 loss 28.782415 loss_att 34.094444 loss_ctc 41.297405 loss_rnnt 25.946363 hw_loss 0.196842 lr 0.00069024 rank 4
2023-02-17 16:27:39,430 DEBUG TRAIN Batch 2/600 loss 25.759903 loss_att 31.077030 loss_ctc 33.856606 loss_rnnt 23.550619 hw_loss 0.124311 lr 0.00069212 rank 2
2023-02-17 16:27:39,431 DEBUG TRAIN Batch 2/600 loss 25.179398 loss_att 32.881241 loss_ctc 32.089550 loss_rnnt 22.675167 hw_loss 0.079705 lr 0.00069248 rank 1
2023-02-17 16:27:39,437 DEBUG TRAIN Batch 2/600 loss 38.266548 loss_att 47.449814 loss_ctc 51.055145 loss_rnnt 34.724461 hw_loss 0.000533 lr 0.00069080 rank 3
2023-02-17 16:27:39,439 DEBUG TRAIN Batch 2/600 loss 44.212845 loss_att 53.022141 loss_ctc 56.355854 loss_rnnt 40.749592 hw_loss 0.154366 lr 0.00069044 rank 5
2023-02-17 16:28:54,304 DEBUG TRAIN Batch 2/700 loss 70.445274 loss_att 90.853172 loss_ctc 91.921509 loss_rnnt 63.488411 hw_loss 0.022080 lr 0.00069424 rank 4
2023-02-17 16:28:54,307 DEBUG TRAIN Batch 2/700 loss 23.981361 loss_att 46.015697 loss_ctc 31.753773 loss_rnnt 18.530838 hw_loss 0.013750 lr 0.00069532 rank 6
2023-02-17 16:28:54,309 DEBUG TRAIN Batch 2/700 loss 39.233177 loss_att 56.696896 loss_ctc 63.017876 loss_rnnt 32.541420 hw_loss 0.051977 lr 0.00069612 rank 2
2023-02-17 16:28:54,311 DEBUG TRAIN Batch 2/700 loss 39.174026 loss_att 59.726303 loss_ctc 52.371208 loss_rnnt 33.241646 hw_loss 0.116808 lr 0.00069480 rank 3
2023-02-17 16:28:54,314 DEBUG TRAIN Batch 2/700 loss 62.832787 loss_att 84.177963 loss_ctc 84.551025 loss_rnnt 55.643738 hw_loss 0.045475 lr 0.00069444 rank 5
2023-02-17 16:28:54,320 DEBUG TRAIN Batch 2/700 loss 58.004669 loss_att 77.228836 loss_ctc 77.012047 loss_rnnt 51.624966 hw_loss 0.001029 lr 0.00069472 rank 7
2023-02-17 16:28:54,352 DEBUG TRAIN Batch 2/700 loss 30.875858 loss_att 51.929581 loss_ctc 47.738644 loss_rnnt 24.354366 hw_loss 0.116956 lr 0.00069588 rank 0
2023-02-17 16:28:54,365 DEBUG TRAIN Batch 2/700 loss 72.382210 loss_att 100.427620 loss_ctc 98.165329 loss_rnnt 63.257473 hw_loss 0.146092 lr 0.00069648 rank 1
2023-02-17 16:30:07,161 DEBUG TRAIN Batch 2/800 loss 33.967369 loss_att 53.244816 loss_ctc 47.740875 loss_rnnt 28.215187 hw_loss 0.112922 lr 0.00069824 rank 4
2023-02-17 16:30:07,163 DEBUG TRAIN Batch 2/800 loss 56.036770 loss_att 78.258430 loss_ctc 81.208786 loss_rnnt 48.165112 hw_loss 0.133233 lr 0.00070048 rank 1
2023-02-17 16:30:07,164 DEBUG TRAIN Batch 2/800 loss 41.989586 loss_att 56.168049 loss_ctc 62.750027 loss_rnnt 36.312313 hw_loss 0.137854 lr 0.00069988 rank 0
2023-02-17 16:30:07,165 DEBUG TRAIN Batch 2/800 loss 59.164616 loss_att 80.682579 loss_ctc 90.427513 loss_rnnt 50.599937 hw_loss 0.173813 lr 0.00069872 rank 7
2023-02-17 16:30:07,166 DEBUG TRAIN Batch 2/800 loss 37.067234 loss_att 50.102711 loss_ctc 52.003578 loss_rnnt 32.444351 hw_loss 0.045523 lr 0.00069932 rank 6
2023-02-17 16:30:07,168 DEBUG TRAIN Batch 2/800 loss 26.425734 loss_att 39.703941 loss_ctc 34.974945 loss_rnnt 22.567282 hw_loss 0.117967 lr 0.00069880 rank 3
2023-02-17 16:30:07,169 DEBUG TRAIN Batch 2/800 loss 29.556223 loss_att 46.726398 loss_ctc 45.636837 loss_rnnt 23.933411 hw_loss 0.083798 lr 0.00070012 rank 2
2023-02-17 16:30:07,173 DEBUG TRAIN Batch 2/800 loss 62.076859 loss_att 85.710236 loss_ctc 84.016464 loss_rnnt 54.424480 hw_loss 0.000787 lr 0.00069844 rank 5
2023-02-17 16:31:19,633 DEBUG TRAIN Batch 2/900 loss 46.073921 loss_att 59.889713 loss_ctc 62.863853 loss_rnnt 41.063042 hw_loss 0.016997 lr 0.00070224 rank 4
2023-02-17 16:31:19,635 DEBUG TRAIN Batch 2/900 loss 51.593838 loss_att 78.055237 loss_ctc 68.722244 loss_rnnt 43.942509 hw_loss 0.141111 lr 0.00070448 rank 1
2023-02-17 16:31:19,638 DEBUG TRAIN Batch 2/900 loss 40.435863 loss_att 63.811340 loss_ctc 48.950531 loss_rnnt 34.615089 hw_loss 0.019485 lr 0.00070272 rank 7
2023-02-17 16:31:19,640 DEBUG TRAIN Batch 2/900 loss 40.071697 loss_att 56.514500 loss_ctc 59.412369 loss_rnnt 34.164875 hw_loss 0.074069 lr 0.00070388 rank 0
2023-02-17 16:31:19,640 DEBUG TRAIN Batch 2/900 loss 33.019608 loss_att 56.510540 loss_ctc 51.068748 loss_rnnt 25.877972 hw_loss 0.069179 lr 0.00070412 rank 2
2023-02-17 16:31:19,640 DEBUG TRAIN Batch 2/900 loss 59.378990 loss_att 79.862984 loss_ctc 74.558899 loss_rnnt 53.228943 hw_loss 0.054859 lr 0.00070332 rank 6
2023-02-17 16:31:19,641 DEBUG TRAIN Batch 2/900 loss 36.152370 loss_att 62.269424 loss_ctc 47.983055 loss_rnnt 29.325516 hw_loss 0.048784 lr 0.00070244 rank 5
2023-02-17 16:31:19,642 DEBUG TRAIN Batch 2/900 loss 29.619581 loss_att 43.610180 loss_ctc 45.057983 loss_rnnt 24.698652 hw_loss 0.120667 lr 0.00070280 rank 3
2023-02-17 16:32:32,993 DEBUG TRAIN Batch 2/1000 loss 41.648815 loss_att 52.684998 loss_ctc 57.640099 loss_rnnt 37.250645 hw_loss 0.110176 lr 0.00070732 rank 6
2023-02-17 16:32:33,011 DEBUG TRAIN Batch 2/1000 loss 24.166790 loss_att 37.696690 loss_ctc 35.169380 loss_rnnt 19.978676 hw_loss 0.028351 lr 0.00070848 rank 1
2023-02-17 16:32:33,011 DEBUG TRAIN Batch 2/1000 loss 31.484303 loss_att 47.827396 loss_ctc 41.906628 loss_rnnt 26.761681 hw_loss 0.120672 lr 0.00070624 rank 4
2023-02-17 16:32:33,012 DEBUG TRAIN Batch 2/1000 loss 52.178974 loss_att 73.209480 loss_ctc 68.654099 loss_rnnt 45.728310 hw_loss 0.089774 lr 0.00070680 rank 3
2023-02-17 16:32:33,014 DEBUG TRAIN Batch 2/1000 loss 49.613674 loss_att 67.697449 loss_ctc 66.904709 loss_rnnt 43.667534 hw_loss 0.044840 lr 0.00070644 rank 5
2023-02-17 16:32:33,019 DEBUG TRAIN Batch 2/1000 loss 44.920990 loss_att 55.613773 loss_ctc 62.258766 loss_rnnt 40.383198 hw_loss 0.164117 lr 0.00070788 rank 0
2023-02-17 16:32:33,035 DEBUG TRAIN Batch 2/1000 loss 64.885735 loss_att 80.738907 loss_ctc 89.762383 loss_rnnt 58.316032 hw_loss 0.154087 lr 0.00070672 rank 7
2023-02-17 16:32:33,037 DEBUG TRAIN Batch 2/1000 loss 65.840317 loss_att 79.473724 loss_ctc 94.905106 loss_rnnt 59.190643 hw_loss 0.089413 lr 0.00070812 rank 2
2023-02-17 16:33:47,870 DEBUG TRAIN Batch 2/1100 loss 44.845070 loss_att 56.387695 loss_ctc 58.420967 loss_rnnt 40.636261 hw_loss 0.169051 lr 0.00071024 rank 4
2023-02-17 16:33:47,871 DEBUG TRAIN Batch 2/1100 loss 44.619987 loss_att 60.652092 loss_ctc 62.023533 loss_rnnt 39.022472 hw_loss 0.132413 lr 0.00071072 rank 7
2023-02-17 16:33:47,871 DEBUG TRAIN Batch 2/1100 loss 42.363548 loss_att 58.412575 loss_ctc 57.714497 loss_rnnt 37.085182 hw_loss 0.040816 lr 0.00071132 rank 6
2023-02-17 16:33:47,872 DEBUG TRAIN Batch 2/1100 loss 41.109295 loss_att 53.844032 loss_ctc 51.387772 loss_rnnt 37.130554 hw_loss 0.114988 lr 0.00071044 rank 5
2023-02-17 16:33:47,875 DEBUG TRAIN Batch 2/1100 loss 47.888260 loss_att 66.214569 loss_ctc 68.638107 loss_rnnt 41.441223 hw_loss 0.028359 lr 0.00071248 rank 1
2023-02-17 16:33:47,875 DEBUG TRAIN Batch 2/1100 loss 21.848763 loss_att 38.816795 loss_ctc 36.817680 loss_rnnt 16.439255 hw_loss 0.037583 lr 0.00071188 rank 0
2023-02-17 16:33:47,879 DEBUG TRAIN Batch 2/1100 loss 35.061008 loss_att 48.429985 loss_ctc 50.463528 loss_rnnt 30.272991 hw_loss 0.113536 lr 0.00071080 rank 3
2023-02-17 16:33:47,919 DEBUG TRAIN Batch 2/1100 loss 53.637665 loss_att 68.080490 loss_ctc 69.418106 loss_rnnt 48.600574 hw_loss 0.083374 lr 0.00071212 rank 2
2023-02-17 16:35:00,739 DEBUG TRAIN Batch 2/1200 loss 30.279694 loss_att 41.109795 loss_ctc 44.394852 loss_rnnt 26.214102 hw_loss 0.032908 lr 0.00071532 rank 6
2023-02-17 16:35:00,739 DEBUG TRAIN Batch 2/1200 loss 21.373020 loss_att 24.726189 loss_ctc 28.398451 loss_rnnt 19.756662 hw_loss 0.016872 lr 0.00071424 rank 4
2023-02-17 16:35:00,739 DEBUG TRAIN Batch 2/1200 loss 49.578602 loss_att 52.804054 loss_ctc 67.997696 loss_rnnt 46.463844 hw_loss 0.025849 lr 0.00071444 rank 5
2023-02-17 16:35:00,741 DEBUG TRAIN Batch 2/1200 loss 37.369236 loss_att 46.342392 loss_ctc 55.286945 loss_rnnt 33.185280 hw_loss 0.000555 lr 0.00071648 rank 1
2023-02-17 16:35:00,741 DEBUG TRAIN Batch 2/1200 loss 24.452499 loss_att 33.093285 loss_ctc 38.549080 loss_rnnt 20.774149 hw_loss 0.132463 lr 0.00071472 rank 7
2023-02-17 16:35:00,743 DEBUG TRAIN Batch 2/1200 loss 40.248184 loss_att 51.153339 loss_ctc 59.884369 loss_rnnt 35.384212 hw_loss 0.121465 lr 0.00071612 rank 2
2023-02-17 16:35:00,745 DEBUG TRAIN Batch 2/1200 loss 47.106239 loss_att 67.695992 loss_ctc 64.405815 loss_rnnt 40.638885 hw_loss 0.080229 lr 0.00071480 rank 3
2023-02-17 16:35:00,749 DEBUG TRAIN Batch 2/1200 loss 34.527542 loss_att 41.898975 loss_ctc 50.010712 loss_rnnt 30.948456 hw_loss 0.075707 lr 0.00071588 rank 0
2023-02-17 16:36:13,182 DEBUG TRAIN Batch 2/1300 loss 48.252670 loss_att 64.152893 loss_ctc 57.932144 loss_rnnt 43.715984 hw_loss 0.123843 lr 0.00072048 rank 1
2023-02-17 16:36:13,184 DEBUG TRAIN Batch 2/1300 loss 21.562151 loss_att 30.364296 loss_ctc 28.962708 loss_rnnt 18.781813 hw_loss 0.062193 lr 0.00071824 rank 4
2023-02-17 16:36:13,183 DEBUG TRAIN Batch 2/1300 loss 46.622208 loss_att 62.833195 loss_ctc 65.385178 loss_rnnt 40.860405 hw_loss 0.033510 lr 0.00071932 rank 6
2023-02-17 16:36:13,186 DEBUG TRAIN Batch 2/1300 loss 26.582630 loss_att 41.024063 loss_ctc 35.578239 loss_rnnt 22.494709 hw_loss 0.000412 lr 0.00071988 rank 0
2023-02-17 16:36:13,186 DEBUG TRAIN Batch 2/1300 loss 32.945530 loss_att 52.761009 loss_ctc 45.561749 loss_rnnt 27.299847 hw_loss 0.000800 lr 0.00072012 rank 2
2023-02-17 16:36:13,187 DEBUG TRAIN Batch 2/1300 loss 29.288752 loss_att 34.545891 loss_ctc 34.924671 loss_rnnt 27.423000 hw_loss 0.117880 lr 0.00071880 rank 3
2023-02-17 16:36:13,233 DEBUG TRAIN Batch 2/1300 loss 46.108536 loss_att 61.091888 loss_ctc 63.917442 loss_rnnt 40.717018 hw_loss 0.038106 lr 0.00071872 rank 7
2023-02-17 16:36:13,258 DEBUG TRAIN Batch 2/1300 loss 26.621014 loss_att 43.280479 loss_ctc 39.665901 loss_rnnt 21.477804 hw_loss 0.134990 lr 0.00071844 rank 5
2023-02-17 16:37:28,141 DEBUG TRAIN Batch 2/1400 loss 55.730186 loss_att 79.056931 loss_ctc 78.372139 loss_rnnt 48.018860 hw_loss 0.050717 lr 0.00072224 rank 4
2023-02-17 16:37:28,146 DEBUG TRAIN Batch 2/1400 loss 49.905350 loss_att 71.927597 loss_ctc 73.109970 loss_rnnt 42.345226 hw_loss 0.115742 lr 0.00072332 rank 6
2023-02-17 16:37:28,150 DEBUG TRAIN Batch 2/1400 loss 48.628948 loss_att 66.904221 loss_ctc 68.984436 loss_rnnt 42.188839 hw_loss 0.133105 lr 0.00072272 rank 7
2023-02-17 16:37:28,156 DEBUG TRAIN Batch 2/1400 loss 37.449116 loss_att 48.670387 loss_ctc 61.634308 loss_rnnt 31.895229 hw_loss 0.159261 lr 0.00072388 rank 0
2023-02-17 16:37:28,156 DEBUG TRAIN Batch 2/1400 loss 39.326202 loss_att 58.243889 loss_ctc 67.085876 loss_rnnt 31.814438 hw_loss 0.050503 lr 0.00072448 rank 1
2023-02-17 16:37:28,156 DEBUG TRAIN Batch 2/1400 loss 41.753132 loss_att 59.703190 loss_ctc 58.532104 loss_rnnt 35.925240 hw_loss 0.001279 lr 0.00072412 rank 2
2023-02-17 16:37:28,165 DEBUG TRAIN Batch 2/1400 loss 60.435101 loss_att 72.043518 loss_ctc 74.310692 loss_rnnt 56.231255 hw_loss 0.060163 lr 0.00072244 rank 5
2023-02-17 16:37:28,171 DEBUG TRAIN Batch 2/1400 loss 39.218685 loss_att 60.986336 loss_ctc 63.155556 loss_rnnt 31.629498 hw_loss 0.082637 lr 0.00072280 rank 3
2023-02-17 16:38:41,681 DEBUG TRAIN Batch 2/1500 loss 36.425274 loss_att 50.169544 loss_ctc 48.841965 loss_rnnt 31.984386 hw_loss 0.068386 lr 0.00072788 rank 0
2023-02-17 16:38:41,686 DEBUG TRAIN Batch 2/1500 loss 40.279873 loss_att 55.883064 loss_ctc 56.616348 loss_rnnt 34.921089 hw_loss 0.112413 lr 0.00072732 rank 6
2023-02-17 16:38:41,702 DEBUG TRAIN Batch 2/1500 loss 41.164345 loss_att 57.138485 loss_ctc 51.699448 loss_rnnt 36.549622 hw_loss 0.028524 lr 0.00072848 rank 1
2023-02-17 16:38:41,703 DEBUG TRAIN Batch 2/1500 loss 36.054005 loss_att 52.304142 loss_ctc 61.470043 loss_rnnt 29.357908 hw_loss 0.107366 lr 0.00072812 rank 2
2023-02-17 16:38:41,704 DEBUG TRAIN Batch 2/1500 loss 44.568821 loss_att 66.738319 loss_ctc 66.839180 loss_rnnt 37.141872 hw_loss 0.044374 lr 0.00072680 rank 3
2023-02-17 16:38:41,706 DEBUG TRAIN Batch 2/1500 loss 25.791285 loss_att 39.752449 loss_ctc 41.715637 loss_rnnt 20.862944 hw_loss 0.024115 lr 0.00072672 rank 7
2023-02-17 16:38:41,707 DEBUG TRAIN Batch 2/1500 loss 52.831989 loss_att 70.685501 loss_ctc 77.144302 loss_rnnt 45.981972 hw_loss 0.070629 lr 0.00072624 rank 4
2023-02-17 16:38:41,750 DEBUG TRAIN Batch 2/1500 loss 25.368717 loss_att 40.959423 loss_ctc 40.602962 loss_rnnt 20.195744 hw_loss 0.044249 lr 0.00072644 rank 5
2023-02-17 16:39:53,380 DEBUG TRAIN Batch 2/1600 loss 45.949615 loss_att 67.328201 loss_ctc 65.450317 loss_rnnt 39.049812 hw_loss 0.044986 lr 0.00073132 rank 6
2023-02-17 16:39:53,380 DEBUG TRAIN Batch 2/1600 loss 42.009483 loss_att 58.723804 loss_ctc 60.201328 loss_rnnt 36.206661 hw_loss 0.064459 lr 0.00073212 rank 2
2023-02-17 16:39:53,384 DEBUG TRAIN Batch 2/1600 loss 44.276535 loss_att 60.507034 loss_ctc 64.654144 loss_rnnt 38.288200 hw_loss 0.047284 lr 0.00073188 rank 0
2023-02-17 16:39:53,386 DEBUG TRAIN Batch 2/1600 loss 39.670506 loss_att 58.273991 loss_ctc 57.070251 loss_rnnt 33.609039 hw_loss 0.039005 lr 0.00073024 rank 4
2023-02-17 16:39:53,387 DEBUG TRAIN Batch 2/1600 loss 39.438499 loss_att 57.292366 loss_ctc 59.102737 loss_rnnt 33.198296 hw_loss 0.089122 lr 0.00073080 rank 3
2023-02-17 16:39:53,390 DEBUG TRAIN Batch 2/1600 loss 67.198952 loss_att 79.512680 loss_ctc 88.083450 loss_rnnt 61.951057 hw_loss 0.001015 lr 0.00073044 rank 5
2023-02-17 16:39:53,390 DEBUG TRAIN Batch 2/1600 loss 46.079746 loss_att 64.176872 loss_ctc 66.595139 loss_rnnt 39.591019 hw_loss 0.251092 lr 0.00073248 rank 1
2023-02-17 16:39:53,395 DEBUG TRAIN Batch 2/1600 loss 37.616886 loss_att 51.084793 loss_ctc 55.694794 loss_rnnt 32.501156 hw_loss 0.022060 lr 0.00073072 rank 7
2023-02-17 16:41:05,760 DEBUG TRAIN Batch 2/1700 loss 28.285326 loss_att 39.189590 loss_ctc 43.032639 loss_rnnt 24.101505 hw_loss 0.068733 lr 0.00073648 rank 1
2023-02-17 16:41:05,762 DEBUG TRAIN Batch 2/1700 loss 46.871342 loss_att 62.114933 loss_ctc 69.469681 loss_rnnt 40.768547 hw_loss 0.076809 lr 0.00073588 rank 0
2023-02-17 16:41:05,762 DEBUG TRAIN Batch 2/1700 loss 41.438091 loss_att 58.032574 loss_ctc 57.902657 loss_rnnt 35.877937 hw_loss 0.086214 lr 0.00073480 rank 3
2023-02-17 16:41:05,767 DEBUG TRAIN Batch 2/1700 loss 46.989471 loss_att 62.462608 loss_ctc 67.233376 loss_rnnt 41.126194 hw_loss 0.130234 lr 0.00073424 rank 4
2023-02-17 16:41:05,769 DEBUG TRAIN Batch 2/1700 loss 54.974548 loss_att 67.800797 loss_ctc 74.375450 loss_rnnt 49.751137 hw_loss 0.133817 lr 0.00073532 rank 6
2023-02-17 16:41:05,770 DEBUG TRAIN Batch 2/1700 loss 31.490421 loss_att 47.509281 loss_ctc 47.403347 loss_rnnt 26.107086 hw_loss 0.108448 lr 0.00073444 rank 5
2023-02-17 16:41:05,771 DEBUG TRAIN Batch 2/1700 loss 31.652693 loss_att 46.339920 loss_ctc 49.062996 loss_rnnt 26.318584 hw_loss 0.141168 lr 0.00073612 rank 2
2023-02-17 16:41:05,782 DEBUG TRAIN Batch 2/1700 loss 51.485779 loss_att 66.637291 loss_ctc 77.644997 loss_rnnt 44.932400 hw_loss 0.065968 lr 0.00073472 rank 7
2023-02-17 16:42:20,456 DEBUG TRAIN Batch 2/1800 loss 32.290928 loss_att 40.095863 loss_ctc 46.908863 loss_rnnt 28.748856 hw_loss 0.060048 lr 0.00073824 rank 4
2023-02-17 16:42:20,459 DEBUG TRAIN Batch 2/1800 loss 47.243290 loss_att 57.726791 loss_ctc 62.537819 loss_rnnt 43.092331 hw_loss 0.028103 lr 0.00073988 rank 0
2023-02-17 16:42:20,459 DEBUG TRAIN Batch 2/1800 loss 38.016159 loss_att 51.430161 loss_ctc 51.730312 loss_rnnt 33.474361 hw_loss 0.057083 lr 0.00073932 rank 6
2023-02-17 16:42:20,462 DEBUG TRAIN Batch 2/1800 loss 40.996098 loss_att 51.035629 loss_ctc 49.758381 loss_rnnt 37.790672 hw_loss 0.054769 lr 0.00073880 rank 3
2023-02-17 16:42:20,465 DEBUG TRAIN Batch 2/1800 loss 46.969219 loss_att 56.232468 loss_ctc 61.068481 loss_rnnt 43.170486 hw_loss 0.124085 lr 0.00073872 rank 7
2023-02-17 16:42:20,464 DEBUG TRAIN Batch 2/1800 loss 37.455887 loss_att 49.375740 loss_ctc 54.938667 loss_rnnt 32.727276 hw_loss 0.025509 lr 0.00074048 rank 1
2023-02-17 16:42:20,465 DEBUG TRAIN Batch 2/1800 loss 40.956837 loss_att 51.943443 loss_ctc 59.109924 loss_rnnt 36.278088 hw_loss 0.114408 lr 0.00074012 rank 2
2023-02-17 16:42:20,467 DEBUG TRAIN Batch 2/1800 loss 31.013004 loss_att 37.396309 loss_ctc 44.183411 loss_rnnt 27.963501 hw_loss 0.031482 lr 0.00073844 rank 5
2023-02-17 16:43:33,288 DEBUG TRAIN Batch 2/1900 loss 18.736855 loss_att 20.032936 loss_ctc 24.632217 loss_rnnt 17.583338 hw_loss 0.202972 lr 0.00074388 rank 0
2023-02-17 16:43:33,291 DEBUG TRAIN Batch 2/1900 loss 23.883955 loss_att 26.613297 loss_ctc 31.338879 loss_rnnt 22.251255 hw_loss 0.174075 lr 0.00074332 rank 6
2023-02-17 16:43:33,291 DEBUG TRAIN Batch 2/1900 loss 48.287197 loss_att 74.219536 loss_ctc 76.920227 loss_rnnt 39.282692 hw_loss 0.000565 lr 0.00074224 rank 4
2023-02-17 16:43:33,293 DEBUG TRAIN Batch 2/1900 loss 43.154583 loss_att 69.011490 loss_ctc 65.089630 loss_rnnt 35.043903 hw_loss 0.027420 lr 0.00074412 rank 2
2023-02-17 16:43:33,293 DEBUG TRAIN Batch 2/1900 loss 43.387257 loss_att 52.988670 loss_ctc 57.873466 loss_rnnt 39.472736 hw_loss 0.117650 lr 0.00074280 rank 3
2023-02-17 16:43:33,295 DEBUG TRAIN Batch 2/1900 loss 16.289101 loss_att 18.486057 loss_ctc 20.137398 loss_rnnt 15.236805 hw_loss 0.187120 lr 0.00074244 rank 5
2023-02-17 16:43:33,300 DEBUG TRAIN Batch 2/1900 loss 29.012760 loss_att 46.928219 loss_ctc 52.464657 loss_rnnt 22.242504 hw_loss 0.112963 lr 0.00074272 rank 7
2023-02-17 16:43:33,342 DEBUG TRAIN Batch 2/1900 loss 22.770920 loss_att 29.960888 loss_ctc 32.266865 loss_rnnt 19.973417 hw_loss 0.175093 lr 0.00074448 rank 1
2023-02-17 16:44:45,700 DEBUG TRAIN Batch 2/2000 loss 36.508884 loss_att 57.741703 loss_ctc 57.281078 loss_rnnt 29.462034 hw_loss 0.057491 lr 0.00074788 rank 0
2023-02-17 16:44:45,717 DEBUG TRAIN Batch 2/2000 loss 10.514135 loss_att 26.983849 loss_ctc 15.918228 loss_rnnt 6.466681 hw_loss 0.061813 lr 0.00074848 rank 1
2023-02-17 16:44:45,723 DEBUG TRAIN Batch 2/2000 loss 28.204605 loss_att 41.788509 loss_ctc 47.517132 loss_rnnt 22.912342 hw_loss 0.000895 lr 0.00074624 rank 4
2023-02-17 16:44:45,724 DEBUG TRAIN Batch 2/2000 loss 34.433937 loss_att 46.213753 loss_ctc 51.417839 loss_rnnt 29.793707 hw_loss 0.037024 lr 0.00074732 rank 6
2023-02-17 16:44:45,724 DEBUG TRAIN Batch 2/2000 loss 24.863373 loss_att 38.115936 loss_ctc 32.040909 loss_rnnt 21.228249 hw_loss 0.051764 lr 0.00074812 rank 2
2023-02-17 16:44:45,724 DEBUG TRAIN Batch 2/2000 loss 34.290394 loss_att 56.980858 loss_ctc 57.664913 loss_rnnt 26.523550 hw_loss 0.210277 lr 0.00074680 rank 3
2023-02-17 16:44:45,728 DEBUG TRAIN Batch 2/2000 loss 56.130985 loss_att 77.781937 loss_ctc 80.948288 loss_rnnt 48.406303 hw_loss 0.160351 lr 0.00074644 rank 5
2023-02-17 16:44:45,771 DEBUG TRAIN Batch 2/2000 loss 37.831684 loss_att 52.770309 loss_ctc 54.749435 loss_rnnt 32.566490 hw_loss 0.040817 lr 0.00074672 rank 7
2023-02-17 16:46:00,519 DEBUG TRAIN Batch 2/2100 loss 55.319691 loss_att 78.255836 loss_ctc 80.267426 loss_rnnt 47.383743 hw_loss 0.041908 lr 0.00075024 rank 4
2023-02-17 16:46:00,522 DEBUG TRAIN Batch 2/2100 loss 25.918814 loss_att 34.086311 loss_ctc 36.059719 loss_rnnt 22.848547 hw_loss 0.158710 lr 0.00075188 rank 0
2023-02-17 16:46:00,524 DEBUG TRAIN Batch 2/2100 loss 33.136009 loss_att 46.145992 loss_ctc 44.095398 loss_rnnt 29.005285 hw_loss 0.126513 lr 0.00075248 rank 1
2023-02-17 16:46:00,525 DEBUG TRAIN Batch 2/2100 loss 32.126068 loss_att 50.763443 loss_ctc 48.041988 loss_rnnt 26.261227 hw_loss 0.028578 lr 0.00075212 rank 2
2023-02-17 16:46:00,528 DEBUG TRAIN Batch 2/2100 loss 37.890846 loss_att 47.832504 loss_ctc 54.086559 loss_rnnt 33.670525 hw_loss 0.136051 lr 0.00075132 rank 6
2023-02-17 16:46:00,528 DEBUG TRAIN Batch 2/2100 loss 49.776154 loss_att 69.757675 loss_ctc 80.526047 loss_rnnt 41.601086 hw_loss 0.147707 lr 0.00075072 rank 7
2023-02-17 16:46:00,529 DEBUG TRAIN Batch 2/2100 loss 24.818495 loss_att 37.216953 loss_ctc 34.177414 loss_rnnt 21.079754 hw_loss 0.020985 lr 0.00075044 rank 5
2023-02-17 16:46:00,531 DEBUG TRAIN Batch 2/2100 loss 44.908382 loss_att 61.855019 loss_ctc 65.584763 loss_rnnt 38.761757 hw_loss 0.000829 lr 0.00075080 rank 3
2023-02-17 16:47:14,347 DEBUG TRAIN Batch 2/2200 loss 28.637592 loss_att 37.923306 loss_ctc 38.852722 loss_rnnt 25.398123 hw_loss 0.038081 lr 0.00075648 rank 1
2023-02-17 16:47:14,350 DEBUG TRAIN Batch 2/2200 loss 43.623466 loss_att 54.800224 loss_ctc 64.737503 loss_rnnt 38.572762 hw_loss 0.000289 lr 0.00075532 rank 6
2023-02-17 16:47:14,353 DEBUG TRAIN Batch 2/2200 loss 27.060768 loss_att 40.681652 loss_ctc 40.939392 loss_rnnt 22.484520 hw_loss 0.002974 lr 0.00075480 rank 3
2023-02-17 16:47:14,353 DEBUG TRAIN Batch 2/2200 loss 34.268700 loss_att 42.956039 loss_ctc 49.250546 loss_rnnt 30.507431 hw_loss 0.049165 lr 0.00075612 rank 2
2023-02-17 16:47:14,355 DEBUG TRAIN Batch 2/2200 loss 33.627411 loss_att 47.823753 loss_ctc 52.275143 loss_rnnt 28.289402 hw_loss 0.023199 lr 0.00075424 rank 4
2023-02-17 16:47:14,357 DEBUG TRAIN Batch 2/2200 loss 49.803314 loss_att 61.667175 loss_ctc 80.386536 loss_rnnt 43.291855 hw_loss 0.114233 lr 0.00075588 rank 0
2023-02-17 16:47:14,359 DEBUG TRAIN Batch 2/2200 loss 40.080322 loss_att 57.345455 loss_ctc 56.179535 loss_rnnt 34.423122 hw_loss 0.108027 lr 0.00075472 rank 7
2023-02-17 16:47:14,360 DEBUG TRAIN Batch 2/2200 loss 46.855709 loss_att 62.856628 loss_ctc 70.491089 loss_rnnt 40.482433 hw_loss 0.040699 lr 0.00075444 rank 5
2023-02-17 16:48:26,559 DEBUG TRAIN Batch 2/2300 loss 52.753700 loss_att 60.229279 loss_ctc 68.926331 loss_rnnt 49.058525 hw_loss 0.081959 lr 0.00076012 rank 2
2023-02-17 16:48:26,564 DEBUG TRAIN Batch 2/2300 loss 40.745625 loss_att 56.076450 loss_ctc 60.098709 loss_rnnt 35.040363 hw_loss 0.110023 lr 0.00075824 rank 4
2023-02-17 16:48:26,566 DEBUG TRAIN Batch 2/2300 loss 23.524965 loss_att 35.752743 loss_ctc 39.855091 loss_rnnt 18.861292 hw_loss 0.076439 lr 0.00075932 rank 6
2023-02-17 16:48:26,567 DEBUG TRAIN Batch 2/2300 loss 60.398464 loss_att 77.809036 loss_ctc 94.270607 loss_rnnt 52.280445 hw_loss 0.224279 lr 0.00075880 rank 3
2023-02-17 16:48:26,570 DEBUG TRAIN Batch 2/2300 loss 29.082899 loss_att 39.187305 loss_ctc 42.641888 loss_rnnt 25.167372 hw_loss 0.162714 lr 0.00075988 rank 0
2023-02-17 16:48:26,571 DEBUG TRAIN Batch 2/2300 loss 29.897261 loss_att 42.921871 loss_ctc 46.492180 loss_rnnt 25.016319 hw_loss 0.118804 lr 0.00076048 rank 1
2023-02-17 16:48:26,574 DEBUG TRAIN Batch 2/2300 loss 34.193268 loss_att 46.324570 loss_ctc 45.553108 loss_rnnt 30.230576 hw_loss 0.040850 lr 0.00075844 rank 5
2023-02-17 16:48:26,615 DEBUG TRAIN Batch 2/2300 loss 31.068504 loss_att 43.559792 loss_ctc 43.289898 loss_rnnt 26.893654 hw_loss 0.088263 lr 0.00075872 rank 7
2023-02-17 16:49:39,478 DEBUG TRAIN Batch 2/2400 loss 38.096733 loss_att 54.598312 loss_ctc 60.324154 loss_rnnt 31.797554 hw_loss 0.066013 lr 0.00076448 rank 1
2023-02-17 16:49:39,480 DEBUG TRAIN Batch 2/2400 loss 31.027243 loss_att 41.363380 loss_ctc 47.572338 loss_rnnt 26.659620 hw_loss 0.176962 lr 0.00076244 rank 5
2023-02-17 16:49:39,480 DEBUG TRAIN Batch 2/2400 loss 16.923492 loss_att 28.176704 loss_ctc 26.266829 loss_rnnt 13.359534 hw_loss 0.126636 lr 0.00076224 rank 4
2023-02-17 16:49:39,482 DEBUG TRAIN Batch 2/2400 loss 35.030544 loss_att 60.042992 loss_ctc 54.100445 loss_rnnt 27.403072 hw_loss 0.154371 lr 0.00076332 rank 6
2023-02-17 16:49:39,483 DEBUG TRAIN Batch 2/2400 loss 37.589546 loss_att 49.065609 loss_ctc 46.778923 loss_rnnt 34.061584 hw_loss 0.014061 lr 0.00076388 rank 0
2023-02-17 16:49:39,485 DEBUG TRAIN Batch 2/2400 loss 24.465960 loss_att 33.778442 loss_ctc 35.072586 loss_rnnt 21.186121 hw_loss 0.005855 lr 0.00076280 rank 3
2023-02-17 16:49:39,486 DEBUG TRAIN Batch 2/2400 loss 39.429321 loss_att 53.871635 loss_ctc 57.168423 loss_rnnt 34.149597 hw_loss 0.048836 lr 0.00076412 rank 2
2023-02-17 16:49:39,489 DEBUG TRAIN Batch 2/2400 loss 55.868446 loss_att 65.012177 loss_ctc 80.263458 loss_rnnt 50.750931 hw_loss 0.067679 lr 0.00076272 rank 7
2023-02-17 16:50:56,068 DEBUG TRAIN Batch 2/2500 loss 18.018154 loss_att 21.771168 loss_ctc 25.252037 loss_rnnt 16.196083 hw_loss 0.200535 lr 0.00076624 rank 4
2023-02-17 16:50:56,070 DEBUG TRAIN Batch 2/2500 loss 39.998695 loss_att 48.855942 loss_ctc 58.957767 loss_rnnt 35.619747 hw_loss 0.149290 lr 0.00076732 rank 6
2023-02-17 16:50:56,074 DEBUG TRAIN Batch 2/2500 loss 36.415562 loss_att 43.842304 loss_ctc 50.253647 loss_rnnt 33.035484 hw_loss 0.093090 lr 0.00076788 rank 0
2023-02-17 16:50:56,080 DEBUG TRAIN Batch 2/2500 loss 49.468742 loss_att 58.152027 loss_ctc 62.518295 loss_rnnt 45.961334 hw_loss 0.057764 lr 0.00076848 rank 1
2023-02-17 16:50:56,080 DEBUG TRAIN Batch 2/2500 loss 38.536850 loss_att 44.559685 loss_ctc 51.735855 loss_rnnt 35.536343 hw_loss 0.067632 lr 0.00076680 rank 3
2023-02-17 16:50:56,081 DEBUG TRAIN Batch 2/2500 loss 34.521313 loss_att 38.936005 loss_ctc 46.569016 loss_rnnt 31.995789 hw_loss 0.067925 lr 0.00076644 rank 5
2023-02-17 16:50:56,082 DEBUG TRAIN Batch 2/2500 loss 59.904915 loss_att 77.586197 loss_ctc 90.731888 loss_rnnt 52.254288 hw_loss 0.007709 lr 0.00076672 rank 7
2023-02-17 16:50:56,121 DEBUG TRAIN Batch 2/2500 loss 18.946810 loss_att 23.138865 loss_ctc 25.875721 loss_rnnt 17.132315 hw_loss 0.097932 lr 0.00076812 rank 2
2023-02-17 16:52:09,927 DEBUG TRAIN Batch 2/2600 loss 22.902054 loss_att 38.434978 loss_ctc 31.181005 loss_rnnt 18.663044 hw_loss 0.053558 lr 0.00077024 rank 4
2023-02-17 16:52:09,933 DEBUG TRAIN Batch 2/2600 loss 33.850273 loss_att 53.913948 loss_ctc 49.977612 loss_rnnt 27.598372 hw_loss 0.166610 lr 0.00077080 rank 3
2023-02-17 16:52:09,933 DEBUG TRAIN Batch 2/2600 loss 52.490631 loss_att 61.557602 loss_ctc 77.621468 loss_rnnt 47.298950 hw_loss 0.051572 lr 0.00077212 rank 2
2023-02-17 16:52:09,933 DEBUG TRAIN Batch 2/2600 loss 60.191532 loss_att 71.244644 loss_ctc 77.208122 loss_rnnt 55.649567 hw_loss 0.117121 lr 0.00077248 rank 1
2023-02-17 16:52:09,934 DEBUG TRAIN Batch 2/2600 loss 26.798206 loss_att 44.302692 loss_ctc 40.701767 loss_rnnt 21.413330 hw_loss 0.056566 lr 0.00077132 rank 6
2023-02-17 16:52:09,933 DEBUG TRAIN Batch 2/2600 loss 46.299992 loss_att 67.221344 loss_ctc 67.284012 loss_rnnt 39.303589 hw_loss 0.026745 lr 0.00077072 rank 7
2023-02-17 16:52:09,936 DEBUG TRAIN Batch 2/2600 loss 47.439659 loss_att 69.501068 loss_ctc 71.495087 loss_rnnt 39.749306 hw_loss 0.132533 lr 0.00077044 rank 5
2023-02-17 16:52:09,938 DEBUG TRAIN Batch 2/2600 loss 31.981907 loss_att 43.440796 loss_ctc 48.499279 loss_rnnt 27.487411 hw_loss 0.000753 lr 0.00077188 rank 0
2023-02-17 16:53:21,604 DEBUG TRAIN Batch 2/2700 loss 61.201748 loss_att 81.101692 loss_ctc 83.191879 loss_rnnt 54.249146 hw_loss 0.076109 lr 0.00077588 rank 0
2023-02-17 16:53:21,606 DEBUG TRAIN Batch 2/2700 loss 54.808369 loss_att 77.259521 loss_ctc 64.328255 loss_rnnt 48.971123 hw_loss 0.145682 lr 0.00077648 rank 1
2023-02-17 16:53:21,608 DEBUG TRAIN Batch 2/2700 loss 29.426352 loss_att 44.850792 loss_ctc 44.671715 loss_rnnt 24.280849 hw_loss 0.052311 lr 0.00077532 rank 6
2023-02-17 16:53:21,608 DEBUG TRAIN Batch 2/2700 loss 37.779861 loss_att 49.794106 loss_ctc 52.587334 loss_rnnt 33.380135 hw_loss 0.042279 lr 0.00077612 rank 2
2023-02-17 16:53:21,610 DEBUG TRAIN Batch 2/2700 loss 49.343891 loss_att 69.388687 loss_ctc 76.375809 loss_rnnt 41.719280 hw_loss 0.021373 lr 0.00077472 rank 7
2023-02-17 16:53:21,610 DEBUG TRAIN Batch 2/2700 loss 21.676462 loss_att 37.187187 loss_ctc 32.688457 loss_rnnt 17.030775 hw_loss 0.141140 lr 0.00077480 rank 3
2023-02-17 16:53:21,612 DEBUG TRAIN Batch 2/2700 loss 33.962276 loss_att 49.501801 loss_ctc 45.386395 loss_rnnt 29.305077 hw_loss 0.048892 lr 0.00077424 rank 4
2023-02-17 16:53:21,619 DEBUG TRAIN Batch 2/2700 loss 40.912659 loss_att 59.048454 loss_ctc 61.278389 loss_rnnt 34.523438 hw_loss 0.087437 lr 0.00077444 rank 5
2023-02-17 16:54:35,285 DEBUG TRAIN Batch 2/2800 loss 25.808149 loss_att 38.724216 loss_ctc 32.693096 loss_rnnt 22.281116 hw_loss 0.048423 lr 0.00077932 rank 6
2023-02-17 16:54:35,296 DEBUG TRAIN Batch 2/2800 loss 47.001122 loss_att 64.198318 loss_ctc 68.153900 loss_rnnt 40.723995 hw_loss 0.032463 lr 0.00077824 rank 4
2023-02-17 16:54:35,300 DEBUG TRAIN Batch 2/2800 loss 29.469175 loss_att 42.657406 loss_ctc 43.665398 loss_rnnt 24.907097 hw_loss 0.059252 lr 0.00077844 rank 5
2023-02-17 16:54:35,305 DEBUG TRAIN Batch 2/2800 loss 49.206451 loss_att 65.634239 loss_ctc 76.523766 loss_rnnt 42.249847 hw_loss 0.053881 lr 0.00077988 rank 0
2023-02-17 16:54:35,307 DEBUG TRAIN Batch 2/2800 loss 44.475658 loss_att 60.518402 loss_ctc 76.604355 loss_rnnt 36.898270 hw_loss 0.159403 lr 0.00078012 rank 2
2023-02-17 16:54:35,318 DEBUG TRAIN Batch 2/2800 loss 73.765152 loss_att 86.680809 loss_ctc 97.752197 loss_rnnt 67.981804 hw_loss 0.003646 lr 0.00077880 rank 3
2023-02-17 16:54:35,332 DEBUG TRAIN Batch 2/2800 loss 37.101501 loss_att 57.018414 loss_ctc 54.310753 loss_rnnt 30.782564 hw_loss 0.076850 lr 0.00078048 rank 1
2023-02-17 16:54:35,342 DEBUG TRAIN Batch 2/2800 loss 22.204361 loss_att 34.309639 loss_ctc 34.337563 loss_rnnt 18.158741 hw_loss 0.012757 lr 0.00077872 rank 7
2023-02-17 16:55:48,553 DEBUG TRAIN Batch 2/2900 loss 55.657375 loss_att 77.003235 loss_ctc 78.013382 loss_rnnt 48.376152 hw_loss 0.058604 lr 0.00078388 rank 0
2023-02-17 16:55:48,553 DEBUG TRAIN Batch 2/2900 loss 44.302246 loss_att 62.636288 loss_ctc 55.933712 loss_rnnt 39.067650 hw_loss 0.031737 lr 0.00078448 rank 1
2023-02-17 16:55:48,555 DEBUG TRAIN Batch 2/2900 loss 33.891792 loss_att 46.719883 loss_ctc 50.164867 loss_rnnt 29.106165 hw_loss 0.094244 lr 0.00078412 rank 2
2023-02-17 16:55:48,557 DEBUG TRAIN Batch 2/2900 loss 41.784016 loss_att 52.907288 loss_ctc 62.588997 loss_rnnt 36.765507 hw_loss 0.037229 lr 0.00078224 rank 4
2023-02-17 16:55:48,556 DEBUG TRAIN Batch 2/2900 loss 29.141144 loss_att 41.825382 loss_ctc 44.200680 loss_rnnt 24.596107 hw_loss 0.000466 lr 0.00078244 rank 5
2023-02-17 16:55:48,558 DEBUG TRAIN Batch 2/2900 loss 64.013069 loss_att 84.293884 loss_ctc 91.501137 loss_rnnt 56.225708 hw_loss 0.123982 lr 0.00078332 rank 6
2023-02-17 16:55:48,558 DEBUG TRAIN Batch 2/2900 loss 42.896599 loss_att 47.589695 loss_ctc 57.132160 loss_rnnt 40.029900 hw_loss 0.056257 lr 0.00078280 rank 3
2023-02-17 16:55:48,608 DEBUG TRAIN Batch 2/2900 loss 46.512192 loss_att 59.097847 loss_ctc 66.301712 loss_rnnt 41.340569 hw_loss 0.029794 lr 0.00078272 rank 7
2023-02-17 16:57:00,580 DEBUG TRAIN Batch 2/3000 loss 25.039190 loss_att 36.552917 loss_ctc 40.709515 loss_rnnt 20.588158 hw_loss 0.110459 lr 0.00078624 rank 4
2023-02-17 16:57:00,580 DEBUG TRAIN Batch 2/3000 loss 25.182852 loss_att 30.852896 loss_ctc 39.449936 loss_rnnt 22.121529 hw_loss 0.046943 lr 0.00078732 rank 6
2023-02-17 16:57:00,581 DEBUG TRAIN Batch 2/3000 loss 36.585598 loss_att 46.897167 loss_ctc 51.416046 loss_rnnt 32.517387 hw_loss 0.053444 lr 0.00078788 rank 0
2023-02-17 16:57:00,582 DEBUG TRAIN Batch 2/3000 loss 48.904518 loss_att 58.286514 loss_ctc 76.177292 loss_rnnt 43.367981 hw_loss 0.044566 lr 0.00078812 rank 2
2023-02-17 16:57:00,583 DEBUG TRAIN Batch 2/3000 loss 35.392109 loss_att 42.273724 loss_ctc 43.030640 loss_rnnt 32.952637 hw_loss 0.083772 lr 0.00078644 rank 5
2023-02-17 16:57:00,583 DEBUG TRAIN Batch 2/3000 loss 21.984871 loss_att 31.084784 loss_ctc 32.020267 loss_rnnt 18.788391 hw_loss 0.072084 lr 0.00078672 rank 7
2023-02-17 16:57:00,590 DEBUG TRAIN Batch 2/3000 loss 45.468349 loss_att 62.820518 loss_ctc 67.483910 loss_rnnt 39.060871 hw_loss 0.003057 lr 0.00078680 rank 3
2023-02-17 16:57:00,636 DEBUG TRAIN Batch 2/3000 loss 42.759090 loss_att 54.063164 loss_ctc 65.202103 loss_rnnt 37.465916 hw_loss 0.074916 lr 0.00078848 rank 1
2023-02-17 16:58:12,856 DEBUG TRAIN Batch 2/3100 loss 42.299995 loss_att 52.470741 loss_ctc 59.349831 loss_rnnt 37.958344 hw_loss 0.064105 lr 0.00079024 rank 4
2023-02-17 16:58:12,862 DEBUG TRAIN Batch 2/3100 loss 28.550299 loss_att 33.180679 loss_ctc 43.304939 loss_rnnt 25.591103 hw_loss 0.123437 lr 0.00079132 rank 6
2023-02-17 16:58:12,866 DEBUG TRAIN Batch 2/3100 loss 37.083694 loss_att 43.921787 loss_ctc 51.984142 loss_rnnt 33.612118 hw_loss 0.219809 lr 0.00079072 rank 7
2023-02-17 16:58:12,867 DEBUG TRAIN Batch 2/3100 loss 36.827309 loss_att 40.564278 loss_ctc 49.610588 loss_rnnt 34.327019 hw_loss 0.090857 lr 0.00079188 rank 0
2023-02-17 16:58:12,872 DEBUG TRAIN Batch 2/3100 loss 25.234919 loss_att 37.021675 loss_ctc 43.098370 loss_rnnt 20.459030 hw_loss 0.068895 lr 0.00079044 rank 5
2023-02-17 16:58:12,873 DEBUG TRAIN Batch 2/3100 loss 36.714886 loss_att 45.562279 loss_ctc 49.897213 loss_rnnt 33.127018 hw_loss 0.113898 lr 0.00079080 rank 3
2023-02-17 16:58:12,898 DEBUG TRAIN Batch 2/3100 loss 35.250977 loss_att 41.019844 loss_ctc 48.111610 loss_rnnt 32.382030 hw_loss 0.000796 lr 0.00079212 rank 2
2023-02-17 16:58:12,913 DEBUG TRAIN Batch 2/3100 loss 39.361874 loss_att 46.779011 loss_ctc 58.185329 loss_rnnt 35.342464 hw_loss 0.049105 lr 0.00079248 rank 1
2023-02-17 16:59:28,606 DEBUG TRAIN Batch 2/3200 loss 41.116909 loss_att 70.878937 loss_ctc 61.418606 loss_rnnt 32.457375 hw_loss 0.000440 lr 0.00079532 rank 6
2023-02-17 16:59:28,612 DEBUG TRAIN Batch 2/3200 loss 24.890043 loss_att 39.123615 loss_ctc 34.971092 loss_rnnt 20.635838 hw_loss 0.118785 lr 0.00079424 rank 4
2023-02-17 16:59:28,617 DEBUG TRAIN Batch 2/3200 loss 33.966743 loss_att 47.796848 loss_ctc 53.233807 loss_rnnt 28.622784 hw_loss 0.016870 lr 0.00079472 rank 7
2023-02-17 16:59:28,618 DEBUG TRAIN Batch 2/3200 loss 31.881281 loss_att 38.678810 loss_ctc 42.254677 loss_rnnt 29.127293 hw_loss 0.021299 lr 0.00079648 rank 1
2023-02-17 16:59:28,619 DEBUG TRAIN Batch 2/3200 loss 60.905499 loss_att 82.438004 loss_ctc 80.823082 loss_rnnt 53.925549 hw_loss 0.033324 lr 0.00079588 rank 0
2023-02-17 16:59:28,619 DEBUG TRAIN Batch 2/3200 loss 46.749699 loss_att 64.758560 loss_ctc 72.101868 loss_rnnt 39.709412 hw_loss 0.109178 lr 0.00079612 rank 2
2023-02-17 16:59:28,624 DEBUG TRAIN Batch 2/3200 loss 18.016457 loss_att 20.305798 loss_ctc 24.153242 loss_rnnt 16.637716 hw_loss 0.192444 lr 0.00079480 rank 3
2023-02-17 16:59:28,669 DEBUG TRAIN Batch 2/3200 loss 54.160496 loss_att 67.183212 loss_ctc 84.266563 loss_rnnt 47.490288 hw_loss 0.096600 lr 0.00079444 rank 5
2023-02-17 17:00:41,285 DEBUG TRAIN Batch 2/3300 loss 16.641172 loss_att 32.997494 loss_ctc 27.305412 loss_rnnt 11.923088 hw_loss 0.046727 lr 0.00079824 rank 4
2023-02-17 17:00:41,287 DEBUG TRAIN Batch 2/3300 loss 56.161850 loss_att 68.575653 loss_ctc 76.600853 loss_rnnt 50.878521 hw_loss 0.141313 lr 0.00079988 rank 0
2023-02-17 17:00:41,287 DEBUG TRAIN Batch 2/3300 loss 20.881151 loss_att 29.995047 loss_ctc 27.789047 loss_rnnt 18.092855 hw_loss 0.083377 lr 0.00079932 rank 6
2023-02-17 17:00:41,289 DEBUG TRAIN Batch 2/3300 loss 48.370811 loss_att 63.276207 loss_ctc 64.836594 loss_rnnt 43.109390 hw_loss 0.159192 lr 0.00079844 rank 5
2023-02-17 17:00:41,291 DEBUG TRAIN Batch 2/3300 loss 44.764317 loss_att 54.414108 loss_ctc 66.361305 loss_rnnt 39.947327 hw_loss 0.013932 lr 0.00080012 rank 2
2023-02-17 17:00:41,291 DEBUG TRAIN Batch 2/3300 loss 43.967510 loss_att 49.381893 loss_ctc 73.159294 loss_rnnt 38.962296 hw_loss 0.056439 lr 0.00079880 rank 3
2023-02-17 17:00:41,292 DEBUG TRAIN Batch 2/3300 loss 52.303528 loss_att 67.984344 loss_ctc 78.033859 loss_rnnt 45.736374 hw_loss 0.000527 lr 0.00080048 rank 1
2023-02-17 17:00:41,295 DEBUG TRAIN Batch 2/3300 loss 55.292103 loss_att 71.658852 loss_ctc 84.919456 loss_rnnt 48.036598 hw_loss 0.059693 lr 0.00079872 rank 7
2023-02-17 17:01:53,940 DEBUG TRAIN Batch 2/3400 loss 50.412468 loss_att 72.815292 loss_ctc 76.758331 loss_rnnt 42.380905 hw_loss 0.071654 lr 0.00080224 rank 4
2023-02-17 17:01:53,944 DEBUG TRAIN Batch 2/3400 loss 60.677029 loss_att 75.192238 loss_ctc 87.838684 loss_rnnt 54.077927 hw_loss 0.139690 lr 0.00080280 rank 3
2023-02-17 17:01:53,949 DEBUG TRAIN Batch 2/3400 loss 30.804380 loss_att 44.297554 loss_ctc 44.156425 loss_rnnt 26.325371 hw_loss 0.000185 lr 0.00080272 rank 7
2023-02-17 17:01:53,949 DEBUG TRAIN Batch 2/3400 loss 29.494839 loss_att 41.951279 loss_ctc 42.945999 loss_rnnt 25.168287 hw_loss 0.078328 lr 0.00080332 rank 6
2023-02-17 17:01:53,949 DEBUG TRAIN Batch 2/3400 loss 34.408928 loss_att 44.788685 loss_ctc 50.910950 loss_rnnt 30.111374 hw_loss 0.039994 lr 0.00080412 rank 2
2023-02-17 17:01:53,951 DEBUG TRAIN Batch 2/3400 loss 32.289001 loss_att 48.991436 loss_ctc 58.671806 loss_rnnt 25.415260 hw_loss 0.029145 lr 0.00080448 rank 1
2023-02-17 17:01:53,951 DEBUG TRAIN Batch 2/3400 loss 36.120956 loss_att 46.380241 loss_ctc 49.183990 loss_rnnt 32.261105 hw_loss 0.124231 lr 0.00080388 rank 0
2023-02-17 17:01:53,953 DEBUG TRAIN Batch 2/3400 loss 38.648121 loss_att 54.921349 loss_ctc 50.919949 loss_rnnt 33.706863 hw_loss 0.094442 lr 0.00080244 rank 5
2023-02-17 17:03:07,144 DEBUG TRAIN Batch 2/3500 loss 53.787960 loss_att 65.170105 loss_ctc 81.589951 loss_rnnt 47.768791 hw_loss 0.067139 lr 0.00080624 rank 4
2023-02-17 17:03:07,145 DEBUG TRAIN Batch 2/3500 loss 38.156029 loss_att 54.120407 loss_ctc 60.173264 loss_rnnt 31.999725 hw_loss 0.052116 lr 0.00080848 rank 1
2023-02-17 17:03:07,148 DEBUG TRAIN Batch 2/3500 loss 40.875710 loss_att 51.073086 loss_ctc 61.886459 loss_rnnt 36.020252 hw_loss 0.027278 lr 0.00080812 rank 2
2023-02-17 17:03:07,149 DEBUG TRAIN Batch 2/3500 loss 35.682148 loss_att 50.792992 loss_ctc 47.611691 loss_rnnt 31.013607 hw_loss 0.104560 lr 0.00080788 rank 0
2023-02-17 17:03:07,149 DEBUG TRAIN Batch 2/3500 loss 28.399044 loss_att 41.722435 loss_ctc 35.881058 loss_rnnt 24.725113 hw_loss 0.021844 lr 0.00080644 rank 5
2023-02-17 17:03:07,150 DEBUG TRAIN Batch 2/3500 loss 29.474594 loss_att 47.041134 loss_ctc 48.583443 loss_rnnt 23.365097 hw_loss 0.090637 lr 0.00080680 rank 3
2023-02-17 17:03:07,150 DEBUG TRAIN Batch 2/3500 loss 27.936073 loss_att 41.717514 loss_ctc 47.960480 loss_rnnt 22.490906 hw_loss 0.035549 lr 0.00080732 rank 6
2023-02-17 17:03:07,152 DEBUG TRAIN Batch 2/3500 loss 42.332760 loss_att 50.781799 loss_ctc 61.470074 loss_rnnt 38.020836 hw_loss 0.132133 lr 0.00080672 rank 7
2023-02-17 17:04:21,378 DEBUG TRAIN Batch 2/3600 loss 22.466675 loss_att 30.782215 loss_ctc 37.606419 loss_rnnt 18.745842 hw_loss 0.073298 lr 0.00081024 rank 4
2023-02-17 17:04:21,380 DEBUG TRAIN Batch 2/3600 loss 40.558811 loss_att 50.136784 loss_ctc 59.731220 loss_rnnt 36.021744 hw_loss 0.122157 lr 0.00081132 rank 6
2023-02-17 17:04:21,384 DEBUG TRAIN Batch 2/3600 loss 40.932140 loss_att 55.993706 loss_ctc 65.723602 loss_rnnt 34.580551 hw_loss 0.063275 lr 0.00081248 rank 1
2023-02-17 17:04:21,387 DEBUG TRAIN Batch 2/3600 loss 23.496634 loss_att 30.637505 loss_ctc 38.852249 loss_rnnt 20.000755 hw_loss 0.038038 lr 0.00081212 rank 2
2023-02-17 17:04:21,387 DEBUG TRAIN Batch 2/3600 loss 14.662235 loss_att 20.692064 loss_ctc 23.146755 loss_rnnt 12.293702 hw_loss 0.058685 lr 0.00081188 rank 0
2023-02-17 17:04:21,390 DEBUG TRAIN Batch 2/3600 loss 38.908482 loss_att 50.005177 loss_ctc 50.695034 loss_rnnt 35.071026 hw_loss 0.087332 lr 0.00081044 rank 5
2023-02-17 17:04:21,391 DEBUG TRAIN Batch 2/3600 loss 38.151669 loss_att 49.658989 loss_ctc 58.640842 loss_rnnt 33.025551 hw_loss 0.173934 lr 0.00081072 rank 7
2023-02-17 17:04:21,441 DEBUG TRAIN Batch 2/3600 loss 39.648079 loss_att 52.162968 loss_ctc 53.177483 loss_rnnt 35.317509 hw_loss 0.044384 lr 0.00081080 rank 3
2023-02-17 17:05:34,188 DEBUG TRAIN Batch 2/3700 loss 31.671873 loss_att 37.623558 loss_ctc 50.027081 loss_rnnt 27.891052 hw_loss 0.268355 lr 0.00081424 rank 4
2023-02-17 17:05:34,193 DEBUG TRAIN Batch 2/3700 loss 21.910036 loss_att 27.876389 loss_ctc 33.848324 loss_rnnt 19.066196 hw_loss 0.110249 lr 0.00081588 rank 0
2023-02-17 17:05:34,194 DEBUG TRAIN Batch 2/3700 loss 36.970757 loss_att 44.259407 loss_ctc 52.029205 loss_rnnt 33.500641 hw_loss 0.008612 lr 0.00081444 rank 5
2023-02-17 17:05:34,195 DEBUG TRAIN Batch 2/3700 loss 43.370712 loss_att 54.316135 loss_ctc 65.699402 loss_rnnt 38.163551 hw_loss 0.076710 lr 0.00081648 rank 1
2023-02-17 17:05:34,196 DEBUG TRAIN Batch 2/3700 loss 26.224754 loss_att 33.499458 loss_ctc 42.626728 loss_rnnt 22.529297 hw_loss 0.100473 lr 0.00081472 rank 7
2023-02-17 17:05:34,197 DEBUG TRAIN Batch 2/3700 loss 37.385311 loss_att 48.518047 loss_ctc 54.134815 loss_rnnt 32.881908 hw_loss 0.081731 lr 0.00081612 rank 2
2023-02-17 17:05:34,198 DEBUG TRAIN Batch 2/3700 loss 73.654449 loss_att 81.639938 loss_ctc 101.930321 loss_rnnt 68.274162 hw_loss 0.024513 lr 0.00081480 rank 3
2023-02-17 17:05:34,200 DEBUG TRAIN Batch 2/3700 loss 32.148563 loss_att 46.703941 loss_ctc 45.208862 loss_rnnt 27.440269 hw_loss 0.104705 lr 0.00081532 rank 6
2023-02-17 17:06:47,125 DEBUG TRAIN Batch 2/3800 loss 32.953793 loss_att 39.367947 loss_ctc 46.508373 loss_rnnt 29.815176 hw_loss 0.090949 lr 0.00081880 rank 3
2023-02-17 17:06:47,128 DEBUG TRAIN Batch 2/3800 loss 33.383076 loss_att 47.205132 loss_ctc 48.689812 loss_rnnt 28.577301 hw_loss 0.000875 lr 0.00081824 rank 4
2023-02-17 17:06:47,148 DEBUG TRAIN Batch 2/3800 loss 25.377409 loss_att 31.042652 loss_ctc 36.634075 loss_rnnt 22.679531 hw_loss 0.119891 lr 0.00082048 rank 1
2023-02-17 17:06:47,148 DEBUG TRAIN Batch 2/3800 loss 16.196959 loss_att 19.911442 loss_ctc 25.169119 loss_rnnt 14.199576 hw_loss 0.109125 lr 0.00081844 rank 5
2023-02-17 17:06:47,149 DEBUG TRAIN Batch 2/3800 loss 26.158981 loss_att 28.415180 loss_ctc 35.170326 loss_rnnt 24.362171 hw_loss 0.270102 lr 0.00081932 rank 6
2023-02-17 17:06:47,149 DEBUG TRAIN Batch 2/3800 loss 38.352390 loss_att 51.582901 loss_ctc 43.779594 loss_rnnt 34.951660 hw_loss 0.058122 lr 0.00082012 rank 2
2023-02-17 17:06:47,151 DEBUG TRAIN Batch 2/3800 loss 28.481541 loss_att 31.604733 loss_ctc 40.940208 loss_rnnt 26.133041 hw_loss 0.117570 lr 0.00081988 rank 0
2023-02-17 17:06:47,151 DEBUG TRAIN Batch 2/3800 loss 25.731518 loss_att 28.110466 loss_ctc 34.185760 loss_rnnt 24.064495 hw_loss 0.120004 lr 0.00081872 rank 7
2023-02-17 17:08:01,911 DEBUG TRAIN Batch 2/3900 loss 43.249748 loss_att 66.728989 loss_ctc 68.981934 loss_rnnt 35.042671 hw_loss 0.150499 lr 0.00082272 rank 7
2023-02-17 17:08:01,924 DEBUG TRAIN Batch 2/3900 loss 46.260902 loss_att 73.343567 loss_ctc 67.347878 loss_rnnt 37.996292 hw_loss 0.068394 lr 0.00082388 rank 0
2023-02-17 17:08:01,925 DEBUG TRAIN Batch 2/3900 loss 36.572308 loss_att 58.898258 loss_ctc 52.736385 loss_rnnt 29.928833 hw_loss 0.043257 lr 0.00082332 rank 6
2023-02-17 17:08:01,928 DEBUG TRAIN Batch 2/3900 loss 46.199783 loss_att 66.653069 loss_ctc 65.969551 loss_rnnt 39.412945 hw_loss 0.112895 lr 0.00082244 rank 5
2023-02-17 17:08:01,928 DEBUG TRAIN Batch 2/3900 loss 96.626923 loss_att 114.026421 loss_ctc 120.497032 loss_rnnt 89.925011 hw_loss 0.073739 lr 0.00082224 rank 4
2023-02-17 17:08:01,928 DEBUG TRAIN Batch 2/3900 loss 38.880249 loss_att 52.058182 loss_ctc 50.302460 loss_rnnt 34.705086 hw_loss 0.031148 lr 0.00082448 rank 1
2023-02-17 17:08:01,930 DEBUG TRAIN Batch 2/3900 loss 37.197170 loss_att 49.974380 loss_ctc 52.688751 loss_rnnt 32.538441 hw_loss 0.070758 lr 0.00082280 rank 3
2023-02-17 17:08:01,958 DEBUG TRAIN Batch 2/3900 loss 34.585331 loss_att 44.243725 loss_ctc 56.399532 loss_rnnt 29.689596 hw_loss 0.104055 lr 0.00082412 rank 2
2023-02-17 17:09:14,935 DEBUG TRAIN Batch 2/4000 loss 44.257103 loss_att 59.053822 loss_ctc 71.198624 loss_rnnt 37.705353 hw_loss 0.000384 lr 0.00082732 rank 6
2023-02-17 17:09:14,936 DEBUG TRAIN Batch 2/4000 loss 19.420969 loss_att 32.591480 loss_ctc 27.182533 loss_rnnt 15.739996 hw_loss 0.022495 lr 0.00082848 rank 1
2023-02-17 17:09:14,938 DEBUG TRAIN Batch 2/4000 loss 41.994160 loss_att 48.904610 loss_ctc 53.732647 loss_rnnt 39.020699 hw_loss 0.049187 lr 0.00082624 rank 4
2023-02-17 17:09:14,942 DEBUG TRAIN Batch 2/4000 loss 34.486546 loss_att 42.405090 loss_ctc 50.661407 loss_rnnt 30.678053 hw_loss 0.127756 lr 0.00082812 rank 2
2023-02-17 17:09:14,943 DEBUG TRAIN Batch 2/4000 loss 23.019741 loss_att 32.753071 loss_ctc 36.559738 loss_rnnt 19.236691 hw_loss 0.058222 lr 0.00082672 rank 7
2023-02-17 17:09:14,945 DEBUG TRAIN Batch 2/4000 loss 37.372673 loss_att 56.031876 loss_ctc 50.241661 loss_rnnt 31.890057 hw_loss 0.065457 lr 0.00082788 rank 0
2023-02-17 17:09:14,947 DEBUG TRAIN Batch 2/4000 loss 32.440407 loss_att 52.935200 loss_ctc 53.168274 loss_rnnt 25.577530 hw_loss 0.000386 lr 0.00082644 rank 5
2023-02-17 17:09:14,992 DEBUG TRAIN Batch 2/4000 loss 25.359262 loss_att 40.562016 loss_ctc 40.376022 loss_rnnt 20.273611 hw_loss 0.080376 lr 0.00082680 rank 3
2023-02-17 17:10:27,682 DEBUG TRAIN Batch 2/4100 loss 45.117882 loss_att 65.406548 loss_ctc 67.076370 loss_rnnt 38.081005 hw_loss 0.096272 lr 0.00083024 rank 4
2023-02-17 17:10:27,683 DEBUG TRAIN Batch 2/4100 loss 24.954411 loss_att 37.963745 loss_ctc 39.343922 loss_rnnt 20.414637 hw_loss 0.036194 lr 0.00083212 rank 2
2023-02-17 17:10:27,685 DEBUG TRAIN Batch 2/4100 loss 34.992264 loss_att 43.489075 loss_ctc 51.241127 loss_rnnt 31.090307 hw_loss 0.067646 lr 0.00083132 rank 6
2023-02-17 17:10:27,685 DEBUG TRAIN Batch 2/4100 loss 28.086611 loss_att 40.372658 loss_ctc 44.564522 loss_rnnt 23.366547 hw_loss 0.123375 lr 0.00083080 rank 3
2023-02-17 17:10:27,691 DEBUG TRAIN Batch 2/4100 loss 51.356968 loss_att 71.017891 loss_ctc 79.292244 loss_rnnt 43.645596 hw_loss 0.102154 lr 0.00083044 rank 5
2023-02-17 17:10:27,690 DEBUG TRAIN Batch 2/4100 loss 52.520855 loss_att 69.325111 loss_ctc 82.631645 loss_rnnt 45.138031 hw_loss 0.013492 lr 0.00083072 rank 7
2023-02-17 17:10:27,691 DEBUG TRAIN Batch 2/4100 loss 51.202209 loss_att 60.214096 loss_ctc 63.772629 loss_rnnt 47.723587 hw_loss 0.000358 lr 0.00083188 rank 0
2023-02-17 17:10:27,736 DEBUG TRAIN Batch 2/4100 loss 19.235760 loss_att 32.758556 loss_ctc 33.742970 loss_rnnt 14.564596 hw_loss 0.060585 lr 0.00083248 rank 1
2023-02-17 17:11:40,955 DEBUG TRAIN Batch 2/4200 loss 36.075871 loss_att 50.966045 loss_ctc 55.562443 loss_rnnt 30.435619 hw_loss 0.120024 lr 0.00083648 rank 1
2023-02-17 17:11:40,967 DEBUG TRAIN Batch 2/4200 loss 32.400311 loss_att 52.101143 loss_ctc 56.285423 loss_rnnt 25.255417 hw_loss 0.037582 lr 0.00083472 rank 7
2023-02-17 17:11:40,970 DEBUG TRAIN Batch 2/4200 loss 27.252747 loss_att 39.119190 loss_ctc 39.301529 loss_rnnt 23.248226 hw_loss 0.046369 lr 0.00083532 rank 6
2023-02-17 17:11:40,974 DEBUG TRAIN Batch 2/4200 loss 34.443432 loss_att 48.502052 loss_ctc 54.366013 loss_rnnt 28.969767 hw_loss 0.010487 lr 0.00083588 rank 0
2023-02-17 17:11:40,973 DEBUG TRAIN Batch 2/4200 loss 41.541435 loss_att 52.414364 loss_ctc 66.883110 loss_rnnt 35.941856 hw_loss 0.086442 lr 0.00083424 rank 4
2023-02-17 17:11:40,979 DEBUG TRAIN Batch 2/4200 loss 32.693546 loss_att 36.725254 loss_ctc 49.730778 loss_rnnt 29.544346 hw_loss 0.133556 lr 0.00083612 rank 2
2023-02-17 17:11:40,980 DEBUG TRAIN Batch 2/4200 loss 49.806015 loss_att 63.104950 loss_ctc 73.152763 loss_rnnt 43.973572 hw_loss 0.112039 lr 0.00083444 rank 5
2023-02-17 17:11:40,982 DEBUG TRAIN Batch 2/4200 loss 55.542366 loss_att 72.336472 loss_ctc 84.908752 loss_rnnt 48.252262 hw_loss 0.029551 lr 0.00083480 rank 3
2023-02-17 17:12:55,069 DEBUG TRAIN Batch 2/4300 loss 43.709332 loss_att 53.454735 loss_ctc 59.378468 loss_rnnt 39.627518 hw_loss 0.081587 lr 0.00083824 rank 4
2023-02-17 17:12:55,071 DEBUG TRAIN Batch 2/4300 loss 35.473278 loss_att 44.525360 loss_ctc 52.267529 loss_rnnt 31.405376 hw_loss 0.034226 lr 0.00083880 rank 3
2023-02-17 17:12:55,073 DEBUG TRAIN Batch 2/4300 loss 49.108303 loss_att 62.555557 loss_ctc 67.773781 loss_rnnt 43.871136 hw_loss 0.110600 lr 0.00083932 rank 6
2023-02-17 17:12:55,073 DEBUG TRAIN Batch 2/4300 loss 42.918976 loss_att 50.330696 loss_ctc 63.205635 loss_rnnt 38.704536 hw_loss 0.051008 lr 0.00083844 rank 5
2023-02-17 17:12:55,075 DEBUG TRAIN Batch 2/4300 loss 49.631454 loss_att 62.507000 loss_ctc 65.386047 loss_rnnt 44.908493 hw_loss 0.088579 lr 0.00084048 rank 1
2023-02-17 17:12:55,076 DEBUG TRAIN Batch 2/4300 loss 22.484522 loss_att 29.673073 loss_ctc 31.137632 loss_rnnt 19.849842 hw_loss 0.081038 lr 0.00083988 rank 0
2023-02-17 17:12:55,083 DEBUG TRAIN Batch 2/4300 loss 32.919098 loss_att 41.404121 loss_ctc 48.720455 loss_rnnt 29.074978 hw_loss 0.075502 lr 0.00083872 rank 7
2023-02-17 17:12:55,119 DEBUG TRAIN Batch 2/4300 loss 44.270947 loss_att 50.257099 loss_ctc 57.741596 loss_rnnt 41.197628 hw_loss 0.150003 lr 0.00084012 rank 2
2023-02-17 17:14:07,698 DEBUG TRAIN Batch 2/4400 loss 39.351337 loss_att 44.754723 loss_ctc 53.694824 loss_rnnt 36.241806 hw_loss 0.218233 lr 0.00084332 rank 6
2023-02-17 17:14:07,699 DEBUG TRAIN Batch 2/4400 loss 33.735592 loss_att 38.307213 loss_ctc 42.746838 loss_rnnt 31.576855 hw_loss 0.080460 lr 0.00084224 rank 4
2023-02-17 17:14:07,706 DEBUG TRAIN Batch 2/4400 loss 22.935621 loss_att 24.690750 loss_ctc 30.971008 loss_rnnt 21.420456 hw_loss 0.173910 lr 0.00084388 rank 0
2023-02-17 17:14:07,706 DEBUG TRAIN Batch 2/4400 loss 26.197092 loss_att 32.156391 loss_ctc 37.886589 loss_rnnt 23.417562 hw_loss 0.054507 lr 0.00084272 rank 7
2023-02-17 17:14:07,706 DEBUG TRAIN Batch 2/4400 loss 39.619160 loss_att 55.041859 loss_ctc 57.965675 loss_rnnt 34.041348 hw_loss 0.088262 lr 0.00084448 rank 1
2023-02-17 17:14:07,707 DEBUG TRAIN Batch 2/4400 loss 21.969345 loss_att 30.382290 loss_ctc 37.232086 loss_rnnt 18.231165 hw_loss 0.038549 lr 0.00084244 rank 5
2023-02-17 17:14:07,708 DEBUG TRAIN Batch 2/4400 loss 29.746969 loss_att 35.919735 loss_ctc 39.629189 loss_rnnt 27.143234 hw_loss 0.096659 lr 0.00084412 rank 2
2023-02-17 17:14:07,708 DEBUG TRAIN Batch 2/4400 loss 25.888645 loss_att 31.255230 loss_ctc 37.211800 loss_rnnt 23.280949 hw_loss 0.046169 lr 0.00084280 rank 3
2023-02-17 17:15:20,228 DEBUG TRAIN Batch 2/4500 loss 58.122654 loss_att 76.853638 loss_ctc 85.588669 loss_rnnt 50.678947 hw_loss 0.066330 lr 0.00084788 rank 0
2023-02-17 17:15:20,229 DEBUG TRAIN Batch 2/4500 loss 33.525661 loss_att 43.483486 loss_ctc 48.870476 loss_rnnt 29.448622 hw_loss 0.074064 lr 0.00084624 rank 4
2023-02-17 17:15:20,230 DEBUG TRAIN Batch 2/4500 loss 23.714796 loss_att 29.807571 loss_ctc 36.172279 loss_rnnt 20.822445 hw_loss 0.023996 lr 0.00084848 rank 1
2023-02-17 17:15:20,231 DEBUG TRAIN Batch 2/4500 loss 39.871773 loss_att 52.365387 loss_ctc 65.646698 loss_rnnt 33.843040 hw_loss 0.175040 lr 0.00084732 rank 6
2023-02-17 17:15:20,235 DEBUG TRAIN Batch 2/4500 loss 45.732277 loss_att 56.240829 loss_ctc 65.403152 loss_rnnt 40.972546 hw_loss 0.066070 lr 0.00084812 rank 2
2023-02-17 17:15:20,236 DEBUG TRAIN Batch 2/4500 loss 25.892809 loss_att 39.633556 loss_ctc 39.222244 loss_rnnt 21.327005 hw_loss 0.075742 lr 0.00084680 rank 3
2023-02-17 17:15:20,238 DEBUG TRAIN Batch 2/4500 loss 36.125877 loss_att 52.618896 loss_ctc 59.700073 loss_rnnt 29.620155 hw_loss 0.119801 lr 0.00084644 rank 5
2023-02-17 17:15:20,247 DEBUG TRAIN Batch 2/4500 loss 43.218048 loss_att 53.560425 loss_ctc 63.475689 loss_rnnt 38.397602 hw_loss 0.095531 lr 0.00084672 rank 7
2023-02-17 17:16:34,889 DEBUG TRAIN Batch 2/4600 loss 34.126373 loss_att 47.943848 loss_ctc 50.594097 loss_rnnt 29.145500 hw_loss 0.040653 lr 0.00085132 rank 6
2023-02-17 17:16:34,889 DEBUG TRAIN Batch 2/4600 loss 59.349640 loss_att 72.987831 loss_ctc 89.693260 loss_rnnt 52.536140 hw_loss 0.075074 lr 0.00085044 rank 5
2023-02-17 17:16:34,891 DEBUG TRAIN Batch 2/4600 loss 35.517372 loss_att 47.915043 loss_ctc 53.353951 loss_rnnt 30.659554 hw_loss 0.000138 lr 0.00085248 rank 1
2023-02-17 17:16:34,894 DEBUG TRAIN Batch 2/4600 loss 54.386662 loss_att 71.999466 loss_ctc 66.612274 loss_rnnt 49.211433 hw_loss 0.042346 lr 0.00085188 rank 0
2023-02-17 17:16:34,893 DEBUG TRAIN Batch 2/4600 loss 28.136505 loss_att 43.196739 loss_ctc 42.786385 loss_rnnt 23.134092 hw_loss 0.069466 lr 0.00085080 rank 3
2023-02-17 17:16:34,893 DEBUG TRAIN Batch 2/4600 loss 43.255928 loss_att 49.351223 loss_ctc 56.517860 loss_rnnt 40.268509 hw_loss 0.000188 lr 0.00085024 rank 4
2023-02-17 17:16:34,895 DEBUG TRAIN Batch 2/4600 loss 32.960926 loss_att 43.168579 loss_ctc 49.859783 loss_rnnt 28.630642 hw_loss 0.066699 lr 0.00085072 rank 7
2023-02-17 17:16:34,907 DEBUG TRAIN Batch 2/4600 loss 31.474274 loss_att 51.295349 loss_ctc 43.702007 loss_rnnt 25.818768 hw_loss 0.114239 lr 0.00085212 rank 2
2023-02-17 17:17:47,964 DEBUG TRAIN Batch 2/4700 loss 29.862709 loss_att 45.017822 loss_ctc 44.207161 loss_rnnt 24.897293 hw_loss 0.040876 lr 0.00085588 rank 0
2023-02-17 17:17:47,977 DEBUG TRAIN Batch 2/4700 loss 36.566315 loss_att 47.136101 loss_ctc 48.058472 loss_rnnt 32.919628 hw_loss 0.000832 lr 0.00085424 rank 4
2023-02-17 17:17:47,980 DEBUG TRAIN Batch 2/4700 loss 32.218327 loss_att 42.086128 loss_ctc 46.304916 loss_rnnt 28.349112 hw_loss 0.032704 lr 0.00085472 rank 7
2023-02-17 17:17:47,980 DEBUG TRAIN Batch 2/4700 loss 48.026623 loss_att 61.340057 loss_ctc 67.012146 loss_rnnt 42.832184 hw_loss 0.000659 lr 0.00085648 rank 1
2023-02-17 17:17:47,982 DEBUG TRAIN Batch 2/4700 loss 26.496412 loss_att 35.869900 loss_ctc 40.188347 loss_rnnt 22.743443 hw_loss 0.098770 lr 0.00085612 rank 2
2023-02-17 17:17:47,982 DEBUG TRAIN Batch 2/4700 loss 13.676374 loss_att 20.401447 loss_ctc 22.860884 loss_rnnt 11.106441 hw_loss 0.000595 lr 0.00085532 rank 6
2023-02-17 17:17:48,009 DEBUG TRAIN Batch 2/4700 loss 50.223915 loss_att 61.159088 loss_ctc 76.919937 loss_rnnt 44.429008 hw_loss 0.090763 lr 0.00085480 rank 3
2023-02-17 17:17:48,012 DEBUG TRAIN Batch 2/4700 loss 34.938023 loss_att 46.884209 loss_ctc 49.200478 loss_rnnt 30.603760 hw_loss 0.081308 lr 0.00085444 rank 5
2023-02-17 17:18:59,802 DEBUG TRAIN Batch 2/4800 loss 25.095259 loss_att 37.492199 loss_ctc 43.412949 loss_rnnt 20.143147 hw_loss 0.056941 lr 0.00085824 rank 4
2023-02-17 17:18:59,803 DEBUG TRAIN Batch 2/4800 loss 28.733770 loss_att 37.659355 loss_ctc 42.579227 loss_rnnt 25.042089 hw_loss 0.113446 lr 0.00085988 rank 0
2023-02-17 17:18:59,805 DEBUG TRAIN Batch 2/4800 loss 34.320091 loss_att 46.791775 loss_ctc 50.624908 loss_rnnt 29.598728 hw_loss 0.099470 lr 0.00086012 rank 2
2023-02-17 17:18:59,805 DEBUG TRAIN Batch 2/4800 loss 26.309471 loss_att 36.627861 loss_ctc 41.897984 loss_rnnt 22.128635 hw_loss 0.072539 lr 0.00085932 rank 6
2023-02-17 17:18:59,807 DEBUG TRAIN Batch 2/4800 loss 43.933739 loss_att 60.067440 loss_ctc 67.161674 loss_rnnt 37.591118 hw_loss 0.035286 lr 0.00085844 rank 5
2023-02-17 17:18:59,807 DEBUG TRAIN Batch 2/4800 loss 31.767509 loss_att 48.524773 loss_ctc 55.946838 loss_rnnt 25.137955 hw_loss 0.101603 lr 0.00086048 rank 1
2023-02-17 17:18:59,810 DEBUG TRAIN Batch 2/4800 loss 33.079777 loss_att 45.101818 loss_ctc 49.026104 loss_rnnt 28.495796 hw_loss 0.100115 lr 0.00085872 rank 7
2023-02-17 17:18:59,812 DEBUG TRAIN Batch 2/4800 loss 43.794952 loss_att 56.394409 loss_ctc 63.134266 loss_rnnt 38.633595 hw_loss 0.117915 lr 0.00085880 rank 3
2023-02-17 17:20:12,159 DEBUG TRAIN Batch 2/4900 loss 48.424351 loss_att 59.568386 loss_ctc 80.002541 loss_rnnt 41.918015 hw_loss 0.125816 lr 0.00086448 rank 1
2023-02-17 17:20:12,171 DEBUG TRAIN Batch 2/4900 loss 39.633347 loss_att 49.980392 loss_ctc 61.285721 loss_rnnt 34.650990 hw_loss 0.048687 lr 0.00086224 rank 4
2023-02-17 17:20:12,171 DEBUG TRAIN Batch 2/4900 loss 27.183573 loss_att 38.865463 loss_ctc 45.156921 loss_rnnt 22.421432 hw_loss 0.054965 lr 0.00086244 rank 5
2023-02-17 17:20:12,173 DEBUG TRAIN Batch 2/4900 loss 27.079327 loss_att 44.575851 loss_ctc 45.714165 loss_rnnt 21.094967 hw_loss 0.000766 lr 0.00086332 rank 6
2023-02-17 17:20:12,174 DEBUG TRAIN Batch 2/4900 loss 44.260998 loss_att 53.035919 loss_ctc 56.501884 loss_rnnt 40.845413 hw_loss 0.053410 lr 0.00086280 rank 3
2023-02-17 17:20:12,178 DEBUG TRAIN Batch 2/4900 loss 42.114498 loss_att 54.029755 loss_ctc 63.332985 loss_rnnt 36.849998 hw_loss 0.098085 lr 0.00086412 rank 2
2023-02-17 17:20:12,178 DEBUG TRAIN Batch 2/4900 loss 45.122540 loss_att 53.263454 loss_ctc 64.259445 loss_rnnt 40.894569 hw_loss 0.090379 lr 0.00086388 rank 0
2023-02-17 17:20:12,184 DEBUG TRAIN Batch 2/4900 loss 22.212025 loss_att 31.720915 loss_ctc 36.282833 loss_rnnt 18.401390 hw_loss 0.061409 lr 0.00086272 rank 7
2023-02-17 17:21:27,750 DEBUG TRAIN Batch 2/5000 loss 34.545094 loss_att 39.805614 loss_ctc 49.431107 loss_rnnt 31.448683 hw_loss 0.111566 lr 0.00086732 rank 6
2023-02-17 17:21:27,750 DEBUG TRAIN Batch 2/5000 loss 34.228222 loss_att 40.490391 loss_ctc 51.294434 loss_rnnt 30.675367 hw_loss 0.046744 lr 0.00086624 rank 4
2023-02-17 17:21:27,752 DEBUG TRAIN Batch 2/5000 loss 29.650724 loss_att 34.388546 loss_ctc 40.745625 loss_rnnt 27.177662 hw_loss 0.086579 lr 0.00086788 rank 0
2023-02-17 17:21:27,756 DEBUG TRAIN Batch 2/5000 loss 28.974028 loss_att 37.571129 loss_ctc 40.867249 loss_rnnt 25.595888 hw_loss 0.136789 lr 0.00086812 rank 2
2023-02-17 17:21:27,758 DEBUG TRAIN Batch 2/5000 loss 27.839939 loss_att 29.731503 loss_ctc 38.758564 loss_rnnt 25.942581 hw_loss 0.118552 lr 0.00086680 rank 3
2023-02-17 17:21:27,759 DEBUG TRAIN Batch 2/5000 loss 36.216789 loss_att 38.836502 loss_ctc 53.163921 loss_rnnt 33.432537 hw_loss 0.001293 lr 0.00086672 rank 7
2023-02-17 17:21:27,763 DEBUG TRAIN Batch 2/5000 loss 42.160282 loss_att 53.264839 loss_ctc 64.180733 loss_rnnt 36.964420 hw_loss 0.072921 lr 0.00086848 rank 1
2023-02-17 17:21:27,803 DEBUG TRAIN Batch 2/5000 loss 20.617168 loss_att 24.529436 loss_ctc 28.009371 loss_rnnt 18.753338 hw_loss 0.179532 lr 0.00086644 rank 5
2023-02-17 17:22:40,160 DEBUG TRAIN Batch 2/5100 loss 35.848671 loss_att 40.240925 loss_ctc 52.402328 loss_rnnt 32.762070 hw_loss 0.001871 lr 0.00087248 rank 1
2023-02-17 17:22:40,160 DEBUG TRAIN Batch 2/5100 loss 36.506233 loss_att 48.778870 loss_ctc 49.333172 loss_rnnt 32.341290 hw_loss 0.000293 lr 0.00087132 rank 6
2023-02-17 17:22:40,164 DEBUG TRAIN Batch 2/5100 loss 24.142908 loss_att 30.548515 loss_ctc 36.466309 loss_rnnt 21.158848 hw_loss 0.112162 lr 0.00087024 rank 4
2023-02-17 17:22:40,165 DEBUG TRAIN Batch 2/5100 loss 29.952377 loss_att 41.714752 loss_ctc 45.903633 loss_rnnt 25.448280 hw_loss 0.046475 lr 0.00087188 rank 0
2023-02-17 17:22:40,167 DEBUG TRAIN Batch 2/5100 loss 44.505470 loss_att 69.883125 loss_ctc 67.870941 loss_rnnt 36.313747 hw_loss 0.001487 lr 0.00087212 rank 2
2023-02-17 17:22:40,167 DEBUG TRAIN Batch 2/5100 loss 31.499245 loss_att 45.839603 loss_ctc 48.632172 loss_rnnt 26.346647 hw_loss 0.000254 lr 0.00087072 rank 7
2023-02-17 17:22:40,170 DEBUG TRAIN Batch 2/5100 loss 56.731567 loss_att 64.506004 loss_ctc 85.219696 loss_rnnt 51.363190 hw_loss 0.028269 lr 0.00087044 rank 5
2023-02-17 17:22:40,217 DEBUG TRAIN Batch 2/5100 loss 62.934254 loss_att 78.562332 loss_ctc 95.035370 loss_rnnt 55.516197 hw_loss 0.023053 lr 0.00087080 rank 3
2023-02-17 17:23:52,770 DEBUG TRAIN Batch 2/5200 loss 21.636284 loss_att 34.180893 loss_ctc 30.502876 loss_rnnt 17.941219 hw_loss 0.007368 lr 0.00087424 rank 4
2023-02-17 17:23:52,776 DEBUG TRAIN Batch 2/5200 loss 40.888756 loss_att 51.063210 loss_ctc 49.630844 loss_rnnt 37.653072 hw_loss 0.065969 lr 0.00087532 rank 6
2023-02-17 17:23:52,777 DEBUG TRAIN Batch 2/5200 loss 32.675468 loss_att 43.463875 loss_ctc 45.680626 loss_rnnt 28.782995 hw_loss 0.001452 lr 0.00087588 rank 0
2023-02-17 17:23:52,779 DEBUG TRAIN Batch 2/5200 loss 46.841660 loss_att 63.731712 loss_ctc 71.124084 loss_rnnt 40.202946 hw_loss 0.043211 lr 0.00087648 rank 1
2023-02-17 17:23:52,779 DEBUG TRAIN Batch 2/5200 loss 53.574951 loss_att 68.517548 loss_ctc 79.282562 loss_rnnt 47.129787 hw_loss 0.054305 lr 0.00087480 rank 3
2023-02-17 17:23:52,781 DEBUG TRAIN Batch 2/5200 loss 35.999969 loss_att 46.541912 loss_ctc 49.485455 loss_rnnt 31.992895 hw_loss 0.188664 lr 0.00087472 rank 7
2023-02-17 17:23:52,782 DEBUG TRAIN Batch 2/5200 loss 16.523375 loss_att 29.968416 loss_ctc 27.447943 loss_rnnt 12.359054 hw_loss 0.035072 lr 0.00087612 rank 2
2023-02-17 17:23:52,786 DEBUG TRAIN Batch 2/5200 loss 25.825211 loss_att 33.734886 loss_ctc 40.792091 loss_rnnt 22.217415 hw_loss 0.056761 lr 0.00087444 rank 5
2023-02-17 17:25:07,261 DEBUG TRAIN Batch 2/5300 loss 22.116219 loss_att 31.270912 loss_ctc 36.234066 loss_rnnt 18.355541 hw_loss 0.088798 lr 0.00088012 rank 2
2023-02-17 17:25:07,268 DEBUG TRAIN Batch 2/5300 loss 45.150078 loss_att 55.708366 loss_ctc 72.696030 loss_rnnt 39.349091 hw_loss 0.031002 lr 0.00087872 rank 7
2023-02-17 17:25:07,270 DEBUG TRAIN Batch 2/5300 loss 30.186556 loss_att 42.751991 loss_ctc 47.523949 loss_rnnt 25.358755 hw_loss 0.005734 lr 0.00087988 rank 0
2023-02-17 17:25:07,270 DEBUG TRAIN Batch 2/5300 loss 66.769936 loss_att 67.822212 loss_ctc 81.285522 loss_rnnt 64.599434 hw_loss 0.046178 lr 0.00087824 rank 4
2023-02-17 17:25:07,271 DEBUG TRAIN Batch 2/5300 loss 54.746483 loss_att 68.521133 loss_ctc 75.231003 loss_rnnt 49.237148 hw_loss 0.043377 lr 0.00087880 rank 3
2023-02-17 17:25:07,274 DEBUG TRAIN Batch 2/5300 loss 37.591625 loss_att 46.575661 loss_ctc 62.403908 loss_rnnt 32.429813 hw_loss 0.106310 lr 0.00087844 rank 5
2023-02-17 17:25:07,276 DEBUG TRAIN Batch 2/5300 loss 49.148514 loss_att 68.401833 loss_ctc 67.117302 loss_rnnt 42.829220 hw_loss 0.136493 lr 0.00087932 rank 6
2023-02-17 17:25:07,297 DEBUG TRAIN Batch 2/5300 loss 67.122849 loss_att 74.501007 loss_ctc 85.837273 loss_rnnt 63.111084 hw_loss 0.076641 lr 0.00088048 rank 1
2023-02-17 17:26:20,351 DEBUG TRAIN Batch 2/5400 loss 59.874332 loss_att 74.976524 loss_ctc 87.470703 loss_rnnt 53.161282 hw_loss 0.024560 lr 0.00088280 rank 3
2023-02-17 17:26:20,352 DEBUG TRAIN Batch 2/5400 loss 53.806202 loss_att 69.131432 loss_ctc 85.818604 loss_rnnt 46.461422 hw_loss 0.021412 lr 0.00088272 rank 7
2023-02-17 17:26:20,364 DEBUG TRAIN Batch 2/5400 loss 27.222656 loss_att 36.785107 loss_ctc 37.503304 loss_rnnt 23.866205 hw_loss 0.137262 lr 0.00088244 rank 5
2023-02-17 17:26:20,365 DEBUG TRAIN Batch 2/5400 loss 32.461964 loss_att 43.489677 loss_ctc 48.218620 loss_rnnt 28.116558 hw_loss 0.073081 lr 0.00088224 rank 4
2023-02-17 17:26:20,368 DEBUG TRAIN Batch 2/5400 loss 54.589485 loss_att 65.682961 loss_ctc 73.109207 loss_rnnt 49.881004 hw_loss 0.038428 lr 0.00088332 rank 6
2023-02-17 17:26:20,371 DEBUG TRAIN Batch 2/5400 loss 37.934299 loss_att 50.708035 loss_ctc 60.784302 loss_rnnt 32.268990 hw_loss 0.119798 lr 0.00088388 rank 0
2023-02-17 17:26:20,373 DEBUG TRAIN Batch 2/5400 loss 43.753216 loss_att 57.836323 loss_ctc 65.323509 loss_rnnt 38.023857 hw_loss 0.068811 lr 0.00088448 rank 1
2023-02-17 17:26:20,413 DEBUG TRAIN Batch 2/5400 loss 44.432941 loss_att 59.382652 loss_ctc 63.806572 loss_rnnt 38.821991 hw_loss 0.070980 lr 0.00088412 rank 2
2023-02-17 17:27:33,115 DEBUG TRAIN Batch 2/5500 loss 48.248783 loss_att 58.237316 loss_ctc 77.079231 loss_rnnt 42.339699 hw_loss 0.126223 lr 0.00088624 rank 4
2023-02-17 17:27:33,116 DEBUG TRAIN Batch 2/5500 loss 39.319317 loss_att 51.743248 loss_ctc 61.478539 loss_rnnt 33.798904 hw_loss 0.151993 lr 0.00088788 rank 0
2023-02-17 17:27:33,117 DEBUG TRAIN Batch 2/5500 loss 33.195450 loss_att 43.481037 loss_ctc 47.033123 loss_rnnt 29.292688 hw_loss 0.001163 lr 0.00088732 rank 6
2023-02-17 17:27:33,119 DEBUG TRAIN Batch 2/5500 loss 43.540333 loss_att 63.362526 loss_ctc 69.337181 loss_rnnt 36.063606 hw_loss 0.136329 lr 0.00088644 rank 5
2023-02-17 17:27:33,120 DEBUG TRAIN Batch 2/5500 loss 45.041603 loss_att 49.097046 loss_ctc 66.862656 loss_rnnt 41.320488 hw_loss 0.001031 lr 0.00088812 rank 2
2023-02-17 17:27:33,120 DEBUG TRAIN Batch 2/5500 loss 28.672647 loss_att 40.981400 loss_ctc 44.620171 loss_rnnt 24.015001 hw_loss 0.130418 lr 0.00088672 rank 7
2023-02-17 17:27:33,120 DEBUG TRAIN Batch 2/5500 loss 24.199459 loss_att 33.076908 loss_ctc 41.065762 loss_rnnt 20.103453 hw_loss 0.134393 lr 0.00088680 rank 3
2023-02-17 17:27:33,124 DEBUG TRAIN Batch 2/5500 loss 29.815065 loss_att 37.587627 loss_ctc 46.831741 loss_rnnt 25.930286 hw_loss 0.115076 lr 0.00088848 rank 1
2023-02-17 17:28:46,445 DEBUG TRAIN Batch 2/5600 loss 43.172020 loss_att 48.794895 loss_ctc 58.724537 loss_rnnt 39.944798 hw_loss 0.054334 lr 0.00089072 rank 7
2023-02-17 17:28:46,451 DEBUG TRAIN Batch 2/5600 loss 24.246904 loss_att 32.440079 loss_ctc 39.270897 loss_rnnt 20.586760 hw_loss 0.034334 lr 0.00089132 rank 6
2023-02-17 17:28:46,452 DEBUG TRAIN Batch 2/5600 loss 31.739349 loss_att 45.687534 loss_ctc 49.793808 loss_rnnt 26.541977 hw_loss 0.000889 lr 0.00089248 rank 1
2023-02-17 17:28:46,453 DEBUG TRAIN Batch 2/5600 loss 32.047005 loss_att 41.536354 loss_ctc 48.485046 loss_rnnt 27.950520 hw_loss 0.012893 lr 0.00089188 rank 0
2023-02-17 17:28:46,453 DEBUG TRAIN Batch 2/5600 loss 45.941875 loss_att 57.403755 loss_ctc 68.106163 loss_rnnt 40.648689 hw_loss 0.085440 lr 0.00089212 rank 2
2023-02-17 17:28:46,455 DEBUG TRAIN Batch 2/5600 loss 32.808674 loss_att 42.839798 loss_ctc 48.895382 loss_rnnt 28.656975 hw_loss 0.001086 lr 0.00089024 rank 4
2023-02-17 17:28:46,456 DEBUG TRAIN Batch 2/5600 loss 30.264147 loss_att 38.342682 loss_ctc 49.219566 loss_rnnt 26.058371 hw_loss 0.117527 lr 0.00089080 rank 3
2023-02-17 17:28:46,461 DEBUG TRAIN Batch 2/5600 loss 19.386663 loss_att 24.335079 loss_ctc 27.466541 loss_rnnt 17.247488 hw_loss 0.135326 lr 0.00089044 rank 5
2023-02-17 17:30:02,231 DEBUG TRAIN Batch 2/5700 loss 27.448584 loss_att 30.315311 loss_ctc 36.894264 loss_rnnt 25.539618 hw_loss 0.142871 lr 0.00089424 rank 4
2023-02-17 17:30:02,236 DEBUG TRAIN Batch 2/5700 loss 12.412925 loss_att 14.411045 loss_ctc 16.917978 loss_rnnt 11.336190 hw_loss 0.143315 lr 0.00089472 rank 7
2023-02-17 17:30:02,239 DEBUG TRAIN Batch 2/5700 loss 35.433205 loss_att 42.391411 loss_ctc 46.158688 loss_rnnt 32.573219 hw_loss 0.071773 lr 0.00089648 rank 1
2023-02-17 17:30:02,241 DEBUG TRAIN Batch 2/5700 loss 27.519739 loss_att 27.938019 loss_ctc 34.299255 loss_rnnt 26.485136 hw_loss 0.088149 lr 0.00089588 rank 0
2023-02-17 17:30:02,241 DEBUG TRAIN Batch 2/5700 loss 28.298124 loss_att 37.915718 loss_ctc 41.868572 loss_rnnt 24.533752 hw_loss 0.058992 lr 0.00089532 rank 6
2023-02-17 17:30:02,241 DEBUG TRAIN Batch 2/5700 loss 23.313354 loss_att 24.036669 loss_ctc 32.329765 loss_rnnt 21.863989 hw_loss 0.192211 lr 0.00089612 rank 2
2023-02-17 17:30:02,242 DEBUG TRAIN Batch 2/5700 loss 35.821903 loss_att 53.678627 loss_ctc 52.950043 loss_rnnt 29.905005 hw_loss 0.115879 lr 0.00089444 rank 5
2023-02-17 17:30:02,243 DEBUG TRAIN Batch 2/5700 loss 37.299194 loss_att 41.744633 loss_ctc 53.539410 loss_rnnt 34.220589 hw_loss 0.045286 lr 0.00089480 rank 3
2023-02-17 17:31:14,341 DEBUG TRAIN Batch 2/5800 loss 47.574890 loss_att 57.498077 loss_ctc 66.927490 loss_rnnt 42.966644 hw_loss 0.081123 lr 0.00089988 rank 0
2023-02-17 17:31:14,344 DEBUG TRAIN Batch 2/5800 loss 25.522282 loss_att 36.621376 loss_ctc 41.628231 loss_rnnt 21.103470 hw_loss 0.096626 lr 0.00090012 rank 2
2023-02-17 17:31:14,346 DEBUG TRAIN Batch 2/5800 loss 38.171528 loss_att 40.855827 loss_ctc 50.112228 loss_rnnt 35.996735 hw_loss 0.085945 lr 0.00090048 rank 1
2023-02-17 17:31:14,349 DEBUG TRAIN Batch 2/5800 loss 37.168098 loss_att 44.866199 loss_ctc 51.473480 loss_rnnt 33.720901 hw_loss 0.000359 lr 0.00089844 rank 5
2023-02-17 17:31:14,350 DEBUG TRAIN Batch 2/5800 loss 45.249737 loss_att 62.608051 loss_ctc 70.103516 loss_rnnt 38.464058 hw_loss 0.000331 lr 0.00089880 rank 3
2023-02-17 17:31:14,350 DEBUG TRAIN Batch 2/5800 loss 45.902706 loss_att 58.516270 loss_ctc 66.956108 loss_rnnt 40.572548 hw_loss 0.000596 lr 0.00089824 rank 4
2023-02-17 17:31:14,350 DEBUG TRAIN Batch 2/5800 loss 36.682289 loss_att 50.257698 loss_ctc 45.856945 loss_rnnt 32.723347 hw_loss 0.038565 lr 0.00089932 rank 6
2023-02-17 17:31:14,398 DEBUG TRAIN Batch 2/5800 loss 37.783478 loss_att 50.103958 loss_ctc 53.318966 loss_rnnt 33.223091 hw_loss 0.046677 lr 0.00089872 rank 7
2023-02-17 17:32:27,664 DEBUG TRAIN Batch 2/5900 loss 45.167416 loss_att 56.111931 loss_ctc 69.075790 loss_rnnt 39.790569 hw_loss 0.000303 lr 0.00090448 rank 1
2023-02-17 17:32:27,672 DEBUG TRAIN Batch 2/5900 loss 27.363260 loss_att 37.544834 loss_ctc 41.150391 loss_rnnt 23.488537 hw_loss 0.000233 lr 0.00090332 rank 6
2023-02-17 17:32:27,675 DEBUG TRAIN Batch 2/5900 loss 28.232061 loss_att 40.699612 loss_ctc 47.560711 loss_rnnt 23.089092 hw_loss 0.135576 lr 0.00090224 rank 4
2023-02-17 17:32:27,675 DEBUG TRAIN Batch 2/5900 loss 30.965767 loss_att 43.856190 loss_ctc 57.001511 loss_rnnt 24.884930 hw_loss 0.058729 lr 0.00090244 rank 5
2023-02-17 17:32:27,676 DEBUG TRAIN Batch 2/5900 loss 46.946846 loss_att 52.491524 loss_ctc 62.445030 loss_rnnt 43.740448 hw_loss 0.058197 lr 0.00090388 rank 0
2023-02-17 17:32:27,680 DEBUG TRAIN Batch 2/5900 loss 32.197662 loss_att 43.423508 loss_ctc 48.629478 loss_rnnt 27.708792 hw_loss 0.098993 lr 0.00090272 rank 7
2023-02-17 17:32:27,682 DEBUG TRAIN Batch 2/5900 loss 22.270493 loss_att 33.902111 loss_ctc 32.492123 loss_rnnt 18.554287 hw_loss 0.050620 lr 0.00090412 rank 2
2023-02-17 17:32:27,728 DEBUG TRAIN Batch 2/5900 loss 32.083393 loss_att 37.542656 loss_ctc 43.366798 loss_rnnt 29.471176 hw_loss 0.029832 lr 0.00090280 rank 3
2023-02-17 17:33:41,480 DEBUG TRAIN Batch 2/6000 loss 36.816128 loss_att 42.794075 loss_ctc 55.783005 loss_rnnt 33.072041 hw_loss 0.036704 lr 0.00090732 rank 6
2023-02-17 17:33:41,485 DEBUG TRAIN Batch 2/6000 loss 37.358276 loss_att 47.979843 loss_ctc 58.777824 loss_rnnt 32.340492 hw_loss 0.070376 lr 0.00090624 rank 4
2023-02-17 17:33:41,490 DEBUG TRAIN Batch 2/6000 loss 48.724731 loss_att 55.418030 loss_ctc 56.821491 loss_rnnt 46.258026 hw_loss 0.090892 lr 0.00090644 rank 5
2023-02-17 17:33:41,490 DEBUG TRAIN Batch 2/6000 loss 24.136530 loss_att 40.025879 loss_ctc 38.210571 loss_rnnt 19.059484 hw_loss 0.042446 lr 0.00090848 rank 1
2023-02-17 17:33:41,490 DEBUG TRAIN Batch 2/6000 loss 30.401999 loss_att 41.398750 loss_ctc 42.740593 loss_rnnt 26.521023 hw_loss 0.068401 lr 0.00090788 rank 0
2023-02-17 17:33:41,491 DEBUG TRAIN Batch 2/6000 loss 35.289593 loss_att 52.042046 loss_ctc 58.065407 loss_rnnt 28.890171 hw_loss 0.022787 lr 0.00090672 rank 7
2023-02-17 17:33:41,495 DEBUG TRAIN Batch 2/6000 loss 58.437050 loss_att 76.074982 loss_ctc 88.592682 loss_rnnt 50.880409 hw_loss 0.015566 lr 0.00090812 rank 2
2023-02-17 17:33:41,496 DEBUG TRAIN Batch 2/6000 loss 45.388729 loss_att 61.198872 loss_ctc 71.041740 loss_rnnt 38.760674 hw_loss 0.085540 lr 0.00090680 rank 3
2023-02-17 17:34:55,494 DEBUG TRAIN Batch 2/6100 loss 34.903530 loss_att 41.290348 loss_ctc 43.106987 loss_rnnt 32.475189 hw_loss 0.107209 lr 0.00091024 rank 4
2023-02-17 17:34:55,496 DEBUG TRAIN Batch 2/6100 loss 38.621204 loss_att 50.130417 loss_ctc 54.518505 loss_rnnt 34.175621 hw_loss 0.045189 lr 0.00091188 rank 0
2023-02-17 17:34:55,496 DEBUG TRAIN Batch 2/6100 loss 29.811972 loss_att 37.962498 loss_ctc 51.117104 loss_rnnt 25.325933 hw_loss 0.028597 lr 0.00091248 rank 1
2023-02-17 17:34:55,499 DEBUG TRAIN Batch 2/6100 loss 35.494656 loss_att 43.736115 loss_ctc 54.551632 loss_rnnt 31.272722 hw_loss 0.061329 lr 0.00091212 rank 2
2023-02-17 17:34:55,500 DEBUG TRAIN Batch 2/6100 loss 22.738110 loss_att 30.063145 loss_ctc 41.050735 loss_rnnt 18.773512 hw_loss 0.108576 lr 0.00091132 rank 6
2023-02-17 17:34:55,500 DEBUG TRAIN Batch 2/6100 loss 43.143452 loss_att 49.370560 loss_ctc 59.606422 loss_rnnt 39.688896 hw_loss 0.026387 lr 0.00091044 rank 5
2023-02-17 17:34:55,505 DEBUG TRAIN Batch 2/6100 loss 32.220825 loss_att 40.493553 loss_ctc 47.493057 loss_rnnt 28.458340 hw_loss 0.134339 lr 0.00091080 rank 3
2023-02-17 17:34:55,512 DEBUG TRAIN Batch 2/6100 loss 33.186684 loss_att 40.540028 loss_ctc 45.861164 loss_rnnt 30.008802 hw_loss 0.032408 lr 0.00091072 rank 7
2023-02-17 17:36:07,307 DEBUG TRAIN Batch 2/6200 loss 26.747423 loss_att 30.384640 loss_ctc 36.890980 loss_rnnt 24.626839 hw_loss 0.076251 lr 0.00091532 rank 6
2023-02-17 17:36:07,311 DEBUG TRAIN Batch 2/6200 loss 37.893612 loss_att 46.549759 loss_ctc 49.568459 loss_rnnt 34.581078 hw_loss 0.046235 lr 0.00091424 rank 4
2023-02-17 17:36:07,314 DEBUG TRAIN Batch 2/6200 loss 28.062311 loss_att 37.938896 loss_ctc 43.672058 loss_rnnt 23.993513 hw_loss 0.022842 lr 0.00091612 rank 2
2023-02-17 17:36:07,315 DEBUG TRAIN Batch 2/6200 loss 41.590698 loss_att 49.179600 loss_ctc 56.392303 loss_rnnt 37.979126 hw_loss 0.225458 lr 0.00091588 rank 0
2023-02-17 17:36:07,317 DEBUG TRAIN Batch 2/6200 loss 21.877668 loss_att 30.549671 loss_ctc 41.496841 loss_rnnt 17.448402 hw_loss 0.148078 lr 0.00091444 rank 5
2023-02-17 17:36:07,320 DEBUG TRAIN Batch 2/6200 loss 43.181507 loss_att 48.773788 loss_ctc 53.625168 loss_rnnt 40.670273 hw_loss 0.000544 lr 0.00091472 rank 7
2023-02-17 17:36:07,320 DEBUG TRAIN Batch 2/6200 loss 44.093861 loss_att 54.963509 loss_ctc 64.022110 loss_rnnt 39.234261 hw_loss 0.053570 lr 0.00091480 rank 3
2023-02-17 17:36:07,368 DEBUG TRAIN Batch 2/6200 loss 25.483488 loss_att 34.918320 loss_ctc 38.791985 loss_rnnt 21.800632 hw_loss 0.040162 lr 0.00091648 rank 1
2023-02-17 17:37:21,676 DEBUG TRAIN Batch 2/6300 loss 52.617702 loss_att 57.425331 loss_ctc 72.612869 loss_rnnt 48.975624 hw_loss 0.027238 lr 0.00091824 rank 4
2023-02-17 17:37:21,679 DEBUG TRAIN Batch 2/6300 loss 27.837160 loss_att 36.312195 loss_ctc 41.505356 loss_rnnt 24.228781 hw_loss 0.170525 lr 0.00091988 rank 0
2023-02-17 17:37:21,685 DEBUG TRAIN Batch 2/6300 loss 26.913397 loss_att 28.129635 loss_ctc 38.212563 loss_rnnt 25.093256 hw_loss 0.131880 lr 0.00092012 rank 2
2023-02-17 17:37:21,688 DEBUG TRAIN Batch 2/6300 loss 44.956249 loss_att 50.763229 loss_ctc 59.893887 loss_rnnt 41.759933 hw_loss 0.081069 lr 0.00091932 rank 6
2023-02-17 17:37:21,689 DEBUG TRAIN Batch 2/6300 loss 27.009193 loss_att 26.884056 loss_ctc 34.598022 loss_rnnt 25.948858 hw_loss 0.137850 lr 0.00091844 rank 5
2023-02-17 17:37:21,693 DEBUG TRAIN Batch 2/6300 loss 36.818623 loss_att 39.559357 loss_ctc 46.731045 loss_rnnt 34.896072 hw_loss 0.098892 lr 0.00091880 rank 3
2023-02-17 17:37:21,694 DEBUG TRAIN Batch 2/6300 loss 36.356327 loss_att 50.170879 loss_ctc 57.408916 loss_rnnt 30.728653 hw_loss 0.108281 lr 0.00092048 rank 1
2023-02-17 17:37:21,742 DEBUG TRAIN Batch 2/6300 loss 25.957405 loss_att 31.480080 loss_ctc 35.120831 loss_rnnt 23.597254 hw_loss 0.063424 lr 0.00091872 rank 7
2023-02-17 17:38:38,155 DEBUG TRAIN Batch 2/6400 loss 36.744801 loss_att 50.243984 loss_ctc 67.280243 loss_rnnt 29.973488 hw_loss 0.000152 lr 0.00092412 rank 2
2023-02-17 17:38:38,164 DEBUG TRAIN Batch 2/6400 loss 16.673767 loss_att 19.482380 loss_ctc 25.101379 loss_rnnt 14.885417 hw_loss 0.193022 lr 0.00092224 rank 4
2023-02-17 17:38:38,168 DEBUG TRAIN Batch 2/6400 loss 32.154316 loss_att 36.865902 loss_ctc 46.585030 loss_rnnt 29.241041 hw_loss 0.087868 lr 0.00092448 rank 1
2023-02-17 17:38:38,168 DEBUG TRAIN Batch 2/6400 loss 17.768726 loss_att 18.359510 loss_ctc 23.725399 loss_rnnt 16.759899 hw_loss 0.180834 lr 0.00092332 rank 6
2023-02-17 17:38:38,172 DEBUG TRAIN Batch 2/6400 loss 18.869381 loss_att 20.118835 loss_ctc 24.545584 loss_rnnt 17.811907 hw_loss 0.095169 lr 0.00092388 rank 0
2023-02-17 17:38:38,176 DEBUG TRAIN Batch 2/6400 loss 40.167377 loss_att 56.358604 loss_ctc 71.069321 loss_rnnt 32.769318 hw_loss 0.074163 lr 0.00092272 rank 7
2023-02-17 17:38:38,177 DEBUG TRAIN Batch 2/6400 loss 39.992813 loss_att 59.136658 loss_ctc 61.800957 loss_rnnt 33.178856 hw_loss 0.145189 lr 0.00092280 rank 3
2023-02-17 17:38:38,181 DEBUG TRAIN Batch 2/6400 loss 56.171486 loss_att 62.351677 loss_ctc 76.922943 loss_rnnt 52.156616 hw_loss 0.022442 lr 0.00092244 rank 5
2023-02-17 17:39:51,236 DEBUG TRAIN Batch 2/6500 loss 38.258881 loss_att 47.424179 loss_ctc 62.490204 loss_rnnt 33.181690 hw_loss 0.024915 lr 0.00092788 rank 0
2023-02-17 17:39:51,240 DEBUG TRAIN Batch 2/6500 loss 21.187271 loss_att 29.469856 loss_ctc 39.071175 loss_rnnt 17.145870 hw_loss 0.000683 lr 0.00092812 rank 2
2023-02-17 17:39:51,240 DEBUG TRAIN Batch 2/6500 loss 41.659184 loss_att 57.501442 loss_ctc 62.086830 loss_rnnt 35.727768 hw_loss 0.073642 lr 0.00092848 rank 1
2023-02-17 17:39:51,242 DEBUG TRAIN Batch 2/6500 loss 34.867146 loss_att 49.120255 loss_ctc 55.212994 loss_rnnt 29.247723 hw_loss 0.105037 lr 0.00092732 rank 6
2023-02-17 17:39:51,243 DEBUG TRAIN Batch 2/6500 loss 36.150440 loss_att 42.174820 loss_ctc 52.658775 loss_rnnt 32.684940 hw_loss 0.111594 lr 0.00092624 rank 4
2023-02-17 17:39:51,245 DEBUG TRAIN Batch 2/6500 loss 20.786085 loss_att 29.192217 loss_ctc 25.572670 loss_rnnt 18.442167 hw_loss 0.045903 lr 0.00092644 rank 5
2023-02-17 17:39:51,248 DEBUG TRAIN Batch 2/6500 loss 58.288338 loss_att 61.276375 loss_ctc 72.495499 loss_rnnt 55.769707 hw_loss 0.050133 lr 0.00092672 rank 7
2023-02-17 17:39:51,285 DEBUG TRAIN Batch 2/6500 loss 25.647125 loss_att 33.167336 loss_ctc 35.095898 loss_rnnt 22.847464 hw_loss 0.067091 lr 0.00092680 rank 3
2023-02-17 17:41:03,157 DEBUG TRAIN Batch 2/6600 loss 34.505337 loss_att 48.375919 loss_ctc 42.159584 loss_rnnt 30.662766 hw_loss 0.089791 lr 0.00093024 rank 4
2023-02-17 17:41:03,158 DEBUG TRAIN Batch 2/6600 loss 44.369610 loss_att 54.047768 loss_ctc 68.640656 loss_rnnt 39.185249 hw_loss 0.023608 lr 0.00093212 rank 2
2023-02-17 17:41:03,159 DEBUG TRAIN Batch 2/6600 loss 56.715149 loss_att 59.159325 loss_ctc 84.598930 loss_rnnt 52.508007 hw_loss 0.000877 lr 0.00093132 rank 6
2023-02-17 17:41:03,160 DEBUG TRAIN Batch 2/6600 loss 20.116369 loss_att 26.263273 loss_ctc 29.484327 loss_rnnt 17.637489 hw_loss 0.000820 lr 0.00093188 rank 0
2023-02-17 17:41:03,163 DEBUG TRAIN Batch 2/6600 loss 55.649967 loss_att 64.503525 loss_ctc 79.889359 loss_rnnt 50.611931 hw_loss 0.066386 lr 0.00093248 rank 1
2023-02-17 17:41:03,165 DEBUG TRAIN Batch 2/6600 loss 28.530149 loss_att 40.924835 loss_ctc 50.441582 loss_rnnt 23.095779 hw_loss 0.063578 lr 0.00093044 rank 5
2023-02-17 17:41:03,169 DEBUG TRAIN Batch 2/6600 loss 39.115910 loss_att 45.728069 loss_ctc 46.854706 loss_rnnt 36.721268 hw_loss 0.075695 lr 0.00093080 rank 3
2023-02-17 17:41:03,170 DEBUG TRAIN Batch 2/6600 loss 31.456287 loss_att 39.534946 loss_ctc 46.079372 loss_rnnt 27.853382 hw_loss 0.070178 lr 0.00093072 rank 7
2023-02-17 17:42:16,614 DEBUG TRAIN Batch 2/6700 loss 29.328815 loss_att 42.042145 loss_ctc 41.538818 loss_rnnt 25.143253 hw_loss 0.027928 lr 0.00093480 rank 3
2023-02-17 17:42:16,616 DEBUG TRAIN Batch 2/6700 loss 59.081451 loss_att 69.350060 loss_ctc 82.156891 loss_rnnt 53.916931 hw_loss 0.063878 lr 0.00093648 rank 1
2023-02-17 17:42:16,623 DEBUG TRAIN Batch 2/6700 loss 49.309200 loss_att 60.455528 loss_ctc 69.270645 loss_rnnt 44.383289 hw_loss 0.065856 lr 0.00093424 rank 4
2023-02-17 17:42:16,623 DEBUG TRAIN Batch 2/6700 loss 31.791254 loss_att 45.729595 loss_ctc 48.419083 loss_rnnt 26.750116 hw_loss 0.068299 lr 0.00093532 rank 6
2023-02-17 17:42:16,624 DEBUG TRAIN Batch 2/6700 loss 28.588718 loss_att 39.294903 loss_ctc 42.531494 loss_rnnt 24.562405 hw_loss 0.048827 lr 0.00093612 rank 2
2023-02-17 17:42:16,629 DEBUG TRAIN Batch 2/6700 loss 22.234034 loss_att 28.266851 loss_ctc 34.842937 loss_rnnt 19.310097 hw_loss 0.067851 lr 0.00093472 rank 7
2023-02-17 17:42:16,631 DEBUG TRAIN Batch 2/6700 loss 37.386139 loss_att 47.317928 loss_ctc 50.636551 loss_rnnt 33.586048 hw_loss 0.088140 lr 0.00093588 rank 0
2023-02-17 17:42:16,644 DEBUG TRAIN Batch 2/6700 loss 33.569042 loss_att 44.300133 loss_ctc 51.477776 loss_rnnt 29.017281 hw_loss 0.033211 lr 0.00093444 rank 5
2023-02-17 17:43:30,782 DEBUG TRAIN Batch 2/6800 loss 40.494316 loss_att 47.223122 loss_ctc 51.808620 loss_rnnt 37.625290 hw_loss 0.027545 lr 0.00093824 rank 4
2023-02-17 17:43:30,783 DEBUG TRAIN Batch 2/6800 loss 42.978287 loss_att 50.485924 loss_ctc 63.391808 loss_rnnt 38.746468 hw_loss 0.015919 lr 0.00093844 rank 5
2023-02-17 17:43:30,783 DEBUG TRAIN Batch 2/6800 loss 27.760229 loss_att 37.014709 loss_ctc 43.852531 loss_rnnt 23.732397 hw_loss 0.058677 lr 0.00094012 rank 2
2023-02-17 17:43:30,784 DEBUG TRAIN Batch 2/6800 loss 42.731213 loss_att 56.918869 loss_ctc 60.079941 loss_rnnt 37.527122 hw_loss 0.100113 lr 0.00094048 rank 1
2023-02-17 17:43:30,785 DEBUG TRAIN Batch 2/6800 loss 40.932541 loss_att 53.231270 loss_ctc 59.948021 loss_rnnt 35.893929 hw_loss 0.081501 lr 0.00093932 rank 6
2023-02-17 17:43:30,786 DEBUG TRAIN Batch 2/6800 loss 50.187218 loss_att 60.669144 loss_ctc 67.039207 loss_rnnt 45.805183 hw_loss 0.072582 lr 0.00093880 rank 3
2023-02-17 17:43:30,788 DEBUG TRAIN Batch 2/6800 loss 34.674274 loss_att 42.204025 loss_ctc 45.464226 loss_rnnt 31.682125 hw_loss 0.089144 lr 0.00093988 rank 0
2023-02-17 17:43:30,790 DEBUG TRAIN Batch 2/6800 loss 33.975903 loss_att 42.309444 loss_ctc 49.963669 loss_rnnt 30.118496 hw_loss 0.110617 lr 0.00093872 rank 7
2023-02-17 17:44:43,472 DEBUG TRAIN Batch 2/6900 loss 44.567490 loss_att 53.073193 loss_ctc 58.242325 loss_rnnt 41.028648 hw_loss 0.026981 lr 0.00094272 rank 7
2023-02-17 17:44:43,473 DEBUG TRAIN Batch 2/6900 loss 24.444664 loss_att 31.040516 loss_ctc 39.828190 loss_rnnt 21.010700 hw_loss 0.119352 lr 0.00094224 rank 4
2023-02-17 17:44:43,473 DEBUG TRAIN Batch 2/6900 loss 22.639074 loss_att 26.256933 loss_ctc 33.284256 loss_rnnt 20.462307 hw_loss 0.063449 lr 0.00094412 rank 2
2023-02-17 17:44:43,474 DEBUG TRAIN Batch 2/6900 loss 23.272781 loss_att 23.602112 loss_ctc 32.370808 loss_rnnt 21.891520 hw_loss 0.191856 lr 0.00094244 rank 5
2023-02-17 17:44:43,475 DEBUG TRAIN Batch 2/6900 loss 27.258728 loss_att 39.914787 loss_ctc 41.682426 loss_rnnt 22.747005 hw_loss 0.107530 lr 0.00094388 rank 0
2023-02-17 17:44:43,475 DEBUG TRAIN Batch 2/6900 loss 31.884739 loss_att 38.934601 loss_ctc 45.387367 loss_rnnt 28.612703 hw_loss 0.115710 lr 0.00094332 rank 6
2023-02-17 17:44:43,480 DEBUG TRAIN Batch 2/6900 loss 25.858767 loss_att 33.779449 loss_ctc 43.606968 loss_rnnt 21.854622 hw_loss 0.100466 lr 0.00094448 rank 1
2023-02-17 17:44:43,485 DEBUG TRAIN Batch 2/6900 loss 30.945446 loss_att 40.314049 loss_ctc 50.115456 loss_rnnt 26.427406 hw_loss 0.165597 lr 0.00094280 rank 3
2023-02-17 17:45:56,090 DEBUG TRAIN Batch 2/7000 loss 24.614967 loss_att 29.843933 loss_ctc 41.438240 loss_rnnt 21.284657 hw_loss 0.077647 lr 0.00094624 rank 4
2023-02-17 17:45:56,093 DEBUG TRAIN Batch 2/7000 loss 27.460670 loss_att 33.875542 loss_ctc 39.488350 loss_rnnt 24.557295 hw_loss 0.031335 lr 0.00094732 rank 6
2023-02-17 17:45:56,095 DEBUG TRAIN Batch 2/7000 loss 21.964556 loss_att 25.573654 loss_ctc 33.057632 loss_rnnt 19.704817 hw_loss 0.110330 lr 0.00094788 rank 0
2023-02-17 17:45:56,096 DEBUG TRAIN Batch 2/7000 loss 32.951431 loss_att 36.461693 loss_ctc 42.880558 loss_rnnt 30.895184 hw_loss 0.056840 lr 0.00094672 rank 7
2023-02-17 17:45:56,099 DEBUG TRAIN Batch 2/7000 loss 50.644863 loss_att 62.889328 loss_ctc 70.344261 loss_rnnt 45.536358 hw_loss 0.061919 lr 0.00094812 rank 2
2023-02-17 17:45:56,100 DEBUG TRAIN Batch 2/7000 loss 20.826643 loss_att 24.329081 loss_ctc 35.033241 loss_rnnt 18.131603 hw_loss 0.188136 lr 0.00094680 rank 3
2023-02-17 17:45:56,102 DEBUG TRAIN Batch 2/7000 loss 21.876673 loss_att 27.555439 loss_ctc 29.649292 loss_rnnt 19.658064 hw_loss 0.087201 lr 0.00094848 rank 1
2023-02-17 17:45:56,106 DEBUG TRAIN Batch 2/7000 loss 52.361988 loss_att 64.895683 loss_ctc 75.435623 loss_rnnt 46.759178 hw_loss 0.036723 lr 0.00094644 rank 5
2023-02-17 17:47:12,116 DEBUG TRAIN Batch 2/7100 loss 26.766779 loss_att 34.468609 loss_ctc 33.196381 loss_rnnt 24.341816 hw_loss 0.051219 lr 0.00095132 rank 6
2023-02-17 17:47:12,117 DEBUG TRAIN Batch 2/7100 loss 29.069414 loss_att 47.970589 loss_ctc 46.039825 loss_rnnt 23.004889 hw_loss 0.040439 lr 0.00095024 rank 4
2023-02-17 17:47:12,118 DEBUG TRAIN Batch 2/7100 loss 36.310192 loss_att 42.558121 loss_ctc 56.175850 loss_rnnt 32.359550 hw_loss 0.098067 lr 0.00095044 rank 5
2023-02-17 17:47:12,120 DEBUG TRAIN Batch 2/7100 loss 22.618944 loss_att 25.208725 loss_ctc 31.093172 loss_rnnt 20.924503 hw_loss 0.087354 lr 0.00095248 rank 1
2023-02-17 17:47:12,122 DEBUG TRAIN Batch 2/7100 loss 64.124550 loss_att 74.807106 loss_ctc 84.776657 loss_rnnt 59.189922 hw_loss 0.083446 lr 0.00095188 rank 0
2023-02-17 17:47:12,121 DEBUG TRAIN Batch 2/7100 loss 24.938677 loss_att 39.184692 loss_ctc 40.658257 loss_rnnt 19.945564 hw_loss 0.089930 lr 0.00095212 rank 2
2023-02-17 17:47:12,122 DEBUG TRAIN Batch 2/7100 loss 27.601395 loss_att 35.942356 loss_ctc 42.429657 loss_rnnt 23.954565 hw_loss 0.002881 lr 0.00095072 rank 7
2023-02-17 17:47:12,127 DEBUG TRAIN Batch 2/7100 loss 33.986015 loss_att 40.631466 loss_ctc 53.148888 loss_rnnt 30.045631 hw_loss 0.105456 lr 0.00095080 rank 3
2023-02-17 17:48:25,015 DEBUG TRAIN Batch 2/7200 loss 32.196754 loss_att 44.559814 loss_ctc 45.755630 loss_rnnt 27.886826 hw_loss 0.055246 lr 0.00095424 rank 4
2023-02-17 17:48:25,015 DEBUG TRAIN Batch 2/7200 loss 23.858255 loss_att 33.530754 loss_ctc 36.112236 loss_rnnt 20.225670 hw_loss 0.120413 lr 0.00095612 rank 2
2023-02-17 17:48:25,015 DEBUG TRAIN Batch 2/7200 loss 30.247816 loss_att 38.103722 loss_ctc 49.200890 loss_rnnt 26.149353 hw_loss 0.000382 lr 0.00095532 rank 6
2023-02-17 17:48:25,016 DEBUG TRAIN Batch 2/7200 loss 20.488447 loss_att 38.054626 loss_ctc 28.157475 loss_rnnt 15.952506 hw_loss 0.000313 lr 0.00095588 rank 0
2023-02-17 17:48:25,016 DEBUG TRAIN Batch 2/7200 loss 37.475250 loss_att 47.588966 loss_ctc 55.433163 loss_rnnt 33.043106 hw_loss 0.028148 lr 0.00095444 rank 5
2023-02-17 17:48:25,018 DEBUG TRAIN Batch 2/7200 loss 28.743666 loss_att 42.534206 loss_ctc 49.027538 loss_rnnt 23.247755 hw_loss 0.062412 lr 0.00095480 rank 3
2023-02-17 17:48:25,022 DEBUG TRAIN Batch 2/7200 loss 49.515804 loss_att 54.617775 loss_ctc 62.314705 loss_rnnt 46.750488 hw_loss 0.071996 lr 0.00095472 rank 7
2023-02-17 17:48:25,067 DEBUG TRAIN Batch 2/7200 loss 44.194622 loss_att 54.404522 loss_ctc 66.182144 loss_rnnt 39.184208 hw_loss 0.068935 lr 0.00095648 rank 1
2023-02-17 17:49:38,048 DEBUG TRAIN Batch 2/7300 loss 39.053097 loss_att 51.640312 loss_ctc 65.159988 loss_rnnt 33.014641 hw_loss 0.075175 lr 0.00095988 rank 0
2023-02-17 17:49:38,048 DEBUG TRAIN Batch 2/7300 loss 25.339594 loss_att 36.574303 loss_ctc 38.763256 loss_rnnt 21.273136 hw_loss 0.055678 lr 0.00095932 rank 6
2023-02-17 17:49:38,050 DEBUG TRAIN Batch 2/7300 loss 31.645306 loss_att 44.484318 loss_ctc 53.036911 loss_rnnt 26.173210 hw_loss 0.097648 lr 0.00095824 rank 4
2023-02-17 17:49:38,051 DEBUG TRAIN Batch 2/7300 loss 22.984207 loss_att 30.145168 loss_ctc 31.166172 loss_rnnt 20.460457 hw_loss 0.001182 lr 0.00096048 rank 1
2023-02-17 17:49:38,055 DEBUG TRAIN Batch 2/7300 loss 28.657305 loss_att 38.157120 loss_ctc 41.375183 loss_rnnt 25.010500 hw_loss 0.095860 lr 0.00095880 rank 3
2023-02-17 17:49:38,057 DEBUG TRAIN Batch 2/7300 loss 25.460386 loss_att 36.103142 loss_ctc 39.716400 loss_rnnt 21.375702 hw_loss 0.103747 lr 0.00095872 rank 7
2023-02-17 17:49:38,058 DEBUG TRAIN Batch 2/7300 loss 35.667030 loss_att 49.350536 loss_ctc 60.621307 loss_rnnt 29.536379 hw_loss 0.125083 lr 0.00096012 rank 2
2023-02-17 17:49:38,063 DEBUG TRAIN Batch 2/7300 loss 32.350170 loss_att 38.789108 loss_ctc 50.902298 loss_rnnt 28.573593 hw_loss 0.028452 lr 0.00095844 rank 5
2023-02-17 17:50:51,166 DEBUG TRAIN Batch 2/7400 loss 22.271832 loss_att 34.310085 loss_ctc 38.341469 loss_rnnt 17.710644 hw_loss 0.020472 lr 0.00096388 rank 0
2023-02-17 17:50:51,168 DEBUG TRAIN Batch 2/7400 loss 17.707834 loss_att 23.745504 loss_ctc 28.686943 loss_rnnt 15.001931 hw_loss 0.064662 lr 0.00096412 rank 2
2023-02-17 17:50:51,174 DEBUG TRAIN Batch 2/7400 loss 24.448214 loss_att 30.099268 loss_ctc 39.198471 loss_rnnt 21.312328 hw_loss 0.073074 lr 0.00096224 rank 4
2023-02-17 17:50:51,178 DEBUG TRAIN Batch 2/7400 loss 55.869659 loss_att 63.788311 loss_ctc 82.014145 loss_rnnt 50.736351 hw_loss 0.119341 lr 0.00096332 rank 6
2023-02-17 17:50:51,178 DEBUG TRAIN Batch 2/7400 loss 36.516804 loss_att 52.427452 loss_ctc 56.032204 loss_rnnt 30.698139 hw_loss 0.064648 lr 0.00096244 rank 5
2023-02-17 17:50:51,183 DEBUG TRAIN Batch 2/7400 loss 28.294920 loss_att 38.641495 loss_ctc 51.276310 loss_rnnt 23.156120 hw_loss 0.009939 lr 0.00096448 rank 1
2023-02-17 17:50:51,206 DEBUG TRAIN Batch 2/7400 loss 41.931427 loss_att 54.972683 loss_ctc 63.252213 loss_rnnt 36.437283 hw_loss 0.080852 lr 0.00096272 rank 7
2023-02-17 17:50:51,233 DEBUG TRAIN Batch 2/7400 loss 38.769833 loss_att 43.003887 loss_ctc 64.486649 loss_rnnt 34.427349 hw_loss 0.125183 lr 0.00096280 rank 3
2023-02-17 17:52:05,780 DEBUG TRAIN Batch 2/7500 loss 28.640884 loss_att 37.881912 loss_ctc 44.338131 loss_rnnt 24.665104 hw_loss 0.064888 lr 0.00096624 rank 4
2023-02-17 17:52:05,784 DEBUG TRAIN Batch 2/7500 loss 43.196449 loss_att 56.526222 loss_ctc 64.985321 loss_rnnt 37.625076 hw_loss 0.000442 lr 0.00096732 rank 6
2023-02-17 17:52:05,786 DEBUG TRAIN Batch 2/7500 loss 39.388287 loss_att 45.810562 loss_ctc 58.411579 loss_rnnt 35.559235 hw_loss 0.015300 lr 0.00096680 rank 3
2023-02-17 17:52:05,788 DEBUG TRAIN Batch 2/7500 loss 47.669449 loss_att 51.801376 loss_ctc 62.369926 loss_rnnt 44.781586 hw_loss 0.190156 lr 0.00096812 rank 2
2023-02-17 17:52:05,789 DEBUG TRAIN Batch 2/7500 loss 36.832951 loss_att 48.094864 loss_ctc 55.218609 loss_rnnt 32.109711 hw_loss 0.036442 lr 0.00096788 rank 0
2023-02-17 17:52:05,790 DEBUG TRAIN Batch 2/7500 loss 28.303318 loss_att 39.396427 loss_ctc 42.276577 loss_rnnt 24.181812 hw_loss 0.074594 lr 0.00096672 rank 7
2023-02-17 17:52:05,792 DEBUG TRAIN Batch 2/7500 loss 48.487759 loss_att 55.721550 loss_ctc 64.342896 loss_rnnt 44.860065 hw_loss 0.125471 lr 0.00096848 rank 1
2023-02-17 17:52:05,793 DEBUG TRAIN Batch 2/7500 loss 44.166893 loss_att 45.461861 loss_ctc 59.539246 loss_rnnt 41.821587 hw_loss 0.068746 lr 0.00096644 rank 5
2023-02-17 17:53:19,143 DEBUG TRAIN Batch 2/7600 loss 37.065323 loss_att 43.856380 loss_ctc 54.011223 loss_rnnt 33.435711 hw_loss 0.022398 lr 0.00097212 rank 2
2023-02-17 17:53:19,158 DEBUG TRAIN Batch 2/7600 loss 62.225876 loss_att 79.748665 loss_ctc 76.285042 loss_rnnt 56.789001 hw_loss 0.108299 lr 0.00097024 rank 4
2023-02-17 17:53:19,161 DEBUG TRAIN Batch 2/7600 loss 44.346748 loss_att 52.829826 loss_ctc 67.743736 loss_rnnt 39.499264 hw_loss 0.058640 lr 0.00097132 rank 6
2023-02-17 17:53:19,161 DEBUG TRAIN Batch 2/7600 loss 33.673363 loss_att 39.594051 loss_ctc 44.199623 loss_rnnt 31.065016 hw_loss 0.038835 lr 0.00097188 rank 0
2023-02-17 17:53:19,162 DEBUG TRAIN Batch 2/7600 loss 31.258621 loss_att 39.193935 loss_ctc 50.977673 loss_rnnt 27.041832 hw_loss 0.000974 lr 0.00097248 rank 1
2023-02-17 17:53:19,163 DEBUG TRAIN Batch 2/7600 loss 26.088839 loss_att 30.981125 loss_ctc 39.401310 loss_rnnt 23.278164 hw_loss 0.107292 lr 0.00097080 rank 3
2023-02-17 17:53:19,168 DEBUG TRAIN Batch 2/7600 loss 32.141815 loss_att 46.907715 loss_ctc 57.463608 loss_rnnt 25.763321 hw_loss 0.092007 lr 0.00097044 rank 5
2023-02-17 17:53:19,169 DEBUG TRAIN Batch 2/7600 loss 32.232990 loss_att 39.262390 loss_ctc 47.402294 loss_rnnt 28.761526 hw_loss 0.080639 lr 0.00097072 rank 7
2023-02-17 17:54:31,220 DEBUG TRAIN Batch 2/7700 loss 23.598928 loss_att 26.963696 loss_ctc 35.824886 loss_rnnt 21.230333 hw_loss 0.122840 lr 0.00097424 rank 4
2023-02-17 17:54:31,226 DEBUG TRAIN Batch 2/7700 loss 50.057270 loss_att 68.044746 loss_ctc 62.903023 loss_rnnt 44.689934 hw_loss 0.107015 lr 0.00097444 rank 5
2023-02-17 17:54:31,226 DEBUG TRAIN Batch 2/7700 loss 31.157944 loss_att 32.592663 loss_ctc 43.612617 loss_rnnt 29.139492 hw_loss 0.132915 lr 0.00097648 rank 1
2023-02-17 17:54:31,227 DEBUG TRAIN Batch 2/7700 loss 23.083763 loss_att 25.911356 loss_ctc 31.859304 loss_rnnt 21.273760 hw_loss 0.139525 lr 0.00097532 rank 6
2023-02-17 17:54:31,230 DEBUG TRAIN Batch 2/7700 loss 33.085033 loss_att 46.511875 loss_ctc 55.673737 loss_rnnt 27.294640 hw_loss 0.174754 lr 0.00097480 rank 3
2023-02-17 17:54:31,232 DEBUG TRAIN Batch 2/7700 loss 22.227911 loss_att 33.590561 loss_ctc 34.869827 loss_rnnt 18.208418 hw_loss 0.115079 lr 0.00097588 rank 0
2023-02-17 17:54:31,232 DEBUG TRAIN Batch 2/7700 loss 30.221598 loss_att 38.305164 loss_ctc 48.472160 loss_rnnt 26.126698 hw_loss 0.083957 lr 0.00097612 rank 2
2023-02-17 17:54:31,279 DEBUG TRAIN Batch 2/7700 loss 32.577778 loss_att 44.549515 loss_ctc 56.124695 loss_rnnt 27.042868 hw_loss 0.001829 lr 0.00097472 rank 7
2023-02-17 17:55:46,248 DEBUG TRAIN Batch 2/7800 loss 24.227961 loss_att 40.111748 loss_ctc 37.317463 loss_rnnt 19.305784 hw_loss 0.000287 lr 0.00097872 rank 7
2023-02-17 17:55:46,251 DEBUG TRAIN Batch 2/7800 loss 31.650646 loss_att 35.062050 loss_ctc 40.505959 loss_rnnt 29.738928 hw_loss 0.091366 lr 0.00097844 rank 5
2023-02-17 17:55:46,262 DEBUG TRAIN Batch 2/7800 loss 25.630581 loss_att 32.831306 loss_ctc 36.483501 loss_rnnt 22.743217 hw_loss 0.000305 lr 0.00097932 rank 6
2023-02-17 17:55:46,265 DEBUG TRAIN Batch 2/7800 loss 49.403297 loss_att 63.416210 loss_ctc 67.496674 loss_rnnt 44.188026 hw_loss 0.000440 lr 0.00097824 rank 4
2023-02-17 17:55:46,268 DEBUG TRAIN Batch 2/7800 loss 24.142694 loss_att 33.671211 loss_ctc 31.342001 loss_rnnt 21.276941 hw_loss 0.000261 lr 0.00097988 rank 0
2023-02-17 17:55:46,288 DEBUG TRAIN Batch 2/7800 loss 39.208530 loss_att 41.535526 loss_ctc 63.166714 loss_rnnt 35.511089 hw_loss 0.070530 lr 0.00097880 rank 3
2023-02-17 17:55:46,287 DEBUG TRAIN Batch 2/7800 loss 37.298717 loss_att 46.398853 loss_ctc 56.352627 loss_rnnt 32.895443 hw_loss 0.080105 lr 0.00098048 rank 1
2023-02-17 17:55:46,295 DEBUG TRAIN Batch 2/7800 loss 48.105873 loss_att 57.472980 loss_ctc 67.018944 loss_rnnt 43.710571 hw_loss 0.000261 lr 0.00098012 rank 2
2023-02-17 17:57:00,649 DEBUG TRAIN Batch 2/7900 loss 40.829586 loss_att 56.582283 loss_ctc 66.901527 loss_rnnt 34.189098 hw_loss 0.025669 lr 0.00098224 rank 4
2023-02-17 17:57:00,653 DEBUG TRAIN Batch 2/7900 loss 45.177288 loss_att 61.724350 loss_ctc 75.104065 loss_rnnt 37.877579 hw_loss 0.000113 lr 0.00098332 rank 6
2023-02-17 17:57:00,653 DEBUG TRAIN Batch 2/7900 loss 26.533276 loss_att 36.668194 loss_ctc 42.648716 loss_rnnt 22.286638 hw_loss 0.132989 lr 0.00098412 rank 2
2023-02-17 17:57:00,657 DEBUG TRAIN Batch 2/7900 loss 41.421108 loss_att 49.593201 loss_ctc 54.748108 loss_rnnt 38.002041 hw_loss 0.014468 lr 0.00098448 rank 1
2023-02-17 17:57:00,657 DEBUG TRAIN Batch 2/7900 loss 43.714066 loss_att 53.204132 loss_ctc 65.585945 loss_rnnt 38.899746 hw_loss 0.000109 lr 0.00098388 rank 0
2023-02-17 17:57:00,663 DEBUG TRAIN Batch 2/7900 loss 36.386620 loss_att 40.766033 loss_ctc 52.452972 loss_rnnt 33.339226 hw_loss 0.054992 lr 0.00098280 rank 3
2023-02-17 17:57:00,665 DEBUG TRAIN Batch 2/7900 loss 25.049704 loss_att 29.603260 loss_ctc 38.552254 loss_rnnt 22.335026 hw_loss 0.006800 lr 0.00098244 rank 5
2023-02-17 17:57:00,709 DEBUG TRAIN Batch 2/7900 loss 29.801502 loss_att 35.660736 loss_ctc 44.393871 loss_rnnt 26.683945 hw_loss 0.000115 lr 0.00098272 rank 7
2023-02-17 17:58:12,973 DEBUG TRAIN Batch 2/8000 loss 26.583080 loss_att 39.396809 loss_ctc 39.601021 loss_rnnt 22.207899 hw_loss 0.143831 lr 0.00098624 rank 4
2023-02-17 17:58:12,974 DEBUG TRAIN Batch 2/8000 loss 30.816225 loss_att 37.700020 loss_ctc 45.222050 loss_rnnt 27.429285 hw_loss 0.167631 lr 0.00098788 rank 0
2023-02-17 17:58:12,976 DEBUG TRAIN Batch 2/8000 loss 38.563343 loss_att 47.591686 loss_ctc 61.332771 loss_rnnt 33.672691 hw_loss 0.091978 lr 0.00098732 rank 6
2023-02-17 17:58:12,978 DEBUG TRAIN Batch 2/8000 loss 24.994020 loss_att 31.762527 loss_ctc 37.563789 loss_rnnt 21.916073 hw_loss 0.090523 lr 0.00098672 rank 7
2023-02-17 17:58:12,982 DEBUG TRAIN Batch 2/8000 loss 27.176565 loss_att 34.511177 loss_ctc 39.801048 loss_rnnt 24.014383 hw_loss 0.022495 lr 0.00098644 rank 5
2023-02-17 17:58:12,982 DEBUG TRAIN Batch 2/8000 loss 26.679436 loss_att 35.092464 loss_ctc 46.393650 loss_rnnt 22.367386 hw_loss 0.001652 lr 0.00098812 rank 2
2023-02-17 17:58:12,985 DEBUG TRAIN Batch 2/8000 loss 16.743685 loss_att 24.986706 loss_ctc 26.021313 loss_rnnt 13.805179 hw_loss 0.099159 lr 0.00098848 rank 1
2023-02-17 17:58:12,985 DEBUG TRAIN Batch 2/8000 loss 32.006264 loss_att 44.110729 loss_ctc 62.327034 loss_rnnt 25.521441 hw_loss 0.039675 lr 0.00098680 rank 3
2023-02-17 17:59:26,662 DEBUG TRAIN Batch 2/8100 loss 48.118958 loss_att 57.820709 loss_ctc 62.819008 loss_rnnt 44.218269 hw_loss 0.000606 lr 0.00099188 rank 0
2023-02-17 17:59:26,663 DEBUG TRAIN Batch 2/8100 loss 48.706882 loss_att 59.554867 loss_ctc 66.950462 loss_rnnt 44.077244 hw_loss 0.051681 lr 0.00099024 rank 4
2023-02-17 17:59:26,665 DEBUG TRAIN Batch 2/8100 loss 39.369850 loss_att 49.608646 loss_ctc 55.572906 loss_rnnt 35.139706 hw_loss 0.041204 lr 0.00099080 rank 3
2023-02-17 17:59:26,666 DEBUG TRAIN Batch 2/8100 loss 36.915760 loss_att 47.271263 loss_ctc 52.915344 loss_rnnt 32.689117 hw_loss 0.041741 lr 0.00099248 rank 1
2023-02-17 17:59:26,665 DEBUG TRAIN Batch 2/8100 loss 65.096733 loss_att 75.502243 loss_ctc 90.705582 loss_rnnt 59.546387 hw_loss 0.102639 lr 0.00099132 rank 6
2023-02-17 17:59:26,667 DEBUG TRAIN Batch 2/8100 loss 29.773602 loss_att 34.466911 loss_ctc 43.489079 loss_rnnt 26.994110 hw_loss 0.022686 lr 0.00099212 rank 2
2023-02-17 17:59:26,669 DEBUG TRAIN Batch 2/8100 loss 25.195976 loss_att 28.301277 loss_ctc 35.236259 loss_rnnt 23.182755 hw_loss 0.100233 lr 0.00099044 rank 5
2023-02-17 17:59:26,676 DEBUG TRAIN Batch 2/8100 loss 35.043152 loss_att 41.838028 loss_ctc 50.245544 loss_rnnt 31.611713 hw_loss 0.085271 lr 0.00099072 rank 7
2023-02-17 18:00:39,831 DEBUG TRAIN Batch 2/8200 loss 21.868408 loss_att 27.361351 loss_ctc 35.612831 loss_rnnt 18.908371 hw_loss 0.054113 lr 0.00099424 rank 4
2023-02-17 18:00:39,841 DEBUG TRAIN Batch 2/8200 loss 41.815742 loss_att 55.236553 loss_ctc 48.433426 loss_rnnt 38.227722 hw_loss 0.040309 lr 0.00099472 rank 7
2023-02-17 18:00:39,842 DEBUG TRAIN Batch 2/8200 loss 21.870785 loss_att 30.076786 loss_ctc 37.368530 loss_rnnt 18.128269 hw_loss 0.065528 lr 0.00099532 rank 6
2023-02-17 18:00:39,843 DEBUG TRAIN Batch 2/8200 loss 32.334862 loss_att 38.188980 loss_ctc 42.004406 loss_rnnt 29.824736 hw_loss 0.093802 lr 0.00099588 rank 0
2023-02-17 18:00:39,845 DEBUG TRAIN Batch 2/8200 loss 18.020285 loss_att 21.568371 loss_ctc 28.298038 loss_rnnt 15.891349 hw_loss 0.091782 lr 0.00099612 rank 2
2023-02-17 18:00:39,845 DEBUG TRAIN Batch 2/8200 loss 29.061455 loss_att 46.738026 loss_ctc 43.705425 loss_rnnt 23.573364 hw_loss 0.000463 lr 0.00099444 rank 5
2023-02-17 18:00:39,846 DEBUG TRAIN Batch 2/8200 loss 34.104454 loss_att 40.706181 loss_ctc 54.514042 loss_rnnt 30.034624 hw_loss 0.052883 lr 0.00099648 rank 1
2023-02-17 18:00:39,854 DEBUG TRAIN Batch 2/8200 loss 27.870718 loss_att 30.414963 loss_ctc 39.467163 loss_rnnt 25.798475 hw_loss 0.032253 lr 0.00099480 rank 3
2023-02-17 18:01:51,616 DEBUG TRAIN Batch 2/8300 loss 48.242085 loss_att 55.492691 loss_ctc 67.963531 loss_rnnt 44.089691 hw_loss 0.136408 lr 0.00099976 rank 1
2023-02-17 18:01:51,630 DEBUG TRAIN Batch 2/8300 loss 30.107616 loss_att 42.784546 loss_ctc 42.288097 loss_rnnt 25.881893 hw_loss 0.124262 lr 0.00099994 rank 2
2023-02-17 18:01:51,632 DEBUG TRAIN Batch 2/8300 loss 18.703756 loss_att 23.403458 loss_ctc 26.950058 loss_rnnt 16.614803 hw_loss 0.092820 lr 0.00099824 rank 4
2023-02-17 18:01:51,632 DEBUG TRAIN Batch 2/8300 loss 42.050713 loss_att 55.763321 loss_ctc 62.154800 loss_rnnt 36.590614 hw_loss 0.069434 lr 0.00099988 rank 0
2023-02-17 18:01:51,634 DEBUG TRAIN Batch 2/8300 loss 44.067837 loss_att 58.212364 loss_ctc 74.209381 loss_rnnt 37.213448 hw_loss 0.012385 lr 0.00099844 rank 5
2023-02-17 18:01:51,634 DEBUG TRAIN Batch 2/8300 loss 46.637947 loss_att 52.884495 loss_ctc 61.034595 loss_rnnt 43.432541 hw_loss 0.068518 lr 0.00099932 rank 6
2023-02-17 18:01:51,635 DEBUG TRAIN Batch 2/8300 loss 26.391031 loss_att 34.335217 loss_ctc 36.956367 loss_rnnt 23.392509 hw_loss 0.001824 lr 0.00099872 rank 7
2023-02-17 18:01:51,642 DEBUG TRAIN Batch 2/8300 loss 27.806181 loss_att 38.701565 loss_ctc 43.277710 loss_rnnt 23.509167 hw_loss 0.103254 lr 0.00099880 rank 3
2023-02-17 18:02:44,225 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 7
2023-02-17 18:02:44,226 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 5
2023-02-17 18:02:44,229 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 1
2023-02-17 18:02:44,229 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 4
2023-02-17 18:02:44,235 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 2
2023-02-17 18:02:44,247 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 6
2023-02-17 18:02:44,248 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 0
2023-02-17 18:02:44,250 DEBUG CV Batch 2/0 loss 7.112351 loss_att 7.424127 loss_ctc 11.486814 loss_rnnt 6.318381 hw_loss 0.278163 history loss 6.848931 rank 3
2023-02-17 18:02:55,191 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 1
2023-02-17 18:02:55,213 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 2
2023-02-17 18:02:55,355 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 5
2023-02-17 18:02:55,399 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 3
2023-02-17 18:02:55,549 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 7
2023-02-17 18:02:55,702 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 4
2023-02-17 18:02:55,919 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 6
2023-02-17 18:02:55,982 DEBUG CV Batch 2/100 loss 23.115568 loss_att 25.260000 loss_ctc 35.965733 loss_rnnt 20.924324 hw_loss 0.091881 history loss 12.595270 rank 0
2023-02-17 18:03:08,214 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 1
2023-02-17 18:03:08,388 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 2
2023-02-17 18:03:08,618 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 5
2023-02-17 18:03:08,673 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 7
2023-02-17 18:03:08,828 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 3
2023-02-17 18:03:09,418 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 4
2023-02-17 18:03:09,740 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 6
2023-02-17 18:03:09,921 DEBUG CV Batch 2/200 loss 35.636738 loss_att 55.449440 loss_ctc 49.682030 loss_rnnt 29.800081 hw_loss 0.002646 history loss 14.078884 rank 0
2023-02-17 18:03:20,297 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 1
2023-02-17 18:03:20,679 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 2
2023-02-17 18:03:20,751 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 7
2023-02-17 18:03:20,889 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 5
2023-02-17 18:03:21,264 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 3
2023-02-17 18:03:21,678 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 4
2023-02-17 18:03:22,213 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 6
2023-02-17 18:03:22,554 DEBUG CV Batch 2/300 loss 13.512271 loss_att 15.162291 loss_ctc 21.996887 loss_rnnt 11.987461 hw_loss 0.119106 history loss 14.134724 rank 0
2023-02-17 18:03:32,075 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 1
2023-02-17 18:03:32,505 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 7
2023-02-17 18:03:32,531 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 2
2023-02-17 18:03:33,126 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 5
2023-02-17 18:03:33,273 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 3
2023-02-17 18:03:33,930 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 4
2023-02-17 18:03:34,462 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 6
2023-02-17 18:03:34,987 DEBUG CV Batch 2/400 loss 74.775169 loss_att 182.963440 loss_ctc 84.302475 loss_rnnt 51.841003 hw_loss 0.049135 history loss 15.825096 rank 0
2023-02-17 18:03:42,569 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 1
2023-02-17 18:03:42,981 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 7
2023-02-17 18:03:43,184 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 2
2023-02-17 18:03:43,881 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 3
2023-02-17 18:03:43,968 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 5
2023-02-17 18:03:44,857 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 4
2023-02-17 18:03:45,465 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 6
2023-02-17 18:03:46,186 DEBUG CV Batch 2/500 loss 26.181444 loss_att 27.640110 loss_ctc 36.310467 loss_rnnt 24.532751 hw_loss 0.012043 history loss 16.961450 rank 0
2023-02-17 18:03:54,441 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 1
2023-02-17 18:03:55,033 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 7
2023-02-17 18:03:55,390 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 2
2023-02-17 18:03:55,929 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 3
2023-02-17 18:03:56,049 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 5
2023-02-17 18:03:57,187 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 4
2023-02-17 18:03:57,981 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 6
2023-02-17 18:03:58,932 DEBUG CV Batch 2/600 loss 15.395668 loss_att 15.976120 loss_ctc 23.531483 loss_rnnt 14.112393 hw_loss 0.154513 history loss 18.419805 rank 0
2023-02-17 18:04:05,536 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 1
2023-02-17 18:04:06,334 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 7
2023-02-17 18:04:07,095 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 2
2023-02-17 18:04:07,306 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 3
2023-02-17 18:04:07,307 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 5
2023-02-17 18:04:08,843 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 4
2023-02-17 18:04:09,748 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 6
2023-02-17 18:04:10,839 DEBUG CV Batch 2/700 loss 79.190208 loss_att 134.224426 loss_ctc 88.499992 loss_rnnt 66.941711 hw_loss 0.000648 history loss 19.513091 rank 0
2023-02-17 18:04:16,809 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 1
2023-02-17 18:04:17,587 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 7
2023-02-17 18:04:18,518 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 3
2023-02-17 18:04:18,529 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 2
2023-02-17 18:04:18,557 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 5
2023-02-17 18:04:20,472 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 4
2023-02-17 18:04:21,565 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 6
2023-02-17 18:04:22,802 DEBUG CV Batch 2/800 loss 25.774719 loss_att 26.988546 loss_ctc 40.154804 loss_rnnt 23.483776 hw_loss 0.245312 history loss 18.576474 rank 0
2023-02-17 18:04:29,874 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 1
2023-02-17 18:04:31,132 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 7
2023-02-17 18:04:31,865 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 3
2023-02-17 18:04:31,888 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 2
2023-02-17 18:04:32,319 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 5
2023-02-17 18:04:33,857 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 4
2023-02-17 18:04:35,275 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 6
2023-02-17 18:04:36,618 DEBUG CV Batch 2/900 loss 34.207184 loss_att 61.644714 loss_ctc 59.091713 loss_rnnt 25.346966 hw_loss 0.102703 history loss 18.335281 rank 0
2023-02-17 18:04:42,114 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 1
2023-02-17 18:04:43,293 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 7
2023-02-17 18:04:44,166 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 2
2023-02-17 18:04:44,190 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 3
2023-02-17 18:04:44,704 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 5
2023-02-17 18:04:46,222 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 4
2023-02-17 18:04:47,894 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 6
2023-02-17 18:04:49,490 DEBUG CV Batch 2/1000 loss 13.115928 loss_att 14.624933 loss_ctc 19.677053 loss_rnnt 11.864472 hw_loss 0.140318 history loss 17.937119 rank 0
2023-02-17 18:04:53,780 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 1
2023-02-17 18:04:55,191 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 7
2023-02-17 18:04:56,047 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 2
2023-02-17 18:04:56,342 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 3
2023-02-17 18:04:56,577 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 5
2023-02-17 18:04:58,284 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 4
2023-02-17 18:05:00,167 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 6
2023-02-17 18:05:01,993 DEBUG CV Batch 2/1100 loss 11.602531 loss_att 10.116982 loss_ctc 16.737701 loss_rnnt 11.103142 hw_loss 0.209644 history loss 17.952440 rank 0
2023-02-17 18:05:04,328 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 1
2023-02-17 18:05:05,927 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 7
2023-02-17 18:05:06,710 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 2
2023-02-17 18:05:06,978 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 3
2023-02-17 18:05:07,248 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 5
2023-02-17 18:05:09,301 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 4
2023-02-17 18:05:11,237 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 6
2023-02-17 18:05:13,285 DEBUG CV Batch 2/1200 loss 28.542858 loss_att 32.307617 loss_ctc 39.045376 loss_rnnt 26.310999 hw_loss 0.147321 history loss 18.438642 rank 0
2023-02-17 18:05:16,390 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 1
2023-02-17 18:05:18,031 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 7
2023-02-17 18:05:18,777 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 2
2023-02-17 18:05:19,024 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 3
2023-02-17 18:05:19,354 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 5
2023-02-17 18:05:21,516 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 4
2023-02-17 18:05:23,566 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 6
2023-02-17 18:05:25,794 DEBUG CV Batch 2/1300 loss 14.539307 loss_att 15.530602 loss_ctc 22.317039 loss_rnnt 13.263919 hw_loss 0.075183 history loss 18.891167 rank 0
2023-02-17 18:05:27,964 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 1
2023-02-17 18:05:29,259 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 7
2023-02-17 18:05:29,984 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 2
2023-02-17 18:05:30,346 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 3
2023-02-17 18:05:30,825 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 5
2023-02-17 18:05:32,907 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 4
2023-02-17 18:05:35,181 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 6
2023-02-17 18:05:37,577 DEBUG CV Batch 2/1400 loss 41.619801 loss_att 80.051521 loss_ctc 55.337246 loss_rnnt 32.065655 hw_loss 0.072767 history loss 19.449615 rank 0
2023-02-17 18:05:39,291 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 1
2023-02-17 18:05:40,768 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 7
2023-02-17 18:05:41,609 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 2
2023-02-17 18:05:41,985 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 3
2023-02-17 18:05:42,711 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 5
2023-02-17 18:05:44,711 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 4
2023-02-17 18:05:47,245 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 6
2023-02-17 18:05:49,668 DEBUG CV Batch 2/1500 loss 23.529875 loss_att 23.984226 loss_ctc 31.603889 loss_rnnt 22.328590 hw_loss 0.063525 history loss 19.078023 rank 0
2023-02-17 18:05:52,144 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 1
2023-02-17 18:05:53,804 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 7
2023-02-17 18:05:54,737 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 2
2023-02-17 18:05:55,518 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 3
2023-02-17 18:05:55,835 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 5
2023-02-17 18:05:57,938 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 4
2023-02-17 18:06:00,738 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 6
2023-02-17 18:06:03,150 DEBUG CV Batch 2/1600 loss 31.387600 loss_att 52.695301 loss_ctc 47.296898 loss_rnnt 24.981352 hw_loss 0.044001 history loss 18.969936 rank 0
2023-02-17 18:06:04,596 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 1
2023-02-17 18:06:06,187 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 7
2023-02-17 18:06:07,347 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 2
2023-02-17 18:06:07,979 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 3
2023-02-17 18:06:08,303 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 5
2023-02-17 18:06:10,419 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 4
2023-02-17 18:06:13,203 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 6
2023-02-17 18:06:13,613 INFO Epoch 2 CV info cv_loss 18.72164593154355
2023-02-17 18:06:13,614 INFO Epoch 3 TRAIN info lr 0.0009994404699613898
2023-02-17 18:06:13,619 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:15,558 DEBUG CV Batch 2/1700 loss 26.088392 loss_att 25.013954 loss_ctc 38.660633 loss_rnnt 24.587866 hw_loss 0.073345 history loss 18.759968 rank 0
2023-02-17 18:06:15,887 INFO Epoch 2 CV info cv_loss 18.72164593547182
2023-02-17 18:06:15,888 INFO Epoch 3 TRAIN info lr 0.00099968
2023-02-17 18:06:15,893 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:16,584 INFO Epoch 2 CV info cv_loss 18.721645930923295
2023-02-17 18:06:16,584 INFO Epoch 3 TRAIN info lr 0.0009988420145056651
2023-02-17 18:06:16,590 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:17,156 INFO Epoch 2 CV info cv_loss 18.721645930234125
2023-02-17 18:06:17,157 INFO Epoch 3 TRAIN info lr 0.0009997800725733902
2023-02-17 18:06:17,162 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:17,469 INFO Epoch 2 CV info cv_loss 18.721645931853676
2023-02-17 18:06:17,470 INFO Epoch 3 TRAIN info lr 0.0009995003746877732
2023-02-17 18:06:17,475 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:19,362 INFO Epoch 2 CV info cv_loss 18.721645932301637
2023-02-17 18:06:19,363 INFO Epoch 3 TRAIN info lr 0.00099908
2023-02-17 18:06:19,368 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:22,603 INFO Epoch 2 CV info cv_loss 18.721645931302337
2023-02-17 18:06:22,604 INFO Epoch 3 TRAIN info lr 0.00099992000959872
2023-02-17 18:06:22,606 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:06:25,264 INFO Epoch 2 CV info cv_loss 18.721645934713735
2023-02-17 18:06:25,264 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word/2.pt
2023-02-17 18:06:26,204 INFO Epoch 3 TRAIN info lr 0.0009993007341435492
2023-02-17 18:06:26,207 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 18:07:25,027 DEBUG TRAIN Batch 3/0 loss 18.224951 loss_att 18.548878 loss_ctc 24.618816 loss_rnnt 17.173815 hw_loss 0.250938 lr 0.00099912 rank 4
2023-02-17 18:07:25,032 DEBUG TRAIN Batch 3/0 loss 14.756532 loss_att 15.190301 loss_ctc 20.093349 loss_rnnt 13.875929 hw_loss 0.154264 lr 0.00099972 rank 7
2023-02-17 18:07:25,033 DEBUG TRAIN Batch 3/0 loss 15.249990 loss_att 14.789503 loss_ctc 18.558064 loss_rnnt 14.825855 hw_loss 0.140918 lr 0.00099990 rank 6
2023-02-17 18:07:25,035 DEBUG TRAIN Batch 3/0 loss 16.293234 loss_att 16.329273 loss_ctc 19.682812 loss_rnnt 15.735766 hw_loss 0.184347 lr 0.00099942 rank 1
2023-02-17 18:07:25,039 DEBUG TRAIN Batch 3/0 loss 22.323292 loss_att 21.628435 loss_ctc 27.746069 loss_rnnt 21.684620 hw_loss 0.102385 lr 0.00099882 rank 2
2023-02-17 18:07:25,039 DEBUG TRAIN Batch 3/0 loss 17.602499 loss_att 17.881035 loss_ctc 23.270952 loss_rnnt 16.680338 hw_loss 0.207486 lr 0.00099948 rank 5
2023-02-17 18:07:25,041 DEBUG TRAIN Batch 3/0 loss 17.866680 loss_att 19.247438 loss_ctc 22.984449 loss_rnnt 16.835356 hw_loss 0.136508 lr 0.00099976 rank 3
2023-02-17 18:07:25,050 DEBUG TRAIN Batch 3/0 loss 19.022865 loss_att 18.812965 loss_ctc 23.615793 loss_rnnt 18.342268 hw_loss 0.206598 lr 0.00099928 rank 0
2023-02-17 18:08:37,247 DEBUG TRAIN Batch 3/100 loss 51.552090 loss_att 62.426712 loss_ctc 67.773209 loss_rnnt 47.183655 hw_loss 0.057556 lr 0.00099844 rank 4
2023-02-17 18:08:37,247 DEBUG TRAIN Batch 3/100 loss 20.014053 loss_att 35.448410 loss_ctc 27.093430 loss_rnnt 15.933757 hw_loss 0.092830 lr 0.00099791 rank 6
2023-02-17 18:08:37,248 DEBUG TRAIN Batch 3/100 loss 45.042328 loss_att 57.558769 loss_ctc 75.867973 loss_rnnt 38.366665 hw_loss 0.116800 lr 0.00099777 rank 3
2023-02-17 18:08:37,248 DEBUG TRAIN Batch 3/100 loss 50.007652 loss_att 64.698639 loss_ctc 69.032913 loss_rnnt 44.488811 hw_loss 0.082396 lr 0.00099729 rank 0
2023-02-17 18:08:37,249 DEBUG TRAIN Batch 3/100 loss 50.729080 loss_att 65.446350 loss_ctc 78.960205 loss_rnnt 43.985237 hw_loss 0.067946 lr 0.00099684 rank 2
2023-02-17 18:08:37,251 DEBUG TRAIN Batch 3/100 loss 30.901146 loss_att 43.553913 loss_ctc 47.366192 loss_rnnt 26.135473 hw_loss 0.074586 lr 0.00099749 rank 5
2023-02-17 18:08:37,252 DEBUG TRAIN Batch 3/100 loss 41.626316 loss_att 45.299747 loss_ctc 49.851894 loss_rnnt 39.794682 hw_loss 0.000378 lr 0.00099743 rank 1
2023-02-17 18:08:37,261 DEBUG TRAIN Batch 3/100 loss 26.073462 loss_att 41.112129 loss_ctc 40.023514 loss_rnnt 21.205635 hw_loss 0.000157 lr 0.00099815 rank 7
2023-02-17 18:09:49,201 DEBUG TRAIN Batch 3/200 loss 36.176308 loss_att 46.152740 loss_ctc 50.357861 loss_rnnt 32.230663 hw_loss 0.111537 lr 0.00099593 rank 6
2023-02-17 18:09:49,201 DEBUG TRAIN Batch 3/200 loss 47.688755 loss_att 62.737823 loss_ctc 75.575714 loss_rnnt 40.943069 hw_loss 0.033015 lr 0.00099646 rank 4
2023-02-17 18:09:49,205 DEBUG TRAIN Batch 3/200 loss 54.521294 loss_att 60.386143 loss_ctc 70.562653 loss_rnnt 51.204079 hw_loss 0.010110 lr 0.00099551 rank 5
2023-02-17 18:09:49,206 DEBUG TRAIN Batch 3/200 loss 43.969479 loss_att 49.817665 loss_ctc 61.943428 loss_rnnt 40.352325 hw_loss 0.095609 lr 0.00099531 rank 0
2023-02-17 18:09:49,208 DEBUG TRAIN Batch 3/200 loss 28.873716 loss_att 36.824390 loss_ctc 45.809937 loss_rnnt 24.946281 hw_loss 0.148380 lr 0.00099486 rank 2
2023-02-17 18:09:49,208 DEBUG TRAIN Batch 3/200 loss 18.826481 loss_att 28.156687 loss_ctc 26.107016 loss_rnnt 15.949242 hw_loss 0.075864 lr 0.00099545 rank 1
2023-02-17 18:09:49,212 DEBUG TRAIN Batch 3/200 loss 39.824043 loss_att 47.342728 loss_ctc 56.363518 loss_rnnt 36.007698 hw_loss 0.201277 lr 0.00099616 rank 7
2023-02-17 18:09:49,214 DEBUG TRAIN Batch 3/200 loss 47.342106 loss_att 58.756634 loss_ctc 78.944878 loss_rnnt 40.806252 hw_loss 0.073587 lr 0.00099579 rank 3
2023-02-17 18:11:02,813 DEBUG TRAIN Batch 3/300 loss 25.664640 loss_att 29.235643 loss_ctc 37.567860 loss_rnnt 23.347889 hw_loss 0.028970 lr 0.00099449 rank 4
2023-02-17 18:11:02,818 DEBUG TRAIN Batch 3/300 loss 26.598803 loss_att 34.895741 loss_ctc 44.323856 loss_rnnt 22.511600 hw_loss 0.120883 lr 0.00099419 rank 7
2023-02-17 18:11:02,824 DEBUG TRAIN Batch 3/300 loss 29.038044 loss_att 40.877880 loss_ctc 45.037743 loss_rnnt 24.499226 hw_loss 0.070421 lr 0.00099335 rank 0
2023-02-17 18:11:02,827 DEBUG TRAIN Batch 3/300 loss 20.525015 loss_att 33.846912 loss_ctc 34.146950 loss_rnnt 16.034153 hw_loss 0.019175 lr 0.00099290 rank 2
2023-02-17 18:11:02,832 DEBUG TRAIN Batch 3/300 loss 45.697525 loss_att 51.625633 loss_ctc 60.756901 loss_rnnt 42.459709 hw_loss 0.083014 lr 0.00099354 rank 5
2023-02-17 18:11:02,834 DEBUG TRAIN Batch 3/300 loss 36.255753 loss_att 45.079388 loss_ctc 52.322273 loss_rnnt 32.303905 hw_loss 0.084223 lr 0.00099348 rank 1
2023-02-17 18:11:02,835 DEBUG TRAIN Batch 3/300 loss 35.937313 loss_att 38.738289 loss_ctc 52.758316 loss_rnnt 33.063351 hw_loss 0.133067 lr 0.00099396 rank 6
2023-02-17 18:11:02,879 DEBUG TRAIN Batch 3/300 loss 25.490065 loss_att 31.166868 loss_ctc 37.541954 loss_rnnt 22.674587 hw_loss 0.137249 lr 0.00099382 rank 3
2023-02-17 18:12:16,402 DEBUG TRAIN Batch 3/400 loss 55.950836 loss_att 65.551605 loss_ctc 83.422760 loss_rnnt 50.298218 hw_loss 0.130385 lr 0.00099159 rank 5
2023-02-17 18:12:16,401 DEBUG TRAIN Batch 3/400 loss 21.298611 loss_att 30.684242 loss_ctc 30.223049 loss_rnnt 18.217346 hw_loss 0.026650 lr 0.00099186 rank 3
2023-02-17 18:12:16,404 DEBUG TRAIN Batch 3/400 loss 23.524147 loss_att 31.084232 loss_ctc 40.365631 loss_rnnt 19.747351 hw_loss 0.036086 lr 0.00099223 rank 7
2023-02-17 18:12:16,405 DEBUG TRAIN Batch 3/400 loss 21.444061 loss_att 30.399704 loss_ctc 36.686657 loss_rnnt 17.578733 hw_loss 0.078480 lr 0.00099252 rank 4
2023-02-17 18:12:16,405 DEBUG TRAIN Batch 3/400 loss 24.980082 loss_att 31.865282 loss_ctc 44.101620 loss_rnnt 20.976789 hw_loss 0.143838 lr 0.00099200 rank 6
2023-02-17 18:12:16,407 DEBUG TRAIN Batch 3/400 loss 37.509171 loss_att 51.936558 loss_ctc 45.935459 loss_rnnt 33.452797 hw_loss 0.088855 lr 0.00099139 rank 0
2023-02-17 18:12:16,410 DEBUG TRAIN Batch 3/400 loss 48.871101 loss_att 56.624634 loss_ctc 72.585747 loss_rnnt 44.091652 hw_loss 0.125227 lr 0.00099094 rank 2
2023-02-17 18:12:16,410 DEBUG TRAIN Batch 3/400 loss 39.093609 loss_att 51.982239 loss_ctc 56.220394 loss_rnnt 34.207581 hw_loss 0.046372 lr 0.00099153 rank 1
2023-02-17 18:13:29,037 DEBUG TRAIN Batch 3/500 loss 40.941967 loss_att 46.624237 loss_ctc 54.374634 loss_rnnt 37.955944 hw_loss 0.109781 lr 0.00098958 rank 1
2023-02-17 18:13:29,039 DEBUG TRAIN Batch 3/500 loss 43.504986 loss_att 44.343014 loss_ctc 65.372414 loss_rnnt 40.357964 hw_loss 0.119550 lr 0.00098945 rank 0
2023-02-17 18:13:29,041 DEBUG TRAIN Batch 3/500 loss 24.790413 loss_att 36.516777 loss_ctc 44.845139 loss_rnnt 19.728790 hw_loss 0.079476 lr 0.00099028 rank 7
2023-02-17 18:13:29,041 DEBUG TRAIN Batch 3/500 loss 30.053207 loss_att 32.918808 loss_ctc 40.682011 loss_rnnt 28.058533 hw_loss 0.008213 lr 0.00099057 rank 4
2023-02-17 18:13:29,042 DEBUG TRAIN Batch 3/500 loss 18.963387 loss_att 27.542320 loss_ctc 37.186142 loss_rnnt 14.786764 hw_loss 0.058379 lr 0.00098991 rank 3
2023-02-17 18:13:29,042 DEBUG TRAIN Batch 3/500 loss 29.160715 loss_att 34.990993 loss_ctc 43.880157 loss_rnnt 25.983187 hw_loss 0.091649 lr 0.00098964 rank 5
2023-02-17 18:13:29,043 DEBUG TRAIN Batch 3/500 loss 31.642071 loss_att 34.435738 loss_ctc 48.958973 loss_rnnt 28.740211 hw_loss 0.064132 lr 0.00099005 rank 6
2023-02-17 18:13:29,090 DEBUG TRAIN Batch 3/500 loss 23.176880 loss_att 33.599434 loss_ctc 30.472700 loss_rnnt 20.103571 hw_loss 0.030038 lr 0.00098900 rank 2
2023-02-17 18:14:42,534 DEBUG TRAIN Batch 3/600 loss 19.362246 loss_att 22.388744 loss_ctc 26.818649 loss_rnnt 17.629000 hw_loss 0.250796 lr 0.00098864 rank 4
2023-02-17 18:14:42,535 DEBUG TRAIN Batch 3/600 loss 17.695127 loss_att 19.127623 loss_ctc 28.173292 loss_rnnt 15.900620 hw_loss 0.207970 lr 0.00098707 rank 2
2023-02-17 18:14:42,537 DEBUG TRAIN Batch 3/600 loss 26.358892 loss_att 32.669918 loss_ctc 36.967300 loss_rnnt 23.618317 hw_loss 0.119845 lr 0.00098812 rank 6
2023-02-17 18:14:42,537 DEBUG TRAIN Batch 3/600 loss 22.412542 loss_att 28.187771 loss_ctc 33.070522 loss_rnnt 19.748260 hw_loss 0.165324 lr 0.00098752 rank 0
2023-02-17 18:14:42,538 DEBUG TRAIN Batch 3/600 loss 26.028988 loss_att 30.278893 loss_ctc 41.196796 loss_rnnt 23.069202 hw_loss 0.163928 lr 0.00098771 rank 5
2023-02-17 18:14:42,542 DEBUG TRAIN Batch 3/600 loss 32.265587 loss_att 36.189804 loss_ctc 43.933578 loss_rnnt 29.884836 hw_loss 0.075322 lr 0.00098798 rank 3
2023-02-17 18:14:42,546 DEBUG TRAIN Batch 3/600 loss 20.084433 loss_att 27.324963 loss_ctc 36.056751 loss_rnnt 16.415390 hw_loss 0.171180 lr 0.00098765 rank 1
2023-02-17 18:14:42,548 DEBUG TRAIN Batch 3/600 loss 36.194744 loss_att 37.209229 loss_ctc 49.495247 loss_rnnt 34.090652 hw_loss 0.239607 lr 0.00098835 rank 7
2023-02-17 18:15:57,744 DEBUG TRAIN Batch 3/700 loss 34.632206 loss_att 44.006954 loss_ctc 57.836853 loss_rnnt 29.635809 hw_loss 0.051558 lr 0.00098671 rank 4
2023-02-17 18:15:57,752 DEBUG TRAIN Batch 3/700 loss 49.235680 loss_att 58.934425 loss_ctc 76.561325 loss_rnnt 43.636211 hw_loss 0.030567 lr 0.00098560 rank 0
2023-02-17 18:15:57,753 DEBUG TRAIN Batch 3/700 loss 34.728611 loss_att 42.611603 loss_ctc 47.475594 loss_rnnt 31.451763 hw_loss 0.001235 lr 0.00098619 rank 6
2023-02-17 18:15:57,754 DEBUG TRAIN Batch 3/700 loss 33.143387 loss_att 47.409203 loss_ctc 47.439163 loss_rnnt 28.367451 hw_loss 0.031252 lr 0.00098579 rank 5
2023-02-17 18:15:57,753 DEBUG TRAIN Batch 3/700 loss 26.123142 loss_att 43.635914 loss_ctc 41.037971 loss_rnnt 20.572735 hw_loss 0.111016 lr 0.00098642 rank 7
2023-02-17 18:15:57,755 DEBUG TRAIN Batch 3/700 loss 17.782003 loss_att 28.263313 loss_ctc 29.921545 loss_rnnt 13.983065 hw_loss 0.157629 lr 0.00098606 rank 3
2023-02-17 18:15:57,787 DEBUG TRAIN Batch 3/700 loss 34.063335 loss_att 49.735081 loss_ctc 48.668282 loss_rnnt 28.944990 hw_loss 0.068755 lr 0.00098573 rank 1
2023-02-17 18:15:57,794 DEBUG TRAIN Batch 3/700 loss 26.471699 loss_att 43.608803 loss_ctc 48.696850 loss_rnnt 20.052395 hw_loss 0.053493 lr 0.00098516 rank 2
2023-02-17 18:17:10,581 DEBUG TRAIN Batch 3/800 loss 40.281170 loss_att 44.637421 loss_ctc 59.441601 loss_rnnt 36.855034 hw_loss 0.000309 lr 0.00098428 rank 6
2023-02-17 18:17:10,583 DEBUG TRAIN Batch 3/800 loss 54.021618 loss_att 59.303276 loss_ctc 81.378899 loss_rnnt 49.278961 hw_loss 0.072532 lr 0.00098369 rank 0
2023-02-17 18:17:10,586 DEBUG TRAIN Batch 3/800 loss 34.650879 loss_att 44.040852 loss_ctc 61.320953 loss_rnnt 29.180883 hw_loss 0.067487 lr 0.00098479 rank 4
2023-02-17 18:17:10,587 DEBUG TRAIN Batch 3/800 loss 48.152065 loss_att 65.890991 loss_ctc 67.764328 loss_rnnt 41.970325 hw_loss 0.035603 lr 0.00098382 rank 1
2023-02-17 18:17:10,587 DEBUG TRAIN Batch 3/800 loss 21.477020 loss_att 36.168320 loss_ctc 32.496525 loss_rnnt 17.043119 hw_loss 0.049446 lr 0.00098325 rank 2
2023-02-17 18:17:10,589 DEBUG TRAIN Batch 3/800 loss 36.104893 loss_att 47.845215 loss_ctc 52.945309 loss_rnnt 31.511330 hw_loss 0.000204 lr 0.00098451 rank 7
2023-02-17 18:17:10,588 DEBUG TRAIN Batch 3/800 loss 67.080101 loss_att 70.387756 loss_ctc 92.943748 loss_rnnt 62.927864 hw_loss 0.079163 lr 0.00098388 rank 5
2023-02-17 18:17:10,590 DEBUG TRAIN Batch 3/800 loss 21.650591 loss_att 31.628126 loss_ctc 33.162167 loss_rnnt 18.066692 hw_loss 0.100339 lr 0.00098415 rank 3
2023-02-17 18:18:23,576 DEBUG TRAIN Batch 3/900 loss 62.799358 loss_att 76.609543 loss_ctc 95.160812 loss_rnnt 55.681854 hw_loss 0.076134 lr 0.00098289 rank 4
2023-02-17 18:18:23,580 DEBUG TRAIN Batch 3/900 loss 24.135691 loss_att 37.770802 loss_ctc 38.898731 loss_rnnt 19.439438 hw_loss 0.001545 lr 0.00098238 rank 6
2023-02-17 18:18:23,580 DEBUG TRAIN Batch 3/900 loss 61.329063 loss_att 72.055046 loss_ctc 89.494049 loss_rnnt 55.413113 hw_loss 0.028912 lr 0.00098179 rank 0
2023-02-17 18:18:23,582 DEBUG TRAIN Batch 3/900 loss 20.657169 loss_att 24.481417 loss_ctc 32.274559 loss_rnnt 18.239302 hw_loss 0.195062 lr 0.00098260 rank 7
2023-02-17 18:18:23,591 DEBUG TRAIN Batch 3/900 loss 39.955597 loss_att 51.633152 loss_ctc 55.344658 loss_rnnt 35.523750 hw_loss 0.083370 lr 0.00098198 rank 5
2023-02-17 18:18:23,606 DEBUG TRAIN Batch 3/900 loss 62.258945 loss_att 71.157288 loss_ctc 88.563217 loss_rnnt 56.971451 hw_loss 0.001098 lr 0.00098192 rank 1
2023-02-17 18:18:23,613 DEBUG TRAIN Batch 3/900 loss 42.732895 loss_att 50.886757 loss_ctc 62.470505 loss_rnnt 38.436562 hw_loss 0.063523 lr 0.00098224 rank 3
2023-02-17 18:18:23,634 DEBUG TRAIN Batch 3/900 loss 35.738621 loss_att 44.947330 loss_ctc 54.632725 loss_rnnt 31.361044 hw_loss 0.031169 lr 0.00098135 rank 2
2023-02-17 18:19:37,081 DEBUG TRAIN Batch 3/1000 loss 32.158295 loss_att 41.128639 loss_ctc 52.431431 loss_rnnt 27.595629 hw_loss 0.122834 lr 0.00098100 rank 4
2023-02-17 18:19:37,088 DEBUG TRAIN Batch 3/1000 loss 51.121326 loss_att 66.769562 loss_ctc 77.120346 loss_rnnt 44.492828 hw_loss 0.060591 lr 0.00098035 rank 3
2023-02-17 18:19:37,088 DEBUG TRAIN Batch 3/1000 loss 66.517189 loss_att 76.667023 loss_ctc 91.284119 loss_rnnt 61.179272 hw_loss 0.010683 lr 0.00097947 rank 2
2023-02-17 18:19:37,102 DEBUG TRAIN Batch 3/1000 loss 42.713943 loss_att 47.154583 loss_ctc 58.984741 loss_rnnt 39.616653 hw_loss 0.074471 lr 0.00097990 rank 0
2023-02-17 18:19:37,102 DEBUG TRAIN Batch 3/1000 loss 29.904840 loss_att 37.683357 loss_ctc 48.066574 loss_rnnt 25.900120 hw_loss 0.051468 lr 0.00098049 rank 6
2023-02-17 18:19:37,107 DEBUG TRAIN Batch 3/1000 loss 36.611778 loss_att 44.878510 loss_ctc 51.336994 loss_rnnt 32.955711 hw_loss 0.073798 lr 0.00098009 rank 5
2023-02-17 18:19:37,107 DEBUG TRAIN Batch 3/1000 loss 28.310350 loss_att 38.200508 loss_ctc 44.277168 loss_rnnt 24.197432 hw_loss 0.011213 lr 0.00098071 rank 7
2023-02-17 18:19:37,144 DEBUG TRAIN Batch 3/1000 loss 46.238541 loss_att 58.190453 loss_ctc 65.110336 loss_rnnt 41.314629 hw_loss 0.032411 lr 0.00098003 rank 1
2023-02-17 18:20:51,978 DEBUG TRAIN Batch 3/1100 loss 22.163319 loss_att 26.887814 loss_ctc 34.187111 loss_rnnt 19.614784 hw_loss 0.000866 lr 0.00097816 rank 1
2023-02-17 18:20:51,981 DEBUG TRAIN Batch 3/1100 loss 30.712561 loss_att 39.736561 loss_ctc 48.302628 loss_rnnt 26.561581 hw_loss 0.001567 lr 0.00097911 rank 4
2023-02-17 18:20:51,986 DEBUG TRAIN Batch 3/1100 loss 32.615395 loss_att 41.061584 loss_ctc 49.827568 loss_rnnt 28.608313 hw_loss 0.042912 lr 0.00097803 rank 0
2023-02-17 18:20:51,987 DEBUG TRAIN Batch 3/1100 loss 24.628464 loss_att 37.073055 loss_ctc 39.070518 loss_rnnt 20.129578 hw_loss 0.158177 lr 0.00097861 rank 6
2023-02-17 18:20:51,990 DEBUG TRAIN Batch 3/1100 loss 38.036892 loss_att 49.044750 loss_ctc 59.483772 loss_rnnt 32.949940 hw_loss 0.048359 lr 0.00097760 rank 2
2023-02-17 18:20:51,993 DEBUG TRAIN Batch 3/1100 loss 38.453659 loss_att 46.994492 loss_ctc 61.532860 loss_rnnt 33.644432 hw_loss 0.044692 lr 0.00097821 rank 5
2023-02-17 18:20:51,995 DEBUG TRAIN Batch 3/1100 loss 33.970867 loss_att 38.829559 loss_ctc 50.253002 loss_rnnt 30.766117 hw_loss 0.116359 lr 0.00097883 rank 7
2023-02-17 18:20:51,997 DEBUG TRAIN Batch 3/1100 loss 26.976160 loss_att 31.674507 loss_ctc 47.090858 loss_rnnt 23.297335 hw_loss 0.107240 lr 0.00097848 rank 3
2023-02-17 18:22:05,009 DEBUG TRAIN Batch 3/1200 loss 48.132988 loss_att 54.898865 loss_ctc 73.217949 loss_rnnt 43.427956 hw_loss 0.013487 lr 0.00097674 rank 6
2023-02-17 18:22:05,010 DEBUG TRAIN Batch 3/1200 loss 18.440840 loss_att 23.696041 loss_ctc 23.743698 loss_rnnt 16.658800 hw_loss 0.044908 lr 0.00097696 rank 7
2023-02-17 18:22:05,010 DEBUG TRAIN Batch 3/1200 loss 36.949802 loss_att 38.893303 loss_ctc 53.533539 loss_rnnt 34.280224 hw_loss 0.130721 lr 0.00097629 rank 1
2023-02-17 18:22:05,011 DEBUG TRAIN Batch 3/1200 loss 25.051533 loss_att 29.580297 loss_ctc 37.032238 loss_rnnt 22.471462 hw_loss 0.144172 lr 0.00097724 rank 4
2023-02-17 18:22:05,013 DEBUG TRAIN Batch 3/1200 loss 18.680992 loss_att 25.046589 loss_ctc 29.156372 loss_rnnt 16.001028 hw_loss 0.018986 lr 0.00097573 rank 2
2023-02-17 18:22:05,013 DEBUG TRAIN Batch 3/1200 loss 24.315037 loss_att 29.854319 loss_ctc 38.770630 loss_rnnt 21.246269 hw_loss 0.062809 lr 0.00097616 rank 0
2023-02-17 18:22:05,016 DEBUG TRAIN Batch 3/1200 loss 23.201466 loss_att 28.368982 loss_ctc 36.591602 loss_rnnt 20.350368 hw_loss 0.060453 lr 0.00097661 rank 3
2023-02-17 18:22:05,060 DEBUG TRAIN Batch 3/1200 loss 22.533176 loss_att 26.620605 loss_ctc 33.267181 loss_rnnt 20.221218 hw_loss 0.118631 lr 0.00097635 rank 5
2023-02-17 18:23:18,090 DEBUG TRAIN Batch 3/1300 loss 33.448730 loss_att 39.190079 loss_ctc 41.504230 loss_rnnt 31.226294 hw_loss 0.000193 lr 0.00097475 rank 3
2023-02-17 18:23:18,109 DEBUG TRAIN Batch 3/1300 loss 21.978003 loss_att 25.407322 loss_ctc 27.763609 loss_rnnt 20.432005 hw_loss 0.166348 lr 0.00097510 rank 7
2023-02-17 18:23:18,110 DEBUG TRAIN Batch 3/1300 loss 49.056702 loss_att 58.122829 loss_ctc 78.120758 loss_rnnt 43.326920 hw_loss 0.077525 lr 0.00097431 rank 0
2023-02-17 18:23:18,111 DEBUG TRAIN Batch 3/1300 loss 22.039223 loss_att 32.397423 loss_ctc 33.956371 loss_rnnt 18.378544 hw_loss 0.000161 lr 0.00097443 rank 1
2023-02-17 18:23:18,112 DEBUG TRAIN Batch 3/1300 loss 22.312845 loss_att 30.638407 loss_ctc 34.072609 loss_rnnt 19.040428 hw_loss 0.073755 lr 0.00097488 rank 6
2023-02-17 18:23:18,112 DEBUG TRAIN Batch 3/1300 loss 24.651384 loss_att 36.184631 loss_ctc 39.259041 loss_rnnt 20.367550 hw_loss 0.055306 lr 0.00097538 rank 4
2023-02-17 18:23:18,112 DEBUG TRAIN Batch 3/1300 loss 34.261368 loss_att 49.518269 loss_ctc 50.298149 loss_rnnt 29.046919 hw_loss 0.046558 lr 0.00097449 rank 5
2023-02-17 18:23:18,113 DEBUG TRAIN Batch 3/1300 loss 60.073238 loss_att 65.897736 loss_ctc 73.900139 loss_rnnt 57.055122 hw_loss 0.018057 lr 0.00097388 rank 2
2023-02-17 18:24:32,835 DEBUG TRAIN Batch 3/1400 loss 23.744320 loss_att 31.084831 loss_ctc 36.749046 loss_rnnt 20.482094 hw_loss 0.112800 lr 0.00097259 rank 1
2023-02-17 18:24:32,845 DEBUG TRAIN Batch 3/1400 loss 40.924568 loss_att 52.948162 loss_ctc 55.926647 loss_rnnt 36.465424 hw_loss 0.101527 lr 0.00097353 rank 4
2023-02-17 18:24:32,849 DEBUG TRAIN Batch 3/1400 loss 26.900238 loss_att 38.501236 loss_ctc 41.394444 loss_rnnt 22.595230 hw_loss 0.097960 lr 0.00097204 rank 2
2023-02-17 18:24:32,850 DEBUG TRAIN Batch 3/1400 loss 30.299089 loss_att 41.323967 loss_ctc 50.143078 loss_rnnt 25.426563 hw_loss 0.040656 lr 0.00097246 rank 0
2023-02-17 18:24:32,850 DEBUG TRAIN Batch 3/1400 loss 38.451305 loss_att 51.009022 loss_ctc 64.449249 loss_rnnt 32.420696 hw_loss 0.098762 lr 0.00097325 rank 7
2023-02-17 18:24:32,851 DEBUG TRAIN Batch 3/1400 loss 15.812385 loss_att 21.522186 loss_ctc 24.045696 loss_rnnt 13.544524 hw_loss 0.052735 lr 0.00097303 rank 6
2023-02-17 18:24:32,851 DEBUG TRAIN Batch 3/1400 loss 40.228176 loss_att 55.152344 loss_ctc 55.394562 loss_rnnt 35.201687 hw_loss 0.036518 lr 0.00097290 rank 3
2023-02-17 18:24:32,879 DEBUG TRAIN Batch 3/1400 loss 24.443930 loss_att 35.091537 loss_ctc 35.440819 loss_rnnt 20.827091 hw_loss 0.039495 lr 0.00097264 rank 5
2023-02-17 18:25:46,771 DEBUG TRAIN Batch 3/1500 loss 40.569187 loss_att 46.413921 loss_ctc 61.776939 loss_rnnt 36.522717 hw_loss 0.093420 lr 0.00097075 rank 1
2023-02-17 18:25:46,772 DEBUG TRAIN Batch 3/1500 loss 33.095646 loss_att 41.534348 loss_ctc 46.535095 loss_rnnt 29.550400 hw_loss 0.122957 lr 0.00097169 rank 4
2023-02-17 18:25:46,775 DEBUG TRAIN Batch 3/1500 loss 52.926704 loss_att 67.231270 loss_ctc 86.254112 loss_rnnt 45.579979 hw_loss 0.079047 lr 0.00097081 rank 5
2023-02-17 18:25:46,774 DEBUG TRAIN Batch 3/1500 loss 29.813953 loss_att 36.996811 loss_ctc 43.100174 loss_rnnt 26.571255 hw_loss 0.064933 lr 0.00097063 rank 0
2023-02-17 18:25:46,777 DEBUG TRAIN Batch 3/1500 loss 18.503885 loss_att 26.223614 loss_ctc 30.088102 loss_rnnt 15.414171 hw_loss 0.002258 lr 0.00097021 rank 2
2023-02-17 18:25:46,777 DEBUG TRAIN Batch 3/1500 loss 35.375668 loss_att 42.595402 loss_ctc 57.346542 loss_rnnt 30.950275 hw_loss 0.097491 lr 0.00097107 rank 3
2023-02-17 18:25:46,779 DEBUG TRAIN Batch 3/1500 loss 21.797920 loss_att 30.764380 loss_ctc 39.014248 loss_rnnt 17.683495 hw_loss 0.048045 lr 0.00097141 rank 7
2023-02-17 18:25:46,779 DEBUG TRAIN Batch 3/1500 loss 25.154856 loss_att 32.069145 loss_ctc 39.225792 loss_rnnt 21.894323 hw_loss 0.002911 lr 0.00097119 rank 6
2023-02-17 18:26:58,988 DEBUG TRAIN Batch 3/1600 loss 26.465965 loss_att 33.540543 loss_ctc 44.348133 loss_rnnt 22.599953 hw_loss 0.125264 lr 0.00096937 rank 6
2023-02-17 18:26:58,990 DEBUG TRAIN Batch 3/1600 loss 36.720547 loss_att 48.914139 loss_ctc 49.264450 loss_rnnt 32.569969 hw_loss 0.073755 lr 0.00096880 rank 0
2023-02-17 18:26:58,996 DEBUG TRAIN Batch 3/1600 loss 39.618336 loss_att 46.962456 loss_ctc 61.250515 loss_rnnt 35.211258 hw_loss 0.101182 lr 0.00096839 rank 2
2023-02-17 18:26:58,996 DEBUG TRAIN Batch 3/1600 loss 21.651443 loss_att 29.109295 loss_ctc 32.220985 loss_rnnt 18.732754 hw_loss 0.033466 lr 0.00096986 rank 4
2023-02-17 18:26:58,997 DEBUG TRAIN Batch 3/1600 loss 39.129429 loss_att 45.481079 loss_ctc 56.177856 loss_rnnt 35.554039 hw_loss 0.059885 lr 0.00096924 rank 3
2023-02-17 18:26:59,001 DEBUG TRAIN Batch 3/1600 loss 32.821041 loss_att 50.606552 loss_ctc 53.400036 loss_rnnt 26.497873 hw_loss 0.041618 lr 0.00096959 rank 7
2023-02-17 18:26:59,007 DEBUG TRAIN Batch 3/1600 loss 28.374302 loss_att 30.810123 loss_ctc 34.957588 loss_rnnt 26.970577 hw_loss 0.072732 lr 0.00096898 rank 5
2023-02-17 18:26:59,043 DEBUG TRAIN Batch 3/1600 loss 32.787537 loss_att 43.546936 loss_ctc 47.522831 loss_rnnt 28.622770 hw_loss 0.090344 lr 0.00096893 rank 1
2023-02-17 18:28:12,455 DEBUG TRAIN Batch 3/1700 loss 24.824732 loss_att 34.510727 loss_ctc 37.294270 loss_rnnt 21.178642 hw_loss 0.086783 lr 0.00096699 rank 0
2023-02-17 18:28:12,458 DEBUG TRAIN Batch 3/1700 loss 28.295881 loss_att 33.109520 loss_ctc 42.980522 loss_rnnt 25.326702 hw_loss 0.090937 lr 0.00096712 rank 1
2023-02-17 18:28:12,459 DEBUG TRAIN Batch 3/1700 loss 29.035664 loss_att 38.381664 loss_ctc 46.161381 loss_rnnt 24.827248 hw_loss 0.104598 lr 0.00096755 rank 6
2023-02-17 18:28:12,459 DEBUG TRAIN Batch 3/1700 loss 28.505062 loss_att 34.354156 loss_ctc 40.477234 loss_rnnt 25.701984 hw_loss 0.069313 lr 0.00096657 rank 2
2023-02-17 18:28:12,459 DEBUG TRAIN Batch 3/1700 loss 39.317139 loss_att 46.100014 loss_ctc 57.186943 loss_rnnt 35.577728 hw_loss 0.000375 lr 0.00096742 rank 3
2023-02-17 18:28:12,459 DEBUG TRAIN Batch 3/1700 loss 31.578650 loss_att 41.897221 loss_ctc 51.049500 loss_rnnt 26.903841 hw_loss 0.028084 lr 0.00096777 rank 7
2023-02-17 18:28:12,463 DEBUG TRAIN Batch 3/1700 loss 50.043438 loss_att 57.662102 loss_ctc 82.670418 loss_rnnt 44.157234 hw_loss 0.022883 lr 0.00096804 rank 4
2023-02-17 18:28:12,465 DEBUG TRAIN Batch 3/1700 loss 26.357346 loss_att 36.009518 loss_ctc 40.159679 loss_rnnt 22.548290 hw_loss 0.071830 lr 0.00096717 rank 5
2023-02-17 18:29:28,664 DEBUG TRAIN Batch 3/1800 loss 30.732355 loss_att 37.930176 loss_ctc 54.212837 loss_rnnt 26.078676 hw_loss 0.156339 lr 0.00096574 rank 6
2023-02-17 18:29:28,664 DEBUG TRAIN Batch 3/1800 loss 33.163311 loss_att 39.743141 loss_ctc 54.572468 loss_rnnt 28.930769 hw_loss 0.116293 lr 0.00096519 rank 0
2023-02-17 18:29:28,664 DEBUG TRAIN Batch 3/1800 loss 22.430553 loss_att 29.970169 loss_ctc 40.534645 loss_rnnt 18.472929 hw_loss 0.067170 lr 0.00096623 rank 4
2023-02-17 18:29:28,668 DEBUG TRAIN Batch 3/1800 loss 17.092670 loss_att 21.664150 loss_ctc 27.379974 loss_rnnt 14.775908 hw_loss 0.057795 lr 0.00096537 rank 5
2023-02-17 18:29:28,668 DEBUG TRAIN Batch 3/1800 loss 33.242229 loss_att 40.680283 loss_ctc 56.709785 loss_rnnt 28.601171 hw_loss 0.045828 lr 0.00096562 rank 3
2023-02-17 18:29:28,669 DEBUG TRAIN Batch 3/1800 loss 23.666807 loss_att 30.538441 loss_ctc 38.604614 loss_rnnt 20.256372 hw_loss 0.083248 lr 0.00096477 rank 2
2023-02-17 18:29:28,670 DEBUG TRAIN Batch 3/1800 loss 22.053240 loss_att 26.608393 loss_ctc 34.667801 loss_rnnt 19.447718 hw_loss 0.023530 lr 0.00096596 rank 7
2023-02-17 18:29:28,719 DEBUG TRAIN Batch 3/1800 loss 16.553711 loss_att 23.090031 loss_ctc 24.544397 loss_rnnt 14.132906 hw_loss 0.090216 lr 0.00096531 rank 1
2023-02-17 18:30:43,432 DEBUG TRAIN Batch 3/1900 loss 15.058065 loss_att 15.352165 loss_ctc 17.884254 loss_rnnt 14.513396 hw_loss 0.204421 lr 0.00096357 rank 5
2023-02-17 18:30:43,443 DEBUG TRAIN Batch 3/1900 loss 40.245430 loss_att 53.252216 loss_ctc 63.299873 loss_rnnt 34.541588 hw_loss 0.053550 lr 0.00096298 rank 2
2023-02-17 18:30:43,445 DEBUG TRAIN Batch 3/1900 loss 65.779915 loss_att 80.674553 loss_ctc 96.364426 loss_rnnt 58.705822 hw_loss 0.032313 lr 0.00096352 rank 1
2023-02-17 18:30:43,448 DEBUG TRAIN Batch 3/1900 loss 19.170664 loss_att 21.645893 loss_ctc 25.864029 loss_rnnt 17.698694 hw_loss 0.158389 lr 0.00096382 rank 3
2023-02-17 18:30:43,448 DEBUG TRAIN Batch 3/1900 loss 26.935843 loss_att 30.573376 loss_ctc 40.340183 loss_rnnt 24.376015 hw_loss 0.084519 lr 0.00096443 rank 4
2023-02-17 18:30:43,451 DEBUG TRAIN Batch 3/1900 loss 14.817406 loss_att 16.418520 loss_ctc 22.114712 loss_rnnt 13.412324 hw_loss 0.209783 lr 0.00096339 rank 0
2023-02-17 18:30:43,454 DEBUG TRAIN Batch 3/1900 loss 25.672709 loss_att 26.572098 loss_ctc 32.393074 loss_rnnt 24.549633 hw_loss 0.088404 lr 0.00096395 rank 6
2023-02-17 18:30:43,454 DEBUG TRAIN Batch 3/1900 loss 14.264531 loss_att 19.547625 loss_ctc 22.121262 loss_rnnt 12.077029 hw_loss 0.156223 lr 0.00096416 rank 7
2023-02-17 18:31:56,687 DEBUG TRAIN Batch 3/2000 loss 24.360329 loss_att 31.571419 loss_ctc 43.846039 loss_rnnt 20.279463 hw_loss 0.076035 lr 0.00096216 rank 6
2023-02-17 18:31:56,702 DEBUG TRAIN Batch 3/2000 loss 40.601284 loss_att 50.794804 loss_ctc 58.742840 loss_rnnt 36.125713 hw_loss 0.033743 lr 0.00096264 rank 4
2023-02-17 18:31:56,703 DEBUG TRAIN Batch 3/2000 loss 34.181664 loss_att 47.849831 loss_ctc 60.498352 loss_rnnt 27.938950 hw_loss 0.000351 lr 0.00096173 rank 1
2023-02-17 18:31:56,704 DEBUG TRAIN Batch 3/2000 loss 48.511856 loss_att 61.996841 loss_ctc 76.750267 loss_rnnt 42.022690 hw_loss 0.050717 lr 0.00096179 rank 5
2023-02-17 18:31:56,705 DEBUG TRAIN Batch 3/2000 loss 35.330147 loss_att 42.903946 loss_ctc 53.224541 loss_rnnt 31.380325 hw_loss 0.092140 lr 0.00096161 rank 0
2023-02-17 18:31:56,705 DEBUG TRAIN Batch 3/2000 loss 20.719353 loss_att 27.392754 loss_ctc 37.610863 loss_rnnt 17.071396 hw_loss 0.114517 lr 0.00096238 rank 7
2023-02-17 18:31:56,707 DEBUG TRAIN Batch 3/2000 loss 46.917732 loss_att 56.715004 loss_ctc 63.393219 loss_rnnt 42.695400 hw_loss 0.124026 lr 0.00096204 rank 3
2023-02-17 18:31:56,753 DEBUG TRAIN Batch 3/2000 loss 18.721945 loss_att 29.406265 loss_ctc 29.271475 loss_rnnt 15.153705 hw_loss 0.046449 lr 0.00096120 rank 2
2023-02-17 18:33:12,263 DEBUG TRAIN Batch 3/2100 loss 39.937160 loss_att 46.892418 loss_ctc 53.703827 loss_rnnt 36.710373 hw_loss 0.000338 lr 0.00096086 rank 4
2023-02-17 18:33:12,265 DEBUG TRAIN Batch 3/2100 loss 30.760426 loss_att 40.245094 loss_ctc 46.651108 loss_rnnt 26.687336 hw_loss 0.107616 lr 0.00096001 rank 5
2023-02-17 18:33:12,266 DEBUG TRAIN Batch 3/2100 loss 36.340237 loss_att 48.793633 loss_ctc 58.261414 loss_rnnt 30.894232 hw_loss 0.060940 lr 0.00096026 rank 3
2023-02-17 18:33:12,266 DEBUG TRAIN Batch 3/2100 loss 39.376080 loss_att 43.383057 loss_ctc 62.931297 loss_rnnt 35.371151 hw_loss 0.117830 lr 0.00096038 rank 6
2023-02-17 18:33:12,270 DEBUG TRAIN Batch 3/2100 loss 21.470049 loss_att 28.333050 loss_ctc 34.024624 loss_rnnt 18.382887 hw_loss 0.076159 lr 0.00095996 rank 1
2023-02-17 18:33:12,273 DEBUG TRAIN Batch 3/2100 loss 62.038067 loss_att 69.371567 loss_ctc 89.144485 loss_rnnt 56.887100 hw_loss 0.131398 lr 0.00095943 rank 2
2023-02-17 18:33:12,276 DEBUG TRAIN Batch 3/2100 loss 39.928493 loss_att 45.593842 loss_ctc 57.020504 loss_rnnt 36.503757 hw_loss 0.023866 lr 0.00095984 rank 0
2023-02-17 18:33:12,315 DEBUG TRAIN Batch 3/2100 loss 21.536257 loss_att 31.571722 loss_ctc 36.025307 loss_rnnt 17.577076 hw_loss 0.037906 lr 0.00096060 rank 7
2023-02-17 18:34:32,571 DEBUG TRAIN Batch 3/2200 loss 31.605675 loss_att 34.921867 loss_ctc 43.940670 loss_rnnt 29.246197 hw_loss 0.096702 lr 0.00095909 rank 4
2023-02-17 18:34:32,572 DEBUG TRAIN Batch 3/2200 loss 31.151571 loss_att 42.448479 loss_ctc 48.677399 loss_rnnt 26.532963 hw_loss 0.042090 lr 0.00095807 rank 0
2023-02-17 18:34:32,573 DEBUG TRAIN Batch 3/2200 loss 46.768242 loss_att 52.995567 loss_ctc 75.258347 loss_rnnt 41.689774 hw_loss 0.064358 lr 0.00095862 rank 6
2023-02-17 18:34:32,577 DEBUG TRAIN Batch 3/2200 loss 47.164543 loss_att 59.771111 loss_ctc 71.769913 loss_rnnt 41.295628 hw_loss 0.125403 lr 0.00095883 rank 7
2023-02-17 18:34:32,578 DEBUG TRAIN Batch 3/2200 loss 30.204260 loss_att 37.551567 loss_ctc 49.226357 loss_rnnt 26.194622 hw_loss 0.007311 lr 0.00095820 rank 1
2023-02-17 18:34:32,580 DEBUG TRAIN Batch 3/2200 loss 28.842024 loss_att 37.044853 loss_ctc 41.588570 loss_rnnt 25.460264 hw_loss 0.078101 lr 0.00095849 rank 3
2023-02-17 18:34:32,584 DEBUG TRAIN Batch 3/2200 loss 48.531487 loss_att 58.035378 loss_ctc 69.001099 loss_rnnt 43.877914 hw_loss 0.044084 lr 0.00095767 rank 2
2023-02-17 18:34:32,633 DEBUG TRAIN Batch 3/2200 loss 12.987192 loss_att 24.687937 loss_ctc 22.286528 loss_rnnt 9.406829 hw_loss 0.000568 lr 0.00095825 rank 5
2023-02-17 18:35:45,179 DEBUG TRAIN Batch 3/2300 loss 31.149284 loss_att 36.926571 loss_ctc 55.493187 loss_rnnt 26.730236 hw_loss 0.033259 lr 0.00095686 rank 6
2023-02-17 18:35:45,181 DEBUG TRAIN Batch 3/2300 loss 52.047913 loss_att 55.553230 loss_ctc 84.471481 loss_rnnt 47.006416 hw_loss 0.032415 lr 0.00095733 rank 4
2023-02-17 18:35:45,182 DEBUG TRAIN Batch 3/2300 loss 26.040215 loss_att 31.437042 loss_ctc 42.615261 loss_rnnt 22.713526 hw_loss 0.069965 lr 0.00095592 rank 2
2023-02-17 18:35:45,183 DEBUG TRAIN Batch 3/2300 loss 47.473232 loss_att 54.057549 loss_ctc 72.281631 loss_rnnt 42.818298 hw_loss 0.056775 lr 0.00095707 rank 7
2023-02-17 18:35:45,185 DEBUG TRAIN Batch 3/2300 loss 53.773403 loss_att 65.316040 loss_ctc 84.144279 loss_rnnt 47.369377 hw_loss 0.086342 lr 0.00095649 rank 5
2023-02-17 18:35:45,185 DEBUG TRAIN Batch 3/2300 loss 67.140144 loss_att 68.191132 loss_ctc 78.962280 loss_rnnt 65.316216 hw_loss 0.070219 lr 0.00095632 rank 0
2023-02-17 18:35:45,187 DEBUG TRAIN Batch 3/2300 loss 23.453680 loss_att 28.360430 loss_ctc 31.971134 loss_rnnt 21.293671 hw_loss 0.080625 lr 0.00095674 rank 3
2023-02-17 18:35:45,188 DEBUG TRAIN Batch 3/2300 loss 33.357563 loss_att 45.230350 loss_ctc 53.431213 loss_rnnt 28.236567 hw_loss 0.131162 lr 0.00095644 rank 1
2023-02-17 18:36:57,943 DEBUG TRAIN Batch 3/2400 loss 18.589752 loss_att 27.874020 loss_ctc 38.482666 loss_rnnt 14.046436 hw_loss 0.063886 lr 0.00095558 rank 4
2023-02-17 18:36:57,948 DEBUG TRAIN Batch 3/2400 loss 22.148874 loss_att 29.611393 loss_ctc 31.991337 loss_rnnt 19.320320 hw_loss 0.044480 lr 0.00095457 rank 0
2023-02-17 18:36:57,948 DEBUG TRAIN Batch 3/2400 loss 32.406036 loss_att 37.922249 loss_ctc 47.002861 loss_rnnt 29.321085 hw_loss 0.066494 lr 0.00095511 rank 6
2023-02-17 18:36:57,950 DEBUG TRAIN Batch 3/2400 loss 42.003452 loss_att 48.401646 loss_ctc 67.251160 loss_rnnt 37.333641 hw_loss 0.044646 lr 0.00095475 rank 5
2023-02-17 18:36:57,951 DEBUG TRAIN Batch 3/2400 loss 31.591774 loss_att 38.852364 loss_ctc 46.315746 loss_rnnt 28.167814 hw_loss 0.016214 lr 0.00095417 rank 2
2023-02-17 18:36:57,955 DEBUG TRAIN Batch 3/2400 loss 19.172819 loss_att 23.546947 loss_ctc 28.622011 loss_rnnt 17.008081 hw_loss 0.056287 lr 0.00095532 rank 7
2023-02-17 18:36:57,958 DEBUG TRAIN Batch 3/2400 loss 25.930008 loss_att 29.601482 loss_ctc 39.590469 loss_rnnt 23.354876 hw_loss 0.036448 lr 0.00095499 rank 3
2023-02-17 18:36:57,968 DEBUG TRAIN Batch 3/2400 loss 38.539528 loss_att 45.614296 loss_ctc 56.766487 loss_rnnt 34.636627 hw_loss 0.108160 lr 0.00095470 rank 1
2023-02-17 18:38:13,317 DEBUG TRAIN Batch 3/2500 loss 16.631632 loss_att 25.009850 loss_ctc 24.530146 loss_rnnt 13.816913 hw_loss 0.161136 lr 0.00095384 rank 4
2023-02-17 18:38:13,319 DEBUG TRAIN Batch 3/2500 loss 27.530972 loss_att 31.254290 loss_ctc 45.891785 loss_rnnt 24.291685 hw_loss 0.087213 lr 0.00095338 rank 6
2023-02-17 18:38:13,319 DEBUG TRAIN Batch 3/2500 loss 21.286209 loss_att 23.111246 loss_ctc 32.425880 loss_rnnt 19.401115 hw_loss 0.065245 lr 0.00095325 rank 3
2023-02-17 18:38:13,322 DEBUG TRAIN Batch 3/2500 loss 15.390390 loss_att 20.903923 loss_ctc 24.698921 loss_rnnt 12.941075 hw_loss 0.197757 lr 0.00095358 rank 7
2023-02-17 18:38:13,323 DEBUG TRAIN Batch 3/2500 loss 18.388845 loss_att 21.000088 loss_ctc 27.457829 loss_rnnt 16.593477 hw_loss 0.119853 lr 0.00095296 rank 1
2023-02-17 18:38:13,324 DEBUG TRAIN Batch 3/2500 loss 35.440941 loss_att 39.213860 loss_ctc 43.705151 loss_rnnt 33.549938 hw_loss 0.064732 lr 0.00095284 rank 0
2023-02-17 18:38:13,329 DEBUG TRAIN Batch 3/2500 loss 28.996391 loss_att 33.850624 loss_ctc 45.875195 loss_rnnt 25.735769 hw_loss 0.073625 lr 0.00095301 rank 5
2023-02-17 18:38:13,373 DEBUG TRAIN Batch 3/2500 loss 19.736702 loss_att 31.924595 loss_ctc 31.762480 loss_rnnt 15.695484 hw_loss 0.000383 lr 0.00095244 rank 2
2023-02-17 18:39:25,508 DEBUG TRAIN Batch 3/2600 loss 22.859297 loss_att 33.829876 loss_ctc 45.504192 loss_rnnt 17.565710 hw_loss 0.150284 lr 0.00095123 rank 1
2023-02-17 18:39:25,511 DEBUG TRAIN Batch 3/2600 loss 45.262428 loss_att 64.585083 loss_ctc 78.002182 loss_rnnt 36.998283 hw_loss 0.064330 lr 0.00095111 rank 0
2023-02-17 18:39:25,511 DEBUG TRAIN Batch 3/2600 loss 14.969881 loss_att 23.260521 loss_ctc 27.192310 loss_rnnt 11.674608 hw_loss 0.014039 lr 0.00095211 rank 4
2023-02-17 18:39:25,520 DEBUG TRAIN Batch 3/2600 loss 35.559433 loss_att 40.608891 loss_ctc 49.350842 loss_rnnt 32.690990 hw_loss 0.036930 lr 0.00095153 rank 3
2023-02-17 18:39:25,520 DEBUG TRAIN Batch 3/2600 loss 43.335232 loss_att 50.943451 loss_ctc 67.707397 loss_rnnt 38.532871 hw_loss 0.058297 lr 0.00095165 rank 6
2023-02-17 18:39:25,523 DEBUG TRAIN Batch 3/2600 loss 39.961346 loss_att 51.288486 loss_ctc 68.054031 loss_rnnt 33.916702 hw_loss 0.062857 lr 0.00095072 rank 2
2023-02-17 18:39:25,522 DEBUG TRAIN Batch 3/2600 loss 39.355457 loss_att 44.594425 loss_ctc 48.548031 loss_rnnt 37.072262 hw_loss 0.018243 lr 0.00095129 rank 5
2023-02-17 18:39:25,536 DEBUG TRAIN Batch 3/2600 loss 11.787897 loss_att 21.351501 loss_ctc 22.377531 loss_rnnt 8.456117 hw_loss 0.013328 lr 0.00095185 rank 7
2023-02-17 18:40:38,694 DEBUG TRAIN Batch 3/2700 loss 38.529152 loss_att 46.975681 loss_ctc 47.281151 loss_rnnt 35.651360 hw_loss 0.040418 lr 0.00095039 rank 4
2023-02-17 18:40:38,697 DEBUG TRAIN Batch 3/2700 loss 28.270128 loss_att 40.672562 loss_ctc 49.233471 loss_rnnt 22.937788 hw_loss 0.106387 lr 0.00094981 rank 3
2023-02-17 18:40:38,700 DEBUG TRAIN Batch 3/2700 loss 32.464436 loss_att 39.875862 loss_ctc 49.921585 loss_rnnt 28.646648 hw_loss 0.014779 lr 0.00094940 rank 0
2023-02-17 18:40:38,701 DEBUG TRAIN Batch 3/2700 loss 39.905624 loss_att 50.173031 loss_ctc 67.900284 loss_rnnt 34.106510 hw_loss 0.024397 lr 0.00095013 rank 7
2023-02-17 18:40:38,701 DEBUG TRAIN Batch 3/2700 loss 16.989405 loss_att 26.446276 loss_ctc 29.823132 loss_rnnt 13.353369 hw_loss 0.062809 lr 0.00094993 rank 6
2023-02-17 18:40:38,702 DEBUG TRAIN Batch 3/2700 loss 31.758987 loss_att 44.922131 loss_ctc 49.659786 loss_rnnt 26.665152 hw_loss 0.139566 lr 0.00094957 rank 5
2023-02-17 18:40:38,703 DEBUG TRAIN Batch 3/2700 loss 46.638680 loss_att 52.775909 loss_ctc 75.073730 loss_rnnt 41.604469 hw_loss 0.028921 lr 0.00094900 rank 2
2023-02-17 18:40:38,706 DEBUG TRAIN Batch 3/2700 loss 19.244720 loss_att 23.215406 loss_ctc 30.216610 loss_rnnt 16.945107 hw_loss 0.079801 lr 0.00094952 rank 1
2023-02-17 18:41:52,944 DEBUG TRAIN Batch 3/2800 loss 22.050833 loss_att 33.416534 loss_ctc 37.062988 loss_rnnt 17.737812 hw_loss 0.071737 lr 0.00094781 rank 1
2023-02-17 18:41:52,945 DEBUG TRAIN Batch 3/2800 loss 21.961052 loss_att 27.500557 loss_ctc 38.274334 loss_rnnt 18.631193 hw_loss 0.087853 lr 0.00094842 rank 7
2023-02-17 18:41:52,947 DEBUG TRAIN Batch 3/2800 loss 30.029127 loss_att 37.489342 loss_ctc 45.446037 loss_rnnt 26.446873 hw_loss 0.064917 lr 0.00094786 rank 5
2023-02-17 18:41:52,954 DEBUG TRAIN Batch 3/2800 loss 24.739325 loss_att 32.693535 loss_ctc 32.604404 loss_rnnt 22.089607 hw_loss 0.019123 lr 0.00094822 rank 6
2023-02-17 18:41:52,955 DEBUG TRAIN Batch 3/2800 loss 28.923439 loss_att 34.378189 loss_ctc 45.571178 loss_rnnt 25.591530 hw_loss 0.039863 lr 0.00094769 rank 0
2023-02-17 18:41:52,955 DEBUG TRAIN Batch 3/2800 loss 36.983166 loss_att 51.392094 loss_ctc 58.772919 loss_rnnt 31.145132 hw_loss 0.095533 lr 0.00094868 rank 4
2023-02-17 18:41:52,959 DEBUG TRAIN Batch 3/2800 loss 25.531168 loss_att 34.032333 loss_ctc 37.572407 loss_rnnt 22.182343 hw_loss 0.080798 lr 0.00094810 rank 3
2023-02-17 18:41:52,963 DEBUG TRAIN Batch 3/2800 loss 29.473635 loss_att 30.645205 loss_ctc 39.748360 loss_rnnt 27.845989 hw_loss 0.043818 lr 0.00094730 rank 2
2023-02-17 18:43:07,009 DEBUG TRAIN Batch 3/2900 loss 19.278706 loss_att 29.896400 loss_ctc 35.931442 loss_rnnt 14.867884 hw_loss 0.125472 lr 0.00094698 rank 4
2023-02-17 18:43:07,010 DEBUG TRAIN Batch 3/2900 loss 34.176556 loss_att 43.902611 loss_ctc 52.264389 loss_rnnt 29.799738 hw_loss 0.037297 lr 0.00094652 rank 6
2023-02-17 18:43:07,010 DEBUG TRAIN Batch 3/2900 loss 36.982731 loss_att 47.002815 loss_ctc 59.833145 loss_rnnt 31.870577 hw_loss 0.115155 lr 0.00094611 rank 1
2023-02-17 18:43:07,013 DEBUG TRAIN Batch 3/2900 loss 36.915985 loss_att 46.848907 loss_ctc 55.425640 loss_rnnt 32.449127 hw_loss 0.023101 lr 0.00094599 rank 0
2023-02-17 18:43:07,012 DEBUG TRAIN Batch 3/2900 loss 31.081949 loss_att 39.927391 loss_ctc 48.384079 loss_rnnt 26.977858 hw_loss 0.052601 lr 0.00094616 rank 5
2023-02-17 18:43:07,014 DEBUG TRAIN Batch 3/2900 loss 20.731462 loss_att 33.610207 loss_ctc 30.655161 loss_rnnt 16.780857 hw_loss 0.096929 lr 0.00094560 rank 2
2023-02-17 18:43:07,015 DEBUG TRAIN Batch 3/2900 loss 18.960316 loss_att 31.685272 loss_ctc 32.067108 loss_rnnt 14.655419 hw_loss 0.023120 lr 0.00094640 rank 3
2023-02-17 18:43:07,017 DEBUG TRAIN Batch 3/2900 loss 44.443432 loss_att 51.668015 loss_ctc 64.829979 loss_rnnt 40.266914 hw_loss 0.025123 lr 0.00094672 rank 7
2023-02-17 18:44:19,686 DEBUG TRAIN Batch 3/3000 loss 27.972851 loss_att 33.345207 loss_ctc 40.877625 loss_rnnt 25.119846 hw_loss 0.108553 lr 0.00094528 rank 4
2023-02-17 18:44:19,689 DEBUG TRAIN Batch 3/3000 loss 31.702478 loss_att 35.672741 loss_ctc 41.468227 loss_rnnt 29.606054 hw_loss 0.000512 lr 0.00094392 rank 2
2023-02-17 18:44:19,691 DEBUG TRAIN Batch 3/3000 loss 21.458113 loss_att 32.307068 loss_ctc 41.666618 loss_rnnt 16.542681 hw_loss 0.095948 lr 0.00094430 rank 0
2023-02-17 18:44:19,691 DEBUG TRAIN Batch 3/3000 loss 38.832306 loss_att 47.112560 loss_ctc 57.032188 loss_rnnt 34.698738 hw_loss 0.095379 lr 0.00094447 rank 5
2023-02-17 18:44:19,695 DEBUG TRAIN Batch 3/3000 loss 20.303080 loss_att 30.019901 loss_ctc 33.413109 loss_rnnt 16.611546 hw_loss 0.000310 lr 0.00094503 rank 7
2023-02-17 18:44:19,695 DEBUG TRAIN Batch 3/3000 loss 24.701662 loss_att 28.470753 loss_ctc 36.698929 loss_rnnt 22.266376 hw_loss 0.153431 lr 0.00094483 rank 6
2023-02-17 18:44:19,695 DEBUG TRAIN Batch 3/3000 loss 15.456758 loss_att 20.294186 loss_ctc 24.892223 loss_rnnt 13.187384 hw_loss 0.082177 lr 0.00094442 rank 1
2023-02-17 18:44:19,696 DEBUG TRAIN Batch 3/3000 loss 30.946634 loss_att 35.329136 loss_ctc 47.202354 loss_rnnt 27.890833 hw_loss 0.022254 lr 0.00094471 rank 3
2023-02-17 18:45:32,595 DEBUG TRAIN Batch 3/3100 loss 29.773327 loss_att 31.742996 loss_ctc 42.697212 loss_rnnt 27.570684 hw_loss 0.160354 lr 0.00094274 rank 1
2023-02-17 18:45:32,597 DEBUG TRAIN Batch 3/3100 loss 18.103237 loss_att 18.631222 loss_ctc 26.637312 loss_rnnt 16.777140 hw_loss 0.154914 lr 0.00094224 rank 2
2023-02-17 18:45:32,597 DEBUG TRAIN Batch 3/3100 loss 27.761324 loss_att 30.769995 loss_ctc 39.120083 loss_rnnt 25.620865 hw_loss 0.045419 lr 0.00094279 rank 5
2023-02-17 18:45:32,598 DEBUG TRAIN Batch 3/3100 loss 36.225853 loss_att 43.190742 loss_ctc 53.333202 loss_rnnt 32.529255 hw_loss 0.042443 lr 0.00094314 rank 6
2023-02-17 18:45:32,599 DEBUG TRAIN Batch 3/3100 loss 46.467659 loss_att 50.891716 loss_ctc 64.929771 loss_rnnt 43.071011 hw_loss 0.094169 lr 0.00094360 rank 4
2023-02-17 18:45:32,599 DEBUG TRAIN Batch 3/3100 loss 22.693455 loss_att 22.705168 loss_ctc 33.696045 loss_rnnt 21.218912 hw_loss 0.009726 lr 0.00094303 rank 3
2023-02-17 18:45:32,600 DEBUG TRAIN Batch 3/3100 loss 22.480898 loss_att 28.486485 loss_ctc 36.559036 loss_rnnt 19.347443 hw_loss 0.103603 lr 0.00094262 rank 0
2023-02-17 18:45:32,606 DEBUG TRAIN Batch 3/3100 loss 23.545052 loss_att 27.547848 loss_ctc 43.435005 loss_rnnt 20.061613 hw_loss 0.057913 lr 0.00094335 rank 7
2023-02-17 18:46:47,822 DEBUG TRAIN Batch 3/3200 loss 18.206905 loss_att 18.342209 loss_ctc 26.468159 loss_rnnt 17.038694 hw_loss 0.074344 lr 0.00094192 rank 4
2023-02-17 18:46:47,827 DEBUG TRAIN Batch 3/3200 loss 19.011044 loss_att 21.334684 loss_ctc 26.899384 loss_rnnt 17.401615 hw_loss 0.174229 lr 0.00094147 rank 6
2023-02-17 18:46:47,832 DEBUG TRAIN Batch 3/3200 loss 28.049431 loss_att 35.287304 loss_ctc 37.750107 loss_rnnt 25.308197 hw_loss 0.000439 lr 0.00094095 rank 0
2023-02-17 18:46:47,833 DEBUG TRAIN Batch 3/3200 loss 34.071239 loss_att 37.133167 loss_ctc 46.383930 loss_rnnt 31.747646 hw_loss 0.130337 lr 0.00094112 rank 5
2023-02-17 18:46:47,834 DEBUG TRAIN Batch 3/3200 loss 50.235355 loss_att 64.205917 loss_ctc 79.356705 loss_rnnt 43.557732 hw_loss 0.001246 lr 0.00094057 rank 2
2023-02-17 18:46:47,834 DEBUG TRAIN Batch 3/3200 loss 55.723000 loss_att 65.766144 loss_ctc 89.071167 loss_rnnt 49.243389 hw_loss 0.046054 lr 0.00094167 rank 7
2023-02-17 18:46:47,873 DEBUG TRAIN Batch 3/3200 loss 37.954136 loss_att 50.576752 loss_ctc 57.096916 loss_rnnt 32.851055 hw_loss 0.049103 lr 0.00094135 rank 3
2023-02-17 18:46:47,906 DEBUG TRAIN Batch 3/3200 loss 31.317801 loss_att 42.797497 loss_ctc 42.331264 loss_rnnt 27.534557 hw_loss 0.035327 lr 0.00094107 rank 1
2023-02-17 18:48:00,085 DEBUG TRAIN Batch 3/3300 loss 43.073418 loss_att 52.495472 loss_ctc 68.323090 loss_rnnt 37.822308 hw_loss 0.000139 lr 0.00093891 rank 2
2023-02-17 18:48:00,085 DEBUG TRAIN Batch 3/3300 loss 31.963318 loss_att 42.668304 loss_ctc 51.426235 loss_rnnt 27.227177 hw_loss 0.000160 lr 0.00094026 rank 4
2023-02-17 18:48:00,085 DEBUG TRAIN Batch 3/3300 loss 19.747244 loss_att 29.882591 loss_ctc 38.872597 loss_rnnt 15.116324 hw_loss 0.100881 lr 0.00093981 rank 6
2023-02-17 18:48:00,086 DEBUG TRAIN Batch 3/3300 loss 25.024427 loss_att 32.230396 loss_ctc 37.377087 loss_rnnt 21.890219 hw_loss 0.086236 lr 0.00094001 rank 7
2023-02-17 18:48:00,086 DEBUG TRAIN Batch 3/3300 loss 25.606550 loss_att 32.138149 loss_ctc 36.588699 loss_rnnt 22.827641 hw_loss 0.015566 lr 0.00093969 rank 3
2023-02-17 18:48:00,086 DEBUG TRAIN Batch 3/3300 loss 30.226608 loss_att 36.005379 loss_ctc 41.122463 loss_rnnt 27.617981 hw_loss 0.000174 lr 0.00093941 rank 1
2023-02-17 18:48:00,089 DEBUG TRAIN Batch 3/3300 loss 16.240152 loss_att 22.915724 loss_ctc 24.971350 loss_rnnt 13.671677 hw_loss 0.129750 lr 0.00093929 rank 0
2023-02-17 18:48:00,091 DEBUG TRAIN Batch 3/3300 loss 23.434179 loss_att 31.591333 loss_ctc 34.482582 loss_rnnt 20.278797 hw_loss 0.095305 lr 0.00093946 rank 5
2023-02-17 18:49:13,993 DEBUG TRAIN Batch 3/3400 loss 23.792253 loss_att 32.317642 loss_ctc 32.700230 loss_rnnt 20.883512 hw_loss 0.029873 lr 0.00093775 rank 1
2023-02-17 18:49:13,994 DEBUG TRAIN Batch 3/3400 loss 21.365198 loss_att 29.386482 loss_ctc 44.740364 loss_rnnt 16.626738 hw_loss 0.032840 lr 0.00093860 rank 4
2023-02-17 18:49:13,997 DEBUG TRAIN Batch 3/3400 loss 40.078987 loss_att 48.192745 loss_ctc 61.474735 loss_rnnt 35.594982 hw_loss 0.015913 lr 0.00093726 rank 2
2023-02-17 18:49:13,998 DEBUG TRAIN Batch 3/3400 loss 24.026426 loss_att 31.255142 loss_ctc 38.018227 loss_rnnt 20.681459 hw_loss 0.063092 lr 0.00093804 rank 3
2023-02-17 18:49:13,999 DEBUG TRAIN Batch 3/3400 loss 20.691389 loss_att 29.933613 loss_ctc 34.395832 loss_rnnt 16.987793 hw_loss 0.052298 lr 0.00093764 rank 0
2023-02-17 18:49:14,001 DEBUG TRAIN Batch 3/3400 loss 20.087738 loss_att 33.409325 loss_ctc 27.077766 loss_rnnt 16.478142 hw_loss 0.024890 lr 0.00093835 rank 7
2023-02-17 18:49:14,002 DEBUG TRAIN Batch 3/3400 loss 25.097200 loss_att 29.151886 loss_ctc 34.816067 loss_rnnt 22.942970 hw_loss 0.088955 lr 0.00093815 rank 6
2023-02-17 18:49:14,009 DEBUG TRAIN Batch 3/3400 loss 36.489647 loss_att 48.597298 loss_ctc 56.891167 loss_rnnt 31.332958 hw_loss 0.028039 lr 0.00093780 rank 5
2023-02-17 18:50:28,027 DEBUG TRAIN Batch 3/3500 loss 33.642521 loss_att 40.564812 loss_ctc 39.142227 loss_rnnt 31.504044 hw_loss 0.038866 lr 0.00093616 rank 5
2023-02-17 18:50:28,028 DEBUG TRAIN Batch 3/3500 loss 37.276165 loss_att 47.526779 loss_ctc 58.314178 loss_rnnt 32.379169 hw_loss 0.078384 lr 0.00093639 rank 3
2023-02-17 18:50:28,029 DEBUG TRAIN Batch 3/3500 loss 47.344646 loss_att 50.867935 loss_ctc 65.963516 loss_rnnt 44.107742 hw_loss 0.093237 lr 0.00093611 rank 1
2023-02-17 18:50:28,033 DEBUG TRAIN Batch 3/3500 loss 41.963524 loss_att 47.452751 loss_ctc 57.349312 loss_rnnt 38.813885 hw_loss 0.000668 lr 0.00093599 rank 0
2023-02-17 18:50:28,036 DEBUG TRAIN Batch 3/3500 loss 25.806282 loss_att 36.370453 loss_ctc 44.143826 loss_rnnt 21.184177 hw_loss 0.120499 lr 0.00093695 rank 4
2023-02-17 18:50:28,038 DEBUG TRAIN Batch 3/3500 loss 36.455906 loss_att 44.698418 loss_ctc 61.966431 loss_rnnt 31.390598 hw_loss 0.028873 lr 0.00093650 rank 6
2023-02-17 18:50:28,038 DEBUG TRAIN Batch 3/3500 loss 29.638737 loss_att 37.318783 loss_ctc 47.658882 loss_rnnt 25.693771 hw_loss 0.011758 lr 0.00093562 rank 2
2023-02-17 18:50:28,042 DEBUG TRAIN Batch 3/3500 loss 18.884571 loss_att 25.251156 loss_ctc 31.437639 loss_rnnt 15.930506 hw_loss 0.013133 lr 0.00093670 rank 7
2023-02-17 18:51:42,342 DEBUG TRAIN Batch 3/3600 loss 32.576084 loss_att 41.346851 loss_ctc 45.556168 loss_rnnt 29.063593 hw_loss 0.051859 lr 0.00093506 rank 7
2023-02-17 18:51:42,349 DEBUG TRAIN Batch 3/3600 loss 42.083435 loss_att 47.876778 loss_ctc 57.281635 loss_rnnt 38.886864 hw_loss 0.021512 lr 0.00093487 rank 6
2023-02-17 18:51:42,351 DEBUG TRAIN Batch 3/3600 loss 37.936779 loss_att 44.356575 loss_ctc 55.447044 loss_rnnt 34.298401 hw_loss 0.036964 lr 0.00093531 rank 4
2023-02-17 18:51:42,352 DEBUG TRAIN Batch 3/3600 loss 21.746792 loss_att 26.200855 loss_ctc 33.887650 loss_rnnt 19.195854 hw_loss 0.077521 lr 0.00093436 rank 0
2023-02-17 18:51:42,355 DEBUG TRAIN Batch 3/3600 loss 13.186468 loss_att 18.844223 loss_ctc 21.466347 loss_rnnt 10.881184 hw_loss 0.130781 lr 0.00093447 rank 1
2023-02-17 18:51:42,356 DEBUG TRAIN Batch 3/3600 loss 19.833626 loss_att 28.011930 loss_ctc 28.064247 loss_rnnt 17.079927 hw_loss 0.038660 lr 0.00093475 rank 3
2023-02-17 18:51:42,358 DEBUG TRAIN Batch 3/3600 loss 31.925489 loss_att 36.132660 loss_ctc 48.053520 loss_rnnt 28.889606 hw_loss 0.082584 lr 0.00093398 rank 2
2023-02-17 18:51:42,361 DEBUG TRAIN Batch 3/3600 loss 37.503143 loss_att 45.317352 loss_ctc 54.157063 loss_rnnt 33.632683 hw_loss 0.163311 lr 0.00093452 rank 5
2023-02-17 18:52:55,088 DEBUG TRAIN Batch 3/3700 loss 34.003513 loss_att 35.094749 loss_ctc 49.000561 loss_rnnt 31.752163 hw_loss 0.062812 lr 0.00093285 rank 1
2023-02-17 18:52:55,089 DEBUG TRAIN Batch 3/3700 loss 25.230646 loss_att 30.365629 loss_ctc 34.337936 loss_rnnt 22.962208 hw_loss 0.050880 lr 0.00093324 rank 6
2023-02-17 18:52:55,091 DEBUG TRAIN Batch 3/3700 loss 33.920486 loss_att 45.034294 loss_ctc 53.334686 loss_rnnt 29.108837 hw_loss 0.000606 lr 0.00093367 rank 4
2023-02-17 18:52:55,092 DEBUG TRAIN Batch 3/3700 loss 20.507128 loss_att 24.169950 loss_ctc 31.379417 loss_rnnt 18.250385 hw_loss 0.139761 lr 0.00093273 rank 0
2023-02-17 18:52:55,094 DEBUG TRAIN Batch 3/3700 loss 31.060125 loss_att 44.193840 loss_ctc 49.387398 loss_rnnt 25.973553 hw_loss 0.030364 lr 0.00093289 rank 5
2023-02-17 18:52:55,095 DEBUG TRAIN Batch 3/3700 loss 24.083879 loss_att 31.357573 loss_ctc 34.077366 loss_rnnt 21.296545 hw_loss 0.000249 lr 0.00093343 rank 7
2023-02-17 18:52:55,096 DEBUG TRAIN Batch 3/3700 loss 27.101044 loss_att 32.704575 loss_ctc 41.965237 loss_rnnt 23.934956 hw_loss 0.119039 lr 0.00093236 rank 2
2023-02-17 18:52:55,097 DEBUG TRAIN Batch 3/3700 loss 20.135853 loss_att 25.862297 loss_ctc 32.700443 loss_rnnt 17.281937 hw_loss 0.062527 lr 0.00093312 rank 3
2023-02-17 18:54:08,470 DEBUG TRAIN Batch 3/3800 loss 40.484901 loss_att 56.256363 loss_ctc 62.385307 loss_rnnt 34.410488 hw_loss 0.000133 lr 0.00093074 rank 2
2023-02-17 18:54:08,482 DEBUG TRAIN Batch 3/3800 loss 24.131756 loss_att 27.220448 loss_ctc 34.926743 loss_rnnt 21.991011 hw_loss 0.156896 lr 0.00093205 rank 4
2023-02-17 18:54:08,483 DEBUG TRAIN Batch 3/3800 loss 23.706596 loss_att 24.575947 loss_ctc 32.361568 loss_rnnt 22.290047 hw_loss 0.166284 lr 0.00093161 rank 6
2023-02-17 18:54:08,484 DEBUG TRAIN Batch 3/3800 loss 19.036530 loss_att 20.928045 loss_ctc 28.031599 loss_rnnt 17.399721 hw_loss 0.110930 lr 0.00093127 rank 5
2023-02-17 18:54:08,486 DEBUG TRAIN Batch 3/3800 loss 38.261051 loss_att 50.967461 loss_ctc 68.642326 loss_rnnt 31.668844 hw_loss 0.000162 lr 0.00093123 rank 1
2023-02-17 18:54:08,487 DEBUG TRAIN Batch 3/3800 loss 23.945335 loss_att 25.203684 loss_ctc 30.859282 loss_rnnt 22.700058 hw_loss 0.134528 lr 0.00093111 rank 0
2023-02-17 18:54:08,487 DEBUG TRAIN Batch 3/3800 loss 23.650646 loss_att 28.805983 loss_ctc 37.297440 loss_rnnt 20.752691 hw_loss 0.088717 lr 0.00093150 rank 3
2023-02-17 18:54:08,494 DEBUG TRAIN Batch 3/3800 loss 18.552099 loss_att 21.777075 loss_ctc 27.540012 loss_rnnt 16.622097 hw_loss 0.162409 lr 0.00093181 rank 7
2023-02-17 18:55:23,389 DEBUG TRAIN Batch 3/3900 loss 32.542038 loss_att 39.793411 loss_ctc 45.912781 loss_rnnt 29.251112 hw_loss 0.108537 lr 0.00092989 rank 3
2023-02-17 18:55:23,402 DEBUG TRAIN Batch 3/3900 loss 28.660311 loss_att 33.098045 loss_ctc 43.888298 loss_rnnt 25.738338 hw_loss 0.007553 lr 0.00093000 rank 6
2023-02-17 18:55:23,404 DEBUG TRAIN Batch 3/3900 loss 29.450254 loss_att 36.284275 loss_ctc 46.381771 loss_rnnt 25.825867 hw_loss 0.000091 lr 0.00092913 rank 2
2023-02-17 18:55:23,409 DEBUG TRAIN Batch 3/3900 loss 38.367413 loss_att 39.678730 loss_ctc 56.455994 loss_rnnt 35.687668 hw_loss 0.010635 lr 0.00092962 rank 1
2023-02-17 18:55:23,425 DEBUG TRAIN Batch 3/3900 loss 35.004745 loss_att 40.378826 loss_ctc 59.703442 loss_rnnt 30.597855 hw_loss 0.072965 lr 0.00092966 rank 5
2023-02-17 18:55:23,432 DEBUG TRAIN Batch 3/3900 loss 25.211584 loss_att 26.505030 loss_ctc 40.921364 loss_rnnt 22.842400 hw_loss 0.029734 lr 0.00093044 rank 4
2023-02-17 18:55:23,439 DEBUG TRAIN Batch 3/3900 loss 37.864040 loss_att 46.731354 loss_ctc 59.649071 loss_rnnt 33.136154 hw_loss 0.093285 lr 0.00092950 rank 0
2023-02-17 18:55:23,458 DEBUG TRAIN Batch 3/3900 loss 28.508877 loss_att 37.015465 loss_ctc 47.912315 loss_rnnt 24.166983 hw_loss 0.100222 lr 0.00093019 rank 7
2023-02-17 18:56:36,986 DEBUG TRAIN Batch 3/4000 loss 33.926205 loss_att 37.738987 loss_ctc 50.163631 loss_rnnt 30.940090 hw_loss 0.109818 lr 0.00092883 rank 4
2023-02-17 18:56:36,991 DEBUG TRAIN Batch 3/4000 loss 16.264006 loss_att 24.415546 loss_ctc 21.870687 loss_rnnt 13.866835 hw_loss 0.036195 lr 0.00092806 rank 5
2023-02-17 18:56:36,991 DEBUG TRAIN Batch 3/4000 loss 37.302128 loss_att 42.510521 loss_ctc 54.360458 loss_rnnt 33.985924 hw_loss 0.000156 lr 0.00092753 rank 2
2023-02-17 18:56:36,995 DEBUG TRAIN Batch 3/4000 loss 36.359390 loss_att 42.663689 loss_ctc 49.706764 loss_rnnt 33.304005 hw_loss 0.027884 lr 0.00092790 rank 0
2023-02-17 18:56:36,997 DEBUG TRAIN Batch 3/4000 loss 21.139952 loss_att 30.745430 loss_ctc 38.793098 loss_rnnt 16.823891 hw_loss 0.077273 lr 0.00092840 rank 6
2023-02-17 18:56:36,999 DEBUG TRAIN Batch 3/4000 loss 22.162548 loss_att 30.693918 loss_ctc 31.773380 loss_rnnt 19.155193 hw_loss 0.036816 lr 0.00092828 rank 3
2023-02-17 18:56:37,016 DEBUG TRAIN Batch 3/4000 loss 32.253361 loss_att 38.179497 loss_ctc 51.670013 loss_rnnt 28.459911 hw_loss 0.036250 lr 0.00092801 rank 1
2023-02-17 18:56:37,019 DEBUG TRAIN Batch 3/4000 loss 23.084309 loss_att 32.824604 loss_ctc 35.821072 loss_rnnt 19.374969 hw_loss 0.118208 lr 0.00092859 rank 7
2023-02-17 18:57:49,662 DEBUG TRAIN Batch 3/4100 loss 48.648109 loss_att 54.313175 loss_ctc 72.242760 loss_rnnt 44.323429 hw_loss 0.085704 lr 0.00092642 rank 1
2023-02-17 18:57:49,665 DEBUG TRAIN Batch 3/4100 loss 27.062687 loss_att 36.804882 loss_ctc 47.410271 loss_rnnt 22.400291 hw_loss 0.001771 lr 0.00092680 rank 6
2023-02-17 18:57:49,665 DEBUG TRAIN Batch 3/4100 loss 28.638277 loss_att 33.413689 loss_ctc 43.703411 loss_rnnt 25.619038 hw_loss 0.104005 lr 0.00092631 rank 0
2023-02-17 18:57:49,668 DEBUG TRAIN Batch 3/4100 loss 40.882244 loss_att 51.008728 loss_ctc 64.651352 loss_rnnt 35.677868 hw_loss 0.018497 lr 0.00092669 rank 3
2023-02-17 18:57:49,668 DEBUG TRAIN Batch 3/4100 loss 23.951574 loss_att 31.486462 loss_ctc 32.570751 loss_rnnt 21.287268 hw_loss 0.015199 lr 0.00092723 rank 4
2023-02-17 18:57:49,669 DEBUG TRAIN Batch 3/4100 loss 31.358982 loss_att 43.538258 loss_ctc 47.414803 loss_rnnt 26.769279 hw_loss 0.024507 lr 0.00092647 rank 5
2023-02-17 18:57:49,671 DEBUG TRAIN Batch 3/4100 loss 31.357887 loss_att 42.618610 loss_ctc 53.891972 loss_rnnt 26.090256 hw_loss 0.020515 lr 0.00092699 rank 7
2023-02-17 18:57:49,672 DEBUG TRAIN Batch 3/4100 loss 32.948059 loss_att 35.153606 loss_ctc 51.271744 loss_rnnt 30.020868 hw_loss 0.080482 lr 0.00092594 rank 2
2023-02-17 18:59:02,408 DEBUG TRAIN Batch 3/4200 loss 29.490561 loss_att 38.164677 loss_ctc 48.713364 loss_rnnt 25.176062 hw_loss 0.031191 lr 0.00092564 rank 4
2023-02-17 18:59:02,422 DEBUG TRAIN Batch 3/4200 loss 38.422775 loss_att 47.953926 loss_ctc 59.633125 loss_rnnt 33.643890 hw_loss 0.083639 lr 0.00092521 rank 6
2023-02-17 18:59:02,426 DEBUG TRAIN Batch 3/4200 loss 19.182503 loss_att 28.564093 loss_ctc 33.887001 loss_rnnt 15.239170 hw_loss 0.199527 lr 0.00092483 rank 1
2023-02-17 18:59:02,427 DEBUG TRAIN Batch 3/4200 loss 34.414478 loss_att 39.033607 loss_ctc 53.517830 loss_rnnt 30.924341 hw_loss 0.035989 lr 0.00092472 rank 0
2023-02-17 18:59:02,429 DEBUG TRAIN Batch 3/4200 loss 21.963232 loss_att 28.105883 loss_ctc 28.585861 loss_rnnt 19.851574 hw_loss 0.000207 lr 0.00092510 rank 3
2023-02-17 18:59:02,429 DEBUG TRAIN Batch 3/4200 loss 21.387766 loss_att 31.796478 loss_ctc 37.859653 loss_rnnt 17.095524 hw_loss 0.026712 lr 0.00092540 rank 7
2023-02-17 18:59:02,433 DEBUG TRAIN Batch 3/4200 loss 20.848125 loss_att 30.087700 loss_ctc 31.692989 loss_rnnt 17.516560 hw_loss 0.070631 lr 0.00092436 rank 2
2023-02-17 18:59:02,435 DEBUG TRAIN Batch 3/4200 loss 42.063911 loss_att 51.920174 loss_ctc 65.351868 loss_rnnt 36.956886 hw_loss 0.057587 lr 0.00092488 rank 5
2023-02-17 19:00:16,902 DEBUG TRAIN Batch 3/4300 loss 42.554680 loss_att 48.832253 loss_ctc 65.492950 loss_rnnt 38.206524 hw_loss 0.064141 lr 0.00092406 rank 4
2023-02-17 19:00:16,908 DEBUG TRAIN Batch 3/4300 loss 24.064098 loss_att 30.792900 loss_ctc 40.706085 loss_rnnt 20.433765 hw_loss 0.123078 lr 0.00092363 rank 6
2023-02-17 19:00:16,913 DEBUG TRAIN Batch 3/4300 loss 27.054888 loss_att 31.878731 loss_ctc 44.838539 loss_rnnt 23.683012 hw_loss 0.067408 lr 0.00092352 rank 3
2023-02-17 19:00:16,915 DEBUG TRAIN Batch 3/4300 loss 26.467016 loss_att 34.983749 loss_ctc 43.305103 loss_rnnt 22.517277 hw_loss 0.002463 lr 0.00092278 rank 2
2023-02-17 19:00:16,916 DEBUG TRAIN Batch 3/4300 loss 31.524214 loss_att 36.215805 loss_ctc 43.194099 loss_rnnt 29.018467 hw_loss 0.021453 lr 0.00092314 rank 0
2023-02-17 19:00:16,917 DEBUG TRAIN Batch 3/4300 loss 26.116833 loss_att 28.073380 loss_ctc 41.917538 loss_rnnt 23.589264 hw_loss 0.055312 lr 0.00092325 rank 1
2023-02-17 19:00:16,925 DEBUG TRAIN Batch 3/4300 loss 23.578564 loss_att 27.752798 loss_ctc 38.461388 loss_rnnt 20.734173 hw_loss 0.047187 lr 0.00092330 rank 5
2023-02-17 19:00:16,968 DEBUG TRAIN Batch 3/4300 loss 32.947491 loss_att 42.103664 loss_ctc 53.154339 loss_rnnt 28.354097 hw_loss 0.127332 lr 0.00092382 rank 7
2023-02-17 19:01:30,298 DEBUG TRAIN Batch 3/4400 loss 29.232700 loss_att 31.271019 loss_ctc 43.238064 loss_rnnt 26.913269 hw_loss 0.083221 lr 0.00092206 rank 6
2023-02-17 19:01:30,303 DEBUG TRAIN Batch 3/4400 loss 53.167328 loss_att 58.922653 loss_ctc 72.957191 loss_rnnt 49.348370 hw_loss 0.054835 lr 0.00092248 rank 4
2023-02-17 19:01:30,306 DEBUG TRAIN Batch 3/4400 loss 21.133501 loss_att 21.699268 loss_ctc 30.106083 loss_rnnt 19.800278 hw_loss 0.044484 lr 0.00092122 rank 2
2023-02-17 19:01:30,318 DEBUG TRAIN Batch 3/4400 loss 17.757145 loss_att 17.555168 loss_ctc 23.673891 loss_rnnt 16.964085 hw_loss 0.083542 lr 0.00092168 rank 1
2023-02-17 19:01:30,320 DEBUG TRAIN Batch 3/4400 loss 19.562473 loss_att 25.857403 loss_ctc 36.696308 loss_rnnt 15.977379 hw_loss 0.077990 lr 0.00092225 rank 7
2023-02-17 19:01:30,320 DEBUG TRAIN Batch 3/4400 loss 34.678265 loss_att 36.771378 loss_ctc 51.065674 loss_rnnt 32.028561 hw_loss 0.086435 lr 0.00092157 rank 0
2023-02-17 19:01:30,321 DEBUG TRAIN Batch 3/4400 loss 22.989666 loss_att 26.588821 loss_ctc 35.217823 loss_rnnt 20.554579 hw_loss 0.159067 lr 0.00092195 rank 3
2023-02-17 19:01:30,364 DEBUG TRAIN Batch 3/4400 loss 27.794691 loss_att 27.066797 loss_ctc 36.425659 loss_rnnt 26.744583 hw_loss 0.084164 lr 0.00092173 rank 5
2023-02-17 19:02:42,881 DEBUG TRAIN Batch 3/4500 loss 16.707850 loss_att 16.819071 loss_ctc 21.652802 loss_rnnt 15.950188 hw_loss 0.142670 lr 0.00092092 rank 4
2023-02-17 19:02:42,886 DEBUG TRAIN Batch 3/4500 loss 44.539303 loss_att 58.690163 loss_ctc 69.997467 loss_rnnt 38.292160 hw_loss 0.042281 lr 0.00092012 rank 1
2023-02-17 19:02:42,888 DEBUG TRAIN Batch 3/4500 loss 25.179564 loss_att 30.710426 loss_ctc 36.663967 loss_rnnt 22.463882 hw_loss 0.146729 lr 0.00092050 rank 6
2023-02-17 19:02:42,891 DEBUG TRAIN Batch 3/4500 loss 29.789282 loss_att 39.952206 loss_ctc 37.899906 loss_rnnt 26.650360 hw_loss 0.046723 lr 0.00092001 rank 0
2023-02-17 19:02:42,894 DEBUG TRAIN Batch 3/4500 loss 26.143139 loss_att 33.447212 loss_ctc 37.043633 loss_rnnt 23.179016 hw_loss 0.093581 lr 0.00091966 rank 2
2023-02-17 19:02:42,903 DEBUG TRAIN Batch 3/4500 loss 32.359688 loss_att 44.390606 loss_ctc 64.517075 loss_rnnt 25.665792 hw_loss 0.000110 lr 0.00092039 rank 3
2023-02-17 19:02:42,910 DEBUG TRAIN Batch 3/4500 loss 27.129328 loss_att 31.887306 loss_ctc 40.931957 loss_rnnt 24.275974 hw_loss 0.115132 lr 0.00092068 rank 7
2023-02-17 19:02:42,940 DEBUG TRAIN Batch 3/4500 loss 31.269279 loss_att 48.142815 loss_ctc 50.766811 loss_rnnt 25.262220 hw_loss 0.061277 lr 0.00092017 rank 5
2023-02-17 19:03:57,569 DEBUG TRAIN Batch 3/4600 loss 27.488029 loss_att 35.149719 loss_ctc 44.517723 loss_rnnt 23.619568 hw_loss 0.122805 lr 0.00091936 rank 4
2023-02-17 19:03:57,574 DEBUG TRAIN Batch 3/4600 loss 31.368423 loss_att 37.307144 loss_ctc 49.443249 loss_rnnt 27.699612 hw_loss 0.133302 lr 0.00091846 rank 0
2023-02-17 19:03:57,579 DEBUG TRAIN Batch 3/4600 loss 23.535973 loss_att 27.646885 loss_ctc 41.355469 loss_rnnt 20.303492 hw_loss 0.064432 lr 0.00091857 rank 1
2023-02-17 19:03:57,582 DEBUG TRAIN Batch 3/4600 loss 36.496532 loss_att 43.322662 loss_ctc 52.351177 loss_rnnt 32.979485 hw_loss 0.071002 lr 0.00091883 rank 3
2023-02-17 19:03:57,583 DEBUG TRAIN Batch 3/4600 loss 74.922928 loss_att 103.635300 loss_ctc 126.910049 loss_rnnt 62.169708 hw_loss 0.148361 lr 0.00091913 rank 7
2023-02-17 19:03:57,584 DEBUG TRAIN Batch 3/4600 loss 15.291904 loss_att 21.913042 loss_ctc 20.539288 loss_rnnt 13.222958 hw_loss 0.084501 lr 0.00091894 rank 6
2023-02-17 19:03:57,590 DEBUG TRAIN Batch 3/4600 loss 23.721125 loss_att 29.809948 loss_ctc 40.534130 loss_rnnt 20.237614 hw_loss 0.045023 lr 0.00091810 rank 2
2023-02-17 19:03:57,590 DEBUG TRAIN Batch 3/4600 loss 23.736351 loss_att 31.767525 loss_ctc 34.718964 loss_rnnt 20.621265 hw_loss 0.083443 lr 0.00091861 rank 5
2023-02-17 19:05:11,276 DEBUG TRAIN Batch 3/4700 loss 25.769869 loss_att 33.102928 loss_ctc 45.876564 loss_rnnt 21.622200 hw_loss 0.000311 lr 0.00091691 rank 0
2023-02-17 19:05:11,277 DEBUG TRAIN Batch 3/4700 loss 14.245255 loss_att 21.428993 loss_ctc 25.085112 loss_rnnt 11.323265 hw_loss 0.074865 lr 0.00091781 rank 4
2023-02-17 19:05:11,278 DEBUG TRAIN Batch 3/4700 loss 34.395576 loss_att 41.169796 loss_ctc 46.831341 loss_rnnt 31.369411 hw_loss 0.024786 lr 0.00091656 rank 2
2023-02-17 19:05:11,281 DEBUG TRAIN Batch 3/4700 loss 18.525217 loss_att 24.243668 loss_ctc 30.513985 loss_rnnt 15.745382 hw_loss 0.070582 lr 0.00091739 rank 6
2023-02-17 19:05:11,283 DEBUG TRAIN Batch 3/4700 loss 14.165176 loss_att 22.081610 loss_ctc 21.580029 loss_rnnt 11.547191 hw_loss 0.086347 lr 0.00091758 rank 7
2023-02-17 19:05:11,285 DEBUG TRAIN Batch 3/4700 loss 18.039110 loss_att 25.867109 loss_ctc 23.963663 loss_rnnt 15.659197 hw_loss 0.045700 lr 0.00091728 rank 3
2023-02-17 19:05:11,286 DEBUG TRAIN Batch 3/4700 loss 30.330431 loss_att 39.042427 loss_ctc 54.077888 loss_rnnt 25.411079 hw_loss 0.019919 lr 0.00091702 rank 1
2023-02-17 19:05:11,288 DEBUG TRAIN Batch 3/4700 loss 18.704155 loss_att 26.151810 loss_ctc 33.492935 loss_rnnt 15.235498 hw_loss 0.013661 lr 0.00091707 rank 5
2023-02-17 19:06:24,294 DEBUG TRAIN Batch 3/4800 loss 34.291832 loss_att 40.284191 loss_ctc 53.255600 loss_rnnt 30.500759 hw_loss 0.120189 lr 0.00091627 rank 4
2023-02-17 19:06:24,301 DEBUG TRAIN Batch 3/4800 loss 32.542820 loss_att 39.174568 loss_ctc 51.000916 loss_rnnt 28.751083 hw_loss 0.008078 lr 0.00091585 rank 6
2023-02-17 19:06:24,302 DEBUG TRAIN Batch 3/4800 loss 30.005684 loss_att 37.304043 loss_ctc 48.992332 loss_rnnt 25.981583 hw_loss 0.061645 lr 0.00091538 rank 0
2023-02-17 19:06:24,307 DEBUG TRAIN Batch 3/4800 loss 24.435020 loss_att 30.188366 loss_ctc 36.785221 loss_rnnt 21.601761 hw_loss 0.067304 lr 0.00091548 rank 1
2023-02-17 19:06:24,308 DEBUG TRAIN Batch 3/4800 loss 21.961647 loss_att 27.318785 loss_ctc 39.436214 loss_rnnt 18.558975 hw_loss 0.002444 lr 0.00091553 rank 5
2023-02-17 19:06:24,309 DEBUG TRAIN Batch 3/4800 loss 17.278513 loss_att 26.899105 loss_ctc 29.182373 loss_rnnt 13.747972 hw_loss 0.036079 lr 0.00091574 rank 3
2023-02-17 19:06:24,309 DEBUG TRAIN Batch 3/4800 loss 48.320816 loss_att 51.460258 loss_ctc 74.954514 loss_rnnt 44.131733 hw_loss 0.018808 lr 0.00091604 rank 7
2023-02-17 19:06:24,356 DEBUG TRAIN Batch 3/4800 loss 26.546841 loss_att 36.037521 loss_ctc 46.285126 loss_rnnt 22.013548 hw_loss 0.006347 lr 0.00091502 rank 2
2023-02-17 19:07:37,842 DEBUG TRAIN Batch 3/4900 loss 21.776573 loss_att 25.907713 loss_ctc 32.897278 loss_rnnt 19.422268 hw_loss 0.084972 lr 0.00091385 rank 0
2023-02-17 19:07:37,850 DEBUG TRAIN Batch 3/4900 loss 18.847277 loss_att 26.286343 loss_ctc 30.259136 loss_rnnt 15.829802 hw_loss 0.015154 lr 0.00091473 rank 4
2023-02-17 19:07:37,851 DEBUG TRAIN Batch 3/4900 loss 16.097466 loss_att 22.519251 loss_ctc 32.962872 loss_rnnt 12.555014 hw_loss 0.017578 lr 0.00091350 rank 2
2023-02-17 19:07:37,854 DEBUG TRAIN Batch 3/4900 loss 25.702946 loss_att 29.743099 loss_ctc 39.321976 loss_rnnt 23.078892 hw_loss 0.000285 lr 0.00091432 rank 6
2023-02-17 19:07:37,855 DEBUG TRAIN Batch 3/4900 loss 25.140884 loss_att 31.037811 loss_ctc 37.666893 loss_rnnt 22.230303 hw_loss 0.114488 lr 0.00091450 rank 7
2023-02-17 19:07:37,855 DEBUG TRAIN Batch 3/4900 loss 46.756157 loss_att 54.220711 loss_ctc 62.683311 loss_rnnt 43.127182 hw_loss 0.023334 lr 0.00091421 rank 3
2023-02-17 19:07:37,860 DEBUG TRAIN Batch 3/4900 loss 31.505196 loss_att 41.473499 loss_ctc 47.153885 loss_rnnt 27.408367 hw_loss 0.031268 lr 0.00091395 rank 1
2023-02-17 19:07:37,862 DEBUG TRAIN Batch 3/4900 loss 12.653867 loss_att 17.167675 loss_ctc 22.323273 loss_rnnt 10.434645 hw_loss 0.051012 lr 0.00091400 rank 5
2023-02-17 19:08:53,213 DEBUG TRAIN Batch 3/5000 loss 19.072689 loss_att 26.399166 loss_ctc 38.757378 loss_rnnt 14.941240 hw_loss 0.077862 lr 0.00091279 rank 6
2023-02-17 19:08:53,218 DEBUG TRAIN Batch 3/5000 loss 34.392010 loss_att 37.956459 loss_ctc 57.538570 loss_rnnt 30.536774 hw_loss 0.105259 lr 0.00091232 rank 0
2023-02-17 19:08:53,219 DEBUG TRAIN Batch 3/5000 loss 37.412193 loss_att 35.750809 loss_ctc 52.501003 loss_rnnt 35.663143 hw_loss 0.130288 lr 0.00091269 rank 3
2023-02-17 19:08:53,220 DEBUG TRAIN Batch 3/5000 loss 38.726345 loss_att 43.935436 loss_ctc 59.426517 loss_rnnt 34.899334 hw_loss 0.047193 lr 0.00091321 rank 4
2023-02-17 19:08:53,222 DEBUG TRAIN Batch 3/5000 loss 20.042433 loss_att 24.342787 loss_ctc 37.118462 loss_rnnt 16.845695 hw_loss 0.112244 lr 0.00091298 rank 7
2023-02-17 19:08:53,223 DEBUG TRAIN Batch 3/5000 loss 51.093487 loss_att 57.019783 loss_ctc 79.965759 loss_rnnt 46.021545 hw_loss 0.069468 lr 0.00091248 rank 5
2023-02-17 19:08:53,224 DEBUG TRAIN Batch 3/5000 loss 18.253897 loss_att 20.716755 loss_ctc 28.045692 loss_rnnt 16.392330 hw_loss 0.118919 lr 0.00091243 rank 1
2023-02-17 19:08:53,225 DEBUG TRAIN Batch 3/5000 loss 24.972141 loss_att 29.849476 loss_ctc 37.987167 loss_rnnt 22.205128 hw_loss 0.105388 lr 0.00091197 rank 2
2023-02-17 19:10:05,964 DEBUG TRAIN Batch 3/5100 loss 22.555578 loss_att 25.518150 loss_ctc 31.117508 loss_rnnt 20.747437 hw_loss 0.138815 lr 0.00091169 rank 4
2023-02-17 19:10:05,967 DEBUG TRAIN Batch 3/5100 loss 32.546394 loss_att 32.552868 loss_ctc 40.092133 loss_rnnt 31.537483 hw_loss 0.002838 lr 0.00091128 rank 6
2023-02-17 19:10:05,967 DEBUG TRAIN Batch 3/5100 loss 17.696287 loss_att 20.266569 loss_ctc 27.079330 loss_rnnt 15.875998 hw_loss 0.103424 lr 0.00091081 rank 0
2023-02-17 19:10:05,972 DEBUG TRAIN Batch 3/5100 loss 12.485117 loss_att 14.733801 loss_ctc 21.427603 loss_rnnt 10.788626 hw_loss 0.102042 lr 0.00091096 rank 5
2023-02-17 19:10:05,973 DEBUG TRAIN Batch 3/5100 loss 32.001801 loss_att 39.458870 loss_ctc 44.867798 loss_rnnt 28.783482 hw_loss 0.021444 lr 0.00091046 rank 2
2023-02-17 19:10:05,975 DEBUG TRAIN Batch 3/5100 loss 34.244522 loss_att 44.220051 loss_ctc 52.205711 loss_rnnt 29.854315 hw_loss 0.000520 lr 0.00091091 rank 1
2023-02-17 19:10:05,985 DEBUG TRAIN Batch 3/5100 loss 49.805855 loss_att 49.400070 loss_ctc 70.532318 loss_rnnt 47.122345 hw_loss 0.002132 lr 0.00091146 rank 7
2023-02-17 19:10:06,018 DEBUG TRAIN Batch 3/5100 loss 22.070698 loss_att 33.356506 loss_ctc 33.510780 loss_rnnt 18.287874 hw_loss 0.000591 lr 0.00091117 rank 3
2023-02-17 19:11:19,503 DEBUG TRAIN Batch 3/5200 loss 25.758997 loss_att 30.810314 loss_ctc 41.043556 loss_rnnt 22.667103 hw_loss 0.081918 lr 0.00091017 rank 4
2023-02-17 19:11:19,504 DEBUG TRAIN Batch 3/5200 loss 37.580589 loss_att 40.801338 loss_ctc 53.186104 loss_rnnt 34.774788 hw_loss 0.151706 lr 0.00090977 rank 6
2023-02-17 19:11:19,509 DEBUG TRAIN Batch 3/5200 loss 22.943939 loss_att 31.230888 loss_ctc 32.592850 loss_rnnt 19.899183 hw_loss 0.189085 lr 0.00090896 rank 2
2023-02-17 19:11:19,511 DEBUG TRAIN Batch 3/5200 loss 28.157026 loss_att 36.469318 loss_ctc 49.039154 loss_rnnt 23.696678 hw_loss 0.025511 lr 0.00090995 rank 7
2023-02-17 19:11:19,512 DEBUG TRAIN Batch 3/5200 loss 14.985570 loss_att 20.278561 loss_ctc 21.999184 loss_rnnt 12.975201 hw_loss 0.031165 lr 0.00090941 rank 1
2023-02-17 19:11:19,514 DEBUG TRAIN Batch 3/5200 loss 18.084404 loss_att 26.358206 loss_ctc 28.916504 loss_rnnt 14.956522 hw_loss 0.054077 lr 0.00090930 rank 0
2023-02-17 19:11:19,513 DEBUG TRAIN Batch 3/5200 loss 16.644211 loss_att 25.731476 loss_ctc 27.498562 loss_rnnt 13.376815 hw_loss 0.005055 lr 0.00090945 rank 5
2023-02-17 19:11:19,522 DEBUG TRAIN Batch 3/5200 loss 26.405386 loss_att 29.387056 loss_ctc 36.893444 loss_rnnt 24.373825 hw_loss 0.069035 lr 0.00090966 rank 3
2023-02-17 19:12:33,881 DEBUG TRAIN Batch 3/5300 loss 15.344671 loss_att 23.425819 loss_ctc 25.043358 loss_rnnt 12.435184 hw_loss 0.000189 lr 0.00090867 rank 4
2023-02-17 19:12:33,883 DEBUG TRAIN Batch 3/5300 loss 15.974751 loss_att 25.208902 loss_ctc 27.401571 loss_rnnt 12.600708 hw_loss 0.006817 lr 0.00090827 rank 6
2023-02-17 19:12:33,885 DEBUG TRAIN Batch 3/5300 loss 21.213808 loss_att 31.110828 loss_ctc 38.925873 loss_rnnt 16.842596 hw_loss 0.056624 lr 0.00090791 rank 1
2023-02-17 19:12:33,887 DEBUG TRAIN Batch 3/5300 loss 19.032482 loss_att 27.629349 loss_ctc 37.270905 loss_rnnt 14.850981 hw_loss 0.056888 lr 0.00090780 rank 0
2023-02-17 19:12:33,889 DEBUG TRAIN Batch 3/5300 loss 21.065811 loss_att 26.704948 loss_ctc 31.739971 loss_rnnt 18.442812 hw_loss 0.134903 lr 0.00090816 rank 3
2023-02-17 19:12:33,892 DEBUG TRAIN Batch 3/5300 loss 27.321068 loss_att 34.497948 loss_ctc 41.349152 loss_rnnt 23.970552 hw_loss 0.083862 lr 0.00090845 rank 7
2023-02-17 19:12:33,894 DEBUG TRAIN Batch 3/5300 loss 24.170553 loss_att 35.829758 loss_ctc 42.071938 loss_rnnt 19.430771 hw_loss 0.039544 lr 0.00090795 rank 5
2023-02-17 19:12:33,930 DEBUG TRAIN Batch 3/5300 loss 17.603033 loss_att 25.834375 loss_ctc 28.346905 loss_rnnt 14.473372 hw_loss 0.095396 lr 0.00090746 rank 2
2023-02-17 19:13:48,203 DEBUG TRAIN Batch 3/5400 loss 36.799129 loss_att 44.923866 loss_ctc 61.430344 loss_rnnt 31.867163 hw_loss 0.042859 lr 0.00090717 rank 4
2023-02-17 19:13:48,206 DEBUG TRAIN Batch 3/5400 loss 19.203020 loss_att 25.059929 loss_ctc 35.866936 loss_rnnt 15.744450 hw_loss 0.122502 lr 0.00090677 rank 6
2023-02-17 19:13:48,210 DEBUG TRAIN Batch 3/5400 loss 24.305964 loss_att 28.643290 loss_ctc 38.652229 loss_rnnt 21.468693 hw_loss 0.106816 lr 0.00090631 rank 0
2023-02-17 19:13:48,210 DEBUG TRAIN Batch 3/5400 loss 15.931286 loss_att 20.985521 loss_ctc 25.052120 loss_rnnt 13.664904 hw_loss 0.073919 lr 0.00090597 rank 2
2023-02-17 19:13:48,210 DEBUG TRAIN Batch 3/5400 loss 18.448063 loss_att 27.704044 loss_ctc 35.020874 loss_rnnt 14.330202 hw_loss 0.106792 lr 0.00090641 rank 1
2023-02-17 19:13:48,216 DEBUG TRAIN Batch 3/5400 loss 37.727531 loss_att 50.317242 loss_ctc 57.059711 loss_rnnt 32.590973 hw_loss 0.076863 lr 0.00090646 rank 5
2023-02-17 19:13:48,222 DEBUG TRAIN Batch 3/5400 loss 45.658413 loss_att 55.782578 loss_ctc 78.545776 loss_rnnt 39.248161 hw_loss 0.000824 lr 0.00090695 rank 7
2023-02-17 19:13:48,258 DEBUG TRAIN Batch 3/5400 loss 45.696957 loss_att 56.878784 loss_ctc 71.855133 loss_rnnt 39.920193 hw_loss 0.098703 lr 0.00090667 rank 3
2023-02-17 19:15:00,751 DEBUG TRAIN Batch 3/5500 loss 17.014793 loss_att 21.725965 loss_ctc 28.747528 loss_rnnt 14.445402 hw_loss 0.117735 lr 0.00090528 rank 6
2023-02-17 19:15:00,759 DEBUG TRAIN Batch 3/5500 loss 25.671122 loss_att 32.201008 loss_ctc 41.661175 loss_rnnt 22.145138 hw_loss 0.165000 lr 0.00090568 rank 4
2023-02-17 19:15:00,762 DEBUG TRAIN Batch 3/5500 loss 30.897013 loss_att 38.687820 loss_ctc 52.360233 loss_rnnt 26.468037 hw_loss 0.016970 lr 0.00090493 rank 1
2023-02-17 19:15:00,764 DEBUG TRAIN Batch 3/5500 loss 28.860294 loss_att 34.955505 loss_ctc 43.195065 loss_rnnt 25.666533 hw_loss 0.118910 lr 0.00090448 rank 2
2023-02-17 19:15:00,765 DEBUG TRAIN Batch 3/5500 loss 26.071035 loss_att 35.857235 loss_ctc 40.330002 loss_rnnt 22.136362 hw_loss 0.142942 lr 0.00090546 rank 7
2023-02-17 19:15:00,767 DEBUG TRAIN Batch 3/5500 loss 19.265030 loss_att 22.899483 loss_ctc 37.192787 loss_rnnt 16.130495 hw_loss 0.032392 lr 0.00090482 rank 0
2023-02-17 19:15:00,779 DEBUG TRAIN Batch 3/5500 loss 29.846783 loss_att 36.915966 loss_ctc 49.699505 loss_rnnt 25.763449 hw_loss 0.042121 lr 0.00090497 rank 5
2023-02-17 19:15:00,813 DEBUG TRAIN Batch 3/5500 loss 27.296242 loss_att 39.041008 loss_ctc 49.638695 loss_rnnt 21.893002 hw_loss 0.141175 lr 0.00090518 rank 3
2023-02-17 19:16:14,767 DEBUG TRAIN Batch 3/5600 loss 26.179375 loss_att 30.912983 loss_ctc 34.812458 loss_rnnt 24.028206 hw_loss 0.100069 lr 0.00090398 rank 7
2023-02-17 19:16:14,766 DEBUG TRAIN Batch 3/5600 loss 34.957520 loss_att 38.117119 loss_ctc 53.174488 loss_rnnt 31.883778 hw_loss 0.024166 lr 0.00090380 rank 6
2023-02-17 19:16:14,766 DEBUG TRAIN Batch 3/5600 loss 32.798298 loss_att 36.499073 loss_ctc 51.282494 loss_rnnt 29.567326 hw_loss 0.049227 lr 0.00090420 rank 4
2023-02-17 19:16:14,767 DEBUG TRAIN Batch 3/5600 loss 17.866127 loss_att 21.674400 loss_ctc 26.105045 loss_rnnt 15.942863 hw_loss 0.118287 lr 0.00090349 rank 5
2023-02-17 19:16:14,768 DEBUG TRAIN Batch 3/5600 loss 47.351551 loss_att 52.165192 loss_ctc 65.742516 loss_rnnt 43.818359 hw_loss 0.221874 lr 0.00090301 rank 2
2023-02-17 19:16:14,771 DEBUG TRAIN Batch 3/5600 loss 29.843597 loss_att 36.507515 loss_ctc 56.313694 loss_rnnt 24.981293 hw_loss 0.000324 lr 0.00090335 rank 0
2023-02-17 19:16:14,774 DEBUG TRAIN Batch 3/5600 loss 31.321575 loss_att 30.387787 loss_ctc 46.678219 loss_rnnt 29.424250 hw_loss 0.068494 lr 0.00090370 rank 3
2023-02-17 19:16:14,820 DEBUG TRAIN Batch 3/5600 loss 20.430561 loss_att 23.447760 loss_ctc 33.498840 loss_rnnt 18.075266 hw_loss 0.017655 lr 0.00090345 rank 1
2023-02-17 19:17:31,319 DEBUG TRAIN Batch 3/5700 loss 26.704823 loss_att 26.654877 loss_ctc 41.662895 loss_rnnt 24.702856 hw_loss 0.032899 lr 0.00090273 rank 4
2023-02-17 19:17:31,323 DEBUG TRAIN Batch 3/5700 loss 22.025404 loss_att 27.433685 loss_ctc 36.936188 loss_rnnt 18.955599 hw_loss 0.000083 lr 0.00090187 rank 0
2023-02-17 19:17:31,328 DEBUG TRAIN Batch 3/5700 loss 21.250910 loss_att 24.235453 loss_ctc 33.597908 loss_rnnt 18.971996 hw_loss 0.067011 lr 0.00090202 rank 5
2023-02-17 19:17:31,331 DEBUG TRAIN Batch 3/5700 loss 23.927660 loss_att 28.214199 loss_ctc 36.652443 loss_rnnt 21.298330 hw_loss 0.141344 lr 0.00090233 rank 6
2023-02-17 19:17:31,330 DEBUG TRAIN Batch 3/5700 loss 15.930419 loss_att 24.151627 loss_ctc 23.957228 loss_rnnt 13.215385 hw_loss 0.001033 lr 0.00090223 rank 3
2023-02-17 19:17:31,351 DEBUG TRAIN Batch 3/5700 loss 19.759134 loss_att 27.442993 loss_ctc 32.744183 loss_rnnt 16.490902 hw_loss 0.000222 lr 0.00090251 rank 7
2023-02-17 19:17:31,354 DEBUG TRAIN Batch 3/5700 loss 19.391100 loss_att 18.155434 loss_ctc 26.449606 loss_rnnt 18.641632 hw_loss 0.104005 lr 0.00090198 rank 1
2023-02-17 19:17:31,378 DEBUG TRAIN Batch 3/5700 loss 16.049736 loss_att 21.591660 loss_ctc 24.131895 loss_rnnt 13.838814 hw_loss 0.046718 lr 0.00090154 rank 2
2023-02-17 19:18:44,080 DEBUG TRAIN Batch 3/5800 loss 24.639893 loss_att 30.485031 loss_ctc 37.644424 loss_rnnt 21.685497 hw_loss 0.096435 lr 0.00090126 rank 4
2023-02-17 19:18:44,083 DEBUG TRAIN Batch 3/5800 loss 37.491249 loss_att 46.307350 loss_ctc 52.755173 loss_rnnt 33.658226 hw_loss 0.064900 lr 0.00090008 rank 2
2023-02-17 19:18:44,083 DEBUG TRAIN Batch 3/5800 loss 33.752705 loss_att 43.182053 loss_ctc 57.128551 loss_rnnt 28.716183 hw_loss 0.063501 lr 0.00090051 rank 1
2023-02-17 19:18:44,085 DEBUG TRAIN Batch 3/5800 loss 29.825909 loss_att 36.986614 loss_ctc 46.882629 loss_rnnt 26.119366 hw_loss 0.000326 lr 0.00090076 rank 3
2023-02-17 19:18:44,087 DEBUG TRAIN Batch 3/5800 loss 32.917259 loss_att 44.102547 loss_ctc 53.676208 loss_rnnt 27.877066 hw_loss 0.066147 lr 0.00090041 rank 0
2023-02-17 19:18:44,089 DEBUG TRAIN Batch 3/5800 loss 31.472801 loss_att 38.598488 loss_ctc 50.621223 loss_rnnt 27.461477 hw_loss 0.061995 lr 0.00090086 rank 6
2023-02-17 19:18:44,091 DEBUG TRAIN Batch 3/5800 loss 17.571289 loss_att 26.278240 loss_ctc 38.059643 loss_rnnt 13.080322 hw_loss 0.033371 lr 0.00090056 rank 5
2023-02-17 19:18:44,134 DEBUG TRAIN Batch 3/5800 loss 27.531729 loss_att 37.645897 loss_ctc 40.248322 loss_rnnt 23.813179 hw_loss 0.000318 lr 0.00090104 rank 7
2023-02-17 19:19:57,331 DEBUG TRAIN Batch 3/5900 loss 43.376297 loss_att 45.771004 loss_ctc 66.741364 loss_rnnt 39.737541 hw_loss 0.083391 lr 0.00089862 rank 2
2023-02-17 19:19:57,331 DEBUG TRAIN Batch 3/5900 loss 28.599350 loss_att 31.876823 loss_ctc 42.558567 loss_rnnt 26.015167 hw_loss 0.126486 lr 0.00089980 rank 4
2023-02-17 19:19:57,334 DEBUG TRAIN Batch 3/5900 loss 38.922958 loss_att 47.162746 loss_ctc 64.821823 loss_rnnt 33.785484 hw_loss 0.068116 lr 0.00089941 rank 6
2023-02-17 19:19:57,334 DEBUG TRAIN Batch 3/5900 loss 25.845745 loss_att 32.131767 loss_ctc 44.994762 loss_rnnt 22.016426 hw_loss 0.035462 lr 0.00089906 rank 1
2023-02-17 19:19:57,335 DEBUG TRAIN Batch 3/5900 loss 21.517197 loss_att 29.797844 loss_ctc 42.272007 loss_rnnt 17.067228 hw_loss 0.049743 lr 0.00089930 rank 3
2023-02-17 19:19:57,336 DEBUG TRAIN Batch 3/5900 loss 35.444729 loss_att 44.737869 loss_ctc 57.965298 loss_rnnt 30.570412 hw_loss 0.024267 lr 0.00089958 rank 7
2023-02-17 19:19:57,339 DEBUG TRAIN Batch 3/5900 loss 31.299458 loss_att 35.128139 loss_ctc 46.647583 loss_rnnt 28.474869 hw_loss 0.023319 lr 0.00089895 rank 0
2023-02-17 19:19:57,386 DEBUG TRAIN Batch 3/5900 loss 51.568348 loss_att 62.848763 loss_ctc 81.640747 loss_rnnt 45.260601 hw_loss 0.078777 lr 0.00089910 rank 5
2023-02-17 19:21:11,308 DEBUG TRAIN Batch 3/6000 loss 25.707777 loss_att 33.749107 loss_ctc 48.371841 loss_rnnt 21.043854 hw_loss 0.063340 lr 0.00089835 rank 4
2023-02-17 19:21:11,309 DEBUG TRAIN Batch 3/6000 loss 25.180851 loss_att 33.959686 loss_ctc 35.685265 loss_rnnt 22.023640 hw_loss 0.001603 lr 0.00089717 rank 2
2023-02-17 19:21:11,310 DEBUG TRAIN Batch 3/6000 loss 59.498001 loss_att 63.671013 loss_ctc 78.754128 loss_rnnt 56.095726 hw_loss 0.000367 lr 0.00089795 rank 6
2023-02-17 19:21:11,310 DEBUG TRAIN Batch 3/6000 loss 26.763893 loss_att 37.160309 loss_ctc 48.252350 loss_rnnt 21.765484 hw_loss 0.101245 lr 0.00089761 rank 1
2023-02-17 19:21:11,312 DEBUG TRAIN Batch 3/6000 loss 27.832325 loss_att 31.898808 loss_ctc 39.071072 loss_rnnt 25.520262 hw_loss 0.000503 lr 0.00089765 rank 5
2023-02-17 19:21:11,313 DEBUG TRAIN Batch 3/6000 loss 11.842682 loss_att 15.025009 loss_ctc 21.782394 loss_rnnt 9.848297 hw_loss 0.061169 lr 0.00089751 rank 0
2023-02-17 19:21:11,314 DEBUG TRAIN Batch 3/6000 loss 27.710258 loss_att 36.513733 loss_ctc 44.999908 loss_rnnt 23.628778 hw_loss 0.029058 lr 0.00089785 rank 3
2023-02-17 19:21:11,317 DEBUG TRAIN Batch 3/6000 loss 34.626472 loss_att 48.043049 loss_ctc 61.984875 loss_rnnt 28.263823 hw_loss 0.059142 lr 0.00089813 rank 7
2023-02-17 19:22:24,887 DEBUG TRAIN Batch 3/6100 loss 22.870909 loss_att 28.687349 loss_ctc 33.502831 loss_rnnt 20.264477 hw_loss 0.047912 lr 0.00089651 rank 6
2023-02-17 19:22:24,889 DEBUG TRAIN Batch 3/6100 loss 24.749701 loss_att 31.068913 loss_ctc 45.792057 loss_rnnt 20.648022 hw_loss 0.060351 lr 0.00089690 rank 4
2023-02-17 19:22:24,889 DEBUG TRAIN Batch 3/6100 loss 36.760410 loss_att 40.627991 loss_ctc 47.083927 loss_rnnt 34.585415 hw_loss 0.046892 lr 0.00089616 rank 1
2023-02-17 19:22:24,892 DEBUG TRAIN Batch 3/6100 loss 14.092860 loss_att 22.988840 loss_ctc 23.664330 loss_rnnt 11.008246 hw_loss 0.054791 lr 0.00089668 rank 7
2023-02-17 19:22:24,895 DEBUG TRAIN Batch 3/6100 loss 19.879917 loss_att 25.841007 loss_ctc 32.092102 loss_rnnt 17.000660 hw_loss 0.110149 lr 0.00089621 rank 5
2023-02-17 19:22:24,895 DEBUG TRAIN Batch 3/6100 loss 16.248384 loss_att 21.026628 loss_ctc 21.818325 loss_rnnt 14.483212 hw_loss 0.125373 lr 0.00089573 rank 2
2023-02-17 19:22:24,896 DEBUG TRAIN Batch 3/6100 loss 40.083923 loss_att 50.387878 loss_ctc 62.484467 loss_rnnt 34.998596 hw_loss 0.070867 lr 0.00089606 rank 0
2023-02-17 19:22:24,941 DEBUG TRAIN Batch 3/6100 loss 32.219296 loss_att 36.590012 loss_ctc 48.208302 loss_rnnt 29.209908 hw_loss 0.006337 lr 0.00089641 rank 3
2023-02-17 19:23:37,334 DEBUG TRAIN Batch 3/6200 loss 23.998320 loss_att 27.446400 loss_ctc 35.660835 loss_rnnt 21.707455 hw_loss 0.086713 lr 0.00089546 rank 4
2023-02-17 19:23:37,337 DEBUG TRAIN Batch 3/6200 loss 34.163254 loss_att 41.154137 loss_ctc 47.109921 loss_rnnt 31.038559 hw_loss 0.000556 lr 0.00089524 rank 7
2023-02-17 19:23:37,338 DEBUG TRAIN Batch 3/6200 loss 31.687931 loss_att 41.652355 loss_ctc 54.093170 loss_rnnt 26.707518 hw_loss 0.000306 lr 0.00089463 rank 0
2023-02-17 19:23:37,341 DEBUG TRAIN Batch 3/6200 loss 33.230125 loss_att 35.981186 loss_ctc 49.485500 loss_rnnt 30.483288 hw_loss 0.054824 lr 0.00089473 rank 1
2023-02-17 19:23:37,346 DEBUG TRAIN Batch 3/6200 loss 18.781176 loss_att 24.045221 loss_ctc 34.340294 loss_rnnt 15.629972 hw_loss 0.044710 lr 0.00089497 rank 3
2023-02-17 19:23:37,346 DEBUG TRAIN Batch 3/6200 loss 42.040302 loss_att 57.954700 loss_ctc 63.196594 loss_rnnt 36.021214 hw_loss 0.028812 lr 0.00089477 rank 5
2023-02-17 19:23:37,369 DEBUG TRAIN Batch 3/6200 loss 17.130066 loss_att 23.314537 loss_ctc 27.041601 loss_rnnt 14.513176 hw_loss 0.109607 lr 0.00089430 rank 2
2023-02-17 19:23:37,377 DEBUG TRAIN Batch 3/6200 loss 31.590239 loss_att 37.440147 loss_ctc 52.671234 loss_rnnt 27.596245 hw_loss 0.024771 lr 0.00089507 rank 6
2023-02-17 19:24:50,917 DEBUG TRAIN Batch 3/6300 loss 34.786747 loss_att 44.952213 loss_ctc 50.733227 loss_rnnt 30.571711 hw_loss 0.104521 lr 0.00089330 rank 1
2023-02-17 19:24:50,919 DEBUG TRAIN Batch 3/6300 loss 18.354874 loss_att 20.627371 loss_ctc 26.875246 loss_rnnt 16.760748 hw_loss 0.006702 lr 0.00089381 rank 7
2023-02-17 19:24:50,919 DEBUG TRAIN Batch 3/6300 loss 36.011276 loss_att 37.567913 loss_ctc 56.215878 loss_rnnt 32.943394 hw_loss 0.117385 lr 0.00089334 rank 5
2023-02-17 19:24:50,922 DEBUG TRAIN Batch 3/6300 loss 15.673902 loss_att 17.674726 loss_ctc 27.589458 loss_rnnt 13.629907 hw_loss 0.103291 lr 0.00089354 rank 3
2023-02-17 19:24:50,922 DEBUG TRAIN Batch 3/6300 loss 44.652481 loss_att 48.775848 loss_ctc 60.869518 loss_rnnt 41.605057 hw_loss 0.113390 lr 0.00089287 rank 2
2023-02-17 19:24:50,926 DEBUG TRAIN Batch 3/6300 loss 24.159563 loss_att 29.006510 loss_ctc 31.951513 loss_rnnt 22.076412 hw_loss 0.140313 lr 0.00089403 rank 4
2023-02-17 19:24:50,928 DEBUG TRAIN Batch 3/6300 loss 37.084301 loss_att 40.852448 loss_ctc 49.172405 loss_rnnt 34.655590 hw_loss 0.118755 lr 0.00089364 rank 6
2023-02-17 19:24:50,929 DEBUG TRAIN Batch 3/6300 loss 40.358006 loss_att 40.876099 loss_ctc 59.435333 loss_rnnt 37.641445 hw_loss 0.129928 lr 0.00089320 rank 0
2023-02-17 19:26:05,736 DEBUG TRAIN Batch 3/6400 loss 29.209705 loss_att 35.911598 loss_ctc 41.783497 loss_rnnt 26.159437 hw_loss 0.062594 lr 0.00089222 rank 6
2023-02-17 19:26:05,735 DEBUG TRAIN Batch 3/6400 loss 16.894344 loss_att 29.773046 loss_ctc 40.818504 loss_rnnt 11.085537 hw_loss 0.080958 lr 0.00089260 rank 4
2023-02-17 19:26:05,735 DEBUG TRAIN Batch 3/6400 loss 14.101546 loss_att 14.686531 loss_ctc 21.726280 loss_rnnt 12.869322 hw_loss 0.184867 lr 0.00089178 rank 0
2023-02-17 19:26:05,738 DEBUG TRAIN Batch 3/6400 loss 27.081846 loss_att 34.474796 loss_ctc 50.262798 loss_rnnt 22.485472 hw_loss 0.050605 lr 0.00089239 rank 7
2023-02-17 19:26:05,740 DEBUG TRAIN Batch 3/6400 loss 16.277292 loss_att 24.354328 loss_ctc 25.177359 loss_rnnt 13.475067 hw_loss 0.000267 lr 0.00089212 rank 3
2023-02-17 19:26:05,746 DEBUG TRAIN Batch 3/6400 loss 17.492758 loss_att 27.225965 loss_ctc 27.498409 loss_rnnt 14.190932 hw_loss 0.039558 lr 0.00089145 rank 2
2023-02-17 19:26:05,747 DEBUG TRAIN Batch 3/6400 loss 15.958522 loss_att 16.817526 loss_ctc 22.220678 loss_rnnt 14.860587 hw_loss 0.170963 lr 0.00089192 rank 5
2023-02-17 19:26:05,761 DEBUG TRAIN Batch 3/6400 loss 14.772660 loss_att 15.883610 loss_ctc 19.385717 loss_rnnt 13.892133 hw_loss 0.081117 lr 0.00089188 rank 1
2023-02-17 19:27:18,235 DEBUG TRAIN Batch 3/6500 loss 27.944418 loss_att 35.772808 loss_ctc 52.644222 loss_rnnt 23.072464 hw_loss 0.024314 lr 0.00089118 rank 4
2023-02-17 19:27:18,243 DEBUG TRAIN Batch 3/6500 loss 24.820068 loss_att 29.980473 loss_ctc 43.186287 loss_rnnt 21.338919 hw_loss 0.000449 lr 0.00089036 rank 0
2023-02-17 19:27:18,243 DEBUG TRAIN Batch 3/6500 loss 21.295732 loss_att 31.259487 loss_ctc 34.117645 loss_rnnt 17.570803 hw_loss 0.042358 lr 0.00089070 rank 3
2023-02-17 19:27:18,244 DEBUG TRAIN Batch 3/6500 loss 40.203995 loss_att 43.130337 loss_ctc 61.147884 loss_rnnt 36.781372 hw_loss 0.084066 lr 0.00089046 rank 1
2023-02-17 19:27:18,244 DEBUG TRAIN Batch 3/6500 loss 20.579123 loss_att 24.420862 loss_ctc 28.796673 loss_rnnt 18.701057 hw_loss 0.026333 lr 0.00089004 rank 2
2023-02-17 19:27:18,249 DEBUG TRAIN Batch 3/6500 loss 23.160048 loss_att 29.300432 loss_ctc 35.435234 loss_rnnt 20.260410 hw_loss 0.065378 lr 0.00089080 rank 6
2023-02-17 19:27:18,249 DEBUG TRAIN Batch 3/6500 loss 23.887325 loss_att 34.191986 loss_ctc 45.854870 loss_rnnt 18.826607 hw_loss 0.132714 lr 0.00089097 rank 7
2023-02-17 19:27:18,292 DEBUG TRAIN Batch 3/6500 loss 25.836443 loss_att 33.578495 loss_ctc 44.434673 loss_rnnt 21.807425 hw_loss 0.001578 lr 0.00089050 rank 5
2023-02-17 19:28:30,833 DEBUG TRAIN Batch 3/6600 loss 41.384254 loss_att 46.107498 loss_ctc 62.073181 loss_rnnt 37.643063 hw_loss 0.071289 lr 0.00088977 rank 4
2023-02-17 19:28:30,837 DEBUG TRAIN Batch 3/6600 loss 23.026825 loss_att 30.611252 loss_ctc 35.215591 loss_rnnt 19.868690 hw_loss 0.030150 lr 0.00088956 rank 7
2023-02-17 19:28:30,837 DEBUG TRAIN Batch 3/6600 loss 28.283377 loss_att 42.976707 loss_ctc 47.935665 loss_rnnt 22.724255 hw_loss 0.000280 lr 0.00088863 rank 2
2023-02-17 19:28:30,839 DEBUG TRAIN Batch 3/6600 loss 17.963451 loss_att 24.795034 loss_ctc 24.950001 loss_rnnt 15.626899 hw_loss 0.072556 lr 0.00088939 rank 6
2023-02-17 19:28:30,838 DEBUG TRAIN Batch 3/6600 loss 25.470865 loss_att 29.560444 loss_ctc 34.326736 loss_rnnt 23.462896 hw_loss 0.017383 lr 0.00088905 rank 1
2023-02-17 19:28:30,846 DEBUG TRAIN Batch 3/6600 loss 34.721703 loss_att 38.242172 loss_ctc 38.719921 loss_rnnt 33.472267 hw_loss 0.022969 lr 0.00088909 rank 5
2023-02-17 19:28:30,847 DEBUG TRAIN Batch 3/6600 loss 20.834309 loss_att 24.244972 loss_ctc 28.707937 loss_rnnt 19.053465 hw_loss 0.091677 lr 0.00088895 rank 0
2023-02-17 19:28:30,849 DEBUG TRAIN Batch 3/6600 loss 23.230591 loss_att 33.524395 loss_ctc 45.026917 loss_rnnt 18.233353 hw_loss 0.060563 lr 0.00088929 rank 3
2023-02-17 19:29:44,114 DEBUG TRAIN Batch 3/6700 loss 17.914053 loss_att 24.203972 loss_ctc 27.918030 loss_rnnt 15.282722 hw_loss 0.074027 lr 0.00088815 rank 7
2023-02-17 19:29:44,118 DEBUG TRAIN Batch 3/6700 loss 30.171873 loss_att 35.350605 loss_ctc 48.889477 loss_rnnt 26.603111 hw_loss 0.069998 lr 0.00088836 rank 4
2023-02-17 19:29:44,119 DEBUG TRAIN Batch 3/6700 loss 21.645824 loss_att 31.259560 loss_ctc 39.461990 loss_rnnt 17.347525 hw_loss 0.000118 lr 0.00088765 rank 1
2023-02-17 19:29:44,125 DEBUG TRAIN Batch 3/6700 loss 40.278069 loss_att 40.087063 loss_ctc 53.857681 loss_rnnt 38.494247 hw_loss 0.021380 lr 0.00088799 rank 6
2023-02-17 19:29:44,129 DEBUG TRAIN Batch 3/6700 loss 17.513615 loss_att 23.572794 loss_ctc 34.135887 loss_rnnt 14.064110 hw_loss 0.040065 lr 0.00088723 rank 2
2023-02-17 19:29:44,129 DEBUG TRAIN Batch 3/6700 loss 24.919519 loss_att 27.906237 loss_ctc 33.472374 loss_rnnt 23.112642 hw_loss 0.129662 lr 0.00088789 rank 3
2023-02-17 19:29:44,130 DEBUG TRAIN Batch 3/6700 loss 34.719383 loss_att 47.422211 loss_ctc 56.456650 loss_rnnt 29.241280 hw_loss 0.073566 lr 0.00088769 rank 5
2023-02-17 19:29:44,132 DEBUG TRAIN Batch 3/6700 loss 19.376261 loss_att 27.693916 loss_ctc 39.255455 loss_rnnt 15.043409 hw_loss 0.035177 lr 0.00088755 rank 0
2023-02-17 19:30:58,412 DEBUG TRAIN Batch 3/6800 loss 24.468273 loss_att 32.889011 loss_ctc 42.070530 loss_rnnt 20.422989 hw_loss 0.026564 lr 0.00088659 rank 6
2023-02-17 19:30:58,416 DEBUG TRAIN Batch 3/6800 loss 36.524899 loss_att 43.194691 loss_ctc 51.356354 loss_rnnt 33.180397 hw_loss 0.061899 lr 0.00088697 rank 4
2023-02-17 19:30:58,419 DEBUG TRAIN Batch 3/6800 loss 24.331900 loss_att 31.482319 loss_ctc 37.647484 loss_rnnt 21.126350 hw_loss 0.000100 lr 0.00088676 rank 7
2023-02-17 19:30:58,419 DEBUG TRAIN Batch 3/6800 loss 23.854021 loss_att 30.142845 loss_ctc 37.104912 loss_rnnt 20.824512 hw_loss 0.009294 lr 0.00088616 rank 0
2023-02-17 19:30:58,420 DEBUG TRAIN Batch 3/6800 loss 40.628273 loss_att 38.096077 loss_ctc 56.303936 loss_rnnt 38.988811 hw_loss 0.104644 lr 0.00088625 rank 1
2023-02-17 19:30:58,421 DEBUG TRAIN Batch 3/6800 loss 25.949219 loss_att 31.628761 loss_ctc 43.329243 loss_rnnt 22.486853 hw_loss 0.017105 lr 0.00088584 rank 2
2023-02-17 19:30:58,422 DEBUG TRAIN Batch 3/6800 loss 31.103540 loss_att 38.048756 loss_ctc 48.898819 loss_rnnt 27.317436 hw_loss 0.045668 lr 0.00088630 rank 5
2023-02-17 19:30:58,427 DEBUG TRAIN Batch 3/6800 loss 31.186584 loss_att 37.613808 loss_ctc 53.629051 loss_rnnt 26.843441 hw_loss 0.122572 lr 0.00088649 rank 3
2023-02-17 19:32:11,643 DEBUG TRAIN Batch 3/6900 loss 30.903975 loss_att 33.773235 loss_ctc 42.658001 loss_rnnt 28.734091 hw_loss 0.054048 lr 0.00088557 rank 4
2023-02-17 19:32:11,644 DEBUG TRAIN Batch 3/6900 loss 25.339855 loss_att 27.461645 loss_ctc 36.909477 loss_rnnt 23.286253 hw_loss 0.162426 lr 0.00088445 rank 2
2023-02-17 19:32:11,644 DEBUG TRAIN Batch 3/6900 loss 16.781221 loss_att 25.632526 loss_ctc 34.263016 loss_rnnt 12.633473 hw_loss 0.087339 lr 0.00088520 rank 6
2023-02-17 19:32:11,644 DEBUG TRAIN Batch 3/6900 loss 29.462997 loss_att 36.984169 loss_ctc 43.853401 loss_rnnt 26.011896 hw_loss 0.052774 lr 0.00088487 rank 1
2023-02-17 19:32:11,645 DEBUG TRAIN Batch 3/6900 loss 25.596779 loss_att 30.477833 loss_ctc 40.197033 loss_rnnt 22.644836 hw_loss 0.054437 lr 0.00088477 rank 0
2023-02-17 19:32:11,648 DEBUG TRAIN Batch 3/6900 loss 25.103456 loss_att 27.459999 loss_ctc 37.610237 loss_rnnt 22.951323 hw_loss 0.024852 lr 0.00088536 rank 7
2023-02-17 19:32:11,655 DEBUG TRAIN Batch 3/6900 loss 15.589418 loss_att 19.638996 loss_ctc 23.730688 loss_rnnt 13.664535 hw_loss 0.055248 lr 0.00088491 rank 5
2023-02-17 19:32:11,695 DEBUG TRAIN Batch 3/6900 loss 25.589912 loss_att 32.352165 loss_ctc 40.401436 loss_rnnt 22.219513 hw_loss 0.080772 lr 0.00088510 rank 3
2023-02-17 19:33:24,644 DEBUG TRAIN Batch 3/7000 loss 24.007509 loss_att 34.252060 loss_ctc 42.986519 loss_rnnt 19.386343 hw_loss 0.078226 lr 0.00088419 rank 4
2023-02-17 19:33:24,646 DEBUG TRAIN Batch 3/7000 loss 28.397997 loss_att 26.908329 loss_ctc 35.973129 loss_rnnt 27.588209 hw_loss 0.183198 lr 0.00088381 rank 6
2023-02-17 19:33:24,649 DEBUG TRAIN Batch 3/7000 loss 19.594452 loss_att 20.683739 loss_ctc 25.903728 loss_rnnt 18.453857 hw_loss 0.152815 lr 0.00088339 rank 0
2023-02-17 19:33:24,652 DEBUG TRAIN Batch 3/7000 loss 16.458595 loss_att 20.042953 loss_ctc 27.898951 loss_rnnt 14.144925 hw_loss 0.133904 lr 0.00088348 rank 1
2023-02-17 19:33:24,653 DEBUG TRAIN Batch 3/7000 loss 30.686081 loss_att 39.359482 loss_ctc 53.806915 loss_rnnt 25.843639 hw_loss 0.046846 lr 0.00088372 rank 3
2023-02-17 19:33:24,655 DEBUG TRAIN Batch 3/7000 loss 29.293024 loss_att 35.084545 loss_ctc 45.606117 loss_rnnt 25.923834 hw_loss 0.067138 lr 0.00088352 rank 5
2023-02-17 19:33:24,663 DEBUG TRAIN Batch 3/7000 loss 18.635180 loss_att 23.509136 loss_ctc 26.702774 loss_rnnt 16.544027 hw_loss 0.076278 lr 0.00088398 rank 7
2023-02-17 19:33:24,707 DEBUG TRAIN Batch 3/7000 loss 30.863256 loss_att 43.203960 loss_ctc 48.379967 loss_rnnt 26.024750 hw_loss 0.065255 lr 0.00088307 rank 2
2023-02-17 19:34:39,856 DEBUG TRAIN Batch 3/7100 loss 29.703064 loss_att 39.256565 loss_ctc 45.858437 loss_rnnt 25.624319 hw_loss 0.026240 lr 0.00088170 rank 2
2023-02-17 19:34:39,857 DEBUG TRAIN Batch 3/7100 loss 26.772125 loss_att 34.001671 loss_ctc 39.614494 loss_rnnt 23.583298 hw_loss 0.057385 lr 0.00088281 rank 4
2023-02-17 19:34:39,858 DEBUG TRAIN Batch 3/7100 loss 21.086504 loss_att 30.124153 loss_ctc 29.278622 loss_rnnt 18.135731 hw_loss 0.095552 lr 0.00088244 rank 6
2023-02-17 19:34:39,862 DEBUG TRAIN Batch 3/7100 loss 33.588757 loss_att 47.211319 loss_ctc 57.602814 loss_rnnt 27.579380 hw_loss 0.155597 lr 0.00088201 rank 0
2023-02-17 19:34:39,864 DEBUG TRAIN Batch 3/7100 loss 25.882290 loss_att 36.849369 loss_ctc 39.484901 loss_rnnt 21.853024 hw_loss 0.041570 lr 0.00088211 rank 1
2023-02-17 19:34:39,865 DEBUG TRAIN Batch 3/7100 loss 32.184036 loss_att 35.631580 loss_ctc 42.781494 loss_rnnt 30.072073 hw_loss 0.017736 lr 0.00088260 rank 7
2023-02-17 19:34:39,871 DEBUG TRAIN Batch 3/7100 loss 19.605728 loss_att 24.150930 loss_ctc 30.233465 loss_rnnt 17.263735 hw_loss 0.029848 lr 0.00088234 rank 3
2023-02-17 19:34:39,884 DEBUG TRAIN Batch 3/7100 loss 39.631939 loss_att 53.507256 loss_ctc 61.215546 loss_rnnt 33.967026 hw_loss 0.022568 lr 0.00088215 rank 5
2023-02-17 19:35:52,327 DEBUG TRAIN Batch 3/7200 loss 45.406380 loss_att 50.246140 loss_ctc 66.775032 loss_rnnt 41.532158 hw_loss 0.107089 lr 0.00088144 rank 4
2023-02-17 19:35:52,329 DEBUG TRAIN Batch 3/7200 loss 22.859747 loss_att 24.695309 loss_ctc 40.266979 loss_rnnt 20.124609 hw_loss 0.088241 lr 0.00088064 rank 0
2023-02-17 19:35:52,333 DEBUG TRAIN Batch 3/7200 loss 44.231834 loss_att 48.497025 loss_ctc 71.712051 loss_rnnt 39.713295 hw_loss 0.002760 lr 0.00088107 rank 6
2023-02-17 19:35:52,335 DEBUG TRAIN Batch 3/7200 loss 29.409952 loss_att 40.281094 loss_ctc 55.597622 loss_rnnt 23.742695 hw_loss 0.002507 lr 0.00088097 rank 3
2023-02-17 19:35:52,336 DEBUG TRAIN Batch 3/7200 loss 38.166023 loss_att 41.775436 loss_ctc 46.557217 loss_rnnt 36.292892 hw_loss 0.060784 lr 0.00088078 rank 5
2023-02-17 19:35:52,338 DEBUG TRAIN Batch 3/7200 loss 21.949759 loss_att 26.473293 loss_ctc 32.302994 loss_rnnt 19.662872 hw_loss 0.003278 lr 0.00088033 rank 2
2023-02-17 19:35:52,340 DEBUG TRAIN Batch 3/7200 loss 43.242588 loss_att 52.825611 loss_ctc 64.368637 loss_rnnt 38.503632 hw_loss 0.010400 lr 0.00088074 rank 1
2023-02-17 19:35:52,388 DEBUG TRAIN Batch 3/7200 loss 19.965635 loss_att 23.347065 loss_ctc 34.421124 loss_rnnt 17.326620 hw_loss 0.066247 lr 0.00088123 rank 7
2023-02-17 19:37:04,402 DEBUG TRAIN Batch 3/7300 loss 33.010811 loss_att 36.291706 loss_ctc 52.495583 loss_rnnt 29.756577 hw_loss 0.000165 lr 0.00087970 rank 6
2023-02-17 19:37:04,404 DEBUG TRAIN Batch 3/7300 loss 21.146502 loss_att 28.827183 loss_ctc 34.867218 loss_rnnt 17.749161 hw_loss 0.059582 lr 0.00088007 rank 4
2023-02-17 19:37:04,406 DEBUG TRAIN Batch 3/7300 loss 20.445927 loss_att 26.256420 loss_ctc 34.753284 loss_rnnt 17.374882 hw_loss 0.002437 lr 0.00087897 rank 2
2023-02-17 19:37:04,407 DEBUG TRAIN Batch 3/7300 loss 19.419891 loss_att 26.759392 loss_ctc 30.304817 loss_rnnt 16.456093 hw_loss 0.083580 lr 0.00087986 rank 7
2023-02-17 19:37:04,412 DEBUG TRAIN Batch 3/7300 loss 33.537010 loss_att 42.501541 loss_ctc 52.472286 loss_rnnt 29.211126 hw_loss 0.015517 lr 0.00087942 rank 5
2023-02-17 19:37:04,413 DEBUG TRAIN Batch 3/7300 loss 49.742641 loss_att 56.540344 loss_ctc 74.481041 loss_rnnt 45.027298 hw_loss 0.107523 lr 0.00087928 rank 0
2023-02-17 19:37:04,413 DEBUG TRAIN Batch 3/7300 loss 25.426235 loss_att 31.482563 loss_ctc 37.768917 loss_rnnt 22.552914 hw_loss 0.030688 lr 0.00087937 rank 1
2023-02-17 19:37:04,419 DEBUG TRAIN Batch 3/7300 loss 30.412914 loss_att 39.108814 loss_ctc 46.887352 loss_rnnt 26.440582 hw_loss 0.068552 lr 0.00087961 rank 3
2023-02-17 19:38:16,764 DEBUG TRAIN Batch 3/7400 loss 35.158066 loss_att 36.500584 loss_ctc 40.849785 loss_rnnt 34.104053 hw_loss 0.049896 lr 0.00087851 rank 7
2023-02-17 19:38:16,772 DEBUG TRAIN Batch 3/7400 loss 23.147457 loss_att 28.133411 loss_ctc 36.419724 loss_rnnt 20.355026 hw_loss 0.048009 lr 0.00087871 rank 4
2023-02-17 19:38:16,776 DEBUG TRAIN Batch 3/7400 loss 42.055523 loss_att 46.411270 loss_ctc 55.741932 loss_rnnt 39.325157 hw_loss 0.064424 lr 0.00087834 rank 6
2023-02-17 19:38:16,776 DEBUG TRAIN Batch 3/7400 loss 22.437092 loss_att 31.020046 loss_ctc 41.113392 loss_rnnt 18.210344 hw_loss 0.037464 lr 0.00087792 rank 0
2023-02-17 19:38:16,782 DEBUG TRAIN Batch 3/7400 loss 35.692410 loss_att 41.853329 loss_ctc 56.039673 loss_rnnt 31.686045 hw_loss 0.114774 lr 0.00087802 rank 1
2023-02-17 19:38:16,782 DEBUG TRAIN Batch 3/7400 loss 38.333622 loss_att 38.790642 loss_ctc 52.926453 loss_rnnt 36.248749 hw_loss 0.089546 lr 0.00087806 rank 5
2023-02-17 19:38:16,783 DEBUG TRAIN Batch 3/7400 loss 47.840973 loss_att 56.024437 loss_ctc 73.316925 loss_rnnt 42.765015 hw_loss 0.079635 lr 0.00087825 rank 3
2023-02-17 19:38:16,785 DEBUG TRAIN Batch 3/7400 loss 19.563446 loss_att 23.391912 loss_ctc 30.744902 loss_rnnt 17.298996 hw_loss 0.014805 lr 0.00087761 rank 2
2023-02-17 19:39:32,060 DEBUG TRAIN Batch 3/7500 loss 25.381159 loss_att 30.695141 loss_ctc 39.078606 loss_rnnt 22.491989 hw_loss 0.000089 lr 0.00087736 rank 4
2023-02-17 19:39:32,061 DEBUG TRAIN Batch 3/7500 loss 25.785456 loss_att 31.135944 loss_ctc 40.253139 loss_rnnt 22.749157 hw_loss 0.069707 lr 0.00087690 rank 3
2023-02-17 19:39:32,062 DEBUG TRAIN Batch 3/7500 loss 22.574322 loss_att 31.740992 loss_ctc 35.399185 loss_rnnt 18.977047 hw_loss 0.101174 lr 0.00087699 rank 6
2023-02-17 19:39:32,063 DEBUG TRAIN Batch 3/7500 loss 26.333340 loss_att 35.296417 loss_ctc 41.803967 loss_rnnt 22.436674 hw_loss 0.077439 lr 0.00087715 rank 7
2023-02-17 19:39:32,063 DEBUG TRAIN Batch 3/7500 loss 31.717165 loss_att 39.627781 loss_ctc 50.631706 loss_rnnt 27.575731 hw_loss 0.070070 lr 0.00087657 rank 0
2023-02-17 19:39:32,064 DEBUG TRAIN Batch 3/7500 loss 22.869188 loss_att 26.419596 loss_ctc 37.239998 loss_rnnt 20.233511 hw_loss 0.017793 lr 0.00087626 rank 2
2023-02-17 19:39:32,068 DEBUG TRAIN Batch 3/7500 loss 19.222876 loss_att 22.350533 loss_ctc 25.776829 loss_rnnt 17.680553 hw_loss 0.080494 lr 0.00087667 rank 1
2023-02-17 19:39:32,117 DEBUG TRAIN Batch 3/7500 loss 33.704887 loss_att 45.058342 loss_ctc 53.198235 loss_rnnt 28.808531 hw_loss 0.049778 lr 0.00087671 rank 5
2023-02-17 19:40:45,745 DEBUG TRAIN Batch 3/7600 loss 13.645319 loss_att 13.315016 loss_ctc 16.828388 loss_rnnt 13.238495 hw_loss 0.090893 lr 0.00087581 rank 7
2023-02-17 19:40:45,758 DEBUG TRAIN Batch 3/7600 loss 15.644801 loss_att 17.517941 loss_ctc 26.621265 loss_rnnt 13.736723 hw_loss 0.131103 lr 0.00087601 rank 4
2023-02-17 19:40:45,761 DEBUG TRAIN Batch 3/7600 loss 24.990192 loss_att 30.481548 loss_ctc 37.554836 loss_rnnt 22.194351 hw_loss 0.041782 lr 0.00087564 rank 6
2023-02-17 19:40:45,765 DEBUG TRAIN Batch 3/7600 loss 35.745720 loss_att 39.822678 loss_ctc 45.276592 loss_rnnt 33.659405 hw_loss 0.000264 lr 0.00087555 rank 3
2023-02-17 19:40:45,767 DEBUG TRAIN Batch 3/7600 loss 26.106972 loss_att 27.992092 loss_ctc 43.034386 loss_rnnt 23.429861 hw_loss 0.080811 lr 0.00087532 rank 1
2023-02-17 19:40:45,767 DEBUG TRAIN Batch 3/7600 loss 16.651924 loss_att 19.483322 loss_ctc 26.348871 loss_rnnt 14.780635 hw_loss 0.022653 lr 0.00087523 rank 0
2023-02-17 19:40:45,768 DEBUG TRAIN Batch 3/7600 loss 27.252762 loss_att 31.286659 loss_ctc 40.123352 loss_rnnt 24.722431 hw_loss 0.014010 lr 0.00087536 rank 5
2023-02-17 19:40:45,770 DEBUG TRAIN Batch 3/7600 loss 19.907581 loss_att 28.816324 loss_ctc 32.907200 loss_rnnt 16.373539 hw_loss 0.035650 lr 0.00087492 rank 2
2023-02-17 19:41:58,702 DEBUG TRAIN Batch 3/7700 loss 24.997458 loss_att 33.544689 loss_ctc 40.208237 loss_rnnt 21.237240 hw_loss 0.042500 lr 0.00087467 rank 4
2023-02-17 19:41:58,705 DEBUG TRAIN Batch 3/7700 loss 26.408854 loss_att 41.818596 loss_ctc 46.367279 loss_rnnt 20.615089 hw_loss 0.095049 lr 0.00087398 rank 1
2023-02-17 19:41:58,705 DEBUG TRAIN Batch 3/7700 loss 27.235662 loss_att 37.401810 loss_ctc 45.772942 loss_rnnt 22.663471 hw_loss 0.126235 lr 0.00087389 rank 0
2023-02-17 19:41:58,707 DEBUG TRAIN Batch 3/7700 loss 28.852016 loss_att 33.973877 loss_ctc 44.827389 loss_rnnt 25.684332 hw_loss 0.024872 lr 0.00087430 rank 6
2023-02-17 19:41:58,707 DEBUG TRAIN Batch 3/7700 loss 37.040565 loss_att 49.860863 loss_ctc 60.232506 loss_rnnt 31.371954 hw_loss 0.023052 lr 0.00087358 rank 2
2023-02-17 19:41:58,712 DEBUG TRAIN Batch 3/7700 loss 18.999567 loss_att 22.967525 loss_ctc 30.964935 loss_rnnt 16.610527 hw_loss 0.000123 lr 0.00087421 rank 3
2023-02-17 19:41:58,718 DEBUG TRAIN Batch 3/7700 loss 18.014584 loss_att 25.136688 loss_ctc 35.521255 loss_rnnt 14.227081 hw_loss 0.054108 lr 0.00087447 rank 7
2023-02-17 19:41:58,723 DEBUG TRAIN Batch 3/7700 loss 17.659151 loss_att 20.553061 loss_ctc 26.705595 loss_rnnt 15.837000 hw_loss 0.069702 lr 0.00087402 rank 5
2023-02-17 19:43:13,587 DEBUG TRAIN Batch 3/7800 loss 40.223682 loss_att 43.765171 loss_ctc 57.312080 loss_rnnt 37.213779 hw_loss 0.043406 lr 0.00087225 rank 2
2023-02-17 19:43:13,595 DEBUG TRAIN Batch 3/7800 loss 18.023537 loss_att 23.674725 loss_ctc 28.002508 loss_rnnt 15.552803 hw_loss 0.018687 lr 0.00087333 rank 4
2023-02-17 19:43:13,596 DEBUG TRAIN Batch 3/7800 loss 32.052841 loss_att 42.992863 loss_ctc 53.351669 loss_rnnt 27.024845 hw_loss 0.000279 lr 0.00087265 rank 1
2023-02-17 19:43:13,597 DEBUG TRAIN Batch 3/7800 loss 22.686617 loss_att 31.502575 loss_ctc 39.453636 loss_rnnt 18.687706 hw_loss 0.000216 lr 0.00087256 rank 0
2023-02-17 19:43:13,601 DEBUG TRAIN Batch 3/7800 loss 11.986190 loss_att 20.660694 loss_ctc 17.839397 loss_rnnt 9.470716 hw_loss 0.000271 lr 0.00087288 rank 3
2023-02-17 19:43:13,601 DEBUG TRAIN Batch 3/7800 loss 17.388912 loss_att 21.787865 loss_ctc 27.592159 loss_rnnt 15.128132 hw_loss 0.038543 lr 0.00087297 rank 6
2023-02-17 19:43:13,603 DEBUG TRAIN Batch 3/7800 loss 65.525200 loss_att 76.736923 loss_ctc 96.293335 loss_rnnt 59.139496 hw_loss 0.076771 lr 0.00087313 rank 7
2023-02-17 19:43:13,631 DEBUG TRAIN Batch 3/7800 loss 26.271902 loss_att 32.384411 loss_ctc 46.833019 loss_rnnt 22.242689 hw_loss 0.122302 lr 0.00087269 rank 5
2023-02-17 19:44:27,179 DEBUG TRAIN Batch 3/7900 loss 18.352474 loss_att 25.847500 loss_ctc 33.059166 loss_rnnt 14.867839 hw_loss 0.046381 lr 0.00087123 rank 0
2023-02-17 19:44:27,190 DEBUG TRAIN Batch 3/7900 loss 23.794302 loss_att 31.105530 loss_ctc 42.356964 loss_rnnt 19.825489 hw_loss 0.059149 lr 0.00087200 rank 4
2023-02-17 19:44:27,195 DEBUG TRAIN Batch 3/7900 loss 19.935333 loss_att 27.763794 loss_ctc 37.571518 loss_rnnt 16.000919 hw_loss 0.032307 lr 0.00087180 rank 7
2023-02-17 19:44:27,195 DEBUG TRAIN Batch 3/7900 loss 28.252504 loss_att 37.879288 loss_ctc 42.344101 loss_rnnt 24.360945 hw_loss 0.163731 lr 0.00087133 rank 1
2023-02-17 19:44:27,196 DEBUG TRAIN Batch 3/7900 loss 22.338104 loss_att 25.599979 loss_ctc 35.753395 loss_rnnt 19.812584 hw_loss 0.158325 lr 0.00087093 rank 2
2023-02-17 19:44:27,197 DEBUG TRAIN Batch 3/7900 loss 28.972046 loss_att 37.512947 loss_ctc 52.784286 loss_rnnt 24.059658 hw_loss 0.054829 lr 0.00087137 rank 5
2023-02-17 19:44:27,198 DEBUG TRAIN Batch 3/7900 loss 41.591221 loss_att 49.654343 loss_ctc 71.899582 loss_rnnt 35.904778 hw_loss 0.061319 lr 0.00087164 rank 6
2023-02-17 19:44:27,198 DEBUG TRAIN Batch 3/7900 loss 31.609558 loss_att 35.266560 loss_ctc 45.140747 loss_rnnt 29.045071 hw_loss 0.054246 lr 0.00087155 rank 3
2023-02-17 19:45:39,664 DEBUG TRAIN Batch 3/8000 loss 15.748388 loss_att 20.960255 loss_ctc 26.076309 loss_rnnt 13.328917 hw_loss 0.000080 lr 0.00086961 rank 2
2023-02-17 19:45:39,668 DEBUG TRAIN Batch 3/8000 loss 23.965559 loss_att 30.008080 loss_ctc 31.313850 loss_rnnt 21.764774 hw_loss 0.023447 lr 0.00087001 rank 1
2023-02-17 19:45:39,669 DEBUG TRAIN Batch 3/8000 loss 34.831417 loss_att 32.852173 loss_ctc 40.627098 loss_rnnt 34.424877 hw_loss 0.055557 lr 0.00087068 rank 4
2023-02-17 19:45:39,670 DEBUG TRAIN Batch 3/8000 loss 15.030280 loss_att 20.897221 loss_ctc 26.402821 loss_rnnt 12.322035 hw_loss 0.034723 lr 0.00087032 rank 6
2023-02-17 19:45:39,670 DEBUG TRAIN Batch 3/8000 loss 39.691483 loss_att 43.544334 loss_ctc 57.499802 loss_rnnt 36.514587 hw_loss 0.059774 lr 0.00087023 rank 3
2023-02-17 19:45:39,670 DEBUG TRAIN Batch 3/8000 loss 27.430693 loss_att 38.524902 loss_ctc 45.779537 loss_rnnt 22.713993 hw_loss 0.096269 lr 0.00087048 rank 7
2023-02-17 19:45:39,673 DEBUG TRAIN Batch 3/8000 loss 23.180109 loss_att 30.232141 loss_ctc 40.291256 loss_rnnt 19.456253 hw_loss 0.059933 lr 0.00086991 rank 0
2023-02-17 19:45:39,718 DEBUG TRAIN Batch 3/8000 loss 39.156815 loss_att 47.197762 loss_ctc 61.338375 loss_rnnt 34.527260 hw_loss 0.119664 lr 0.00087005 rank 5
2023-02-17 19:46:52,489 DEBUG TRAIN Batch 3/8100 loss 24.431883 loss_att 27.003910 loss_ctc 36.169765 loss_rnnt 22.298124 hw_loss 0.101817 lr 0.00086830 rank 2
2023-02-17 19:46:52,499 DEBUG TRAIN Batch 3/8100 loss 23.531380 loss_att 25.431892 loss_ctc 35.251846 loss_rnnt 21.573215 hw_loss 0.028745 lr 0.00086901 rank 6
2023-02-17 19:46:52,499 DEBUG TRAIN Batch 3/8100 loss 30.977346 loss_att 34.962948 loss_ctc 52.349464 loss_rnnt 27.310425 hw_loss 0.037846 lr 0.00086869 rank 1
2023-02-17 19:46:52,503 DEBUG TRAIN Batch 3/8100 loss 28.117096 loss_att 33.147129 loss_ctc 39.277939 loss_rnnt 25.619431 hw_loss 0.006648 lr 0.00086936 rank 4
2023-02-17 19:46:52,504 DEBUG TRAIN Batch 3/8100 loss 37.276482 loss_att 45.523674 loss_ctc 60.801704 loss_rnnt 32.438190 hw_loss 0.097790 lr 0.00086892 rank 3
2023-02-17 19:46:52,505 DEBUG TRAIN Batch 3/8100 loss 21.903957 loss_att 25.589348 loss_ctc 37.656715 loss_rnnt 19.018391 hw_loss 0.090226 lr 0.00086860 rank 0
2023-02-17 19:46:52,506 DEBUG TRAIN Batch 3/8100 loss 15.266116 loss_att 19.150627 loss_ctc 27.178883 loss_rnnt 12.870567 hw_loss 0.056773 lr 0.00086916 rank 7
2023-02-17 19:46:52,512 DEBUG TRAIN Batch 3/8100 loss 24.716791 loss_att 33.318981 loss_ctc 39.447773 loss_rnnt 21.013588 hw_loss 0.034940 lr 0.00086873 rank 5
2023-02-17 19:48:05,695 DEBUG TRAIN Batch 3/8200 loss 20.620123 loss_att 22.360798 loss_ctc 31.029461 loss_rnnt 18.868179 hw_loss 0.029806 lr 0.00086805 rank 4
2023-02-17 19:48:05,695 DEBUG TRAIN Batch 3/8200 loss 29.089922 loss_att 37.089485 loss_ctc 48.710964 loss_rnnt 24.853849 hw_loss 0.037540 lr 0.00086738 rank 1
2023-02-17 19:48:05,699 DEBUG TRAIN Batch 3/8200 loss 39.235264 loss_att 43.158302 loss_ctc 52.286297 loss_rnnt 36.704338 hw_loss 0.011590 lr 0.00086729 rank 0
2023-02-17 19:48:05,700 DEBUG TRAIN Batch 3/8200 loss 20.209974 loss_att 23.872158 loss_ctc 34.301121 loss_rnnt 17.533009 hw_loss 0.123204 lr 0.00086785 rank 7
2023-02-17 19:48:05,700 DEBUG TRAIN Batch 3/8200 loss 26.650423 loss_att 36.075325 loss_ctc 44.314621 loss_rnnt 22.409672 hw_loss 0.001022 lr 0.00086770 rank 6
2023-02-17 19:48:05,701 DEBUG TRAIN Batch 3/8200 loss 14.923765 loss_att 21.345551 loss_ctc 27.707829 loss_rnnt 11.934790 hw_loss 0.000146 lr 0.00086699 rank 2
2023-02-17 19:48:05,702 DEBUG TRAIN Batch 3/8200 loss 22.690020 loss_att 29.675045 loss_ctc 34.528481 loss_rnnt 19.697004 hw_loss 0.032908 lr 0.00086742 rank 5
2023-02-17 19:48:05,702 DEBUG TRAIN Batch 3/8200 loss 19.235189 loss_att 23.452797 loss_ctc 28.702652 loss_rnnt 17.055141 hw_loss 0.139122 lr 0.00086761 rank 3
2023-02-17 19:49:18,167 DEBUG TRAIN Batch 3/8300 loss 32.074646 loss_att 38.497223 loss_ctc 47.521309 loss_rnnt 28.717436 hw_loss 0.024640 lr 0.00086655 rank 7
2023-02-17 19:49:18,171 DEBUG TRAIN Batch 3/8300 loss 29.020567 loss_att 38.702892 loss_ctc 46.191784 loss_rnnt 24.776955 hw_loss 0.033098 lr 0.00086639 rank 6
2023-02-17 19:49:18,173 DEBUG TRAIN Batch 3/8300 loss 18.740963 loss_att 30.917347 loss_ctc 30.601601 loss_rnnt 14.724203 hw_loss 0.000125 lr 0.00086675 rank 4
2023-02-17 19:49:18,175 DEBUG TRAIN Batch 3/8300 loss 23.822659 loss_att 32.977589 loss_ctc 35.961880 loss_rnnt 20.339321 hw_loss 0.063353 lr 0.00086612 rank 5
2023-02-17 19:49:18,179 DEBUG TRAIN Batch 3/8300 loss 34.372299 loss_att 43.566925 loss_ctc 55.882927 loss_rnnt 29.665226 hw_loss 0.000117 lr 0.00086599 rank 0
2023-02-17 19:49:18,183 DEBUG TRAIN Batch 3/8300 loss 17.312634 loss_att 23.659458 loss_ctc 33.322777 loss_rnnt 13.907997 hw_loss 0.001096 lr 0.00086630 rank 3
2023-02-17 19:49:18,185 DEBUG TRAIN Batch 3/8300 loss 31.958975 loss_att 38.868103 loss_ctc 56.346527 loss_rnnt 27.282927 hw_loss 0.079780 lr 0.00086569 rank 2
2023-02-17 19:49:18,224 DEBUG TRAIN Batch 3/8300 loss 17.821814 loss_att 19.307678 loss_ctc 24.866894 loss_rnnt 16.549429 hw_loss 0.067254 lr 0.00086608 rank 1
2023-02-17 19:50:11,830 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 5
2023-02-17 19:50:11,849 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 7
2023-02-17 19:50:11,853 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 6
2023-02-17 19:50:11,855 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 4
2023-02-17 19:50:11,856 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 3
2023-02-17 19:50:11,858 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 0
2023-02-17 19:50:11,861 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 1
2023-02-17 19:50:11,868 DEBUG CV Batch 3/0 loss 3.905271 loss_att 4.370811 loss_ctc 6.712632 loss_rnnt 3.314250 hw_loss 0.231745 history loss 3.760631 rank 2
2023-02-17 19:50:23,053 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 1
2023-02-17 19:50:23,078 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 4
2023-02-17 19:50:23,103 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 2
2023-02-17 19:50:23,124 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 3
2023-02-17 19:50:23,176 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 5
2023-02-17 19:50:23,251 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 6
2023-02-17 19:50:23,321 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 7
2023-02-17 19:50:23,410 DEBUG CV Batch 3/100 loss 18.838638 loss_att 22.165028 loss_ctc 35.230736 loss_rnnt 15.952051 hw_loss 0.066930 history loss 8.189706 rank 0
2023-02-17 19:50:36,445 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 1
2023-02-17 19:50:36,587 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 5
2023-02-17 19:50:36,598 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 2
2023-02-17 19:50:36,599 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 4
2023-02-17 19:50:36,631 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 7
2023-02-17 19:50:36,904 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 6
2023-02-17 19:50:37,006 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 3
2023-02-17 19:50:37,099 DEBUG CV Batch 3/200 loss 25.182528 loss_att 33.240055 loss_ctc 30.927101 loss_rnnt 22.796318 hw_loss 0.016425 history loss 9.259849 rank 0
2023-02-17 19:50:48,582 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 4
2023-02-17 19:50:48,728 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 1
2023-02-17 19:50:48,854 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 7
2023-02-17 19:50:48,893 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 5
2023-02-17 19:50:49,056 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 2
2023-02-17 19:50:49,100 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 6
2023-02-17 19:50:49,275 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 3
2023-02-17 19:50:49,578 DEBUG CV Batch 3/300 loss 10.556505 loss_att 12.408717 loss_ctc 19.334717 loss_rnnt 8.942754 hw_loss 0.136652 history loss 9.466316 rank 0
2023-02-17 19:51:00,672 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 1
2023-02-17 19:51:00,795 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 4
2023-02-17 19:51:00,808 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 7
2023-02-17 19:51:00,988 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 5
2023-02-17 19:51:01,287 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 6
2023-02-17 19:51:01,331 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 2
2023-02-17 19:51:01,574 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 3
2023-02-17 19:51:02,041 DEBUG CV Batch 3/400 loss 53.894005 loss_att 147.860016 loss_ctc 50.697006 loss_rnnt 35.525585 hw_loss 0.002784 history loss 10.928431 rank 0
2023-02-17 19:51:11,461 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 1
2023-02-17 19:51:11,507 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 7
2023-02-17 19:51:11,604 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 4
2023-02-17 19:51:12,083 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 5
2023-02-17 19:51:12,107 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 6
2023-02-17 19:51:12,187 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 2
2023-02-17 19:51:12,395 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 3
2023-02-17 19:51:13,217 DEBUG CV Batch 3/500 loss 14.911257 loss_att 15.459521 loss_ctc 22.481901 loss_rnnt 13.769135 hw_loss 0.043221 history loss 11.963079 rank 0
2023-02-17 19:51:23,403 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 1
2023-02-17 19:51:23,610 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 7
2023-02-17 19:51:23,731 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 4
2023-02-17 19:51:24,163 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 5
2023-02-17 19:51:24,353 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 6
2023-02-17 19:51:24,384 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 2
2023-02-17 19:51:24,873 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 3
2023-02-17 19:51:25,914 DEBUG CV Batch 3/600 loss 11.464315 loss_att 12.185660 loss_ctc 18.224653 loss_rnnt 10.338415 hw_loss 0.150474 history loss 13.233410 rank 0
2023-02-17 19:51:34,619 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 1
2023-02-17 19:51:35,019 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 7
2023-02-17 19:51:35,166 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 4
2023-02-17 19:51:35,471 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 5
2023-02-17 19:51:35,848 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 2
2023-02-17 19:51:35,934 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 6
2023-02-17 19:51:36,387 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 3
2023-02-17 19:51:37,777 DEBUG CV Batch 3/700 loss 48.857613 loss_att 105.025421 loss_ctc 70.627647 loss_rnnt 34.687195 hw_loss 0.064098 history loss 14.232715 rank 0
2023-02-17 19:51:46,050 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 1
2023-02-17 19:51:46,432 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 7
2023-02-17 19:51:46,555 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 4
2023-02-17 19:51:46,891 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 5
2023-02-17 19:51:47,355 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 6
2023-02-17 19:51:47,461 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 2
2023-02-17 19:51:48,015 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 3
2023-02-17 19:51:49,587 DEBUG CV Batch 3/800 loss 18.937494 loss_att 21.752499 loss_ctc 36.135651 loss_rnnt 16.051794 hw_loss 0.055522 history loss 13.418486 rank 0
2023-02-17 19:51:59,310 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 1
2023-02-17 19:51:59,754 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 7
2023-02-17 19:51:59,877 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 4
2023-02-17 19:52:00,152 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 5
2023-02-17 19:52:00,910 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 6
2023-02-17 19:52:00,933 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 2
2023-02-17 19:52:01,608 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 3
2023-02-17 19:52:03,241 DEBUG CV Batch 3/900 loss 31.047598 loss_att 43.050503 loss_ctc 56.022728 loss_rnnt 25.267319 hw_loss 0.093151 history loss 13.160458 rank 0
2023-02-17 19:52:11,522 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 1
2023-02-17 19:52:12,021 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 4
2023-02-17 19:52:12,095 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 7
2023-02-17 19:52:12,461 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 5
2023-02-17 19:52:13,210 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 6
2023-02-17 19:52:13,459 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 2
2023-02-17 19:52:14,096 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 3
2023-02-17 19:52:16,060 DEBUG CV Batch 3/1000 loss 12.508991 loss_att 12.237539 loss_ctc 16.050686 loss_rnnt 12.023501 hw_loss 0.126664 history loss 12.837219 rank 0
2023-02-17 19:52:23,384 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 1
2023-02-17 19:52:23,977 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 4
2023-02-17 19:52:24,038 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 7
2023-02-17 19:52:24,824 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 5
2023-02-17 19:52:25,352 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 6
2023-02-17 19:52:25,537 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 2
2023-02-17 19:52:26,263 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 3
2023-02-17 19:52:28,419 DEBUG CV Batch 3/1100 loss 8.643384 loss_att 8.467028 loss_ctc 14.307749 loss_rnnt 7.853933 hw_loss 0.130265 history loss 12.842996 rank 0
2023-02-17 19:52:34,099 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 1
2023-02-17 19:52:34,737 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 4
2023-02-17 19:52:34,986 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 7
2023-02-17 19:52:35,711 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 5
2023-02-17 19:52:36,286 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 6
2023-02-17 19:52:36,374 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 2
2023-02-17 19:52:37,532 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 3
2023-02-17 19:52:39,683 DEBUG CV Batch 3/1200 loss 17.842119 loss_att 20.044170 loss_ctc 27.329275 loss_rnnt 16.121443 hw_loss 0.028710 history loss 13.285008 rank 0
2023-02-17 19:52:46,121 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 1
2023-02-17 19:52:46,799 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 4
2023-02-17 19:52:47,357 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 7
2023-02-17 19:52:47,811 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 5
2023-02-17 19:52:48,447 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 6
2023-02-17 19:52:48,494 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 2
2023-02-17 19:52:49,861 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 3
2023-02-17 19:52:52,281 DEBUG CV Batch 3/1300 loss 12.045195 loss_att 11.991860 loss_ctc 17.112236 loss_rnnt 11.340137 hw_loss 0.075221 history loss 13.684807 rank 0
2023-02-17 19:52:57,456 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 1
2023-02-17 19:52:57,994 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 4
2023-02-17 19:52:58,787 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 7
2023-02-17 19:52:59,088 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 5
2023-02-17 19:52:59,908 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 6
2023-02-17 19:52:59,969 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 2
2023-02-17 19:53:01,437 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 3
2023-02-17 19:53:03,933 DEBUG CV Batch 3/1400 loss 18.194681 loss_att 45.045242 loss_ctc 32.603054 loss_rnnt 10.876358 hw_loss 0.050800 history loss 14.169806 rank 0
2023-02-17 19:53:08,991 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 1
2023-02-17 19:53:09,773 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 4
2023-02-17 19:53:10,560 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 7
2023-02-17 19:53:10,679 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 5
2023-02-17 19:53:11,605 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 6
2023-02-17 19:53:12,444 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 2
2023-02-17 19:53:13,511 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 3
2023-02-17 19:53:15,945 DEBUG CV Batch 3/1500 loss 14.389389 loss_att 16.617136 loss_ctc 21.101479 loss_rnnt 13.042872 hw_loss 0.011291 history loss 13.863098 rank 0
2023-02-17 19:53:22,088 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 1
2023-02-17 19:53:22,852 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 4
2023-02-17 19:53:23,782 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 5
2023-02-17 19:53:23,900 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 7
2023-02-17 19:53:24,848 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 6
2023-02-17 19:53:25,791 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 2
2023-02-17 19:53:26,783 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 3
2023-02-17 19:53:29,360 DEBUG CV Batch 3/1600 loss 13.283800 loss_att 26.466057 loss_ctc 26.278791 loss_rnnt 8.881246 hw_loss 0.062694 history loss 13.739285 rank 0
2023-02-17 19:53:34,493 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 1
2023-02-17 19:53:35,328 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 4
2023-02-17 19:53:36,289 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 5
2023-02-17 19:53:36,477 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 7
2023-02-17 19:53:37,337 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 6
2023-02-17 19:53:38,894 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 2
2023-02-17 19:53:39,397 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 3
2023-02-17 19:53:41,875 DEBUG CV Batch 3/1700 loss 19.676538 loss_att 18.843632 loss_ctc 29.653324 loss_rnnt 18.437855 hw_loss 0.140671 history loss 13.566795 rank 0
2023-02-17 19:53:43,538 INFO Epoch 3 CV info cv_loss 13.512944953550853
2023-02-17 19:53:43,539 INFO Epoch 4 TRAIN info lr 0.0008656273067645919
2023-02-17 19:53:43,545 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:44,193 INFO Epoch 3 CV info cv_loss 13.512944953826521
2023-02-17 19:53:44,193 INFO Epoch 4 TRAIN info lr 0.000866211659315814
2023-02-17 19:53:44,195 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:45,540 INFO Epoch 3 CV info cv_loss 13.512944952792767
2023-02-17 19:53:45,540 INFO Epoch 4 TRAIN info lr 0.0008657440826648283
2023-02-17 19:53:45,545 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:45,731 INFO Epoch 3 CV info cv_loss 13.512944953826521
2023-02-17 19:53:45,731 INFO Epoch 4 TRAIN info lr 0.00086586090583815
2023-02-17 19:53:45,733 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:46,422 INFO Epoch 3 CV info cv_loss 13.512944951069839
2023-02-17 19:53:46,423 INFO Epoch 4 TRAIN info lr 0.0008659907648466097
2023-02-17 19:53:46,427 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:48,175 INFO Epoch 3 CV info cv_loss 13.512944952999517
2023-02-17 19:53:48,176 INFO Epoch 4 TRAIN info lr 0.0008648500065029802
2023-02-17 19:53:48,181 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:48,753 INFO Epoch 3 CV info cv_loss 13.512944951621176
2023-02-17 19:53:48,754 INFO Epoch 4 TRAIN info lr 0.0008657181283805701
2023-02-17 19:53:48,756 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:53:51,253 INFO Epoch 3 CV info cv_loss 13.512944956100785
2023-02-17 19:53:51,253 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word/3.pt
2023-02-17 19:53:51,863 INFO Epoch 4 TRAIN info lr 0.000865419821788753
2023-02-17 19:53:51,867 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:54:52,859 DEBUG TRAIN Batch 4/0 loss 18.684156 loss_att 17.894957 loss_ctc 24.853640 loss_rnnt 17.879442 hw_loss 0.262416 lr 0.00086620 rank 4
2023-02-17 19:54:52,861 DEBUG TRAIN Batch 4/0 loss 12.368359 loss_att 12.535735 loss_ctc 16.905573 loss_rnnt 11.683173 hw_loss 0.087653 lr 0.00086561 rank 1
2023-02-17 19:54:52,862 DEBUG TRAIN Batch 4/0 loss 11.741044 loss_att 12.100967 loss_ctc 17.319096 loss_rnnt 10.812646 hw_loss 0.211261 lr 0.00086573 rank 5
2023-02-17 19:54:52,867 DEBUG TRAIN Batch 4/0 loss 9.728012 loss_att 10.758196 loss_ctc 13.376878 loss_rnnt 8.981091 hw_loss 0.101941 lr 0.00086484 rank 2
2023-02-17 19:54:52,868 DEBUG TRAIN Batch 4/0 loss 14.866218 loss_att 15.156864 loss_ctc 21.017685 loss_rnnt 13.943666 hw_loss 0.082921 lr 0.00086598 rank 6
2023-02-17 19:54:52,876 DEBUG TRAIN Batch 4/0 loss 17.644707 loss_att 16.283133 loss_ctc 22.105848 loss_rnnt 17.251511 hw_loss 0.132547 lr 0.00086585 rank 7
2023-02-17 19:54:52,908 DEBUG TRAIN Batch 4/0 loss 12.679316 loss_att 12.420414 loss_ctc 16.290209 loss_rnnt 12.144612 hw_loss 0.196934 lr 0.00086571 rank 3
2023-02-17 19:54:52,914 DEBUG TRAIN Batch 4/0 loss 16.237772 loss_att 17.879847 loss_ctc 22.414074 loss_rnnt 14.990055 hw_loss 0.179617 lr 0.00086541 rank 0
2023-02-17 19:56:05,566 DEBUG TRAIN Batch 4/100 loss 26.817991 loss_att 34.995773 loss_ctc 42.045330 loss_rnnt 23.116138 hw_loss 0.067469 lr 0.00086490 rank 4
2023-02-17 19:56:05,570 DEBUG TRAIN Batch 4/100 loss 9.597086 loss_att 14.615333 loss_ctc 14.224655 loss_rnnt 7.935897 hw_loss 0.075997 lr 0.00086432 rank 1
2023-02-17 19:56:05,572 DEBUG TRAIN Batch 4/100 loss 29.163731 loss_att 37.032753 loss_ctc 52.904854 loss_rnnt 24.337851 hw_loss 0.162364 lr 0.00086441 rank 3
2023-02-17 19:56:05,574 DEBUG TRAIN Batch 4/100 loss 34.266586 loss_att 37.641769 loss_ctc 55.760422 loss_rnnt 30.645103 hw_loss 0.151127 lr 0.00086444 rank 5
2023-02-17 19:56:05,575 DEBUG TRAIN Batch 4/100 loss 25.892834 loss_att 34.445183 loss_ctc 37.843132 loss_rnnt 22.548092 hw_loss 0.076684 lr 0.00086468 rank 6
2023-02-17 19:56:05,575 DEBUG TRAIN Batch 4/100 loss 25.369892 loss_att 35.287910 loss_ctc 42.862793 loss_rnnt 21.044687 hw_loss 0.017278 lr 0.00086411 rank 0
2023-02-17 19:56:05,579 DEBUG TRAIN Batch 4/100 loss 19.454535 loss_att 29.345900 loss_ctc 33.424492 loss_rnnt 15.566635 hw_loss 0.088057 lr 0.00086455 rank 7
2023-02-17 19:56:05,625 DEBUG TRAIN Batch 4/100 loss 17.206333 loss_att 25.828510 loss_ctc 24.926901 loss_rnnt 14.425449 hw_loss 0.050698 lr 0.00086355 rank 2
2023-02-17 19:57:17,738 DEBUG TRAIN Batch 4/200 loss 18.674080 loss_att 26.439856 loss_ctc 34.142303 loss_rnnt 15.058397 hw_loss 0.000181 lr 0.00086283 rank 0
2023-02-17 19:57:17,741 DEBUG TRAIN Batch 4/200 loss 9.055357 loss_att 17.883162 loss_ctc 14.036922 loss_rnnt 6.619983 hw_loss 0.010510 lr 0.00086339 rank 6
2023-02-17 19:57:17,742 DEBUG TRAIN Batch 4/200 loss 34.304245 loss_att 41.476696 loss_ctc 48.228249 loss_rnnt 31.005020 hw_loss 0.015377 lr 0.00086303 rank 1
2023-02-17 19:57:17,742 DEBUG TRAIN Batch 4/200 loss 12.918911 loss_att 20.277357 loss_ctc 25.279989 loss_rnnt 9.798887 hw_loss 0.000356 lr 0.00086361 rank 4
2023-02-17 19:57:17,744 DEBUG TRAIN Batch 4/200 loss 25.960918 loss_att 33.687828 loss_ctc 41.084362 loss_rnnt 22.397507 hw_loss 0.002944 lr 0.00086326 rank 7
2023-02-17 19:57:17,747 DEBUG TRAIN Batch 4/200 loss 32.744026 loss_att 43.004189 loss_ctc 55.330944 loss_rnnt 27.664587 hw_loss 0.029664 lr 0.00086226 rank 2
2023-02-17 19:57:17,749 DEBUG TRAIN Batch 4/200 loss 32.067257 loss_att 35.527184 loss_ctc 45.593304 loss_rnnt 29.555630 hw_loss 0.030313 lr 0.00086312 rank 3
2023-02-17 19:57:17,759 DEBUG TRAIN Batch 4/200 loss 45.440403 loss_att 57.857605 loss_ctc 78.399811 loss_rnnt 38.509033 hw_loss 0.100014 lr 0.00086315 rank 5
2023-02-17 19:58:31,410 DEBUG TRAIN Batch 4/300 loss 31.651255 loss_att 41.233784 loss_ctc 52.051159 loss_rnnt 26.973782 hw_loss 0.076836 lr 0.00086198 rank 7
2023-02-17 19:58:31,411 DEBUG TRAIN Batch 4/300 loss 39.840061 loss_att 51.837601 loss_ctc 65.482437 loss_rnnt 34.014297 hw_loss 0.013627 lr 0.00086154 rank 0
2023-02-17 19:58:31,412 DEBUG TRAIN Batch 4/300 loss 24.202801 loss_att 30.387255 loss_ctc 36.367977 loss_rnnt 21.339130 hw_loss 0.008915 lr 0.00086233 rank 4
2023-02-17 19:58:31,415 DEBUG TRAIN Batch 4/300 loss 25.957258 loss_att 35.826096 loss_ctc 39.837326 loss_rnnt 22.117592 hw_loss 0.028542 lr 0.00086211 rank 6
2023-02-17 19:58:31,416 DEBUG TRAIN Batch 4/300 loss 28.033081 loss_att 35.539474 loss_ctc 44.789627 loss_rnnt 24.242081 hw_loss 0.104094 lr 0.00086175 rank 1
2023-02-17 19:58:31,417 DEBUG TRAIN Batch 4/300 loss 24.430759 loss_att 28.216803 loss_ctc 41.310776 loss_rnnt 21.389008 hw_loss 0.063515 lr 0.00086184 rank 3
2023-02-17 19:58:31,432 DEBUG TRAIN Batch 4/300 loss 22.175341 loss_att 30.188450 loss_ctc 35.168251 loss_rnnt 18.791729 hw_loss 0.091128 lr 0.00086098 rank 2
2023-02-17 19:58:31,439 DEBUG TRAIN Batch 4/300 loss 20.174311 loss_att 27.006084 loss_ctc 39.247913 loss_rnnt 16.256327 hw_loss 0.015903 lr 0.00086186 rank 5
2023-02-17 19:59:45,342 DEBUG TRAIN Batch 4/400 loss 33.270805 loss_att 41.952789 loss_ctc 57.653023 loss_rnnt 28.273521 hw_loss 0.018604 lr 0.00086083 rank 6
2023-02-17 19:59:45,343 DEBUG TRAIN Batch 4/400 loss 30.066219 loss_att 36.255219 loss_ctc 40.466709 loss_rnnt 27.404615 hw_loss 0.069511 lr 0.00086105 rank 4
2023-02-17 19:59:45,344 DEBUG TRAIN Batch 4/400 loss 14.917397 loss_att 20.864084 loss_ctc 24.741631 loss_rnnt 12.350193 hw_loss 0.127440 lr 0.00086047 rank 1
2023-02-17 19:59:45,345 DEBUG TRAIN Batch 4/400 loss 28.249933 loss_att 35.631107 loss_ctc 46.302891 loss_rnnt 24.326813 hw_loss 0.074669 lr 0.00086027 rank 0
2023-02-17 19:59:45,345 DEBUG TRAIN Batch 4/400 loss 24.086321 loss_att 31.215248 loss_ctc 32.608566 loss_rnnt 21.499537 hw_loss 0.046312 lr 0.00086059 rank 5
2023-02-17 19:59:45,346 DEBUG TRAIN Batch 4/400 loss 20.264376 loss_att 27.268757 loss_ctc 37.662842 loss_rnnt 16.489098 hw_loss 0.102383 lr 0.00086056 rank 3
2023-02-17 19:59:45,355 DEBUG TRAIN Batch 4/400 loss 25.809616 loss_att 36.798374 loss_ctc 40.493034 loss_rnnt 21.634895 hw_loss 0.035964 lr 0.00085971 rank 2
2023-02-17 19:59:45,358 DEBUG TRAIN Batch 4/400 loss 17.292192 loss_att 22.669968 loss_ctc 26.746778 loss_rnnt 14.933287 hw_loss 0.042641 lr 0.00086070 rank 7
2023-02-17 20:00:57,948 DEBUG TRAIN Batch 4/500 loss 43.104198 loss_att 44.740150 loss_ctc 67.173065 loss_rnnt 39.541664 hw_loss 0.049060 lr 0.00085929 rank 3
2023-02-17 20:00:57,950 DEBUG TRAIN Batch 4/500 loss 17.946190 loss_att 21.504852 loss_ctc 27.272791 loss_rnnt 15.902921 hw_loss 0.164975 lr 0.00085900 rank 0
2023-02-17 20:00:57,963 DEBUG TRAIN Batch 4/500 loss 30.465908 loss_att 35.205025 loss_ctc 40.686802 loss_rnnt 28.111410 hw_loss 0.082292 lr 0.00085977 rank 4
2023-02-17 20:00:57,965 DEBUG TRAIN Batch 4/500 loss 20.383278 loss_att 24.956097 loss_ctc 37.034260 loss_rnnt 17.201824 hw_loss 0.087674 lr 0.00085920 rank 1
2023-02-17 20:00:57,965 DEBUG TRAIN Batch 4/500 loss 19.569199 loss_att 30.374287 loss_ctc 36.084625 loss_rnnt 15.172316 hw_loss 0.063392 lr 0.00085844 rank 2
2023-02-17 20:00:57,968 DEBUG TRAIN Batch 4/500 loss 14.058382 loss_att 17.265923 loss_ctc 23.482662 loss_rnnt 12.152815 hw_loss 0.014042 lr 0.00085943 rank 7
2023-02-17 20:00:57,970 DEBUG TRAIN Batch 4/500 loss 16.084946 loss_att 20.576469 loss_ctc 27.889664 loss_rnnt 13.573454 hw_loss 0.073548 lr 0.00085931 rank 5
2023-02-17 20:00:57,973 DEBUG TRAIN Batch 4/500 loss 23.080893 loss_att 25.377937 loss_ctc 39.070515 loss_rnnt 20.484119 hw_loss 0.010150 lr 0.00085956 rank 6
2023-02-17 20:02:12,700 DEBUG TRAIN Batch 4/600 loss 17.050230 loss_att 20.705450 loss_ctc 25.722095 loss_rnnt 15.092203 hw_loss 0.132629 lr 0.00085829 rank 6
2023-02-17 20:02:12,700 DEBUG TRAIN Batch 4/600 loss 15.204755 loss_att 16.701927 loss_ctc 23.016584 loss_rnnt 13.845057 hw_loss 0.035035 lr 0.00085850 rank 4
2023-02-17 20:02:12,707 DEBUG TRAIN Batch 4/600 loss 21.379751 loss_att 22.805305 loss_ctc 29.163298 loss_rnnt 20.048409 hw_loss 0.015800 lr 0.00085802 rank 3
2023-02-17 20:02:12,708 DEBUG TRAIN Batch 4/600 loss 15.027978 loss_att 14.341372 loss_ctc 18.761570 loss_rnnt 14.566923 hw_loss 0.188557 lr 0.00085805 rank 5
2023-02-17 20:02:12,711 DEBUG TRAIN Batch 4/600 loss 41.480305 loss_att 45.015984 loss_ctc 58.654186 loss_rnnt 38.455650 hw_loss 0.051872 lr 0.00085816 rank 7
2023-02-17 20:02:12,711 DEBUG TRAIN Batch 4/600 loss 23.307339 loss_att 25.518192 loss_ctc 34.098705 loss_rnnt 21.415091 hw_loss 0.021057 lr 0.00085773 rank 0
2023-02-17 20:02:12,713 DEBUG TRAIN Batch 4/600 loss 18.342941 loss_att 23.601482 loss_ctc 28.738525 loss_rnnt 15.884796 hw_loss 0.038173 lr 0.00085793 rank 1
2023-02-17 20:02:12,721 DEBUG TRAIN Batch 4/600 loss 23.962015 loss_att 24.027670 loss_ctc 30.778835 loss_rnnt 22.974199 hw_loss 0.123329 lr 0.00085718 rank 2
2023-02-17 20:03:27,331 DEBUG TRAIN Batch 4/700 loss 26.650631 loss_att 37.646534 loss_ctc 45.219627 loss_rnnt 21.962399 hw_loss 0.024725 lr 0.00085724 rank 4
2023-02-17 20:03:27,335 DEBUG TRAIN Batch 4/700 loss 39.298649 loss_att 48.293602 loss_ctc 60.247723 loss_rnnt 34.662884 hw_loss 0.081686 lr 0.00085703 rank 6
2023-02-17 20:03:27,337 DEBUG TRAIN Batch 4/700 loss 19.654758 loss_att 26.195210 loss_ctc 34.778477 loss_rnnt 16.330070 hw_loss 0.000191 lr 0.00085690 rank 7
2023-02-17 20:03:27,339 DEBUG TRAIN Batch 4/700 loss 26.394135 loss_att 30.888973 loss_ctc 38.073521 loss_rnnt 23.856159 hw_loss 0.153288 lr 0.00085667 rank 1
2023-02-17 20:03:27,340 DEBUG TRAIN Batch 4/700 loss 9.979431 loss_att 17.516163 loss_ctc 18.196064 loss_rnnt 7.357593 hw_loss 0.035515 lr 0.00085676 rank 3
2023-02-17 20:03:27,341 DEBUG TRAIN Batch 4/700 loss 27.404924 loss_att 39.099682 loss_ctc 47.657158 loss_rnnt 22.347137 hw_loss 0.034756 lr 0.00085647 rank 0
2023-02-17 20:03:27,343 DEBUG TRAIN Batch 4/700 loss 23.084412 loss_att 29.295078 loss_ctc 47.454201 loss_rnnt 18.592888 hw_loss 0.000159 lr 0.00085679 rank 5
2023-02-17 20:03:27,370 DEBUG TRAIN Batch 4/700 loss 27.634640 loss_att 36.166904 loss_ctc 46.465450 loss_rnnt 23.416718 hw_loss 0.001303 lr 0.00085592 rank 2
2023-02-17 20:04:41,204 DEBUG TRAIN Batch 4/800 loss 30.837891 loss_att 34.626343 loss_ctc 46.966988 loss_rnnt 27.921501 hw_loss 0.015287 lr 0.00085598 rank 4
2023-02-17 20:04:41,212 DEBUG TRAIN Batch 4/800 loss 19.661398 loss_att 27.856659 loss_ctc 28.678925 loss_rnnt 16.819172 hw_loss 0.001570 lr 0.00085542 rank 1
2023-02-17 20:04:41,215 DEBUG TRAIN Batch 4/800 loss 30.193060 loss_att 35.964088 loss_ctc 45.317451 loss_rnnt 26.989361 hw_loss 0.061701 lr 0.00085467 rank 2
2023-02-17 20:04:41,217 DEBUG TRAIN Batch 4/800 loss 18.272501 loss_att 22.119534 loss_ctc 27.102306 loss_rnnt 16.267521 hw_loss 0.109246 lr 0.00085551 rank 3
2023-02-17 20:04:41,218 DEBUG TRAIN Batch 4/800 loss 18.462645 loss_att 22.328716 loss_ctc 27.211731 loss_rnnt 16.522802 hw_loss 0.000156 lr 0.00085522 rank 0
2023-02-17 20:04:41,219 DEBUG TRAIN Batch 4/800 loss 21.342411 loss_att 31.632643 loss_ctc 42.061878 loss_rnnt 16.499823 hw_loss 0.041152 lr 0.00085577 rank 6
2023-02-17 20:04:41,221 DEBUG TRAIN Batch 4/800 loss 24.614981 loss_att 35.182537 loss_ctc 39.501156 loss_rnnt 20.461073 hw_loss 0.104204 lr 0.00085553 rank 5
2023-02-17 20:04:41,225 DEBUG TRAIN Batch 4/800 loss 30.940857 loss_att 35.016380 loss_ctc 50.674236 loss_rnnt 27.427811 hw_loss 0.125293 lr 0.00085565 rank 7
2023-02-17 20:05:52,991 DEBUG TRAIN Batch 4/900 loss 24.747036 loss_att 28.395634 loss_ctc 42.052563 loss_rnnt 21.709774 hw_loss 0.000256 lr 0.00085473 rank 4
2023-02-17 20:05:52,996 DEBUG TRAIN Batch 4/900 loss 28.446970 loss_att 34.008522 loss_ctc 43.882965 loss_rnnt 25.276491 hw_loss 0.000068 lr 0.00085428 rank 5
2023-02-17 20:05:52,996 DEBUG TRAIN Batch 4/900 loss 19.896906 loss_att 26.153282 loss_ctc 25.889458 loss_rnnt 17.811666 hw_loss 0.065546 lr 0.00085426 rank 3
2023-02-17 20:05:52,997 DEBUG TRAIN Batch 4/900 loss 16.804760 loss_att 20.310450 loss_ctc 28.637810 loss_rnnt 14.477978 hw_loss 0.089820 lr 0.00085440 rank 7
2023-02-17 20:05:52,998 DEBUG TRAIN Batch 4/900 loss 19.121334 loss_att 29.074829 loss_ctc 32.192558 loss_rnnt 15.387487 hw_loss 0.000596 lr 0.00085397 rank 0
2023-02-17 20:05:53,000 DEBUG TRAIN Batch 4/900 loss 14.203927 loss_att 19.569164 loss_ctc 20.833826 loss_rnnt 12.246448 hw_loss 0.000836 lr 0.00085452 rank 6
2023-02-17 20:05:53,005 DEBUG TRAIN Batch 4/900 loss 20.499460 loss_att 31.655602 loss_ctc 32.089005 loss_rnnt 16.697079 hw_loss 0.048528 lr 0.00085342 rank 2
2023-02-17 20:05:53,043 DEBUG TRAIN Batch 4/900 loss 15.302328 loss_att 20.428280 loss_ctc 27.970152 loss_rnnt 12.552888 hw_loss 0.066011 lr 0.00085417 rank 1
2023-02-17 20:07:05,876 DEBUG TRAIN Batch 4/1000 loss 29.169510 loss_att 33.095467 loss_ctc 42.379395 loss_rnnt 26.622295 hw_loss 0.001324 lr 0.00085301 rank 3
2023-02-17 20:07:05,882 DEBUG TRAIN Batch 4/1000 loss 43.722824 loss_att 44.118725 loss_ctc 52.123634 loss_rnnt 42.494709 hw_loss 0.054044 lr 0.00085349 rank 4
2023-02-17 20:07:05,886 DEBUG TRAIN Batch 4/1000 loss 34.021935 loss_att 38.953056 loss_ctc 53.728428 loss_rnnt 30.376448 hw_loss 0.059493 lr 0.00085293 rank 1
2023-02-17 20:07:05,886 DEBUG TRAIN Batch 4/1000 loss 17.847601 loss_att 23.207462 loss_ctc 27.753376 loss_rnnt 15.452383 hw_loss 0.004643 lr 0.00085327 rank 6
2023-02-17 20:07:05,889 DEBUG TRAIN Batch 4/1000 loss 25.492519 loss_att 28.576637 loss_ctc 38.331398 loss_rnnt 23.132286 hw_loss 0.059176 lr 0.00085304 rank 5
2023-02-17 20:07:05,889 DEBUG TRAIN Batch 4/1000 loss 48.016850 loss_att 51.726391 loss_ctc 70.882912 loss_rnnt 44.191162 hw_loss 0.065564 lr 0.00085315 rank 7
2023-02-17 20:07:05,889 DEBUG TRAIN Batch 4/1000 loss 25.509026 loss_att 34.114189 loss_ctc 46.953926 loss_rnnt 20.924862 hw_loss 0.007148 lr 0.00085273 rank 0
2023-02-17 20:07:05,889 DEBUG TRAIN Batch 4/1000 loss 28.152227 loss_att 33.597424 loss_ctc 37.800789 loss_rnnt 25.727955 hw_loss 0.091419 lr 0.00085218 rank 2
2023-02-17 20:08:20,192 DEBUG TRAIN Batch 4/1100 loss 17.720583 loss_att 22.143135 loss_ctc 31.972637 loss_rnnt 14.925416 hw_loss 0.019464 lr 0.00085225 rank 4
2023-02-17 20:08:20,198 DEBUG TRAIN Batch 4/1100 loss 21.346334 loss_att 28.502865 loss_ctc 33.124458 loss_rnnt 18.327639 hw_loss 0.031821 lr 0.00085191 rank 7
2023-02-17 20:08:20,200 DEBUG TRAIN Batch 4/1100 loss 23.922388 loss_att 28.970421 loss_ctc 34.792809 loss_rnnt 21.403919 hw_loss 0.111510 lr 0.00085095 rank 2
2023-02-17 20:08:20,202 DEBUG TRAIN Batch 4/1100 loss 28.917933 loss_att 32.944454 loss_ctc 55.288570 loss_rnnt 24.551310 hw_loss 0.084807 lr 0.00085178 rank 3
2023-02-17 20:08:20,203 DEBUG TRAIN Batch 4/1100 loss 34.178066 loss_att 36.931221 loss_ctc 50.502953 loss_rnnt 31.450451 hw_loss 0.000631 lr 0.00085180 rank 5
2023-02-17 20:08:20,204 DEBUG TRAIN Batch 4/1100 loss 8.871599 loss_att 16.869562 loss_ctc 18.556648 loss_rnnt 5.961170 hw_loss 0.036556 lr 0.00085169 rank 1
2023-02-17 20:08:20,205 DEBUG TRAIN Batch 4/1100 loss 17.371126 loss_att 22.187693 loss_ctc 25.528290 loss_rnnt 15.310360 hw_loss 0.018433 lr 0.00085203 rank 6
2023-02-17 20:08:20,207 DEBUG TRAIN Batch 4/1100 loss 31.154503 loss_att 35.888451 loss_ctc 44.676815 loss_rnnt 28.398865 hw_loss 0.011012 lr 0.00085149 rank 0
2023-02-17 20:09:34,191 DEBUG TRAIN Batch 4/1200 loss 23.360445 loss_att 22.577795 loss_ctc 30.527645 loss_rnnt 22.461168 hw_loss 0.187834 lr 0.00085101 rank 4
2023-02-17 20:09:34,193 DEBUG TRAIN Batch 4/1200 loss 34.870747 loss_att 34.974503 loss_ctc 51.793549 loss_rnnt 32.558064 hw_loss 0.066671 lr 0.00085026 rank 0
2023-02-17 20:09:34,195 DEBUG TRAIN Batch 4/1200 loss 23.552139 loss_att 31.306614 loss_ctc 41.992199 loss_rnnt 19.497879 hw_loss 0.083791 lr 0.00084972 rank 2
2023-02-17 20:09:34,195 DEBUG TRAIN Batch 4/1200 loss 19.403721 loss_att 22.728024 loss_ctc 31.360327 loss_rnnt 17.075043 hw_loss 0.130505 lr 0.00085080 rank 6
2023-02-17 20:09:34,195 DEBUG TRAIN Batch 4/1200 loss 24.355787 loss_att 29.783070 loss_ctc 41.799091 loss_rnnt 20.928873 hw_loss 0.029408 lr 0.00085057 rank 5
2023-02-17 20:09:34,196 DEBUG TRAIN Batch 4/1200 loss 18.602371 loss_att 21.251717 loss_ctc 27.502827 loss_rnnt 16.871777 hw_loss 0.026243 lr 0.00085046 rank 1
2023-02-17 20:09:34,201 DEBUG TRAIN Batch 4/1200 loss 22.573805 loss_att 25.538662 loss_ctc 36.727978 loss_rnnt 20.061962 hw_loss 0.059345 lr 0.00085068 rank 7
2023-02-17 20:09:34,202 DEBUG TRAIN Batch 4/1200 loss 26.369843 loss_att 27.480015 loss_ctc 37.150139 loss_rnnt 24.680235 hw_loss 0.056628 lr 0.00085054 rank 3
2023-02-17 20:10:48,053 DEBUG TRAIN Batch 4/1300 loss 20.706068 loss_att 28.687609 loss_ctc 35.024902 loss_rnnt 17.162975 hw_loss 0.070511 lr 0.00084978 rank 4
2023-02-17 20:10:48,056 DEBUG TRAIN Batch 4/1300 loss 39.872215 loss_att 42.044201 loss_ctc 58.359875 loss_rnnt 36.959633 hw_loss 0.024678 lr 0.00084903 rank 0
2023-02-17 20:10:48,058 DEBUG TRAIN Batch 4/1300 loss 24.571369 loss_att 26.342255 loss_ctc 30.684044 loss_rnnt 23.350607 hw_loss 0.096680 lr 0.00084957 rank 6
2023-02-17 20:10:48,059 DEBUG TRAIN Batch 4/1300 loss 33.562313 loss_att 37.947884 loss_ctc 47.174297 loss_rnnt 30.841774 hw_loss 0.053424 lr 0.00084923 rank 1
2023-02-17 20:10:48,062 DEBUG TRAIN Batch 4/1300 loss 12.927305 loss_att 14.244722 loss_ctc 16.971302 loss_rnnt 12.071959 hw_loss 0.098746 lr 0.00084849 rank 2
2023-02-17 20:10:48,064 DEBUG TRAIN Batch 4/1300 loss 26.801849 loss_att 38.515255 loss_ctc 44.134727 loss_rnnt 22.132963 hw_loss 0.028413 lr 0.00084931 rank 3
2023-02-17 20:10:48,066 DEBUG TRAIN Batch 4/1300 loss 40.309227 loss_att 46.798584 loss_ctc 72.146713 loss_rnnt 34.752701 hw_loss 0.025604 lr 0.00084934 rank 5
2023-02-17 20:10:48,071 DEBUG TRAIN Batch 4/1300 loss 12.415983 loss_att 12.459707 loss_ctc 17.538700 loss_rnnt 11.648190 hw_loss 0.142537 lr 0.00084945 rank 7
2023-02-17 20:12:02,649 DEBUG TRAIN Batch 4/1400 loss 28.033396 loss_att 39.482948 loss_ctc 41.394756 loss_rnnt 23.932213 hw_loss 0.055797 lr 0.00084823 rank 7
2023-02-17 20:12:02,653 DEBUG TRAIN Batch 4/1400 loss 19.015564 loss_att 28.934454 loss_ctc 35.981075 loss_rnnt 14.769598 hw_loss 0.000222 lr 0.00084812 rank 5
2023-02-17 20:12:02,664 DEBUG TRAIN Batch 4/1400 loss 33.984169 loss_att 40.212616 loss_ctc 48.686951 loss_rnnt 30.777998 hw_loss 0.000213 lr 0.00084835 rank 6
2023-02-17 20:12:02,665 DEBUG TRAIN Batch 4/1400 loss 27.339411 loss_att 35.308479 loss_ctc 38.102581 loss_rnnt 24.275257 hw_loss 0.066089 lr 0.00084801 rank 1
2023-02-17 20:12:02,670 DEBUG TRAIN Batch 4/1400 loss 25.141245 loss_att 31.817345 loss_ctc 39.375443 loss_rnnt 21.886930 hw_loss 0.039756 lr 0.00084781 rank 0
2023-02-17 20:12:02,670 DEBUG TRAIN Batch 4/1400 loss 18.170950 loss_att 23.997932 loss_ctc 34.339123 loss_rnnt 14.849654 hw_loss 0.000266 lr 0.00084809 rank 3
2023-02-17 20:12:02,679 DEBUG TRAIN Batch 4/1400 loss 32.916126 loss_att 39.843071 loss_ctc 54.354836 loss_rnnt 28.645597 hw_loss 0.049960 lr 0.00084728 rank 2
2023-02-17 20:12:02,681 DEBUG TRAIN Batch 4/1400 loss 31.401819 loss_att 37.115395 loss_ctc 56.640636 loss_rnnt 26.893719 hw_loss 0.000395 lr 0.00084856 rank 4
2023-02-17 20:13:16,488 DEBUG TRAIN Batch 4/1500 loss 29.861343 loss_att 33.035954 loss_ctc 45.492569 loss_rnnt 27.051743 hw_loss 0.169714 lr 0.00084734 rank 4
2023-02-17 20:13:16,492 DEBUG TRAIN Batch 4/1500 loss 17.652815 loss_att 21.896839 loss_ctc 30.772625 loss_rnnt 15.013896 hw_loss 0.076509 lr 0.00084713 rank 6
2023-02-17 20:13:16,493 DEBUG TRAIN Batch 4/1500 loss 56.709023 loss_att 63.945892 loss_ctc 91.506271 loss_rnnt 50.608608 hw_loss 0.025139 lr 0.00084659 rank 0
2023-02-17 20:13:16,493 DEBUG TRAIN Batch 4/1500 loss 20.560913 loss_att 29.501404 loss_ctc 35.293865 loss_rnnt 16.791698 hw_loss 0.031352 lr 0.00084690 rank 5
2023-02-17 20:13:16,494 DEBUG TRAIN Batch 4/1500 loss 53.568470 loss_att 56.676682 loss_ctc 79.804893 loss_rnnt 49.420639 hw_loss 0.052503 lr 0.00084701 rank 7
2023-02-17 20:13:16,501 DEBUG TRAIN Batch 4/1500 loss 28.163166 loss_att 38.889923 loss_ctc 48.787849 loss_rnnt 23.235207 hw_loss 0.061218 lr 0.00084606 rank 2
2023-02-17 20:13:16,508 DEBUG TRAIN Batch 4/1500 loss 14.021756 loss_att 22.460270 loss_ctc 23.945751 loss_rnnt 10.981204 hw_loss 0.055595 lr 0.00084687 rank 3
2023-02-17 20:13:16,542 DEBUG TRAIN Batch 4/1500 loss 17.081766 loss_att 22.005590 loss_ctc 34.490532 loss_rnnt 13.763815 hw_loss 0.022536 lr 0.00084679 rank 1
2023-02-17 20:14:28,736 DEBUG TRAIN Batch 4/1600 loss 28.265114 loss_att 34.824730 loss_ctc 43.711891 loss_rnnt 24.848797 hw_loss 0.084048 lr 0.00084579 rank 7
2023-02-17 20:14:28,745 DEBUG TRAIN Batch 4/1600 loss 17.104527 loss_att 22.483334 loss_ctc 33.286800 loss_rnnt 13.809879 hw_loss 0.114840 lr 0.00084569 rank 5
2023-02-17 20:14:28,745 DEBUG TRAIN Batch 4/1600 loss 25.727406 loss_att 37.577438 loss_ctc 44.968948 loss_rnnt 20.758419 hw_loss 0.062704 lr 0.00084612 rank 4
2023-02-17 20:14:28,749 DEBUG TRAIN Batch 4/1600 loss 16.661890 loss_att 20.970556 loss_ctc 30.115622 loss_rnnt 13.990811 hw_loss 0.029088 lr 0.00084592 rank 6
2023-02-17 20:14:28,751 DEBUG TRAIN Batch 4/1600 loss 33.761539 loss_att 36.684296 loss_ctc 48.193600 loss_rnnt 31.204020 hw_loss 0.091291 lr 0.00084566 rank 3
2023-02-17 20:14:28,758 DEBUG TRAIN Batch 4/1600 loss 43.039440 loss_att 48.257874 loss_ctc 62.530174 loss_rnnt 39.363781 hw_loss 0.062264 lr 0.00084558 rank 1
2023-02-17 20:14:28,758 DEBUG TRAIN Batch 4/1600 loss 25.345947 loss_att 27.185966 loss_ctc 42.702477 loss_rnnt 22.596397 hw_loss 0.126268 lr 0.00084538 rank 0
2023-02-17 20:14:28,765 DEBUG TRAIN Batch 4/1600 loss 21.034613 loss_att 29.792381 loss_ctc 40.212467 loss_rnnt 16.659536 hw_loss 0.124640 lr 0.00084485 rank 2
2023-02-17 20:15:42,144 DEBUG TRAIN Batch 4/1700 loss 36.127377 loss_att 44.078770 loss_ctc 55.289238 loss_rnnt 31.968060 hw_loss 0.026484 lr 0.00084471 rank 6
2023-02-17 20:15:42,148 DEBUG TRAIN Batch 4/1700 loss 21.077572 loss_att 25.320461 loss_ctc 36.128864 loss_rnnt 18.218254 hw_loss 0.007313 lr 0.00084448 rank 5
2023-02-17 20:15:42,148 DEBUG TRAIN Batch 4/1700 loss 22.492517 loss_att 24.969345 loss_ctc 30.634750 loss_rnnt 20.901535 hw_loss 0.018726 lr 0.00084365 rank 2
2023-02-17 20:15:42,148 DEBUG TRAIN Batch 4/1700 loss 24.945190 loss_att 29.903097 loss_ctc 41.353127 loss_rnnt 21.729532 hw_loss 0.068159 lr 0.00084445 rank 3
2023-02-17 20:15:42,158 DEBUG TRAIN Batch 4/1700 loss 23.285194 loss_att 28.327522 loss_ctc 41.323879 loss_rnnt 19.843479 hw_loss 0.052670 lr 0.00084437 rank 1
2023-02-17 20:15:42,173 DEBUG TRAIN Batch 4/1700 loss 23.975229 loss_att 28.510551 loss_ctc 40.148956 loss_rnnt 20.911524 hw_loss 0.000267 lr 0.00084418 rank 0
2023-02-17 20:15:42,174 DEBUG TRAIN Batch 4/1700 loss 31.249557 loss_att 31.915380 loss_ctc 41.995796 loss_rnnt 29.664783 hw_loss 0.035203 lr 0.00084491 rank 4
2023-02-17 20:15:42,200 DEBUG TRAIN Batch 4/1700 loss 23.687248 loss_att 30.979687 loss_ctc 34.815399 loss_rnnt 20.744888 hw_loss 0.000221 lr 0.00084459 rank 7
2023-02-17 20:16:57,743 DEBUG TRAIN Batch 4/1800 loss 15.015778 loss_att 14.823141 loss_ctc 21.684919 loss_rnnt 14.063971 hw_loss 0.189590 lr 0.00084371 rank 4
2023-02-17 20:16:57,744 DEBUG TRAIN Batch 4/1800 loss 34.205940 loss_att 39.919563 loss_ctc 51.927578 loss_rnnt 30.694429 hw_loss 0.011066 lr 0.00084328 rank 5
2023-02-17 20:16:57,745 DEBUG TRAIN Batch 4/1800 loss 21.218746 loss_att 25.939064 loss_ctc 33.408253 loss_rnnt 18.572512 hw_loss 0.144191 lr 0.00084351 rank 6
2023-02-17 20:16:57,745 DEBUG TRAIN Batch 4/1800 loss 20.640213 loss_att 23.722612 loss_ctc 31.131054 loss_rnnt 18.602547 hw_loss 0.042013 lr 0.00084325 rank 3
2023-02-17 20:16:57,746 DEBUG TRAIN Batch 4/1800 loss 30.862963 loss_att 32.487480 loss_ctc 43.566639 loss_rnnt 28.788879 hw_loss 0.103795 lr 0.00084298 rank 0
2023-02-17 20:16:57,747 DEBUG TRAIN Batch 4/1800 loss 18.456619 loss_att 26.971235 loss_ctc 29.997280 loss_rnnt 15.185804 hw_loss 0.054633 lr 0.00084245 rank 2
2023-02-17 20:16:57,749 DEBUG TRAIN Batch 4/1800 loss 25.916822 loss_att 30.152185 loss_ctc 43.615425 loss_rnnt 22.645124 hw_loss 0.121522 lr 0.00084317 rank 1
2023-02-17 20:16:57,797 DEBUG TRAIN Batch 4/1800 loss 38.106018 loss_att 40.866623 loss_ctc 57.162693 loss_rnnt 34.994839 hw_loss 0.034069 lr 0.00084339 rank 7
2023-02-17 20:18:10,567 DEBUG TRAIN Batch 4/1900 loss 19.738682 loss_att 27.266956 loss_ctc 39.483238 loss_rnnt 15.584921 hw_loss 0.029061 lr 0.00084251 rank 4
2023-02-17 20:18:10,578 DEBUG TRAIN Batch 4/1900 loss 11.934439 loss_att 12.213146 loss_ctc 17.528528 loss_rnnt 11.041492 hw_loss 0.171239 lr 0.00084197 rank 1
2023-02-17 20:18:10,581 DEBUG TRAIN Batch 4/1900 loss 14.756290 loss_att 16.686005 loss_ctc 19.341431 loss_rnnt 13.698277 hw_loss 0.113845 lr 0.00084178 rank 0
2023-02-17 20:18:10,584 DEBUG TRAIN Batch 4/1900 loss 24.858534 loss_att 29.301542 loss_ctc 41.995728 loss_rnnt 21.622702 hw_loss 0.116758 lr 0.00084206 rank 3
2023-02-17 20:18:10,588 DEBUG TRAIN Batch 4/1900 loss 22.433521 loss_att 23.639767 loss_ctc 35.433281 loss_rnnt 20.419470 hw_loss 0.074065 lr 0.00084219 rank 7
2023-02-17 20:18:10,589 DEBUG TRAIN Batch 4/1900 loss 25.332281 loss_att 29.776451 loss_ctc 36.734890 loss_rnnt 22.852283 hw_loss 0.132777 lr 0.00084208 rank 5
2023-02-17 20:18:10,594 DEBUG TRAIN Batch 4/1900 loss 13.550632 loss_att 14.996188 loss_ctc 19.927572 loss_rnnt 12.309416 hw_loss 0.190961 lr 0.00084231 rank 6
2023-02-17 20:18:10,631 DEBUG TRAIN Batch 4/1900 loss 24.383942 loss_att 26.041313 loss_ctc 36.602489 loss_rnnt 22.358501 hw_loss 0.121551 lr 0.00084126 rank 2
2023-02-17 20:19:23,694 DEBUG TRAIN Batch 4/2000 loss 27.692932 loss_att 34.387272 loss_ctc 45.574902 loss_rnnt 23.947561 hw_loss 0.041700 lr 0.00084132 rank 4
2023-02-17 20:19:23,697 DEBUG TRAIN Batch 4/2000 loss 35.092728 loss_att 41.369011 loss_ctc 51.283012 loss_rnnt 31.644545 hw_loss 0.064163 lr 0.00084059 rank 0
2023-02-17 20:19:23,699 DEBUG TRAIN Batch 4/2000 loss 46.097095 loss_att 51.320724 loss_ctc 62.038055 loss_rnnt 42.911964 hw_loss 0.028022 lr 0.00084086 rank 3
2023-02-17 20:19:23,700 DEBUG TRAIN Batch 4/2000 loss 39.920658 loss_att 46.482033 loss_ctc 68.756340 loss_rnnt 34.744511 hw_loss 0.035842 lr 0.00084100 rank 7
2023-02-17 20:19:23,701 DEBUG TRAIN Batch 4/2000 loss 41.647636 loss_att 53.314308 loss_ctc 76.485321 loss_rnnt 34.627686 hw_loss 0.077986 lr 0.00084078 rank 1
2023-02-17 20:19:23,703 DEBUG TRAIN Batch 4/2000 loss 12.588017 loss_att 21.855461 loss_ctc 29.111769 loss_rnnt 8.520689 hw_loss 0.020007 lr 0.00084111 rank 6
2023-02-17 20:19:23,707 DEBUG TRAIN Batch 4/2000 loss 42.608421 loss_att 47.492702 loss_ctc 68.862206 loss_rnnt 38.096573 hw_loss 0.064669 lr 0.00084089 rank 5
2023-02-17 20:19:23,755 DEBUG TRAIN Batch 4/2000 loss 15.329591 loss_att 26.137814 loss_ctc 36.314606 loss_rnnt 10.342554 hw_loss 0.051354 lr 0.00084007 rank 2
2023-02-17 20:20:38,061 DEBUG TRAIN Batch 4/2100 loss 17.774303 loss_att 21.573975 loss_ctc 25.708912 loss_rnnt 15.922447 hw_loss 0.063701 lr 0.00083889 rank 2
2023-02-17 20:20:38,062 DEBUG TRAIN Batch 4/2100 loss 28.696898 loss_att 34.156532 loss_ctc 38.032131 loss_rnnt 26.332226 hw_loss 0.052591 lr 0.00083968 rank 3
2023-02-17 20:20:38,071 DEBUG TRAIN Batch 4/2100 loss 19.568378 loss_att 23.712528 loss_ctc 27.143322 loss_rnnt 17.704281 hw_loss 0.047388 lr 0.00084013 rank 4
2023-02-17 20:20:38,076 DEBUG TRAIN Batch 4/2100 loss 49.905529 loss_att 64.381943 loss_ctc 72.854851 loss_rnnt 43.942360 hw_loss 0.014950 lr 0.00083981 rank 7
2023-02-17 20:20:38,076 DEBUG TRAIN Batch 4/2100 loss 26.944057 loss_att 30.297974 loss_ctc 50.785091 loss_rnnt 23.094269 hw_loss 0.000372 lr 0.00083993 rank 6
2023-02-17 20:20:38,077 DEBUG TRAIN Batch 4/2100 loss 34.357948 loss_att 37.783733 loss_ctc 54.376854 loss_rnnt 31.003525 hw_loss 0.000150 lr 0.00083960 rank 1
2023-02-17 20:20:38,078 DEBUG TRAIN Batch 4/2100 loss 15.832405 loss_att 16.631287 loss_ctc 21.519400 loss_rnnt 14.893854 hw_loss 0.038452 lr 0.00083970 rank 5
2023-02-17 20:20:38,080 DEBUG TRAIN Batch 4/2100 loss 10.725023 loss_att 15.328251 loss_ctc 17.420149 loss_rnnt 8.872754 hw_loss 0.073013 lr 0.00083941 rank 0
2023-02-17 20:21:51,673 DEBUG TRAIN Batch 4/2200 loss 24.539856 loss_att 30.838961 loss_ctc 40.517582 loss_rnnt 21.132313 hw_loss 0.032546 lr 0.00083895 rank 4
2023-02-17 20:21:51,673 DEBUG TRAIN Batch 4/2200 loss 18.364157 loss_att 24.695538 loss_ctc 32.525940 loss_rnnt 15.198577 hw_loss 0.020748 lr 0.00083874 rank 6
2023-02-17 20:21:51,676 DEBUG TRAIN Batch 4/2200 loss 17.021130 loss_att 23.070021 loss_ctc 32.124489 loss_rnnt 13.775871 hw_loss 0.040683 lr 0.00083850 rank 3
2023-02-17 20:21:51,679 DEBUG TRAIN Batch 4/2200 loss 35.436516 loss_att 41.532333 loss_ctc 57.384064 loss_rnnt 31.246532 hw_loss 0.083398 lr 0.00083841 rank 1
2023-02-17 20:21:51,680 DEBUG TRAIN Batch 4/2200 loss 18.181713 loss_att 21.931805 loss_ctc 35.737457 loss_rnnt 15.017729 hw_loss 0.137253 lr 0.00083852 rank 5
2023-02-17 20:21:51,680 DEBUG TRAIN Batch 4/2200 loss 28.873571 loss_att 31.411800 loss_ctc 40.190651 loss_rnnt 26.856794 hw_loss 0.000347 lr 0.00083771 rank 2
2023-02-17 20:21:51,686 DEBUG TRAIN Batch 4/2200 loss 24.264988 loss_att 28.083647 loss_ctc 34.873482 loss_rnnt 22.067806 hw_loss 0.035594 lr 0.00083823 rank 0
2023-02-17 20:21:51,723 DEBUG TRAIN Batch 4/2200 loss 13.771986 loss_att 19.622551 loss_ctc 24.219360 loss_rnnt 11.172953 hw_loss 0.067382 lr 0.00083863 rank 7
2023-02-17 20:23:04,648 DEBUG TRAIN Batch 4/2300 loss 9.429024 loss_att 12.538240 loss_ctc 13.756071 loss_rnnt 8.172668 hw_loss 0.107949 lr 0.00083777 rank 4
2023-02-17 20:23:04,649 DEBUG TRAIN Batch 4/2300 loss 31.944967 loss_att 33.921703 loss_ctc 50.806496 loss_rnnt 28.994251 hw_loss 0.075926 lr 0.00083705 rank 0
2023-02-17 20:23:04,652 DEBUG TRAIN Batch 4/2300 loss 26.002739 loss_att 36.744755 loss_ctc 44.235840 loss_rnnt 21.405939 hw_loss 0.032469 lr 0.00083732 rank 3
2023-02-17 20:23:04,653 DEBUG TRAIN Batch 4/2300 loss 30.485023 loss_att 32.492233 loss_ctc 56.273842 loss_rnnt 26.584127 hw_loss 0.114276 lr 0.00083734 rank 5
2023-02-17 20:23:04,656 DEBUG TRAIN Batch 4/2300 loss 20.440851 loss_att 25.976727 loss_ctc 35.160522 loss_rnnt 17.354904 hw_loss 0.030280 lr 0.00083757 rank 6
2023-02-17 20:23:04,686 DEBUG TRAIN Batch 4/2300 loss 22.812763 loss_att 25.869570 loss_ctc 30.857756 loss_rnnt 21.072823 hw_loss 0.104835 lr 0.00083745 rank 7
2023-02-17 20:23:04,690 DEBUG TRAIN Batch 4/2300 loss 21.031881 loss_att 24.965038 loss_ctc 33.104286 loss_rnnt 18.596298 hw_loss 0.073679 lr 0.00083724 rank 1
2023-02-17 20:23:04,702 DEBUG TRAIN Batch 4/2300 loss 34.427635 loss_att 43.487419 loss_ctc 61.446693 loss_rnnt 28.994619 hw_loss 0.034722 lr 0.00083653 rank 2
2023-02-17 20:24:16,922 DEBUG TRAIN Batch 4/2400 loss 40.332901 loss_att 49.660427 loss_ctc 58.631176 loss_rnnt 36.000294 hw_loss 0.051254 lr 0.00083615 rank 3
2023-02-17 20:24:16,924 DEBUG TRAIN Batch 4/2400 loss 20.088936 loss_att 27.632507 loss_ctc 33.765930 loss_rnnt 16.725613 hw_loss 0.058141 lr 0.00083639 rank 6
2023-02-17 20:24:16,926 DEBUG TRAIN Batch 4/2400 loss 12.618341 loss_att 15.443487 loss_ctc 21.334700 loss_rnnt 10.821467 hw_loss 0.130621 lr 0.00083659 rank 4
2023-02-17 20:24:16,927 DEBUG TRAIN Batch 4/2400 loss 27.211489 loss_att 29.160484 loss_ctc 39.463707 loss_rnnt 25.104595 hw_loss 0.156499 lr 0.00083607 rank 1
2023-02-17 20:24:16,928 DEBUG TRAIN Batch 4/2400 loss 16.542208 loss_att 22.437634 loss_ctc 28.757145 loss_rnnt 13.668447 hw_loss 0.123780 lr 0.00083588 rank 0
2023-02-17 20:24:16,931 DEBUG TRAIN Batch 4/2400 loss 23.111870 loss_att 26.871420 loss_ctc 37.741104 loss_rnnt 20.374081 hw_loss 0.066211 lr 0.00083628 rank 7
2023-02-17 20:24:16,930 DEBUG TRAIN Batch 4/2400 loss 39.551216 loss_att 44.688213 loss_ctc 58.517937 loss_rnnt 35.915455 hw_loss 0.148988 lr 0.00083537 rank 2
2023-02-17 20:24:16,981 DEBUG TRAIN Batch 4/2400 loss 18.419991 loss_att 23.423578 loss_ctc 27.768305 loss_rnnt 16.150578 hw_loss 0.041724 lr 0.00083617 rank 5
2023-02-17 20:25:32,377 DEBUG TRAIN Batch 4/2500 loss 20.704353 loss_att 30.905739 loss_ctc 40.136414 loss_rnnt 16.072914 hw_loss 0.000413 lr 0.00083542 rank 4
2023-02-17 20:25:32,380 DEBUG TRAIN Batch 4/2500 loss 21.812029 loss_att 23.385683 loss_ctc 34.301460 loss_rnnt 19.770304 hw_loss 0.115759 lr 0.00083523 rank 6
2023-02-17 20:25:32,379 DEBUG TRAIN Batch 4/2500 loss 21.626863 loss_att 26.353960 loss_ctc 34.180775 loss_rnnt 18.942467 hw_loss 0.122105 lr 0.00083490 rank 1
2023-02-17 20:25:32,380 DEBUG TRAIN Batch 4/2500 loss 18.731640 loss_att 20.235029 loss_ctc 25.688282 loss_rnnt 17.463520 hw_loss 0.074790 lr 0.00083498 rank 3
2023-02-17 20:25:32,383 DEBUG TRAIN Batch 4/2500 loss 28.592234 loss_att 28.290394 loss_ctc 41.230103 loss_rnnt 26.908100 hw_loss 0.111469 lr 0.00083501 rank 5
2023-02-17 20:25:32,387 DEBUG TRAIN Batch 4/2500 loss 15.039682 loss_att 17.264606 loss_ctc 25.413908 loss_rnnt 13.184283 hw_loss 0.050969 lr 0.00083471 rank 0
2023-02-17 20:25:32,390 DEBUG TRAIN Batch 4/2500 loss 21.534103 loss_att 27.224785 loss_ctc 39.963894 loss_rnnt 17.907490 hw_loss 0.058448 lr 0.00083511 rank 7
2023-02-17 20:25:32,429 DEBUG TRAIN Batch 4/2500 loss 19.426300 loss_att 24.609247 loss_ctc 36.131512 loss_rnnt 16.122030 hw_loss 0.075596 lr 0.00083420 rank 2
2023-02-17 20:26:45,573 DEBUG TRAIN Batch 4/2600 loss 18.831617 loss_att 23.120943 loss_ctc 25.621740 loss_rnnt 17.029312 hw_loss 0.073293 lr 0.00083374 rank 1
2023-02-17 20:26:45,586 DEBUG TRAIN Batch 4/2600 loss 22.475117 loss_att 27.666155 loss_ctc 33.974125 loss_rnnt 19.866320 hw_loss 0.070100 lr 0.00083426 rank 4
2023-02-17 20:26:45,588 DEBUG TRAIN Batch 4/2600 loss 22.908459 loss_att 31.560116 loss_ctc 50.991257 loss_rnnt 17.433483 hw_loss 0.000506 lr 0.00083355 rank 0
2023-02-17 20:26:45,589 DEBUG TRAIN Batch 4/2600 loss 12.303770 loss_att 22.143814 loss_ctc 27.077877 loss_rnnt 8.354863 hw_loss 0.020657 lr 0.00083382 rank 3
2023-02-17 20:26:45,591 DEBUG TRAIN Batch 4/2600 loss 29.176756 loss_att 34.043533 loss_ctc 49.257687 loss_rnnt 25.488262 hw_loss 0.070651 lr 0.00083406 rank 6
2023-02-17 20:26:45,592 DEBUG TRAIN Batch 4/2600 loss 17.442041 loss_att 17.447502 loss_ctc 23.854084 loss_rnnt 16.513842 hw_loss 0.135313 lr 0.00083395 rank 7
2023-02-17 20:26:45,592 DEBUG TRAIN Batch 4/2600 loss 20.124334 loss_att 31.265717 loss_ctc 31.469585 loss_rnnt 16.383268 hw_loss 0.000169 lr 0.00083384 rank 5
2023-02-17 20:26:45,642 DEBUG TRAIN Batch 4/2600 loss 49.948280 loss_att 55.187038 loss_ctc 74.665894 loss_rnnt 45.530682 hw_loss 0.139053 lr 0.00083304 rank 2
2023-02-17 20:27:58,635 DEBUG TRAIN Batch 4/2700 loss 15.837755 loss_att 21.226406 loss_ctc 20.914810 loss_rnnt 14.064964 hw_loss 0.033974 lr 0.00083258 rank 1
2023-02-17 20:27:58,640 DEBUG TRAIN Batch 4/2700 loss 21.259098 loss_att 20.632412 loss_ctc 32.615486 loss_rnnt 19.797115 hw_loss 0.137129 lr 0.00083240 rank 0
2023-02-17 20:27:58,642 DEBUG TRAIN Batch 4/2700 loss 22.433580 loss_att 23.774414 loss_ctc 35.649075 loss_rnnt 20.370628 hw_loss 0.061348 lr 0.00083291 rank 6
2023-02-17 20:27:58,642 DEBUG TRAIN Batch 4/2700 loss 42.722969 loss_att 54.877827 loss_ctc 62.872498 loss_rnnt 37.554916 hw_loss 0.094645 lr 0.00083269 rank 5
2023-02-17 20:27:58,643 DEBUG TRAIN Batch 4/2700 loss 31.982986 loss_att 42.111168 loss_ctc 60.112774 loss_rnnt 26.158878 hw_loss 0.089686 lr 0.00083310 rank 4
2023-02-17 20:27:58,644 DEBUG TRAIN Batch 4/2700 loss 41.315277 loss_att 50.296318 loss_ctc 57.614624 loss_rnnt 37.329758 hw_loss 0.030119 lr 0.00083189 rank 2
2023-02-17 20:27:58,644 DEBUG TRAIN Batch 4/2700 loss 32.541821 loss_att 37.333824 loss_ctc 46.120056 loss_rnnt 29.724951 hw_loss 0.090076 lr 0.00083266 rank 3
2023-02-17 20:27:58,691 DEBUG TRAIN Batch 4/2700 loss 20.031195 loss_att 22.858099 loss_ctc 33.711582 loss_rnnt 17.627663 hw_loss 0.026434 lr 0.00083279 rank 7
2023-02-17 20:29:12,597 DEBUG TRAIN Batch 4/2800 loss 18.730007 loss_att 28.717899 loss_ctc 36.140289 loss_rnnt 14.394912 hw_loss 0.030276 lr 0.00083164 rank 7
2023-02-17 20:29:12,603 DEBUG TRAIN Batch 4/2800 loss 29.677036 loss_att 35.019909 loss_ctc 44.241817 loss_rnnt 26.619501 hw_loss 0.088101 lr 0.00083195 rank 4
2023-02-17 20:29:12,608 DEBUG TRAIN Batch 4/2800 loss 20.890053 loss_att 27.915604 loss_ctc 36.115520 loss_rnnt 17.414091 hw_loss 0.076483 lr 0.00083125 rank 0
2023-02-17 20:29:12,609 DEBUG TRAIN Batch 4/2800 loss 17.486452 loss_att 21.883102 loss_ctc 28.975836 loss_rnnt 15.074595 hw_loss 0.001142 lr 0.00083151 rank 3
2023-02-17 20:29:12,610 DEBUG TRAIN Batch 4/2800 loss 26.454531 loss_att 32.613228 loss_ctc 42.251217 loss_rnnt 23.108479 hw_loss 0.015161 lr 0.00083175 rank 6
2023-02-17 20:29:12,620 DEBUG TRAIN Batch 4/2800 loss 16.827728 loss_att 20.953671 loss_ctc 27.138111 loss_rnnt 14.570094 hw_loss 0.108243 lr 0.00083153 rank 5
2023-02-17 20:29:12,639 DEBUG TRAIN Batch 4/2800 loss 22.828402 loss_att 23.077892 loss_ctc 30.236341 loss_rnnt 21.760273 hw_loss 0.057194 lr 0.00083074 rank 2
2023-02-17 20:29:12,651 DEBUG TRAIN Batch 4/2800 loss 18.709106 loss_att 28.699089 loss_ctc 35.018162 loss_rnnt 14.441717 hw_loss 0.177851 lr 0.00083143 rank 1
2023-02-17 20:30:25,864 DEBUG TRAIN Batch 4/2900 loss 25.911352 loss_att 30.157852 loss_ctc 44.696716 loss_rnnt 22.525511 hw_loss 0.059678 lr 0.00083028 rank 1
2023-02-17 20:30:25,866 DEBUG TRAIN Batch 4/2900 loss 29.546068 loss_att 35.393044 loss_ctc 44.308327 loss_rnnt 26.407887 hw_loss 0.000911 lr 0.00082960 rank 2
2023-02-17 20:30:25,886 DEBUG TRAIN Batch 4/2900 loss 18.286890 loss_att 21.948618 loss_ctc 26.595638 loss_rnnt 16.416470 hw_loss 0.056699 lr 0.00083080 rank 4
2023-02-17 20:30:25,886 DEBUG TRAIN Batch 4/2900 loss 35.662575 loss_att 38.866547 loss_ctc 54.002480 loss_rnnt 32.576382 hw_loss 0.000152 lr 0.00083060 rank 6
2023-02-17 20:30:25,886 DEBUG TRAIN Batch 4/2900 loss 24.263220 loss_att 26.822384 loss_ctc 32.689560 loss_rnnt 22.574936 hw_loss 0.099265 lr 0.00083036 rank 3
2023-02-17 20:30:25,889 DEBUG TRAIN Batch 4/2900 loss 20.988810 loss_att 29.082825 loss_ctc 34.666065 loss_rnnt 17.496208 hw_loss 0.094056 lr 0.00083010 rank 0
2023-02-17 20:30:25,891 DEBUG TRAIN Batch 4/2900 loss 21.600378 loss_att 28.237331 loss_ctc 38.112450 loss_rnnt 18.061653 hw_loss 0.018234 lr 0.00083049 rank 7
2023-02-17 20:30:25,892 DEBUG TRAIN Batch 4/2900 loss 30.317394 loss_att 40.301060 loss_ctc 47.993484 loss_rnnt 25.916403 hw_loss 0.088957 lr 0.00083039 rank 5
2023-02-17 20:31:38,476 DEBUG TRAIN Batch 4/3000 loss 24.485699 loss_att 27.637165 loss_ctc 38.264023 loss_rnnt 21.978172 hw_loss 0.075229 lr 0.00082965 rank 4
2023-02-17 20:31:38,476 DEBUG TRAIN Batch 4/3000 loss 30.945368 loss_att 36.579388 loss_ctc 47.442818 loss_rnnt 27.575596 hw_loss 0.081200 lr 0.00082914 rank 1
2023-02-17 20:31:38,478 DEBUG TRAIN Batch 4/3000 loss 27.323483 loss_att 31.455910 loss_ctc 44.516346 loss_rnnt 24.204502 hw_loss 0.000208 lr 0.00082924 rank 5
2023-02-17 20:31:38,479 DEBUG TRAIN Batch 4/3000 loss 25.806393 loss_att 28.348829 loss_ctc 35.714584 loss_rnnt 23.944054 hw_loss 0.061423 lr 0.00082946 rank 6
2023-02-17 20:31:38,484 DEBUG TRAIN Batch 4/3000 loss 25.256485 loss_att 31.037354 loss_ctc 42.536171 loss_rnnt 21.786724 hw_loss 0.018056 lr 0.00082846 rank 2
2023-02-17 20:31:38,484 DEBUG TRAIN Batch 4/3000 loss 40.941357 loss_att 46.561810 loss_ctc 62.237251 loss_rnnt 36.963440 hw_loss 0.026950 lr 0.00082896 rank 0
2023-02-17 20:31:38,484 DEBUG TRAIN Batch 4/3000 loss 29.890070 loss_att 35.224449 loss_ctc 45.175919 loss_rnnt 26.773430 hw_loss 0.021844 lr 0.00082935 rank 7
2023-02-17 20:31:38,485 DEBUG TRAIN Batch 4/3000 loss 36.476604 loss_att 40.766403 loss_ctc 43.018532 loss_rnnt 34.712479 hw_loss 0.063584 lr 0.00082922 rank 3
2023-02-17 20:32:51,542 DEBUG TRAIN Batch 4/3100 loss 13.238343 loss_att 14.530724 loss_ctc 22.046307 loss_rnnt 11.746517 hw_loss 0.110541 lr 0.00082851 rank 4
2023-02-17 20:32:51,542 DEBUG TRAIN Batch 4/3100 loss 21.754799 loss_att 27.251488 loss_ctc 38.202286 loss_rnnt 18.421888 hw_loss 0.076080 lr 0.00082782 rank 0
2023-02-17 20:32:51,543 DEBUG TRAIN Batch 4/3100 loss 21.357494 loss_att 25.098320 loss_ctc 32.621120 loss_rnnt 19.081762 hw_loss 0.048285 lr 0.00082832 rank 6
2023-02-17 20:32:51,547 DEBUG TRAIN Batch 4/3100 loss 29.017834 loss_att 34.872261 loss_ctc 44.627769 loss_rnnt 25.691654 hw_loss 0.138693 lr 0.00082821 rank 7
2023-02-17 20:32:51,551 DEBUG TRAIN Batch 4/3100 loss 30.820230 loss_att 34.574070 loss_ctc 43.717403 loss_rnnt 28.290737 hw_loss 0.110821 lr 0.00082811 rank 5
2023-02-17 20:32:51,570 DEBUG TRAIN Batch 4/3100 loss 18.741489 loss_att 20.579477 loss_ctc 26.694691 loss_rnnt 17.285999 hw_loss 0.051499 lr 0.00082800 rank 1
2023-02-17 20:32:51,585 DEBUG TRAIN Batch 4/3100 loss 31.158573 loss_att 34.376587 loss_ctc 51.119518 loss_rnnt 27.826513 hw_loss 0.050620 lr 0.00082808 rank 3
2023-02-17 20:32:51,617 DEBUG TRAIN Batch 4/3100 loss 29.549963 loss_att 37.459141 loss_ctc 42.695278 loss_rnnt 26.201557 hw_loss 0.025987 lr 0.00082732 rank 2
2023-02-17 20:34:07,394 DEBUG TRAIN Batch 4/3200 loss 41.511261 loss_att 48.694443 loss_ctc 63.382477 loss_rnnt 37.137352 hw_loss 0.039574 lr 0.00082669 rank 0
2023-02-17 20:34:07,396 DEBUG TRAIN Batch 4/3200 loss 11.919098 loss_att 14.229009 loss_ctc 16.585798 loss_rnnt 10.732310 hw_loss 0.192334 lr 0.00082697 rank 5
2023-02-17 20:34:07,397 DEBUG TRAIN Batch 4/3200 loss 12.572449 loss_att 25.701361 loss_ctc 25.350471 loss_rnnt 8.231903 hw_loss 0.020673 lr 0.00082738 rank 4
2023-02-17 20:34:07,399 DEBUG TRAIN Batch 4/3200 loss 14.881586 loss_att 16.203846 loss_ctc 23.919048 loss_rnnt 13.305601 hw_loss 0.199760 lr 0.00082619 rank 2
2023-02-17 20:34:07,400 DEBUG TRAIN Batch 4/3200 loss 28.395952 loss_att 39.892384 loss_ctc 45.063160 loss_rnnt 23.857868 hw_loss 0.030937 lr 0.00082719 rank 6
2023-02-17 20:34:07,403 DEBUG TRAIN Batch 4/3200 loss 19.086195 loss_att 25.904934 loss_ctc 33.184578 loss_rnnt 15.832740 hw_loss 0.018609 lr 0.00082707 rank 7
2023-02-17 20:34:07,448 DEBUG TRAIN Batch 4/3200 loss 13.407935 loss_att 14.646809 loss_ctc 20.690819 loss_rnnt 12.047099 hw_loss 0.266268 lr 0.00082695 rank 3
2023-02-17 20:34:07,473 DEBUG TRAIN Batch 4/3200 loss 44.975658 loss_att 57.824860 loss_ctc 60.061615 loss_rnnt 40.388836 hw_loss 0.010355 lr 0.00082687 rank 1
2023-02-17 20:35:19,954 DEBUG TRAIN Batch 4/3300 loss 26.803371 loss_att 35.538338 loss_ctc 47.984734 loss_rnnt 22.199799 hw_loss 0.060746 lr 0.00082574 rank 1
2023-02-17 20:35:19,956 DEBUG TRAIN Batch 4/3300 loss 17.347139 loss_att 27.687210 loss_ctc 29.116371 loss_rnnt 13.709759 hw_loss 0.000251 lr 0.00082556 rank 0
2023-02-17 20:35:19,956 DEBUG TRAIN Batch 4/3300 loss 43.130638 loss_att 51.782715 loss_ctc 57.982758 loss_rnnt 39.360840 hw_loss 0.110818 lr 0.00082584 rank 5
2023-02-17 20:35:19,959 DEBUG TRAIN Batch 4/3300 loss 19.808495 loss_att 32.629955 loss_ctc 32.792427 loss_rnnt 15.503517 hw_loss 0.017802 lr 0.00082625 rank 4
2023-02-17 20:35:19,959 DEBUG TRAIN Batch 4/3300 loss 28.973030 loss_att 35.676147 loss_ctc 48.953758 loss_rnnt 24.968130 hw_loss 0.000334 lr 0.00082507 rank 2
2023-02-17 20:35:19,960 DEBUG TRAIN Batch 4/3300 loss 29.045647 loss_att 28.585688 loss_ctc 38.046047 loss_rnnt 27.926605 hw_loss 0.020583 lr 0.00082606 rank 6
2023-02-17 20:35:19,959 DEBUG TRAIN Batch 4/3300 loss 27.750788 loss_att 35.734207 loss_ctc 40.924469 loss_rnnt 24.397503 hw_loss 0.000206 lr 0.00082594 rank 7
2023-02-17 20:35:19,964 DEBUG TRAIN Batch 4/3300 loss 18.162672 loss_att 24.199785 loss_ctc 34.455116 loss_rnnt 14.767988 hw_loss 0.028004 lr 0.00082582 rank 3
2023-02-17 20:36:32,263 DEBUG TRAIN Batch 4/3400 loss 19.065794 loss_att 26.133827 loss_ctc 37.189690 loss_rnnt 15.128157 hw_loss 0.201582 lr 0.00082470 rank 3
2023-02-17 20:36:32,272 DEBUG TRAIN Batch 4/3400 loss 16.293692 loss_att 23.045805 loss_ctc 26.345219 loss_rnnt 13.556954 hw_loss 0.086456 lr 0.00082512 rank 4
2023-02-17 20:36:32,282 DEBUG TRAIN Batch 4/3400 loss 25.434990 loss_att 28.619553 loss_ctc 45.051414 loss_rnnt 22.154461 hw_loss 0.052676 lr 0.00082493 rank 6
2023-02-17 20:36:32,282 DEBUG TRAIN Batch 4/3400 loss 23.674566 loss_att 25.888727 loss_ctc 39.994442 loss_rnnt 20.991385 hw_loss 0.120689 lr 0.00082395 rank 2
2023-02-17 20:36:32,282 DEBUG TRAIN Batch 4/3400 loss 18.603521 loss_att 23.530052 loss_ctc 32.655010 loss_rnnt 15.662975 hw_loss 0.153201 lr 0.00082462 rank 1
2023-02-17 20:36:32,284 DEBUG TRAIN Batch 4/3400 loss 33.817810 loss_att 36.793587 loss_ctc 51.682789 loss_rnnt 30.831280 hw_loss 0.017586 lr 0.00082444 rank 0
2023-02-17 20:36:32,288 DEBUG TRAIN Batch 4/3400 loss 22.230013 loss_att 25.503462 loss_ctc 36.063789 loss_rnnt 19.701460 hw_loss 0.055051 lr 0.00082482 rank 7
2023-02-17 20:36:32,289 DEBUG TRAIN Batch 4/3400 loss 18.264065 loss_att 20.609615 loss_ctc 30.408855 loss_rnnt 16.163563 hw_loss 0.022661 lr 0.00082472 rank 5
2023-02-17 20:37:45,523 DEBUG TRAIN Batch 4/3500 loss 17.928467 loss_att 24.487085 loss_ctc 28.845890 loss_rnnt 15.161003 hw_loss 0.000156 lr 0.00082400 rank 4
2023-02-17 20:37:45,527 DEBUG TRAIN Batch 4/3500 loss 17.007442 loss_att 20.237108 loss_ctc 20.589087 loss_rnnt 15.871491 hw_loss 0.023372 lr 0.00082360 rank 5
2023-02-17 20:37:45,528 DEBUG TRAIN Batch 4/3500 loss 24.930202 loss_att 29.479967 loss_ctc 37.808056 loss_rnnt 22.290745 hw_loss 0.023355 lr 0.00082350 rank 1
2023-02-17 20:37:45,529 DEBUG TRAIN Batch 4/3500 loss 21.321537 loss_att 27.769562 loss_ctc 36.960114 loss_rnnt 17.931828 hw_loss 0.028052 lr 0.00082332 rank 0
2023-02-17 20:37:45,529 DEBUG TRAIN Batch 4/3500 loss 29.131042 loss_att 30.355639 loss_ctc 42.686092 loss_rnnt 27.059010 hw_loss 0.037068 lr 0.00082358 rank 3
2023-02-17 20:37:45,530 DEBUG TRAIN Batch 4/3500 loss 34.149323 loss_att 37.382496 loss_ctc 47.754326 loss_rnnt 31.631918 hw_loss 0.106441 lr 0.00082381 rank 6
2023-02-17 20:37:45,531 DEBUG TRAIN Batch 4/3500 loss 12.662301 loss_att 17.346855 loss_ctc 24.935970 loss_rnnt 10.048309 hw_loss 0.076111 lr 0.00082370 rank 7
2023-02-17 20:37:45,532 DEBUG TRAIN Batch 4/3500 loss 15.884057 loss_att 22.624817 loss_ctc 33.701340 loss_rnnt 12.132694 hw_loss 0.051700 lr 0.00082283 rank 2
2023-02-17 20:38:59,194 DEBUG TRAIN Batch 4/3600 loss 23.096617 loss_att 29.612938 loss_ctc 30.416977 loss_rnnt 20.780148 hw_loss 0.069669 lr 0.00082246 rank 3
2023-02-17 20:38:59,200 DEBUG TRAIN Batch 4/3600 loss 23.182310 loss_att 28.037886 loss_ctc 39.342045 loss_rnnt 20.056419 hw_loss 0.000274 lr 0.00082289 rank 4
2023-02-17 20:38:59,207 DEBUG TRAIN Batch 4/3600 loss 25.864971 loss_att 26.508873 loss_ctc 39.920391 loss_rnnt 23.862026 hw_loss 0.000203 lr 0.00082238 rank 1
2023-02-17 20:38:59,209 DEBUG TRAIN Batch 4/3600 loss 16.127502 loss_att 21.605446 loss_ctc 27.778797 loss_rnnt 13.474276 hw_loss 0.007742 lr 0.00082221 rank 0
2023-02-17 20:38:59,210 DEBUG TRAIN Batch 4/3600 loss 18.054224 loss_att 23.548302 loss_ctc 27.452335 loss_rnnt 15.697380 hw_loss 0.009274 lr 0.00082172 rank 2
2023-02-17 20:38:59,212 DEBUG TRAIN Batch 4/3600 loss 28.183634 loss_att 35.145370 loss_ctc 45.363499 loss_rnnt 24.498020 hw_loss 0.004909 lr 0.00082270 rank 6
2023-02-17 20:38:59,211 DEBUG TRAIN Batch 4/3600 loss 27.168232 loss_att 38.022659 loss_ctc 47.657646 loss_rnnt 22.216312 hw_loss 0.092083 lr 0.00082248 rank 5
2023-02-17 20:38:59,211 DEBUG TRAIN Batch 4/3600 loss 23.635540 loss_att 31.420324 loss_ctc 40.308304 loss_rnnt 19.840073 hw_loss 0.029018 lr 0.00082258 rank 7
2023-02-17 20:40:12,541 DEBUG TRAIN Batch 4/3700 loss 25.509073 loss_att 32.143475 loss_ctc 40.562218 loss_rnnt 22.138350 hw_loss 0.068919 lr 0.00082158 rank 6
2023-02-17 20:40:12,542 DEBUG TRAIN Batch 4/3700 loss 25.237455 loss_att 29.593828 loss_ctc 38.844513 loss_rnnt 22.551817 hw_loss 0.000167 lr 0.00082147 rank 7
2023-02-17 20:40:12,543 DEBUG TRAIN Batch 4/3700 loss 25.776989 loss_att 29.920778 loss_ctc 37.373363 loss_rnnt 23.352303 hw_loss 0.093270 lr 0.00082110 rank 0
2023-02-17 20:40:12,545 DEBUG TRAIN Batch 4/3700 loss 17.676889 loss_att 20.211252 loss_ctc 26.741425 loss_rnnt 15.924314 hw_loss 0.069558 lr 0.00082177 rank 4
2023-02-17 20:40:12,545 DEBUG TRAIN Batch 4/3700 loss 12.014253 loss_att 15.472744 loss_ctc 20.120935 loss_rnnt 10.190375 hw_loss 0.096166 lr 0.00082127 rank 1
2023-02-17 20:40:12,546 DEBUG TRAIN Batch 4/3700 loss 27.118565 loss_att 35.991001 loss_ctc 46.678883 loss_rnnt 22.717979 hw_loss 0.033857 lr 0.00082137 rank 5
2023-02-17 20:40:12,546 DEBUG TRAIN Batch 4/3700 loss 21.461332 loss_att 24.410297 loss_ctc 36.047123 loss_rnnt 18.855864 hw_loss 0.132945 lr 0.00082135 rank 3
2023-02-17 20:40:12,549 DEBUG TRAIN Batch 4/3700 loss 16.458391 loss_att 23.702852 loss_ctc 24.649986 loss_rnnt 13.917142 hw_loss 0.000273 lr 0.00082061 rank 2
2023-02-17 20:41:25,907 DEBUG TRAIN Batch 4/3800 loss 31.641726 loss_att 42.019291 loss_ctc 39.897026 loss_rnnt 28.447174 hw_loss 0.034369 lr 0.00082067 rank 4
2023-02-17 20:41:25,908 DEBUG TRAIN Batch 4/3800 loss 21.754564 loss_att 25.064915 loss_ctc 34.350525 loss_rnnt 19.361980 hw_loss 0.095726 lr 0.00082048 rank 6
2023-02-17 20:41:25,912 DEBUG TRAIN Batch 4/3800 loss 21.357779 loss_att 24.262678 loss_ctc 33.447998 loss_rnnt 19.113171 hw_loss 0.096743 lr 0.00082027 rank 5
2023-02-17 20:41:25,915 DEBUG TRAIN Batch 4/3800 loss 18.364618 loss_att 29.368267 loss_ctc 39.638035 loss_rnnt 13.303535 hw_loss 0.044810 lr 0.00082037 rank 7
2023-02-17 20:41:25,917 DEBUG TRAIN Batch 4/3800 loss 15.986155 loss_att 19.814770 loss_ctc 26.225904 loss_rnnt 13.807028 hw_loss 0.090195 lr 0.00081999 rank 0
2023-02-17 20:41:25,918 DEBUG TRAIN Batch 4/3800 loss 20.642345 loss_att 21.347395 loss_ctc 30.309429 loss_rnnt 19.161156 hw_loss 0.096065 lr 0.00081951 rank 2
2023-02-17 20:41:25,919 DEBUG TRAIN Batch 4/3800 loss 34.273872 loss_att 42.675156 loss_ctc 56.510460 loss_rnnt 29.628632 hw_loss 0.000194 lr 0.00082017 rank 1
2023-02-17 20:41:25,937 DEBUG TRAIN Batch 4/3800 loss 17.368124 loss_att 17.379055 loss_ctc 20.973997 loss_rnnt 16.843115 hw_loss 0.078828 lr 0.00082025 rank 3
2023-02-17 20:42:40,490 DEBUG TRAIN Batch 4/3900 loss 4.196625 loss_att 10.678860 loss_ctc 11.551069 loss_rnnt 1.901943 hw_loss 0.033079 lr 0.00081937 rank 6
2023-02-17 20:42:40,490 DEBUG TRAIN Batch 4/3900 loss 15.748460 loss_att 20.689070 loss_ctc 29.395908 loss_rnnt 12.940451 hw_loss 0.000426 lr 0.00081956 rank 4
2023-02-17 20:42:40,493 DEBUG TRAIN Batch 4/3900 loss 25.739059 loss_att 27.802469 loss_ctc 38.219849 loss_rnnt 23.651970 hw_loss 0.019319 lr 0.00081907 rank 1
2023-02-17 20:42:40,494 DEBUG TRAIN Batch 4/3900 loss 16.557270 loss_att 24.675457 loss_ctc 27.370861 loss_rnnt 13.467628 hw_loss 0.045358 lr 0.00081841 rank 2
2023-02-17 20:42:40,495 DEBUG TRAIN Batch 4/3900 loss 12.535978 loss_att 21.488504 loss_ctc 19.147089 loss_rnnt 9.857610 hw_loss 0.011965 lr 0.00081889 rank 0
2023-02-17 20:42:40,498 DEBUG TRAIN Batch 4/3900 loss 36.379627 loss_att 40.279053 loss_ctc 50.799576 loss_rnnt 33.662228 hw_loss 0.027860 lr 0.00081914 rank 3
2023-02-17 20:42:40,499 DEBUG TRAIN Batch 4/3900 loss 22.350658 loss_att 22.054384 loss_ctc 34.205677 loss_rnnt 20.795010 hw_loss 0.064191 lr 0.00081926 rank 7
2023-02-17 20:42:40,546 DEBUG TRAIN Batch 4/3900 loss 30.725817 loss_att 34.102638 loss_ctc 37.942474 loss_rnnt 29.088087 hw_loss 0.000272 lr 0.00081917 rank 5
2023-02-17 20:43:53,580 DEBUG TRAIN Batch 4/4000 loss 22.254650 loss_att 28.846079 loss_ctc 31.161770 loss_rnnt 19.715874 hw_loss 0.061642 lr 0.00081817 rank 7
2023-02-17 20:43:53,580 DEBUG TRAIN Batch 4/4000 loss 32.192986 loss_att 38.268547 loss_ctc 42.394924 loss_rnnt 29.537235 hw_loss 0.150710 lr 0.00081731 rank 2
2023-02-17 20:43:53,597 DEBUG TRAIN Batch 4/4000 loss 16.481447 loss_att 24.468210 loss_ctc 22.167679 loss_rnnt 14.085245 hw_loss 0.076285 lr 0.00081780 rank 0
2023-02-17 20:43:53,597 DEBUG TRAIN Batch 4/4000 loss 16.567940 loss_att 20.780842 loss_ctc 25.368391 loss_rnnt 14.508856 hw_loss 0.080833 lr 0.00081846 rank 4
2023-02-17 20:43:53,597 DEBUG TRAIN Batch 4/4000 loss 30.135965 loss_att 35.187416 loss_ctc 45.518547 loss_rnnt 27.055271 hw_loss 0.036357 lr 0.00081828 rank 6
2023-02-17 20:43:53,602 DEBUG TRAIN Batch 4/4000 loss 19.804342 loss_att 25.667616 loss_ctc 30.496315 loss_rnnt 17.190147 hw_loss 0.029894 lr 0.00081805 rank 3
2023-02-17 20:43:53,603 DEBUG TRAIN Batch 4/4000 loss 19.074331 loss_att 23.186464 loss_ctc 32.689697 loss_rnnt 16.422535 hw_loss 0.026227 lr 0.00081807 rank 5
2023-02-17 20:43:53,644 DEBUG TRAIN Batch 4/4000 loss 13.005863 loss_att 17.526306 loss_ctc 16.448345 loss_rnnt 11.629153 hw_loss 0.025545 lr 0.00081797 rank 1
2023-02-17 20:45:05,705 DEBUG TRAIN Batch 4/4100 loss 11.814286 loss_att 16.596319 loss_ctc 17.069921 loss_rnnt 10.116306 hw_loss 0.076540 lr 0.00081737 rank 4
2023-02-17 20:45:05,706 DEBUG TRAIN Batch 4/4100 loss 26.304787 loss_att 34.829380 loss_ctc 46.705673 loss_rnnt 21.879683 hw_loss 0.000126 lr 0.00081718 rank 6
2023-02-17 20:45:05,710 DEBUG TRAIN Batch 4/4100 loss 23.428297 loss_att 31.484470 loss_ctc 40.445545 loss_rnnt 19.548038 hw_loss 0.000109 lr 0.00081688 rank 1
2023-02-17 20:45:05,711 DEBUG TRAIN Batch 4/4100 loss 34.589546 loss_att 35.271500 loss_ctc 55.949406 loss_rnnt 31.605110 hw_loss 0.000127 lr 0.00081670 rank 0
2023-02-17 20:45:05,711 DEBUG TRAIN Batch 4/4100 loss 8.492739 loss_att 12.236442 loss_ctc 15.478102 loss_rnnt 6.783473 hw_loss 0.054645 lr 0.00081698 rank 5
2023-02-17 20:45:05,714 DEBUG TRAIN Batch 4/4100 loss 30.767685 loss_att 36.644531 loss_ctc 54.128647 loss_rnnt 26.477215 hw_loss 0.000577 lr 0.00081622 rank 2
2023-02-17 20:45:05,716 DEBUG TRAIN Batch 4/4100 loss 28.313137 loss_att 41.208744 loss_ctc 50.457737 loss_rnnt 22.739357 hw_loss 0.078831 lr 0.00081695 rank 3
2023-02-17 20:45:05,721 DEBUG TRAIN Batch 4/4100 loss 22.009956 loss_att 29.505930 loss_ctc 36.224945 loss_rnnt 18.594160 hw_loss 0.039883 lr 0.00081707 rank 7
