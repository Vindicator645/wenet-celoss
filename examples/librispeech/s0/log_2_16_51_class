/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_16_rnnt_bias_both.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_16_rnnt_bias_loss_51_class_both/ddp_init
2023-02-17 02:33:11,169 INFO training on multiple gpus, this gpu 3
2023-02-17 02:33:11,171 INFO training on multiple gpus, this gpu 2
2023-02-17 02:33:11,174 INFO training on multiple gpus, this gpu 5
2023-02-17 02:33:11,175 INFO training on multiple gpus, this gpu 4
2023-02-17 02:33:11,175 INFO training on multiple gpus, this gpu 0
2023-02-17 02:33:11,175 INFO training on multiple gpus, this gpu 1
2023-02-17 02:33:11,185 INFO training on multiple gpus, this gpu 6
2023-02-17 02:33:11,223 INFO training on multiple gpus, this gpu 7
2023-02-17 02:33:11,497 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-17 02:33:11,530 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-17 02:33:12,015 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-17 02:33:12,022 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-17 02:33:12,034 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-17 02:33:12,039 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-17 02:33:12,040 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-17 02:33:12,042 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-17 02:33:12,043 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,044 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,045 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,047 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,048 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,050 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,051 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:12,053 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 02:33:30,859 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,861 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:30,886 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,888 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:30,892 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,894 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:30,903 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,905 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,905 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:33:30,906 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,907 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:30,908 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:30,938 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/init.pt
2023-02-17 02:33:30,962 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:30,964 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:33:31,967 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 02:33:31,971 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=51, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58380379
2023-02-17 02:34:46,361 DEBUG TRAIN Batch 0/0 loss 510.899994 loss_att 72.905106 loss_ctc 503.576538 loss_rnnt 597.154358 hw_loss 4.351941 lr 0.00000004 rank 3
2023-02-17 02:34:46,381 DEBUG TRAIN Batch 0/0 loss 479.168884 loss_att 70.787781 loss_ctc 474.788971 loss_rnnt 559.246704 hw_loss 4.091963 lr 0.00000004 rank 7
2023-02-17 02:34:46,382 DEBUG TRAIN Batch 0/0 loss 553.468018 loss_att 86.315926 loss_ctc 548.431213 loss_rnnt 645.152222 hw_loss 4.533417 lr 0.00000004 rank 6
2023-02-17 02:34:46,420 DEBUG TRAIN Batch 0/0 loss 514.972473 loss_att 77.436935 loss_ctc 508.125916 loss_rnnt 601.018738 hw_loss 4.450759 lr 0.00000004 rank 0
2023-02-17 02:34:46,432 DEBUG TRAIN Batch 0/0 loss 514.630920 loss_att 74.934761 loss_ctc 514.206543 loss_rnnt 600.276550 hw_loss 4.406519 lr 0.00000004 rank 5
2023-02-17 02:34:46,439 DEBUG TRAIN Batch 0/0 loss 530.127014 loss_att 71.472931 loss_ctc 552.739075 loss_rnnt 616.279602 hw_loss 4.806012 lr 0.00000004 rank 1
2023-02-17 02:34:46,439 DEBUG TRAIN Batch 0/0 loss 536.631775 loss_att 74.633781 loss_ctc 544.287598 loss_rnnt 625.514526 hw_loss 4.680191 lr 0.00000004 rank 4
2023-02-17 02:34:46,558 DEBUG TRAIN Batch 0/0 loss 559.355652 loss_att 81.403610 loss_ctc 552.894653 loss_rnnt 653.445618 hw_loss 4.428439 lr 0.00000004 rank 2
2023-02-17 02:36:04,067 DEBUG TRAIN Batch 0/100 loss 2260.832520 loss_att 362.959900 loss_ctc 3155.251709 loss_rnnt 2519.161133 hw_loss 3.731255 lr 0.00000404 rank 6
2023-02-17 02:36:04,073 DEBUG TRAIN Batch 0/100 loss 2164.510742 loss_att 390.857361 loss_ctc 3012.296387 loss_rnnt 2404.200684 hw_loss 3.755076 lr 0.00000404 rank 1
2023-02-17 02:36:04,073 DEBUG TRAIN Batch 0/100 loss 2126.463867 loss_att 374.761536 loss_ctc 3167.307373 loss_rnnt 2336.067871 hw_loss 3.669693 lr 0.00000404 rank 3
2023-02-17 02:36:04,074 DEBUG TRAIN Batch 0/100 loss 2189.587646 loss_att 371.344513 loss_ctc 3001.743652 loss_rnnt 2443.075684 hw_loss 3.511769 lr 0.00000404 rank 5
2023-02-17 02:36:04,075 DEBUG TRAIN Batch 0/100 loss 2203.859131 loss_att 379.527771 loss_ctc 3105.882324 loss_rnnt 2446.553711 hw_loss 3.566440 lr 0.00000404 rank 7
2023-02-17 02:36:04,076 DEBUG TRAIN Batch 0/100 loss 2232.277344 loss_att 365.429932 loss_ctc 3184.789062 loss_rnnt 2476.669678 hw_loss 3.704276 lr 0.00000404 rank 4
2023-02-17 02:36:04,077 DEBUG TRAIN Batch 0/100 loss 2185.042236 loss_att 372.432892 loss_ctc 2949.168945 loss_rnnt 2443.525146 hw_loss 4.041415 lr 0.00000404 rank 2
2023-02-17 02:36:04,078 DEBUG TRAIN Batch 0/100 loss 2212.055908 loss_att 394.162598 loss_ctc 3110.625488 loss_rnnt 2453.755371 hw_loss 3.880460 lr 0.00000404 rank 0
2023-02-17 02:37:20,035 DEBUG TRAIN Batch 0/200 loss 873.925415 loss_att 388.924255 loss_ctc 2882.657959 loss_rnnt 702.381531 hw_loss 1.337057 lr 0.00000804 rank 3
2023-02-17 02:37:20,040 DEBUG TRAIN Batch 0/200 loss 842.379272 loss_att 397.465637 loss_ctc 2737.317139 loss_rnnt 677.998291 hw_loss 1.322346 lr 0.00000804 rank 7
2023-02-17 02:37:20,041 DEBUG TRAIN Batch 0/200 loss 830.429443 loss_att 319.558990 loss_ctc 2835.247803 loss_rnnt 664.613220 hw_loss 1.277277 lr 0.00000804 rank 1
2023-02-17 02:37:20,041 DEBUG TRAIN Batch 0/200 loss 897.250610 loss_att 410.501556 loss_ctc 2526.676758 loss_rnnt 776.694458 hw_loss 1.217060 lr 0.00000804 rank 6
2023-02-17 02:37:20,042 DEBUG TRAIN Batch 0/200 loss 839.478333 loss_att 412.259033 loss_ctc 2743.821045 loss_rnnt 670.300598 hw_loss 1.329863 lr 0.00000804 rank 5
2023-02-17 02:37:20,043 DEBUG TRAIN Batch 0/200 loss 813.176392 loss_att 358.027679 loss_ctc 2688.022949 loss_rnnt 653.510376 hw_loss 1.342977 lr 0.00000804 rank 4
2023-02-17 02:37:20,046 DEBUG TRAIN Batch 0/200 loss 838.227966 loss_att 402.725403 loss_ctc 2501.405273 loss_rnnt 702.937256 hw_loss 1.189096 lr 0.00000804 rank 2
2023-02-17 02:37:20,050 DEBUG TRAIN Batch 0/200 loss 850.623779 loss_att 369.392670 loss_ctc 2807.072754 loss_rnnt 685.352417 hw_loss 1.233367 lr 0.00000804 rank 0
2023-02-17 02:38:35,957 DEBUG TRAIN Batch 0/300 loss 435.221741 loss_att 319.710876 loss_ctc 1299.020020 loss_rnnt 342.888489 hw_loss 0.491794 lr 0.00001204 rank 7
2023-02-17 02:38:35,957 DEBUG TRAIN Batch 0/300 loss 430.198242 loss_att 327.026337 loss_ctc 1228.195312 loss_rnnt 344.228546 hw_loss 0.383333 lr 0.00001204 rank 6
2023-02-17 02:38:35,962 DEBUG TRAIN Batch 0/300 loss 500.227753 loss_att 399.747528 loss_ctc 1276.715820 loss_rnnt 416.490051 hw_loss 0.566269 lr 0.00001204 rank 0
2023-02-17 02:38:35,964 DEBUG TRAIN Batch 0/300 loss 532.447510 loss_att 341.531342 loss_ctc 2078.325684 loss_rnnt 364.234924 hw_loss 0.522614 lr 0.00001204 rank 4
2023-02-17 02:38:35,967 DEBUG TRAIN Batch 0/300 loss 425.510284 loss_att 329.909607 loss_ctc 1107.198242 loss_rnnt 353.449036 hw_loss 0.543053 lr 0.00001204 rank 2
2023-02-17 02:38:35,972 DEBUG TRAIN Batch 0/300 loss 541.489807 loss_att 435.800232 loss_ctc 1332.301147 loss_rnnt 456.944946 hw_loss 0.452212 lr 0.00001204 rank 3
2023-02-17 02:38:35,989 DEBUG TRAIN Batch 0/300 loss 465.376434 loss_att 332.751678 loss_ctc 1508.113403 loss_rnnt 352.572296 hw_loss 0.557788 lr 0.00001204 rank 5
2023-02-17 02:38:35,995 DEBUG TRAIN Batch 0/300 loss 430.942963 loss_att 332.555878 loss_ctc 1226.399536 loss_rnnt 344.305542 hw_loss 0.476163 lr 0.00001204 rank 1
2023-02-17 02:39:53,621 DEBUG TRAIN Batch 0/400 loss 334.123779 loss_att 284.830078 loss_ctc 603.590759 loss_rnnt 307.806091 hw_loss 0.463996 lr 0.00001604 rank 6
2023-02-17 02:39:53,622 DEBUG TRAIN Batch 0/400 loss 334.984070 loss_att 278.830505 loss_ctc 680.467346 loss_rnnt 299.907227 hw_loss 0.455786 lr 0.00001604 rank 7
2023-02-17 02:39:53,625 DEBUG TRAIN Batch 0/400 loss 329.192963 loss_att 275.178101 loss_ctc 694.607666 loss_rnnt 291.027649 hw_loss 0.461775 lr 0.00001604 rank 3
2023-02-17 02:39:53,625 DEBUG TRAIN Batch 0/400 loss 310.182343 loss_att 257.534515 loss_ctc 674.868103 loss_rnnt 271.821320 hw_loss 0.498366 lr 0.00001604 rank 0
2023-02-17 02:39:53,626 DEBUG TRAIN Batch 0/400 loss 416.654419 loss_att 354.738525 loss_ctc 773.523071 loss_rnnt 381.273560 hw_loss 0.340340 lr 0.00001604 rank 1
2023-02-17 02:39:53,626 DEBUG TRAIN Batch 0/400 loss 381.703918 loss_att 312.502258 loss_ctc 827.650513 loss_rnnt 335.858154 hw_loss 0.424827 lr 0.00001604 rank 5
2023-02-17 02:39:53,627 DEBUG TRAIN Batch 0/400 loss 342.278351 loss_att 284.944031 loss_ctc 703.338440 loss_rnnt 305.379089 hw_loss 0.421483 lr 0.00001604 rank 2
2023-02-17 02:39:53,628 DEBUG TRAIN Batch 0/400 loss 387.292023 loss_att 332.388794 loss_ctc 654.983398 loss_rnnt 362.341309 hw_loss 0.448402 lr 0.00001604 rank 4
2023-02-17 02:41:09,551 DEBUG TRAIN Batch 0/500 loss 368.379059 loss_att 300.104248 loss_ctc 739.862793 loss_rnnt 332.332275 hw_loss 0.319926 lr 0.00002004 rank 6
2023-02-17 02:41:09,553 DEBUG TRAIN Batch 0/500 loss 276.377441 loss_att 211.867096 loss_ctc 665.495361 loss_rnnt 237.142197 hw_loss 0.477964 lr 0.00002004 rank 0
2023-02-17 02:41:09,553 DEBUG TRAIN Batch 0/500 loss 301.798859 loss_att 254.818924 loss_ctc 461.974426 loss_rnnt 289.538818 hw_loss 0.561117 lr 0.00002004 rank 7
2023-02-17 02:41:09,554 DEBUG TRAIN Batch 0/500 loss 302.765961 loss_att 219.696198 loss_ctc 857.415833 loss_rnnt 245.184235 hw_loss 0.454428 lr 0.00002004 rank 1
2023-02-17 02:41:09,555 DEBUG TRAIN Batch 0/500 loss 328.114685 loss_att 219.107880 loss_ctc 1138.873779 loss_rnnt 241.601837 hw_loss 0.399291 lr 0.00002004 rank 3
2023-02-17 02:41:09,559 DEBUG TRAIN Batch 0/500 loss 322.231598 loss_att 275.474304 loss_ctc 494.628021 loss_rnnt 308.379272 hw_loss 0.408038 lr 0.00002004 rank 5
2023-02-17 02:41:09,560 DEBUG TRAIN Batch 0/500 loss 271.902039 loss_att 218.041214 loss_ctc 532.236694 loss_rnnt 247.713318 hw_loss 0.467975 lr 0.00002004 rank 4
2023-02-17 02:41:09,602 DEBUG TRAIN Batch 0/500 loss 332.530823 loss_att 255.711975 loss_ctc 817.897217 loss_rnnt 282.963989 hw_loss 0.403267 lr 0.00002004 rank 2
2023-02-17 02:42:25,964 DEBUG TRAIN Batch 0/600 loss 274.593414 loss_att 184.746918 loss_ctc 880.498474 loss_rnnt 211.509079 hw_loss 0.499272 lr 0.00002404 rank 5
2023-02-17 02:42:25,965 DEBUG TRAIN Batch 0/600 loss 257.618896 loss_att 198.760132 loss_ctc 562.585754 loss_rnnt 228.428879 hw_loss 0.561586 lr 0.00002404 rank 6
2023-02-17 02:42:25,967 DEBUG TRAIN Batch 0/600 loss 174.184662 loss_att 119.652176 loss_ctc 538.379578 loss_rnnt 136.255447 hw_loss 0.518238 lr 0.00002404 rank 1
2023-02-17 02:42:25,967 DEBUG TRAIN Batch 0/600 loss 168.411499 loss_att 125.526474 loss_ctc 419.451660 loss_rnnt 143.176956 hw_loss 0.636611 lr 0.00002404 rank 3
2023-02-17 02:42:25,968 DEBUG TRAIN Batch 0/600 loss 126.898399 loss_att 88.974861 loss_ctc 377.917542 loss_rnnt 100.712593 hw_loss 0.564912 lr 0.00002404 rank 7
2023-02-17 02:42:25,970 DEBUG TRAIN Batch 0/600 loss 182.433319 loss_att 128.905975 loss_ctc 525.567322 loss_rnnt 147.130035 hw_loss 0.482919 lr 0.00002404 rank 2
2023-02-17 02:42:25,971 DEBUG TRAIN Batch 0/600 loss 189.652100 loss_att 139.359451 loss_ctc 506.589142 loss_rnnt 157.141739 hw_loss 0.582404 lr 0.00002404 rank 4
2023-02-17 02:42:25,972 DEBUG TRAIN Batch 0/600 loss 232.779404 loss_att 179.110931 loss_ctc 515.215454 loss_rnnt 205.561737 hw_loss 0.549824 lr 0.00002404 rank 0
2023-02-17 02:43:45,679 DEBUG TRAIN Batch 0/700 loss 349.756561 loss_att 308.002167 loss_ctc 364.984741 loss_rnnt 355.881195 hw_loss 0.367195 lr 0.00002804 rank 2
2023-02-17 02:43:45,680 DEBUG TRAIN Batch 0/700 loss 336.708923 loss_att 296.182617 loss_ctc 347.582336 loss_rnnt 343.167084 hw_loss 0.369978 lr 0.00002804 rank 1
2023-02-17 02:43:45,682 DEBUG TRAIN Batch 0/700 loss 313.659241 loss_att 276.883484 loss_ctc 326.168640 loss_rnnt 319.098633 hw_loss 0.464673 lr 0.00002804 rank 3
2023-02-17 02:43:45,684 DEBUG TRAIN Batch 0/700 loss 357.922943 loss_att 316.715637 loss_ctc 374.961182 loss_rnnt 363.677063 hw_loss 0.404145 lr 0.00002804 rank 7
2023-02-17 02:43:45,686 DEBUG TRAIN Batch 0/700 loss 327.799500 loss_att 288.307800 loss_ctc 340.142517 loss_rnnt 333.810791 hw_loss 0.452397 lr 0.00002804 rank 0
2023-02-17 02:43:45,687 DEBUG TRAIN Batch 0/700 loss 344.981476 loss_att 306.332214 loss_ctc 360.658447 loss_rnnt 350.468964 hw_loss 0.285187 lr 0.00002804 rank 4
2023-02-17 02:43:45,707 DEBUG TRAIN Batch 0/700 loss 400.362183 loss_att 352.591431 loss_ctc 420.450623 loss_rnnt 407.068054 hw_loss 0.318468 lr 0.00002804 rank 6
2023-02-17 02:43:45,711 DEBUG TRAIN Batch 0/700 loss 376.554138 loss_att 330.460876 loss_ctc 390.544189 loss_rnnt 383.676361 hw_loss 0.433313 lr 0.00002804 rank 5
2023-02-17 02:45:02,561 DEBUG TRAIN Batch 0/800 loss 298.272583 loss_att 262.219055 loss_ctc 305.979431 loss_rnnt 304.236694 hw_loss 0.410686 lr 0.00003204 rank 1
2023-02-17 02:45:02,565 DEBUG TRAIN Batch 0/800 loss 324.655396 loss_att 285.066589 loss_ctc 335.012390 loss_rnnt 330.964935 hw_loss 0.426209 lr 0.00003204 rank 7
2023-02-17 02:45:02,566 DEBUG TRAIN Batch 0/800 loss 388.306671 loss_att 341.194519 loss_ctc 397.858734 loss_rnnt 396.246460 hw_loss 0.391965 lr 0.00003204 rank 4
2023-02-17 02:45:02,566 DEBUG TRAIN Batch 0/800 loss 275.327454 loss_att 241.831604 loss_ctc 284.642212 loss_rnnt 280.597595 hw_loss 0.350733 lr 0.00003204 rank 6
2023-02-17 02:45:02,569 DEBUG TRAIN Batch 0/800 loss 377.541626 loss_att 332.123566 loss_ctc 390.157776 loss_rnnt 384.711121 hw_loss 0.434924 lr 0.00003204 rank 5
2023-02-17 02:45:02,571 DEBUG TRAIN Batch 0/800 loss 376.284515 loss_att 329.282623 loss_ctc 387.978210 loss_rnnt 383.908691 hw_loss 0.406924 lr 0.00003204 rank 0
2023-02-17 02:45:02,572 DEBUG TRAIN Batch 0/800 loss 307.259186 loss_att 271.570251 loss_ctc 317.598480 loss_rnnt 312.786926 hw_loss 0.434036 lr 0.00003204 rank 2
2023-02-17 02:45:02,612 DEBUG TRAIN Batch 0/800 loss 347.484802 loss_att 307.969635 loss_ctc 356.104980 loss_rnnt 354.037201 hw_loss 0.377425 lr 0.00003204 rank 3
2023-02-17 02:46:19,444 DEBUG TRAIN Batch 0/900 loss 283.450134 loss_att 250.107086 loss_ctc 294.285614 loss_rnnt 288.474060 hw_loss 0.374921 lr 0.00003604 rank 1
2023-02-17 02:46:19,449 DEBUG TRAIN Batch 0/900 loss 281.035828 loss_att 245.040741 loss_ctc 289.546387 loss_rnnt 286.875214 hw_loss 0.421670 lr 0.00003604 rank 6
2023-02-17 02:46:19,451 DEBUG TRAIN Batch 0/900 loss 354.597351 loss_att 310.189087 loss_ctc 367.456360 loss_rnnt 361.539673 hw_loss 0.421457 lr 0.00003604 rank 0
2023-02-17 02:46:19,453 DEBUG TRAIN Batch 0/900 loss 295.753601 loss_att 260.764771 loss_ctc 307.359772 loss_rnnt 300.939270 hw_loss 0.496134 lr 0.00003604 rank 2
2023-02-17 02:46:19,455 DEBUG TRAIN Batch 0/900 loss 350.044678 loss_att 310.822388 loss_ctc 362.573303 loss_rnnt 356.033691 hw_loss 0.346855 lr 0.00003604 rank 7
2023-02-17 02:46:19,455 DEBUG TRAIN Batch 0/900 loss 318.533905 loss_att 280.630188 loss_ctc 327.220398 loss_rnnt 324.788879 hw_loss 0.314227 lr 0.00003604 rank 4
2023-02-17 02:46:19,455 DEBUG TRAIN Batch 0/900 loss 251.887665 loss_att 221.496185 loss_ctc 259.767731 loss_rnnt 256.662659 hw_loss 0.473664 lr 0.00003604 rank 3
2023-02-17 02:46:19,460 DEBUG TRAIN Batch 0/900 loss 381.569916 loss_att 335.446198 loss_ctc 393.013611 loss_rnnt 389.085510 hw_loss 0.343670 lr 0.00003604 rank 5
2023-02-17 02:47:37,627 DEBUG TRAIN Batch 0/1000 loss 322.502106 loss_att 284.778259 loss_ctc 332.688171 loss_rnnt 328.484192 hw_loss 0.383516 lr 0.00004004 rank 6
2023-02-17 02:47:37,633 DEBUG TRAIN Batch 0/1000 loss 347.655182 loss_att 307.119873 loss_ctc 359.520935 loss_rnnt 353.974121 hw_loss 0.386264 lr 0.00004004 rank 5
2023-02-17 02:47:37,637 DEBUG TRAIN Batch 0/1000 loss 308.381500 loss_att 273.441559 loss_ctc 322.351624 loss_rnnt 313.277466 hw_loss 0.429962 lr 0.00004004 rank 1
2023-02-17 02:47:37,637 DEBUG TRAIN Batch 0/1000 loss 295.732269 loss_att 260.448547 loss_ctc 305.904663 loss_rnnt 301.198364 hw_loss 0.439401 lr 0.00004004 rank 7
2023-02-17 02:47:37,638 DEBUG TRAIN Batch 0/1000 loss 323.672974 loss_att 283.137756 loss_ctc 340.234894 loss_rnnt 329.387756 hw_loss 0.345030 lr 0.00004004 rank 4
2023-02-17 02:47:37,641 DEBUG TRAIN Batch 0/1000 loss 295.854797 loss_att 261.866119 loss_ctc 306.330414 loss_rnnt 301.037659 hw_loss 0.408999 lr 0.00004004 rank 0
2023-02-17 02:47:37,642 DEBUG TRAIN Batch 0/1000 loss 327.219604 loss_att 288.162720 loss_ctc 339.821381 loss_rnnt 333.100281 hw_loss 0.469699 lr 0.00004004 rank 3
2023-02-17 02:47:37,643 DEBUG TRAIN Batch 0/1000 loss 288.760590 loss_att 255.015793 loss_ctc 300.370148 loss_rnnt 293.732971 hw_loss 0.428688 lr 0.00004004 rank 2
2023-02-17 02:48:56,784 DEBUG TRAIN Batch 0/1100 loss 249.466248 loss_att 221.587616 loss_ctc 260.021942 loss_rnnt 253.455368 hw_loss 0.335947 lr 0.00004404 rank 3
2023-02-17 02:48:56,785 DEBUG TRAIN Batch 0/1100 loss 273.218567 loss_att 241.182709 loss_ctc 284.193176 loss_rnnt 277.918457 hw_loss 0.457510 lr 0.00004404 rank 6
2023-02-17 02:48:56,788 DEBUG TRAIN Batch 0/1100 loss 281.616760 loss_att 247.422607 loss_ctc 293.972046 loss_rnnt 286.568054 hw_loss 0.450307 lr 0.00004404 rank 7
2023-02-17 02:48:56,788 DEBUG TRAIN Batch 0/1100 loss 295.720276 loss_att 260.448914 loss_ctc 304.992737 loss_rnnt 301.324463 hw_loss 0.400872 lr 0.00004404 rank 5
2023-02-17 02:48:56,789 DEBUG TRAIN Batch 0/1100 loss 227.633392 loss_att 200.371094 loss_ctc 237.827240 loss_rnnt 231.494171 hw_loss 0.435890 lr 0.00004404 rank 0
2023-02-17 02:48:56,790 DEBUG TRAIN Batch 0/1100 loss 211.097000 loss_att 184.625717 loss_ctc 220.815674 loss_rnnt 214.849564 hw_loss 0.460969 lr 0.00004404 rank 1
2023-02-17 02:48:56,791 DEBUG TRAIN Batch 0/1100 loss 241.255600 loss_att 212.807724 loss_ctc 251.941467 loss_rnnt 245.268875 hw_loss 0.471573 lr 0.00004404 rank 4
2023-02-17 02:48:56,794 DEBUG TRAIN Batch 0/1100 loss 272.473694 loss_att 240.539307 loss_ctc 285.234924 loss_rnnt 276.908936 hw_loss 0.468975 lr 0.00004404 rank 2
2023-02-17 02:50:10,933 DEBUG TRAIN Batch 0/1200 loss 262.883118 loss_att 232.584213 loss_ctc 277.794037 loss_rnnt 266.726685 hw_loss 0.427675 lr 0.00004804 rank 5
2023-02-17 02:50:10,933 DEBUG TRAIN Batch 0/1200 loss 246.453629 loss_att 216.950729 loss_ctc 263.487396 loss_rnnt 249.867386 hw_loss 0.404316 lr 0.00004804 rank 4
2023-02-17 02:50:10,933 DEBUG TRAIN Batch 0/1200 loss 215.599518 loss_att 191.527344 loss_ctc 224.963867 loss_rnnt 218.883240 hw_loss 0.529006 lr 0.00004804 rank 7
2023-02-17 02:50:10,934 DEBUG TRAIN Batch 0/1200 loss 221.599106 loss_att 196.532196 loss_ctc 234.561203 loss_rnnt 224.598373 hw_loss 0.535949 lr 0.00004804 rank 3
2023-02-17 02:50:10,937 DEBUG TRAIN Batch 0/1200 loss 276.118958 loss_att 244.386688 loss_ctc 289.818695 loss_rnnt 280.402405 hw_loss 0.443198 lr 0.00004804 rank 6
2023-02-17 02:50:10,939 DEBUG TRAIN Batch 0/1200 loss 155.486420 loss_att 137.943359 loss_ctc 163.099625 loss_rnnt 157.658600 hw_loss 0.602493 lr 0.00004804 rank 2
2023-02-17 02:50:10,939 DEBUG TRAIN Batch 0/1200 loss 202.451340 loss_att 179.612991 loss_ctc 213.598602 loss_rnnt 205.267883 hw_loss 0.496521 lr 0.00004804 rank 1
2023-02-17 02:50:10,941 DEBUG TRAIN Batch 0/1200 loss 274.240845 loss_att 243.497162 loss_ctc 288.355194 loss_rnnt 278.275330 hw_loss 0.435604 lr 0.00004804 rank 0
2023-02-17 02:51:27,595 DEBUG TRAIN Batch 0/1300 loss 390.377838 loss_att 345.443726 loss_ctc 408.788940 loss_rnnt 396.710388 hw_loss 0.373984 lr 0.00005204 rank 7
2023-02-17 02:51:27,597 DEBUG TRAIN Batch 0/1300 loss 307.812225 loss_att 273.038574 loss_ctc 321.049652 loss_rnnt 312.791321 hw_loss 0.394993 lr 0.00005204 rank 3
2023-02-17 02:51:27,602 DEBUG TRAIN Batch 0/1300 loss 164.018005 loss_att 147.134216 loss_ctc 174.250885 loss_rnnt 165.771759 hw_loss 0.484927 lr 0.00005204 rank 5
2023-02-17 02:51:27,604 DEBUG TRAIN Batch 0/1300 loss 77.005440 loss_att 69.046745 loss_ctc 79.990448 loss_rnnt 77.826660 hw_loss 0.698472 lr 0.00005204 rank 6
2023-02-17 02:51:27,609 DEBUG TRAIN Batch 0/1300 loss 306.501099 loss_att 270.548340 loss_ctc 324.489716 loss_rnnt 311.059998 hw_loss 0.437239 lr 0.00005204 rank 2
2023-02-17 02:51:27,610 DEBUG TRAIN Batch 0/1300 loss 67.284752 loss_att 60.645630 loss_ctc 70.633568 loss_rnnt 67.842461 hw_loss 0.606769 lr 0.00005204 rank 4
2023-02-17 02:51:27,614 DEBUG TRAIN Batch 0/1300 loss 329.630615 loss_att 292.795868 loss_ctc 342.228271 loss_rnnt 335.093109 hw_loss 0.421466 lr 0.00005204 rank 1
2023-02-17 02:51:27,618 DEBUG TRAIN Batch 0/1300 loss 399.336700 loss_att 354.296906 loss_ctc 427.034058 loss_rnnt 404.488861 hw_loss 0.305345 lr 0.00005204 rank 0
2023-02-17 02:52:47,441 DEBUG TRAIN Batch 0/1400 loss 308.766266 loss_att 277.474365 loss_ctc 330.503296 loss_rnnt 311.922974 hw_loss 0.381414 lr 0.00005604 rank 6
2023-02-17 02:52:47,443 DEBUG TRAIN Batch 0/1400 loss 323.621643 loss_att 284.745392 loss_ctc 341.293518 loss_rnnt 328.858826 hw_loss 0.340847 lr 0.00005604 rank 3
2023-02-17 02:52:47,445 DEBUG TRAIN Batch 0/1400 loss 341.411316 loss_att 305.917603 loss_ctc 370.838135 loss_rnnt 344.372375 hw_loss 0.401428 lr 0.00005604 rank 4
2023-02-17 02:52:47,448 DEBUG TRAIN Batch 0/1400 loss 316.225220 loss_att 279.841766 loss_ctc 336.336395 loss_rnnt 320.560181 hw_loss 0.487984 lr 0.00005604 rank 7
2023-02-17 02:52:47,450 DEBUG TRAIN Batch 0/1400 loss 330.935883 loss_att 295.056091 loss_ctc 350.438477 loss_rnnt 335.285400 hw_loss 0.423917 lr 0.00005604 rank 5
2023-02-17 02:52:47,451 DEBUG TRAIN Batch 0/1400 loss 320.938202 loss_att 281.207794 loss_ctc 338.715637 loss_rnnt 326.239258 hw_loss 0.515028 lr 0.00005604 rank 0
2023-02-17 02:52:47,452 DEBUG TRAIN Batch 0/1400 loss 320.768616 loss_att 283.330078 loss_ctc 336.376923 loss_rnnt 325.956665 hw_loss 0.409763 lr 0.00005604 rank 2
2023-02-17 02:52:47,453 DEBUG TRAIN Batch 0/1400 loss 370.411133 loss_att 331.206055 loss_ctc 394.083496 loss_rnnt 374.899353 hw_loss 0.368442 lr 0.00005604 rank 1
2023-02-17 02:54:04,936 DEBUG TRAIN Batch 0/1500 loss 326.784149 loss_att 293.766876 loss_ctc 356.808807 loss_rnnt 329.182709 hw_loss 0.378071 lr 0.00006004 rank 5
2023-02-17 02:54:04,937 DEBUG TRAIN Batch 0/1500 loss 362.570190 loss_att 321.644226 loss_ctc 379.938446 loss_rnnt 368.245819 hw_loss 0.363378 lr 0.00006004 rank 6
2023-02-17 02:54:04,938 DEBUG TRAIN Batch 0/1500 loss 285.559479 loss_att 255.475998 loss_ctc 305.816132 loss_rnnt 288.634094 hw_loss 0.452218 lr 0.00006004 rank 7
2023-02-17 02:54:04,938 DEBUG TRAIN Batch 0/1500 loss 321.293427 loss_att 286.841949 loss_ctc 343.823975 loss_rnnt 324.970306 hw_loss 0.392511 lr 0.00006004 rank 4
2023-02-17 02:54:04,942 DEBUG TRAIN Batch 0/1500 loss 306.895172 loss_att 276.385864 loss_ctc 328.761414 loss_rnnt 309.884338 hw_loss 0.369725 lr 0.00006004 rank 3
2023-02-17 02:54:04,943 DEBUG TRAIN Batch 0/1500 loss 325.096924 loss_att 287.959625 loss_ctc 344.087341 loss_rnnt 329.767700 hw_loss 0.421205 lr 0.00006004 rank 2
2023-02-17 02:54:04,946 DEBUG TRAIN Batch 0/1500 loss 337.382141 loss_att 301.792480 loss_ctc 361.920898 loss_rnnt 341.057007 hw_loss 0.321043 lr 0.00006004 rank 0
2023-02-17 02:54:04,950 DEBUG TRAIN Batch 0/1500 loss 294.207275 loss_att 260.203369 loss_ctc 311.959808 loss_rnnt 298.369568 hw_loss 0.509054 lr 0.00006004 rank 1
2023-02-17 02:55:20,512 DEBUG TRAIN Batch 0/1600 loss 285.045471 loss_att 258.370880 loss_ctc 313.127136 loss_rnnt 286.381653 hw_loss 0.477208 lr 0.00006404 rank 5
2023-02-17 02:55:20,513 DEBUG TRAIN Batch 0/1600 loss 307.957367 loss_att 277.571045 loss_ctc 332.717957 loss_rnnt 310.523895 hw_loss 0.392525 lr 0.00006404 rank 6
2023-02-17 02:55:20,513 DEBUG TRAIN Batch 0/1600 loss 285.367340 loss_att 256.949341 loss_ctc 310.772156 loss_rnnt 287.408569 hw_loss 0.478288 lr 0.00006404 rank 3
2023-02-17 02:55:20,515 DEBUG TRAIN Batch 0/1600 loss 277.160309 loss_att 249.728729 loss_ctc 298.663483 loss_rnnt 279.566010 hw_loss 0.400388 lr 0.00006404 rank 1
2023-02-17 02:55:20,516 DEBUG TRAIN Batch 0/1600 loss 345.307617 loss_att 308.615051 loss_ctc 368.815674 loss_rnnt 349.334686 hw_loss 0.331973 lr 0.00006404 rank 7
2023-02-17 02:55:20,517 DEBUG TRAIN Batch 0/1600 loss 308.994629 loss_att 274.658386 loss_ctc 335.314758 loss_rnnt 312.146057 hw_loss 0.387141 lr 0.00006404 rank 0
2023-02-17 02:55:20,516 DEBUG TRAIN Batch 0/1600 loss 300.712952 loss_att 267.253052 loss_ctc 321.109406 loss_rnnt 304.482422 hw_loss 0.380642 lr 0.00006404 rank 2
2023-02-17 02:55:20,561 DEBUG TRAIN Batch 0/1600 loss 355.147217 loss_att 320.768738 loss_ctc 379.928955 loss_rnnt 358.546112 hw_loss 0.323571 lr 0.00006404 rank 4
2023-02-17 02:56:38,650 DEBUG TRAIN Batch 0/1700 loss 285.042542 loss_att 254.666595 loss_ctc 308.269836 loss_rnnt 287.794464 hw_loss 0.424292 lr 0.00006804 rank 3
2023-02-17 02:56:38,651 DEBUG TRAIN Batch 0/1700 loss 287.537872 loss_att 259.984802 loss_ctc 312.563507 loss_rnnt 289.521759 hw_loss 0.356210 lr 0.00006804 rank 2
2023-02-17 02:56:38,651 DEBUG TRAIN Batch 0/1700 loss 294.703217 loss_att 263.561920 loss_ctc 311.215790 loss_rnnt 298.500916 hw_loss 0.429177 lr 0.00006804 rank 4
2023-02-17 02:56:38,653 DEBUG TRAIN Batch 0/1700 loss 303.461487 loss_att 270.688965 loss_ctc 325.499451 loss_rnnt 306.859344 hw_loss 0.409209 lr 0.00006804 rank 6
2023-02-17 02:56:38,654 DEBUG TRAIN Batch 0/1700 loss 273.166626 loss_att 244.243256 loss_ctc 299.632416 loss_rnnt 275.245605 hw_loss 0.331701 lr 0.00006804 rank 7
2023-02-17 02:56:38,657 DEBUG TRAIN Batch 0/1700 loss 267.441895 loss_att 240.092743 loss_ctc 287.947113 loss_rnnt 269.965637 hw_loss 0.397657 lr 0.00006804 rank 1
2023-02-17 02:56:38,659 DEBUG TRAIN Batch 0/1700 loss 296.097076 loss_att 267.710358 loss_ctc 320.408447 loss_rnnt 298.320496 hw_loss 0.398247 lr 0.00006804 rank 5
2023-02-17 02:56:38,661 DEBUG TRAIN Batch 0/1700 loss 330.834015 loss_att 294.126953 loss_ctc 354.282715 loss_rnnt 334.850342 hw_loss 0.372289 lr 0.00006804 rank 0
2023-02-17 02:57:58,331 DEBUG TRAIN Batch 0/1800 loss 302.126526 loss_att 269.399933 loss_ctc 329.918945 loss_rnnt 304.753296 hw_loss 0.399147 lr 0.00007204 rank 7
2023-02-17 02:57:58,337 DEBUG TRAIN Batch 0/1800 loss 198.809402 loss_att 179.051788 loss_ctc 217.149445 loss_rnnt 200.106903 hw_loss 0.391278 lr 0.00007204 rank 3
2023-02-17 02:57:58,339 DEBUG TRAIN Batch 0/1800 loss 281.846802 loss_att 251.497559 loss_ctc 307.734833 loss_rnnt 284.164825 hw_loss 0.562704 lr 0.00007204 rank 6
2023-02-17 02:57:58,341 DEBUG TRAIN Batch 0/1800 loss 312.447357 loss_att 278.382019 loss_ctc 331.779816 loss_rnnt 316.425110 hw_loss 0.483121 lr 0.00007204 rank 4
2023-02-17 02:57:58,344 DEBUG TRAIN Batch 0/1800 loss 250.410477 loss_att 225.764923 loss_ctc 278.521851 loss_rnnt 251.426849 hw_loss 0.308560 lr 0.00007204 rank 5
2023-02-17 02:57:58,345 DEBUG TRAIN Batch 0/1800 loss 167.993835 loss_att 151.319244 loss_ctc 181.223083 loss_rnnt 169.304245 hw_loss 0.488625 lr 0.00007204 rank 1
2023-02-17 02:57:58,349 DEBUG TRAIN Batch 0/1800 loss 210.422058 loss_att 187.911972 loss_ctc 227.370697 loss_rnnt 212.426254 hw_loss 0.446241 lr 0.00007204 rank 0
2023-02-17 02:57:58,382 DEBUG TRAIN Batch 0/1800 loss 212.094330 loss_att 187.211349 loss_ctc 227.636154 loss_rnnt 214.780243 hw_loss 0.409614 lr 0.00007204 rank 2
2023-02-17 02:59:14,910 DEBUG TRAIN Batch 0/1900 loss 192.079041 loss_att 170.129501 loss_ctc 213.514282 loss_rnnt 193.400192 hw_loss 0.395115 lr 0.00007604 rank 5
2023-02-17 02:59:14,912 DEBUG TRAIN Batch 0/1900 loss 188.353607 loss_att 168.293671 loss_ctc 204.263885 loss_rnnt 190.057053 hw_loss 0.350961 lr 0.00007604 rank 7
2023-02-17 02:59:14,913 DEBUG TRAIN Batch 0/1900 loss 140.923660 loss_att 126.349586 loss_ctc 155.720306 loss_rnnt 141.613998 hw_loss 0.471757 lr 0.00007604 rank 4
2023-02-17 02:59:14,913 DEBUG TRAIN Batch 0/1900 loss 175.522705 loss_att 156.919540 loss_ctc 193.253540 loss_rnnt 176.645187 hw_loss 0.438859 lr 0.00007604 rank 2
2023-02-17 02:59:14,914 DEBUG TRAIN Batch 0/1900 loss 227.520966 loss_att 204.150177 loss_ctc 245.584885 loss_rnnt 229.538818 hw_loss 0.464574 lr 0.00007604 rank 6
2023-02-17 02:59:14,915 DEBUG TRAIN Batch 0/1900 loss 265.187714 loss_att 237.037979 loss_ctc 289.241821 loss_rnnt 267.381317 hw_loss 0.429619 lr 0.00007604 rank 1
2023-02-17 02:59:14,916 DEBUG TRAIN Batch 0/1900 loss 169.187347 loss_att 150.790588 loss_ctc 181.640533 loss_rnnt 170.978928 hw_loss 0.426298 lr 0.00007604 rank 0
2023-02-17 02:59:14,918 DEBUG TRAIN Batch 0/1900 loss 147.753922 loss_att 132.855804 loss_ctc 162.893906 loss_rnnt 148.481934 hw_loss 0.436776 lr 0.00007604 rank 3
2023-02-17 03:00:31,820 DEBUG TRAIN Batch 0/2000 loss 366.314972 loss_att 324.085449 loss_ctc 397.954346 loss_rnnt 370.328613 hw_loss 0.400650 lr 0.00008004 rank 1
2023-02-17 03:00:31,819 DEBUG TRAIN Batch 0/2000 loss 304.289764 loss_att 267.353394 loss_ctc 329.135437 loss_rnnt 308.133057 hw_loss 0.433583 lr 0.00008004 rank 5
2023-02-17 03:00:31,820 DEBUG TRAIN Batch 0/2000 loss 265.310638 loss_att 233.926468 loss_ctc 287.004150 loss_rnnt 268.464111 hw_loss 0.432953 lr 0.00008004 rank 7
2023-02-17 03:00:31,820 DEBUG TRAIN Batch 0/2000 loss 332.214294 loss_att 297.535217 loss_ctc 360.321411 loss_rnnt 335.185913 hw_loss 0.406143 lr 0.00008004 rank 6
2023-02-17 03:00:31,821 DEBUG TRAIN Batch 0/2000 loss 318.547394 loss_att 284.928528 loss_ctc 359.663788 loss_rnnt 319.601868 hw_loss 0.350766 lr 0.00008004 rank 4
2023-02-17 03:00:31,822 DEBUG TRAIN Batch 0/2000 loss 288.309753 loss_att 260.161682 loss_ctc 317.036713 loss_rnnt 289.897400 hw_loss 0.396931 lr 0.00008004 rank 3
2023-02-17 03:00:31,823 DEBUG TRAIN Batch 0/2000 loss 335.048279 loss_att 299.217560 loss_ctc 368.628906 loss_rnnt 337.552490 hw_loss 0.345957 lr 0.00008004 rank 2
2023-02-17 03:00:31,868 DEBUG TRAIN Batch 0/2000 loss 357.759003 loss_att 319.388458 loss_ctc 389.810150 loss_rnnt 360.944794 hw_loss 0.402833 lr 0.00008004 rank 0
2023-02-17 03:01:52,004 DEBUG TRAIN Batch 0/2100 loss 298.099060 loss_att 267.693939 loss_ctc 331.845642 loss_rnnt 299.501434 hw_loss 0.335813 lr 0.00008404 rank 3
2023-02-17 03:01:52,005 DEBUG TRAIN Batch 0/2100 loss 294.254761 loss_att 263.885925 loss_ctc 332.337341 loss_rnnt 295.023499 hw_loss 0.426225 lr 0.00008404 rank 7
2023-02-17 03:01:52,007 DEBUG TRAIN Batch 0/2100 loss 322.598419 loss_att 288.594177 loss_ctc 342.593018 loss_rnnt 326.549683 hw_loss 0.344311 lr 0.00008404 rank 5
2023-02-17 03:01:52,007 DEBUG TRAIN Batch 0/2100 loss 249.228622 loss_att 222.540710 loss_ctc 273.593994 loss_rnnt 251.115723 hw_loss 0.378309 lr 0.00008404 rank 4
2023-02-17 03:01:52,008 DEBUG TRAIN Batch 0/2100 loss 299.421509 loss_att 269.428802 loss_ctc 325.147217 loss_rnnt 301.752686 hw_loss 0.444944 lr 0.00008404 rank 1
2023-02-17 03:01:52,008 DEBUG TRAIN Batch 0/2100 loss 310.455780 loss_att 278.448822 loss_ctc 343.961548 loss_rnnt 312.142181 hw_loss 0.464144 lr 0.00008404 rank 6
2023-02-17 03:01:52,037 DEBUG TRAIN Batch 0/2100 loss 330.802948 loss_att 296.066284 loss_ctc 361.878845 loss_rnnt 333.365936 hw_loss 0.451668 lr 0.00008404 rank 0
2023-02-17 03:01:52,058 DEBUG TRAIN Batch 0/2100 loss 304.491302 loss_att 271.944061 loss_ctc 334.077820 loss_rnnt 306.825653 hw_loss 0.431735 lr 0.00008404 rank 2
2023-02-17 03:03:09,116 DEBUG TRAIN Batch 0/2200 loss 325.125732 loss_att 291.422424 loss_ctc 351.765350 loss_rnnt 328.114960 hw_loss 0.374016 lr 0.00008804 rank 6
2023-02-17 03:03:09,123 DEBUG TRAIN Batch 0/2200 loss 317.383392 loss_att 285.148376 loss_ctc 354.959076 loss_rnnt 318.651855 hw_loss 0.315848 lr 0.00008804 rank 5
2023-02-17 03:03:09,126 DEBUG TRAIN Batch 0/2200 loss 287.681732 loss_att 257.320312 loss_ctc 311.400818 loss_rnnt 290.398193 hw_loss 0.362406 lr 0.00008804 rank 1
2023-02-17 03:03:09,128 DEBUG TRAIN Batch 0/2200 loss 284.025848 loss_att 254.687531 loss_ctc 315.704163 loss_rnnt 285.475342 hw_loss 0.364436 lr 0.00008804 rank 7
2023-02-17 03:03:09,128 DEBUG TRAIN Batch 0/2200 loss 295.198242 loss_att 264.884674 loss_ctc 317.554596 loss_rnnt 298.063416 hw_loss 0.406256 lr 0.00008804 rank 3
2023-02-17 03:03:09,129 DEBUG TRAIN Batch 0/2200 loss 348.154877 loss_att 315.288452 loss_ctc 377.813202 loss_rnnt 350.570312 hw_loss 0.381395 lr 0.00008804 rank 2
2023-02-17 03:03:09,129 DEBUG TRAIN Batch 0/2200 loss 286.093842 loss_att 254.164459 loss_ctc 316.566589 loss_rnnt 288.217255 hw_loss 0.373912 lr 0.00008804 rank 4
2023-02-17 03:03:09,132 DEBUG TRAIN Batch 0/2200 loss 264.692505 loss_att 238.252319 loss_ctc 293.435120 loss_rnnt 265.940948 hw_loss 0.388553 lr 0.00008804 rank 0
2023-02-17 03:04:26,730 DEBUG TRAIN Batch 0/2300 loss 253.219849 loss_att 226.687561 loss_ctc 275.196106 loss_rnnt 255.391998 hw_loss 0.382770 lr 0.00009204 rank 1
2023-02-17 03:04:26,734 DEBUG TRAIN Batch 0/2300 loss 304.287567 loss_att 274.181702 loss_ctc 331.447540 loss_rnnt 306.478607 hw_loss 0.391562 lr 0.00009204 rank 7
2023-02-17 03:04:26,735 DEBUG TRAIN Batch 0/2300 loss 255.565292 loss_att 229.637222 loss_ctc 282.283813 loss_rnnt 256.973907 hw_loss 0.402237 lr 0.00009204 rank 6
2023-02-17 03:04:26,736 DEBUG TRAIN Batch 0/2300 loss 300.860107 loss_att 268.154602 loss_ctc 326.476379 loss_rnnt 303.785187 hw_loss 0.375951 lr 0.00009204 rank 3
2023-02-17 03:04:26,739 DEBUG TRAIN Batch 0/2300 loss 282.712616 loss_att 250.723099 loss_ctc 310.888092 loss_rnnt 285.108643 hw_loss 0.459690 lr 0.00009204 rank 0
2023-02-17 03:04:26,740 DEBUG TRAIN Batch 0/2300 loss 293.652069 loss_att 266.052734 loss_ctc 321.868958 loss_rnnt 295.181549 hw_loss 0.427789 lr 0.00009204 rank 5
2023-02-17 03:04:26,740 DEBUG TRAIN Batch 0/2300 loss 283.823181 loss_att 255.602203 loss_ctc 306.231781 loss_rnnt 286.250366 hw_loss 0.429651 lr 0.00009204 rank 4
2023-02-17 03:04:26,795 DEBUG TRAIN Batch 0/2300 loss 258.313904 loss_att 230.445419 loss_ctc 280.856110 loss_rnnt 260.678833 hw_loss 0.380888 lr 0.00009204 rank 2
2023-02-17 03:05:44,268 DEBUG TRAIN Batch 0/2400 loss 287.298950 loss_att 260.009705 loss_ctc 312.083984 loss_rnnt 289.292297 hw_loss 0.299698 lr 0.00009604 rank 6
2023-02-17 03:05:44,270 DEBUG TRAIN Batch 0/2400 loss 284.950714 loss_att 255.467438 loss_ctc 314.024323 loss_rnnt 286.742584 hw_loss 0.428000 lr 0.00009604 rank 7
2023-02-17 03:05:44,272 DEBUG TRAIN Batch 0/2400 loss 268.905121 loss_att 242.155533 loss_ctc 305.152100 loss_rnnt 269.204407 hw_loss 0.408208 lr 0.00009604 rank 5
2023-02-17 03:05:44,274 DEBUG TRAIN Batch 0/2400 loss 291.876038 loss_att 261.416565 loss_ctc 312.270874 loss_rnnt 294.990875 hw_loss 0.483228 lr 0.00009604 rank 4
2023-02-17 03:05:44,274 DEBUG TRAIN Batch 0/2400 loss 298.066101 loss_att 266.599121 loss_ctc 323.343018 loss_rnnt 300.759888 hw_loss 0.430050 lr 0.00009604 rank 2
2023-02-17 03:05:44,275 DEBUG TRAIN Batch 0/2400 loss 218.194733 loss_att 195.624710 loss_ctc 230.393112 loss_rnnt 220.787445 hw_loss 0.552882 lr 0.00009604 rank 1
2023-02-17 03:05:44,279 DEBUG TRAIN Batch 0/2400 loss 237.388092 loss_att 213.401825 loss_ctc 261.850189 loss_rnnt 238.733932 hw_loss 0.355929 lr 0.00009604 rank 3
2023-02-17 03:05:44,280 DEBUG TRAIN Batch 0/2400 loss 307.179230 loss_att 272.942566 loss_ctc 333.164246 loss_rnnt 310.320648 hw_loss 0.452376 lr 0.00009604 rank 0
2023-02-17 03:07:05,384 DEBUG TRAIN Batch 0/2500 loss 219.973236 loss_att 199.331833 loss_ctc 234.709259 loss_rnnt 221.939362 hw_loss 0.369989 lr 0.00010004 rank 7
2023-02-17 03:07:05,387 DEBUG TRAIN Batch 0/2500 loss 173.020706 loss_att 155.273010 loss_ctc 190.669373 loss_rnnt 173.989166 hw_loss 0.427305 lr 0.00010004 rank 0
2023-02-17 03:07:05,388 DEBUG TRAIN Batch 0/2500 loss 232.817337 loss_att 208.411484 loss_ctc 257.440491 loss_rnnt 234.230377 hw_loss 0.346935 lr 0.00010004 rank 4
2023-02-17 03:07:05,388 DEBUG TRAIN Batch 0/2500 loss 225.715591 loss_att 203.048447 loss_ctc 246.051025 loss_rnnt 227.310699 hw_loss 0.425446 lr 0.00010004 rank 6
2023-02-17 03:07:05,390 DEBUG TRAIN Batch 0/2500 loss 258.097992 loss_att 234.861389 loss_ctc 285.903290 loss_rnnt 258.868652 hw_loss 0.317419 lr 0.00010004 rank 5
2023-02-17 03:07:05,392 DEBUG TRAIN Batch 0/2500 loss 219.296860 loss_att 197.962418 loss_ctc 242.543259 loss_rnnt 220.241653 hw_loss 0.417291 lr 0.00010004 rank 2
2023-02-17 03:07:05,396 DEBUG TRAIN Batch 0/2500 loss 245.114731 loss_att 221.607925 loss_ctc 264.441284 loss_rnnt 246.994827 hw_loss 0.458234 lr 0.00010004 rank 3
2023-02-17 03:07:05,465 DEBUG TRAIN Batch 0/2500 loss 283.334564 loss_att 255.101807 loss_ctc 315.984985 loss_rnnt 284.405212 hw_loss 0.417254 lr 0.00010004 rank 1
2023-02-17 03:08:21,785 DEBUG TRAIN Batch 0/2600 loss 374.091949 loss_att 341.884094 loss_ctc 402.513855 loss_rnnt 376.564148 hw_loss 0.337145 lr 0.00010404 rank 3
2023-02-17 03:08:21,788 DEBUG TRAIN Batch 0/2600 loss 304.931580 loss_att 278.953827 loss_ctc 338.207825 loss_rnnt 305.455688 hw_loss 0.439913 lr 0.00010404 rank 0
2023-02-17 03:08:21,789 DEBUG TRAIN Batch 0/2600 loss 297.503571 loss_att 269.754761 loss_ctc 323.972656 loss_rnnt 299.287689 hw_loss 0.443245 lr 0.00010404 rank 6
2023-02-17 03:08:21,789 DEBUG TRAIN Batch 0/2600 loss 105.156975 loss_att 97.009247 loss_ctc 113.876808 loss_rnnt 105.362244 hw_loss 0.490553 lr 0.00010404 rank 5
2023-02-17 03:08:21,789 DEBUG TRAIN Batch 0/2600 loss 93.603958 loss_att 84.703972 loss_ctc 101.767670 loss_rnnt 93.982994 hw_loss 0.585886 lr 0.00010404 rank 7
2023-02-17 03:08:21,791 DEBUG TRAIN Batch 0/2600 loss 264.530396 loss_att 243.353363 loss_ctc 290.558533 loss_rnnt 265.119263 hw_loss 0.330204 lr 0.00010404 rank 1
2023-02-17 03:08:21,794 DEBUG TRAIN Batch 0/2600 loss 325.813995 loss_att 289.055908 loss_ctc 354.762939 loss_rnnt 329.078674 hw_loss 0.425719 lr 0.00010404 rank 2
2023-02-17 03:08:21,833 DEBUG TRAIN Batch 0/2600 loss 63.196625 loss_att 58.848011 loss_ctc 69.924530 loss_rnnt 62.802338 hw_loss 0.688042 lr 0.00010404 rank 4
2023-02-17 03:09:38,938 DEBUG TRAIN Batch 0/2700 loss 283.743958 loss_att 259.115692 loss_ctc 309.122864 loss_rnnt 285.078125 hw_loss 0.389236 lr 0.00010804 rank 5
2023-02-17 03:09:38,941 DEBUG TRAIN Batch 0/2700 loss 296.116119 loss_att 271.838318 loss_ctc 325.635498 loss_rnnt 296.789551 hw_loss 0.461626 lr 0.00010804 rank 7
2023-02-17 03:09:38,944 DEBUG TRAIN Batch 0/2700 loss 318.377441 loss_att 297.553345 loss_ctc 347.043396 loss_rnnt 318.515259 hw_loss 0.384120 lr 0.00010804 rank 4
2023-02-17 03:09:38,945 DEBUG TRAIN Batch 0/2700 loss 200.721954 loss_att 185.365662 loss_ctc 220.924500 loss_rnnt 200.881577 hw_loss 0.408681 lr 0.00010804 rank 6
2023-02-17 03:09:38,946 DEBUG TRAIN Batch 0/2700 loss 304.160980 loss_att 281.526581 loss_ctc 328.935547 loss_rnnt 305.155792 hw_loss 0.429024 lr 0.00010804 rank 2
2023-02-17 03:09:38,946 DEBUG TRAIN Batch 0/2700 loss 219.710495 loss_att 204.826859 loss_ctc 238.522461 loss_rnnt 219.978928 hw_loss 0.375042 lr 0.00010804 rank 3
2023-02-17 03:09:38,947 DEBUG TRAIN Batch 0/2700 loss 257.694153 loss_att 239.622849 loss_ctc 285.361755 loss_rnnt 257.403931 hw_loss 0.403964 lr 0.00010804 rank 1
2023-02-17 03:09:38,992 DEBUG TRAIN Batch 0/2700 loss 285.483673 loss_att 266.392609 loss_ctc 311.389740 loss_rnnt 285.652313 hw_loss 0.366466 lr 0.00010804 rank 0
2023-02-17 03:10:57,819 DEBUG TRAIN Batch 0/2800 loss 260.707947 loss_att 240.473373 loss_ctc 274.849243 loss_rnnt 262.603699 hw_loss 0.498127 lr 0.00011204 rank 3
2023-02-17 03:10:57,820 DEBUG TRAIN Batch 0/2800 loss 278.630707 loss_att 261.545074 loss_ctc 299.549927 loss_rnnt 279.042267 hw_loss 0.405640 lr 0.00011204 rank 7
2023-02-17 03:10:57,820 DEBUG TRAIN Batch 0/2800 loss 271.259918 loss_att 256.829742 loss_ctc 288.858215 loss_rnnt 271.637939 hw_loss 0.302946 lr 0.00011204 rank 0
2023-02-17 03:10:57,821 DEBUG TRAIN Batch 0/2800 loss 295.440063 loss_att 272.864136 loss_ctc 323.280396 loss_rnnt 296.020020 hw_loss 0.418464 lr 0.00011204 rank 6
2023-02-17 03:10:57,823 DEBUG TRAIN Batch 0/2800 loss 271.309631 loss_att 252.325058 loss_ctc 289.100830 loss_rnnt 272.550629 hw_loss 0.344535 lr 0.00011204 rank 5
2023-02-17 03:10:57,824 DEBUG TRAIN Batch 0/2800 loss 291.043396 loss_att 270.634918 loss_ctc 316.598999 loss_rnnt 291.548462 hw_loss 0.317312 lr 0.00011204 rank 1
2023-02-17 03:10:57,825 DEBUG TRAIN Batch 0/2800 loss 308.965210 loss_att 290.472412 loss_ctc 333.847778 loss_rnnt 309.183014 hw_loss 0.305790 lr 0.00011204 rank 4
2023-02-17 03:10:57,828 DEBUG TRAIN Batch 0/2800 loss 275.834106 loss_att 259.227417 loss_ctc 298.782776 loss_rnnt 275.877075 hw_loss 0.409782 lr 0.00011204 rank 2
2023-02-17 03:12:15,975 DEBUG TRAIN Batch 0/2900 loss 237.948334 loss_att 227.205093 loss_ctc 257.871216 loss_rnnt 237.257385 hw_loss 0.343497 lr 0.00011604 rank 3
2023-02-17 03:12:15,976 DEBUG TRAIN Batch 0/2900 loss 288.197021 loss_att 274.689789 loss_ctc 317.134613 loss_rnnt 286.889893 hw_loss 0.281650 lr 0.00011604 rank 5
2023-02-17 03:12:15,982 DEBUG TRAIN Batch 0/2900 loss 290.817505 loss_att 272.282532 loss_ctc 316.309113 loss_rnnt 290.892731 hw_loss 0.436594 lr 0.00011604 rank 2
2023-02-17 03:12:15,982 DEBUG TRAIN Batch 0/2900 loss 266.660492 loss_att 249.384125 loss_ctc 289.933777 loss_rnnt 266.805603 hw_loss 0.388251 lr 0.00011604 rank 6
2023-02-17 03:12:15,982 DEBUG TRAIN Batch 0/2900 loss 257.381775 loss_att 242.971970 loss_ctc 277.940216 loss_rnnt 257.341309 hw_loss 0.339934 lr 0.00011604 rank 1
2023-02-17 03:12:15,983 DEBUG TRAIN Batch 0/2900 loss 294.624786 loss_att 276.478210 loss_ctc 313.554779 loss_rnnt 295.478760 hw_loss 0.471281 lr 0.00011604 rank 7
2023-02-17 03:12:15,988 DEBUG TRAIN Batch 0/2900 loss 233.093613 loss_att 219.338196 loss_ctc 248.947601 loss_rnnt 233.567490 hw_loss 0.306280 lr 0.00011604 rank 0
2023-02-17 03:12:16,034 DEBUG TRAIN Batch 0/2900 loss 253.951981 loss_att 242.551575 loss_ctc 271.346008 loss_rnnt 253.704987 hw_loss 0.389760 lr 0.00011604 rank 4
2023-02-17 03:13:33,432 DEBUG TRAIN Batch 0/3000 loss 260.877319 loss_att 246.156631 loss_ctc 284.177734 loss_rnnt 260.520752 hw_loss 0.363702 lr 0.00012004 rank 7
2023-02-17 03:13:33,434 DEBUG TRAIN Batch 0/3000 loss 268.361450 loss_att 255.415573 loss_ctc 290.109222 loss_rnnt 267.833923 hw_loss 0.406888 lr 0.00012004 rank 0
2023-02-17 03:13:33,439 DEBUG TRAIN Batch 0/3000 loss 288.814484 loss_att 275.867523 loss_ctc 313.387421 loss_rnnt 287.919189 hw_loss 0.390571 lr 0.00012004 rank 5
2023-02-17 03:13:33,440 DEBUG TRAIN Batch 0/3000 loss 249.183670 loss_att 232.571899 loss_ctc 272.344849 loss_rnnt 249.246552 hw_loss 0.321219 lr 0.00012004 rank 2
2023-02-17 03:13:33,440 DEBUG TRAIN Batch 0/3000 loss 278.161713 loss_att 264.311859 loss_ctc 302.872009 loss_rnnt 277.455994 hw_loss 0.339368 lr 0.00012004 rank 4
2023-02-17 03:13:33,442 DEBUG TRAIN Batch 0/3000 loss 198.937607 loss_att 190.422897 loss_ctc 211.217499 loss_rnnt 198.755905 hw_loss 0.463758 lr 0.00012004 rank 1
2023-02-17 03:13:33,441 DEBUG TRAIN Batch 0/3000 loss 242.789246 loss_att 232.566864 loss_ctc 263.174255 loss_rnnt 241.907761 hw_loss 0.389953 lr 0.00012004 rank 6
2023-02-17 03:13:33,486 DEBUG TRAIN Batch 0/3000 loss 252.382324 loss_att 242.827484 loss_ctc 267.250580 loss_rnnt 252.107620 hw_loss 0.381074 lr 0.00012004 rank 3
2023-02-17 03:14:50,284 DEBUG TRAIN Batch 0/3100 loss 180.533554 loss_att 176.289337 loss_ctc 188.150085 loss_rnnt 180.123596 hw_loss 0.456142 lr 0.00012404 rank 5
2023-02-17 03:14:50,286 DEBUG TRAIN Batch 0/3100 loss 244.106415 loss_att 236.399353 loss_ctc 262.107666 loss_rnnt 243.109680 hw_loss 0.258700 lr 0.00012404 rank 4
2023-02-17 03:14:50,289 DEBUG TRAIN Batch 0/3100 loss 262.335022 loss_att 263.338287 loss_ctc 271.317322 loss_rnnt 260.735931 hw_loss 0.376492 lr 0.00012404 rank 1
2023-02-17 03:14:50,290 DEBUG TRAIN Batch 0/3100 loss 235.904999 loss_att 238.161423 loss_ctc 254.360046 loss_rnnt 232.782639 hw_loss 0.394520 lr 0.00012404 rank 6
2023-02-17 03:14:50,290 DEBUG TRAIN Batch 0/3100 loss 214.144577 loss_att 204.404526 loss_ctc 219.403091 loss_rnnt 215.159851 hw_loss 0.434263 lr 0.00012404 rank 7
2023-02-17 03:14:50,291 DEBUG TRAIN Batch 0/3100 loss 224.421295 loss_att 213.141769 loss_ctc 232.960358 loss_rnnt 225.333054 hw_loss 0.385523 lr 0.00012404 rank 2
2023-02-17 03:14:50,292 DEBUG TRAIN Batch 0/3100 loss 209.456009 loss_att 203.413727 loss_ctc 217.876999 loss_rnnt 209.387009 hw_loss 0.290007 lr 0.00012404 rank 0
2023-02-17 03:14:50,292 DEBUG TRAIN Batch 0/3100 loss 178.061234 loss_att 177.298935 loss_ctc 189.307053 loss_rnnt 176.490158 hw_loss 0.420172 lr 0.00012404 rank 3
2023-02-17 03:16:10,718 DEBUG TRAIN Batch 0/3200 loss 169.941315 loss_att 169.761475 loss_ctc 173.541077 loss_rnnt 169.271088 hw_loss 0.424213 lr 0.00012804 rank 4
2023-02-17 03:16:10,719 DEBUG TRAIN Batch 0/3200 loss 98.383240 loss_att 97.660019 loss_ctc 105.544609 loss_rnnt 97.336906 hw_loss 0.442761 lr 0.00012804 rank 5
2023-02-17 03:16:10,719 DEBUG TRAIN Batch 0/3200 loss 71.478226 loss_att 69.181778 loss_ctc 75.677750 loss_rnnt 71.107849 hw_loss 0.505735 lr 0.00012804 rank 6
2023-02-17 03:16:10,724 DEBUG TRAIN Batch 0/3200 loss 147.453690 loss_att 152.450821 loss_ctc 154.779938 loss_rnnt 145.260223 hw_loss 0.407249 lr 0.00012804 rank 7
2023-02-17 03:16:10,726 DEBUG TRAIN Batch 0/3200 loss 102.259926 loss_att 100.765991 loss_ctc 109.653496 loss_rnnt 101.318192 hw_loss 0.477578 lr 0.00012804 rank 3
2023-02-17 03:16:10,726 DEBUG TRAIN Batch 0/3200 loss 265.674194 loss_att 269.311096 loss_ctc 270.886353 loss_rnnt 264.050964 hw_loss 0.376670 lr 0.00012804 rank 1
2023-02-17 03:16:10,740 DEBUG TRAIN Batch 0/3200 loss 251.317261 loss_att 251.133041 loss_ctc 256.952148 loss_rnnt 250.406464 hw_loss 0.368110 lr 0.00012804 rank 0
2023-02-17 03:16:10,751 DEBUG TRAIN Batch 0/3200 loss 91.249931 loss_att 90.588066 loss_ctc 95.204285 loss_rnnt 90.582222 hw_loss 0.511554 lr 0.00012804 rank 2
2023-02-17 03:17:28,839 DEBUG TRAIN Batch 0/3300 loss 224.851486 loss_att 233.807220 loss_ctc 232.694992 loss_rnnt 221.807343 hw_loss 0.388526 lr 0.00013204 rank 7
2023-02-17 03:17:28,842 DEBUG TRAIN Batch 0/3300 loss 257.184906 loss_att 266.403412 loss_ctc 253.193604 loss_rnnt 255.635956 hw_loss 0.445160 lr 0.00013204 rank 0
2023-02-17 03:17:28,842 DEBUG TRAIN Batch 0/3300 loss 224.109024 loss_att 237.777588 loss_ctc 224.065903 loss_rnnt 221.176636 hw_loss 0.383306 lr 0.00013204 rank 5
2023-02-17 03:17:28,843 DEBUG TRAIN Batch 0/3300 loss 279.671326 loss_att 288.208191 loss_ctc 297.158569 loss_rnnt 275.469116 hw_loss 0.306043 lr 0.00013204 rank 3
2023-02-17 03:17:28,843 DEBUG TRAIN Batch 0/3300 loss 247.599991 loss_att 251.818665 loss_ctc 248.227707 loss_rnnt 246.476425 hw_loss 0.367785 lr 0.00013204 rank 6
2023-02-17 03:17:28,845 DEBUG TRAIN Batch 0/3300 loss 256.356964 loss_att 262.018402 loss_ctc 271.187439 loss_rnnt 253.065643 hw_loss 0.340607 lr 0.00013204 rank 2
2023-02-17 03:17:28,845 DEBUG TRAIN Batch 0/3300 loss 266.421814 loss_att 268.649597 loss_ctc 273.951721 loss_rnnt 264.764618 hw_loss 0.389375 lr 0.00013204 rank 1
2023-02-17 03:17:28,886 DEBUG TRAIN Batch 0/3300 loss 235.085175 loss_att 237.929504 loss_ctc 239.669357 loss_rnnt 233.696228 hw_loss 0.391601 lr 0.00013204 rank 4
2023-02-17 03:18:45,064 DEBUG TRAIN Batch 0/3400 loss 257.212616 loss_att 263.030334 loss_ctc 262.823303 loss_rnnt 255.106445 hw_loss 0.364759 lr 0.00013604 rank 6
2023-02-17 03:18:45,066 DEBUG TRAIN Batch 0/3400 loss 195.877991 loss_att 212.409134 loss_ctc 204.848999 loss_rnnt 191.160919 hw_loss 0.402584 lr 0.00013604 rank 3
2023-02-17 03:18:45,071 DEBUG TRAIN Batch 0/3400 loss 275.725403 loss_att 294.710175 loss_ctc 283.496765 loss_rnnt 270.719177 hw_loss 0.324548 lr 0.00013604 rank 1
2023-02-17 03:18:45,072 DEBUG TRAIN Batch 0/3400 loss 213.207047 loss_att 230.431213 loss_ctc 217.025085 loss_rnnt 209.056305 hw_loss 0.369036 lr 0.00013604 rank 4
2023-02-17 03:18:45,073 DEBUG TRAIN Batch 0/3400 loss 234.743149 loss_att 257.662933 loss_ctc 243.095459 loss_rnnt 228.839874 hw_loss 0.385613 lr 0.00013604 rank 7
2023-02-17 03:18:45,073 DEBUG TRAIN Batch 0/3400 loss 274.764130 loss_att 279.154022 loss_ctc 283.469604 loss_rnnt 272.517639 hw_loss 0.389601 lr 0.00013604 rank 5
2023-02-17 03:18:45,076 DEBUG TRAIN Batch 0/3400 loss 258.465393 loss_att 262.408630 loss_ctc 274.870483 loss_rnnt 255.225952 hw_loss 0.493924 lr 0.00013604 rank 2
2023-02-17 03:18:45,082 DEBUG TRAIN Batch 0/3400 loss 243.884781 loss_att 266.806030 loss_ctc 250.113617 loss_rnnt 238.287811 hw_loss 0.341635 lr 0.00013604 rank 0
2023-02-17 03:20:02,987 DEBUG TRAIN Batch 0/3500 loss 187.398376 loss_att 209.401947 loss_ctc 192.703659 loss_rnnt 182.083221 hw_loss 0.388270 lr 0.00014004 rank 7
2023-02-17 03:20:02,988 DEBUG TRAIN Batch 0/3500 loss 249.597672 loss_att 253.360291 loss_ctc 249.555161 loss_rnnt 248.630096 hw_loss 0.413855 lr 0.00014004 rank 6
2023-02-17 03:20:02,994 DEBUG TRAIN Batch 0/3500 loss 207.247849 loss_att 226.246887 loss_ctc 199.214661 loss_rnnt 204.287216 hw_loss 0.434852 lr 0.00014004 rank 5
2023-02-17 03:20:02,994 DEBUG TRAIN Batch 0/3500 loss 283.468445 loss_att 288.371063 loss_ctc 293.339783 loss_rnnt 280.951111 hw_loss 0.413694 lr 0.00014004 rank 0
2023-02-17 03:20:02,995 DEBUG TRAIN Batch 0/3500 loss 227.867691 loss_att 248.096619 loss_ctc 238.392395 loss_rnnt 222.226669 hw_loss 0.359935 lr 0.00014004 rank 3
2023-02-17 03:20:02,995 DEBUG TRAIN Batch 0/3500 loss 243.443741 loss_att 257.294647 loss_ctc 236.393707 loss_rnnt 241.405914 hw_loss 0.389282 lr 0.00014004 rank 4
2023-02-17 03:20:02,999 DEBUG TRAIN Batch 0/3500 loss 191.331329 loss_att 202.755035 loss_ctc 190.108292 loss_rnnt 188.952850 hw_loss 0.481521 lr 0.00014004 rank 1
2023-02-17 03:20:03,003 DEBUG TRAIN Batch 0/3500 loss 207.521179 loss_att 230.602692 loss_ctc 209.221298 loss_rnnt 202.463013 hw_loss 0.403454 lr 0.00014004 rank 2
2023-02-17 03:21:22,071 DEBUG TRAIN Batch 0/3600 loss 229.998215 loss_att 243.880005 loss_ctc 233.773071 loss_rnnt 226.536926 hw_loss 0.340530 lr 0.00014404 rank 7
2023-02-17 03:21:22,074 DEBUG TRAIN Batch 0/3600 loss 190.188812 loss_att 219.902252 loss_ctc 192.352310 loss_rnnt 183.744568 hw_loss 0.399535 lr 0.00014404 rank 4
2023-02-17 03:21:22,076 DEBUG TRAIN Batch 0/3600 loss 168.398483 loss_att 190.443359 loss_ctc 174.542114 loss_rnnt 162.958984 hw_loss 0.396306 lr 0.00014404 rank 1
2023-02-17 03:21:22,077 DEBUG TRAIN Batch 0/3600 loss 276.043396 loss_att 289.754211 loss_ctc 281.957214 loss_rnnt 272.291779 hw_loss 0.414287 lr 0.00014404 rank 3
2023-02-17 03:21:22,077 DEBUG TRAIN Batch 0/3600 loss 218.145477 loss_att 240.667938 loss_ctc 220.461884 loss_rnnt 213.167297 hw_loss 0.309081 lr 0.00014404 rank 5
2023-02-17 03:21:22,077 DEBUG TRAIN Batch 0/3600 loss 214.137207 loss_att 227.670441 loss_ctc 215.446243 loss_rnnt 211.067078 hw_loss 0.354221 lr 0.00014404 rank 2
2023-02-17 03:21:22,078 DEBUG TRAIN Batch 0/3600 loss 180.565521 loss_att 197.799820 loss_ctc 183.741882 loss_rnnt 176.510498 hw_loss 0.346238 lr 0.00014404 rank 6
2023-02-17 03:21:22,079 DEBUG TRAIN Batch 0/3600 loss 232.042648 loss_att 249.634033 loss_ctc 235.366135 loss_rnnt 227.875916 hw_loss 0.385007 lr 0.00014404 rank 0
2023-02-17 03:22:40,718 DEBUG TRAIN Batch 0/3700 loss 198.369507 loss_att 219.489792 loss_ctc 212.450119 loss_rnnt 192.071533 hw_loss 0.368413 lr 0.00014804 rank 7
2023-02-17 03:22:40,722 DEBUG TRAIN Batch 0/3700 loss 130.600159 loss_att 148.806488 loss_ctc 132.591217 loss_rnnt 126.494278 hw_loss 0.373393 lr 0.00014804 rank 0
2023-02-17 03:22:40,723 DEBUG TRAIN Batch 0/3700 loss 197.089752 loss_att 212.126297 loss_ctc 201.732666 loss_rnnt 193.262970 hw_loss 0.375772 lr 0.00014804 rank 4
2023-02-17 03:22:40,725 DEBUG TRAIN Batch 0/3700 loss 195.567322 loss_att 217.556488 loss_ctc 203.531433 loss_rnnt 189.875946 hw_loss 0.434329 lr 0.00014804 rank 6
2023-02-17 03:22:40,728 DEBUG TRAIN Batch 0/3700 loss 189.934128 loss_att 217.844254 loss_ctc 189.053391 loss_rnnt 184.245438 hw_loss 0.420192 lr 0.00014804 rank 3
2023-02-17 03:22:40,729 DEBUG TRAIN Batch 0/3700 loss 192.967621 loss_att 218.271301 loss_ctc 197.342972 loss_rnnt 187.076126 hw_loss 0.463837 lr 0.00014804 rank 2
2023-02-17 03:22:40,736 DEBUG TRAIN Batch 0/3700 loss 169.735672 loss_att 193.435516 loss_ctc 172.641205 loss_rnnt 164.373703 hw_loss 0.439876 lr 0.00014804 rank 5
2023-02-17 03:22:40,747 DEBUG TRAIN Batch 0/3700 loss 149.447174 loss_att 162.243896 loss_ctc 157.276596 loss_rnnt 145.648621 hw_loss 0.366120 lr 0.00014804 rank 1
2023-02-17 03:23:58,965 DEBUG TRAIN Batch 0/3800 loss 245.408997 loss_att 281.007172 loss_ctc 241.737396 loss_rnnt 238.597626 hw_loss 0.339942 lr 0.00015204 rank 5
2023-02-17 03:23:58,967 DEBUG TRAIN Batch 0/3800 loss 152.666290 loss_att 168.974075 loss_ctc 156.692017 loss_rnnt 148.623474 hw_loss 0.458399 lr 0.00015204 rank 7
2023-02-17 03:23:58,968 DEBUG TRAIN Batch 0/3800 loss 65.012642 loss_att 71.347717 loss_ctc 68.342979 loss_rnnt 63.075432 hw_loss 0.424037 lr 0.00015204 rank 2
2023-02-17 03:23:58,969 DEBUG TRAIN Batch 0/3800 loss 153.620270 loss_att 168.385941 loss_ctc 159.699066 loss_rnnt 149.637207 hw_loss 0.411414 lr 0.00015204 rank 6
2023-02-17 03:23:58,970 DEBUG TRAIN Batch 0/3800 loss 178.191849 loss_att 190.708649 loss_ctc 179.141449 loss_rnnt 175.276840 hw_loss 0.534482 lr 0.00015204 rank 4
2023-02-17 03:23:58,971 DEBUG TRAIN Batch 0/3800 loss 146.131058 loss_att 163.424500 loss_ctc 149.958908 loss_rnnt 141.938339 hw_loss 0.419332 lr 0.00015204 rank 3
2023-02-17 03:23:58,972 DEBUG TRAIN Batch 0/3800 loss 263.635956 loss_att 287.181458 loss_ctc 270.439148 loss_rnnt 257.806824 hw_loss 0.399232 lr 0.00015204 rank 1
2023-02-17 03:23:59,018 DEBUG TRAIN Batch 0/3800 loss 117.226257 loss_att 120.883942 loss_ctc 124.393631 loss_rnnt 115.248398 hw_loss 0.545030 lr 0.00015204 rank 0
2023-02-17 03:25:19,588 DEBUG TRAIN Batch 0/3900 loss 237.858643 loss_att 278.183167 loss_ctc 243.668015 loss_rnnt 228.817795 hw_loss 0.377539 lr 0.00015604 rank 4
2023-02-17 03:25:19,587 DEBUG TRAIN Batch 0/3900 loss 258.822113 loss_att 296.060425 loss_ctc 261.425293 loss_rnnt 250.809601 hw_loss 0.408264 lr 0.00015604 rank 1
2023-02-17 03:25:19,588 DEBUG TRAIN Batch 0/3900 loss 239.693344 loss_att 293.234802 loss_ctc 233.253143 loss_rnnt 229.675079 hw_loss 0.316209 lr 0.00015604 rank 7
2023-02-17 03:25:19,591 DEBUG TRAIN Batch 0/3900 loss 183.482559 loss_att 231.439728 loss_ctc 182.277679 loss_rnnt 173.873291 hw_loss 0.334671 lr 0.00015604 rank 6
2023-02-17 03:25:19,592 DEBUG TRAIN Batch 0/3900 loss 216.188858 loss_att 250.135803 loss_ctc 211.068817 loss_rnnt 209.852081 hw_loss 0.431331 lr 0.00015604 rank 2
2023-02-17 03:25:19,592 DEBUG TRAIN Batch 0/3900 loss 244.941986 loss_att 276.839508 loss_ctc 249.060547 loss_rnnt 237.849701 hw_loss 0.306848 lr 0.00015604 rank 5
2023-02-17 03:25:19,593 DEBUG TRAIN Batch 0/3900 loss 211.184494 loss_att 245.692078 loss_ctc 219.001434 loss_rnnt 203.058960 hw_loss 0.340782 lr 0.00015604 rank 0
2023-02-17 03:25:19,655 DEBUG TRAIN Batch 0/3900 loss 211.549545 loss_att 263.760223 loss_ctc 194.530258 loss_rnnt 203.192474 hw_loss 0.345338 lr 0.00015604 rank 3
2023-02-17 03:26:36,731 DEBUG TRAIN Batch 0/4000 loss 193.095978 loss_att 266.596283 loss_ctc 188.097321 loss_rnnt 178.854660 hw_loss 0.389522 lr 0.00016004 rank 7
2023-02-17 03:26:36,734 DEBUG TRAIN Batch 0/4000 loss 162.529984 loss_att 226.953552 loss_ctc 156.808075 loss_rnnt 150.195251 hw_loss 0.399268 lr 0.00016004 rank 6
2023-02-17 03:26:36,735 DEBUG TRAIN Batch 0/4000 loss 209.251984 loss_att 258.978516 loss_ctc 204.959076 loss_rnnt 199.690063 hw_loss 0.354383 lr 0.00016004 rank 3
2023-02-17 03:26:36,736 DEBUG TRAIN Batch 0/4000 loss 163.040710 loss_att 205.721756 loss_ctc 155.407623 loss_rnnt 155.312073 hw_loss 0.394085 lr 0.00016004 rank 1
2023-02-17 03:26:36,736 DEBUG TRAIN Batch 0/4000 loss 206.809708 loss_att 252.732330 loss_ctc 198.827194 loss_rnnt 198.489746 hw_loss 0.374555 lr 0.00016004 rank 5
2023-02-17 03:26:36,740 DEBUG TRAIN Batch 0/4000 loss 166.320877 loss_att 214.006317 loss_ctc 165.068527 loss_rnnt 156.742310 hw_loss 0.390859 lr 0.00016004 rank 4
2023-02-17 03:26:36,741 DEBUG TRAIN Batch 0/4000 loss 263.235443 loss_att 289.839722 loss_ctc 283.518402 loss_rnnt 255.009338 hw_loss 0.376587 lr 0.00016004 rank 2
2023-02-17 03:26:36,788 DEBUG TRAIN Batch 0/4000 loss 233.476166 loss_att 291.743469 loss_ctc 230.718262 loss_rnnt 221.984955 hw_loss 0.385273 lr 0.00016004 rank 0
2023-02-17 03:27:52,285 DEBUG TRAIN Batch 0/4100 loss 208.858292 loss_att 267.495880 loss_ctc 193.559006 loss_rnnt 198.968842 hw_loss 0.378472 lr 0.00016404 rank 4
2023-02-17 03:27:52,289 DEBUG TRAIN Batch 0/4100 loss 180.318817 loss_att 228.711823 loss_ctc 172.009033 loss_rnnt 171.527557 hw_loss 0.413701 lr 0.00016404 rank 6
2023-02-17 03:27:52,291 DEBUG TRAIN Batch 0/4100 loss 171.703735 loss_att 224.783356 loss_ctc 170.011612 loss_rnnt 161.053375 hw_loss 0.487611 lr 0.00016404 rank 7
2023-02-17 03:27:52,292 DEBUG TRAIN Batch 0/4100 loss 215.363251 loss_att 251.839706 loss_ctc 222.119492 loss_rnnt 206.998535 hw_loss 0.316129 lr 0.00016404 rank 2
2023-02-17 03:27:52,293 DEBUG TRAIN Batch 0/4100 loss 185.324203 loss_att 234.190186 loss_ctc 184.066193 loss_rnnt 175.538834 hw_loss 0.337332 lr 0.00016404 rank 5
2023-02-17 03:27:52,293 DEBUG TRAIN Batch 0/4100 loss 204.240570 loss_att 235.134811 loss_ctc 198.414978 loss_rnnt 198.637863 hw_loss 0.376182 lr 0.00016404 rank 0
2023-02-17 03:27:52,293 DEBUG TRAIN Batch 0/4100 loss 165.789291 loss_att 206.440979 loss_ctc 162.259476 loss_rnnt 157.866272 hw_loss 0.493744 lr 0.00016404 rank 1
2023-02-17 03:27:52,343 DEBUG TRAIN Batch 0/4100 loss 213.770630 loss_att 260.506348 loss_ctc 206.318573 loss_rnnt 205.173248 hw_loss 0.457202 lr 0.00016404 rank 3
2023-02-17 03:29:09,246 DEBUG TRAIN Batch 0/4200 loss 188.547592 loss_att 237.092041 loss_ctc 189.754517 loss_rnnt 178.473602 hw_loss 0.382834 lr 0.00016804 rank 6
2023-02-17 03:29:09,254 DEBUG TRAIN Batch 0/4200 loss 189.227905 loss_att 248.153809 loss_ctc 185.711975 loss_rnnt 177.733734 hw_loss 0.333382 lr 0.00016804 rank 5
2023-02-17 03:29:09,254 DEBUG TRAIN Batch 0/4200 loss 182.999710 loss_att 228.400085 loss_ctc 182.005524 loss_rnnt 173.849655 hw_loss 0.379807 lr 0.00016804 rank 0
2023-02-17 03:29:09,256 DEBUG TRAIN Batch 0/4200 loss 172.908539 loss_att 207.790436 loss_ctc 176.970734 loss_rnnt 165.182037 hw_loss 0.390938 lr 0.00016804 rank 7
2023-02-17 03:29:09,256 DEBUG TRAIN Batch 0/4200 loss 170.988800 loss_att 220.503632 loss_ctc 165.281830 loss_rnnt 161.646210 hw_loss 0.376013 lr 0.00016804 rank 3
2023-02-17 03:29:09,260 DEBUG TRAIN Batch 0/4200 loss 209.359253 loss_att 257.075989 loss_ctc 210.026306 loss_rnnt 199.503876 hw_loss 0.418309 lr 0.00016804 rank 4
2023-02-17 03:29:09,263 DEBUG TRAIN Batch 0/4200 loss 158.133896 loss_att 208.818741 loss_ctc 153.747543 loss_rnnt 148.402405 hw_loss 0.336286 lr 0.00016804 rank 1
2023-02-17 03:29:09,263 DEBUG TRAIN Batch 0/4200 loss 172.158356 loss_att 229.920166 loss_ctc 168.758728 loss_rnnt 160.824722 hw_loss 0.439782 lr 0.00016804 rank 2
2023-02-17 03:30:28,416 DEBUG TRAIN Batch 0/4300 loss 148.453186 loss_att 207.570053 loss_ctc 145.928986 loss_rnnt 136.735229 hw_loss 0.433406 lr 0.00017204 rank 6
2023-02-17 03:30:28,417 DEBUG TRAIN Batch 0/4300 loss 161.119263 loss_att 214.590714 loss_ctc 156.381165 loss_rnnt 150.851685 hw_loss 0.384398 lr 0.00017204 rank 5
2023-02-17 03:30:28,420 DEBUG TRAIN Batch 0/4300 loss 174.305923 loss_att 219.248672 loss_ctc 175.062622 loss_rnnt 165.012970 hw_loss 0.381577 lr 0.00017204 rank 2
2023-02-17 03:30:28,421 DEBUG TRAIN Batch 0/4300 loss 192.182953 loss_att 237.705017 loss_ctc 185.495178 loss_rnnt 183.750000 hw_loss 0.412970 lr 0.00017204 rank 4
2023-02-17 03:30:28,422 DEBUG TRAIN Batch 0/4300 loss 142.149902 loss_att 176.706116 loss_ctc 134.929184 loss_rnnt 135.980042 hw_loss 0.415070 lr 0.00017204 rank 1
2023-02-17 03:30:28,423 DEBUG TRAIN Batch 0/4300 loss 223.495758 loss_att 267.552673 loss_ctc 229.558807 loss_rnnt 213.662827 hw_loss 0.399645 lr 0.00017204 rank 7
2023-02-17 03:30:28,427 DEBUG TRAIN Batch 0/4300 loss 157.432114 loss_att 211.037277 loss_ctc 149.868011 loss_rnnt 147.516418 hw_loss 0.381002 lr 0.00017204 rank 0
2023-02-17 03:30:28,466 DEBUG TRAIN Batch 0/4300 loss 184.835922 loss_att 234.822327 loss_ctc 183.807297 loss_rnnt 174.748260 hw_loss 0.426586 lr 0.00017204 rank 3
2023-02-17 03:31:46,976 DEBUG TRAIN Batch 0/4400 loss 123.143684 loss_att 174.040222 loss_ctc 118.731651 loss_rnnt 113.326843 hw_loss 0.423369 lr 0.00017604 rank 6
2023-02-17 03:31:46,978 DEBUG TRAIN Batch 0/4400 loss 201.472763 loss_att 240.743942 loss_ctc 196.356384 loss_rnnt 194.108932 hw_loss 0.359609 lr 0.00017604 rank 3
2023-02-17 03:31:46,978 DEBUG TRAIN Batch 0/4400 loss 212.268997 loss_att 266.683533 loss_ctc 199.516479 loss_rnnt 202.916367 hw_loss 0.318877 lr 0.00017604 rank 1
2023-02-17 03:31:46,981 DEBUG TRAIN Batch 0/4400 loss 124.398682 loss_att 182.591873 loss_ctc 124.789818 loss_rnnt 112.510559 hw_loss 0.369994 lr 0.00017604 rank 7
2023-02-17 03:31:46,983 DEBUG TRAIN Batch 0/4400 loss 115.630188 loss_att 152.367889 loss_ctc 114.539696 loss_rnnt 108.169785 hw_loss 0.484237 lr 0.00017604 rank 0
2023-02-17 03:31:46,983 DEBUG TRAIN Batch 0/4400 loss 110.340004 loss_att 132.447189 loss_ctc 109.351425 loss_rnnt 105.920715 hw_loss 0.243116 lr 0.00017604 rank 5
2023-02-17 03:31:46,986 DEBUG TRAIN Batch 0/4400 loss 173.443008 loss_att 217.123383 loss_ctc 170.580109 loss_rnnt 164.900879 hw_loss 0.352040 lr 0.00017604 rank 4
2023-02-17 03:31:46,990 DEBUG TRAIN Batch 0/4400 loss 90.503960 loss_att 131.486008 loss_ctc 89.119286 loss_rnnt 82.238060 hw_loss 0.476469 lr 0.00017604 rank 2
2023-02-17 03:33:02,746 DEBUG TRAIN Batch 0/4500 loss 60.913197 loss_att 80.693626 loss_ctc 61.645493 loss_rnnt 56.639961 hw_loss 0.411570 lr 0.00018004 rank 7
2023-02-17 03:33:02,749 DEBUG TRAIN Batch 0/4500 loss 222.355286 loss_att 291.520721 loss_ctc 229.280426 loss_rnnt 207.434540 hw_loss 0.308067 lr 0.00018004 rank 6
2023-02-17 03:33:02,751 DEBUG TRAIN Batch 0/4500 loss 202.200546 loss_att 269.967041 loss_ctc 186.204498 loss_rnnt 190.594574 hw_loss 0.347804 lr 0.00018004 rank 5
2023-02-17 03:33:02,751 DEBUG TRAIN Batch 0/4500 loss 166.162125 loss_att 243.978485 loss_ctc 174.753952 loss_rnnt 149.277039 hw_loss 0.330455 lr 0.00018004 rank 1
2023-02-17 03:33:02,755 DEBUG TRAIN Batch 0/4500 loss 222.094879 loss_att 300.042358 loss_ctc 216.975708 loss_rnnt 207.029358 hw_loss 0.297304 lr 0.00018004 rank 0
2023-02-17 03:33:02,756 DEBUG TRAIN Batch 0/4500 loss 41.019127 loss_att 51.882893 loss_ctc 41.929371 loss_rnnt 38.429386 hw_loss 0.554299 lr 0.00018004 rank 4
2023-02-17 03:33:02,757 DEBUG TRAIN Batch 0/4500 loss 174.635727 loss_att 251.903152 loss_ctc 176.910980 loss_rnnt 158.666565 hw_loss 0.398070 lr 0.00018004 rank 2
2023-02-17 03:33:02,799 DEBUG TRAIN Batch 0/4500 loss 90.836327 loss_att 117.944183 loss_ctc 91.353729 loss_rnnt 85.074272 hw_loss 0.509045 lr 0.00018004 rank 3
2023-02-17 03:34:22,183 DEBUG TRAIN Batch 0/4600 loss 144.194931 loss_att 199.090134 loss_ctc 141.888809 loss_rnnt 133.309891 hw_loss 0.400258 lr 0.00018404 rank 1
2023-02-17 03:34:22,184 DEBUG TRAIN Batch 0/4600 loss 130.651505 loss_att 215.829697 loss_ctc 124.142838 loss_rnnt 114.278313 hw_loss 0.385090 lr 0.00018404 rank 3
2023-02-17 03:34:22,184 DEBUG TRAIN Batch 0/4600 loss 149.021973 loss_att 226.030304 loss_ctc 143.203583 loss_rnnt 134.175583 hw_loss 0.413418 lr 0.00018404 rank 6
2023-02-17 03:34:22,185 DEBUG TRAIN Batch 0/4600 loss 199.844604 loss_att 271.642548 loss_ctc 199.686813 loss_rnnt 185.368378 hw_loss 0.258144 lr 0.00018404 rank 7
2023-02-17 03:34:22,187 DEBUG TRAIN Batch 0/4600 loss 162.852722 loss_att 239.881897 loss_ctc 161.689484 loss_rnnt 147.420212 hw_loss 0.340831 lr 0.00018404 rank 4
2023-02-17 03:34:22,188 DEBUG TRAIN Batch 0/4600 loss 200.658936 loss_att 275.393158 loss_ctc 200.238449 loss_rnnt 185.586639 hw_loss 0.340332 lr 0.00018404 rank 5
2023-02-17 03:34:22,190 DEBUG TRAIN Batch 0/4600 loss 207.288818 loss_att 275.657135 loss_ctc 207.788376 loss_rnnt 193.353851 hw_loss 0.365076 lr 0.00018404 rank 2
2023-02-17 03:34:22,191 DEBUG TRAIN Batch 0/4600 loss 167.646225 loss_att 242.907379 loss_ctc 167.742676 loss_rnnt 152.386078 hw_loss 0.365727 lr 0.00018404 rank 0
2023-02-17 03:35:40,472 DEBUG TRAIN Batch 0/4700 loss 156.299850 loss_att 235.696045 loss_ctc 151.302307 loss_rnnt 140.924194 hw_loss 0.305132 lr 0.00018804 rank 6
2023-02-17 03:35:40,472 DEBUG TRAIN Batch 0/4700 loss 141.850647 loss_att 222.589569 loss_ctc 135.866714 loss_rnnt 126.245911 hw_loss 0.477758 lr 0.00018804 rank 7
2023-02-17 03:35:40,473 DEBUG TRAIN Batch 0/4700 loss 155.705734 loss_att 231.335114 loss_ctc 149.678116 loss_rnnt 141.194702 hw_loss 0.354068 lr 0.00018804 rank 5
2023-02-17 03:35:40,473 DEBUG TRAIN Batch 0/4700 loss 173.642288 loss_att 252.406860 loss_ctc 172.911545 loss_rnnt 157.753113 hw_loss 0.438194 lr 0.00018804 rank 3
2023-02-17 03:35:40,473 DEBUG TRAIN Batch 0/4700 loss 138.286118 loss_att 205.761932 loss_ctc 134.455170 loss_rnnt 125.088417 hw_loss 0.399980 lr 0.00018804 rank 1
2023-02-17 03:35:40,474 DEBUG TRAIN Batch 0/4700 loss 152.811768 loss_att 214.550629 loss_ctc 152.798752 loss_rnnt 140.287231 hw_loss 0.334695 lr 0.00018804 rank 0
2023-02-17 03:35:40,479 DEBUG TRAIN Batch 0/4700 loss 138.684235 loss_att 186.552338 loss_ctc 137.388855 loss_rnnt 129.094238 hw_loss 0.354538 lr 0.00018804 rank 2
2023-02-17 03:35:40,482 DEBUG TRAIN Batch 0/4700 loss 133.058014 loss_att 219.662048 loss_ctc 132.845688 loss_rnnt 115.556633 hw_loss 0.391657 lr 0.00018804 rank 4
2023-02-17 03:36:56,600 DEBUG TRAIN Batch 0/4800 loss 175.550949 loss_att 244.686340 loss_ctc 181.478226 loss_rnnt 160.715851 hw_loss 0.408200 lr 0.00019204 rank 6
2023-02-17 03:36:56,600 DEBUG TRAIN Batch 0/4800 loss 165.381866 loss_att 253.138748 loss_ctc 165.792023 loss_rnnt 147.561295 hw_loss 0.402184 lr 0.00019204 rank 3
2023-02-17 03:36:56,602 DEBUG TRAIN Batch 0/4800 loss 190.923737 loss_att 261.726227 loss_ctc 191.941589 loss_rnnt 176.371857 hw_loss 0.479337 lr 0.00019204 rank 5
2023-02-17 03:36:56,604 DEBUG TRAIN Batch 0/4800 loss 175.429276 loss_att 255.535736 loss_ctc 181.534103 loss_rnnt 158.410614 hw_loss 0.343880 lr 0.00019204 rank 7
2023-02-17 03:36:56,605 DEBUG TRAIN Batch 0/4800 loss 160.863174 loss_att 230.724762 loss_ctc 158.918106 loss_rnnt 146.966141 hw_loss 0.345115 lr 0.00019204 rank 4
2023-02-17 03:36:56,605 DEBUG TRAIN Batch 0/4800 loss 145.672867 loss_att 229.399185 loss_ctc 138.487167 loss_rnnt 129.719696 hw_loss 0.311233 lr 0.00019204 rank 2
2023-02-17 03:36:56,605 DEBUG TRAIN Batch 0/4800 loss 161.835114 loss_att 242.773712 loss_ctc 165.995605 loss_rnnt 144.922684 hw_loss 0.318670 lr 0.00019204 rank 1
2023-02-17 03:36:56,608 DEBUG TRAIN Batch 0/4800 loss 159.535965 loss_att 229.983765 loss_ctc 149.065948 loss_rnnt 146.667786 hw_loss 0.327397 lr 0.00019204 rank 0
2023-02-17 03:38:13,705 DEBUG TRAIN Batch 0/4900 loss 130.481079 loss_att 194.036469 loss_ctc 129.492218 loss_rnnt 117.653564 hw_loss 0.465529 lr 0.00019604 rank 5
2023-02-17 03:38:13,706 DEBUG TRAIN Batch 0/4900 loss 168.804245 loss_att 236.559143 loss_ctc 168.705795 loss_rnnt 155.007904 hw_loss 0.484652 lr 0.00019604 rank 7
2023-02-17 03:38:13,711 DEBUG TRAIN Batch 0/4900 loss 148.432220 loss_att 220.998001 loss_ctc 152.726318 loss_rnnt 133.154327 hw_loss 0.360389 lr 0.00019604 rank 6
2023-02-17 03:38:13,714 DEBUG TRAIN Batch 0/4900 loss 117.061829 loss_att 162.055542 loss_ctc 121.310417 loss_rnnt 107.205910 hw_loss 0.545037 lr 0.00019604 rank 1
2023-02-17 03:38:13,715 DEBUG TRAIN Batch 0/4900 loss 159.804840 loss_att 227.690979 loss_ctc 153.475555 loss_rnnt 146.865250 hw_loss 0.386725 lr 0.00019604 rank 2
2023-02-17 03:38:13,716 DEBUG TRAIN Batch 0/4900 loss 172.394394 loss_att 237.222229 loss_ctc 167.326080 loss_rnnt 159.877243 hw_loss 0.426280 lr 0.00019604 rank 3
2023-02-17 03:38:13,718 DEBUG TRAIN Batch 0/4900 loss 184.743958 loss_att 244.305237 loss_ctc 189.643875 loss_rnnt 171.977509 hw_loss 0.376645 lr 0.00019604 rank 0
2023-02-17 03:38:13,765 DEBUG TRAIN Batch 0/4900 loss 144.270782 loss_att 217.355316 loss_ctc 133.191956 loss_rnnt 130.961365 hw_loss 0.318136 lr 0.00019604 rank 4
2023-02-17 03:39:33,629 DEBUG TRAIN Batch 0/5000 loss 116.761940 loss_att 177.311493 loss_ctc 114.573723 loss_rnnt 104.701736 hw_loss 0.453826 lr 0.00020004 rank 2
2023-02-17 03:39:33,629 DEBUG TRAIN Batch 0/5000 loss 128.141617 loss_att 184.875244 loss_ctc 127.735184 loss_rnnt 116.648613 hw_loss 0.375882 lr 0.00020004 rank 6
2023-02-17 03:39:33,630 DEBUG TRAIN Batch 0/5000 loss 158.999374 loss_att 240.073380 loss_ctc 161.918594 loss_rnnt 142.169373 hw_loss 0.423686 lr 0.00020004 rank 1
2023-02-17 03:39:33,631 DEBUG TRAIN Batch 0/5000 loss 70.846840 loss_att 96.251694 loss_ctc 71.920898 loss_rnnt 65.360931 hw_loss 0.490745 lr 0.00020004 rank 5
2023-02-17 03:39:33,632 DEBUG TRAIN Batch 0/5000 loss 134.233246 loss_att 220.294586 loss_ctc 127.921524 loss_rnnt 117.649765 hw_loss 0.398933 lr 0.00020004 rank 7
2023-02-17 03:39:33,635 DEBUG TRAIN Batch 0/5000 loss 132.339020 loss_att 198.792679 loss_ctc 135.701981 loss_rnnt 118.319397 hw_loss 0.525924 lr 0.00020004 rank 4
2023-02-17 03:39:33,637 DEBUG TRAIN Batch 0/5000 loss 137.217941 loss_att 211.468903 loss_ctc 139.800293 loss_rnnt 121.787971 hw_loss 0.441486 lr 0.00020004 rank 0
2023-02-17 03:39:33,684 DEBUG TRAIN Batch 0/5000 loss 143.967712 loss_att 222.299225 loss_ctc 147.467438 loss_rnnt 127.674637 hw_loss 0.300272 lr 0.00020004 rank 3
2023-02-17 03:40:51,035 DEBUG TRAIN Batch 0/5100 loss 131.197678 loss_att 236.753052 loss_ctc 120.505272 loss_rnnt 111.300621 hw_loss 0.396808 lr 0.00020404 rank 5
2023-02-17 03:40:51,037 DEBUG TRAIN Batch 0/5100 loss 77.978279 loss_att 120.571365 loss_ctc 77.424164 loss_rnnt 69.250771 hw_loss 0.530182 lr 0.00020404 rank 7
2023-02-17 03:40:51,039 DEBUG TRAIN Batch 0/5100 loss 132.086365 loss_att 197.620193 loss_ctc 136.868927 loss_rnnt 118.145386 hw_loss 0.368482 lr 0.00020404 rank 3
2023-02-17 03:40:51,040 DEBUG TRAIN Batch 0/5100 loss 71.469742 loss_att 104.349464 loss_ctc 69.791214 loss_rnnt 64.850197 hw_loss 0.501373 lr 0.00020404 rank 0
2023-02-17 03:40:51,041 DEBUG TRAIN Batch 0/5100 loss 175.701096 loss_att 282.938232 loss_ctc 172.462494 loss_rnnt 154.470825 hw_loss 0.402483 lr 0.00020404 rank 2
2023-02-17 03:40:51,041 DEBUG TRAIN Batch 0/5100 loss 57.341454 loss_att 81.809219 loss_ctc 55.291164 loss_rnnt 52.449219 hw_loss 0.510098 lr 0.00020404 rank 6
2023-02-17 03:40:51,042 DEBUG TRAIN Batch 0/5100 loss 82.858437 loss_att 118.495850 loss_ctc 84.153053 loss_rnnt 75.285713 hw_loss 0.511176 lr 0.00020404 rank 4
2023-02-17 03:40:51,043 DEBUG TRAIN Batch 0/5100 loss 132.607285 loss_att 233.427216 loss_ctc 121.956932 loss_rnnt 113.636703 hw_loss 0.424940 lr 0.00020404 rank 1
2023-02-17 03:42:08,137 DEBUG TRAIN Batch 0/5200 loss 137.211411 loss_att 227.586319 loss_ctc 134.074142 loss_rnnt 119.333313 hw_loss 0.415158 lr 0.00020804 rank 6
2023-02-17 03:42:08,137 DEBUG TRAIN Batch 0/5200 loss 150.431946 loss_att 254.724945 loss_ctc 154.552826 loss_rnnt 128.850311 hw_loss 0.325451 lr 0.00020804 rank 5
2023-02-17 03:42:08,141 DEBUG TRAIN Batch 0/5200 loss 122.869438 loss_att 203.215256 loss_ctc 121.642265 loss_rnnt 106.766136 hw_loss 0.370792 lr 0.00020804 rank 4
2023-02-17 03:42:08,142 DEBUG TRAIN Batch 0/5200 loss 130.676620 loss_att 261.906982 loss_ctc 120.077011 loss_rnnt 105.635864 hw_loss 0.389954 lr 0.00020804 rank 7
2023-02-17 03:42:08,141 DEBUG TRAIN Batch 0/5200 loss 136.714767 loss_att 245.492493 loss_ctc 132.160950 loss_rnnt 115.423828 hw_loss 0.267295 lr 0.00020804 rank 1
2023-02-17 03:42:08,143 DEBUG TRAIN Batch 0/5200 loss 152.042999 loss_att 240.533966 loss_ctc 154.647446 loss_rnnt 133.788864 hw_loss 0.391269 lr 0.00020804 rank 2
2023-02-17 03:42:08,143 DEBUG TRAIN Batch 0/5200 loss 174.632187 loss_att 298.763885 loss_ctc 178.179230 loss_rnnt 149.157562 hw_loss 0.328804 lr 0.00020804 rank 3
2023-02-17 03:42:08,144 DEBUG TRAIN Batch 0/5200 loss 151.039185 loss_att 253.463959 loss_ctc 150.026855 loss_rnnt 130.471725 hw_loss 0.407789 lr 0.00020804 rank 0
2023-02-17 03:43:27,196 DEBUG TRAIN Batch 0/5300 loss 156.619049 loss_att 246.381180 loss_ctc 144.136276 loss_rnnt 140.155884 hw_loss 0.328321 lr 0.00021204 rank 3
2023-02-17 03:43:27,196 DEBUG TRAIN Batch 0/5300 loss 142.675278 loss_att 225.982071 loss_ctc 148.586929 loss_rnnt 125.027588 hw_loss 0.371434 lr 0.00021204 rank 4
2023-02-17 03:43:27,199 DEBUG TRAIN Batch 0/5300 loss 133.954208 loss_att 216.680954 loss_ctc 129.245651 loss_rnnt 117.869598 hw_loss 0.313224 lr 0.00021204 rank 6
2023-02-17 03:43:27,202 DEBUG TRAIN Batch 0/5300 loss 132.445496 loss_att 234.265900 loss_ctc 128.797363 loss_rnnt 112.367416 hw_loss 0.375783 lr 0.00021204 rank 7
2023-02-17 03:43:27,202 DEBUG TRAIN Batch 0/5300 loss 144.738297 loss_att 238.793854 loss_ctc 140.658417 loss_rnnt 126.292999 hw_loss 0.334054 lr 0.00021204 rank 5
2023-02-17 03:43:27,203 DEBUG TRAIN Batch 0/5300 loss 117.690002 loss_att 216.880524 loss_ctc 120.632431 loss_rnnt 97.286888 hw_loss 0.323786 lr 0.00021204 rank 2
2023-02-17 03:43:27,205 DEBUG TRAIN Batch 0/5300 loss 125.433083 loss_att 226.367615 loss_ctc 126.705147 loss_rnnt 104.874817 hw_loss 0.378293 lr 0.00021204 rank 1
2023-02-17 03:43:27,207 DEBUG TRAIN Batch 0/5300 loss 140.958710 loss_att 231.790298 loss_ctc 137.864197 loss_rnnt 123.017715 hw_loss 0.351118 lr 0.00021204 rank 0
2023-02-17 03:44:45,216 DEBUG TRAIN Batch 0/5400 loss 137.117325 loss_att 236.322235 loss_ctc 133.351471 loss_rnnt 117.630356 hw_loss 0.277716 lr 0.00021604 rank 1
2023-02-17 03:44:45,216 DEBUG TRAIN Batch 0/5400 loss 145.623871 loss_att 263.180298 loss_ctc 143.004837 loss_rnnt 122.277931 hw_loss 0.344752 lr 0.00021604 rank 6
2023-02-17 03:44:45,217 DEBUG TRAIN Batch 0/5400 loss 152.654526 loss_att 260.954590 loss_ctc 149.928162 loss_rnnt 131.126984 hw_loss 0.433251 lr 0.00021604 rank 5
2023-02-17 03:44:45,221 DEBUG TRAIN Batch 0/5400 loss 177.999786 loss_att 268.939789 loss_ctc 181.121399 loss_rnnt 159.221191 hw_loss 0.326944 lr 0.00021604 rank 3
2023-02-17 03:44:45,221 DEBUG TRAIN Batch 0/5400 loss 129.235474 loss_att 253.617813 loss_ctc 127.825562 loss_rnnt 104.381996 hw_loss 0.309353 lr 0.00021604 rank 7
2023-02-17 03:44:45,225 DEBUG TRAIN Batch 0/5400 loss 144.462875 loss_att 242.090454 loss_ctc 144.188873 loss_rnnt 124.779900 hw_loss 0.363726 lr 0.00021604 rank 0
2023-02-17 03:44:45,227 DEBUG TRAIN Batch 0/5400 loss 152.693283 loss_att 263.644348 loss_ctc 147.752747 loss_rnnt 130.972168 hw_loss 0.355571 lr 0.00021604 rank 2
2023-02-17 03:44:45,227 DEBUG TRAIN Batch 0/5400 loss 108.816887 loss_att 203.807800 loss_ctc 107.277794 loss_rnnt 89.761841 hw_loss 0.491393 lr 0.00021604 rank 4
2023-02-17 03:46:01,330 DEBUG TRAIN Batch 0/5500 loss 129.310776 loss_att 226.761765 loss_ctc 121.360794 loss_rnnt 110.713501 hw_loss 0.313266 lr 0.00022004 rank 3
2023-02-17 03:46:01,334 DEBUG TRAIN Batch 0/5500 loss 158.361923 loss_att 263.652588 loss_ctc 158.004974 loss_rnnt 137.158630 hw_loss 0.361397 lr 0.00022004 rank 6
2023-02-17 03:46:01,334 DEBUG TRAIN Batch 0/5500 loss 114.994202 loss_att 196.596405 loss_ctc 108.825310 loss_rnnt 99.306168 hw_loss 0.356436 lr 0.00022004 rank 5
2023-02-17 03:46:01,334 DEBUG TRAIN Batch 0/5500 loss 139.778412 loss_att 213.353241 loss_ctc 142.109680 loss_rnnt 124.581802 hw_loss 0.320262 lr 0.00022004 rank 1
2023-02-17 03:46:01,334 DEBUG TRAIN Batch 0/5500 loss 108.180664 loss_att 179.895706 loss_ctc 110.190628 loss_rnnt 93.362373 hw_loss 0.388645 lr 0.00022004 rank 2
2023-02-17 03:46:01,336 DEBUG TRAIN Batch 0/5500 loss 140.213013 loss_att 242.853394 loss_ctc 135.036560 loss_rnnt 120.169044 hw_loss 0.386422 lr 0.00022004 rank 0
2023-02-17 03:46:01,338 DEBUG TRAIN Batch 0/5500 loss 146.158112 loss_att 237.882065 loss_ctc 145.285156 loss_rnnt 127.710663 hw_loss 0.410673 lr 0.00022004 rank 7
2023-02-17 03:46:01,340 DEBUG TRAIN Batch 0/5500 loss 103.980209 loss_att 198.232513 loss_ctc 99.975822 loss_rnnt 85.451294 hw_loss 0.398205 lr 0.00022004 rank 4
2023-02-17 03:47:18,636 DEBUG TRAIN Batch 0/5600 loss 117.999771 loss_att 209.156830 loss_ctc 115.301682 loss_rnnt 99.918137 hw_loss 0.393696 lr 0.00022404 rank 7
2023-02-17 03:47:18,643 DEBUG TRAIN Batch 0/5600 loss 103.584648 loss_att 168.875793 loss_ctc 99.165855 loss_rnnt 90.904594 hw_loss 0.395619 lr 0.00022404 rank 5
2023-02-17 03:47:18,645 DEBUG TRAIN Batch 0/5600 loss 62.318813 loss_att 111.714050 loss_ctc 62.987907 loss_rnnt 52.102940 hw_loss 0.464273 lr 0.00022404 rank 1
2023-02-17 03:47:18,646 DEBUG TRAIN Batch 0/5600 loss 119.119217 loss_att 222.340714 loss_ctc 120.507980 loss_rnnt 98.082726 hw_loss 0.388174 lr 0.00022404 rank 0
2023-02-17 03:47:18,647 DEBUG TRAIN Batch 0/5600 loss 119.647873 loss_att 201.303329 loss_ctc 115.556786 loss_rnnt 103.618370 hw_loss 0.457289 lr 0.00022404 rank 2
2023-02-17 03:47:18,647 DEBUG TRAIN Batch 0/5600 loss 135.263367 loss_att 225.714325 loss_ctc 140.607727 loss_rnnt 116.239243 hw_loss 0.415051 lr 0.00022404 rank 6
2023-02-17 03:47:18,648 DEBUG TRAIN Batch 0/5600 loss 126.750847 loss_att 213.526642 loss_ctc 119.214767 loss_rnnt 110.220947 hw_loss 0.336641 lr 0.00022404 rank 3
2023-02-17 03:47:18,652 DEBUG TRAIN Batch 0/5600 loss 120.429588 loss_att 208.267349 loss_ctc 121.055573 loss_rnnt 102.579170 hw_loss 0.373853 lr 0.00022404 rank 4
2023-02-17 03:48:39,215 DEBUG TRAIN Batch 0/5700 loss 98.072113 loss_att 185.218018 loss_ctc 95.067406 loss_rnnt 80.855171 hw_loss 0.353248 lr 0.00022804 rank 3
2023-02-17 03:48:39,216 DEBUG TRAIN Batch 0/5700 loss 75.696991 loss_att 140.005905 loss_ctc 73.273544 loss_rnnt 62.979706 hw_loss 0.334931 lr 0.00022804 rank 2
2023-02-17 03:48:39,218 DEBUG TRAIN Batch 0/5700 loss 60.704586 loss_att 97.030266 loss_ctc 59.549843 loss_rnnt 53.342354 hw_loss 0.470747 lr 0.00022804 rank 6
2023-02-17 03:48:39,218 DEBUG TRAIN Batch 0/5700 loss 134.177109 loss_att 234.317184 loss_ctc 132.170471 loss_rnnt 114.221802 hw_loss 0.365330 lr 0.00022804 rank 1
2023-02-17 03:48:39,230 DEBUG TRAIN Batch 0/5700 loss 152.602615 loss_att 209.213516 loss_ctc 163.204819 loss_rnnt 139.680542 hw_loss 0.349236 lr 0.00022804 rank 7
2023-02-17 03:48:39,244 DEBUG TRAIN Batch 0/5700 loss 96.532936 loss_att 178.366638 loss_ctc 93.655090 loss_rnnt 80.315186 hw_loss 0.440111 lr 0.00022804 rank 4
2023-02-17 03:48:39,285 DEBUG TRAIN Batch 0/5700 loss 107.272072 loss_att 168.272888 loss_ctc 110.975090 loss_rnnt 94.372787 hw_loss 0.385091 lr 0.00022804 rank 0
2023-02-17 03:48:39,298 DEBUG TRAIN Batch 0/5700 loss 168.110840 loss_att 285.042511 loss_ctc 181.641159 loss_rnnt 142.743835 hw_loss 0.331180 lr 0.00022804 rank 5
2023-02-17 03:49:55,389 DEBUG TRAIN Batch 0/5800 loss 29.094950 loss_att 40.045677 loss_ctc 29.991886 loss_rnnt 26.482723 hw_loss 0.567170 lr 0.00023204 rank 3
2023-02-17 03:49:55,390 DEBUG TRAIN Batch 0/5800 loss 132.462402 loss_att 248.993774 loss_ctc 129.763397 loss_rnnt 109.320786 hw_loss 0.366034 lr 0.00023204 rank 6
2023-02-17 03:49:55,392 DEBUG TRAIN Batch 0/5800 loss 122.297226 loss_att 228.908630 loss_ctc 119.466751 loss_rnnt 101.158234 hw_loss 0.363937 lr 0.00023204 rank 1
2023-02-17 03:49:55,393 DEBUG TRAIN Batch 0/5800 loss 126.697998 loss_att 213.054245 loss_ctc 126.829247 loss_rnnt 109.226257 hw_loss 0.343104 lr 0.00023204 rank 5
2023-02-17 03:49:55,398 DEBUG TRAIN Batch 0/5800 loss 168.849274 loss_att 283.380737 loss_ctc 177.562317 loss_rnnt 144.618759 hw_loss 0.304658 lr 0.00023204 rank 4
2023-02-17 03:49:55,398 DEBUG TRAIN Batch 0/5800 loss 117.894028 loss_att 262.036072 loss_ctc 110.245514 loss_rnnt 89.943588 hw_loss 0.265933 lr 0.00023204 rank 7
2023-02-17 03:49:55,401 DEBUG TRAIN Batch 0/5800 loss 136.681381 loss_att 245.399170 loss_ctc 133.162872 loss_rnnt 115.234642 hw_loss 0.323101 lr 0.00023204 rank 0
2023-02-17 03:49:55,439 DEBUG TRAIN Batch 0/5800 loss 126.693771 loss_att 250.564804 loss_ctc 122.620331 loss_rnnt 102.298729 hw_loss 0.307428 lr 0.00023204 rank 2
2023-02-17 03:51:13,138 DEBUG TRAIN Batch 0/5900 loss 140.495605 loss_att 249.604980 loss_ctc 144.462738 loss_rnnt 117.895569 hw_loss 0.467276 lr 0.00023604 rank 3
2023-02-17 03:51:13,145 DEBUG TRAIN Batch 0/5900 loss 125.968170 loss_att 226.680573 loss_ctc 122.596649 loss_rnnt 106.068008 hw_loss 0.388522 lr 0.00023604 rank 7
2023-02-17 03:51:13,145 DEBUG TRAIN Batch 0/5900 loss 135.160233 loss_att 245.519165 loss_ctc 129.230499 loss_rnnt 113.678047 hw_loss 0.376938 lr 0.00023604 rank 1
2023-02-17 03:51:13,146 DEBUG TRAIN Batch 0/5900 loss 143.546539 loss_att 259.117981 loss_ctc 138.192780 loss_rnnt 120.928917 hw_loss 0.407186 lr 0.00023604 rank 5
2023-02-17 03:51:13,148 DEBUG TRAIN Batch 0/5900 loss 117.856461 loss_att 233.797974 loss_ctc 117.295860 loss_rnnt 94.517441 hw_loss 0.422741 lr 0.00023604 rank 2
2023-02-17 03:51:13,149 DEBUG TRAIN Batch 0/5900 loss 121.108528 loss_att 238.352951 loss_ctc 112.303032 loss_rnnt 98.659531 hw_loss 0.326598 lr 0.00023604 rank 6
2023-02-17 03:51:13,152 DEBUG TRAIN Batch 0/5900 loss 137.145706 loss_att 269.847107 loss_ctc 133.668671 loss_rnnt 110.939392 hw_loss 0.243091 lr 0.00023604 rank 4
2023-02-17 03:51:13,190 DEBUG TRAIN Batch 0/5900 loss 120.718185 loss_att 253.832153 loss_ctc 114.024315 loss_rnnt 94.774704 hw_loss 0.399746 lr 0.00023604 rank 0
2023-02-17 03:52:31,525 DEBUG TRAIN Batch 0/6000 loss 106.485451 loss_att 216.063339 loss_ctc 103.762589 loss_rnnt 84.733276 hw_loss 0.374312 lr 0.00024004 rank 6
2023-02-17 03:52:31,528 DEBUG TRAIN Batch 0/6000 loss 109.592453 loss_att 206.291306 loss_ctc 105.987350 loss_rnnt 90.537537 hw_loss 0.367175 lr 0.00024004 rank 7
2023-02-17 03:52:31,530 DEBUG TRAIN Batch 0/6000 loss 110.189301 loss_att 248.856232 loss_ctc 103.657898 loss_rnnt 83.104317 hw_loss 0.417087 lr 0.00024004 rank 5
2023-02-17 03:52:31,532 DEBUG TRAIN Batch 0/6000 loss 143.069839 loss_att 247.902405 loss_ctc 141.580139 loss_rnnt 122.126526 hw_loss 0.328940 lr 0.00024004 rank 4
2023-02-17 03:52:31,535 DEBUG TRAIN Batch 0/6000 loss 91.053322 loss_att 212.343292 loss_ctc 86.735611 loss_rnnt 67.199265 hw_loss 0.322035 lr 0.00024004 rank 3
2023-02-17 03:52:31,536 DEBUG TRAIN Batch 0/6000 loss 87.697235 loss_att 171.689072 loss_ctc 82.402321 loss_rnnt 71.440079 hw_loss 0.308957 lr 0.00024004 rank 1
2023-02-17 03:52:31,539 DEBUG TRAIN Batch 0/6000 loss 109.889076 loss_att 206.868286 loss_ctc 105.451736 loss_rnnt 90.880142 hw_loss 0.383872 lr 0.00024004 rank 2
2023-02-17 03:52:31,583 DEBUG TRAIN Batch 0/6000 loss 99.424370 loss_att 192.796768 loss_ctc 94.308136 loss_rnnt 81.236259 hw_loss 0.367106 lr 0.00024004 rank 0
2023-02-17 03:53:50,163 DEBUG TRAIN Batch 0/6100 loss 101.995102 loss_att 200.546173 loss_ctc 101.408401 loss_rnnt 82.169472 hw_loss 0.363076 lr 0.00024404 rank 1
2023-02-17 03:53:50,164 DEBUG TRAIN Batch 0/6100 loss 120.902130 loss_att 213.584747 loss_ctc 129.040771 loss_rnnt 101.079750 hw_loss 0.376319 lr 0.00024404 rank 6
2023-02-17 03:53:50,168 DEBUG TRAIN Batch 0/6100 loss 104.059387 loss_att 211.131104 loss_ctc 102.847458 loss_rnnt 82.539787 hw_loss 0.500329 lr 0.00024404 rank 4
2023-02-17 03:53:50,169 DEBUG TRAIN Batch 0/6100 loss 124.963638 loss_att 229.553009 loss_ctc 122.541771 loss_rnnt 104.209946 hw_loss 0.297626 lr 0.00024404 rank 5
2023-02-17 03:53:50,169 DEBUG TRAIN Batch 0/6100 loss 113.186958 loss_att 219.886917 loss_ctc 112.890060 loss_rnnt 91.688278 hw_loss 0.371768 lr 0.00024404 rank 3
2023-02-17 03:53:50,170 DEBUG TRAIN Batch 0/6100 loss 116.241364 loss_att 221.603088 loss_ctc 115.277557 loss_rnnt 95.130470 hw_loss 0.313228 lr 0.00024404 rank 7
2023-02-17 03:53:50,172 DEBUG TRAIN Batch 0/6100 loss 117.656708 loss_att 240.178772 loss_ctc 112.363037 loss_rnnt 93.634361 hw_loss 0.419532 lr 0.00024404 rank 0
2023-02-17 03:53:50,211 DEBUG TRAIN Batch 0/6100 loss 112.801888 loss_att 231.869781 loss_ctc 106.966675 loss_rnnt 89.559929 hw_loss 0.387019 lr 0.00024404 rank 2
2023-02-17 03:55:04,773 DEBUG TRAIN Batch 0/6200 loss 107.396271 loss_att 194.455750 loss_ctc 108.494644 loss_rnnt 89.612961 hw_loss 0.421807 lr 0.00024804 rank 7
2023-02-17 03:55:04,775 DEBUG TRAIN Batch 0/6200 loss 99.940987 loss_att 203.217407 loss_ctc 96.944382 loss_rnnt 79.473389 hw_loss 0.397238 lr 0.00024804 rank 6
2023-02-17 03:55:04,775 DEBUG TRAIN Batch 0/6200 loss 79.981354 loss_att 172.129410 loss_ctc 76.542725 loss_rnnt 61.833080 hw_loss 0.332155 lr 0.00024804 rank 1
2023-02-17 03:55:04,776 DEBUG TRAIN Batch 0/6200 loss 98.728455 loss_att 201.087891 loss_ctc 97.170898 loss_rnnt 78.271172 hw_loss 0.362018 lr 0.00024804 rank 3
2023-02-17 03:55:04,777 DEBUG TRAIN Batch 0/6200 loss 123.728058 loss_att 235.346222 loss_ctc 119.478371 loss_rnnt 101.762680 hw_loss 0.390700 lr 0.00024804 rank 4
2023-02-17 03:55:04,777 DEBUG TRAIN Batch 0/6200 loss 94.759094 loss_att 166.850220 loss_ctc 95.613403 loss_rnnt 80.023453 hw_loss 0.381566 lr 0.00024804 rank 5
2023-02-17 03:55:04,782 DEBUG TRAIN Batch 0/6200 loss 109.559090 loss_att 210.183685 loss_ctc 111.436806 loss_rnnt 88.999176 hw_loss 0.346167 lr 0.00024804 rank 2
2023-02-17 03:55:04,782 DEBUG TRAIN Batch 0/6200 loss 161.157913 loss_att 262.838196 loss_ctc 168.145325 loss_rnnt 139.703430 hw_loss 0.350188 lr 0.00024804 rank 0
2023-02-17 03:56:21,122 DEBUG TRAIN Batch 0/6300 loss 100.404594 loss_att 205.644104 loss_ctc 98.188477 loss_rnnt 79.454910 hw_loss 0.369863 lr 0.00025204 rank 3
2023-02-17 03:56:21,124 DEBUG TRAIN Batch 0/6300 loss 42.240566 loss_att 70.409500 loss_ctc 40.425743 loss_rnnt 36.571865 hw_loss 0.519175 lr 0.00025204 rank 5
2023-02-17 03:56:21,125 DEBUG TRAIN Batch 0/6300 loss 99.419067 loss_att 187.637634 loss_ctc 97.797272 loss_rnnt 81.766113 hw_loss 0.422791 lr 0.00025204 rank 7
2023-02-17 03:56:21,125 DEBUG TRAIN Batch 0/6300 loss 177.145020 loss_att 299.838684 loss_ctc 183.062561 loss_rnnt 151.617172 hw_loss 0.375228 lr 0.00025204 rank 1
2023-02-17 03:56:21,127 DEBUG TRAIN Batch 0/6300 loss 97.118370 loss_att 181.080093 loss_ctc 97.063103 loss_rnnt 80.146622 hw_loss 0.350196 lr 0.00025204 rank 0
2023-02-17 03:56:21,129 DEBUG TRAIN Batch 0/6300 loss 72.831421 loss_att 147.623627 loss_ctc 72.413795 loss_rnnt 57.729225 hw_loss 0.373933 lr 0.00025204 rank 6
2023-02-17 03:56:21,132 DEBUG TRAIN Batch 0/6300 loss 153.127228 loss_att 238.889847 loss_ctc 162.835236 loss_rnnt 134.472488 hw_loss 0.389627 lr 0.00025204 rank 4
2023-02-17 03:56:21,178 DEBUG TRAIN Batch 0/6300 loss 93.258507 loss_att 166.061966 loss_ctc 95.564903 loss_rnnt 78.151443 hw_loss 0.447837 lr 0.00025204 rank 2
2023-02-17 03:57:41,365 DEBUG TRAIN Batch 0/6400 loss 81.903702 loss_att 149.783722 loss_ctc 81.414047 loss_rnnt 68.160110 hw_loss 0.436632 lr 0.00025604 rank 7
2023-02-17 03:57:41,366 DEBUG TRAIN Batch 0/6400 loss 142.310272 loss_att 268.239441 loss_ctc 140.538177 loss_rnnt 117.116600 hw_loss 0.457727 lr 0.00025604 rank 6
2023-02-17 03:57:41,367 DEBUG TRAIN Batch 0/6400 loss 91.371552 loss_att 150.479568 loss_ctc 93.875946 loss_rnnt 78.925491 hw_loss 0.544749 lr 0.00025604 rank 3
2023-02-17 03:57:41,370 DEBUG TRAIN Batch 0/6400 loss 130.418045 loss_att 244.293243 loss_ctc 128.455887 loss_rnnt 107.732140 hw_loss 0.323406 lr 0.00025604 rank 1
2023-02-17 03:57:41,371 DEBUG TRAIN Batch 0/6400 loss 61.073936 loss_att 102.701813 loss_ctc 63.514561 loss_rnnt 52.204884 hw_loss 0.408858 lr 0.00025604 rank 0
2023-02-17 03:57:41,373 DEBUG TRAIN Batch 0/6400 loss 129.536392 loss_att 236.669662 loss_ctc 125.623405 loss_rnnt 108.358994 hw_loss 0.510909 lr 0.00025604 rank 5
2023-02-17 03:57:41,379 DEBUG TRAIN Batch 0/6400 loss 43.711483 loss_att 62.363785 loss_ctc 46.137108 loss_rnnt 39.352417 hw_loss 0.572221 lr 0.00025604 rank 2
2023-02-17 03:57:41,379 DEBUG TRAIN Batch 0/6400 loss 82.095444 loss_att 131.444748 loss_ctc 84.852631 loss_rnnt 71.664650 hw_loss 0.362442 lr 0.00025604 rank 4
2023-02-17 03:58:57,568 DEBUG TRAIN Batch 0/6500 loss 106.241974 loss_att 222.566589 loss_ctc 96.987907 loss_rnnt 83.999954 hw_loss 0.395593 lr 0.00026004 rank 6
2023-02-17 03:58:57,573 DEBUG TRAIN Batch 0/6500 loss 135.244217 loss_att 245.459457 loss_ctc 134.865341 loss_rnnt 112.995529 hw_loss 0.480314 lr 0.00026004 rank 1
2023-02-17 03:58:57,574 DEBUG TRAIN Batch 0/6500 loss 95.229332 loss_att 205.328629 loss_ctc 89.622795 loss_rnnt 73.793358 hw_loss 0.306857 lr 0.00026004 rank 2
2023-02-17 03:58:57,578 DEBUG TRAIN Batch 0/6500 loss 131.301102 loss_att 264.310669 loss_ctc 130.911377 loss_rnnt 104.557098 hw_loss 0.363856 lr 0.00026004 rank 5
2023-02-17 03:58:57,578 DEBUG TRAIN Batch 0/6500 loss 116.033951 loss_att 242.373566 loss_ctc 105.145683 loss_rnnt 92.014671 hw_loss 0.380861 lr 0.00026004 rank 7
2023-02-17 03:58:57,580 DEBUG TRAIN Batch 0/6500 loss 92.933182 loss_att 204.791626 loss_ctc 85.390343 loss_rnnt 71.384476 hw_loss 0.342618 lr 0.00026004 rank 0
2023-02-17 03:58:57,580 DEBUG TRAIN Batch 0/6500 loss 124.210403 loss_att 244.722900 loss_ctc 118.931389 loss_rnnt 100.581688 hw_loss 0.431403 lr 0.00026004 rank 4
2023-02-17 03:58:57,623 DEBUG TRAIN Batch 0/6500 loss 168.549240 loss_att 312.793213 loss_ctc 169.440247 loss_rnnt 139.342682 hw_loss 0.448022 lr 0.00026004 rank 3
2023-02-17 04:00:13,068 DEBUG TRAIN Batch 0/6600 loss 123.994194 loss_att 219.251450 loss_ctc 120.221146 loss_rnnt 105.249290 hw_loss 0.368474 lr 0.00026404 rank 7
2023-02-17 04:00:13,070 DEBUG TRAIN Batch 0/6600 loss 110.245628 loss_att 232.227234 loss_ctc 101.527000 loss_rnnt 86.786682 hw_loss 0.422074 lr 0.00026404 rank 1
2023-02-17 04:00:13,072 DEBUG TRAIN Batch 0/6600 loss 122.561852 loss_att 251.751465 loss_ctc 120.995819 loss_rnnt 96.688644 hw_loss 0.457667 lr 0.00026404 rank 4
2023-02-17 04:00:13,072 DEBUG TRAIN Batch 0/6600 loss 96.904808 loss_att 207.000977 loss_ctc 104.662903 loss_rnnt 73.678650 hw_loss 0.323449 lr 0.00026404 rank 6
2023-02-17 04:00:13,072 DEBUG TRAIN Batch 0/6600 loss 100.584602 loss_att 216.718170 loss_ctc 96.096794 loss_rnnt 77.769257 hw_loss 0.350625 lr 0.00026404 rank 5
2023-02-17 04:00:13,075 DEBUG TRAIN Batch 0/6600 loss 127.178360 loss_att 203.801636 loss_ctc 137.504501 loss_rnnt 110.218590 hw_loss 0.484323 lr 0.00026404 rank 0
2023-02-17 04:00:13,076 DEBUG TRAIN Batch 0/6600 loss 149.870544 loss_att 258.321838 loss_ctc 143.488831 loss_rnnt 128.858459 hw_loss 0.323883 lr 0.00026404 rank 2
2023-02-17 04:00:13,122 DEBUG TRAIN Batch 0/6600 loss 120.603050 loss_att 226.127228 loss_ctc 122.417725 loss_rnnt 98.996941 hw_loss 0.486197 lr 0.00026404 rank 3
2023-02-17 04:01:31,036 DEBUG TRAIN Batch 0/6700 loss 101.523865 loss_att 206.447327 loss_ctc 99.497040 loss_rnnt 80.604446 hw_loss 0.384317 lr 0.00026804 rank 3
2023-02-17 04:01:31,038 DEBUG TRAIN Batch 0/6700 loss 126.674652 loss_att 237.314697 loss_ctc 134.584259 loss_rnnt 103.305832 hw_loss 0.349113 lr 0.00026804 rank 7
2023-02-17 04:01:31,042 DEBUG TRAIN Batch 0/6700 loss 122.449165 loss_att 231.561066 loss_ctc 119.641609 loss_rnnt 100.805801 hw_loss 0.366224 lr 0.00026804 rank 2
2023-02-17 04:01:31,042 DEBUG TRAIN Batch 0/6700 loss 116.325264 loss_att 213.853271 loss_ctc 127.101402 loss_rnnt 95.151566 hw_loss 0.433632 lr 0.00026804 rank 1
2023-02-17 04:01:31,042 DEBUG TRAIN Batch 0/6700 loss 147.613953 loss_att 254.057281 loss_ctc 149.092773 loss_rnnt 125.925995 hw_loss 0.378969 lr 0.00026804 rank 6
2023-02-17 04:01:31,043 DEBUG TRAIN Batch 0/6700 loss 117.176971 loss_att 217.699005 loss_ctc 113.154709 loss_rnnt 97.404083 hw_loss 0.383974 lr 0.00026804 rank 0
2023-02-17 04:01:31,044 DEBUG TRAIN Batch 0/6700 loss 95.883514 loss_att 211.564697 loss_ctc 88.914337 loss_rnnt 73.470673 hw_loss 0.385931 lr 0.00026804 rank 4
2023-02-17 04:01:31,044 DEBUG TRAIN Batch 0/6700 loss 118.045914 loss_att 216.993622 loss_ctc 121.766258 loss_rnnt 97.555130 hw_loss 0.384722 lr 0.00026804 rank 5
2023-02-17 04:02:50,262 DEBUG TRAIN Batch 0/6800 loss 97.422302 loss_att 190.176498 loss_ctc 96.654175 loss_rnnt 78.798386 hw_loss 0.329051 lr 0.00027204 rank 6
2023-02-17 04:02:50,265 DEBUG TRAIN Batch 0/6800 loss 91.401527 loss_att 178.544006 loss_ctc 97.169388 loss_rnnt 72.959099 hw_loss 0.459159 lr 0.00027204 rank 5
2023-02-17 04:02:50,266 DEBUG TRAIN Batch 0/6800 loss 124.186623 loss_att 239.936005 loss_ctc 118.800697 loss_rnnt 101.534851 hw_loss 0.412532 lr 0.00027204 rank 3
2023-02-17 04:02:50,267 DEBUG TRAIN Batch 0/6800 loss 101.728218 loss_att 216.416229 loss_ctc 99.700882 loss_rnnt 78.831367 hw_loss 0.430439 lr 0.00027204 rank 4
2023-02-17 04:02:50,267 DEBUG TRAIN Batch 0/6800 loss 119.405609 loss_att 231.008606 loss_ctc 117.200882 loss_rnnt 97.199760 hw_loss 0.336044 lr 0.00027204 rank 2
2023-02-17 04:02:50,267 DEBUG TRAIN Batch 0/6800 loss 86.674461 loss_att 170.944153 loss_ctc 94.014252 loss_rnnt 68.620720 hw_loss 0.414678 lr 0.00027204 rank 0
2023-02-17 04:02:50,295 DEBUG TRAIN Batch 0/6800 loss 97.439659 loss_att 184.548462 loss_ctc 99.167519 loss_rnnt 79.524796 hw_loss 0.492607 lr 0.00027204 rank 1
2023-02-17 04:02:50,299 DEBUG TRAIN Batch 0/6800 loss 88.151085 loss_att 202.046661 loss_ctc 91.777351 loss_rnnt 64.705200 hw_loss 0.343621 lr 0.00027204 rank 7
2023-02-17 04:04:06,233 DEBUG TRAIN Batch 0/6900 loss 97.492882 loss_att 199.148560 loss_ctc 88.689461 loss_rnnt 78.094826 hw_loss 0.451343 lr 0.00027604 rank 7
2023-02-17 04:04:06,235 DEBUG TRAIN Batch 0/6900 loss 116.312767 loss_att 196.454590 loss_ctc 120.682266 loss_rnnt 99.480179 hw_loss 0.415518 lr 0.00027604 rank 3
2023-02-17 04:04:06,237 DEBUG TRAIN Batch 0/6900 loss 100.390762 loss_att 192.616257 loss_ctc 105.378204 loss_rnnt 81.077286 hw_loss 0.381342 lr 0.00027604 rank 2
2023-02-17 04:04:06,239 DEBUG TRAIN Batch 0/6900 loss 127.740715 loss_att 225.017242 loss_ctc 135.840363 loss_rnnt 106.977875 hw_loss 0.426705 lr 0.00027604 rank 4
2023-02-17 04:04:06,239 DEBUG TRAIN Batch 0/6900 loss 129.099609 loss_att 213.099213 loss_ctc 138.021286 loss_rnnt 110.917511 hw_loss 0.361127 lr 0.00027604 rank 6
2023-02-17 04:04:06,240 DEBUG TRAIN Batch 0/6900 loss 114.843155 loss_att 202.526520 loss_ctc 122.330788 loss_rnnt 96.108788 hw_loss 0.373771 lr 0.00027604 rank 5
2023-02-17 04:04:06,241 DEBUG TRAIN Batch 0/6900 loss 85.727585 loss_att 192.846725 loss_ctc 81.111298 loss_rnnt 64.743790 hw_loss 0.329020 lr 0.00027604 rank 1
2023-02-17 04:04:06,245 DEBUG TRAIN Batch 0/6900 loss 94.267044 loss_att 198.968674 loss_ctc 93.111305 loss_rnnt 73.248825 hw_loss 0.434999 lr 0.00027604 rank 0
2023-02-17 04:05:22,835 DEBUG TRAIN Batch 0/7000 loss 120.700462 loss_att 205.014191 loss_ctc 123.461929 loss_rnnt 103.240761 hw_loss 0.428932 lr 0.00028004 rank 4
2023-02-17 04:05:22,838 DEBUG TRAIN Batch 0/7000 loss 50.645481 loss_att 100.683289 loss_ctc 50.361515 loss_rnnt 40.439800 hw_loss 0.442463 lr 0.00028004 rank 6
2023-02-17 04:05:22,838 DEBUG TRAIN Batch 0/7000 loss 118.091782 loss_att 225.235168 loss_ctc 111.736290 loss_rnnt 97.338547 hw_loss 0.322419 lr 0.00028004 rank 1
2023-02-17 04:05:22,839 DEBUG TRAIN Batch 0/7000 loss 53.259136 loss_att 95.150810 loss_ctc 52.412613 loss_rnnt 44.755775 hw_loss 0.446052 lr 0.00028004 rank 2
2023-02-17 04:05:22,841 DEBUG TRAIN Batch 0/7000 loss 51.564899 loss_att 105.944458 loss_ctc 49.684906 loss_rnnt 40.716610 hw_loss 0.418204 lr 0.00028004 rank 7
2023-02-17 04:05:22,842 DEBUG TRAIN Batch 0/7000 loss 38.085506 loss_att 68.655853 loss_ctc 38.142952 loss_rnnt 31.670593 hw_loss 0.549727 lr 0.00028004 rank 5
2023-02-17 04:05:22,842 DEBUG TRAIN Batch 0/7000 loss 93.471352 loss_att 187.336060 loss_ctc 88.957542 loss_rnnt 75.085228 hw_loss 0.403174 lr 0.00028004 rank 3
2023-02-17 04:05:22,846 DEBUG TRAIN Batch 0/7000 loss 59.917347 loss_att 106.990906 loss_ctc 62.986526 loss_rnnt 49.875347 hw_loss 0.408864 lr 0.00028004 rank 0
2023-02-17 04:06:42,778 DEBUG TRAIN Batch 0/7100 loss 100.229614 loss_att 237.510818 loss_ctc 89.219803 loss_rnnt 74.052704 hw_loss 0.353705 lr 0.00028404 rank 6
2023-02-17 04:06:42,780 DEBUG TRAIN Batch 0/7100 loss 80.893692 loss_att 183.811081 loss_ctc 86.148758 loss_rnnt 59.402679 hw_loss 0.387852 lr 0.00028404 rank 1
2023-02-17 04:06:42,781 DEBUG TRAIN Batch 0/7100 loss 102.095436 loss_att 219.970398 loss_ctc 93.769829 loss_rnnt 79.483246 hw_loss 0.276152 lr 0.00028404 rank 2
2023-02-17 04:06:42,781 DEBUG TRAIN Batch 0/7100 loss 113.082138 loss_att 222.828796 loss_ctc 108.447556 loss_rnnt 91.569244 hw_loss 0.340313 lr 0.00028404 rank 7
2023-02-17 04:06:42,782 DEBUG TRAIN Batch 0/7100 loss 86.289764 loss_att 221.914154 loss_ctc 77.844788 loss_rnnt 60.134972 hw_loss 0.292348 lr 0.00028404 rank 3
2023-02-17 04:06:42,784 DEBUG TRAIN Batch 0/7100 loss 87.852066 loss_att 193.887741 loss_ctc 82.083725 loss_rnnt 67.229965 hw_loss 0.345132 lr 0.00028404 rank 5
2023-02-17 04:06:42,786 DEBUG TRAIN Batch 0/7100 loss 110.294640 loss_att 215.997253 loss_ctc 105.229271 loss_rnnt 89.655273 hw_loss 0.326670 lr 0.00028404 rank 0
2023-02-17 04:06:42,787 DEBUG TRAIN Batch 0/7100 loss 35.004639 loss_att 59.031891 loss_ctc 37.890079 loss_rnnt 29.475351 hw_loss 0.635836 lr 0.00028404 rank 4
2023-02-17 04:07:59,225 DEBUG TRAIN Batch 0/7200 loss 119.244232 loss_att 223.242249 loss_ctc 128.037247 loss_rnnt 97.051903 hw_loss 0.413095 lr 0.00028804 rank 4
2023-02-17 04:07:59,228 DEBUG TRAIN Batch 0/7200 loss 115.471146 loss_att 259.359344 loss_ctc 98.165306 loss_rnnt 88.842697 hw_loss 0.296743 lr 0.00028804 rank 5
2023-02-17 04:07:59,229 DEBUG TRAIN Batch 0/7200 loss 154.084167 loss_att 275.159973 loss_ctc 158.052582 loss_rnnt 129.151825 hw_loss 0.352585 lr 0.00028804 rank 6
2023-02-17 04:07:59,233 DEBUG TRAIN Batch 0/7200 loss 89.769180 loss_att 180.075684 loss_ctc 88.409332 loss_rnnt 71.629700 hw_loss 0.486546 lr 0.00028804 rank 3
2023-02-17 04:07:59,234 DEBUG TRAIN Batch 0/7200 loss 117.813538 loss_att 230.634216 loss_ctc 126.507256 loss_rnnt 93.973900 hw_loss 0.218122 lr 0.00028804 rank 7
2023-02-17 04:07:59,235 DEBUG TRAIN Batch 0/7200 loss 117.088844 loss_att 231.191696 loss_ctc 116.973877 loss_rnnt 94.072014 hw_loss 0.396722 lr 0.00028804 rank 1
2023-02-17 04:07:59,237 DEBUG TRAIN Batch 0/7200 loss 142.347900 loss_att 226.887024 loss_ctc 150.577866 loss_rnnt 124.110855 hw_loss 0.434817 lr 0.00028804 rank 2
2023-02-17 04:07:59,284 DEBUG TRAIN Batch 0/7200 loss 90.803894 loss_att 196.137222 loss_ctc 84.190552 loss_rnnt 70.387634 hw_loss 0.433826 lr 0.00028804 rank 0
2023-02-17 04:09:18,467 DEBUG TRAIN Batch 0/7300 loss 105.909492 loss_att 231.794632 loss_ctc 105.294754 loss_rnnt 80.628754 hw_loss 0.348148 lr 0.00029204 rank 6
2023-02-17 04:09:18,470 DEBUG TRAIN Batch 0/7300 loss 109.549568 loss_att 187.678864 loss_ctc 117.096428 loss_rnnt 92.678116 hw_loss 0.448760 lr 0.00029204 rank 4
2023-02-17 04:09:18,470 DEBUG TRAIN Batch 0/7300 loss 121.188179 loss_att 238.855637 loss_ctc 132.945572 loss_rnnt 95.959869 hw_loss 0.238434 lr 0.00029204 rank 5
2023-02-17 04:09:18,471 DEBUG TRAIN Batch 0/7300 loss 81.242226 loss_att 199.462982 loss_ctc 74.428307 loss_rnnt 58.336266 hw_loss 0.319379 lr 0.00029204 rank 7
2023-02-17 04:09:18,472 DEBUG TRAIN Batch 0/7300 loss 92.094002 loss_att 205.374527 loss_ctc 88.018654 loss_rnnt 69.770538 hw_loss 0.395134 lr 0.00029204 rank 1
2023-02-17 04:09:18,474 DEBUG TRAIN Batch 0/7300 loss 118.581627 loss_att 242.833633 loss_ctc 124.091148 loss_rnnt 92.824638 hw_loss 0.322476 lr 0.00029204 rank 0
2023-02-17 04:09:18,474 DEBUG TRAIN Batch 0/7300 loss 95.973915 loss_att 229.438934 loss_ctc 95.685883 loss_rnnt 69.139763 hw_loss 0.336666 lr 0.00029204 rank 3
2023-02-17 04:09:18,522 DEBUG TRAIN Batch 0/7300 loss 131.322739 loss_att 252.014282 loss_ctc 140.814423 loss_rnnt 105.730202 hw_loss 0.353754 lr 0.00029204 rank 2
2023-02-17 04:10:36,199 DEBUG TRAIN Batch 0/7400 loss 79.537178 loss_att 178.327942 loss_ctc 76.618011 loss_rnnt 59.972969 hw_loss 0.366140 lr 0.00029604 rank 6
2023-02-17 04:10:36,202 DEBUG TRAIN Batch 0/7400 loss 118.624077 loss_att 240.396530 loss_ctc 122.422783 loss_rnnt 93.594612 hw_loss 0.315868 lr 0.00029604 rank 3
2023-02-17 04:10:36,203 DEBUG TRAIN Batch 0/7400 loss 102.808258 loss_att 202.087250 loss_ctc 103.822548 loss_rnnt 82.609444 hw_loss 0.389572 lr 0.00029604 rank 2
2023-02-17 04:10:36,204 DEBUG TRAIN Batch 0/7400 loss 90.326813 loss_att 186.904236 loss_ctc 85.352119 loss_rnnt 71.491966 hw_loss 0.342483 lr 0.00029604 rank 7
2023-02-17 04:10:36,204 DEBUG TRAIN Batch 0/7400 loss 115.622871 loss_att 207.898849 loss_ctc 115.964233 loss_rnnt 96.921112 hw_loss 0.376964 lr 0.00029604 rank 0
2023-02-17 04:10:36,205 DEBUG TRAIN Batch 0/7400 loss 94.685005 loss_att 199.784454 loss_ctc 93.366882 loss_rnnt 73.647095 hw_loss 0.363329 lr 0.00029604 rank 1
2023-02-17 04:10:36,207 DEBUG TRAIN Batch 0/7400 loss 77.154938 loss_att 178.709320 loss_ctc 76.198708 loss_rnnt 56.797279 hw_loss 0.326767 lr 0.00029604 rank 4
2023-02-17 04:10:36,210 DEBUG TRAIN Batch 0/7400 loss 82.606392 loss_att 184.860306 loss_ctc 80.555359 loss_rnnt 62.255821 hw_loss 0.324865 lr 0.00029604 rank 5
2023-02-17 04:11:55,686 DEBUG TRAIN Batch 0/7500 loss 83.228569 loss_att 187.947067 loss_ctc 74.594513 loss_rnnt 63.216019 hw_loss 0.412616 lr 0.00030004 rank 6
2023-02-17 04:11:55,688 DEBUG TRAIN Batch 0/7500 loss 94.794128 loss_att 190.806427 loss_ctc 92.944138 loss_rnnt 75.604958 hw_loss 0.437581 lr 0.00030004 rank 5
2023-02-17 04:11:55,688 DEBUG TRAIN Batch 0/7500 loss 77.614952 loss_att 167.153503 loss_ctc 77.413498 loss_rnnt 59.539688 hw_loss 0.364534 lr 0.00030004 rank 7
2023-02-17 04:11:55,689 DEBUG TRAIN Batch 0/7500 loss 79.321114 loss_att 183.458038 loss_ctc 79.117813 loss_rnnt 58.329659 hw_loss 0.358447 lr 0.00030004 rank 3
2023-02-17 04:11:55,691 DEBUG TRAIN Batch 0/7500 loss 126.927902 loss_att 212.250000 loss_ctc 130.600449 loss_rnnt 109.179993 hw_loss 0.363398 lr 0.00030004 rank 0
2023-02-17 04:11:55,694 DEBUG TRAIN Batch 0/7500 loss 101.370201 loss_att 205.684235 loss_ctc 99.146957 loss_rnnt 80.620117 hw_loss 0.344450 lr 0.00030004 rank 4
2023-02-17 04:11:55,698 DEBUG TRAIN Batch 0/7500 loss 83.722076 loss_att 186.127808 loss_ctc 84.944473 loss_rnnt 62.885559 hw_loss 0.360724 lr 0.00030004 rank 2
2023-02-17 04:11:55,699 DEBUG TRAIN Batch 0/7500 loss 68.916779 loss_att 129.019806 loss_ctc 70.161423 loss_rnnt 56.527283 hw_loss 0.380497 lr 0.00030004 rank 1
2023-02-17 04:13:12,856 DEBUG TRAIN Batch 0/7600 loss 84.575775 loss_att 156.194977 loss_ctc 90.812859 loss_rnnt 69.193550 hw_loss 0.425192 lr 0.00030404 rank 6
2023-02-17 04:13:12,857 DEBUG TRAIN Batch 0/7600 loss 68.393044 loss_att 157.172623 loss_ctc 63.097065 loss_rnnt 51.085197 hw_loss 0.483884 lr 0.00030404 rank 7
2023-02-17 04:13:12,858 DEBUG TRAIN Batch 0/7600 loss 88.313339 loss_att 156.707596 loss_ctc 92.533836 loss_rnnt 73.850540 hw_loss 0.414770 lr 0.00030404 rank 5
2023-02-17 04:13:12,858 DEBUG TRAIN Batch 0/7600 loss 63.044720 loss_att 140.260513 loss_ctc 59.062878 loss_rnnt 47.898849 hw_loss 0.438043 lr 0.00030404 rank 4
2023-02-17 04:13:12,860 DEBUG TRAIN Batch 0/7600 loss 79.255302 loss_att 151.111237 loss_ctc 76.298401 loss_rnnt 65.087784 hw_loss 0.357336 lr 0.00030404 rank 2
2023-02-17 04:13:12,862 DEBUG TRAIN Batch 0/7600 loss 85.260399 loss_att 203.986130 loss_ctc 81.735130 loss_rnnt 61.802898 hw_loss 0.341976 lr 0.00030404 rank 1
2023-02-17 04:13:12,914 DEBUG TRAIN Batch 0/7600 loss 70.611687 loss_att 154.876526 loss_ctc 73.154442 loss_rnnt 53.193764 hw_loss 0.423607 lr 0.00030404 rank 0
2023-02-17 04:13:12,932 DEBUG TRAIN Batch 0/7600 loss 76.895317 loss_att 156.463867 loss_ctc 77.211266 loss_rnnt 60.753551 hw_loss 0.348598 lr 0.00030404 rank 3
2023-02-17 04:14:30,545 DEBUG TRAIN Batch 0/7700 loss 88.918785 loss_att 219.338135 loss_ctc 81.891235 loss_rnnt 63.562740 hw_loss 0.392206 lr 0.00030804 rank 7
2023-02-17 04:14:30,545 DEBUG TRAIN Batch 0/7700 loss 119.853096 loss_att 228.511597 loss_ctc 131.505157 loss_rnnt 96.396324 hw_loss 0.321497 lr 0.00030804 rank 1
2023-02-17 04:14:30,546 DEBUG TRAIN Batch 0/7700 loss 55.551826 loss_att 116.660522 loss_ctc 56.319580 loss_rnnt 42.958977 hw_loss 0.503905 lr 0.00030804 rank 4
2023-02-17 04:14:30,547 DEBUG TRAIN Batch 0/7700 loss 89.373405 loss_att 214.917175 loss_ctc 87.625290 loss_rnnt 64.274597 hw_loss 0.418380 lr 0.00030804 rank 6
2023-02-17 04:14:30,549 DEBUG TRAIN Batch 0/7700 loss 77.849495 loss_att 189.625610 loss_ctc 74.478500 loss_rnnt 55.767746 hw_loss 0.329994 lr 0.00030804 rank 5
2023-02-17 04:14:30,550 DEBUG TRAIN Batch 0/7700 loss 51.695107 loss_att 97.118782 loss_ctc 52.277855 loss_rnnt 42.356236 hw_loss 0.330807 lr 0.00030804 rank 3
2023-02-17 04:14:30,559 DEBUG TRAIN Batch 0/7700 loss 122.166298 loss_att 237.168015 loss_ctc 138.614059 loss_rnnt 96.798615 hw_loss 0.326823 lr 0.00030804 rank 0
2023-02-17 04:14:30,598 DEBUG TRAIN Batch 0/7700 loss 83.138710 loss_att 182.795532 loss_ctc 79.197800 loss_rnnt 63.526390 hw_loss 0.387026 lr 0.00030804 rank 2
2023-02-17 04:15:49,109 DEBUG TRAIN Batch 0/7800 loss 86.606674 loss_att 204.661377 loss_ctc 83.624565 loss_rnnt 63.202423 hw_loss 0.357976 lr 0.00031204 rank 4
2023-02-17 04:15:49,111 DEBUG TRAIN Batch 0/7800 loss 59.786354 loss_att 170.232086 loss_ctc 55.836609 loss_rnnt 38.035912 hw_loss 0.352359 lr 0.00031204 rank 6
2023-02-17 04:15:49,113 DEBUG TRAIN Batch 0/7800 loss 103.916351 loss_att 212.328430 loss_ctc 105.235191 loss_rnnt 81.872742 hw_loss 0.347511 lr 0.00031204 rank 3
2023-02-17 04:15:49,114 DEBUG TRAIN Batch 0/7800 loss 90.804977 loss_att 186.683441 loss_ctc 95.924576 loss_rnnt 70.761803 hw_loss 0.346645 lr 0.00031204 rank 1
2023-02-17 04:15:49,115 DEBUG TRAIN Batch 0/7800 loss 84.335747 loss_att 192.213959 loss_ctc 83.529099 loss_rnnt 62.698692 hw_loss 0.316806 lr 0.00031204 rank 7
2023-02-17 04:15:49,116 DEBUG TRAIN Batch 0/7800 loss 92.843483 loss_att 208.500397 loss_ctc 87.444977 loss_rnnt 70.273132 hw_loss 0.297688 lr 0.00031204 rank 5
2023-02-17 04:15:49,117 DEBUG TRAIN Batch 0/7800 loss 101.391365 loss_att 210.194504 loss_ctc 108.806274 loss_rnnt 78.452927 hw_loss 0.354675 lr 0.00031204 rank 0
2023-02-17 04:15:49,121 DEBUG TRAIN Batch 0/7800 loss 83.191849 loss_att 213.447296 loss_ctc 82.651398 loss_rnnt 57.034149 hw_loss 0.335014 lr 0.00031204 rank 2
2023-02-17 04:17:06,297 DEBUG TRAIN Batch 0/7900 loss 106.321777 loss_att 201.380219 loss_ctc 109.387375 loss_rnnt 86.659988 hw_loss 0.452542 lr 0.00031604 rank 6
2023-02-17 04:17:06,299 DEBUG TRAIN Batch 0/7900 loss 82.296066 loss_att 181.563477 loss_ctc 82.778275 loss_rnnt 62.194740 hw_loss 0.344139 lr 0.00031604 rank 1
2023-02-17 04:17:06,302 DEBUG TRAIN Batch 0/7900 loss 108.278908 loss_att 234.448517 loss_ctc 110.822952 loss_rnnt 82.490356 hw_loss 0.403918 lr 0.00031604 rank 7
2023-02-17 04:17:06,304 DEBUG TRAIN Batch 0/7900 loss 70.130302 loss_att 163.877701 loss_ctc 72.338272 loss_rnnt 50.817375 hw_loss 0.504467 lr 0.00031604 rank 5
2023-02-17 04:17:06,306 DEBUG TRAIN Batch 0/7900 loss 122.181274 loss_att 226.920135 loss_ctc 135.492523 loss_rnnt 99.271118 hw_loss 0.351643 lr 0.00031604 rank 3
2023-02-17 04:17:06,308 DEBUG TRAIN Batch 0/7900 loss 78.812424 loss_att 192.586594 loss_ctc 78.964935 loss_rnnt 55.848030 hw_loss 0.354777 lr 0.00031604 rank 0
2023-02-17 04:17:06,308 DEBUG TRAIN Batch 0/7900 loss 100.009453 loss_att 215.001556 loss_ctc 103.350090 loss_rnnt 76.401062 hw_loss 0.308526 lr 0.00031604 rank 4
2023-02-17 04:17:06,308 DEBUG TRAIN Batch 0/7900 loss 91.667305 loss_att 205.274445 loss_ctc 92.404175 loss_rnnt 68.595406 hw_loss 0.472902 lr 0.00031604 rank 2
2023-02-17 04:18:22,746 DEBUG TRAIN Batch 0/8000 loss 64.888176 loss_att 172.055191 loss_ctc 60.059631 loss_rnnt 43.923885 hw_loss 0.327556 lr 0.00032004 rank 5
2023-02-17 04:18:22,748 DEBUG TRAIN Batch 0/8000 loss 95.170860 loss_att 199.348419 loss_ctc 101.255051 loss_rnnt 73.305801 hw_loss 0.409341 lr 0.00032004 rank 4
2023-02-17 04:18:22,749 DEBUG TRAIN Batch 0/8000 loss 112.122948 loss_att 225.098633 loss_ctc 121.887825 loss_rnnt 88.051537 hw_loss 0.326786 lr 0.00032004 rank 6
2023-02-17 04:18:22,750 DEBUG TRAIN Batch 0/8000 loss 62.664322 loss_att 156.346497 loss_ctc 60.905003 loss_rnnt 43.987064 hw_loss 0.328868 lr 0.00032004 rank 1
2023-02-17 04:18:22,752 DEBUG TRAIN Batch 0/8000 loss 97.313538 loss_att 188.794189 loss_ctc 103.777611 loss_rnnt 77.974365 hw_loss 0.339677 lr 0.00032004 rank 7
2023-02-17 04:18:22,755 DEBUG TRAIN Batch 0/8000 loss 100.628616 loss_att 202.138412 loss_ctc 101.800003 loss_rnnt 79.996399 hw_loss 0.326386 lr 0.00032004 rank 3
2023-02-17 04:18:22,758 DEBUG TRAIN Batch 0/8000 loss 74.511551 loss_att 181.362823 loss_ctc 77.304794 loss_rnnt 52.584892 hw_loss 0.344948 lr 0.00032004 rank 0
2023-02-17 04:18:22,758 DEBUG TRAIN Batch 0/8000 loss 89.096291 loss_att 184.267044 loss_ctc 94.026604 loss_rnnt 69.187073 hw_loss 0.408181 lr 0.00032004 rank 2
2023-02-17 04:19:39,935 DEBUG TRAIN Batch 0/8100 loss 73.507042 loss_att 174.092377 loss_ctc 71.506287 loss_rnnt 53.452919 hw_loss 0.382180 lr 0.00032404 rank 6
2023-02-17 04:19:39,936 DEBUG TRAIN Batch 0/8100 loss 113.844078 loss_att 231.715729 loss_ctc 120.396049 loss_rnnt 89.200592 hw_loss 0.366673 lr 0.00032404 rank 4
2023-02-17 04:19:39,936 DEBUG TRAIN Batch 0/8100 loss 77.940620 loss_att 145.054047 loss_ctc 87.062317 loss_rnnt 63.084126 hw_loss 0.407965 lr 0.00032404 rank 5
2023-02-17 04:19:39,940 DEBUG TRAIN Batch 0/8100 loss 57.837456 loss_att 94.616333 loss_ctc 60.763950 loss_rnnt 49.860706 hw_loss 0.432697 lr 0.00032404 rank 1
2023-02-17 04:19:39,941 DEBUG TRAIN Batch 0/8100 loss 76.582581 loss_att 165.628387 loss_ctc 79.847191 loss_rnnt 58.157894 hw_loss 0.337956 lr 0.00032404 rank 3
2023-02-17 04:19:39,944 DEBUG TRAIN Batch 0/8100 loss 78.733826 loss_att 174.421768 loss_ctc 76.098412 loss_rnnt 59.732056 hw_loss 0.404184 lr 0.00032404 rank 2
2023-02-17 04:19:39,945 DEBUG TRAIN Batch 0/8100 loss 93.809395 loss_att 194.689011 loss_ctc 94.898254 loss_rnnt 73.287071 hw_loss 0.377302 lr 0.00032404 rank 0
2023-02-17 04:19:39,950 DEBUG TRAIN Batch 0/8100 loss 92.751236 loss_att 185.614120 loss_ctc 90.767647 loss_rnnt 74.305534 hw_loss 0.258013 lr 0.00032404 rank 7
2023-02-17 04:20:56,974 DEBUG TRAIN Batch 0/8200 loss 85.409988 loss_att 155.413239 loss_ctc 91.434227 loss_rnnt 70.378815 hw_loss 0.426167 lr 0.00032804 rank 6
2023-02-17 04:20:56,973 DEBUG TRAIN Batch 0/8200 loss 72.549072 loss_att 161.624573 loss_ctc 77.063004 loss_rnnt 53.861595 hw_loss 0.507239 lr 0.00032804 rank 3
2023-02-17 04:20:56,980 DEBUG TRAIN Batch 0/8200 loss 115.289391 loss_att 206.290070 loss_ctc 123.413422 loss_rnnt 95.785477 hw_loss 0.413565 lr 0.00032804 rank 7
2023-02-17 04:20:56,981 DEBUG TRAIN Batch 0/8200 loss 94.810562 loss_att 209.741638 loss_ctc 95.404099 loss_rnnt 71.593979 hw_loss 0.283553 lr 0.00032804 rank 1
2023-02-17 04:20:56,981 DEBUG TRAIN Batch 0/8200 loss 87.002014 loss_att 178.463821 loss_ctc 86.574692 loss_rnnt 68.577576 hw_loss 0.354460 lr 0.00032804 rank 4
2023-02-17 04:20:56,982 DEBUG TRAIN Batch 0/8200 loss 76.374863 loss_att 153.217346 loss_ctc 77.329666 loss_rnnt 60.630493 hw_loss 0.466062 lr 0.00032804 rank 2
2023-02-17 04:20:56,985 DEBUG TRAIN Batch 0/8200 loss 59.413929 loss_att 129.304260 loss_ctc 60.779583 loss_rnnt 45.018387 hw_loss 0.441353 lr 0.00032804 rank 5
2023-02-17 04:20:56,985 DEBUG TRAIN Batch 0/8200 loss 80.897026 loss_att 140.520813 loss_ctc 85.023766 loss_rnnt 68.238861 hw_loss 0.343454 lr 0.00032804 rank 0
2023-02-17 04:22:12,808 DEBUG TRAIN Batch 0/8300 loss 49.546303 loss_att 75.828033 loss_ctc 54.799053 loss_rnnt 43.314240 hw_loss 0.516286 lr 0.00033204 rank 6
2023-02-17 04:22:12,814 DEBUG TRAIN Batch 0/8300 loss 95.136284 loss_att 221.122620 loss_ctc 91.253708 loss_rnnt 70.293083 hw_loss 0.306756 lr 0.00033204 rank 5
2023-02-17 04:22:12,816 DEBUG TRAIN Batch 0/8300 loss 86.362686 loss_att 179.432465 loss_ctc 88.305481 loss_rnnt 67.235718 hw_loss 0.476195 lr 0.00033204 rank 1
2023-02-17 04:22:12,818 DEBUG TRAIN Batch 0/8300 loss 82.486115 loss_att 164.211761 loss_ctc 82.741661 loss_rnnt 65.937241 hw_loss 0.318130 lr 0.00033204 rank 4
2023-02-17 04:22:12,818 DEBUG TRAIN Batch 0/8300 loss 97.701561 loss_att 193.751846 loss_ctc 103.418686 loss_rnnt 77.522903 hw_loss 0.386820 lr 0.00033204 rank 7
2023-02-17 04:22:12,819 DEBUG TRAIN Batch 0/8300 loss 92.336365 loss_att 181.822998 loss_ctc 95.936371 loss_rnnt 73.762405 hw_loss 0.368675 lr 0.00033204 rank 3
2023-02-17 04:22:12,819 DEBUG TRAIN Batch 0/8300 loss 77.239754 loss_att 153.537155 loss_ctc 78.402298 loss_rnnt 61.591305 hw_loss 0.438668 lr 0.00033204 rank 0
2023-02-17 04:22:12,821 DEBUG TRAIN Batch 0/8300 loss 49.542553 loss_att 101.363678 loss_ctc 52.877251 loss_rnnt 38.488472 hw_loss 0.459800 lr 0.00033204 rank 2
2023-02-17 04:23:07,503 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 7
2023-02-17 04:23:07,503 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 4
2023-02-17 04:23:07,506 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 0
2023-02-17 04:23:07,508 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 5
2023-02-17 04:23:07,514 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 6
2023-02-17 04:23:07,515 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 1
2023-02-17 04:23:07,525 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 2
2023-02-17 04:23:07,529 DEBUG CV Batch 0/0 loss 15.640746 loss_att 21.425957 loss_ctc 17.304413 loss_rnnt 13.930661 hw_loss 0.621037 history loss 15.061459 rank 3
2023-02-17 04:23:18,811 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 1
2023-02-17 04:23:18,839 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 6
2023-02-17 04:23:18,846 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 7
2023-02-17 04:23:19,122 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 3
2023-02-17 04:23:19,267 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 4
2023-02-17 04:23:19,334 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 5
2023-02-17 04:23:19,335 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 2
2023-02-17 04:23:19,463 DEBUG CV Batch 0/100 loss 65.035515 loss_att 120.243660 loss_ctc 75.851410 loss_rnnt 52.314922 hw_loss 0.444093 history loss 38.927580 rank 0
2023-02-17 04:23:33,002 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 7
2023-02-17 04:23:33,032 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 2
2023-02-17 04:23:33,044 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 5
2023-02-17 04:23:33,049 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 1
2023-02-17 04:23:33,068 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 6
2023-02-17 04:23:33,114 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 4
2023-02-17 04:23:33,282 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 3
2023-02-17 04:23:33,436 DEBUG CV Batch 0/200 loss 134.113403 loss_att 322.171356 loss_ctc 131.562439 loss_rnnt 96.665863 hw_loss 0.330112 history loss 43.774794 rank 0
2023-02-17 04:23:45,111 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 1
2023-02-17 04:23:45,116 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 5
2023-02-17 04:23:45,167 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 7
2023-02-17 04:23:45,201 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 2
2023-02-17 04:23:45,234 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 4
2023-02-17 04:23:45,428 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 6
2023-02-17 04:23:45,671 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 0
2023-02-17 04:23:45,751 DEBUG CV Batch 0/300 loss 44.077065 loss_att 87.574265 loss_ctc 52.020233 loss_rnnt 34.044434 hw_loss 0.513943 history loss 42.955370 rank 3
2023-02-17 04:23:57,016 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 1
2023-02-17 04:23:57,047 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 5
2023-02-17 04:23:57,115 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 7
2023-02-17 04:23:57,222 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 4
2023-02-17 04:23:57,301 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 2
2023-02-17 04:23:57,381 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 6
2023-02-17 04:23:57,831 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 0
2023-02-17 04:23:57,919 DEBUG CV Batch 0/400 loss 195.611725 loss_att 469.834656 loss_ctc 169.197830 loss_rnnt 144.125198 hw_loss 0.307064 history loss 45.910764 rank 3
2023-02-17 04:24:07,482 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 5
2023-02-17 04:24:07,527 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 7
2023-02-17 04:24:07,547 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 1
2023-02-17 04:24:07,834 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 4
2023-02-17 04:24:07,843 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 6
2023-02-17 04:24:08,611 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 0
2023-02-17 04:24:08,641 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 3
2023-02-17 04:24:08,645 DEBUG CV Batch 0/500 loss 70.985664 loss_att 125.926689 loss_ctc 75.367332 loss_rnnt 59.248177 hw_loss 0.309487 history loss 46.006562 rank 2
2023-02-17 04:24:19,781 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 7
2023-02-17 04:24:19,845 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 6
2023-02-17 04:24:19,964 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 5
2023-02-17 04:24:20,204 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 1
2023-02-17 04:24:20,820 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 3
2023-02-17 04:24:20,845 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 2
2023-02-17 04:24:21,135 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 4
2023-02-17 04:24:21,663 DEBUG CV Batch 0/600 loss 35.067028 loss_att 48.151016 loss_ctc 38.056362 loss_rnnt 31.781252 hw_loss 0.507000 history loss 47.568445 rank 0
2023-02-17 04:24:32,226 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 6
2023-02-17 04:24:32,282 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 7
2023-02-17 04:24:32,317 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 1
2023-02-17 04:24:32,377 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 2
2023-02-17 04:24:32,403 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 5
2023-02-17 04:24:32,882 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 3
2023-02-17 04:24:33,463 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 4
2023-02-17 04:24:36,115 DEBUG CV Batch 0/700 loss 183.410187 loss_att 412.021576 loss_ctc 188.072922 loss_rnnt 136.938721 hw_loss 0.239015 history loss 48.635751 rank 0
2023-02-17 04:24:43,787 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 1
2023-02-17 04:24:44,166 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 6
2023-02-17 04:24:44,347 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 5
2023-02-17 04:24:44,399 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 7
2023-02-17 04:24:44,832 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 2
2023-02-17 04:24:45,188 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 3
2023-02-17 04:24:45,390 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 4
2023-02-17 04:24:48,511 DEBUG CV Batch 0/800 loss 65.073662 loss_att 120.836899 loss_ctc 76.146004 loss_rnnt 52.151276 hw_loss 0.550174 history loss 47.208002 rank 0
2023-02-17 04:24:57,869 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 1
2023-02-17 04:24:58,438 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 5
2023-02-17 04:24:58,492 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 6
2023-02-17 04:24:58,514 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 7
2023-02-17 04:24:59,273 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 3
2023-02-17 04:24:59,617 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 2
2023-02-17 04:25:00,062 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 4
2023-02-17 04:25:04,058 DEBUG CV Batch 0/900 loss 105.732613 loss_att 280.370819 loss_ctc 103.709991 loss_rnnt 70.941376 hw_loss 0.249890 history loss 47.458580 rank 0
2023-02-17 04:25:10,093 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573472 history loss 46.937438 rank 1
2023-02-17 04:25:10,678 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 5
2023-02-17 04:25:10,707 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 6
2023-02-17 04:25:10,729 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 7
2023-02-17 04:25:11,989 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 3
2023-02-17 04:25:12,035 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 2
2023-02-17 04:25:12,336 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 4
2023-02-17 04:25:16,309 DEBUG CV Batch 0/1000 loss 40.426823 loss_att 77.454445 loss_ctc 41.809013 loss_rnnt 32.531151 hw_loss 0.573473 history loss 46.937438 rank 0
2023-02-17 04:25:21,981 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 1
2023-02-17 04:25:22,501 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 5
2023-02-17 04:25:22,506 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 6
2023-02-17 04:25:22,553 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 7
2023-02-17 04:25:23,993 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 2
2023-02-17 04:25:24,398 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 3
2023-02-17 04:25:24,971 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 4
2023-02-17 04:25:28,319 DEBUG CV Batch 0/1100 loss 18.967226 loss_att 23.907410 loss_ctc 22.436287 loss_rnnt 17.158768 hw_loss 0.671026 history loss 47.141276 rank 0
2023-02-17 04:25:32,308 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 1
2023-02-17 04:25:32,925 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 6
2023-02-17 04:25:33,036 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 5
2023-02-17 04:25:33,105 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 7
2023-02-17 04:25:34,635 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 2
2023-02-17 04:25:35,379 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 3
2023-02-17 04:25:35,458 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 4
2023-02-17 04:25:38,891 DEBUG CV Batch 0/1200 loss 77.117401 loss_att 137.022278 loss_ctc 86.576469 loss_rnnt 63.678223 hw_loss 0.369367 history loss 47.501117 rank 0
2023-02-17 04:25:44,246 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 1
2023-02-17 04:25:44,833 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 6
2023-02-17 04:25:44,981 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 7
2023-02-17 04:25:44,982 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 5
2023-02-17 04:25:46,622 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 2
2023-02-17 04:25:47,431 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 4
2023-02-17 04:25:47,711 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 3
2023-02-17 04:25:50,969 DEBUG CV Batch 0/1300 loss 35.709724 loss_att 47.631245 loss_ctc 40.232098 loss_rnnt 32.408302 hw_loss 0.589001 history loss 47.932571 rank 0
2023-02-17 04:25:56,056 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 1
2023-02-17 04:25:56,960 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 6
2023-02-17 04:25:57,388 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 5
2023-02-17 04:25:57,729 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 7
2023-02-17 04:25:59,277 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 3
2023-02-17 04:25:59,492 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 2
2023-02-17 04:25:59,593 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 4
2023-02-17 04:26:02,516 DEBUG CV Batch 0/1400 loss 128.580872 loss_att 298.673492 loss_ctc 117.121857 loss_rnnt 95.937637 hw_loss 0.286053 history loss 48.489313 rank 0
2023-02-17 04:26:08,584 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 1
2023-02-17 04:26:09,610 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 6
2023-02-17 04:26:09,957 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 5
2023-02-17 04:26:10,327 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 7
2023-02-17 04:26:11,923 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 3
2023-02-17 04:26:12,157 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 2
2023-02-17 04:26:12,573 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 4
2023-02-17 04:26:15,300 DEBUG CV Batch 0/1500 loss 68.277802 loss_att 126.147171 loss_ctc 68.987778 loss_rnnt 56.413185 hw_loss 0.367635 history loss 47.819650 rank 0
2023-02-17 04:26:22,476 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 1
2023-02-17 04:26:23,862 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 6
2023-02-17 04:26:23,918 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 5
2023-02-17 04:26:23,943 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 7
2023-02-17 04:26:25,863 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 3
2023-02-17 04:26:26,073 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 2
2023-02-17 04:26:26,714 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 4
2023-02-17 04:26:28,982 DEBUG CV Batch 0/1600 loss 113.903877 loss_att 303.260376 loss_ctc 108.679352 loss_rnnt 76.527908 hw_loss 0.377362 history loss 47.982711 rank 0
2023-02-17 04:26:34,872 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 1
2023-02-17 04:26:36,285 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 6
2023-02-17 04:26:36,375 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 7
2023-02-17 04:26:36,438 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 5
2023-02-17 04:26:38,414 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 3
2023-02-17 04:26:38,694 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 2
2023-02-17 04:26:39,217 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 4
2023-02-17 04:26:41,945 DEBUG CV Batch 0/1700 loss 58.898006 loss_att 97.578934 loss_ctc 66.062988 loss_rnnt 49.918003 hw_loss 0.540903 history loss 47.643020 rank 0
2023-02-17 04:26:44,180 INFO Epoch 0 CV info cv_loss 47.8209619919057
2023-02-17 04:26:44,181 INFO Epoch 1 TRAIN info lr 0.00033468
2023-02-17 04:26:44,184 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:45,430 INFO Epoch 0 CV info cv_loss 47.82096198749501
2023-02-17 04:26:45,432 INFO Epoch 1 TRAIN info lr 0.00033348
2023-02-17 04:26:45,435 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:45,614 INFO Epoch 0 CV info cv_loss 47.82096198590992
2023-02-17 04:26:45,615 INFO Epoch 1 TRAIN info lr 0.00033328
2023-02-17 04:26:45,619 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:45,737 INFO Epoch 0 CV info cv_loss 47.82096198970036
2023-02-17 04:26:45,738 INFO Epoch 1 TRAIN info lr 0.00033363999999999996
2023-02-17 04:26:45,740 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:47,690 INFO Epoch 0 CV info cv_loss 47.82096198535858
2023-02-17 04:26:47,691 INFO Epoch 1 TRAIN info lr 0.000333
2023-02-17 04:26:47,694 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:48,407 INFO Epoch 0 CV info cv_loss 47.82096198901118
2023-02-17 04:26:48,408 INFO Epoch 1 TRAIN info lr 0.00033288
2023-02-17 04:26:48,413 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:48,766 INFO Epoch 0 CV info cv_loss 47.820961990182774
2023-02-17 04:26:48,767 INFO Epoch 1 TRAIN info lr 0.00033348
2023-02-17 04:26:48,770 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:26:51,292 INFO Epoch 0 CV info cv_loss 47.8209619919057
2023-02-17 04:26:51,293 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/0.pt
2023-02-17 04:26:53,079 INFO Epoch 1 TRAIN info lr 0.00033332
2023-02-17 04:26:53,084 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:28:08,330 DEBUG TRAIN Batch 1/0 loss 29.230486 loss_att 38.158501 loss_ctc 32.280811 loss_rnnt 26.741789 hw_loss 0.555718 lr 0.00033352 rank 6
2023-02-17 04:28:08,331 DEBUG TRAIN Batch 1/0 loss 21.469765 loss_att 28.428936 loss_ctc 21.830608 loss_rnnt 19.717859 hw_loss 0.584917 lr 0.00033472 rank 1
2023-02-17 04:28:08,331 DEBUG TRAIN Batch 1/0 loss 28.547459 loss_att 37.290958 loss_ctc 33.029667 loss_rnnt 25.878756 hw_loss 0.604447 lr 0.00033332 rank 7
2023-02-17 04:28:08,332 DEBUG TRAIN Batch 1/0 loss 34.642445 loss_att 40.107342 loss_ctc 38.484924 loss_rnnt 32.741699 hw_loss 0.553945 lr 0.00033368 rank 5
2023-02-17 04:28:08,343 DEBUG TRAIN Batch 1/0 loss 32.385868 loss_att 39.705509 loss_ctc 35.879326 loss_rnnt 30.155075 hw_loss 0.564506 lr 0.00033304 rank 3
2023-02-17 04:28:08,345 DEBUG TRAIN Batch 1/0 loss 26.163202 loss_att 36.107376 loss_ctc 28.121346 loss_rnnt 23.627735 hw_loss 0.535399 lr 0.00033336 rank 0
2023-02-17 04:28:08,353 DEBUG TRAIN Batch 1/0 loss 25.123102 loss_att 31.927307 loss_ctc 25.921722 loss_rnnt 23.439791 hw_loss 0.404974 lr 0.00033292 rank 4
2023-02-17 04:28:08,362 DEBUG TRAIN Batch 1/0 loss 28.103088 loss_att 37.631195 loss_ctc 30.529860 loss_rnnt 25.621836 hw_loss 0.472612 lr 0.00033352 rank 2
2023-02-17 04:29:23,062 DEBUG TRAIN Batch 1/100 loss 95.521477 loss_att 193.781952 loss_ctc 102.694595 loss_rnnt 74.721344 hw_loss 0.359284 lr 0.00033752 rank 6
2023-02-17 04:29:23,063 DEBUG TRAIN Batch 1/100 loss 106.821266 loss_att 199.491058 loss_ctc 115.158287 loss_rnnt 86.957199 hw_loss 0.409693 lr 0.00033872 rank 1
2023-02-17 04:29:23,065 DEBUG TRAIN Batch 1/100 loss 82.406921 loss_att 171.842361 loss_ctc 90.145470 loss_rnnt 63.223156 hw_loss 0.496627 lr 0.00033736 rank 0
2023-02-17 04:29:23,068 DEBUG TRAIN Batch 1/100 loss 96.951859 loss_att 205.636154 loss_ctc 101.569565 loss_rnnt 74.390503 hw_loss 0.391506 lr 0.00033768 rank 5
2023-02-17 04:29:23,070 DEBUG TRAIN Batch 1/100 loss 101.411545 loss_att 204.589050 loss_ctc 111.498108 loss_rnnt 79.239944 hw_loss 0.358542 lr 0.00033752 rank 2
2023-02-17 04:29:23,070 DEBUG TRAIN Batch 1/100 loss 77.031441 loss_att 171.468811 loss_ctc 73.377457 loss_rnnt 58.460571 hw_loss 0.319861 lr 0.00033732 rank 7
2023-02-17 04:29:23,072 DEBUG TRAIN Batch 1/100 loss 94.655418 loss_att 187.745758 loss_ctc 106.555664 loss_rnnt 74.282883 hw_loss 0.314558 lr 0.00033692 rank 4
2023-02-17 04:29:23,114 DEBUG TRAIN Batch 1/100 loss 101.637199 loss_att 197.451248 loss_ctc 94.484154 loss_rnnt 83.231178 hw_loss 0.369261 lr 0.00033704 rank 3
2023-02-17 04:30:38,013 DEBUG TRAIN Batch 1/200 loss 94.962639 loss_att 175.436722 loss_ctc 101.870262 loss_rnnt 77.714981 hw_loss 0.434668 lr 0.00034152 rank 6
2023-02-17 04:30:38,017 DEBUG TRAIN Batch 1/200 loss 67.681549 loss_att 160.108154 loss_ctc 67.737991 loss_rnnt 48.998772 hw_loss 0.356124 lr 0.00034092 rank 4
2023-02-17 04:30:38,018 DEBUG TRAIN Batch 1/200 loss 67.178162 loss_att 143.200912 loss_ctc 64.800110 loss_rnnt 52.117657 hw_loss 0.324447 lr 0.00034168 rank 5
2023-02-17 04:30:38,021 DEBUG TRAIN Batch 1/200 loss 76.486565 loss_att 154.366669 loss_ctc 79.035965 loss_rnnt 60.364258 hw_loss 0.386936 lr 0.00034104 rank 3
2023-02-17 04:30:38,020 DEBUG TRAIN Batch 1/200 loss 118.885635 loss_att 216.846802 loss_ctc 136.851639 loss_rnnt 96.708359 hw_loss 0.355447 lr 0.00034132 rank 7
2023-02-17 04:30:38,022 DEBUG TRAIN Batch 1/200 loss 59.125771 loss_att 152.739304 loss_ctc 57.757103 loss_rnnt 40.379440 hw_loss 0.386462 lr 0.00034272 rank 1
2023-02-17 04:30:38,024 DEBUG TRAIN Batch 1/200 loss 68.950455 loss_att 153.316986 loss_ctc 65.155571 loss_rnnt 52.363731 hw_loss 0.411385 lr 0.00034152 rank 2
2023-02-17 04:30:38,025 DEBUG TRAIN Batch 1/200 loss 113.123543 loss_att 216.079956 loss_ctc 115.838036 loss_rnnt 91.962524 hw_loss 0.389630 lr 0.00034136 rank 0
2023-02-17 04:31:54,248 DEBUG TRAIN Batch 1/300 loss 83.842239 loss_att 191.336044 loss_ctc 91.154846 loss_rnnt 61.180073 hw_loss 0.353230 lr 0.00034532 rank 7
2023-02-17 04:31:54,248 DEBUG TRAIN Batch 1/300 loss 123.342682 loss_att 217.661377 loss_ctc 134.114059 loss_rnnt 102.823425 hw_loss 0.411244 lr 0.00034492 rank 4
2023-02-17 04:31:54,249 DEBUG TRAIN Batch 1/300 loss 94.849007 loss_att 185.177032 loss_ctc 101.859894 loss_rnnt 75.689667 hw_loss 0.298014 lr 0.00034552 rank 2
2023-02-17 04:31:54,250 DEBUG TRAIN Batch 1/300 loss 75.709999 loss_att 170.110428 loss_ctc 80.063339 loss_rnnt 56.052654 hw_loss 0.369033 lr 0.00034504 rank 3
2023-02-17 04:31:54,251 DEBUG TRAIN Batch 1/300 loss 99.774475 loss_att 201.474915 loss_ctc 102.393059 loss_rnnt 78.837692 hw_loss 0.464161 lr 0.00034672 rank 1
2023-02-17 04:31:54,252 DEBUG TRAIN Batch 1/300 loss 93.347809 loss_att 180.110062 loss_ctc 98.747147 loss_rnnt 75.090027 hw_loss 0.347655 lr 0.00034568 rank 5
2023-02-17 04:31:54,252 DEBUG TRAIN Batch 1/300 loss 108.947746 loss_att 227.397720 loss_ctc 115.955177 loss_rnnt 84.118683 hw_loss 0.383899 lr 0.00034552 rank 6
2023-02-17 04:31:54,258 DEBUG TRAIN Batch 1/300 loss 63.431385 loss_att 146.370377 loss_ctc 72.193665 loss_rnnt 45.478134 hw_loss 0.369653 lr 0.00034536 rank 0
2023-02-17 04:33:12,039 DEBUG TRAIN Batch 1/400 loss 82.389832 loss_att 170.621262 loss_ctc 84.565094 loss_rnnt 64.208092 hw_loss 0.460167 lr 0.00034952 rank 6
2023-02-17 04:33:12,041 DEBUG TRAIN Batch 1/400 loss 113.199623 loss_att 218.660583 loss_ctc 115.531868 loss_rnnt 91.565796 hw_loss 0.432519 lr 0.00034932 rank 7
2023-02-17 04:33:12,044 DEBUG TRAIN Batch 1/400 loss 57.610680 loss_att 145.294586 loss_ctc 53.051834 loss_rnnt 40.500137 hw_loss 0.340512 lr 0.00034968 rank 5
2023-02-17 04:33:12,046 DEBUG TRAIN Batch 1/400 loss 83.723534 loss_att 177.721497 loss_ctc 87.630692 loss_rnnt 64.181023 hw_loss 0.416191 lr 0.00034936 rank 0
2023-02-17 04:33:12,046 DEBUG TRAIN Batch 1/400 loss 71.822540 loss_att 150.052917 loss_ctc 68.042938 loss_rnnt 56.477161 hw_loss 0.381097 lr 0.00034952 rank 2
2023-02-17 04:33:12,046 DEBUG TRAIN Batch 1/400 loss 97.709770 loss_att 174.875900 loss_ctc 99.074974 loss_rnnt 81.856743 hw_loss 0.445842 lr 0.00035072 rank 1
2023-02-17 04:33:12,046 DEBUG TRAIN Batch 1/400 loss 75.265793 loss_att 156.023636 loss_ctc 74.927170 loss_rnnt 58.955982 hw_loss 0.381356 lr 0.00034904 rank 3
2023-02-17 04:33:12,048 DEBUG TRAIN Batch 1/400 loss 98.737305 loss_att 185.665176 loss_ctc 107.328194 loss_rnnt 79.967751 hw_loss 0.447241 lr 0.00034892 rank 4
2023-02-17 04:34:27,863 DEBUG TRAIN Batch 1/500 loss 110.739487 loss_att 201.230286 loss_ctc 125.894005 loss_rnnt 90.432793 hw_loss 0.352370 lr 0.00035332 rank 7
2023-02-17 04:34:27,867 DEBUG TRAIN Batch 1/500 loss 56.470425 loss_att 126.568253 loss_ctc 60.709106 loss_rnnt 41.693573 hw_loss 0.360233 lr 0.00035352 rank 6
2023-02-17 04:34:27,867 DEBUG TRAIN Batch 1/500 loss 67.415726 loss_att 138.653137 loss_ctc 74.379158 loss_rnnt 52.072857 hw_loss 0.312993 lr 0.00035304 rank 3
2023-02-17 04:34:27,868 DEBUG TRAIN Batch 1/500 loss 72.238640 loss_att 160.598175 loss_ctc 86.688904 loss_rnnt 52.400661 hw_loss 0.448813 lr 0.00035292 rank 4
2023-02-17 04:34:27,868 DEBUG TRAIN Batch 1/500 loss 109.653076 loss_att 176.587021 loss_ctc 121.599205 loss_rnnt 94.478676 hw_loss 0.365247 lr 0.00035472 rank 1
2023-02-17 04:34:27,870 DEBUG TRAIN Batch 1/500 loss 75.969986 loss_att 161.427261 loss_ctc 84.628143 loss_rnnt 57.523468 hw_loss 0.376209 lr 0.00035368 rank 5
2023-02-17 04:34:27,871 DEBUG TRAIN Batch 1/500 loss 55.391739 loss_att 123.648460 loss_ctc 53.676010 loss_rnnt 41.760098 hw_loss 0.392002 lr 0.00035352 rank 2
2023-02-17 04:34:27,916 DEBUG TRAIN Batch 1/500 loss 69.480057 loss_att 166.100754 loss_ctc 71.816612 loss_rnnt 49.647163 hw_loss 0.369773 lr 0.00035336 rank 0
2023-02-17 04:35:43,940 DEBUG TRAIN Batch 1/600 loss 53.319843 loss_att 78.033066 loss_ctc 62.875530 loss_rnnt 46.874161 hw_loss 0.429273 lr 0.00035704 rank 3
2023-02-17 04:35:43,941 DEBUG TRAIN Batch 1/600 loss 62.392029 loss_att 104.928131 loss_ctc 65.497833 loss_rnnt 53.221657 hw_loss 0.466961 lr 0.00035732 rank 7
2023-02-17 04:35:43,946 DEBUG TRAIN Batch 1/600 loss 32.697998 loss_att 40.256218 loss_ctc 36.273865 loss_rnnt 30.445944 hw_loss 0.494305 lr 0.00035752 rank 2
2023-02-17 04:35:43,947 DEBUG TRAIN Batch 1/600 loss 72.565498 loss_att 134.391541 loss_ctc 74.709274 loss_rnnt 59.708862 hw_loss 0.385479 lr 0.00035872 rank 1
2023-02-17 04:35:43,947 DEBUG TRAIN Batch 1/600 loss 29.945351 loss_att 46.540260 loss_ctc 34.693489 loss_rnnt 25.740789 hw_loss 0.473429 lr 0.00035736 rank 0
2023-02-17 04:35:43,948 DEBUG TRAIN Batch 1/600 loss 61.342216 loss_att 163.609085 loss_ctc 56.623657 loss_rnnt 41.372616 hw_loss 0.272556 lr 0.00035752 rank 6
2023-02-17 04:35:43,950 DEBUG TRAIN Batch 1/600 loss 50.995640 loss_att 89.580521 loss_ctc 56.968796 loss_rnnt 42.235321 hw_loss 0.462980 lr 0.00035768 rank 5
2023-02-17 04:35:43,991 DEBUG TRAIN Batch 1/600 loss 55.116772 loss_att 91.429466 loss_ctc 63.269024 loss_rnnt 46.508244 hw_loss 0.485667 lr 0.00035692 rank 4
2023-02-17 04:37:03,552 DEBUG TRAIN Batch 1/700 loss 75.726738 loss_att 169.519653 loss_ctc 73.836960 loss_rnnt 57.016430 hw_loss 0.381920 lr 0.00036152 rank 6
2023-02-17 04:37:03,556 DEBUG TRAIN Batch 1/700 loss 67.861107 loss_att 166.160126 loss_ctc 68.707970 loss_rnnt 47.899872 hw_loss 0.353479 lr 0.00036132 rank 7
2023-02-17 04:37:03,556 DEBUG TRAIN Batch 1/700 loss 79.242538 loss_att 184.886063 loss_ctc 82.878540 loss_rnnt 57.445965 hw_loss 0.343255 lr 0.00036092 rank 4
2023-02-17 04:37:03,557 DEBUG TRAIN Batch 1/700 loss 110.289268 loss_att 212.592773 loss_ctc 108.236328 loss_rnnt 89.906425 hw_loss 0.367256 lr 0.00036168 rank 5
2023-02-17 04:37:03,560 DEBUG TRAIN Batch 1/700 loss 64.022049 loss_att 147.285782 loss_ctc 73.209854 loss_rnnt 45.981750 hw_loss 0.304699 lr 0.00036136 rank 0
2023-02-17 04:37:03,561 DEBUG TRAIN Batch 1/700 loss 119.389351 loss_att 223.417267 loss_ctc 121.962692 loss_rnnt 98.047783 hw_loss 0.361648 lr 0.00036272 rank 1
2023-02-17 04:37:03,560 DEBUG TRAIN Batch 1/700 loss 81.263069 loss_att 156.527176 loss_ctc 81.841599 loss_rnnt 65.966141 hw_loss 0.313048 lr 0.00036104 rank 3
2023-02-17 04:37:03,603 DEBUG TRAIN Batch 1/700 loss 77.344582 loss_att 179.623230 loss_ctc 86.805229 loss_rnnt 55.424938 hw_loss 0.379678 lr 0.00036152 rank 2
2023-02-17 04:38:19,537 DEBUG TRAIN Batch 1/800 loss 79.793640 loss_att 172.711319 loss_ctc 86.281738 loss_rnnt 60.159615 hw_loss 0.347629 lr 0.00036532 rank 7
2023-02-17 04:38:19,538 DEBUG TRAIN Batch 1/800 loss 92.379539 loss_att 182.200882 loss_ctc 93.402084 loss_rnnt 74.074600 hw_loss 0.383126 lr 0.00036552 rank 6
2023-02-17 04:38:19,538 DEBUG TRAIN Batch 1/800 loss 64.423981 loss_att 150.096466 loss_ctc 61.232143 loss_rnnt 47.477215 hw_loss 0.445955 lr 0.00036672 rank 1
2023-02-17 04:38:19,539 DEBUG TRAIN Batch 1/800 loss 57.846409 loss_att 131.523727 loss_ctc 58.980278 loss_rnnt 42.770470 hw_loss 0.354930 lr 0.00036504 rank 3
2023-02-17 04:38:19,538 DEBUG TRAIN Batch 1/800 loss 93.718124 loss_att 170.275391 loss_ctc 102.892212 loss_rnnt 76.988251 hw_loss 0.366018 lr 0.00036568 rank 5
2023-02-17 04:38:19,539 DEBUG TRAIN Batch 1/800 loss 61.914101 loss_att 153.276581 loss_ctc 70.090225 loss_rnnt 42.364384 hw_loss 0.350759 lr 0.00036492 rank 4
2023-02-17 04:38:19,542 DEBUG TRAIN Batch 1/800 loss 74.674004 loss_att 158.514374 loss_ctc 77.342003 loss_rnnt 57.390404 hw_loss 0.299629 lr 0.00036552 rank 2
2023-02-17 04:38:19,543 DEBUG TRAIN Batch 1/800 loss 64.343140 loss_att 164.905014 loss_ctc 63.962830 loss_rnnt 44.128593 hw_loss 0.286649 lr 0.00036536 rank 0
2023-02-17 04:39:34,605 DEBUG TRAIN Batch 1/900 loss 72.300201 loss_att 159.670959 loss_ctc 73.810204 loss_rnnt 54.433834 hw_loss 0.357900 lr 0.00037072 rank 1
2023-02-17 04:39:34,608 DEBUG TRAIN Batch 1/900 loss 95.860611 loss_att 169.593857 loss_ctc 105.416924 loss_rnnt 79.626236 hw_loss 0.400415 lr 0.00036904 rank 3
2023-02-17 04:39:34,609 DEBUG TRAIN Batch 1/900 loss 77.703133 loss_att 155.467422 loss_ctc 78.816986 loss_rnnt 61.790691 hw_loss 0.395749 lr 0.00036968 rank 5
2023-02-17 04:39:34,609 DEBUG TRAIN Batch 1/900 loss 82.017326 loss_att 170.036758 loss_ctc 91.290009 loss_rnnt 62.963215 hw_loss 0.400995 lr 0.00036952 rank 6
2023-02-17 04:39:34,610 DEBUG TRAIN Batch 1/900 loss 84.861572 loss_att 188.652740 loss_ctc 91.295303 loss_rnnt 63.084229 hw_loss 0.302383 lr 0.00036932 rank 7
2023-02-17 04:39:34,610 DEBUG TRAIN Batch 1/900 loss 51.100506 loss_att 138.877213 loss_ctc 57.909588 loss_rnnt 32.501038 hw_loss 0.255457 lr 0.00036952 rank 2
2023-02-17 04:39:34,611 DEBUG TRAIN Batch 1/900 loss 103.120552 loss_att 196.318344 loss_ctc 108.330795 loss_rnnt 83.645218 hw_loss 0.264505 lr 0.00036892 rank 4
2023-02-17 04:39:34,611 DEBUG TRAIN Batch 1/900 loss 51.406792 loss_att 130.932236 loss_ctc 55.692757 loss_rnnt 34.752144 hw_loss 0.333927 lr 0.00036936 rank 0
2023-02-17 04:40:50,962 DEBUG TRAIN Batch 1/1000 loss 96.114151 loss_att 199.553497 loss_ctc 103.323174 loss_rnnt 74.233932 hw_loss 0.433383 lr 0.00037332 rank 7
2023-02-17 04:40:50,962 DEBUG TRAIN Batch 1/1000 loss 87.627640 loss_att 174.948502 loss_ctc 92.975563 loss_rnnt 69.276596 hw_loss 0.325887 lr 0.00037304 rank 3
2023-02-17 04:40:50,964 DEBUG TRAIN Batch 1/1000 loss 85.004189 loss_att 164.968658 loss_ctc 96.365555 loss_rnnt 67.324631 hw_loss 0.322158 lr 0.00037292 rank 4
2023-02-17 04:40:50,964 DEBUG TRAIN Batch 1/1000 loss 77.122749 loss_att 158.474838 loss_ctc 75.384018 loss_rnnt 60.921299 hw_loss 0.305362 lr 0.00037352 rank 2
2023-02-17 04:40:50,966 DEBUG TRAIN Batch 1/1000 loss 116.115685 loss_att 204.148727 loss_ctc 131.839508 loss_rnnt 96.216995 hw_loss 0.366691 lr 0.00037352 rank 6
2023-02-17 04:40:50,967 DEBUG TRAIN Batch 1/1000 loss 86.128365 loss_att 169.176270 loss_ctc 91.934647 loss_rnnt 68.564133 hw_loss 0.338378 lr 0.00037368 rank 5
2023-02-17 04:40:50,974 DEBUG TRAIN Batch 1/1000 loss 47.539703 loss_att 123.736252 loss_ctc 43.330925 loss_rnnt 32.688019 hw_loss 0.325390 lr 0.00037472 rank 1
2023-02-17 04:40:50,975 DEBUG TRAIN Batch 1/1000 loss 92.740356 loss_att 189.465164 loss_ctc 102.792221 loss_rnnt 71.892807 hw_loss 0.304369 lr 0.00037336 rank 0
2023-02-17 04:42:09,242 DEBUG TRAIN Batch 1/1100 loss 85.869820 loss_att 140.898163 loss_ctc 101.667786 loss_rnnt 72.536575 hw_loss 0.414692 lr 0.00037752 rank 6
2023-02-17 04:42:09,246 DEBUG TRAIN Batch 1/1100 loss 109.006699 loss_att 199.254089 loss_ctc 110.730942 loss_rnnt 90.514572 hw_loss 0.398901 lr 0.00037704 rank 3
2023-02-17 04:42:09,246 DEBUG TRAIN Batch 1/1100 loss 99.901871 loss_att 178.827103 loss_ctc 116.089943 loss_rnnt 81.774178 hw_loss 0.345441 lr 0.00037732 rank 7
2023-02-17 04:42:09,247 DEBUG TRAIN Batch 1/1100 loss 62.230595 loss_att 142.872986 loss_ctc 64.591927 loss_rnnt 45.571766 hw_loss 0.404066 lr 0.00037692 rank 4
2023-02-17 04:42:09,249 DEBUG TRAIN Batch 1/1100 loss 110.992126 loss_att 173.197021 loss_ctc 128.730637 loss_rnnt 95.981491 hw_loss 0.383487 lr 0.00037872 rank 1
2023-02-17 04:42:09,251 DEBUG TRAIN Batch 1/1100 loss 58.423813 loss_att 123.055313 loss_ctc 61.772316 loss_rnnt 44.774307 hw_loss 0.518881 lr 0.00037752 rank 2
2023-02-17 04:42:09,252 DEBUG TRAIN Batch 1/1100 loss 51.522575 loss_att 114.404716 loss_ctc 57.752983 loss_rnnt 37.940804 hw_loss 0.327417 lr 0.00037768 rank 5
2023-02-17 04:42:09,252 DEBUG TRAIN Batch 1/1100 loss 57.602360 loss_att 129.023376 loss_ctc 61.792526 loss_rnnt 42.565735 hw_loss 0.363248 lr 0.00037736 rank 0
2023-02-17 04:43:25,295 DEBUG TRAIN Batch 1/1200 loss 44.546566 loss_att 72.641251 loss_ctc 50.539993 loss_rnnt 37.893356 hw_loss 0.440901 lr 0.00038152 rank 6
2023-02-17 04:43:25,300 DEBUG TRAIN Batch 1/1200 loss 43.857567 loss_att 102.499580 loss_ctc 51.242058 loss_rnnt 30.954731 hw_loss 0.355935 lr 0.00038132 rank 7
2023-02-17 04:43:25,302 DEBUG TRAIN Batch 1/1200 loss 65.549431 loss_att 131.289520 loss_ctc 68.024513 loss_rnnt 51.849384 hw_loss 0.416281 lr 0.00038168 rank 5
2023-02-17 04:43:25,303 DEBUG TRAIN Batch 1/1200 loss 32.128544 loss_att 55.991329 loss_ctc 36.593407 loss_rnnt 26.564316 hw_loss 0.368162 lr 0.00038152 rank 2
2023-02-17 04:43:25,306 DEBUG TRAIN Batch 1/1200 loss 89.930878 loss_att 140.415070 loss_ctc 94.088707 loss_rnnt 79.044838 hw_loss 0.440265 lr 0.00038272 rank 1
2023-02-17 04:43:25,327 DEBUG TRAIN Batch 1/1200 loss 70.881966 loss_att 138.988892 loss_ctc 78.781967 loss_rnnt 55.935726 hw_loss 0.509097 lr 0.00038092 rank 4
2023-02-17 04:43:25,341 DEBUG TRAIN Batch 1/1200 loss 52.159470 loss_att 99.085007 loss_ctc 59.706432 loss_rnnt 41.529694 hw_loss 0.447012 lr 0.00038104 rank 3
2023-02-17 04:43:25,347 DEBUG TRAIN Batch 1/1200 loss 53.043095 loss_att 108.582115 loss_ctc 59.128990 loss_rnnt 40.905014 hw_loss 0.410295 lr 0.00038136 rank 0
2023-02-17 04:44:42,169 DEBUG TRAIN Batch 1/1300 loss 52.074532 loss_att 70.962357 loss_ctc 58.474052 loss_rnnt 47.175156 hw_loss 0.503518 lr 0.00038532 rank 7
2023-02-17 04:44:42,173 DEBUG TRAIN Batch 1/1300 loss 37.664322 loss_att 57.945419 loss_ctc 43.444023 loss_rnnt 32.580448 hw_loss 0.481921 lr 0.00038672 rank 1
2023-02-17 04:44:42,173 DEBUG TRAIN Batch 1/1300 loss 35.936562 loss_att 50.123707 loss_ctc 40.998756 loss_rnnt 32.153774 hw_loss 0.507000 lr 0.00038568 rank 5
2023-02-17 04:44:42,176 DEBUG TRAIN Batch 1/1300 loss 89.032066 loss_att 160.744705 loss_ctc 102.394394 loss_rnnt 72.717087 hw_loss 0.357758 lr 0.00038552 rank 6
2023-02-17 04:44:42,177 DEBUG TRAIN Batch 1/1300 loss 57.215824 loss_att 128.558929 loss_ctc 61.539589 loss_rnnt 42.123455 hw_loss 0.463593 lr 0.00038552 rank 2
2023-02-17 04:44:42,180 DEBUG TRAIN Batch 1/1300 loss 89.252899 loss_att 194.619751 loss_ctc 94.372192 loss_rnnt 67.333878 hw_loss 0.305763 lr 0.00038492 rank 4
2023-02-17 04:44:42,182 DEBUG TRAIN Batch 1/1300 loss 94.126923 loss_att 170.420380 loss_ctc 95.683334 loss_rnnt 78.459030 hw_loss 0.378145 lr 0.00038536 rank 0
2023-02-17 04:44:42,221 DEBUG TRAIN Batch 1/1300 loss 97.639809 loss_att 194.838562 loss_ctc 107.820511 loss_rnnt 76.585716 hw_loss 0.481718 lr 0.00038504 rank 3
2023-02-17 04:46:01,486 DEBUG TRAIN Batch 1/1400 loss 110.994881 loss_att 163.873749 loss_ctc 131.304352 loss_rnnt 97.561981 hw_loss 0.279747 lr 0.00038932 rank 7
2023-02-17 04:46:01,487 DEBUG TRAIN Batch 1/1400 loss 60.267288 loss_att 150.640549 loss_ctc 61.962795 loss_rnnt 41.756905 hw_loss 0.393123 lr 0.00039072 rank 1
2023-02-17 04:46:01,489 DEBUG TRAIN Batch 1/1400 loss 95.005264 loss_att 174.566071 loss_ctc 103.910217 loss_rnnt 77.715317 hw_loss 0.357112 lr 0.00038904 rank 3
2023-02-17 04:46:01,490 DEBUG TRAIN Batch 1/1400 loss 97.992546 loss_att 176.635788 loss_ctc 114.543777 loss_rnnt 79.824081 hw_loss 0.436835 lr 0.00038952 rank 6
2023-02-17 04:46:01,491 DEBUG TRAIN Batch 1/1400 loss 93.774170 loss_att 187.572739 loss_ctc 99.994568 loss_rnnt 74.024490 hw_loss 0.301103 lr 0.00038968 rank 5
2023-02-17 04:46:01,491 DEBUG TRAIN Batch 1/1400 loss 112.740997 loss_att 208.822876 loss_ctc 120.702660 loss_rnnt 92.251610 hw_loss 0.396486 lr 0.00038952 rank 2
2023-02-17 04:46:01,496 DEBUG TRAIN Batch 1/1400 loss 78.982689 loss_att 161.018570 loss_ctc 89.852722 loss_rnnt 60.933655 hw_loss 0.360969 lr 0.00038892 rank 4
2023-02-17 04:46:01,500 DEBUG TRAIN Batch 1/1400 loss 51.458199 loss_att 125.389435 loss_ctc 50.749168 loss_rnnt 36.564281 hw_loss 0.379142 lr 0.00038936 rank 0
2023-02-17 04:47:19,092 DEBUG TRAIN Batch 1/1500 loss 75.440544 loss_att 142.829132 loss_ctc 82.318474 loss_rnnt 60.896202 hw_loss 0.280438 lr 0.00039368 rank 5
2023-02-17 04:47:19,093 DEBUG TRAIN Batch 1/1500 loss 83.523537 loss_att 167.430603 loss_ctc 94.892563 loss_rnnt 65.072937 hw_loss 0.287454 lr 0.00039472 rank 1
2023-02-17 04:47:19,094 DEBUG TRAIN Batch 1/1500 loss 76.846725 loss_att 143.901001 loss_ctc 80.048424 loss_rnnt 62.736488 hw_loss 0.510902 lr 0.00039332 rank 7
2023-02-17 04:47:19,094 DEBUG TRAIN Batch 1/1500 loss 68.235741 loss_att 137.532745 loss_ctc 77.349808 loss_rnnt 52.916737 hw_loss 0.458240 lr 0.00039352 rank 6
2023-02-17 04:47:19,097 DEBUG TRAIN Batch 1/1500 loss 100.069656 loss_att 198.712692 loss_ctc 109.328072 loss_rnnt 78.958801 hw_loss 0.277102 lr 0.00039352 rank 2
2023-02-17 04:47:19,097 DEBUG TRAIN Batch 1/1500 loss 67.300056 loss_att 129.299744 loss_ctc 77.678146 loss_rnnt 53.327454 hw_loss 0.354212 lr 0.00039336 rank 0
2023-02-17 04:47:19,105 DEBUG TRAIN Batch 1/1500 loss 61.644051 loss_att 141.044754 loss_ctc 66.920036 loss_rnnt 44.844128 hw_loss 0.405601 lr 0.00039304 rank 3
2023-02-17 04:47:19,142 DEBUG TRAIN Batch 1/1500 loss 64.628326 loss_att 128.492920 loss_ctc 77.094635 loss_rnnt 49.977547 hw_loss 0.404402 lr 0.00039292 rank 4
2023-02-17 04:48:35,862 DEBUG TRAIN Batch 1/1600 loss 64.168190 loss_att 138.741806 loss_ctc 70.459198 loss_rnnt 48.239555 hw_loss 0.328331 lr 0.00039732 rank 7
2023-02-17 04:48:35,862 DEBUG TRAIN Batch 1/1600 loss 63.522793 loss_att 115.173996 loss_ctc 68.166473 loss_rnnt 52.423901 hw_loss 0.280300 lr 0.00039752 rank 6
2023-02-17 04:48:35,864 DEBUG TRAIN Batch 1/1600 loss 70.068947 loss_att 129.843231 loss_ctc 81.602127 loss_rnnt 56.358841 hw_loss 0.407791 lr 0.00039768 rank 5
2023-02-17 04:48:35,869 DEBUG TRAIN Batch 1/1600 loss 69.228119 loss_att 147.775543 loss_ctc 77.207092 loss_rnnt 52.320766 hw_loss 0.251253 lr 0.00039736 rank 0
2023-02-17 04:48:35,870 DEBUG TRAIN Batch 1/1600 loss 69.233116 loss_att 153.506226 loss_ctc 77.479492 loss_rnnt 51.099403 hw_loss 0.336705 lr 0.00039692 rank 4
2023-02-17 04:48:35,870 DEBUG TRAIN Batch 1/1600 loss 91.629227 loss_att 183.085114 loss_ctc 105.970215 loss_rnnt 71.297943 hw_loss 0.239937 lr 0.00039752 rank 2
2023-02-17 04:48:35,871 DEBUG TRAIN Batch 1/1600 loss 49.262833 loss_att 114.830978 loss_ctc 52.783005 loss_rnnt 35.489441 hw_loss 0.357006 lr 0.00039704 rank 3
2023-02-17 04:48:35,873 DEBUG TRAIN Batch 1/1600 loss 76.807053 loss_att 152.814880 loss_ctc 87.274841 loss_rnnt 60.010815 hw_loss 0.373057 lr 0.00039872 rank 1
2023-02-17 04:49:52,379 DEBUG TRAIN Batch 1/1700 loss 67.449997 loss_att 124.866028 loss_ctc 68.730385 loss_rnnt 55.622070 hw_loss 0.326251 lr 0.00040132 rank 7
2023-02-17 04:49:52,380 DEBUG TRAIN Batch 1/1700 loss 65.206665 loss_att 128.789337 loss_ctc 73.638008 loss_rnnt 51.171768 hw_loss 0.364086 lr 0.00040104 rank 3
2023-02-17 04:49:52,382 DEBUG TRAIN Batch 1/1700 loss 68.653366 loss_att 140.796844 loss_ctc 82.704842 loss_rnnt 52.114265 hw_loss 0.444124 lr 0.00040092 rank 4
2023-02-17 04:49:52,384 DEBUG TRAIN Batch 1/1700 loss 72.998718 loss_att 129.531403 loss_ctc 88.337555 loss_rnnt 59.383396 hw_loss 0.494271 lr 0.00040152 rank 6
2023-02-17 04:49:52,385 DEBUG TRAIN Batch 1/1700 loss 51.367802 loss_att 116.268646 loss_ctc 55.270714 loss_rnnt 37.615776 hw_loss 0.471508 lr 0.00040168 rank 5
2023-02-17 04:49:52,388 DEBUG TRAIN Batch 1/1700 loss 109.530617 loss_att 200.917297 loss_ctc 120.042038 loss_rnnt 89.636658 hw_loss 0.403309 lr 0.00040272 rank 1
2023-02-17 04:49:52,390 DEBUG TRAIN Batch 1/1700 loss 75.331116 loss_att 149.251663 loss_ctc 88.641983 loss_rnnt 58.590790 hw_loss 0.340192 lr 0.00040136 rank 0
2023-02-17 04:49:52,391 DEBUG TRAIN Batch 1/1700 loss 67.341530 loss_att 141.995117 loss_ctc 78.954750 loss_rnnt 50.667381 hw_loss 0.365637 lr 0.00040152 rank 2
2023-02-17 04:51:12,532 DEBUG TRAIN Batch 1/1800 loss 70.410797 loss_att 143.876358 loss_ctc 81.892532 loss_rnnt 53.991074 hw_loss 0.366954 lr 0.00040672 rank 1
2023-02-17 04:51:12,534 DEBUG TRAIN Batch 1/1800 loss 31.161955 loss_att 55.278404 loss_ctc 41.515915 loss_rnnt 24.796598 hw_loss 0.302882 lr 0.00040552 rank 2
2023-02-17 04:51:12,534 DEBUG TRAIN Batch 1/1800 loss 93.235443 loss_att 176.830872 loss_ctc 105.520599 loss_rnnt 74.692459 hw_loss 0.348502 lr 0.00040568 rank 5
2023-02-17 04:51:12,539 DEBUG TRAIN Batch 1/1800 loss 52.957966 loss_att 96.205688 loss_ctc 53.922283 loss_rnnt 43.962166 hw_loss 0.408140 lr 0.00040552 rank 6
2023-02-17 04:51:12,539 DEBUG TRAIN Batch 1/1800 loss 50.716045 loss_att 114.845657 loss_ctc 51.212261 loss_rnnt 37.639084 hw_loss 0.346642 lr 0.00040532 rank 7
2023-02-17 04:51:12,541 DEBUG TRAIN Batch 1/1800 loss 74.694473 loss_att 128.370300 loss_ctc 82.769630 loss_rnnt 62.674526 hw_loss 0.390174 lr 0.00040504 rank 3
2023-02-17 04:51:12,550 DEBUG TRAIN Batch 1/1800 loss 52.529167 loss_att 101.295212 loss_ctc 63.215149 loss_rnnt 41.123482 hw_loss 0.426898 lr 0.00040536 rank 0
2023-02-17 04:51:12,595 DEBUG TRAIN Batch 1/1800 loss 78.024178 loss_att 146.159195 loss_ctc 93.931946 loss_rnnt 62.074390 hw_loss 0.378274 lr 0.00040492 rank 4
2023-02-17 04:52:28,178 DEBUG TRAIN Batch 1/1900 loss 86.034035 loss_att 170.649933 loss_ctc 90.200554 loss_rnnt 68.323914 hw_loss 0.433888 lr 0.00040904 rank 3
2023-02-17 04:52:28,179 DEBUG TRAIN Batch 1/1900 loss 50.151402 loss_att 73.273689 loss_ctc 55.157185 loss_rnnt 44.593075 hw_loss 0.499558 lr 0.00040892 rank 4
2023-02-17 04:52:28,179 DEBUG TRAIN Batch 1/1900 loss 52.264809 loss_att 95.334084 loss_ctc 56.184917 loss_rnnt 42.959084 hw_loss 0.317220 lr 0.00040932 rank 7
2023-02-17 04:52:28,180 DEBUG TRAIN Batch 1/1900 loss 75.206146 loss_att 164.189606 loss_ctc 91.746399 loss_rnnt 55.017090 hw_loss 0.350606 lr 0.00040952 rank 6
2023-02-17 04:52:28,183 DEBUG TRAIN Batch 1/1900 loss 46.294926 loss_att 87.550987 loss_ctc 56.424393 loss_rnnt 36.451057 hw_loss 0.453859 lr 0.00041072 rank 1
2023-02-17 04:52:28,182 DEBUG TRAIN Batch 1/1900 loss 37.400429 loss_att 67.230034 loss_ctc 42.868698 loss_rnnt 30.487450 hw_loss 0.408672 lr 0.00040968 rank 5
2023-02-17 04:52:28,186 DEBUG TRAIN Batch 1/1900 loss 77.011314 loss_att 159.627457 loss_ctc 89.917084 loss_rnnt 58.558346 hw_loss 0.391827 lr 0.00040936 rank 0
2023-02-17 04:52:28,186 DEBUG TRAIN Batch 1/1900 loss 70.038445 loss_att 147.510986 loss_ctc 78.460403 loss_rnnt 53.255638 hw_loss 0.310087 lr 0.00040952 rank 2
2023-02-17 04:53:45,355 DEBUG TRAIN Batch 1/2000 loss 85.553741 loss_att 171.640869 loss_ctc 94.094086 loss_rnnt 67.010132 hw_loss 0.351503 lr 0.00041304 rank 3
2023-02-17 04:53:45,356 DEBUG TRAIN Batch 1/2000 loss 78.671692 loss_att 170.022202 loss_ctc 89.468315 loss_rnnt 58.767284 hw_loss 0.365162 lr 0.00041352 rank 6
2023-02-17 04:53:45,357 DEBUG TRAIN Batch 1/2000 loss 52.034180 loss_att 107.663483 loss_ctc 53.127068 loss_rnnt 40.574062 hw_loss 0.353506 lr 0.00041368 rank 5
2023-02-17 04:53:45,360 DEBUG TRAIN Batch 1/2000 loss 72.406952 loss_att 159.914658 loss_ctc 76.955032 loss_rnnt 54.096954 hw_loss 0.378830 lr 0.00041472 rank 1
2023-02-17 04:53:45,361 DEBUG TRAIN Batch 1/2000 loss 48.803719 loss_att 131.440521 loss_ctc 51.180962 loss_rnnt 31.748882 hw_loss 0.394710 lr 0.00041332 rank 7
2023-02-17 04:53:45,362 DEBUG TRAIN Batch 1/2000 loss 56.014057 loss_att 127.672577 loss_ctc 63.522015 loss_rnnt 40.463665 hw_loss 0.408049 lr 0.00041292 rank 4
2023-02-17 04:53:45,365 DEBUG TRAIN Batch 1/2000 loss 99.804665 loss_att 195.332718 loss_ctc 111.521103 loss_rnnt 78.953186 hw_loss 0.344389 lr 0.00041336 rank 0
2023-02-17 04:53:45,406 DEBUG TRAIN Batch 1/2000 loss 61.156448 loss_att 140.406708 loss_ctc 67.199455 loss_rnnt 44.227673 hw_loss 0.511854 lr 0.00041352 rank 2
2023-02-17 04:55:04,879 DEBUG TRAIN Batch 1/2100 loss 73.203255 loss_att 147.574677 loss_ctc 90.186996 loss_rnnt 55.858246 hw_loss 0.386673 lr 0.00041704 rank 3
2023-02-17 04:55:04,880 DEBUG TRAIN Batch 1/2100 loss 89.167107 loss_att 170.166534 loss_ctc 110.042038 loss_rnnt 69.978119 hw_loss 0.385837 lr 0.00041732 rank 7
2023-02-17 04:55:04,880 DEBUG TRAIN Batch 1/2100 loss 47.129066 loss_att 107.256844 loss_ctc 49.504631 loss_rnnt 34.611458 hw_loss 0.328698 lr 0.00041752 rank 2
2023-02-17 04:55:04,880 DEBUG TRAIN Batch 1/2100 loss 40.246346 loss_att 106.929047 loss_ctc 43.967480 loss_rnnt 26.221067 hw_loss 0.361102 lr 0.00041752 rank 6
2023-02-17 04:55:04,886 DEBUG TRAIN Batch 1/2100 loss 49.871723 loss_att 117.188316 loss_ctc 50.003082 loss_rnnt 36.161224 hw_loss 0.430609 lr 0.00041736 rank 0
2023-02-17 04:55:04,886 DEBUG TRAIN Batch 1/2100 loss 76.596092 loss_att 164.672775 loss_ctc 90.758873 loss_rnnt 56.892029 hw_loss 0.375661 lr 0.00041692 rank 4
2023-02-17 04:55:04,905 DEBUG TRAIN Batch 1/2100 loss 67.168259 loss_att 128.651413 loss_ctc 78.122879 loss_rnnt 53.191265 hw_loss 0.412017 lr 0.00041872 rank 1
2023-02-17 04:55:04,911 DEBUG TRAIN Batch 1/2100 loss 41.410442 loss_att 104.758850 loss_ctc 53.679790 loss_rnnt 26.876669 hw_loss 0.427839 lr 0.00041768 rank 5
2023-02-17 04:56:23,061 DEBUG TRAIN Batch 1/2200 loss 63.904247 loss_att 139.251083 loss_ctc 67.377350 loss_rnnt 48.212715 hw_loss 0.298278 lr 0.00042132 rank 7
2023-02-17 04:56:23,063 DEBUG TRAIN Batch 1/2200 loss 95.721931 loss_att 153.478653 loss_ctc 110.200035 loss_rnnt 82.013458 hw_loss 0.425090 lr 0.00042168 rank 5
2023-02-17 04:56:23,063 DEBUG TRAIN Batch 1/2200 loss 86.977547 loss_att 180.144180 loss_ctc 93.632500 loss_rnnt 67.278275 hw_loss 0.334918 lr 0.00042092 rank 4
2023-02-17 04:56:23,063 DEBUG TRAIN Batch 1/2200 loss 61.788460 loss_att 131.393127 loss_ctc 67.948334 loss_rnnt 46.854275 hw_loss 0.359881 lr 0.00042104 rank 3
2023-02-17 04:56:23,063 DEBUG TRAIN Batch 1/2200 loss 54.514679 loss_att 112.516731 loss_ctc 63.866928 loss_rnnt 41.499664 hw_loss 0.314325 lr 0.00042136 rank 0
2023-02-17 04:56:23,064 DEBUG TRAIN Batch 1/2200 loss 58.533848 loss_att 122.631203 loss_ctc 65.806396 loss_rnnt 44.577972 hw_loss 0.312624 lr 0.00042152 rank 6
2023-02-17 04:56:23,065 DEBUG TRAIN Batch 1/2200 loss 43.630016 loss_att 96.623329 loss_ctc 40.941139 loss_rnnt 33.199535 hw_loss 0.356872 lr 0.00042272 rank 1
2023-02-17 04:56:23,071 DEBUG TRAIN Batch 1/2200 loss 86.782112 loss_att 163.358978 loss_ctc 94.847900 loss_rnnt 70.191246 hw_loss 0.375090 lr 0.00042152 rank 2
2023-02-17 04:57:37,850 DEBUG TRAIN Batch 1/2300 loss 66.633003 loss_att 122.231033 loss_ctc 73.050446 loss_rnnt 54.461369 hw_loss 0.368167 lr 0.00042672 rank 1
2023-02-17 04:57:37,852 DEBUG TRAIN Batch 1/2300 loss 50.626259 loss_att 105.576721 loss_ctc 55.736931 loss_rnnt 38.800571 hw_loss 0.289068 lr 0.00042532 rank 7
2023-02-17 04:57:37,851 DEBUG TRAIN Batch 1/2300 loss 60.644123 loss_att 108.839485 loss_ctc 74.573425 loss_rnnt 48.956696 hw_loss 0.358330 lr 0.00042568 rank 5
2023-02-17 04:57:37,852 DEBUG TRAIN Batch 1/2300 loss 60.462067 loss_att 132.660217 loss_ctc 68.312241 loss_rnnt 44.785770 hw_loss 0.356207 lr 0.00042492 rank 4
2023-02-17 04:57:37,852 DEBUG TRAIN Batch 1/2300 loss 51.529034 loss_att 114.535156 loss_ctc 60.477516 loss_rnnt 37.530701 hw_loss 0.382453 lr 0.00042552 rank 6
2023-02-17 04:57:37,854 DEBUG TRAIN Batch 1/2300 loss 91.259727 loss_att 167.663010 loss_ctc 104.945160 loss_rnnt 73.989990 hw_loss 0.308176 lr 0.00042552 rank 2
2023-02-17 04:57:37,855 DEBUG TRAIN Batch 1/2300 loss 40.753540 loss_att 100.224609 loss_ctc 44.000843 loss_rnnt 28.239962 hw_loss 0.349483 lr 0.00042536 rank 0
2023-02-17 04:57:37,899 DEBUG TRAIN Batch 1/2300 loss 46.388149 loss_att 108.052200 loss_ctc 50.335060 loss_rnnt 33.354263 hw_loss 0.327790 lr 0.00042504 rank 3
2023-02-17 04:58:55,068 DEBUG TRAIN Batch 1/2400 loss 77.132179 loss_att 136.024017 loss_ctc 90.923637 loss_rnnt 63.363430 hw_loss 0.284094 lr 0.00042952 rank 6
2023-02-17 04:58:55,070 DEBUG TRAIN Batch 1/2400 loss 63.084099 loss_att 116.997681 loss_ctc 68.430771 loss_rnnt 51.408447 hw_loss 0.337584 lr 0.00042952 rank 2
2023-02-17 04:58:55,071 DEBUG TRAIN Batch 1/2400 loss 66.712479 loss_att 119.201912 loss_ctc 77.644783 loss_rnnt 54.499214 hw_loss 0.483240 lr 0.00043072 rank 1
2023-02-17 04:58:55,072 DEBUG TRAIN Batch 1/2400 loss 74.690231 loss_att 141.753311 loss_ctc 93.292572 loss_rnnt 58.577900 hw_loss 0.411375 lr 0.00042932 rank 7
2023-02-17 04:58:55,072 DEBUG TRAIN Batch 1/2400 loss 58.759995 loss_att 121.669495 loss_ctc 68.791382 loss_rnnt 44.651833 hw_loss 0.353888 lr 0.00042968 rank 5
2023-02-17 04:58:55,072 DEBUG TRAIN Batch 1/2400 loss 50.726444 loss_att 96.311996 loss_ctc 68.093597 loss_rnnt 39.078434 hw_loss 0.403647 lr 0.00042904 rank 3
2023-02-17 04:58:55,075 DEBUG TRAIN Batch 1/2400 loss 59.712399 loss_att 136.093826 loss_ctc 67.367912 loss_rnnt 43.196663 hw_loss 0.410086 lr 0.00042892 rank 4
2023-02-17 04:58:55,077 DEBUG TRAIN Batch 1/2400 loss 67.973892 loss_att 126.780869 loss_ctc 81.175766 loss_rnnt 54.245831 hw_loss 0.387041 lr 0.00042936 rank 0
2023-02-17 05:00:15,360 DEBUG TRAIN Batch 1/2500 loss 48.214703 loss_att 84.114693 loss_ctc 55.803139 loss_rnnt 39.765442 hw_loss 0.482763 lr 0.00043292 rank 4
2023-02-17 05:00:15,364 DEBUG TRAIN Batch 1/2500 loss 37.979553 loss_att 107.593384 loss_ctc 34.794918 loss_rnnt 24.311470 hw_loss 0.318623 lr 0.00043352 rank 6
2023-02-17 05:00:15,365 DEBUG TRAIN Batch 1/2500 loss 57.378113 loss_att 98.716187 loss_ctc 71.541176 loss_rnnt 46.987545 hw_loss 0.439777 lr 0.00043368 rank 5
2023-02-17 05:00:15,366 DEBUG TRAIN Batch 1/2500 loss 50.348530 loss_att 85.728561 loss_ctc 56.340317 loss_rnnt 42.382294 hw_loss 0.171229 lr 0.00043352 rank 2
2023-02-17 05:00:15,366 DEBUG TRAIN Batch 1/2500 loss 57.247116 loss_att 120.205978 loss_ctc 68.402435 loss_rnnt 42.950771 hw_loss 0.407240 lr 0.00043472 rank 1
2023-02-17 05:00:15,368 DEBUG TRAIN Batch 1/2500 loss 42.255260 loss_att 72.555695 loss_ctc 48.557117 loss_rnnt 35.101212 hw_loss 0.475707 lr 0.00043332 rank 7
2023-02-17 05:00:15,370 DEBUG TRAIN Batch 1/2500 loss 23.702633 loss_att 30.988424 loss_ctc 28.126566 loss_rnnt 21.414192 hw_loss 0.452670 lr 0.00043336 rank 0
2023-02-17 05:00:15,413 DEBUG TRAIN Batch 1/2500 loss 26.709570 loss_att 34.767658 loss_ctc 33.400425 loss_rnnt 23.859625 hw_loss 0.649153 lr 0.00043304 rank 3
2023-02-17 05:01:31,388 DEBUG TRAIN Batch 1/2600 loss 65.311508 loss_att 130.031204 loss_ctc 79.445816 loss_rnnt 50.323116 hw_loss 0.299773 lr 0.00043752 rank 6
2023-02-17 05:01:31,391 DEBUG TRAIN Batch 1/2600 loss 66.984901 loss_att 133.336792 loss_ctc 79.655540 loss_rnnt 51.841644 hw_loss 0.343995 lr 0.00043732 rank 7
2023-02-17 05:01:31,392 DEBUG TRAIN Batch 1/2600 loss 59.661472 loss_att 124.007561 loss_ctc 64.472549 loss_rnnt 45.982224 hw_loss 0.316041 lr 0.00043768 rank 5
2023-02-17 05:01:31,394 DEBUG TRAIN Batch 1/2600 loss 60.237488 loss_att 129.994141 loss_ctc 69.626968 loss_rnnt 44.858551 hw_loss 0.329391 lr 0.00043704 rank 3
2023-02-17 05:01:31,397 DEBUG TRAIN Batch 1/2600 loss 52.096443 loss_att 124.298828 loss_ctc 54.307190 loss_rnnt 37.197571 hw_loss 0.306809 lr 0.00043752 rank 2
2023-02-17 05:01:31,397 DEBUG TRAIN Batch 1/2600 loss 83.792404 loss_att 163.371826 loss_ctc 96.799858 loss_rnnt 65.980576 hw_loss 0.303037 lr 0.00043872 rank 1
2023-02-17 05:01:31,399 DEBUG TRAIN Batch 1/2600 loss 70.573288 loss_att 134.366241 loss_ctc 73.387993 loss_rnnt 57.222881 hw_loss 0.405973 lr 0.00043692 rank 4
2023-02-17 05:01:31,439 DEBUG TRAIN Batch 1/2600 loss 54.942299 loss_att 114.302338 loss_ctc 66.134308 loss_rnnt 41.415489 hw_loss 0.304751 lr 0.00043736 rank 0
2023-02-17 05:02:47,254 DEBUG TRAIN Batch 1/2700 loss 73.399040 loss_att 122.932274 loss_ctc 86.480118 loss_rnnt 61.536011 hw_loss 0.397946 lr 0.00044168 rank 5
2023-02-17 05:02:47,254 DEBUG TRAIN Batch 1/2700 loss 61.841007 loss_att 128.975876 loss_ctc 72.947052 loss_rnnt 46.752274 hw_loss 0.339286 lr 0.00044152 rank 6
2023-02-17 05:02:47,259 DEBUG TRAIN Batch 1/2700 loss 92.188606 loss_att 158.285065 loss_ctc 97.661179 loss_rnnt 78.018814 hw_loss 0.414054 lr 0.00044152 rank 2
2023-02-17 05:02:47,260 DEBUG TRAIN Batch 1/2700 loss 60.557751 loss_att 134.171097 loss_ctc 65.435989 loss_rnnt 44.965431 hw_loss 0.411023 lr 0.00044132 rank 7
2023-02-17 05:02:47,262 DEBUG TRAIN Batch 1/2700 loss 73.915924 loss_att 152.893188 loss_ctc 84.222916 loss_rnnt 56.557644 hw_loss 0.353545 lr 0.00044092 rank 4
2023-02-17 05:02:47,262 DEBUG TRAIN Batch 1/2700 loss 62.596340 loss_att 132.938416 loss_ctc 68.764000 loss_rnnt 47.499962 hw_loss 0.385522 lr 0.00044104 rank 3
2023-02-17 05:02:47,266 DEBUG TRAIN Batch 1/2700 loss 63.545490 loss_att 146.207428 loss_ctc 70.509140 loss_rnnt 45.920921 hw_loss 0.306928 lr 0.00044272 rank 1
2023-02-17 05:02:47,268 DEBUG TRAIN Batch 1/2700 loss 54.740078 loss_att 137.397369 loss_ctc 63.059658 loss_rnnt 36.926388 hw_loss 0.324299 lr 0.00044136 rank 0
2023-02-17 05:04:04,878 DEBUG TRAIN Batch 1/2800 loss 59.303375 loss_att 129.503387 loss_ctc 66.004356 loss_rnnt 44.174232 hw_loss 0.366877 lr 0.00044568 rank 5
2023-02-17 05:04:04,883 DEBUG TRAIN Batch 1/2800 loss 56.580772 loss_att 112.505852 loss_ctc 64.003204 loss_rnnt 44.204575 hw_loss 0.377849 lr 0.00044536 rank 0
2023-02-17 05:04:04,883 DEBUG TRAIN Batch 1/2800 loss 57.291817 loss_att 119.917374 loss_ctc 66.569084 loss_rnnt 43.341900 hw_loss 0.352187 lr 0.00044552 rank 6
2023-02-17 05:04:04,887 DEBUG TRAIN Batch 1/2800 loss 52.144932 loss_att 124.649933 loss_ctc 64.227768 loss_rnnt 35.856998 hw_loss 0.329789 lr 0.00044492 rank 4
2023-02-17 05:04:04,888 DEBUG TRAIN Batch 1/2800 loss 61.449486 loss_att 112.904022 loss_ctc 72.634262 loss_rnnt 49.482250 hw_loss 0.346922 lr 0.00044532 rank 7
2023-02-17 05:04:04,889 DEBUG TRAIN Batch 1/2800 loss 56.582722 loss_att 110.685471 loss_ctc 71.890717 loss_rnnt 43.540661 hw_loss 0.338334 lr 0.00044672 rank 1
2023-02-17 05:04:04,892 DEBUG TRAIN Batch 1/2800 loss 76.830414 loss_att 134.681915 loss_ctc 89.834244 loss_rnnt 63.362709 hw_loss 0.306668 lr 0.00044552 rank 2
2023-02-17 05:04:04,932 DEBUG TRAIN Batch 1/2800 loss 76.775642 loss_att 148.040253 loss_ctc 93.134224 loss_rnnt 60.136932 hw_loss 0.383699 lr 0.00044504 rank 3
2023-02-17 05:05:23,578 DEBUG TRAIN Batch 1/2900 loss 47.376362 loss_att 94.731155 loss_ctc 54.892567 loss_rnnt 36.668758 hw_loss 0.439650 lr 0.00044952 rank 2
2023-02-17 05:05:23,579 DEBUG TRAIN Batch 1/2900 loss 59.594917 loss_att 116.364380 loss_ctc 68.309738 loss_rnnt 46.825157 hw_loss 0.476049 lr 0.00044904 rank 3
2023-02-17 05:05:23,580 DEBUG TRAIN Batch 1/2900 loss 45.878151 loss_att 93.815948 loss_ctc 52.241730 loss_rnnt 35.238403 hw_loss 0.381956 lr 0.00045072 rank 1
2023-02-17 05:05:23,580 DEBUG TRAIN Batch 1/2900 loss 62.810764 loss_att 111.210297 loss_ctc 70.890045 loss_rnnt 51.853729 hw_loss 0.374795 lr 0.00044968 rank 5
2023-02-17 05:05:23,581 DEBUG TRAIN Batch 1/2900 loss 66.691559 loss_att 125.214722 loss_ctc 71.085846 loss_rnnt 54.202099 hw_loss 0.372991 lr 0.00044892 rank 4
2023-02-17 05:05:23,581 DEBUG TRAIN Batch 1/2900 loss 55.826870 loss_att 139.568283 loss_ctc 65.113800 loss_rnnt 37.662937 hw_loss 0.332616 lr 0.00044932 rank 7
2023-02-17 05:05:23,588 DEBUG TRAIN Batch 1/2900 loss 52.355736 loss_att 101.653175 loss_ctc 63.619873 loss_rnnt 40.821571 hw_loss 0.323981 lr 0.00044952 rank 6
2023-02-17 05:05:23,631 DEBUG TRAIN Batch 1/2900 loss 58.848038 loss_att 101.795227 loss_ctc 66.145515 loss_rnnt 49.117538 hw_loss 0.315127 lr 0.00044936 rank 0
2023-02-17 05:06:40,266 DEBUG TRAIN Batch 1/3000 loss 49.252045 loss_att 101.334320 loss_ctc 60.614288 loss_rnnt 37.104004 hw_loss 0.406163 lr 0.00045304 rank 3
2023-02-17 05:06:40,268 DEBUG TRAIN Batch 1/3000 loss 74.823441 loss_att 132.958755 loss_ctc 88.150269 loss_rnnt 61.233646 hw_loss 0.348402 lr 0.00045292 rank 4
2023-02-17 05:06:40,270 DEBUG TRAIN Batch 1/3000 loss 63.737450 loss_att 112.967972 loss_ctc 76.795998 loss_rnnt 51.954163 hw_loss 0.367586 lr 0.00045332 rank 7
2023-02-17 05:06:40,271 DEBUG TRAIN Batch 1/3000 loss 80.953873 loss_att 136.220001 loss_ctc 100.707382 loss_rnnt 67.102234 hw_loss 0.308657 lr 0.00045352 rank 6
2023-02-17 05:06:40,273 DEBUG TRAIN Batch 1/3000 loss 58.106323 loss_att 127.026978 loss_ctc 73.245300 loss_rnnt 42.111740 hw_loss 0.359852 lr 0.00045368 rank 5
2023-02-17 05:06:40,274 DEBUG TRAIN Batch 1/3000 loss 49.077656 loss_att 103.556259 loss_ctc 61.279213 loss_rnnt 36.335220 hw_loss 0.412198 lr 0.00045472 rank 1
2023-02-17 05:06:40,276 DEBUG TRAIN Batch 1/3000 loss 59.326515 loss_att 115.247620 loss_ctc 64.868820 loss_rnnt 47.214149 hw_loss 0.354690 lr 0.00045336 rank 0
2023-02-17 05:06:40,277 DEBUG TRAIN Batch 1/3000 loss 49.390301 loss_att 92.142471 loss_ctc 63.605659 loss_rnnt 38.761612 hw_loss 0.342897 lr 0.00045352 rank 2
2023-02-17 05:07:56,880 DEBUG TRAIN Batch 1/3100 loss 25.550871 loss_att 41.291279 loss_ctc 28.784590 loss_rnnt 21.691729 hw_loss 0.524812 lr 0.00045752 rank 6
2023-02-17 05:07:56,881 DEBUG TRAIN Batch 1/3100 loss 61.028435 loss_att 107.005493 loss_ctc 70.776604 loss_rnnt 50.378464 hw_loss 0.290262 lr 0.00045872 rank 1
2023-02-17 05:07:56,885 DEBUG TRAIN Batch 1/3100 loss 33.325279 loss_att 42.181732 loss_ctc 38.423615 loss_rnnt 30.585751 hw_loss 0.540865 lr 0.00045736 rank 0
2023-02-17 05:07:56,886 DEBUG TRAIN Batch 1/3100 loss 48.020672 loss_att 86.150108 loss_ctc 53.410015 loss_rnnt 39.486141 hw_loss 0.356383 lr 0.00045704 rank 3
2023-02-17 05:07:56,886 DEBUG TRAIN Batch 1/3100 loss 60.817913 loss_att 115.016029 loss_ctc 64.578629 loss_rnnt 49.324986 hw_loss 0.284762 lr 0.00045768 rank 5
2023-02-17 05:07:56,887 DEBUG TRAIN Batch 1/3100 loss 45.696529 loss_att 82.098831 loss_ctc 51.252605 loss_rnnt 37.469028 hw_loss 0.386683 lr 0.00045692 rank 4
2023-02-17 05:07:56,887 DEBUG TRAIN Batch 1/3100 loss 61.016163 loss_att 105.681900 loss_ctc 71.716507 loss_rnnt 50.445152 hw_loss 0.395915 lr 0.00045732 rank 7
2023-02-17 05:07:56,934 DEBUG TRAIN Batch 1/3100 loss 56.777809 loss_att 80.857422 loss_ctc 68.726112 loss_rnnt 50.189846 hw_loss 0.335506 lr 0.00045752 rank 2
2023-02-17 05:09:19,744 DEBUG TRAIN Batch 1/3200 loss 24.622850 loss_att 43.062153 loss_ctc 27.868921 loss_rnnt 20.262718 hw_loss 0.448989 lr 0.00046132 rank 7
2023-02-17 05:09:19,748 DEBUG TRAIN Batch 1/3200 loss 43.666580 loss_att 77.661453 loss_ctc 55.506195 loss_rnnt 35.030209 hw_loss 0.485221 lr 0.00046092 rank 4
2023-02-17 05:09:19,748 DEBUG TRAIN Batch 1/3200 loss 41.324581 loss_att 78.337326 loss_ctc 44.535992 loss_rnnt 33.264320 hw_loss 0.430357 lr 0.00046168 rank 5
2023-02-17 05:09:19,750 DEBUG TRAIN Batch 1/3200 loss 73.880661 loss_att 135.734772 loss_ctc 84.520676 loss_rnnt 59.918777 hw_loss 0.323243 lr 0.00046104 rank 3
2023-02-17 05:09:19,750 DEBUG TRAIN Batch 1/3200 loss 35.614624 loss_att 51.979828 loss_ctc 42.355953 loss_rnnt 31.223518 hw_loss 0.411032 lr 0.00046272 rank 1
2023-02-17 05:09:19,751 DEBUG TRAIN Batch 1/3200 loss 47.414375 loss_att 108.993309 loss_ctc 60.449913 loss_rnnt 33.195690 hw_loss 0.309048 lr 0.00046152 rank 6
2023-02-17 05:09:19,753 DEBUG TRAIN Batch 1/3200 loss 72.483757 loss_att 136.671265 loss_ctc 88.785904 loss_rnnt 57.263592 hw_loss 0.391960 lr 0.00046152 rank 2
2023-02-17 05:09:19,754 DEBUG TRAIN Batch 1/3200 loss 63.857933 loss_att 127.874313 loss_ctc 77.891632 loss_rnnt 48.952972 hw_loss 0.432236 lr 0.00046136 rank 0
2023-02-17 05:10:37,334 DEBUG TRAIN Batch 1/3300 loss 92.802948 loss_att 168.216492 loss_ctc 109.628708 loss_rnnt 75.283012 hw_loss 0.363373 lr 0.00046492 rank 4
2023-02-17 05:10:37,339 DEBUG TRAIN Batch 1/3300 loss 37.437862 loss_att 93.516006 loss_ctc 44.798126 loss_rnnt 25.039091 hw_loss 0.378321 lr 0.00046504 rank 3
2023-02-17 05:10:37,338 DEBUG TRAIN Batch 1/3300 loss 61.818550 loss_att 135.327637 loss_ctc 70.634857 loss_rnnt 45.740231 hw_loss 0.376873 lr 0.00046532 rank 7
2023-02-17 05:10:37,339 DEBUG TRAIN Batch 1/3300 loss 60.934807 loss_att 133.082474 loss_ctc 63.410442 loss_rnnt 45.952782 hw_loss 0.417011 lr 0.00046552 rank 6
2023-02-17 05:10:37,340 DEBUG TRAIN Batch 1/3300 loss 55.276657 loss_att 121.209229 loss_ctc 60.444672 loss_rnnt 41.205814 hw_loss 0.366109 lr 0.00046672 rank 1
2023-02-17 05:10:37,342 DEBUG TRAIN Batch 1/3300 loss 56.513969 loss_att 113.364342 loss_ctc 64.422089 loss_rnnt 43.931087 hw_loss 0.296993 lr 0.00046552 rank 2
2023-02-17 05:10:37,342 DEBUG TRAIN Batch 1/3300 loss 97.186752 loss_att 177.751465 loss_ctc 116.172714 loss_rnnt 78.363327 hw_loss 0.335664 lr 0.00046568 rank 5
2023-02-17 05:10:37,345 DEBUG TRAIN Batch 1/3300 loss 65.238571 loss_att 121.708359 loss_ctc 81.877029 loss_rnnt 51.551537 hw_loss 0.327409 lr 0.00046536 rank 0
2023-02-17 05:11:54,552 DEBUG TRAIN Batch 1/3400 loss 57.406052 loss_att 99.507141 loss_ctc 60.230881 loss_rnnt 48.427521 hw_loss 0.340636 lr 0.00046968 rank 5
2023-02-17 05:11:54,552 DEBUG TRAIN Batch 1/3400 loss 56.799328 loss_att 119.295479 loss_ctc 76.138947 loss_rnnt 41.563019 hw_loss 0.297115 lr 0.00046892 rank 4
2023-02-17 05:11:54,557 DEBUG TRAIN Batch 1/3400 loss 44.426792 loss_att 93.090027 loss_ctc 54.283367 loss_rnnt 33.151539 hw_loss 0.428236 lr 0.00046932 rank 7
2023-02-17 05:11:54,557 DEBUG TRAIN Batch 1/3400 loss 61.572269 loss_att 118.913986 loss_ctc 72.679245 loss_rnnt 48.457935 hw_loss 0.309479 lr 0.00046936 rank 0
2023-02-17 05:11:54,557 DEBUG TRAIN Batch 1/3400 loss 54.223312 loss_att 109.697906 loss_ctc 70.105133 loss_rnnt 40.799328 hw_loss 0.396541 lr 0.00046952 rank 6
2023-02-17 05:11:54,559 DEBUG TRAIN Batch 1/3400 loss 79.122421 loss_att 138.973816 loss_ctc 92.414749 loss_rnnt 65.142952 hw_loss 0.444151 lr 0.00047072 rank 1
2023-02-17 05:11:54,559 DEBUG TRAIN Batch 1/3400 loss 70.475861 loss_att 128.940887 loss_ctc 80.894058 loss_rnnt 57.181492 hw_loss 0.398003 lr 0.00046952 rank 2
2023-02-17 05:11:54,598 DEBUG TRAIN Batch 1/3400 loss 53.232082 loss_att 129.364685 loss_ctc 65.883636 loss_rnnt 36.140678 hw_loss 0.333775 lr 0.00046904 rank 3
2023-02-17 05:13:12,798 DEBUG TRAIN Batch 1/3500 loss 59.952110 loss_att 115.043869 loss_ctc 73.352692 loss_rnnt 46.937302 hw_loss 0.393209 lr 0.00047352 rank 6
2023-02-17 05:13:12,801 DEBUG TRAIN Batch 1/3500 loss 64.421036 loss_att 113.280426 loss_ctc 71.149933 loss_rnnt 53.584679 hw_loss 0.313683 lr 0.00047292 rank 4
2023-02-17 05:13:12,802 DEBUG TRAIN Batch 1/3500 loss 64.013657 loss_att 114.974747 loss_ctc 80.245163 loss_rnnt 51.464024 hw_loss 0.362272 lr 0.00047332 rank 7
2023-02-17 05:13:12,804 DEBUG TRAIN Batch 1/3500 loss 51.098598 loss_att 91.008835 loss_ctc 62.984642 loss_rnnt 41.276302 hw_loss 0.478951 lr 0.00047472 rank 1
2023-02-17 05:13:12,807 DEBUG TRAIN Batch 1/3500 loss 98.880310 loss_att 142.862640 loss_ctc 125.134109 loss_rnnt 86.409531 hw_loss 0.325884 lr 0.00047352 rank 2
2023-02-17 05:13:12,813 DEBUG TRAIN Batch 1/3500 loss 67.622604 loss_att 113.230202 loss_ctc 85.706390 loss_rnnt 55.862854 hw_loss 0.425727 lr 0.00047336 rank 0
2023-02-17 05:13:12,813 DEBUG TRAIN Batch 1/3500 loss 60.544048 loss_att 115.377159 loss_ctc 62.355156 loss_rnnt 49.143616 hw_loss 0.360612 lr 0.00047368 rank 5
2023-02-17 05:13:12,846 DEBUG TRAIN Batch 1/3500 loss 59.266975 loss_att 112.656555 loss_ctc 70.739319 loss_rnnt 46.823357 hw_loss 0.442607 lr 0.00047304 rank 3
2023-02-17 05:14:31,898 DEBUG TRAIN Batch 1/3600 loss 31.638235 loss_att 65.595619 loss_ctc 38.825504 loss_rnnt 23.688892 hw_loss 0.374181 lr 0.00047752 rank 6
2023-02-17 05:14:31,898 DEBUG TRAIN Batch 1/3600 loss 46.248276 loss_att 84.937256 loss_ctc 54.690880 loss_rnnt 37.179207 hw_loss 0.385485 lr 0.00047704 rank 3
2023-02-17 05:14:31,900 DEBUG TRAIN Batch 1/3600 loss 53.254650 loss_att 95.960022 loss_ctc 69.195747 loss_rnnt 42.380058 hw_loss 0.390058 lr 0.00047736 rank 0
2023-02-17 05:14:31,901 DEBUG TRAIN Batch 1/3600 loss 30.055283 loss_att 73.345932 loss_ctc 34.811913 loss_rnnt 20.570858 hw_loss 0.360146 lr 0.00047752 rank 2
2023-02-17 05:14:31,901 DEBUG TRAIN Batch 1/3600 loss 50.654121 loss_att 103.093582 loss_ctc 58.004322 loss_rnnt 38.966301 hw_loss 0.412307 lr 0.00047692 rank 4
2023-02-17 05:14:31,902 DEBUG TRAIN Batch 1/3600 loss 73.221939 loss_att 131.271439 loss_ctc 90.103958 loss_rnnt 59.213627 hw_loss 0.276522 lr 0.00047732 rank 7
2023-02-17 05:14:31,902 DEBUG TRAIN Batch 1/3600 loss 42.462444 loss_att 86.010429 loss_ctc 51.282372 loss_rnnt 32.328030 hw_loss 0.466557 lr 0.00047768 rank 5
2023-02-17 05:14:31,905 DEBUG TRAIN Batch 1/3600 loss 75.307442 loss_att 143.024963 loss_ctc 84.281799 loss_rnnt 60.370037 hw_loss 0.369973 lr 0.00047872 rank 1
2023-02-17 05:15:47,405 DEBUG TRAIN Batch 1/3700 loss 56.402924 loss_att 97.119186 loss_ctc 71.037354 loss_rnnt 46.108917 hw_loss 0.374054 lr 0.00048104 rank 3
2023-02-17 05:15:47,406 DEBUG TRAIN Batch 1/3700 loss 29.574800 loss_att 49.590141 loss_ctc 37.720711 loss_rnnt 24.209856 hw_loss 0.517037 lr 0.00048152 rank 6
2023-02-17 05:15:47,407 DEBUG TRAIN Batch 1/3700 loss 49.417351 loss_att 108.854126 loss_ctc 62.507805 loss_rnnt 35.626862 hw_loss 0.295751 lr 0.00048168 rank 5
2023-02-17 05:15:47,408 DEBUG TRAIN Batch 1/3700 loss 55.705498 loss_att 92.739624 loss_ctc 69.287949 loss_rnnt 46.305252 hw_loss 0.342048 lr 0.00048132 rank 7
2023-02-17 05:15:47,410 DEBUG TRAIN Batch 1/3700 loss 54.741581 loss_att 103.827423 loss_ctc 69.962128 loss_rnnt 42.692249 hw_loss 0.380166 lr 0.00048272 rank 1
2023-02-17 05:15:47,410 DEBUG TRAIN Batch 1/3700 loss 56.384972 loss_att 88.821312 loss_ctc 69.025406 loss_rnnt 47.983379 hw_loss 0.429250 lr 0.00048152 rank 2
2023-02-17 05:15:47,411 DEBUG TRAIN Batch 1/3700 loss 32.699253 loss_att 40.166786 loss_ctc 42.708775 loss_rnnt 29.620611 hw_loss 0.469751 lr 0.00048136 rank 0
2023-02-17 05:15:47,411 DEBUG TRAIN Batch 1/3700 loss 40.621407 loss_att 85.946579 loss_ctc 50.675835 loss_rnnt 30.003973 hw_loss 0.397143 lr 0.00048092 rank 4
2023-02-17 05:17:04,433 DEBUG TRAIN Batch 1/3800 loss 73.903053 loss_att 127.884232 loss_ctc 78.057846 loss_rnnt 62.355812 hw_loss 0.369436 lr 0.00048552 rank 6
2023-02-17 05:17:04,437 DEBUG TRAIN Batch 1/3800 loss 48.822365 loss_att 83.419479 loss_ctc 61.377563 loss_rnnt 40.014542 hw_loss 0.401951 lr 0.00048532 rank 7
2023-02-17 05:17:04,439 DEBUG TRAIN Batch 1/3800 loss 45.372402 loss_att 66.247322 loss_ctc 56.062252 loss_rnnt 39.563618 hw_loss 0.390916 lr 0.00048672 rank 1
2023-02-17 05:17:04,444 DEBUG TRAIN Batch 1/3800 loss 60.188988 loss_att 109.303001 loss_ctc 65.653763 loss_rnnt 49.462204 hw_loss 0.328769 lr 0.00048552 rank 2
2023-02-17 05:17:04,445 DEBUG TRAIN Batch 1/3800 loss 38.811668 loss_att 84.193802 loss_ctc 42.564663 loss_rnnt 29.038670 hw_loss 0.367818 lr 0.00048536 rank 0
2023-02-17 05:17:04,446 DEBUG TRAIN Batch 1/3800 loss 65.895149 loss_att 105.756058 loss_ctc 78.351265 loss_rnnt 56.057262 hw_loss 0.384170 lr 0.00048492 rank 4
2023-02-17 05:17:04,449 DEBUG TRAIN Batch 1/3800 loss 59.160824 loss_att 102.700424 loss_ctc 71.225014 loss_rnnt 48.709621 hw_loss 0.252612 lr 0.00048568 rank 5
2023-02-17 05:17:04,482 DEBUG TRAIN Batch 1/3800 loss 33.488274 loss_att 42.444572 loss_ctc 37.410801 loss_rnnt 30.914551 hw_loss 0.486485 lr 0.00048504 rank 3
2023-02-17 05:18:23,533 DEBUG TRAIN Batch 1/3900 loss 61.145283 loss_att 111.077332 loss_ctc 74.364899 loss_rnnt 49.153240 hw_loss 0.455659 lr 0.00048952 rank 6
2023-02-17 05:18:23,535 DEBUG TRAIN Batch 1/3900 loss 65.087448 loss_att 135.080032 loss_ctc 79.238213 loss_rnnt 48.995342 hw_loss 0.387804 lr 0.00049072 rank 1
2023-02-17 05:18:23,539 DEBUG TRAIN Batch 1/3900 loss 23.178621 loss_att 31.628107 loss_ctc 25.728579 loss_rnnt 20.840551 hw_loss 0.577836 lr 0.00048932 rank 7
2023-02-17 05:18:23,539 DEBUG TRAIN Batch 1/3900 loss 56.966946 loss_att 96.361115 loss_ctc 69.903893 loss_rnnt 47.195412 hw_loss 0.314571 lr 0.00048968 rank 5
2023-02-17 05:18:23,540 DEBUG TRAIN Batch 1/3900 loss 49.128059 loss_att 93.023727 loss_ctc 57.490704 loss_rnnt 39.034462 hw_loss 0.373950 lr 0.00048936 rank 0
2023-02-17 05:18:23,545 DEBUG TRAIN Batch 1/3900 loss 39.784283 loss_att 89.156212 loss_ctc 49.682537 loss_rnnt 28.355488 hw_loss 0.439953 lr 0.00048952 rank 2
2023-02-17 05:18:23,545 DEBUG TRAIN Batch 1/3900 loss 54.274605 loss_att 112.707733 loss_ctc 69.220100 loss_rnnt 40.367218 hw_loss 0.427548 lr 0.00048892 rank 4
2023-02-17 05:18:23,587 DEBUG TRAIN Batch 1/3900 loss 51.796371 loss_att 112.255959 loss_ctc 70.587524 loss_rnnt 37.029510 hw_loss 0.317731 lr 0.00048904 rank 3
2023-02-17 05:19:40,151 DEBUG TRAIN Batch 1/4000 loss 52.161514 loss_att 98.606155 loss_ctc 64.471741 loss_rnnt 41.063232 hw_loss 0.314975 lr 0.00049472 rank 1
2023-02-17 05:19:40,155 DEBUG TRAIN Batch 1/4000 loss 76.268158 loss_att 140.924026 loss_ctc 99.297760 loss_rnnt 60.052521 hw_loss 0.400967 lr 0.00049304 rank 3
2023-02-17 05:19:40,155 DEBUG TRAIN Batch 1/4000 loss 47.109818 loss_att 95.427673 loss_ctc 58.781391 loss_rnnt 35.708496 hw_loss 0.340383 lr 0.00049352 rank 6
2023-02-17 05:19:40,156 DEBUG TRAIN Batch 1/4000 loss 46.079430 loss_att 91.301743 loss_ctc 54.070351 loss_rnnt 35.778759 hw_loss 0.357656 lr 0.00049292 rank 4
2023-02-17 05:19:40,160 DEBUG TRAIN Batch 1/4000 loss 40.838112 loss_att 87.952148 loss_ctc 49.261200 loss_rnnt 30.133818 hw_loss 0.297016 lr 0.00049368 rank 5
2023-02-17 05:19:40,161 DEBUG TRAIN Batch 1/4000 loss 49.204041 loss_att 104.428017 loss_ctc 63.386841 loss_rnnt 36.017776 hw_loss 0.469555 lr 0.00049332 rank 7
2023-02-17 05:19:40,167 DEBUG TRAIN Batch 1/4000 loss 46.210667 loss_att 94.588211 loss_ctc 51.861240 loss_rnnt 35.543221 hw_loss 0.447232 lr 0.00049336 rank 0
2023-02-17 05:19:40,207 DEBUG TRAIN Batch 1/4000 loss 60.947510 loss_att 121.596214 loss_ctc 75.562859 loss_rnnt 46.667248 hw_loss 0.378392 lr 0.00049352 rank 2
2023-02-17 05:20:57,065 DEBUG TRAIN Batch 1/4100 loss 68.001839 loss_att 124.208191 loss_ctc 81.506393 loss_rnnt 54.773632 hw_loss 0.349378 lr 0.00049752 rank 6
2023-02-17 05:20:57,067 DEBUG TRAIN Batch 1/4100 loss 79.151909 loss_att 142.255722 loss_ctc 88.624443 loss_rnnt 65.100967 hw_loss 0.313425 lr 0.00049732 rank 7
2023-02-17 05:20:57,071 DEBUG TRAIN Batch 1/4100 loss 41.412678 loss_att 74.121819 loss_ctc 57.087593 loss_rnnt 32.558144 hw_loss 0.417597 lr 0.00049752 rank 2
2023-02-17 05:20:57,071 DEBUG TRAIN Batch 1/4100 loss 35.930157 loss_att 77.304771 loss_ctc 45.501888 loss_rnnt 26.193022 hw_loss 0.348711 lr 0.00049692 rank 4
2023-02-17 05:20:57,071 DEBUG TRAIN Batch 1/4100 loss 46.745487 loss_att 88.681252 loss_ctc 52.639702 loss_rnnt 37.354759 hw_loss 0.408147 lr 0.00049768 rank 5
2023-02-17 05:20:57,071 DEBUG TRAIN Batch 1/4100 loss 45.247620 loss_att 85.182205 loss_ctc 56.403545 loss_rnnt 35.547241 hw_loss 0.423754 lr 0.00049872 rank 1
2023-02-17 05:20:57,075 DEBUG TRAIN Batch 1/4100 loss 57.949913 loss_att 104.432022 loss_ctc 69.970612 loss_rnnt 46.886543 hw_loss 0.307845 lr 0.00049736 rank 0
2023-02-17 05:20:57,120 DEBUG TRAIN Batch 1/4100 loss 62.017990 loss_att 114.342224 loss_ctc 79.096252 loss_rnnt 49.103271 hw_loss 0.323941 lr 0.00049704 rank 3
2023-02-17 05:22:15,477 DEBUG TRAIN Batch 1/4200 loss 46.124378 loss_att 90.122246 loss_ctc 58.890053 loss_rnnt 35.422092 hw_loss 0.376165 lr 0.00050168 rank 5
2023-02-17 05:22:15,477 DEBUG TRAIN Batch 1/4200 loss 77.178802 loss_att 132.263763 loss_ctc 100.263519 loss_rnnt 62.938927 hw_loss 0.271714 lr 0.00050152 rank 6
2023-02-17 05:22:15,480 DEBUG TRAIN Batch 1/4200 loss 44.749104 loss_att 87.964317 loss_ctc 53.950230 loss_rnnt 34.709953 hw_loss 0.317422 lr 0.00050104 rank 3
2023-02-17 05:22:15,481 DEBUG TRAIN Batch 1/4200 loss 71.474968 loss_att 114.686890 loss_ctc 84.043884 loss_rnnt 60.976620 hw_loss 0.337689 lr 0.00050272 rank 1
2023-02-17 05:22:15,485 DEBUG TRAIN Batch 1/4200 loss 45.469772 loss_att 90.422455 loss_ctc 57.940811 loss_rnnt 34.663845 hw_loss 0.286103 lr 0.00050132 rank 7
2023-02-17 05:22:15,486 DEBUG TRAIN Batch 1/4200 loss 47.821194 loss_att 84.478065 loss_ctc 61.309853 loss_rnnt 38.471619 hw_loss 0.411956 lr 0.00050152 rank 2
2023-02-17 05:22:15,487 DEBUG TRAIN Batch 1/4200 loss 57.702606 loss_att 110.348473 loss_ctc 67.543396 loss_rnnt 45.659111 hw_loss 0.379160 lr 0.00050092 rank 4
2023-02-17 05:22:15,489 DEBUG TRAIN Batch 1/4200 loss 39.174118 loss_att 76.976532 loss_ctc 53.410492 loss_rnnt 29.484737 hw_loss 0.432588 lr 0.00050136 rank 0
2023-02-17 05:23:35,273 DEBUG TRAIN Batch 1/4300 loss 60.199532 loss_att 85.183937 loss_ctc 74.406822 loss_rnnt 53.077129 hw_loss 0.433531 lr 0.00050552 rank 6
2023-02-17 05:23:35,278 DEBUG TRAIN Batch 1/4300 loss 50.522499 loss_att 101.017029 loss_ctc 64.938545 loss_rnnt 38.340759 hw_loss 0.301307 lr 0.00050532 rank 7
2023-02-17 05:23:35,279 DEBUG TRAIN Batch 1/4300 loss 46.706028 loss_att 83.847275 loss_ctc 57.588909 loss_rnnt 37.654747 hw_loss 0.322463 lr 0.00050568 rank 5
2023-02-17 05:23:35,279 DEBUG TRAIN Batch 1/4300 loss 56.355980 loss_att 97.122795 loss_ctc 72.399902 loss_rnnt 45.854538 hw_loss 0.391660 lr 0.00050504 rank 3
2023-02-17 05:23:35,282 DEBUG TRAIN Batch 1/4300 loss 39.152859 loss_att 84.317307 loss_ctc 53.527267 loss_rnnt 27.971014 hw_loss 0.435682 lr 0.00050672 rank 1
2023-02-17 05:23:35,282 DEBUG TRAIN Batch 1/4300 loss 59.526382 loss_att 98.569351 loss_ctc 73.194565 loss_rnnt 49.669884 hw_loss 0.422778 lr 0.00050492 rank 4
2023-02-17 05:23:35,286 DEBUG TRAIN Batch 1/4300 loss 48.005505 loss_att 74.677765 loss_ctc 57.817257 loss_rnnt 41.124168 hw_loss 0.447476 lr 0.00050552 rank 2
2023-02-17 05:23:35,288 DEBUG TRAIN Batch 1/4300 loss 27.186632 loss_att 41.732491 loss_ctc 38.664562 loss_rnnt 22.476034 hw_loss 0.508189 lr 0.00050536 rank 0
2023-02-17 05:24:50,296 DEBUG TRAIN Batch 1/4400 loss 60.584686 loss_att 91.812569 loss_ctc 74.630875 loss_rnnt 52.185608 hw_loss 0.526269 lr 0.00050904 rank 3
2023-02-17 05:24:50,300 DEBUG TRAIN Batch 1/4400 loss 76.908646 loss_att 143.389359 loss_ctc 104.706192 loss_rnnt 59.737469 hw_loss 0.316315 lr 0.00050952 rank 6
2023-02-17 05:24:50,301 DEBUG TRAIN Batch 1/4400 loss 69.583687 loss_att 115.818909 loss_ctc 91.130791 loss_rnnt 57.256916 hw_loss 0.387710 lr 0.00050968 rank 5
2023-02-17 05:24:50,302 DEBUG TRAIN Batch 1/4400 loss 43.109535 loss_att 74.099915 loss_ctc 51.963997 loss_rnnt 35.531067 hw_loss 0.374624 lr 0.00050932 rank 7
2023-02-17 05:24:50,305 DEBUG TRAIN Batch 1/4400 loss 84.872200 loss_att 143.219025 loss_ctc 102.108063 loss_rnnt 70.736183 hw_loss 0.315996 lr 0.00050952 rank 2
2023-02-17 05:24:50,305 DEBUG TRAIN Batch 1/4400 loss 50.811375 loss_att 86.167725 loss_ctc 65.189011 loss_rnnt 41.660465 hw_loss 0.304905 lr 0.00050892 rank 4
2023-02-17 05:24:50,306 DEBUG TRAIN Batch 1/4400 loss 38.629063 loss_att 75.897980 loss_ctc 50.799335 loss_rnnt 29.363867 hw_loss 0.353833 lr 0.00051072 rank 1
2023-02-17 05:24:50,308 DEBUG TRAIN Batch 1/4400 loss 70.573280 loss_att 121.286736 loss_ctc 88.140404 loss_rnnt 57.889191 hw_loss 0.373338 lr 0.00050936 rank 0
2023-02-17 05:26:05,200 DEBUG TRAIN Batch 1/4500 loss 81.675232 loss_att 141.028015 loss_ctc 106.962082 loss_rnnt 66.234459 hw_loss 0.372443 lr 0.00051304 rank 3
2023-02-17 05:26:05,201 DEBUG TRAIN Batch 1/4500 loss 47.004692 loss_att 84.550194 loss_ctc 60.091057 loss_rnnt 37.539577 hw_loss 0.395934 lr 0.00051352 rank 6
2023-02-17 05:26:05,205 DEBUG TRAIN Batch 1/4500 loss 52.726955 loss_att 109.496262 loss_ctc 55.729843 loss_rnnt 40.781700 hw_loss 0.358135 lr 0.00051336 rank 0
2023-02-17 05:26:05,206 DEBUG TRAIN Batch 1/4500 loss 27.888588 loss_att 38.628204 loss_ctc 35.331375 loss_rnnt 24.520201 hw_loss 0.427672 lr 0.00051292 rank 4
2023-02-17 05:26:05,207 DEBUG TRAIN Batch 1/4500 loss 36.750313 loss_att 53.713081 loss_ctc 49.516228 loss_rnnt 31.414133 hw_loss 0.452820 lr 0.00051332 rank 7
2023-02-17 05:26:05,207 DEBUG TRAIN Batch 1/4500 loss 33.765285 loss_att 58.727402 loss_ctc 38.932796 loss_rnnt 27.883558 hw_loss 0.375564 lr 0.00051472 rank 1
2023-02-17 05:26:05,208 DEBUG TRAIN Batch 1/4500 loss 38.810112 loss_att 82.464508 loss_ctc 45.199371 loss_rnnt 29.033043 hw_loss 0.364289 lr 0.00051368 rank 5
2023-02-17 05:26:05,250 DEBUG TRAIN Batch 1/4500 loss 79.278557 loss_att 133.727356 loss_ctc 102.346497 loss_rnnt 65.103668 hw_loss 0.392631 lr 0.00051352 rank 2
2023-02-17 05:27:24,261 DEBUG TRAIN Batch 1/4600 loss 52.885242 loss_att 106.317581 loss_ctc 69.947708 loss_rnnt 39.744194 hw_loss 0.336718 lr 0.00051732 rank 7
2023-02-17 05:27:24,261 DEBUG TRAIN Batch 1/4600 loss 45.080845 loss_att 91.829117 loss_ctc 58.710060 loss_rnnt 33.752071 hw_loss 0.303542 lr 0.00051872 rank 1
2023-02-17 05:27:24,263 DEBUG TRAIN Batch 1/4600 loss 54.071144 loss_att 108.961578 loss_ctc 66.593918 loss_rnnt 41.238228 hw_loss 0.347107 lr 0.00051704 rank 3
2023-02-17 05:27:24,264 DEBUG TRAIN Batch 1/4600 loss 46.777939 loss_att 99.964874 loss_ctc 57.532730 loss_rnnt 34.521294 hw_loss 0.347400 lr 0.00051692 rank 4
2023-02-17 05:27:24,265 DEBUG TRAIN Batch 1/4600 loss 59.624344 loss_att 99.309357 loss_ctc 74.243301 loss_rnnt 49.490761 hw_loss 0.463835 lr 0.00051736 rank 0
2023-02-17 05:27:24,265 DEBUG TRAIN Batch 1/4600 loss 51.157711 loss_att 85.192368 loss_ctc 58.674622 loss_rnnt 43.202450 hw_loss 0.273895 lr 0.00051752 rank 6
2023-02-17 05:27:24,269 DEBUG TRAIN Batch 1/4600 loss 40.235676 loss_att 86.308685 loss_ctc 49.705780 loss_rnnt 29.539190 hw_loss 0.411006 lr 0.00051768 rank 5
2023-02-17 05:27:24,311 DEBUG TRAIN Batch 1/4600 loss 43.480816 loss_att 88.804260 loss_ctc 51.628918 loss_rnnt 33.150276 hw_loss 0.336451 lr 0.00051752 rank 2
2023-02-17 05:28:40,440 DEBUG TRAIN Batch 1/4700 loss 38.706257 loss_att 74.698738 loss_ctc 48.140106 loss_rnnt 29.977905 hw_loss 0.510019 lr 0.00052168 rank 5
2023-02-17 05:28:40,442 DEBUG TRAIN Batch 1/4700 loss 49.224491 loss_att 88.951973 loss_ctc 67.154709 loss_rnnt 38.719513 hw_loss 0.316476 lr 0.00052152 rank 6
2023-02-17 05:28:40,444 DEBUG TRAIN Batch 1/4700 loss 52.334526 loss_att 90.070290 loss_ctc 73.961746 loss_rnnt 41.699276 hw_loss 0.383380 lr 0.00052272 rank 1
2023-02-17 05:28:40,445 DEBUG TRAIN Batch 1/4700 loss 55.281357 loss_att 96.142410 loss_ctc 63.674568 loss_rnnt 45.774414 hw_loss 0.404324 lr 0.00052132 rank 7
2023-02-17 05:28:40,447 DEBUG TRAIN Batch 1/4700 loss 50.254349 loss_att 91.921768 loss_ctc 64.314301 loss_rnnt 39.815693 hw_loss 0.432206 lr 0.00052092 rank 4
2023-02-17 05:28:40,448 DEBUG TRAIN Batch 1/4700 loss 65.761414 loss_att 112.177170 loss_ctc 83.605827 loss_rnnt 53.931080 hw_loss 0.314858 lr 0.00052136 rank 0
2023-02-17 05:28:40,451 DEBUG TRAIN Batch 1/4700 loss 53.282715 loss_att 95.329506 loss_ctc 66.363396 loss_rnnt 42.949097 hw_loss 0.337809 lr 0.00052104 rank 3
2023-02-17 05:28:40,453 DEBUG TRAIN Batch 1/4700 loss 71.285751 loss_att 123.989601 loss_ctc 100.458160 loss_rnnt 56.679398 hw_loss 0.329856 lr 0.00052152 rank 2
2023-02-17 05:29:56,866 DEBUG TRAIN Batch 1/4800 loss 55.138294 loss_att 86.190659 loss_ctc 63.222031 loss_rnnt 47.652519 hw_loss 0.370252 lr 0.00052552 rank 6
2023-02-17 05:29:56,867 DEBUG TRAIN Batch 1/4800 loss 48.262119 loss_att 89.543671 loss_ctc 63.038944 loss_rnnt 37.805908 hw_loss 0.430613 lr 0.00052504 rank 3
2023-02-17 05:29:56,871 DEBUG TRAIN Batch 1/4800 loss 45.282555 loss_att 74.141235 loss_ctc 56.721725 loss_rnnt 37.784248 hw_loss 0.377519 lr 0.00052568 rank 5
2023-02-17 05:29:56,871 DEBUG TRAIN Batch 1/4800 loss 40.553967 loss_att 71.403290 loss_ctc 49.524239 loss_rnnt 33.017563 hw_loss 0.319692 lr 0.00052492 rank 4
2023-02-17 05:29:56,873 DEBUG TRAIN Batch 1/4800 loss 58.826752 loss_att 112.425446 loss_ctc 79.624847 loss_rnnt 45.152050 hw_loss 0.341033 lr 0.00052672 rank 1
2023-02-17 05:29:56,873 DEBUG TRAIN Batch 1/4800 loss 31.714289 loss_att 67.676025 loss_ctc 40.633629 loss_rnnt 23.152559 hw_loss 0.337748 lr 0.00052552 rank 2
2023-02-17 05:29:56,874 DEBUG TRAIN Batch 1/4800 loss 54.426929 loss_att 89.609100 loss_ctc 74.172020 loss_rnnt 44.605919 hw_loss 0.284814 lr 0.00052536 rank 0
2023-02-17 05:29:56,874 DEBUG TRAIN Batch 1/4800 loss 43.212875 loss_att 75.086105 loss_ctc 53.677841 loss_rnnt 35.280720 hw_loss 0.304090 lr 0.00052532 rank 7
2023-02-17 05:31:13,849 DEBUG TRAIN Batch 1/4900 loss 58.388973 loss_att 96.456825 loss_ctc 77.231339 loss_rnnt 48.093632 hw_loss 0.317724 lr 0.00052968 rank 5
2023-02-17 05:31:13,850 DEBUG TRAIN Batch 1/4900 loss 54.080246 loss_att 87.113327 loss_ctc 61.284920 loss_rnnt 46.299732 hw_loss 0.399896 lr 0.00053072 rank 1
2023-02-17 05:31:13,854 DEBUG TRAIN Batch 1/4900 loss 41.682934 loss_att 68.635529 loss_ctc 54.637733 loss_rnnt 34.359173 hw_loss 0.386135 lr 0.00052952 rank 6
2023-02-17 05:31:13,855 DEBUG TRAIN Batch 1/4900 loss 41.749538 loss_att 77.405502 loss_ctc 51.351662 loss_rnnt 33.128166 hw_loss 0.393554 lr 0.00052932 rank 7
2023-02-17 05:31:13,857 DEBUG TRAIN Batch 1/4900 loss 38.163357 loss_att 67.419479 loss_ctc 46.857857 loss_rnnt 30.972023 hw_loss 0.339081 lr 0.00052904 rank 3
2023-02-17 05:31:13,858 DEBUG TRAIN Batch 1/4900 loss 38.573708 loss_att 65.655167 loss_ctc 55.198635 loss_rnnt 30.715551 hw_loss 0.422272 lr 0.00052952 rank 2
2023-02-17 05:31:13,860 DEBUG TRAIN Batch 1/4900 loss 30.996326 loss_att 44.391464 loss_ctc 36.635818 loss_rnnt 27.287436 hw_loss 0.521123 lr 0.00052936 rank 0
2023-02-17 05:31:13,899 DEBUG TRAIN Batch 1/4900 loss 53.357548 loss_att 97.024376 loss_ctc 67.451019 loss_rnnt 42.599937 hw_loss 0.272090 lr 0.00052892 rank 4
2023-02-17 05:32:34,650 DEBUG TRAIN Batch 1/5000 loss 41.011971 loss_att 67.407242 loss_ctc 53.908783 loss_rnnt 33.780579 hw_loss 0.436427 lr 0.00053368 rank 5
2023-02-17 05:32:34,651 DEBUG TRAIN Batch 1/5000 loss 22.766041 loss_att 25.821934 loss_ctc 28.981039 loss_rnnt 21.080881 hw_loss 0.459964 lr 0.00053352 rank 6
2023-02-17 05:32:34,651 DEBUG TRAIN Batch 1/5000 loss 38.926140 loss_att 66.257545 loss_ctc 51.245644 loss_rnnt 31.602310 hw_loss 0.403028 lr 0.00053304 rank 3
2023-02-17 05:32:34,651 DEBUG TRAIN Batch 1/5000 loss 25.755243 loss_att 34.381008 loss_ctc 34.048836 loss_rnnt 22.672115 hw_loss 0.472810 lr 0.00053352 rank 2
2023-02-17 05:32:34,655 DEBUG TRAIN Batch 1/5000 loss 51.011105 loss_att 85.390770 loss_ctc 62.008118 loss_rnnt 42.477188 hw_loss 0.359463 lr 0.00053292 rank 4
2023-02-17 05:32:34,656 DEBUG TRAIN Batch 1/5000 loss 41.711857 loss_att 72.708481 loss_ctc 55.829269 loss_rnnt 33.434696 hw_loss 0.366582 lr 0.00053472 rank 1
2023-02-17 05:32:34,656 DEBUG TRAIN Batch 1/5000 loss 64.593620 loss_att 94.054947 loss_ctc 80.022942 loss_rnnt 56.480137 hw_loss 0.307448 lr 0.00053332 rank 7
2023-02-17 05:32:34,687 DEBUG TRAIN Batch 1/5000 loss 63.671051 loss_att 122.623482 loss_ctc 81.626480 loss_rnnt 49.270172 hw_loss 0.405638 lr 0.00053336 rank 0
2023-02-17 05:33:51,028 DEBUG TRAIN Batch 1/5100 loss 64.604263 loss_att 115.261909 loss_ctc 84.066360 loss_rnnt 51.672508 hw_loss 0.384900 lr 0.00053752 rank 6
2023-02-17 05:33:51,032 DEBUG TRAIN Batch 1/5100 loss 49.960258 loss_att 89.132515 loss_ctc 67.715454 loss_rnnt 39.548195 hw_loss 0.394216 lr 0.00053752 rank 2
2023-02-17 05:33:51,032 DEBUG TRAIN Batch 1/5100 loss 56.329220 loss_att 81.773293 loss_ctc 74.493820 loss_rnnt 48.630913 hw_loss 0.351645 lr 0.00053692 rank 4
2023-02-17 05:33:51,035 DEBUG TRAIN Batch 1/5100 loss 44.188263 loss_att 74.165581 loss_ctc 61.904644 loss_rnnt 35.644695 hw_loss 0.348605 lr 0.00053872 rank 1
2023-02-17 05:33:51,039 DEBUG TRAIN Batch 1/5100 loss 52.381298 loss_att 95.700645 loss_ctc 72.252197 loss_rnnt 40.882736 hw_loss 0.347315 lr 0.00053704 rank 3
2023-02-17 05:33:51,038 DEBUG TRAIN Batch 1/5100 loss 32.550667 loss_att 45.392117 loss_ctc 41.294136 loss_rnnt 28.574272 hw_loss 0.454333 lr 0.00053732 rank 7
2023-02-17 05:33:51,040 DEBUG TRAIN Batch 1/5100 loss 31.927269 loss_att 46.162350 loss_ctc 40.359627 loss_rnnt 27.748251 hw_loss 0.389415 lr 0.00053768 rank 5
2023-02-17 05:33:51,043 DEBUG TRAIN Batch 1/5100 loss 35.049770 loss_att 67.920303 loss_ctc 41.724579 loss_rnnt 27.410635 hw_loss 0.328231 lr 0.00053736 rank 0
2023-02-17 05:35:07,156 DEBUG TRAIN Batch 1/5200 loss 60.750088 loss_att 105.042984 loss_ctc 85.406067 loss_rnnt 48.460903 hw_loss 0.268396 lr 0.00054152 rank 6
2023-02-17 05:35:07,157 DEBUG TRAIN Batch 1/5200 loss 54.085644 loss_att 88.836533 loss_ctc 67.214317 loss_rnnt 45.240417 hw_loss 0.271040 lr 0.00054092 rank 4
2023-02-17 05:35:07,158 DEBUG TRAIN Batch 1/5200 loss 64.156746 loss_att 106.813850 loss_ctc 78.757782 loss_rnnt 53.519073 hw_loss 0.298957 lr 0.00054104 rank 3
2023-02-17 05:35:07,160 DEBUG TRAIN Batch 1/5200 loss 68.218689 loss_att 112.457733 loss_ctc 86.346275 loss_rnnt 56.750774 hw_loss 0.380801 lr 0.00054168 rank 5
2023-02-17 05:35:07,161 DEBUG TRAIN Batch 1/5200 loss 44.461399 loss_att 89.785217 loss_ctc 68.755333 loss_rnnt 31.988792 hw_loss 0.316217 lr 0.00054272 rank 1
2023-02-17 05:35:07,162 DEBUG TRAIN Batch 1/5200 loss 40.019348 loss_att 69.851562 loss_ctc 55.962708 loss_rnnt 31.706190 hw_loss 0.414250 lr 0.00054132 rank 7
2023-02-17 05:35:07,164 DEBUG TRAIN Batch 1/5200 loss 37.438587 loss_att 68.845261 loss_ctc 50.235359 loss_rnnt 29.286627 hw_loss 0.308223 lr 0.00054152 rank 2
2023-02-17 05:35:07,165 DEBUG TRAIN Batch 1/5200 loss 28.070749 loss_att 64.049789 loss_ctc 38.381622 loss_rnnt 19.262386 hw_loss 0.445818 lr 0.00054136 rank 0
2023-02-17 05:36:25,036 DEBUG TRAIN Batch 1/5300 loss 63.336796 loss_att 100.763977 loss_ctc 78.894409 loss_rnnt 53.593781 hw_loss 0.343556 lr 0.00054552 rank 6
2023-02-17 05:36:25,036 DEBUG TRAIN Batch 1/5300 loss 65.157341 loss_att 110.064125 loss_ctc 76.535217 loss_rnnt 54.485565 hw_loss 0.325070 lr 0.00054492 rank 4
2023-02-17 05:36:25,036 DEBUG TRAIN Batch 1/5300 loss 35.486221 loss_att 68.754044 loss_ctc 46.248589 loss_rnnt 27.132435 hw_loss 0.497324 lr 0.00054672 rank 1
2023-02-17 05:36:25,040 DEBUG TRAIN Batch 1/5300 loss 56.740215 loss_att 97.527977 loss_ctc 71.948662 loss_rnnt 46.381207 hw_loss 0.325625 lr 0.00054568 rank 5
2023-02-17 05:36:25,042 DEBUG TRAIN Batch 1/5300 loss 44.437962 loss_att 78.232155 loss_ctc 61.807014 loss_rnnt 35.151638 hw_loss 0.396781 lr 0.00054536 rank 0
2023-02-17 05:36:25,043 DEBUG TRAIN Batch 1/5300 loss 66.593742 loss_att 106.524529 loss_ctc 80.008644 loss_rnnt 56.651855 hw_loss 0.313254 lr 0.00054532 rank 7
2023-02-17 05:36:25,055 DEBUG TRAIN Batch 1/5300 loss 58.260452 loss_att 104.170982 loss_ctc 76.390045 loss_rnnt 46.490601 hw_loss 0.319628 lr 0.00054552 rank 2
2023-02-17 05:36:25,064 DEBUG TRAIN Batch 1/5300 loss 80.874329 loss_att 138.086044 loss_ctc 105.722496 loss_rnnt 65.886864 hw_loss 0.435066 lr 0.00054504 rank 3
2023-02-17 05:37:42,764 DEBUG TRAIN Batch 1/5400 loss 74.316368 loss_att 103.940277 loss_ctc 88.799690 loss_rnnt 66.316498 hw_loss 0.269953 lr 0.00054904 rank 3
2023-02-17 05:37:42,767 DEBUG TRAIN Batch 1/5400 loss 63.165333 loss_att 93.127235 loss_ctc 73.594696 loss_rnnt 55.579872 hw_loss 0.379675 lr 0.00054892 rank 4
2023-02-17 05:37:42,768 DEBUG TRAIN Batch 1/5400 loss 55.696911 loss_att 90.992058 loss_ctc 72.409119 loss_rnnt 46.205559 hw_loss 0.382552 lr 0.00054952 rank 6
2023-02-17 05:37:42,769 DEBUG TRAIN Batch 1/5400 loss 45.204746 loss_att 77.386421 loss_ctc 58.754196 loss_rnnt 36.781441 hw_loss 0.338204 lr 0.00054932 rank 7
2023-02-17 05:37:42,769 DEBUG TRAIN Batch 1/5400 loss 35.707787 loss_att 68.950790 loss_ctc 51.357193 loss_rnnt 26.787483 hw_loss 0.347092 lr 0.00055072 rank 1
2023-02-17 05:37:42,770 DEBUG TRAIN Batch 1/5400 loss 67.006104 loss_att 107.172089 loss_ctc 85.136009 loss_rnnt 56.382561 hw_loss 0.324427 lr 0.00054968 rank 5
2023-02-17 05:37:42,771 DEBUG TRAIN Batch 1/5400 loss 48.050495 loss_att 74.515297 loss_ctc 65.362793 loss_rnnt 40.262650 hw_loss 0.349832 lr 0.00054952 rank 2
2023-02-17 05:37:42,773 DEBUG TRAIN Batch 1/5400 loss 65.498268 loss_att 107.836456 loss_ctc 89.321548 loss_rnnt 53.663826 hw_loss 0.356925 lr 0.00054936 rank 0
2023-02-17 05:38:58,085 DEBUG TRAIN Batch 1/5500 loss 45.345943 loss_att 82.758179 loss_ctc 60.672100 loss_rnnt 35.648014 hw_loss 0.322488 lr 0.00055304 rank 3
2023-02-17 05:38:58,087 DEBUG TRAIN Batch 1/5500 loss 46.699387 loss_att 70.096802 loss_ctc 60.325493 loss_rnnt 40.023361 hw_loss 0.336990 lr 0.00055472 rank 1
2023-02-17 05:38:58,088 DEBUG TRAIN Batch 1/5500 loss 46.515961 loss_att 71.350853 loss_ctc 57.404892 loss_rnnt 39.913670 hw_loss 0.343970 lr 0.00055352 rank 6
2023-02-17 05:38:58,088 DEBUG TRAIN Batch 1/5500 loss 43.202694 loss_att 68.319450 loss_ctc 58.921928 loss_rnnt 35.909306 hw_loss 0.326514 lr 0.00055352 rank 2
2023-02-17 05:38:58,090 DEBUG TRAIN Batch 1/5500 loss 48.213215 loss_att 81.547073 loss_ctc 63.408669 loss_rnnt 39.363758 hw_loss 0.293664 lr 0.00055332 rank 7
2023-02-17 05:38:58,091 DEBUG TRAIN Batch 1/5500 loss 66.911385 loss_att 103.399506 loss_ctc 85.569290 loss_rnnt 56.953407 hw_loss 0.323673 lr 0.00055292 rank 4
2023-02-17 05:38:58,092 DEBUG TRAIN Batch 1/5500 loss 51.004025 loss_att 87.749268 loss_ctc 64.909485 loss_rnnt 41.598595 hw_loss 0.379340 lr 0.00055368 rank 5
2023-02-17 05:38:58,094 DEBUG TRAIN Batch 1/5500 loss 49.799286 loss_att 74.161499 loss_ctc 71.468079 loss_rnnt 41.791889 hw_loss 0.460831 lr 0.00055336 rank 0
2023-02-17 05:40:14,583 DEBUG TRAIN Batch 1/5600 loss 61.956821 loss_att 89.343338 loss_ctc 80.903664 loss_rnnt 53.722122 hw_loss 0.433410 lr 0.00055732 rank 7
2023-02-17 05:40:14,584 DEBUG TRAIN Batch 1/5600 loss 49.858757 loss_att 78.159332 loss_ctc 71.856033 loss_rnnt 41.095818 hw_loss 0.318484 lr 0.00055768 rank 5
2023-02-17 05:40:14,585 DEBUG TRAIN Batch 1/5600 loss 56.186745 loss_att 75.971397 loss_ctc 71.159462 loss_rnnt 50.053791 hw_loss 0.336861 lr 0.00055704 rank 3
2023-02-17 05:40:14,586 DEBUG TRAIN Batch 1/5600 loss 28.530663 loss_att 34.507275 loss_ctc 34.735153 loss_rnnt 26.266232 hw_loss 0.453463 lr 0.00055736 rank 0
2023-02-17 05:40:14,586 DEBUG TRAIN Batch 1/5600 loss 36.924507 loss_att 49.115601 loss_ctc 46.579784 loss_rnnt 32.984230 hw_loss 0.402537 lr 0.00055752 rank 6
2023-02-17 05:40:14,588 DEBUG TRAIN Batch 1/5600 loss 42.896286 loss_att 59.862778 loss_ctc 54.693081 loss_rnnt 37.737061 hw_loss 0.361915 lr 0.00055872 rank 1
2023-02-17 05:40:14,590 DEBUG TRAIN Batch 1/5600 loss 39.165947 loss_att 68.365326 loss_ctc 50.463989 loss_rnnt 31.650827 hw_loss 0.316574 lr 0.00055692 rank 4
2023-02-17 05:40:14,595 DEBUG TRAIN Batch 1/5600 loss 31.770111 loss_att 46.991009 loss_ctc 44.255795 loss_rnnt 26.835567 hw_loss 0.423011 lr 0.00055752 rank 2
2023-02-17 05:41:34,064 DEBUG TRAIN Batch 1/5700 loss 50.463890 loss_att 69.458580 loss_ctc 65.349648 loss_rnnt 44.486935 hw_loss 0.362343 lr 0.00056168 rank 5
2023-02-17 05:41:34,065 DEBUG TRAIN Batch 1/5700 loss 34.879147 loss_att 41.218891 loss_ctc 45.002251 loss_rnnt 32.056698 hw_loss 0.383913 lr 0.00056104 rank 3
2023-02-17 05:41:34,070 DEBUG TRAIN Batch 1/5700 loss 35.366695 loss_att 47.466740 loss_ctc 46.895607 loss_rnnt 31.147068 hw_loss 0.492061 lr 0.00056092 rank 4
2023-02-17 05:41:34,071 DEBUG TRAIN Batch 1/5700 loss 49.202938 loss_att 72.909348 loss_ctc 63.058533 loss_rnnt 42.413063 hw_loss 0.377218 lr 0.00056152 rank 2
2023-02-17 05:41:34,074 DEBUG TRAIN Batch 1/5700 loss 41.631332 loss_att 56.962402 loss_ctc 55.231010 loss_rnnt 36.575939 hw_loss 0.329794 lr 0.00056132 rank 7
2023-02-17 05:41:34,075 DEBUG TRAIN Batch 1/5700 loss 42.270023 loss_att 59.144524 loss_ctc 55.331886 loss_rnnt 36.935898 hw_loss 0.408086 lr 0.00056272 rank 1
2023-02-17 05:41:34,078 DEBUG TRAIN Batch 1/5700 loss 52.798172 loss_att 88.248520 loss_ctc 73.817108 loss_rnnt 42.657280 hw_loss 0.465560 lr 0.00056152 rank 6
2023-02-17 05:41:34,122 DEBUG TRAIN Batch 1/5700 loss 33.170300 loss_att 62.184326 loss_ctc 41.916145 loss_rnnt 26.023060 hw_loss 0.334350 lr 0.00056136 rank 0
2023-02-17 05:42:49,908 DEBUG TRAIN Batch 1/5800 loss 49.213795 loss_att 79.558578 loss_ctc 61.832405 loss_rnnt 41.255852 hw_loss 0.387192 lr 0.00056552 rank 6
2023-02-17 05:42:49,911 DEBUG TRAIN Batch 1/5800 loss 55.117062 loss_att 83.424591 loss_ctc 74.243019 loss_rnnt 46.726242 hw_loss 0.335967 lr 0.00056672 rank 1
2023-02-17 05:42:49,913 DEBUG TRAIN Batch 1/5800 loss 68.969452 loss_att 110.989441 loss_ctc 93.263626 loss_rnnt 57.183861 hw_loss 0.266961 lr 0.00056552 rank 2
2023-02-17 05:42:49,914 DEBUG TRAIN Batch 1/5800 loss 62.428642 loss_att 99.956619 loss_ctc 81.219162 loss_rnnt 52.272228 hw_loss 0.272654 lr 0.00056492 rank 4
2023-02-17 05:42:49,914 DEBUG TRAIN Batch 1/5800 loss 38.337994 loss_att 62.967472 loss_ctc 55.108727 loss_rnnt 31.002380 hw_loss 0.325534 lr 0.00056504 rank 3
2023-02-17 05:42:49,914 DEBUG TRAIN Batch 1/5800 loss 52.526333 loss_att 83.830208 loss_ctc 73.956070 loss_rnnt 43.203941 hw_loss 0.383100 lr 0.00056568 rank 5
2023-02-17 05:42:49,915 DEBUG TRAIN Batch 1/5800 loss 53.540691 loss_att 93.637939 loss_ctc 80.411003 loss_rnnt 41.786926 hw_loss 0.284264 lr 0.00056532 rank 7
2023-02-17 05:42:49,917 DEBUG TRAIN Batch 1/5800 loss 51.341038 loss_att 94.928947 loss_ctc 73.587730 loss_rnnt 39.443768 hw_loss 0.400244 lr 0.00056536 rank 0
2023-02-17 05:44:05,520 DEBUG TRAIN Batch 1/5900 loss 41.421223 loss_att 62.915764 loss_ctc 59.097103 loss_rnnt 34.573410 hw_loss 0.360232 lr 0.00056952 rank 6
2023-02-17 05:44:05,528 DEBUG TRAIN Batch 1/5900 loss 31.313103 loss_att 59.804817 loss_ctc 38.430244 loss_rnnt 24.452892 hw_loss 0.399217 lr 0.00056932 rank 7
2023-02-17 05:44:05,528 DEBUG TRAIN Batch 1/5900 loss 69.793663 loss_att 100.243828 loss_ctc 88.479515 loss_rnnt 61.071636 hw_loss 0.263512 lr 0.00056968 rank 5
2023-02-17 05:44:05,529 DEBUG TRAIN Batch 1/5900 loss 48.025681 loss_att 72.182373 loss_ctc 58.699722 loss_rnnt 41.599182 hw_loss 0.322421 lr 0.00056952 rank 2
2023-02-17 05:44:05,530 DEBUG TRAIN Batch 1/5900 loss 81.837517 loss_att 124.604813 loss_ctc 101.701187 loss_rnnt 70.449532 hw_loss 0.348793 lr 0.00057072 rank 1
2023-02-17 05:44:05,531 DEBUG TRAIN Batch 1/5900 loss 71.714264 loss_att 115.506500 loss_ctc 87.884392 loss_rnnt 60.601311 hw_loss 0.372168 lr 0.00056904 rank 3
2023-02-17 05:44:05,535 DEBUG TRAIN Batch 1/5900 loss 62.109341 loss_att 90.148003 loss_ctc 81.212715 loss_rnnt 53.719574 hw_loss 0.440469 lr 0.00056936 rank 0
2023-02-17 05:44:05,578 DEBUG TRAIN Batch 1/5900 loss 54.693016 loss_att 78.181900 loss_ctc 62.969406 loss_rnnt 48.710022 hw_loss 0.340695 lr 0.00056892 rank 4
2023-02-17 05:45:22,889 DEBUG TRAIN Batch 1/6000 loss 29.437569 loss_att 54.469528 loss_ctc 40.270004 loss_rnnt 22.770626 hw_loss 0.405425 lr 0.00057332 rank 7
2023-02-17 05:45:22,890 DEBUG TRAIN Batch 1/6000 loss 59.055321 loss_att 85.987671 loss_ctc 85.474693 loss_rnnt 49.992664 hw_loss 0.288005 lr 0.00057304 rank 3
2023-02-17 05:45:22,890 DEBUG TRAIN Batch 1/6000 loss 48.137550 loss_att 69.254959 loss_ctc 60.065422 loss_rnnt 42.118454 hw_loss 0.384804 lr 0.00057472 rank 1
2023-02-17 05:45:22,892 DEBUG TRAIN Batch 1/6000 loss 26.371723 loss_att 56.636154 loss_ctc 33.052540 loss_rnnt 19.242573 hw_loss 0.347794 lr 0.00057368 rank 5
2023-02-17 05:45:22,895 DEBUG TRAIN Batch 1/6000 loss 71.248169 loss_att 100.402351 loss_ctc 93.389389 loss_rnnt 62.283829 hw_loss 0.339998 lr 0.00057352 rank 6
2023-02-17 05:45:22,899 DEBUG TRAIN Batch 1/6000 loss 36.908096 loss_att 60.301491 loss_ctc 45.497723 loss_rnnt 30.884092 hw_loss 0.375078 lr 0.00057292 rank 4
2023-02-17 05:45:22,903 DEBUG TRAIN Batch 1/6000 loss 39.976929 loss_att 66.221672 loss_ctc 53.970726 loss_rnnt 32.647434 hw_loss 0.402572 lr 0.00057336 rank 0
2023-02-17 05:45:22,942 DEBUG TRAIN Batch 1/6000 loss 29.475716 loss_att 53.110542 loss_ctc 43.183846 loss_rnnt 22.694658 hw_loss 0.424389 lr 0.00057352 rank 2
2023-02-17 05:46:42,707 DEBUG TRAIN Batch 1/6100 loss 28.263336 loss_att 51.084145 loss_ctc 40.396912 loss_rnnt 21.865206 hw_loss 0.405291 lr 0.00057752 rank 6
2023-02-17 05:46:42,707 DEBUG TRAIN Batch 1/6100 loss 39.465004 loss_att 65.174278 loss_ctc 55.173645 loss_rnnt 32.036957 hw_loss 0.359454 lr 0.00057768 rank 5
2023-02-17 05:46:42,708 DEBUG TRAIN Batch 1/6100 loss 44.411301 loss_att 65.844727 loss_ctc 62.782150 loss_rnnt 37.414040 hw_loss 0.489616 lr 0.00057732 rank 7
2023-02-17 05:46:42,710 DEBUG TRAIN Batch 1/6100 loss 47.208866 loss_att 78.382370 loss_ctc 71.088211 loss_rnnt 37.592468 hw_loss 0.370847 lr 0.00057692 rank 4
2023-02-17 05:46:42,711 DEBUG TRAIN Batch 1/6100 loss 43.781670 loss_att 76.480545 loss_ctc 59.064739 loss_rnnt 35.011215 hw_loss 0.361749 lr 0.00057872 rank 1
2023-02-17 05:46:42,714 DEBUG TRAIN Batch 1/6100 loss 62.271008 loss_att 80.271927 loss_ctc 80.894356 loss_rnnt 55.996521 hw_loss 0.358474 lr 0.00057704 rank 3
2023-02-17 05:46:42,717 DEBUG TRAIN Batch 1/6100 loss 48.932327 loss_att 69.796135 loss_ctc 60.544952 loss_rnnt 43.028870 hw_loss 0.341893 lr 0.00057736 rank 0
2023-02-17 05:46:42,717 DEBUG TRAIN Batch 1/6100 loss 41.591690 loss_att 60.333111 loss_ctc 58.589828 loss_rnnt 35.371658 hw_loss 0.384999 lr 0.00057752 rank 2
2023-02-17 05:47:57,655 DEBUG TRAIN Batch 1/6200 loss 65.555107 loss_att 98.316154 loss_ctc 86.248924 loss_rnnt 56.114822 hw_loss 0.241672 lr 0.00058168 rank 5
2023-02-17 05:47:57,658 DEBUG TRAIN Batch 1/6200 loss 38.423058 loss_att 57.386452 loss_ctc 56.375366 loss_rnnt 32.032574 hw_loss 0.382806 lr 0.00058152 rank 6
2023-02-17 05:47:57,661 DEBUG TRAIN Batch 1/6200 loss 51.485737 loss_att 79.540031 loss_ctc 72.153175 loss_rnnt 42.875927 hw_loss 0.456172 lr 0.00058272 rank 1
2023-02-17 05:47:57,663 DEBUG TRAIN Batch 1/6200 loss 32.652344 loss_att 43.142082 loss_ctc 39.006409 loss_rnnt 29.512371 hw_loss 0.365290 lr 0.00058104 rank 3
2023-02-17 05:47:57,663 DEBUG TRAIN Batch 1/6200 loss 38.407433 loss_att 60.313259 loss_ctc 53.232967 loss_rnnt 31.882795 hw_loss 0.312627 lr 0.00058152 rank 2
2023-02-17 05:47:57,662 DEBUG TRAIN Batch 1/6200 loss 42.649818 loss_att 66.532654 loss_ctc 54.875706 loss_rnnt 36.095612 hw_loss 0.276600 lr 0.00058092 rank 4
2023-02-17 05:47:57,664 DEBUG TRAIN Batch 1/6200 loss 40.995247 loss_att 68.207909 loss_ctc 61.556625 loss_rnnt 32.641121 hw_loss 0.318886 lr 0.00058132 rank 7
2023-02-17 05:47:57,667 DEBUG TRAIN Batch 1/6200 loss 39.221741 loss_att 52.609329 loss_ctc 49.806480 loss_rnnt 34.955853 hw_loss 0.332009 lr 0.00058136 rank 0
2023-02-17 05:49:12,749 DEBUG TRAIN Batch 1/6300 loss 50.491280 loss_att 81.424919 loss_ctc 70.418732 loss_rnnt 41.487823 hw_loss 0.299499 lr 0.00058532 rank 7
2023-02-17 05:49:12,751 DEBUG TRAIN Batch 1/6300 loss 27.504520 loss_att 44.931225 loss_ctc 38.509869 loss_rnnt 22.362268 hw_loss 0.355366 lr 0.00058492 rank 4
2023-02-17 05:49:12,751 DEBUG TRAIN Batch 1/6300 loss 36.802010 loss_att 48.472748 loss_ctc 47.164959 loss_rnnt 32.928017 hw_loss 0.296468 lr 0.00058504 rank 3
2023-02-17 05:49:12,755 DEBUG TRAIN Batch 1/6300 loss 23.433542 loss_att 27.213158 loss_ctc 29.066931 loss_rnnt 21.682785 hw_loss 0.456966 lr 0.00058552 rank 2
2023-02-17 05:49:12,757 DEBUG TRAIN Batch 1/6300 loss 45.095722 loss_att 78.668823 loss_ctc 61.068451 loss_rnnt 36.085121 hw_loss 0.311781 lr 0.00058552 rank 6
2023-02-17 05:49:12,756 DEBUG TRAIN Batch 1/6300 loss 36.512226 loss_att 57.670357 loss_ctc 48.815182 loss_rnnt 30.448948 hw_loss 0.358605 lr 0.00058568 rank 5
2023-02-17 05:49:12,759 DEBUG TRAIN Batch 1/6300 loss 56.870903 loss_att 71.989914 loss_ctc 74.817291 loss_rnnt 51.275673 hw_loss 0.334825 lr 0.00058672 rank 1
2023-02-17 05:49:12,805 DEBUG TRAIN Batch 1/6300 loss 44.023895 loss_att 62.828892 loss_ctc 56.007050 loss_rnnt 38.494320 hw_loss 0.320292 lr 0.00058536 rank 0
2023-02-17 05:50:32,514 DEBUG TRAIN Batch 1/6400 loss 25.066467 loss_att 32.647190 loss_ctc 33.018166 loss_rnnt 22.251125 hw_loss 0.448075 lr 0.00058932 rank 7
2023-02-17 05:50:32,519 DEBUG TRAIN Batch 1/6400 loss 23.446302 loss_att 29.512445 loss_ctc 28.528696 loss_rnnt 21.304029 hw_loss 0.471363 lr 0.00059072 rank 1
2023-02-17 05:50:32,524 DEBUG TRAIN Batch 1/6400 loss 48.115700 loss_att 71.763191 loss_ctc 63.566311 loss_rnnt 41.147778 hw_loss 0.334398 lr 0.00058904 rank 3
2023-02-17 05:50:32,524 DEBUG TRAIN Batch 1/6400 loss 42.743965 loss_att 64.633835 loss_ctc 57.814342 loss_rnnt 36.169197 hw_loss 0.351391 lr 0.00058952 rank 2
2023-02-17 05:50:32,525 DEBUG TRAIN Batch 1/6400 loss 16.032221 loss_att 19.104006 loss_ctc 21.508791 loss_rnnt 14.429021 hw_loss 0.484939 lr 0.00058892 rank 4
2023-02-17 05:50:32,526 DEBUG TRAIN Batch 1/6400 loss 29.309099 loss_att 35.312038 loss_ctc 41.094479 loss_rnnt 26.325974 hw_loss 0.395914 lr 0.00058968 rank 5
2023-02-17 05:50:32,527 DEBUG TRAIN Batch 1/6400 loss 53.068367 loss_att 79.791046 loss_ctc 68.721954 loss_rnnt 45.439407 hw_loss 0.369893 lr 0.00058952 rank 6
2023-02-17 05:50:32,529 DEBUG TRAIN Batch 1/6400 loss 26.973091 loss_att 44.539677 loss_ctc 37.767159 loss_rnnt 21.805479 hw_loss 0.403287 lr 0.00058936 rank 0
2023-02-17 05:51:47,335 DEBUG TRAIN Batch 1/6500 loss 57.273857 loss_att 71.499268 loss_ctc 72.415451 loss_rnnt 52.228661 hw_loss 0.339818 lr 0.00059352 rank 6
2023-02-17 05:51:47,340 DEBUG TRAIN Batch 1/6500 loss 27.858130 loss_att 54.080055 loss_ctc 41.937962 loss_rnnt 20.502119 hw_loss 0.439338 lr 0.00059304 rank 3
2023-02-17 05:51:47,341 DEBUG TRAIN Batch 1/6500 loss 45.117455 loss_att 68.205048 loss_ctc 57.751263 loss_rnnt 38.586983 hw_loss 0.428332 lr 0.00059336 rank 0
2023-02-17 05:51:47,342 DEBUG TRAIN Batch 1/6500 loss 29.911451 loss_att 53.329346 loss_ctc 44.224651 loss_rnnt 23.110851 hw_loss 0.391116 lr 0.00059352 rank 2
2023-02-17 05:51:47,343 DEBUG TRAIN Batch 1/6500 loss 59.157455 loss_att 86.741211 loss_ctc 81.511650 loss_rnnt 50.483135 hw_loss 0.331895 lr 0.00059472 rank 1
2023-02-17 05:51:47,344 DEBUG TRAIN Batch 1/6500 loss 44.603027 loss_att 66.415771 loss_ctc 58.109062 loss_rnnt 38.242908 hw_loss 0.368936 lr 0.00059292 rank 4
2023-02-17 05:51:47,363 DEBUG TRAIN Batch 1/6500 loss 51.389061 loss_att 84.108643 loss_ctc 71.962204 loss_rnnt 41.941803 hw_loss 0.300481 lr 0.00059332 rank 7
2023-02-17 05:51:47,374 DEBUG TRAIN Batch 1/6500 loss 32.562595 loss_att 54.117939 loss_ctc 47.316689 loss_rnnt 26.125629 hw_loss 0.297533 lr 0.00059368 rank 5
2023-02-17 05:53:05,622 DEBUG TRAIN Batch 1/6600 loss 41.128948 loss_att 76.746475 loss_ctc 58.383160 loss_rnnt 31.478201 hw_loss 0.425021 lr 0.00059872 rank 1
2023-02-17 05:53:05,626 DEBUG TRAIN Batch 1/6600 loss 35.872997 loss_att 57.820473 loss_ctc 49.811043 loss_rnnt 29.419638 hw_loss 0.385240 lr 0.00059768 rank 5
2023-02-17 05:53:05,627 DEBUG TRAIN Batch 1/6600 loss 44.932808 loss_att 66.708633 loss_ctc 65.019684 loss_rnnt 37.695930 hw_loss 0.381494 lr 0.00059752 rank 6
2023-02-17 05:53:05,627 DEBUG TRAIN Batch 1/6600 loss 68.927155 loss_att 103.817368 loss_ctc 94.624634 loss_rnnt 58.337460 hw_loss 0.347472 lr 0.00059752 rank 2
2023-02-17 05:53:05,629 DEBUG TRAIN Batch 1/6600 loss 46.662895 loss_att 74.652672 loss_ctc 64.317268 loss_rnnt 38.536148 hw_loss 0.327894 lr 0.00059704 rank 3
2023-02-17 05:53:05,631 DEBUG TRAIN Batch 1/6600 loss 44.239113 loss_att 69.464218 loss_ctc 57.328094 loss_rnnt 37.228455 hw_loss 0.413323 lr 0.00059732 rank 7
2023-02-17 05:53:05,633 DEBUG TRAIN Batch 1/6600 loss 49.075947 loss_att 71.689247 loss_ctc 56.758850 loss_rnnt 43.340485 hw_loss 0.353279 lr 0.00059692 rank 4
2023-02-17 05:53:05,677 DEBUG TRAIN Batch 1/6600 loss 51.927151 loss_att 80.703659 loss_ctc 73.325645 loss_rnnt 43.176003 hw_loss 0.267577 lr 0.00059736 rank 0
2023-02-17 05:54:24,012 DEBUG TRAIN Batch 1/6700 loss 31.006889 loss_att 49.489735 loss_ctc 39.540985 loss_rnnt 25.980907 hw_loss 0.359132 lr 0.00060168 rank 5
2023-02-17 05:54:24,013 DEBUG TRAIN Batch 1/6700 loss 64.765114 loss_att 84.427467 loss_ctc 81.788933 loss_rnnt 58.363064 hw_loss 0.374508 lr 0.00060132 rank 7
2023-02-17 05:54:24,017 DEBUG TRAIN Batch 1/6700 loss 48.420982 loss_att 70.889954 loss_ctc 66.315971 loss_rnnt 41.343410 hw_loss 0.370846 lr 0.00060136 rank 0
2023-02-17 05:54:24,018 DEBUG TRAIN Batch 1/6700 loss 61.642059 loss_att 93.844826 loss_ctc 90.901627 loss_rnnt 51.133339 hw_loss 0.312914 lr 0.00060104 rank 3
2023-02-17 05:54:24,021 DEBUG TRAIN Batch 1/6700 loss 70.640137 loss_att 92.828239 loss_ctc 94.081558 loss_rnnt 62.930717 hw_loss 0.274273 lr 0.00060092 rank 4
2023-02-17 05:54:24,021 DEBUG TRAIN Batch 1/6700 loss 33.608212 loss_att 56.175213 loss_ctc 51.524391 loss_rnnt 26.533718 hw_loss 0.323006 lr 0.00060152 rank 2
2023-02-17 05:54:24,046 DEBUG TRAIN Batch 1/6700 loss 35.887527 loss_att 49.009342 loss_ctc 47.464622 loss_rnnt 31.516727 hw_loss 0.380300 lr 0.00060152 rank 6
2023-02-17 05:54:24,054 DEBUG TRAIN Batch 1/6700 loss 50.349152 loss_att 69.490234 loss_ctc 62.941307 loss_rnnt 44.693398 hw_loss 0.278588 lr 0.00060272 rank 1
2023-02-17 05:55:42,947 DEBUG TRAIN Batch 1/6800 loss 51.196198 loss_att 65.828140 loss_ctc 67.192444 loss_rnnt 45.975525 hw_loss 0.302712 lr 0.00060552 rank 2
2023-02-17 05:55:42,950 DEBUG TRAIN Batch 1/6800 loss 50.447468 loss_att 72.684158 loss_ctc 78.742981 loss_rnnt 41.980366 hw_loss 0.463166 lr 0.00060568 rank 5
2023-02-17 05:55:42,952 DEBUG TRAIN Batch 1/6800 loss 41.721550 loss_att 65.854210 loss_ctc 59.023460 loss_rnnt 34.388786 hw_loss 0.373703 lr 0.00060532 rank 7
2023-02-17 05:55:42,953 DEBUG TRAIN Batch 1/6800 loss 50.575699 loss_att 68.380295 loss_ctc 72.197472 loss_rnnt 43.976871 hw_loss 0.290635 lr 0.00060552 rank 6
2023-02-17 05:55:42,953 DEBUG TRAIN Batch 1/6800 loss 30.878794 loss_att 52.329605 loss_ctc 41.829414 loss_rnnt 24.926586 hw_loss 0.378681 lr 0.00060672 rank 1
2023-02-17 05:55:42,955 DEBUG TRAIN Batch 1/6800 loss 44.828068 loss_att 61.761829 loss_ctc 61.928829 loss_rnnt 38.979977 hw_loss 0.339824 lr 0.00060536 rank 0
2023-02-17 05:55:42,955 DEBUG TRAIN Batch 1/6800 loss 32.439938 loss_att 49.829323 loss_ctc 47.205254 loss_rnnt 26.827824 hw_loss 0.310364 lr 0.00060504 rank 3
2023-02-17 05:55:43,001 DEBUG TRAIN Batch 1/6800 loss 32.584412 loss_att 57.474808 loss_ctc 50.333939 loss_rnnt 25.067383 hw_loss 0.323144 lr 0.00060492 rank 4
2023-02-17 05:56:59,257 DEBUG TRAIN Batch 1/6900 loss 38.608559 loss_att 52.638481 loss_ctc 51.139862 loss_rnnt 33.880417 hw_loss 0.471219 lr 0.00060932 rank 7
2023-02-17 05:56:59,257 DEBUG TRAIN Batch 1/6900 loss 33.008877 loss_att 36.866814 loss_ctc 40.718563 loss_rnnt 30.963779 hw_loss 0.460410 lr 0.00060952 rank 6
2023-02-17 05:56:59,257 DEBUG TRAIN Batch 1/6900 loss 40.600513 loss_att 52.953796 loss_ctc 53.299694 loss_rnnt 36.232128 hw_loss 0.383443 lr 0.00061072 rank 1
2023-02-17 05:56:59,259 DEBUG TRAIN Batch 1/6900 loss 38.014854 loss_att 44.662460 loss_ctc 49.503639 loss_rnnt 34.895832 hw_loss 0.483116 lr 0.00060952 rank 2
2023-02-17 05:56:59,258 DEBUG TRAIN Batch 1/6900 loss 45.317818 loss_att 67.673523 loss_ctc 59.689568 loss_rnnt 38.708401 hw_loss 0.416334 lr 0.00060968 rank 5
2023-02-17 05:56:59,259 DEBUG TRAIN Batch 1/6900 loss 29.061155 loss_att 40.171631 loss_ctc 41.303818 loss_rnnt 24.931316 hw_loss 0.516355 lr 0.00060904 rank 3
2023-02-17 05:56:59,262 DEBUG TRAIN Batch 1/6900 loss 22.388741 loss_att 25.659685 loss_ctc 28.232594 loss_rnnt 20.685266 hw_loss 0.506445 lr 0.00060936 rank 0
2023-02-17 05:56:59,264 DEBUG TRAIN Batch 1/6900 loss 40.783035 loss_att 56.649830 loss_ctc 54.523262 loss_rnnt 35.597466 hw_loss 0.337839 lr 0.00060892 rank 4
2023-02-17 05:58:15,908 DEBUG TRAIN Batch 1/7000 loss 40.026707 loss_att 42.797016 loss_ctc 49.765923 loss_rnnt 37.915783 hw_loss 0.484317 lr 0.00061368 rank 5
2023-02-17 05:58:15,912 DEBUG TRAIN Batch 1/7000 loss 44.936260 loss_att 72.197723 loss_ctc 61.349483 loss_rnnt 37.132401 hw_loss 0.305877 lr 0.00061352 rank 2
2023-02-17 05:58:15,913 DEBUG TRAIN Batch 1/7000 loss 34.004105 loss_att 44.272400 loss_ctc 48.588490 loss_rnnt 29.767767 hw_loss 0.446423 lr 0.00061292 rank 4
2023-02-17 05:58:15,914 DEBUG TRAIN Batch 1/7000 loss 30.310822 loss_att 43.939716 loss_ctc 42.236649 loss_rnnt 25.796490 hw_loss 0.372079 lr 0.00061332 rank 7
2023-02-17 05:58:15,914 DEBUG TRAIN Batch 1/7000 loss 42.221558 loss_att 74.020264 loss_ctc 61.687023 loss_rnnt 33.063927 hw_loss 0.379673 lr 0.00061304 rank 3
2023-02-17 05:58:15,917 DEBUG TRAIN Batch 1/7000 loss 60.895950 loss_att 87.793678 loss_ctc 75.918259 loss_rnnt 53.393211 hw_loss 0.225408 lr 0.00061336 rank 0
2023-02-17 05:58:15,931 DEBUG TRAIN Batch 1/7000 loss 32.384060 loss_att 38.457218 loss_ctc 40.680481 loss_rnnt 29.811884 hw_loss 0.471286 lr 0.00061472 rank 1
2023-02-17 05:58:15,935 DEBUG TRAIN Batch 1/7000 loss 96.110306 loss_att 121.624359 loss_ctc 117.561722 loss_rnnt 87.939194 hw_loss 0.390227 lr 0.00061352 rank 6
2023-02-17 05:59:36,213 DEBUG TRAIN Batch 1/7100 loss 78.630241 loss_att 119.965729 loss_ctc 112.500183 loss_rnnt 65.678986 hw_loss 0.315309 lr 0.00061732 rank 7
2023-02-17 05:59:36,216 DEBUG TRAIN Batch 1/7100 loss 81.470802 loss_att 111.403763 loss_ctc 101.450005 loss_rnnt 72.636963 hw_loss 0.343786 lr 0.00061752 rank 2
2023-02-17 05:59:36,216 DEBUG TRAIN Batch 1/7100 loss 60.531326 loss_att 87.061043 loss_ctc 86.513077 loss_rnnt 51.584656 hw_loss 0.330924 lr 0.00061704 rank 3
2023-02-17 05:59:36,218 DEBUG TRAIN Batch 1/7100 loss 58.868965 loss_att 86.545425 loss_ctc 86.039780 loss_rnnt 49.552719 hw_loss 0.296581 lr 0.00061752 rank 6
2023-02-17 05:59:36,222 DEBUG TRAIN Batch 1/7100 loss 33.051636 loss_att 52.849327 loss_ctc 49.122936 loss_rnnt 26.774059 hw_loss 0.328494 lr 0.00061736 rank 0
2023-02-17 05:59:36,222 DEBUG TRAIN Batch 1/7100 loss 63.462803 loss_att 92.815895 loss_ctc 87.650040 loss_rnnt 54.180702 hw_loss 0.349713 lr 0.00061872 rank 1
2023-02-17 05:59:36,238 DEBUG TRAIN Batch 1/7100 loss 53.289703 loss_att 68.603577 loss_ctc 76.380478 loss_rnnt 46.935944 hw_loss 0.397911 lr 0.00061768 rank 5
2023-02-17 05:59:36,267 DEBUG TRAIN Batch 1/7100 loss 61.836437 loss_att 88.094887 loss_ctc 77.562645 loss_rnnt 54.303749 hw_loss 0.345318 lr 0.00061692 rank 4
2023-02-17 06:00:54,374 DEBUG TRAIN Batch 1/7200 loss 38.483555 loss_att 56.958916 loss_ctc 59.424732 loss_rnnt 31.842991 hw_loss 0.287492 lr 0.00062152 rank 6
2023-02-17 06:00:54,375 DEBUG TRAIN Batch 1/7200 loss 35.160774 loss_att 53.172348 loss_ctc 50.123569 loss_rnnt 29.371346 hw_loss 0.360136 lr 0.00062136 rank 0
2023-02-17 06:00:54,376 DEBUG TRAIN Batch 1/7200 loss 35.738140 loss_att 53.308491 loss_ctc 57.005718 loss_rnnt 29.140667 hw_loss 0.464484 lr 0.00062272 rank 1
2023-02-17 06:00:54,378 DEBUG TRAIN Batch 1/7200 loss 40.996998 loss_att 64.672630 loss_ctc 66.986176 loss_rnnt 32.567116 hw_loss 0.430365 lr 0.00062104 rank 3
2023-02-17 06:00:54,378 DEBUG TRAIN Batch 1/7200 loss 49.553650 loss_att 73.377396 loss_ctc 67.854408 loss_rnnt 42.136341 hw_loss 0.398354 lr 0.00062132 rank 7
2023-02-17 06:00:54,380 DEBUG TRAIN Batch 1/7200 loss 28.931580 loss_att 52.175449 loss_ctc 45.616787 loss_rnnt 21.843288 hw_loss 0.402792 lr 0.00062092 rank 4
2023-02-17 06:00:54,382 DEBUG TRAIN Batch 1/7200 loss 30.024366 loss_att 53.712597 loss_ctc 39.917831 loss_rnnt 23.791645 hw_loss 0.329901 lr 0.00062168 rank 5
2023-02-17 06:00:54,426 DEBUG TRAIN Batch 1/7200 loss 46.716358 loss_att 66.752625 loss_ctc 69.138519 loss_rnnt 39.508408 hw_loss 0.395765 lr 0.00062152 rank 2
2023-02-17 06:02:11,294 DEBUG TRAIN Batch 1/7300 loss 56.389477 loss_att 77.906052 loss_ctc 78.240532 loss_rnnt 49.002632 hw_loss 0.318840 lr 0.00062552 rank 6
2023-02-17 06:02:11,296 DEBUG TRAIN Batch 1/7300 loss 40.660892 loss_att 59.677544 loss_ctc 52.854080 loss_rnnt 35.028851 hw_loss 0.380541 lr 0.00062492 rank 4
2023-02-17 06:02:11,299 DEBUG TRAIN Batch 1/7300 loss 37.272663 loss_att 55.789734 loss_ctc 55.024864 loss_rnnt 31.013090 hw_loss 0.354750 lr 0.00062504 rank 3
2023-02-17 06:02:11,299 DEBUG TRAIN Batch 1/7300 loss 62.884533 loss_att 73.984505 loss_ctc 81.244980 loss_rnnt 58.037777 hw_loss 0.335068 lr 0.00062552 rank 2
2023-02-17 06:02:11,300 DEBUG TRAIN Batch 1/7300 loss 58.052593 loss_att 72.065491 loss_ctc 76.404236 loss_rnnt 52.579781 hw_loss 0.418769 lr 0.00062568 rank 5
2023-02-17 06:02:11,301 DEBUG TRAIN Batch 1/7300 loss 49.548885 loss_att 67.250458 loss_ctc 67.667427 loss_rnnt 43.413898 hw_loss 0.335372 lr 0.00062672 rank 1
2023-02-17 06:02:11,302 DEBUG TRAIN Batch 1/7300 loss 42.946537 loss_att 66.090874 loss_ctc 62.209377 loss_rnnt 35.567112 hw_loss 0.341590 lr 0.00062532 rank 7
2023-02-17 06:02:11,348 DEBUG TRAIN Batch 1/7300 loss 63.683372 loss_att 89.289490 loss_ctc 89.778885 loss_rnnt 54.936417 hw_loss 0.274367 lr 0.00062536 rank 0
2023-02-17 06:03:29,475 DEBUG TRAIN Batch 1/7400 loss 41.618652 loss_att 64.004562 loss_ctc 60.057457 loss_rnnt 34.530319 hw_loss 0.286209 lr 0.00062952 rank 6
2023-02-17 06:03:29,477 DEBUG TRAIN Batch 1/7400 loss 48.913776 loss_att 60.392326 loss_ctc 64.993896 loss_rnnt 44.301071 hw_loss 0.324333 lr 0.00062932 rank 7
2023-02-17 06:03:29,477 DEBUG TRAIN Batch 1/7400 loss 32.246704 loss_att 48.916348 loss_ctc 49.617085 loss_rnnt 26.441866 hw_loss 0.290351 lr 0.00062904 rank 3
2023-02-17 06:03:29,485 DEBUG TRAIN Batch 1/7400 loss 37.353817 loss_att 52.492062 loss_ctc 55.296997 loss_rnnt 31.782509 hw_loss 0.283564 lr 0.00063072 rank 1
2023-02-17 06:03:29,486 DEBUG TRAIN Batch 1/7400 loss 49.675812 loss_att 73.196342 loss_ctc 66.225555 loss_rnnt 42.547997 hw_loss 0.407006 lr 0.00062936 rank 0
2023-02-17 06:03:29,487 DEBUG TRAIN Batch 1/7400 loss 57.727104 loss_att 77.749535 loss_ctc 72.804871 loss_rnnt 51.567223 hw_loss 0.271930 lr 0.00062968 rank 5
2023-02-17 06:03:29,488 DEBUG TRAIN Batch 1/7400 loss 32.037903 loss_att 55.793282 loss_ctc 46.192780 loss_rnnt 25.230492 hw_loss 0.316902 lr 0.00062952 rank 2
2023-02-17 06:03:29,489 DEBUG TRAIN Batch 1/7400 loss 25.085878 loss_att 44.527588 loss_ctc 38.548187 loss_rnnt 19.224573 hw_loss 0.333729 lr 0.00062892 rank 4
2023-02-17 06:04:49,332 DEBUG TRAIN Batch 1/7500 loss 28.192673 loss_att 40.211761 loss_ctc 42.617397 loss_rnnt 23.627623 hw_loss 0.446128 lr 0.00063352 rank 6
2023-02-17 06:04:49,335 DEBUG TRAIN Batch 1/7500 loss 56.613480 loss_att 71.642525 loss_ctc 66.933662 loss_rnnt 52.038307 hw_loss 0.362502 lr 0.00063332 rank 7
2023-02-17 06:04:49,338 DEBUG TRAIN Batch 1/7500 loss 52.473473 loss_att 68.616150 loss_ctc 72.950745 loss_rnnt 46.354622 hw_loss 0.300016 lr 0.00063368 rank 5
2023-02-17 06:04:49,338 DEBUG TRAIN Batch 1/7500 loss 49.257786 loss_att 65.672096 loss_ctc 68.744705 loss_rnnt 43.193069 hw_loss 0.344252 lr 0.00063292 rank 4
2023-02-17 06:04:49,338 DEBUG TRAIN Batch 1/7500 loss 30.196251 loss_att 45.840504 loss_ctc 51.098751 loss_rnnt 24.061838 hw_loss 0.409805 lr 0.00063472 rank 1
2023-02-17 06:04:49,344 DEBUG TRAIN Batch 1/7500 loss 25.180380 loss_att 35.565697 loss_ctc 34.934155 loss_rnnt 21.580624 hw_loss 0.416608 lr 0.00063304 rank 3
2023-02-17 06:04:49,348 DEBUG TRAIN Batch 1/7500 loss 28.193394 loss_att 37.722980 loss_ctc 39.554115 loss_rnnt 24.536552 hw_loss 0.442802 lr 0.00063336 rank 0
2023-02-17 06:04:49,400 DEBUG TRAIN Batch 1/7500 loss 28.651184 loss_att 43.428917 loss_ctc 40.130119 loss_rnnt 23.942284 hw_loss 0.417803 lr 0.00063352 rank 2
2023-02-17 06:06:05,258 DEBUG TRAIN Batch 1/7600 loss 46.705772 loss_att 69.679756 loss_ctc 60.497368 loss_rnnt 40.085411 hw_loss 0.350034 lr 0.00063752 rank 6
2023-02-17 06:06:05,260 DEBUG TRAIN Batch 1/7600 loss 30.428921 loss_att 46.047096 loss_ctc 41.218719 loss_rnnt 25.659266 hw_loss 0.388836 lr 0.00063692 rank 4
2023-02-17 06:06:05,261 DEBUG TRAIN Batch 1/7600 loss 29.666550 loss_att 36.358818 loss_ctc 41.926464 loss_rnnt 26.505383 hw_loss 0.352613 lr 0.00063732 rank 7
2023-02-17 06:06:05,264 DEBUG TRAIN Batch 1/7600 loss 42.897232 loss_att 63.431889 loss_ctc 65.151451 loss_rnnt 35.646576 hw_loss 0.330927 lr 0.00063736 rank 0
2023-02-17 06:06:05,265 DEBUG TRAIN Batch 1/7600 loss 71.632553 loss_att 91.745697 loss_ctc 91.340370 loss_rnnt 64.765854 hw_loss 0.405678 lr 0.00063872 rank 1
2023-02-17 06:06:05,267 DEBUG TRAIN Batch 1/7600 loss 65.846031 loss_att 97.032578 loss_ctc 97.871277 loss_rnnt 55.147949 hw_loss 0.357621 lr 0.00063704 rank 3
2023-02-17 06:06:05,266 DEBUG TRAIN Batch 1/7600 loss 37.296463 loss_att 52.000046 loss_ctc 53.751614 loss_rnnt 31.964146 hw_loss 0.370458 lr 0.00063768 rank 5
2023-02-17 06:06:05,309 DEBUG TRAIN Batch 1/7600 loss 54.882751 loss_att 83.329163 loss_ctc 82.750809 loss_rnnt 45.285782 hw_loss 0.359895 lr 0.00063752 rank 2
2023-02-17 06:07:20,026 DEBUG TRAIN Batch 1/7700 loss 17.706394 loss_att 19.189842 loss_ctc 22.109657 loss_rnnt 16.533051 hw_loss 0.542906 lr 0.00064092 rank 4
2023-02-17 06:07:20,028 DEBUG TRAIN Batch 1/7700 loss 42.668552 loss_att 58.206947 loss_ctc 56.298077 loss_rnnt 37.524391 hw_loss 0.411025 lr 0.00064152 rank 6
2023-02-17 06:07:20,029 DEBUG TRAIN Batch 1/7700 loss 39.737541 loss_att 58.306061 loss_ctc 55.639038 loss_rnnt 33.706367 hw_loss 0.369886 lr 0.00064104 rank 3
2023-02-17 06:07:20,030 DEBUG TRAIN Batch 1/7700 loss 38.807709 loss_att 56.525372 loss_ctc 48.216656 loss_rnnt 33.856842 hw_loss 0.286516 lr 0.00064132 rank 7
2023-02-17 06:07:20,031 DEBUG TRAIN Batch 1/7700 loss 39.825417 loss_att 49.745167 loss_ctc 54.675121 loss_rnnt 35.567642 hw_loss 0.550997 lr 0.00064152 rank 2
2023-02-17 06:07:20,032 DEBUG TRAIN Batch 1/7700 loss 54.302170 loss_att 77.711777 loss_ctc 74.638290 loss_rnnt 46.687309 hw_loss 0.415235 lr 0.00064168 rank 5
2023-02-17 06:07:20,032 DEBUG TRAIN Batch 1/7700 loss 80.325195 loss_att 107.118919 loss_ctc 97.925560 loss_rnnt 72.407570 hw_loss 0.397789 lr 0.00064136 rank 0
2023-02-17 06:07:20,037 DEBUG TRAIN Batch 1/7700 loss 43.713005 loss_att 65.495834 loss_ctc 59.204388 loss_rnnt 37.087536 hw_loss 0.381344 lr 0.00064272 rank 1
2023-02-17 06:08:39,120 DEBUG TRAIN Batch 1/7800 loss 44.789719 loss_att 57.291988 loss_ctc 64.229202 loss_rnnt 39.465118 hw_loss 0.435396 lr 0.00064672 rank 1
2023-02-17 06:08:39,121 DEBUG TRAIN Batch 1/7800 loss 26.223330 loss_att 37.558239 loss_ctc 41.872429 loss_rnnt 21.641827 hw_loss 0.427457 lr 0.00064536 rank 0
2023-02-17 06:08:39,122 DEBUG TRAIN Batch 1/7800 loss 34.573147 loss_att 51.451160 loss_ctc 56.006863 loss_rnnt 28.146015 hw_loss 0.363187 lr 0.00064532 rank 7
2023-02-17 06:08:39,124 DEBUG TRAIN Batch 1/7800 loss 51.866276 loss_att 81.325684 loss_ctc 75.629700 loss_rnnt 42.584816 hw_loss 0.414600 lr 0.00064552 rank 6
2023-02-17 06:08:39,124 DEBUG TRAIN Batch 1/7800 loss 37.928375 loss_att 56.571587 loss_ctc 53.767853 loss_rnnt 31.822174 hw_loss 0.498044 lr 0.00064552 rank 2
2023-02-17 06:08:39,125 DEBUG TRAIN Batch 1/7800 loss 35.887604 loss_att 53.903347 loss_ctc 51.141663 loss_rnnt 30.120192 hw_loss 0.244485 lr 0.00064504 rank 3
2023-02-17 06:08:39,125 DEBUG TRAIN Batch 1/7800 loss 36.976227 loss_att 57.914612 loss_ctc 56.727596 loss_rnnt 30.011913 hw_loss 0.268358 lr 0.00064568 rank 5
2023-02-17 06:08:39,127 DEBUG TRAIN Batch 1/7800 loss 41.336952 loss_att 57.652393 loss_ctc 47.110725 loss_rnnt 37.091103 hw_loss 0.399227 lr 0.00064492 rank 4
2023-02-17 06:09:57,829 DEBUG TRAIN Batch 1/7900 loss 43.124359 loss_att 65.800674 loss_ctc 57.152790 loss_rnnt 36.542042 hw_loss 0.331115 lr 0.00064952 rank 6
2023-02-17 06:09:57,832 DEBUG TRAIN Batch 1/7900 loss 42.118073 loss_att 57.772331 loss_ctc 60.980385 loss_rnnt 36.252800 hw_loss 0.411458 lr 0.00064932 rank 7
2023-02-17 06:09:57,841 DEBUG TRAIN Batch 1/7900 loss 43.953457 loss_att 65.568184 loss_ctc 59.921928 loss_rnnt 37.360687 hw_loss 0.263801 lr 0.00064936 rank 0
2023-02-17 06:09:57,841 DEBUG TRAIN Batch 1/7900 loss 32.655357 loss_att 48.755653 loss_ctc 49.143143 loss_rnnt 26.939995 hw_loss 0.556750 lr 0.00064968 rank 5
2023-02-17 06:09:57,841 DEBUG TRAIN Batch 1/7900 loss 51.329914 loss_att 77.243950 loss_ctc 71.798721 loss_rnnt 43.201580 hw_loss 0.405663 lr 0.00064892 rank 4
2023-02-17 06:09:57,844 DEBUG TRAIN Batch 1/7900 loss 34.678223 loss_att 46.758896 loss_ctc 48.589535 loss_rnnt 30.206631 hw_loss 0.376157 lr 0.00064952 rank 2
2023-02-17 06:09:57,844 DEBUG TRAIN Batch 1/7900 loss 42.294624 loss_att 57.342125 loss_ctc 61.751587 loss_rnnt 36.471619 hw_loss 0.411085 lr 0.00065072 rank 1
2023-02-17 06:09:57,885 DEBUG TRAIN Batch 1/7900 loss 37.907436 loss_att 58.185135 loss_ctc 54.733131 loss_rnnt 31.380966 hw_loss 0.426570 lr 0.00064904 rank 3
2023-02-17 06:11:14,249 DEBUG TRAIN Batch 1/8000 loss 33.213776 loss_att 45.608597 loss_ctc 46.489136 loss_rnnt 28.819450 hw_loss 0.272453 lr 0.00065368 rank 5
2023-02-17 06:11:14,255 DEBUG TRAIN Batch 1/8000 loss 77.334244 loss_att 89.908127 loss_ctc 99.637604 loss_rnnt 71.639931 hw_loss 0.385783 lr 0.00065332 rank 7
2023-02-17 06:11:14,258 DEBUG TRAIN Batch 1/8000 loss 36.114273 loss_att 46.210045 loss_ctc 53.043396 loss_rnnt 31.628534 hw_loss 0.392562 lr 0.00065352 rank 2
2023-02-17 06:11:14,259 DEBUG TRAIN Batch 1/8000 loss 36.004250 loss_att 45.898880 loss_ctc 42.931999 loss_rnnt 32.921432 hw_loss 0.337857 lr 0.00065304 rank 3
2023-02-17 06:11:14,259 DEBUG TRAIN Batch 1/8000 loss 49.248398 loss_att 64.251099 loss_ctc 73.395233 loss_rnnt 42.824722 hw_loss 0.381665 lr 0.00065472 rank 1
2023-02-17 06:11:14,262 DEBUG TRAIN Batch 1/8000 loss 31.120544 loss_att 40.405983 loss_ctc 45.516823 loss_rnnt 27.110659 hw_loss 0.437421 lr 0.00065336 rank 0
2023-02-17 06:11:14,261 DEBUG TRAIN Batch 1/8000 loss 37.072872 loss_att 52.212040 loss_ctc 50.140411 loss_rnnt 32.086727 hw_loss 0.404945 lr 0.00065352 rank 6
2023-02-17 06:11:14,266 DEBUG TRAIN Batch 1/8000 loss 31.677258 loss_att 49.752453 loss_ctc 52.938576 loss_rnnt 25.029840 hw_loss 0.370379 lr 0.00065292 rank 4
2023-02-17 06:12:30,058 DEBUG TRAIN Batch 1/8100 loss 59.067665 loss_att 85.580940 loss_ctc 83.175446 loss_rnnt 50.347305 hw_loss 0.381242 lr 0.00065872 rank 1
2023-02-17 06:12:30,059 DEBUG TRAIN Batch 1/8100 loss 47.379520 loss_att 68.581161 loss_ctc 77.117943 loss_rnnt 38.972527 hw_loss 0.377891 lr 0.00065768 rank 5
2023-02-17 06:12:30,061 DEBUG TRAIN Batch 1/8100 loss 44.343204 loss_att 63.647373 loss_ctc 63.222950 loss_rnnt 37.742756 hw_loss 0.416832 lr 0.00065732 rank 7
2023-02-17 06:12:30,063 DEBUG TRAIN Batch 1/8100 loss 35.114155 loss_att 44.444031 loss_ctc 45.331909 loss_rnnt 31.711920 hw_loss 0.326056 lr 0.00065752 rank 2
2023-02-17 06:12:30,063 DEBUG TRAIN Batch 1/8100 loss 39.666790 loss_att 55.015549 loss_ctc 49.005386 loss_rnnt 35.156883 hw_loss 0.365636 lr 0.00065752 rank 6
2023-02-17 06:12:30,064 DEBUG TRAIN Batch 1/8100 loss 35.680550 loss_att 46.764599 loss_ctc 52.487839 loss_rnnt 31.036112 hw_loss 0.349978 lr 0.00065704 rank 3
2023-02-17 06:12:30,067 DEBUG TRAIN Batch 1/8100 loss 31.584606 loss_att 44.655296 loss_ctc 44.035908 loss_rnnt 27.144871 hw_loss 0.310163 lr 0.00065692 rank 4
2023-02-17 06:12:30,070 DEBUG TRAIN Batch 1/8100 loss 33.753101 loss_att 39.904930 loss_ctc 44.981274 loss_rnnt 30.821335 hw_loss 0.383085 lr 0.00065736 rank 0
2023-02-17 06:13:47,498 DEBUG TRAIN Batch 1/8200 loss 38.087215 loss_att 50.959541 loss_ctc 55.634045 loss_rnnt 32.941097 hw_loss 0.435139 lr 0.00066132 rank 7
2023-02-17 06:13:47,501 DEBUG TRAIN Batch 1/8200 loss 23.619652 loss_att 31.434063 loss_ctc 32.281792 loss_rnnt 20.656517 hw_loss 0.459937 lr 0.00066152 rank 6
2023-02-17 06:13:47,501 DEBUG TRAIN Batch 1/8200 loss 35.413040 loss_att 47.941189 loss_ctc 46.972527 loss_rnnt 31.161108 hw_loss 0.384448 lr 0.00066272 rank 1
2023-02-17 06:13:47,501 DEBUG TRAIN Batch 1/8200 loss 32.453220 loss_att 40.480495 loss_ctc 43.403557 loss_rnnt 29.152767 hw_loss 0.440539 lr 0.00066104 rank 3
2023-02-17 06:13:47,501 DEBUG TRAIN Batch 1/8200 loss 18.643219 loss_att 33.511230 loss_ctc 25.887815 loss_rnnt 14.448169 hw_loss 0.479068 lr 0.00066168 rank 5
2023-02-17 06:13:47,503 DEBUG TRAIN Batch 1/8200 loss 72.878479 loss_att 101.339592 loss_ctc 86.459496 loss_rnnt 65.173134 hw_loss 0.379337 lr 0.00066136 rank 0
2023-02-17 06:13:47,502 DEBUG TRAIN Batch 1/8200 loss 44.500626 loss_att 64.985184 loss_ctc 60.319252 loss_rnnt 38.087151 hw_loss 0.388892 lr 0.00066092 rank 4
2023-02-17 06:13:47,509 DEBUG TRAIN Batch 1/8200 loss 36.080566 loss_att 59.470676 loss_ctc 58.769817 loss_rnnt 28.179724 hw_loss 0.370475 lr 0.00066152 rank 2
2023-02-17 06:15:03,088 DEBUG TRAIN Batch 1/8300 loss 23.050343 loss_att 28.013515 loss_ctc 32.894611 loss_rnnt 20.514090 hw_loss 0.433215 lr 0.00066532 rank 7
2023-02-17 06:15:03,089 DEBUG TRAIN Batch 1/8300 loss 28.603584 loss_att 47.290310 loss_ctc 48.843880 loss_rnnt 21.998953 hw_loss 0.316088 lr 0.00066672 rank 1
2023-02-17 06:15:03,095 DEBUG TRAIN Batch 1/8300 loss 57.372864 loss_att 88.231987 loss_ctc 89.056908 loss_rnnt 46.855724 hw_loss 0.226454 lr 0.00066536 rank 0
2023-02-17 06:15:03,096 DEBUG TRAIN Batch 1/8300 loss 25.586191 loss_att 35.954975 loss_ctc 34.398888 loss_rnnt 22.102911 hw_loss 0.439685 lr 0.00066492 rank 4
2023-02-17 06:15:03,099 DEBUG TRAIN Batch 1/8300 loss 39.217529 loss_att 58.484718 loss_ctc 51.206585 loss_rnnt 33.608261 hw_loss 0.294913 lr 0.00066552 rank 2
2023-02-17 06:15:03,100 DEBUG TRAIN Batch 1/8300 loss 50.307396 loss_att 69.321693 loss_ctc 71.958740 loss_rnnt 43.424156 hw_loss 0.362883 lr 0.00066552 rank 6
2023-02-17 06:15:03,101 DEBUG TRAIN Batch 1/8300 loss 35.731392 loss_att 49.634758 loss_ctc 46.258812 loss_rnnt 31.359406 hw_loss 0.351858 lr 0.00066568 rank 5
2023-02-17 06:15:03,139 DEBUG TRAIN Batch 1/8300 loss 23.781311 loss_att 36.648457 loss_ctc 35.867096 loss_rnnt 19.411856 hw_loss 0.346104 lr 0.00066504 rank 3
2023-02-17 06:15:57,533 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 3
2023-02-17 06:15:57,537 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 5
2023-02-17 06:15:57,542 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 0
2023-02-17 06:15:57,542 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 6
2023-02-17 06:15:57,551 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 1
2023-02-17 06:15:57,560 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 4
2023-02-17 06:15:57,564 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 7
2023-02-17 06:15:57,571 DEBUG CV Batch 1/0 loss 7.419260 loss_att 7.953763 loss_ctc 10.226542 loss_rnnt 6.685554 hw_loss 0.473440 history loss 7.144472 rank 2
2023-02-17 06:16:08,725 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 5
2023-02-17 06:16:08,725 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 4
2023-02-17 06:16:08,737 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 6
2023-02-17 06:16:08,781 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 1
2023-02-17 06:16:08,790 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 7
2023-02-17 06:16:08,838 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 0
2023-02-17 06:16:09,083 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 3
2023-02-17 06:16:09,093 DEBUG CV Batch 1/100 loss 27.926586 loss_att 35.645355 loss_ctc 41.466881 loss_rnnt 24.327314 hw_loss 0.469022 history loss 15.068718 rank 2
2023-02-17 06:16:22,720 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 1
2023-02-17 06:16:22,738 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 6
2023-02-17 06:16:22,789 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 7
2023-02-17 06:16:22,900 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 5
2023-02-17 06:16:22,943 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 0
2023-02-17 06:16:22,991 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 4
2023-02-17 06:16:23,393 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 2
2023-02-17 06:16:23,586 DEBUG CV Batch 1/200 loss 38.266251 loss_att 91.207596 loss_ctc 52.999863 loss_rnnt 25.559517 hw_loss 0.288711 history loss 16.638964 rank 3
2023-02-17 06:16:34,676 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 6
2023-02-17 06:16:34,699 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 1
2023-02-17 06:16:34,716 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 7
2023-02-17 06:16:34,948 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 5
2023-02-17 06:16:35,081 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 4
2023-02-17 06:16:35,186 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 0
2023-02-17 06:16:35,607 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 2
2023-02-17 06:16:35,743 DEBUG CV Batch 1/300 loss 19.702259 loss_att 22.806856 loss_ctc 31.740299 loss_rnnt 17.225899 hw_loss 0.469443 history loss 16.524464 rank 3
2023-02-17 06:16:46,604 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 7
2023-02-17 06:16:46,611 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 6
2023-02-17 06:16:46,622 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 1
2023-02-17 06:16:46,896 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 5
2023-02-17 06:16:47,228 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 0
2023-02-17 06:16:47,632 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 2
2023-02-17 06:16:47,800 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 3
2023-02-17 06:16:48,278 DEBUG CV Batch 1/400 loss 80.633461 loss_att 267.846313 loss_ctc 80.345322 loss_rnnt 43.105072 hw_loss 0.232943 history loss 18.259510 rank 4
2023-02-17 06:16:57,181 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 1
2023-02-17 06:16:57,304 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 6
2023-02-17 06:16:57,318 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 7
2023-02-17 06:16:57,471 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 5
2023-02-17 06:16:57,902 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 0
2023-02-17 06:16:58,661 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 2
2023-02-17 06:16:59,788 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 3
2023-02-17 06:16:59,829 DEBUG CV Batch 1/500 loss 26.948776 loss_att 32.923206 loss_ctc 41.475658 loss_rnnt 23.648727 hw_loss 0.315458 history loss 19.662191 rank 4
2023-02-17 06:17:10,116 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469221 history loss 21.163428 rank 1
2023-02-17 06:17:10,256 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469222 history loss 21.163428 rank 7
2023-02-17 06:17:10,320 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469222 history loss 21.163428 rank 6
2023-02-17 06:17:10,506 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469221 history loss 21.163428 rank 5
2023-02-17 06:17:10,533 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469222 history loss 21.163428 rank 0
2023-02-17 06:17:12,351 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469222 history loss 21.163428 rank 3
2023-02-17 06:17:12,521 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469221 history loss 21.163428 rank 2
2023-02-17 06:17:12,837 DEBUG CV Batch 1/600 loss 19.236044 loss_att 22.296852 loss_ctc 27.504848 loss_rnnt 17.271124 hw_loss 0.469222 history loss 21.163428 rank 4
2023-02-17 06:17:22,552 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 1
2023-02-17 06:17:22,682 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 7
2023-02-17 06:17:22,726 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 6
2023-02-17 06:17:22,761 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 5
2023-02-17 06:17:22,833 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 0
2023-02-17 06:17:24,725 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 2
2023-02-17 06:17:25,102 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 3
2023-02-17 06:17:25,936 DEBUG CV Batch 1/700 loss 91.565445 loss_att 196.524506 loss_ctc 113.243393 loss_rnnt 67.516838 hw_loss 0.312001 history loss 22.328861 rank 4
2023-02-17 06:17:34,548 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 1
2023-02-17 06:17:34,708 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 7
2023-02-17 06:17:34,923 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 5
2023-02-17 06:17:34,967 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 0
2023-02-17 06:17:35,204 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 6
2023-02-17 06:17:37,028 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 3
2023-02-17 06:17:37,723 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 2
2023-02-17 06:17:38,267 DEBUG CV Batch 1/800 loss 27.787334 loss_att 34.947826 loss_ctc 42.306042 loss_rnnt 24.199841 hw_loss 0.411692 history loss 21.332719 rank 4
2023-02-17 06:17:48,559 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 1
2023-02-17 06:17:48,716 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 7
2023-02-17 06:17:48,913 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 0
2023-02-17 06:17:48,926 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 5
2023-02-17 06:17:49,091 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 6
2023-02-17 06:17:51,336 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 3
2023-02-17 06:17:52,340 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 2
2023-02-17 06:17:52,408 DEBUG CV Batch 1/900 loss 53.765400 loss_att 96.461418 loss_ctc 73.481628 loss_rnnt 42.438232 hw_loss 0.298379 history loss 21.101282 rank 4
2023-02-17 06:18:00,769 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 1
2023-02-17 06:18:00,839 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 7
2023-02-17 06:18:01,106 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 0
2023-02-17 06:18:01,234 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 6
2023-02-17 06:18:01,395 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 5
2023-02-17 06:18:03,739 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 3
2023-02-17 06:18:04,695 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 4
2023-02-17 06:18:05,418 DEBUG CV Batch 1/1000 loss 15.612574 loss_att 22.382732 loss_ctc 20.838110 loss_rnnt 13.320652 hw_loss 0.452161 history loss 20.629084 rank 2
2023-02-17 06:18:12,565 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 7
2023-02-17 06:18:12,649 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 1
2023-02-17 06:18:13,053 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 6
2023-02-17 06:18:13,071 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 0
2023-02-17 06:18:13,316 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 5
2023-02-17 06:18:15,661 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 3
2023-02-17 06:18:16,581 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 4
2023-02-17 06:18:17,747 DEBUG CV Batch 1/1100 loss 12.636944 loss_att 12.305943 loss_ctc 17.695412 loss_rnnt 11.767375 hw_loss 0.489949 history loss 20.609094 rank 2
2023-02-17 06:18:23,047 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 7
2023-02-17 06:18:23,073 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 1
2023-02-17 06:18:23,578 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 6
2023-02-17 06:18:23,668 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 0
2023-02-17 06:18:23,818 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 5
2023-02-17 06:18:26,727 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 3
2023-02-17 06:18:27,414 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 4
2023-02-17 06:18:28,296 DEBUG CV Batch 1/1200 loss 33.661015 loss_att 42.011047 loss_ctc 46.676720 loss_rnnt 30.031889 hw_loss 0.419417 history loss 21.190733 rank 2
2023-02-17 06:18:34,856 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 7
2023-02-17 06:18:35,006 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 1
2023-02-17 06:18:35,529 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 6
2023-02-17 06:18:35,768 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 5
2023-02-17 06:18:35,802 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 0
2023-02-17 06:18:39,193 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 3
2023-02-17 06:18:39,549 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 4
2023-02-17 06:18:40,362 DEBUG CV Batch 1/1300 loss 17.858173 loss_att 18.885265 loss_ctc 23.762165 loss_rnnt 16.655653 hw_loss 0.393567 history loss 21.656759 rank 2
2023-02-17 06:18:46,610 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 1
2023-02-17 06:18:47,260 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 7
2023-02-17 06:18:47,369 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 6
2023-02-17 06:18:47,447 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 5
2023-02-17 06:18:47,838 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 0
2023-02-17 06:18:52,026 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 3
2023-02-17 06:18:52,276 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 2
2023-02-17 06:18:52,313 DEBUG CV Batch 1/1400 loss 52.545998 loss_att 139.813843 loss_ctc 61.580082 loss_rnnt 33.758263 hw_loss 0.243030 history loss 22.254626 rank 4
2023-02-17 06:18:59,394 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 1
2023-02-17 06:18:59,769 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 7
2023-02-17 06:19:00,204 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 6
2023-02-17 06:19:00,230 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 5
2023-02-17 06:19:00,733 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 0
2023-02-17 06:19:04,947 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 4
2023-02-17 06:19:05,190 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 2
2023-02-17 06:19:05,255 DEBUG CV Batch 1/1500 loss 26.066185 loss_att 35.639290 loss_ctc 36.103588 loss_rnnt 22.601927 hw_loss 0.396221 history loss 21.841757 rank 3
2023-02-17 06:19:13,255 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 1
2023-02-17 06:19:13,497 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 7
2023-02-17 06:19:13,715 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 6
2023-02-17 06:19:14,041 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 5
2023-02-17 06:19:14,691 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 0
2023-02-17 06:19:18,896 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 4
2023-02-17 06:19:19,110 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 2
2023-02-17 06:19:20,402 DEBUG CV Batch 1/1600 loss 42.379848 loss_att 86.189758 loss_ctc 57.224197 loss_rnnt 31.481510 hw_loss 0.294582 history loss 21.718219 rank 3
2023-02-17 06:19:25,759 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 1
2023-02-17 06:19:26,021 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 7
2023-02-17 06:19:26,215 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 6
2023-02-17 06:19:26,539 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 5
2023-02-17 06:19:27,869 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 0
2023-02-17 06:19:31,319 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 4
2023-02-17 06:19:31,584 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 2
2023-02-17 06:19:32,979 DEBUG CV Batch 1/1700 loss 33.121971 loss_att 34.367065 loss_ctc 46.934753 loss_rnnt 30.785290 hw_loss 0.461176 history loss 21.455031 rank 3
2023-02-17 06:19:34,983 INFO Epoch 1 CV info cv_loss 21.40747423288467
2023-02-17 06:19:34,984 INFO Epoch 2 TRAIN info lr 0.0006686
2023-02-17 06:19:34,987 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:35,230 INFO Epoch 1 CV info cv_loss 21.407474239569623
2023-02-17 06:19:35,232 INFO Epoch 2 TRAIN info lr 0.00066664
2023-02-17 06:19:35,237 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:35,460 INFO Epoch 1 CV info cv_loss 21.40747423853587
2023-02-17 06:19:35,461 INFO Epoch 2 TRAIN info lr 0.0006674399999999999
2023-02-17 06:19:35,464 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:35,669 INFO Epoch 1 CV info cv_loss 21.407474237226445
2023-02-17 06:19:35,670 INFO Epoch 2 TRAIN info lr 0.0006667999999999999
2023-02-17 06:19:35,673 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:37,276 INFO Epoch 1 CV info cv_loss 21.407474237226445
2023-02-17 06:19:37,277 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/1.pt
2023-02-17 06:19:38,643 INFO Epoch 2 TRAIN info lr 0.0006676799999999999
2023-02-17 06:19:38,649 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:40,479 INFO Epoch 1 CV info cv_loss 21.40747423943179
2023-02-17 06:19:40,480 INFO Epoch 2 TRAIN info lr 0.00066624
2023-02-17 06:19:40,483 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:40,798 INFO Epoch 1 CV info cv_loss 21.407474234676513
2023-02-17 06:19:40,799 INFO Epoch 2 TRAIN info lr 0.00066804
2023-02-17 06:19:40,803 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:19:43,039 INFO Epoch 1 CV info cv_loss 21.407474239638542
2023-02-17 06:19:43,041 INFO Epoch 2 TRAIN info lr 0.00066696
2023-02-17 06:19:43,045 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:20:56,046 DEBUG TRAIN Batch 2/0 loss 18.655655 loss_att 20.391872 loss_ctc 23.444229 loss_rnnt 17.447414 hw_loss 0.417223 lr 0.00066684 rank 5
2023-02-17 06:20:56,047 DEBUG TRAIN Batch 2/0 loss 15.587085 loss_att 17.064503 loss_ctc 19.390741 loss_rnnt 14.558740 hw_loss 0.423200 lr 0.00066668 rank 7
2023-02-17 06:20:56,051 DEBUG TRAIN Batch 2/0 loss 18.047726 loss_att 19.296902 loss_ctc 23.311522 loss_rnnt 16.810694 hw_loss 0.535044 lr 0.00066864 rank 1
2023-02-17 06:20:56,055 DEBUG TRAIN Batch 2/0 loss 17.920412 loss_att 19.618055 loss_ctc 23.005367 loss_rnnt 16.570084 hw_loss 0.624006 lr 0.00066748 rank 6
2023-02-17 06:20:56,057 DEBUG TRAIN Batch 2/0 loss 26.107679 loss_att 24.893412 loss_ctc 31.382908 loss_rnnt 25.406841 hw_loss 0.450614 lr 0.00066808 rank 2
2023-02-17 06:20:56,064 DEBUG TRAIN Batch 2/0 loss 27.397774 loss_att 28.470459 loss_ctc 34.232990 loss_rnnt 26.058105 hw_loss 0.400814 lr 0.00066700 rank 3
2023-02-17 06:20:56,068 DEBUG TRAIN Batch 2/0 loss 20.894569 loss_att 22.265852 loss_ctc 28.177715 loss_rnnt 19.358335 hw_loss 0.545423 lr 0.00066772 rank 0
2023-02-17 06:20:56,088 DEBUG TRAIN Batch 2/0 loss 22.809145 loss_att 22.708042 loss_ctc 28.305708 loss_rnnt 21.887846 hw_loss 0.391210 lr 0.00066628 rank 4
2023-02-17 06:22:09,657 DEBUG TRAIN Batch 2/100 loss 59.654339 loss_att 79.507431 loss_ctc 82.022629 loss_rnnt 52.532772 hw_loss 0.315961 lr 0.00067068 rank 7
2023-02-17 06:22:09,663 DEBUG TRAIN Batch 2/100 loss 36.701321 loss_att 52.946472 loss_ctc 47.489151 loss_rnnt 31.827131 hw_loss 0.350216 lr 0.00067148 rank 6
2023-02-17 06:22:09,663 DEBUG TRAIN Batch 2/100 loss 39.099068 loss_att 56.139549 loss_ctc 65.150589 loss_rnnt 32.023315 hw_loss 0.363978 lr 0.00067264 rank 1
2023-02-17 06:22:09,665 DEBUG TRAIN Batch 2/100 loss 33.394867 loss_att 53.424927 loss_ctc 45.897655 loss_rnnt 27.554062 hw_loss 0.314541 lr 0.00067172 rank 0
2023-02-17 06:22:09,665 DEBUG TRAIN Batch 2/100 loss 41.986420 loss_att 71.303291 loss_ctc 64.422546 loss_rnnt 32.915512 hw_loss 0.405097 lr 0.00067084 rank 5
2023-02-17 06:22:09,666 DEBUG TRAIN Batch 2/100 loss 61.089993 loss_att 99.974854 loss_ctc 96.928093 loss_rnnt 48.359138 hw_loss 0.328998 lr 0.00067100 rank 3
2023-02-17 06:22:09,666 DEBUG TRAIN Batch 2/100 loss 63.933754 loss_att 79.067093 loss_ctc 86.325912 loss_rnnt 57.742065 hw_loss 0.336368 lr 0.00067028 rank 4
2023-02-17 06:22:09,666 DEBUG TRAIN Batch 2/100 loss 30.598166 loss_att 51.609558 loss_ctc 43.721207 loss_rnnt 24.498434 hw_loss 0.276968 lr 0.00067208 rank 2
2023-02-17 06:23:24,957 DEBUG TRAIN Batch 2/200 loss 31.368290 loss_att 61.031597 loss_ctc 54.962658 loss_rnnt 22.114769 hw_loss 0.328016 lr 0.00067548 rank 6
2023-02-17 06:23:24,961 DEBUG TRAIN Batch 2/200 loss 33.830540 loss_att 55.164612 loss_ctc 43.552319 loss_rnnt 28.043331 hw_loss 0.420292 lr 0.00067484 rank 5
2023-02-17 06:23:24,964 DEBUG TRAIN Batch 2/200 loss 48.408085 loss_att 71.933731 loss_ctc 74.089996 loss_rnnt 40.077328 hw_loss 0.377570 lr 0.00067500 rank 3
2023-02-17 06:23:24,965 DEBUG TRAIN Batch 2/200 loss 53.258640 loss_att 76.736366 loss_ctc 75.875427 loss_rnnt 45.357994 hw_loss 0.355366 lr 0.00067608 rank 2
2023-02-17 06:23:24,964 DEBUG TRAIN Batch 2/200 loss 43.901688 loss_att 60.488873 loss_ctc 58.032921 loss_rnnt 38.498596 hw_loss 0.377797 lr 0.00067468 rank 7
2023-02-17 06:23:24,969 DEBUG TRAIN Batch 2/200 loss 36.706207 loss_att 55.357574 loss_ctc 52.560112 loss_rnnt 30.673565 hw_loss 0.353468 lr 0.00067428 rank 4
2023-02-17 06:23:24,969 DEBUG TRAIN Batch 2/200 loss 49.308903 loss_att 71.918579 loss_ctc 62.468029 loss_rnnt 42.848583 hw_loss 0.344688 lr 0.00067664 rank 1
2023-02-17 06:23:24,972 DEBUG TRAIN Batch 2/200 loss 41.525314 loss_att 59.738945 loss_ctc 67.718491 loss_rnnt 34.216694 hw_loss 0.325254 lr 0.00067572 rank 0
2023-02-17 06:24:42,220 DEBUG TRAIN Batch 2/300 loss 37.280941 loss_att 50.855343 loss_ctc 51.833031 loss_rnnt 32.420807 hw_loss 0.384318 lr 0.00067828 rank 4
2023-02-17 06:24:42,222 DEBUG TRAIN Batch 2/300 loss 48.768959 loss_att 64.640793 loss_ctc 68.771683 loss_rnnt 42.744518 hw_loss 0.343211 lr 0.00067884 rank 5
2023-02-17 06:24:42,221 DEBUG TRAIN Batch 2/300 loss 39.115551 loss_att 47.830002 loss_ctc 51.051929 loss_rnnt 35.586716 hw_loss 0.364547 lr 0.00068064 rank 1
2023-02-17 06:24:42,221 DEBUG TRAIN Batch 2/300 loss 31.697323 loss_att 47.926483 loss_ctc 44.379341 loss_rnnt 26.593649 hw_loss 0.312948 lr 0.00067868 rank 7
2023-02-17 06:24:42,222 DEBUG TRAIN Batch 2/300 loss 41.366753 loss_att 59.419674 loss_ctc 57.069809 loss_rnnt 35.504269 hw_loss 0.296542 lr 0.00067972 rank 0
2023-02-17 06:24:42,223 DEBUG TRAIN Batch 2/300 loss 52.913986 loss_att 69.427658 loss_ctc 67.455017 loss_rnnt 47.526299 hw_loss 0.274033 lr 0.00067900 rank 3
2023-02-17 06:24:42,224 DEBUG TRAIN Batch 2/300 loss 37.497368 loss_att 57.112534 loss_ctc 56.207840 loss_rnnt 30.931858 hw_loss 0.277019 lr 0.00068008 rank 2
2023-02-17 06:24:42,224 DEBUG TRAIN Batch 2/300 loss 36.350697 loss_att 58.833450 loss_ctc 61.080662 loss_rnnt 28.334370 hw_loss 0.417091 lr 0.00067948 rank 6
2023-02-17 06:26:00,607 DEBUG TRAIN Batch 2/400 loss 39.869030 loss_att 47.827896 loss_ctc 51.658630 loss_rnnt 36.565304 hw_loss 0.262515 lr 0.00068300 rank 3
2023-02-17 06:26:00,608 DEBUG TRAIN Batch 2/400 loss 39.379463 loss_att 60.776825 loss_ctc 49.849720 loss_rnnt 33.554276 hw_loss 0.280645 lr 0.00068464 rank 1
2023-02-17 06:26:00,613 DEBUG TRAIN Batch 2/400 loss 33.963730 loss_att 48.076931 loss_ctc 49.615612 loss_rnnt 28.812704 hw_loss 0.452757 lr 0.00068284 rank 5
2023-02-17 06:26:00,613 DEBUG TRAIN Batch 2/400 loss 40.646950 loss_att 55.332932 loss_ctc 55.025486 loss_rnnt 35.593655 hw_loss 0.373060 lr 0.00068348 rank 6
2023-02-17 06:26:00,614 DEBUG TRAIN Batch 2/400 loss 32.742531 loss_att 51.064774 loss_ctc 48.149967 loss_rnnt 26.883257 hw_loss 0.263434 lr 0.00068408 rank 2
2023-02-17 06:26:00,615 DEBUG TRAIN Batch 2/400 loss 40.160385 loss_att 57.324776 loss_ctc 59.587929 loss_rnnt 33.945877 hw_loss 0.358669 lr 0.00068268 rank 7
2023-02-17 06:26:00,617 DEBUG TRAIN Batch 2/400 loss 37.376797 loss_att 49.110096 loss_ctc 50.638275 loss_rnnt 33.013466 hw_loss 0.465887 lr 0.00068228 rank 4
2023-02-17 06:26:00,662 DEBUG TRAIN Batch 2/400 loss 28.401390 loss_att 52.725655 loss_ctc 38.735878 loss_rnnt 22.004089 hw_loss 0.289719 lr 0.00068372 rank 0
2023-02-17 06:27:16,254 DEBUG TRAIN Batch 2/500 loss 32.260738 loss_att 46.722496 loss_ctc 42.881607 loss_rnnt 27.729778 hw_loss 0.417179 lr 0.00068668 rank 7
2023-02-17 06:27:16,257 DEBUG TRAIN Batch 2/500 loss 48.451859 loss_att 60.470314 loss_ctc 61.694061 loss_rnnt 44.091362 hw_loss 0.358471 lr 0.00068748 rank 6
2023-02-17 06:27:16,258 DEBUG TRAIN Batch 2/500 loss 15.508706 loss_att 33.635895 loss_ctc 20.804087 loss_rnnt 10.976153 hw_loss 0.376994 lr 0.00068700 rank 3
2023-02-17 06:27:16,259 DEBUG TRAIN Batch 2/500 loss 38.604282 loss_att 54.226604 loss_ctc 51.563866 loss_rnnt 33.546928 hw_loss 0.384263 lr 0.00068684 rank 5
2023-02-17 06:27:16,260 DEBUG TRAIN Batch 2/500 loss 43.419304 loss_att 55.971024 loss_ctc 54.695259 loss_rnnt 39.250835 hw_loss 0.290001 lr 0.00068864 rank 1
2023-02-17 06:27:16,261 DEBUG TRAIN Batch 2/500 loss 48.861855 loss_att 63.492146 loss_ctc 69.018539 loss_rnnt 43.060650 hw_loss 0.351721 lr 0.00068628 rank 4
2023-02-17 06:27:16,263 DEBUG TRAIN Batch 2/500 loss 38.951664 loss_att 56.493923 loss_ctc 60.517761 loss_rnnt 32.374977 hw_loss 0.361416 lr 0.00068808 rank 2
2023-02-17 06:27:16,264 DEBUG TRAIN Batch 2/500 loss 41.470882 loss_att 61.764908 loss_ctc 62.249157 loss_rnnt 34.488941 hw_loss 0.286311 lr 0.00068772 rank 0
2023-02-17 06:28:32,179 DEBUG TRAIN Batch 2/600 loss 22.441610 loss_att 24.492195 loss_ctc 30.643866 loss_rnnt 20.678171 hw_loss 0.486912 lr 0.00069084 rank 5
2023-02-17 06:28:32,180 DEBUG TRAIN Batch 2/600 loss 34.156738 loss_att 48.168274 loss_ctc 46.238041 loss_rnnt 29.515888 hw_loss 0.426945 lr 0.00069068 rank 7
2023-02-17 06:28:32,180 DEBUG TRAIN Batch 2/600 loss 32.087296 loss_att 40.758907 loss_ctc 39.756435 loss_rnnt 29.078636 hw_loss 0.472100 lr 0.00069148 rank 6
2023-02-17 06:28:32,183 DEBUG TRAIN Batch 2/600 loss 30.171646 loss_att 35.930592 loss_ctc 41.585796 loss_rnnt 27.241369 hw_loss 0.481126 lr 0.00069100 rank 3
2023-02-17 06:28:32,182 DEBUG TRAIN Batch 2/600 loss 33.837017 loss_att 44.183426 loss_ctc 44.873295 loss_rnnt 30.054235 hw_loss 0.453739 lr 0.00069264 rank 1
2023-02-17 06:28:32,184 DEBUG TRAIN Batch 2/600 loss 29.367022 loss_att 35.624916 loss_ctc 40.649372 loss_rnnt 26.337906 hw_loss 0.512289 lr 0.00069172 rank 0
2023-02-17 06:28:32,187 DEBUG TRAIN Batch 2/600 loss 31.816139 loss_att 38.678211 loss_ctc 41.025978 loss_rnnt 28.958393 hw_loss 0.482532 lr 0.00069208 rank 2
2023-02-17 06:28:32,187 DEBUG TRAIN Batch 2/600 loss 23.599710 loss_att 29.984604 loss_ctc 33.775475 loss_rnnt 20.776554 hw_loss 0.355142 lr 0.00069028 rank 4
2023-02-17 06:29:51,129 DEBUG TRAIN Batch 2/700 loss 42.391682 loss_att 66.223518 loss_ctc 61.418652 loss_rnnt 34.944313 hw_loss 0.270133 lr 0.00069548 rank 6
2023-02-17 06:29:51,139 DEBUG TRAIN Batch 2/700 loss 53.164536 loss_att 72.936600 loss_ctc 77.840843 loss_rnnt 45.759048 hw_loss 0.301686 lr 0.00069428 rank 4
2023-02-17 06:29:51,146 DEBUG TRAIN Batch 2/700 loss 42.960976 loss_att 62.675751 loss_ctc 60.689125 loss_rnnt 36.493454 hw_loss 0.301523 lr 0.00069572 rank 0
2023-02-17 06:29:51,146 DEBUG TRAIN Batch 2/700 loss 61.907383 loss_att 83.629234 loss_ctc 91.143433 loss_rnnt 53.432907 hw_loss 0.434925 lr 0.00069608 rank 2
2023-02-17 06:29:51,153 DEBUG TRAIN Batch 2/700 loss 63.656124 loss_att 89.910843 loss_ctc 73.981194 loss_rnnt 56.824173 hw_loss 0.383119 lr 0.00069500 rank 3
2023-02-17 06:29:51,156 DEBUG TRAIN Batch 2/700 loss 42.072800 loss_att 56.017635 loss_ctc 56.107826 loss_rnnt 37.224937 hw_loss 0.351670 lr 0.00069468 rank 7
2023-02-17 06:29:51,166 DEBUG TRAIN Batch 2/700 loss 22.837395 loss_att 36.033928 loss_ctc 31.156294 loss_rnnt 18.947716 hw_loss 0.264719 lr 0.00069484 rank 5
2023-02-17 06:29:51,185 DEBUG TRAIN Batch 2/700 loss 49.960407 loss_att 66.895187 loss_ctc 73.271011 loss_rnnt 43.264252 hw_loss 0.377096 lr 0.00069664 rank 1
2023-02-17 06:31:07,776 DEBUG TRAIN Batch 2/800 loss 66.363091 loss_att 82.643417 loss_ctc 91.980240 loss_rnnt 59.498795 hw_loss 0.361144 lr 0.00069900 rank 3
2023-02-17 06:31:07,778 DEBUG TRAIN Batch 2/800 loss 38.996170 loss_att 58.154808 loss_ctc 61.607384 loss_rnnt 31.976151 hw_loss 0.325242 lr 0.00069868 rank 7
2023-02-17 06:31:07,781 DEBUG TRAIN Batch 2/800 loss 46.712662 loss_att 69.215988 loss_ctc 65.296036 loss_rnnt 39.550793 hw_loss 0.343915 lr 0.00069948 rank 6
2023-02-17 06:31:07,781 DEBUG TRAIN Batch 2/800 loss 52.294998 loss_att 71.076324 loss_ctc 76.657547 loss_rnnt 45.090801 hw_loss 0.374230 lr 0.00069884 rank 5
2023-02-17 06:31:07,782 DEBUG TRAIN Batch 2/800 loss 46.305340 loss_att 58.190002 loss_ctc 60.858456 loss_rnnt 41.807629 hw_loss 0.338182 lr 0.00069828 rank 4
2023-02-17 06:31:07,784 DEBUG TRAIN Batch 2/800 loss 66.137833 loss_att 77.026619 loss_ctc 86.456177 loss_rnnt 61.003319 hw_loss 0.464319 lr 0.00070064 rank 1
2023-02-17 06:31:07,785 DEBUG TRAIN Batch 2/800 loss 40.888508 loss_att 51.204369 loss_ctc 58.137749 loss_rnnt 36.325294 hw_loss 0.375265 lr 0.00070008 rank 2
2023-02-17 06:31:07,787 DEBUG TRAIN Batch 2/800 loss 37.214058 loss_att 54.763969 loss_ctc 51.900356 loss_rnnt 31.537979 hw_loss 0.389848 lr 0.00069972 rank 0
2023-02-17 06:32:23,656 DEBUG TRAIN Batch 2/900 loss 41.829758 loss_att 56.383324 loss_ctc 52.134171 loss_rnnt 37.368660 hw_loss 0.330866 lr 0.00070348 rank 6
2023-02-17 06:32:23,658 DEBUG TRAIN Batch 2/900 loss 36.448566 loss_att 59.602089 loss_ctc 57.008251 loss_rnnt 28.889256 hw_loss 0.351210 lr 0.00070228 rank 4
2023-02-17 06:32:23,659 DEBUG TRAIN Batch 2/900 loss 33.589142 loss_att 53.355652 loss_ctc 54.344379 loss_rnnt 26.678886 hw_loss 0.355468 lr 0.00070372 rank 0
2023-02-17 06:32:23,661 DEBUG TRAIN Batch 2/900 loss 50.084415 loss_att 59.383415 loss_ctc 64.432175 loss_rnnt 46.132660 hw_loss 0.335474 lr 0.00070300 rank 3
2023-02-17 06:32:23,663 DEBUG TRAIN Batch 2/900 loss 61.154995 loss_att 82.632164 loss_ctc 91.811981 loss_rnnt 52.597744 hw_loss 0.326663 lr 0.00070408 rank 2
2023-02-17 06:32:23,663 DEBUG TRAIN Batch 2/900 loss 43.788960 loss_att 70.014954 loss_ctc 54.896690 loss_rnnt 36.857422 hw_loss 0.384952 lr 0.00070268 rank 7
2023-02-17 06:32:23,667 DEBUG TRAIN Batch 2/900 loss 48.911617 loss_att 64.460815 loss_ctc 73.607361 loss_rnnt 42.300797 hw_loss 0.390407 lr 0.00070284 rank 5
2023-02-17 06:32:23,667 DEBUG TRAIN Batch 2/900 loss 48.874420 loss_att 65.618034 loss_ctc 65.233093 loss_rnnt 43.204479 hw_loss 0.262624 lr 0.00070464 rank 1
2023-02-17 06:33:40,226 DEBUG TRAIN Batch 2/1000 loss 36.510136 loss_att 47.718109 loss_ctc 51.680790 loss_rnnt 32.044434 hw_loss 0.377528 lr 0.00070700 rank 3
2023-02-17 06:33:40,227 DEBUG TRAIN Batch 2/1000 loss 38.352592 loss_att 53.564690 loss_ctc 48.066261 loss_rnnt 33.810719 hw_loss 0.383053 lr 0.00070668 rank 7
2023-02-17 06:33:40,231 DEBUG TRAIN Batch 2/1000 loss 42.347191 loss_att 56.738838 loss_ctc 67.091644 loss_rnnt 36.010475 hw_loss 0.298359 lr 0.00070748 rank 6
2023-02-17 06:33:40,237 DEBUG TRAIN Batch 2/1000 loss 35.651741 loss_att 46.130608 loss_ctc 52.950005 loss_rnnt 31.038681 hw_loss 0.395350 lr 0.00070808 rank 2
2023-02-17 06:33:40,245 DEBUG TRAIN Batch 2/1000 loss 41.702164 loss_att 66.183273 loss_ctc 59.293564 loss_rnnt 34.285404 hw_loss 0.328160 lr 0.00070772 rank 0
2023-02-17 06:33:40,247 DEBUG TRAIN Batch 2/1000 loss 37.160297 loss_att 51.749710 loss_ctc 49.302349 loss_rnnt 32.452538 hw_loss 0.320497 lr 0.00070864 rank 1
2023-02-17 06:33:40,251 DEBUG TRAIN Batch 2/1000 loss 39.956818 loss_att 54.088291 loss_ctc 50.916046 loss_rnnt 35.503563 hw_loss 0.310748 lr 0.00070684 rank 5
2023-02-17 06:33:40,270 DEBUG TRAIN Batch 2/1000 loss 46.824013 loss_att 63.372955 loss_ctc 60.178085 loss_rnnt 41.555428 hw_loss 0.334222 lr 0.00070628 rank 4
2023-02-17 06:34:59,926 DEBUG TRAIN Batch 2/1100 loss 37.490662 loss_att 48.329102 loss_ctc 53.893066 loss_rnnt 32.979179 hw_loss 0.294012 lr 0.00071264 rank 1
2023-02-17 06:34:59,930 DEBUG TRAIN Batch 2/1100 loss 21.845961 loss_att 35.385086 loss_ctc 32.108437 loss_rnnt 17.600861 hw_loss 0.316769 lr 0.00071148 rank 6
2023-02-17 06:34:59,931 DEBUG TRAIN Batch 2/1100 loss 43.283241 loss_att 55.715828 loss_ctc 66.869370 loss_rnnt 37.425911 hw_loss 0.423734 lr 0.00071068 rank 7
2023-02-17 06:34:59,932 DEBUG TRAIN Batch 2/1100 loss 30.821590 loss_att 46.804203 loss_ctc 45.456100 loss_rnnt 25.457523 hw_loss 0.405511 lr 0.00071100 rank 3
2023-02-17 06:34:59,933 DEBUG TRAIN Batch 2/1100 loss 34.011887 loss_att 44.607925 loss_ctc 46.747086 loss_rnnt 30.008770 hw_loss 0.348533 lr 0.00071084 rank 5
2023-02-17 06:34:59,936 DEBUG TRAIN Batch 2/1100 loss 28.009766 loss_att 41.073601 loss_ctc 46.449509 loss_rnnt 22.747971 hw_loss 0.356987 lr 0.00071028 rank 4
2023-02-17 06:34:59,939 DEBUG TRAIN Batch 2/1100 loss 52.966274 loss_att 77.193489 loss_ctc 77.353966 loss_rnnt 44.643066 hw_loss 0.423882 lr 0.00071208 rank 2
2023-02-17 06:34:59,941 DEBUG TRAIN Batch 2/1100 loss 33.257957 loss_att 46.955414 loss_ctc 44.956337 loss_rnnt 28.744831 hw_loss 0.400959 lr 0.00071172 rank 0
2023-02-17 06:36:16,694 DEBUG TRAIN Batch 2/1200 loss 31.268845 loss_att 43.835434 loss_ctc 49.356121 loss_rnnt 26.156912 hw_loss 0.350586 lr 0.00071548 rank 6
2023-02-17 06:36:16,699 DEBUG TRAIN Batch 2/1200 loss 30.065058 loss_att 36.721798 loss_ctc 39.462688 loss_rnnt 27.206060 hw_loss 0.514935 lr 0.00071608 rank 2
2023-02-17 06:36:16,699 DEBUG TRAIN Batch 2/1200 loss 30.233717 loss_att 43.658207 loss_ctc 44.934929 loss_rnnt 25.404591 hw_loss 0.345126 lr 0.00071468 rank 7
2023-02-17 06:36:16,702 DEBUG TRAIN Batch 2/1200 loss 58.873096 loss_att 62.971680 loss_ctc 70.628540 loss_rnnt 56.293072 hw_loss 0.361718 lr 0.00071484 rank 5
2023-02-17 06:36:16,702 DEBUG TRAIN Batch 2/1200 loss 35.303139 loss_att 49.500420 loss_ctc 53.415993 loss_rnnt 29.803375 hw_loss 0.459866 lr 0.00071500 rank 3
2023-02-17 06:36:16,703 DEBUG TRAIN Batch 2/1200 loss 23.067940 loss_att 29.967937 loss_ctc 34.544342 loss_rnnt 19.888885 hw_loss 0.504130 lr 0.00071428 rank 4
2023-02-17 06:36:16,704 DEBUG TRAIN Batch 2/1200 loss 32.524696 loss_att 40.821941 loss_ctc 42.053879 loss_rnnt 29.368620 hw_loss 0.423880 lr 0.00071664 rank 1
2023-02-17 06:36:16,711 DEBUG TRAIN Batch 2/1200 loss 22.120148 loss_att 28.789856 loss_ctc 31.230707 loss_rnnt 19.337177 hw_loss 0.439291 lr 0.00071572 rank 0
2023-02-17 06:37:33,159 DEBUG TRAIN Batch 2/1300 loss 35.771515 loss_att 52.064327 loss_ctc 52.071220 loss_rnnt 30.187805 hw_loss 0.284716 lr 0.00071868 rank 7
2023-02-17 06:37:33,159 DEBUG TRAIN Batch 2/1300 loss 20.232132 loss_att 22.223551 loss_ctc 25.176252 loss_rnnt 18.913864 hw_loss 0.488940 lr 0.00071948 rank 6
2023-02-17 06:37:33,160 DEBUG TRAIN Batch 2/1300 loss 66.762268 loss_att 88.818878 loss_ctc 91.425468 loss_rnnt 58.899757 hw_loss 0.305167 lr 0.00071884 rank 5
2023-02-17 06:37:33,161 DEBUG TRAIN Batch 2/1300 loss 54.411346 loss_att 76.229614 loss_ctc 86.643494 loss_rnnt 45.599045 hw_loss 0.283187 lr 0.00071972 rank 0
2023-02-17 06:37:33,163 DEBUG TRAIN Batch 2/1300 loss 51.313061 loss_att 82.903038 loss_ctc 77.589439 loss_rnnt 41.326393 hw_loss 0.309663 lr 0.00072008 rank 2
2023-02-17 06:37:33,167 DEBUG TRAIN Batch 2/1300 loss 20.189432 loss_att 35.688896 loss_ctc 27.551710 loss_rnnt 15.978527 hw_loss 0.242580 lr 0.00071828 rank 4
2023-02-17 06:37:33,167 DEBUG TRAIN Batch 2/1300 loss 23.728111 loss_att 41.841633 loss_ctc 37.435825 loss_rnnt 18.086515 hw_loss 0.358496 lr 0.00072064 rank 1
2023-02-17 06:37:33,213 DEBUG TRAIN Batch 2/1300 loss 21.297230 loss_att 29.093220 loss_ctc 23.771772 loss_rnnt 19.313614 hw_loss 0.177143 lr 0.00071900 rank 3
2023-02-17 06:38:52,030 DEBUG TRAIN Batch 2/1400 loss 29.101233 loss_att 51.017117 loss_ctc 50.746605 loss_rnnt 21.616003 hw_loss 0.405000 lr 0.00072408 rank 2
2023-02-17 06:38:52,032 DEBUG TRAIN Batch 2/1400 loss 29.187969 loss_att 51.197746 loss_ctc 50.301949 loss_rnnt 21.806952 hw_loss 0.307246 lr 0.00072348 rank 6
2023-02-17 06:38:52,033 DEBUG TRAIN Batch 2/1400 loss 48.926250 loss_att 68.900543 loss_ctc 69.128906 loss_rnnt 42.019806 hw_loss 0.408554 lr 0.00072284 rank 5
2023-02-17 06:38:52,034 DEBUG TRAIN Batch 2/1400 loss 17.399555 loss_att 31.952814 loss_ctc 30.603195 loss_rnnt 12.543262 hw_loss 0.347167 lr 0.00072464 rank 1
2023-02-17 06:38:52,034 DEBUG TRAIN Batch 2/1400 loss 34.759586 loss_att 53.406467 loss_ctc 55.743679 loss_rnnt 28.011154 hw_loss 0.414708 lr 0.00072228 rank 4
2023-02-17 06:38:52,035 DEBUG TRAIN Batch 2/1400 loss 48.077671 loss_att 63.655487 loss_ctc 66.072342 loss_rnnt 42.417526 hw_loss 0.272412 lr 0.00072268 rank 7
2023-02-17 06:38:52,039 DEBUG TRAIN Batch 2/1400 loss 72.143906 loss_att 96.008942 loss_ctc 98.258720 loss_rnnt 63.720299 hw_loss 0.316158 lr 0.00072300 rank 3
2023-02-17 06:38:52,040 DEBUG TRAIN Batch 2/1400 loss 49.267315 loss_att 77.430885 loss_ctc 75.435837 loss_rnnt 40.001019 hw_loss 0.270830 lr 0.00072372 rank 0
2023-02-17 06:40:08,619 DEBUG TRAIN Batch 2/1500 loss 35.629192 loss_att 51.457958 loss_ctc 54.200279 loss_rnnt 29.810568 hw_loss 0.331362 lr 0.00072700 rank 3
2023-02-17 06:40:08,624 DEBUG TRAIN Batch 2/1500 loss 46.408142 loss_att 72.547142 loss_ctc 69.559525 loss_rnnt 37.917572 hw_loss 0.329851 lr 0.00072748 rank 6
2023-02-17 06:40:08,625 DEBUG TRAIN Batch 2/1500 loss 60.487396 loss_att 77.622574 loss_ctc 92.474144 loss_rnnt 52.592789 hw_loss 0.380010 lr 0.00072668 rank 7
2023-02-17 06:40:08,626 DEBUG TRAIN Batch 2/1500 loss 30.923450 loss_att 41.585632 loss_ctc 46.872543 loss_rnnt 26.484665 hw_loss 0.337133 lr 0.00072864 rank 1
2023-02-17 06:40:08,626 DEBUG TRAIN Batch 2/1500 loss 33.785770 loss_att 49.791435 loss_ctc 52.406418 loss_rnnt 27.915474 hw_loss 0.349508 lr 0.00072684 rank 5
2023-02-17 06:40:08,627 DEBUG TRAIN Batch 2/1500 loss 26.728460 loss_att 45.053101 loss_ctc 40.410255 loss_rnnt 21.035542 hw_loss 0.382033 lr 0.00072628 rank 4
2023-02-17 06:40:08,630 DEBUG TRAIN Batch 2/1500 loss 35.207008 loss_att 54.735817 loss_ctc 49.806911 loss_rnnt 29.145781 hw_loss 0.391517 lr 0.00072808 rank 2
2023-02-17 06:40:08,634 DEBUG TRAIN Batch 2/1500 loss 41.336361 loss_att 58.893909 loss_ctc 62.261414 loss_rnnt 34.871502 hw_loss 0.306263 lr 0.00072772 rank 0
2023-02-17 06:41:22,985 DEBUG TRAIN Batch 2/1600 loss 41.174782 loss_att 55.431625 loss_ctc 66.847260 loss_rnnt 34.739777 hw_loss 0.301195 lr 0.00073148 rank 6
2023-02-17 06:41:22,989 DEBUG TRAIN Batch 2/1600 loss 79.682320 loss_att 96.294937 loss_ctc 112.064697 loss_rnnt 71.878410 hw_loss 0.307008 lr 0.00073068 rank 7
2023-02-17 06:41:22,990 DEBUG TRAIN Batch 2/1600 loss 36.356010 loss_att 53.075836 loss_ctc 49.769260 loss_rnnt 31.005939 hw_loss 0.408135 lr 0.00073100 rank 3
2023-02-17 06:41:22,990 DEBUG TRAIN Batch 2/1600 loss 46.410160 loss_att 73.706207 loss_ctc 64.030853 loss_rnnt 38.403435 hw_loss 0.371419 lr 0.00073264 rank 1
2023-02-17 06:41:22,992 DEBUG TRAIN Batch 2/1600 loss 25.717852 loss_att 38.196690 loss_ctc 35.858452 loss_rnnt 21.704609 hw_loss 0.310109 lr 0.00073084 rank 5
2023-02-17 06:41:22,992 DEBUG TRAIN Batch 2/1600 loss 35.168339 loss_att 53.587578 loss_ctc 56.776253 loss_rnnt 28.449320 hw_loss 0.288963 lr 0.00073172 rank 0
2023-02-17 06:41:22,998 DEBUG TRAIN Batch 2/1600 loss 45.170494 loss_att 65.290260 loss_ctc 60.087475 loss_rnnt 38.928566 hw_loss 0.429455 lr 0.00073208 rank 2
2023-02-17 06:41:23,034 DEBUG TRAIN Batch 2/1600 loss 61.645538 loss_att 72.085487 loss_ctc 77.023346 loss_rnnt 57.297287 hw_loss 0.393530 lr 0.00073028 rank 4
2023-02-17 06:42:37,867 DEBUG TRAIN Batch 2/1700 loss 32.875290 loss_att 47.396057 loss_ctc 47.731804 loss_rnnt 27.774769 hw_loss 0.404066 lr 0.00073500 rank 3
2023-02-17 06:42:37,868 DEBUG TRAIN Batch 2/1700 loss 22.932285 loss_att 36.693047 loss_ctc 32.768833 loss_rnnt 18.699148 hw_loss 0.317705 lr 0.00073548 rank 6
2023-02-17 06:42:37,869 DEBUG TRAIN Batch 2/1700 loss 37.279816 loss_att 54.159088 loss_ctc 64.532967 loss_rnnt 30.070919 hw_loss 0.373668 lr 0.00073484 rank 5
2023-02-17 06:42:37,872 DEBUG TRAIN Batch 2/1700 loss 51.801239 loss_att 64.144455 loss_ctc 72.242737 loss_rnnt 46.466480 hw_loss 0.263587 lr 0.00073468 rank 7
2023-02-17 06:42:37,874 DEBUG TRAIN Batch 2/1700 loss 42.544109 loss_att 53.558720 loss_ctc 57.208702 loss_rnnt 38.185928 hw_loss 0.374958 lr 0.00073428 rank 4
2023-02-17 06:42:37,875 DEBUG TRAIN Batch 2/1700 loss 19.163416 loss_att 27.403877 loss_ctc 33.160904 loss_rnnt 15.411382 hw_loss 0.445518 lr 0.00073664 rank 1
2023-02-17 06:42:37,885 DEBUG TRAIN Batch 2/1700 loss 43.098274 loss_att 57.174454 loss_ctc 53.774254 loss_rnnt 38.693970 hw_loss 0.310507 lr 0.00073572 rank 0
2023-02-17 06:42:37,927 DEBUG TRAIN Batch 2/1700 loss 42.445782 loss_att 55.645370 loss_ctc 57.736633 loss_rnnt 37.557457 hw_loss 0.393051 lr 0.00073608 rank 2
2023-02-17 06:43:56,945 DEBUG TRAIN Batch 2/1800 loss 29.934736 loss_att 38.219284 loss_ctc 37.161819 loss_rnnt 27.110510 hw_loss 0.381948 lr 0.00073828 rank 4
2023-02-17 06:43:56,945 DEBUG TRAIN Batch 2/1800 loss 28.604258 loss_att 38.221992 loss_ctc 44.551056 loss_rnnt 24.373196 hw_loss 0.339894 lr 0.00073868 rank 7
2023-02-17 06:43:56,947 DEBUG TRAIN Batch 2/1800 loss 43.904709 loss_att 48.926964 loss_ctc 62.434738 loss_rnnt 40.255768 hw_loss 0.325910 lr 0.00073884 rank 5
2023-02-17 06:43:56,948 DEBUG TRAIN Batch 2/1800 loss 24.471394 loss_att 33.210148 loss_ctc 30.821388 loss_rnnt 21.640299 hw_loss 0.443774 lr 0.00073900 rank 3
2023-02-17 06:43:56,949 DEBUG TRAIN Batch 2/1800 loss 36.750778 loss_att 50.922157 loss_ctc 48.937679 loss_rnnt 32.112427 hw_loss 0.335916 lr 0.00074008 rank 2
2023-02-17 06:43:56,950 DEBUG TRAIN Batch 2/1800 loss 21.044447 loss_att 27.527306 loss_ctc 29.664015 loss_rnnt 18.414263 hw_loss 0.345628 lr 0.00073948 rank 6
2023-02-17 06:43:56,951 DEBUG TRAIN Batch 2/1800 loss 33.931347 loss_att 40.194466 loss_ctc 44.994038 loss_rnnt 30.967951 hw_loss 0.442015 lr 0.00074064 rank 1
2023-02-17 06:43:56,955 DEBUG TRAIN Batch 2/1800 loss 41.539223 loss_att 55.287083 loss_ctc 56.221275 loss_rnnt 36.615162 hw_loss 0.406651 lr 0.00073972 rank 0
2023-02-17 06:45:12,040 DEBUG TRAIN Batch 2/1900 loss 40.314217 loss_att 55.634773 loss_ctc 58.669750 loss_rnnt 34.639717 hw_loss 0.305597 lr 0.00074464 rank 1
2023-02-17 06:45:12,040 DEBUG TRAIN Batch 2/1900 loss 19.551889 loss_att 22.300943 loss_ctc 25.746908 loss_rnnt 17.898861 hw_loss 0.519778 lr 0.00074348 rank 6
2023-02-17 06:45:12,041 DEBUG TRAIN Batch 2/1900 loss 32.422565 loss_att 45.423203 loss_ctc 43.510113 loss_rnnt 28.148800 hw_loss 0.366185 lr 0.00074284 rank 5
2023-02-17 06:45:12,041 DEBUG TRAIN Batch 2/1900 loss 22.353024 loss_att 22.578611 loss_ctc 27.964861 loss_rnnt 21.269598 hw_loss 0.543869 lr 0.00074228 rank 4
2023-02-17 06:45:12,042 DEBUG TRAIN Batch 2/1900 loss 28.545534 loss_att 34.573158 loss_ctc 38.396049 loss_rnnt 25.836843 hw_loss 0.355809 lr 0.00074300 rank 3
2023-02-17 06:45:12,044 DEBUG TRAIN Batch 2/1900 loss 51.310791 loss_att 71.221405 loss_ctc 72.897011 loss_rnnt 44.210258 hw_loss 0.450466 lr 0.00074372 rank 0
2023-02-17 06:45:12,044 DEBUG TRAIN Batch 2/1900 loss 29.534670 loss_att 39.019455 loss_ctc 41.662289 loss_rnnt 25.821821 hw_loss 0.372892 lr 0.00074268 rank 7
2023-02-17 06:45:12,046 DEBUG TRAIN Batch 2/1900 loss 21.999826 loss_att 25.059196 loss_ctc 27.751179 loss_rnnt 20.349371 hw_loss 0.509504 lr 0.00074408 rank 2
2023-02-17 06:46:27,086 DEBUG TRAIN Batch 2/2000 loss 27.944809 loss_att 35.461437 loss_ctc 40.206779 loss_rnnt 24.610733 hw_loss 0.367166 lr 0.00074700 rank 3
2023-02-17 06:46:27,092 DEBUG TRAIN Batch 2/2000 loss 26.250235 loss_att 38.604225 loss_ctc 39.031540 loss_rnnt 21.891861 hw_loss 0.343877 lr 0.00074864 rank 1
2023-02-17 06:46:27,093 DEBUG TRAIN Batch 2/2000 loss 37.491337 loss_att 42.426868 loss_ctc 45.346031 loss_rnnt 35.284225 hw_loss 0.323843 lr 0.00074684 rank 5
2023-02-17 06:46:27,093 DEBUG TRAIN Batch 2/2000 loss 59.398502 loss_att 74.691544 loss_ctc 87.220978 loss_rnnt 52.493561 hw_loss 0.256248 lr 0.00074748 rank 6
2023-02-17 06:46:27,096 DEBUG TRAIN Batch 2/2000 loss 67.380760 loss_att 84.679207 loss_ctc 98.807854 loss_rnnt 59.575127 hw_loss 0.291884 lr 0.00074628 rank 4
2023-02-17 06:46:27,097 DEBUG TRAIN Batch 2/2000 loss 36.818275 loss_att 56.761795 loss_ctc 61.893456 loss_rnnt 29.304981 hw_loss 0.339799 lr 0.00074772 rank 0
2023-02-17 06:46:27,098 DEBUG TRAIN Batch 2/2000 loss 50.098198 loss_att 59.941238 loss_ctc 63.837780 loss_rnnt 46.128601 hw_loss 0.316966 lr 0.00074808 rank 2
2023-02-17 06:46:27,098 DEBUG TRAIN Batch 2/2000 loss 42.782341 loss_att 62.420483 loss_ctc 66.628357 loss_rnnt 35.466736 hw_loss 0.390955 lr 0.00074668 rank 7
2023-02-17 06:47:45,839 DEBUG TRAIN Batch 2/2100 loss 36.585644 loss_att 56.898354 loss_ctc 52.375771 loss_rnnt 30.289686 hw_loss 0.240121 lr 0.00075068 rank 7
2023-02-17 06:47:45,839 DEBUG TRAIN Batch 2/2100 loss 34.654758 loss_att 48.725204 loss_ctc 53.933296 loss_rnnt 29.102468 hw_loss 0.314489 lr 0.00075264 rank 1
2023-02-17 06:47:45,840 DEBUG TRAIN Batch 2/2100 loss 23.315308 loss_att 35.556381 loss_ctc 33.158749 loss_rnnt 19.362621 hw_loss 0.360022 lr 0.00075084 rank 5
2023-02-17 06:47:45,842 DEBUG TRAIN Batch 2/2100 loss 23.959099 loss_att 37.109650 loss_ctc 33.316353 loss_rnnt 19.948698 hw_loss 0.248735 lr 0.00075028 rank 4
2023-02-17 06:47:45,843 DEBUG TRAIN Batch 2/2100 loss 32.370266 loss_att 49.230713 loss_ctc 45.449188 loss_rnnt 27.046696 hw_loss 0.389300 lr 0.00075100 rank 3
2023-02-17 06:47:45,846 DEBUG TRAIN Batch 2/2100 loss 47.830284 loss_att 66.070145 loss_ctc 73.291718 loss_rnnt 40.612335 hw_loss 0.328353 lr 0.00075148 rank 6
2023-02-17 06:47:45,846 DEBUG TRAIN Batch 2/2100 loss 24.473240 loss_att 37.169529 loss_ctc 44.403603 loss_rnnt 19.107655 hw_loss 0.316773 lr 0.00075172 rank 0
2023-02-17 06:47:45,846 DEBUG TRAIN Batch 2/2100 loss 36.818237 loss_att 45.856567 loss_ctc 55.142982 loss_rnnt 32.401596 hw_loss 0.310643 lr 0.00075208 rank 2
2023-02-17 06:49:02,003 DEBUG TRAIN Batch 2/2200 loss 62.040073 loss_att 84.342873 loss_ctc 93.412018 loss_rnnt 53.216026 hw_loss 0.338552 lr 0.00075548 rank 6
2023-02-17 06:49:02,005 DEBUG TRAIN Batch 2/2200 loss 33.255501 loss_att 43.287849 loss_ctc 44.534515 loss_rnnt 29.528240 hw_loss 0.406729 lr 0.00075500 rank 3
2023-02-17 06:49:02,006 DEBUG TRAIN Batch 2/2200 loss 35.490005 loss_att 43.992870 loss_ctc 48.824669 loss_rnnt 31.814655 hw_loss 0.369035 lr 0.00075484 rank 5
2023-02-17 06:49:02,006 DEBUG TRAIN Batch 2/2200 loss 53.128754 loss_att 67.621819 loss_ctc 75.705811 loss_rnnt 47.010323 hw_loss 0.392899 lr 0.00075468 rank 7
2023-02-17 06:49:02,013 DEBUG TRAIN Batch 2/2200 loss 62.250805 loss_att 82.216827 loss_ctc 87.290009 loss_rnnt 54.747299 hw_loss 0.322018 lr 0.00075608 rank 2
2023-02-17 06:49:02,013 DEBUG TRAIN Batch 2/2200 loss 40.942993 loss_att 57.211914 loss_ctc 51.061634 loss_rnnt 36.151672 hw_loss 0.353225 lr 0.00075428 rank 4
2023-02-17 06:49:02,013 DEBUG TRAIN Batch 2/2200 loss 31.365276 loss_att 48.172661 loss_ctc 48.894188 loss_rnnt 25.523390 hw_loss 0.268540 lr 0.00075664 rank 1
2023-02-17 06:49:02,073 DEBUG TRAIN Batch 2/2200 loss 37.267368 loss_att 47.348679 loss_ctc 49.417328 loss_rnnt 33.422245 hw_loss 0.391622 lr 0.00075572 rank 0
2023-02-17 06:50:18,659 DEBUG TRAIN Batch 2/2300 loss 47.915211 loss_att 53.037395 loss_ctc 67.061211 loss_rnnt 44.128117 hw_loss 0.393482 lr 0.00075828 rank 4
2023-02-17 06:50:18,665 DEBUG TRAIN Batch 2/2300 loss 46.132496 loss_att 64.874374 loss_ctc 63.240593 loss_rnnt 39.975849 hw_loss 0.238485 lr 0.00075868 rank 7
2023-02-17 06:50:18,666 DEBUG TRAIN Batch 2/2300 loss 46.612144 loss_att 61.637100 loss_ctc 63.825741 loss_rnnt 41.128944 hw_loss 0.343250 lr 0.00076064 rank 1
2023-02-17 06:50:18,667 DEBUG TRAIN Batch 2/2300 loss 54.956474 loss_att 74.865967 loss_ctc 75.970184 loss_rnnt 48.022781 hw_loss 0.281179 lr 0.00075900 rank 3
2023-02-17 06:50:18,668 DEBUG TRAIN Batch 2/2300 loss 50.303612 loss_att 67.143494 loss_ctc 70.709213 loss_rnnt 44.035954 hw_loss 0.335489 lr 0.00075948 rank 6
2023-02-17 06:50:18,669 DEBUG TRAIN Batch 2/2300 loss 62.176498 loss_att 81.387268 loss_ctc 84.530426 loss_rnnt 55.144943 hw_loss 0.391635 lr 0.00075972 rank 0
2023-02-17 06:50:18,669 DEBUG TRAIN Batch 2/2300 loss 43.868633 loss_att 56.810196 loss_ctc 60.058594 loss_rnnt 38.964149 hw_loss 0.295321 lr 0.00076008 rank 2
2023-02-17 06:50:18,671 DEBUG TRAIN Batch 2/2300 loss 49.389130 loss_att 63.006126 loss_ctc 72.138939 loss_rnnt 43.419060 hw_loss 0.400057 lr 0.00075884 rank 5
2023-02-17 06:51:35,446 DEBUG TRAIN Batch 2/2400 loss 27.415150 loss_att 37.180908 loss_ctc 40.661530 loss_rnnt 23.498409 hw_loss 0.370138 lr 0.00076268 rank 7
2023-02-17 06:51:35,448 DEBUG TRAIN Batch 2/2400 loss 32.386723 loss_att 42.589710 loss_ctc 43.266655 loss_rnnt 28.683395 hw_loss 0.397634 lr 0.00076284 rank 5
2023-02-17 06:51:35,450 DEBUG TRAIN Batch 2/2400 loss 43.652905 loss_att 59.595238 loss_ctc 59.830940 loss_rnnt 38.100555 hw_loss 0.387776 lr 0.00076348 rank 6
2023-02-17 06:51:35,450 DEBUG TRAIN Batch 2/2400 loss 43.795197 loss_att 57.595631 loss_ctc 67.986923 loss_rnnt 37.652428 hw_loss 0.294590 lr 0.00076228 rank 4
2023-02-17 06:51:35,453 DEBUG TRAIN Batch 2/2400 loss 28.990532 loss_att 42.523834 loss_ctc 41.794300 loss_rnnt 24.381489 hw_loss 0.366020 lr 0.00076464 rank 1
2023-02-17 06:51:35,453 DEBUG TRAIN Batch 2/2400 loss 39.408951 loss_att 56.896549 loss_ctc 61.890518 loss_rnnt 32.742443 hw_loss 0.321466 lr 0.00076372 rank 0
2023-02-17 06:51:35,454 DEBUG TRAIN Batch 2/2400 loss 64.848190 loss_att 77.798141 loss_ctc 79.805420 loss_rnnt 60.054920 hw_loss 0.391848 lr 0.00076408 rank 2
2023-02-17 06:51:35,496 DEBUG TRAIN Batch 2/2400 loss 44.184277 loss_att 54.868156 loss_ctc 57.460175 loss_rnnt 40.088009 hw_loss 0.355068 lr 0.00076300 rank 3
2023-02-17 06:52:56,279 DEBUG TRAIN Batch 2/2500 loss 24.349812 loss_att 27.670898 loss_ctc 36.755913 loss_rnnt 21.834465 hw_loss 0.369343 lr 0.00076684 rank 5
2023-02-17 06:52:56,279 DEBUG TRAIN Batch 2/2500 loss 33.277122 loss_att 39.758823 loss_ctc 46.559948 loss_rnnt 29.978012 hw_loss 0.434484 lr 0.00076628 rank 4
2023-02-17 06:52:56,280 DEBUG TRAIN Batch 2/2500 loss 24.991169 loss_att 29.432220 loss_ctc 33.437294 loss_rnnt 22.760757 hw_loss 0.405098 lr 0.00076864 rank 1
2023-02-17 06:52:56,282 DEBUG TRAIN Batch 2/2500 loss 21.712915 loss_att 26.767015 loss_ctc 32.177086 loss_rnnt 19.120819 hw_loss 0.348849 lr 0.00076668 rank 7
2023-02-17 06:52:56,284 DEBUG TRAIN Batch 2/2500 loss 21.861809 loss_att 24.929804 loss_ctc 32.590958 loss_rnnt 19.639486 hw_loss 0.334067 lr 0.00076748 rank 6
2023-02-17 06:52:56,288 DEBUG TRAIN Batch 2/2500 loss 27.124517 loss_att 34.539192 loss_ctc 36.145809 loss_rnnt 24.175089 hw_loss 0.494351 lr 0.00076808 rank 2
2023-02-17 06:52:56,289 DEBUG TRAIN Batch 2/2500 loss 23.551153 loss_att 31.105900 loss_ctc 33.079449 loss_rnnt 20.591673 hw_loss 0.333924 lr 0.00076700 rank 3
2023-02-17 06:52:56,333 DEBUG TRAIN Batch 2/2500 loss 17.610657 loss_att 21.804348 loss_ctc 23.829990 loss_rnnt 15.723387 hw_loss 0.411161 lr 0.00076772 rank 0
2023-02-17 06:54:12,507 DEBUG TRAIN Batch 2/2600 loss 65.389442 loss_att 92.584007 loss_ctc 102.091522 loss_rnnt 54.865654 hw_loss 0.358611 lr 0.00077068 rank 7
2023-02-17 06:54:12,508 DEBUG TRAIN Batch 2/2600 loss 52.404079 loss_att 64.539078 loss_ctc 73.460068 loss_rnnt 46.978020 hw_loss 0.359239 lr 0.00077148 rank 6
2023-02-17 06:54:12,510 DEBUG TRAIN Batch 2/2600 loss 36.484051 loss_att 52.031979 loss_ctc 43.074963 loss_rnnt 32.316093 hw_loss 0.336717 lr 0.00077264 rank 1
2023-02-17 06:54:12,512 DEBUG TRAIN Batch 2/2600 loss 49.358421 loss_att 64.577065 loss_ctc 61.381409 loss_rnnt 44.530731 hw_loss 0.339182 lr 0.00077028 rank 4
2023-02-17 06:54:12,513 DEBUG TRAIN Batch 2/2600 loss 60.005692 loss_att 80.543350 loss_ctc 82.939346 loss_rnnt 52.641685 hw_loss 0.372483 lr 0.00077084 rank 5
2023-02-17 06:54:12,516 DEBUG TRAIN Batch 2/2600 loss 56.750797 loss_att 69.880836 loss_ctc 75.338470 loss_rnnt 51.468620 hw_loss 0.333390 lr 0.00077208 rank 2
2023-02-17 06:54:12,520 DEBUG TRAIN Batch 2/2600 loss 32.853939 loss_att 51.247997 loss_ctc 43.828491 loss_rnnt 27.503057 hw_loss 0.391494 lr 0.00077172 rank 0
2023-02-17 06:54:12,933 DEBUG TRAIN Batch 2/2600 loss 63.889317 loss_att 92.488335 loss_ctc 94.617172 loss_rnnt 53.900055 hw_loss 0.323270 lr 0.00077100 rank 3
2023-02-17 06:55:28,046 DEBUG TRAIN Batch 2/2700 loss 34.990383 loss_att 48.913288 loss_ctc 51.998947 loss_rnnt 29.742849 hw_loss 0.365900 lr 0.00077548 rank 6
2023-02-17 06:55:28,048 DEBUG TRAIN Batch 2/2700 loss 43.556625 loss_att 65.801987 loss_ctc 75.220673 loss_rnnt 34.655819 hw_loss 0.430986 lr 0.00077500 rank 3
2023-02-17 06:55:28,053 DEBUG TRAIN Batch 2/2700 loss 43.517944 loss_att 55.138130 loss_ctc 62.115288 loss_rnnt 38.486584 hw_loss 0.426904 lr 0.00077608 rank 2
2023-02-17 06:55:28,053 DEBUG TRAIN Batch 2/2700 loss 33.844196 loss_att 53.644165 loss_ctc 51.633568 loss_rnnt 27.317261 hw_loss 0.365664 lr 0.00077484 rank 5
2023-02-17 06:55:28,053 DEBUG TRAIN Batch 2/2700 loss 35.488640 loss_att 44.077698 loss_ctc 61.578979 loss_rnnt 30.062391 hw_loss 0.430732 lr 0.00077468 rank 7
2023-02-17 06:55:28,054 DEBUG TRAIN Batch 2/2700 loss 27.315020 loss_att 40.112309 loss_ctc 41.908756 loss_rnnt 22.567909 hw_loss 0.453411 lr 0.00077664 rank 1
2023-02-17 06:55:28,056 DEBUG TRAIN Batch 2/2700 loss 43.212322 loss_att 58.058914 loss_ctc 58.420258 loss_rnnt 38.000069 hw_loss 0.403513 lr 0.00077428 rank 4
2023-02-17 06:55:28,101 DEBUG TRAIN Batch 2/2700 loss 41.031265 loss_att 54.416153 loss_ctc 68.410545 loss_rnnt 34.549065 hw_loss 0.289973 lr 0.00077572 rank 0
2023-02-17 06:56:46,994 DEBUG TRAIN Batch 2/2800 loss 40.464993 loss_att 55.864937 loss_ctc 61.968750 loss_rnnt 34.300156 hw_loss 0.408154 lr 0.00077948 rank 6
2023-02-17 06:56:46,999 DEBUG TRAIN Batch 2/2800 loss 38.712517 loss_att 52.174866 loss_ctc 61.385735 loss_rnnt 32.828335 hw_loss 0.316161 lr 0.00078064 rank 1
2023-02-17 06:56:47,002 DEBUG TRAIN Batch 2/2800 loss 35.427719 loss_att 51.809349 loss_ctc 56.031525 loss_rnnt 29.200481 hw_loss 0.382000 lr 0.00077972 rank 0
2023-02-17 06:56:47,004 DEBUG TRAIN Batch 2/2800 loss 20.682243 loss_att 42.250393 loss_ctc 36.382885 loss_rnnt 14.129809 hw_loss 0.272597 lr 0.00077900 rank 3
2023-02-17 06:56:47,005 DEBUG TRAIN Batch 2/2800 loss 45.494633 loss_att 62.808071 loss_ctc 65.316605 loss_rnnt 39.221176 hw_loss 0.314696 lr 0.00077868 rank 7
2023-02-17 06:56:47,004 DEBUG TRAIN Batch 2/2800 loss 67.501350 loss_att 78.689308 loss_ctc 94.055939 loss_rnnt 61.509499 hw_loss 0.400582 lr 0.00077884 rank 5
2023-02-17 06:56:47,005 DEBUG TRAIN Batch 2/2800 loss 40.371021 loss_att 50.947685 loss_ctc 57.183361 loss_rnnt 35.835487 hw_loss 0.334792 lr 0.00077828 rank 4
2023-02-17 06:56:47,008 DEBUG TRAIN Batch 2/2800 loss 44.533371 loss_att 57.524059 loss_ctc 59.470909 loss_rnnt 39.737381 hw_loss 0.386595 lr 0.00078008 rank 2
2023-02-17 06:58:04,627 DEBUG TRAIN Batch 2/2900 loss 42.505180 loss_att 63.411831 loss_ctc 63.858292 loss_rnnt 35.290043 hw_loss 0.350103 lr 0.00078348 rank 6
2023-02-17 06:58:04,627 DEBUG TRAIN Batch 2/2900 loss 24.447268 loss_att 37.536045 loss_ctc 39.836205 loss_rnnt 19.576149 hw_loss 0.377823 lr 0.00078372 rank 0
2023-02-17 06:58:04,628 DEBUG TRAIN Batch 2/2900 loss 42.951836 loss_att 58.482460 loss_ctc 61.267303 loss_rnnt 37.171017 hw_loss 0.436183 lr 0.00078268 rank 7
2023-02-17 06:58:04,628 DEBUG TRAIN Batch 2/2900 loss 24.680140 loss_att 36.002121 loss_ctc 41.121384 loss_rnnt 20.039516 hw_loss 0.345116 lr 0.00078228 rank 4
2023-02-17 06:58:04,629 DEBUG TRAIN Batch 2/2900 loss 49.626659 loss_att 68.109894 loss_ctc 67.662247 loss_rnnt 43.340748 hw_loss 0.345973 lr 0.00078408 rank 2
2023-02-17 06:58:04,631 DEBUG TRAIN Batch 2/2900 loss 44.445705 loss_att 54.208939 loss_ctc 62.372982 loss_rnnt 39.925606 hw_loss 0.332152 lr 0.00078464 rank 1
2023-02-17 06:58:04,632 DEBUG TRAIN Batch 2/2900 loss 35.020973 loss_att 54.984306 loss_ctc 55.530766 loss_rnnt 28.095337 hw_loss 0.371865 lr 0.00078284 rank 5
2023-02-17 06:58:04,634 DEBUG TRAIN Batch 2/2900 loss 50.459473 loss_att 66.092438 loss_ctc 75.709656 loss_rnnt 43.728157 hw_loss 0.446302 lr 0.00078300 rank 3
2023-02-17 06:59:20,429 DEBUG TRAIN Batch 2/3000 loss 54.851376 loss_att 77.276993 loss_ctc 75.184662 loss_rnnt 47.480301 hw_loss 0.327834 lr 0.00078700 rank 3
2023-02-17 06:59:20,429 DEBUG TRAIN Batch 2/3000 loss 44.659760 loss_att 60.186928 loss_ctc 65.323517 loss_rnnt 38.574905 hw_loss 0.420471 lr 0.00078684 rank 5
2023-02-17 06:59:20,431 DEBUG TRAIN Batch 2/3000 loss 30.549433 loss_att 45.408768 loss_ctc 46.034138 loss_rnnt 25.281269 hw_loss 0.434373 lr 0.00078748 rank 6
2023-02-17 06:59:20,431 DEBUG TRAIN Batch 2/3000 loss 40.136070 loss_att 57.600384 loss_ctc 58.253914 loss_rnnt 33.998924 hw_loss 0.428567 lr 0.00078808 rank 2
2023-02-17 06:59:20,432 DEBUG TRAIN Batch 2/3000 loss 28.864111 loss_att 45.172836 loss_ctc 51.883331 loss_rnnt 22.308743 hw_loss 0.420739 lr 0.00078668 rank 7
2023-02-17 06:59:20,433 DEBUG TRAIN Batch 2/3000 loss 33.418732 loss_att 40.740810 loss_ctc 44.470917 loss_rnnt 30.279678 hw_loss 0.376902 lr 0.00078864 rank 1
2023-02-17 06:59:20,434 DEBUG TRAIN Batch 2/3000 loss 54.903313 loss_att 73.427109 loss_ctc 75.085609 loss_rnnt 48.308495 hw_loss 0.373283 lr 0.00078772 rank 0
2023-02-17 06:59:20,479 DEBUG TRAIN Batch 2/3000 loss 51.516220 loss_att 73.729065 loss_ctc 81.178040 loss_rnnt 42.954033 hw_loss 0.308831 lr 0.00078628 rank 4
2023-02-17 07:00:37,560 DEBUG TRAIN Batch 2/3100 loss 39.125904 loss_att 57.077003 loss_ctc 62.301929 loss_rnnt 32.204388 hw_loss 0.452177 lr 0.00079068 rank 7
2023-02-17 07:00:37,562 DEBUG TRAIN Batch 2/3100 loss 32.117168 loss_att 50.701004 loss_ctc 46.671154 loss_rnnt 26.284803 hw_loss 0.328249 lr 0.00079148 rank 6
2023-02-17 07:00:37,563 DEBUG TRAIN Batch 2/3100 loss 26.621819 loss_att 35.486961 loss_ctc 37.590828 loss_rnnt 23.159992 hw_loss 0.424244 lr 0.00079028 rank 4
2023-02-17 07:00:37,563 DEBUG TRAIN Batch 2/3100 loss 49.767223 loss_att 61.040268 loss_ctc 71.964233 loss_rnnt 44.400612 hw_loss 0.285751 lr 0.00079084 rank 5
2023-02-17 07:00:37,568 DEBUG TRAIN Batch 2/3100 loss 43.475834 loss_att 57.475094 loss_ctc 63.529690 loss_rnnt 37.822346 hw_loss 0.337095 lr 0.00079208 rank 2
2023-02-17 07:00:37,568 DEBUG TRAIN Batch 2/3100 loss 37.466606 loss_att 47.670353 loss_ctc 51.619507 loss_rnnt 33.300522 hw_loss 0.446773 lr 0.00079172 rank 0
2023-02-17 07:00:37,570 DEBUG TRAIN Batch 2/3100 loss 41.898319 loss_att 59.060524 loss_ctc 57.561001 loss_rnnt 36.205391 hw_loss 0.322737 lr 0.00079100 rank 3
2023-02-17 07:00:37,573 DEBUG TRAIN Batch 2/3100 loss 26.948410 loss_att 35.301098 loss_ctc 42.049923 loss_rnnt 23.080021 hw_loss 0.345591 lr 0.00079264 rank 1
2023-02-17 07:01:57,916 DEBUG TRAIN Batch 2/3200 loss 21.303846 loss_att 24.356073 loss_ctc 27.214655 loss_rnnt 19.689459 hw_loss 0.404685 lr 0.00079548 rank 6
2023-02-17 07:01:57,917 DEBUG TRAIN Batch 2/3200 loss 37.150143 loss_att 52.421181 loss_ctc 59.341759 loss_rnnt 30.929428 hw_loss 0.389298 lr 0.00079484 rank 5
2023-02-17 07:01:57,919 DEBUG TRAIN Batch 2/3200 loss 85.744293 loss_att 103.883339 loss_ctc 111.013245 loss_rnnt 78.546112 hw_loss 0.377189 lr 0.00079608 rank 2
2023-02-17 07:01:57,922 DEBUG TRAIN Batch 2/3200 loss 14.775862 loss_att 19.315912 loss_ctc 20.292316 loss_rnnt 12.834853 hw_loss 0.557757 lr 0.00079468 rank 7
2023-02-17 07:01:57,922 DEBUG TRAIN Batch 2/3200 loss 36.988468 loss_att 58.751366 loss_ctc 50.883934 loss_rnnt 30.604383 hw_loss 0.335202 lr 0.00079664 rank 1
2023-02-17 07:01:57,930 DEBUG TRAIN Batch 2/3200 loss 33.349209 loss_att 44.966667 loss_ctc 43.832245 loss_rnnt 29.419781 hw_loss 0.390375 lr 0.00079572 rank 0
2023-02-17 07:01:57,935 DEBUG TRAIN Batch 2/3200 loss 27.512299 loss_att 41.911278 loss_ctc 46.588463 loss_rnnt 21.905132 hw_loss 0.344780 lr 0.00079428 rank 4
2023-02-17 07:01:57,964 DEBUG TRAIN Batch 2/3200 loss 39.677990 loss_att 53.271843 loss_ctc 58.714611 loss_rnnt 34.211552 hw_loss 0.392715 lr 0.00079500 rank 3
2023-02-17 07:03:14,793 DEBUG TRAIN Batch 2/3300 loss 49.327484 loss_att 64.986954 loss_ctc 68.768082 loss_rnnt 43.332684 hw_loss 0.507805 lr 0.00079868 rank 7
2023-02-17 07:03:14,798 DEBUG TRAIN Batch 2/3300 loss 46.368587 loss_att 62.996094 loss_ctc 72.674980 loss_rnnt 39.332348 hw_loss 0.381043 lr 0.00079900 rank 3
2023-02-17 07:03:14,799 DEBUG TRAIN Batch 2/3300 loss 50.166714 loss_att 63.296654 loss_ctc 75.140778 loss_rnnt 44.039394 hw_loss 0.321481 lr 0.00079948 rank 6
2023-02-17 07:03:14,800 DEBUG TRAIN Batch 2/3300 loss 43.813366 loss_att 59.426567 loss_ctc 60.525707 loss_rnnt 38.243065 hw_loss 0.411270 lr 0.00080064 rank 1
2023-02-17 07:03:14,800 DEBUG TRAIN Batch 2/3300 loss 42.647896 loss_att 67.167206 loss_ctc 59.341095 loss_rnnt 35.394592 hw_loss 0.231909 lr 0.00080008 rank 2
2023-02-17 07:03:14,801 DEBUG TRAIN Batch 2/3300 loss 28.178881 loss_att 42.418686 loss_ctc 41.618649 loss_rnnt 23.362555 hw_loss 0.330742 lr 0.00079828 rank 4
2023-02-17 07:03:14,805 DEBUG TRAIN Batch 2/3300 loss 42.337315 loss_att 63.874580 loss_ctc 74.834869 loss_rnnt 33.591656 hw_loss 0.197249 lr 0.00079972 rank 0
2023-02-17 07:03:14,806 DEBUG TRAIN Batch 2/3300 loss 44.701351 loss_att 58.709267 loss_ctc 60.931438 loss_rnnt 39.553879 hw_loss 0.341022 lr 0.00079884 rank 5
2023-02-17 07:04:31,086 DEBUG TRAIN Batch 2/3400 loss 29.318361 loss_att 44.731289 loss_ctc 45.818665 loss_rnnt 23.864845 hw_loss 0.320416 lr 0.00080284 rank 5
2023-02-17 07:04:31,087 DEBUG TRAIN Batch 2/3400 loss 45.871784 loss_att 62.781708 loss_ctc 64.631058 loss_rnnt 39.822296 hw_loss 0.311754 lr 0.00080464 rank 1
2023-02-17 07:04:31,089 DEBUG TRAIN Batch 2/3400 loss 40.635258 loss_att 58.709084 loss_ctc 55.617809 loss_rnnt 34.862419 hw_loss 0.300753 lr 0.00080348 rank 6
2023-02-17 07:04:31,092 DEBUG TRAIN Batch 2/3400 loss 19.759432 loss_att 30.404350 loss_ctc 34.053986 loss_rnnt 15.512697 hw_loss 0.397145 lr 0.00080228 rank 4
2023-02-17 07:04:31,093 DEBUG TRAIN Batch 2/3400 loss 46.537384 loss_att 55.773056 loss_ctc 65.126419 loss_rnnt 42.002998 hw_loss 0.391327 lr 0.00080408 rank 2
2023-02-17 07:04:31,094 DEBUG TRAIN Batch 2/3400 loss 41.748898 loss_att 57.304314 loss_ctc 64.640411 loss_rnnt 35.411541 hw_loss 0.326375 lr 0.00080268 rank 7
2023-02-17 07:04:31,095 DEBUG TRAIN Batch 2/3400 loss 34.532059 loss_att 51.319008 loss_ctc 51.082638 loss_rnnt 28.763512 hw_loss 0.383274 lr 0.00080372 rank 0
2023-02-17 07:04:31,140 DEBUG TRAIN Batch 2/3400 loss 28.606234 loss_att 41.513535 loss_ctc 45.596794 loss_rnnt 23.554222 hw_loss 0.384644 lr 0.00080300 rank 3
2023-02-17 07:05:47,968 DEBUG TRAIN Batch 2/3500 loss 34.586353 loss_att 43.448841 loss_ctc 54.434055 loss_rnnt 29.987270 hw_loss 0.337923 lr 0.00080772 rank 0
2023-02-17 07:05:47,969 DEBUG TRAIN Batch 2/3500 loss 35.474266 loss_att 52.256783 loss_ctc 58.795609 loss_rnnt 28.827868 hw_loss 0.338216 lr 0.00080864 rank 1
2023-02-17 07:05:47,970 DEBUG TRAIN Batch 2/3500 loss 24.759184 loss_att 37.027225 loss_ctc 36.309486 loss_rnnt 20.649105 hw_loss 0.218311 lr 0.00080684 rank 5
2023-02-17 07:05:47,972 DEBUG TRAIN Batch 2/3500 loss 55.746078 loss_att 65.912674 loss_ctc 76.144348 loss_rnnt 50.790501 hw_loss 0.379663 lr 0.00080668 rank 7
2023-02-17 07:05:47,974 DEBUG TRAIN Batch 2/3500 loss 24.226780 loss_att 39.645123 loss_ctc 34.645668 loss_rnnt 19.527639 hw_loss 0.424290 lr 0.00080748 rank 6
2023-02-17 07:05:47,975 DEBUG TRAIN Batch 2/3500 loss 50.487858 loss_att 66.141083 loss_ctc 67.650307 loss_rnnt 44.883560 hw_loss 0.347494 lr 0.00080808 rank 2
2023-02-17 07:05:47,978 DEBUG TRAIN Batch 2/3500 loss 31.501911 loss_att 40.510750 loss_ctc 42.903534 loss_rnnt 27.985300 hw_loss 0.364925 lr 0.00080628 rank 4
2023-02-17 07:05:48,017 DEBUG TRAIN Batch 2/3500 loss 34.525208 loss_att 46.214188 loss_ctc 52.141491 loss_rnnt 29.666145 hw_loss 0.323307 lr 0.00080700 rank 3
2023-02-17 07:07:06,526 DEBUG TRAIN Batch 2/3600 loss 35.942024 loss_att 39.673367 loss_ctc 45.934052 loss_rnnt 33.666286 hw_loss 0.369751 lr 0.00081068 rank 7
2023-02-17 07:07:06,527 DEBUG TRAIN Batch 2/3600 loss 54.291237 loss_att 68.226967 loss_ctc 72.362305 loss_rnnt 48.929489 hw_loss 0.309617 lr 0.00081084 rank 5
2023-02-17 07:07:06,527 DEBUG TRAIN Batch 2/3600 loss 50.422997 loss_att 66.499344 loss_ctc 66.784714 loss_rnnt 44.831207 hw_loss 0.365541 lr 0.00081148 rank 6
2023-02-17 07:07:06,532 DEBUG TRAIN Batch 2/3600 loss 38.253380 loss_att 44.714684 loss_ctc 54.907818 loss_rnnt 34.560234 hw_loss 0.338046 lr 0.00081100 rank 3
2023-02-17 07:07:06,533 DEBUG TRAIN Batch 2/3600 loss 32.337757 loss_att 41.336376 loss_ctc 45.757042 loss_rnnt 28.544701 hw_loss 0.382682 lr 0.00081264 rank 1
2023-02-17 07:07:06,536 DEBUG TRAIN Batch 2/3600 loss 33.623478 loss_att 44.799137 loss_ctc 46.175358 loss_rnnt 29.523123 hw_loss 0.359325 lr 0.00081172 rank 0
2023-02-17 07:07:06,536 DEBUG TRAIN Batch 2/3600 loss 33.650845 loss_att 44.040955 loss_ctc 47.024628 loss_rnnt 29.621912 hw_loss 0.314510 lr 0.00081208 rank 2
2023-02-17 07:07:06,577 DEBUG TRAIN Batch 2/3600 loss 49.073994 loss_att 58.949265 loss_ctc 66.892105 loss_rnnt 44.514114 hw_loss 0.392028 lr 0.00081028 rank 4
2023-02-17 07:08:22,641 DEBUG TRAIN Batch 2/3700 loss 39.037319 loss_att 51.578934 loss_ctc 61.160748 loss_rnnt 33.370453 hw_loss 0.391408 lr 0.00081548 rank 6
2023-02-17 07:08:22,643 DEBUG TRAIN Batch 2/3700 loss 37.448902 loss_att 50.823380 loss_ctc 62.343212 loss_rnnt 31.297182 hw_loss 0.295463 lr 0.00081664 rank 1
2023-02-17 07:08:22,643 DEBUG TRAIN Batch 2/3700 loss 26.423620 loss_att 32.333168 loss_ctc 36.129799 loss_rnnt 23.743063 hw_loss 0.383422 lr 0.00081484 rank 5
2023-02-17 07:08:22,649 DEBUG TRAIN Batch 2/3700 loss 46.085487 loss_att 51.485123 loss_ctc 58.914234 loss_rnnt 43.087177 hw_loss 0.389785 lr 0.00081608 rank 2
2023-02-17 07:08:22,652 DEBUG TRAIN Batch 2/3700 loss 28.810200 loss_att 36.391644 loss_ctc 37.403450 loss_rnnt 25.958736 hw_loss 0.355140 lr 0.00081428 rank 4
2023-02-17 07:08:22,651 DEBUG TRAIN Batch 2/3700 loss 32.748661 loss_att 46.927216 loss_ctc 46.045910 loss_rnnt 27.956306 hw_loss 0.344394 lr 0.00081468 rank 7
2023-02-17 07:08:22,654 DEBUG TRAIN Batch 2/3700 loss 29.213776 loss_att 36.194206 loss_ctc 46.389927 loss_rnnt 25.355946 hw_loss 0.321731 lr 0.00081572 rank 0
2023-02-17 07:08:22,697 DEBUG TRAIN Batch 2/3700 loss 49.263035 loss_att 57.416412 loss_ctc 72.254250 loss_rnnt 44.352753 hw_loss 0.401463 lr 0.00081500 rank 3
2023-02-17 07:09:38,756 DEBUG TRAIN Batch 2/3800 loss 20.358858 loss_att 21.338387 loss_ctc 25.657011 loss_rnnt 19.204172 hw_loss 0.473173 lr 0.00081884 rank 5
2023-02-17 07:09:38,758 DEBUG TRAIN Batch 2/3800 loss 31.805065 loss_att 32.750935 loss_ctc 43.431908 loss_rnnt 29.838923 hw_loss 0.425102 lr 0.00081948 rank 6
2023-02-17 07:09:38,763 DEBUG TRAIN Batch 2/3800 loss 33.991299 loss_att 50.826061 loss_ctc 57.261929 loss_rnnt 27.324867 hw_loss 0.368860 lr 0.00081972 rank 0
2023-02-17 07:09:38,763 DEBUG TRAIN Batch 2/3800 loss 22.635365 loss_att 24.922176 loss_ctc 27.414188 loss_rnnt 21.341267 hw_loss 0.374171 lr 0.00081868 rank 7
2023-02-17 07:09:38,764 DEBUG TRAIN Batch 2/3800 loss 32.889114 loss_att 38.778671 loss_ctc 44.396305 loss_rnnt 29.974655 hw_loss 0.379231 lr 0.00082064 rank 1
2023-02-17 07:09:38,765 DEBUG TRAIN Batch 2/3800 loss 17.128237 loss_att 18.505726 loss_ctc 22.949097 loss_rnnt 15.867643 hw_loss 0.391843 lr 0.00082008 rank 2
2023-02-17 07:09:38,766 DEBUG TRAIN Batch 2/3800 loss 20.345467 loss_att 23.402842 loss_ctc 27.068705 loss_rnnt 18.613205 hw_loss 0.420667 lr 0.00081900 rank 3
2023-02-17 07:09:38,807 DEBUG TRAIN Batch 2/3800 loss 50.583496 loss_att 60.401562 loss_ctc 83.438751 loss_rnnt 44.025169 hw_loss 0.401273 lr 0.00081828 rank 4
2023-02-17 07:10:57,989 DEBUG TRAIN Batch 2/3900 loss 43.687389 loss_att 62.124039 loss_ctc 59.945553 loss_rnnt 37.621147 hw_loss 0.395921 lr 0.00082284 rank 5
2023-02-17 07:10:57,990 DEBUG TRAIN Batch 2/3900 loss 37.690456 loss_att 44.827797 loss_ctc 52.652351 loss_rnnt 34.048088 hw_loss 0.412461 lr 0.00082268 rank 7
2023-02-17 07:10:57,992 DEBUG TRAIN Batch 2/3900 loss 35.150398 loss_att 48.694538 loss_ctc 58.221508 loss_rnnt 29.146362 hw_loss 0.410738 lr 0.00082300 rank 3
2023-02-17 07:10:57,993 DEBUG TRAIN Batch 2/3900 loss 42.447517 loss_att 64.925430 loss_ctc 61.130089 loss_rnnt 35.302872 hw_loss 0.296347 lr 0.00082348 rank 6
2023-02-17 07:10:57,996 DEBUG TRAIN Batch 2/3900 loss 26.416870 loss_att 38.287788 loss_ctc 40.247421 loss_rnnt 21.991028 hw_loss 0.389223 lr 0.00082372 rank 0
2023-02-17 07:10:57,998 DEBUG TRAIN Batch 2/3900 loss 50.028046 loss_att 67.151535 loss_ctc 68.485275 loss_rnnt 43.950897 hw_loss 0.359039 lr 0.00082464 rank 1
2023-02-17 07:10:57,998 DEBUG TRAIN Batch 2/3900 loss 31.944046 loss_att 39.566326 loss_ctc 49.828697 loss_rnnt 27.817190 hw_loss 0.408333 lr 0.00082408 rank 2
2023-02-17 07:10:58,023 DEBUG TRAIN Batch 2/3900 loss 49.642399 loss_att 66.548622 loss_ctc 74.783119 loss_rnnt 42.652477 hw_loss 0.481077 lr 0.00082228 rank 4
2023-02-17 07:12:14,668 DEBUG TRAIN Batch 2/4000 loss 17.865084 loss_att 28.818764 loss_ctc 30.820791 loss_rnnt 13.785395 hw_loss 0.302860 lr 0.00082700 rank 3
2023-02-17 07:12:14,669 DEBUG TRAIN Batch 2/4000 loss 41.046780 loss_att 58.315849 loss_ctc 60.877090 loss_rnnt 34.763119 hw_loss 0.348386 lr 0.00082668 rank 7
2023-02-17 07:12:14,670 DEBUG TRAIN Batch 2/4000 loss 51.537334 loss_att 64.977089 loss_ctc 61.347198 loss_rnnt 47.330475 hw_loss 0.395488 lr 0.00082808 rank 2
2023-02-17 07:12:14,673 DEBUG TRAIN Batch 2/4000 loss 31.818296 loss_att 41.747341 loss_ctc 46.027542 loss_rnnt 27.747398 hw_loss 0.357226 lr 0.00082628 rank 4
2023-02-17 07:12:14,674 DEBUG TRAIN Batch 2/4000 loss 35.340176 loss_att 45.362274 loss_ctc 48.109375 loss_rnnt 31.470846 hw_loss 0.304402 lr 0.00082684 rank 5
2023-02-17 07:12:14,675 DEBUG TRAIN Batch 2/4000 loss 37.071201 loss_att 46.165245 loss_ctc 53.777554 loss_rnnt 32.858875 hw_loss 0.311251 lr 0.00082748 rank 6
2023-02-17 07:12:14,675 DEBUG TRAIN Batch 2/4000 loss 35.905148 loss_att 43.495647 loss_ctc 50.496593 loss_rnnt 32.225594 hw_loss 0.404859 lr 0.00082864 rank 1
2023-02-17 07:12:14,677 DEBUG TRAIN Batch 2/4000 loss 36.010864 loss_att 49.744617 loss_ctc 53.649467 loss_rnnt 30.763601 hw_loss 0.278816 lr 0.00082772 rank 0
2023-02-17 07:13:31,166 DEBUG TRAIN Batch 2/4100 loss 64.093002 loss_att 76.175026 loss_ctc 86.608963 loss_rnnt 58.464119 hw_loss 0.394395 lr 0.00083100 rank 3
2023-02-17 07:13:31,168 DEBUG TRAIN Batch 2/4100 loss 43.407909 loss_att 51.678848 loss_ctc 52.817806 loss_rnnt 40.308117 hw_loss 0.358034 lr 0.00083148 rank 6
2023-02-17 07:13:31,169 DEBUG TRAIN Batch 2/4100 loss 42.877800 loss_att 56.516960 loss_ctc 59.583511 loss_rnnt 37.735558 hw_loss 0.350592 lr 0.00083028 rank 4
2023-02-17 07:13:31,169 DEBUG TRAIN Batch 2/4100 loss 17.724178 loss_att 28.816002 loss_ctc 29.827278 loss_rnnt 13.725550 hw_loss 0.312218 lr 0.00083084 rank 5
2023-02-17 07:13:31,171 DEBUG TRAIN Batch 2/4100 loss 35.235558 loss_att 44.113075 loss_ctc 47.235466 loss_rnnt 31.656435 hw_loss 0.381802 lr 0.00083068 rank 7
2023-02-17 07:13:31,173 DEBUG TRAIN Batch 2/4100 loss 27.705435 loss_att 39.572556 loss_ctc 50.649727 loss_rnnt 22.092272 hw_loss 0.338433 lr 0.00083264 rank 1
2023-02-17 07:13:31,175 DEBUG TRAIN Batch 2/4100 loss 39.522560 loss_att 50.503242 loss_ctc 64.594460 loss_rnnt 33.820618 hw_loss 0.305410 lr 0.00083208 rank 2
2023-02-17 07:13:31,179 DEBUG TRAIN Batch 2/4100 loss 44.713654 loss_att 59.654655 loss_ctc 55.845585 loss_rnnt 40.056274 hw_loss 0.346733 lr 0.00083172 rank 0
2023-02-17 07:14:47,489 DEBUG TRAIN Batch 2/4200 loss 45.440830 loss_att 55.321930 loss_ctc 61.724854 loss_rnnt 41.106094 hw_loss 0.351212 lr 0.00083468 rank 7
2023-02-17 07:14:47,491 DEBUG TRAIN Batch 2/4200 loss 41.409706 loss_att 50.217567 loss_ctc 55.130775 loss_rnnt 37.612568 hw_loss 0.386417 lr 0.00083548 rank 6
2023-02-17 07:14:47,493 DEBUG TRAIN Batch 2/4200 loss 26.529860 loss_att 30.684092 loss_ctc 32.757431 loss_rnnt 24.631001 hw_loss 0.445630 lr 0.00083500 rank 3
2023-02-17 07:14:47,495 DEBUG TRAIN Batch 2/4200 loss 32.486134 loss_att 42.799225 loss_ctc 48.475342 loss_rnnt 28.112326 hw_loss 0.336180 lr 0.00083664 rank 1
2023-02-17 07:14:47,496 DEBUG TRAIN Batch 2/4200 loss 39.498894 loss_att 43.321690 loss_ctc 58.328732 loss_rnnt 35.980461 hw_loss 0.456060 lr 0.00083428 rank 4
2023-02-17 07:14:47,496 DEBUG TRAIN Batch 2/4200 loss 46.708004 loss_att 58.120323 loss_ctc 61.626850 loss_rnnt 42.225025 hw_loss 0.396255 lr 0.00083484 rank 5
2023-02-17 07:14:47,498 DEBUG TRAIN Batch 2/4200 loss 26.280464 loss_att 35.061176 loss_ctc 36.177124 loss_rnnt 22.957331 hw_loss 0.463942 lr 0.00083572 rank 0
2023-02-17 07:14:47,514 DEBUG TRAIN Batch 2/4200 loss 30.792789 loss_att 43.252357 loss_ctc 46.820972 loss_rnnt 25.913353 hw_loss 0.469561 lr 0.00083608 rank 2
2023-02-17 07:16:07,372 DEBUG TRAIN Batch 2/4300 loss 19.277180 loss_att 23.963036 loss_ctc 28.087933 loss_rnnt 17.003418 hw_loss 0.303422 lr 0.00083948 rank 6
2023-02-17 07:16:07,376 DEBUG TRAIN Batch 2/4300 loss 32.377327 loss_att 37.768147 loss_ctc 45.685017 loss_rnnt 29.337957 hw_loss 0.350342 lr 0.00083868 rank 7
2023-02-17 07:16:07,376 DEBUG TRAIN Batch 2/4300 loss 18.241922 loss_att 25.509068 loss_ctc 28.991184 loss_rnnt 15.182002 hw_loss 0.324855 lr 0.00083828 rank 4
2023-02-17 07:16:07,379 DEBUG TRAIN Batch 2/4300 loss 34.827171 loss_att 45.034321 loss_ctc 49.543095 loss_rnnt 30.660217 hw_loss 0.306382 lr 0.00083884 rank 5
2023-02-17 07:16:07,380 DEBUG TRAIN Batch 2/4300 loss 43.085754 loss_att 57.773849 loss_ctc 68.503723 loss_rnnt 36.631386 hw_loss 0.239410 lr 0.00084064 rank 1
2023-02-17 07:16:07,383 DEBUG TRAIN Batch 2/4300 loss 35.364033 loss_att 43.805546 loss_ctc 50.377613 loss_rnnt 31.511780 hw_loss 0.304014 lr 0.00084008 rank 2
2023-02-17 07:16:07,384 DEBUG TRAIN Batch 2/4300 loss 32.835594 loss_att 39.621086 loss_ctc 53.241444 loss_rnnt 28.596003 hw_loss 0.303210 lr 0.00083900 rank 3
2023-02-17 07:16:07,387 DEBUG TRAIN Batch 2/4300 loss 33.144699 loss_att 47.130447 loss_ctc 54.164284 loss_rnnt 27.371235 hw_loss 0.325689 lr 0.00083972 rank 0
2023-02-17 07:17:24,489 DEBUG TRAIN Batch 2/4400 loss 34.307404 loss_att 38.044632 loss_ctc 45.117550 loss_rnnt 31.875669 hw_loss 0.455508 lr 0.00084348 rank 6
2023-02-17 07:17:24,492 DEBUG TRAIN Batch 2/4400 loss 32.706184 loss_att 34.135807 loss_ctc 42.070190 loss_rnnt 30.962662 hw_loss 0.391992 lr 0.00084228 rank 4
2023-02-17 07:17:24,495 DEBUG TRAIN Batch 2/4400 loss 17.807152 loss_att 21.985506 loss_ctc 26.672197 loss_rnnt 15.543596 hw_loss 0.461021 lr 0.00084284 rank 5
2023-02-17 07:17:24,496 DEBUG TRAIN Batch 2/4400 loss 27.629911 loss_att 32.741180 loss_ctc 39.946457 loss_rnnt 24.787228 hw_loss 0.334171 lr 0.00084464 rank 1
2023-02-17 07:17:24,497 DEBUG TRAIN Batch 2/4400 loss 21.095533 loss_att 35.859852 loss_ctc 28.457592 loss_rnnt 17.002457 hw_loss 0.297384 lr 0.00084372 rank 0
2023-02-17 07:17:24,497 DEBUG TRAIN Batch 2/4400 loss 36.891350 loss_att 43.379284 loss_ctc 49.201149 loss_rnnt 33.743050 hw_loss 0.392634 lr 0.00084268 rank 7
2023-02-17 07:17:24,500 DEBUG TRAIN Batch 2/4400 loss 32.013336 loss_att 36.719383 loss_ctc 45.542557 loss_rnnt 29.066656 hw_loss 0.377955 lr 0.00084408 rank 2
2023-02-17 07:17:24,542 DEBUG TRAIN Batch 2/4400 loss 20.199146 loss_att 23.135620 loss_ctc 29.679886 loss_rnnt 18.117645 hw_loss 0.431450 lr 0.00084300 rank 3
2023-02-17 07:18:39,828 DEBUG TRAIN Batch 2/4500 loss 34.845673 loss_att 47.813114 loss_ctc 52.516232 loss_rnnt 29.692944 hw_loss 0.380931 lr 0.00084700 rank 3
2023-02-17 07:18:39,830 DEBUG TRAIN Batch 2/4500 loss 42.624817 loss_att 57.627266 loss_ctc 77.058159 loss_rnnt 34.806068 hw_loss 0.425895 lr 0.00084748 rank 6
2023-02-17 07:18:39,832 DEBUG TRAIN Batch 2/4500 loss 62.995487 loss_att 72.376053 loss_ctc 90.119522 loss_rnnt 57.344696 hw_loss 0.296517 lr 0.00084628 rank 4
2023-02-17 07:18:39,832 DEBUG TRAIN Batch 2/4500 loss 26.107155 loss_att 43.026146 loss_ctc 43.737320 loss_rnnt 20.182003 hw_loss 0.357494 lr 0.00084668 rank 7
2023-02-17 07:18:39,834 DEBUG TRAIN Batch 2/4500 loss 30.951683 loss_att 47.025307 loss_ctc 44.508232 loss_rnnt 25.726318 hw_loss 0.380814 lr 0.00084864 rank 1
2023-02-17 07:18:39,835 DEBUG TRAIN Batch 2/4500 loss 32.001133 loss_att 40.014378 loss_ctc 46.657135 loss_rnnt 28.273293 hw_loss 0.320730 lr 0.00084684 rank 5
2023-02-17 07:18:39,836 DEBUG TRAIN Batch 2/4500 loss 38.996563 loss_att 50.942173 loss_ctc 55.342758 loss_rnnt 34.267429 hw_loss 0.300970 lr 0.00084808 rank 2
2023-02-17 07:18:39,841 DEBUG TRAIN Batch 2/4500 loss 37.808441 loss_att 44.713432 loss_ctc 54.109020 loss_rnnt 34.077339 hw_loss 0.331295 lr 0.00084772 rank 0
2023-02-17 07:19:58,473 DEBUG TRAIN Batch 2/4600 loss 43.472916 loss_att 54.639633 loss_ctc 64.917656 loss_rnnt 38.195511 hw_loss 0.346429 lr 0.00085264 rank 1
2023-02-17 07:19:58,477 DEBUG TRAIN Batch 2/4600 loss 58.930161 loss_att 71.420135 loss_ctc 80.660217 loss_rnnt 53.290184 hw_loss 0.458712 lr 0.00085068 rank 7
2023-02-17 07:19:58,477 DEBUG TRAIN Batch 2/4600 loss 31.314091 loss_att 48.981285 loss_ctc 41.877167 loss_rnnt 26.174290 hw_loss 0.371156 lr 0.00085084 rank 5
2023-02-17 07:19:58,477 DEBUG TRAIN Batch 2/4600 loss 30.160494 loss_att 44.750156 loss_ctc 43.003216 loss_rnnt 25.300177 hw_loss 0.431289 lr 0.00085148 rank 6
2023-02-17 07:19:58,478 DEBUG TRAIN Batch 2/4600 loss 40.996368 loss_att 51.804779 loss_ctc 54.035946 loss_rnnt 36.903374 hw_loss 0.361315 lr 0.00085028 rank 4
2023-02-17 07:19:58,479 DEBUG TRAIN Batch 2/4600 loss 54.665936 loss_att 71.901489 loss_ctc 75.282730 loss_rnnt 48.303780 hw_loss 0.311500 lr 0.00085100 rank 3
2023-02-17 07:19:58,486 DEBUG TRAIN Batch 2/4600 loss 50.034138 loss_att 55.982735 loss_ctc 50.959911 loss_rnnt 48.508865 hw_loss 0.397720 lr 0.00085172 rank 0
2023-02-17 07:19:58,526 DEBUG TRAIN Batch 2/4600 loss 40.964996 loss_att 49.630226 loss_ctc 51.468994 loss_rnnt 37.661232 hw_loss 0.319102 lr 0.00085208 rank 2
2023-02-17 07:21:15,995 DEBUG TRAIN Batch 2/4700 loss 27.367153 loss_att 35.896099 loss_ctc 42.010841 loss_rnnt 23.538080 hw_loss 0.320233 lr 0.00085484 rank 5
2023-02-17 07:21:15,999 DEBUG TRAIN Batch 2/4700 loss 34.955822 loss_att 43.950981 loss_ctc 47.838970 loss_rnnt 31.207355 hw_loss 0.434401 lr 0.00085548 rank 6
2023-02-17 07:21:16,001 DEBUG TRAIN Batch 2/4700 loss 45.140671 loss_att 55.460182 loss_ctc 66.271294 loss_rnnt 40.108765 hw_loss 0.282350 lr 0.00085664 rank 1
2023-02-17 07:21:16,002 DEBUG TRAIN Batch 2/4700 loss 22.353685 loss_att 33.541985 loss_ctc 36.466690 loss_rnnt 18.057274 hw_loss 0.331908 lr 0.00085428 rank 4
2023-02-17 07:21:16,002 DEBUG TRAIN Batch 2/4700 loss 62.417854 loss_att 73.291527 loss_ctc 93.474518 loss_rnnt 55.904343 hw_loss 0.371036 lr 0.00085468 rank 7
2023-02-17 07:21:16,004 DEBUG TRAIN Batch 2/4700 loss 33.605976 loss_att 45.314503 loss_ctc 48.526043 loss_rnnt 29.084450 hw_loss 0.357155 lr 0.00085500 rank 3
2023-02-17 07:21:16,004 DEBUG TRAIN Batch 2/4700 loss 28.212355 loss_att 42.059647 loss_ctc 41.269913 loss_rnnt 23.513842 hw_loss 0.352587 lr 0.00085608 rank 2
2023-02-17 07:21:16,008 DEBUG TRAIN Batch 2/4700 loss 65.860931 loss_att 80.151718 loss_ctc 85.671967 loss_rnnt 60.155952 hw_loss 0.385026 lr 0.00085572 rank 0
2023-02-17 07:22:31,458 DEBUG TRAIN Batch 2/4800 loss 28.000326 loss_att 36.032616 loss_ctc 35.955318 loss_rnnt 25.185570 hw_loss 0.276814 lr 0.00085900 rank 3
2023-02-17 07:22:31,459 DEBUG TRAIN Batch 2/4800 loss 79.628708 loss_att 89.506439 loss_ctc 112.204262 loss_rnnt 73.165009 hw_loss 0.271400 lr 0.00085948 rank 6
2023-02-17 07:22:31,462 DEBUG TRAIN Batch 2/4800 loss 28.731976 loss_att 38.003204 loss_ctc 48.875801 loss_rnnt 24.001587 hw_loss 0.356808 lr 0.00086064 rank 1
2023-02-17 07:22:31,463 DEBUG TRAIN Batch 2/4800 loss 35.586548 loss_att 47.474598 loss_ctc 49.844284 loss_rnnt 31.123444 hw_loss 0.345873 lr 0.00085868 rank 7
2023-02-17 07:22:31,463 DEBUG TRAIN Batch 2/4800 loss 37.618889 loss_att 46.185543 loss_ctc 52.060497 loss_rnnt 33.827782 hw_loss 0.285439 lr 0.00085828 rank 4
2023-02-17 07:22:31,464 DEBUG TRAIN Batch 2/4800 loss 38.451473 loss_att 47.005657 loss_ctc 55.622379 loss_rnnt 34.266796 hw_loss 0.345727 lr 0.00085884 rank 5
2023-02-17 07:22:31,467 DEBUG TRAIN Batch 2/4800 loss 43.926907 loss_att 52.918072 loss_ctc 65.178772 loss_rnnt 39.128738 hw_loss 0.311917 lr 0.00086008 rank 2
2023-02-17 07:22:31,468 DEBUG TRAIN Batch 2/4800 loss 24.672773 loss_att 35.222084 loss_ctc 41.422615 loss_rnnt 20.131250 hw_loss 0.371900 lr 0.00085972 rank 0
2023-02-17 07:23:47,726 DEBUG TRAIN Batch 2/4900 loss 50.250271 loss_att 59.473152 loss_ctc 69.040543 loss_rnnt 45.706257 hw_loss 0.363878 lr 0.00086348 rank 6
2023-02-17 07:23:47,728 DEBUG TRAIN Batch 2/4900 loss 83.402168 loss_att 87.522949 loss_ctc 109.119843 loss_rnnt 78.914200 hw_loss 0.440237 lr 0.00086300 rank 3
2023-02-17 07:23:47,729 DEBUG TRAIN Batch 2/4900 loss 58.621338 loss_att 76.245056 loss_ctc 91.047806 loss_rnnt 50.589264 hw_loss 0.344622 lr 0.00086408 rank 2
2023-02-17 07:23:47,730 DEBUG TRAIN Batch 2/4900 loss 24.458649 loss_att 33.879292 loss_ctc 39.532917 loss_rnnt 20.410912 hw_loss 0.288201 lr 0.00086284 rank 5
2023-02-17 07:23:47,732 DEBUG TRAIN Batch 2/4900 loss 44.560238 loss_att 52.021122 loss_ctc 66.189751 loss_rnnt 40.031845 hw_loss 0.285518 lr 0.00086268 rank 7
2023-02-17 07:23:47,734 DEBUG TRAIN Batch 2/4900 loss 37.198093 loss_att 49.409328 loss_ctc 55.637562 loss_rnnt 32.138897 hw_loss 0.296912 lr 0.00086464 rank 1
2023-02-17 07:23:47,735 DEBUG TRAIN Batch 2/4900 loss 35.606018 loss_att 46.117104 loss_ctc 49.776459 loss_rnnt 31.452164 hw_loss 0.304211 lr 0.00086372 rank 0
2023-02-17 07:23:47,738 DEBUG TRAIN Batch 2/4900 loss 43.834621 loss_att 54.305405 loss_ctc 71.532303 loss_rnnt 37.872013 hw_loss 0.328930 lr 0.00086228 rank 4
2023-02-17 07:25:07,302 DEBUG TRAIN Batch 2/5000 loss 34.820789 loss_att 46.295128 loss_ctc 52.295094 loss_rnnt 29.959202 hw_loss 0.444018 lr 0.00086628 rank 4
2023-02-17 07:25:07,303 DEBUG TRAIN Batch 2/5000 loss 22.548464 loss_att 27.799107 loss_ctc 32.048218 loss_rnnt 19.974714 hw_loss 0.481847 lr 0.00086864 rank 1
2023-02-17 07:25:07,303 DEBUG TRAIN Batch 2/5000 loss 37.163425 loss_att 43.863018 loss_ctc 52.737343 loss_rnnt 33.478931 hw_loss 0.502599 lr 0.00086748 rank 6
2023-02-17 07:25:07,305 DEBUG TRAIN Batch 2/5000 loss 29.786209 loss_att 35.005943 loss_ctc 45.351166 loss_rnnt 26.474281 hw_loss 0.361225 lr 0.00086684 rank 5
2023-02-17 07:25:07,306 DEBUG TRAIN Batch 2/5000 loss 38.941296 loss_att 47.972061 loss_ctc 60.485367 loss_rnnt 34.058617 hw_loss 0.382470 lr 0.00086700 rank 3
2023-02-17 07:25:07,309 DEBUG TRAIN Batch 2/5000 loss 39.579216 loss_att 45.708580 loss_ctc 54.362083 loss_rnnt 36.191483 hw_loss 0.357774 lr 0.00086772 rank 0
2023-02-17 07:25:07,309 DEBUG TRAIN Batch 2/5000 loss 33.338497 loss_att 37.802525 loss_ctc 54.574261 loss_rnnt 29.385036 hw_loss 0.429789 lr 0.00086668 rank 7
2023-02-17 07:25:07,356 DEBUG TRAIN Batch 2/5000 loss 41.237560 loss_att 49.982265 loss_ctc 46.883892 loss_rnnt 38.519482 hw_loss 0.405546 lr 0.00086808 rank 2
2023-02-17 07:26:25,019 DEBUG TRAIN Batch 2/5100 loss 44.602657 loss_att 51.422619 loss_ctc 59.911877 loss_rnnt 40.990803 hw_loss 0.387432 lr 0.00087100 rank 3
2023-02-17 07:26:25,021 DEBUG TRAIN Batch 2/5100 loss 19.393190 loss_att 20.971767 loss_ctc 21.948645 loss_rnnt 18.407980 hw_loss 0.616443 lr 0.00087264 rank 1
2023-02-17 07:26:25,022 DEBUG TRAIN Batch 2/5100 loss 18.568348 loss_att 22.909563 loss_ctc 24.883057 loss_rnnt 16.673395 hw_loss 0.346402 lr 0.00087208 rank 2
2023-02-17 07:26:25,024 DEBUG TRAIN Batch 2/5100 loss 22.026846 loss_att 25.439610 loss_ctc 28.710350 loss_rnnt 20.206129 hw_loss 0.463188 lr 0.00087148 rank 6
2023-02-17 07:26:25,025 DEBUG TRAIN Batch 2/5100 loss 42.547169 loss_att 53.648403 loss_ctc 62.505234 loss_rnnt 37.488220 hw_loss 0.333055 lr 0.00087172 rank 0
2023-02-17 07:26:25,026 DEBUG TRAIN Batch 2/5100 loss 39.927544 loss_att 53.843975 loss_ctc 59.158417 loss_rnnt 34.404461 hw_loss 0.329402 lr 0.00087084 rank 5
2023-02-17 07:26:25,026 DEBUG TRAIN Batch 2/5100 loss 29.761528 loss_att 44.726486 loss_ctc 54.410378 loss_rnnt 23.245892 hw_loss 0.442746 lr 0.00087028 rank 4
2023-02-17 07:26:25,026 DEBUG TRAIN Batch 2/5100 loss 44.071709 loss_att 61.865059 loss_ctc 64.055664 loss_rnnt 37.676361 hw_loss 0.322784 lr 0.00087068 rank 7
2023-02-17 07:27:41,016 DEBUG TRAIN Batch 2/5200 loss 42.290981 loss_att 53.181427 loss_ctc 73.560150 loss_rnnt 35.747223 hw_loss 0.368339 lr 0.00087484 rank 5
2023-02-17 07:27:41,020 DEBUG TRAIN Batch 2/5200 loss 33.735744 loss_att 41.422737 loss_ctc 50.777355 loss_rnnt 29.706900 hw_loss 0.411066 lr 0.00087548 rank 6
2023-02-17 07:27:41,023 DEBUG TRAIN Batch 2/5200 loss 39.872490 loss_att 54.878063 loss_ctc 65.228172 loss_rnnt 33.303570 hw_loss 0.350714 lr 0.00087500 rank 3
2023-02-17 07:27:41,022 DEBUG TRAIN Batch 2/5200 loss 80.275085 loss_att 99.909836 loss_ctc 108.988953 loss_rnnt 72.361221 hw_loss 0.296988 lr 0.00087608 rank 2
2023-02-17 07:27:41,022 DEBUG TRAIN Batch 2/5200 loss 34.026253 loss_att 46.759171 loss_ctc 50.983727 loss_rnnt 29.013830 hw_loss 0.384081 lr 0.00087664 rank 1
2023-02-17 07:27:41,024 DEBUG TRAIN Batch 2/5200 loss 15.595914 loss_att 28.686619 loss_ctc 32.489189 loss_rnnt 10.545034 hw_loss 0.338065 lr 0.00087468 rank 7
2023-02-17 07:27:41,025 DEBUG TRAIN Batch 2/5200 loss 34.018173 loss_att 41.127979 loss_ctc 47.089825 loss_rnnt 30.703817 hw_loss 0.280319 lr 0.00087428 rank 4
2023-02-17 07:27:41,026 DEBUG TRAIN Batch 2/5200 loss 40.069279 loss_att 60.119061 loss_ctc 66.521072 loss_rnnt 32.360416 hw_loss 0.322496 lr 0.00087572 rank 0
2023-02-17 07:28:59,007 DEBUG TRAIN Batch 2/5300 loss 52.756748 loss_att 68.548294 loss_ctc 71.123230 loss_rnnt 46.998665 hw_loss 0.282945 lr 0.00087948 rank 6
2023-02-17 07:28:59,009 DEBUG TRAIN Batch 2/5300 loss 14.166243 loss_att 25.362041 loss_ctc 23.115772 loss_rnnt 10.529243 hw_loss 0.383567 lr 0.00087900 rank 3
2023-02-17 07:28:59,010 DEBUG TRAIN Batch 2/5300 loss 38.706654 loss_att 56.387115 loss_ctc 66.366974 loss_rnnt 31.279896 hw_loss 0.379912 lr 0.00087828 rank 4
2023-02-17 07:28:59,012 DEBUG TRAIN Batch 2/5300 loss 45.726501 loss_att 61.599907 loss_ctc 73.997971 loss_rnnt 38.608158 hw_loss 0.326502 lr 0.00087868 rank 7
2023-02-17 07:28:59,012 DEBUG TRAIN Batch 2/5300 loss 31.852468 loss_att 42.172173 loss_ctc 47.311108 loss_rnnt 27.550337 hw_loss 0.331947 lr 0.00087884 rank 5
2023-02-17 07:28:59,013 DEBUG TRAIN Batch 2/5300 loss 27.283533 loss_att 41.930592 loss_ctc 38.828644 loss_rnnt 22.627556 hw_loss 0.351035 lr 0.00088064 rank 1
2023-02-17 07:28:59,017 DEBUG TRAIN Batch 2/5300 loss 24.120510 loss_att 32.113960 loss_ctc 35.558182 loss_rnnt 20.836285 hw_loss 0.300959 lr 0.00088008 rank 2
2023-02-17 07:28:59,046 DEBUG TRAIN Batch 2/5300 loss 37.502079 loss_att 45.885574 loss_ctc 64.197266 loss_rnnt 32.043682 hw_loss 0.416892 lr 0.00087972 rank 0
2023-02-17 07:30:16,103 DEBUG TRAIN Batch 2/5400 loss 23.840487 loss_att 34.463425 loss_ctc 38.631905 loss_rnnt 19.541077 hw_loss 0.379939 lr 0.00088268 rank 7
2023-02-17 07:30:16,108 DEBUG TRAIN Batch 2/5400 loss 44.460743 loss_att 54.994556 loss_ctc 63.251831 loss_rnnt 39.649124 hw_loss 0.373827 lr 0.00088228 rank 4
2023-02-17 07:30:16,109 DEBUG TRAIN Batch 2/5400 loss 40.944523 loss_att 51.958965 loss_ctc 55.535202 loss_rnnt 36.636791 hw_loss 0.298911 lr 0.00088348 rank 6
2023-02-17 07:30:16,111 DEBUG TRAIN Batch 2/5400 loss 39.290791 loss_att 42.420856 loss_ctc 59.838741 loss_rnnt 35.760612 hw_loss 0.308324 lr 0.00088300 rank 3
2023-02-17 07:30:16,111 DEBUG TRAIN Batch 2/5400 loss 27.896816 loss_att 39.147533 loss_ctc 42.840546 loss_rnnt 23.471680 hw_loss 0.342178 lr 0.00088284 rank 5
2023-02-17 07:30:16,111 DEBUG TRAIN Batch 2/5400 loss 24.640642 loss_att 33.850067 loss_ctc 40.474388 loss_rnnt 20.487663 hw_loss 0.374859 lr 0.00088464 rank 1
2023-02-17 07:30:16,112 DEBUG TRAIN Batch 2/5400 loss 27.332993 loss_att 40.297989 loss_ctc 43.097183 loss_rnnt 22.458328 hw_loss 0.337070 lr 0.00088408 rank 2
2023-02-17 07:30:16,158 DEBUG TRAIN Batch 2/5400 loss 49.320118 loss_att 57.241375 loss_ctc 61.613148 loss_rnnt 45.874138 hw_loss 0.417489 lr 0.00088372 rank 0
2023-02-17 07:31:32,681 DEBUG TRAIN Batch 2/5500 loss 28.733547 loss_att 37.305248 loss_ctc 47.690552 loss_rnnt 24.296457 hw_loss 0.365906 lr 0.00088684 rank 5
2023-02-17 07:31:32,682 DEBUG TRAIN Batch 2/5500 loss 41.334915 loss_att 49.036591 loss_ctc 57.469620 loss_rnnt 37.442051 hw_loss 0.377307 lr 0.00088772 rank 0
2023-02-17 07:31:32,682 DEBUG TRAIN Batch 2/5500 loss 36.132801 loss_att 43.938988 loss_ctc 56.330238 loss_rnnt 31.677740 hw_loss 0.376561 lr 0.00088748 rank 6
2023-02-17 07:31:32,683 DEBUG TRAIN Batch 2/5500 loss 24.979033 loss_att 32.553947 loss_ctc 35.600742 loss_rnnt 21.833586 hw_loss 0.401695 lr 0.00088700 rank 3
2023-02-17 07:31:32,682 DEBUG TRAIN Batch 2/5500 loss 34.199841 loss_att 40.925194 loss_ctc 49.185284 loss_rnnt 30.662857 hw_loss 0.363474 lr 0.00088668 rank 7
2023-02-17 07:31:32,682 DEBUG TRAIN Batch 2/5500 loss 30.413975 loss_att 37.490578 loss_ctc 43.481430 loss_rnnt 27.067410 hw_loss 0.354226 lr 0.00088864 rank 1
2023-02-17 07:31:32,684 DEBUG TRAIN Batch 2/5500 loss 42.121685 loss_att 48.575657 loss_ctc 61.870907 loss_rnnt 37.953175 hw_loss 0.458399 lr 0.00088808 rank 2
2023-02-17 07:31:32,728 DEBUG TRAIN Batch 2/5500 loss 36.410793 loss_att 48.413792 loss_ctc 63.023113 loss_rnnt 30.298090 hw_loss 0.307116 lr 0.00088628 rank 4
2023-02-17 07:32:48,773 DEBUG TRAIN Batch 2/5600 loss 25.091927 loss_att 30.569294 loss_ctc 37.431805 loss_rnnt 22.189751 hw_loss 0.302597 lr 0.00089084 rank 5
2023-02-17 07:32:48,773 DEBUG TRAIN Batch 2/5600 loss 52.387329 loss_att 53.472656 loss_ctc 68.607292 loss_rnnt 49.803829 hw_loss 0.382079 lr 0.00089068 rank 7
2023-02-17 07:32:48,773 DEBUG TRAIN Batch 2/5600 loss 50.343933 loss_att 59.161674 loss_ctc 67.145630 loss_rnnt 46.132332 hw_loss 0.389672 lr 0.00089208 rank 2
2023-02-17 07:32:48,775 DEBUG TRAIN Batch 2/5600 loss 24.140034 loss_att 30.919361 loss_ctc 36.422348 loss_rnnt 20.950001 hw_loss 0.368483 lr 0.00089264 rank 1
2023-02-17 07:32:48,776 DEBUG TRAIN Batch 2/5600 loss 43.225395 loss_att 55.632957 loss_ctc 63.790504 loss_rnnt 37.839046 hw_loss 0.305290 lr 0.00089028 rank 4
2023-02-17 07:32:48,780 DEBUG TRAIN Batch 2/5600 loss 45.517975 loss_att 52.086510 loss_ctc 57.783234 loss_rnnt 42.365658 hw_loss 0.381077 lr 0.00089100 rank 3
2023-02-17 07:32:48,781 DEBUG TRAIN Batch 2/5600 loss 35.381504 loss_att 48.304848 loss_ctc 53.771168 loss_rnnt 30.096588 hw_loss 0.465541 lr 0.00089172 rank 0
2023-02-17 07:32:48,788 DEBUG TRAIN Batch 2/5600 loss 28.660278 loss_att 36.509426 loss_ctc 39.669472 loss_rnnt 25.418179 hw_loss 0.383210 lr 0.00089148 rank 6
2023-02-17 07:34:08,788 DEBUG TRAIN Batch 2/5700 loss 17.816170 loss_att 21.975986 loss_ctc 25.856859 loss_rnnt 15.672119 hw_loss 0.449993 lr 0.00089500 rank 3
2023-02-17 07:34:08,796 DEBUG TRAIN Batch 2/5700 loss 28.814892 loss_att 38.476326 loss_ctc 42.770004 loss_rnnt 24.826252 hw_loss 0.366879 lr 0.00089548 rank 6
2023-02-17 07:34:08,799 DEBUG TRAIN Batch 2/5700 loss 52.891277 loss_att 65.388763 loss_ctc 71.866592 loss_rnnt 47.637505 hw_loss 0.420435 lr 0.00089484 rank 5
2023-02-17 07:34:08,799 DEBUG TRAIN Batch 2/5700 loss 21.687868 loss_att 26.147842 loss_ctc 29.480865 loss_rnnt 19.509125 hw_loss 0.464408 lr 0.00089428 rank 4
2023-02-17 07:34:08,800 DEBUG TRAIN Batch 2/5700 loss 30.774601 loss_att 34.658428 loss_ctc 41.655609 loss_rnnt 28.291645 hw_loss 0.478849 lr 0.00089608 rank 2
2023-02-17 07:34:08,801 DEBUG TRAIN Batch 2/5700 loss 51.878128 loss_att 62.587067 loss_ctc 76.264389 loss_rnnt 46.262955 hw_loss 0.416033 lr 0.00089572 rank 0
2023-02-17 07:34:08,826 DEBUG TRAIN Batch 2/5700 loss 50.172600 loss_att 58.216049 loss_ctc 72.772141 loss_rnnt 45.285454 hw_loss 0.497230 lr 0.00089468 rank 7
2023-02-17 07:34:08,835 DEBUG TRAIN Batch 2/5700 loss 25.738546 loss_att 27.831333 loss_ctc 38.490044 loss_rnnt 23.387653 hw_loss 0.435250 lr 0.00089664 rank 1
2023-02-17 07:35:24,818 DEBUG TRAIN Batch 2/5800 loss 53.932373 loss_att 56.019379 loss_ctc 81.764740 loss_rnnt 49.595016 hw_loss 0.391816 lr 0.00089900 rank 3
2023-02-17 07:35:24,819 DEBUG TRAIN Batch 2/5800 loss 33.396492 loss_att 45.960606 loss_ctc 47.501366 loss_rnnt 28.807493 hw_loss 0.366613 lr 0.00089868 rank 7
2023-02-17 07:35:24,820 DEBUG TRAIN Batch 2/5800 loss 48.465240 loss_att 68.247513 loss_ctc 78.141586 loss_rnnt 40.419701 hw_loss 0.247959 lr 0.00089948 rank 6
2023-02-17 07:35:24,820 DEBUG TRAIN Batch 2/5800 loss 29.802082 loss_att 43.356274 loss_ctc 47.520630 loss_rnnt 24.600121 hw_loss 0.241219 lr 0.00089884 rank 5
2023-02-17 07:35:24,822 DEBUG TRAIN Batch 2/5800 loss 21.766293 loss_att 31.723999 loss_ctc 32.489365 loss_rnnt 18.121506 hw_loss 0.419070 lr 0.00089828 rank 4
2023-02-17 07:35:24,822 DEBUG TRAIN Batch 2/5800 loss 51.680729 loss_att 64.817078 loss_ctc 72.433792 loss_rnnt 46.076962 hw_loss 0.392675 lr 0.00090064 rank 1
2023-02-17 07:35:24,825 DEBUG TRAIN Batch 2/5800 loss 49.944046 loss_att 63.635735 loss_ctc 69.728195 loss_rnnt 44.409271 hw_loss 0.297282 lr 0.00089972 rank 0
2023-02-17 07:35:24,865 DEBUG TRAIN Batch 2/5800 loss 50.926723 loss_att 64.457291 loss_ctc 77.930359 loss_rnnt 44.454662 hw_loss 0.310241 lr 0.00090008 rank 2
2023-02-17 07:36:41,452 DEBUG TRAIN Batch 2/5900 loss 35.158371 loss_att 47.961731 loss_ctc 46.698040 loss_rnnt 30.808310 hw_loss 0.470194 lr 0.00090348 rank 6
2023-02-17 07:36:41,457 DEBUG TRAIN Batch 2/5900 loss 40.474239 loss_att 50.742584 loss_ctc 59.365608 loss_rnnt 35.676422 hw_loss 0.422431 lr 0.00090268 rank 7
2023-02-17 07:36:41,459 DEBUG TRAIN Batch 2/5900 loss 38.956596 loss_att 47.747005 loss_ctc 59.176773 loss_rnnt 34.283409 hw_loss 0.410779 lr 0.00090284 rank 5
2023-02-17 07:36:41,460 DEBUG TRAIN Batch 2/5900 loss 37.432720 loss_att 43.570480 loss_ctc 56.260574 loss_rnnt 33.478477 hw_loss 0.405581 lr 0.00090228 rank 4
2023-02-17 07:36:41,462 DEBUG TRAIN Batch 2/5900 loss 54.499306 loss_att 65.488258 loss_ctc 78.190125 loss_rnnt 48.953247 hw_loss 0.355299 lr 0.00090464 rank 1
2023-02-17 07:36:41,464 DEBUG TRAIN Batch 2/5900 loss 21.147680 loss_att 34.204166 loss_ctc 32.454746 loss_rnnt 16.838276 hw_loss 0.357184 lr 0.00090408 rank 2
2023-02-17 07:36:41,465 DEBUG TRAIN Batch 2/5900 loss 63.138111 loss_att 72.386047 loss_ctc 87.908165 loss_rnnt 57.776009 hw_loss 0.393450 lr 0.00090300 rank 3
2023-02-17 07:36:41,469 DEBUG TRAIN Batch 2/5900 loss 30.381084 loss_att 40.919357 loss_ctc 48.076000 loss_rnnt 25.738335 hw_loss 0.329578 lr 0.00090372 rank 0
2023-02-17 07:37:58,279 DEBUG TRAIN Batch 2/6000 loss 33.355011 loss_att 43.321434 loss_ctc 47.856998 loss_rnnt 29.245508 hw_loss 0.342409 lr 0.00090668 rank 7
2023-02-17 07:37:58,279 DEBUG TRAIN Batch 2/6000 loss 18.188932 loss_att 32.084187 loss_ctc 37.455635 loss_rnnt 12.625532 hw_loss 0.403982 lr 0.00090864 rank 1
2023-02-17 07:37:58,281 DEBUG TRAIN Batch 2/6000 loss 33.800995 loss_att 44.328789 loss_ctc 55.062561 loss_rnnt 28.690258 hw_loss 0.319317 lr 0.00090684 rank 5
2023-02-17 07:37:58,282 DEBUG TRAIN Batch 2/6000 loss 29.577841 loss_att 44.471306 loss_ctc 38.268627 loss_rnnt 25.248310 hw_loss 0.360120 lr 0.00090772 rank 0
2023-02-17 07:37:58,283 DEBUG TRAIN Batch 2/6000 loss 35.699535 loss_att 43.272110 loss_ctc 45.867432 loss_rnnt 32.643738 hw_loss 0.347925 lr 0.00090748 rank 6
2023-02-17 07:37:58,284 DEBUG TRAIN Batch 2/6000 loss 40.439556 loss_att 50.841297 loss_ctc 53.809647 loss_rnnt 36.372784 hw_loss 0.382027 lr 0.00090628 rank 4
2023-02-17 07:37:58,285 DEBUG TRAIN Batch 2/6000 loss 34.469856 loss_att 48.136459 loss_ctc 49.372227 loss_rnnt 29.565836 hw_loss 0.344469 lr 0.00090700 rank 3
2023-02-17 07:37:58,289 DEBUG TRAIN Batch 2/6000 loss 25.715696 loss_att 40.958912 loss_ctc 41.628151 loss_rnnt 20.362953 hw_loss 0.342075 lr 0.00090808 rank 2
2023-02-17 07:39:15,039 DEBUG TRAIN Batch 2/6100 loss 32.435596 loss_att 46.040112 loss_ctc 53.946625 loss_rnnt 26.662539 hw_loss 0.345028 lr 0.00091264 rank 1
2023-02-17 07:39:15,041 DEBUG TRAIN Batch 2/6100 loss 27.404507 loss_att 40.209782 loss_ctc 44.181305 loss_rnnt 22.425840 hw_loss 0.338819 lr 0.00091100 rank 3
2023-02-17 07:39:15,043 DEBUG TRAIN Batch 2/6100 loss 23.403622 loss_att 34.465828 loss_ctc 31.712809 loss_rnnt 19.888895 hw_loss 0.364489 lr 0.00091084 rank 5
2023-02-17 07:39:15,044 DEBUG TRAIN Batch 2/6100 loss 32.141365 loss_att 42.461998 loss_ctc 42.171368 loss_rnnt 28.521471 hw_loss 0.409561 lr 0.00091068 rank 7
2023-02-17 07:39:15,045 DEBUG TRAIN Batch 2/6100 loss 30.977167 loss_att 40.246574 loss_ctc 45.230358 loss_rnnt 27.097183 hw_loss 0.235639 lr 0.00091148 rank 6
2023-02-17 07:39:15,048 DEBUG TRAIN Batch 2/6100 loss 22.979364 loss_att 33.443764 loss_ctc 34.943977 loss_rnnt 19.112486 hw_loss 0.335093 lr 0.00091172 rank 0
2023-02-17 07:39:15,051 DEBUG TRAIN Batch 2/6100 loss 31.984432 loss_att 38.034779 loss_ctc 45.441181 loss_rnnt 28.824631 hw_loss 0.291557 lr 0.00091028 rank 4
2023-02-17 07:39:15,058 DEBUG TRAIN Batch 2/6100 loss 26.351288 loss_att 35.139370 loss_ctc 40.979851 loss_rnnt 22.464996 hw_loss 0.334132 lr 0.00091208 rank 2
2023-02-17 07:40:30,494 DEBUG TRAIN Batch 2/6200 loss 24.865673 loss_att 37.187908 loss_ctc 41.316368 loss_rnnt 19.993004 hw_loss 0.402743 lr 0.00091548 rank 6
2023-02-17 07:40:30,495 DEBUG TRAIN Batch 2/6200 loss 35.415890 loss_att 43.510883 loss_ctc 52.741249 loss_rnnt 31.301857 hw_loss 0.346844 lr 0.00091664 rank 1
2023-02-17 07:40:30,495 DEBUG TRAIN Batch 2/6200 loss 38.691589 loss_att 50.528069 loss_ctc 55.980026 loss_rnnt 33.875843 hw_loss 0.268729 lr 0.00091608 rank 2
2023-02-17 07:40:30,496 DEBUG TRAIN Batch 2/6200 loss 38.720085 loss_att 48.436718 loss_ctc 58.227394 loss_rnnt 33.988785 hw_loss 0.350632 lr 0.00091484 rank 5
2023-02-17 07:40:30,498 DEBUG TRAIN Batch 2/6200 loss 41.348866 loss_att 46.140373 loss_ctc 54.320549 loss_rnnt 38.482094 hw_loss 0.335466 lr 0.00091500 rank 3
2023-02-17 07:40:30,498 DEBUG TRAIN Batch 2/6200 loss 36.693024 loss_att 44.014740 loss_ctc 48.617256 loss_rnnt 33.438953 hw_loss 0.374683 lr 0.00091468 rank 7
2023-02-17 07:40:30,499 DEBUG TRAIN Batch 2/6200 loss 38.576149 loss_att 45.631653 loss_ctc 63.715508 loss_rnnt 33.607658 hw_loss 0.385270 lr 0.00091428 rank 4
2023-02-17 07:40:30,508 DEBUG TRAIN Batch 2/6200 loss 45.311409 loss_att 52.502979 loss_ctc 67.046570 loss_rnnt 40.775661 hw_loss 0.373897 lr 0.00091572 rank 0
2023-02-17 07:41:46,976 DEBUG TRAIN Batch 2/6300 loss 29.889698 loss_att 43.084019 loss_ctc 51.322338 loss_rnnt 24.157797 hw_loss 0.441282 lr 0.00091948 rank 6
2023-02-17 07:41:46,978 DEBUG TRAIN Batch 2/6300 loss 35.507271 loss_att 60.162487 loss_ctc 49.410568 loss_rnnt 28.556717 hw_loss 0.310757 lr 0.00091884 rank 5
2023-02-17 07:41:46,979 DEBUG TRAIN Batch 2/6300 loss 21.347691 loss_att 25.687954 loss_ctc 32.241940 loss_rnnt 18.804554 hw_loss 0.417218 lr 0.00091868 rank 7
2023-02-17 07:41:46,980 DEBUG TRAIN Batch 2/6300 loss 44.260826 loss_att 56.037510 loss_ctc 62.331696 loss_rnnt 39.332581 hw_loss 0.306482 lr 0.00092064 rank 1
2023-02-17 07:41:46,980 DEBUG TRAIN Batch 2/6300 loss 27.046085 loss_att 32.294521 loss_ctc 39.597759 loss_rnnt 24.100718 hw_loss 0.416478 lr 0.00091900 rank 3
2023-02-17 07:41:46,983 DEBUG TRAIN Batch 2/6300 loss 27.722654 loss_att 29.214066 loss_ctc 39.216209 loss_rnnt 25.667698 hw_loss 0.420371 lr 0.00092008 rank 2
2023-02-17 07:41:46,984 DEBUG TRAIN Batch 2/6300 loss 30.307901 loss_att 33.494297 loss_ctc 41.216721 loss_rnnt 27.998455 hw_loss 0.408110 lr 0.00091972 rank 0
2023-02-17 07:41:47,026 DEBUG TRAIN Batch 2/6300 loss 38.135178 loss_att 45.894867 loss_ctc 55.112358 loss_rnnt 34.124897 hw_loss 0.365099 lr 0.00091828 rank 4
2023-02-17 07:43:07,236 DEBUG TRAIN Batch 2/6400 loss 36.986950 loss_att 50.005970 loss_ctc 53.515198 loss_rnnt 31.970938 hw_loss 0.390824 lr 0.00092268 rank 7
2023-02-17 07:43:07,236 DEBUG TRAIN Batch 2/6400 loss 30.621714 loss_att 35.455944 loss_ctc 45.574745 loss_rnnt 27.436684 hw_loss 0.420835 lr 0.00092348 rank 6
2023-02-17 07:43:07,236 DEBUG TRAIN Batch 2/6400 loss 24.675247 loss_att 38.283039 loss_ctc 34.569992 loss_rnnt 20.477652 hw_loss 0.293883 lr 0.00092464 rank 1
2023-02-17 07:43:07,242 DEBUG TRAIN Batch 2/6400 loss 39.447182 loss_att 56.597820 loss_ctc 68.851929 loss_rnnt 31.899481 hw_loss 0.369261 lr 0.00092300 rank 3
2023-02-17 07:43:07,247 DEBUG TRAIN Batch 2/6400 loss 33.206963 loss_att 49.612003 loss_ctc 53.627556 loss_rnnt 27.046499 hw_loss 0.293841 lr 0.00092408 rank 2
2023-02-17 07:43:07,265 DEBUG TRAIN Batch 2/6400 loss 16.355307 loss_att 19.489548 loss_ctc 25.191904 loss_rnnt 14.351769 hw_loss 0.372140 lr 0.00092228 rank 4
2023-02-17 07:43:07,267 DEBUG TRAIN Batch 2/6400 loss 30.405024 loss_att 48.600712 loss_ctc 48.341888 loss_rnnt 24.186443 hw_loss 0.352240 lr 0.00092372 rank 0
2023-02-17 07:43:07,553 DEBUG TRAIN Batch 2/6400 loss 39.593460 loss_att 58.864635 loss_ctc 56.619396 loss_rnnt 33.301380 hw_loss 0.314478 lr 0.00092284 rank 5
2023-02-17 07:44:25,135 DEBUG TRAIN Batch 2/6500 loss 59.830078 loss_att 83.679123 loss_ctc 85.028618 loss_rnnt 51.494804 hw_loss 0.385618 lr 0.00092684 rank 5
2023-02-17 07:44:25,137 DEBUG TRAIN Batch 2/6500 loss 36.703434 loss_att 45.470711 loss_ctc 53.219189 loss_rnnt 32.579247 hw_loss 0.316182 lr 0.00092748 rank 6
2023-02-17 07:44:25,138 DEBUG TRAIN Batch 2/6500 loss 33.627388 loss_att 48.185837 loss_ctc 49.414989 loss_rnnt 28.418049 hw_loss 0.361181 lr 0.00092668 rank 7
2023-02-17 07:44:25,145 DEBUG TRAIN Batch 2/6500 loss 32.853886 loss_att 51.829613 loss_ctc 43.466003 loss_rnnt 27.462978 hw_loss 0.339032 lr 0.00092864 rank 1
2023-02-17 07:44:25,145 DEBUG TRAIN Batch 2/6500 loss 43.913029 loss_att 54.202019 loss_ctc 57.417015 loss_rnnt 39.846252 hw_loss 0.390831 lr 0.00092628 rank 4
2023-02-17 07:44:25,146 DEBUG TRAIN Batch 2/6500 loss 44.426846 loss_att 53.028522 loss_ctc 62.164909 loss_rnnt 40.179222 hw_loss 0.304155 lr 0.00092772 rank 0
2023-02-17 07:44:25,146 DEBUG TRAIN Batch 2/6500 loss 25.568768 loss_att 32.309025 loss_ctc 35.179615 loss_rnnt 22.782482 hw_loss 0.293978 lr 0.00092808 rank 2
2023-02-17 07:44:25,447 DEBUG TRAIN Batch 2/6500 loss 48.571678 loss_att 67.283577 loss_ctc 71.468758 loss_rnnt 41.648750 hw_loss 0.239262 lr 0.00092700 rank 3
2023-02-17 07:45:40,963 DEBUG TRAIN Batch 2/6600 loss 51.987476 loss_att 67.658554 loss_ctc 77.925720 loss_rnnt 45.224575 hw_loss 0.319219 lr 0.00093028 rank 4
2023-02-17 07:45:40,964 DEBUG TRAIN Batch 2/6600 loss 20.181587 loss_att 27.355854 loss_ctc 33.521721 loss_rnnt 16.771927 hw_loss 0.367725 lr 0.00093068 rank 7
2023-02-17 07:45:40,967 DEBUG TRAIN Batch 2/6600 loss 31.892006 loss_att 44.486824 loss_ctc 45.408279 loss_rnnt 27.382841 hw_loss 0.352559 lr 0.00093084 rank 5
2023-02-17 07:45:40,969 DEBUG TRAIN Batch 2/6600 loss 52.578388 loss_att 73.080132 loss_ctc 80.854034 loss_rnnt 44.572247 hw_loss 0.254453 lr 0.00093148 rank 6
2023-02-17 07:45:40,969 DEBUG TRAIN Batch 2/6600 loss 21.598360 loss_att 34.468502 loss_ctc 30.478954 loss_rnnt 17.680084 hw_loss 0.300315 lr 0.00093264 rank 1
2023-02-17 07:45:40,972 DEBUG TRAIN Batch 2/6600 loss 35.520882 loss_att 54.067966 loss_ctc 57.305405 loss_rnnt 28.731758 hw_loss 0.328315 lr 0.00093172 rank 0
2023-02-17 07:45:40,974 DEBUG TRAIN Batch 2/6600 loss 26.545582 loss_att 36.822933 loss_ctc 42.087505 loss_rnnt 22.226610 hw_loss 0.358582 lr 0.00093100 rank 3
2023-02-17 07:45:40,977 DEBUG TRAIN Batch 2/6600 loss 48.268280 loss_att 61.383995 loss_ctc 66.182541 loss_rnnt 43.055206 hw_loss 0.377548 lr 0.00093208 rank 2
2023-02-17 07:46:58,411 DEBUG TRAIN Batch 2/6700 loss 35.928692 loss_att 49.981281 loss_ctc 53.491928 loss_rnnt 30.579126 hw_loss 0.369900 lr 0.00093500 rank 3
2023-02-17 07:46:58,412 DEBUG TRAIN Batch 2/6700 loss 36.125416 loss_att 39.724358 loss_ctc 52.227921 loss_rnnt 33.077549 hw_loss 0.339518 lr 0.00093484 rank 5
2023-02-17 07:46:58,413 DEBUG TRAIN Batch 2/6700 loss 30.205935 loss_att 37.794556 loss_ctc 49.157578 loss_rnnt 26.011833 hw_loss 0.280295 lr 0.00093548 rank 6
2023-02-17 07:46:58,414 DEBUG TRAIN Batch 2/6700 loss 26.722052 loss_att 36.062817 loss_ctc 47.271194 loss_rnnt 21.985865 hw_loss 0.240275 lr 0.00093664 rank 1
2023-02-17 07:46:58,418 DEBUG TRAIN Batch 2/6700 loss 50.996620 loss_att 66.612213 loss_ctc 78.865257 loss_rnnt 43.932915 hw_loss 0.421451 lr 0.00093468 rank 7
2023-02-17 07:46:58,420 DEBUG TRAIN Batch 2/6700 loss 48.615673 loss_att 58.132538 loss_ctc 74.401566 loss_rnnt 43.068836 hw_loss 0.385031 lr 0.00093608 rank 2
2023-02-17 07:46:58,422 DEBUG TRAIN Batch 2/6700 loss 37.760815 loss_att 46.438774 loss_ctc 59.604275 loss_rnnt 32.928455 hw_loss 0.345560 lr 0.00093428 rank 4
2023-02-17 07:46:58,470 DEBUG TRAIN Batch 2/6700 loss 21.724174 loss_att 32.526176 loss_ctc 36.570961 loss_rnnt 17.395771 hw_loss 0.353305 lr 0.00093572 rank 0
2023-02-17 07:48:16,845 DEBUG TRAIN Batch 2/6800 loss 35.511448 loss_att 50.627731 loss_ctc 49.437855 loss_rnnt 30.471348 hw_loss 0.299978 lr 0.00093948 rank 6
2023-02-17 07:48:16,846 DEBUG TRAIN Batch 2/6800 loss 37.593136 loss_att 44.990723 loss_ctc 52.607536 loss_rnnt 33.919392 hw_loss 0.360577 lr 0.00094008 rank 2
2023-02-17 07:48:16,847 DEBUG TRAIN Batch 2/6800 loss 28.535376 loss_att 35.555706 loss_ctc 39.769421 loss_rnnt 25.364681 hw_loss 0.503915 lr 0.00094064 rank 1
2023-02-17 07:48:16,847 DEBUG TRAIN Batch 2/6800 loss 24.073118 loss_att 32.980156 loss_ctc 35.692184 loss_rnnt 20.530365 hw_loss 0.397757 lr 0.00093868 rank 7
2023-02-17 07:48:16,848 DEBUG TRAIN Batch 2/6800 loss 32.206627 loss_att 40.023903 loss_ctc 38.717545 loss_rnnt 29.604891 hw_loss 0.319045 lr 0.00093828 rank 4
2023-02-17 07:48:16,848 DEBUG TRAIN Batch 2/6800 loss 29.580952 loss_att 37.194931 loss_ctc 45.939400 loss_rnnt 25.705122 hw_loss 0.322322 lr 0.00093884 rank 5
2023-02-17 07:48:16,856 DEBUG TRAIN Batch 2/6800 loss 46.242210 loss_att 60.805016 loss_ctc 59.366394 loss_rnnt 41.405746 hw_loss 0.326271 lr 0.00093972 rank 0
2023-02-17 07:48:16,890 DEBUG TRAIN Batch 2/6800 loss 33.072956 loss_att 40.364346 loss_ctc 47.450584 loss_rnnt 29.517727 hw_loss 0.337384 lr 0.00093900 rank 3
2023-02-17 07:49:34,002 DEBUG TRAIN Batch 2/6900 loss 48.786476 loss_att 57.618073 loss_ctc 66.441826 loss_rnnt 44.459881 hw_loss 0.386678 lr 0.00094300 rank 3
2023-02-17 07:49:34,004 DEBUG TRAIN Batch 2/6900 loss 33.519279 loss_att 48.391003 loss_ctc 50.626328 loss_rnnt 28.086874 hw_loss 0.332100 lr 0.00094268 rank 7
2023-02-17 07:49:34,005 DEBUG TRAIN Batch 2/6900 loss 30.841522 loss_att 44.692101 loss_ctc 48.205559 loss_rnnt 25.569302 hw_loss 0.350438 lr 0.00094348 rank 6
2023-02-17 07:49:34,006 DEBUG TRAIN Batch 2/6900 loss 23.519827 loss_att 31.817579 loss_ctc 32.297283 loss_rnnt 20.621265 hw_loss 0.128786 lr 0.00094284 rank 5
2023-02-17 07:49:34,007 DEBUG TRAIN Batch 2/6900 loss 20.821230 loss_att 26.801159 loss_ctc 29.046591 loss_rnnt 18.285788 hw_loss 0.455139 lr 0.00094408 rank 2
2023-02-17 07:49:34,010 DEBUG TRAIN Batch 2/6900 loss 26.681652 loss_att 36.074974 loss_ctc 42.592964 loss_rnnt 22.442366 hw_loss 0.448342 lr 0.00094464 rank 1
2023-02-17 07:49:34,012 DEBUG TRAIN Batch 2/6900 loss 49.249908 loss_att 61.050949 loss_ctc 67.328201 loss_rnnt 44.286751 hw_loss 0.360957 lr 0.00094228 rank 4
2023-02-17 07:49:34,014 DEBUG TRAIN Batch 2/6900 loss 26.049047 loss_att 36.773468 loss_ctc 39.176094 loss_rnnt 21.958681 hw_loss 0.366015 lr 0.00094372 rank 0
2023-02-17 07:50:51,837 DEBUG TRAIN Batch 2/7000 loss 31.711271 loss_att 41.546829 loss_ctc 42.766689 loss_rnnt 28.087143 hw_loss 0.343053 lr 0.00094748 rank 6
2023-02-17 07:50:51,839 DEBUG TRAIN Batch 2/7000 loss 49.067074 loss_att 61.499451 loss_ctc 66.599167 loss_rnnt 44.077984 hw_loss 0.309381 lr 0.00094700 rank 3
2023-02-17 07:50:51,845 DEBUG TRAIN Batch 2/7000 loss 20.006781 loss_att 23.141203 loss_ctc 30.520033 loss_rnnt 17.727713 hw_loss 0.469533 lr 0.00094668 rank 7
2023-02-17 07:50:51,845 DEBUG TRAIN Batch 2/7000 loss 32.330608 loss_att 41.704136 loss_ctc 48.087212 loss_rnnt 28.174810 hw_loss 0.337901 lr 0.00094684 rank 5
2023-02-17 07:50:51,848 DEBUG TRAIN Batch 2/7000 loss 31.926615 loss_att 35.211151 loss_ctc 42.808136 loss_rnnt 29.580196 hw_loss 0.447453 lr 0.00094864 rank 1
2023-02-17 07:50:51,848 DEBUG TRAIN Batch 2/7000 loss 37.506557 loss_att 53.005760 loss_ctc 58.386925 loss_rnnt 31.439430 hw_loss 0.343570 lr 0.00094808 rank 2
2023-02-17 07:50:51,849 DEBUG TRAIN Batch 2/7000 loss 28.584801 loss_att 34.114922 loss_ctc 43.277634 loss_rnnt 25.316616 hw_loss 0.380845 lr 0.00094628 rank 4
2023-02-17 07:50:51,895 DEBUG TRAIN Batch 2/7000 loss 30.940096 loss_att 38.782150 loss_ctc 53.453300 loss_rnnt 26.221527 hw_loss 0.278249 lr 0.00094772 rank 0
2023-02-17 07:52:11,791 DEBUG TRAIN Batch 2/7100 loss 62.989025 loss_att 73.939873 loss_ctc 88.005424 loss_rnnt 57.277489 hw_loss 0.348459 lr 0.00095100 rank 3
2023-02-17 07:52:11,792 DEBUG TRAIN Batch 2/7100 loss 49.450794 loss_att 51.436996 loss_ctc 73.350990 loss_rnnt 45.692455 hw_loss 0.327018 lr 0.00095172 rank 0
2023-02-17 07:52:11,793 DEBUG TRAIN Batch 2/7100 loss 43.982021 loss_att 56.653347 loss_ctc 64.861282 loss_rnnt 38.417793 hw_loss 0.461368 lr 0.00095148 rank 6
2023-02-17 07:52:11,794 DEBUG TRAIN Batch 2/7100 loss 18.266920 loss_att 26.707703 loss_ctc 30.267332 loss_rnnt 14.768499 hw_loss 0.394143 lr 0.00095068 rank 7
2023-02-17 07:52:11,795 DEBUG TRAIN Batch 2/7100 loss 17.238081 loss_att 24.399601 loss_ctc 24.156620 loss_rnnt 14.685036 hw_loss 0.371758 lr 0.00095084 rank 5
2023-02-17 07:52:11,797 DEBUG TRAIN Batch 2/7100 loss 39.155407 loss_att 55.662376 loss_ctc 62.893990 loss_rnnt 32.530960 hw_loss 0.296082 lr 0.00095028 rank 4
2023-02-17 07:52:11,798 DEBUG TRAIN Batch 2/7100 loss 24.880825 loss_att 31.575672 loss_ctc 39.471985 loss_rnnt 21.431824 hw_loss 0.308520 lr 0.00095264 rank 1
2023-02-17 07:52:11,803 DEBUG TRAIN Batch 2/7100 loss 47.794216 loss_att 59.616802 loss_ctc 74.248436 loss_rnnt 41.761314 hw_loss 0.264661 lr 0.00095208 rank 2
2023-02-17 07:53:29,316 DEBUG TRAIN Batch 2/7200 loss 21.070293 loss_att 29.064110 loss_ctc 32.244511 loss_rnnt 17.797531 hw_loss 0.345192 lr 0.00095548 rank 6
2023-02-17 07:53:29,317 DEBUG TRAIN Batch 2/7200 loss 28.413383 loss_att 37.506882 loss_ctc 47.225372 loss_rnnt 23.897032 hw_loss 0.355099 lr 0.00095468 rank 7
2023-02-17 07:53:29,317 DEBUG TRAIN Batch 2/7200 loss 37.712845 loss_att 50.583290 loss_ctc 64.755203 loss_rnnt 31.358587 hw_loss 0.327227 lr 0.00095428 rank 4
2023-02-17 07:53:29,317 DEBUG TRAIN Batch 2/7200 loss 28.747934 loss_att 38.283073 loss_ctc 39.708950 loss_rnnt 25.218475 hw_loss 0.301803 lr 0.00095664 rank 1
2023-02-17 07:53:29,318 DEBUG TRAIN Batch 2/7200 loss 34.367619 loss_att 46.111847 loss_ctc 50.467102 loss_rnnt 29.679510 hw_loss 0.361246 lr 0.00095484 rank 5
2023-02-17 07:53:29,318 DEBUG TRAIN Batch 2/7200 loss 37.350033 loss_att 48.407310 loss_ctc 51.514324 loss_rnnt 33.035595 hw_loss 0.402017 lr 0.00095500 rank 3
2023-02-17 07:53:29,319 DEBUG TRAIN Batch 2/7200 loss 29.440084 loss_att 42.180389 loss_ctc 43.881950 loss_rnnt 24.792965 hw_loss 0.325270 lr 0.00095608 rank 2
2023-02-17 07:53:29,323 DEBUG TRAIN Batch 2/7200 loss 41.256844 loss_att 53.625092 loss_ctc 60.583008 loss_rnnt 36.018044 hw_loss 0.353109 lr 0.00095572 rank 0
2023-02-17 07:54:45,148 DEBUG TRAIN Batch 2/7300 loss 35.883335 loss_att 50.861607 loss_ctc 65.522614 loss_rnnt 28.718445 hw_loss 0.407499 lr 0.00095828 rank 4
2023-02-17 07:54:45,149 DEBUG TRAIN Batch 2/7300 loss 31.235489 loss_att 37.524658 loss_ctc 44.274677 loss_rnnt 28.022987 hw_loss 0.405204 lr 0.00095868 rank 7
2023-02-17 07:54:45,150 DEBUG TRAIN Batch 2/7300 loss 32.404270 loss_att 39.332607 loss_ctc 56.141098 loss_rnnt 27.668594 hw_loss 0.347062 lr 0.00096064 rank 1
2023-02-17 07:54:45,151 DEBUG TRAIN Batch 2/7300 loss 27.608648 loss_att 35.146622 loss_ctc 42.234768 loss_rnnt 23.937862 hw_loss 0.399458 lr 0.00095900 rank 3
2023-02-17 07:54:45,152 DEBUG TRAIN Batch 2/7300 loss 37.124424 loss_att 54.125210 loss_ctc 52.848072 loss_rnnt 31.386930 hw_loss 0.451595 lr 0.00095972 rank 0
2023-02-17 07:54:45,154 DEBUG TRAIN Batch 2/7300 loss 64.067551 loss_att 72.612747 loss_ctc 87.475609 loss_rnnt 59.005505 hw_loss 0.434868 lr 0.00095948 rank 6
2023-02-17 07:54:45,156 DEBUG TRAIN Batch 2/7300 loss 51.795227 loss_att 65.409645 loss_ctc 82.503273 loss_rnnt 44.793598 hw_loss 0.345633 lr 0.00095884 rank 5
2023-02-17 07:54:45,200 DEBUG TRAIN Batch 2/7300 loss 29.405308 loss_att 37.598564 loss_ctc 48.119846 loss_rnnt 25.055977 hw_loss 0.403891 lr 0.00096008 rank 2
2023-02-17 07:56:02,912 DEBUG TRAIN Batch 2/7400 loss 44.636806 loss_att 50.153904 loss_ctc 54.839775 loss_rnnt 41.992657 hw_loss 0.338127 lr 0.00096268 rank 7
2023-02-17 07:56:02,913 DEBUG TRAIN Batch 2/7400 loss 18.720165 loss_att 25.009691 loss_ctc 27.996346 loss_rnnt 16.042191 hw_loss 0.343584 lr 0.00096284 rank 5
2023-02-17 07:56:02,918 DEBUG TRAIN Batch 2/7400 loss 58.314247 loss_att 68.544952 loss_ctc 86.215546 loss_rnnt 52.387299 hw_loss 0.301187 lr 0.00096300 rank 3
2023-02-17 07:56:02,921 DEBUG TRAIN Batch 2/7400 loss 28.630545 loss_att 37.278419 loss_ctc 46.778488 loss_rnnt 24.337870 hw_loss 0.268825 lr 0.00096464 rank 1
2023-02-17 07:56:02,921 DEBUG TRAIN Batch 2/7400 loss 40.425201 loss_att 48.949654 loss_ctc 57.881622 loss_rnnt 36.225822 hw_loss 0.313064 lr 0.00096348 rank 6
2023-02-17 07:56:02,922 DEBUG TRAIN Batch 2/7400 loss 39.198643 loss_att 46.706429 loss_ctc 52.471439 loss_rnnt 35.774059 hw_loss 0.287475 lr 0.00096228 rank 4
2023-02-17 07:56:02,924 DEBUG TRAIN Batch 2/7400 loss 26.119474 loss_att 38.993416 loss_ctc 43.881905 loss_rnnt 20.995953 hw_loss 0.338270 lr 0.00096372 rank 0
2023-02-17 07:56:02,928 DEBUG TRAIN Batch 2/7400 loss 37.805035 loss_att 39.876030 loss_ctc 52.575874 loss_rnnt 35.208527 hw_loss 0.399118 lr 0.00096408 rank 2
2023-02-17 07:57:23,049 DEBUG TRAIN Batch 2/7500 loss 35.358540 loss_att 39.650452 loss_ctc 46.289131 loss_rnnt 32.797386 hw_loss 0.460043 lr 0.00096668 rank 7
2023-02-17 07:57:23,054 DEBUG TRAIN Batch 2/7500 loss 29.791056 loss_att 36.164337 loss_ctc 46.032051 loss_rnnt 26.181036 hw_loss 0.318563 lr 0.00096700 rank 3
2023-02-17 07:57:23,053 DEBUG TRAIN Batch 2/7500 loss 22.730822 loss_att 31.849684 loss_ctc 34.503494 loss_rnnt 19.158169 hw_loss 0.335986 lr 0.00096864 rank 1
2023-02-17 07:57:23,054 DEBUG TRAIN Batch 2/7500 loss 35.804981 loss_att 44.502201 loss_ctc 52.839424 loss_rnnt 31.611898 hw_loss 0.341956 lr 0.00096748 rank 6
2023-02-17 07:57:23,055 DEBUG TRAIN Batch 2/7500 loss 19.792076 loss_att 23.047543 loss_ctc 27.612532 loss_rnnt 17.880812 hw_loss 0.407708 lr 0.00096684 rank 5
2023-02-17 07:57:23,057 DEBUG TRAIN Batch 2/7500 loss 25.747698 loss_att 35.373482 loss_ctc 39.093544 loss_rnnt 21.826742 hw_loss 0.405661 lr 0.00096772 rank 0
2023-02-17 07:57:23,059 DEBUG TRAIN Batch 2/7500 loss 39.964283 loss_att 47.193161 loss_ctc 56.487770 loss_rnnt 36.128014 hw_loss 0.351294 lr 0.00096628 rank 4
2023-02-17 07:57:23,060 DEBUG TRAIN Batch 2/7500 loss 23.357344 loss_att 24.222454 loss_ctc 35.316147 loss_rnnt 21.411976 hw_loss 0.333447 lr 0.00096808 rank 2
2023-02-17 07:58:39,151 DEBUG TRAIN Batch 2/7600 loss 32.045452 loss_att 46.786839 loss_ctc 47.415955 loss_rnnt 26.803703 hw_loss 0.457633 lr 0.00097084 rank 5
2023-02-17 07:58:39,152 DEBUG TRAIN Batch 2/7600 loss 26.264252 loss_att 35.657566 loss_ctc 43.796547 loss_rnnt 21.883305 hw_loss 0.308711 lr 0.00097148 rank 6
2023-02-17 07:58:39,154 DEBUG TRAIN Batch 2/7600 loss 67.596382 loss_att 76.663063 loss_ctc 89.090469 loss_rnnt 62.747025 hw_loss 0.319020 lr 0.00097028 rank 4
2023-02-17 07:58:39,156 DEBUG TRAIN Batch 2/7600 loss 27.999531 loss_att 30.394817 loss_ctc 39.799492 loss_rnnt 25.734421 hw_loss 0.398855 lr 0.00097100 rank 3
2023-02-17 07:58:39,156 DEBUG TRAIN Batch 2/7600 loss 25.713902 loss_att 27.723684 loss_ctc 36.312923 loss_rnnt 23.732063 hw_loss 0.312522 lr 0.00097264 rank 1
2023-02-17 07:58:39,156 DEBUG TRAIN Batch 2/7600 loss 40.886120 loss_att 43.497753 loss_ctc 52.530556 loss_rnnt 38.627834 hw_loss 0.343819 lr 0.00097068 rank 7
2023-02-17 07:58:39,161 DEBUG TRAIN Batch 2/7600 loss 46.162582 loss_att 56.782455 loss_ctc 70.306465 loss_rnnt 40.634521 hw_loss 0.346692 lr 0.00097208 rank 2
2023-02-17 07:58:39,162 DEBUG TRAIN Batch 2/7600 loss 26.547775 loss_att 40.599026 loss_ctc 43.375534 loss_rnnt 21.355635 hw_loss 0.259103 lr 0.00097172 rank 0
2023-02-17 07:59:55,458 DEBUG TRAIN Batch 2/7700 loss 46.686722 loss_att 61.632965 loss_ctc 74.022057 loss_rnnt 39.930946 hw_loss 0.228407 lr 0.00097500 rank 3
2023-02-17 07:59:55,459 DEBUG TRAIN Batch 2/7700 loss 22.135998 loss_att 27.757473 loss_ctc 36.068523 loss_rnnt 18.951807 hw_loss 0.379170 lr 0.00097548 rank 6
2023-02-17 07:59:55,459 DEBUG TRAIN Batch 2/7700 loss 25.492754 loss_att 36.932564 loss_ctc 48.683975 loss_rnnt 19.962080 hw_loss 0.282279 lr 0.00097664 rank 1
2023-02-17 07:59:55,460 DEBUG TRAIN Batch 2/7700 loss 52.253906 loss_att 70.017410 loss_ctc 68.890060 loss_rnnt 46.306084 hw_loss 0.331812 lr 0.00097468 rank 7
2023-02-17 07:59:55,462 DEBUG TRAIN Batch 2/7700 loss 22.065643 loss_att 30.282024 loss_ctc 36.981457 loss_rnnt 18.231173 hw_loss 0.379534 lr 0.00097484 rank 5
2023-02-17 07:59:55,461 DEBUG TRAIN Batch 2/7700 loss 46.956799 loss_att 62.979477 loss_ctc 67.821213 loss_rnnt 40.805988 hw_loss 0.308159 lr 0.00097572 rank 0
2023-02-17 07:59:55,464 DEBUG TRAIN Batch 2/7700 loss 30.847549 loss_att 43.397774 loss_ctc 48.718872 loss_rnnt 25.777420 hw_loss 0.332328 lr 0.00097608 rank 2
2023-02-17 07:59:55,464 DEBUG TRAIN Batch 2/7700 loss 25.499851 loss_att 30.850655 loss_ctc 36.812614 loss_rnnt 22.679953 hw_loss 0.452573 lr 0.00097428 rank 4
2023-02-17 08:01:13,930 DEBUG TRAIN Batch 2/7800 loss 34.373707 loss_att 44.101364 loss_ctc 53.108131 loss_rnnt 29.707241 hw_loss 0.418148 lr 0.00097884 rank 5
2023-02-17 08:01:13,932 DEBUG TRAIN Batch 2/7800 loss 19.811506 loss_att 22.487446 loss_ctc 27.986231 loss_rnnt 17.932472 hw_loss 0.476029 lr 0.00097948 rank 6
2023-02-17 08:01:13,933 DEBUG TRAIN Batch 2/7800 loss 41.188850 loss_att 52.529243 loss_ctc 68.863045 loss_rnnt 35.106991 hw_loss 0.232298 lr 0.00097972 rank 0
2023-02-17 08:01:13,935 DEBUG TRAIN Batch 2/7800 loss 41.948914 loss_att 56.307831 loss_ctc 69.622139 loss_rnnt 35.219017 hw_loss 0.315657 lr 0.00098064 rank 1
2023-02-17 08:01:13,939 DEBUG TRAIN Batch 2/7800 loss 44.221577 loss_att 54.027496 loss_ctc 62.227638 loss_rnnt 39.721279 hw_loss 0.259319 lr 0.00097828 rank 4
2023-02-17 08:01:13,941 DEBUG TRAIN Batch 2/7800 loss 24.406372 loss_att 31.297791 loss_ctc 40.386269 loss_rnnt 20.731449 hw_loss 0.311222 lr 0.00098008 rank 2
2023-02-17 08:01:13,941 DEBUG TRAIN Batch 2/7800 loss 33.382950 loss_att 41.598766 loss_ctc 54.921562 loss_rnnt 28.680616 hw_loss 0.351285 lr 0.00097900 rank 3
2023-02-17 08:01:13,939 DEBUG TRAIN Batch 2/7800 loss 25.868286 loss_att 33.154610 loss_ctc 43.496395 loss_rnnt 21.884399 hw_loss 0.330387 lr 0.00097868 rank 7
2023-02-17 08:02:32,702 DEBUG TRAIN Batch 2/7900 loss 14.861508 loss_att 22.074547 loss_ctc 27.674175 loss_rnnt 11.502918 hw_loss 0.389299 lr 0.00098348 rank 6
2023-02-17 08:02:32,705 DEBUG TRAIN Batch 2/7900 loss 43.296009 loss_att 52.113708 loss_ctc 65.131729 loss_rnnt 38.461578 hw_loss 0.298990 lr 0.00098284 rank 5
2023-02-17 08:02:32,714 DEBUG TRAIN Batch 2/7900 loss 49.107292 loss_att 57.262642 loss_ctc 67.784653 loss_rnnt 44.847191 hw_loss 0.260094 lr 0.00098268 rank 7
2023-02-17 08:02:32,714 DEBUG TRAIN Batch 2/7900 loss 36.575710 loss_att 48.728577 loss_ctc 55.479340 loss_rnnt 31.463673 hw_loss 0.301834 lr 0.00098408 rank 2
2023-02-17 08:02:32,716 DEBUG TRAIN Batch 2/7900 loss 45.675655 loss_att 52.780514 loss_ctc 69.735390 loss_rnnt 40.830456 hw_loss 0.405501 lr 0.00098228 rank 4
2023-02-17 08:02:32,716 DEBUG TRAIN Batch 2/7900 loss 32.096767 loss_att 38.811821 loss_ctc 51.320801 loss_rnnt 28.034592 hw_loss 0.292424 lr 0.00098464 rank 1
2023-02-17 08:02:32,718 DEBUG TRAIN Batch 2/7900 loss 33.068714 loss_att 42.968838 loss_ctc 55.415199 loss_rnnt 27.861532 hw_loss 0.464293 lr 0.00098372 rank 0
2023-02-17 08:02:32,758 DEBUG TRAIN Batch 2/7900 loss 26.089638 loss_att 34.917416 loss_ctc 37.029884 loss_rnnt 22.712070 hw_loss 0.287456 lr 0.00098300 rank 3
2023-02-17 08:03:48,978 DEBUG TRAIN Batch 2/8000 loss 30.815596 loss_att 35.059669 loss_ctc 44.672504 loss_rnnt 27.912998 hw_loss 0.386610 lr 0.00098684 rank 5
2023-02-17 08:03:48,981 DEBUG TRAIN Batch 2/8000 loss 35.387913 loss_att 38.212200 loss_ctc 43.384384 loss_rnnt 33.522919 hw_loss 0.438640 lr 0.00098628 rank 4
2023-02-17 08:03:48,983 DEBUG TRAIN Batch 2/8000 loss 31.484118 loss_att 36.651974 loss_ctc 43.775455 loss_rnnt 28.668034 hw_loss 0.269368 lr 0.00098864 rank 1
2023-02-17 08:03:48,984 DEBUG TRAIN Batch 2/8000 loss 40.764095 loss_att 49.167244 loss_ctc 56.537659 loss_rnnt 36.800846 hw_loss 0.336518 lr 0.00098748 rank 6
2023-02-17 08:03:48,985 DEBUG TRAIN Batch 2/8000 loss 25.217388 loss_att 36.210796 loss_ctc 42.332222 loss_rnnt 20.567600 hw_loss 0.317117 lr 0.00098700 rank 3
2023-02-17 08:03:48,985 DEBUG TRAIN Batch 2/8000 loss 35.063644 loss_att 39.767750 loss_ctc 46.957195 loss_rnnt 32.324383 hw_loss 0.398686 lr 0.00098668 rank 7
2023-02-17 08:03:48,988 DEBUG TRAIN Batch 2/8000 loss 29.730509 loss_att 37.222485 loss_ctc 48.530529 loss_rnnt 25.543644 hw_loss 0.340867 lr 0.00098772 rank 0
2023-02-17 08:03:48,990 DEBUG TRAIN Batch 2/8000 loss 36.377140 loss_att 49.498543 loss_ctc 62.003937 loss_rnnt 30.193977 hw_loss 0.266200 lr 0.00098808 rank 2
2023-02-17 08:05:06,501 DEBUG TRAIN Batch 2/8100 loss 52.813637 loss_att 65.747025 loss_ctc 73.649033 loss_rnnt 47.263901 hw_loss 0.346893 lr 0.00099148 rank 6
2023-02-17 08:05:06,506 DEBUG TRAIN Batch 2/8100 loss 28.344175 loss_att 32.010777 loss_ctc 34.033367 loss_rnnt 26.614464 hw_loss 0.445934 lr 0.00099084 rank 5
2023-02-17 08:05:06,508 DEBUG TRAIN Batch 2/8100 loss 26.074787 loss_att 33.733093 loss_ctc 43.617676 loss_rnnt 22.028343 hw_loss 0.329492 lr 0.00099172 rank 0
2023-02-17 08:05:06,509 DEBUG TRAIN Batch 2/8100 loss 35.942081 loss_att 47.274567 loss_ctc 55.820457 loss_rnnt 30.834108 hw_loss 0.358169 lr 0.00099068 rank 7
2023-02-17 08:05:06,511 DEBUG TRAIN Batch 2/8100 loss 21.492064 loss_att 30.948151 loss_ctc 32.292816 loss_rnnt 17.963505 hw_loss 0.369825 lr 0.00099264 rank 1
2023-02-17 08:05:06,514 DEBUG TRAIN Batch 2/8100 loss 32.004246 loss_att 37.199757 loss_ctc 41.937283 loss_rnnt 29.415344 hw_loss 0.422614 lr 0.00099100 rank 3
2023-02-17 08:05:06,541 DEBUG TRAIN Batch 2/8100 loss 25.893412 loss_att 32.700310 loss_ctc 35.287571 loss_rnnt 23.072796 hw_loss 0.387532 lr 0.00099208 rank 2
2023-02-17 08:05:06,545 DEBUG TRAIN Batch 2/8100 loss 23.142494 loss_att 33.331551 loss_ctc 33.533512 loss_rnnt 19.588177 hw_loss 0.245695 lr 0.00099028 rank 4
2023-02-17 08:06:23,324 DEBUG TRAIN Batch 2/8200 loss 32.451599 loss_att 43.830395 loss_ctc 47.853603 loss_rnnt 27.969063 hw_loss 0.287210 lr 0.00099484 rank 5
2023-02-17 08:06:23,331 DEBUG TRAIN Batch 2/8200 loss 42.359089 loss_att 54.527916 loss_ctc 57.909702 loss_rnnt 37.682953 hw_loss 0.316793 lr 0.00099468 rank 7
2023-02-17 08:06:23,330 DEBUG TRAIN Batch 2/8200 loss 37.315174 loss_att 41.684113 loss_ctc 52.912727 loss_rnnt 34.175659 hw_loss 0.348843 lr 0.00099548 rank 6
2023-02-17 08:06:23,332 DEBUG TRAIN Batch 2/8200 loss 46.472103 loss_att 55.999065 loss_ctc 61.200932 loss_rnnt 42.376923 hw_loss 0.423645 lr 0.00099428 rank 4
2023-02-17 08:06:23,332 DEBUG TRAIN Batch 2/8200 loss 36.052174 loss_att 36.610855 loss_ctc 45.341240 loss_rnnt 34.483452 hw_loss 0.409579 lr 0.00099608 rank 2
2023-02-17 08:06:23,333 DEBUG TRAIN Batch 2/8200 loss 35.796501 loss_att 40.933201 loss_ctc 57.633411 loss_rnnt 31.712172 hw_loss 0.272627 lr 0.00099500 rank 3
2023-02-17 08:06:23,337 DEBUG TRAIN Batch 2/8200 loss 23.530266 loss_att 31.508446 loss_ctc 34.359425 loss_rnnt 20.338488 hw_loss 0.285476 lr 0.00099664 rank 1
2023-02-17 08:06:23,339 DEBUG TRAIN Batch 2/8200 loss 24.891281 loss_att 30.653233 loss_ctc 34.123135 loss_rnnt 22.262514 hw_loss 0.460244 lr 0.00099572 rank 0
2023-02-17 08:07:40,258 DEBUG TRAIN Batch 2/8300 loss 34.730186 loss_att 43.540459 loss_ctc 53.513954 loss_rnnt 30.248245 hw_loss 0.403842 lr 0.00099884 rank 5
2023-02-17 08:07:40,258 DEBUG TRAIN Batch 2/8300 loss 39.766632 loss_att 47.990425 loss_ctc 61.599098 loss_rnnt 35.023354 hw_loss 0.351613 lr 0.00099900 rank 3
2023-02-17 08:07:40,259 DEBUG TRAIN Batch 2/8300 loss 22.621414 loss_att 32.425720 loss_ctc 35.812061 loss_rnnt 18.713808 hw_loss 0.352486 lr 0.00099996 rank 2
2023-02-17 08:07:40,259 DEBUG TRAIN Batch 2/8300 loss 48.450626 loss_att 51.367371 loss_ctc 68.141876 loss_rnnt 45.108139 hw_loss 0.250581 lr 0.00099868 rank 7
2023-02-17 08:07:40,260 DEBUG TRAIN Batch 2/8300 loss 25.439760 loss_att 34.850044 loss_ctc 31.820097 loss_rnnt 22.508492 hw_loss 0.372189 lr 0.00099948 rank 6
2023-02-17 08:07:40,261 DEBUG TRAIN Batch 2/8300 loss 24.747709 loss_att 31.095453 loss_ctc 44.335732 loss_rnnt 20.738394 hw_loss 0.240059 lr 0.00099968 rank 1
2023-02-17 08:07:40,263 DEBUG TRAIN Batch 2/8300 loss 26.057896 loss_att 35.741890 loss_ctc 36.756191 loss_rnnt 22.509087 hw_loss 0.347944 lr 0.00099828 rank 4
2023-02-17 08:07:40,311 DEBUG TRAIN Batch 2/8300 loss 44.376560 loss_att 52.180038 loss_ctc 67.621063 loss_rnnt 39.537102 hw_loss 0.336554 lr 0.00099972 rank 0
2023-02-17 08:08:38,001 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 2
2023-02-17 08:08:38,001 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 5
2023-02-17 08:08:38,001 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 3
2023-02-17 08:08:38,002 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 4
2023-02-17 08:08:38,002 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 6
2023-02-17 08:08:38,008 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 1
2023-02-17 08:08:38,010 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 7
2023-02-17 08:08:38,018 DEBUG CV Batch 2/0 loss 5.980157 loss_att 5.502371 loss_ctc 9.642120 loss_rnnt 5.306391 hw_loss 0.526991 history loss 5.758670 rank 0
2023-02-17 08:08:49,150 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 7
2023-02-17 08:08:49,191 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 6
2023-02-17 08:08:49,211 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 4
2023-02-17 08:08:49,255 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 0
2023-02-17 08:08:49,279 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 5
2023-02-17 08:08:49,286 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 3
2023-02-17 08:08:49,365 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 1
2023-02-17 08:08:49,806 DEBUG CV Batch 2/100 loss 26.438696 loss_att 30.985016 loss_ctc 42.000595 loss_rnnt 23.221922 hw_loss 0.436104 history loss 13.039724 rank 2
2023-02-17 08:09:03,172 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 7
2023-02-17 08:09:03,218 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 5
2023-02-17 08:09:03,270 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 6
2023-02-17 08:09:03,517 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 0
2023-02-17 08:09:03,580 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 1
2023-02-17 08:09:03,644 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 3
2023-02-17 08:09:04,021 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 2
2023-02-17 08:09:04,074 DEBUG CV Batch 2/200 loss 32.170300 loss_att 52.451523 loss_ctc 51.785263 loss_rnnt 25.376053 hw_loss 0.230019 history loss 14.401752 rank 4
2023-02-17 08:09:15,150 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 7
2023-02-17 08:09:15,282 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 6
2023-02-17 08:09:15,302 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 5
2023-02-17 08:09:15,644 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 1
2023-02-17 08:09:15,717 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 0
2023-02-17 08:09:15,861 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 3
2023-02-17 08:09:16,270 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 2
2023-02-17 08:09:16,912 DEBUG CV Batch 2/300 loss 17.567896 loss_att 17.999918 loss_ctc 28.551470 loss_rnnt 15.771027 hw_loss 0.461223 history loss 14.485909 rank 4
2023-02-17 08:09:27,158 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 7
2023-02-17 08:09:27,214 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 5
2023-02-17 08:09:27,246 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 6
2023-02-17 08:09:27,553 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 1
2023-02-17 08:09:27,975 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 3
2023-02-17 08:09:28,314 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 2
2023-02-17 08:09:28,566 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 0
2023-02-17 08:09:29,240 DEBUG CV Batch 2/400 loss 64.367966 loss_att 152.475601 loss_ctc 86.478722 loss_rnnt 43.671688 hw_loss 0.237466 history loss 16.234325 rank 4
2023-02-17 08:09:37,609 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 7
2023-02-17 08:09:37,672 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 6
2023-02-17 08:09:37,799 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 5
2023-02-17 08:09:37,991 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 1
2023-02-17 08:09:38,620 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 3
2023-02-17 08:09:39,128 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 0
2023-02-17 08:09:39,147 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 2
2023-02-17 08:09:40,468 DEBUG CV Batch 2/500 loss 27.959541 loss_att 27.230759 loss_ctc 40.205021 loss_rnnt 26.263952 hw_loss 0.391157 history loss 17.622853 rank 4
2023-02-17 08:09:49,486 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 7
2023-02-17 08:09:49,670 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 6
2023-02-17 08:09:50,053 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 1
2023-02-17 08:09:50,066 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 5
2023-02-17 08:09:50,790 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 3
2023-02-17 08:09:51,335 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 0
2023-02-17 08:09:52,537 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 4
2023-02-17 08:09:52,690 DEBUG CV Batch 2/600 loss 18.716078 loss_att 19.452015 loss_ctc 26.870810 loss_rnnt 17.184124 hw_loss 0.557750 history loss 19.214387 rank 2
2023-02-17 08:10:01,406 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 7
2023-02-17 08:10:01,634 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 6
2023-02-17 08:10:01,791 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 1
2023-02-17 08:10:02,316 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 3
2023-02-17 08:10:02,538 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 5
2023-02-17 08:10:03,258 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 0
2023-02-17 08:10:03,861 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 4
2023-02-17 08:10:04,591 DEBUG CV Batch 2/700 loss 63.697544 loss_att 122.959061 loss_ctc 81.698624 loss_rnnt 49.336487 hw_loss 0.203649 history loss 20.537505 rank 2
2023-02-17 08:10:13,489 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 7
2023-02-17 08:10:13,490 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 6
2023-02-17 08:10:13,763 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 1
2023-02-17 08:10:14,413 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 5
2023-02-17 08:10:14,698 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 0
2023-02-17 08:10:14,897 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 3
2023-02-17 08:10:15,344 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 4
2023-02-17 08:10:15,995 DEBUG CV Batch 2/800 loss 26.385532 loss_att 28.945305 loss_ctc 41.897537 loss_rnnt 23.545538 hw_loss 0.487072 history loss 19.507007 rank 2
2023-02-17 08:10:27,303 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 6
2023-02-17 08:10:27,470 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 1
2023-02-17 08:10:27,572 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 7
2023-02-17 08:10:28,163 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 0
2023-02-17 08:10:28,406 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 5
2023-02-17 08:10:28,930 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 4
2023-02-17 08:10:29,401 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 3
2023-02-17 08:10:29,515 DEBUG CV Batch 2/900 loss 32.791447 loss_att 55.796883 loss_ctc 51.073792 loss_rnnt 25.633022 hw_loss 0.224420 history loss 19.205245 rank 2
2023-02-17 08:10:39,450 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 6
2023-02-17 08:10:39,639 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 1
2023-02-17 08:10:39,705 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 7
2023-02-17 08:10:40,525 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 0
2023-02-17 08:10:40,743 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 5
2023-02-17 08:10:41,207 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 4
2023-02-17 08:10:41,798 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 2
2023-02-17 08:10:42,581 DEBUG CV Batch 2/1000 loss 15.654642 loss_att 16.909409 loss_ctc 21.240013 loss_rnnt 14.438681 hw_loss 0.413048 history loss 18.765382 rank 3
2023-02-17 08:10:51,294 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 6
2023-02-17 08:10:51,484 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 1
2023-02-17 08:10:51,495 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 7
2023-02-17 08:10:52,581 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 0
2023-02-17 08:10:52,597 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 5
2023-02-17 08:10:53,677 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 2
2023-02-17 08:10:53,874 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 4
2023-02-17 08:10:54,604 DEBUG CV Batch 2/1100 loss 10.762716 loss_att 10.618252 loss_ctc 15.705715 loss_rnnt 9.850912 hw_loss 0.528056 history loss 18.760528 rank 3
2023-02-17 08:11:01,722 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 6
2023-02-17 08:11:01,902 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 1
2023-02-17 08:11:01,932 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 7
2023-02-17 08:11:03,145 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 5
2023-02-17 08:11:03,214 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 0
2023-02-17 08:11:04,282 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 2
2023-02-17 08:11:04,344 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 4
2023-02-17 08:11:05,868 DEBUG CV Batch 2/1200 loss 30.226906 loss_att 32.986969 loss_ctc 43.410641 loss_rnnt 27.709503 hw_loss 0.389171 history loss 19.307868 rank 3
2023-02-17 08:11:13,636 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 6
2023-02-17 08:11:13,820 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 1
2023-02-17 08:11:13,908 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 7
2023-02-17 08:11:15,192 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 5
2023-02-17 08:11:16,231 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 0
2023-02-17 08:11:16,253 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 4
2023-02-17 08:11:16,323 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 2
2023-02-17 08:11:17,923 DEBUG CV Batch 2/1300 loss 18.763924 loss_att 19.278442 loss_ctc 25.721493 loss_rnnt 17.521637 hw_loss 0.396947 history loss 19.795974 rank 3
2023-02-17 08:11:24,968 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 6
2023-02-17 08:11:25,024 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 1
2023-02-17 08:11:26,238 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 7
2023-02-17 08:11:26,511 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 5
2023-02-17 08:11:27,442 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 4
2023-02-17 08:11:28,106 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 0
2023-02-17 08:11:28,273 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 2
2023-02-17 08:11:29,270 DEBUG CV Batch 2/1400 loss 36.312077 loss_att 75.801971 loss_ctc 47.662979 loss_rnnt 26.713108 hw_loss 0.351625 history loss 20.453060 rank 3
2023-02-17 08:11:37,495 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 1
2023-02-17 08:11:37,526 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 6
2023-02-17 08:11:38,992 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 5
2023-02-17 08:11:39,041 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 7
2023-02-17 08:11:39,458 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 0
2023-02-17 08:11:39,721 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 4
2023-02-17 08:11:40,081 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 2
2023-02-17 08:11:40,688 DEBUG CV Batch 2/1500 loss 27.049307 loss_att 28.980021 loss_ctc 36.875156 loss_rnnt 25.176300 hw_loss 0.331407 history loss 20.033547 rank 3
2023-02-17 08:11:51,320 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 1
2023-02-17 08:11:51,506 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 6
2023-02-17 08:11:52,602 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 7
2023-02-17 08:11:52,709 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 5
2023-02-17 08:11:52,943 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 0
2023-02-17 08:11:53,581 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 4
2023-02-17 08:11:53,829 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 3
2023-02-17 08:11:54,137 DEBUG CV Batch 2/1600 loss 28.354456 loss_att 46.618271 loss_ctc 41.327003 loss_rnnt 22.784086 hw_loss 0.352378 history loss 19.880375 rank 2
2023-02-17 08:12:03,734 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 1
2023-02-17 08:12:03,894 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 6
2023-02-17 08:12:05,074 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 7
2023-02-17 08:12:05,214 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 5
2023-02-17 08:12:05,612 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 0
2023-02-17 08:12:06,201 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 4
2023-02-17 08:12:06,364 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 3
2023-02-17 08:12:06,819 DEBUG CV Batch 2/1700 loss 24.429947 loss_att 24.589592 loss_ctc 40.163811 loss_rnnt 22.104349 hw_loss 0.367162 history loss 19.626501 rank 2
2023-02-17 08:12:13,033 INFO Epoch 2 CV info cv_loss 19.562935543208475
2023-02-17 08:12:13,034 INFO Epoch 3 TRAIN info lr 0.0009989616195929486
2023-02-17 08:12:13,036 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:13,072 INFO Epoch 2 CV info cv_loss 19.562935543794268
2023-02-17 08:12:13,073 INFO Epoch 3 TRAIN info lr 0.00099984
2023-02-17 08:12:13,076 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:14,205 INFO Epoch 2 CV info cv_loss 19.56293554537936
2023-02-17 08:12:14,206 INFO Epoch 3 TRAIN info lr 0.00099992
2023-02-17 08:12:14,210 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:14,336 INFO Epoch 2 CV info cv_loss 19.562935545275987
2023-02-17 08:12:14,337 INFO Epoch 3 TRAIN info lr 0.0009992009587217895
2023-02-17 08:12:14,340 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:15,364 INFO Epoch 2 CV info cv_loss 19.562935543036183
2023-02-17 08:12:15,365 INFO Epoch 3 TRAIN info lr 0.0009992
2023-02-17 08:12:15,368 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:15,592 INFO Epoch 2 CV info cv_loss 19.562935545310445
2023-02-17 08:12:15,594 INFO Epoch 3 TRAIN info lr 0.000999620216462911
2023-02-17 08:12:15,597 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:15,714 INFO Epoch 2 CV info cv_loss 19.562935543897645
2023-02-17 08:12:15,716 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/2.pt
2023-02-17 08:12:16,020 INFO Epoch 2 CV info cv_loss 19.56293554469019
2023-02-17 08:12:16,021 INFO Epoch 3 TRAIN info lr 0.0009989616195929486
2023-02-17 08:12:16,023 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:12:16,405 INFO Epoch 3 TRAIN info lr 0.0009991810072235551
2023-02-17 08:12:16,409 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:13:30,065 DEBUG TRAIN Batch 3/0 loss 22.220764 loss_att 22.896080 loss_ctc 28.791052 loss_rnnt 20.950130 hw_loss 0.486624 lr 0.00099988 rank 6
2023-02-17 08:13:30,066 DEBUG TRAIN Batch 3/0 loss 14.036036 loss_att 14.696476 loss_ctc 19.790489 loss_rnnt 12.900891 hw_loss 0.442118 lr 0.00099996 rank 7
2023-02-17 08:13:30,068 DEBUG TRAIN Batch 3/0 loss 20.735798 loss_att 20.640087 loss_ctc 26.162081 loss_rnnt 19.793026 hw_loss 0.447016 lr 0.00099918 rank 5
2023-02-17 08:13:30,070 DEBUG TRAIN Batch 3/0 loss 21.241493 loss_att 20.542679 loss_ctc 26.486664 loss_rnnt 20.468828 hw_loss 0.399507 lr 0.00099960 rank 3
2023-02-17 08:13:30,070 DEBUG TRAIN Batch 3/0 loss 13.858852 loss_att 14.004865 loss_ctc 17.596758 loss_rnnt 13.064300 hw_loss 0.500555 lr 0.00099894 rank 1
2023-02-17 08:13:30,095 DEBUG TRAIN Batch 3/0 loss 19.047089 loss_att 20.016516 loss_ctc 24.280474 loss_rnnt 17.861252 hw_loss 0.551563 lr 0.00099916 rank 0
2023-02-17 08:13:30,108 DEBUG TRAIN Batch 3/0 loss 14.609888 loss_att 16.096115 loss_ctc 18.432127 loss_rnnt 13.528477 hw_loss 0.514750 lr 0.00099894 rank 2
2023-02-17 08:13:30,131 DEBUG TRAIN Batch 3/0 loss 18.658285 loss_att 17.775318 loss_ctc 22.624102 loss_rnnt 18.097017 hw_loss 0.392030 lr 0.00099924 rank 4
2023-02-17 08:14:45,692 DEBUG TRAIN Batch 3/100 loss 49.945641 loss_att 69.202454 loss_ctc 73.675034 loss_rnnt 42.672543 hw_loss 0.483404 lr 0.00099807 rank 6
2023-02-17 08:14:45,694 DEBUG TRAIN Batch 3/100 loss 18.122766 loss_att 34.938595 loss_ctc 29.195532 loss_rnnt 13.134501 hw_loss 0.278871 lr 0.00099719 rank 5
2023-02-17 08:14:45,700 DEBUG TRAIN Batch 3/100 loss 49.915031 loss_att 62.700531 loss_ctc 70.048981 loss_rnnt 44.449600 hw_loss 0.419638 lr 0.00099838 rank 4
2023-02-17 08:14:45,700 DEBUG TRAIN Batch 3/100 loss 22.138138 loss_att 33.853706 loss_ctc 37.771637 loss_rnnt 17.553699 hw_loss 0.294106 lr 0.00099803 rank 7
2023-02-17 08:14:45,702 DEBUG TRAIN Batch 3/100 loss 40.339680 loss_att 48.296120 loss_ctc 58.733910 loss_rnnt 36.148193 hw_loss 0.276818 lr 0.00099761 rank 3
2023-02-17 08:14:45,703 DEBUG TRAIN Batch 3/100 loss 21.420826 loss_att 29.965322 loss_ctc 35.764328 loss_rnnt 17.567247 hw_loss 0.435395 lr 0.00099695 rank 1
2023-02-17 08:14:45,703 DEBUG TRAIN Batch 3/100 loss 59.303570 loss_att 68.005280 loss_ctc 85.171341 loss_rnnt 53.989086 hw_loss 0.234576 lr 0.00099717 rank 0
2023-02-17 08:14:45,748 DEBUG TRAIN Batch 3/100 loss 35.049301 loss_att 46.667812 loss_ctc 46.370953 loss_rnnt 31.054169 hw_loss 0.303514 lr 0.00099695 rank 2
2023-02-17 08:16:00,865 DEBUG TRAIN Batch 3/200 loss 36.009220 loss_att 41.705612 loss_ctc 51.180962 loss_rnnt 32.654068 hw_loss 0.361832 lr 0.00099604 rank 7
2023-02-17 08:16:00,865 DEBUG TRAIN Batch 3/200 loss 45.492851 loss_att 60.321163 loss_ctc 65.858078 loss_rnnt 39.605690 hw_loss 0.386498 lr 0.00099521 rank 5
2023-02-17 08:16:00,867 DEBUG TRAIN Batch 3/200 loss 34.355267 loss_att 45.138748 loss_ctc 57.756084 loss_rnnt 28.907209 hw_loss 0.321101 lr 0.00099608 rank 6
2023-02-17 08:16:00,868 DEBUG TRAIN Batch 3/200 loss 30.971325 loss_att 37.409828 loss_ctc 40.297134 loss_rnnt 28.236073 hw_loss 0.382703 lr 0.00099498 rank 1
2023-02-17 08:16:00,870 DEBUG TRAIN Batch 3/200 loss 45.474182 loss_att 56.141380 loss_ctc 70.423279 loss_rnnt 39.821953 hw_loss 0.360459 lr 0.00099519 rank 0
2023-02-17 08:16:00,871 DEBUG TRAIN Batch 3/200 loss 27.891340 loss_att 39.601566 loss_ctc 42.478069 loss_rnnt 23.428684 hw_loss 0.329461 lr 0.00099498 rank 2
2023-02-17 08:16:00,872 DEBUG TRAIN Batch 3/200 loss 23.876225 loss_att 38.889999 loss_ctc 34.522152 loss_rnnt 19.250637 hw_loss 0.381326 lr 0.00099640 rank 4
2023-02-17 08:16:00,918 DEBUG TRAIN Batch 3/200 loss 35.031525 loss_att 43.616955 loss_ctc 57.913784 loss_rnnt 30.117363 hw_loss 0.273949 lr 0.00099563 rank 3
2023-02-17 08:17:17,561 DEBUG TRAIN Batch 3/300 loss 45.306248 loss_att 51.958759 loss_ctc 70.178040 loss_rnnt 40.476391 hw_loss 0.343345 lr 0.00099411 rank 6
2023-02-17 08:17:17,561 DEBUG TRAIN Batch 3/300 loss 42.883797 loss_att 44.301300 loss_ctc 62.111374 loss_rnnt 39.832710 hw_loss 0.382324 lr 0.00099301 rank 2
2023-02-17 08:17:17,562 DEBUG TRAIN Batch 3/300 loss 41.468178 loss_att 50.643333 loss_ctc 56.775810 loss_rnnt 37.398636 hw_loss 0.362800 lr 0.00099407 rank 7
2023-02-17 08:17:17,564 DEBUG TRAIN Batch 3/300 loss 46.345135 loss_att 47.165043 loss_ctc 62.643318 loss_rnnt 43.858021 hw_loss 0.281322 lr 0.00099443 rank 4
2023-02-17 08:17:17,566 DEBUG TRAIN Batch 3/300 loss 36.078583 loss_att 46.910393 loss_ctc 52.959145 loss_rnnt 31.493793 hw_loss 0.314411 lr 0.00099301 rank 1
2023-02-17 08:17:17,566 DEBUG TRAIN Batch 3/300 loss 33.467075 loss_att 44.820660 loss_ctc 52.471893 loss_rnnt 28.457016 hw_loss 0.385065 lr 0.00099325 rank 5
2023-02-17 08:17:17,566 DEBUG TRAIN Batch 3/300 loss 43.526676 loss_att 51.162720 loss_ctc 64.509903 loss_rnnt 39.049255 hw_loss 0.285847 lr 0.00099366 rank 3
2023-02-17 08:17:17,574 DEBUG TRAIN Batch 3/300 loss 32.749489 loss_att 43.724609 loss_ctc 48.635448 loss_rnnt 28.234331 hw_loss 0.378757 lr 0.00099323 rank 0
2023-02-17 08:18:34,202 DEBUG TRAIN Batch 3/400 loss 33.449287 loss_att 45.490616 loss_ctc 56.317417 loss_rnnt 27.789936 hw_loss 0.378748 lr 0.00099106 rank 2
2023-02-17 08:18:34,203 DEBUG TRAIN Batch 3/400 loss 26.676247 loss_att 34.749714 loss_ctc 39.110619 loss_rnnt 23.211466 hw_loss 0.360316 lr 0.00099106 rank 1
2023-02-17 08:18:34,203 DEBUG TRAIN Batch 3/400 loss 40.799221 loss_att 48.812836 loss_ctc 54.273514 loss_rnnt 37.223125 hw_loss 0.331494 lr 0.00099129 rank 5
2023-02-17 08:18:34,204 DEBUG TRAIN Batch 3/400 loss 28.536972 loss_att 39.205658 loss_ctc 42.241692 loss_rnnt 24.400249 hw_loss 0.329418 lr 0.00099211 rank 7
2023-02-17 08:18:34,204 DEBUG TRAIN Batch 3/400 loss 32.856705 loss_att 40.886337 loss_ctc 55.531116 loss_rnnt 28.052008 hw_loss 0.329092 lr 0.00099247 rank 4
2023-02-17 08:18:34,204 DEBUG TRAIN Batch 3/400 loss 33.358307 loss_att 44.190216 loss_ctc 49.688385 loss_rnnt 28.828197 hw_loss 0.349477 lr 0.00099215 rank 6
2023-02-17 08:18:34,205 DEBUG TRAIN Batch 3/400 loss 30.008791 loss_att 38.822556 loss_ctc 45.399025 loss_rnnt 26.008949 hw_loss 0.346981 lr 0.00099170 rank 3
2023-02-17 08:18:34,206 DEBUG TRAIN Batch 3/400 loss 38.140686 loss_att 46.667229 loss_ctc 56.978256 loss_rnnt 33.760662 hw_loss 0.305702 lr 0.00099128 rank 0
2023-02-17 08:19:49,102 DEBUG TRAIN Batch 3/500 loss 38.545624 loss_att 47.191410 loss_ctc 49.484776 loss_rnnt 35.213783 hw_loss 0.270240 lr 0.00099021 rank 6
2023-02-17 08:19:49,108 DEBUG TRAIN Batch 3/500 loss 28.268723 loss_att 33.267776 loss_ctc 44.306099 loss_rnnt 24.927853 hw_loss 0.380146 lr 0.00099017 rank 7
2023-02-17 08:19:49,109 DEBUG TRAIN Batch 3/500 loss 24.477362 loss_att 30.356401 loss_ctc 34.778290 loss_rnnt 21.732405 hw_loss 0.366922 lr 0.00099052 rank 4
2023-02-17 08:19:49,111 DEBUG TRAIN Batch 3/500 loss 20.639849 loss_att 26.279942 loss_ctc 29.932190 loss_rnnt 18.047655 hw_loss 0.422238 lr 0.00098933 rank 0
2023-02-17 08:19:49,111 DEBUG TRAIN Batch 3/500 loss 29.764107 loss_att 35.816833 loss_ctc 38.918255 loss_rnnt 27.175953 hw_loss 0.294476 lr 0.00098912 rank 2
2023-02-17 08:19:49,114 DEBUG TRAIN Batch 3/500 loss 31.937170 loss_att 36.648846 loss_ctc 49.133873 loss_rnnt 28.485001 hw_loss 0.406768 lr 0.00098912 rank 1
2023-02-17 08:19:49,114 DEBUG TRAIN Batch 3/500 loss 23.152559 loss_att 34.732704 loss_ctc 39.568550 loss_rnnt 18.427286 hw_loss 0.413331 lr 0.00098935 rank 5
2023-02-17 08:19:49,116 DEBUG TRAIN Batch 3/500 loss 30.821270 loss_att 41.302490 loss_ctc 52.097492 loss_rnnt 25.660950 hw_loss 0.426085 lr 0.00098976 rank 3
2023-02-17 08:21:05,907 DEBUG TRAIN Batch 3/600 loss 22.054005 loss_att 26.911354 loss_ctc 31.033907 loss_rnnt 19.623314 hw_loss 0.491062 lr 0.00098827 rank 6
2023-02-17 08:21:05,909 DEBUG TRAIN Batch 3/600 loss 33.753902 loss_att 39.382011 loss_ctc 52.431362 loss_rnnt 29.944582 hw_loss 0.362572 lr 0.00098783 rank 3
2023-02-17 08:21:05,910 DEBUG TRAIN Batch 3/600 loss 30.904837 loss_att 34.593239 loss_ctc 44.077545 loss_rnnt 28.210762 hw_loss 0.375063 lr 0.00098823 rank 7
2023-02-17 08:21:05,911 DEBUG TRAIN Batch 3/600 loss 22.256271 loss_att 21.758936 loss_ctc 28.331129 loss_rnnt 21.301468 hw_loss 0.458046 lr 0.00098719 rank 1
2023-02-17 08:21:05,914 DEBUG TRAIN Batch 3/600 loss 22.084106 loss_att 27.405418 loss_ctc 37.197010 loss_rnnt 18.760786 hw_loss 0.457505 lr 0.00098742 rank 5
2023-02-17 08:21:05,917 DEBUG TRAIN Batch 3/600 loss 23.489212 loss_att 29.358585 loss_ctc 36.555397 loss_rnnt 20.350687 hw_loss 0.417171 lr 0.00098719 rank 2
2023-02-17 08:21:05,918 DEBUG TRAIN Batch 3/600 loss 20.869305 loss_att 23.289791 loss_ctc 31.952135 loss_rnnt 18.692505 hw_loss 0.403108 lr 0.00098740 rank 0
2023-02-17 08:21:05,963 DEBUG TRAIN Batch 3/600 loss 23.700113 loss_att 26.305893 loss_ctc 33.750122 loss_rnnt 21.582214 hw_loss 0.481390 lr 0.00098858 rank 4
2023-02-17 08:22:26,036 DEBUG TRAIN Batch 3/700 loss 34.884594 loss_att 44.114220 loss_ctc 46.604797 loss_rnnt 31.290798 hw_loss 0.347209 lr 0.00098634 rank 6
2023-02-17 08:22:26,037 DEBUG TRAIN Batch 3/700 loss 27.938221 loss_att 37.864120 loss_ctc 40.449265 loss_rnnt 24.068203 hw_loss 0.406312 lr 0.00098527 rank 1
2023-02-17 08:22:26,038 DEBUG TRAIN Batch 3/700 loss 74.096046 loss_att 92.988510 loss_ctc 102.504776 loss_rnnt 66.378143 hw_loss 0.284216 lr 0.00098665 rank 4
2023-02-17 08:22:26,040 DEBUG TRAIN Batch 3/700 loss 32.378265 loss_att 40.708012 loss_ctc 51.539078 loss_rnnt 28.009705 hw_loss 0.277192 lr 0.00098550 rank 5
2023-02-17 08:22:26,044 DEBUG TRAIN Batch 3/700 loss 25.737812 loss_att 35.018871 loss_ctc 40.529667 loss_rnnt 21.726341 hw_loss 0.343149 lr 0.00098631 rank 7
2023-02-17 08:22:26,044 DEBUG TRAIN Batch 3/700 loss 19.342302 loss_att 24.846912 loss_ctc 32.965141 loss_rnnt 16.231703 hw_loss 0.362434 lr 0.00098527 rank 2
2023-02-17 08:22:26,053 DEBUG TRAIN Batch 3/700 loss 33.935196 loss_att 37.132301 loss_ctc 48.291374 loss_rnnt 31.173023 hw_loss 0.391109 lr 0.00098548 rank 0
2023-02-17 08:22:26,060 DEBUG TRAIN Batch 3/700 loss 51.886166 loss_att 59.040409 loss_ctc 71.770630 loss_rnnt 47.611965 hw_loss 0.360175 lr 0.00098590 rank 3
2023-02-17 08:23:42,047 DEBUG TRAIN Batch 3/800 loss 40.234818 loss_att 50.795010 loss_ctc 61.786621 loss_rnnt 35.011032 hw_loss 0.446577 lr 0.00098399 rank 3
2023-02-17 08:23:42,049 DEBUG TRAIN Batch 3/800 loss 59.839664 loss_att 74.969757 loss_ctc 85.302528 loss_rnnt 53.199108 hw_loss 0.411544 lr 0.00098439 rank 7
2023-02-17 08:23:42,050 DEBUG TRAIN Batch 3/800 loss 43.926792 loss_att 45.508781 loss_ctc 61.040993 loss_rnnt 41.174812 hw_loss 0.288168 lr 0.00098357 rank 0
2023-02-17 08:23:42,050 DEBUG TRAIN Batch 3/800 loss 33.307224 loss_att 37.778023 loss_ctc 46.459549 loss_rnnt 30.488567 hw_loss 0.320358 lr 0.00098443 rank 6
2023-02-17 08:23:42,054 DEBUG TRAIN Batch 3/800 loss 57.677101 loss_att 66.067078 loss_ctc 75.030098 loss_rnnt 53.548126 hw_loss 0.257336 lr 0.00098359 rank 5
2023-02-17 08:23:42,054 DEBUG TRAIN Batch 3/800 loss 28.656626 loss_att 37.251873 loss_ctc 45.571373 loss_rnnt 24.513601 hw_loss 0.316272 lr 0.00098336 rank 1
2023-02-17 08:23:42,055 DEBUG TRAIN Batch 3/800 loss 30.964018 loss_att 38.543213 loss_ctc 42.454689 loss_rnnt 27.753416 hw_loss 0.305009 lr 0.00098474 rank 4
2023-02-17 08:23:42,058 DEBUG TRAIN Batch 3/800 loss 31.423677 loss_att 41.798962 loss_ctc 61.490383 loss_rnnt 25.140280 hw_loss 0.373966 lr 0.00098336 rank 2
2023-02-17 08:24:58,180 DEBUG TRAIN Batch 3/900 loss 21.367098 loss_att 26.155920 loss_ctc 32.597168 loss_rnnt 18.706282 hw_loss 0.385709 lr 0.00098253 rank 6
2023-02-17 08:24:58,185 DEBUG TRAIN Batch 3/900 loss 29.096785 loss_att 38.415226 loss_ctc 51.660988 loss_rnnt 24.041908 hw_loss 0.342423 lr 0.00098283 rank 4
2023-02-17 08:24:58,185 DEBUG TRAIN Batch 3/900 loss 30.449240 loss_att 34.693756 loss_ctc 42.683147 loss_rnnt 27.812199 hw_loss 0.294280 lr 0.00098209 rank 3
2023-02-17 08:24:58,186 DEBUG TRAIN Batch 3/900 loss 30.076092 loss_att 40.093170 loss_ctc 44.382545 loss_rnnt 25.966854 hw_loss 0.371803 lr 0.00098170 rank 5
2023-02-17 08:24:58,189 DEBUG TRAIN Batch 3/900 loss 24.036390 loss_att 32.766468 loss_ctc 37.934715 loss_rnnt 20.265896 hw_loss 0.321319 lr 0.00098147 rank 2
2023-02-17 08:24:58,189 DEBUG TRAIN Batch 3/900 loss 37.659908 loss_att 50.229343 loss_ctc 56.527847 loss_rnnt 32.458691 hw_loss 0.321758 lr 0.00098147 rank 1
2023-02-17 08:24:58,191 DEBUG TRAIN Batch 3/900 loss 24.386683 loss_att 40.664619 loss_ctc 48.134056 loss_rnnt 17.841091 hw_loss 0.231913 lr 0.00098249 rank 7
2023-02-17 08:24:58,235 DEBUG TRAIN Batch 3/900 loss 33.335194 loss_att 43.627747 loss_ctc 47.593918 loss_rnnt 29.206417 hw_loss 0.317070 lr 0.00098168 rank 0
2023-02-17 08:26:14,524 DEBUG TRAIN Batch 3/1000 loss 34.932301 loss_att 40.421619 loss_ctc 49.497231 loss_rnnt 31.722990 hw_loss 0.317730 lr 0.00097981 rank 5
2023-02-17 08:26:14,527 DEBUG TRAIN Batch 3/1000 loss 37.328518 loss_att 44.343151 loss_ctc 58.437294 loss_rnnt 32.931000 hw_loss 0.337669 lr 0.00097958 rank 2
2023-02-17 08:26:14,527 DEBUG TRAIN Batch 3/1000 loss 37.925102 loss_att 42.211460 loss_ctc 52.148041 loss_rnnt 34.934448 hw_loss 0.444355 lr 0.00098094 rank 4
2023-02-17 08:26:14,527 DEBUG TRAIN Batch 3/1000 loss 36.441151 loss_att 46.412777 loss_ctc 52.763088 loss_rnnt 32.043732 hw_loss 0.425306 lr 0.00098020 rank 3
2023-02-17 08:26:14,530 DEBUG TRAIN Batch 3/1000 loss 49.579403 loss_att 55.894356 loss_ctc 76.230843 loss_rnnt 44.587788 hw_loss 0.328305 lr 0.00098060 rank 7
2023-02-17 08:26:14,530 DEBUG TRAIN Batch 3/1000 loss 28.544905 loss_att 39.391510 loss_ctc 43.471588 loss_rnnt 24.198610 hw_loss 0.350155 lr 0.00097958 rank 1
2023-02-17 08:26:14,532 DEBUG TRAIN Batch 3/1000 loss 35.555325 loss_att 49.301315 loss_ctc 51.889099 loss_rnnt 30.465086 hw_loss 0.306001 lr 0.00098064 rank 6
2023-02-17 08:26:14,535 DEBUG TRAIN Batch 3/1000 loss 26.111025 loss_att 34.495090 loss_ctc 43.999680 loss_rnnt 21.857876 hw_loss 0.358471 lr 0.00097979 rank 0
2023-02-17 08:27:33,678 DEBUG TRAIN Batch 3/1100 loss 24.364950 loss_att 30.195347 loss_ctc 36.134285 loss_rnnt 21.429159 hw_loss 0.375876 lr 0.00097876 rank 6
2023-02-17 08:27:33,678 DEBUG TRAIN Batch 3/1100 loss 35.150204 loss_att 40.796833 loss_ctc 45.437904 loss_rnnt 32.478340 hw_loss 0.320333 lr 0.00097872 rank 7
2023-02-17 08:27:33,678 DEBUG TRAIN Batch 3/1100 loss 38.751335 loss_att 43.008339 loss_ctc 57.009087 loss_rnnt 35.294861 hw_loss 0.320070 lr 0.00097833 rank 3
2023-02-17 08:27:33,679 DEBUG TRAIN Batch 3/1100 loss 35.884090 loss_att 44.687199 loss_ctc 52.252644 loss_rnnt 31.683250 hw_loss 0.483279 lr 0.00097793 rank 5
2023-02-17 08:27:33,680 DEBUG TRAIN Batch 3/1100 loss 42.148266 loss_att 48.255386 loss_ctc 62.878433 loss_rnnt 37.932091 hw_loss 0.432615 lr 0.00097771 rank 1
2023-02-17 08:27:33,684 DEBUG TRAIN Batch 3/1100 loss 43.832047 loss_att 51.892303 loss_ctc 63.878128 loss_rnnt 39.364014 hw_loss 0.343441 lr 0.00097791 rank 0
2023-02-17 08:27:33,686 DEBUG TRAIN Batch 3/1100 loss 29.544514 loss_att 37.317356 loss_ctc 42.710945 loss_rnnt 26.004047 hw_loss 0.431949 lr 0.00097771 rank 2
2023-02-17 08:27:33,687 DEBUG TRAIN Batch 3/1100 loss 43.443691 loss_att 48.105434 loss_ctc 55.294464 loss_rnnt 40.760811 hw_loss 0.319549 lr 0.00097906 rank 4
2023-02-17 08:28:50,083 DEBUG TRAIN Batch 3/1200 loss 21.171408 loss_att 24.421614 loss_ctc 29.464926 loss_rnnt 19.238533 hw_loss 0.331929 lr 0.00097584 rank 1
2023-02-17 08:28:50,085 DEBUG TRAIN Batch 3/1200 loss 39.988861 loss_att 43.622810 loss_ctc 56.459106 loss_rnnt 36.895844 hw_loss 0.319106 lr 0.00097689 rank 6
2023-02-17 08:28:50,086 DEBUG TRAIN Batch 3/1200 loss 35.405560 loss_att 46.430714 loss_ctc 46.477520 loss_rnnt 31.549431 hw_loss 0.327812 lr 0.00097685 rank 7
2023-02-17 08:28:50,086 DEBUG TRAIN Batch 3/1200 loss 17.473812 loss_att 22.584822 loss_ctc 30.091797 loss_rnnt 14.510619 hw_loss 0.484861 lr 0.00097607 rank 5
2023-02-17 08:28:50,090 DEBUG TRAIN Batch 3/1200 loss 19.399439 loss_att 27.164734 loss_ctc 29.039066 loss_rnnt 16.404539 hw_loss 0.293546 lr 0.00097646 rank 3
2023-02-17 08:28:50,090 DEBUG TRAIN Batch 3/1200 loss 30.292833 loss_att 35.163048 loss_ctc 42.889988 loss_rnnt 27.441755 hw_loss 0.370151 lr 0.00097605 rank 0
2023-02-17 08:28:50,091 DEBUG TRAIN Batch 3/1200 loss 42.630520 loss_att 50.883949 loss_ctc 64.388512 loss_rnnt 37.883553 hw_loss 0.366030 lr 0.00097584 rank 2
2023-02-17 08:28:50,127 DEBUG TRAIN Batch 3/1200 loss 20.194229 loss_att 24.464830 loss_ctc 32.471668 loss_rnnt 17.541218 hw_loss 0.303560 lr 0.00097719 rank 4
2023-02-17 08:30:05,821 DEBUG TRAIN Batch 3/1300 loss 49.399445 loss_att 62.543251 loss_ctc 87.358322 loss_rnnt 41.502396 hw_loss 0.388321 lr 0.00097503 rank 6
2023-02-17 08:30:05,825 DEBUG TRAIN Batch 3/1300 loss 29.788647 loss_att 36.181019 loss_ctc 39.948509 loss_rnnt 26.964346 hw_loss 0.358461 lr 0.00097499 rank 7
2023-02-17 08:30:05,828 DEBUG TRAIN Batch 3/1300 loss 55.250729 loss_att 66.168709 loss_ctc 82.596375 loss_rnnt 49.266235 hw_loss 0.290268 lr 0.00097460 rank 3
2023-02-17 08:30:05,828 DEBUG TRAIN Batch 3/1300 loss 30.477533 loss_att 40.694717 loss_ctc 52.007881 loss_rnnt 25.388786 hw_loss 0.327370 lr 0.00097399 rank 2
2023-02-17 08:30:05,828 DEBUG TRAIN Batch 3/1300 loss 13.849186 loss_att 22.304707 loss_ctc 25.071808 loss_rnnt 10.507940 hw_loss 0.288360 lr 0.00097421 rank 5
2023-02-17 08:30:05,830 DEBUG TRAIN Batch 3/1300 loss 35.213650 loss_att 45.631771 loss_ctc 49.359093 loss_rnnt 31.039625 hw_loss 0.383142 lr 0.00097532 rank 4
2023-02-17 08:30:05,833 DEBUG TRAIN Batch 3/1300 loss 40.615875 loss_att 59.390923 loss_ctc 69.111626 loss_rnnt 32.900005 hw_loss 0.302680 lr 0.00097399 rank 1
2023-02-17 08:30:05,875 DEBUG TRAIN Batch 3/1300 loss 78.317513 loss_att 100.634216 loss_ctc 115.026917 loss_rnnt 68.798096 hw_loss 0.302800 lr 0.00097419 rank 0
2023-02-17 08:31:23,236 DEBUG TRAIN Batch 3/1400 loss 31.401804 loss_att 43.832554 loss_ctc 51.001244 loss_rnnt 26.130653 hw_loss 0.322014 lr 0.00097276 rank 3
2023-02-17 08:31:23,237 DEBUG TRAIN Batch 3/1400 loss 43.067005 loss_att 50.198467 loss_ctc 69.630753 loss_rnnt 37.892033 hw_loss 0.387840 lr 0.00097237 rank 5
2023-02-17 08:31:23,238 DEBUG TRAIN Batch 3/1400 loss 27.849215 loss_att 39.056660 loss_ctc 38.369930 loss_rnnt 24.054028 hw_loss 0.283006 lr 0.00097347 rank 4
2023-02-17 08:31:23,243 DEBUG TRAIN Batch 3/1400 loss 34.083866 loss_att 42.735401 loss_ctc 49.560738 loss_rnnt 30.137411 hw_loss 0.286062 lr 0.00097235 rank 0
2023-02-17 08:31:23,244 DEBUG TRAIN Batch 3/1400 loss 38.349155 loss_att 46.188824 loss_ctc 59.393108 loss_rnnt 33.808907 hw_loss 0.312111 lr 0.00097318 rank 6
2023-02-17 08:31:23,244 DEBUG TRAIN Batch 3/1400 loss 20.777882 loss_att 31.228979 loss_ctc 36.269257 loss_rnnt 16.392715 hw_loss 0.430181 lr 0.00097215 rank 2
2023-02-17 08:31:23,246 DEBUG TRAIN Batch 3/1400 loss 27.268200 loss_att 36.290192 loss_ctc 42.620121 loss_rnnt 23.232977 hw_loss 0.344813 lr 0.00097215 rank 1
2023-02-17 08:31:23,248 DEBUG TRAIN Batch 3/1400 loss 17.213657 loss_att 22.996912 loss_ctc 22.167288 loss_rnnt 15.175190 hw_loss 0.414997 lr 0.00097314 rank 7
2023-02-17 08:32:39,516 DEBUG TRAIN Batch 3/1500 loss 23.617264 loss_att 28.621103 loss_ctc 30.985550 loss_rnnt 21.399399 hw_loss 0.439988 lr 0.00097054 rank 5
2023-02-17 08:32:39,516 DEBUG TRAIN Batch 3/1500 loss 25.306482 loss_att 31.033737 loss_ctc 42.653908 loss_rnnt 21.688009 hw_loss 0.300062 lr 0.00097032 rank 1
2023-02-17 08:32:39,518 DEBUG TRAIN Batch 3/1500 loss 32.060402 loss_att 41.884087 loss_ctc 48.602264 loss_rnnt 27.736610 hw_loss 0.287757 lr 0.00097163 rank 4
2023-02-17 08:32:39,519 DEBUG TRAIN Batch 3/1500 loss 32.377144 loss_att 37.185837 loss_ctc 51.888165 loss_rnnt 28.668518 hw_loss 0.272660 lr 0.00097130 rank 7
2023-02-17 08:32:39,521 DEBUG TRAIN Batch 3/1500 loss 31.830502 loss_att 41.694256 loss_ctc 46.949303 loss_rnnt 27.626745 hw_loss 0.403431 lr 0.00097134 rank 6
2023-02-17 08:32:39,522 DEBUG TRAIN Batch 3/1500 loss 18.718597 loss_att 21.502596 loss_ctc 26.284393 loss_rnnt 16.968513 hw_loss 0.345959 lr 0.00097032 rank 2
2023-02-17 08:32:39,523 DEBUG TRAIN Batch 3/1500 loss 22.916533 loss_att 25.702974 loss_ctc 34.596920 loss_rnnt 20.602121 hw_loss 0.374509 lr 0.00097052 rank 0
2023-02-17 08:32:39,523 DEBUG TRAIN Batch 3/1500 loss 38.313675 loss_att 46.614521 loss_ctc 57.654022 loss_rnnt 33.922295 hw_loss 0.285929 lr 0.00097092 rank 3
2023-02-17 08:33:54,067 DEBUG TRAIN Batch 3/1600 loss 38.024021 loss_att 50.721352 loss_ctc 61.771915 loss_rnnt 32.139950 hw_loss 0.334158 lr 0.00096871 rank 5
2023-02-17 08:33:54,069 DEBUG TRAIN Batch 3/1600 loss 21.869808 loss_att 27.543072 loss_ctc 37.515442 loss_rnnt 18.489290 hw_loss 0.299589 lr 0.00096849 rank 1
2023-02-17 08:33:54,070 DEBUG TRAIN Batch 3/1600 loss 29.294724 loss_att 37.597584 loss_ctc 48.329510 loss_rnnt 24.937321 hw_loss 0.297861 lr 0.00096909 rank 3
2023-02-17 08:33:54,070 DEBUG TRAIN Batch 3/1600 loss 25.059740 loss_att 32.441608 loss_ctc 38.631054 loss_rnnt 21.618217 hw_loss 0.291824 lr 0.00096951 rank 6
2023-02-17 08:33:54,072 DEBUG TRAIN Batch 3/1600 loss 48.482220 loss_att 58.801865 loss_ctc 75.131180 loss_rnnt 42.730827 hw_loss 0.251760 lr 0.00096948 rank 7
2023-02-17 08:33:54,073 DEBUG TRAIN Batch 3/1600 loss 29.596355 loss_att 36.985828 loss_ctc 51.218323 loss_rnnt 25.090816 hw_loss 0.271343 lr 0.00096980 rank 4
2023-02-17 08:33:54,073 DEBUG TRAIN Batch 3/1600 loss 31.305462 loss_att 44.781845 loss_ctc 57.826057 loss_rnnt 24.958534 hw_loss 0.216692 lr 0.00096869 rank 0
2023-02-17 08:33:54,074 DEBUG TRAIN Batch 3/1600 loss 27.530930 loss_att 34.675022 loss_ctc 49.042793 loss_rnnt 23.082790 hw_loss 0.283258 lr 0.00096849 rank 2
2023-02-17 08:35:11,205 DEBUG TRAIN Batch 3/1700 loss 26.332283 loss_att 40.664150 loss_ctc 47.140648 loss_rnnt 20.511269 hw_loss 0.337856 lr 0.00096770 rank 6
2023-02-17 08:35:11,206 DEBUG TRAIN Batch 3/1700 loss 37.403553 loss_att 44.277172 loss_ctc 49.973392 loss_rnnt 34.140236 hw_loss 0.398658 lr 0.00096668 rank 1
2023-02-17 08:35:11,208 DEBUG TRAIN Batch 3/1700 loss 37.383804 loss_att 41.506512 loss_ctc 58.132278 loss_rnnt 33.620304 hw_loss 0.323430 lr 0.00096690 rank 5
2023-02-17 08:35:11,210 DEBUG TRAIN Batch 3/1700 loss 41.447350 loss_att 52.194775 loss_ctc 62.178097 loss_rnnt 36.387928 hw_loss 0.273435 lr 0.00096728 rank 3
2023-02-17 08:35:11,210 DEBUG TRAIN Batch 3/1700 loss 21.268316 loss_att 29.697582 loss_ctc 33.583618 loss_rnnt 17.788635 hw_loss 0.284599 lr 0.00096766 rank 7
2023-02-17 08:35:11,214 DEBUG TRAIN Batch 3/1700 loss 48.453396 loss_att 58.774452 loss_ctc 66.483025 loss_rnnt 43.766365 hw_loss 0.410376 lr 0.00096688 rank 0
2023-02-17 08:35:11,215 DEBUG TRAIN Batch 3/1700 loss 27.090748 loss_att 32.689011 loss_ctc 49.222816 loss_rnnt 22.842445 hw_loss 0.333203 lr 0.00096668 rank 2
2023-02-17 08:35:11,219 DEBUG TRAIN Batch 3/1700 loss 27.278585 loss_att 33.013718 loss_ctc 43.923912 loss_rnnt 23.740925 hw_loss 0.321107 lr 0.00096799 rank 4
2023-02-17 08:36:30,903 DEBUG TRAIN Batch 3/1800 loss 24.201040 loss_att 29.325300 loss_ctc 34.362961 loss_rnnt 21.640701 hw_loss 0.338559 lr 0.00096618 rank 4
2023-02-17 08:36:30,903 DEBUG TRAIN Batch 3/1800 loss 34.375187 loss_att 40.891724 loss_ctc 51.217426 loss_rnnt 30.654758 hw_loss 0.321543 lr 0.00096589 rank 6
2023-02-17 08:36:30,906 DEBUG TRAIN Batch 3/1800 loss 38.943481 loss_att 50.854431 loss_ctc 61.879250 loss_rnnt 33.336742 hw_loss 0.312091 lr 0.00096547 rank 3
2023-02-17 08:36:30,907 DEBUG TRAIN Batch 3/1800 loss 14.593724 loss_att 17.736383 loss_ctc 27.156927 loss_rnnt 12.096071 hw_loss 0.363800 lr 0.00096510 rank 5
2023-02-17 08:36:30,909 DEBUG TRAIN Batch 3/1800 loss 44.139084 loss_att 53.019718 loss_ctc 64.951706 loss_rnnt 39.433693 hw_loss 0.289208 lr 0.00096585 rank 7
2023-02-17 08:36:30,909 DEBUG TRAIN Batch 3/1800 loss 20.696123 loss_att 22.248579 loss_ctc 33.526344 loss_rnnt 18.453501 hw_loss 0.415186 lr 0.00096488 rank 1
2023-02-17 08:36:30,912 DEBUG TRAIN Batch 3/1800 loss 27.271168 loss_att 29.970161 loss_ctc 40.551697 loss_rnnt 24.808462 hw_loss 0.285321 lr 0.00096508 rank 0
2023-02-17 08:36:30,958 DEBUG TRAIN Batch 3/1800 loss 30.492956 loss_att 32.944958 loss_ctc 48.063198 loss_rnnt 27.406469 hw_loss 0.475100 lr 0.00096488 rank 2
2023-02-17 08:37:49,367 DEBUG TRAIN Batch 3/1900 loss 27.763542 loss_att 30.733200 loss_ctc 40.693474 loss_rnnt 25.252047 hw_loss 0.362948 lr 0.00096409 rank 6
2023-02-17 08:37:49,368 DEBUG TRAIN Batch 3/1900 loss 31.080465 loss_att 40.359837 loss_ctc 48.014591 loss_rnnt 26.783701 hw_loss 0.343131 lr 0.00096406 rank 7
2023-02-17 08:37:49,372 DEBUG TRAIN Batch 3/1900 loss 26.343248 loss_att 32.187328 loss_ctc 42.538383 loss_rnnt 22.773993 hw_loss 0.452046 lr 0.00096309 rank 2
2023-02-17 08:37:49,373 DEBUG TRAIN Batch 3/1900 loss 30.239435 loss_att 27.749849 loss_ctc 40.942757 loss_rnnt 29.057302 hw_loss 0.474263 lr 0.00096438 rank 4
2023-02-17 08:37:49,374 DEBUG TRAIN Batch 3/1900 loss 17.411749 loss_att 19.704193 loss_ctc 24.902702 loss_rnnt 15.731928 hw_loss 0.417256 lr 0.00096368 rank 3
2023-02-17 08:37:49,377 DEBUG TRAIN Batch 3/1900 loss 47.171970 loss_att 56.426559 loss_ctc 66.650368 loss_rnnt 42.566681 hw_loss 0.294850 lr 0.00096309 rank 1
2023-02-17 08:37:49,377 DEBUG TRAIN Batch 3/1900 loss 18.122766 loss_att 19.739370 loss_ctc 24.056469 loss_rnnt 16.757999 hw_loss 0.469285 lr 0.00096330 rank 5
2023-02-17 08:37:49,423 DEBUG TRAIN Batch 3/1900 loss 19.912807 loss_att 22.112118 loss_ctc 25.753798 loss_rnnt 18.403326 hw_loss 0.545285 lr 0.00096329 rank 0
2023-02-17 08:39:06,797 DEBUG TRAIN Batch 3/2000 loss 37.199982 loss_att 47.640614 loss_ctc 57.507622 loss_rnnt 32.270065 hw_loss 0.251443 lr 0.00096230 rank 6
2023-02-17 08:39:06,804 DEBUG TRAIN Batch 3/2000 loss 49.599552 loss_att 54.415672 loss_ctc 65.691757 loss_rnnt 46.345348 hw_loss 0.272532 lr 0.00096152 rank 5
2023-02-17 08:39:06,805 DEBUG TRAIN Batch 3/2000 loss 54.871292 loss_att 65.271301 loss_ctc 83.858978 loss_rnnt 48.731926 hw_loss 0.364392 lr 0.00096131 rank 2
2023-02-17 08:39:06,805 DEBUG TRAIN Batch 3/2000 loss 32.620007 loss_att 39.769478 loss_ctc 58.194702 loss_rnnt 27.671743 hw_loss 0.203278 lr 0.00096227 rank 7
2023-02-17 08:39:06,809 DEBUG TRAIN Batch 3/2000 loss 18.126331 loss_att 26.340981 loss_ctc 34.126732 loss_rnnt 14.155639 hw_loss 0.364456 lr 0.00096259 rank 4
2023-02-17 08:39:06,809 DEBUG TRAIN Batch 3/2000 loss 20.557581 loss_att 31.303616 loss_ctc 33.322559 loss_rnnt 16.545862 hw_loss 0.300965 lr 0.00096189 rank 3
2023-02-17 08:39:06,810 DEBUG TRAIN Batch 3/2000 loss 35.622108 loss_att 43.135967 loss_ctc 62.951096 loss_rnnt 30.332790 hw_loss 0.267538 lr 0.00096150 rank 0
2023-02-17 08:39:06,815 DEBUG TRAIN Batch 3/2000 loss 44.377983 loss_att 55.460476 loss_ctc 71.356949 loss_rnnt 38.357918 hw_loss 0.386940 lr 0.00096131 rank 1
2023-02-17 08:40:27,184 DEBUG TRAIN Batch 3/2100 loss 33.238552 loss_att 42.407280 loss_ctc 46.513958 loss_rnnt 29.468178 hw_loss 0.312327 lr 0.00095975 rank 5
2023-02-17 08:40:27,186 DEBUG TRAIN Batch 3/2100 loss 29.671352 loss_att 37.682831 loss_ctc 48.016762 loss_rnnt 25.455540 hw_loss 0.313989 lr 0.00096053 rank 6
2023-02-17 08:40:27,188 DEBUG TRAIN Batch 3/2100 loss 31.562180 loss_att 37.659294 loss_ctc 48.128063 loss_rnnt 27.952667 hw_loss 0.339947 lr 0.00096049 rank 7
2023-02-17 08:40:27,189 DEBUG TRAIN Batch 3/2100 loss 48.769798 loss_att 65.429039 loss_ctc 82.213852 loss_rnnt 40.790245 hw_loss 0.353430 lr 0.00096012 rank 3
2023-02-17 08:40:27,191 DEBUG TRAIN Batch 3/2100 loss 16.916187 loss_att 29.347710 loss_ctc 32.736237 loss_rnnt 12.118246 hw_loss 0.379303 lr 0.00095954 rank 1
2023-02-17 08:40:27,190 DEBUG TRAIN Batch 3/2100 loss 23.965874 loss_att 33.181156 loss_ctc 37.396835 loss_rnnt 20.158604 hw_loss 0.325153 lr 0.00095954 rank 2
2023-02-17 08:40:27,191 DEBUG TRAIN Batch 3/2100 loss 37.759789 loss_att 46.822128 loss_ctc 52.648071 loss_rnnt 33.815487 hw_loss 0.275119 lr 0.00096081 rank 4
2023-02-17 08:40:27,194 DEBUG TRAIN Batch 3/2100 loss 38.734901 loss_att 50.306732 loss_ctc 59.465927 loss_rnnt 33.509663 hw_loss 0.275123 lr 0.00095973 rank 0
2023-02-17 08:41:44,686 DEBUG TRAIN Batch 3/2200 loss 34.648140 loss_att 41.327442 loss_ctc 50.876793 loss_rnnt 31.001308 hw_loss 0.275912 lr 0.00095876 rank 6
2023-02-17 08:41:44,687 DEBUG TRAIN Batch 3/2200 loss 45.043224 loss_att 53.418839 loss_ctc 73.731194 loss_rnnt 39.333183 hw_loss 0.393475 lr 0.00095835 rank 3
2023-02-17 08:41:44,689 DEBUG TRAIN Batch 3/2200 loss 46.882580 loss_att 59.524544 loss_ctc 68.438187 loss_rnnt 41.319450 hw_loss 0.301234 lr 0.00095904 rank 4
2023-02-17 08:41:44,689 DEBUG TRAIN Batch 3/2200 loss 40.413872 loss_att 48.869976 loss_ctc 60.991440 loss_rnnt 35.789284 hw_loss 0.355667 lr 0.00095777 rank 1
2023-02-17 08:41:44,689 DEBUG TRAIN Batch 3/2200 loss 39.298138 loss_att 44.538750 loss_ctc 60.146191 loss_rnnt 35.258087 hw_loss 0.397850 lr 0.00095872 rank 7
2023-02-17 08:41:44,691 DEBUG TRAIN Batch 3/2200 loss 30.490690 loss_att 32.220730 loss_ctc 43.198860 loss_rnnt 28.239864 hw_loss 0.394494 lr 0.00095798 rank 5
2023-02-17 08:41:44,693 DEBUG TRAIN Batch 3/2200 loss 17.012184 loss_att 24.242723 loss_ctc 30.815338 loss_rnnt 13.555276 hw_loss 0.319462 lr 0.00095777 rank 2
2023-02-17 08:41:44,694 DEBUG TRAIN Batch 3/2200 loss 25.564260 loss_att 35.423035 loss_ctc 44.533081 loss_rnnt 20.871078 hw_loss 0.360471 lr 0.00095797 rank 0
2023-02-17 08:43:00,286 DEBUG TRAIN Batch 3/2300 loss 51.376312 loss_att 58.917267 loss_ctc 71.714035 loss_rnnt 47.005730 hw_loss 0.282566 lr 0.00095700 rank 6
2023-02-17 08:43:00,288 DEBUG TRAIN Batch 3/2300 loss 35.109638 loss_att 40.890118 loss_ctc 51.726059 loss_rnnt 31.549568 hw_loss 0.353349 lr 0.00095697 rank 7
2023-02-17 08:43:00,289 DEBUG TRAIN Batch 3/2300 loss 20.179064 loss_att 28.968784 loss_ctc 35.752338 loss_rnnt 16.147001 hw_loss 0.370654 lr 0.00095602 rank 1
2023-02-17 08:43:00,290 DEBUG TRAIN Batch 3/2300 loss 39.023064 loss_att 48.020618 loss_ctc 59.929951 loss_rnnt 34.262062 hw_loss 0.326070 lr 0.00095660 rank 3
2023-02-17 08:43:00,292 DEBUG TRAIN Batch 3/2300 loss 39.740086 loss_att 49.480522 loss_ctc 57.130737 loss_rnnt 35.280224 hw_loss 0.361916 lr 0.00095602 rank 2
2023-02-17 08:43:00,292 DEBUG TRAIN Batch 3/2300 loss 16.263828 loss_att 22.608637 loss_ctc 29.987034 loss_rnnt 12.960044 hw_loss 0.384490 lr 0.00095623 rank 5
2023-02-17 08:43:00,294 DEBUG TRAIN Batch 3/2300 loss 30.642292 loss_att 38.954716 loss_ctc 47.102859 loss_rnnt 26.554804 hw_loss 0.431737 lr 0.00095728 rank 4
2023-02-17 08:43:00,343 DEBUG TRAIN Batch 3/2300 loss 50.476757 loss_att 56.185719 loss_ctc 69.242706 loss_rnnt 46.587181 hw_loss 0.460610 lr 0.00095621 rank 0
2023-02-17 08:44:15,791 DEBUG TRAIN Batch 3/2400 loss 32.716412 loss_att 43.597015 loss_ctc 53.640038 loss_rnnt 27.493748 hw_loss 0.481361 lr 0.00095485 rank 3
2023-02-17 08:44:15,793 DEBUG TRAIN Batch 3/2400 loss 25.304178 loss_att 31.733910 loss_ctc 41.999249 loss_rnnt 21.603834 hw_loss 0.353230 lr 0.00095522 rank 7
2023-02-17 08:44:15,795 DEBUG TRAIN Batch 3/2400 loss 41.462177 loss_att 50.585075 loss_ctc 64.672134 loss_rnnt 36.385666 hw_loss 0.294889 lr 0.00095525 rank 6
2023-02-17 08:44:15,795 DEBUG TRAIN Batch 3/2400 loss 39.071342 loss_att 51.398499 loss_ctc 69.727409 loss_rnnt 32.338642 hw_loss 0.337116 lr 0.00095449 rank 5
2023-02-17 08:44:15,796 DEBUG TRAIN Batch 3/2400 loss 32.761364 loss_att 40.983749 loss_ctc 54.505619 loss_rnnt 27.980076 hw_loss 0.445456 lr 0.00095447 rank 0
2023-02-17 08:44:15,797 DEBUG TRAIN Batch 3/2400 loss 30.293221 loss_att 37.026035 loss_ctc 39.948593 loss_rnnt 27.393187 hw_loss 0.498914 lr 0.00095428 rank 2
2023-02-17 08:44:15,802 DEBUG TRAIN Batch 3/2400 loss 36.065815 loss_att 40.331715 loss_ctc 56.488983 loss_rnnt 32.271122 hw_loss 0.409536 lr 0.00095428 rank 1
2023-02-17 08:44:15,817 DEBUG TRAIN Batch 3/2400 loss 49.527161 loss_att 61.664833 loss_ctc 74.431671 loss_rnnt 43.624939 hw_loss 0.288911 lr 0.00095553 rank 4
2023-02-17 08:45:35,709 DEBUG TRAIN Batch 3/2500 loss 21.390303 loss_att 24.453815 loss_ctc 31.232082 loss_rnnt 19.200552 hw_loss 0.496521 lr 0.00095255 rank 1
2023-02-17 08:45:35,710 DEBUG TRAIN Batch 3/2500 loss 22.439648 loss_att 27.400288 loss_ctc 36.428574 loss_rnnt 19.376118 hw_loss 0.386649 lr 0.00095348 rank 7
2023-02-17 08:45:35,711 DEBUG TRAIN Batch 3/2500 loss 35.869972 loss_att 44.479336 loss_ctc 49.044380 loss_rnnt 32.209698 hw_loss 0.340902 lr 0.00095351 rank 6
2023-02-17 08:45:35,713 DEBUG TRAIN Batch 3/2500 loss 23.148558 loss_att 24.121960 loss_ctc 30.043406 loss_rnnt 21.850340 hw_loss 0.345422 lr 0.00095275 rank 5
2023-02-17 08:45:35,714 DEBUG TRAIN Batch 3/2500 loss 44.668842 loss_att 57.353081 loss_ctc 77.038727 loss_rnnt 37.666435 hw_loss 0.280458 lr 0.00095255 rank 2
2023-02-17 08:45:35,715 DEBUG TRAIN Batch 3/2500 loss 36.422691 loss_att 37.451244 loss_ctc 48.348709 loss_rnnt 34.392784 hw_loss 0.438863 lr 0.00095312 rank 3
2023-02-17 08:45:35,717 DEBUG TRAIN Batch 3/2500 loss 27.565710 loss_att 32.958668 loss_ctc 37.924835 loss_rnnt 24.946819 hw_loss 0.298282 lr 0.00095274 rank 0
2023-02-17 08:45:35,720 DEBUG TRAIN Batch 3/2500 loss 46.102608 loss_att 53.691994 loss_ctc 61.017849 loss_rnnt 42.336227 hw_loss 0.487134 lr 0.00095379 rank 4
2023-02-17 08:46:50,144 DEBUG TRAIN Batch 3/2600 loss 44.268295 loss_att 55.601082 loss_ctc 69.764297 loss_rnnt 38.418438 hw_loss 0.344692 lr 0.00095082 rank 1
2023-02-17 08:46:50,144 DEBUG TRAIN Batch 3/2600 loss 55.520554 loss_att 59.696266 loss_ctc 67.237679 loss_rnnt 52.970589 hw_loss 0.286004 lr 0.00095139 rank 3
2023-02-17 08:46:50,148 DEBUG TRAIN Batch 3/2600 loss 69.956787 loss_att 89.162079 loss_ctc 104.360252 loss_rnnt 61.397987 hw_loss 0.244886 lr 0.00095179 rank 6
2023-02-17 08:46:50,151 DEBUG TRAIN Batch 3/2600 loss 38.279964 loss_att 49.842400 loss_ctc 56.392742 loss_rnnt 33.345615 hw_loss 0.387796 lr 0.00095082 rank 2
2023-02-17 08:46:50,151 DEBUG TRAIN Batch 3/2600 loss 20.596556 loss_att 27.494007 loss_ctc 30.715797 loss_rnnt 17.646107 hw_loss 0.415734 lr 0.00095175 rank 7
2023-02-17 08:46:50,152 DEBUG TRAIN Batch 3/2600 loss 37.724712 loss_att 55.798126 loss_ctc 67.210495 loss_rnnt 30.033653 hw_loss 0.271756 lr 0.00095206 rank 4
2023-02-17 08:46:50,152 DEBUG TRAIN Batch 3/2600 loss 32.008171 loss_att 39.986588 loss_ctc 53.954266 loss_rnnt 27.293030 hw_loss 0.362463 lr 0.00095101 rank 0
2023-02-17 08:46:50,156 DEBUG TRAIN Batch 3/2600 loss 39.686474 loss_att 52.010979 loss_ctc 66.290329 loss_rnnt 33.456905 hw_loss 0.407788 lr 0.00095103 rank 5
2023-02-17 08:48:05,209 DEBUG TRAIN Batch 3/2700 loss 25.890831 loss_att 36.557339 loss_ctc 42.554775 loss_rnnt 21.418903 hw_loss 0.218937 lr 0.00095007 rank 6
2023-02-17 08:48:05,209 DEBUG TRAIN Batch 3/2700 loss 22.948544 loss_att 33.677979 loss_ctc 38.985188 loss_rnnt 18.492714 hw_loss 0.321981 lr 0.00095003 rank 7
2023-02-17 08:48:05,210 DEBUG TRAIN Batch 3/2700 loss 20.226223 loss_att 27.977808 loss_ctc 31.406889 loss_rnnt 17.048649 hw_loss 0.255938 lr 0.00094967 rank 3
2023-02-17 08:48:05,210 DEBUG TRAIN Batch 3/2700 loss 42.691929 loss_att 44.290001 loss_ctc 59.973167 loss_rnnt 39.878719 hw_loss 0.355173 lr 0.00094911 rank 1
2023-02-17 08:48:05,210 DEBUG TRAIN Batch 3/2700 loss 26.250509 loss_att 37.558735 loss_ctc 44.491573 loss_rnnt 21.462460 hw_loss 0.176746 lr 0.00094931 rank 5
2023-02-17 08:48:05,214 DEBUG TRAIN Batch 3/2700 loss 27.119322 loss_att 32.532272 loss_ctc 37.300774 loss_rnnt 24.482563 hw_loss 0.368702 lr 0.00094929 rank 0
2023-02-17 08:48:05,217 DEBUG TRAIN Batch 3/2700 loss 29.604136 loss_att 34.403709 loss_ctc 40.090881 loss_rnnt 27.023521 hw_loss 0.417122 lr 0.00094911 rank 2
2023-02-17 08:48:05,257 DEBUG TRAIN Batch 3/2700 loss 19.685234 loss_att 28.783741 loss_ctc 29.340178 loss_rnnt 16.401686 hw_loss 0.330976 lr 0.00095034 rank 4
2023-02-17 08:49:24,210 DEBUG TRAIN Batch 3/2800 loss 48.434460 loss_att 55.509285 loss_ctc 77.606644 loss_rnnt 42.937233 hw_loss 0.361184 lr 0.00094832 rank 7
2023-02-17 08:49:24,212 DEBUG TRAIN Batch 3/2800 loss 43.836708 loss_att 52.239544 loss_ctc 60.273071 loss_rnnt 39.749916 hw_loss 0.402580 lr 0.00094796 rank 3
2023-02-17 08:49:24,214 DEBUG TRAIN Batch 3/2800 loss 35.380787 loss_att 38.914772 loss_ctc 51.960815 loss_rnnt 32.268288 hw_loss 0.365686 lr 0.00094761 rank 5
2023-02-17 08:49:24,214 DEBUG TRAIN Batch 3/2800 loss 33.117195 loss_att 54.040848 loss_ctc 64.137817 loss_rnnt 24.552441 hw_loss 0.457390 lr 0.00094836 rank 6
2023-02-17 08:49:24,217 DEBUG TRAIN Batch 3/2800 loss 23.425600 loss_att 30.724380 loss_ctc 37.960091 loss_rnnt 19.830570 hw_loss 0.370012 lr 0.00094863 rank 4
2023-02-17 08:49:24,218 DEBUG TRAIN Batch 3/2800 loss 41.279854 loss_att 53.782833 loss_ctc 58.747391 loss_rnnt 36.242641 hw_loss 0.389267 lr 0.00094740 rank 1
2023-02-17 08:49:24,220 DEBUG TRAIN Batch 3/2800 loss 46.216396 loss_att 53.210247 loss_ctc 73.400146 loss_rnnt 41.022556 hw_loss 0.319815 lr 0.00094759 rank 0
2023-02-17 08:49:24,222 DEBUG TRAIN Batch 3/2800 loss 19.625013 loss_att 28.247372 loss_ctc 33.230003 loss_rnnt 15.923973 hw_loss 0.304816 lr 0.00094740 rank 2
2023-02-17 08:50:42,560 DEBUG TRAIN Batch 3/2900 loss 19.114832 loss_att 26.269203 loss_ctc 30.096460 loss_rnnt 16.016205 hw_loss 0.381625 lr 0.00094662 rank 7
2023-02-17 08:50:42,563 DEBUG TRAIN Batch 3/2900 loss 55.427471 loss_att 71.025414 loss_ctc 74.408409 loss_rnnt 49.614281 hw_loss 0.305274 lr 0.00094626 rank 3
2023-02-17 08:50:42,565 DEBUG TRAIN Batch 3/2900 loss 32.169968 loss_att 37.595665 loss_ctc 46.392738 loss_rnnt 29.009720 hw_loss 0.335128 lr 0.00094665 rank 6
2023-02-17 08:50:42,567 DEBUG TRAIN Batch 3/2900 loss 19.234295 loss_att 26.327999 loss_ctc 26.852112 loss_rnnt 16.601837 hw_loss 0.371265 lr 0.00094589 rank 0
2023-02-17 08:50:42,567 DEBUG TRAIN Batch 3/2900 loss 26.895580 loss_att 37.873386 loss_ctc 51.531319 loss_rnnt 21.213776 hw_loss 0.377769 lr 0.00094591 rank 5
2023-02-17 08:50:42,568 DEBUG TRAIN Batch 3/2900 loss 30.673168 loss_att 40.945648 loss_ctc 49.058781 loss_rnnt 25.982643 hw_loss 0.346152 lr 0.00094693 rank 4
2023-02-17 08:50:42,573 DEBUG TRAIN Batch 3/2900 loss 26.821037 loss_att 32.428101 loss_ctc 39.007332 loss_rnnt 23.902359 hw_loss 0.323294 lr 0.00094571 rank 1
2023-02-17 08:50:42,573 DEBUG TRAIN Batch 3/2900 loss 22.198282 loss_att 33.483688 loss_ctc 36.558823 loss_rnnt 17.855686 hw_loss 0.320199 lr 0.00094571 rank 2
2023-02-17 08:51:57,482 DEBUG TRAIN Batch 3/3000 loss 22.503689 loss_att 30.172457 loss_ctc 40.160660 loss_rnnt 18.450562 hw_loss 0.309580 lr 0.00094496 rank 6
2023-02-17 08:51:57,487 DEBUG TRAIN Batch 3/3000 loss 37.825218 loss_att 44.515667 loss_ctc 54.058788 loss_rnnt 34.144043 hw_loss 0.334889 lr 0.00094420 rank 0
2023-02-17 08:51:57,487 DEBUG TRAIN Batch 3/3000 loss 30.724642 loss_att 42.002052 loss_ctc 51.877953 loss_rnnt 25.488138 hw_loss 0.301083 lr 0.00094422 rank 5
2023-02-17 08:51:57,489 DEBUG TRAIN Batch 3/3000 loss 29.648046 loss_att 40.253788 loss_ctc 45.814552 loss_rnnt 25.214857 hw_loss 0.293448 lr 0.00094493 rank 7
2023-02-17 08:51:57,489 DEBUG TRAIN Batch 3/3000 loss 21.429489 loss_att 24.467487 loss_ctc 32.870934 loss_rnnt 19.120733 hw_loss 0.329305 lr 0.00094402 rank 1
2023-02-17 08:51:57,490 DEBUG TRAIN Batch 3/3000 loss 16.117451 loss_att 24.750679 loss_ctc 32.023247 loss_rnnt 12.113519 hw_loss 0.293462 lr 0.00094457 rank 3
2023-02-17 08:51:57,490 DEBUG TRAIN Batch 3/3000 loss 26.579920 loss_att 32.975430 loss_ctc 50.482494 loss_rnnt 21.931034 hw_loss 0.342697 lr 0.00094402 rank 2
2023-02-17 08:51:57,493 DEBUG TRAIN Batch 3/3000 loss 20.222588 loss_att 28.856165 loss_ctc 34.211231 loss_rnnt 16.463516 hw_loss 0.313507 lr 0.00094523 rank 4
2023-02-17 08:53:15,221 DEBUG TRAIN Batch 3/3100 loss 25.173626 loss_att 26.012506 loss_ctc 32.744560 loss_rnnt 23.793501 hw_loss 0.380420 lr 0.00094234 rank 1
2023-02-17 08:53:15,221 DEBUG TRAIN Batch 3/3100 loss 34.971886 loss_att 42.185390 loss_ctc 52.740070 loss_rnnt 30.995316 hw_loss 0.308950 lr 0.00094254 rank 5
2023-02-17 08:53:15,223 DEBUG TRAIN Batch 3/3100 loss 33.016228 loss_att 40.364262 loss_ctc 53.192291 loss_rnnt 28.675097 hw_loss 0.340086 lr 0.00094328 rank 6
2023-02-17 08:53:15,224 DEBUG TRAIN Batch 3/3100 loss 32.184761 loss_att 41.955109 loss_ctc 44.892162 loss_rnnt 28.384762 hw_loss 0.284275 lr 0.00094325 rank 7
2023-02-17 08:53:15,225 DEBUG TRAIN Batch 3/3100 loss 28.456762 loss_att 32.362854 loss_ctc 44.833881 loss_rnnt 25.264551 hw_loss 0.426327 lr 0.00094289 rank 3
2023-02-17 08:53:15,225 DEBUG TRAIN Batch 3/3100 loss 22.942759 loss_att 23.810137 loss_ctc 32.150688 loss_rnnt 21.300400 hw_loss 0.452176 lr 0.00094234 rank 2
2023-02-17 08:53:15,228 DEBUG TRAIN Batch 3/3100 loss 20.708527 loss_att 24.798471 loss_ctc 35.093868 loss_rnnt 17.784098 hw_loss 0.353240 lr 0.00094252 rank 0
2023-02-17 08:53:15,272 DEBUG TRAIN Batch 3/3100 loss 25.957445 loss_att 27.083763 loss_ctc 36.255848 loss_rnnt 24.119671 hw_loss 0.448858 lr 0.00094355 rank 4
2023-02-17 08:54:35,030 DEBUG TRAIN Batch 3/3200 loss 25.603056 loss_att 27.721706 loss_ctc 37.759197 loss_rnnt 23.313881 hw_loss 0.458676 lr 0.00094160 rank 6
2023-02-17 08:54:35,032 DEBUG TRAIN Batch 3/3200 loss 41.732662 loss_att 49.623245 loss_ctc 67.930634 loss_rnnt 36.475353 hw_loss 0.349000 lr 0.00094122 rank 3
2023-02-17 08:54:35,034 DEBUG TRAIN Batch 3/3200 loss 39.736984 loss_att 47.541260 loss_ctc 60.109459 loss_rnnt 35.234695 hw_loss 0.422078 lr 0.00094157 rank 7
2023-02-17 08:54:35,035 DEBUG TRAIN Batch 3/3200 loss 24.963438 loss_att 27.534893 loss_ctc 38.280205 loss_rnnt 22.490274 hw_loss 0.343695 lr 0.00094087 rank 5
2023-02-17 08:54:35,049 DEBUG TRAIN Batch 3/3200 loss 25.577833 loss_att 35.214230 loss_ctc 35.958508 loss_rnnt 22.095440 hw_loss 0.320674 lr 0.00094067 rank 1
2023-02-17 08:54:35,056 DEBUG TRAIN Batch 3/3200 loss 30.259527 loss_att 39.334015 loss_ctc 44.914520 loss_rnnt 26.354925 hw_loss 0.254450 lr 0.00094067 rank 2
2023-02-17 08:54:35,062 DEBUG TRAIN Batch 3/3200 loss 30.705008 loss_att 42.791878 loss_ctc 49.510147 loss_rnnt 25.585560 hw_loss 0.365100 lr 0.00094085 rank 0
2023-02-17 08:54:35,083 DEBUG TRAIN Batch 3/3200 loss 28.715609 loss_att 30.001938 loss_ctc 38.644604 loss_rnnt 26.917860 hw_loss 0.406156 lr 0.00094187 rank 4
2023-02-17 08:55:51,354 DEBUG TRAIN Batch 3/3300 loss 19.612204 loss_att 26.880749 loss_ctc 28.571554 loss_rnnt 16.795570 hw_loss 0.315643 lr 0.00093901 rank 2
2023-02-17 08:55:51,355 DEBUG TRAIN Batch 3/3300 loss 28.606319 loss_att 38.382942 loss_ctc 44.597633 loss_rnnt 24.370682 hw_loss 0.277759 lr 0.00093919 rank 0
2023-02-17 08:55:51,355 DEBUG TRAIN Batch 3/3300 loss 17.929945 loss_att 27.495171 loss_ctc 31.461227 loss_rnnt 14.033306 hw_loss 0.336420 lr 0.00093956 rank 3
2023-02-17 08:55:51,357 DEBUG TRAIN Batch 3/3300 loss 29.155119 loss_att 38.331474 loss_ctc 41.436012 loss_rnnt 25.537186 hw_loss 0.272270 lr 0.00093921 rank 5
2023-02-17 08:55:51,357 DEBUG TRAIN Batch 3/3300 loss 20.022980 loss_att 24.286934 loss_ctc 34.228233 loss_rnnt 17.062141 hw_loss 0.401276 lr 0.00093991 rank 7
2023-02-17 08:55:51,357 DEBUG TRAIN Batch 3/3300 loss 42.958553 loss_att 50.677654 loss_ctc 58.748474 loss_rnnt 39.154751 hw_loss 0.289986 lr 0.00093994 rank 6
2023-02-17 08:55:51,358 DEBUG TRAIN Batch 3/3300 loss 33.464607 loss_att 42.096905 loss_ctc 56.283875 loss_rnnt 28.541359 hw_loss 0.289167 lr 0.00093901 rank 1
2023-02-17 08:55:51,360 DEBUG TRAIN Batch 3/3300 loss 55.489922 loss_att 65.625084 loss_ctc 77.640930 loss_rnnt 50.353600 hw_loss 0.292165 lr 0.00094021 rank 4
2023-02-17 08:57:08,488 DEBUG TRAIN Batch 3/3400 loss 69.653915 loss_att 72.025558 loss_ctc 98.289612 loss_rnnt 65.159843 hw_loss 0.378097 lr 0.00093790 rank 3
2023-02-17 08:57:08,489 DEBUG TRAIN Batch 3/3400 loss 27.623386 loss_att 37.255100 loss_ctc 51.524315 loss_rnnt 22.359051 hw_loss 0.283509 lr 0.00093825 rank 7
2023-02-17 08:57:08,489 DEBUG TRAIN Batch 3/3400 loss 23.304583 loss_att 33.436893 loss_ctc 38.591919 loss_rnnt 19.014984 hw_loss 0.421547 lr 0.00093736 rank 2
2023-02-17 08:57:08,489 DEBUG TRAIN Batch 3/3400 loss 20.527723 loss_att 32.506134 loss_ctc 40.841702 loss_rnnt 15.274548 hw_loss 0.279309 lr 0.00093828 rank 6
2023-02-17 08:57:08,490 DEBUG TRAIN Batch 3/3400 loss 29.623741 loss_att 34.842182 loss_ctc 46.965191 loss_rnnt 26.099155 hw_loss 0.316315 lr 0.00093855 rank 4
2023-02-17 08:57:08,492 DEBUG TRAIN Batch 3/3400 loss 22.039129 loss_att 31.667614 loss_ctc 42.381065 loss_rnnt 17.224743 hw_loss 0.330804 lr 0.00093754 rank 0
2023-02-17 08:57:08,492 DEBUG TRAIN Batch 3/3400 loss 57.521217 loss_att 69.927971 loss_ctc 86.206055 loss_rnnt 51.071564 hw_loss 0.269355 lr 0.00093736 rank 1
2023-02-17 08:57:08,493 DEBUG TRAIN Batch 3/3400 loss 29.303173 loss_att 31.459198 loss_ctc 40.871407 loss_rnnt 27.147451 hw_loss 0.341411 lr 0.00093756 rank 5
2023-02-17 08:58:27,617 DEBUG TRAIN Batch 3/3500 loss 28.859640 loss_att 37.287636 loss_ctc 49.061565 loss_rnnt 24.353849 hw_loss 0.237377 lr 0.00093664 rank 6
2023-02-17 08:58:27,618 DEBUG TRAIN Batch 3/3500 loss 16.441097 loss_att 24.467123 loss_ctc 31.539700 loss_rnnt 12.651274 hw_loss 0.321513 lr 0.00093591 rank 5
2023-02-17 08:58:27,622 DEBUG TRAIN Batch 3/3500 loss 45.282185 loss_att 57.009262 loss_ctc 76.685074 loss_rnnt 38.560246 hw_loss 0.355251 lr 0.00093660 rank 7
2023-02-17 08:58:27,623 DEBUG TRAIN Batch 3/3500 loss 31.180895 loss_att 44.206516 loss_ctc 53.977058 loss_rnnt 25.355814 hw_loss 0.338377 lr 0.00093690 rank 4
2023-02-17 08:58:27,625 DEBUG TRAIN Batch 3/3500 loss 30.228422 loss_att 38.182091 loss_ctc 42.919502 loss_rnnt 26.746132 hw_loss 0.373897 lr 0.00093590 rank 0
2023-02-17 08:58:27,628 DEBUG TRAIN Batch 3/3500 loss 24.154608 loss_att 30.571293 loss_ctc 33.881760 loss_rnnt 21.376097 hw_loss 0.371662 lr 0.00093572 rank 1
2023-02-17 08:58:27,628 DEBUG TRAIN Batch 3/3500 loss 23.299402 loss_att 28.865753 loss_ctc 36.930779 loss_rnnt 20.161125 hw_loss 0.389048 lr 0.00093626 rank 3
2023-02-17 08:58:27,677 DEBUG TRAIN Batch 3/3500 loss 24.275244 loss_att 32.421661 loss_ctc 47.750526 loss_rnnt 19.321213 hw_loss 0.365082 lr 0.00093572 rank 2
2023-02-17 08:59:45,595 DEBUG TRAIN Batch 3/3600 loss 27.089281 loss_att 33.127392 loss_ctc 44.829147 loss_rnnt 23.286270 hw_loss 0.431388 lr 0.00093408 rank 1
2023-02-17 08:59:45,595 DEBUG TRAIN Batch 3/3600 loss 37.006985 loss_att 43.846966 loss_ctc 56.857155 loss_rnnt 32.809189 hw_loss 0.343336 lr 0.00093500 rank 6
2023-02-17 08:59:45,596 DEBUG TRAIN Batch 3/3600 loss 30.189556 loss_att 36.444489 loss_ctc 44.630058 loss_rnnt 26.842228 hw_loss 0.320518 lr 0.00093408 rank 2
2023-02-17 08:59:45,597 DEBUG TRAIN Batch 3/3600 loss 23.002531 loss_att 29.818626 loss_ctc 41.467869 loss_rnnt 18.964981 hw_loss 0.398033 lr 0.00093462 rank 3
2023-02-17 08:59:45,597 DEBUG TRAIN Batch 3/3600 loss 17.151056 loss_att 27.785805 loss_ctc 31.339645 loss_rnnt 12.930327 hw_loss 0.378688 lr 0.00093496 rank 7
2023-02-17 08:59:45,599 DEBUG TRAIN Batch 3/3600 loss 23.110777 loss_att 31.168720 loss_ctc 36.470680 loss_rnnt 19.507378 hw_loss 0.394669 lr 0.00093426 rank 0
2023-02-17 08:59:45,599 DEBUG TRAIN Batch 3/3600 loss 20.557766 loss_att 31.600796 loss_ctc 34.266499 loss_rnnt 16.400322 hw_loss 0.226889 lr 0.00093428 rank 5
2023-02-17 08:59:45,641 DEBUG TRAIN Batch 3/3600 loss 53.100933 loss_att 57.255196 loss_ctc 71.162827 loss_rnnt 49.681469 hw_loss 0.338171 lr 0.00093526 rank 4
2023-02-17 09:01:02,712 DEBUG TRAIN Batch 3/3700 loss 24.262518 loss_att 32.107269 loss_ctc 40.912476 loss_rnnt 20.271204 hw_loss 0.379442 lr 0.00093337 rank 6
2023-02-17 09:01:02,713 DEBUG TRAIN Batch 3/3700 loss 44.771221 loss_att 49.505093 loss_ctc 69.187012 loss_rnnt 40.357590 hw_loss 0.396414 lr 0.00093265 rank 5
2023-02-17 09:01:02,714 DEBUG TRAIN Batch 3/3700 loss 14.259518 loss_att 18.533249 loss_ctc 21.923605 loss_rnnt 12.226212 hw_loss 0.293777 lr 0.00093246 rank 1
2023-02-17 09:01:02,716 DEBUG TRAIN Batch 3/3700 loss 28.580177 loss_att 37.319458 loss_ctc 47.612175 loss_rnnt 24.151802 hw_loss 0.267972 lr 0.00093263 rank 0
2023-02-17 09:01:02,717 DEBUG TRAIN Batch 3/3700 loss 21.634916 loss_att 27.121540 loss_ctc 35.793808 loss_rnnt 18.491602 hw_loss 0.296506 lr 0.00093299 rank 3
2023-02-17 09:01:02,719 DEBUG TRAIN Batch 3/3700 loss 30.645300 loss_att 36.177437 loss_ctc 46.714039 loss_rnnt 27.187222 hw_loss 0.392165 lr 0.00093246 rank 2
2023-02-17 09:01:02,721 DEBUG TRAIN Batch 3/3700 loss 22.713106 loss_att 32.130939 loss_ctc 36.778233 loss_rnnt 18.775887 hw_loss 0.334315 lr 0.00093333 rank 7
2023-02-17 09:01:02,765 DEBUG TRAIN Batch 3/3700 loss 29.837837 loss_att 41.455368 loss_ctc 39.924553 loss_rnnt 26.004581 hw_loss 0.309098 lr 0.00093363 rank 4
2023-02-17 09:02:19,682 DEBUG TRAIN Batch 3/3800 loss 43.767017 loss_att 49.406300 loss_ctc 61.322437 loss_rnnt 40.106106 hw_loss 0.360620 lr 0.00093174 rank 6
2023-02-17 09:02:19,685 DEBUG TRAIN Batch 3/3800 loss 28.554043 loss_att 35.785133 loss_ctc 40.682705 loss_rnnt 25.290905 hw_loss 0.374555 lr 0.00093084 rank 1
2023-02-17 09:02:19,688 DEBUG TRAIN Batch 3/3800 loss 26.381403 loss_att 27.755875 loss_ctc 36.529121 loss_rnnt 24.536789 hw_loss 0.406292 lr 0.00093200 rank 4
2023-02-17 09:02:19,688 DEBUG TRAIN Batch 3/3800 loss 23.664387 loss_att 27.394722 loss_ctc 37.472633 loss_rnnt 20.833296 hw_loss 0.457356 lr 0.00093102 rank 0
2023-02-17 09:02:19,688 DEBUG TRAIN Batch 3/3800 loss 19.608589 loss_att 26.845411 loss_ctc 33.562614 loss_rnnt 16.258068 hw_loss 0.079914 lr 0.00093084 rank 2
2023-02-17 09:02:19,688 DEBUG TRAIN Batch 3/3800 loss 17.106579 loss_att 25.409615 loss_ctc 28.836304 loss_rnnt 13.670578 hw_loss 0.396433 lr 0.00093103 rank 5
2023-02-17 09:02:19,690 DEBUG TRAIN Batch 3/3800 loss 25.114174 loss_att 29.478054 loss_ctc 38.358074 loss_rnnt 22.333658 hw_loss 0.266038 lr 0.00093171 rank 7
2023-02-17 09:02:19,692 DEBUG TRAIN Batch 3/3800 loss 12.442053 loss_att 13.586437 loss_ctc 15.867502 loss_rnnt 11.487110 hw_loss 0.505011 lr 0.00093137 rank 3
2023-02-17 09:03:39,932 DEBUG TRAIN Batch 3/3900 loss 17.184282 loss_att 16.401211 loss_ctc 21.601398 loss_rnnt 16.517687 hw_loss 0.439240 lr 0.00093013 rank 6
2023-02-17 09:03:39,939 DEBUG TRAIN Batch 3/3900 loss 57.788677 loss_att 75.919121 loss_ctc 76.277130 loss_rnnt 51.542259 hw_loss 0.291012 lr 0.00092942 rank 5
2023-02-17 09:03:39,941 DEBUG TRAIN Batch 3/3900 loss 34.022881 loss_att 47.435204 loss_ctc 51.074917 loss_rnnt 28.866516 hw_loss 0.375547 lr 0.00093010 rank 7
2023-02-17 09:03:39,940 DEBUG TRAIN Batch 3/3900 loss 10.143900 loss_att 19.464064 loss_ctc 15.833700 loss_rnnt 7.296412 hw_loss 0.421530 lr 0.00092923 rank 1
2023-02-17 09:03:39,945 DEBUG TRAIN Batch 3/3900 loss 64.179901 loss_att 73.743271 loss_ctc 92.919090 loss_rnnt 58.293083 hw_loss 0.266718 lr 0.00092941 rank 0
2023-02-17 09:03:39,960 DEBUG TRAIN Batch 3/3900 loss 33.050953 loss_att 41.110840 loss_ctc 53.565598 loss_rnnt 28.520582 hw_loss 0.343326 lr 0.00093039 rank 4
2023-02-17 09:03:39,967 DEBUG TRAIN Batch 3/3900 loss 41.485939 loss_att 49.763084 loss_ctc 62.756374 loss_rnnt 36.810638 hw_loss 0.344652 lr 0.00092976 rank 3
2023-02-17 09:03:39,991 DEBUG TRAIN Batch 3/3900 loss 17.941490 loss_att 27.181047 loss_ctc 33.158760 loss_rnnt 13.844466 hw_loss 0.412770 lr 0.00092923 rank 2
2023-02-17 09:04:56,349 DEBUG TRAIN Batch 3/4000 loss 61.036125 loss_att 75.548279 loss_ctc 94.621384 loss_rnnt 53.519924 hw_loss 0.254508 lr 0.00092852 rank 6
2023-02-17 09:04:56,358 DEBUG TRAIN Batch 3/4000 loss 27.997746 loss_att 33.547619 loss_ctc 33.217388 loss_rnnt 26.025932 hw_loss 0.311036 lr 0.00092849 rank 7
2023-02-17 09:04:56,359 DEBUG TRAIN Batch 3/4000 loss 34.675518 loss_att 39.899597 loss_ctc 51.209789 loss_rnnt 31.271980 hw_loss 0.289025 lr 0.00092763 rank 2
2023-02-17 09:04:56,360 DEBUG TRAIN Batch 3/4000 loss 47.090267 loss_att 57.053490 loss_ctc 75.807213 loss_rnnt 41.176693 hw_loss 0.172511 lr 0.00092878 rank 4
2023-02-17 09:04:56,362 DEBUG TRAIN Batch 3/4000 loss 32.679550 loss_att 37.236488 loss_ctc 52.806683 loss_rnnt 28.923620 hw_loss 0.301732 lr 0.00092763 rank 1
2023-02-17 09:04:56,364 DEBUG TRAIN Batch 3/4000 loss 25.251251 loss_att 30.543789 loss_ctc 37.686501 loss_rnnt 22.353773 hw_loss 0.339260 lr 0.00092782 rank 5
2023-02-17 09:04:56,364 DEBUG TRAIN Batch 3/4000 loss 28.018496 loss_att 34.349232 loss_ctc 49.723755 loss_rnnt 23.716711 hw_loss 0.265507 lr 0.00092816 rank 3
2023-02-17 09:04:56,364 DEBUG TRAIN Batch 3/4000 loss 20.720812 loss_att 29.000275 loss_ctc 43.716095 loss_rnnt 15.729513 hw_loss 0.505069 lr 0.00092781 rank 0
2023-02-17 09:06:12,195 DEBUG TRAIN Batch 3/4100 loss 19.105639 loss_att 33.070549 loss_ctc 34.150703 loss_rnnt 14.143114 hw_loss 0.306621 lr 0.00092604 rank 2
2023-02-17 09:06:12,197 DEBUG TRAIN Batch 3/4100 loss 29.721876 loss_att 36.909931 loss_ctc 47.126385 loss_rnnt 25.786581 hw_loss 0.332032 lr 0.00092693 rank 6
2023-02-17 09:06:12,197 DEBUG TRAIN Batch 3/4100 loss 21.333742 loss_att 30.864674 loss_ctc 33.574017 loss_rnnt 17.628904 hw_loss 0.312403 lr 0.00092623 rank 5
2023-02-17 09:06:12,197 DEBUG TRAIN Batch 3/4100 loss 21.378824 loss_att 34.506767 loss_ctc 41.531609 loss_rnnt 15.929164 hw_loss 0.256937 lr 0.00092690 rank 7
2023-02-17 09:06:12,198 DEBUG TRAIN Batch 3/4100 loss 22.136497 loss_att 31.170244 loss_ctc 31.928288 loss_rnnt 18.791424 hw_loss 0.436413 lr 0.00092656 rank 3
2023-02-17 09:06:12,200 DEBUG TRAIN Batch 3/4100 loss 23.429737 loss_att 30.693607 loss_ctc 39.631947 loss_rnnt 19.637665 hw_loss 0.335634 lr 0.00092604 rank 1
2023-02-17 09:06:12,203 DEBUG TRAIN Batch 3/4100 loss 28.927731 loss_att 32.476753 loss_ctc 39.682632 loss_rnnt 26.596443 hw_loss 0.351552 lr 0.00092718 rank 4
2023-02-17 09:06:12,205 DEBUG TRAIN Batch 3/4100 loss 33.083817 loss_att 40.387207 loss_ctc 47.134369 loss_rnnt 29.588112 hw_loss 0.303030 lr 0.00092621 rank 0
2023-02-17 09:07:29,772 DEBUG TRAIN Batch 3/4200 loss 34.305946 loss_att 40.332363 loss_ctc 55.734283 loss_rnnt 30.061317 hw_loss 0.341679 lr 0.00092463 rank 0
2023-02-17 09:07:29,775 DEBUG TRAIN Batch 3/4200 loss 40.272118 loss_att 42.520348 loss_ctc 59.025108 loss_rnnt 37.134064 hw_loss 0.352516 lr 0.00092464 rank 5
2023-02-17 09:07:29,777 DEBUG TRAIN Batch 3/4200 loss 41.533909 loss_att 55.523117 loss_ctc 63.901417 loss_rnnt 35.589661 hw_loss 0.307641 lr 0.00092531 rank 7
2023-02-17 09:07:29,777 DEBUG TRAIN Batch 3/4200 loss 29.341972 loss_att 33.380016 loss_ctc 44.883728 loss_rnnt 26.289484 hw_loss 0.323708 lr 0.00092534 rank 6
2023-02-17 09:07:29,779 DEBUG TRAIN Batch 3/4200 loss 17.584982 loss_att 26.275999 loss_ctc 33.078690 loss_rnnt 13.620629 hw_loss 0.300602 lr 0.00092497 rank 3
2023-02-17 09:07:29,779 DEBUG TRAIN Batch 3/4200 loss 21.989325 loss_att 31.187378 loss_ctc 38.416397 loss_rnnt 17.803823 hw_loss 0.291775 lr 0.00092559 rank 4
2023-02-17 09:07:29,779 DEBUG TRAIN Batch 3/4200 loss 39.118050 loss_att 45.228806 loss_ctc 55.067154 loss_rnnt 35.583290 hw_loss 0.348859 lr 0.00092445 rank 1
2023-02-17 09:07:29,779 DEBUG TRAIN Batch 3/4200 loss 26.301016 loss_att 32.780067 loss_ctc 40.209579 loss_rnnt 22.951530 hw_loss 0.373501 lr 0.00092445 rank 2
2023-02-17 09:08:48,451 DEBUG TRAIN Batch 3/4300 loss 24.986839 loss_att 30.218626 loss_ctc 37.014057 loss_rnnt 22.156927 hw_loss 0.337363 lr 0.00092373 rank 7
2023-02-17 09:08:48,453 DEBUG TRAIN Batch 3/4300 loss 30.976685 loss_att 42.315243 loss_ctc 52.989609 loss_rnnt 25.600481 hw_loss 0.325189 lr 0.00092376 rank 6
2023-02-17 09:08:48,457 DEBUG TRAIN Batch 3/4300 loss 19.314926 loss_att 20.303143 loss_ctc 25.495846 loss_rnnt 18.089897 hw_loss 0.381117 lr 0.00092401 rank 4
2023-02-17 09:08:48,459 DEBUG TRAIN Batch 3/4300 loss 30.245262 loss_att 36.564461 loss_ctc 42.609360 loss_rnnt 27.143194 hw_loss 0.355655 lr 0.00092307 rank 5
2023-02-17 09:08:48,462 DEBUG TRAIN Batch 3/4300 loss 25.809795 loss_att 32.010303 loss_ctc 37.839737 loss_rnnt 22.790432 hw_loss 0.328632 lr 0.00092288 rank 1
2023-02-17 09:08:48,465 DEBUG TRAIN Batch 3/4300 loss 18.798462 loss_att 25.091742 loss_ctc 29.315643 loss_rnnt 15.931134 hw_loss 0.386964 lr 0.00092288 rank 2
2023-02-17 09:08:48,467 DEBUG TRAIN Batch 3/4300 loss 26.881245 loss_att 34.768330 loss_ctc 41.489773 loss_rnnt 23.155968 hw_loss 0.375103 lr 0.00092340 rank 3
2023-02-17 09:08:48,468 DEBUG TRAIN Batch 3/4300 loss 48.723728 loss_att 54.083015 loss_ctc 61.759624 loss_rnnt 45.749687 hw_loss 0.307617 lr 0.00092305 rank 0
2023-02-17 09:10:05,746 DEBUG TRAIN Batch 3/4400 loss 27.434978 loss_att 29.393341 loss_ctc 37.999687 loss_rnnt 25.427614 hw_loss 0.388242 lr 0.00092219 rank 6
2023-02-17 09:10:05,750 DEBUG TRAIN Batch 3/4400 loss 26.124086 loss_att 35.523689 loss_ctc 43.851013 loss_rnnt 21.676773 hw_loss 0.382129 lr 0.00092131 rank 1
2023-02-17 09:10:05,754 DEBUG TRAIN Batch 3/4400 loss 17.367201 loss_att 17.365919 loss_ctc 25.235783 loss_rnnt 16.082329 hw_loss 0.442471 lr 0.00092183 rank 3
2023-02-17 09:10:05,756 DEBUG TRAIN Batch 3/4400 loss 43.447582 loss_att 49.791534 loss_ctc 55.836994 loss_rnnt 40.329250 hw_loss 0.370545 lr 0.00092244 rank 4
2023-02-17 09:10:05,755 DEBUG TRAIN Batch 3/4400 loss 20.923145 loss_att 26.022503 loss_ctc 37.977280 loss_rnnt 17.424465 hw_loss 0.384229 lr 0.00092215 rank 7
2023-02-17 09:10:05,758 DEBUG TRAIN Batch 3/4400 loss 21.898811 loss_att 22.795334 loss_ctc 30.052429 loss_rnnt 20.370647 hw_loss 0.490710 lr 0.00092131 rank 2
2023-02-17 09:10:05,761 DEBUG TRAIN Batch 3/4400 loss 30.668987 loss_att 36.967018 loss_ctc 44.113808 loss_rnnt 27.467422 hw_loss 0.279963 lr 0.00092150 rank 5
2023-02-17 09:10:05,763 DEBUG TRAIN Batch 3/4400 loss 26.223597 loss_att 31.400505 loss_ctc 42.867764 loss_rnnt 22.776794 hw_loss 0.360370 lr 0.00092148 rank 0
2023-02-17 09:11:20,204 DEBUG TRAIN Batch 3/4500 loss 15.011873 loss_att 15.739971 loss_ctc 22.063953 loss_rnnt 13.696712 hw_loss 0.429868 lr 0.00092062 rank 6
2023-02-17 09:11:20,208 DEBUG TRAIN Batch 3/4500 loss 20.724009 loss_att 26.859600 loss_ctc 37.258270 loss_rnnt 17.092365 hw_loss 0.374919 lr 0.00091992 rank 0
2023-02-17 09:11:20,209 DEBUG TRAIN Batch 3/4500 loss 19.587536 loss_att 20.971539 loss_ctc 26.891241 loss_rnnt 18.086416 hw_loss 0.469673 lr 0.00091994 rank 5
2023-02-17 09:11:20,210 DEBUG TRAIN Batch 3/4500 loss 14.708557 loss_att 22.356869 loss_ctc 23.796177 loss_rnnt 11.770831 hw_loss 0.368215 lr 0.00092026 rank 3
2023-02-17 09:11:20,210 DEBUG TRAIN Batch 3/4500 loss 28.294991 loss_att 28.406609 loss_ctc 38.230030 loss_rnnt 26.750805 hw_loss 0.369729 lr 0.00092087 rank 4
2023-02-17 09:11:20,210 DEBUG TRAIN Batch 3/4500 loss 35.253731 loss_att 36.057049 loss_ctc 42.225235 loss_rnnt 33.976395 hw_loss 0.350888 lr 0.00091975 rank 1
2023-02-17 09:11:20,210 DEBUG TRAIN Batch 3/4500 loss 16.105171 loss_att 17.005075 loss_ctc 21.065334 loss_rnnt 15.004125 hw_loss 0.486959 lr 0.00092059 rank 7
2023-02-17 09:11:20,213 DEBUG TRAIN Batch 3/4500 loss 46.084354 loss_att 49.937645 loss_ctc 64.987938 loss_rnnt 42.583035 hw_loss 0.394098 lr 0.00091975 rank 2
2023-02-17 09:12:38,184 DEBUG TRAIN Batch 3/4600 loss 31.104698 loss_att 39.972008 loss_ctc 48.694458 loss_rnnt 26.813023 hw_loss 0.324204 lr 0.00091931 rank 4
2023-02-17 09:12:38,188 DEBUG TRAIN Batch 3/4600 loss 28.919350 loss_att 34.701363 loss_ctc 43.056152 loss_rnnt 25.696110 hw_loss 0.341122 lr 0.00091871 rank 3
2023-02-17 09:12:38,189 DEBUG TRAIN Batch 3/4600 loss 46.568935 loss_att 52.452484 loss_ctc 64.623428 loss_rnnt 42.817436 hw_loss 0.314113 lr 0.00091906 rank 6
2023-02-17 09:12:38,193 DEBUG TRAIN Batch 3/4600 loss 41.083115 loss_att 49.267353 loss_ctc 65.100418 loss_rnnt 36.052376 hw_loss 0.359220 lr 0.00091837 rank 0
2023-02-17 09:12:38,193 DEBUG TRAIN Batch 3/4600 loss 26.391684 loss_att 35.254387 loss_ctc 37.014549 loss_rnnt 23.050726 hw_loss 0.285069 lr 0.00091820 rank 2
2023-02-17 09:12:38,194 DEBUG TRAIN Batch 3/4600 loss 37.633690 loss_att 47.155277 loss_ctc 61.615005 loss_rnnt 32.374512 hw_loss 0.295032 lr 0.00091820 rank 1
2023-02-17 09:12:38,194 DEBUG TRAIN Batch 3/4600 loss 21.087376 loss_att 32.202801 loss_ctc 40.782806 loss_rnnt 16.055992 hw_loss 0.341700 lr 0.00091838 rank 5
2023-02-17 09:12:38,196 DEBUG TRAIN Batch 3/4600 loss 27.235826 loss_att 29.808592 loss_ctc 39.719673 loss_rnnt 24.830265 hw_loss 0.424675 lr 0.00091903 rank 7
2023-02-17 09:13:55,261 DEBUG TRAIN Batch 3/4700 loss 26.520727 loss_att 27.962101 loss_ctc 46.762260 loss_rnnt 23.335995 hw_loss 0.370479 lr 0.00091752 rank 6
2023-02-17 09:13:55,269 DEBUG TRAIN Batch 3/4700 loss 20.150858 loss_att 29.495411 loss_ctc 32.590485 loss_rnnt 16.445696 hw_loss 0.333066 lr 0.00091749 rank 7
2023-02-17 09:13:55,270 DEBUG TRAIN Batch 3/4700 loss 30.922396 loss_att 45.131866 loss_ctc 45.340446 loss_rnnt 25.936386 hw_loss 0.415702 lr 0.00091665 rank 1
2023-02-17 09:13:55,272 DEBUG TRAIN Batch 3/4700 loss 26.079676 loss_att 30.419052 loss_ctc 40.854183 loss_rnnt 23.069149 hw_loss 0.323842 lr 0.00091716 rank 3
2023-02-17 09:13:55,272 DEBUG TRAIN Batch 3/4700 loss 30.707104 loss_att 37.002769 loss_ctc 41.175072 loss_rnnt 27.861685 hw_loss 0.357294 lr 0.00091682 rank 0
2023-02-17 09:13:55,275 DEBUG TRAIN Batch 3/4700 loss 29.765177 loss_att 37.580723 loss_ctc 47.560768 loss_rnnt 25.639505 hw_loss 0.355907 lr 0.00091665 rank 2
2023-02-17 09:13:55,276 DEBUG TRAIN Batch 3/4700 loss 15.658025 loss_att 25.388086 loss_ctc 26.819332 loss_rnnt 12.011230 hw_loss 0.398641 lr 0.00091776 rank 4
2023-02-17 09:13:55,275 DEBUG TRAIN Batch 3/4700 loss 28.125505 loss_att 34.671837 loss_ctc 41.643322 loss_rnnt 24.830976 hw_loss 0.342916 lr 0.00091684 rank 5
2023-02-17 09:15:10,558 DEBUG TRAIN Batch 3/4800 loss 17.405088 loss_att 24.973194 loss_ctc 30.138508 loss_rnnt 14.068455 hw_loss 0.234788 lr 0.00091594 rank 7
2023-02-17 09:15:10,558 DEBUG TRAIN Batch 3/4800 loss 42.373573 loss_att 47.948582 loss_ctc 66.442513 loss_rnnt 37.878601 hw_loss 0.320203 lr 0.00091598 rank 6
2023-02-17 09:15:10,561 DEBUG TRAIN Batch 3/4800 loss 39.589645 loss_att 46.551468 loss_ctc 55.840748 loss_rnnt 35.894108 hw_loss 0.255670 lr 0.00091562 rank 3
2023-02-17 09:15:10,561 DEBUG TRAIN Batch 3/4800 loss 42.665653 loss_att 52.272240 loss_ctc 68.686020 loss_rnnt 37.063251 hw_loss 0.396941 lr 0.00091530 rank 5
2023-02-17 09:15:10,562 DEBUG TRAIN Batch 3/4800 loss 40.452694 loss_att 44.001476 loss_ctc 55.148151 loss_rnnt 37.569592 hw_loss 0.401168 lr 0.00091622 rank 4
2023-02-17 09:15:10,563 DEBUG TRAIN Batch 3/4800 loss 21.613579 loss_att 29.178089 loss_ctc 33.371841 loss_rnnt 18.333229 hw_loss 0.374395 lr 0.00091512 rank 2
2023-02-17 09:15:10,565 DEBUG TRAIN Batch 3/4800 loss 25.925953 loss_att 26.656467 loss_ctc 36.261734 loss_rnnt 24.250916 hw_loss 0.282808 lr 0.00091512 rank 1
2023-02-17 09:15:10,568 DEBUG TRAIN Batch 3/4800 loss 41.730297 loss_att 52.217426 loss_ctc 61.270775 loss_rnnt 36.845230 hw_loss 0.341702 lr 0.00091528 rank 0
2023-02-17 09:16:28,018 DEBUG TRAIN Batch 3/4900 loss 25.574608 loss_att 31.552420 loss_ctc 37.611923 loss_rnnt 22.622574 hw_loss 0.284057 lr 0.00091444 rank 6
2023-02-17 09:16:28,020 DEBUG TRAIN Batch 3/4900 loss 37.756718 loss_att 42.574020 loss_ctc 53.013756 loss_rnnt 34.571842 hw_loss 0.350885 lr 0.00091359 rank 2
2023-02-17 09:16:28,025 DEBUG TRAIN Batch 3/4900 loss 24.497610 loss_att 32.467316 loss_ctc 39.314018 loss_rnnt 20.714157 hw_loss 0.401231 lr 0.00091469 rank 4
2023-02-17 09:16:28,025 DEBUG TRAIN Batch 3/4900 loss 32.311348 loss_att 46.490776 loss_ctc 50.712753 loss_rnnt 26.822865 hw_loss 0.373264 lr 0.00091375 rank 0
2023-02-17 09:16:28,030 DEBUG TRAIN Batch 3/4900 loss 28.272573 loss_att 33.256245 loss_ctc 45.101231 loss_rnnt 24.844118 hw_loss 0.352311 lr 0.00091359 rank 1
2023-02-17 09:16:28,042 DEBUG TRAIN Batch 3/4900 loss 21.508009 loss_att 29.249683 loss_ctc 31.328461 loss_rnnt 18.439587 hw_loss 0.395049 lr 0.00091409 rank 3
2023-02-17 09:16:28,066 DEBUG TRAIN Batch 3/4900 loss 36.604511 loss_att 41.313492 loss_ctc 50.294777 loss_rnnt 33.668587 hw_loss 0.316423 lr 0.00091377 rank 5
2023-02-17 09:16:28,073 DEBUG TRAIN Batch 3/4900 loss 41.875980 loss_att 49.121433 loss_ctc 58.357975 loss_rnnt 38.047134 hw_loss 0.341549 lr 0.00091441 rank 7
2023-02-17 09:17:46,898 DEBUG TRAIN Batch 3/5000 loss 19.229488 loss_att 21.130625 loss_ctc 27.958773 loss_rnnt 17.505924 hw_loss 0.336438 lr 0.00091207 rank 1
2023-02-17 09:17:46,901 DEBUG TRAIN Batch 3/5000 loss 28.684790 loss_att 31.357794 loss_ctc 42.546753 loss_rnnt 26.111521 hw_loss 0.357013 lr 0.00091223 rank 0
2023-02-17 09:17:46,903 DEBUG TRAIN Batch 3/5000 loss 24.570610 loss_att 27.567650 loss_ctc 34.409302 loss_rnnt 22.494867 hw_loss 0.308457 lr 0.00091289 rank 7
2023-02-17 09:17:46,903 DEBUG TRAIN Batch 3/5000 loss 36.786922 loss_att 40.701675 loss_ctc 51.304165 loss_rnnt 33.901756 hw_loss 0.312339 lr 0.00091257 rank 3
2023-02-17 09:17:46,905 DEBUG TRAIN Batch 3/5000 loss 27.092918 loss_att 31.677811 loss_ctc 41.534142 loss_rnnt 24.076210 hw_loss 0.326690 lr 0.00091225 rank 5
2023-02-17 09:17:46,906 DEBUG TRAIN Batch 3/5000 loss 35.324905 loss_att 41.910667 loss_ctc 61.263733 loss_rnnt 30.409540 hw_loss 0.261938 lr 0.00091292 rank 6
2023-02-17 09:17:46,909 DEBUG TRAIN Batch 3/5000 loss 13.359967 loss_att 21.612240 loss_ctc 25.120800 loss_rnnt 9.977505 hw_loss 0.307305 lr 0.00091316 rank 4
2023-02-17 09:17:46,916 DEBUG TRAIN Batch 3/5000 loss 24.785366 loss_att 29.558697 loss_ctc 44.571217 loss_rnnt 21.018406 hw_loss 0.326584 lr 0.00091207 rank 2
2023-02-17 09:19:02,614 DEBUG TRAIN Batch 3/5100 loss 23.879808 loss_att 25.534439 loss_ctc 37.002426 loss_rnnt 21.602900 hw_loss 0.368061 lr 0.00091137 rank 7
2023-02-17 09:19:02,615 DEBUG TRAIN Batch 3/5100 loss 21.266958 loss_att 29.884756 loss_ctc 31.851606 loss_rnnt 17.959970 hw_loss 0.322762 lr 0.00091073 rank 5
2023-02-17 09:19:02,617 DEBUG TRAIN Batch 3/5100 loss 22.696388 loss_att 31.964016 loss_ctc 39.131390 loss_rnnt 18.473707 hw_loss 0.333416 lr 0.00091055 rank 2
2023-02-17 09:19:02,618 DEBUG TRAIN Batch 3/5100 loss 25.372023 loss_att 29.448177 loss_ctc 40.277218 loss_rnnt 22.340296 hw_loss 0.429629 lr 0.00091140 rank 6
2023-02-17 09:19:02,619 DEBUG TRAIN Batch 3/5100 loss 22.686367 loss_att 28.547384 loss_ctc 42.753845 loss_rnnt 18.662010 hw_loss 0.330914 lr 0.00091164 rank 4
2023-02-17 09:19:02,622 DEBUG TRAIN Batch 3/5100 loss 18.225338 loss_att 26.783390 loss_ctc 34.026958 loss_rnnt 14.193535 hw_loss 0.399957 lr 0.00091105 rank 3
2023-02-17 09:19:02,622 DEBUG TRAIN Batch 3/5100 loss 29.085890 loss_att 36.985023 loss_ctc 52.472183 loss_rnnt 24.205578 hw_loss 0.341833 lr 0.00091055 rank 1
2023-02-17 09:19:02,626 DEBUG TRAIN Batch 3/5100 loss 17.535189 loss_att 20.956051 loss_ctc 25.048868 loss_rnnt 15.615698 hw_loss 0.437805 lr 0.00091072 rank 0
2023-02-17 09:20:17,322 DEBUG TRAIN Batch 3/5200 loss 27.214132 loss_att 33.396786 loss_ctc 41.662121 loss_rnnt 23.915909 hw_loss 0.253677 lr 0.00090989 rank 6
2023-02-17 09:20:17,323 DEBUG TRAIN Batch 3/5200 loss 33.043293 loss_att 45.163754 loss_ctc 55.577785 loss_rnnt 27.420553 hw_loss 0.363835 lr 0.00090923 rank 5
2023-02-17 09:20:17,325 DEBUG TRAIN Batch 3/5200 loss 49.583755 loss_att 62.085159 loss_ctc 79.600670 loss_rnnt 42.901733 hw_loss 0.336537 lr 0.00090905 rank 2
2023-02-17 09:20:17,326 DEBUG TRAIN Batch 3/5200 loss 25.835243 loss_att 34.665733 loss_ctc 38.173832 loss_rnnt 22.216351 hw_loss 0.389346 lr 0.00090905 rank 1
2023-02-17 09:20:17,326 DEBUG TRAIN Batch 3/5200 loss 31.192074 loss_att 38.159603 loss_ctc 49.024544 loss_rnnt 27.210827 hw_loss 0.393898 lr 0.00090986 rank 7
2023-02-17 09:20:17,328 DEBUG TRAIN Batch 3/5200 loss 29.085003 loss_att 37.703358 loss_ctc 43.480614 loss_rnnt 25.288504 hw_loss 0.287650 lr 0.00090954 rank 3
2023-02-17 09:20:17,328 DEBUG TRAIN Batch 3/5200 loss 22.536577 loss_att 34.100964 loss_ctc 40.607597 loss_rnnt 17.639688 hw_loss 0.327264 lr 0.00091013 rank 4
2023-02-17 09:20:17,330 DEBUG TRAIN Batch 3/5200 loss 38.689484 loss_att 45.753502 loss_ctc 64.093781 loss_rnnt 33.713104 hw_loss 0.330631 lr 0.00090921 rank 0
2023-02-17 09:21:35,754 DEBUG TRAIN Batch 3/5300 loss 21.071108 loss_att 25.650663 loss_ctc 29.989664 loss_rnnt 18.800537 hw_loss 0.310350 lr 0.00090755 rank 1
2023-02-17 09:21:35,756 DEBUG TRAIN Batch 3/5300 loss 27.754051 loss_att 37.149830 loss_ctc 48.443886 loss_rnnt 22.962269 hw_loss 0.288718 lr 0.00090836 rank 7
2023-02-17 09:21:35,757 DEBUG TRAIN Batch 3/5300 loss 35.259026 loss_att 45.198326 loss_ctc 51.111767 loss_rnnt 30.976728 hw_loss 0.338878 lr 0.00090839 rank 6
2023-02-17 09:21:35,758 DEBUG TRAIN Batch 3/5300 loss 26.041986 loss_att 32.982853 loss_ctc 46.400787 loss_rnnt 21.684278 hw_loss 0.478178 lr 0.00090863 rank 4
2023-02-17 09:21:35,758 DEBUG TRAIN Batch 3/5300 loss 27.224535 loss_att 32.143467 loss_ctc 43.695328 loss_rnnt 23.849552 hw_loss 0.365793 lr 0.00090755 rank 2
2023-02-17 09:21:35,758 DEBUG TRAIN Batch 3/5300 loss 23.520157 loss_att 32.481991 loss_ctc 39.805096 loss_rnnt 19.351814 hw_loss 0.383722 lr 0.00090773 rank 5
2023-02-17 09:21:35,760 DEBUG TRAIN Batch 3/5300 loss 22.538504 loss_att 28.688633 loss_ctc 30.715353 loss_rnnt 19.953579 hw_loss 0.496223 lr 0.00090771 rank 0
2023-02-17 09:21:35,761 DEBUG TRAIN Batch 3/5300 loss 23.000830 loss_att 31.419186 loss_ctc 37.830364 loss_rnnt 19.175097 hw_loss 0.308975 lr 0.00090804 rank 3
2023-02-17 09:22:53,324 DEBUG TRAIN Batch 3/5400 loss 36.879791 loss_att 43.705101 loss_ctc 53.249207 loss_rnnt 33.153976 hw_loss 0.334050 lr 0.00090655 rank 3
2023-02-17 09:22:53,324 DEBUG TRAIN Batch 3/5400 loss 37.188763 loss_att 46.174835 loss_ctc 64.712486 loss_rnnt 31.552481 hw_loss 0.317319 lr 0.00090686 rank 7
2023-02-17 09:22:53,326 DEBUG TRAIN Batch 3/5400 loss 31.801426 loss_att 42.486084 loss_ctc 55.648098 loss_rnnt 26.331985 hw_loss 0.286785 lr 0.00090713 rank 4
2023-02-17 09:22:53,326 DEBUG TRAIN Batch 3/5400 loss 16.929211 loss_att 23.819418 loss_ctc 34.655468 loss_rnnt 12.953564 hw_loss 0.438948 lr 0.00090606 rank 1
2023-02-17 09:22:53,331 DEBUG TRAIN Batch 3/5400 loss 32.152023 loss_att 38.461430 loss_ctc 49.237396 loss_rnnt 28.464317 hw_loss 0.277079 lr 0.00090622 rank 0
2023-02-17 09:22:53,332 DEBUG TRAIN Batch 3/5400 loss 41.273106 loss_att 49.029549 loss_ctc 61.147224 loss_rnnt 36.900345 hw_loss 0.321729 lr 0.00090606 rank 2
2023-02-17 09:22:53,347 DEBUG TRAIN Batch 3/5400 loss 20.943316 loss_att 25.645828 loss_ctc 34.418159 loss_rnnt 18.005043 hw_loss 0.377108 lr 0.00090623 rank 5
2023-02-17 09:22:53,350 DEBUG TRAIN Batch 3/5400 loss 26.482758 loss_att 35.427044 loss_ctc 46.268856 loss_rnnt 21.885818 hw_loss 0.318629 lr 0.00090689 rank 6
2023-02-17 09:24:09,092 DEBUG TRAIN Batch 3/5500 loss 44.029396 loss_att 52.634369 loss_ctc 65.999832 loss_rnnt 39.171665 hw_loss 0.388765 lr 0.00090473 rank 0
2023-02-17 09:24:09,093 DEBUG TRAIN Batch 3/5500 loss 24.891703 loss_att 27.805813 loss_ctc 32.538792 loss_rnnt 23.114271 hw_loss 0.328119 lr 0.00090540 rank 6
2023-02-17 09:24:09,093 DEBUG TRAIN Batch 3/5500 loss 32.821991 loss_att 38.020344 loss_ctc 55.127548 loss_rnnt 28.601545 hw_loss 0.387563 lr 0.00090475 rank 5
2023-02-17 09:24:09,094 DEBUG TRAIN Batch 3/5500 loss 23.921265 loss_att 30.547840 loss_ctc 41.149208 loss_rnnt 20.113552 hw_loss 0.347511 lr 0.00090537 rank 7
2023-02-17 09:24:09,096 DEBUG TRAIN Batch 3/5500 loss 28.693947 loss_att 33.011932 loss_ctc 34.824837 loss_rnnt 26.880486 hw_loss 0.248273 lr 0.00090564 rank 4
2023-02-17 09:24:09,098 DEBUG TRAIN Batch 3/5500 loss 36.609035 loss_att 40.204697 loss_ctc 55.496056 loss_rnnt 33.179764 hw_loss 0.359757 lr 0.00090457 rank 2
2023-02-17 09:24:09,100 DEBUG TRAIN Batch 3/5500 loss 19.819263 loss_att 27.523640 loss_ctc 32.404945 loss_rnnt 16.407845 hw_loss 0.360846 lr 0.00090457 rank 1
2023-02-17 09:24:09,144 DEBUG TRAIN Batch 3/5500 loss 25.576815 loss_att 32.593044 loss_ctc 34.462242 loss_rnnt 22.843227 hw_loss 0.273032 lr 0.00090506 rank 3
2023-02-17 09:25:25,813 DEBUG TRAIN Batch 3/5600 loss 24.344187 loss_att 29.613888 loss_ctc 42.481487 loss_rnnt 20.684889 hw_loss 0.350723 lr 0.00090358 rank 3
2023-02-17 09:25:25,815 DEBUG TRAIN Batch 3/5600 loss 23.750887 loss_att 33.946472 loss_ctc 34.911842 loss_rnnt 20.020782 hw_loss 0.380366 lr 0.00090416 rank 4
2023-02-17 09:25:25,817 DEBUG TRAIN Batch 3/5600 loss 29.825884 loss_att 30.336811 loss_ctc 38.700577 loss_rnnt 28.382900 hw_loss 0.295324 lr 0.00090310 rank 1
2023-02-17 09:25:25,818 DEBUG TRAIN Batch 3/5600 loss 22.112942 loss_att 27.953716 loss_ctc 36.628540 loss_rnnt 18.818710 hw_loss 0.357495 lr 0.00090389 rank 7
2023-02-17 09:25:25,821 DEBUG TRAIN Batch 3/5600 loss 36.261829 loss_att 45.894047 loss_ctc 67.496315 loss_rnnt 29.992550 hw_loss 0.334191 lr 0.00090392 rank 6
2023-02-17 09:25:25,821 DEBUG TRAIN Batch 3/5600 loss 19.045158 loss_att 23.957136 loss_ctc 28.558630 loss_rnnt 16.617949 hw_loss 0.330658 lr 0.00090310 rank 2
2023-02-17 09:25:25,825 DEBUG TRAIN Batch 3/5600 loss 33.852451 loss_att 37.276787 loss_ctc 51.531342 loss_rnnt 30.634773 hw_loss 0.329293 lr 0.00090327 rank 5
2023-02-17 09:25:25,825 DEBUG TRAIN Batch 3/5600 loss 31.344147 loss_att 39.238441 loss_ctc 52.487869 loss_rnnt 26.740650 hw_loss 0.385262 lr 0.00090326 rank 0
2023-02-17 09:26:45,410 DEBUG TRAIN Batch 3/5700 loss 16.286707 loss_att 17.677282 loss_ctc 24.921532 loss_rnnt 14.573154 hw_loss 0.532738 lr 0.00090163 rank 1
2023-02-17 09:26:45,411 DEBUG TRAIN Batch 3/5700 loss 26.729618 loss_att 34.035835 loss_ctc 40.556549 loss_rnnt 23.244238 hw_loss 0.338523 lr 0.00090245 rank 6
2023-02-17 09:26:45,413 DEBUG TRAIN Batch 3/5700 loss 17.625399 loss_att 20.594141 loss_ctc 28.031006 loss_rnnt 15.451367 hw_loss 0.361630 lr 0.00090242 rank 7
2023-02-17 09:26:45,415 DEBUG TRAIN Batch 3/5700 loss 21.351707 loss_att 22.485067 loss_ctc 30.491816 loss_rnnt 19.722853 hw_loss 0.344065 lr 0.00090180 rank 5
2023-02-17 09:26:45,415 DEBUG TRAIN Batch 3/5700 loss 17.426357 loss_att 24.427086 loss_ctc 27.138159 loss_rnnt 14.538196 hw_loss 0.362083 lr 0.00090268 rank 4
2023-02-17 09:26:45,416 DEBUG TRAIN Batch 3/5700 loss 17.507923 loss_att 20.550354 loss_ctc 26.870773 loss_rnnt 15.383710 hw_loss 0.501275 lr 0.00090211 rank 3
2023-02-17 09:26:45,418 DEBUG TRAIN Batch 3/5700 loss 15.731754 loss_att 15.372061 loss_ctc 21.064842 loss_rnnt 14.885692 hw_loss 0.387980 lr 0.00090163 rank 2
2023-02-17 09:26:45,423 DEBUG TRAIN Batch 3/5700 loss 24.891542 loss_att 28.055973 loss_ctc 35.213223 loss_rnnt 22.691898 hw_loss 0.357252 lr 0.00090179 rank 0
2023-02-17 09:28:02,852 DEBUG TRAIN Batch 3/5800 loss 30.403860 loss_att 32.597900 loss_ctc 48.653984 loss_rnnt 27.379499 hw_loss 0.285382 lr 0.00090065 rank 3
2023-02-17 09:28:02,852 DEBUG TRAIN Batch 3/5800 loss 33.923550 loss_att 41.436699 loss_ctc 58.752956 loss_rnnt 28.991705 hw_loss 0.222421 lr 0.00090098 rank 6
2023-02-17 09:28:02,852 DEBUG TRAIN Batch 3/5800 loss 16.684549 loss_att 17.941446 loss_ctc 21.885979 loss_rnnt 15.489533 hw_loss 0.468959 lr 0.00090034 rank 5
2023-02-17 09:28:02,852 DEBUG TRAIN Batch 3/5800 loss 31.332348 loss_att 38.294804 loss_ctc 52.088020 loss_rnnt 26.979095 hw_loss 0.362509 lr 0.00090095 rank 7
2023-02-17 09:28:02,853 DEBUG TRAIN Batch 3/5800 loss 22.380253 loss_att 28.670376 loss_ctc 35.602215 loss_rnnt 19.195141 hw_loss 0.307797 lr 0.00090016 rank 2
2023-02-17 09:28:02,854 DEBUG TRAIN Batch 3/5800 loss 41.700195 loss_att 53.962337 loss_ctc 74.066292 loss_rnnt 34.767681 hw_loss 0.308647 lr 0.00090016 rank 1
2023-02-17 09:28:02,858 DEBUG TRAIN Batch 3/5800 loss 22.008326 loss_att 27.931587 loss_ctc 30.230268 loss_rnnt 19.536314 hw_loss 0.358308 lr 0.00090122 rank 4
2023-02-17 09:28:02,862 DEBUG TRAIN Batch 3/5800 loss 19.828714 loss_att 28.119282 loss_ctc 31.586529 loss_rnnt 16.391003 hw_loss 0.397291 lr 0.00090032 rank 0
2023-02-17 09:29:19,170 DEBUG TRAIN Batch 3/5900 loss 36.018360 loss_att 50.273090 loss_ctc 65.034042 loss_rnnt 29.124283 hw_loss 0.326952 lr 0.00089949 rank 7
2023-02-17 09:29:19,172 DEBUG TRAIN Batch 3/5900 loss 37.892124 loss_att 42.678043 loss_ctc 50.789375 loss_rnnt 35.019146 hw_loss 0.367806 lr 0.00089952 rank 6
2023-02-17 09:29:19,173 DEBUG TRAIN Batch 3/5900 loss 23.446724 loss_att 22.773394 loss_ctc 32.674973 loss_rnnt 22.161034 hw_loss 0.356102 lr 0.00089871 rank 1
2023-02-17 09:29:19,174 DEBUG TRAIN Batch 3/5900 loss 18.020344 loss_att 26.058588 loss_ctc 40.931240 loss_rnnt 13.223259 hw_loss 0.252466 lr 0.00089919 rank 3
2023-02-17 09:29:19,176 DEBUG TRAIN Batch 3/5900 loss 25.193964 loss_att 31.478504 loss_ctc 37.410538 loss_rnnt 22.120113 hw_loss 0.352624 lr 0.00089871 rank 2
2023-02-17 09:29:19,176 DEBUG TRAIN Batch 3/5900 loss 37.218636 loss_att 40.555290 loss_ctc 53.588116 loss_rnnt 34.129948 hw_loss 0.447671 lr 0.00089888 rank 5
2023-02-17 09:29:19,177 DEBUG TRAIN Batch 3/5900 loss 25.000448 loss_att 36.831291 loss_ctc 41.182720 loss_rnnt 20.314054 hw_loss 0.304849 lr 0.00089887 rank 0
2023-02-17 09:29:19,223 DEBUG TRAIN Batch 3/5900 loss 27.954550 loss_att 34.349567 loss_ctc 41.681171 loss_rnnt 24.685186 hw_loss 0.300270 lr 0.00089976 rank 4
2023-02-17 09:30:39,858 DEBUG TRAIN Batch 3/6000 loss 26.362549 loss_att 30.990219 loss_ctc 41.558968 loss_rnnt 23.242916 hw_loss 0.314833 lr 0.00089774 rank 3
2023-02-17 09:30:39,861 DEBUG TRAIN Batch 3/6000 loss 16.419222 loss_att 25.285477 loss_ctc 34.472832 loss_rnnt 12.075743 hw_loss 0.305774 lr 0.00089743 rank 5
2023-02-17 09:30:39,861 DEBUG TRAIN Batch 3/6000 loss 29.842556 loss_att 39.986412 loss_ctc 44.972363 loss_rnnt 25.642696 hw_loss 0.288345 lr 0.00089804 rank 7
2023-02-17 09:30:39,862 DEBUG TRAIN Batch 3/6000 loss 37.312634 loss_att 44.052895 loss_ctc 61.608829 loss_rnnt 32.525600 hw_loss 0.374043 lr 0.00089726 rank 2
2023-02-17 09:30:39,867 DEBUG TRAIN Batch 3/6000 loss 23.463306 loss_att 26.786781 loss_ctc 34.543213 loss_rnnt 21.099228 hw_loss 0.416371 lr 0.00089807 rank 6
2023-02-17 09:30:39,870 DEBUG TRAIN Batch 3/6000 loss 22.106094 loss_att 30.244503 loss_ctc 37.755974 loss_rnnt 18.236713 hw_loss 0.290717 lr 0.00089726 rank 1
2023-02-17 09:30:39,872 DEBUG TRAIN Batch 3/6000 loss 15.362037 loss_att 23.839840 loss_ctc 26.040258 loss_rnnt 12.064615 hw_loss 0.333931 lr 0.00089742 rank 0
2023-02-17 09:30:39,876 DEBUG TRAIN Batch 3/6000 loss 42.250549 loss_att 47.042675 loss_ctc 57.513306 loss_rnnt 39.010094 hw_loss 0.463122 lr 0.00089830 rank 4
2023-02-17 09:31:56,575 DEBUG TRAIN Batch 3/6100 loss 26.782583 loss_att 30.681396 loss_ctc 38.452244 loss_rnnt 24.285355 hw_loss 0.302837 lr 0.00089582 rank 2
2023-02-17 09:31:56,575 DEBUG TRAIN Batch 3/6100 loss 23.452354 loss_att 30.977802 loss_ctc 44.519905 loss_rnnt 19.001751 hw_loss 0.255946 lr 0.00089629 rank 3
2023-02-17 09:31:56,577 DEBUG TRAIN Batch 3/6100 loss 30.125891 loss_att 36.045280 loss_ctc 49.391815 loss_rnnt 26.245218 hw_loss 0.240008 lr 0.00089660 rank 7
2023-02-17 09:31:56,578 DEBUG TRAIN Batch 3/6100 loss 23.272964 loss_att 32.826859 loss_ctc 38.970963 loss_rnnt 19.038786 hw_loss 0.431876 lr 0.00089582 rank 1
2023-02-17 09:31:56,581 DEBUG TRAIN Batch 3/6100 loss 37.536793 loss_att 47.138847 loss_ctc 57.852955 loss_rnnt 32.730026 hw_loss 0.332878 lr 0.00089662 rank 6
2023-02-17 09:31:56,582 DEBUG TRAIN Batch 3/6100 loss 32.716904 loss_att 41.058395 loss_ctc 52.650513 loss_rnnt 28.224043 hw_loss 0.312653 lr 0.00089599 rank 5
2023-02-17 09:31:56,584 DEBUG TRAIN Batch 3/6100 loss 15.720028 loss_att 27.987667 loss_ctc 31.315403 loss_rnnt 11.022738 hw_loss 0.308208 lr 0.00089686 rank 4
2023-02-17 09:31:56,590 DEBUG TRAIN Batch 3/6100 loss 35.216534 loss_att 45.168655 loss_ctc 51.823448 loss_rnnt 30.829388 hw_loss 0.342130 lr 0.00089598 rank 0
2023-02-17 09:33:13,120 DEBUG TRAIN Batch 3/6200 loss 31.776112 loss_att 42.718468 loss_ctc 54.673874 loss_rnnt 26.370945 hw_loss 0.306857 lr 0.00089516 rank 7
2023-02-17 09:33:13,122 DEBUG TRAIN Batch 3/6200 loss 22.274809 loss_att 25.603157 loss_ctc 39.514381 loss_rnnt 19.143229 hw_loss 0.313688 lr 0.00089438 rank 1
2023-02-17 09:33:13,122 DEBUG TRAIN Batch 3/6200 loss 28.025530 loss_att 33.103424 loss_ctc 43.585514 loss_rnnt 24.727104 hw_loss 0.390344 lr 0.00089519 rank 6
2023-02-17 09:33:13,123 DEBUG TRAIN Batch 3/6200 loss 23.554928 loss_att 25.662930 loss_ctc 34.457928 loss_rnnt 21.498562 hw_loss 0.339435 lr 0.00089456 rank 5
2023-02-17 09:33:13,127 DEBUG TRAIN Batch 3/6200 loss 37.998436 loss_att 44.550240 loss_ctc 56.590069 loss_rnnt 34.058228 hw_loss 0.283053 lr 0.00089542 rank 4
2023-02-17 09:33:13,127 DEBUG TRAIN Batch 3/6200 loss 26.318424 loss_att 33.470676 loss_ctc 48.837475 loss_rnnt 21.716417 hw_loss 0.316905 lr 0.00089438 rank 2
2023-02-17 09:33:13,128 DEBUG TRAIN Batch 3/6200 loss 38.228672 loss_att 44.286232 loss_ctc 60.299355 loss_rnnt 33.918770 hw_loss 0.291806 lr 0.00089486 rank 3
2023-02-17 09:33:13,165 DEBUG TRAIN Batch 3/6200 loss 21.766653 loss_att 27.008320 loss_ctc 37.729366 loss_rnnt 18.377781 hw_loss 0.397828 lr 0.00089454 rank 0
2023-02-17 09:34:30,709 DEBUG TRAIN Batch 3/6300 loss 16.924904 loss_att 21.687250 loss_ctc 23.767605 loss_rnnt 14.879250 hw_loss 0.339046 lr 0.00089313 rank 5
2023-02-17 09:34:30,713 DEBUG TRAIN Batch 3/6300 loss 24.394371 loss_att 29.533522 loss_ctc 36.503777 loss_rnnt 21.576380 hw_loss 0.329198 lr 0.00089376 rank 6
2023-02-17 09:34:30,713 DEBUG TRAIN Batch 3/6300 loss 29.355537 loss_att 33.326077 loss_ctc 43.121834 loss_rnnt 26.478683 hw_loss 0.463572 lr 0.00089343 rank 3
2023-02-17 09:34:30,713 DEBUG TRAIN Batch 3/6300 loss 31.721792 loss_att 37.307739 loss_ctc 44.744915 loss_rnnt 28.662697 hw_loss 0.385296 lr 0.00089398 rank 4
2023-02-17 09:34:30,714 DEBUG TRAIN Batch 3/6300 loss 14.549032 loss_att 16.238401 loss_ctc 21.066923 loss_rnnt 13.094323 hw_loss 0.464593 lr 0.00089296 rank 2
2023-02-17 09:34:30,714 DEBUG TRAIN Batch 3/6300 loss 24.866598 loss_att 28.257792 loss_ctc 39.416893 loss_rnnt 22.098942 hw_loss 0.280084 lr 0.00089296 rank 1
2023-02-17 09:34:30,718 DEBUG TRAIN Batch 3/6300 loss 21.566702 loss_att 26.424395 loss_ctc 27.902771 loss_rnnt 19.526087 hw_loss 0.420498 lr 0.00089373 rank 7
2023-02-17 09:34:30,722 DEBUG TRAIN Batch 3/6300 loss 20.768202 loss_att 24.547575 loss_ctc 37.392448 loss_rnnt 17.612370 hw_loss 0.343859 lr 0.00089311 rank 0
2023-02-17 09:35:49,833 DEBUG TRAIN Batch 3/6400 loss 21.673565 loss_att 23.346983 loss_ctc 29.837076 loss_rnnt 19.991484 hw_loss 0.485499 lr 0.00089230 rank 7
2023-02-17 09:35:49,836 DEBUG TRAIN Batch 3/6400 loss 11.056982 loss_att 11.026684 loss_ctc 14.731135 loss_rnnt 10.315291 hw_loss 0.483493 lr 0.00089154 rank 1
2023-02-17 09:35:49,836 DEBUG TRAIN Batch 3/6400 loss 28.419670 loss_att 30.699888 loss_ctc 38.665703 loss_rnnt 26.372787 hw_loss 0.421313 lr 0.00089171 rank 5
2023-02-17 09:35:49,838 DEBUG TRAIN Batch 3/6400 loss 25.331053 loss_att 26.875635 loss_ctc 33.946564 loss_rnnt 23.663700 hw_loss 0.393186 lr 0.00089233 rank 6
2023-02-17 09:35:49,844 DEBUG TRAIN Batch 3/6400 loss 30.381901 loss_att 32.477127 loss_ctc 44.041729 loss_rnnt 27.983200 hw_loss 0.296896 lr 0.00089154 rank 2
2023-02-17 09:35:49,844 DEBUG TRAIN Batch 3/6400 loss 24.903917 loss_att 28.391218 loss_ctc 35.107178 loss_rnnt 22.657915 hw_loss 0.352703 lr 0.00089169 rank 0
2023-02-17 09:35:49,846 DEBUG TRAIN Batch 3/6400 loss 17.956718 loss_att 22.034311 loss_ctc 27.728600 loss_rnnt 15.617721 hw_loss 0.413551 lr 0.00089256 rank 4
2023-02-17 09:35:49,853 DEBUG TRAIN Batch 3/6400 loss 48.675411 loss_att 50.966526 loss_ctc 54.712902 loss_rnnt 47.243759 hw_loss 0.315806 lr 0.00089200 rank 3
2023-02-17 09:37:06,039 DEBUG TRAIN Batch 3/6500 loss 29.095930 loss_att 36.259644 loss_ctc 47.095074 loss_rnnt 25.117310 hw_loss 0.273739 lr 0.00089091 rank 6
2023-02-17 09:37:06,041 DEBUG TRAIN Batch 3/6500 loss 22.957897 loss_att 28.701443 loss_ctc 40.745964 loss_rnnt 19.240446 hw_loss 0.369379 lr 0.00089012 rank 1
2023-02-17 09:37:06,042 DEBUG TRAIN Batch 3/6500 loss 44.407486 loss_att 49.638969 loss_ctc 67.328697 loss_rnnt 40.121460 hw_loss 0.344195 lr 0.00089029 rank 5
2023-02-17 09:37:06,043 DEBUG TRAIN Batch 3/6500 loss 18.924278 loss_att 25.728260 loss_ctc 29.199821 loss_rnnt 16.057968 hw_loss 0.253956 lr 0.00089088 rank 7
2023-02-17 09:37:06,044 DEBUG TRAIN Batch 3/6500 loss 26.713861 loss_att 33.814491 loss_ctc 41.792980 loss_rnnt 23.137884 hw_loss 0.272441 lr 0.00089028 rank 0
2023-02-17 09:37:06,045 DEBUG TRAIN Batch 3/6500 loss 18.180145 loss_att 28.762543 loss_ctc 31.177284 loss_rnnt 14.157595 hw_loss 0.324597 lr 0.00089059 rank 3
2023-02-17 09:37:06,046 DEBUG TRAIN Batch 3/6500 loss 33.864044 loss_att 36.118370 loss_ctc 50.612419 loss_rnnt 30.988003 hw_loss 0.360117 lr 0.00089012 rank 2
2023-02-17 09:37:06,099 DEBUG TRAIN Batch 3/6500 loss 40.355103 loss_att 47.160198 loss_ctc 62.990936 loss_rnnt 35.762981 hw_loss 0.399359 lr 0.00089114 rank 4
2023-02-17 09:38:21,012 DEBUG TRAIN Batch 3/6600 loss 18.453274 loss_att 24.764837 loss_ctc 32.049023 loss_rnnt 15.230106 hw_loss 0.277663 lr 0.00088947 rank 7
2023-02-17 09:38:21,015 DEBUG TRAIN Batch 3/6600 loss 28.253136 loss_att 36.271362 loss_ctc 44.608913 loss_rnnt 24.248760 hw_loss 0.412427 lr 0.00088918 rank 3
2023-02-17 09:38:21,017 DEBUG TRAIN Batch 3/6600 loss 15.297512 loss_att 23.918550 loss_ctc 32.284363 loss_rnnt 11.145721 hw_loss 0.305005 lr 0.00088950 rank 6
2023-02-17 09:38:21,018 DEBUG TRAIN Batch 3/6600 loss 28.461334 loss_att 30.077997 loss_ctc 36.928242 loss_rnnt 26.794342 hw_loss 0.402636 lr 0.00088888 rank 5
2023-02-17 09:38:21,019 DEBUG TRAIN Batch 3/6600 loss 22.427685 loss_att 30.182365 loss_ctc 35.350204 loss_rnnt 18.998320 hw_loss 0.291425 lr 0.00088973 rank 4
2023-02-17 09:38:21,021 DEBUG TRAIN Batch 3/6600 loss 32.952003 loss_att 43.268654 loss_ctc 58.196701 loss_rnnt 27.348152 hw_loss 0.327298 lr 0.00088887 rank 0
2023-02-17 09:38:21,021 DEBUG TRAIN Batch 3/6600 loss 22.889566 loss_att 32.023064 loss_ctc 39.805996 loss_rnnt 18.599396 hw_loss 0.389899 lr 0.00088872 rank 1
2023-02-17 09:38:21,021 DEBUG TRAIN Batch 3/6600 loss 36.908501 loss_att 41.093880 loss_ctc 59.714005 loss_rnnt 32.870872 hw_loss 0.299657 lr 0.00088872 rank 2
2023-02-17 09:39:39,367 DEBUG TRAIN Batch 3/6700 loss 66.698349 loss_att 74.711800 loss_ctc 102.292572 loss_rnnt 60.180588 hw_loss 0.317189 lr 0.00088807 rank 7
2023-02-17 09:39:39,370 DEBUG TRAIN Batch 3/6700 loss 18.442919 loss_att 27.847033 loss_ctc 25.709063 loss_rnnt 15.421249 hw_loss 0.322552 lr 0.00088778 rank 3
2023-02-17 09:39:39,372 DEBUG TRAIN Batch 3/6700 loss 25.421164 loss_att 31.600491 loss_ctc 34.114983 loss_rnnt 22.816559 hw_loss 0.392927 lr 0.00088810 rank 6
2023-02-17 09:39:39,374 DEBUG TRAIN Batch 3/6700 loss 30.255175 loss_att 36.107143 loss_ctc 54.715267 loss_rnnt 25.631910 hw_loss 0.359109 lr 0.00088832 rank 4
2023-02-17 09:39:39,376 DEBUG TRAIN Batch 3/6700 loss 28.474804 loss_att 37.847382 loss_ctc 38.276207 loss_rnnt 25.091007 hw_loss 0.379549 lr 0.00088731 rank 1
2023-02-17 09:39:39,376 DEBUG TRAIN Batch 3/6700 loss 35.655903 loss_att 40.216042 loss_ctc 48.597664 loss_rnnt 32.855476 hw_loss 0.305313 lr 0.00088748 rank 5
2023-02-17 09:39:39,377 DEBUG TRAIN Batch 3/6700 loss 20.805773 loss_att 31.274265 loss_ctc 38.215965 loss_rnnt 16.207157 hw_loss 0.344165 lr 0.00088731 rank 2
2023-02-17 09:39:39,424 DEBUG TRAIN Batch 3/6700 loss 33.741840 loss_att 42.078442 loss_ctc 51.028408 loss_rnnt 29.581654 hw_loss 0.352474 lr 0.00088747 rank 0
2023-02-17 09:40:57,250 DEBUG TRAIN Batch 3/6800 loss 27.567686 loss_att 31.279558 loss_ctc 39.789230 loss_rnnt 24.994907 hw_loss 0.376616 lr 0.00088592 rank 1
2023-02-17 09:40:57,252 DEBUG TRAIN Batch 3/6800 loss 15.019574 loss_att 22.586842 loss_ctc 27.782993 loss_rnnt 11.591145 hw_loss 0.399726 lr 0.00088667 rank 7
2023-02-17 09:40:57,253 DEBUG TRAIN Batch 3/6800 loss 23.607309 loss_att 28.462957 loss_ctc 37.023582 loss_rnnt 20.689739 hw_loss 0.295510 lr 0.00088670 rank 6
2023-02-17 09:40:57,256 DEBUG TRAIN Batch 3/6800 loss 50.593262 loss_att 57.314919 loss_ctc 72.123009 loss_rnnt 46.237282 hw_loss 0.264401 lr 0.00088609 rank 5
2023-02-17 09:40:57,258 DEBUG TRAIN Batch 3/6800 loss 18.757948 loss_att 24.971123 loss_ctc 30.529737 loss_rnnt 15.771788 hw_loss 0.326159 lr 0.00088638 rank 3
2023-02-17 09:40:57,259 DEBUG TRAIN Batch 3/6800 loss 33.725521 loss_att 38.553837 loss_ctc 46.628792 loss_rnnt 30.848459 hw_loss 0.358046 lr 0.00088692 rank 4
2023-02-17 09:40:57,260 DEBUG TRAIN Batch 3/6800 loss 39.099983 loss_att 43.871719 loss_ctc 61.664932 loss_rnnt 34.999321 hw_loss 0.258098 lr 0.00088607 rank 0
2023-02-17 09:40:57,260 DEBUG TRAIN Batch 3/6800 loss 24.582323 loss_att 28.538872 loss_ctc 35.508514 loss_rnnt 22.114063 hw_loss 0.412730 lr 0.00088592 rank 2
2023-02-17 09:42:14,058 DEBUG TRAIN Batch 3/6900 loss 37.719807 loss_att 41.354355 loss_ctc 56.679771 loss_rnnt 34.270409 hw_loss 0.364673 lr 0.00088528 rank 7
2023-02-17 09:42:14,059 DEBUG TRAIN Batch 3/6900 loss 32.592873 loss_att 44.912926 loss_ctc 54.566841 loss_rnnt 27.040821 hw_loss 0.296578 lr 0.00088531 rank 6
2023-02-17 09:42:14,064 DEBUG TRAIN Batch 3/6900 loss 33.222652 loss_att 37.110229 loss_ctc 50.490330 loss_rnnt 29.960840 hw_loss 0.341134 lr 0.00088499 rank 3
2023-02-17 09:42:14,065 DEBUG TRAIN Batch 3/6900 loss 26.829861 loss_att 30.645111 loss_ctc 36.063480 loss_rnnt 24.673977 hw_loss 0.303160 lr 0.00088553 rank 4
2023-02-17 09:42:14,066 DEBUG TRAIN Batch 3/6900 loss 26.834986 loss_att 40.533596 loss_ctc 48.358410 loss_rnnt 21.040897 hw_loss 0.346080 lr 0.00088470 rank 5
2023-02-17 09:42:14,069 DEBUG TRAIN Batch 3/6900 loss 21.457781 loss_att 24.553699 loss_ctc 33.696644 loss_rnnt 19.049574 hw_loss 0.294701 lr 0.00088453 rank 1
2023-02-17 09:42:14,071 DEBUG TRAIN Batch 3/6900 loss 29.431833 loss_att 31.240616 loss_ctc 40.585068 loss_rnnt 27.371590 hw_loss 0.396353 lr 0.00088469 rank 0
2023-02-17 09:42:14,113 DEBUG TRAIN Batch 3/6900 loss 34.478268 loss_att 35.899086 loss_ctc 48.054962 loss_rnnt 32.155144 hw_loss 0.428876 lr 0.00088453 rank 2
2023-02-17 09:43:29,749 DEBUG TRAIN Batch 3/7000 loss 14.429229 loss_att 16.620697 loss_ctc 23.526285 loss_rnnt 12.541072 hw_loss 0.444230 lr 0.00088361 rank 3
2023-02-17 09:43:29,750 DEBUG TRAIN Batch 3/7000 loss 28.828529 loss_att 38.304802 loss_ctc 51.561695 loss_rnnt 23.787907 hw_loss 0.214271 lr 0.00088392 rank 6
2023-02-17 09:43:29,751 DEBUG TRAIN Batch 3/7000 loss 20.296764 loss_att 23.324896 loss_ctc 31.502300 loss_rnnt 18.027504 hw_loss 0.317925 lr 0.00088315 rank 1
2023-02-17 09:43:29,751 DEBUG TRAIN Batch 3/7000 loss 22.051229 loss_att 27.704140 loss_ctc 39.758732 loss_rnnt 18.393385 hw_loss 0.311742 lr 0.00088415 rank 4
2023-02-17 09:43:29,754 DEBUG TRAIN Batch 3/7000 loss 31.688709 loss_att 35.052120 loss_ctc 43.166275 loss_rnnt 29.339417 hw_loss 0.274254 lr 0.00088332 rank 5
2023-02-17 09:43:29,753 DEBUG TRAIN Batch 3/7000 loss 37.625000 loss_att 45.413097 loss_ctc 63.468437 loss_rnnt 32.434738 hw_loss 0.350344 lr 0.00088315 rank 2
2023-02-17 09:43:29,754 DEBUG TRAIN Batch 3/7000 loss 17.404846 loss_att 24.607294 loss_ctc 29.774616 loss_rnnt 14.135976 hw_loss 0.335771 lr 0.00088330 rank 0
2023-02-17 09:43:29,755 DEBUG TRAIN Batch 3/7000 loss 30.720966 loss_att 33.174732 loss_ctc 42.098053 loss_rnnt 28.517363 hw_loss 0.367323 lr 0.00088390 rank 7
2023-02-17 09:44:50,202 DEBUG TRAIN Batch 3/7100 loss 21.798450 loss_att 28.506567 loss_ctc 33.253036 loss_rnnt 18.702667 hw_loss 0.425402 lr 0.00088252 rank 7
2023-02-17 09:44:50,205 DEBUG TRAIN Batch 3/7100 loss 21.460810 loss_att 29.722782 loss_ctc 32.151779 loss_rnnt 18.223797 hw_loss 0.298416 lr 0.00088255 rank 6
2023-02-17 09:44:50,206 DEBUG TRAIN Batch 3/7100 loss 20.852703 loss_att 29.034990 loss_ctc 33.628075 loss_rnnt 17.323921 hw_loss 0.354263 lr 0.00088178 rank 1
2023-02-17 09:44:50,207 DEBUG TRAIN Batch 3/7100 loss 24.223080 loss_att 35.085487 loss_ctc 42.798668 loss_rnnt 19.383068 hw_loss 0.357723 lr 0.00088178 rank 2
2023-02-17 09:44:50,209 DEBUG TRAIN Batch 3/7100 loss 23.786627 loss_att 25.424973 loss_ctc 35.941284 loss_rnnt 21.664318 hw_loss 0.326290 lr 0.00088223 rank 3
2023-02-17 09:44:50,210 DEBUG TRAIN Batch 3/7100 loss 16.851002 loss_att 21.060122 loss_ctc 26.949224 loss_rnnt 14.450522 hw_loss 0.397927 lr 0.00088194 rank 5
2023-02-17 09:44:50,212 DEBUG TRAIN Batch 3/7100 loss 71.134972 loss_att 71.347969 loss_ctc 92.048363 loss_rnnt 68.133698 hw_loss 0.319154 lr 0.00088277 rank 4
2023-02-17 09:44:50,213 DEBUG TRAIN Batch 3/7100 loss 22.867638 loss_att 32.790421 loss_ctc 40.118507 loss_rnnt 18.419838 hw_loss 0.305862 lr 0.00088193 rank 0
2023-02-17 09:46:05,770 DEBUG TRAIN Batch 3/7200 loss 24.114622 loss_att 33.078281 loss_ctc 39.955299 loss_rnnt 20.048759 hw_loss 0.301953 lr 0.00088086 rank 3
2023-02-17 09:46:05,773 DEBUG TRAIN Batch 3/7200 loss 29.325487 loss_att 34.526688 loss_ctc 34.974220 loss_rnnt 27.383291 hw_loss 0.278986 lr 0.00088057 rank 5
2023-02-17 09:46:05,774 DEBUG TRAIN Batch 3/7200 loss 24.101118 loss_att 32.485016 loss_ctc 39.914757 loss_rnnt 20.152447 hw_loss 0.306386 lr 0.00088115 rank 7
2023-02-17 09:46:05,774 DEBUG TRAIN Batch 3/7200 loss 26.572443 loss_att 32.019100 loss_ctc 38.937523 loss_rnnt 23.670933 hw_loss 0.306561 lr 0.00088118 rank 6
2023-02-17 09:46:05,776 DEBUG TRAIN Batch 3/7200 loss 15.574974 loss_att 22.193239 loss_ctc 25.555607 loss_rnnt 12.747559 hw_loss 0.324394 lr 0.00088139 rank 4
2023-02-17 09:46:05,776 DEBUG TRAIN Batch 3/7200 loss 15.733624 loss_att 19.727358 loss_ctc 24.319580 loss_rnnt 13.614009 hw_loss 0.330141 lr 0.00088056 rank 0
2023-02-17 09:46:05,776 DEBUG TRAIN Batch 3/7200 loss 25.442381 loss_att 35.129333 loss_ctc 44.178982 loss_rnnt 20.855925 hw_loss 0.282848 lr 0.00088041 rank 1
2023-02-17 09:46:05,777 DEBUG TRAIN Batch 3/7200 loss 11.622510 loss_att 15.050415 loss_ctc 17.705605 loss_rnnt 9.958624 hw_loss 0.313546 lr 0.00088041 rank 2
2023-02-17 09:47:22,188 DEBUG TRAIN Batch 3/7300 loss 30.154026 loss_att 42.255180 loss_ctc 57.172440 loss_rnnt 23.910872 hw_loss 0.413377 lr 0.00087921 rank 5
2023-02-17 09:47:22,191 DEBUG TRAIN Batch 3/7300 loss 26.118185 loss_att 31.527992 loss_ctc 43.708061 loss_rnnt 22.543077 hw_loss 0.277182 lr 0.00087905 rank 2
2023-02-17 09:47:22,194 DEBUG TRAIN Batch 3/7300 loss 25.026121 loss_att 33.946037 loss_ctc 42.794960 loss_rnnt 20.684738 hw_loss 0.352919 lr 0.00087950 rank 3
2023-02-17 09:47:22,194 DEBUG TRAIN Batch 3/7300 loss 30.845814 loss_att 33.439663 loss_ctc 40.291641 loss_rnnt 28.826519 hw_loss 0.452030 lr 0.00087920 rank 0
2023-02-17 09:47:22,196 DEBUG TRAIN Batch 3/7300 loss 20.160532 loss_att 26.892670 loss_ctc 34.730858 loss_rnnt 16.642681 hw_loss 0.428833 lr 0.00087981 rank 6
2023-02-17 09:47:22,197 DEBUG TRAIN Batch 3/7300 loss 37.772953 loss_att 43.041740 loss_ctc 62.173340 loss_rnnt 33.301823 hw_loss 0.307474 lr 0.00087905 rank 1
2023-02-17 09:47:22,197 DEBUG TRAIN Batch 3/7300 loss 15.429295 loss_att 20.317833 loss_ctc 28.076218 loss_rnnt 12.549437 hw_loss 0.404801 lr 0.00087978 rank 7
2023-02-17 09:47:22,252 DEBUG TRAIN Batch 3/7300 loss 33.918468 loss_att 43.598000 loss_ctc 56.179031 loss_rnnt 28.863869 hw_loss 0.282409 lr 0.00088003 rank 4
2023-02-17 09:48:40,283 DEBUG TRAIN Batch 3/7400 loss 24.459000 loss_att 31.903526 loss_ctc 40.754509 loss_rnnt 20.588268 hw_loss 0.392047 lr 0.00087845 rank 6
2023-02-17 09:48:40,284 DEBUG TRAIN Batch 3/7400 loss 21.845238 loss_att 28.939404 loss_ctc 30.020279 loss_rnnt 19.157345 hw_loss 0.335720 lr 0.00087786 rank 5
2023-02-17 09:48:40,286 DEBUG TRAIN Batch 3/7400 loss 30.775278 loss_att 38.928131 loss_ctc 51.750088 loss_rnnt 26.155388 hw_loss 0.361271 lr 0.00087769 rank 1
2023-02-17 09:48:40,290 DEBUG TRAIN Batch 3/7400 loss 24.549591 loss_att 30.916922 loss_ctc 48.084797 loss_rnnt 19.988804 hw_loss 0.279923 lr 0.00087784 rank 0
2023-02-17 09:48:40,291 DEBUG TRAIN Batch 3/7400 loss 28.843521 loss_att 37.718071 loss_ctc 41.686253 loss_rnnt 25.194529 hw_loss 0.303219 lr 0.00087842 rank 7
2023-02-17 09:48:40,292 DEBUG TRAIN Batch 3/7400 loss 30.102633 loss_att 33.219162 loss_ctc 53.202030 loss_rnnt 26.217842 hw_loss 0.340434 lr 0.00087769 rank 2
2023-02-17 09:48:40,293 DEBUG TRAIN Batch 3/7400 loss 21.014481 loss_att 24.126675 loss_ctc 30.590237 loss_rnnt 18.875509 hw_loss 0.449562 lr 0.00087867 rank 4
2023-02-17 09:48:40,294 DEBUG TRAIN Batch 3/7400 loss 27.107302 loss_att 31.235577 loss_ctc 41.932659 loss_rnnt 24.105364 hw_loss 0.374190 lr 0.00087814 rank 3
2023-02-17 09:49:58,694 DEBUG TRAIN Batch 3/7500 loss 31.152752 loss_att 38.451283 loss_ctc 50.933895 loss_rnnt 26.860779 hw_loss 0.365215 lr 0.00087707 rank 7
2023-02-17 09:49:58,696 DEBUG TRAIN Batch 3/7500 loss 45.356873 loss_att 55.621246 loss_ctc 66.258896 loss_rnnt 40.314121 hw_loss 0.380513 lr 0.00087710 rank 6
2023-02-17 09:49:58,698 DEBUG TRAIN Batch 3/7500 loss 24.827244 loss_att 29.280499 loss_ctc 39.892319 loss_rnnt 21.732595 hw_loss 0.366229 lr 0.00087679 rank 3
2023-02-17 09:49:58,698 DEBUG TRAIN Batch 3/7500 loss 31.676226 loss_att 40.668865 loss_ctc 52.595642 loss_rnnt 26.886992 hw_loss 0.377715 lr 0.00087651 rank 5
2023-02-17 09:49:58,699 DEBUG TRAIN Batch 3/7500 loss 34.915146 loss_att 41.027725 loss_ctc 52.711681 loss_rnnt 31.151327 hw_loss 0.315809 lr 0.00087634 rank 2
2023-02-17 09:49:58,700 DEBUG TRAIN Batch 3/7500 loss 19.965937 loss_att 23.698711 loss_ctc 30.989378 loss_rnnt 17.586990 hw_loss 0.304872 lr 0.00087634 rank 1
2023-02-17 09:49:58,705 DEBUG TRAIN Batch 3/7500 loss 37.680477 loss_att 46.601898 loss_ctc 58.830944 loss_rnnt 32.916859 hw_loss 0.298632 lr 0.00087731 rank 4
2023-02-17 09:49:58,706 DEBUG TRAIN Batch 3/7500 loss 34.109871 loss_att 37.520126 loss_ctc 52.931282 loss_rnnt 30.689438 hw_loss 0.429112 lr 0.00087649 rank 0
2023-02-17 09:51:15,144 DEBUG TRAIN Batch 3/7600 loss 28.285295 loss_att 31.133993 loss_ctc 49.876503 loss_rnnt 24.634441 hw_loss 0.379290 lr 0.00087575 rank 6
2023-02-17 09:51:15,148 DEBUG TRAIN Batch 3/7600 loss 29.523582 loss_att 35.026588 loss_ctc 45.595287 loss_rnnt 26.025375 hw_loss 0.477586 lr 0.00087597 rank 4
2023-02-17 09:51:15,148 DEBUG TRAIN Batch 3/7600 loss 29.346020 loss_att 35.081451 loss_ctc 39.198505 loss_rnnt 26.668385 hw_loss 0.406653 lr 0.00087573 rank 7
2023-02-17 09:51:15,149 DEBUG TRAIN Batch 3/7600 loss 34.509529 loss_att 36.840996 loss_ctc 49.194424 loss_rnnt 31.926411 hw_loss 0.297815 lr 0.00087515 rank 0
2023-02-17 09:51:15,148 DEBUG TRAIN Batch 3/7600 loss 22.928820 loss_att 33.261208 loss_ctc 34.995594 loss_rnnt 19.089430 hw_loss 0.307515 lr 0.00087516 rank 5
2023-02-17 09:51:15,150 DEBUG TRAIN Batch 3/7600 loss 33.806171 loss_att 41.767727 loss_ctc 55.458607 loss_rnnt 29.109119 hw_loss 0.408274 lr 0.00087500 rank 1
2023-02-17 09:51:15,150 DEBUG TRAIN Batch 3/7600 loss 18.841419 loss_att 20.567375 loss_ctc 29.054390 loss_rnnt 16.892107 hw_loss 0.454481 lr 0.00087544 rank 3
2023-02-17 09:51:15,152 DEBUG TRAIN Batch 3/7600 loss 27.747417 loss_att 36.889248 loss_ctc 52.389843 loss_rnnt 22.496002 hw_loss 0.257606 lr 0.00087500 rank 2
2023-02-17 09:52:31,824 DEBUG TRAIN Batch 3/7700 loss 47.390038 loss_att 47.626820 loss_ctc 69.453079 loss_rnnt 44.255997 hw_loss 0.271766 lr 0.00087381 rank 0
2023-02-17 09:52:31,825 DEBUG TRAIN Batch 3/7700 loss 13.054926 loss_att 12.082026 loss_ctc 15.342651 loss_rnnt 12.685698 hw_loss 0.485209 lr 0.00087439 rank 7
2023-02-17 09:52:31,827 DEBUG TRAIN Batch 3/7700 loss 11.001541 loss_att 10.528303 loss_ctc 14.888830 loss_rnnt 10.384011 hw_loss 0.363510 lr 0.00087463 rank 4
2023-02-17 09:52:31,828 DEBUG TRAIN Batch 3/7700 loss 31.988720 loss_att 33.611393 loss_ctc 46.939907 loss_rnnt 29.432911 hw_loss 0.445843 lr 0.00087366 rank 1
2023-02-17 09:52:31,829 DEBUG TRAIN Batch 3/7700 loss 36.320179 loss_att 43.119881 loss_ctc 50.071697 loss_rnnt 32.979706 hw_loss 0.275619 lr 0.00087410 rank 3
2023-02-17 09:52:31,830 DEBUG TRAIN Batch 3/7700 loss 28.892134 loss_att 42.103455 loss_ctc 55.041389 loss_rnnt 22.607193 hw_loss 0.292706 lr 0.00087366 rank 2
2023-02-17 09:52:31,856 DEBUG TRAIN Batch 3/7700 loss 35.596169 loss_att 35.396599 loss_ctc 52.918972 loss_rnnt 33.112026 hw_loss 0.401903 lr 0.00087382 rank 5
2023-02-17 09:52:31,860 DEBUG TRAIN Batch 3/7700 loss 18.703436 loss_att 20.172981 loss_ctc 28.042778 loss_rnnt 16.942711 hw_loss 0.415446 lr 0.00087441 rank 6
2023-02-17 09:53:51,073 DEBUG TRAIN Batch 3/7800 loss 14.849625 loss_att 14.884169 loss_ctc 21.284637 loss_rnnt 13.749583 hw_loss 0.440870 lr 0.00087249 rank 5
2023-02-17 09:53:51,073 DEBUG TRAIN Batch 3/7800 loss 16.841705 loss_att 23.838379 loss_ctc 33.906921 loss_rnnt 12.987293 hw_loss 0.336962 lr 0.00087233 rank 1
2023-02-17 09:53:51,074 DEBUG TRAIN Batch 3/7800 loss 35.823364 loss_att 38.319443 loss_ctc 55.890022 loss_rnnt 32.433819 hw_loss 0.402711 lr 0.00087308 rank 6
2023-02-17 09:53:51,075 DEBUG TRAIN Batch 3/7800 loss 24.182171 loss_att 32.898617 loss_ctc 38.937233 loss_rnnt 20.235897 hw_loss 0.441827 lr 0.00087277 rank 3
2023-02-17 09:53:51,077 DEBUG TRAIN Batch 3/7800 loss 23.682243 loss_att 29.181358 loss_ctc 31.887539 loss_rnnt 21.318403 hw_loss 0.318711 lr 0.00087305 rank 7
2023-02-17 09:53:51,085 DEBUG TRAIN Batch 3/7800 loss 23.345901 loss_att 35.158096 loss_ctc 47.375549 loss_rnnt 17.598364 hw_loss 0.339649 lr 0.00087329 rank 4
2023-02-17 09:53:51,091 DEBUG TRAIN Batch 3/7800 loss 34.050968 loss_att 41.321693 loss_ctc 47.880272 loss_rnnt 30.598341 hw_loss 0.289832 lr 0.00087248 rank 0
2023-02-17 09:53:51,095 DEBUG TRAIN Batch 3/7800 loss 18.089529 loss_att 20.736984 loss_ctc 25.464788 loss_rnnt 16.343597 hw_loss 0.437012 lr 0.00087233 rank 2
2023-02-17 09:55:07,260 DEBUG TRAIN Batch 3/7900 loss 24.108538 loss_att 32.866867 loss_ctc 46.806900 loss_rnnt 19.090488 hw_loss 0.449876 lr 0.00087101 rank 1
2023-02-17 09:55:07,262 DEBUG TRAIN Batch 3/7900 loss 40.560993 loss_att 48.954189 loss_ctc 64.538292 loss_rnnt 35.484932 hw_loss 0.375842 lr 0.00087172 rank 7
2023-02-17 09:55:07,263 DEBUG TRAIN Batch 3/7900 loss 32.931503 loss_att 37.883869 loss_ctc 47.411583 loss_rnnt 29.811508 hw_loss 0.372842 lr 0.00087145 rank 3
2023-02-17 09:55:07,266 DEBUG TRAIN Batch 3/7900 loss 24.957726 loss_att 29.346106 loss_ctc 38.819366 loss_rnnt 22.088650 hw_loss 0.268462 lr 0.00087101 rank 2
2023-02-17 09:55:07,265 DEBUG TRAIN Batch 3/7900 loss 29.905424 loss_att 36.958183 loss_ctc 45.683819 loss_rnnt 26.122246 hw_loss 0.504074 lr 0.00087175 rank 6
2023-02-17 09:55:07,266 DEBUG TRAIN Batch 3/7900 loss 20.861225 loss_att 25.639421 loss_ctc 25.197546 loss_rnnt 19.103952 hw_loss 0.418982 lr 0.00087115 rank 0
2023-02-17 09:55:07,267 DEBUG TRAIN Batch 3/7900 loss 24.472555 loss_att 30.770557 loss_ctc 34.781319 loss_rnnt 21.593954 hw_loss 0.458437 lr 0.00087117 rank 5
2023-02-17 09:55:07,311 DEBUG TRAIN Batch 3/7900 loss 27.387672 loss_att 35.997498 loss_ctc 47.114269 loss_rnnt 22.865431 hw_loss 0.318872 lr 0.00087196 rank 4
2023-02-17 09:56:23,860 DEBUG TRAIN Batch 3/8000 loss 42.877941 loss_att 46.062572 loss_ctc 64.566093 loss_rnnt 39.168716 hw_loss 0.338523 lr 0.00087043 rank 6
2023-02-17 09:56:23,861 DEBUG TRAIN Batch 3/8000 loss 20.500465 loss_att 28.696396 loss_ctc 30.705549 loss_rnnt 17.347960 hw_loss 0.286208 lr 0.00087064 rank 4
2023-02-17 09:56:23,861 DEBUG TRAIN Batch 3/8000 loss 18.434462 loss_att 25.119446 loss_ctc 31.032604 loss_rnnt 15.227835 hw_loss 0.356023 lr 0.00087040 rank 7
2023-02-17 09:56:23,861 DEBUG TRAIN Batch 3/8000 loss 14.086711 loss_att 17.807833 loss_ctc 26.457392 loss_rnnt 11.472828 hw_loss 0.412938 lr 0.00086969 rank 2
2023-02-17 09:56:23,861 DEBUG TRAIN Batch 3/8000 loss 24.416752 loss_att 36.266693 loss_ctc 38.307697 loss_rnnt 20.023449 hw_loss 0.320980 lr 0.00086985 rank 5
2023-02-17 09:56:23,863 DEBUG TRAIN Batch 3/8000 loss 19.863235 loss_att 22.912516 loss_ctc 33.070824 loss_rnnt 17.325550 hw_loss 0.312781 lr 0.00087012 rank 3
2023-02-17 09:56:23,864 DEBUG TRAIN Batch 3/8000 loss 40.548321 loss_att 48.440987 loss_ctc 60.530647 loss_rnnt 36.122253 hw_loss 0.343541 lr 0.00086969 rank 1
2023-02-17 09:56:23,867 DEBUG TRAIN Batch 3/8000 loss 25.302748 loss_att 32.793663 loss_ctc 37.943588 loss_rnnt 21.961287 hw_loss 0.295937 lr 0.00086983 rank 0
2023-02-17 09:57:41,270 DEBUG TRAIN Batch 3/8100 loss 19.766644 loss_att 25.015614 loss_ctc 34.060314 loss_rnnt 16.590694 hw_loss 0.413124 lr 0.00086909 rank 7
2023-02-17 09:57:41,270 DEBUG TRAIN Batch 3/8100 loss 28.359589 loss_att 37.806000 loss_ctc 42.155731 loss_rnnt 24.375895 hw_loss 0.477987 lr 0.00086881 rank 3
2023-02-17 09:57:41,271 DEBUG TRAIN Batch 3/8100 loss 19.769938 loss_att 26.358135 loss_ctc 31.517672 loss_rnnt 16.682648 hw_loss 0.381159 lr 0.00086853 rank 5
2023-02-17 09:57:41,274 DEBUG TRAIN Batch 3/8100 loss 22.348291 loss_att 28.364153 loss_ctc 34.469070 loss_rnnt 19.349209 hw_loss 0.337139 lr 0.00086852 rank 0
2023-02-17 09:57:41,274 DEBUG TRAIN Batch 3/8100 loss 24.734478 loss_att 28.886566 loss_ctc 39.131462 loss_rnnt 21.830479 hw_loss 0.288715 lr 0.00086838 rank 2
2023-02-17 09:57:41,274 DEBUG TRAIN Batch 3/8100 loss 47.618649 loss_att 55.446648 loss_ctc 68.073547 loss_rnnt 43.176907 hw_loss 0.279044 lr 0.00086911 rank 6
2023-02-17 09:57:41,276 DEBUG TRAIN Batch 3/8100 loss 42.578896 loss_att 42.473774 loss_ctc 69.602242 loss_rnnt 38.781555 hw_loss 0.403596 lr 0.00086838 rank 1
2023-02-17 09:57:41,277 DEBUG TRAIN Batch 3/8100 loss 24.040348 loss_att 31.508205 loss_ctc 41.378563 loss_rnnt 20.116011 hw_loss 0.223130 lr 0.00086932 rank 4
2023-02-17 09:58:58,075 DEBUG TRAIN Batch 3/8200 loss 23.762751 loss_att 30.522392 loss_ctc 46.499348 loss_rnnt 19.160471 hw_loss 0.410255 lr 0.00086750 rank 3
2023-02-17 09:58:58,075 DEBUG TRAIN Batch 3/8200 loss 20.272278 loss_att 22.576450 loss_ctc 34.304146 loss_rnnt 17.756142 hw_loss 0.345724 lr 0.00086707 rank 2
2023-02-17 09:58:58,077 DEBUG TRAIN Batch 3/8200 loss 19.474709 loss_att 24.839773 loss_ctc 32.788567 loss_rnnt 16.411190 hw_loss 0.403734 lr 0.00086778 rank 7
2023-02-17 09:58:58,077 DEBUG TRAIN Batch 3/8200 loss 23.037886 loss_att 25.577095 loss_ctc 32.821060 loss_rnnt 21.053280 hw_loss 0.323137 lr 0.00086723 rank 5
2023-02-17 09:58:58,078 DEBUG TRAIN Batch 3/8200 loss 30.833200 loss_att 34.847675 loss_ctc 45.522514 loss_rnnt 27.885643 hw_loss 0.348913 lr 0.00086801 rank 4
2023-02-17 09:58:58,080 DEBUG TRAIN Batch 3/8200 loss 32.906078 loss_att 34.897503 loss_ctc 52.009315 loss_rnnt 29.763233 hw_loss 0.370239 lr 0.00086780 rank 6
2023-02-17 09:58:58,082 DEBUG TRAIN Batch 3/8200 loss 19.766972 loss_att 25.160200 loss_ctc 35.872246 loss_rnnt 16.340439 hw_loss 0.375968 lr 0.00086707 rank 1
2023-02-17 09:58:58,124 DEBUG TRAIN Batch 3/8200 loss 27.384262 loss_att 29.378254 loss_ctc 38.549664 loss_rnnt 25.317225 hw_loss 0.336596 lr 0.00086721 rank 0
2023-02-17 10:00:13,828 DEBUG TRAIN Batch 3/8300 loss 26.910809 loss_att 36.967163 loss_ctc 50.209595 loss_rnnt 21.621801 hw_loss 0.321060 lr 0.00086647 rank 7
2023-02-17 10:00:13,829 DEBUG TRAIN Batch 3/8300 loss 14.142393 loss_att 22.587433 loss_ctc 21.974779 loss_rnnt 11.194496 hw_loss 0.402320 lr 0.00086671 rank 4
2023-02-17 10:00:13,829 DEBUG TRAIN Batch 3/8300 loss 35.989540 loss_att 44.698380 loss_ctc 53.773876 loss_rnnt 31.666630 hw_loss 0.393550 lr 0.00086593 rank 5
2023-02-17 10:00:13,829 DEBUG TRAIN Batch 3/8300 loss 18.086855 loss_att 23.232471 loss_ctc 26.580128 loss_rnnt 15.720050 hw_loss 0.384835 lr 0.00086577 rank 2
2023-02-17 10:00:13,831 DEBUG TRAIN Batch 3/8300 loss 28.718618 loss_att 36.910110 loss_ctc 43.707039 loss_rnnt 24.918312 hw_loss 0.306660 lr 0.00086650 rank 6
2023-02-17 10:00:13,832 DEBUG TRAIN Batch 3/8300 loss 26.512365 loss_att 34.162621 loss_ctc 44.675812 loss_rnnt 22.399336 hw_loss 0.302225 lr 0.00086620 rank 3
2023-02-17 10:00:13,832 DEBUG TRAIN Batch 3/8300 loss 28.125963 loss_att 34.943230 loss_ctc 46.520260 loss_rnnt 24.116121 hw_loss 0.363404 lr 0.00086577 rank 1
2023-02-17 10:00:13,833 DEBUG TRAIN Batch 3/8300 loss 44.249611 loss_att 42.271095 loss_ctc 61.771576 loss_rnnt 42.133278 hw_loss 0.329579 lr 0.00086591 rank 0
2023-02-17 10:01:01,802 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 7
2023-02-17 10:01:01,803 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 5
2023-02-17 10:01:01,804 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 1
2023-02-17 10:01:01,811 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 0
2023-02-17 10:01:01,813 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 6
2023-02-17 10:01:01,815 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 2
2023-02-17 10:01:01,821 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 3
2023-02-17 10:01:01,824 DEBUG CV Batch 3/0 loss 4.691505 loss_att 4.282709 loss_ctc 8.077874 loss_rnnt 4.075552 hw_loss 0.461616 history loss 4.517746 rank 4
2023-02-17 10:01:12,921 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 6
2023-02-17 10:01:13,024 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 4
2023-02-17 10:01:13,034 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 7
2023-02-17 10:01:13,097 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 0
2023-02-17 10:01:13,119 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 5
2023-02-17 10:01:13,154 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 3
2023-02-17 10:01:13,168 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 1
2023-02-17 10:01:13,686 DEBUG CV Batch 3/100 loss 16.533186 loss_att 18.170380 loss_ctc 26.376440 loss_rnnt 14.718383 hw_loss 0.327996 history loss 8.255802 rank 2
2023-02-17 10:01:26,693 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 6
2023-02-17 10:01:26,874 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 4
2023-02-17 10:01:27,020 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 1
2023-02-17 10:01:27,102 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 5
2023-02-17 10:01:27,141 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 2
2023-02-17 10:01:27,264 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 0
2023-02-17 10:01:27,283 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 7
2023-02-17 10:01:28,643 DEBUG CV Batch 3/200 loss 23.289692 loss_att 35.938225 loss_ctc 26.951126 loss_rnnt 20.120100 hw_loss 0.284430 history loss 9.126875 rank 3
2023-02-17 10:01:38,780 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 6
2023-02-17 10:01:38,975 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 4
2023-02-17 10:01:39,047 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 1
2023-02-17 10:01:39,158 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 5
2023-02-17 10:01:39,250 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 7
2023-02-17 10:01:39,378 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 2
2023-02-17 10:01:39,546 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 0
2023-02-17 10:01:40,796 DEBUG CV Batch 3/300 loss 8.972362 loss_att 11.057156 loss_ctc 16.626104 loss_rnnt 7.343642 hw_loss 0.358614 history loss 9.308433 rank 3
2023-02-17 10:01:50,821 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 6
2023-02-17 10:01:50,961 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 1
2023-02-17 10:01:51,083 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 7
2023-02-17 10:01:51,109 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 4
2023-02-17 10:01:51,178 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 5
2023-02-17 10:01:51,488 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 2
2023-02-17 10:01:51,654 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 0
2023-02-17 10:01:52,915 DEBUG CV Batch 3/400 loss 37.461765 loss_att 127.352646 loss_ctc 41.072361 loss_rnnt 18.813391 hw_loss 0.353971 history loss 10.719114 rank 3
2023-02-17 10:02:01,295 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 6
2023-02-17 10:02:01,333 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 1
2023-02-17 10:02:01,588 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 7
2023-02-17 10:02:01,605 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 5
2023-02-17 10:02:02,122 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 2
2023-02-17 10:02:02,320 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 0
2023-02-17 10:02:03,151 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 4
2023-02-17 10:02:03,527 DEBUG CV Batch 3/500 loss 15.415850 loss_att 17.682314 loss_ctc 28.019201 loss_rnnt 13.080066 hw_loss 0.378832 history loss 12.141867 rank 3
2023-02-17 10:02:13,365 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 1
2023-02-17 10:02:13,412 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 6
2023-02-17 10:02:13,748 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 5
2023-02-17 10:02:13,754 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 7
2023-02-17 10:02:14,377 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 2
2023-02-17 10:02:14,575 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 0
2023-02-17 10:02:15,688 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 3
2023-02-17 10:02:16,604 DEBUG CV Batch 3/600 loss 12.383151 loss_att 12.383040 loss_ctc 18.187313 loss_rnnt 11.285466 hw_loss 0.607158 history loss 13.460266 rank 4
2023-02-17 10:02:25,044 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 6
2023-02-17 10:02:25,063 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 1
2023-02-17 10:02:25,664 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 5
2023-02-17 10:02:26,071 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 7
2023-02-17 10:02:26,333 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 2
2023-02-17 10:02:27,089 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 3
2023-02-17 10:02:27,145 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 0
2023-02-17 10:02:28,356 DEBUG CV Batch 3/700 loss 45.704319 loss_att 112.271301 loss_ctc 73.262260 loss_rnnt 28.627283 hw_loss 0.167339 history loss 14.361082 rank 4
2023-02-17 10:02:37,055 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 1
2023-02-17 10:02:37,241 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 6
2023-02-17 10:02:37,757 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 5
2023-02-17 10:02:38,202 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 7
2023-02-17 10:02:38,329 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 2
2023-02-17 10:02:38,338 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 3
2023-02-17 10:02:39,057 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 0
2023-02-17 10:02:39,531 DEBUG CV Batch 3/800 loss 17.893253 loss_att 19.185257 loss_ctc 27.808434 loss_rnnt 16.122063 hw_loss 0.357685 history loss 13.546876 rank 4
2023-02-17 10:02:50,995 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 1
2023-02-17 10:02:51,112 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 6
2023-02-17 10:02:51,733 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 5
2023-02-17 10:02:52,014 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 7
2023-02-17 10:02:52,201 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 2
2023-02-17 10:02:52,342 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 3
2023-02-17 10:02:52,895 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 4
2023-02-17 10:02:53,782 DEBUG CV Batch 3/900 loss 26.144171 loss_att 52.433743 loss_ctc 38.073437 loss_rnnt 19.194397 hw_loss 0.189917 history loss 13.241852 rank 0
2023-02-17 10:03:03,162 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 1
2023-02-17 10:03:03,309 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 6
2023-02-17 10:03:03,918 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 5
2023-02-17 10:03:04,142 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 7
2023-02-17 10:03:04,644 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 3
2023-02-17 10:03:05,023 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 2
2023-02-17 10:03:05,209 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 4
2023-02-17 10:03:06,177 DEBUG CV Batch 3/1000 loss 7.558489 loss_att 9.158147 loss_ctc 12.851410 loss_rnnt 6.315280 hw_loss 0.407916 history loss 12.883917 rank 0
2023-02-17 10:03:14,967 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 1
2023-02-17 10:03:15,133 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 6
2023-02-17 10:03:15,820 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 5
2023-02-17 10:03:15,903 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 7
2023-02-17 10:03:16,618 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 3
2023-02-17 10:03:17,271 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 4
2023-02-17 10:03:17,298 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 2
2023-02-17 10:03:18,106 DEBUG CV Batch 3/1100 loss 8.916627 loss_att 8.742742 loss_ctc 12.831088 loss_rnnt 8.148566 hw_loss 0.526706 history loss 12.863617 rank 0
2023-02-17 10:03:25,383 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 1
2023-02-17 10:03:25,509 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 6
2023-02-17 10:03:26,298 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 7
2023-02-17 10:03:26,350 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 5
2023-02-17 10:03:27,241 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 3
2023-02-17 10:03:27,748 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 4
2023-02-17 10:03:27,820 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 2
2023-02-17 10:03:28,944 DEBUG CV Batch 3/1200 loss 17.392241 loss_att 20.285694 loss_ctc 24.959299 loss_rnnt 15.574188 hw_loss 0.432038 history loss 13.422880 rank 0
2023-02-17 10:03:37,278 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 1
2023-02-17 10:03:37,326 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 6
2023-02-17 10:03:38,178 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 7
2023-02-17 10:03:38,363 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 5
2023-02-17 10:03:39,786 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 4
2023-02-17 10:03:39,860 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 2
2023-02-17 10:03:40,021 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 3
2023-02-17 10:03:41,458 DEBUG CV Batch 3/1300 loss 12.603068 loss_att 11.587535 loss_ctc 17.553219 loss_rnnt 11.890259 hw_loss 0.479806 history loss 13.849960 rank 0
2023-02-17 10:03:48,690 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 1
2023-02-17 10:03:49,366 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 6
2023-02-17 10:03:49,706 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 5
2023-02-17 10:03:49,753 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 7
2023-02-17 10:03:50,882 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 4
2023-02-17 10:03:51,074 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 2
2023-02-17 10:03:51,230 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 3
2023-02-17 10:03:53,733 DEBUG CV Batch 3/1400 loss 23.000942 loss_att 61.858833 loss_ctc 39.227249 loss_rnnt 12.902130 hw_loss 0.306987 history loss 14.295381 rank 0
2023-02-17 10:04:01,138 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 1
2023-02-17 10:04:01,860 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 6
2023-02-17 10:04:02,200 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 7
2023-02-17 10:04:02,225 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 5
2023-02-17 10:04:03,253 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 3
2023-02-17 10:04:03,301 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 4
2023-02-17 10:04:03,378 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 2
2023-02-17 10:04:06,223 DEBUG CV Batch 3/1500 loss 15.316493 loss_att 16.863234 loss_ctc 17.712538 loss_rnnt 14.476973 hw_loss 0.395062 history loss 13.994105 rank 0
2023-02-17 10:04:14,870 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 1
2023-02-17 10:04:15,668 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 6
2023-02-17 10:04:15,772 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 5
2023-02-17 10:04:15,833 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 7
2023-02-17 10:04:16,971 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 3
2023-02-17 10:04:17,257 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 2
2023-02-17 10:04:17,810 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 4
2023-02-17 10:04:20,056 DEBUG CV Batch 3/1600 loss 16.032536 loss_att 33.045628 loss_ctc 26.314064 loss_rnnt 11.072905 hw_loss 0.349017 history loss 13.861458 rank 0
2023-02-17 10:04:27,415 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 1
2023-02-17 10:04:28,114 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 6
2023-02-17 10:04:28,214 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 7
2023-02-17 10:04:28,249 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 5
2023-02-17 10:04:29,863 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 2
2023-02-17 10:04:30,117 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 3
2023-02-17 10:04:30,406 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 4
2023-02-17 10:04:32,545 DEBUG CV Batch 3/1700 loss 17.975624 loss_att 17.572556 loss_ctc 27.818325 loss_rnnt 16.463314 hw_loss 0.526057 history loss 13.673559 rank 0
2023-02-17 10:04:36,655 INFO Epoch 3 CV info cv_loss 13.614197355385615
2023-02-17 10:04:36,657 INFO Epoch 4 TRAIN info lr 0.0008653550133248297
2023-02-17 10:04:36,661 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:37,311 INFO Epoch 3 CV info cv_loss 13.6141973529046
2023-02-17 10:04:37,312 INFO Epoch 4 TRAIN info lr 0.0008661856629574307
2023-02-17 10:04:37,316 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:37,343 INFO Epoch 3 CV info cv_loss 13.614197351319508
2023-02-17 10:04:37,344 INFO Epoch 4 TRAIN info lr 0.0008659518010093379
2023-02-17 10:04:37,349 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:37,416 INFO Epoch 3 CV info cv_loss 13.614197353645459
2023-02-17 10:04:37,417 INFO Epoch 4 TRAIN info lr 0.000865757060682363
2023-02-17 10:04:37,420 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:39,055 INFO Epoch 3 CV info cv_loss 13.61419735314581
2023-02-17 10:04:39,056 INFO Epoch 4 TRAIN info lr 0.0008650570818097914
2023-02-17 10:04:39,058 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:39,537 INFO Epoch 3 CV info cv_loss 13.614197355833577
2023-02-17 10:04:39,538 INFO Epoch 4 TRAIN info lr 0.000866315668160702
2023-02-17 10:04:39,542 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:40,258 INFO Epoch 3 CV info cv_loss 13.61419735342148
2023-02-17 10:04:40,259 INFO Epoch 4 TRAIN info lr 0.0008656792013305546
2023-02-17 10:04:40,262 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:04:41,822 INFO Epoch 3 CV info cv_loss 13.614197354317401
2023-02-17 10:04:41,824 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/3.pt
2023-02-17 10:04:42,449 INFO Epoch 4 TRAIN info lr 0.000865419821788753
2023-02-17 10:04:42,453 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:05:53,200 DEBUG TRAIN Batch 4/0 loss 13.141980 loss_att 12.535892 loss_ctc 16.862175 loss_rnnt 12.446358 hw_loss 0.601528 lr 0.00086574 rank 5
2023-02-17 10:05:53,202 DEBUG TRAIN Batch 4/0 loss 11.477623 loss_att 10.885622 loss_ctc 15.842267 loss_rnnt 10.790608 hw_loss 0.418992 lr 0.00086534 rank 1
2023-02-17 10:05:53,202 DEBUG TRAIN Batch 4/0 loss 10.484229 loss_att 11.404615 loss_ctc 14.372589 loss_rnnt 9.498583 hw_loss 0.530850 lr 0.00086617 rank 6
2023-02-17 10:05:53,205 DEBUG TRAIN Batch 4/0 loss 13.305262 loss_att 12.619688 loss_ctc 15.594746 loss_rnnt 12.935485 hw_loss 0.378052 lr 0.00086594 rank 7
2023-02-17 10:05:53,221 DEBUG TRAIN Batch 4/0 loss 16.113096 loss_att 15.799724 loss_ctc 22.954496 loss_rnnt 15.025344 hw_loss 0.446701 lr 0.00086630 rank 4
2023-02-17 10:05:53,242 DEBUG TRAIN Batch 4/0 loss 12.753676 loss_att 13.590712 loss_ctc 17.498245 loss_rnnt 11.672685 hw_loss 0.526828 lr 0.00086541 rank 0
2023-02-17 10:05:53,268 DEBUG TRAIN Batch 4/0 loss 14.568936 loss_att 13.430140 loss_ctc 17.271799 loss_rnnt 14.204155 hw_loss 0.435297 lr 0.00086567 rank 3
2023-02-17 10:05:53,279 DEBUG TRAIN Batch 4/0 loss 13.982882 loss_att 13.884883 loss_ctc 14.915020 loss_rnnt 13.664370 hw_loss 0.400925 lr 0.00086504 rank 2
2023-02-17 10:07:07,828 DEBUG TRAIN Batch 4/100 loss 13.353066 loss_att 19.178461 loss_ctc 22.874161 loss_rnnt 10.719924 hw_loss 0.372346 lr 0.00086437 rank 3
2023-02-17 10:07:07,832 DEBUG TRAIN Batch 4/100 loss 30.164904 loss_att 40.114815 loss_ctc 42.745628 loss_rnnt 26.326559 hw_loss 0.320495 lr 0.00086464 rank 7
2023-02-17 10:07:07,833 DEBUG TRAIN Batch 4/100 loss 20.187208 loss_att 24.034658 loss_ctc 30.356274 loss_rnnt 17.887419 hw_loss 0.327048 lr 0.00086501 rank 4
2023-02-17 10:07:07,834 DEBUG TRAIN Batch 4/100 loss 25.442890 loss_att 35.658600 loss_ctc 50.188904 loss_rnnt 19.900642 hw_loss 0.374317 lr 0.00086445 rank 5
2023-02-17 10:07:07,836 DEBUG TRAIN Batch 4/100 loss 27.454092 loss_att 31.231369 loss_ctc 43.580196 loss_rnnt 24.397579 hw_loss 0.282956 lr 0.00086375 rank 2
2023-02-17 10:07:07,838 DEBUG TRAIN Batch 4/100 loss 41.745651 loss_att 48.188942 loss_ctc 54.514359 loss_rnnt 38.566936 hw_loss 0.351686 lr 0.00086411 rank 0
2023-02-17 10:07:07,862 DEBUG TRAIN Batch 4/100 loss 21.585438 loss_att 26.383102 loss_ctc 44.740398 loss_rnnt 17.360771 hw_loss 0.333384 lr 0.00086488 rank 6
2023-02-17 10:07:07,877 DEBUG TRAIN Batch 4/100 loss 14.797131 loss_att 19.866985 loss_ctc 21.080769 loss_rnnt 12.722762 hw_loss 0.417334 lr 0.00086405 rank 1
2023-02-17 10:08:23,457 DEBUG TRAIN Batch 4/200 loss 16.030268 loss_att 18.995562 loss_ctc 24.382198 loss_rnnt 14.058512 hw_loss 0.497074 lr 0.00086335 rank 7
2023-02-17 10:08:23,457 DEBUG TRAIN Batch 4/200 loss 22.260912 loss_att 31.326321 loss_ctc 37.570007 loss_rnnt 18.244942 hw_loss 0.303144 lr 0.00086358 rank 6
2023-02-17 10:08:23,458 DEBUG TRAIN Batch 4/200 loss 54.238483 loss_att 62.479599 loss_ctc 77.217476 loss_rnnt 49.411743 hw_loss 0.214970 lr 0.00086247 rank 2
2023-02-17 10:08:23,462 DEBUG TRAIN Batch 4/200 loss 19.858368 loss_att 28.905113 loss_ctc 31.873871 loss_rnnt 16.260611 hw_loss 0.349387 lr 0.00086316 rank 5
2023-02-17 10:08:23,463 DEBUG TRAIN Batch 4/200 loss 21.141157 loss_att 25.328342 loss_ctc 31.957357 loss_rnnt 18.694670 hw_loss 0.312918 lr 0.00086276 rank 1
2023-02-17 10:08:23,465 DEBUG TRAIN Batch 4/200 loss 13.361529 loss_att 19.196236 loss_ctc 25.142096 loss_rnnt 10.458448 hw_loss 0.310120 lr 0.00086371 rank 4
2023-02-17 10:08:23,465 DEBUG TRAIN Batch 4/200 loss 25.416941 loss_att 32.675766 loss_ctc 44.007423 loss_rnnt 21.334270 hw_loss 0.285331 lr 0.00086283 rank 0
2023-02-17 10:08:23,466 DEBUG TRAIN Batch 4/200 loss 20.321472 loss_att 26.883667 loss_ctc 35.092445 loss_rnnt 16.838682 hw_loss 0.376663 lr 0.00086308 rank 3
2023-02-17 10:09:41,131 DEBUG TRAIN Batch 4/300 loss 21.313538 loss_att 32.132271 loss_ctc 39.361885 loss_rnnt 16.567829 hw_loss 0.329090 lr 0.00086243 rank 4
2023-02-17 10:09:41,131 DEBUG TRAIN Batch 4/300 loss 18.598921 loss_att 26.359463 loss_ctc 33.536911 loss_rnnt 14.838984 hw_loss 0.405177 lr 0.00086230 rank 6
2023-02-17 10:09:41,133 DEBUG TRAIN Batch 4/300 loss 27.855068 loss_att 36.991093 loss_ctc 45.649765 loss_rnnt 23.487112 hw_loss 0.315236 lr 0.00086207 rank 7
2023-02-17 10:09:41,136 DEBUG TRAIN Batch 4/300 loss 30.293240 loss_att 34.236263 loss_ctc 46.802647 loss_rnnt 27.158087 hw_loss 0.272427 lr 0.00086180 rank 3
2023-02-17 10:09:41,138 DEBUG TRAIN Batch 4/300 loss 21.808363 loss_att 30.035929 loss_ctc 45.340523 loss_rnnt 16.827618 hw_loss 0.370517 lr 0.00086188 rank 5
2023-02-17 10:09:41,140 DEBUG TRAIN Batch 4/300 loss 29.423843 loss_att 33.483170 loss_ctc 46.451721 loss_rnnt 26.205521 hw_loss 0.255134 lr 0.00086119 rank 2
2023-02-17 10:09:41,141 DEBUG TRAIN Batch 4/300 loss 26.102039 loss_att 30.449081 loss_ctc 36.384998 loss_rnnt 23.674202 hw_loss 0.351314 lr 0.00086148 rank 1
2023-02-17 10:09:41,188 DEBUG TRAIN Batch 4/300 loss 26.310940 loss_att 29.042055 loss_ctc 34.575695 loss_rnnt 24.509247 hw_loss 0.287821 lr 0.00086154 rank 0
2023-02-17 10:10:59,092 DEBUG TRAIN Batch 4/400 loss 28.089281 loss_att 35.132393 loss_ctc 42.942162 loss_rnnt 24.492289 hw_loss 0.389972 lr 0.00086102 rank 6
2023-02-17 10:10:59,093 DEBUG TRAIN Batch 4/400 loss 32.391785 loss_att 37.485603 loss_ctc 44.361824 loss_rnnt 29.605324 hw_loss 0.321924 lr 0.00086079 rank 7
2023-02-17 10:10:59,097 DEBUG TRAIN Batch 4/400 loss 27.360668 loss_att 30.906738 loss_ctc 40.110477 loss_rnnt 24.760698 hw_loss 0.357715 lr 0.00086020 rank 1
2023-02-17 10:10:59,099 DEBUG TRAIN Batch 4/400 loss 24.118492 loss_att 31.830265 loss_ctc 41.131638 loss_rnnt 20.093662 hw_loss 0.401355 lr 0.00086060 rank 5
2023-02-17 10:10:59,099 DEBUG TRAIN Batch 4/400 loss 28.933758 loss_att 32.224129 loss_ctc 47.464478 loss_rnnt 25.612198 hw_loss 0.361353 lr 0.00086027 rank 0
2023-02-17 10:10:59,100 DEBUG TRAIN Batch 4/400 loss 23.924189 loss_att 30.186184 loss_ctc 35.626377 loss_rnnt 20.895226 hw_loss 0.405512 lr 0.00085991 rank 2
2023-02-17 10:10:59,101 DEBUG TRAIN Batch 4/400 loss 22.203915 loss_att 30.821507 loss_ctc 39.074188 loss_rnnt 18.087921 hw_loss 0.268323 lr 0.00086115 rank 4
2023-02-17 10:10:59,102 DEBUG TRAIN Batch 4/400 loss 43.156658 loss_att 50.578503 loss_ctc 72.008499 loss_rnnt 37.596264 hw_loss 0.429582 lr 0.00086052 rank 3
2023-02-17 10:12:14,169 DEBUG TRAIN Batch 4/500 loss 21.106945 loss_att 22.442078 loss_ctc 32.477489 loss_rnnt 19.147055 hw_loss 0.331481 lr 0.00085975 rank 6
2023-02-17 10:12:14,171 DEBUG TRAIN Batch 4/500 loss 25.994846 loss_att 31.539347 loss_ctc 39.946297 loss_rnnt 22.858711 hw_loss 0.313208 lr 0.00085925 rank 3
2023-02-17 10:12:14,172 DEBUG TRAIN Batch 4/500 loss 28.412821 loss_att 37.531960 loss_ctc 50.993210 loss_rnnt 23.390701 hw_loss 0.351698 lr 0.00085952 rank 7
2023-02-17 10:12:14,173 DEBUG TRAIN Batch 4/500 loss 24.339994 loss_att 28.758961 loss_ctc 39.835762 loss_rnnt 21.160999 hw_loss 0.429558 lr 0.00085864 rank 2
2023-02-17 10:12:14,173 DEBUG TRAIN Batch 4/500 loss 20.574865 loss_att 24.048679 loss_ctc 27.987656 loss_rnnt 18.687946 hw_loss 0.382095 lr 0.00085893 rank 1
2023-02-17 10:12:14,174 DEBUG TRAIN Batch 4/500 loss 24.895803 loss_att 28.725622 loss_ctc 34.056053 loss_rnnt 22.713110 hw_loss 0.366305 lr 0.00085933 rank 5
2023-02-17 10:12:14,175 DEBUG TRAIN Batch 4/500 loss 32.623089 loss_att 37.199684 loss_ctc 45.592766 loss_rnnt 29.803234 hw_loss 0.328584 lr 0.00085987 rank 4
2023-02-17 10:12:14,181 DEBUG TRAIN Batch 4/500 loss 24.950272 loss_att 31.863674 loss_ctc 42.509979 loss_rnnt 21.052357 hw_loss 0.326137 lr 0.00085900 rank 0
2023-02-17 10:13:29,102 DEBUG TRAIN Batch 4/600 loss 19.156206 loss_att 22.122833 loss_ctc 31.117096 loss_rnnt 16.747448 hw_loss 0.413711 lr 0.00085767 rank 1
2023-02-17 10:13:29,103 DEBUG TRAIN Batch 4/600 loss 27.278065 loss_att 29.104811 loss_ctc 39.166973 loss_rnnt 25.116257 hw_loss 0.396136 lr 0.00085799 rank 3
2023-02-17 10:13:29,105 DEBUG TRAIN Batch 4/600 loss 15.220688 loss_att 19.609625 loss_ctc 26.023932 loss_rnnt 12.652758 hw_loss 0.468206 lr 0.00085848 rank 6
2023-02-17 10:13:29,113 DEBUG TRAIN Batch 4/600 loss 42.777054 loss_att 42.393307 loss_ctc 59.024559 loss_rnnt 40.468468 hw_loss 0.410619 lr 0.00085860 rank 4
2023-02-17 10:13:29,114 DEBUG TRAIN Batch 4/600 loss 16.614727 loss_att 17.176065 loss_ctc 22.663830 loss_rnnt 15.471659 hw_loss 0.420476 lr 0.00085738 rank 2
2023-02-17 10:13:29,114 DEBUG TRAIN Batch 4/600 loss 15.425508 loss_att 19.226355 loss_ctc 22.624748 loss_rnnt 13.491824 hw_loss 0.400530 lr 0.00085773 rank 0
2023-02-17 10:13:29,147 DEBUG TRAIN Batch 4/600 loss 19.107655 loss_att 21.048418 loss_ctc 27.098074 loss_rnnt 17.431620 hw_loss 0.417178 lr 0.00085806 rank 5
2023-02-17 10:13:29,161 DEBUG TRAIN Batch 4/600 loss 19.385689 loss_att 24.315937 loss_ctc 32.449257 loss_rnnt 16.462345 hw_loss 0.366537 lr 0.00085825 rank 7
2023-02-17 10:14:48,131 DEBUG TRAIN Batch 4/700 loss 50.203842 loss_att 65.152412 loss_ctc 71.074295 loss_rnnt 44.243572 hw_loss 0.352185 lr 0.00085680 rank 5
2023-02-17 10:14:48,135 DEBUG TRAIN Batch 4/700 loss 18.664467 loss_att 29.304531 loss_ctc 33.631050 loss_rnnt 14.359462 hw_loss 0.340215 lr 0.00085722 rank 6
2023-02-17 10:14:48,136 DEBUG TRAIN Batch 4/700 loss 26.757668 loss_att 31.993893 loss_ctc 45.368645 loss_rnnt 23.089283 hw_loss 0.261887 lr 0.00085641 rank 1
2023-02-17 10:14:48,138 DEBUG TRAIN Batch 4/700 loss 16.621033 loss_att 28.445833 loss_ctc 30.777100 loss_rnnt 12.190532 hw_loss 0.333870 lr 0.00085699 rank 7
2023-02-17 10:14:48,142 DEBUG TRAIN Batch 4/700 loss 33.209316 loss_att 34.783867 loss_ctc 52.798901 loss_rnnt 30.121647 hw_loss 0.301519 lr 0.00085672 rank 3
2023-02-17 10:14:48,144 DEBUG TRAIN Batch 4/700 loss 14.261993 loss_att 18.893961 loss_ctc 28.901306 loss_rnnt 11.170016 hw_loss 0.400641 lr 0.00085734 rank 4
2023-02-17 10:14:48,146 DEBUG TRAIN Batch 4/700 loss 26.617306 loss_att 33.041458 loss_ctc 39.834831 loss_rnnt 23.389267 hw_loss 0.339135 lr 0.00085612 rank 2
2023-02-17 10:14:48,157 DEBUG TRAIN Batch 4/700 loss 24.583082 loss_att 28.142464 loss_ctc 35.417023 loss_rnnt 22.261497 hw_loss 0.309719 lr 0.00085647 rank 0
2023-02-17 10:16:06,229 DEBUG TRAIN Batch 4/800 loss 20.082495 loss_att 23.777786 loss_ctc 34.884338 loss_rnnt 17.167952 hw_loss 0.378576 lr 0.00085596 rank 6
2023-02-17 10:16:06,231 DEBUG TRAIN Batch 4/800 loss 23.476187 loss_att 26.579025 loss_ctc 29.583767 loss_rnnt 21.839737 hw_loss 0.377883 lr 0.00085573 rank 7
2023-02-17 10:16:06,235 DEBUG TRAIN Batch 4/800 loss 18.454878 loss_att 23.857262 loss_ctc 35.234165 loss_rnnt 14.979376 hw_loss 0.295853 lr 0.00085522 rank 0
2023-02-17 10:16:06,236 DEBUG TRAIN Batch 4/800 loss 25.929497 loss_att 30.451710 loss_ctc 40.162567 loss_rnnt 22.924603 hw_loss 0.380083 lr 0.00085516 rank 1
2023-02-17 10:16:06,238 DEBUG TRAIN Batch 4/800 loss 27.044510 loss_att 37.831001 loss_ctc 48.096203 loss_rnnt 21.909496 hw_loss 0.320294 lr 0.00085608 rank 4
2023-02-17 10:16:06,238 DEBUG TRAIN Batch 4/800 loss 28.296492 loss_att 36.226669 loss_ctc 42.983822 loss_rnnt 24.575109 hw_loss 0.331946 lr 0.00085487 rank 2
2023-02-17 10:16:06,239 DEBUG TRAIN Batch 4/800 loss 27.703199 loss_att 37.440002 loss_ctc 43.382889 loss_rnnt 23.486767 hw_loss 0.334588 lr 0.00085547 rank 3
2023-02-17 10:16:06,241 DEBUG TRAIN Batch 4/800 loss 20.713827 loss_att 29.634203 loss_ctc 32.697163 loss_rnnt 17.185369 hw_loss 0.274884 lr 0.00085555 rank 5
2023-02-17 10:17:23,212 DEBUG TRAIN Batch 4/900 loss 29.493277 loss_att 32.773575 loss_ctc 37.392437 loss_rnnt 27.610176 hw_loss 0.325909 lr 0.00085391 rank 1
2023-02-17 10:17:23,213 DEBUG TRAIN Batch 4/900 loss 42.013466 loss_att 42.741657 loss_ctc 66.391937 loss_rnnt 38.454464 hw_loss 0.305445 lr 0.00085448 rank 7
2023-02-17 10:17:23,214 DEBUG TRAIN Batch 4/900 loss 15.456220 loss_att 22.447481 loss_ctc 28.887386 loss_rnnt 12.106255 hw_loss 0.301668 lr 0.00085430 rank 5
2023-02-17 10:17:23,217 DEBUG TRAIN Batch 4/900 loss 26.436810 loss_att 34.175018 loss_ctc 42.274059 loss_rnnt 22.562700 hw_loss 0.402811 lr 0.00085362 rank 2
2023-02-17 10:17:23,217 DEBUG TRAIN Batch 4/900 loss 37.630535 loss_att 41.746937 loss_ctc 58.574234 loss_rnnt 33.837059 hw_loss 0.333191 lr 0.00085471 rank 6
2023-02-17 10:17:23,220 DEBUG TRAIN Batch 4/900 loss 29.052931 loss_att 32.661499 loss_ctc 40.060696 loss_rnnt 26.667063 hw_loss 0.368345 lr 0.00085483 rank 4
2023-02-17 10:17:23,222 DEBUG TRAIN Batch 4/900 loss 32.705967 loss_att 44.476933 loss_ctc 46.610409 loss_rnnt 28.339186 hw_loss 0.297489 lr 0.00085397 rank 0
2023-02-17 10:17:23,262 DEBUG TRAIN Batch 4/900 loss 22.687185 loss_att 27.664822 loss_ctc 34.114708 loss_rnnt 19.978836 hw_loss 0.354660 lr 0.00085422 rank 3
2023-02-17 10:18:40,504 DEBUG TRAIN Batch 4/1000 loss 31.021349 loss_att 35.446335 loss_ctc 45.205132 loss_rnnt 28.040123 hw_loss 0.384482 lr 0.00085324 rank 7
2023-02-17 10:18:40,505 DEBUG TRAIN Batch 4/1000 loss 49.470329 loss_att 55.797295 loss_ctc 73.113136 loss_rnnt 44.879013 hw_loss 0.325402 lr 0.00085346 rank 6
2023-02-17 10:18:40,508 DEBUG TRAIN Batch 4/1000 loss 25.446413 loss_att 31.360609 loss_ctc 44.883873 loss_rnnt 21.520929 hw_loss 0.283087 lr 0.00085359 rank 4
2023-02-17 10:18:40,511 DEBUG TRAIN Batch 4/1000 loss 23.304598 loss_att 31.487627 loss_ctc 37.336906 loss_rnnt 19.653965 hw_loss 0.268223 lr 0.00085298 rank 3
2023-02-17 10:18:40,516 DEBUG TRAIN Batch 4/1000 loss 19.235233 loss_att 28.507675 loss_ctc 31.990261 loss_rnnt 15.510635 hw_loss 0.317696 lr 0.00085267 rank 1
2023-02-17 10:18:40,522 DEBUG TRAIN Batch 4/1000 loss 27.950050 loss_att 31.757248 loss_ctc 45.924355 loss_rnnt 24.607738 hw_loss 0.345557 lr 0.00085305 rank 5
2023-02-17 10:18:40,526 DEBUG TRAIN Batch 4/1000 loss 30.867859 loss_att 43.195461 loss_ctc 45.684502 loss_rnnt 26.205172 hw_loss 0.415528 lr 0.00085273 rank 0
2023-02-17 10:18:40,532 DEBUG TRAIN Batch 4/1000 loss 21.145224 loss_att 25.729721 loss_ctc 29.976120 loss_rnnt 18.818993 hw_loss 0.434772 lr 0.00085238 rank 2
2023-02-17 10:19:58,571 DEBUG TRAIN Batch 4/1100 loss 17.686165 loss_att 24.866951 loss_ctc 33.492798 loss_rnnt 13.985476 hw_loss 0.294336 lr 0.00085181 rank 5
2023-02-17 10:19:58,571 DEBUG TRAIN Batch 4/1100 loss 19.009485 loss_att 26.619886 loss_ctc 29.271070 loss_rnnt 15.928110 hw_loss 0.358279 lr 0.00085174 rank 3
2023-02-17 10:19:58,572 DEBUG TRAIN Batch 4/1100 loss 14.695512 loss_att 20.759315 loss_ctc 25.811392 loss_rnnt 11.838605 hw_loss 0.303802 lr 0.00085222 rank 6
2023-02-17 10:19:58,573 DEBUG TRAIN Batch 4/1100 loss 17.961996 loss_att 22.182217 loss_ctc 31.047386 loss_rnnt 15.143138 hw_loss 0.431427 lr 0.00085200 rank 7
2023-02-17 10:19:58,573 DEBUG TRAIN Batch 4/1100 loss 21.688936 loss_att 29.684872 loss_ctc 33.052166 loss_rnnt 18.354637 hw_loss 0.412525 lr 0.00085143 rank 1
2023-02-17 10:19:58,574 DEBUG TRAIN Batch 4/1100 loss 27.181238 loss_att 30.906567 loss_ctc 42.204174 loss_rnnt 24.169643 hw_loss 0.494008 lr 0.00085234 rank 4
2023-02-17 10:19:58,579 DEBUG TRAIN Batch 4/1100 loss 38.836826 loss_att 42.959034 loss_ctc 62.694485 loss_rnnt 34.650177 hw_loss 0.339732 lr 0.00085149 rank 0
2023-02-17 10:19:58,616 DEBUG TRAIN Batch 4/1100 loss 16.998011 loss_att 23.294006 loss_ctc 30.321424 loss_rnnt 13.782088 hw_loss 0.338000 lr 0.00085115 rank 2
2023-02-17 10:21:16,250 DEBUG TRAIN Batch 4/1200 loss 22.871157 loss_att 25.785128 loss_ctc 36.574471 loss_rnnt 20.157211 hw_loss 0.570080 lr 0.00085058 rank 5
2023-02-17 10:21:16,251 DEBUG TRAIN Batch 4/1200 loss 31.018867 loss_att 33.999191 loss_ctc 39.874378 loss_rnnt 29.070692 hw_loss 0.321330 lr 0.00085050 rank 3
2023-02-17 10:21:16,251 DEBUG TRAIN Batch 4/1200 loss 22.056805 loss_att 23.278542 loss_ctc 31.965015 loss_rnnt 20.288984 hw_loss 0.379464 lr 0.00085076 rank 7
2023-02-17 10:21:16,254 DEBUG TRAIN Batch 4/1200 loss 22.844975 loss_att 28.697418 loss_ctc 37.363487 loss_rnnt 19.612766 hw_loss 0.236096 lr 0.00085020 rank 1
2023-02-17 10:21:16,255 DEBUG TRAIN Batch 4/1200 loss 26.768618 loss_att 32.278706 loss_ctc 41.640308 loss_rnnt 23.486637 hw_loss 0.369503 lr 0.00085099 rank 6
2023-02-17 10:21:16,258 DEBUG TRAIN Batch 4/1200 loss 25.338871 loss_att 29.106615 loss_ctc 34.835842 loss_rnnt 23.141397 hw_loss 0.333118 lr 0.00084991 rank 2
2023-02-17 10:21:16,259 DEBUG TRAIN Batch 4/1200 loss 13.606174 loss_att 15.467838 loss_ctc 20.973104 loss_rnnt 12.049080 hw_loss 0.379692 lr 0.00085111 rank 4
2023-02-17 10:21:16,306 DEBUG TRAIN Batch 4/1200 loss 25.413145 loss_att 27.362167 loss_ctc 43.895023 loss_rnnt 22.344103 hw_loss 0.403105 lr 0.00085026 rank 0
2023-02-17 10:22:33,474 DEBUG TRAIN Batch 4/1300 loss 16.015217 loss_att 15.231912 loss_ctc 21.975754 loss_rnnt 15.132226 hw_loss 0.459211 lr 0.00084976 rank 6
2023-02-17 10:22:33,475 DEBUG TRAIN Batch 4/1300 loss 11.748631 loss_att 15.978552 loss_ctc 18.914082 loss_rnnt 9.795121 hw_loss 0.285247 lr 0.00084903 rank 0
2023-02-17 10:22:33,479 DEBUG TRAIN Batch 4/1300 loss 25.513699 loss_att 33.130020 loss_ctc 45.599709 loss_rnnt 21.125797 hw_loss 0.349688 lr 0.00084935 rank 5
2023-02-17 10:22:33,479 DEBUG TRAIN Batch 4/1300 loss 43.314594 loss_att 52.701721 loss_ctc 67.614731 loss_rnnt 38.053925 hw_loss 0.268541 lr 0.00084928 rank 3
2023-02-17 10:22:33,481 DEBUG TRAIN Batch 4/1300 loss 13.728635 loss_att 21.884943 loss_ctc 21.480322 loss_rnnt 10.878786 hw_loss 0.346928 lr 0.00084897 rank 1
2023-02-17 10:22:33,482 DEBUG TRAIN Batch 4/1300 loss 20.324064 loss_att 26.518190 loss_ctc 30.183561 loss_rnnt 17.607847 hw_loss 0.305240 lr 0.00084953 rank 7
2023-02-17 10:22:33,484 DEBUG TRAIN Batch 4/1300 loss 20.164467 loss_att 25.378399 loss_ctc 34.106453 loss_rnnt 17.096935 hw_loss 0.310897 lr 0.00084988 rank 4
2023-02-17 10:22:33,526 DEBUG TRAIN Batch 4/1300 loss 33.980389 loss_att 36.294147 loss_ctc 49.079922 loss_rnnt 31.289783 hw_loss 0.402334 lr 0.00084869 rank 2
2023-02-17 10:23:52,167 DEBUG TRAIN Batch 4/1400 loss 12.953423 loss_att 16.582928 loss_ctc 22.002491 loss_rnnt 10.792601 hw_loss 0.428212 lr 0.00084831 rank 7
2023-02-17 10:23:52,168 DEBUG TRAIN Batch 4/1400 loss 11.347147 loss_att 15.638590 loss_ctc 22.367111 loss_rnnt 8.820117 hw_loss 0.373898 lr 0.00084775 rank 1
2023-02-17 10:23:52,169 DEBUG TRAIN Batch 4/1400 loss 9.628460 loss_att 19.350410 loss_ctc 23.679861 loss_rnnt 5.611951 hw_loss 0.372372 lr 0.00084853 rank 6
2023-02-17 10:23:52,173 DEBUG TRAIN Batch 4/1400 loss 52.878109 loss_att 67.953903 loss_ctc 84.510536 loss_rnnt 45.465347 hw_loss 0.337398 lr 0.00084813 rank 5
2023-02-17 10:23:52,175 DEBUG TRAIN Batch 4/1400 loss 17.373005 loss_att 21.883032 loss_ctc 25.720100 loss_rnnt 15.119932 hw_loss 0.446477 lr 0.00084805 rank 3
2023-02-17 10:23:52,178 DEBUG TRAIN Batch 4/1400 loss 24.210585 loss_att 28.268160 loss_ctc 37.163345 loss_rnnt 21.442411 hw_loss 0.430543 lr 0.00084865 rank 4
2023-02-17 10:23:52,179 DEBUG TRAIN Batch 4/1400 loss 24.432692 loss_att 29.773323 loss_ctc 38.645081 loss_rnnt 21.266657 hw_loss 0.380484 lr 0.00084747 rank 2
2023-02-17 10:23:52,182 DEBUG TRAIN Batch 4/1400 loss 17.261555 loss_att 29.960217 loss_ctc 27.709623 loss_rnnt 13.168676 hw_loss 0.300130 lr 0.00084781 rank 0
2023-02-17 10:25:08,571 DEBUG TRAIN Batch 4/1500 loss 18.570883 loss_att 22.442177 loss_ctc 33.472260 loss_rnnt 15.639438 hw_loss 0.319382 lr 0.00084659 rank 0
2023-02-17 10:25:08,573 DEBUG TRAIN Batch 4/1500 loss 17.011650 loss_att 21.282986 loss_ctc 30.509756 loss_rnnt 14.147041 hw_loss 0.394865 lr 0.00084731 rank 6
2023-02-17 10:25:08,574 DEBUG TRAIN Batch 4/1500 loss 16.505417 loss_att 22.578207 loss_ctc 27.900808 loss_rnnt 13.585924 hw_loss 0.347899 lr 0.00084684 rank 3
2023-02-17 10:25:08,575 DEBUG TRAIN Batch 4/1500 loss 30.670849 loss_att 34.145088 loss_ctc 52.959896 loss_rnnt 26.783993 hw_loss 0.412753 lr 0.00084691 rank 5
2023-02-17 10:25:08,575 DEBUG TRAIN Batch 4/1500 loss 41.592354 loss_att 46.548195 loss_ctc 62.967026 loss_rnnt 37.548462 hw_loss 0.380183 lr 0.00084653 rank 1
2023-02-17 10:25:08,575 DEBUG TRAIN Batch 4/1500 loss 35.772057 loss_att 42.841125 loss_ctc 58.215496 loss_rnnt 31.193321 hw_loss 0.323358 lr 0.00084709 rank 7
2023-02-17 10:25:08,577 DEBUG TRAIN Batch 4/1500 loss 22.579437 loss_att 29.416941 loss_ctc 37.772118 loss_rnnt 19.028034 hw_loss 0.296649 lr 0.00084743 rank 4
2023-02-17 10:25:08,577 DEBUG TRAIN Batch 4/1500 loss 22.875658 loss_att 25.549292 loss_ctc 32.684189 loss_rnnt 20.816113 hw_loss 0.406904 lr 0.00084626 rank 2
2023-02-17 10:26:26,142 DEBUG TRAIN Batch 4/1600 loss 31.244915 loss_att 37.994709 loss_ctc 40.891380 loss_rnnt 28.450344 hw_loss 0.297030 lr 0.00084505 rank 2
2023-02-17 10:26:26,145 DEBUG TRAIN Batch 4/1600 loss 14.098579 loss_att 23.068226 loss_ctc 29.590076 loss_rnnt 10.033654 hw_loss 0.385241 lr 0.00084563 rank 3
2023-02-17 10:26:26,146 DEBUG TRAIN Batch 4/1600 loss 40.874538 loss_att 46.751762 loss_ctc 59.524490 loss_rnnt 37.007423 hw_loss 0.384398 lr 0.00084570 rank 5
2023-02-17 10:26:26,147 DEBUG TRAIN Batch 4/1600 loss 21.819618 loss_att 33.514885 loss_ctc 40.170208 loss_rnnt 16.862438 hw_loss 0.321342 lr 0.00084532 rank 1
2023-02-17 10:26:26,154 DEBUG TRAIN Batch 4/1600 loss 30.168793 loss_att 32.895649 loss_ctc 38.788765 loss_rnnt 28.270805 hw_loss 0.381163 lr 0.00084622 rank 4
2023-02-17 10:26:26,155 DEBUG TRAIN Batch 4/1600 loss 27.830742 loss_att 31.254475 loss_ctc 40.148731 loss_rnnt 25.325573 hw_loss 0.333797 lr 0.00084588 rank 7
2023-02-17 10:26:26,156 DEBUG TRAIN Batch 4/1600 loss 22.508673 loss_att 34.043770 loss_ctc 29.709719 loss_rnnt 19.073452 hw_loss 0.315119 lr 0.00084610 rank 6
2023-02-17 10:26:26,198 DEBUG TRAIN Batch 4/1600 loss 25.140949 loss_att 30.723972 loss_ctc 41.585915 loss_rnnt 21.624012 hw_loss 0.389374 lr 0.00084538 rank 0
2023-02-17 10:27:42,270 DEBUG TRAIN Batch 4/1700 loss 34.018398 loss_att 38.847477 loss_ctc 48.638268 loss_rnnt 30.945795 hw_loss 0.295262 lr 0.00084449 rank 5
2023-02-17 10:27:42,274 DEBUG TRAIN Batch 4/1700 loss 31.215471 loss_att 37.439308 loss_ctc 49.531456 loss_rnnt 27.326916 hw_loss 0.378106 lr 0.00084489 rank 6
2023-02-17 10:27:42,274 DEBUG TRAIN Batch 4/1700 loss 37.383629 loss_att 41.655403 loss_ctc 59.656784 loss_rnnt 33.358356 hw_loss 0.377176 lr 0.00084442 rank 3
2023-02-17 10:27:42,275 DEBUG TRAIN Batch 4/1700 loss 15.034307 loss_att 21.523914 loss_ctc 22.393782 loss_rnnt 12.546296 hw_loss 0.391548 lr 0.00084501 rank 4
2023-02-17 10:27:42,276 DEBUG TRAIN Batch 4/1700 loss 19.438414 loss_att 25.228619 loss_ctc 27.418121 loss_rnnt 17.063984 hw_loss 0.285803 lr 0.00084412 rank 1
2023-02-17 10:27:42,278 DEBUG TRAIN Batch 4/1700 loss 21.636850 loss_att 25.799063 loss_ctc 29.229694 loss_rnnt 19.593864 hw_loss 0.371558 lr 0.00084467 rank 7
2023-02-17 10:27:42,279 DEBUG TRAIN Batch 4/1700 loss 19.372837 loss_att 23.658146 loss_ctc 31.762783 loss_rnnt 16.685854 hw_loss 0.333612 lr 0.00084418 rank 0
2023-02-17 10:27:42,279 DEBUG TRAIN Batch 4/1700 loss 24.523893 loss_att 25.392313 loss_ctc 39.748703 loss_rnnt 22.201115 hw_loss 0.223345 lr 0.00084384 rank 2
2023-02-17 10:29:01,854 DEBUG TRAIN Batch 4/1800 loss 19.083887 loss_att 23.876398 loss_ctc 28.672554 loss_rnnt 16.666842 hw_loss 0.337603 lr 0.00084347 rank 7
2023-02-17 10:29:01,855 DEBUG TRAIN Batch 4/1800 loss 26.997242 loss_att 28.983429 loss_ctc 40.676205 loss_rnnt 24.536686 hw_loss 0.448976 lr 0.00084322 rank 3
2023-02-17 10:29:01,855 DEBUG TRAIN Batch 4/1800 loss 17.252691 loss_att 24.795952 loss_ctc 33.793751 loss_rnnt 13.403084 hw_loss 0.254026 lr 0.00084369 rank 6
2023-02-17 10:29:01,857 DEBUG TRAIN Batch 4/1800 loss 17.000181 loss_att 23.690754 loss_ctc 33.296486 loss_rnnt 13.286172 hw_loss 0.380726 lr 0.00084298 rank 0
2023-02-17 10:29:01,860 DEBUG TRAIN Batch 4/1800 loss 18.160908 loss_att 21.018696 loss_ctc 25.558167 loss_rnnt 16.345015 hw_loss 0.483813 lr 0.00084381 rank 4
2023-02-17 10:29:01,860 DEBUG TRAIN Batch 4/1800 loss 24.031153 loss_att 27.000462 loss_ctc 35.840431 loss_rnnt 21.693470 hw_loss 0.317340 lr 0.00084264 rank 2
2023-02-17 10:29:01,861 DEBUG TRAIN Batch 4/1800 loss 30.881891 loss_att 33.777416 loss_ctc 44.517963 loss_rnnt 28.300495 hw_loss 0.345271 lr 0.00084292 rank 1
2023-02-17 10:29:01,863 DEBUG TRAIN Batch 4/1800 loss 33.862465 loss_att 39.711906 loss_ctc 57.217911 loss_rnnt 29.365570 hw_loss 0.399274 lr 0.00084329 rank 5
2023-02-17 10:30:17,954 DEBUG TRAIN Batch 4/1900 loss 20.150799 loss_att 21.434006 loss_ctc 26.321594 loss_rnnt 18.895542 hw_loss 0.329703 lr 0.00084209 rank 5
2023-02-17 10:30:17,954 DEBUG TRAIN Batch 4/1900 loss 14.201097 loss_att 15.746042 loss_ctc 21.459526 loss_rnnt 12.699226 hw_loss 0.422047 lr 0.00084227 rank 7
2023-02-17 10:30:17,956 DEBUG TRAIN Batch 4/1900 loss 15.027733 loss_att 14.948756 loss_ctc 20.263378 loss_rnnt 14.130057 hw_loss 0.403847 lr 0.00084249 rank 6
2023-02-17 10:30:17,956 DEBUG TRAIN Batch 4/1900 loss 26.443790 loss_att 32.121113 loss_ctc 40.765526 loss_rnnt 23.230335 hw_loss 0.315795 lr 0.00084202 rank 3
2023-02-17 10:30:17,959 DEBUG TRAIN Batch 4/1900 loss 21.474211 loss_att 20.117405 loss_ctc 28.655319 loss_rnnt 20.546242 hw_loss 0.453468 lr 0.00084172 rank 1
2023-02-17 10:30:17,961 DEBUG TRAIN Batch 4/1900 loss 23.248226 loss_att 24.447273 loss_ctc 35.922569 loss_rnnt 21.136375 hw_loss 0.341494 lr 0.00084178 rank 0
2023-02-17 10:30:17,961 DEBUG TRAIN Batch 4/1900 loss 11.764641 loss_att 14.074947 loss_ctc 19.952576 loss_rnnt 10.049723 hw_loss 0.302121 lr 0.00084145 rank 2
2023-02-17 10:30:17,962 DEBUG TRAIN Batch 4/1900 loss 21.232374 loss_att 31.213425 loss_ctc 31.379276 loss_rnnt 17.705046 hw_loss 0.334119 lr 0.00084261 rank 4
2023-02-17 10:31:32,060 DEBUG TRAIN Batch 4/2000 loss 19.163801 loss_att 25.630774 loss_ctc 29.552372 loss_rnnt 16.311810 hw_loss 0.325228 lr 0.00084083 rank 3
2023-02-17 10:31:32,065 DEBUG TRAIN Batch 4/2000 loss 32.500069 loss_att 38.122551 loss_ctc 68.285118 loss_rnnt 26.439651 hw_loss 0.308589 lr 0.00084129 rank 6
2023-02-17 10:31:32,065 DEBUG TRAIN Batch 4/2000 loss 21.124174 loss_att 27.212603 loss_ctc 39.305084 loss_rnnt 17.289982 hw_loss 0.360720 lr 0.00084090 rank 5
2023-02-17 10:31:32,065 DEBUG TRAIN Batch 4/2000 loss 13.383828 loss_att 20.167713 loss_ctc 20.332901 loss_rnnt 10.941634 hw_loss 0.297890 lr 0.00084108 rank 7
2023-02-17 10:31:32,066 DEBUG TRAIN Batch 4/2000 loss 17.139858 loss_att 24.271267 loss_ctc 29.719122 loss_rnnt 13.857978 hw_loss 0.334430 lr 0.00084026 rank 2
2023-02-17 10:31:32,066 DEBUG TRAIN Batch 4/2000 loss 14.789500 loss_att 20.749630 loss_ctc 27.871250 loss_rnnt 11.679632 hw_loss 0.325518 lr 0.00084053 rank 1
2023-02-17 10:31:32,068 DEBUG TRAIN Batch 4/2000 loss 16.431358 loss_att 22.514540 loss_ctc 26.962744 loss_rnnt 13.636312 hw_loss 0.326671 lr 0.00084141 rank 4
2023-02-17 10:31:32,070 DEBUG TRAIN Batch 4/2000 loss 23.962242 loss_att 26.891254 loss_ctc 38.404491 loss_rnnt 21.231724 hw_loss 0.410774 lr 0.00084059 rank 0
2023-02-17 10:32:49,136 DEBUG TRAIN Batch 4/2100 loss 19.433542 loss_att 22.677608 loss_ctc 36.610016 loss_rnnt 16.365528 hw_loss 0.241880 lr 0.00084010 rank 6
2023-02-17 10:32:49,140 DEBUG TRAIN Batch 4/2100 loss 43.305882 loss_att 50.217030 loss_ctc 64.875351 loss_rnnt 38.810600 hw_loss 0.444605 lr 0.00083941 rank 0
2023-02-17 10:32:49,139 DEBUG TRAIN Batch 4/2100 loss 39.192108 loss_att 48.900955 loss_ctc 69.667999 loss_rnnt 33.036194 hw_loss 0.282546 lr 0.00083971 rank 5
2023-02-17 10:32:49,139 DEBUG TRAIN Batch 4/2100 loss 34.538643 loss_att 45.806286 loss_ctc 65.733780 loss_rnnt 27.932129 hw_loss 0.363062 lr 0.00083964 rank 3
2023-02-17 10:32:49,142 DEBUG TRAIN Batch 4/2100 loss 30.112123 loss_att 35.444321 loss_ctc 53.725090 loss_rnnt 25.741131 hw_loss 0.292796 lr 0.00083989 rank 7
2023-02-17 10:32:49,142 DEBUG TRAIN Batch 4/2100 loss 25.273340 loss_att 30.270702 loss_ctc 39.308064 loss_rnnt 22.252274 hw_loss 0.281806 lr 0.00083935 rank 1
2023-02-17 10:32:49,144 DEBUG TRAIN Batch 4/2100 loss 13.744783 loss_att 21.807524 loss_ctc 24.313892 loss_rnnt 10.561436 hw_loss 0.302971 lr 0.00083908 rank 2
2023-02-17 10:32:49,183 DEBUG TRAIN Batch 4/2100 loss 47.075523 loss_att 46.618801 loss_ctc 63.065510 loss_rnnt 44.890888 hw_loss 0.269969 lr 0.00084022 rank 4
2023-02-17 10:34:05,715 DEBUG TRAIN Batch 4/2200 loss 27.806477 loss_att 29.477802 loss_ctc 45.992249 loss_rnnt 24.864309 hw_loss 0.343373 lr 0.00083871 rank 7
2023-02-17 10:34:05,716 DEBUG TRAIN Batch 4/2200 loss 26.865625 loss_att 31.054659 loss_ctc 42.260319 loss_rnnt 23.797569 hw_loss 0.333044 lr 0.00083892 rank 6
2023-02-17 10:34:05,720 DEBUG TRAIN Batch 4/2200 loss 31.883709 loss_att 39.470009 loss_ctc 47.708202 loss_rnnt 28.085136 hw_loss 0.321338 lr 0.00083853 rank 5
2023-02-17 10:34:05,722 DEBUG TRAIN Batch 4/2200 loss 30.366333 loss_att 40.390152 loss_ctc 50.903820 loss_rnnt 25.445591 hw_loss 0.333085 lr 0.00083817 rank 1
2023-02-17 10:34:05,723 DEBUG TRAIN Batch 4/2200 loss 18.673670 loss_att 25.961731 loss_ctc 32.186440 loss_rnnt 15.224339 hw_loss 0.356279 lr 0.00083846 rank 3
2023-02-17 10:34:05,724 DEBUG TRAIN Batch 4/2200 loss 34.166466 loss_att 36.721630 loss_ctc 49.901199 loss_rnnt 31.380409 hw_loss 0.331985 lr 0.00083904 rank 4
2023-02-17 10:34:05,724 DEBUG TRAIN Batch 4/2200 loss 22.536459 loss_att 26.600842 loss_ctc 33.696121 loss_rnnt 19.990665 hw_loss 0.459302 lr 0.00083790 rank 2
2023-02-17 10:34:05,730 DEBUG TRAIN Batch 4/2200 loss 33.941544 loss_att 39.656281 loss_ctc 54.026810 loss_rnnt 29.946783 hw_loss 0.325837 lr 0.00083823 rank 0
2023-02-17 10:35:20,857 DEBUG TRAIN Batch 4/2300 loss 9.496403 loss_att 12.087972 loss_ctc 15.056931 loss_rnnt 8.020731 hw_loss 0.404914 lr 0.00083728 rank 3
2023-02-17 10:35:20,861 DEBUG TRAIN Batch 4/2300 loss 16.225782 loss_att 20.583811 loss_ctc 28.689522 loss_rnnt 13.483595 hw_loss 0.391409 lr 0.00083672 rank 2
2023-02-17 10:35:20,861 DEBUG TRAIN Batch 4/2300 loss 21.142759 loss_att 25.839581 loss_ctc 28.880077 loss_rnnt 18.994133 hw_loss 0.333039 lr 0.00083774 rank 6
2023-02-17 10:35:20,862 DEBUG TRAIN Batch 4/2300 loss 27.454445 loss_att 34.744980 loss_ctc 39.534767 loss_rnnt 24.181187 hw_loss 0.383329 lr 0.00083753 rank 7
2023-02-17 10:35:20,863 DEBUG TRAIN Batch 4/2300 loss 29.800531 loss_att 35.125183 loss_ctc 50.490746 loss_rnnt 25.762325 hw_loss 0.402339 lr 0.00083736 rank 5
2023-02-17 10:35:20,863 DEBUG TRAIN Batch 4/2300 loss 27.590206 loss_att 35.163513 loss_ctc 46.323353 loss_rnnt 23.399883 hw_loss 0.333580 lr 0.00083699 rank 1
2023-02-17 10:35:20,865 DEBUG TRAIN Batch 4/2300 loss 20.116190 loss_att 26.717035 loss_ctc 37.026962 loss_rnnt 16.378170 hw_loss 0.305776 lr 0.00083786 rank 4
2023-02-17 10:35:20,865 DEBUG TRAIN Batch 4/2300 loss 31.171484 loss_att 38.105225 loss_ctc 49.451469 loss_rnnt 27.150246 hw_loss 0.369672 lr 0.00083705 rank 0
2023-02-17 10:36:36,613 DEBUG TRAIN Batch 4/2400 loss 19.900620 loss_att 24.859570 loss_ctc 31.569839 loss_rnnt 17.151096 hw_loss 0.378446 lr 0.00083669 rank 4
2023-02-17 10:36:36,614 DEBUG TRAIN Batch 4/2400 loss 17.745951 loss_att 23.127058 loss_ctc 34.829384 loss_rnnt 14.217842 hw_loss 0.326428 lr 0.00083611 rank 3
2023-02-17 10:36:36,617 DEBUG TRAIN Batch 4/2400 loss 15.668823 loss_att 18.154026 loss_ctc 27.107714 loss_rnnt 13.459353 hw_loss 0.351082 lr 0.00083555 rank 2
2023-02-17 10:36:36,618 DEBUG TRAIN Batch 4/2400 loss 14.981665 loss_att 20.160603 loss_ctc 28.264456 loss_rnnt 12.025074 hw_loss 0.280807 lr 0.00083588 rank 0
2023-02-17 10:36:36,617 DEBUG TRAIN Batch 4/2400 loss 18.424082 loss_att 21.456358 loss_ctc 31.912489 loss_rnnt 15.838441 hw_loss 0.338871 lr 0.00083618 rank 5
2023-02-17 10:36:36,619 DEBUG TRAIN Batch 4/2400 loss 25.282522 loss_att 27.860435 loss_ctc 35.778023 loss_rnnt 23.177513 hw_loss 0.356297 lr 0.00083657 rank 6
2023-02-17 10:36:36,619 DEBUG TRAIN Batch 4/2400 loss 12.598697 loss_att 17.498577 loss_ctc 22.578039 loss_rnnt 10.066673 hw_loss 0.415253 lr 0.00083582 rank 1
2023-02-17 10:36:36,621 DEBUG TRAIN Batch 4/2400 loss 24.549400 loss_att 30.936419 loss_ctc 37.295666 loss_rnnt 21.429893 hw_loss 0.267375 lr 0.00083636 rank 7
2023-02-17 10:37:56,821 DEBUG TRAIN Batch 4/2500 loss 26.844528 loss_att 30.226982 loss_ctc 43.171959 loss_rnnt 23.768475 hw_loss 0.417325 lr 0.00083502 rank 5
2023-02-17 10:37:56,824 DEBUG TRAIN Batch 4/2500 loss 30.719395 loss_att 31.469215 loss_ctc 44.749653 loss_rnnt 28.460510 hw_loss 0.446660 lr 0.00083495 rank 3
2023-02-17 10:37:56,826 DEBUG TRAIN Batch 4/2500 loss 31.359516 loss_att 33.153744 loss_ctc 42.995987 loss_rnnt 29.254181 hw_loss 0.365554 lr 0.00083540 rank 6
2023-02-17 10:37:56,827 DEBUG TRAIN Batch 4/2500 loss 30.531836 loss_att 30.738199 loss_ctc 41.750969 loss_rnnt 28.775890 hw_loss 0.410228 lr 0.00083466 rank 1
2023-02-17 10:37:56,828 DEBUG TRAIN Batch 4/2500 loss 18.600050 loss_att 20.221003 loss_ctc 26.919935 loss_rnnt 16.966421 hw_loss 0.375228 lr 0.00083519 rank 7
2023-02-17 10:37:56,828 DEBUG TRAIN Batch 4/2500 loss 30.448120 loss_att 31.763655 loss_ctc 46.183533 loss_rnnt 27.876730 hw_loss 0.394185 lr 0.00083439 rank 2
2023-02-17 10:37:56,829 DEBUG TRAIN Batch 4/2500 loss 24.044617 loss_att 33.748196 loss_ctc 36.394630 loss_rnnt 20.266073 hw_loss 0.358425 lr 0.00083552 rank 4
2023-02-17 10:37:56,875 DEBUG TRAIN Batch 4/2500 loss 15.558497 loss_att 20.367126 loss_ctc 28.229538 loss_rnnt 12.726594 hw_loss 0.338822 lr 0.00083471 rank 0
2023-02-17 10:39:12,444 DEBUG TRAIN Batch 4/2600 loss 39.285324 loss_att 44.580284 loss_ctc 56.734764 loss_rnnt 35.736557 hw_loss 0.305972 lr 0.00083385 rank 5
2023-02-17 10:39:12,447 DEBUG TRAIN Batch 4/2600 loss 25.349670 loss_att 31.626352 loss_ctc 43.733345 loss_rnnt 21.498945 hw_loss 0.270433 lr 0.00083323 rank 2
2023-02-17 10:39:12,448 DEBUG TRAIN Batch 4/2600 loss 11.928059 loss_att 13.308535 loss_ctc 15.907042 loss_rnnt 10.902622 hw_loss 0.410268 lr 0.00083403 rank 7
2023-02-17 10:39:12,449 DEBUG TRAIN Batch 4/2600 loss 17.838034 loss_att 28.507046 loss_ctc 30.988836 loss_rnnt 13.795519 hw_loss 0.291132 lr 0.00083424 rank 6
2023-02-17 10:39:12,451 DEBUG TRAIN Batch 4/2600 loss 15.241475 loss_att 23.254936 loss_ctc 26.551758 loss_rnnt 11.902542 hw_loss 0.427881 lr 0.00083350 rank 1
2023-02-17 10:39:12,454 DEBUG TRAIN Batch 4/2600 loss 15.781164 loss_att 22.418259 loss_ctc 29.714132 loss_rnnt 12.444641 hw_loss 0.283827 lr 0.00083379 rank 3
2023-02-17 10:39:12,454 DEBUG TRAIN Batch 4/2600 loss 26.990492 loss_att 29.542871 loss_ctc 42.364197 loss_rnnt 24.233604 hw_loss 0.368593 lr 0.00083435 rank 4
2023-02-17 10:39:12,456 DEBUG TRAIN Batch 4/2600 loss 11.047727 loss_att 11.862056 loss_ctc 14.055699 loss_rnnt 10.248845 hw_loss 0.440536 lr 0.00083355 rank 0
2023-02-17 10:40:27,807 DEBUG TRAIN Batch 4/2700 loss 19.788960 loss_att 28.093168 loss_ctc 42.390373 loss_rnnt 14.913443 hw_loss 0.377163 lr 0.00083287 rank 7
2023-02-17 10:40:27,810 DEBUG TRAIN Batch 4/2700 loss 17.700020 loss_att 26.145201 loss_ctc 35.478367 loss_rnnt 13.452677 hw_loss 0.352240 lr 0.00083319 rank 4
2023-02-17 10:40:27,810 DEBUG TRAIN Batch 4/2700 loss 25.475040 loss_att 25.758875 loss_ctc 40.156673 loss_rnnt 23.259644 hw_loss 0.377028 lr 0.00083207 rank 2
2023-02-17 10:40:27,810 DEBUG TRAIN Batch 4/2700 loss 39.615940 loss_att 43.376537 loss_ctc 67.622360 loss_rnnt 34.904346 hw_loss 0.422400 lr 0.00083270 rank 5
2023-02-17 10:40:27,811 DEBUG TRAIN Batch 4/2700 loss 26.058718 loss_att 28.148842 loss_ctc 39.548576 loss_rnnt 23.676510 hw_loss 0.310381 lr 0.00083240 rank 0
2023-02-17 10:40:27,812 DEBUG TRAIN Batch 4/2700 loss 21.982794 loss_att 31.116276 loss_ctc 34.030407 loss_rnnt 18.410297 hw_loss 0.261472 lr 0.00083308 rank 6
2023-02-17 10:40:27,815 DEBUG TRAIN Batch 4/2700 loss 25.615294 loss_att 32.208370 loss_ctc 39.644524 loss_rnnt 22.209385 hw_loss 0.406363 lr 0.00083234 rank 1
2023-02-17 10:40:27,858 DEBUG TRAIN Batch 4/2700 loss 25.786205 loss_att 31.019876 loss_ctc 40.884064 loss_rnnt 22.547489 hw_loss 0.335505 lr 0.00083263 rank 3
2023-02-17 10:41:46,518 DEBUG TRAIN Batch 4/2800 loss 24.974579 loss_att 30.974031 loss_ctc 40.944687 loss_rnnt 21.486431 hw_loss 0.297954 lr 0.00083192 rank 6
2023-02-17 10:41:46,518 DEBUG TRAIN Batch 4/2800 loss 27.563923 loss_att 37.087563 loss_ctc 38.722061 loss_rnnt 23.978786 hw_loss 0.361234 lr 0.00083172 rank 7
2023-02-17 10:41:46,524 DEBUG TRAIN Batch 4/2800 loss 11.922693 loss_att 15.923562 loss_ctc 24.987793 loss_rnnt 9.146420 hw_loss 0.438911 lr 0.00083125 rank 0
2023-02-17 10:41:46,526 DEBUG TRAIN Batch 4/2800 loss 36.923851 loss_att 41.789722 loss_ctc 55.637939 loss_rnnt 33.262691 hw_loss 0.361450 lr 0.00083092 rank 2
2023-02-17 10:41:46,529 DEBUG TRAIN Batch 4/2800 loss 9.145922 loss_att 14.166346 loss_ctc 18.757084 loss_rnnt 6.704090 hw_loss 0.292985 lr 0.00083148 rank 3
2023-02-17 10:41:46,530 DEBUG TRAIN Batch 4/2800 loss 21.513950 loss_att 28.798393 loss_ctc 35.260490 loss_rnnt 18.047918 hw_loss 0.330510 lr 0.00083155 rank 5
2023-02-17 10:41:46,531 DEBUG TRAIN Batch 4/2800 loss 35.030399 loss_att 36.307735 loss_ctc 42.698418 loss_rnnt 33.599342 hw_loss 0.287231 lr 0.00083204 rank 4
2023-02-17 10:41:46,537 DEBUG TRAIN Batch 4/2800 loss 18.370583 loss_att 23.656174 loss_ctc 33.547058 loss_rnnt 15.108440 hw_loss 0.340298 lr 0.00083119 rank 1
2023-02-17 10:43:02,993 DEBUG TRAIN Batch 4/2900 loss 13.686497 loss_att 19.610542 loss_ctc 26.156319 loss_rnnt 10.686525 hw_loss 0.285970 lr 0.00083040 rank 5
2023-02-17 10:43:02,997 DEBUG TRAIN Batch 4/2900 loss 25.476044 loss_att 29.032211 loss_ctc 46.826271 loss_rnnt 21.741951 hw_loss 0.330304 lr 0.00083089 rank 4
2023-02-17 10:43:02,998 DEBUG TRAIN Batch 4/2900 loss 26.109056 loss_att 31.291451 loss_ctc 39.809486 loss_rnnt 23.050200 hw_loss 0.366850 lr 0.00083010 rank 0
2023-02-17 10:43:02,998 DEBUG TRAIN Batch 4/2900 loss 17.610558 loss_att 23.852734 loss_ctc 26.661243 loss_rnnt 14.951992 hw_loss 0.381321 lr 0.00083078 rank 6
2023-02-17 10:43:02,998 DEBUG TRAIN Batch 4/2900 loss 18.464310 loss_att 25.946135 loss_ctc 26.590931 loss_rnnt 15.683073 hw_loss 0.377481 lr 0.00083004 rank 1
2023-02-17 10:43:02,999 DEBUG TRAIN Batch 4/2900 loss 17.460508 loss_att 21.849691 loss_ctc 27.469641 loss_rnnt 15.030201 hw_loss 0.408599 lr 0.00082978 rank 2
2023-02-17 10:43:02,999 DEBUG TRAIN Batch 4/2900 loss 23.900473 loss_att 30.406670 loss_ctc 40.629223 loss_rnnt 20.245621 hw_loss 0.230830 lr 0.00083033 rank 3
2023-02-17 10:43:03,000 DEBUG TRAIN Batch 4/2900 loss 39.060249 loss_att 43.070263 loss_ctc 62.880302 loss_rnnt 34.909775 hw_loss 0.323368 lr 0.00083057 rank 7
2023-02-17 10:44:19,119 DEBUG TRAIN Batch 4/3000 loss 25.083366 loss_att 29.910141 loss_ctc 36.166237 loss_rnnt 22.447262 hw_loss 0.361942 lr 0.00082975 rank 4
2023-02-17 10:44:19,119 DEBUG TRAIN Batch 4/3000 loss 15.961655 loss_att 18.968563 loss_ctc 25.297504 loss_rnnt 13.933622 hw_loss 0.341008 lr 0.00082890 rank 1
2023-02-17 10:44:19,120 DEBUG TRAIN Batch 4/3000 loss 26.604528 loss_att 30.731350 loss_ctc 42.746910 loss_rnnt 23.388382 hw_loss 0.447122 lr 0.00082943 rank 7
2023-02-17 10:44:19,120 DEBUG TRAIN Batch 4/3000 loss 25.822201 loss_att 30.501844 loss_ctc 38.582809 loss_rnnt 23.015020 hw_loss 0.318444 lr 0.00082925 rank 5
2023-02-17 10:44:19,121 DEBUG TRAIN Batch 4/3000 loss 18.534706 loss_att 24.907148 loss_ctc 29.052393 loss_rnnt 15.686796 hw_loss 0.320744 lr 0.00082963 rank 6
2023-02-17 10:44:19,123 DEBUG TRAIN Batch 4/3000 loss 29.198160 loss_att 34.051407 loss_ctc 46.690140 loss_rnnt 25.708080 hw_loss 0.350938 lr 0.00082896 rank 0
2023-02-17 10:44:19,124 DEBUG TRAIN Batch 4/3000 loss 25.781456 loss_att 29.911793 loss_ctc 39.986061 loss_rnnt 22.879950 hw_loss 0.340299 lr 0.00082919 rank 3
2023-02-17 10:44:19,163 DEBUG TRAIN Batch 4/3000 loss 24.978683 loss_att 35.069542 loss_ctc 50.233482 loss_rnnt 19.388287 hw_loss 0.384222 lr 0.00082864 rank 2
2023-02-17 10:45:34,972 DEBUG TRAIN Batch 4/3100 loss 21.932459 loss_att 24.298143 loss_ctc 36.861015 loss_rnnt 19.291382 hw_loss 0.332745 lr 0.00082812 rank 5
2023-02-17 10:45:34,973 DEBUG TRAIN Batch 4/3100 loss 25.509293 loss_att 24.109959 loss_ctc 35.059418 loss_rnnt 24.220095 hw_loss 0.554465 lr 0.00082861 rank 4
2023-02-17 10:45:34,975 DEBUG TRAIN Batch 4/3100 loss 18.729662 loss_att 20.319759 loss_ctc 24.715052 loss_rnnt 17.433863 hw_loss 0.336985 lr 0.00082776 rank 1
2023-02-17 10:45:34,977 DEBUG TRAIN Batch 4/3100 loss 23.808878 loss_att 22.846214 loss_ctc 38.156460 loss_rnnt 21.821615 hw_loss 0.500225 lr 0.00082849 rank 6
2023-02-17 10:45:34,981 DEBUG TRAIN Batch 4/3100 loss 17.803413 loss_att 21.679291 loss_ctc 28.077942 loss_rnnt 15.487109 hw_loss 0.320985 lr 0.00082829 rank 7
2023-02-17 10:45:34,981 DEBUG TRAIN Batch 4/3100 loss 23.034142 loss_att 26.246880 loss_ctc 32.862659 loss_rnnt 20.908298 hw_loss 0.324051 lr 0.00082782 rank 0
2023-02-17 10:45:34,981 DEBUG TRAIN Batch 4/3100 loss 15.692225 loss_att 17.539677 loss_ctc 26.038193 loss_rnnt 13.764605 hw_loss 0.334999 lr 0.00082750 rank 2
2023-02-17 10:45:34,985 DEBUG TRAIN Batch 4/3100 loss 20.612267 loss_att 25.626194 loss_ctc 30.290897 loss_rnnt 18.132713 hw_loss 0.349284 lr 0.00082805 rank 3
2023-02-17 10:46:54,555 DEBUG TRAIN Batch 4/3200 loss 11.650650 loss_att 14.125435 loss_ctc 18.734657 loss_rnnt 9.994107 hw_loss 0.406970 lr 0.00082715 rank 7
2023-02-17 10:46:54,563 DEBUG TRAIN Batch 4/3200 loss 20.808969 loss_att 23.843777 loss_ctc 31.619110 loss_rnnt 18.563105 hw_loss 0.370409 lr 0.00082698 rank 5
2023-02-17 10:46:54,563 DEBUG TRAIN Batch 4/3200 loss 13.793974 loss_att 14.280626 loss_ctc 17.900976 loss_rnnt 12.900263 hw_loss 0.466461 lr 0.00082669 rank 0
2023-02-17 10:46:54,564 DEBUG TRAIN Batch 4/3200 loss 21.706553 loss_att 29.383560 loss_ctc 36.344254 loss_rnnt 18.024986 hw_loss 0.364633 lr 0.00082736 rank 6
2023-02-17 10:46:54,565 DEBUG TRAIN Batch 4/3200 loss 11.542001 loss_att 12.423335 loss_ctc 15.466567 loss_rnnt 10.577484 hw_loss 0.496826 lr 0.00082692 rank 3
2023-02-17 10:46:54,566 DEBUG TRAIN Batch 4/3200 loss 31.398720 loss_att 33.677589 loss_ctc 47.624367 loss_rnnt 28.622330 hw_loss 0.294741 lr 0.00082747 rank 4
2023-02-17 10:46:54,566 DEBUG TRAIN Batch 4/3200 loss 21.333923 loss_att 26.409397 loss_ctc 32.569515 loss_rnnt 18.634590 hw_loss 0.349045 lr 0.00082637 rank 2
2023-02-17 10:46:54,569 DEBUG TRAIN Batch 4/3200 loss 26.288496 loss_att 30.426310 loss_ctc 52.500572 loss_rnnt 21.835875 hw_loss 0.243965 lr 0.00082663 rank 1
2023-02-17 10:48:09,340 DEBUG TRAIN Batch 4/3300 loss 8.793910 loss_att 14.851044 loss_ctc 16.906332 loss_rnnt 6.323125 hw_loss 0.333191 lr 0.00082623 rank 6
2023-02-17 10:48:09,341 DEBUG TRAIN Batch 4/3300 loss 11.989057 loss_att 18.778805 loss_ctc 20.818916 loss_rnnt 9.285958 hw_loss 0.314686 lr 0.00082602 rank 7
2023-02-17 10:48:09,341 DEBUG TRAIN Batch 4/3300 loss 11.409491 loss_att 20.392452 loss_ctc 21.972725 loss_rnnt 7.982019 hw_loss 0.417089 lr 0.00082579 rank 3
2023-02-17 10:48:09,341 DEBUG TRAIN Batch 4/3300 loss 30.037973 loss_att 39.609043 loss_ctc 44.969494 loss_rnnt 25.884293 hw_loss 0.466116 lr 0.00082525 rank 2
2023-02-17 10:48:09,342 DEBUG TRAIN Batch 4/3300 loss 16.862743 loss_att 20.913944 loss_ctc 23.360752 loss_rnnt 14.939903 hw_loss 0.461620 lr 0.00082585 rank 5
2023-02-17 10:48:09,343 DEBUG TRAIN Batch 4/3300 loss 22.480219 loss_att 27.386715 loss_ctc 25.915081 loss_rnnt 20.825863 hw_loss 0.403267 lr 0.00082551 rank 1
2023-02-17 10:48:09,344 DEBUG TRAIN Batch 4/3300 loss 14.900759 loss_att 19.945629 loss_ctc 26.361433 loss_rnnt 12.178913 hw_loss 0.346465 lr 0.00082634 rank 4
2023-02-17 10:48:09,346 DEBUG TRAIN Batch 4/3300 loss 16.498444 loss_att 19.053907 loss_ctc 22.109091 loss_rnnt 15.082596 hw_loss 0.293754 lr 0.00082556 rank 0
2023-02-17 10:49:26,172 DEBUG TRAIN Batch 4/3400 loss 20.883976 loss_att 29.279919 loss_ctc 40.259041 loss_rnnt 16.429615 hw_loss 0.359678 lr 0.00082510 rank 6
2023-02-17 10:49:26,173 DEBUG TRAIN Batch 4/3400 loss 24.638586 loss_att 32.465179 loss_ctc 39.356133 loss_rnnt 20.936384 hw_loss 0.327266 lr 0.00082466 rank 3
2023-02-17 10:49:26,174 DEBUG TRAIN Batch 4/3400 loss 22.588900 loss_att 24.337519 loss_ctc 31.261345 loss_rnnt 20.890549 hw_loss 0.360567 lr 0.00082438 rank 1
2023-02-17 10:49:26,177 DEBUG TRAIN Batch 4/3400 loss 22.708218 loss_att 30.997007 loss_ctc 37.443096 loss_rnnt 18.930092 hw_loss 0.291966 lr 0.00082490 rank 7
2023-02-17 10:49:26,179 DEBUG TRAIN Batch 4/3400 loss 25.662939 loss_att 32.192616 loss_ctc 36.980118 loss_rnnt 22.677116 hw_loss 0.320497 lr 0.00082473 rank 5
2023-02-17 10:49:26,181 DEBUG TRAIN Batch 4/3400 loss 20.659925 loss_att 34.823082 loss_ctc 32.069427 loss_rnnt 16.190556 hw_loss 0.216510 lr 0.00082444 rank 0
2023-02-17 10:49:26,181 DEBUG TRAIN Batch 4/3400 loss 13.717708 loss_att 18.313656 loss_ctc 23.222906 loss_rnnt 11.366209 hw_loss 0.309281 lr 0.00082412 rank 2
2023-02-17 10:49:26,185 DEBUG TRAIN Batch 4/3400 loss 27.308159 loss_att 34.176716 loss_ctc 45.137486 loss_rnnt 23.346725 hw_loss 0.394648 lr 0.00082521 rank 4
2023-02-17 10:50:43,357 DEBUG TRAIN Batch 4/3500 loss 21.756048 loss_att 25.437256 loss_ctc 36.969200 loss_rnnt 18.800789 hw_loss 0.357369 lr 0.00082378 rank 7
2023-02-17 10:50:43,360 DEBUG TRAIN Batch 4/3500 loss 14.983096 loss_att 21.332134 loss_ctc 26.674353 loss_rnnt 11.970447 hw_loss 0.345013 lr 0.00082398 rank 6
2023-02-17 10:50:43,362 DEBUG TRAIN Batch 4/3500 loss 19.967638 loss_att 26.478071 loss_ctc 33.760040 loss_rnnt 16.667816 hw_loss 0.297652 lr 0.00082354 rank 3
2023-02-17 10:50:43,364 DEBUG TRAIN Batch 4/3500 loss 23.151386 loss_att 28.819530 loss_ctc 36.437477 loss_rnnt 20.077890 hw_loss 0.315730 lr 0.00082332 rank 0
2023-02-17 10:50:43,365 DEBUG TRAIN Batch 4/3500 loss 17.281931 loss_att 25.836678 loss_ctc 30.368923 loss_rnnt 13.689410 hw_loss 0.256200 lr 0.00082361 rank 5
2023-02-17 10:50:43,365 DEBUG TRAIN Batch 4/3500 loss 34.872177 loss_att 40.065834 loss_ctc 55.311264 loss_rnnt 30.940302 hw_loss 0.314879 lr 0.00082326 rank 1
2023-02-17 10:50:43,369 DEBUG TRAIN Batch 4/3500 loss 29.154139 loss_att 34.145458 loss_ctc 47.204700 loss_rnnt 25.569963 hw_loss 0.335942 lr 0.00082409 rank 4
2023-02-17 10:50:43,397 DEBUG TRAIN Batch 4/3500 loss 20.895866 loss_att 25.593937 loss_ctc 27.094017 loss_rnnt 18.953247 hw_loss 0.331099 lr 0.00082301 rank 2
2023-02-17 10:52:02,618 DEBUG TRAIN Batch 4/3600 loss 20.197102 loss_att 25.103634 loss_ctc 36.004322 loss_rnnt 16.901985 hw_loss 0.386593 lr 0.00082215 rank 1
2023-02-17 10:52:02,622 DEBUG TRAIN Batch 4/3600 loss 19.862740 loss_att 25.446657 loss_ctc 31.432281 loss_rnnt 17.007683 hw_loss 0.366874 lr 0.00082286 rank 6
2023-02-17 10:52:02,622 DEBUG TRAIN Batch 4/3600 loss 14.103854 loss_att 17.349716 loss_ctc 19.242897 loss_rnnt 12.558867 hw_loss 0.394895 lr 0.00082243 rank 3
2023-02-17 10:52:02,625 DEBUG TRAIN Batch 4/3600 loss 22.953587 loss_att 30.056562 loss_ctc 39.250877 loss_rnnt 19.178028 hw_loss 0.341236 lr 0.00082266 rank 7
2023-02-17 10:52:02,625 DEBUG TRAIN Batch 4/3600 loss 18.369154 loss_att 25.349529 loss_ctc 38.531513 loss_rnnt 14.135088 hw_loss 0.280642 lr 0.00082297 rank 4
2023-02-17 10:52:02,628 DEBUG TRAIN Batch 4/3600 loss 13.356627 loss_att 18.881187 loss_ctc 21.992634 loss_rnnt 10.866323 hw_loss 0.438608 lr 0.00082189 rank 2
2023-02-17 10:52:02,630 DEBUG TRAIN Batch 4/3600 loss 38.138168 loss_att 45.900768 loss_ctc 59.319107 loss_rnnt 33.602051 hw_loss 0.299009 lr 0.00082250 rank 5
2023-02-17 10:52:02,676 DEBUG TRAIN Batch 4/3600 loss 19.137758 loss_att 22.839535 loss_ctc 32.783562 loss_rnnt 16.380085 hw_loss 0.371016 lr 0.00082221 rank 0
2023-02-17 10:53:17,885 DEBUG TRAIN Batch 4/3700 loss 10.680740 loss_att 14.040400 loss_ctc 15.143566 loss_rnnt 9.238804 hw_loss 0.328053 lr 0.00082138 rank 5
2023-02-17 10:53:17,885 DEBUG TRAIN Batch 4/3700 loss 33.821880 loss_att 41.950649 loss_ctc 53.404156 loss_rnnt 29.382610 hw_loss 0.379769 lr 0.00082175 rank 6
2023-02-17 10:53:17,886 DEBUG TRAIN Batch 4/3700 loss 24.918457 loss_att 31.950882 loss_ctc 36.204445 loss_rnnt 21.783186 hw_loss 0.419973 lr 0.00082110 rank 0
2023-02-17 10:53:17,887 DEBUG TRAIN Batch 4/3700 loss 24.589500 loss_att 24.905668 loss_ctc 35.820534 loss_rnnt 22.824526 hw_loss 0.383006 lr 0.00082104 rank 1
2023-02-17 10:53:17,887 DEBUG TRAIN Batch 4/3700 loss 23.876221 loss_att 29.683846 loss_ctc 42.660103 loss_rnnt 19.951700 hw_loss 0.484642 lr 0.00082132 rank 3
2023-02-17 10:53:17,892 DEBUG TRAIN Batch 4/3700 loss 20.601717 loss_att 22.789162 loss_ctc 30.189438 loss_rnnt 18.695370 hw_loss 0.357180 lr 0.00082155 rank 7
2023-02-17 10:53:17,893 DEBUG TRAIN Batch 4/3700 loss 19.384237 loss_att 23.795002 loss_ctc 27.456150 loss_rnnt 17.221581 hw_loss 0.382967 lr 0.00082186 rank 4
2023-02-17 10:53:17,937 DEBUG TRAIN Batch 4/3700 loss 27.359316 loss_att 33.195492 loss_ctc 40.419502 loss_rnnt 24.287422 hw_loss 0.306189 lr 0.00082079 rank 2
2023-02-17 10:54:33,033 DEBUG TRAIN Batch 4/3800 loss 29.972525 loss_att 34.033379 loss_ctc 41.865303 loss_rnnt 27.352720 hw_loss 0.416115 lr 0.00082021 rank 3
2023-02-17 10:54:33,033 DEBUG TRAIN Batch 4/3800 loss 13.745498 loss_att 16.434134 loss_ctc 21.944046 loss_rnnt 11.885013 hw_loss 0.430535 lr 0.00082044 rank 7
2023-02-17 10:54:33,039 DEBUG TRAIN Batch 4/3800 loss 16.801262 loss_att 17.527149 loss_ctc 23.734669 loss_rnnt 15.527728 hw_loss 0.382319 lr 0.00081968 rank 2
2023-02-17 10:54:33,041 DEBUG TRAIN Batch 4/3800 loss 11.927040 loss_att 14.390856 loss_ctc 20.021881 loss_rnnt 10.208375 hw_loss 0.274853 lr 0.00081999 rank 0
2023-02-17 10:54:33,041 DEBUG TRAIN Batch 4/3800 loss 18.811882 loss_att 25.191986 loss_ctc 28.701195 loss_rnnt 16.077515 hw_loss 0.262070 lr 0.00082075 rank 4
2023-02-17 10:54:33,043 DEBUG TRAIN Batch 4/3800 loss 22.373409 loss_att 26.205185 loss_ctc 31.544830 loss_rnnt 20.172598 hw_loss 0.396748 lr 0.00082028 rank 5
2023-02-17 10:54:33,042 DEBUG TRAIN Batch 4/3800 loss 23.230438 loss_att 24.475000 loss_ctc 33.364254 loss_rnnt 21.471495 hw_loss 0.297857 lr 0.00082064 rank 6
2023-02-17 10:54:33,042 DEBUG TRAIN Batch 4/3800 loss 5.720725 loss_att 9.457832 loss_ctc 9.474230 loss_rnnt 4.354062 hw_loss 0.222700 lr 0.00081994 rank 1
2023-02-17 10:55:52,636 DEBUG TRAIN Batch 4/3900 loss 24.553751 loss_att 33.850349 loss_ctc 44.043545 loss_rnnt 19.931755 hw_loss 0.307570 lr 0.00081918 rank 5
2023-02-17 10:55:52,638 DEBUG TRAIN Batch 4/3900 loss 21.027967 loss_att 29.116211 loss_ctc 41.958733 loss_rnnt 16.444727 hw_loss 0.327791 lr 0.00081934 rank 7
2023-02-17 10:55:52,642 DEBUG TRAIN Batch 4/3900 loss 14.193236 loss_att 19.232594 loss_ctc 21.079157 loss_rnnt 12.068596 hw_loss 0.372460 lr 0.00081911 rank 3
2023-02-17 10:55:52,644 DEBUG TRAIN Batch 4/3900 loss 24.253201 loss_att 30.719477 loss_ctc 35.754463 loss_rnnt 21.219238 hw_loss 0.388506 lr 0.00081889 rank 0
2023-02-17 10:55:52,645 DEBUG TRAIN Batch 4/3900 loss 29.640339 loss_att 34.236206 loss_ctc 42.748192 loss_rnnt 26.777866 hw_loss 0.366721 lr 0.00081965 rank 4
2023-02-17 10:55:52,646 DEBUG TRAIN Batch 4/3900 loss 23.901825 loss_att 31.409191 loss_ctc 40.425117 loss_rnnt 20.015076 hw_loss 0.341570 lr 0.00081884 rank 1
2023-02-17 10:55:52,650 DEBUG TRAIN Batch 4/3900 loss 25.725365 loss_att 30.799229 loss_ctc 39.230804 loss_rnnt 22.733526 hw_loss 0.330639 lr 0.00081858 rank 2
2023-02-17 10:55:52,659 DEBUG TRAIN Batch 4/3900 loss 21.276791 loss_att 31.703506 loss_ctc 34.081375 loss_rnnt 17.244392 hw_loss 0.449581 lr 0.00081954 rank 6
2023-02-17 10:57:10,405 DEBUG TRAIN Batch 4/4000 loss 37.070194 loss_att 47.254711 loss_ctc 70.758049 loss_rnnt 30.350643 hw_loss 0.357996 lr 0.00081780 rank 0
2023-02-17 10:57:10,407 DEBUG TRAIN Batch 4/4000 loss 28.164259 loss_att 34.000305 loss_ctc 39.557140 loss_rnnt 25.290184 hw_loss 0.352154 lr 0.00081774 rank 1
2023-02-17 10:57:10,407 DEBUG TRAIN Batch 4/4000 loss 16.919903 loss_att 24.442680 loss_ctc 32.067780 loss_rnnt 13.231108 hw_loss 0.308482 lr 0.00081808 rank 5
2023-02-17 10:57:10,408 DEBUG TRAIN Batch 4/4000 loss 27.555811 loss_att 38.928619 loss_ctc 49.311783 loss_rnnt 22.199909 hw_loss 0.338516 lr 0.00081844 rank 6
2023-02-17 10:57:10,409 DEBUG TRAIN Batch 4/4000 loss 20.156321 loss_att 24.565466 loss_ctc 32.754211 loss_rnnt 17.415966 hw_loss 0.335263 lr 0.00081801 rank 3
2023-02-17 10:57:10,410 DEBUG TRAIN Batch 4/4000 loss 30.070204 loss_att 36.517277 loss_ctc 42.641556 loss_rnnt 26.923239 hw_loss 0.340064 lr 0.00081855 rank 4
2023-02-17 10:57:10,411 DEBUG TRAIN Batch 4/4000 loss 16.992260 loss_att 20.984823 loss_ctc 33.467533 loss_rnnt 13.801626 hw_loss 0.366409 lr 0.00081824 rank 7
2023-02-17 10:57:10,457 DEBUG TRAIN Batch 4/4000 loss 17.496140 loss_att 24.472157 loss_ctc 27.480185 loss_rnnt 14.568359 hw_loss 0.377570 lr 0.00081749 rank 2
2023-02-17 10:58:27,294 DEBUG TRAIN Batch 4/4100 loss 20.117928 loss_att 23.705135 loss_ctc 32.765774 loss_rnnt 17.487194 hw_loss 0.425458 lr 0.00081699 rank 5
2023-02-17 10:58:27,294 DEBUG TRAIN Batch 4/4100 loss 41.468952 loss_att 53.291962 loss_ctc 71.005402 loss_rnnt 35.017090 hw_loss 0.279502 lr 0.00081735 rank 6
2023-02-17 10:58:27,295 DEBUG TRAIN Batch 4/4100 loss 23.769802 loss_att 32.644104 loss_ctc 41.567261 loss_rnnt 19.483494 hw_loss 0.259600 lr 0.00081670 rank 0
2023-02-17 10:58:27,296 DEBUG TRAIN Batch 4/4100 loss 31.104223 loss_att 38.094994 loss_ctc 50.371086 loss_rnnt 26.960018 hw_loss 0.332129 lr 0.00081665 rank 1
2023-02-17 10:58:27,297 DEBUG TRAIN Batch 4/4100 loss 32.368031 loss_att 37.035458 loss_ctc 43.268269 loss_rnnt 29.808161 hw_loss 0.324412 lr 0.00081692 rank 3
2023-02-17 10:58:27,297 DEBUG TRAIN Batch 4/4100 loss 18.927206 loss_att 27.767118 loss_ctc 32.329384 loss_rnnt 15.163923 hw_loss 0.390646 lr 0.00081715 rank 7
2023-02-17 10:58:27,298 DEBUG TRAIN Batch 4/4100 loss 17.710178 loss_att 22.132950 loss_ctc 28.798321 loss_rnnt 15.188265 hw_loss 0.298011 lr 0.00081746 rank 4
2023-02-17 10:58:27,349 DEBUG TRAIN Batch 4/4100 loss 15.629154 loss_att 21.992271 loss_ctc 20.841034 loss_rnnt 13.501040 hw_loss 0.301075 lr 0.00081640 rank 2
2023-02-17 10:59:43,600 DEBUG TRAIN Batch 4/4200 loss 29.125532 loss_att 32.346527 loss_ctc 39.542965 loss_rnnt 26.927914 hw_loss 0.308309 lr 0.00081556 rank 1
2023-02-17 10:59:43,604 DEBUG TRAIN Batch 4/4200 loss 26.801960 loss_att 30.897854 loss_ctc 44.183708 loss_rnnt 23.488316 hw_loss 0.331685 lr 0.00081626 rank 6
2023-02-17 10:59:43,604 DEBUG TRAIN Batch 4/4200 loss 22.007113 loss_att 27.187607 loss_ctc 37.476715 loss_rnnt 18.737328 hw_loss 0.320763 lr 0.00081590 rank 5
2023-02-17 10:59:43,605 DEBUG TRAIN Batch 4/4200 loss 26.617289 loss_att 36.248665 loss_ctc 46.525444 loss_rnnt 21.862583 hw_loss 0.326266 lr 0.00081606 rank 7
2023-02-17 10:59:43,606 DEBUG TRAIN Batch 4/4200 loss 30.417671 loss_att 38.631939 loss_ctc 52.907848 loss_rnnt 25.596432 hw_loss 0.336935 lr 0.00081637 rank 4
2023-02-17 10:59:43,606 DEBUG TRAIN Batch 4/4200 loss 20.029703 loss_att 27.255825 loss_ctc 34.557152 loss_rnnt 16.405994 hw_loss 0.452797 lr 0.00081531 rank 2
2023-02-17 10:59:43,606 DEBUG TRAIN Batch 4/4200 loss 21.401815 loss_att 31.021130 loss_ctc 39.293053 loss_rnnt 16.973171 hw_loss 0.223657 lr 0.00081583 rank 3
2023-02-17 10:59:43,610 DEBUG TRAIN Batch 4/4200 loss 16.392176 loss_att 21.846676 loss_ctc 27.783066 loss_rnnt 13.621264 hw_loss 0.302294 lr 0.00081562 rank 0
2023-02-17 11:01:02,781 DEBUG TRAIN Batch 4/4300 loss 25.398764 loss_att 27.933290 loss_ctc 39.089993 loss_rnnt 22.874735 hw_loss 0.359300 lr 0.00081528 rank 4
2023-02-17 11:01:02,782 DEBUG TRAIN Batch 4/4300 loss 15.135180 loss_att 17.546215 loss_ctc 21.484270 loss_rnnt 13.617331 hw_loss 0.354559 lr 0.00081517 rank 6
2023-02-17 11:01:02,783 DEBUG TRAIN Batch 4/4300 loss 11.855869 loss_att 15.571844 loss_ctc 19.654308 loss_rnnt 9.889696 hw_loss 0.343475 lr 0.00081481 rank 5
2023-02-17 11:01:02,784 DEBUG TRAIN Batch 4/4300 loss 19.758667 loss_att 23.016815 loss_ctc 32.856022 loss_rnnt 17.162930 hw_loss 0.370865 lr 0.00081475 rank 3
2023-02-17 11:01:02,784 DEBUG TRAIN Batch 4/4300 loss 31.488359 loss_att 35.350388 loss_ctc 51.405182 loss_rnnt 27.897083 hw_loss 0.306174 lr 0.00081498 rank 7
2023-02-17 11:01:02,787 DEBUG TRAIN Batch 4/4300 loss 24.029099 loss_att 30.303370 loss_ctc 40.009521 loss_rnnt 20.462502 hw_loss 0.339412 lr 0.00081448 rank 1
2023-02-17 11:01:02,788 DEBUG TRAIN Batch 4/4300 loss 18.464851 loss_att 22.023981 loss_ctc 24.799467 loss_rnnt 16.724609 hw_loss 0.344623 lr 0.00081423 rank 2
2023-02-17 11:01:02,792 DEBUG TRAIN Batch 4/4300 loss 21.960041 loss_att 23.790842 loss_ctc 33.893703 loss_rnnt 19.868805 hw_loss 0.251103 lr 0.00081453 rank 0
2023-02-17 11:02:19,217 DEBUG TRAIN Batch 4/4400 loss 19.691053 loss_att 21.420181 loss_ctc 32.318970 loss_rnnt 17.477531 hw_loss 0.344950 lr 0.00081340 rank 1
2023-02-17 11:02:19,219 DEBUG TRAIN Batch 4/4400 loss 18.396023 loss_att 24.924664 loss_ctc 31.359602 loss_rnnt 15.149620 hw_loss 0.397869 lr 0.00081390 rank 7
2023-02-17 11:02:19,220 DEBUG TRAIN Batch 4/4400 loss 31.704285 loss_att 38.450996 loss_ctc 53.522133 loss_rnnt 27.270065 hw_loss 0.329687 lr 0.00081409 rank 6
2023-02-17 11:02:19,223 DEBUG TRAIN Batch 4/4400 loss 19.345516 loss_att 22.522779 loss_ctc 29.649956 loss_rnnt 17.132603 hw_loss 0.381630 lr 0.00081420 rank 4
2023-02-17 11:02:19,224 DEBUG TRAIN Batch 4/4400 loss 21.209473 loss_att 21.865955 loss_ctc 33.004303 loss_rnnt 19.318069 hw_loss 0.351490 lr 0.00081315 rank 2
2023-02-17 11:02:19,224 DEBUG TRAIN Batch 4/4400 loss 23.924215 loss_att 30.417698 loss_ctc 39.794094 loss_rnnt 20.320948 hw_loss 0.353597 lr 0.00081345 rank 0
2023-02-17 11:02:19,225 DEBUG TRAIN Batch 4/4400 loss 24.747061 loss_att 33.897114 loss_ctc 40.687134 loss_rnnt 20.573208 hw_loss 0.409686 lr 0.00081373 rank 5
2023-02-17 11:02:19,270 DEBUG TRAIN Batch 4/4400 loss 19.335464 loss_att 22.224613 loss_ctc 30.274460 loss_rnnt 17.084370 hw_loss 0.402624 lr 0.00081367 rank 3
2023-02-17 11:03:34,930 DEBUG TRAIN Batch 4/4500 loss 58.498188 loss_att 54.423889 loss_ctc 83.440308 loss_rnnt 55.825314 hw_loss 0.303961 lr 0.00081312 rank 4
2023-02-17 11:03:34,933 DEBUG TRAIN Batch 4/4500 loss 30.264536 loss_att 35.233994 loss_ctc 43.574276 loss_rnnt 27.269684 hw_loss 0.424364 lr 0.00081233 rank 1
2023-02-17 11:03:34,934 DEBUG TRAIN Batch 4/4500 loss 10.816385 loss_att 14.154994 loss_ctc 20.387302 loss_rnnt 8.723488 hw_loss 0.279474 lr 0.00081259 rank 3
2023-02-17 11:03:34,936 DEBUG TRAIN Batch 4/4500 loss 22.120417 loss_att 25.001019 loss_ctc 26.301243 loss_rnnt 20.778708 hw_loss 0.390274 lr 0.00081208 rank 2
2023-02-17 11:03:34,936 DEBUG TRAIN Batch 4/4500 loss 21.448248 loss_att 25.490807 loss_ctc 31.760895 loss_rnnt 19.040014 hw_loss 0.421313 lr 0.00081266 rank 5
2023-02-17 11:03:34,939 DEBUG TRAIN Batch 4/4500 loss 11.540408 loss_att 12.711828 loss_ctc 16.517771 loss_rnnt 10.356714 hw_loss 0.535802 lr 0.00081282 rank 7
2023-02-17 11:03:34,939 DEBUG TRAIN Batch 4/4500 loss 17.012423 loss_att 19.362823 loss_ctc 23.715948 loss_rnnt 15.458678 hw_loss 0.355992 lr 0.00081238 rank 0
2023-02-17 11:03:34,940 DEBUG TRAIN Batch 4/4500 loss 13.600653 loss_att 18.719971 loss_ctc 23.797148 loss_rnnt 10.992198 hw_loss 0.421983 lr 0.00081301 rank 6
2023-02-17 11:04:53,467 DEBUG TRAIN Batch 4/4600 loss 21.249973 loss_att 28.250561 loss_ctc 40.102238 loss_rnnt 17.182095 hw_loss 0.288985 lr 0.00081126 rank 1
2023-02-17 11:04:53,468 DEBUG TRAIN Batch 4/4600 loss 33.205048 loss_att 31.311180 loss_ctc 51.138088 loss_rnnt 31.017307 hw_loss 0.328954 lr 0.00081194 rank 6
2023-02-17 11:04:53,476 DEBUG TRAIN Batch 4/4600 loss 26.832272 loss_att 32.139771 loss_ctc 51.424477 loss_rnnt 22.318642 hw_loss 0.324689 lr 0.00081175 rank 7
2023-02-17 11:04:53,475 DEBUG TRAIN Batch 4/4600 loss 25.102942 loss_att 33.662712 loss_ctc 43.878815 loss_rnnt 20.751957 hw_loss 0.254217 lr 0.00081205 rank 4
2023-02-17 11:04:53,482 DEBUG TRAIN Batch 4/4600 loss 31.310312 loss_att 36.997181 loss_ctc 48.123672 loss_rnnt 27.752907 hw_loss 0.334217 lr 0.00081101 rank 2
2023-02-17 11:04:53,487 DEBUG TRAIN Batch 4/4600 loss 36.977627 loss_att 51.120163 loss_ctc 52.845955 loss_rnnt 31.868080 hw_loss 0.309869 lr 0.00081131 rank 0
2023-02-17 11:04:53,487 DEBUG TRAIN Batch 4/4600 loss 19.106367 loss_att 20.598091 loss_ctc 29.083740 loss_rnnt 17.343044 hw_loss 0.252490 lr 0.00081159 rank 5
2023-02-17 11:04:53,517 DEBUG TRAIN Batch 4/4600 loss 6.562520 loss_att 10.377842 loss_ctc 14.995399 loss_rnnt 4.498918 hw_loss 0.330287 lr 0.00081152 rank 3
2023-02-17 11:06:10,644 DEBUG TRAIN Batch 4/4700 loss 31.118166 loss_att 40.303238 loss_ctc 49.279610 loss_rnnt 26.718029 hw_loss 0.265494 lr 0.00081052 rank 5
2023-02-17 11:06:10,647 DEBUG TRAIN Batch 4/4700 loss 24.559607 loss_att 35.438026 loss_ctc 41.589577 loss_rnnt 19.884853 hw_loss 0.428264 lr 0.00081087 rank 6
2023-02-17 11:06:10,651 DEBUG TRAIN Batch 4/4700 loss 22.732286 loss_att 29.989676 loss_ctc 35.002670 loss_rnnt 19.522923 hw_loss 0.228440 lr 0.00080995 rank 2
2023-02-17 11:06:10,650 DEBUG TRAIN Batch 4/4700 loss 16.801527 loss_att 21.142998 loss_ctc 33.825111 loss_rnnt 13.432558 hw_loss 0.432869 lr 0.00081098 rank 4
2023-02-17 11:06:10,651 DEBUG TRAIN Batch 4/4700 loss 28.521776 loss_att 35.036766 loss_ctc 42.374908 loss_rnnt 25.164680 hw_loss 0.388150 lr 0.00081068 rank 7
2023-02-17 11:06:10,654 DEBUG TRAIN Batch 4/4700 loss 32.289131 loss_att 35.361237 loss_ctc 49.620934 loss_rnnt 29.128986 hw_loss 0.440278 lr 0.00081024 rank 0
2023-02-17 11:06:10,654 DEBUG TRAIN Batch 4/4700 loss 20.986546 loss_att 26.198780 loss_ctc 40.015709 loss_rnnt 17.210743 hw_loss 0.367753 lr 0.00081046 rank 3
2023-02-17 11:06:10,658 DEBUG TRAIN Batch 4/4700 loss 28.558399 loss_att 32.575943 loss_ctc 43.831085 loss_rnnt 25.531981 hw_loss 0.349787 lr 0.00081019 rank 1
2023-02-17 11:07:25,360 DEBUG TRAIN Batch 4/4800 loss 22.936396 loss_att 31.279112 loss_ctc 33.827667 loss_rnnt 19.628735 hw_loss 0.350528 lr 0.00080981 rank 6
2023-02-17 11:07:25,362 DEBUG TRAIN Batch 4/4800 loss 32.693504 loss_att 38.151058 loss_ctc 50.989849 loss_rnnt 28.963123 hw_loss 0.373789 lr 0.00080913 rank 1
2023-02-17 11:07:25,363 DEBUG TRAIN Batch 4/4800 loss 30.531233 loss_att 34.245903 loss_ctc 49.250183 loss_rnnt 27.091461 hw_loss 0.376831 lr 0.00080939 rank 3
2023-02-17 11:07:25,363 DEBUG TRAIN Batch 4/4800 loss 22.420572 loss_att 29.159569 loss_ctc 36.041351 loss_rnnt 19.143547 hw_loss 0.212103 lr 0.00080962 rank 7
2023-02-17 11:07:25,364 DEBUG TRAIN Batch 4/4800 loss 23.672512 loss_att 30.623487 loss_ctc 40.129829 loss_rnnt 19.892313 hw_loss 0.366927 lr 0.00080946 rank 5
2023-02-17 11:07:25,370 DEBUG TRAIN Batch 4/4800 loss 20.595757 loss_att 25.976608 loss_ctc 36.173473 loss_rnnt 17.260429 hw_loss 0.341488 lr 0.00080991 rank 4
2023-02-17 11:07:25,370 DEBUG TRAIN Batch 4/4800 loss 19.694887 loss_att 20.126472 loss_ctc 29.430019 loss_rnnt 18.072306 hw_loss 0.446709 lr 0.00080918 rank 0
2023-02-17 11:07:25,374 DEBUG TRAIN Batch 4/4800 loss 29.298496 loss_att 39.385185 loss_ctc 49.666145 loss_rnnt 24.319077 hw_loss 0.461991 lr 0.00080889 rank 2
2023-02-17 11:08:42,477 DEBUG TRAIN Batch 4/4900 loss 16.856718 loss_att 20.734829 loss_ctc 32.861794 loss_rnnt 13.796916 hw_loss 0.281565 lr 0.00080875 rank 6
2023-02-17 11:08:42,478 DEBUG TRAIN Batch 4/4900 loss 22.449570 loss_att 26.380644 loss_ctc 31.432152 loss_rnnt 20.231997 hw_loss 0.438153 lr 0.00080856 rank 7
2023-02-17 11:08:42,479 DEBUG TRAIN Batch 4/4900 loss 24.426090 loss_att 28.029444 loss_ctc 46.037712 loss_rnnt 20.642120 hw_loss 0.340781 lr 0.00080834 rank 3
2023-02-17 11:08:42,479 DEBUG TRAIN Batch 4/4900 loss 26.686985 loss_att 33.977901 loss_ctc 43.177013 loss_rnnt 22.826191 hw_loss 0.382386 lr 0.00080840 rank 5
2023-02-17 11:08:42,486 DEBUG TRAIN Batch 4/4900 loss 20.408060 loss_att 21.546223 loss_ctc 30.874809 loss_rnnt 18.575245 hw_loss 0.393029 lr 0.00080783 rank 2
2023-02-17 11:08:42,488 DEBUG TRAIN Batch 4/4900 loss 29.732697 loss_att 36.980293 loss_ctc 50.566650 loss_rnnt 25.323139 hw_loss 0.341576 lr 0.00080807 rank 1
2023-02-17 11:08:42,488 DEBUG TRAIN Batch 4/4900 loss 22.924919 loss_att 29.002754 loss_ctc 35.073315 loss_rnnt 19.927551 hw_loss 0.303772 lr 0.00080812 rank 0
2023-02-17 11:08:42,493 DEBUG TRAIN Batch 4/4900 loss 17.778425 loss_att 22.258141 loss_ctc 26.820190 loss_rnnt 15.443786 hw_loss 0.437112 lr 0.00080885 rank 4
2023-02-17 11:10:01,559 DEBUG TRAIN Batch 4/5000 loss 26.808388 loss_att 33.327614 loss_ctc 46.211716 loss_rnnt 22.754559 hw_loss 0.305390 lr 0.00080769 rank 6
2023-02-17 11:10:01,569 DEBUG TRAIN Batch 4/5000 loss 17.416965 loss_att 21.230225 loss_ctc 26.478485 loss_rnnt 15.263714 hw_loss 0.341991 lr 0.00080750 rank 7
2023-02-17 11:10:01,568 DEBUG TRAIN Batch 4/5000 loss 20.280489 loss_att 26.777861 loss_ctc 30.638311 loss_rnnt 17.344667 hw_loss 0.478692 lr 0.00080702 rank 1
2023-02-17 11:10:01,569 DEBUG TRAIN Batch 4/5000 loss 41.123238 loss_att 42.869339 loss_ctc 58.229439 loss_rnnt 38.326050 hw_loss 0.313383 lr 0.00080780 rank 4
2023-02-17 11:10:01,570 DEBUG TRAIN Batch 4/5000 loss 25.024403 loss_att 27.709270 loss_ctc 35.413570 loss_rnnt 22.913954 hw_loss 0.352974 lr 0.00080707 rank 0
2023-02-17 11:10:01,570 DEBUG TRAIN Batch 4/5000 loss 24.654114 loss_att 28.943462 loss_ctc 41.520401 loss_rnnt 21.351273 hw_loss 0.367748 lr 0.00080734 rank 5
2023-02-17 11:10:01,571 DEBUG TRAIN Batch 4/5000 loss 19.849983 loss_att 22.531569 loss_ctc 31.639465 loss_rnnt 17.522978 hw_loss 0.410170 lr 0.00080728 rank 3
2023-02-17 11:10:01,616 DEBUG TRAIN Batch 4/5000 loss 23.728809 loss_att 25.382874 loss_ctc 35.895645 loss_rnnt 21.611267 hw_loss 0.308410 lr 0.00080678 rank 2
2023-02-17 11:11:17,307 DEBUG TRAIN Batch 4/5100 loss 21.464718 loss_att 29.284691 loss_ctc 34.074398 loss_rnnt 18.058302 hw_loss 0.302119 lr 0.00080629 rank 5
2023-02-17 11:11:17,308 DEBUG TRAIN Batch 4/5100 loss 16.241997 loss_att 20.721115 loss_ctc 26.460293 loss_rnnt 13.818492 hw_loss 0.309830 lr 0.00080623 rank 3
2023-02-17 11:11:17,308 DEBUG TRAIN Batch 4/5100 loss 14.642431 loss_att 18.864426 loss_ctc 24.213266 loss_rnnt 12.289012 hw_loss 0.436703 lr 0.00080645 rank 7
2023-02-17 11:11:17,309 DEBUG TRAIN Batch 4/5100 loss 6.954457 loss_att 14.873117 loss_ctc 17.829111 loss_rnnt 3.745897 hw_loss 0.327886 lr 0.00080573 rank 2
2023-02-17 11:11:17,310 DEBUG TRAIN Batch 4/5100 loss 31.339926 loss_att 34.763687 loss_ctc 46.664520 loss_rnnt 28.498688 hw_loss 0.212257 lr 0.00080597 rank 1
2023-02-17 11:11:17,310 DEBUG TRAIN Batch 4/5100 loss 53.898209 loss_att 56.852783 loss_ctc 70.367722 loss_rnnt 50.936657 hw_loss 0.327564 lr 0.00080675 rank 4
2023-02-17 11:11:17,310 DEBUG TRAIN Batch 4/5100 loss 16.567762 loss_att 17.059891 loss_ctc 21.631737 loss_rnnt 15.564534 hw_loss 0.430513 lr 0.00080664 rank 6
2023-02-17 11:11:17,314 DEBUG TRAIN Batch 4/5100 loss 24.397818 loss_att 24.163748 loss_ctc 36.625252 loss_rnnt 22.576694 hw_loss 0.445524 lr 0.00080602 rank 0
2023-02-17 11:12:34,499 DEBUG TRAIN Batch 4/5200 loss 33.830170 loss_att 41.561409 loss_ctc 54.822247 loss_rnnt 29.262442 hw_loss 0.417254 lr 0.00080540 rank 7
2023-02-17 11:12:34,502 DEBUG TRAIN Batch 4/5200 loss 34.904991 loss_att 35.252502 loss_ctc 42.008263 loss_rnnt 33.723938 hw_loss 0.308339 lr 0.00080492 rank 1
2023-02-17 11:12:34,506 DEBUG TRAIN Batch 4/5200 loss 28.934793 loss_att 32.822769 loss_ctc 48.570862 loss_rnnt 25.345160 hw_loss 0.363555 lr 0.00080525 rank 5
2023-02-17 11:12:34,505 DEBUG TRAIN Batch 4/5200 loss 10.442964 loss_att 14.126269 loss_ctc 19.665909 loss_rnnt 8.385390 hw_loss 0.170974 lr 0.00080498 rank 0
2023-02-17 11:12:34,505 DEBUG TRAIN Batch 4/5200 loss 22.896538 loss_att 30.481794 loss_ctc 33.990150 loss_rnnt 19.766775 hw_loss 0.250432 lr 0.00080519 rank 3
2023-02-17 11:12:34,509 DEBUG TRAIN Batch 4/5200 loss 38.662563 loss_att 39.877155 loss_ctc 54.380058 loss_rnnt 36.102673 hw_loss 0.414945 lr 0.00080559 rank 6
2023-02-17 11:12:34,509 DEBUG TRAIN Batch 4/5200 loss 19.733490 loss_att 23.859518 loss_ctc 33.321095 loss_rnnt 16.907377 hw_loss 0.354799 lr 0.00080570 rank 4
2023-02-17 11:12:34,510 DEBUG TRAIN Batch 4/5200 loss 24.835190 loss_att 27.919865 loss_ctc 46.482281 loss_rnnt 21.184618 hw_loss 0.276298 lr 0.00080468 rank 2
2023-02-17 11:13:53,352 DEBUG TRAIN Batch 4/5300 loss 20.598425 loss_att 26.334606 loss_ctc 30.816198 loss_rnnt 17.858089 hw_loss 0.432615 lr 0.00080421 rank 5
2023-02-17 11:13:53,358 DEBUG TRAIN Batch 4/5300 loss 14.589523 loss_att 18.439013 loss_ctc 25.812323 loss_rnnt 12.062999 hw_loss 0.487977 lr 0.00080455 rank 6
2023-02-17 11:13:53,357 DEBUG TRAIN Batch 4/5300 loss 21.008287 loss_att 23.376591 loss_ctc 27.888470 loss_rnnt 19.398092 hw_loss 0.410958 lr 0.00080394 rank 0
2023-02-17 11:13:53,357 DEBUG TRAIN Batch 4/5300 loss 20.393925 loss_att 25.156784 loss_ctc 28.430672 loss_rnnt 18.140800 hw_loss 0.429351 lr 0.00080388 rank 1
2023-02-17 11:13:53,358 DEBUG TRAIN Batch 4/5300 loss 19.501541 loss_att 25.235315 loss_ctc 36.797638 loss_rnnt 15.947424 hw_loss 0.189781 lr 0.00080414 rank 3
2023-02-17 11:13:53,360 DEBUG TRAIN Batch 4/5300 loss 15.147952 loss_att 25.357426 loss_ctc 29.593893 loss_rnnt 11.008131 hw_loss 0.322128 lr 0.00080364 rank 2
2023-02-17 11:13:53,361 DEBUG TRAIN Batch 4/5300 loss 22.271788 loss_att 22.449955 loss_ctc 33.075878 loss_rnnt 20.621441 hw_loss 0.326567 lr 0.00080436 rank 7
2023-02-17 11:13:53,365 DEBUG TRAIN Batch 4/5300 loss 13.609200 loss_att 18.790405 loss_ctc 22.333176 loss_rnnt 11.190158 hw_loss 0.411760 lr 0.00080465 rank 4
2023-02-17 11:15:11,044 DEBUG TRAIN Batch 4/5400 loss 13.785604 loss_att 20.132742 loss_ctc 26.153885 loss_rnnt 10.687167 hw_loss 0.337320 lr 0.00080351 rank 6
2023-02-17 11:15:11,044 DEBUG TRAIN Batch 4/5400 loss 29.170523 loss_att 38.722435 loss_ctc 44.151154 loss_rnnt 25.093750 hw_loss 0.316827 lr 0.00080332 rank 7
2023-02-17 11:15:11,045 DEBUG TRAIN Batch 4/5400 loss 21.865150 loss_att 30.407536 loss_ctc 38.690376 loss_rnnt 17.689421 hw_loss 0.419796 lr 0.00080317 rank 5
2023-02-17 11:15:11,048 DEBUG TRAIN Batch 4/5400 loss 20.882998 loss_att 25.248917 loss_ctc 36.023170 loss_rnnt 17.790167 hw_loss 0.376791 lr 0.00080361 rank 4
2023-02-17 11:15:11,049 DEBUG TRAIN Batch 4/5400 loss 29.003857 loss_att 35.223549 loss_ctc 40.741104 loss_rnnt 26.049633 hw_loss 0.272473 lr 0.00080311 rank 3
2023-02-17 11:15:11,056 DEBUG TRAIN Batch 4/5400 loss 44.053776 loss_att 46.481514 loss_ctc 60.593727 loss_rnnt 41.253059 hw_loss 0.205945 lr 0.00080290 rank 0
2023-02-17 11:15:11,055 DEBUG TRAIN Batch 4/5400 loss 12.859735 loss_att 17.651226 loss_ctc 25.353958 loss_rnnt 10.102289 hw_loss 0.249843 lr 0.00080285 rank 1
2023-02-17 11:15:11,102 DEBUG TRAIN Batch 4/5400 loss 18.571562 loss_att 24.009853 loss_ctc 28.572041 loss_rnnt 15.958200 hw_loss 0.360569 lr 0.00080261 rank 2
2023-02-17 11:16:27,800 DEBUG TRAIN Batch 4/5500 loss 23.528027 loss_att 28.359303 loss_ctc 35.472305 loss_rnnt 20.780340 hw_loss 0.354120 lr 0.00080247 rank 6
2023-02-17 11:16:27,801 DEBUG TRAIN Batch 4/5500 loss 17.048721 loss_att 18.405056 loss_ctc 29.344738 loss_rnnt 14.959988 hw_loss 0.333748 lr 0.00080258 rank 4
2023-02-17 11:16:27,803 DEBUG TRAIN Batch 4/5500 loss 19.082886 loss_att 30.141689 loss_ctc 33.959995 loss_rnnt 14.699341 hw_loss 0.352817 lr 0.00080207 rank 3
2023-02-17 11:16:27,805 DEBUG TRAIN Batch 4/5500 loss 41.599388 loss_att 44.614506 loss_ctc 64.024208 loss_rnnt 37.829979 hw_loss 0.330764 lr 0.00080229 rank 7
2023-02-17 11:16:27,806 DEBUG TRAIN Batch 4/5500 loss 19.442505 loss_att 30.159857 loss_ctc 32.842529 loss_rnnt 15.347732 hw_loss 0.308684 lr 0.00080213 rank 5
2023-02-17 11:16:27,808 DEBUG TRAIN Batch 4/5500 loss 33.544926 loss_att 40.115452 loss_ctc 49.408646 loss_rnnt 29.947769 hw_loss 0.314793 lr 0.00080158 rank 2
2023-02-17 11:16:27,808 DEBUG TRAIN Batch 4/5500 loss 22.521862 loss_att 31.453735 loss_ctc 42.775726 loss_rnnt 17.801340 hw_loss 0.438062 lr 0.00080181 rank 1
2023-02-17 11:16:27,810 DEBUG TRAIN Batch 4/5500 loss 25.473522 loss_att 30.009636 loss_ctc 37.057487 loss_rnnt 22.839920 hw_loss 0.340966 lr 0.00080187 rank 0
2023-02-17 11:17:42,968 DEBUG TRAIN Batch 4/5600 loss 16.286922 loss_att 21.819603 loss_ctc 27.647243 loss_rnnt 13.483416 hw_loss 0.341742 lr 0.00080144 rank 6
2023-02-17 11:17:42,970 DEBUG TRAIN Batch 4/5600 loss 15.187946 loss_att 23.097166 loss_ctc 29.654110 loss_rnnt 11.478914 hw_loss 0.371935 lr 0.00080078 rank 1
2023-02-17 11:17:42,975 DEBUG TRAIN Batch 4/5600 loss 35.020485 loss_att 41.381248 loss_ctc 54.009018 loss_rnnt 30.979065 hw_loss 0.445246 lr 0.00080104 rank 3
2023-02-17 11:17:42,978 DEBUG TRAIN Batch 4/5600 loss 27.600269 loss_att 30.714890 loss_ctc 34.782188 loss_rnnt 25.806934 hw_loss 0.399043 lr 0.00080084 rank 0
2023-02-17 11:17:42,978 DEBUG TRAIN Batch 4/5600 loss 28.088802 loss_att 33.686874 loss_ctc 42.400642 loss_rnnt 24.898190 hw_loss 0.305160 lr 0.00080126 rank 7
2023-02-17 11:17:42,978 DEBUG TRAIN Batch 4/5600 loss 24.548723 loss_att 24.530180 loss_ctc 32.241436 loss_rnnt 23.320642 hw_loss 0.386426 lr 0.00080155 rank 4
2023-02-17 11:17:42,979 DEBUG TRAIN Batch 4/5600 loss 18.522812 loss_att 22.914511 loss_ctc 36.429813 loss_rnnt 15.024256 hw_loss 0.436151 lr 0.00080055 rank 2
2023-02-17 11:17:42,979 DEBUG TRAIN Batch 4/5600 loss 19.210327 loss_att 23.853462 loss_ctc 30.795177 loss_rnnt 16.542620 hw_loss 0.364560 lr 0.00080110 rank 5
2023-02-17 11:19:03,762 DEBUG TRAIN Batch 4/5700 loss 22.846426 loss_att 27.774506 loss_ctc 38.224537 loss_rnnt 19.599463 hw_loss 0.395497 lr 0.00080042 rank 6
2023-02-17 11:19:03,764 DEBUG TRAIN Batch 4/5700 loss 23.980776 loss_att 28.530632 loss_ctc 32.636993 loss_rnnt 21.737226 hw_loss 0.336409 lr 0.00080002 rank 3
2023-02-17 11:19:03,765 DEBUG TRAIN Batch 4/5700 loss 15.097449 loss_att 18.284611 loss_ctc 27.024841 loss_rnnt 12.648688 hw_loss 0.414393 lr 0.00079981 rank 0
2023-02-17 11:19:03,765 DEBUG TRAIN Batch 4/5700 loss 13.918074 loss_att 19.280510 loss_ctc 23.477654 loss_rnnt 11.388744 hw_loss 0.341682 lr 0.00080008 rank 5
2023-02-17 11:19:03,766 DEBUG TRAIN Batch 4/5700 loss 21.124018 loss_att 24.355480 loss_ctc 27.909245 loss_rnnt 19.422277 hw_loss 0.282657 lr 0.00080023 rank 7
2023-02-17 11:19:03,767 DEBUG TRAIN Batch 4/5700 loss 20.748245 loss_att 34.132923 loss_ctc 30.366131 loss_rnnt 16.650562 hw_loss 0.259427 lr 0.00079952 rank 2
2023-02-17 11:19:03,767 DEBUG TRAIN Batch 4/5700 loss 22.470385 loss_att 24.520639 loss_ctc 32.699333 loss_rnnt 20.460468 hw_loss 0.442510 lr 0.00079976 rank 1
2023-02-17 11:19:03,817 DEBUG TRAIN Batch 4/5700 loss 12.387052 loss_att 12.578860 loss_ctc 16.366875 loss_rnnt 11.582882 hw_loss 0.440933 lr 0.00080052 rank 4
2023-02-17 11:20:19,273 DEBUG TRAIN Batch 4/5800 loss 45.622940 loss_att 46.476189 loss_ctc 54.632412 loss_rnnt 44.070538 hw_loss 0.338418 lr 0.00079874 rank 1
2023-02-17 11:20:19,276 DEBUG TRAIN Batch 4/5800 loss 13.026496 loss_att 18.040926 loss_ctc 24.451452 loss_rnnt 10.319538 hw_loss 0.338895 lr 0.00079850 rank 2
2023-02-17 11:20:19,278 DEBUG TRAIN Batch 4/5800 loss 13.191877 loss_att 13.144559 loss_ctc 17.142710 loss_rnnt 12.448031 hw_loss 0.424750 lr 0.00079939 rank 6
2023-02-17 11:20:19,279 DEBUG TRAIN Batch 4/5800 loss 12.062714 loss_att 16.350870 loss_ctc 19.293461 loss_rnnt 10.095098 hw_loss 0.273534 lr 0.00079921 rank 7
2023-02-17 11:20:19,279 DEBUG TRAIN Batch 4/5800 loss 18.841148 loss_att 22.377703 loss_ctc 34.041470 loss_rnnt 15.909891 hw_loss 0.369817 lr 0.00079949 rank 4
2023-02-17 11:20:19,279 DEBUG TRAIN Batch 4/5800 loss 17.648794 loss_att 18.516909 loss_ctc 27.694128 loss_rnnt 15.947808 hw_loss 0.352473 lr 0.00079905 rank 5
2023-02-17 11:20:19,285 DEBUG TRAIN Batch 4/5800 loss 16.980097 loss_att 17.616259 loss_ctc 42.784454 loss_rnnt 13.193706 hw_loss 0.409830 lr 0.00079879 rank 0
2023-02-17 11:20:19,286 DEBUG TRAIN Batch 4/5800 loss 18.042503 loss_att 24.636497 loss_ctc 26.162909 loss_rnnt 15.513504 hw_loss 0.239024 lr 0.00079899 rank 3
2023-02-17 11:21:36,052 DEBUG TRAIN Batch 4/5900 loss 26.061539 loss_att 32.980495 loss_ctc 38.964088 loss_rnnt 22.788267 hw_loss 0.317141 lr 0.00079837 rank 6
2023-02-17 11:21:36,058 DEBUG TRAIN Batch 4/5900 loss 25.838993 loss_att 31.741070 loss_ctc 39.304516 loss_rnnt 22.692913 hw_loss 0.319243 lr 0.00079804 rank 5
2023-02-17 11:21:36,057 DEBUG TRAIN Batch 4/5900 loss 40.298229 loss_att 44.969509 loss_ctc 63.062950 loss_rnnt 36.129337 hw_loss 0.373764 lr 0.00079819 rank 7
2023-02-17 11:21:36,064 DEBUG TRAIN Batch 4/5900 loss 19.834646 loss_att 23.787117 loss_ctc 24.936920 loss_rnnt 18.232157 hw_loss 0.246923 lr 0.00079777 rank 0
2023-02-17 11:21:36,064 DEBUG TRAIN Batch 4/5900 loss 25.088217 loss_att 34.163841 loss_ctc 40.674671 loss_rnnt 20.980824 hw_loss 0.401389 lr 0.00079749 rank 2
2023-02-17 11:21:36,064 DEBUG TRAIN Batch 4/5900 loss 17.499001 loss_att 22.530485 loss_ctc 28.937277 loss_rnnt 14.784035 hw_loss 0.344183 lr 0.00079847 rank 4
2023-02-17 11:21:36,064 DEBUG TRAIN Batch 4/5900 loss 15.288209 loss_att 19.962917 loss_ctc 25.917248 loss_rnnt 12.765343 hw_loss 0.320098 lr 0.00079772 rank 1
2023-02-17 11:21:36,109 DEBUG TRAIN Batch 4/5900 loss 34.628929 loss_att 38.990849 loss_ctc 55.278534 loss_rnnt 30.823774 hw_loss 0.336541 lr 0.00079798 rank 3
2023-02-17 11:22:54,727 DEBUG TRAIN Batch 4/6000 loss 31.800911 loss_att 39.746132 loss_ctc 44.662922 loss_rnnt 28.267282 hw_loss 0.430588 lr 0.00079746 rank 4
2023-02-17 11:22:54,729 DEBUG TRAIN Batch 4/6000 loss 15.897699 loss_att 19.701508 loss_ctc 25.870846 loss_rnnt 13.637503 hw_loss 0.318156 lr 0.00079736 rank 6
2023-02-17 11:22:54,729 DEBUG TRAIN Batch 4/6000 loss 19.901434 loss_att 22.504274 loss_ctc 27.929022 loss_rnnt 18.124027 hw_loss 0.349676 lr 0.00079702 rank 5
2023-02-17 11:22:54,729 DEBUG TRAIN Batch 4/6000 loss 17.515627 loss_att 23.293760 loss_ctc 26.989689 loss_rnnt 14.978662 hw_loss 0.221492 lr 0.00079648 rank 2
2023-02-17 11:22:54,730 DEBUG TRAIN Batch 4/6000 loss 24.904409 loss_att 34.750607 loss_ctc 43.875378 loss_rnnt 20.236698 hw_loss 0.316891 lr 0.00079671 rank 1
2023-02-17 11:22:54,731 DEBUG TRAIN Batch 4/6000 loss 12.728760 loss_att 16.184904 loss_ctc 19.672333 loss_rnnt 10.962690 hw_loss 0.279432 lr 0.00079696 rank 3
2023-02-17 11:22:54,734 DEBUG TRAIN Batch 4/6000 loss 14.473330 loss_att 18.791887 loss_ctc 22.508427 loss_rnnt 12.385078 hw_loss 0.287238 lr 0.00079676 rank 0
2023-02-17 11:22:54,756 DEBUG TRAIN Batch 4/6000 loss 15.588092 loss_att 21.690725 loss_ctc 24.124540 loss_rnnt 13.056135 hw_loss 0.324818 lr 0.00079717 rank 7
2023-02-17 11:24:12,559 DEBUG TRAIN Batch 4/6100 loss 24.665209 loss_att 29.351677 loss_ctc 38.120132 loss_rnnt 21.744383 hw_loss 0.355393 lr 0.00079601 rank 5
2023-02-17 11:24:12,562 DEBUG TRAIN Batch 4/6100 loss 22.839376 loss_att 29.766266 loss_ctc 40.769157 loss_rnnt 18.837303 hw_loss 0.423858 lr 0.00079616 rank 7
2023-02-17 11:24:12,563 DEBUG TRAIN Batch 4/6100 loss 29.530668 loss_att 33.917675 loss_ctc 42.819954 loss_rnnt 26.689379 hw_loss 0.359967 lr 0.00079634 rank 6
2023-02-17 11:24:12,564 DEBUG TRAIN Batch 4/6100 loss 37.664577 loss_att 43.081596 loss_ctc 58.148472 loss_rnnt 33.698860 hw_loss 0.283367 lr 0.00079644 rank 4
2023-02-17 11:24:12,565 DEBUG TRAIN Batch 4/6100 loss 14.907802 loss_att 18.975796 loss_ctc 22.768658 loss_rnnt 12.918755 hw_loss 0.238751 lr 0.00079570 rank 1
2023-02-17 11:24:12,567 DEBUG TRAIN Batch 4/6100 loss 10.158778 loss_att 17.873152 loss_ctc 21.936836 loss_rnnt 6.842968 hw_loss 0.379738 lr 0.00079547 rank 2
2023-02-17 11:24:12,567 DEBUG TRAIN Batch 4/6100 loss 13.201887 loss_att 19.933128 loss_ctc 21.527523 loss_rnnt 10.595081 hw_loss 0.282136 lr 0.00079595 rank 3
2023-02-17 11:24:12,571 DEBUG TRAIN Batch 4/6100 loss 16.656006 loss_att 22.004412 loss_ctc 24.103172 loss_rnnt 14.461344 hw_loss 0.247543 lr 0.00079575 rank 0
2023-02-17 11:25:28,694 DEBUG TRAIN Batch 4/6200 loss 11.058350 loss_att 15.825155 loss_ctc 24.644371 loss_rnnt 8.105080 hw_loss 0.353323 lr 0.00079544 rank 4
2023-02-17 11:25:28,697 DEBUG TRAIN Batch 4/6200 loss 19.001095 loss_att 24.660057 loss_ctc 33.669468 loss_rnnt 15.741352 hw_loss 0.322815 lr 0.00079515 rank 7
2023-02-17 11:25:28,699 DEBUG TRAIN Batch 4/6200 loss 17.568083 loss_att 20.083563 loss_ctc 25.803366 loss_rnnt 15.798845 hw_loss 0.315194 lr 0.00079469 rank 1
2023-02-17 11:25:28,699 DEBUG TRAIN Batch 4/6200 loss 14.795461 loss_att 19.185757 loss_ctc 21.789326 loss_rnnt 12.743361 hw_loss 0.452861 lr 0.00079474 rank 0
2023-02-17 11:25:28,700 DEBUG TRAIN Batch 4/6200 loss 21.613577 loss_att 26.449289 loss_ctc 36.938053 loss_rnnt 18.468410 hw_loss 0.252678 lr 0.00079500 rank 5
2023-02-17 11:25:28,701 DEBUG TRAIN Batch 4/6200 loss 33.559582 loss_att 40.797855 loss_ctc 58.492004 loss_rnnt 28.616491 hw_loss 0.320837 lr 0.00079494 rank 3
2023-02-17 11:25:28,702 DEBUG TRAIN Batch 4/6200 loss 30.391415 loss_att 33.859051 loss_ctc 46.277573 loss_rnnt 27.400335 hw_loss 0.336370 lr 0.00079534 rank 6
2023-02-17 11:25:28,709 DEBUG TRAIN Batch 4/6200 loss 18.572762 loss_att 20.969515 loss_ctc 29.810163 loss_rnnt 16.348425 hw_loss 0.462499 lr 0.00079446 rank 2
2023-02-17 11:26:44,782 DEBUG TRAIN Batch 4/6300 loss 18.733452 loss_att 22.043150 loss_ctc 30.519678 loss_rnnt 16.342274 hw_loss 0.295770 lr 0.00079394 rank 3
2023-02-17 11:26:44,784 DEBUG TRAIN Batch 4/6300 loss 28.219143 loss_att 32.422325 loss_ctc 41.125561 loss_rnnt 25.527069 hw_loss 0.244837 lr 0.00079415 rank 7
2023-02-17 11:26:44,784 DEBUG TRAIN Batch 4/6300 loss 22.929245 loss_att 25.436153 loss_ctc 39.152283 loss_rnnt 20.027674 hw_loss 0.444593 lr 0.00079443 rank 4
2023-02-17 11:26:44,788 DEBUG TRAIN Batch 4/6300 loss 13.161714 loss_att 14.445311 loss_ctc 21.689142 loss_rnnt 11.581968 hw_loss 0.348817 lr 0.00079346 rank 2
2023-02-17 11:26:44,788 DEBUG TRAIN Batch 4/6300 loss 29.749298 loss_att 38.813023 loss_ctc 55.270294 loss_rnnt 24.371887 hw_loss 0.303498 lr 0.00079433 rank 6
2023-02-17 11:26:44,789 DEBUG TRAIN Batch 4/6300 loss 20.811710 loss_att 24.119331 loss_ctc 30.813913 loss_rnnt 18.640657 hw_loss 0.329809 lr 0.00079400 rank 5
2023-02-17 11:26:44,790 DEBUG TRAIN Batch 4/6300 loss 29.280403 loss_att 27.419785 loss_ctc 39.854340 loss_rnnt 28.031652 hw_loss 0.395656 lr 0.00079374 rank 0
2023-02-17 11:26:44,790 DEBUG TRAIN Batch 4/6300 loss 14.884253 loss_att 18.283108 loss_ctc 26.590948 loss_rnnt 12.455429 hw_loss 0.352801 lr 0.00079369 rank 1
2023-02-17 11:28:04,627 DEBUG TRAIN Batch 4/6400 loss 21.913811 loss_att 25.067921 loss_ctc 34.645264 loss_rnnt 19.413204 hw_loss 0.322983 lr 0.00079300 rank 5
2023-02-17 11:28:04,628 DEBUG TRAIN Batch 4/6400 loss 30.852886 loss_att 40.133537 loss_ctc 53.123009 loss_rnnt 25.896641 hw_loss 0.245180 lr 0.00079315 rank 7
2023-02-17 11:28:04,629 DEBUG TRAIN Batch 4/6400 loss 19.876701 loss_att 19.349545 loss_ctc 25.316645 loss_rnnt 19.080053 hw_loss 0.331418 lr 0.00079333 rank 6
2023-02-17 11:28:04,634 DEBUG TRAIN Batch 4/6400 loss 18.826405 loss_att 24.741159 loss_ctc 30.054182 loss_rnnt 16.014389 hw_loss 0.247549 lr 0.00079343 rank 4
2023-02-17 11:28:04,635 DEBUG TRAIN Batch 4/6400 loss 10.906044 loss_att 14.972416 loss_ctc 18.787724 loss_rnnt 8.844957 hw_loss 0.369229 lr 0.00079246 rank 2
2023-02-17 11:28:04,636 DEBUG TRAIN Batch 4/6400 loss 32.724869 loss_att 38.714622 loss_ctc 51.085228 loss_rnnt 28.906507 hw_loss 0.323177 lr 0.00079294 rank 3
2023-02-17 11:28:04,650 DEBUG TRAIN Batch 4/6400 loss 26.783747 loss_att 29.235020 loss_ctc 38.049614 loss_rnnt 24.639669 hw_loss 0.284447 lr 0.00079274 rank 0
2023-02-17 11:28:04,676 DEBUG TRAIN Batch 4/6400 loss 37.457932 loss_att 44.839211 loss_ctc 57.460903 loss_rnnt 33.192837 hw_loss 0.228328 lr 0.00079269 rank 1
2023-02-17 11:29:20,006 DEBUG TRAIN Batch 4/6500 loss 17.726877 loss_att 22.547653 loss_ctc 23.566708 loss_rnnt 15.789597 hw_loss 0.364653 lr 0.00079233 rank 6
2023-02-17 11:29:20,009 DEBUG TRAIN Batch 4/6500 loss 65.193329 loss_att 71.612152 loss_ctc 93.703308 loss_rnnt 59.904919 hw_loss 0.381219 lr 0.00079216 rank 7
2023-02-17 11:29:20,013 DEBUG TRAIN Batch 4/6500 loss 20.942272 loss_att 23.111931 loss_ctc 26.924034 loss_rnnt 19.488876 hw_loss 0.416056 lr 0.00079195 rank 3
2023-02-17 11:29:20,014 DEBUG TRAIN Batch 4/6500 loss 39.168507 loss_att 41.772022 loss_ctc 62.863472 loss_rnnt 35.324837 hw_loss 0.306813 lr 0.00079243 rank 4
2023-02-17 11:29:20,016 DEBUG TRAIN Batch 4/6500 loss 20.023920 loss_att 25.263388 loss_ctc 35.120556 loss_rnnt 16.786037 hw_loss 0.332067 lr 0.00079175 rank 0
2023-02-17 11:29:20,017 DEBUG TRAIN Batch 4/6500 loss 29.952845 loss_att 32.440357 loss_ctc 45.786236 loss_rnnt 27.165482 hw_loss 0.335135 lr 0.00079201 rank 5
2023-02-17 11:29:20,017 DEBUG TRAIN Batch 4/6500 loss 22.547001 loss_att 26.603376 loss_ctc 35.151031 loss_rnnt 19.842943 hw_loss 0.397960 lr 0.00079170 rank 1
2023-02-17 11:29:20,019 DEBUG TRAIN Batch 4/6500 loss 24.162764 loss_att 34.912003 loss_ctc 40.157646 loss_rnnt 19.703346 hw_loss 0.331719 lr 0.00079147 rank 2
2023-02-17 11:30:35,781 DEBUG TRAIN Batch 4/6600 loss 14.783768 loss_att 21.879745 loss_ctc 22.431591 loss_rnnt 12.171638 hw_loss 0.324794 lr 0.00079134 rank 6
2023-02-17 11:30:35,783 DEBUG TRAIN Batch 4/6600 loss 14.570294 loss_att 25.644823 loss_ctc 26.959015 loss_rnnt 10.510674 hw_loss 0.361660 lr 0.00079101 rank 5
2023-02-17 11:30:35,789 DEBUG TRAIN Batch 4/6600 loss 47.592495 loss_att 52.478981 loss_ctc 60.769035 loss_rnnt 44.657055 hw_loss 0.377380 lr 0.00079076 rank 0
2023-02-17 11:30:35,790 DEBUG TRAIN Batch 4/6600 loss 44.178127 loss_att 52.979748 loss_ctc 64.136215 loss_rnnt 39.638832 hw_loss 0.221047 lr 0.00079144 rank 4
2023-02-17 11:30:35,791 DEBUG TRAIN Batch 4/6600 loss 31.837395 loss_att 41.420166 loss_ctc 52.448639 loss_rnnt 27.018360 hw_loss 0.289335 lr 0.00079116 rank 7
2023-02-17 11:30:35,792 DEBUG TRAIN Batch 4/6600 loss 24.685566 loss_att 33.505737 loss_ctc 34.761181 loss_rnnt 21.372808 hw_loss 0.384950 lr 0.00079071 rank 1
2023-02-17 11:30:35,792 DEBUG TRAIN Batch 4/6600 loss 39.547939 loss_att 45.479275 loss_ctc 72.737473 loss_rnnt 33.706799 hw_loss 0.430510 lr 0.00079096 rank 3
2023-02-17 11:30:35,841 DEBUG TRAIN Batch 4/6600 loss 24.153210 loss_att 26.310049 loss_ctc 36.244530 loss_rnnt 21.920349 hw_loss 0.354969 lr 0.00079048 rank 2
2023-02-17 11:31:53,694 DEBUG TRAIN Batch 4/6700 loss 21.269318 loss_att 24.507673 loss_ctc 33.230850 loss_rnnt 18.820927 hw_loss 0.385967 lr 0.00079045 rank 4
2023-02-17 11:31:53,696 DEBUG TRAIN Batch 4/6700 loss 27.187212 loss_att 33.115849 loss_ctc 43.767502 loss_rnnt 23.622585 hw_loss 0.315365 lr 0.00079035 rank 6
2023-02-17 11:31:53,697 DEBUG TRAIN Batch 4/6700 loss 14.844574 loss_att 19.090508 loss_ctc 26.974541 loss_rnnt 12.215335 hw_loss 0.305106 lr 0.00078972 rank 1
2023-02-17 11:31:53,698 DEBUG TRAIN Batch 4/6700 loss 16.830261 loss_att 24.680769 loss_ctc 31.002403 loss_rnnt 13.201612 hw_loss 0.316740 lr 0.00079017 rank 7
2023-02-17 11:31:53,699 DEBUG TRAIN Batch 4/6700 loss 21.563438 loss_att 25.900219 loss_ctc 31.174860 loss_rnnt 19.190781 hw_loss 0.419589 lr 0.00078949 rank 2
2023-02-17 11:31:53,699 DEBUG TRAIN Batch 4/6700 loss 20.798796 loss_att 29.809536 loss_ctc 47.034069 loss_rnnt 15.317934 hw_loss 0.338769 lr 0.00078997 rank 3
2023-02-17 11:31:53,700 DEBUG TRAIN Batch 4/6700 loss 27.393936 loss_att 35.017700 loss_ctc 43.889862 loss_rnnt 23.486660 hw_loss 0.343250 lr 0.00079003 rank 5
2023-02-17 11:31:53,712 DEBUG TRAIN Batch 4/6700 loss 17.571407 loss_att 21.898159 loss_ctc 24.653748 loss_rnnt 15.539841 hw_loss 0.416068 lr 0.00078977 rank 0
2023-02-17 11:33:12,174 DEBUG TRAIN Batch 4/6800 loss 17.399492 loss_att 18.783432 loss_ctc 24.143032 loss_rnnt 16.056461 hw_loss 0.313320 lr 0.00078919 rank 7
2023-02-17 11:33:12,179 DEBUG TRAIN Batch 4/6800 loss 23.482168 loss_att 27.174162 loss_ctc 35.620796 loss_rnnt 20.942356 hw_loss 0.342995 lr 0.00078937 rank 6
2023-02-17 11:33:12,180 DEBUG TRAIN Batch 4/6800 loss 17.836590 loss_att 21.371000 loss_ctc 25.137424 loss_rnnt 16.004000 hw_loss 0.285491 lr 0.00078851 rank 2
2023-02-17 11:33:12,181 DEBUG TRAIN Batch 4/6800 loss 29.043556 loss_att 33.786156 loss_ctc 41.451096 loss_rnnt 26.276165 hw_loss 0.308503 lr 0.00078946 rank 4
2023-02-17 11:33:12,181 DEBUG TRAIN Batch 4/6800 loss 23.408882 loss_att 28.248514 loss_ctc 38.617069 loss_rnnt 20.248257 hw_loss 0.309263 lr 0.00078904 rank 5
2023-02-17 11:33:12,182 DEBUG TRAIN Batch 4/6800 loss 25.061344 loss_att 30.931278 loss_ctc 35.400726 loss_rnnt 22.312452 hw_loss 0.368100 lr 0.00078874 rank 1
2023-02-17 11:33:12,182 DEBUG TRAIN Batch 4/6800 loss 25.108150 loss_att 27.972069 loss_ctc 38.026703 loss_rnnt 22.635246 hw_loss 0.333092 lr 0.00078898 rank 3
2023-02-17 11:33:12,185 DEBUG TRAIN Batch 4/6800 loss 21.165966 loss_att 25.133568 loss_ctc 32.427883 loss_rnnt 18.716660 hw_loss 0.289121 lr 0.00078879 rank 0
2023-02-17 11:34:26,896 DEBUG TRAIN Batch 4/6900 loss 21.165497 loss_att 26.364176 loss_ctc 36.835453 loss_rnnt 17.861799 hw_loss 0.327440 lr 0.00078800 rank 3
2023-02-17 11:34:26,897 DEBUG TRAIN Batch 4/6900 loss 14.961267 loss_att 17.407125 loss_ctc 24.702120 loss_rnnt 13.044329 hw_loss 0.241848 lr 0.00078821 rank 7
2023-02-17 11:34:26,898 DEBUG TRAIN Batch 4/6900 loss 16.635048 loss_att 20.822805 loss_ctc 26.872198 loss_rnnt 14.226796 hw_loss 0.385774 lr 0.00078781 rank 0
2023-02-17 11:34:26,898 DEBUG TRAIN Batch 4/6900 loss 16.609087 loss_att 18.528809 loss_ctc 22.095875 loss_rnnt 15.311388 hw_loss 0.341592 lr 0.00078838 rank 6
2023-02-17 11:34:26,898 DEBUG TRAIN Batch 4/6900 loss 15.820342 loss_att 19.751623 loss_ctc 26.712952 loss_rnnt 13.443271 hw_loss 0.259626 lr 0.00078848 rank 4
2023-02-17 11:34:26,901 DEBUG TRAIN Batch 4/6900 loss 13.006156 loss_att 14.887149 loss_ctc 20.271885 loss_rnnt 11.437008 hw_loss 0.420349 lr 0.00078806 rank 5
2023-02-17 11:34:26,905 DEBUG TRAIN Batch 4/6900 loss 11.997395 loss_att 13.071081 loss_ctc 20.934700 loss_rnnt 10.377782 hw_loss 0.399816 lr 0.00078753 rank 2
2023-02-17 11:34:26,910 DEBUG TRAIN Batch 4/6900 loss 18.176828 loss_att 20.023767 loss_ctc 26.312162 loss_rnnt 16.511000 hw_loss 0.396997 lr 0.00078776 rank 1
2023-02-17 11:35:43,476 DEBUG TRAIN Batch 4/7000 loss 13.869287 loss_att 17.265430 loss_ctc 27.504503 loss_rnnt 11.165254 hw_loss 0.387706 lr 0.00078708 rank 5
2023-02-17 11:35:43,477 DEBUG TRAIN Batch 4/7000 loss 22.674374 loss_att 22.774874 loss_ctc 31.242575 loss_rnnt 21.340023 hw_loss 0.322172 lr 0.00078741 rank 6
2023-02-17 11:35:43,479 DEBUG TRAIN Batch 4/7000 loss 17.111956 loss_att 19.719597 loss_ctc 25.265966 loss_rnnt 15.314664 hw_loss 0.353550 lr 0.00078750 rank 4
2023-02-17 11:35:43,479 DEBUG TRAIN Batch 4/7000 loss 17.070589 loss_att 21.743309 loss_ctc 28.524481 loss_rnnt 14.417697 hw_loss 0.358430 lr 0.00078678 rank 1
2023-02-17 11:35:43,480 DEBUG TRAIN Batch 4/7000 loss 22.193016 loss_att 21.387058 loss_ctc 30.207672 loss_rnnt 21.051666 hw_loss 0.438599 lr 0.00078723 rank 7
2023-02-17 11:35:43,479 DEBUG TRAIN Batch 4/7000 loss 12.881185 loss_att 14.277194 loss_ctc 21.692600 loss_rnnt 11.225713 hw_loss 0.377652 lr 0.00078703 rank 3
2023-02-17 11:35:43,481 DEBUG TRAIN Batch 4/7000 loss 16.985485 loss_att 16.042761 loss_ctc 21.819260 loss_rnnt 16.312141 hw_loss 0.407597 lr 0.00078683 rank 0
2023-02-17 11:35:43,485 DEBUG TRAIN Batch 4/7000 loss 31.661785 loss_att 36.371338 loss_ctc 46.529907 loss_rnnt 28.595108 hw_loss 0.266904 lr 0.00078656 rank 2
2023-02-17 11:37:02,355 DEBUG TRAIN Batch 4/7100 loss 36.509682 loss_att 39.435192 loss_ctc 58.211422 loss_rnnt 32.863644 hw_loss 0.313825 lr 0.00078611 rank 5
2023-02-17 11:37:02,356 DEBUG TRAIN Batch 4/7100 loss 17.559370 loss_att 22.666250 loss_ctc 28.490553 loss_rnnt 14.917468 hw_loss 0.305692 lr 0.00078626 rank 7
2023-02-17 11:37:02,357 DEBUG TRAIN Batch 4/7100 loss 50.378578 loss_att 57.230949 loss_ctc 76.567978 loss_rnnt 45.338585 hw_loss 0.333004 lr 0.00078581 rank 1
2023-02-17 11:37:02,358 DEBUG TRAIN Batch 4/7100 loss 41.452789 loss_att 51.142204 loss_ctc 69.056458 loss_rnnt 35.655895 hw_loss 0.334734 lr 0.00078643 rank 6
2023-02-17 11:37:02,360 DEBUG TRAIN Batch 4/7100 loss 10.185040 loss_att 14.359190 loss_ctc 17.999195 loss_rnnt 8.112226 hw_loss 0.367681 lr 0.00078586 rank 0
2023-02-17 11:37:02,372 DEBUG TRAIN Batch 4/7100 loss 23.036144 loss_att 28.765591 loss_ctc 42.437325 loss_rnnt 19.136845 hw_loss 0.312348 lr 0.00078559 rank 2
2023-02-17 11:37:02,382 DEBUG TRAIN Batch 4/7100 loss 24.445372 loss_att 24.482050 loss_ctc 31.661205 loss_rnnt 23.280197 hw_loss 0.366989 lr 0.00078653 rank 4
2023-02-17 11:37:02,403 DEBUG TRAIN Batch 4/7100 loss 38.288567 loss_att 45.568745 loss_ctc 50.779049 loss_rnnt 34.971867 hw_loss 0.366119 lr 0.00078605 rank 3
2023-02-17 11:38:18,018 DEBUG TRAIN Batch 4/7200 loss 27.876364 loss_att 31.868155 loss_ctc 42.895973 loss_rnnt 24.877117 hw_loss 0.371765 lr 0.00078508 rank 3
2023-02-17 11:38:18,022 DEBUG TRAIN Batch 4/7200 loss 20.798740 loss_att 24.305859 loss_ctc 36.194328 loss_rnnt 17.859966 hw_loss 0.346130 lr 0.00078529 rank 7
2023-02-17 11:38:18,022 DEBUG TRAIN Batch 4/7200 loss 24.467354 loss_att 29.241577 loss_ctc 40.204132 loss_rnnt 21.216633 hw_loss 0.370575 lr 0.00078514 rank 5
2023-02-17 11:38:18,024 DEBUG TRAIN Batch 4/7200 loss 26.660435 loss_att 31.547096 loss_ctc 43.144222 loss_rnnt 23.355825 hw_loss 0.242695 lr 0.00078489 rank 0
2023-02-17 11:38:18,023 DEBUG TRAIN Batch 4/7200 loss 24.049570 loss_att 31.965153 loss_ctc 34.882416 loss_rnnt 20.806107 hw_loss 0.404941 lr 0.00078484 rank 1
2023-02-17 11:38:18,024 DEBUG TRAIN Batch 4/7200 loss 19.437122 loss_att 27.224918 loss_ctc 31.701139 loss_rnnt 16.102386 hw_loss 0.266200 lr 0.00078556 rank 4
2023-02-17 11:38:18,028 DEBUG TRAIN Batch 4/7200 loss 22.220037 loss_att 24.384762 loss_ctc 34.716461 loss_rnnt 19.923477 hw_loss 0.370173 lr 0.00078546 rank 6
2023-02-17 11:38:18,030 DEBUG TRAIN Batch 4/7200 loss 25.569265 loss_att 30.870329 loss_ctc 43.895805 loss_rnnt 21.821299 hw_loss 0.457903 lr 0.00078462 rank 2
2023-02-17 11:39:34,203 DEBUG TRAIN Batch 4/7300 loss 30.841490 loss_att 44.561001 loss_ctc 46.978065 loss_rnnt 25.782864 hw_loss 0.305960 lr 0.00078449 rank 6
2023-02-17 11:39:34,205 DEBUG TRAIN Batch 4/7300 loss 19.355471 loss_att 23.044992 loss_ctc 31.588024 loss_rnnt 16.808182 hw_loss 0.334460 lr 0.00078418 rank 5
2023-02-17 11:39:34,209 DEBUG TRAIN Batch 4/7300 loss 10.787135 loss_att 15.846722 loss_ctc 20.547760 loss_rnnt 8.297464 hw_loss 0.330631 lr 0.00078432 rank 7
2023-02-17 11:39:34,209 DEBUG TRAIN Batch 4/7300 loss 23.238470 loss_att 26.521648 loss_ctc 36.455147 loss_rnnt 20.644728 hw_loss 0.327904 lr 0.00078365 rank 2
2023-02-17 11:39:34,209 DEBUG TRAIN Batch 4/7300 loss 19.283342 loss_att 26.273783 loss_ctc 34.525738 loss_rnnt 15.649547 hw_loss 0.381357 lr 0.00078412 rank 3
2023-02-17 11:39:34,212 DEBUG TRAIN Batch 4/7300 loss 13.085147 loss_att 18.193405 loss_ctc 23.745930 loss_rnnt 10.485085 hw_loss 0.294325 lr 0.00078392 rank 0
2023-02-17 11:39:34,220 DEBUG TRAIN Batch 4/7300 loss 9.907731 loss_att 14.198694 loss_ctc 15.834373 loss_rnnt 8.076859 hw_loss 0.342111 lr 0.00078388 rank 1
2023-02-17 11:39:34,258 DEBUG TRAIN Batch 4/7300 loss 18.354004 loss_att 23.332443 loss_ctc 32.540508 loss_rnnt 15.289938 hw_loss 0.331585 lr 0.00078459 rank 4
2023-02-17 11:40:51,439 DEBUG TRAIN Batch 4/7400 loss 22.707067 loss_att 25.751051 loss_ctc 30.333082 loss_rnnt 20.886236 hw_loss 0.366062 lr 0.00078353 rank 6
2023-02-17 11:40:51,440 DEBUG TRAIN Batch 4/7400 loss 9.942939 loss_att 14.052782 loss_ctc 16.723721 loss_rnnt 8.041317 hw_loss 0.329153 lr 0.00078336 rank 7
2023-02-17 11:40:51,440 DEBUG TRAIN Batch 4/7400 loss 28.070015 loss_att 35.056248 loss_ctc 51.409561 loss_rnnt 23.388674 hw_loss 0.322791 lr 0.00078363 rank 4
2023-02-17 11:40:51,444 DEBUG TRAIN Batch 4/7400 loss 27.741032 loss_att 31.898359 loss_ctc 44.476879 loss_rnnt 24.461918 hw_loss 0.405372 lr 0.00078315 rank 3
2023-02-17 11:40:51,445 DEBUG TRAIN Batch 4/7400 loss 17.743530 loss_att 19.357555 loss_ctc 33.604427 loss_rnnt 15.158090 hw_loss 0.277215 lr 0.00078321 rank 5
2023-02-17 11:40:51,446 DEBUG TRAIN Batch 4/7400 loss 21.446127 loss_att 22.418159 loss_ctc 32.811630 loss_rnnt 19.559914 hw_loss 0.330764 lr 0.00078296 rank 0
2023-02-17 11:40:51,448 DEBUG TRAIN Batch 4/7400 loss 38.108200 loss_att 41.386421 loss_ctc 65.070801 loss_rnnt 33.678261 hw_loss 0.336154 lr 0.00078269 rank 2
2023-02-17 11:40:51,449 DEBUG TRAIN Batch 4/7400 loss 35.964203 loss_att 39.087837 loss_ctc 52.785591 loss_rnnt 32.892990 hw_loss 0.381816 lr 0.00078291 rank 1
2023-02-17 11:42:11,179 DEBUG TRAIN Batch 4/7500 loss 22.959187 loss_att 28.449127 loss_ctc 33.889435 loss_rnnt 20.233183 hw_loss 0.319965 lr 0.00078220 rank 3
2023-02-17 11:42:11,183 DEBUG TRAIN Batch 4/7500 loss 14.163098 loss_att 15.505574 loss_ctc 19.522419 loss_rnnt 12.981867 hw_loss 0.371550 lr 0.00078257 rank 6
2023-02-17 11:42:11,185 DEBUG TRAIN Batch 4/7500 loss 21.435570 loss_att 23.933764 loss_ctc 31.999260 loss_rnnt 19.348259 hw_loss 0.335962 lr 0.00078225 rank 5
2023-02-17 11:42:11,184 DEBUG TRAIN Batch 4/7500 loss 27.936459 loss_att 31.906349 loss_ctc 45.755615 loss_rnnt 24.588215 hw_loss 0.334454 lr 0.00078240 rank 7
2023-02-17 11:42:11,185 DEBUG TRAIN Batch 4/7500 loss 19.373285 loss_att 22.437473 loss_ctc 31.916555 loss_rnnt 16.889423 hw_loss 0.372355 lr 0.00078196 rank 1
2023-02-17 11:42:11,185 DEBUG TRAIN Batch 4/7500 loss 36.481308 loss_att 39.433228 loss_ctc 46.468212 loss_rnnt 34.460957 hw_loss 0.184464 lr 0.00078200 rank 0
2023-02-17 11:42:11,186 DEBUG TRAIN Batch 4/7500 loss 29.042513 loss_att 32.375626 loss_ctc 43.937412 loss_rnnt 26.199629 hw_loss 0.356766 lr 0.00078174 rank 2
2023-02-17 11:42:11,229 DEBUG TRAIN Batch 4/7500 loss 29.915634 loss_att 32.623989 loss_ctc 46.725258 loss_rnnt 26.936844 hw_loss 0.367193 lr 0.00078267 rank 4
2023-02-17 11:43:28,019 DEBUG TRAIN Batch 4/7600 loss 15.838231 loss_att 23.867220 loss_ctc 23.012844 loss_rnnt 13.110199 hw_loss 0.310536 lr 0.00078078 rank 2
2023-02-17 11:43:28,021 DEBUG TRAIN Batch 4/7600 loss 15.926381 loss_att 22.399837 loss_ctc 26.329069 loss_rnnt 13.072296 hw_loss 0.323190 lr 0.00078161 rank 6
2023-02-17 11:43:28,022 DEBUG TRAIN Batch 4/7600 loss 8.798233 loss_att 13.109472 loss_ctc 15.876075 loss_rnnt 6.806584 hw_loss 0.348165 lr 0.00078130 rank 5
2023-02-17 11:43:28,023 DEBUG TRAIN Batch 4/7600 loss 14.105024 loss_att 14.916052 loss_ctc 19.010305 loss_rnnt 13.042857 hw_loss 0.461109 lr 0.00078100 rank 1
2023-02-17 11:43:28,025 DEBUG TRAIN Batch 4/7600 loss 13.684346 loss_att 13.997735 loss_ctc 18.867748 loss_rnnt 12.714047 hw_loss 0.405938 lr 0.00078144 rank 7
2023-02-17 11:43:28,026 DEBUG TRAIN Batch 4/7600 loss 18.871363 loss_att 23.638597 loss_ctc 33.017448 loss_rnnt 15.789772 hw_loss 0.453749 lr 0.00078171 rank 4
2023-02-17 11:43:28,026 DEBUG TRAIN Batch 4/7600 loss 14.216686 loss_att 14.753139 loss_ctc 21.636572 loss_rnnt 12.944156 hw_loss 0.329855 lr 0.00078105 rank 0
2023-02-17 11:43:28,029 DEBUG TRAIN Batch 4/7600 loss 25.945024 loss_att 28.008347 loss_ctc 40.220978 loss_rnnt 23.438431 hw_loss 0.357125 lr 0.00078124 rank 3
2023-02-17 11:44:44,666 DEBUG TRAIN Batch 4/7700 loss 15.319766 loss_att 22.480173 loss_ctc 22.348831 loss_rnnt 12.794168 hw_loss 0.293075 lr 0.00078066 rank 6
2023-02-17 11:44:44,669 DEBUG TRAIN Batch 4/7700 loss 16.991955 loss_att 21.800671 loss_ctc 29.083679 loss_rnnt 14.276134 hw_loss 0.265959 lr 0.00078035 rank 5
2023-02-17 11:44:44,671 DEBUG TRAIN Batch 4/7700 loss 18.484045 loss_att 25.935783 loss_ctc 28.009741 loss_rnnt 15.550541 hw_loss 0.324491 lr 0.00078005 rank 1
2023-02-17 11:44:44,672 DEBUG TRAIN Batch 4/7700 loss 18.939157 loss_att 25.187347 loss_ctc 35.588280 loss_rnnt 15.263305 hw_loss 0.386873 lr 0.00078075 rank 4
2023-02-17 11:44:44,673 DEBUG TRAIN Batch 4/7700 loss 27.777462 loss_att 38.150959 loss_ctc 41.971680 loss_rnnt 23.627983 hw_loss 0.341652 lr 0.00078029 rank 3
2023-02-17 11:44:44,676 DEBUG TRAIN Batch 4/7700 loss 26.040419 loss_att 27.800777 loss_ctc 37.915771 loss_rnnt 23.942650 hw_loss 0.304347 lr 0.00078049 rank 7
2023-02-17 11:44:44,676 DEBUG TRAIN Batch 4/7700 loss 36.219303 loss_att 37.317650 loss_ctc 45.760849 loss_rnnt 34.536724 hw_loss 0.357569 lr 0.00078010 rank 0
2023-02-17 11:44:44,681 DEBUG TRAIN Batch 4/7700 loss 23.145666 loss_att 28.666662 loss_ctc 41.214939 loss_rnnt 19.500641 hw_loss 0.246729 lr 0.00077983 rank 2
2023-02-17 11:46:05,120 DEBUG TRAIN Batch 4/7800 loss 18.594288 loss_att 25.805058 loss_ctc 28.491875 loss_rnnt 15.653918 hw_loss 0.334761 lr 0.00077940 rank 5
2023-02-17 11:46:05,122 DEBUG TRAIN Batch 4/7800 loss 17.020926 loss_att 22.775375 loss_ctc 28.824383 loss_rnnt 14.087255 hw_loss 0.391852 lr 0.00077954 rank 7
2023-02-17 11:46:05,125 DEBUG TRAIN Batch 4/7800 loss 22.214258 loss_att 28.831181 loss_ctc 36.485886 loss_rnnt 18.832949 hw_loss 0.290702 lr 0.00077915 rank 0
2023-02-17 11:46:05,124 DEBUG TRAIN Batch 4/7800 loss 18.192572 loss_att 23.177864 loss_ctc 29.270454 loss_rnnt 15.589171 hw_loss 0.242421 lr 0.00077971 rank 6
2023-02-17 11:46:05,128 DEBUG TRAIN Batch 4/7800 loss 31.402323 loss_att 31.763435 loss_ctc 54.618629 loss_rnnt 28.048168 hw_loss 0.349550 lr 0.00077910 rank 1
2023-02-17 11:46:05,131 DEBUG TRAIN Batch 4/7800 loss 23.263134 loss_att 29.839354 loss_ctc 35.958496 loss_rnnt 20.083151 hw_loss 0.322545 lr 0.00077934 rank 3
2023-02-17 11:46:05,133 DEBUG TRAIN Batch 4/7800 loss 11.460006 loss_att 15.452062 loss_ctc 14.191951 loss_rnnt 10.122348 hw_loss 0.328103 lr 0.00077980 rank 4
2023-02-17 11:46:05,173 DEBUG TRAIN Batch 4/7800 loss 21.781782 loss_att 27.322487 loss_ctc 31.127689 loss_rnnt 19.253788 hw_loss 0.325747 lr 0.00077889 rank 2
2023-02-17 11:47:22,200 DEBUG TRAIN Batch 4/7900 loss 11.644108 loss_att 17.293434 loss_ctc 16.460497 loss_rnnt 9.685168 hw_loss 0.350418 lr 0.00077840 rank 3
2023-02-17 11:47:22,202 DEBUG TRAIN Batch 4/7900 loss 37.927296 loss_att 47.219944 loss_ctc 64.751442 loss_rnnt 32.317482 hw_loss 0.327612 lr 0.00077794 rank 2
2023-02-17 11:47:22,202 DEBUG TRAIN Batch 4/7900 loss 24.595882 loss_att 28.862514 loss_ctc 36.837013 loss_rnnt 21.921494 hw_loss 0.354216 lr 0.00077876 rank 6
2023-02-17 11:47:22,204 DEBUG TRAIN Batch 4/7900 loss 30.056551 loss_att 34.103073 loss_ctc 43.162788 loss_rnnt 27.380764 hw_loss 0.223096 lr 0.00077845 rank 5
2023-02-17 11:47:22,204 DEBUG TRAIN Batch 4/7900 loss 33.769474 loss_att 33.025852 loss_ctc 50.592289 loss_rnnt 31.492683 hw_loss 0.342144 lr 0.00077859 rank 7
2023-02-17 11:47:22,206 DEBUG TRAIN Batch 4/7900 loss 13.363509 loss_att 18.956678 loss_ctc 18.966619 loss_rnnt 11.333152 hw_loss 0.308705 lr 0.00077816 rank 1
2023-02-17 11:47:22,206 DEBUG TRAIN Batch 4/7900 loss 25.434675 loss_att 31.205767 loss_ctc 38.100327 loss_rnnt 22.424786 hw_loss 0.312974 lr 0.00077821 rank 0
2023-02-17 11:47:22,207 DEBUG TRAIN Batch 4/7900 loss 27.494286 loss_att 33.969891 loss_ctc 38.677830 loss_rnnt 24.541107 hw_loss 0.312971 lr 0.00077886 rank 4
2023-02-17 11:48:37,583 DEBUG TRAIN Batch 4/8000 loss 24.830828 loss_att 34.232674 loss_ctc 35.010811 loss_rnnt 21.393095 hw_loss 0.375060 lr 0.00077782 rank 6
2023-02-17 11:48:37,584 DEBUG TRAIN Batch 4/8000 loss 20.984583 loss_att 25.076965 loss_ctc 33.574444 loss_rnnt 18.319241 hw_loss 0.315407 lr 0.00077791 rank 4
2023-02-17 11:48:37,583 DEBUG TRAIN Batch 4/8000 loss 12.281611 loss_att 17.606091 loss_ctc 21.099535 loss_rnnt 9.842302 hw_loss 0.372542 lr 0.00077751 rank 5
2023-02-17 11:48:37,584 DEBUG TRAIN Batch 4/8000 loss 56.145802 loss_att 62.695824 loss_ctc 80.651703 loss_rnnt 51.392361 hw_loss 0.329972 lr 0.00077765 rank 7
2023-02-17 11:48:37,586 DEBUG TRAIN Batch 4/8000 loss 32.560272 loss_att 34.792278 loss_ctc 51.075344 loss_rnnt 29.455284 hw_loss 0.356082 lr 0.00077722 rank 1
2023-02-17 11:48:37,587 DEBUG TRAIN Batch 4/8000 loss 16.884953 loss_att 23.669682 loss_ctc 28.819138 loss_rnnt 13.775377 hw_loss 0.302629 lr 0.00077745 rank 3
2023-02-17 11:48:37,588 DEBUG TRAIN Batch 4/8000 loss 29.859774 loss_att 32.755936 loss_ctc 46.300938 loss_rnnt 26.909607 hw_loss 0.335208 lr 0.00077727 rank 0
2023-02-17 11:48:37,589 DEBUG TRAIN Batch 4/8000 loss 24.777409 loss_att 32.820629 loss_ctc 46.036331 loss_rnnt 20.204485 hw_loss 0.243293 lr 0.00077700 rank 2
2023-02-17 11:49:53,553 DEBUG TRAIN Batch 4/8100 loss 14.164090 loss_att 16.851692 loss_ctc 23.388302 loss_rnnt 12.205343 hw_loss 0.358749 lr 0.00077652 rank 3
2023-02-17 11:49:53,555 DEBUG TRAIN Batch 4/8100 loss 18.308792 loss_att 22.687376 loss_ctc 29.335426 loss_rnnt 15.766426 hw_loss 0.368308 lr 0.00077697 rank 4
2023-02-17 11:49:53,557 DEBUG TRAIN Batch 4/8100 loss 21.612532 loss_att 25.958931 loss_ctc 34.775558 loss_rnnt 18.780159 hw_loss 0.390036 lr 0.00077628 rank 1
2023-02-17 11:49:53,559 DEBUG TRAIN Batch 4/8100 loss 21.755663 loss_att 28.089884 loss_ctc 37.125023 loss_rnnt 18.237074 hw_loss 0.379682 lr 0.00077657 rank 5
2023-02-17 11:49:53,560 DEBUG TRAIN Batch 4/8100 loss 24.436672 loss_att 26.139187 loss_ctc 37.368328 loss_rnnt 22.137526 hw_loss 0.439546 lr 0.00077607 rank 2
2023-02-17 11:49:53,565 DEBUG TRAIN Batch 4/8100 loss 18.696655 loss_att 22.492994 loss_ctc 35.348892 loss_rnnt 15.558066 hw_loss 0.298166 lr 0.00077633 rank 0
2023-02-17 11:49:53,566 DEBUG TRAIN Batch 4/8100 loss 26.549812 loss_att 27.122154 loss_ctc 37.638580 loss_rnnt 24.766823 hw_loss 0.356282 lr 0.00077671 rank 7
2023-02-17 11:49:53,566 DEBUG TRAIN Batch 4/8100 loss 19.487392 loss_att 25.946308 loss_ctc 26.518856 loss_rnnt 17.058126 hw_loss 0.374914 lr 0.00077688 rank 6
2023-02-17 11:51:11,017 DEBUG TRAIN Batch 4/8200 loss 18.177692 loss_att 20.519890 loss_ctc 26.767025 loss_rnnt 16.341240 hw_loss 0.417690 lr 0.00077513 rank 2
2023-02-17 11:51:11,020 DEBUG TRAIN Batch 4/8200 loss 13.562829 loss_att 17.816475 loss_ctc 20.238131 loss_rnnt 11.654477 hw_loss 0.314218 lr 0.00077564 rank 5
2023-02-17 11:51:11,022 DEBUG TRAIN Batch 4/8200 loss 16.360844 loss_att 20.262016 loss_ctc 27.467651 loss_rnnt 13.918709 hw_loss 0.339360 lr 0.00077535 rank 1
2023-02-17 11:51:11,022 DEBUG TRAIN Batch 4/8200 loss 14.199536 loss_att 17.770643 loss_ctc 24.658325 loss_rnnt 11.824913 hw_loss 0.498556 lr 0.00077594 rank 6
2023-02-17 11:51:11,023 DEBUG TRAIN Batch 4/8200 loss 27.665997 loss_att 28.477757 loss_ctc 46.868793 loss_rnnt 24.757666 hw_loss 0.348012 lr 0.00077558 rank 3
2023-02-17 11:51:11,023 DEBUG TRAIN Batch 4/8200 loss 19.696402 loss_att 20.393797 loss_ctc 30.628668 loss_rnnt 17.907816 hw_loss 0.359007 lr 0.00077539 rank 0
2023-02-17 11:51:11,026 DEBUG TRAIN Batch 4/8200 loss 16.719818 loss_att 18.515514 loss_ctc 24.101412 loss_rnnt 15.125692 hw_loss 0.470202 lr 0.00077578 rank 7
2023-02-17 11:51:11,076 DEBUG TRAIN Batch 4/8200 loss 14.415356 loss_att 15.717145 loss_ctc 22.637802 loss_rnnt 12.858868 hw_loss 0.374631 lr 0.00077604 rank 4
2023-02-17 11:52:26,669 DEBUG TRAIN Batch 4/8300 loss 15.415651 loss_att 18.234777 loss_ctc 30.722748 loss_rnnt 12.590607 hw_loss 0.413012 lr 0.00077501 rank 6
2023-02-17 11:52:26,679 DEBUG TRAIN Batch 4/8300 loss 17.595314 loss_att 26.385677 loss_ctc 37.295532 loss_rnnt 13.002254 hw_loss 0.390544 lr 0.00077420 rank 2
2023-02-17 11:52:26,680 DEBUG TRAIN Batch 4/8300 loss 16.662991 loss_att 15.805427 loss_ctc 20.802448 loss_rnnt 16.051506 hw_loss 0.433254 lr 0.00077465 rank 3
2023-02-17 11:52:26,681 DEBUG TRAIN Batch 4/8300 loss 18.312775 loss_att 18.995975 loss_ctc 25.301382 loss_rnnt 17.032982 hw_loss 0.396262 lr 0.00077471 rank 5
2023-02-17 11:52:26,682 DEBUG TRAIN Batch 4/8300 loss 12.569700 loss_att 20.792147 loss_ctc 21.827599 loss_rnnt 9.475830 hw_loss 0.403114 lr 0.00077484 rank 7
2023-02-17 11:52:26,683 DEBUG TRAIN Batch 4/8300 loss 29.850401 loss_att 41.089035 loss_ctc 44.739170 loss_rnnt 25.521149 hw_loss 0.180668 lr 0.00077442 rank 1
2023-02-17 11:52:26,684 DEBUG TRAIN Batch 4/8300 loss 16.098827 loss_att 20.182247 loss_ctc 28.972784 loss_rnnt 13.423019 hw_loss 0.267366 lr 0.00077511 rank 4
2023-02-17 11:52:26,685 DEBUG TRAIN Batch 4/8300 loss 32.767124 loss_att 37.450523 loss_ctc 51.114967 loss_rnnt 29.185783 hw_loss 0.371770 lr 0.00077446 rank 0
2023-02-17 11:53:13,173 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 1
2023-02-17 11:53:13,175 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 7
2023-02-17 11:53:13,176 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 2
2023-02-17 11:53:13,176 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 0
2023-02-17 11:53:13,176 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 3
2023-02-17 11:53:13,180 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 5
2023-02-17 11:53:13,193 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 4
2023-02-17 11:53:13,195 DEBUG CV Batch 4/0 loss 3.230617 loss_att 2.862420 loss_ctc 4.840662 loss_rnnt 2.867741 hw_loss 0.415954 history loss 3.110964 rank 6
2023-02-17 11:53:24,239 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 6
2023-02-17 11:53:24,326 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 7
2023-02-17 11:53:24,328 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 1
2023-02-17 11:53:24,363 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 5
2023-02-17 11:53:24,485 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 0
2023-02-17 11:53:24,495 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 3
2023-02-17 11:53:24,576 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 2
2023-02-17 11:53:25,805 DEBUG CV Batch 4/100 loss 15.537473 loss_att 14.574710 loss_ctc 24.368763 loss_rnnt 14.316035 hw_loss 0.443408 history loss 6.285032 rank 4
2023-02-17 11:53:38,192 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 1
2023-02-17 11:53:38,415 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 2
2023-02-17 11:53:38,468 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 6
2023-02-17 11:53:38,503 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 7
2023-02-17 11:53:38,564 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 0
2023-02-17 11:53:38,582 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 5
2023-02-17 11:53:39,193 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 3
2023-02-17 11:53:39,623 DEBUG CV Batch 4/200 loss 19.395765 loss_att 27.357779 loss_ctc 31.225304 loss_rnnt 16.031189 hw_loss 0.365441 history loss 7.224206 rank 4
2023-02-17 11:53:50,144 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 1
2023-02-17 11:53:50,497 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 7
2023-02-17 11:53:50,499 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 6
2023-02-17 11:53:50,590 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 2
2023-02-17 11:53:50,666 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 5
2023-02-17 11:53:50,699 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 0
2023-02-17 11:53:51,457 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 3
2023-02-17 11:53:51,791 DEBUG CV Batch 4/300 loss 7.977606 loss_att 9.333681 loss_ctc 16.177059 loss_rnnt 6.372018 hw_loss 0.452086 history loss 7.343334 rank 4
2023-02-17 11:54:02,114 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 1
2023-02-17 11:54:02,396 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 7
2023-02-17 11:54:02,488 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 6
2023-02-17 11:54:02,708 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 5
2023-02-17 11:54:02,783 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 0
2023-02-17 11:54:02,833 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 2
2023-02-17 11:54:03,561 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 3
2023-02-17 11:54:04,590 DEBUG CV Batch 4/400 loss 32.334545 loss_att 113.063156 loss_ctc 29.822088 loss_rnnt 16.389606 hw_loss 0.251642 history loss 8.544287 rank 4
2023-02-17 11:54:12,472 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 1
2023-02-17 11:54:12,884 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 7
2023-02-17 11:54:12,976 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381989 history loss 9.439410 rank 6
2023-02-17 11:54:13,324 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381989 history loss 9.439410 rank 5
2023-02-17 11:54:13,417 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 2
2023-02-17 11:54:13,425 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 0
2023-02-17 11:54:14,208 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 3
2023-02-17 11:54:15,054 DEBUG CV Batch 4/500 loss 9.600828 loss_att 10.865439 loss_ctc 16.477268 loss_rnnt 8.227320 hw_loss 0.381990 history loss 9.439410 rank 4
2023-02-17 11:54:24,487 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 1
2023-02-17 11:54:24,905 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 7
2023-02-17 11:54:25,094 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 6
2023-02-17 11:54:25,490 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 5
2023-02-17 11:54:25,508 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 2
2023-02-17 11:54:26,213 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 0
2023-02-17 11:54:26,456 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 3
2023-02-17 11:54:27,061 DEBUG CV Batch 4/600 loss 10.021377 loss_att 9.758985 loss_ctc 14.221099 loss_rnnt 9.269341 hw_loss 0.458533 history loss 10.425165 rank 4
2023-02-17 11:54:36,273 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 1
2023-02-17 11:54:36,909 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 6
2023-02-17 11:54:37,158 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 2
2023-02-17 11:54:37,160 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 7
2023-02-17 11:54:37,555 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 5
2023-02-17 11:54:38,035 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 0
2023-02-17 11:54:38,403 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 4
2023-02-17 11:54:38,572 DEBUG CV Batch 4/700 loss 35.729282 loss_att 88.021500 loss_ctc 47.829948 loss_rnnt 23.513969 hw_loss 0.268968 history loss 11.210392 rank 3
2023-02-17 11:54:48,264 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 1
2023-02-17 11:54:48,959 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 6
2023-02-17 11:54:49,235 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 7
2023-02-17 11:54:49,485 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 2
2023-02-17 11:54:49,577 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 5
2023-02-17 11:54:49,643 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 4
2023-02-17 11:54:50,288 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 0
2023-02-17 11:54:51,748 DEBUG CV Batch 4/800 loss 18.109875 loss_att 15.996126 loss_ctc 27.470974 loss_rnnt 17.069935 hw_loss 0.402264 history loss 10.541976 rank 3
2023-02-17 11:55:02,004 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 1
2023-02-17 11:55:02,866 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 6
2023-02-17 11:55:03,059 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 7
2023-02-17 11:55:03,491 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 4
2023-02-17 11:55:03,622 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 5
2023-02-17 11:55:04,195 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 0
2023-02-17 11:55:04,505 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 2
2023-02-17 11:55:05,803 DEBUG CV Batch 4/900 loss 23.180006 loss_att 27.792755 loss_ctc 38.796131 loss_rnnt 20.009886 hw_loss 0.310165 history loss 10.345241 rank 3
2023-02-17 11:55:14,208 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 1
2023-02-17 11:55:15,015 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 6
2023-02-17 11:55:15,179 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 7
2023-02-17 11:55:15,682 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 4
2023-02-17 11:55:15,854 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 5
2023-02-17 11:55:16,595 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 0
2023-02-17 11:55:18,134 DEBUG CV Batch 4/1000 loss 6.207638 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 2
2023-02-17 11:55:18,175 DEBUG CV Batch 4/1000 loss 6.207639 loss_att 7.591465 loss_ctc 11.471802 loss_rnnt 5.026941 hw_loss 0.378831 history loss 10.069812 rank 3
2023-02-17 11:55:26,092 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 1
2023-02-17 11:55:26,823 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 6
2023-02-17 11:55:27,025 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 7
2023-02-17 11:55:27,562 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 4
2023-02-17 11:55:27,739 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 5
2023-02-17 11:55:28,720 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 0
2023-02-17 11:55:30,105 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 3
2023-02-17 11:55:30,548 DEBUG CV Batch 4/1100 loss 7.083054 loss_att 7.246809 loss_ctc 11.894087 loss_rnnt 6.163589 hw_loss 0.459829 history loss 10.069449 rank 2
2023-02-17 11:55:36,482 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 1
2023-02-17 11:55:37,223 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 6
2023-02-17 11:55:37,424 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 7
2023-02-17 11:55:38,126 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 4
2023-02-17 11:55:38,284 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 5
2023-02-17 11:55:39,318 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 0
2023-02-17 11:55:40,695 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 3
2023-02-17 11:55:41,634 DEBUG CV Batch 4/1200 loss 10.442282 loss_att 14.137718 loss_ctc 17.142822 loss_rnnt 8.591835 hw_loss 0.408664 history loss 10.458192 rank 2
2023-02-17 11:55:48,322 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 1
2023-02-17 11:55:49,158 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 6
2023-02-17 11:55:49,293 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 7
2023-02-17 11:55:50,138 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 4
2023-02-17 11:55:50,303 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 5
2023-02-17 11:55:51,332 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 0
2023-02-17 11:55:53,606 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 2
2023-02-17 11:55:54,147 DEBUG CV Batch 4/1300 loss 7.900512 loss_att 7.903069 loss_ctc 12.412828 loss_rnnt 7.089836 hw_loss 0.390978 history loss 10.766896 rank 3
2023-02-17 11:56:00,020 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 1
2023-02-17 11:56:00,845 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 6
2023-02-17 11:56:01,179 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 7
2023-02-17 11:56:01,765 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 5
2023-02-17 11:56:01,990 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 4
2023-02-17 11:56:02,712 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 0
2023-02-17 11:56:04,877 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 2
2023-02-17 11:56:05,503 DEBUG CV Batch 4/1400 loss 16.757238 loss_att 55.298817 loss_ctc 26.598564 loss_rnnt 7.611155 hw_loss 0.235483 history loss 11.156829 rank 3
2023-02-17 11:56:12,449 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 1
2023-02-17 11:56:13,226 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 6
2023-02-17 11:56:13,554 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 7
2023-02-17 11:56:14,154 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 5
2023-02-17 11:56:14,462 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 0
2023-02-17 11:56:14,924 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 4
2023-02-17 11:56:16,318 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 2
2023-02-17 11:56:17,403 DEBUG CV Batch 4/1500 loss 11.435884 loss_att 12.723598 loss_ctc 14.223155 loss_rnnt 10.581791 hw_loss 0.421713 history loss 10.917228 rank 3
2023-02-17 11:56:26,231 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 1
2023-02-17 11:56:26,917 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 6
2023-02-17 11:56:27,210 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 7
2023-02-17 11:56:27,768 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 5
2023-02-17 11:56:28,253 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 0
2023-02-17 11:56:28,499 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 4
2023-02-17 11:56:29,550 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 2
2023-02-17 11:56:31,063 DEBUG CV Batch 4/1600 loss 13.791323 loss_att 24.367563 loss_ctc 24.591503 loss_rnnt 10.070892 hw_loss 0.309673 history loss 10.834725 rank 3
2023-02-17 11:56:38,848 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 1
2023-02-17 11:56:39,411 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 6
2023-02-17 11:56:39,627 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 7
2023-02-17 11:56:40,270 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 5
2023-02-17 11:56:41,044 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 4
2023-02-17 11:56:42,250 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 0
2023-02-17 11:56:42,314 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 2
2023-02-17 11:56:43,522 DEBUG CV Batch 4/1700 loss 13.347791 loss_att 12.995079 loss_ctc 24.325783 loss_rnnt 11.737674 hw_loss 0.406739 history loss 10.708764 rank 3
2023-02-17 11:56:48,058 INFO Epoch 4 CV info cv_loss 10.672331750840875
2023-02-17 11:56:48,059 INFO Epoch 5 TRAIN info lr 0.0007740642960791932
2023-02-17 11:56:48,062 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:48,559 INFO Epoch 4 CV info cv_loss 10.672331753011763
2023-02-17 11:56:48,560 INFO Epoch 5 TRAIN info lr 0.0007747237343541112
2023-02-17 11:56:48,565 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:48,713 INFO Epoch 4 CV info cv_loss 10.67233175187463
2023-02-17 11:56:48,714 INFO Epoch 5 TRAIN info lr 0.000774379873224175
2023-02-17 11:56:48,718 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:49,433 INFO Epoch 4 CV info cv_loss 10.672331748980113
2023-02-17 11:56:49,434 INFO Epoch 5 TRAIN info lr 0.000774361299177347
2023-02-17 11:56:49,437 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:50,252 INFO Epoch 4 CV info cv_loss 10.672331750427372
2023-02-17 11:56:50,253 INFO Epoch 5 TRAIN info lr 0.0007746400502990038
2023-02-17 11:56:50,257 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:51,409 INFO Epoch 4 CV info cv_loss 10.672331749669285
2023-02-17 11:56:51,409 INFO Epoch 4 CV info cv_loss 10.672331753115138
2023-02-17 11:56:51,410 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/4.pt
2023-02-17 11:56:51,410 INFO Epoch 5 TRAIN info lr 0.0007737027851885005
2023-02-17 11:56:51,415 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:52,081 INFO Epoch 5 TRAIN info lr 0.0007741014027782907
2023-02-17 11:56:52,086 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:56:52,729 INFO Epoch 4 CV info cv_loss 10.672331753235742
2023-02-17 11:56:52,730 INFO Epoch 5 TRAIN info lr 0.0007743055850550455
2023-02-17 11:56:52,733 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 11:58:05,102 DEBUG TRAIN Batch 5/0 loss 11.620020 loss_att 11.537204 loss_ctc 13.680692 loss_rnnt 11.122101 hw_loss 0.449488 lr 0.00077471 rank 6
2023-02-17 11:58:05,103 DEBUG TRAIN Batch 5/0 loss 12.675024 loss_att 12.026602 loss_ctc 17.980207 loss_rnnt 11.892877 hw_loss 0.383389 lr 0.00077435 rank 5
2023-02-17 11:58:05,115 DEBUG TRAIN Batch 5/0 loss 10.338731 loss_att 10.677041 loss_ctc 14.328534 loss_rnnt 9.550485 hw_loss 0.353643 lr 0.00077406 rank 1
2023-02-17 11:58:05,123 DEBUG TRAIN Batch 5/0 loss 8.989983 loss_att 9.236685 loss_ctc 12.154050 loss_rnnt 8.297492 hw_loss 0.414892 lr 0.00077437 rank 7
2023-02-17 11:58:05,138 DEBUG TRAIN Batch 5/0 loss 17.348421 loss_att 17.292854 loss_ctc 21.321486 loss_rnnt 16.581284 hw_loss 0.465952 lr 0.00077409 rank 0
2023-02-17 11:58:05,172 DEBUG TRAIN Batch 5/0 loss 9.748359 loss_att 10.283710 loss_ctc 13.847531 loss_rnnt 8.899898 hw_loss 0.365314 lr 0.00077430 rank 3
2023-02-17 11:58:05,196 DEBUG TRAIN Batch 5/0 loss 13.479904 loss_att 12.388901 loss_ctc 17.220984 loss_rnnt 12.937373 hw_loss 0.491101 lr 0.00077463 rank 4
2023-02-17 11:58:05,197 DEBUG TRAIN Batch 5/0 loss 10.686774 loss_att 10.589484 loss_ctc 15.717363 loss_rnnt 9.823011 hw_loss 0.398389 lr 0.00077369 rank 2
2023-02-17 11:59:20,978 DEBUG TRAIN Batch 5/100 loss 28.010649 loss_att 37.907299 loss_ctc 49.115303 loss_rnnt 23.047453 hw_loss 0.318578 lr 0.00077343 rank 5
2023-02-17 11:59:20,978 DEBUG TRAIN Batch 5/100 loss 28.699337 loss_att 39.820450 loss_ctc 46.450989 loss_rnnt 23.929035 hw_loss 0.335982 lr 0.00077337 rank 3
2023-02-17 11:59:20,980 DEBUG TRAIN Batch 5/100 loss 14.230921 loss_att 18.800705 loss_ctc 29.798536 loss_rnnt 11.090412 hw_loss 0.282881 lr 0.00077379 rank 6
2023-02-17 11:59:20,981 DEBUG TRAIN Batch 5/100 loss 19.151100 loss_att 20.760006 loss_ctc 37.636467 loss_rnnt 16.185715 hw_loss 0.335416 lr 0.00077344 rank 7
2023-02-17 11:59:20,982 DEBUG TRAIN Batch 5/100 loss 12.049819 loss_att 17.422049 loss_ctc 22.735920 loss_rnnt 9.341503 hw_loss 0.391982 lr 0.00077313 rank 1
2023-02-17 11:59:20,985 DEBUG TRAIN Batch 5/100 loss 13.129303 loss_att 18.273855 loss_ctc 21.348595 loss_rnnt 10.784422 hw_loss 0.412624 lr 0.00077277 rank 2
2023-02-17 11:59:20,984 DEBUG TRAIN Batch 5/100 loss 41.189796 loss_att 48.344646 loss_ctc 64.388153 loss_rnnt 36.516869 hw_loss 0.279086 lr 0.00077370 rank 4
2023-02-17 11:59:21,030 DEBUG TRAIN Batch 5/100 loss 25.836723 loss_att 33.754471 loss_ctc 34.524471 loss_rnnt 22.966858 hw_loss 0.239903 lr 0.00077317 rank 0
2023-02-17 12:00:36,683 DEBUG TRAIN Batch 5/200 loss 17.368210 loss_att 21.865685 loss_ctc 32.765144 loss_rnnt 14.271061 hw_loss 0.271366 lr 0.00077221 rank 1
2023-02-17 12:00:36,684 DEBUG TRAIN Batch 5/200 loss 11.872640 loss_att 16.209015 loss_ctc 20.568787 loss_rnnt 9.630291 hw_loss 0.404224 lr 0.00077245 rank 3
2023-02-17 12:00:36,687 DEBUG TRAIN Batch 5/200 loss 17.307610 loss_att 21.053518 loss_ctc 25.322918 loss_rnnt 15.312965 hw_loss 0.331414 lr 0.00077252 rank 7
2023-02-17 12:00:36,688 DEBUG TRAIN Batch 5/200 loss 25.049435 loss_att 32.975060 loss_ctc 35.720806 loss_rnnt 21.842703 hw_loss 0.372673 lr 0.00077250 rank 5
2023-02-17 12:00:36,689 DEBUG TRAIN Batch 5/200 loss 37.139671 loss_att 48.312447 loss_ctc 60.946320 loss_rnnt 31.591412 hw_loss 0.261540 lr 0.00077286 rank 6
2023-02-17 12:00:36,690 DEBUG TRAIN Batch 5/200 loss 14.292161 loss_att 20.965803 loss_ctc 27.091690 loss_rnnt 11.029799 hw_loss 0.414432 lr 0.00077185 rank 2
2023-02-17 12:00:36,719 DEBUG TRAIN Batch 5/200 loss 42.637180 loss_att 45.358559 loss_ctc 78.914391 loss_rnnt 37.065498 hw_loss 0.357086 lr 0.00077278 rank 4
2023-02-17 12:00:36,732 DEBUG TRAIN Batch 5/200 loss 17.757113 loss_att 30.176273 loss_ctc 30.438065 loss_rnnt 13.398341 hw_loss 0.345275 lr 0.00077224 rank 0
2023-02-17 12:01:53,849 DEBUG TRAIN Batch 5/300 loss 13.962258 loss_att 17.193493 loss_ctc 19.438625 loss_rnnt 12.393667 hw_loss 0.360303 lr 0.00077194 rank 6
2023-02-17 12:01:53,850 DEBUG TRAIN Batch 5/300 loss 14.489871 loss_att 17.942974 loss_ctc 24.365936 loss_rnnt 12.311165 hw_loss 0.321142 lr 0.00077158 rank 5
2023-02-17 12:01:53,854 DEBUG TRAIN Batch 5/300 loss 14.986730 loss_att 19.794518 loss_ctc 27.631161 loss_rnnt 12.124358 hw_loss 0.402921 lr 0.00077093 rank 2
2023-02-17 12:01:53,855 DEBUG TRAIN Batch 5/300 loss 19.459574 loss_att 25.534069 loss_ctc 33.185806 loss_rnnt 16.228615 hw_loss 0.348553 lr 0.00077160 rank 7
2023-02-17 12:01:53,856 DEBUG TRAIN Batch 5/300 loss 9.312497 loss_att 16.427650 loss_ctc 19.186287 loss_rnnt 6.437314 hw_loss 0.254338 lr 0.00077129 rank 1
2023-02-17 12:01:53,860 DEBUG TRAIN Batch 5/300 loss 30.532545 loss_att 39.764854 loss_ctc 51.440544 loss_rnnt 25.723091 hw_loss 0.328612 lr 0.00077132 rank 0
2023-02-17 12:01:53,860 DEBUG TRAIN Batch 5/300 loss 19.943203 loss_att 34.604645 loss_ctc 48.651665 loss_rnnt 13.019669 hw_loss 0.306474 lr 0.00077153 rank 3
2023-02-17 12:01:53,866 DEBUG TRAIN Batch 5/300 loss 17.094116 loss_att 23.451340 loss_ctc 28.174026 loss_rnnt 14.145239 hw_loss 0.375210 lr 0.00077186 rank 4
2023-02-17 12:03:10,152 DEBUG TRAIN Batch 5/400 loss 26.946964 loss_att 34.189072 loss_ctc 46.345501 loss_rnnt 22.809826 hw_loss 0.191712 lr 0.00077102 rank 6
2023-02-17 12:03:10,157 DEBUG TRAIN Batch 5/400 loss 21.519688 loss_att 26.805885 loss_ctc 39.287560 loss_rnnt 17.926178 hw_loss 0.313536 lr 0.00077061 rank 3
2023-02-17 12:03:10,161 DEBUG TRAIN Batch 5/400 loss 25.737379 loss_att 31.903496 loss_ctc 41.007545 loss_rnnt 22.289461 hw_loss 0.335004 lr 0.00077068 rank 7
2023-02-17 12:03:10,162 DEBUG TRAIN Batch 5/400 loss 17.869799 loss_att 22.314537 loss_ctc 36.975418 loss_rnnt 14.272632 hw_loss 0.301505 lr 0.00077037 rank 1
2023-02-17 12:03:10,164 DEBUG TRAIN Batch 5/400 loss 18.745020 loss_att 26.450851 loss_ctc 30.024164 loss_rnnt 15.550552 hw_loss 0.280151 lr 0.00077001 rank 2
2023-02-17 12:03:10,165 DEBUG TRAIN Batch 5/400 loss 28.451977 loss_att 34.208317 loss_ctc 43.649448 loss_rnnt 25.102110 hw_loss 0.323011 lr 0.00077066 rank 5
2023-02-17 12:03:10,168 DEBUG TRAIN Batch 5/400 loss 27.021139 loss_att 32.839615 loss_ctc 44.846489 loss_rnnt 23.309032 hw_loss 0.321937 lr 0.00077041 rank 0
2023-02-17 12:03:10,171 DEBUG TRAIN Batch 5/400 loss 21.284616 loss_att 24.375973 loss_ctc 37.234299 loss_rnnt 18.357964 hw_loss 0.340790 lr 0.00077094 rank 4
2023-02-17 12:04:27,377 DEBUG TRAIN Batch 5/500 loss 14.607843 loss_att 25.257553 loss_ctc 22.914211 loss_rnnt 11.182954 hw_loss 0.351433 lr 0.00076970 rank 3
2023-02-17 12:04:27,380 DEBUG TRAIN Batch 5/500 loss 12.875826 loss_att 14.753149 loss_ctc 22.486357 loss_rnnt 11.063649 hw_loss 0.291201 lr 0.00076975 rank 5
2023-02-17 12:04:27,381 DEBUG TRAIN Batch 5/500 loss 21.143169 loss_att 24.735592 loss_ctc 35.391460 loss_rnnt 18.353270 hw_loss 0.321830 lr 0.00076977 rank 7
2023-02-17 12:04:27,382 DEBUG TRAIN Batch 5/500 loss 15.001659 loss_att 19.683086 loss_ctc 29.912687 loss_rnnt 11.875674 hw_loss 0.377929 lr 0.00076946 rank 1
2023-02-17 12:04:27,382 DEBUG TRAIN Batch 5/500 loss 28.035080 loss_att 31.272717 loss_ctc 44.080677 loss_rnnt 25.022396 hw_loss 0.423271 lr 0.00077011 rank 6
2023-02-17 12:04:27,385 DEBUG TRAIN Batch 5/500 loss 25.808386 loss_att 29.079920 loss_ctc 39.685280 loss_rnnt 23.081820 hw_loss 0.416261 lr 0.00077002 rank 4
2023-02-17 12:04:27,390 DEBUG TRAIN Batch 5/500 loss 27.880281 loss_att 32.704678 loss_ctc 50.762272 loss_rnnt 23.685200 hw_loss 0.336129 lr 0.00076910 rank 2
2023-02-17 12:04:27,435 DEBUG TRAIN Batch 5/500 loss 13.690516 loss_att 19.216415 loss_ctc 19.669958 loss_rnnt 11.597052 hw_loss 0.358173 lr 0.00076949 rank 0
2023-02-17 12:05:44,077 DEBUG TRAIN Batch 5/600 loss 18.931528 loss_att 23.389250 loss_ctc 31.817945 loss_rnnt 16.080818 hw_loss 0.451827 lr 0.00076855 rank 1
2023-02-17 12:05:44,078 DEBUG TRAIN Batch 5/600 loss 20.212463 loss_att 23.493910 loss_ctc 34.760036 loss_rnnt 17.451294 hw_loss 0.309757 lr 0.00076859 rank 0
2023-02-17 12:05:44,081 DEBUG TRAIN Batch 5/600 loss 24.514368 loss_att 28.087765 loss_ctc 38.049194 loss_rnnt 21.809128 hw_loss 0.348593 lr 0.00076919 rank 6
2023-02-17 12:05:44,082 DEBUG TRAIN Batch 5/600 loss 13.368223 loss_att 14.848742 loss_ctc 19.973551 loss_rnnt 11.946899 hw_loss 0.458455 lr 0.00076886 rank 7
2023-02-17 12:05:44,082 DEBUG TRAIN Batch 5/600 loss 14.728368 loss_att 15.653114 loss_ctc 21.756672 loss_rnnt 13.377951 hw_loss 0.428177 lr 0.00076884 rank 5
2023-02-17 12:05:44,083 DEBUG TRAIN Batch 5/600 loss 18.902519 loss_att 17.414497 loss_ctc 25.055029 loss_rnnt 18.187321 hw_loss 0.360876 lr 0.00076879 rank 3
2023-02-17 12:05:44,085 DEBUG TRAIN Batch 5/600 loss 14.931972 loss_att 16.737425 loss_ctc 23.502193 loss_rnnt 13.157379 hw_loss 0.507758 lr 0.00076820 rank 2
2023-02-17 12:05:44,130 DEBUG TRAIN Batch 5/600 loss 15.529082 loss_att 21.457506 loss_ctc 26.579329 loss_rnnt 12.659883 hw_loss 0.394028 lr 0.00076911 rank 4
2023-02-17 12:07:03,412 DEBUG TRAIN Batch 5/700 loss 18.213129 loss_att 24.089790 loss_ctc 24.793259 loss_rnnt 15.969778 hw_loss 0.357503 lr 0.00076793 rank 5
2023-02-17 12:07:03,412 DEBUG TRAIN Batch 5/700 loss 36.231075 loss_att 38.281425 loss_ctc 61.450409 loss_rnnt 32.284317 hw_loss 0.326456 lr 0.00076764 rank 1
2023-02-17 12:07:03,414 DEBUG TRAIN Batch 5/700 loss 38.267891 loss_att 52.932808 loss_ctc 64.063232 loss_rnnt 31.723684 hw_loss 0.322209 lr 0.00076795 rank 7
2023-02-17 12:07:03,414 DEBUG TRAIN Batch 5/700 loss 27.217510 loss_att 31.054392 loss_ctc 44.061199 loss_rnnt 23.975418 hw_loss 0.429169 lr 0.00076829 rank 6
2023-02-17 12:07:03,424 DEBUG TRAIN Batch 5/700 loss 31.423384 loss_att 35.566513 loss_ctc 51.748749 loss_rnnt 27.697014 hw_loss 0.351930 lr 0.00076820 rank 4
2023-02-17 12:07:03,437 DEBUG TRAIN Batch 5/700 loss 26.238258 loss_att 31.678074 loss_ctc 47.455986 loss_rnnt 22.130623 hw_loss 0.357449 lr 0.00076788 rank 3
2023-02-17 12:07:03,441 DEBUG TRAIN Batch 5/700 loss 17.882973 loss_att 23.681007 loss_ctc 27.616392 loss_rnnt 15.253184 hw_loss 0.323235 lr 0.00076768 rank 0
2023-02-17 12:07:03,445 DEBUG TRAIN Batch 5/700 loss 12.799894 loss_att 18.354799 loss_ctc 22.622509 loss_rnnt 10.230515 hw_loss 0.278844 lr 0.00076729 rank 2
2023-02-17 12:08:19,851 DEBUG TRAIN Batch 5/800 loss 15.321863 loss_att 22.128696 loss_ctc 32.254143 loss_rnnt 11.528931 hw_loss 0.326115 lr 0.00076738 rank 6
2023-02-17 12:08:19,856 DEBUG TRAIN Batch 5/800 loss 36.255882 loss_att 44.364731 loss_ctc 55.070873 loss_rnnt 31.950438 hw_loss 0.328141 lr 0.00076674 rank 1
2023-02-17 12:08:19,857 DEBUG TRAIN Batch 5/800 loss 21.991955 loss_att 27.703899 loss_ctc 34.823605 loss_rnnt 18.929209 hw_loss 0.392754 lr 0.00076703 rank 5
2023-02-17 12:08:19,860 DEBUG TRAIN Batch 5/800 loss 22.789923 loss_att 26.130919 loss_ctc 37.477486 loss_rnnt 20.002729 hw_loss 0.301222 lr 0.00076705 rank 7
2023-02-17 12:08:19,861 DEBUG TRAIN Batch 5/800 loss 26.081724 loss_att 25.835754 loss_ctc 39.261833 loss_rnnt 24.130901 hw_loss 0.455005 lr 0.00076697 rank 3
2023-02-17 12:08:19,861 DEBUG TRAIN Batch 5/800 loss 25.953918 loss_att 31.063883 loss_ctc 37.753418 loss_rnnt 23.158947 hw_loss 0.374464 lr 0.00076639 rank 2
2023-02-17 12:08:19,862 DEBUG TRAIN Batch 5/800 loss 13.517417 loss_att 17.884331 loss_ctc 28.816610 loss_rnnt 10.442595 hw_loss 0.302901 lr 0.00076678 rank 0
2023-02-17 12:08:19,864 DEBUG TRAIN Batch 5/800 loss 29.578112 loss_att 37.876904 loss_ctc 55.931229 loss_rnnt 24.251343 hw_loss 0.287367 lr 0.00076730 rank 4
2023-02-17 12:09:35,397 DEBUG TRAIN Batch 5/900 loss 17.647635 loss_att 21.967672 loss_ctc 26.014999 loss_rnnt 15.494378 hw_loss 0.325497 lr 0.00076615 rank 7
2023-02-17 12:09:35,399 DEBUG TRAIN Batch 5/900 loss 25.513809 loss_att 34.853348 loss_ctc 39.402264 loss_rnnt 21.633797 hw_loss 0.300584 lr 0.00076648 rank 6
2023-02-17 12:09:35,403 DEBUG TRAIN Batch 5/900 loss 38.737682 loss_att 39.430794 loss_ctc 58.441940 loss_rnnt 35.745190 hw_loss 0.424948 lr 0.00076613 rank 5
2023-02-17 12:09:35,403 DEBUG TRAIN Batch 5/900 loss 9.250712 loss_att 13.653294 loss_ctc 18.348822 loss_rnnt 6.993280 hw_loss 0.307189 lr 0.00076584 rank 1
2023-02-17 12:09:35,405 DEBUG TRAIN Batch 5/900 loss 28.827312 loss_att 34.564037 loss_ctc 40.432228 loss_rnnt 25.909653 hw_loss 0.418109 lr 0.00076588 rank 0
2023-02-17 12:09:35,406 DEBUG TRAIN Batch 5/900 loss 25.787666 loss_att 25.816702 loss_ctc 35.956600 loss_rnnt 24.215124 hw_loss 0.395398 lr 0.00076640 rank 4
2023-02-17 12:09:35,407 DEBUG TRAIN Batch 5/900 loss 20.178570 loss_att 27.229567 loss_ctc 33.862976 loss_rnnt 16.776028 hw_loss 0.314540 lr 0.00076607 rank 3
2023-02-17 12:09:35,452 DEBUG TRAIN Batch 5/900 loss 16.806787 loss_att 22.088993 loss_ctc 25.133059 loss_rnnt 14.445548 hw_loss 0.364929 lr 0.00076549 rank 2
2023-02-17 12:10:52,222 DEBUG TRAIN Batch 5/1000 loss 11.004060 loss_att 18.223867 loss_ctc 16.870476 loss_rnnt 8.611493 hw_loss 0.312030 lr 0.00076558 rank 6
2023-02-17 12:10:52,226 DEBUG TRAIN Batch 5/1000 loss 28.420025 loss_att 31.804543 loss_ctc 40.885185 loss_rnnt 25.915905 hw_loss 0.309745 lr 0.00076523 rank 5
2023-02-17 12:10:52,227 DEBUG TRAIN Batch 5/1000 loss 20.008450 loss_att 24.385365 loss_ctc 37.459454 loss_rnnt 16.625368 hw_loss 0.339186 lr 0.00076518 rank 3
2023-02-17 12:10:52,230 DEBUG TRAIN Batch 5/1000 loss 20.117054 loss_att 25.409012 loss_ctc 31.696358 loss_rnnt 17.358313 hw_loss 0.293332 lr 0.00076550 rank 4
2023-02-17 12:10:52,236 DEBUG TRAIN Batch 5/1000 loss 20.518904 loss_att 26.812521 loss_ctc 40.451622 loss_rnnt 16.409077 hw_loss 0.362641 lr 0.00076498 rank 0
2023-02-17 12:10:52,236 DEBUG TRAIN Batch 5/1000 loss 19.037195 loss_att 28.706829 loss_ctc 36.274284 loss_rnnt 14.610860 hw_loss 0.363995 lr 0.00076459 rank 2
2023-02-17 12:10:52,253 DEBUG TRAIN Batch 5/1000 loss 17.193336 loss_att 21.780991 loss_ctc 29.947737 loss_rnnt 14.475487 hw_loss 0.186998 lr 0.00076494 rank 1
2023-02-17 12:10:52,256 DEBUG TRAIN Batch 5/1000 loss 19.340740 loss_att 25.589329 loss_ctc 36.072334 loss_rnnt 15.646890 hw_loss 0.399848 lr 0.00076525 rank 7
2023-02-17 12:12:09,956 DEBUG TRAIN Batch 5/1100 loss 29.780989 loss_att 32.182495 loss_ctc 48.731941 loss_rnnt 26.633757 hw_loss 0.262762 lr 0.00076435 rank 7
2023-02-17 12:12:09,957 DEBUG TRAIN Batch 5/1100 loss 23.253971 loss_att 24.607109 loss_ctc 32.956894 loss_rnnt 21.524881 hw_loss 0.308884 lr 0.00076405 rank 1
2023-02-17 12:12:09,961 DEBUG TRAIN Batch 5/1100 loss 24.442446 loss_att 24.807974 loss_ctc 35.258755 loss_rnnt 22.741472 hw_loss 0.348174 lr 0.00076428 rank 3
2023-02-17 12:12:09,961 DEBUG TRAIN Batch 5/1100 loss 19.818026 loss_att 26.030411 loss_ctc 33.522976 loss_rnnt 16.598522 hw_loss 0.280683 lr 0.00076468 rank 6
2023-02-17 12:12:09,962 DEBUG TRAIN Batch 5/1100 loss 12.278513 loss_att 17.395603 loss_ctc 24.496735 loss_rnnt 9.498062 hw_loss 0.239880 lr 0.00076460 rank 4
2023-02-17 12:12:09,962 DEBUG TRAIN Batch 5/1100 loss 22.077892 loss_att 30.068897 loss_ctc 35.043659 loss_rnnt 18.586351 hw_loss 0.308570 lr 0.00076433 rank 5
2023-02-17 12:12:09,967 DEBUG TRAIN Batch 5/1100 loss 12.313149 loss_att 14.386275 loss_ctc 19.642515 loss_rnnt 10.761082 hw_loss 0.300362 lr 0.00076408 rank 0
2023-02-17 12:12:09,967 DEBUG TRAIN Batch 5/1100 loss 44.327553 loss_att 50.648411 loss_ctc 71.376007 loss_rnnt 39.241165 hw_loss 0.404537 lr 0.00076370 rank 2
2023-02-17 12:13:25,725 DEBUG TRAIN Batch 5/1200 loss 23.460863 loss_att 25.667000 loss_ctc 38.266903 loss_rnnt 20.884951 hw_loss 0.301023 lr 0.00076371 rank 4
2023-02-17 12:13:25,726 DEBUG TRAIN Batch 5/1200 loss 11.080953 loss_att 13.490645 loss_ctc 19.934372 loss_rnnt 9.243070 hw_loss 0.329043 lr 0.00076346 rank 7
2023-02-17 12:13:25,726 DEBUG TRAIN Batch 5/1200 loss 14.288136 loss_att 16.245272 loss_ctc 18.875809 loss_rnnt 13.100139 hw_loss 0.346653 lr 0.00076379 rank 6
2023-02-17 12:13:25,727 DEBUG TRAIN Batch 5/1200 loss 23.840851 loss_att 27.627930 loss_ctc 36.975903 loss_rnnt 21.175421 hw_loss 0.293767 lr 0.00076344 rank 5
2023-02-17 12:13:25,727 DEBUG TRAIN Batch 5/1200 loss 21.346687 loss_att 24.729609 loss_ctc 29.030224 loss_rnnt 19.502735 hw_loss 0.267930 lr 0.00076316 rank 1
2023-02-17 12:13:25,728 DEBUG TRAIN Batch 5/1200 loss 23.176523 loss_att 24.687401 loss_ctc 30.901821 loss_rnnt 21.663061 hw_loss 0.339840 lr 0.00076281 rank 2
2023-02-17 12:13:25,730 DEBUG TRAIN Batch 5/1200 loss 21.473465 loss_att 26.251266 loss_ctc 30.575123 loss_rnnt 19.144316 hw_loss 0.300065 lr 0.00076319 rank 0
2023-02-17 12:13:25,771 DEBUG TRAIN Batch 5/1200 loss 33.082077 loss_att 34.194252 loss_ctc 47.129585 loss_rnnt 30.765543 hw_loss 0.414553 lr 0.00076339 rank 3
2023-02-17 12:14:41,891 DEBUG TRAIN Batch 5/1300 loss 20.347687 loss_att 23.762371 loss_ctc 38.365089 loss_rnnt 17.058468 hw_loss 0.382429 lr 0.00076290 rank 6
2023-02-17 12:14:41,892 DEBUG TRAIN Batch 5/1300 loss 28.564272 loss_att 36.916580 loss_ctc 52.798931 loss_rnnt 23.491133 hw_loss 0.321357 lr 0.00076257 rank 7
2023-02-17 12:14:41,896 DEBUG TRAIN Batch 5/1300 loss 30.246613 loss_att 35.730209 loss_ctc 54.334854 loss_rnnt 25.715538 hw_loss 0.417354 lr 0.00076255 rank 5
2023-02-17 12:14:41,901 DEBUG TRAIN Batch 5/1300 loss 13.512937 loss_att 21.664749 loss_ctc 19.026756 loss_rnnt 10.959776 hw_loss 0.351793 lr 0.00076193 rank 2
2023-02-17 12:14:41,902 DEBUG TRAIN Batch 5/1300 loss 16.891191 loss_att 27.056158 loss_ctc 22.199432 loss_rnnt 13.947996 hw_loss 0.379566 lr 0.00076250 rank 3
2023-02-17 12:14:41,904 DEBUG TRAIN Batch 5/1300 loss 11.249325 loss_att 10.857631 loss_ctc 14.492125 loss_rnnt 10.709429 hw_loss 0.348490 lr 0.00076282 rank 4
2023-02-17 12:14:41,904 DEBUG TRAIN Batch 5/1300 loss 16.220844 loss_att 19.285238 loss_ctc 25.029343 loss_rnnt 14.278193 hw_loss 0.291202 lr 0.00076227 rank 1
2023-02-17 12:14:41,950 DEBUG TRAIN Batch 5/1300 loss 14.376996 loss_att 15.857109 loss_ctc 16.576246 loss_rnnt 13.591727 hw_loss 0.367525 lr 0.00076231 rank 0
2023-02-17 12:16:00,100 DEBUG TRAIN Batch 5/1400 loss 11.898545 loss_att 17.391390 loss_ctc 23.838842 loss_rnnt 9.057747 hw_loss 0.281606 lr 0.00076139 rank 1
2023-02-17 12:16:00,100 DEBUG TRAIN Batch 5/1400 loss 19.536835 loss_att 25.207355 loss_ctc 33.354580 loss_rnnt 16.405228 hw_loss 0.290882 lr 0.00076167 rank 5
2023-02-17 12:16:00,104 DEBUG TRAIN Batch 5/1400 loss 31.682896 loss_att 37.715015 loss_ctc 47.441616 loss_rnnt 28.232327 hw_loss 0.268095 lr 0.00076193 rank 4
2023-02-17 12:16:00,104 DEBUG TRAIN Batch 5/1400 loss 10.552305 loss_att 18.311726 loss_ctc 17.306250 loss_rnnt 7.902148 hw_loss 0.370775 lr 0.00076201 rank 6
2023-02-17 12:16:00,109 DEBUG TRAIN Batch 5/1400 loss 18.958696 loss_att 24.297865 loss_ctc 38.396236 loss_rnnt 15.138006 hw_loss 0.302224 lr 0.00076104 rank 2
2023-02-17 12:16:00,111 DEBUG TRAIN Batch 5/1400 loss 12.647826 loss_att 18.399570 loss_ctc 26.738569 loss_rnnt 9.476706 hw_loss 0.266261 lr 0.00076162 rank 3
2023-02-17 12:16:00,111 DEBUG TRAIN Batch 5/1400 loss 18.521616 loss_att 25.621405 loss_ctc 36.331314 loss_rnnt 14.544752 hw_loss 0.341778 lr 0.00076142 rank 0
2023-02-17 12:16:00,127 DEBUG TRAIN Batch 5/1400 loss 30.409601 loss_att 31.715343 loss_ctc 56.345192 loss_rnnt 26.517065 hw_loss 0.324953 lr 0.00076169 rank 7
2023-02-17 12:17:16,138 DEBUG TRAIN Batch 5/1500 loss 13.418547 loss_att 18.978807 loss_ctc 20.721661 loss_rnnt 11.135628 hw_loss 0.369596 lr 0.00076105 rank 4
2023-02-17 12:17:16,140 DEBUG TRAIN Batch 5/1500 loss 27.821468 loss_att 29.585289 loss_ctc 40.818325 loss_rnnt 25.576874 hw_loss 0.297964 lr 0.00076079 rank 5
2023-02-17 12:17:16,140 DEBUG TRAIN Batch 5/1500 loss 20.306898 loss_att 23.771076 loss_ctc 37.356289 loss_rnnt 17.141199 hw_loss 0.374270 lr 0.00076081 rank 7
2023-02-17 12:17:16,143 DEBUG TRAIN Batch 5/1500 loss 9.853141 loss_att 14.113420 loss_ctc 17.864510 loss_rnnt 7.772325 hw_loss 0.301083 lr 0.00076113 rank 6
2023-02-17 12:17:16,143 DEBUG TRAIN Batch 5/1500 loss 32.623180 loss_att 41.155243 loss_ctc 54.008480 loss_rnnt 27.862221 hw_loss 0.380944 lr 0.00076051 rank 1
2023-02-17 12:17:16,144 DEBUG TRAIN Batch 5/1500 loss 33.389595 loss_att 35.959908 loss_ctc 57.081367 loss_rnnt 29.508413 hw_loss 0.390410 lr 0.00076054 rank 0
2023-02-17 12:17:16,145 DEBUG TRAIN Batch 5/1500 loss 13.722939 loss_att 19.108866 loss_ctc 23.244305 loss_rnnt 11.162529 hw_loss 0.400706 lr 0.00076016 rank 2
2023-02-17 12:17:16,146 DEBUG TRAIN Batch 5/1500 loss 40.738152 loss_att 42.769329 loss_ctc 61.166256 loss_rnnt 37.487362 hw_loss 0.226516 lr 0.00076073 rank 3
2023-02-17 12:18:30,718 DEBUG TRAIN Batch 5/1600 loss 16.759224 loss_att 19.440035 loss_ctc 25.117424 loss_rnnt 14.947414 hw_loss 0.302288 lr 0.00076017 rank 4
2023-02-17 12:18:30,719 DEBUG TRAIN Batch 5/1600 loss 13.966437 loss_att 18.841141 loss_ctc 24.919262 loss_rnnt 11.374984 hw_loss 0.292757 lr 0.00075986 rank 3
2023-02-17 12:18:30,719 DEBUG TRAIN Batch 5/1600 loss 13.963797 loss_att 21.010450 loss_ctc 24.931358 loss_rnnt 10.984102 hw_loss 0.202540 lr 0.00076025 rank 6
2023-02-17 12:18:30,721 DEBUG TRAIN Batch 5/1600 loss 23.966951 loss_att 24.611666 loss_ctc 41.879662 loss_rnnt 21.337826 hw_loss 0.209672 lr 0.00075991 rank 5
2023-02-17 12:18:30,721 DEBUG TRAIN Batch 5/1600 loss 10.099645 loss_att 18.326050 loss_ctc 20.693295 loss_rnnt 6.850289 hw_loss 0.359228 lr 0.00075963 rank 1
2023-02-17 12:18:30,726 DEBUG TRAIN Batch 5/1600 loss 18.934856 loss_att 20.895582 loss_ctc 27.273430 loss_rnnt 17.229624 hw_loss 0.377395 lr 0.00075966 rank 0
2023-02-17 12:18:30,727 DEBUG TRAIN Batch 5/1600 loss 24.995302 loss_att 29.004543 loss_ctc 43.793972 loss_rnnt 21.525646 hw_loss 0.302471 lr 0.00075993 rank 7
2023-02-17 12:18:30,729 DEBUG TRAIN Batch 5/1600 loss 20.614130 loss_att 25.113390 loss_ctc 34.827324 loss_rnnt 17.566658 hw_loss 0.473490 lr 0.00075929 rank 2
2023-02-17 12:19:46,248 DEBUG TRAIN Batch 5/1700 loss 12.997493 loss_att 17.225573 loss_ctc 23.285383 loss_rnnt 10.598692 hw_loss 0.340248 lr 0.00075903 rank 5
2023-02-17 12:19:46,249 DEBUG TRAIN Batch 5/1700 loss 19.406246 loss_att 22.520119 loss_ctc 27.651951 loss_rnnt 17.485809 hw_loss 0.371688 lr 0.00075875 rank 1
2023-02-17 12:19:46,251 DEBUG TRAIN Batch 5/1700 loss 18.983488 loss_att 24.503010 loss_ctc 30.208950 loss_rnnt 16.183495 hw_loss 0.373803 lr 0.00075905 rank 7
2023-02-17 12:19:46,251 DEBUG TRAIN Batch 5/1700 loss 26.876490 loss_att 25.972660 loss_ctc 38.763145 loss_rnnt 25.301399 hw_loss 0.320568 lr 0.00075937 rank 6
2023-02-17 12:19:46,251 DEBUG TRAIN Batch 5/1700 loss 34.918030 loss_att 42.336868 loss_ctc 55.697357 loss_rnnt 30.454304 hw_loss 0.392592 lr 0.00075879 rank 0
2023-02-17 12:19:46,254 DEBUG TRAIN Batch 5/1700 loss 27.029125 loss_att 29.673927 loss_ctc 37.798824 loss_rnnt 24.880466 hw_loss 0.344507 lr 0.00075898 rank 3
2023-02-17 12:19:46,277 DEBUG TRAIN Batch 5/1700 loss 9.673773 loss_att 13.879557 loss_ctc 17.858749 loss_rnnt 7.519907 hw_loss 0.415084 lr 0.00075841 rank 2
2023-02-17 12:19:46,301 DEBUG TRAIN Batch 5/1700 loss 16.441723 loss_att 20.327368 loss_ctc 29.364607 loss_rnnt 13.785568 hw_loss 0.292452 lr 0.00075929 rank 4
2023-02-17 12:21:06,010 DEBUG TRAIN Batch 5/1800 loss 18.832802 loss_att 23.699808 loss_ctc 31.472233 loss_rnnt 15.985636 hw_loss 0.353452 lr 0.00075788 rank 1
2023-02-17 12:21:06,011 DEBUG TRAIN Batch 5/1800 loss 28.028622 loss_att 31.511999 loss_ctc 42.459949 loss_rnnt 25.240038 hw_loss 0.314492 lr 0.00075754 rank 2
2023-02-17 12:21:06,011 DEBUG TRAIN Batch 5/1800 loss 9.953113 loss_att 13.306985 loss_ctc 20.510609 loss_rnnt 7.677314 hw_loss 0.370047 lr 0.00075811 rank 3
2023-02-17 12:21:06,014 DEBUG TRAIN Batch 5/1800 loss 25.830746 loss_att 28.348965 loss_ctc 40.075935 loss_rnnt 23.271009 hw_loss 0.293874 lr 0.00075818 rank 7
2023-02-17 12:21:06,015 DEBUG TRAIN Batch 5/1800 loss 22.949299 loss_att 26.754969 loss_ctc 33.082825 loss_rnnt 20.689470 hw_loss 0.276670 lr 0.00075850 rank 6
2023-02-17 12:21:06,018 DEBUG TRAIN Batch 5/1800 loss 15.329945 loss_att 18.069031 loss_ctc 20.025032 loss_rnnt 13.958979 hw_loss 0.369631 lr 0.00075816 rank 5
2023-02-17 12:21:06,019 DEBUG TRAIN Batch 5/1800 loss 12.900915 loss_att 15.009792 loss_ctc 18.671309 loss_rnnt 11.503905 hw_loss 0.385966 lr 0.00075792 rank 0
2023-02-17 12:21:06,022 DEBUG TRAIN Batch 5/1800 loss 16.485273 loss_att 22.600508 loss_ctc 26.163561 loss_rnnt 13.795177 hw_loss 0.331142 lr 0.00075842 rank 4
2023-02-17 12:22:21,236 DEBUG TRAIN Batch 5/1900 loss 15.351960 loss_att 15.267219 loss_ctc 20.911076 loss_rnnt 14.414015 hw_loss 0.400647 lr 0.00075701 rank 1
2023-02-17 12:22:21,238 DEBUG TRAIN Batch 5/1900 loss 37.072361 loss_att 50.480133 loss_ctc 71.441170 loss_rnnt 29.614286 hw_loss 0.363774 lr 0.00075763 rank 6
2023-02-17 12:22:21,238 DEBUG TRAIN Batch 5/1900 loss 27.083147 loss_att 33.466965 loss_ctc 38.202633 loss_rnnt 24.151297 hw_loss 0.323421 lr 0.00075667 rank 2
2023-02-17 12:22:21,239 DEBUG TRAIN Batch 5/1900 loss 12.213887 loss_att 14.642843 loss_ctc 20.516390 loss_rnnt 10.416344 hw_loss 0.383908 lr 0.00075724 rank 3
2023-02-17 12:22:21,243 DEBUG TRAIN Batch 5/1900 loss 14.015922 loss_att 15.288367 loss_ctc 21.316803 loss_rnnt 12.546376 hw_loss 0.453009 lr 0.00075729 rank 5
2023-02-17 12:22:21,243 DEBUG TRAIN Batch 5/1900 loss 20.822248 loss_att 28.352818 loss_ctc 39.172493 loss_rnnt 16.675766 hw_loss 0.363125 lr 0.00075731 rank 7
2023-02-17 12:22:21,244 DEBUG TRAIN Batch 5/1900 loss 17.419670 loss_att 23.740467 loss_ctc 24.521292 loss_rnnt 15.028072 hw_loss 0.338541 lr 0.00075705 rank 0
2023-02-17 12:22:21,245 DEBUG TRAIN Batch 5/1900 loss 27.734180 loss_att 28.571552 loss_ctc 39.317451 loss_rnnt 25.784048 hw_loss 0.446664 lr 0.00075755 rank 4
2023-02-17 12:23:38,192 DEBUG TRAIN Batch 5/2000 loss 18.637503 loss_att 23.595917 loss_ctc 33.315224 loss_rnnt 15.543918 hw_loss 0.271640 lr 0.00075615 rank 1
2023-02-17 12:23:38,195 DEBUG TRAIN Batch 5/2000 loss 37.854004 loss_att 44.847801 loss_ctc 58.593483 loss_rnnt 33.533897 hw_loss 0.292654 lr 0.00075676 rank 6
2023-02-17 12:23:38,197 DEBUG TRAIN Batch 5/2000 loss 37.467224 loss_att 40.305759 loss_ctc 61.054909 loss_rnnt 33.495789 hw_loss 0.485065 lr 0.00075637 rank 3
2023-02-17 12:23:38,197 DEBUG TRAIN Batch 5/2000 loss 22.975826 loss_att 26.105530 loss_ctc 43.119728 loss_rnnt 19.509485 hw_loss 0.289772 lr 0.00075644 rank 7
2023-02-17 12:23:38,201 DEBUG TRAIN Batch 5/2000 loss 19.561255 loss_att 26.782190 loss_ctc 37.169365 loss_rnnt 15.581444 hw_loss 0.352270 lr 0.00075642 rank 5
2023-02-17 12:23:38,202 DEBUG TRAIN Batch 5/2000 loss 26.160442 loss_att 28.570898 loss_ctc 37.635063 loss_rnnt 23.971212 hw_loss 0.332232 lr 0.00075668 rank 4
2023-02-17 12:23:38,205 DEBUG TRAIN Batch 5/2000 loss 20.717031 loss_att 25.183704 loss_ctc 41.192734 loss_rnnt 16.913290 hw_loss 0.338088 lr 0.00075581 rank 2
2023-02-17 12:23:38,207 DEBUG TRAIN Batch 5/2000 loss 7.773062 loss_att 12.160604 loss_ctc 16.802055 loss_rnnt 5.542386 hw_loss 0.279939 lr 0.00075618 rank 0
2023-02-17 12:24:56,700 DEBUG TRAIN Batch 5/2100 loss 22.084949 loss_att 27.841461 loss_ctc 38.324234 loss_rnnt 18.571861 hw_loss 0.368523 lr 0.00075556 rank 5
2023-02-17 12:24:56,703 DEBUG TRAIN Batch 5/2100 loss 13.255807 loss_att 19.784470 loss_ctc 20.450388 loss_rnnt 10.868327 hw_loss 0.229629 lr 0.00075589 rank 6
2023-02-17 12:24:56,705 DEBUG TRAIN Batch 5/2100 loss 12.715411 loss_att 16.086235 loss_ctc 22.015690 loss_rnnt 10.624153 hw_loss 0.331981 lr 0.00075582 rank 4
2023-02-17 12:24:56,705 DEBUG TRAIN Batch 5/2100 loss 24.374233 loss_att 27.487968 loss_ctc 37.697304 loss_rnnt 21.838049 hw_loss 0.256929 lr 0.00075532 rank 0
2023-02-17 12:24:56,707 DEBUG TRAIN Batch 5/2100 loss 22.983038 loss_att 25.988131 loss_ctc 36.892509 loss_rnnt 20.408875 hw_loss 0.222279 lr 0.00075495 rank 2
2023-02-17 12:24:56,710 DEBUG TRAIN Batch 5/2100 loss 8.339210 loss_att 16.881575 loss_ctc 14.642383 loss_rnnt 5.589120 hw_loss 0.377239 lr 0.00075528 rank 1
2023-02-17 12:24:56,710 DEBUG TRAIN Batch 5/2100 loss 15.425858 loss_att 19.795956 loss_ctc 29.641323 loss_rnnt 12.523209 hw_loss 0.249816 lr 0.00075557 rank 7
2023-02-17 12:24:56,753 DEBUG TRAIN Batch 5/2100 loss 15.906953 loss_att 17.578264 loss_ctc 27.024734 loss_rnnt 13.887035 hw_loss 0.381159 lr 0.00075551 rank 3
2023-02-17 12:26:13,794 DEBUG TRAIN Batch 5/2200 loss 33.100548 loss_att 38.139412 loss_ctc 57.357544 loss_rnnt 28.710966 hw_loss 0.276646 lr 0.00075503 rank 6
2023-02-17 12:26:13,798 DEBUG TRAIN Batch 5/2200 loss 16.110096 loss_att 25.100780 loss_ctc 25.829899 loss_rnnt 12.767857 hw_loss 0.465239 lr 0.00075470 rank 5
2023-02-17 12:26:13,800 DEBUG TRAIN Batch 5/2200 loss 12.406966 loss_att 16.073849 loss_ctc 20.894775 loss_rnnt 10.333551 hw_loss 0.390618 lr 0.00075464 rank 3
2023-02-17 12:26:13,801 DEBUG TRAIN Batch 5/2200 loss 15.253731 loss_att 19.183516 loss_ctc 28.823654 loss_rnnt 12.495257 hw_loss 0.305989 lr 0.00075442 rank 1
2023-02-17 12:26:13,802 DEBUG TRAIN Batch 5/2200 loss 17.148378 loss_att 21.517996 loss_ctc 29.855967 loss_rnnt 14.423376 hw_loss 0.293875 lr 0.00075471 rank 7
2023-02-17 12:26:13,804 DEBUG TRAIN Batch 5/2200 loss 19.839331 loss_att 27.192547 loss_ctc 43.246902 loss_rnnt 15.017515 hw_loss 0.431559 lr 0.00075409 rank 2
2023-02-17 12:26:13,804 DEBUG TRAIN Batch 5/2200 loss 15.502267 loss_att 20.365870 loss_ctc 25.004440 loss_rnnt 13.085085 hw_loss 0.332819 lr 0.00075495 rank 4
2023-02-17 12:26:13,810 DEBUG TRAIN Batch 5/2200 loss 17.356657 loss_att 21.480419 loss_ctc 26.742310 loss_rnnt 15.096008 hw_loss 0.345895 lr 0.00075446 rank 0
2023-02-17 12:27:31,121 DEBUG TRAIN Batch 5/2300 loss 16.418472 loss_att 21.686251 loss_ctc 27.890274 loss_rnnt 13.691807 hw_loss 0.269134 lr 0.00075356 rank 1
2023-02-17 12:27:31,124 DEBUG TRAIN Batch 5/2300 loss 16.566582 loss_att 20.875694 loss_ctc 31.206909 loss_rnnt 13.573967 hw_loss 0.335148 lr 0.00075417 rank 6
2023-02-17 12:27:31,126 DEBUG TRAIN Batch 5/2300 loss 31.590807 loss_att 37.760178 loss_ctc 53.311134 loss_rnnt 27.313608 hw_loss 0.276150 lr 0.00075384 rank 5
2023-02-17 12:27:31,126 DEBUG TRAIN Batch 5/2300 loss 25.902916 loss_att 30.884579 loss_ctc 47.425415 loss_rnnt 21.818495 hw_loss 0.409543 lr 0.00075379 rank 3
2023-02-17 12:27:31,127 DEBUG TRAIN Batch 5/2300 loss 27.232147 loss_att 29.722008 loss_ctc 41.690125 loss_rnnt 24.613192 hw_loss 0.362347 lr 0.00075410 rank 4
2023-02-17 12:27:31,127 DEBUG TRAIN Batch 5/2300 loss 17.460836 loss_att 19.972567 loss_ctc 28.073313 loss_rnnt 15.362014 hw_loss 0.340278 lr 0.00075386 rank 7
2023-02-17 12:27:31,128 DEBUG TRAIN Batch 5/2300 loss 30.085566 loss_att 33.093292 loss_ctc 41.380196 loss_rnnt 27.817156 hw_loss 0.301709 lr 0.00075323 rank 2
2023-02-17 12:27:31,174 DEBUG TRAIN Batch 5/2300 loss 26.476498 loss_att 31.919228 loss_ctc 42.753883 loss_rnnt 23.028645 hw_loss 0.354350 lr 0.00075360 rank 0
2023-02-17 12:28:47,823 DEBUG TRAIN Batch 5/2400 loss 10.964149 loss_att 12.522833 loss_ctc 14.802818 loss_rnnt 9.887195 hw_loss 0.475115 lr 0.00075332 rank 6
2023-02-17 12:28:47,827 DEBUG TRAIN Batch 5/2400 loss 13.087757 loss_att 18.392530 loss_ctc 24.900255 loss_rnnt 10.198584 hw_loss 0.474784 lr 0.00075238 rank 2
2023-02-17 12:28:47,827 DEBUG TRAIN Batch 5/2400 loss 13.659755 loss_att 16.816185 loss_ctc 22.199667 loss_rnnt 11.728190 hw_loss 0.303043 lr 0.00075293 rank 3
2023-02-17 12:28:47,827 DEBUG TRAIN Batch 5/2400 loss 17.899755 loss_att 21.007036 loss_ctc 36.322697 loss_rnnt 14.646633 hw_loss 0.328634 lr 0.00075271 rank 1
2023-02-17 12:28:47,828 DEBUG TRAIN Batch 5/2400 loss 18.504267 loss_att 24.813589 loss_ctc 28.035910 loss_rnnt 15.795959 hw_loss 0.329171 lr 0.00075300 rank 7
2023-02-17 12:28:47,831 DEBUG TRAIN Batch 5/2400 loss 25.067432 loss_att 29.725960 loss_ctc 33.627319 loss_rnnt 22.808151 hw_loss 0.349232 lr 0.00075274 rank 0
2023-02-17 12:28:47,830 DEBUG TRAIN Batch 5/2400 loss 16.543030 loss_att 21.813644 loss_ctc 22.059439 loss_rnnt 14.536039 hw_loss 0.407526 lr 0.00075298 rank 5
2023-02-17 12:28:47,872 DEBUG TRAIN Batch 5/2400 loss 35.246635 loss_att 39.633926 loss_ctc 52.319305 loss_rnnt 31.929312 hw_loss 0.306587 lr 0.00075324 rank 4
2023-02-17 12:30:08,055 DEBUG TRAIN Batch 5/2500 loss 14.502913 loss_att 14.658509 loss_ctc 21.522837 loss_rnnt 13.335186 hw_loss 0.376157 lr 0.00075246 rank 6
2023-02-17 12:30:08,056 DEBUG TRAIN Batch 5/2500 loss 16.032583 loss_att 20.152037 loss_ctc 30.210171 loss_rnnt 13.087642 hw_loss 0.432571 lr 0.00075215 rank 7
2023-02-17 12:30:08,060 DEBUG TRAIN Batch 5/2500 loss 30.751574 loss_att 38.640560 loss_ctc 50.618706 loss_rnnt 26.334888 hw_loss 0.356127 lr 0.00075153 rank 2
2023-02-17 12:30:08,062 DEBUG TRAIN Batch 5/2500 loss 22.063572 loss_att 26.764904 loss_ctc 34.727306 loss_rnnt 19.228157 hw_loss 0.387470 lr 0.00075213 rank 5
2023-02-17 12:30:08,062 DEBUG TRAIN Batch 5/2500 loss 18.791649 loss_att 24.820599 loss_ctc 41.927917 loss_rnnt 14.358222 hw_loss 0.267753 lr 0.00075208 rank 3
2023-02-17 12:30:08,065 DEBUG TRAIN Batch 5/2500 loss 13.037808 loss_att 14.582587 loss_ctc 18.758165 loss_rnnt 11.760324 hw_loss 0.385902 lr 0.00075189 rank 0
2023-02-17 12:30:08,089 DEBUG TRAIN Batch 5/2500 loss 28.647350 loss_att 32.871979 loss_ctc 46.944679 loss_rnnt 25.187126 hw_loss 0.329353 lr 0.00075239 rank 4
2023-02-17 12:30:08,124 DEBUG TRAIN Batch 5/2500 loss 16.341339 loss_att 18.363386 loss_ctc 27.783133 loss_rnnt 14.219836 hw_loss 0.359103 lr 0.00075186 rank 1
2023-02-17 12:31:22,864 DEBUG TRAIN Batch 5/2600 loss 12.677872 loss_att 17.813934 loss_ctc 25.790205 loss_rnnt 9.736100 hw_loss 0.311714 lr 0.00075161 rank 6
2023-02-17 12:31:22,864 DEBUG TRAIN Batch 5/2600 loss 13.355442 loss_att 20.052130 loss_ctc 27.089275 loss_rnnt 10.009857 hw_loss 0.328255 lr 0.00075123 rank 3
2023-02-17 12:31:22,869 DEBUG TRAIN Batch 5/2600 loss 22.360807 loss_att 24.189676 loss_ctc 28.332678 loss_rnnt 20.987806 hw_loss 0.395585 lr 0.00075130 rank 7
2023-02-17 12:31:22,870 DEBUG TRAIN Batch 5/2600 loss 14.508004 loss_att 14.406373 loss_ctc 21.082392 loss_rnnt 13.425490 hw_loss 0.424228 lr 0.00075101 rank 1
2023-02-17 12:31:22,870 DEBUG TRAIN Batch 5/2600 loss 18.216778 loss_att 20.556967 loss_ctc 26.728634 loss_rnnt 16.438053 hw_loss 0.329571 lr 0.00075128 rank 5
2023-02-17 12:31:22,871 DEBUG TRAIN Batch 5/2600 loss 19.170750 loss_att 19.653927 loss_ctc 31.261801 loss_rnnt 17.274574 hw_loss 0.351370 lr 0.00075154 rank 4
2023-02-17 12:31:22,872 DEBUG TRAIN Batch 5/2600 loss 21.677841 loss_att 26.957617 loss_ctc 31.948971 loss_rnnt 19.097122 hw_loss 0.291151 lr 0.00075104 rank 0
2023-02-17 12:31:22,916 DEBUG TRAIN Batch 5/2600 loss 16.657219 loss_att 25.824791 loss_ctc 33.122749 loss_rnnt 12.420502 hw_loss 0.389623 lr 0.00075068 rank 2
2023-02-17 12:32:38,840 DEBUG TRAIN Batch 5/2700 loss 11.843044 loss_att 18.993504 loss_ctc 20.870178 loss_rnnt 9.013864 hw_loss 0.366506 lr 0.00075045 rank 7
2023-02-17 12:32:38,845 DEBUG TRAIN Batch 5/2700 loss 12.111202 loss_att 15.156961 loss_ctc 18.883829 loss_rnnt 10.413035 hw_loss 0.348746 lr 0.00075016 rank 1
2023-02-17 12:32:38,848 DEBUG TRAIN Batch 5/2700 loss 22.074480 loss_att 26.106913 loss_ctc 36.537388 loss_rnnt 19.152643 hw_loss 0.350559 lr 0.00075076 rank 6
2023-02-17 12:32:38,848 DEBUG TRAIN Batch 5/2700 loss 17.808123 loss_att 23.581875 loss_ctc 41.313610 loss_rnnt 13.294769 hw_loss 0.421008 lr 0.00075043 rank 5
2023-02-17 12:32:38,850 DEBUG TRAIN Batch 5/2700 loss 18.067602 loss_att 21.948864 loss_ctc 31.551825 loss_rnnt 15.323896 hw_loss 0.317921 lr 0.00074984 rank 2
2023-02-17 12:32:38,851 DEBUG TRAIN Batch 5/2700 loss 12.228524 loss_att 18.166761 loss_ctc 23.619429 loss_rnnt 9.339465 hw_loss 0.342419 lr 0.00075038 rank 3
2023-02-17 12:32:38,853 DEBUG TRAIN Batch 5/2700 loss 15.648356 loss_att 19.578339 loss_ctc 28.545532 loss_rnnt 12.952456 hw_loss 0.356776 lr 0.00075020 rank 0
2023-02-17 12:32:38,854 DEBUG TRAIN Batch 5/2700 loss 18.433817 loss_att 22.043686 loss_ctc 32.137321 loss_rnnt 15.687533 hw_loss 0.369707 lr 0.00075069 rank 4
2023-02-17 12:33:56,692 DEBUG TRAIN Batch 5/2800 loss 23.717297 loss_att 29.272177 loss_ctc 33.779224 loss_rnnt 21.086096 hw_loss 0.334939 lr 0.00074954 rank 3
2023-02-17 12:33:56,696 DEBUG TRAIN Batch 5/2800 loss 24.239838 loss_att 30.090710 loss_ctc 36.516155 loss_rnnt 21.250092 hw_loss 0.342616 lr 0.00074932 rank 1
2023-02-17 12:33:56,700 DEBUG TRAIN Batch 5/2800 loss 9.147556 loss_att 14.888811 loss_ctc 17.023869 loss_rnnt 6.783913 hw_loss 0.309783 lr 0.00074992 rank 6
2023-02-17 12:33:56,701 DEBUG TRAIN Batch 5/2800 loss 26.010046 loss_att 28.221926 loss_ctc 35.848633 loss_rnnt 24.065411 hw_loss 0.357088 lr 0.00074961 rank 7
2023-02-17 12:33:56,702 DEBUG TRAIN Batch 5/2800 loss 54.506050 loss_att 55.491489 loss_ctc 75.760452 loss_rnnt 51.233868 hw_loss 0.452203 lr 0.00074959 rank 5
2023-02-17 12:33:56,708 DEBUG TRAIN Batch 5/2800 loss 27.466272 loss_att 38.473480 loss_ctc 42.934235 loss_rnnt 22.987312 hw_loss 0.403352 lr 0.00074935 rank 0
2023-02-17 12:33:56,709 DEBUG TRAIN Batch 5/2800 loss 23.929701 loss_att 30.781422 loss_ctc 39.395626 loss_rnnt 20.333443 hw_loss 0.307109 lr 0.00074984 rank 4
2023-02-17 12:33:56,750 DEBUG TRAIN Batch 5/2800 loss 20.928877 loss_att 26.196632 loss_ctc 32.800369 loss_rnnt 18.092161 hw_loss 0.375561 lr 0.00074899 rank 2
2023-02-17 12:35:15,622 DEBUG TRAIN Batch 5/2900 loss 19.874943 loss_att 24.156277 loss_ctc 30.089748 loss_rnnt 17.478952 hw_loss 0.333281 lr 0.00074870 rank 3
2023-02-17 12:35:15,624 DEBUG TRAIN Batch 5/2900 loss 14.262097 loss_att 19.000027 loss_ctc 24.510914 loss_rnnt 11.734966 hw_loss 0.399441 lr 0.00074908 rank 6
2023-02-17 12:35:15,624 DEBUG TRAIN Batch 5/2900 loss 17.269314 loss_att 22.857594 loss_ctc 32.104713 loss_rnnt 13.937922 hw_loss 0.441904 lr 0.00074848 rank 1
2023-02-17 12:35:15,629 DEBUG TRAIN Batch 5/2900 loss 28.497095 loss_att 27.773241 loss_ctc 42.324684 loss_rnnt 26.644171 hw_loss 0.288781 lr 0.00074851 rank 0
2023-02-17 12:35:15,629 DEBUG TRAIN Batch 5/2900 loss 16.279547 loss_att 19.186762 loss_ctc 29.201458 loss_rnnt 13.777943 hw_loss 0.369825 lr 0.00074877 rank 7
2023-02-17 12:35:15,631 DEBUG TRAIN Batch 5/2900 loss 19.890680 loss_att 26.610851 loss_ctc 35.638260 loss_rnnt 16.265350 hw_loss 0.340535 lr 0.00074875 rank 5
2023-02-17 12:35:15,632 DEBUG TRAIN Batch 5/2900 loss 21.762836 loss_att 26.594543 loss_ctc 36.603695 loss_rnnt 18.648592 hw_loss 0.317103 lr 0.00074815 rank 2
2023-02-17 12:35:15,676 DEBUG TRAIN Batch 5/2900 loss 21.663084 loss_att 26.059101 loss_ctc 37.190647 loss_rnnt 18.529495 hw_loss 0.345081 lr 0.00074900 rank 4
2023-02-17 12:36:31,385 DEBUG TRAIN Batch 5/3000 loss 20.056345 loss_att 26.128117 loss_ctc 36.489014 loss_rnnt 16.469305 hw_loss 0.340618 lr 0.00074824 rank 6
2023-02-17 12:36:31,387 DEBUG TRAIN Batch 5/3000 loss 14.163558 loss_att 20.127121 loss_ctc 22.477026 loss_rnnt 11.649540 hw_loss 0.399081 lr 0.00074786 rank 3
2023-02-17 12:36:31,387 DEBUG TRAIN Batch 5/3000 loss 14.167200 loss_att 17.040092 loss_ctc 23.041447 loss_rnnt 12.222638 hw_loss 0.350155 lr 0.00074793 rank 7
2023-02-17 12:36:31,388 DEBUG TRAIN Batch 5/3000 loss 41.779633 loss_att 46.529095 loss_ctc 57.846386 loss_rnnt 38.519085 hw_loss 0.315804 lr 0.00074764 rank 1
2023-02-17 12:36:31,387 DEBUG TRAIN Batch 5/3000 loss 10.062293 loss_att 14.698462 loss_ctc 18.963242 loss_rnnt 7.775820 hw_loss 0.323336 lr 0.00074791 rank 5
2023-02-17 12:36:31,391 DEBUG TRAIN Batch 5/3000 loss 10.633117 loss_att 17.771645 loss_ctc 23.895941 loss_rnnt 7.264700 hw_loss 0.323127 lr 0.00074816 rank 4
2023-02-17 12:36:31,392 DEBUG TRAIN Batch 5/3000 loss 22.269089 loss_att 21.984930 loss_ctc 35.133213 loss_rnnt 20.392941 hw_loss 0.408304 lr 0.00074768 rank 0
2023-02-17 12:36:31,433 DEBUG TRAIN Batch 5/3000 loss 19.955332 loss_att 25.586681 loss_ctc 32.735855 loss_rnnt 16.924513 hw_loss 0.375900 lr 0.00074732 rank 2
2023-02-17 12:37:47,809 DEBUG TRAIN Batch 5/3100 loss 21.534582 loss_att 20.740364 loss_ctc 27.095076 loss_rnnt 20.724653 hw_loss 0.426324 lr 0.00074740 rank 6
2023-02-17 12:37:47,814 DEBUG TRAIN Batch 5/3100 loss 15.012531 loss_att 17.334236 loss_ctc 23.384771 loss_rnnt 13.237741 hw_loss 0.364029 lr 0.00074709 rank 7
2023-02-17 12:37:47,814 DEBUG TRAIN Batch 5/3100 loss 17.821966 loss_att 18.014479 loss_ctc 26.585205 loss_rnnt 16.474520 hw_loss 0.263458 lr 0.00074708 rank 5
2023-02-17 12:37:47,817 DEBUG TRAIN Batch 5/3100 loss 31.151638 loss_att 31.308388 loss_ctc 44.553688 loss_rnnt 29.153748 hw_loss 0.336752 lr 0.00074684 rank 0
2023-02-17 12:37:47,818 DEBUG TRAIN Batch 5/3100 loss 9.917789 loss_att 12.247976 loss_ctc 15.801612 loss_rnnt 8.473875 hw_loss 0.362563 lr 0.00074703 rank 3
2023-02-17 12:37:47,819 DEBUG TRAIN Batch 5/3100 loss 21.648066 loss_att 25.970524 loss_ctc 37.275822 loss_rnnt 18.524548 hw_loss 0.328736 lr 0.00074733 rank 4
2023-02-17 12:37:47,819 DEBUG TRAIN Batch 5/3100 loss 13.057968 loss_att 17.151295 loss_ctc 20.589172 loss_rnnt 11.032149 hw_loss 0.380612 lr 0.00074681 rank 1
2023-02-17 12:37:47,821 DEBUG TRAIN Batch 5/3100 loss 14.707981 loss_att 15.083855 loss_ctc 18.493378 loss_rnnt 13.917495 hw_loss 0.394861 lr 0.00074648 rank 2
2023-02-17 12:39:06,761 DEBUG TRAIN Batch 5/3200 loss 50.584976 loss_att 58.844284 loss_ctc 69.790627 loss_rnnt 46.183407 hw_loss 0.354291 lr 0.00074619 rank 3
2023-02-17 12:39:06,761 DEBUG TRAIN Batch 5/3200 loss 19.887569 loss_att 18.825815 loss_ctc 25.923855 loss_rnnt 19.086180 hw_loss 0.391695 lr 0.00074624 rank 5
2023-02-17 12:39:06,766 DEBUG TRAIN Batch 5/3200 loss 12.883312 loss_att 13.411430 loss_ctc 19.034582 loss_rnnt 11.802327 hw_loss 0.290984 lr 0.00074649 rank 4
2023-02-17 12:39:06,770 DEBUG TRAIN Batch 5/3200 loss 14.632389 loss_att 14.084656 loss_ctc 21.317617 loss_rnnt 13.565757 hw_loss 0.534030 lr 0.00074598 rank 1
2023-02-17 12:39:06,772 DEBUG TRAIN Batch 5/3200 loss 15.196318 loss_att 19.693714 loss_ctc 27.720047 loss_rnnt 12.457881 hw_loss 0.317110 lr 0.00074601 rank 0
2023-02-17 12:39:06,773 DEBUG TRAIN Batch 5/3200 loss 28.203829 loss_att 33.209999 loss_ctc 58.669415 loss_rnnt 23.003014 hw_loss 0.257815 lr 0.00074565 rank 2
2023-02-17 12:39:06,792 DEBUG TRAIN Batch 5/3200 loss 24.866829 loss_att 32.658726 loss_ctc 43.593811 loss_rnnt 20.624460 hw_loss 0.350731 lr 0.00074657 rank 6
2023-02-17 12:39:06,801 DEBUG TRAIN Batch 5/3200 loss 15.364162 loss_att 22.937271 loss_ctc 27.713966 loss_rnnt 12.001562 hw_loss 0.377510 lr 0.00074626 rank 7
2023-02-17 12:40:21,317 DEBUG TRAIN Batch 5/3300 loss 56.972958 loss_att 65.981255 loss_ctc 79.578964 loss_rnnt 51.968365 hw_loss 0.353995 lr 0.00074541 rank 5
2023-02-17 12:40:21,319 DEBUG TRAIN Batch 5/3300 loss 17.119541 loss_att 25.774685 loss_ctc 26.460684 loss_rnnt 13.922750 hw_loss 0.413016 lr 0.00074483 rank 2
2023-02-17 12:40:21,320 DEBUG TRAIN Batch 5/3300 loss 21.040304 loss_att 27.872356 loss_ctc 28.863918 loss_rnnt 18.430508 hw_loss 0.375449 lr 0.00074543 rank 7
2023-02-17 12:40:21,321 DEBUG TRAIN Batch 5/3300 loss 39.446255 loss_att 48.621819 loss_ctc 70.089325 loss_rnnt 33.326790 hw_loss 0.372393 lr 0.00074515 rank 1
2023-02-17 12:40:21,321 DEBUG TRAIN Batch 5/3300 loss 11.439258 loss_att 15.356874 loss_ctc 17.888269 loss_rnnt 9.661661 hw_loss 0.251634 lr 0.00074574 rank 6
2023-02-17 12:40:21,323 DEBUG TRAIN Batch 5/3300 loss 22.859940 loss_att 29.635710 loss_ctc 37.172817 loss_rnnt 19.379019 hw_loss 0.407597 lr 0.00074536 rank 3
2023-02-17 12:40:21,323 DEBUG TRAIN Batch 5/3300 loss 20.755341 loss_att 25.366756 loss_ctc 32.891296 loss_rnnt 18.021492 hw_loss 0.362698 lr 0.00074518 rank 0
2023-02-17 12:40:21,325 DEBUG TRAIN Batch 5/3300 loss 18.071238 loss_att 21.613050 loss_ctc 26.576460 loss_rnnt 15.986097 hw_loss 0.455156 lr 0.00074566 rank 4
2023-02-17 12:41:37,052 DEBUG TRAIN Batch 5/3400 loss 29.119448 loss_att 32.047531 loss_ctc 48.911785 loss_rnnt 25.705902 hw_loss 0.354285 lr 0.00074460 rank 7
2023-02-17 12:41:37,051 DEBUG TRAIN Batch 5/3400 loss 19.010498 loss_att 24.764568 loss_ctc 36.909607 loss_rnnt 15.318960 hw_loss 0.289079 lr 0.00074459 rank 5
2023-02-17 12:41:37,053 DEBUG TRAIN Batch 5/3400 loss 10.020715 loss_att 16.506573 loss_ctc 22.062872 loss_rnnt 6.953958 hw_loss 0.307433 lr 0.00074491 rank 6
2023-02-17 12:41:37,059 DEBUG TRAIN Batch 5/3400 loss 22.438881 loss_att 26.692530 loss_ctc 30.733185 loss_rnnt 20.298155 hw_loss 0.345168 lr 0.00074483 rank 4
2023-02-17 12:41:37,059 DEBUG TRAIN Batch 5/3400 loss 21.498442 loss_att 22.355785 loss_ctc 33.962631 loss_rnnt 19.502966 hw_loss 0.303965 lr 0.00074454 rank 3
2023-02-17 12:41:37,060 DEBUG TRAIN Batch 5/3400 loss 14.156382 loss_att 20.136806 loss_ctc 30.293495 loss_rnnt 10.594555 hw_loss 0.401487 lr 0.00074400 rank 2
2023-02-17 12:41:37,064 DEBUG TRAIN Batch 5/3400 loss 23.763783 loss_att 32.795929 loss_ctc 42.994102 loss_rnnt 19.168985 hw_loss 0.420606 lr 0.00074432 rank 1
2023-02-17 12:41:37,104 DEBUG TRAIN Batch 5/3400 loss 29.251513 loss_att 32.084671 loss_ctc 49.661247 loss_rnnt 25.788395 hw_loss 0.328473 lr 0.00074436 rank 0
2023-02-17 12:42:54,772 DEBUG TRAIN Batch 5/3500 loss 20.561617 loss_att 27.504337 loss_ctc 34.986824 loss_rnnt 17.084194 hw_loss 0.310346 lr 0.00074378 rank 7
2023-02-17 12:42:54,772 DEBUG TRAIN Batch 5/3500 loss 9.916322 loss_att 15.425264 loss_ctc 18.033234 loss_rnnt 7.570359 hw_loss 0.303600 lr 0.00074376 rank 5
2023-02-17 12:42:54,772 DEBUG TRAIN Batch 5/3500 loss 18.442810 loss_att 23.496948 loss_ctc 32.425854 loss_rnnt 15.401302 hw_loss 0.311766 lr 0.00074408 rank 6
2023-02-17 12:42:54,773 DEBUG TRAIN Batch 5/3500 loss 31.608145 loss_att 38.031509 loss_ctc 60.104256 loss_rnnt 26.385185 hw_loss 0.260259 lr 0.00074401 rank 4
2023-02-17 12:42:54,773 DEBUG TRAIN Batch 5/3500 loss 25.345448 loss_att 27.516346 loss_ctc 42.912079 loss_rnnt 22.384119 hw_loss 0.346745 lr 0.00074371 rank 3
2023-02-17 12:42:54,776 DEBUG TRAIN Batch 5/3500 loss 14.144408 loss_att 18.759537 loss_ctc 24.681490 loss_rnnt 11.660557 hw_loss 0.292277 lr 0.00074350 rank 1
2023-02-17 12:42:54,787 DEBUG TRAIN Batch 5/3500 loss 25.566601 loss_att 30.611076 loss_ctc 41.965912 loss_rnnt 22.221460 hw_loss 0.280631 lr 0.00074353 rank 0
2023-02-17 12:42:54,787 DEBUG TRAIN Batch 5/3500 loss 20.807957 loss_att 25.352875 loss_ctc 29.314159 loss_rnnt 18.573202 hw_loss 0.359272 lr 0.00074318 rank 2
2023-02-17 12:44:13,435 DEBUG TRAIN Batch 5/3600 loss 8.665759 loss_att 12.799032 loss_ctc 14.562367 loss_rnnt 6.884389 hw_loss 0.315938 lr 0.00074326 rank 6
2023-02-17 12:44:13,434 DEBUG TRAIN Batch 5/3600 loss 19.522497 loss_att 20.539625 loss_ctc 26.744997 loss_rnnt 18.154875 hw_loss 0.377246 lr 0.00074294 rank 5
2023-02-17 12:44:13,435 DEBUG TRAIN Batch 5/3600 loss 27.308187 loss_att 31.702682 loss_ctc 40.256989 loss_rnnt 24.528906 hw_loss 0.326017 lr 0.00074271 rank 0
2023-02-17 12:44:13,435 DEBUG TRAIN Batch 5/3600 loss 32.495338 loss_att 37.796589 loss_ctc 53.598766 loss_rnnt 28.459217 hw_loss 0.303898 lr 0.00074319 rank 4
2023-02-17 12:44:13,437 DEBUG TRAIN Batch 5/3600 loss 14.352012 loss_att 22.313755 loss_ctc 26.983463 loss_rnnt 10.930182 hw_loss 0.272415 lr 0.00074268 rank 1
2023-02-17 12:44:13,438 DEBUG TRAIN Batch 5/3600 loss 9.146132 loss_att 15.661944 loss_ctc 19.895357 loss_rnnt 6.231701 hw_loss 0.333820 lr 0.00074289 rank 3
2023-02-17 12:44:13,439 DEBUG TRAIN Batch 5/3600 loss 15.724876 loss_att 18.685652 loss_ctc 34.061783 loss_rnnt 12.535398 hw_loss 0.285753 lr 0.00074296 rank 7
2023-02-17 12:44:13,480 DEBUG TRAIN Batch 5/3600 loss 35.257484 loss_att 38.266022 loss_ctc 53.530518 loss_rnnt 32.073540 hw_loss 0.273439 lr 0.00074236 rank 2
2023-02-17 12:45:29,550 DEBUG TRAIN Batch 5/3700 loss 18.690229 loss_att 21.534258 loss_ctc 32.923744 loss_rnnt 16.004082 hw_loss 0.411640 lr 0.00074207 rank 3
2023-02-17 12:45:29,554 DEBUG TRAIN Batch 5/3700 loss 27.834463 loss_att 29.904243 loss_ctc 46.791801 loss_rnnt 24.699183 hw_loss 0.363143 lr 0.00074186 rank 1
2023-02-17 12:45:29,555 DEBUG TRAIN Batch 5/3700 loss 20.297943 loss_att 23.261108 loss_ctc 30.625677 loss_rnnt 18.118490 hw_loss 0.393354 lr 0.00074214 rank 7
2023-02-17 12:45:29,558 DEBUG TRAIN Batch 5/3700 loss 22.315956 loss_att 26.651333 loss_ctc 34.697227 loss_rnnt 19.584141 hw_loss 0.401068 lr 0.00074212 rank 5
2023-02-17 12:45:29,559 DEBUG TRAIN Batch 5/3700 loss 17.155081 loss_att 20.510401 loss_ctc 27.571518 loss_rnnt 14.921991 hw_loss 0.324688 lr 0.00074244 rank 6
2023-02-17 12:45:29,562 DEBUG TRAIN Batch 5/3700 loss 29.132357 loss_att 34.343124 loss_ctc 38.496925 loss_rnnt 26.643707 hw_loss 0.371036 lr 0.00074154 rank 2
2023-02-17 12:45:29,564 DEBUG TRAIN Batch 5/3700 loss 10.531631 loss_att 13.218672 loss_ctc 19.251499 loss_rnnt 8.631200 hw_loss 0.375699 lr 0.00074189 rank 0
2023-02-17 12:45:29,602 DEBUG TRAIN Batch 5/3700 loss 22.105846 loss_att 24.766300 loss_ctc 37.373390 loss_rnnt 19.323839 hw_loss 0.401704 lr 0.00074237 rank 4
2023-02-17 12:46:45,575 DEBUG TRAIN Batch 5/3800 loss 12.545919 loss_att 14.339499 loss_ctc 19.710438 loss_rnnt 11.009046 hw_loss 0.417917 lr 0.00074105 rank 1
2023-02-17 12:46:45,577 DEBUG TRAIN Batch 5/3800 loss 7.286644 loss_att 12.703558 loss_ctc 11.227038 loss_rnnt 5.531239 hw_loss 0.274945 lr 0.00074126 rank 3
2023-02-17 12:46:45,578 DEBUG TRAIN Batch 5/3800 loss 21.555569 loss_att 28.142971 loss_ctc 36.176590 loss_rnnt 18.106655 hw_loss 0.341176 lr 0.00074132 rank 7
2023-02-17 12:46:45,578 DEBUG TRAIN Batch 5/3800 loss 12.026719 loss_att 12.430576 loss_ctc 16.176735 loss_rnnt 11.174981 hw_loss 0.408057 lr 0.00074162 rank 6
2023-02-17 12:46:45,580 DEBUG TRAIN Batch 5/3800 loss 29.644365 loss_att 34.240128 loss_ctc 42.997437 loss_rnnt 26.788040 hw_loss 0.293930 lr 0.00074073 rank 2
2023-02-17 12:46:45,581 DEBUG TRAIN Batch 5/3800 loss 15.948853 loss_att 18.651003 loss_ctc 26.292637 loss_rnnt 13.861270 hw_loss 0.314965 lr 0.00074108 rank 0
2023-02-17 12:46:45,583 DEBUG TRAIN Batch 5/3800 loss 13.106816 loss_att 15.229022 loss_ctc 22.211828 loss_rnnt 11.210871 hw_loss 0.482816 lr 0.00074131 rank 5
2023-02-17 12:46:45,582 DEBUG TRAIN Batch 5/3800 loss 16.094595 loss_att 22.334480 loss_ctc 29.321573 loss_rnnt 12.906690 hw_loss 0.330621 lr 0.00074155 rank 4
2023-02-17 12:48:04,229 DEBUG TRAIN Batch 5/3900 loss 12.615665 loss_att 13.250130 loss_ctc 19.500572 loss_rnnt 11.412983 hw_loss 0.295881 lr 0.00074049 rank 5
2023-02-17 12:48:04,232 DEBUG TRAIN Batch 5/3900 loss 47.495827 loss_att 48.650742 loss_ctc 69.203026 loss_rnnt 44.175201 hw_loss 0.366274 lr 0.00074027 rank 0
2023-02-17 12:48:04,235 DEBUG TRAIN Batch 5/3900 loss 22.345211 loss_att 26.932655 loss_ctc 39.107903 loss_rnnt 19.010612 hw_loss 0.341411 lr 0.00074081 rank 6
2023-02-17 12:48:04,236 DEBUG TRAIN Batch 5/3900 loss 13.646345 loss_att 14.861978 loss_ctc 21.850574 loss_rnnt 12.039310 hw_loss 0.506271 lr 0.00074074 rank 4
2023-02-17 12:48:04,235 DEBUG TRAIN Batch 5/3900 loss 16.151167 loss_att 24.512501 loss_ctc 28.700676 loss_rnnt 12.572210 hw_loss 0.437666 lr 0.00074023 rank 1
2023-02-17 12:48:04,239 DEBUG TRAIN Batch 5/3900 loss 23.491732 loss_att 27.306311 loss_ctc 36.899040 loss_rnnt 20.816975 hw_loss 0.232874 lr 0.00074051 rank 7
2023-02-17 12:48:04,239 DEBUG TRAIN Batch 5/3900 loss 41.703781 loss_att 41.525356 loss_ctc 61.186424 loss_rnnt 38.916817 hw_loss 0.421807 lr 0.00073992 rank 2
2023-02-17 12:48:04,284 DEBUG TRAIN Batch 5/3900 loss 26.234642 loss_att 30.292511 loss_ctc 40.591255 loss_rnnt 23.346294 hw_loss 0.304797 lr 0.00074044 rank 3
2023-02-17 12:49:21,684 DEBUG TRAIN Batch 5/4000 loss 45.029125 loss_att 52.515694 loss_ctc 77.657845 loss_rnnt 38.990974 hw_loss 0.356891 lr 0.00074000 rank 6
2023-02-17 12:49:21,686 DEBUG TRAIN Batch 5/4000 loss 22.031775 loss_att 23.884861 loss_ctc 33.794189 loss_rnnt 19.899456 hw_loss 0.362587 lr 0.00073963 rank 3
2023-02-17 12:49:21,686 DEBUG TRAIN Batch 5/4000 loss 23.255051 loss_att 28.956484 loss_ctc 33.948219 loss_rnnt 20.478214 hw_loss 0.395237 lr 0.00073942 rank 1
2023-02-17 12:49:21,689 DEBUG TRAIN Batch 5/4000 loss 27.262119 loss_att 39.846767 loss_ctc 52.413952 loss_rnnt 21.254335 hw_loss 0.257388 lr 0.00073968 rank 5
2023-02-17 12:49:21,689 DEBUG TRAIN Batch 5/4000 loss 24.534395 loss_att 27.978409 loss_ctc 42.499199 loss_rnnt 21.294189 hw_loss 0.292677 lr 0.00073970 rank 7
2023-02-17 12:49:21,690 DEBUG TRAIN Batch 5/4000 loss 20.875929 loss_att 25.054531 loss_ctc 39.131981 loss_rnnt 17.451336 hw_loss 0.290121 lr 0.00073946 rank 0
2023-02-17 12:49:21,693 DEBUG TRAIN Batch 5/4000 loss 14.910125 loss_att 19.480350 loss_ctc 27.031866 loss_rnnt 12.206818 hw_loss 0.324431 lr 0.00073911 rank 2
2023-02-17 12:49:21,732 DEBUG TRAIN Batch 5/4000 loss 15.697007 loss_att 18.200424 loss_ctc 26.091085 loss_rnnt 13.601267 hw_loss 0.392210 lr 0.00073993 rank 4
2023-02-17 12:50:37,621 DEBUG TRAIN Batch 5/4100 loss 24.968292 loss_att 26.788084 loss_ctc 38.890934 loss_rnnt 22.605593 hw_loss 0.266976 lr 0.00073919 rank 6
2023-02-17 12:50:37,625 DEBUG TRAIN Batch 5/4100 loss 22.253963 loss_att 27.928867 loss_ctc 40.201817 loss_rnnt 18.486927 hw_loss 0.448141 lr 0.00073883 rank 3
2023-02-17 12:50:37,629 DEBUG TRAIN Batch 5/4100 loss 9.394856 loss_att 18.393875 loss_ctc 18.222668 loss_rnnt 6.247737 hw_loss 0.319264 lr 0.00073862 rank 1
2023-02-17 12:50:37,630 DEBUG TRAIN Batch 5/4100 loss 16.420492 loss_att 21.330534 loss_ctc 29.791834 loss_rnnt 13.466651 hw_loss 0.354350 lr 0.00073889 rank 7
2023-02-17 12:50:37,630 DEBUG TRAIN Batch 5/4100 loss 17.428642 loss_att 22.613169 loss_ctc 29.795000 loss_rnnt 14.550018 hw_loss 0.361633 lr 0.00073887 rank 5
2023-02-17 12:50:37,631 DEBUG TRAIN Batch 5/4100 loss 39.553200 loss_att 38.786682 loss_ctc 56.403282 loss_rnnt 37.310360 hw_loss 0.280251 lr 0.00073830 rank 2
2023-02-17 12:50:37,632 DEBUG TRAIN Batch 5/4100 loss 28.407839 loss_att 31.739038 loss_ctc 42.631905 loss_rnnt 25.691822 hw_loss 0.287308 lr 0.00073912 rank 4
2023-02-17 12:50:37,633 DEBUG TRAIN Batch 5/4100 loss 14.778452 loss_att 21.470057 loss_ctc 34.027763 loss_rnnt 10.731899 hw_loss 0.265606 lr 0.00073865 rank 0
2023-02-17 12:51:54,812 DEBUG TRAIN Batch 5/4200 loss 14.269271 loss_att 17.256710 loss_ctc 19.981655 loss_rnnt 12.736647 hw_loss 0.325285 lr 0.00073838 rank 6
2023-02-17 12:51:54,814 DEBUG TRAIN Batch 5/4200 loss 9.101532 loss_att 15.692819 loss_ctc 19.475079 loss_rnnt 6.252052 hw_loss 0.277658 lr 0.00073831 rank 4
2023-02-17 12:51:54,816 DEBUG TRAIN Batch 5/4200 loss 16.668545 loss_att 21.747246 loss_ctc 27.675087 loss_rnnt 13.969742 hw_loss 0.404103 lr 0.00073808 rank 7
2023-02-17 12:51:54,819 DEBUG TRAIN Batch 5/4200 loss 18.468472 loss_att 19.103479 loss_ctc 25.128225 loss_rnnt 17.257992 hw_loss 0.366585 lr 0.00073781 rank 1
2023-02-17 12:51:54,820 DEBUG TRAIN Batch 5/4200 loss 34.259342 loss_att 37.774658 loss_ctc 52.288063 loss_rnnt 31.030418 hw_loss 0.228810 lr 0.00073784 rank 0
2023-02-17 12:51:54,821 DEBUG TRAIN Batch 5/4200 loss 21.539600 loss_att 26.761818 loss_ctc 38.648712 loss_rnnt 18.097076 hw_loss 0.219123 lr 0.00073750 rank 2
2023-02-17 12:51:54,822 DEBUG TRAIN Batch 5/4200 loss 18.737322 loss_att 22.465643 loss_ctc 32.632416 loss_rnnt 15.966732 hw_loss 0.322962 lr 0.00073802 rank 3
2023-02-17 12:51:54,822 DEBUG TRAIN Batch 5/4200 loss 19.386303 loss_att 23.867552 loss_ctc 30.984053 loss_rnnt 16.806267 hw_loss 0.257660 lr 0.00073807 rank 5
2023-02-17 12:53:14,199 DEBUG TRAIN Batch 5/4300 loss 9.449708 loss_att 15.547319 loss_ctc 22.217548 loss_rnnt 6.374641 hw_loss 0.287188 lr 0.00073751 rank 4
2023-02-17 12:53:14,199 DEBUG TRAIN Batch 5/4300 loss 16.388876 loss_att 22.817722 loss_ctc 35.073132 loss_rnnt 12.401645 hw_loss 0.394177 lr 0.00073758 rank 6
2023-02-17 12:53:14,201 DEBUG TRAIN Batch 5/4300 loss 16.501385 loss_att 21.510172 loss_ctc 31.279499 loss_rnnt 13.402746 hw_loss 0.237125 lr 0.00073727 rank 5
2023-02-17 12:53:14,204 DEBUG TRAIN Batch 5/4300 loss 29.183838 loss_att 29.999924 loss_ctc 44.846664 loss_rnnt 26.706320 hw_loss 0.423606 lr 0.00073722 rank 3
2023-02-17 12:53:14,205 DEBUG TRAIN Batch 5/4300 loss 20.834621 loss_att 24.582550 loss_ctc 40.112457 loss_rnnt 17.347538 hw_loss 0.313348 lr 0.00073728 rank 7
2023-02-17 12:53:14,206 DEBUG TRAIN Batch 5/4300 loss 28.110413 loss_att 34.118687 loss_ctc 41.517784 loss_rnnt 24.930237 hw_loss 0.357883 lr 0.00073701 rank 1
2023-02-17 12:53:14,207 DEBUG TRAIN Batch 5/4300 loss 27.866354 loss_att 30.814850 loss_ctc 43.597229 loss_rnnt 24.994801 hw_loss 0.345758 lr 0.00073704 rank 0
2023-02-17 12:53:14,246 DEBUG TRAIN Batch 5/4300 loss 30.454735 loss_att 34.703049 loss_ctc 48.669613 loss_rnnt 26.982019 hw_loss 0.364500 lr 0.00073670 rank 2
2023-02-17 12:54:29,461 DEBUG TRAIN Batch 5/4400 loss 11.766857 loss_att 13.051401 loss_ctc 16.904198 loss_rnnt 10.625881 hw_loss 0.373291 lr 0.00073590 rank 2
2023-02-17 12:54:29,461 DEBUG TRAIN Batch 5/4400 loss 11.446054 loss_att 14.189218 loss_ctc 20.656616 loss_rnnt 9.493271 hw_loss 0.330139 lr 0.00073621 rank 1
2023-02-17 12:54:29,462 DEBUG TRAIN Batch 5/4400 loss 29.914108 loss_att 33.448082 loss_ctc 50.509010 loss_rnnt 26.288055 hw_loss 0.324885 lr 0.00073671 rank 4
2023-02-17 12:54:29,464 DEBUG TRAIN Batch 5/4400 loss 22.229712 loss_att 27.264420 loss_ctc 42.800381 loss_rnnt 18.356110 hw_loss 0.232319 lr 0.00073642 rank 3
2023-02-17 12:54:29,465 DEBUG TRAIN Batch 5/4400 loss 17.845829 loss_att 18.516733 loss_ctc 26.773739 loss_rnnt 16.303814 hw_loss 0.407709 lr 0.00073648 rank 7
2023-02-17 12:54:29,465 DEBUG TRAIN Batch 5/4400 loss 19.915598 loss_att 22.046869 loss_ctc 27.566952 loss_rnnt 18.229557 hw_loss 0.449261 lr 0.00073678 rank 6
2023-02-17 12:54:29,465 DEBUG TRAIN Batch 5/4400 loss 21.850216 loss_att 21.338722 loss_ctc 33.057732 loss_rnnt 20.264309 hw_loss 0.363503 lr 0.00073624 rank 0
2023-02-17 12:54:29,466 DEBUG TRAIN Batch 5/4400 loss 23.618921 loss_att 28.606785 loss_ctc 45.367325 loss_rnnt 19.488142 hw_loss 0.437662 lr 0.00073647 rank 5
2023-02-17 12:55:44,929 DEBUG TRAIN Batch 5/4500 loss 12.881539 loss_att 17.434734 loss_ctc 26.763746 loss_rnnt 9.925779 hw_loss 0.364049 lr 0.00073598 rank 6
2023-02-17 12:55:44,931 DEBUG TRAIN Batch 5/4500 loss 23.490463 loss_att 28.523609 loss_ctc 38.317032 loss_rnnt 20.335672 hw_loss 0.321157 lr 0.00073567 rank 5
2023-02-17 12:55:44,932 DEBUG TRAIN Batch 5/4500 loss 16.928562 loss_att 18.918032 loss_ctc 24.200397 loss_rnnt 15.359557 hw_loss 0.377876 lr 0.00073591 rank 4
2023-02-17 12:55:44,932 DEBUG TRAIN Batch 5/4500 loss 16.930271 loss_att 20.654762 loss_ctc 29.355145 loss_rnnt 14.369459 hw_loss 0.298621 lr 0.00073541 rank 1
2023-02-17 12:55:44,934 DEBUG TRAIN Batch 5/4500 loss 11.591334 loss_att 19.114582 loss_ctc 19.572353 loss_rnnt 8.910080 hw_loss 0.210878 lr 0.00073562 rank 3
2023-02-17 12:55:44,936 DEBUG TRAIN Batch 5/4500 loss 17.606655 loss_att 24.778528 loss_ctc 26.008352 loss_rnnt 14.875740 hw_loss 0.330587 lr 0.00073568 rank 7
2023-02-17 12:55:44,936 DEBUG TRAIN Batch 5/4500 loss 15.611858 loss_att 19.419880 loss_ctc 25.416029 loss_rnnt 13.361212 hw_loss 0.340913 lr 0.00073545 rank 0
2023-02-17 12:55:44,984 DEBUG TRAIN Batch 5/4500 loss 26.553627 loss_att 27.665565 loss_ctc 39.885109 loss_rnnt 24.321569 hw_loss 0.435264 lr 0.00073510 rank 2
2023-02-17 12:57:04,036 DEBUG TRAIN Batch 5/4600 loss 13.139573 loss_att 15.795365 loss_ctc 21.224045 loss_rnnt 11.357178 hw_loss 0.324954 lr 0.00073489 rank 7
2023-02-17 12:57:04,037 DEBUG TRAIN Batch 5/4600 loss 7.401692 loss_att 11.119065 loss_ctc 12.746071 loss_rnnt 5.771895 hw_loss 0.325760 lr 0.00073487 rank 5
2023-02-17 12:57:04,041 DEBUG TRAIN Batch 5/4600 loss 33.867340 loss_att 36.500538 loss_ctc 43.228600 loss_rnnt 31.926445 hw_loss 0.311414 lr 0.00073462 rank 1
2023-02-17 12:57:04,041 DEBUG TRAIN Batch 5/4600 loss 18.369165 loss_att 23.894943 loss_ctc 31.120743 loss_rnnt 15.343318 hw_loss 0.413402 lr 0.00073518 rank 6
2023-02-17 12:57:04,042 DEBUG TRAIN Batch 5/4600 loss 19.696684 loss_att 26.810827 loss_ctc 42.195297 loss_rnnt 15.124846 hw_loss 0.279743 lr 0.00073483 rank 3
2023-02-17 12:57:04,045 DEBUG TRAIN Batch 5/4600 loss 20.953495 loss_att 27.471802 loss_ctc 40.239853 loss_rnnt 16.950878 hw_loss 0.238953 lr 0.00073511 rank 4
2023-02-17 12:57:04,047 DEBUG TRAIN Batch 5/4600 loss 39.993027 loss_att 44.019859 loss_ctc 64.024902 loss_rnnt 35.735344 hw_loss 0.465124 lr 0.00073431 rank 2
2023-02-17 12:57:04,053 DEBUG TRAIN Batch 5/4600 loss 21.065590 loss_att 24.083307 loss_ctc 28.732443 loss_rnnt 19.309193 hw_loss 0.244890 lr 0.00073465 rank 0
2023-02-17 12:58:20,829 DEBUG TRAIN Batch 5/4700 loss 29.992224 loss_att 34.318520 loss_ctc 43.704376 loss_rnnt 27.129421 hw_loss 0.317361 lr 0.00073383 rank 1
2023-02-17 12:58:20,832 DEBUG TRAIN Batch 5/4700 loss 18.162718 loss_att 24.427090 loss_ctc 29.969246 loss_rnnt 15.133776 hw_loss 0.378492 lr 0.00073403 rank 3
2023-02-17 12:58:20,833 DEBUG TRAIN Batch 5/4700 loss 17.711283 loss_att 19.822323 loss_ctc 31.586302 loss_rnnt 15.246465 hw_loss 0.361136 lr 0.00073439 rank 6
2023-02-17 12:58:20,836 DEBUG TRAIN Batch 5/4700 loss 14.552303 loss_att 20.101822 loss_ctc 22.723873 loss_rnnt 12.151638 hw_loss 0.377285 lr 0.00073410 rank 7
2023-02-17 12:58:20,836 DEBUG TRAIN Batch 5/4700 loss 12.424595 loss_att 17.095757 loss_ctc 19.559170 loss_rnnt 10.361990 hw_loss 0.332054 lr 0.00073408 rank 5
2023-02-17 12:58:20,838 DEBUG TRAIN Batch 5/4700 loss 23.400274 loss_att 26.338192 loss_ctc 39.657188 loss_rnnt 20.406723 hw_loss 0.446959 lr 0.00073386 rank 0
2023-02-17 12:58:20,839 DEBUG TRAIN Batch 5/4700 loss 11.030425 loss_att 15.703748 loss_ctc 17.709187 loss_rnnt 9.020523 hw_loss 0.346380 lr 0.00073432 rank 4
2023-02-17 12:58:20,844 DEBUG TRAIN Batch 5/4700 loss 25.736807 loss_att 31.660912 loss_ctc 49.360073 loss_rnnt 21.201839 hw_loss 0.375704 lr 0.00073352 rank 2
2023-02-17 12:59:36,394 DEBUG TRAIN Batch 5/4800 loss 22.944090 loss_att 30.001663 loss_ctc 39.702187 loss_rnnt 19.093344 hw_loss 0.384035 lr 0.00073329 rank 5
2023-02-17 12:59:36,395 DEBUG TRAIN Batch 5/4800 loss 23.449610 loss_att 32.056419 loss_ctc 38.854866 loss_rnnt 19.514166 hw_loss 0.300088 lr 0.00073360 rank 6
2023-02-17 12:59:36,399 DEBUG TRAIN Batch 5/4800 loss 34.592674 loss_att 38.308681 loss_ctc 57.937786 loss_rnnt 30.549397 hw_loss 0.351358 lr 0.00073331 rank 7
2023-02-17 12:59:36,399 DEBUG TRAIN Batch 5/4800 loss 37.003410 loss_att 42.438484 loss_ctc 58.992825 loss_rnnt 32.782936 hw_loss 0.377885 lr 0.00073353 rank 4
2023-02-17 12:59:36,399 DEBUG TRAIN Batch 5/4800 loss 15.476313 loss_att 20.127281 loss_ctc 24.729736 loss_rnnt 13.150079 hw_loss 0.304217 lr 0.00073304 rank 1
2023-02-17 12:59:36,400 DEBUG TRAIN Batch 5/4800 loss 25.797878 loss_att 31.984425 loss_ctc 39.135391 loss_rnnt 22.626144 hw_loss 0.292665 lr 0.00073273 rank 2
2023-02-17 12:59:36,400 DEBUG TRAIN Batch 5/4800 loss 24.133953 loss_att 32.699497 loss_ctc 38.589031 loss_rnnt 20.354263 hw_loss 0.261068 lr 0.00073324 rank 3
2023-02-17 12:59:36,402 DEBUG TRAIN Batch 5/4800 loss 16.984428 loss_att 26.416267 loss_ctc 32.721817 loss_rnnt 12.859808 hw_loss 0.262381 lr 0.00073307 rank 0
2023-02-17 13:00:54,162 DEBUG TRAIN Batch 5/4900 loss 15.620240 loss_att 17.961733 loss_ctc 28.206047 loss_rnnt 13.266139 hw_loss 0.389431 lr 0.00073195 rank 2
2023-02-17 13:00:54,164 DEBUG TRAIN Batch 5/4900 loss 21.405363 loss_att 22.945324 loss_ctc 29.651312 loss_rnnt 19.807072 hw_loss 0.357824 lr 0.00073252 rank 7
2023-02-17 13:00:54,164 DEBUG TRAIN Batch 5/4900 loss 16.899334 loss_att 22.389753 loss_ctc 35.517555 loss_rnnt 13.173867 hw_loss 0.271788 lr 0.00073225 rank 1
2023-02-17 13:00:54,164 DEBUG TRAIN Batch 5/4900 loss 9.144614 loss_att 14.163298 loss_ctc 13.592982 loss_rnnt 7.342878 hw_loss 0.384157 lr 0.00073281 rank 6
2023-02-17 13:00:54,165 DEBUG TRAIN Batch 5/4900 loss 15.799970 loss_att 18.746429 loss_ctc 26.595913 loss_rnnt 13.645695 hw_loss 0.235354 lr 0.00073250 rank 5
2023-02-17 13:00:54,166 DEBUG TRAIN Batch 5/4900 loss 23.629505 loss_att 29.660328 loss_ctc 37.976318 loss_rnnt 20.319899 hw_loss 0.357253 lr 0.00073274 rank 4
2023-02-17 13:00:54,167 DEBUG TRAIN Batch 5/4900 loss 19.794514 loss_att 24.332581 loss_ctc 31.864485 loss_rnnt 17.099421 hw_loss 0.334028 lr 0.00073246 rank 3
2023-02-17 13:00:54,173 DEBUG TRAIN Batch 5/4900 loss 20.776920 loss_att 24.226608 loss_ctc 28.586754 loss_rnnt 18.842815 hw_loss 0.380354 lr 0.00073228 rank 0
2023-02-17 13:02:13,780 DEBUG TRAIN Batch 5/5000 loss 13.729906 loss_att 17.314978 loss_ctc 20.529541 loss_rnnt 11.940462 hw_loss 0.310897 lr 0.00073173 rank 7
2023-02-17 13:02:13,785 DEBUG TRAIN Batch 5/5000 loss 10.985780 loss_att 13.796892 loss_ctc 17.091843 loss_rnnt 9.375287 hw_loss 0.438987 lr 0.00073167 rank 3
2023-02-17 13:02:13,784 DEBUG TRAIN Batch 5/5000 loss 8.668655 loss_att 14.259955 loss_ctc 17.038305 loss_rnnt 6.198762 hw_loss 0.441899 lr 0.00073150 rank 0
2023-02-17 13:02:13,785 DEBUG TRAIN Batch 5/5000 loss 19.124025 loss_att 22.807079 loss_ctc 33.642799 loss_rnnt 16.242590 hw_loss 0.391853 lr 0.00073172 rank 5
2023-02-17 13:02:13,785 DEBUG TRAIN Batch 5/5000 loss 20.300173 loss_att 22.162989 loss_ctc 28.310898 loss_rnnt 18.650616 hw_loss 0.391684 lr 0.00073116 rank 2
2023-02-17 13:02:13,786 DEBUG TRAIN Batch 5/5000 loss 18.888620 loss_att 25.392265 loss_ctc 29.147949 loss_rnnt 16.047365 hw_loss 0.323654 lr 0.00073147 rank 1
2023-02-17 13:02:13,787 DEBUG TRAIN Batch 5/5000 loss 11.978849 loss_att 15.838677 loss_ctc 22.022594 loss_rnnt 9.693977 hw_loss 0.325763 lr 0.00073202 rank 6
2023-02-17 13:02:13,793 DEBUG TRAIN Batch 5/5000 loss 22.701910 loss_att 27.372192 loss_ctc 39.317951 loss_rnnt 19.381536 hw_loss 0.320336 lr 0.00073195 rank 4
2023-02-17 13:03:30,272 DEBUG TRAIN Batch 5/5100 loss 18.236792 loss_att 17.587633 loss_ctc 27.723227 loss_rnnt 16.849529 hw_loss 0.472943 lr 0.00073069 rank 1
2023-02-17 13:03:30,272 DEBUG TRAIN Batch 5/5100 loss 18.754587 loss_att 21.698442 loss_ctc 31.586237 loss_rnnt 16.256208 hw_loss 0.372608 lr 0.00073117 rank 4
2023-02-17 13:03:30,275 DEBUG TRAIN Batch 5/5100 loss 8.847725 loss_att 14.032759 loss_ctc 18.745848 loss_rnnt 6.306135 hw_loss 0.346562 lr 0.00073089 rank 3
2023-02-17 13:03:30,276 DEBUG TRAIN Batch 5/5100 loss 14.981959 loss_att 16.619036 loss_ctc 22.919775 loss_rnnt 13.306792 hw_loss 0.542582 lr 0.00073072 rank 0
2023-02-17 13:03:30,277 DEBUG TRAIN Batch 5/5100 loss 14.074366 loss_att 19.215181 loss_ctc 25.105742 loss_rnnt 11.405963 hw_loss 0.317604 lr 0.00073124 rank 6
2023-02-17 13:03:30,278 DEBUG TRAIN Batch 5/5100 loss 17.573242 loss_att 24.786993 loss_ctc 30.340599 loss_rnnt 14.290520 hw_loss 0.258110 lr 0.00073095 rank 7
2023-02-17 13:03:30,278 DEBUG TRAIN Batch 5/5100 loss 14.004254 loss_att 18.710606 loss_ctc 22.274740 loss_rnnt 11.795799 hw_loss 0.308349 lr 0.00073094 rank 5
2023-02-17 13:03:30,319 DEBUG TRAIN Batch 5/5100 loss 16.672142 loss_att 25.353798 loss_ctc 37.668003 loss_rnnt 11.945991 hw_loss 0.356946 lr 0.00073038 rank 2
2023-02-17 13:04:46,515 DEBUG TRAIN Batch 5/5200 loss 13.714217 loss_att 14.915831 loss_ctc 19.280750 loss_rnnt 12.499503 hw_loss 0.435350 lr 0.00073039 rank 4
2023-02-17 13:04:46,517 DEBUG TRAIN Batch 5/5200 loss 18.143421 loss_att 24.532852 loss_ctc 32.653019 loss_rnnt 14.779582 hw_loss 0.283762 lr 0.00073011 rank 3
2023-02-17 13:04:46,520 DEBUG TRAIN Batch 5/5200 loss 14.060873 loss_att 16.665546 loss_ctc 19.986036 loss_rnnt 12.557053 hw_loss 0.361620 lr 0.00073046 rank 6
2023-02-17 13:04:46,520 DEBUG TRAIN Batch 5/5200 loss 16.387159 loss_att 19.538418 loss_ctc 24.514549 loss_rnnt 14.528956 hw_loss 0.270560 lr 0.00073017 rank 7
2023-02-17 13:04:46,521 DEBUG TRAIN Batch 5/5200 loss 19.806364 loss_att 26.883110 loss_ctc 34.420639 loss_rnnt 16.280449 hw_loss 0.303739 lr 0.00073016 rank 5
2023-02-17 13:04:46,523 DEBUG TRAIN Batch 5/5200 loss 5.048172 loss_att 8.554744 loss_ctc 9.042665 loss_rnnt 3.571292 hw_loss 0.455563 lr 0.00072991 rank 1
2023-02-17 13:04:46,524 DEBUG TRAIN Batch 5/5200 loss 22.052355 loss_att 27.277187 loss_ctc 41.126167 loss_rnnt 18.276737 hw_loss 0.351517 lr 0.00072994 rank 0
2023-02-17 13:04:46,568 DEBUG TRAIN Batch 5/5200 loss 14.024784 loss_att 19.649776 loss_ctc 24.526243 loss_rnnt 11.314535 hw_loss 0.346980 lr 0.00072960 rank 2
2023-02-17 13:06:05,485 DEBUG TRAIN Batch 5/5300 loss 26.424858 loss_att 34.579056 loss_ctc 44.730309 loss_rnnt 22.171261 hw_loss 0.341313 lr 0.00072883 rank 2
2023-02-17 13:06:05,485 DEBUG TRAIN Batch 5/5300 loss 33.254944 loss_att 40.764477 loss_ctc 50.585396 loss_rnnt 29.270933 hw_loss 0.321330 lr 0.00072968 rank 6
2023-02-17 13:06:05,485 DEBUG TRAIN Batch 5/5300 loss 36.326458 loss_att 43.000923 loss_ctc 54.346279 loss_rnnt 32.458633 hw_loss 0.244289 lr 0.00072938 rank 5
2023-02-17 13:06:05,487 DEBUG TRAIN Batch 5/5300 loss 19.781231 loss_att 29.218796 loss_ctc 33.687328 loss_rnnt 15.846198 hw_loss 0.362571 lr 0.00072961 rank 4
2023-02-17 13:06:05,489 DEBUG TRAIN Batch 5/5300 loss 9.561183 loss_att 15.199959 loss_ctc 18.118731 loss_rnnt 7.086365 hw_loss 0.386354 lr 0.00072913 rank 1
2023-02-17 13:06:05,489 DEBUG TRAIN Batch 5/5300 loss 22.114065 loss_att 23.864841 loss_ctc 38.011749 loss_rnnt 19.444519 hw_loss 0.374438 lr 0.00072933 rank 3
2023-02-17 13:06:05,490 DEBUG TRAIN Batch 5/5300 loss 21.408651 loss_att 27.053946 loss_ctc 36.903057 loss_rnnt 17.980989 hw_loss 0.436278 lr 0.00072939 rank 7
2023-02-17 13:06:05,491 DEBUG TRAIN Batch 5/5300 loss 9.986734 loss_att 14.104162 loss_ctc 18.354887 loss_rnnt 7.890703 hw_loss 0.293985 lr 0.00072916 rank 0
2023-02-17 13:07:23,282 DEBUG TRAIN Batch 5/5400 loss 37.871494 loss_att 38.494987 loss_ctc 57.917892 loss_rnnt 34.880386 hw_loss 0.362918 lr 0.00072860 rank 5
2023-02-17 13:07:23,283 DEBUG TRAIN Batch 5/5400 loss 21.602650 loss_att 22.671103 loss_ctc 32.077515 loss_rnnt 19.786757 hw_loss 0.385411 lr 0.00072862 rank 7
2023-02-17 13:07:23,285 DEBUG TRAIN Batch 5/5400 loss 25.279009 loss_att 30.529373 loss_ctc 40.755203 loss_rnnt 21.997309 hw_loss 0.315253 lr 0.00072836 rank 1
2023-02-17 13:07:23,287 DEBUG TRAIN Batch 5/5400 loss 20.524885 loss_att 26.325211 loss_ctc 36.003139 loss_rnnt 17.101299 hw_loss 0.374534 lr 0.00072839 rank 0
2023-02-17 13:07:23,287 DEBUG TRAIN Batch 5/5400 loss 25.994501 loss_att 31.794680 loss_ctc 36.197468 loss_rnnt 23.344793 hw_loss 0.242390 lr 0.00072891 rank 6
2023-02-17 13:07:23,289 DEBUG TRAIN Batch 5/5400 loss 17.342314 loss_att 20.760443 loss_ctc 34.341042 loss_rnnt 14.236930 hw_loss 0.291112 lr 0.00072884 rank 4
2023-02-17 13:07:23,292 DEBUG TRAIN Batch 5/5400 loss 13.733099 loss_att 20.612251 loss_ctc 24.511356 loss_rnnt 10.734461 hw_loss 0.348198 lr 0.00072856 rank 3
2023-02-17 13:07:23,339 DEBUG TRAIN Batch 5/5400 loss 15.561085 loss_att 22.546711 loss_ctc 27.407734 loss_rnnt 12.432878 hw_loss 0.284115 lr 0.00072806 rank 2
2023-02-17 13:08:39,079 DEBUG TRAIN Batch 5/5500 loss 11.055887 loss_att 17.248432 loss_ctc 15.615606 loss_rnnt 8.979174 hw_loss 0.431705 lr 0.00072783 rank 5
2023-02-17 13:08:39,085 DEBUG TRAIN Batch 5/5500 loss 32.371357 loss_att 36.372498 loss_ctc 48.312233 loss_rnnt 29.304735 hw_loss 0.264259 lr 0.00072762 rank 0
2023-02-17 13:08:39,085 DEBUG TRAIN Batch 5/5500 loss 17.757046 loss_att 23.043755 loss_ctc 37.917507 loss_rnnt 13.838788 hw_loss 0.324100 lr 0.00072813 rank 6
2023-02-17 13:08:39,088 DEBUG TRAIN Batch 5/5500 loss 18.793030 loss_att 21.420948 loss_ctc 31.483089 loss_rnnt 16.395704 hw_loss 0.337000 lr 0.00072785 rank 7
2023-02-17 13:08:39,088 DEBUG TRAIN Batch 5/5500 loss 15.294881 loss_att 21.989470 loss_ctc 28.838734 loss_rnnt 11.972075 hw_loss 0.333829 lr 0.00072759 rank 1
2023-02-17 13:08:39,089 DEBUG TRAIN Batch 5/5500 loss 21.245575 loss_att 26.754570 loss_ctc 35.546417 loss_rnnt 18.035973 hw_loss 0.376923 lr 0.00072806 rank 4
2023-02-17 13:08:39,089 DEBUG TRAIN Batch 5/5500 loss 29.490711 loss_att 31.751059 loss_ctc 40.071693 loss_rnnt 27.451385 hw_loss 0.330857 lr 0.00072729 rank 2
2023-02-17 13:08:39,091 DEBUG TRAIN Batch 5/5500 loss 16.122284 loss_att 22.883213 loss_ctc 29.370119 loss_rnnt 12.862371 hw_loss 0.265033 lr 0.00072779 rank 3
2023-02-17 13:09:54,310 DEBUG TRAIN Batch 5/5600 loss 20.145386 loss_att 22.076023 loss_ctc 31.294403 loss_rnnt 18.064985 hw_loss 0.389509 lr 0.00072706 rank 5
2023-02-17 13:09:54,315 DEBUG TRAIN Batch 5/5600 loss 19.923222 loss_att 27.661366 loss_ctc 35.530045 loss_rnnt 16.118299 hw_loss 0.330720 lr 0.00072729 rank 4
2023-02-17 13:09:54,315 DEBUG TRAIN Batch 5/5600 loss 17.560270 loss_att 19.553314 loss_ctc 24.552872 loss_rnnt 16.075785 hw_loss 0.287867 lr 0.00072652 rank 2
2023-02-17 13:09:54,317 DEBUG TRAIN Batch 5/5600 loss 20.314127 loss_att 25.810719 loss_ctc 33.422455 loss_rnnt 17.274794 hw_loss 0.360444 lr 0.00072736 rank 6
2023-02-17 13:09:54,318 DEBUG TRAIN Batch 5/5600 loss 18.966719 loss_att 22.187366 loss_ctc 34.083447 loss_rnnt 16.079346 hw_loss 0.426899 lr 0.00072702 rank 3
2023-02-17 13:09:54,321 DEBUG TRAIN Batch 5/5600 loss 27.484369 loss_att 34.031292 loss_ctc 42.907166 loss_rnnt 23.929359 hw_loss 0.354846 lr 0.00072682 rank 1
2023-02-17 13:09:54,321 DEBUG TRAIN Batch 5/5600 loss 14.280263 loss_att 19.635624 loss_ctc 26.714994 loss_rnnt 11.373412 hw_loss 0.333402 lr 0.00072708 rank 7
2023-02-17 13:09:54,322 DEBUG TRAIN Batch 5/5600 loss 17.776930 loss_att 21.101721 loss_ctc 27.099903 loss_rnnt 15.678147 hw_loss 0.357676 lr 0.00072685 rank 0
2023-02-17 13:11:15,239 DEBUG TRAIN Batch 5/5700 loss 13.277631 loss_att 15.206175 loss_ctc 20.280813 loss_rnnt 11.734456 hw_loss 0.419452 lr 0.00072629 rank 5
2023-02-17 13:11:15,242 DEBUG TRAIN Batch 5/5700 loss 11.431694 loss_att 13.510642 loss_ctc 17.993540 loss_rnnt 9.941340 hw_loss 0.374349 lr 0.00072659 rank 6
2023-02-17 13:11:15,245 DEBUG TRAIN Batch 5/5700 loss 17.355810 loss_att 16.309259 loss_ctc 26.651165 loss_rnnt 16.147240 hw_loss 0.334689 lr 0.00072605 rank 1
2023-02-17 13:11:15,248 DEBUG TRAIN Batch 5/5700 loss 18.619576 loss_att 18.295923 loss_ctc 24.920465 loss_rnnt 17.640713 hw_loss 0.381520 lr 0.00072631 rank 7
2023-02-17 13:11:15,249 DEBUG TRAIN Batch 5/5700 loss 6.502029 loss_att 10.947705 loss_ctc 13.483803 loss_rnnt 4.461699 hw_loss 0.413047 lr 0.00072652 rank 4
2023-02-17 13:11:15,252 DEBUG TRAIN Batch 5/5700 loss 12.457163 loss_att 23.060854 loss_ctc 22.965055 loss_rnnt 8.816664 hw_loss 0.222577 lr 0.00072575 rank 2
2023-02-17 13:11:15,270 DEBUG TRAIN Batch 5/5700 loss 27.718479 loss_att 33.103760 loss_ctc 41.033577 loss_rnnt 24.655567 hw_loss 0.394701 lr 0.00072625 rank 3
2023-02-17 13:11:15,308 DEBUG TRAIN Batch 5/5700 loss 33.823715 loss_att 39.306347 loss_ctc 51.591225 loss_rnnt 30.178938 hw_loss 0.336096 lr 0.00072608 rank 0
2023-02-17 13:12:32,327 DEBUG TRAIN Batch 5/5800 loss 15.874541 loss_att 25.704750 loss_ctc 20.875626 loss_rnnt 13.072238 hw_loss 0.317720 lr 0.00072583 rank 6
2023-02-17 13:12:32,328 DEBUG TRAIN Batch 5/5800 loss 12.736895 loss_att 15.094276 loss_ctc 16.107811 loss_rnnt 11.646948 hw_loss 0.316907 lr 0.00072554 rank 7
2023-02-17 13:12:32,329 DEBUG TRAIN Batch 5/5800 loss 20.074907 loss_att 21.487850 loss_ctc 24.499331 loss_rnnt 18.941961 hw_loss 0.488320 lr 0.00072576 rank 4
2023-02-17 13:12:32,330 DEBUG TRAIN Batch 5/5800 loss 17.708151 loss_att 25.079948 loss_ctc 31.949383 loss_rnnt 14.160879 hw_loss 0.326402 lr 0.00072548 rank 3
2023-02-17 13:12:32,331 DEBUG TRAIN Batch 5/5800 loss 11.117574 loss_att 16.710438 loss_ctc 22.142712 loss_rnnt 8.401926 hw_loss 0.238230 lr 0.00072532 rank 0
2023-02-17 13:12:32,330 DEBUG TRAIN Batch 5/5800 loss 23.842775 loss_att 28.036291 loss_ctc 36.573067 loss_rnnt 21.080946 hw_loss 0.423286 lr 0.00072529 rank 1
2023-02-17 13:12:32,331 DEBUG TRAIN Batch 5/5800 loss 13.501334 loss_att 17.488577 loss_ctc 20.585371 loss_rnnt 11.537101 hw_loss 0.416710 lr 0.00072553 rank 5
2023-02-17 13:12:32,331 DEBUG TRAIN Batch 5/5800 loss 15.564174 loss_att 22.854843 loss_ctc 30.558012 loss_rnnt 11.926071 hw_loss 0.338981 lr 0.00072499 rank 2
2023-02-17 13:13:47,853 DEBUG TRAIN Batch 5/5900 loss 32.255985 loss_att 35.400543 loss_ctc 59.852612 loss_rnnt 27.835524 hw_loss 0.209995 lr 0.00072506 rank 6
2023-02-17 13:13:47,853 DEBUG TRAIN Batch 5/5900 loss 14.225851 loss_att 18.008499 loss_ctc 22.596680 loss_rnnt 12.173029 hw_loss 0.337839 lr 0.00072423 rank 2
2023-02-17 13:13:47,857 DEBUG TRAIN Batch 5/5900 loss 26.971371 loss_att 34.455120 loss_ctc 34.321560 loss_rnnt 24.334829 hw_loss 0.299565 lr 0.00072472 rank 3
2023-02-17 13:13:47,857 DEBUG TRAIN Batch 5/5900 loss 29.129684 loss_att 30.270912 loss_ctc 41.208672 loss_rnnt 27.125486 hw_loss 0.310165 lr 0.00072478 rank 7
2023-02-17 13:13:47,858 DEBUG TRAIN Batch 5/5900 loss 12.686414 loss_att 16.474947 loss_ctc 18.123133 loss_rnnt 11.078742 hw_loss 0.234507 lr 0.00072455 rank 0
2023-02-17 13:13:47,862 DEBUG TRAIN Batch 5/5900 loss 20.347357 loss_att 21.201536 loss_ctc 28.101883 loss_rnnt 18.958626 hw_loss 0.344921 lr 0.00072452 rank 1
2023-02-17 13:13:47,862 DEBUG TRAIN Batch 5/5900 loss 25.843531 loss_att 27.871090 loss_ctc 39.451897 loss_rnnt 23.423260 hw_loss 0.375582 lr 0.00072477 rank 5
2023-02-17 13:13:47,908 DEBUG TRAIN Batch 5/5900 loss 11.261548 loss_att 20.414440 loss_ctc 14.428360 loss_rnnt 8.823034 hw_loss 0.348174 lr 0.00072500 rank 4
2023-02-17 13:15:06,418 DEBUG TRAIN Batch 5/6000 loss 27.737297 loss_att 29.399933 loss_ctc 42.412384 loss_rnnt 25.245573 hw_loss 0.379720 lr 0.00072430 rank 6
2023-02-17 13:15:06,419 DEBUG TRAIN Batch 5/6000 loss 23.447733 loss_att 31.391438 loss_ctc 41.613701 loss_rnnt 19.275307 hw_loss 0.302914 lr 0.00072401 rank 5
2023-02-17 13:15:06,423 DEBUG TRAIN Batch 5/6000 loss 21.490826 loss_att 24.978464 loss_ctc 36.611416 loss_rnnt 18.552303 hw_loss 0.421720 lr 0.00072347 rank 2
2023-02-17 13:15:06,423 DEBUG TRAIN Batch 5/6000 loss 30.603512 loss_att 32.540909 loss_ctc 55.553844 loss_rnnt 26.705191 hw_loss 0.345242 lr 0.00072379 rank 0
2023-02-17 13:15:06,423 DEBUG TRAIN Batch 5/6000 loss 23.112404 loss_att 30.578789 loss_ctc 39.862228 loss_rnnt 19.230442 hw_loss 0.291329 lr 0.00072376 rank 1
2023-02-17 13:15:06,424 DEBUG TRAIN Batch 5/6000 loss 17.867512 loss_att 22.537357 loss_ctc 31.438549 loss_rnnt 14.995122 hw_loss 0.241778 lr 0.00072396 rank 3
2023-02-17 13:15:06,426 DEBUG TRAIN Batch 5/6000 loss 10.942412 loss_att 14.464408 loss_ctc 20.627514 loss_rnnt 8.773508 hw_loss 0.324670 lr 0.00072423 rank 4
2023-02-17 13:15:06,426 DEBUG TRAIN Batch 5/6000 loss 14.713016 loss_att 15.828751 loss_ctc 21.467087 loss_rnnt 13.455156 hw_loss 0.251567 lr 0.00072402 rank 7
2023-02-17 13:16:25,068 DEBUG TRAIN Batch 5/6100 loss 30.924469 loss_att 37.954517 loss_ctc 53.307507 loss_rnnt 26.366398 hw_loss 0.314348 lr 0.00072326 rank 7
2023-02-17 13:16:25,071 DEBUG TRAIN Batch 5/6100 loss 20.613596 loss_att 27.284058 loss_ctc 35.838348 loss_rnnt 17.060335 hw_loss 0.354749 lr 0.00072301 rank 1
2023-02-17 13:16:25,073 DEBUG TRAIN Batch 5/6100 loss 15.114692 loss_att 17.467400 loss_ctc 19.955048 loss_rnnt 13.803975 hw_loss 0.365241 lr 0.00072354 rank 6
2023-02-17 13:16:25,074 DEBUG TRAIN Batch 5/6100 loss 45.667362 loss_att 51.580185 loss_ctc 63.746841 loss_rnnt 41.877083 hw_loss 0.369593 lr 0.00072325 rank 5
2023-02-17 13:16:25,075 DEBUG TRAIN Batch 5/6100 loss 10.257498 loss_att 14.843024 loss_ctc 15.959393 loss_rnnt 8.392372 hw_loss 0.352064 lr 0.00072271 rank 2
2023-02-17 13:16:25,076 DEBUG TRAIN Batch 5/6100 loss 29.313307 loss_att 37.898735 loss_ctc 49.102753 loss_rnnt 24.854168 hw_loss 0.193990 lr 0.00072320 rank 3
2023-02-17 13:16:25,076 DEBUG TRAIN Batch 5/6100 loss 31.467211 loss_att 35.949654 loss_ctc 38.085648 loss_rnnt 29.559551 hw_loss 0.241337 lr 0.00072348 rank 4
2023-02-17 13:16:25,118 DEBUG TRAIN Batch 5/6100 loss 22.823841 loss_att 28.637611 loss_ctc 35.434574 loss_rnnt 19.799829 hw_loss 0.337174 lr 0.00072304 rank 0
2023-02-17 13:17:42,231 DEBUG TRAIN Batch 5/6200 loss 13.773662 loss_att 18.398987 loss_ctc 21.467171 loss_rnnt 11.663582 hw_loss 0.298527 lr 0.00072249 rank 5
2023-02-17 13:17:42,237 DEBUG TRAIN Batch 5/6200 loss 16.731873 loss_att 21.718830 loss_ctc 32.368237 loss_rnnt 13.475588 hw_loss 0.326331 lr 0.00072279 rank 6
2023-02-17 13:17:42,239 DEBUG TRAIN Batch 5/6200 loss 13.137391 loss_att 18.049088 loss_ctc 25.176641 loss_rnnt 10.322392 hw_loss 0.426423 lr 0.00072196 rank 2
2023-02-17 13:17:42,239 DEBUG TRAIN Batch 5/6200 loss 23.303539 loss_att 31.669733 loss_ctc 40.394638 loss_rnnt 19.164799 hw_loss 0.350043 lr 0.00072272 rank 4
2023-02-17 13:17:42,243 DEBUG TRAIN Batch 5/6200 loss 24.304161 loss_att 30.061298 loss_ctc 31.917255 loss_rnnt 21.985592 hw_loss 0.285118 lr 0.00072251 rank 7
2023-02-17 13:17:42,244 DEBUG TRAIN Batch 5/6200 loss 11.746416 loss_att 14.845378 loss_ctc 19.215593 loss_rnnt 9.904748 hw_loss 0.423722 lr 0.00072225 rank 1
2023-02-17 13:17:42,247 DEBUG TRAIN Batch 5/6200 loss 19.460386 loss_att 23.341335 loss_ctc 32.798229 loss_rnnt 16.771358 hw_loss 0.252107 lr 0.00072245 rank 3
2023-02-17 13:17:42,293 DEBUG TRAIN Batch 5/6200 loss 21.879332 loss_att 26.730743 loss_ctc 31.115719 loss_rnnt 19.533272 hw_loss 0.270487 lr 0.00072228 rank 0
2023-02-17 13:18:59,151 DEBUG TRAIN Batch 5/6300 loss 13.821590 loss_att 13.969723 loss_ctc 22.427153 loss_rnnt 12.440031 hw_loss 0.383483 lr 0.00072150 rank 1
2023-02-17 13:18:59,155 DEBUG TRAIN Batch 5/6300 loss 14.710133 loss_att 14.632710 loss_ctc 19.219603 loss_rnnt 13.850443 hw_loss 0.513583 lr 0.00072170 rank 3
2023-02-17 13:18:59,156 DEBUG TRAIN Batch 5/6300 loss 20.071846 loss_att 24.483673 loss_ctc 34.422131 loss_rnnt 17.113403 hw_loss 0.305070 lr 0.00072176 rank 7
2023-02-17 13:18:59,156 DEBUG TRAIN Batch 5/6300 loss 28.507261 loss_att 31.578264 loss_ctc 47.716564 loss_rnnt 25.111816 hw_loss 0.412508 lr 0.00072203 rank 6
2023-02-17 13:18:59,158 DEBUG TRAIN Batch 5/6300 loss 25.687750 loss_att 27.427578 loss_ctc 41.773758 loss_rnnt 22.978848 hw_loss 0.405260 lr 0.00072174 rank 5
2023-02-17 13:18:59,159 DEBUG TRAIN Batch 5/6300 loss 18.955801 loss_att 28.036335 loss_ctc 39.162041 loss_rnnt 14.266592 hw_loss 0.335506 lr 0.00072121 rank 2
2023-02-17 13:18:59,163 DEBUG TRAIN Batch 5/6300 loss 17.281307 loss_att 19.808722 loss_ctc 27.858946 loss_rnnt 15.124115 hw_loss 0.452547 lr 0.00072153 rank 0
2023-02-17 13:18:59,165 DEBUG TRAIN Batch 5/6300 loss 30.919924 loss_att 32.930290 loss_ctc 44.726372 loss_rnnt 28.525703 hw_loss 0.283664 lr 0.00072197 rank 4
2023-02-17 13:20:18,920 DEBUG TRAIN Batch 5/6400 loss 23.006002 loss_att 31.398499 loss_ctc 45.166428 loss_rnnt 18.186859 hw_loss 0.348600 lr 0.00072128 rank 6
2023-02-17 13:20:18,924 DEBUG TRAIN Batch 5/6400 loss 25.648645 loss_att 34.845726 loss_ctc 45.986908 loss_rnnt 20.924320 hw_loss 0.324637 lr 0.00072100 rank 7
2023-02-17 13:20:18,925 DEBUG TRAIN Batch 5/6400 loss 12.494247 loss_att 16.600843 loss_ctc 16.222975 loss_rnnt 10.965755 hw_loss 0.393768 lr 0.00072094 rank 3
2023-02-17 13:20:18,925 DEBUG TRAIN Batch 5/6400 loss 26.892654 loss_att 30.441414 loss_ctc 38.474712 loss_rnnt 24.473150 hw_loss 0.310272 lr 0.00072121 rank 4
2023-02-17 13:20:18,925 DEBUG TRAIN Batch 5/6400 loss 16.092304 loss_att 22.814602 loss_ctc 26.770645 loss_rnnt 13.127529 hw_loss 0.368504 lr 0.00072078 rank 0
2023-02-17 13:20:18,926 DEBUG TRAIN Batch 5/6400 loss 24.963982 loss_att 30.292645 loss_ctc 40.531586 loss_rnnt 21.638937 hw_loss 0.344313 lr 0.00072099 rank 5
2023-02-17 13:20:18,933 DEBUG TRAIN Batch 5/6400 loss 20.955772 loss_att 26.642941 loss_ctc 39.481773 loss_rnnt 17.155445 hw_loss 0.361422 lr 0.00072075 rank 1
2023-02-17 13:20:18,935 DEBUG TRAIN Batch 5/6400 loss 20.223314 loss_att 24.731844 loss_ctc 32.751316 loss_rnnt 17.490076 hw_loss 0.302123 lr 0.00072046 rank 2
2023-02-17 13:21:35,593 DEBUG TRAIN Batch 5/6500 loss 27.428930 loss_att 31.633793 loss_ctc 41.091011 loss_rnnt 24.638662 hw_loss 0.239409 lr 0.00072026 rank 7
2023-02-17 13:21:35,593 DEBUG TRAIN Batch 5/6500 loss 19.192583 loss_att 29.686737 loss_ctc 35.877712 loss_rnnt 14.696074 hw_loss 0.324364 lr 0.00072053 rank 6
2023-02-17 13:21:35,593 DEBUG TRAIN Batch 5/6500 loss 16.084747 loss_att 21.099636 loss_ctc 25.680008 loss_rnnt 13.587241 hw_loss 0.403424 lr 0.00072000 rank 1
2023-02-17 13:21:35,594 DEBUG TRAIN Batch 5/6500 loss 24.101078 loss_att 24.066845 loss_ctc 38.094707 loss_rnnt 22.072155 hw_loss 0.318662 lr 0.00072003 rank 0
2023-02-17 13:21:35,594 DEBUG TRAIN Batch 5/6500 loss 16.842009 loss_att 18.045853 loss_ctc 20.464266 loss_rnnt 15.916754 hw_loss 0.377846 lr 0.00072024 rank 5
2023-02-17 13:21:35,596 DEBUG TRAIN Batch 5/6500 loss 13.757380 loss_att 18.936384 loss_ctc 22.968460 loss_rnnt 11.330381 hw_loss 0.305725 lr 0.00072020 rank 3
2023-02-17 13:21:35,598 DEBUG TRAIN Batch 5/6500 loss 13.044337 loss_att 12.519520 loss_ctc 18.545002 loss_rnnt 12.194052 hw_loss 0.415928 lr 0.00072047 rank 4
2023-02-17 13:21:35,603 DEBUG TRAIN Batch 5/6500 loss 31.252813 loss_att 30.970589 loss_ctc 51.953518 loss_rnnt 28.365606 hw_loss 0.344170 lr 0.00071971 rank 2
2023-02-17 13:22:50,928 DEBUG TRAIN Batch 5/6600 loss 9.902705 loss_att 16.917883 loss_ctc 16.558329 loss_rnnt 7.458591 hw_loss 0.288117 lr 0.00071979 rank 6
2023-02-17 13:22:50,931 DEBUG TRAIN Batch 5/6600 loss 22.636448 loss_att 27.474163 loss_ctc 34.431087 loss_rnnt 19.947744 hw_loss 0.278518 lr 0.00071951 rank 7
2023-02-17 13:22:50,932 DEBUG TRAIN Batch 5/6600 loss 22.061535 loss_att 26.849642 loss_ctc 37.398746 loss_rnnt 18.888325 hw_loss 0.319926 lr 0.00071897 rank 2
2023-02-17 13:22:50,934 DEBUG TRAIN Batch 5/6600 loss 24.922480 loss_att 29.889313 loss_ctc 49.447960 loss_rnnt 20.497053 hw_loss 0.303746 lr 0.00071945 rank 3
2023-02-17 13:22:50,936 DEBUG TRAIN Batch 5/6600 loss 25.893457 loss_att 32.523533 loss_ctc 44.051254 loss_rnnt 21.915051 hw_loss 0.433787 lr 0.00071926 rank 1
2023-02-17 13:22:50,937 DEBUG TRAIN Batch 5/6600 loss 21.866083 loss_att 25.330856 loss_ctc 29.912243 loss_rnnt 19.942688 hw_loss 0.295533 lr 0.00071950 rank 5
2023-02-17 13:22:50,939 DEBUG TRAIN Batch 5/6600 loss 26.857489 loss_att 32.431610 loss_ctc 39.164242 loss_rnnt 23.941282 hw_loss 0.300902 lr 0.00071972 rank 4
2023-02-17 13:22:50,940 DEBUG TRAIN Batch 5/6600 loss 25.971474 loss_att 28.954111 loss_ctc 45.464455 loss_rnnt 22.578766 hw_loss 0.369592 lr 0.00071929 rank 0
2023-02-17 13:24:08,150 DEBUG TRAIN Batch 5/6700 loss 19.541241 loss_att 19.648087 loss_ctc 33.422558 loss_rnnt 17.481838 hw_loss 0.350981 lr 0.00071851 rank 1
2023-02-17 13:24:08,152 DEBUG TRAIN Batch 5/6700 loss 23.946140 loss_att 30.171070 loss_ctc 42.689674 loss_rnnt 20.032648 hw_loss 0.317562 lr 0.00071904 rank 6
2023-02-17 13:24:08,154 DEBUG TRAIN Batch 5/6700 loss 27.595198 loss_att 37.919331 loss_ctc 47.354195 loss_rnnt 22.745914 hw_loss 0.281099 lr 0.00071877 rank 7
2023-02-17 13:24:08,155 DEBUG TRAIN Batch 5/6700 loss 8.532722 loss_att 14.298233 loss_ctc 16.035118 loss_rnnt 6.246105 hw_loss 0.249743 lr 0.00071897 rank 4
2023-02-17 13:24:08,158 DEBUG TRAIN Batch 5/6700 loss 15.730585 loss_att 20.967106 loss_ctc 24.269062 loss_rnnt 13.353199 hw_loss 0.359282 lr 0.00071875 rank 5
2023-02-17 13:24:08,158 DEBUG TRAIN Batch 5/6700 loss 25.010008 loss_att 25.431528 loss_ctc 37.862644 loss_rnnt 23.039747 hw_loss 0.323011 lr 0.00071871 rank 3
2023-02-17 13:24:08,159 DEBUG TRAIN Batch 5/6700 loss 14.919482 loss_att 20.193115 loss_ctc 28.079796 loss_rnnt 11.977667 hw_loss 0.248216 lr 0.00071854 rank 0
2023-02-17 13:24:08,161 DEBUG TRAIN Batch 5/6700 loss 6.796821 loss_att 10.681044 loss_ctc 13.008961 loss_rnnt 5.039900 hw_loss 0.284607 lr 0.00071822 rank 2
2023-02-17 13:25:27,334 DEBUG TRAIN Batch 5/6800 loss 20.140839 loss_att 27.475546 loss_ctc 34.572449 loss_rnnt 16.602407 hw_loss 0.276139 lr 0.00071802 rank 7
2023-02-17 13:25:27,334 DEBUG TRAIN Batch 5/6800 loss 26.109158 loss_att 29.301748 loss_ctc 36.194492 loss_rnnt 23.979420 hw_loss 0.274702 lr 0.00071777 rank 1
2023-02-17 13:25:27,335 DEBUG TRAIN Batch 5/6800 loss 14.520802 loss_att 18.140068 loss_ctc 22.879070 loss_rnnt 12.488434 hw_loss 0.363900 lr 0.00071797 rank 3
2023-02-17 13:25:27,336 DEBUG TRAIN Batch 5/6800 loss 34.016277 loss_att 40.544510 loss_ctc 48.570271 loss_rnnt 30.640522 hw_loss 0.242949 lr 0.00071823 rank 4
2023-02-17 13:25:27,335 DEBUG TRAIN Batch 5/6800 loss 22.912277 loss_att 24.497116 loss_ctc 36.911720 loss_rnnt 20.576500 hw_loss 0.285407 lr 0.00071830 rank 6
2023-02-17 13:25:27,338 DEBUG TRAIN Batch 5/6800 loss 18.307114 loss_att 20.594484 loss_ctc 31.458015 loss_rnnt 15.884666 hw_loss 0.396600 lr 0.00071801 rank 5
2023-02-17 13:25:27,343 DEBUG TRAIN Batch 5/6800 loss 24.654299 loss_att 29.281590 loss_ctc 41.111179 loss_rnnt 21.360888 hw_loss 0.325692 lr 0.00071748 rank 2
2023-02-17 13:25:27,350 DEBUG TRAIN Batch 5/6800 loss 13.602465 loss_att 20.107086 loss_ctc 20.682789 loss_rnnt 11.178947 hw_loss 0.334780 lr 0.00071780 rank 0
2023-02-17 13:26:44,601 DEBUG TRAIN Batch 5/6900 loss 16.324619 loss_att 18.401291 loss_ctc 26.828968 loss_rnnt 14.281122 hw_loss 0.426718 lr 0.00071729 rank 7
2023-02-17 13:26:44,601 DEBUG TRAIN Batch 5/6900 loss 25.702028 loss_att 28.881973 loss_ctc 39.314415 loss_rnnt 23.027502 hw_loss 0.419161 lr 0.00071756 rank 6
2023-02-17 13:26:44,602 DEBUG TRAIN Batch 5/6900 loss 12.074196 loss_att 16.937607 loss_ctc 19.876913 loss_rnnt 9.880900 hw_loss 0.337973 lr 0.00071703 rank 1
2023-02-17 13:26:44,603 DEBUG TRAIN Batch 5/6900 loss 15.658857 loss_att 15.059190 loss_ctc 20.663593 loss_rnnt 14.913719 hw_loss 0.370825 lr 0.00071723 rank 3
2023-02-17 13:26:44,607 DEBUG TRAIN Batch 5/6900 loss 7.300038 loss_att 11.723537 loss_ctc 11.819344 loss_rnnt 5.547329 hw_loss 0.497691 lr 0.00071727 rank 5
2023-02-17 13:26:44,608 DEBUG TRAIN Batch 5/6900 loss 10.542959 loss_att 14.313435 loss_ctc 19.444050 loss_rnnt 8.409051 hw_loss 0.361876 lr 0.00071706 rank 0
2023-02-17 13:26:44,609 DEBUG TRAIN Batch 5/6900 loss 18.149046 loss_att 20.128431 loss_ctc 25.935310 loss_rnnt 16.541172 hw_loss 0.325927 lr 0.00071749 rank 4
2023-02-17 13:26:44,656 DEBUG TRAIN Batch 5/6900 loss 17.784941 loss_att 20.544319 loss_ctc 28.671009 loss_rnnt 15.562767 hw_loss 0.410291 lr 0.00071675 rank 2
2023-02-17 13:28:01,351 DEBUG TRAIN Batch 5/7000 loss 14.620742 loss_att 13.918665 loss_ctc 20.305685 loss_rnnt 13.749430 hw_loss 0.475749 lr 0.00071655 rank 7
2023-02-17 13:28:01,352 DEBUG TRAIN Batch 5/7000 loss 17.405758 loss_att 18.050648 loss_ctc 26.487041 loss_rnnt 15.825902 hw_loss 0.450071 lr 0.00071653 rank 5
2023-02-17 13:28:01,353 DEBUG TRAIN Batch 5/7000 loss 22.550325 loss_att 21.814175 loss_ctc 29.755880 loss_rnnt 21.523508 hw_loss 0.399949 lr 0.00071682 rank 6
2023-02-17 13:28:01,353 DEBUG TRAIN Batch 5/7000 loss 8.089764 loss_att 11.056466 loss_ctc 13.036731 loss_rnnt 6.621208 hw_loss 0.404284 lr 0.00071630 rank 1
2023-02-17 13:28:01,358 DEBUG TRAIN Batch 5/7000 loss 20.745625 loss_att 23.154402 loss_ctc 51.507286 loss_rnnt 15.963297 hw_loss 0.373158 lr 0.00071601 rank 2
2023-02-17 13:28:01,359 DEBUG TRAIN Batch 5/7000 loss 22.265331 loss_att 27.130413 loss_ctc 40.659393 loss_rnnt 18.689779 hw_loss 0.281240 lr 0.00071675 rank 4
2023-02-17 13:28:01,360 DEBUG TRAIN Batch 5/7000 loss 20.057507 loss_att 24.859896 loss_ctc 34.552414 loss_rnnt 16.957693 hw_loss 0.387526 lr 0.00071649 rank 3
2023-02-17 13:28:01,406 DEBUG TRAIN Batch 5/7000 loss 19.049118 loss_att 19.678398 loss_ctc 27.698757 loss_rnnt 17.613276 hw_loss 0.293815 lr 0.00071633 rank 0
2023-02-17 13:29:21,309 DEBUG TRAIN Batch 5/7100 loss 18.477818 loss_att 20.751926 loss_ctc 29.508591 loss_rnnt 16.315014 hw_loss 0.444776 lr 0.00071602 rank 4
2023-02-17 13:29:21,312 DEBUG TRAIN Batch 5/7100 loss 9.115942 loss_att 12.394524 loss_ctc 17.402634 loss_rnnt 7.173782 hw_loss 0.340408 lr 0.00071576 rank 3
2023-02-17 13:29:21,313 DEBUG TRAIN Batch 5/7100 loss 23.729610 loss_att 27.639914 loss_ctc 37.748405 loss_rnnt 20.953711 hw_loss 0.233750 lr 0.00071528 rank 2
2023-02-17 13:29:21,323 DEBUG TRAIN Batch 5/7100 loss 14.001220 loss_att 19.964252 loss_ctc 25.441595 loss_rnnt 11.131801 hw_loss 0.283931 lr 0.00071556 rank 1
2023-02-17 13:29:21,327 DEBUG TRAIN Batch 5/7100 loss 40.244736 loss_att 47.091988 loss_ctc 57.140289 loss_rnnt 36.473579 hw_loss 0.279317 lr 0.00071559 rank 0
2023-02-17 13:29:21,328 DEBUG TRAIN Batch 5/7100 loss 28.303942 loss_att 30.174614 loss_ctc 40.527954 loss_rnnt 26.189514 hw_loss 0.207049 lr 0.00071609 rank 6
2023-02-17 13:29:21,330 DEBUG TRAIN Batch 5/7100 loss 13.744009 loss_att 17.109127 loss_ctc 25.118261 loss_rnnt 11.353052 hw_loss 0.377559 lr 0.00071580 rank 5
2023-02-17 13:29:21,334 DEBUG TRAIN Batch 5/7100 loss 18.461403 loss_att 23.964525 loss_ctc 29.924788 loss_rnnt 15.612747 hw_loss 0.411709 lr 0.00071581 rank 7
2023-02-17 13:30:38,360 DEBUG TRAIN Batch 5/7200 loss 28.360399 loss_att 34.635975 loss_ctc 47.060577 loss_rnnt 24.451771 hw_loss 0.300289 lr 0.00071502 rank 3
2023-02-17 13:30:38,363 DEBUG TRAIN Batch 5/7200 loss 10.185163 loss_att 16.684000 loss_ctc 20.915035 loss_rnnt 7.259742 hw_loss 0.365635 lr 0.00071507 rank 5
2023-02-17 13:30:38,365 DEBUG TRAIN Batch 5/7200 loss 28.600437 loss_att 33.645905 loss_ctc 48.055798 loss_rnnt 24.810577 hw_loss 0.350095 lr 0.00071529 rank 4
2023-02-17 13:30:38,364 DEBUG TRAIN Batch 5/7200 loss 49.487396 loss_att 53.570301 loss_ctc 67.731674 loss_rnnt 46.126045 hw_loss 0.210372 lr 0.00071486 rank 0
2023-02-17 13:30:38,365 DEBUG TRAIN Batch 5/7200 loss 26.273161 loss_att 30.711624 loss_ctc 38.761208 loss_rnnt 23.591940 hw_loss 0.240856 lr 0.00071535 rank 6
2023-02-17 13:30:38,365 DEBUG TRAIN Batch 5/7200 loss 21.650063 loss_att 25.156218 loss_ctc 37.562798 loss_rnnt 18.626377 hw_loss 0.376415 lr 0.00071483 rank 1
2023-02-17 13:30:38,367 DEBUG TRAIN Batch 5/7200 loss 11.170622 loss_att 13.308294 loss_ctc 21.150215 loss_rnnt 9.242087 hw_loss 0.319475 lr 0.00071508 rank 7
2023-02-17 13:30:38,367 DEBUG TRAIN Batch 5/7200 loss 25.098639 loss_att 30.731035 loss_ctc 29.795258 loss_rnnt 23.139915 hw_loss 0.386300 lr 0.00071455 rank 2
2023-02-17 13:31:53,979 DEBUG TRAIN Batch 5/7300 loss 24.522022 loss_att 29.273224 loss_ctc 39.614677 loss_rnnt 21.394476 hw_loss 0.309288 lr 0.00071434 rank 5
2023-02-17 13:31:53,983 DEBUG TRAIN Batch 5/7300 loss 30.358028 loss_att 38.798595 loss_ctc 48.724991 loss_rnnt 26.036161 hw_loss 0.346547 lr 0.00071435 rank 7
2023-02-17 13:31:53,987 DEBUG TRAIN Batch 5/7300 loss 18.848116 loss_att 27.615335 loss_ctc 32.982574 loss_rnnt 15.056112 hw_loss 0.288684 lr 0.00071413 rank 0
2023-02-17 13:31:53,987 DEBUG TRAIN Batch 5/7300 loss 14.299280 loss_att 16.205957 loss_ctc 18.780203 loss_rnnt 13.151134 hw_loss 0.317538 lr 0.00071429 rank 3
2023-02-17 13:31:53,988 DEBUG TRAIN Batch 5/7300 loss 17.322115 loss_att 21.270571 loss_ctc 24.492016 loss_rnnt 15.404275 hw_loss 0.322803 lr 0.00071462 rank 6
2023-02-17 13:31:53,991 DEBUG TRAIN Batch 5/7300 loss 17.278152 loss_att 22.668472 loss_ctc 28.456131 loss_rnnt 14.553097 hw_loss 0.293614 lr 0.00071382 rank 2
2023-02-17 13:31:53,991 DEBUG TRAIN Batch 5/7300 loss 19.665930 loss_att 23.840714 loss_ctc 35.895393 loss_rnnt 16.489462 hw_loss 0.332966 lr 0.00071410 rank 1
2023-02-17 13:31:53,992 DEBUG TRAIN Batch 5/7300 loss 16.405579 loss_att 21.929983 loss_ctc 22.358696 loss_rnnt 14.321223 hw_loss 0.348235 lr 0.00071456 rank 4
2023-02-17 13:33:10,807 DEBUG TRAIN Batch 5/7400 loss 14.282627 loss_att 19.598421 loss_ctc 26.606339 loss_rnnt 11.387565 hw_loss 0.353893 lr 0.00071389 rank 6
2023-02-17 13:33:10,808 DEBUG TRAIN Batch 5/7400 loss 20.645971 loss_att 25.068882 loss_ctc 37.795509 loss_rnnt 17.245621 hw_loss 0.429683 lr 0.00071309 rank 2
2023-02-17 13:33:10,809 DEBUG TRAIN Batch 5/7400 loss 16.814434 loss_att 18.749380 loss_ctc 25.528049 loss_rnnt 15.079109 hw_loss 0.349729 lr 0.00071357 rank 3
2023-02-17 13:33:10,809 DEBUG TRAIN Batch 5/7400 loss 21.041895 loss_att 25.887329 loss_ctc 36.072769 loss_rnnt 17.895214 hw_loss 0.325274 lr 0.00071361 rank 5
2023-02-17 13:33:10,809 DEBUG TRAIN Batch 5/7400 loss 18.485134 loss_att 20.705961 loss_ctc 29.681942 loss_rnnt 16.395618 hw_loss 0.285829 lr 0.00071383 rank 4
2023-02-17 13:33:10,817 DEBUG TRAIN Batch 5/7400 loss 20.539227 loss_att 24.759899 loss_ctc 35.556091 loss_rnnt 17.538155 hw_loss 0.290043 lr 0.00071362 rank 7
2023-02-17 13:33:10,818 DEBUG TRAIN Batch 5/7400 loss 21.278769 loss_att 27.774155 loss_ctc 38.028114 loss_rnnt 17.534939 hw_loss 0.396572 lr 0.00071338 rank 1
2023-02-17 13:33:10,840 DEBUG TRAIN Batch 5/7400 loss 10.401734 loss_att 17.162828 loss_ctc 18.426720 loss_rnnt 7.799190 hw_loss 0.338114 lr 0.00071341 rank 0
2023-02-17 13:34:30,544 DEBUG TRAIN Batch 5/7500 loss 7.723677 loss_att 13.282857 loss_ctc 18.872671 loss_rnnt 4.934632 hw_loss 0.357516 lr 0.00071265 rank 1
2023-02-17 13:34:30,545 DEBUG TRAIN Batch 5/7500 loss 14.811166 loss_att 21.362963 loss_ctc 28.859413 loss_rnnt 11.483047 hw_loss 0.271236 lr 0.00071310 rank 4
2023-02-17 13:34:30,545 DEBUG TRAIN Batch 5/7500 loss 12.528127 loss_att 16.813812 loss_ctc 19.396086 loss_rnnt 10.582431 hw_loss 0.324059 lr 0.00071317 rank 6
2023-02-17 13:34:30,546 DEBUG TRAIN Batch 5/7500 loss 19.125288 loss_att 23.672148 loss_ctc 34.392216 loss_rnnt 16.002487 hw_loss 0.333447 lr 0.00071288 rank 5
2023-02-17 13:34:30,547 DEBUG TRAIN Batch 5/7500 loss 18.856514 loss_att 22.954397 loss_ctc 33.718033 loss_rnnt 15.884641 hw_loss 0.320178 lr 0.00071290 rank 7
2023-02-17 13:34:30,547 DEBUG TRAIN Batch 5/7500 loss 12.115899 loss_att 15.372304 loss_ctc 21.507828 loss_rnnt 10.035439 hw_loss 0.331729 lr 0.00071284 rank 3
2023-02-17 13:34:30,547 DEBUG TRAIN Batch 5/7500 loss 5.843046 loss_att 9.353300 loss_ctc 12.139895 loss_rnnt 4.100409 hw_loss 0.376887 lr 0.00071237 rank 2
2023-02-17 13:34:30,556 DEBUG TRAIN Batch 5/7500 loss 14.612484 loss_att 18.767334 loss_ctc 26.458065 loss_rnnt 12.012318 hw_loss 0.355850 lr 0.00071268 rank 0
2023-02-17 13:35:46,554 DEBUG TRAIN Batch 5/7600 loss 13.788054 loss_att 15.906761 loss_ctc 22.480556 loss_rnnt 12.008769 hw_loss 0.368520 lr 0.00071244 rank 6
2023-02-17 13:35:46,556 DEBUG TRAIN Batch 5/7600 loss 17.091415 loss_att 20.732477 loss_ctc 25.531778 loss_rnnt 15.075809 hw_loss 0.303775 lr 0.00071216 rank 5
2023-02-17 13:35:46,559 DEBUG TRAIN Batch 5/7600 loss 8.048573 loss_att 10.949841 loss_ctc 13.577052 loss_rnnt 6.530562 hw_loss 0.376178 lr 0.00071196 rank 0
2023-02-17 13:35:46,560 DEBUG TRAIN Batch 5/7600 loss 20.935404 loss_att 25.312759 loss_ctc 34.211899 loss_rnnt 18.108145 hw_loss 0.340477 lr 0.00071238 rank 4
2023-02-17 13:35:46,562 DEBUG TRAIN Batch 5/7600 loss 12.963008 loss_att 15.896879 loss_ctc 24.763153 loss_rnnt 10.552351 hw_loss 0.469745 lr 0.00071217 rank 7
2023-02-17 13:35:46,563 DEBUG TRAIN Batch 5/7600 loss 13.563793 loss_att 15.062970 loss_ctc 23.529135 loss_rnnt 11.768882 hw_loss 0.311933 lr 0.00071193 rank 1
2023-02-17 13:35:46,565 DEBUG TRAIN Batch 5/7600 loss 22.784201 loss_att 26.970343 loss_ctc 34.229088 loss_rnnt 20.250706 hw_loss 0.319277 lr 0.00071212 rank 3
2023-02-17 13:35:46,567 DEBUG TRAIN Batch 5/7600 loss 8.485192 loss_att 18.723297 loss_ctc 22.142872 loss_rnnt 4.383606 hw_loss 0.436763 lr 0.00071165 rank 2
2023-02-17 13:37:03,116 DEBUG TRAIN Batch 5/7700 loss 23.536734 loss_att 30.761848 loss_ctc 50.504684 loss_rnnt 18.341991 hw_loss 0.288738 lr 0.00071172 rank 6
2023-02-17 13:37:03,121 DEBUG TRAIN Batch 5/7700 loss 20.711126 loss_att 27.702692 loss_ctc 33.528091 loss_rnnt 17.435141 hw_loss 0.316397 lr 0.00071145 rank 7
2023-02-17 13:37:03,122 DEBUG TRAIN Batch 5/7700 loss 10.584259 loss_att 15.866200 loss_ctc 20.753073 loss_rnnt 8.041343 hw_loss 0.245036 lr 0.00071140 rank 3
2023-02-17 13:37:03,122 DEBUG TRAIN Batch 5/7700 loss 25.000355 loss_att 37.104591 loss_ctc 39.167114 loss_rnnt 20.501186 hw_loss 0.355164 lr 0.00071144 rank 5
2023-02-17 13:37:03,124 DEBUG TRAIN Batch 5/7700 loss 14.461336 loss_att 17.914536 loss_ctc 24.850956 loss_rnnt 12.195853 hw_loss 0.355425 lr 0.00071121 rank 1
2023-02-17 13:37:03,124 DEBUG TRAIN Batch 5/7700 loss 9.962760 loss_att 14.096305 loss_ctc 20.948620 loss_rnnt 7.470457 hw_loss 0.376525 lr 0.00071093 rank 2
2023-02-17 13:37:03,130 DEBUG TRAIN Batch 5/7700 loss 25.288040 loss_att 27.887936 loss_ctc 38.320137 loss_rnnt 22.853554 hw_loss 0.331680 lr 0.00071124 rank 0
2023-02-17 13:37:03,168 DEBUG TRAIN Batch 5/7700 loss 24.894733 loss_att 29.267887 loss_ctc 38.221127 loss_rnnt 22.045874 hw_loss 0.370083 lr 0.00071165 rank 4
2023-02-17 13:38:20,998 DEBUG TRAIN Batch 5/7800 loss 20.572027 loss_att 23.070988 loss_ctc 37.777832 loss_rnnt 17.579666 hw_loss 0.372120 lr 0.00071068 rank 3
2023-02-17 13:38:21,000 DEBUG TRAIN Batch 5/7800 loss 17.254009 loss_att 23.857971 loss_ctc 28.896568 loss_rnnt 14.212181 hw_loss 0.316302 lr 0.00071073 rank 7
2023-02-17 13:38:21,001 DEBUG TRAIN Batch 5/7800 loss 20.860722 loss_att 23.362715 loss_ctc 31.252167 loss_rnnt 18.823721 hw_loss 0.283267 lr 0.00071052 rank 0
2023-02-17 13:38:21,000 DEBUG TRAIN Batch 5/7800 loss 19.426283 loss_att 23.759827 loss_ctc 29.686834 loss_rnnt 17.028975 hw_loss 0.304736 lr 0.00071100 rank 6
2023-02-17 13:38:21,001 DEBUG TRAIN Batch 5/7800 loss 20.203037 loss_att 23.328594 loss_ctc 35.592606 loss_rnnt 17.397335 hw_loss 0.241219 lr 0.00071049 rank 1
2023-02-17 13:38:21,003 DEBUG TRAIN Batch 5/7800 loss 24.247736 loss_att 34.047401 loss_ctc 39.423836 loss_rnnt 20.105953 hw_loss 0.296941 lr 0.00071072 rank 5
2023-02-17 13:38:21,008 DEBUG TRAIN Batch 5/7800 loss 27.370262 loss_att 30.670353 loss_ctc 52.788986 loss_rnnt 23.168037 hw_loss 0.286954 lr 0.00071093 rank 4
2023-02-17 13:38:21,008 DEBUG TRAIN Batch 5/7800 loss 14.573681 loss_att 17.104033 loss_ctc 17.941479 loss_rnnt 13.391559 hw_loss 0.425649 lr 0.00071021 rank 2
2023-02-17 13:39:38,608 DEBUG TRAIN Batch 5/7900 loss 38.546726 loss_att 36.728798 loss_ctc 57.454712 loss_rnnt 36.215321 hw_loss 0.326106 lr 0.00071028 rank 6
2023-02-17 13:39:38,612 DEBUG TRAIN Batch 5/7900 loss 23.546810 loss_att 27.720104 loss_ctc 36.652512 loss_rnnt 20.833031 hw_loss 0.246926 lr 0.00070949 rank 2
2023-02-17 13:39:38,612 DEBUG TRAIN Batch 5/7900 loss 27.067650 loss_att 29.278845 loss_ctc 39.930939 loss_rnnt 24.698608 hw_loss 0.396932 lr 0.00071002 rank 7
2023-02-17 13:39:38,614 DEBUG TRAIN Batch 5/7900 loss 17.366673 loss_att 20.874416 loss_ctc 33.703888 loss_rnnt 14.304607 hw_loss 0.341664 lr 0.00070996 rank 3
2023-02-17 13:39:38,615 DEBUG TRAIN Batch 5/7900 loss 8.964980 loss_att 12.811069 loss_ctc 16.589605 loss_rnnt 7.014532 hw_loss 0.308651 lr 0.00071000 rank 5
2023-02-17 13:39:38,618 DEBUG TRAIN Batch 5/7900 loss 30.798027 loss_att 35.567802 loss_ctc 50.014244 loss_rnnt 27.153601 hw_loss 0.240580 lr 0.00071022 rank 4
2023-02-17 13:39:38,618 DEBUG TRAIN Batch 5/7900 loss 28.718790 loss_att 37.078079 loss_ctc 53.204842 loss_rnnt 23.612968 hw_loss 0.317170 lr 0.00070977 rank 1
2023-02-17 13:39:38,622 DEBUG TRAIN Batch 5/7900 loss 17.780153 loss_att 21.615158 loss_ctc 36.352081 loss_rnnt 14.369698 hw_loss 0.313497 lr 0.00070980 rank 0
2023-02-17 13:40:55,136 DEBUG TRAIN Batch 5/8000 loss 30.999125 loss_att 35.086823 loss_ctc 40.833618 loss_rnnt 28.679338 hw_loss 0.358086 lr 0.00070929 rank 5
2023-02-17 13:40:55,137 DEBUG TRAIN Batch 5/8000 loss 15.456077 loss_att 20.352900 loss_ctc 21.624428 loss_rnnt 13.491785 hw_loss 0.304648 lr 0.00070957 rank 6
2023-02-17 13:40:55,138 DEBUG TRAIN Batch 5/8000 loss 21.364737 loss_att 24.905384 loss_ctc 30.577419 loss_rnnt 19.259945 hw_loss 0.315570 lr 0.00070950 rank 4
2023-02-17 13:40:55,138 DEBUG TRAIN Batch 5/8000 loss 15.399676 loss_att 21.302412 loss_ctc 24.330317 loss_rnnt 12.804033 hw_loss 0.420641 lr 0.00070924 rank 3
2023-02-17 13:40:55,139 DEBUG TRAIN Batch 5/8000 loss 16.456228 loss_att 22.554752 loss_ctc 22.107687 loss_rnnt 14.319525 hw_loss 0.306508 lr 0.00070906 rank 1
2023-02-17 13:40:55,140 DEBUG TRAIN Batch 5/8000 loss 18.912601 loss_att 24.769245 loss_ctc 34.829315 loss_rnnt 15.408749 hw_loss 0.394300 lr 0.00070909 rank 0
2023-02-17 13:40:55,139 DEBUG TRAIN Batch 5/8000 loss 26.797596 loss_att 30.103508 loss_ctc 38.456074 loss_rnnt 24.352352 hw_loss 0.430500 lr 0.00070930 rank 7
2023-02-17 13:40:55,188 DEBUG TRAIN Batch 5/8000 loss 17.331909 loss_att 24.947981 loss_ctc 29.356604 loss_rnnt 14.042759 hw_loss 0.304956 lr 0.00070878 rank 2
2023-02-17 13:42:10,880 DEBUG TRAIN Batch 5/8100 loss 12.621185 loss_att 15.368048 loss_ctc 19.454874 loss_rnnt 10.959325 hw_loss 0.377492 lr 0.00070807 rank 2
2023-02-17 13:42:10,881 DEBUG TRAIN Batch 5/8100 loss 15.193206 loss_att 18.735767 loss_ctc 24.604319 loss_rnnt 13.072009 hw_loss 0.296003 lr 0.00070853 rank 3
2023-02-17 13:42:10,881 DEBUG TRAIN Batch 5/8100 loss 17.868801 loss_att 18.709810 loss_ctc 28.612103 loss_rnnt 16.112030 hw_loss 0.292745 lr 0.00070858 rank 5
2023-02-17 13:42:10,882 DEBUG TRAIN Batch 5/8100 loss 17.135265 loss_att 23.382015 loss_ctc 28.073151 loss_rnnt 14.263860 hw_loss 0.306881 lr 0.00070835 rank 1
2023-02-17 13:42:10,882 DEBUG TRAIN Batch 5/8100 loss 11.362731 loss_att 17.320589 loss_ctc 23.423870 loss_rnnt 8.417405 hw_loss 0.273004 lr 0.00070879 rank 4
2023-02-17 13:42:10,883 DEBUG TRAIN Batch 5/8100 loss 21.182327 loss_att 29.950176 loss_ctc 39.369736 loss_rnnt 16.835266 hw_loss 0.315942 lr 0.00070885 rank 6
2023-02-17 13:42:10,885 DEBUG TRAIN Batch 5/8100 loss 13.542809 loss_att 14.640149 loss_ctc 20.017986 loss_rnnt 12.256691 hw_loss 0.381174 lr 0.00070859 rank 7
2023-02-17 13:42:10,888 DEBUG TRAIN Batch 5/8100 loss 26.194704 loss_att 26.750992 loss_ctc 38.795856 loss_rnnt 24.264645 hw_loss 0.259966 lr 0.00070838 rank 0
2023-02-17 13:43:27,806 DEBUG TRAIN Batch 5/8200 loss 10.288344 loss_att 11.877649 loss_ctc 20.609592 loss_rnnt 8.375700 hw_loss 0.409905 lr 0.00070814 rank 6
2023-02-17 13:43:27,811 DEBUG TRAIN Batch 5/8200 loss 17.640261 loss_att 24.447943 loss_ctc 34.168056 loss_rnnt 13.861446 hw_loss 0.400442 lr 0.00070786 rank 5
2023-02-17 13:43:27,814 DEBUG TRAIN Batch 5/8200 loss 11.077239 loss_att 10.852132 loss_ctc 15.437022 loss_rnnt 10.297252 hw_loss 0.456947 lr 0.00070736 rank 2
2023-02-17 13:43:27,814 DEBUG TRAIN Batch 5/8200 loss 12.709023 loss_att 13.567482 loss_ctc 19.715189 loss_rnnt 11.391957 hw_loss 0.396033 lr 0.00070788 rank 7
2023-02-17 13:43:27,816 DEBUG TRAIN Batch 5/8200 loss 12.078726 loss_att 17.152260 loss_ctc 20.816772 loss_rnnt 9.693816 hw_loss 0.384619 lr 0.00070764 rank 1
2023-02-17 13:43:27,816 DEBUG TRAIN Batch 5/8200 loss 25.080482 loss_att 25.814306 loss_ctc 36.089409 loss_rnnt 23.303385 hw_loss 0.304640 lr 0.00070808 rank 4
2023-02-17 13:43:27,818 DEBUG TRAIN Batch 5/8200 loss 17.728485 loss_att 19.591404 loss_ctc 28.609816 loss_rnnt 15.740428 hw_loss 0.308682 lr 0.00070767 rank 0
2023-02-17 13:43:27,862 DEBUG TRAIN Batch 5/8200 loss 16.698084 loss_att 22.397081 loss_ctc 29.183058 loss_rnnt 13.739382 hw_loss 0.289197 lr 0.00070782 rank 3
2023-02-17 13:44:43,032 DEBUG TRAIN Batch 5/8300 loss 32.244560 loss_att 32.654774 loss_ctc 45.332851 loss_rnnt 30.170517 hw_loss 0.462937 lr 0.00070693 rank 1
2023-02-17 13:44:43,035 DEBUG TRAIN Batch 5/8300 loss 22.946432 loss_att 29.200470 loss_ctc 36.684509 loss_rnnt 19.667076 hw_loss 0.369012 lr 0.00070717 rank 7
2023-02-17 13:44:43,036 DEBUG TRAIN Batch 5/8300 loss 8.904451 loss_att 11.535007 loss_ctc 15.674853 loss_rnnt 7.340468 hw_loss 0.253411 lr 0.00070711 rank 3
2023-02-17 13:44:43,036 DEBUG TRAIN Batch 5/8300 loss 16.705246 loss_att 22.494043 loss_ctc 31.137106 loss_rnnt 13.455035 hw_loss 0.315382 lr 0.00070743 rank 6
2023-02-17 13:44:43,037 DEBUG TRAIN Batch 5/8300 loss 33.392918 loss_att 33.871880 loss_ctc 43.675934 loss_rnnt 31.789108 hw_loss 0.256773 lr 0.00070696 rank 0
2023-02-17 13:44:43,038 DEBUG TRAIN Batch 5/8300 loss 11.054247 loss_att 17.618404 loss_ctc 19.210239 loss_rnnt 8.518071 hw_loss 0.254770 lr 0.00070716 rank 5
2023-02-17 13:44:43,038 DEBUG TRAIN Batch 5/8300 loss 17.537556 loss_att 21.161789 loss_ctc 25.015209 loss_rnnt 15.607120 hw_loss 0.391067 lr 0.00070665 rank 2
2023-02-17 13:44:43,043 DEBUG TRAIN Batch 5/8300 loss 39.258675 loss_att 41.194439 loss_ctc 58.164406 loss_rnnt 36.205360 hw_loss 0.272625 lr 0.00070737 rank 4
2023-02-17 13:45:34,750 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 6
2023-02-17 13:45:34,751 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 7
2023-02-17 13:45:34,752 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 3
2023-02-17 13:45:34,754 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 0
2023-02-17 13:45:34,755 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 1
2023-02-17 13:45:34,756 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 4
2023-02-17 13:45:34,759 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 2
2023-02-17 13:45:34,763 DEBUG CV Batch 5/0 loss 2.621481 loss_att 2.968405 loss_ctc 5.103897 loss_rnnt 1.986179 hw_loss 0.440489 history loss 2.524389 rank 5
2023-02-17 13:45:45,801 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 6
2023-02-17 13:45:45,867 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 5
2023-02-17 13:45:45,900 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 7
2023-02-17 13:45:46,057 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 4
2023-02-17 13:45:46,071 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 1
2023-02-17 13:45:46,075 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 2
2023-02-17 13:45:46,310 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 0
2023-02-17 13:45:46,832 DEBUG CV Batch 5/100 loss 12.190494 loss_att 10.974987 loss_ctc 21.784554 loss_rnnt 10.941032 hw_loss 0.400040 history loss 5.351368 rank 3
2023-02-17 13:46:00,222 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 5
2023-02-17 13:46:00,244 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 1
2023-02-17 13:46:00,337 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 7
2023-02-17 13:46:00,412 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 6
2023-02-17 13:46:00,458 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 4
2023-02-17 13:46:00,504 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 2
2023-02-17 13:46:00,641 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 0
2023-02-17 13:46:00,805 DEBUG CV Batch 5/200 loss 13.419197 loss_att 30.001106 loss_ctc 17.900513 loss_rnnt 9.340881 hw_loss 0.308296 history loss 6.070727 rank 3
2023-02-17 13:46:12,240 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 7
2023-02-17 13:46:12,250 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 1
2023-02-17 13:46:12,331 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 5
2023-02-17 13:46:12,346 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 6
2023-02-17 13:46:12,466 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 4
2023-02-17 13:46:12,577 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 2
2023-02-17 13:46:12,933 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 3
2023-02-17 13:46:13,330 DEBUG CV Batch 5/300 loss 8.365968 loss_att 8.564273 loss_ctc 14.524797 loss_rnnt 7.276565 hw_loss 0.428559 history loss 6.270399 rank 0
2023-02-17 13:46:24,054 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 7
2023-02-17 13:46:24,104 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 1
2023-02-17 13:46:24,321 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 6
2023-02-17 13:46:24,380 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 5
2023-02-17 13:46:24,483 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 4
2023-02-17 13:46:24,702 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 2
2023-02-17 13:46:24,998 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 3
2023-02-17 13:46:25,901 DEBUG CV Batch 5/400 loss 42.305630 loss_att 202.973373 loss_ctc 23.192638 loss_rnnt 12.588353 hw_loss 0.247734 history loss 7.365955 rank 0
2023-02-17 13:46:34,547 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 7
2023-02-17 13:46:34,658 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 1
2023-02-17 13:46:34,845 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 5
2023-02-17 13:46:34,845 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 6
2023-02-17 13:46:35,081 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 4
2023-02-17 13:46:35,276 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 2
2023-02-17 13:46:35,648 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 3
2023-02-17 13:46:36,533 DEBUG CV Batch 5/500 loss 10.299467 loss_att 10.289641 loss_ctc 15.894858 loss_rnnt 9.353860 hw_loss 0.377849 history loss 8.335407 rank 0
2023-02-17 13:46:47,142 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 1
2023-02-17 13:46:47,388 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 5
2023-02-17 13:46:47,579 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 7
2023-02-17 13:46:47,626 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 2
2023-02-17 13:46:47,719 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 6
2023-02-17 13:46:48,085 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 3
2023-02-17 13:46:48,605 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 4
2023-02-17 13:46:49,193 DEBUG CV Batch 5/600 loss 9.342294 loss_att 9.694308 loss_ctc 14.616847 loss_rnnt 8.323201 hw_loss 0.460155 history loss 9.389728 rank 0
2023-02-17 13:46:59,765 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 1
2023-02-17 13:46:59,864 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 7
2023-02-17 13:46:59,988 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 2
2023-02-17 13:47:00,061 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 5
2023-02-17 13:47:00,407 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 6
2023-02-17 13:47:01,892 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 4
2023-02-17 13:47:02,445 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 3
2023-02-17 13:47:02,894 DEBUG CV Batch 5/700 loss 31.958715 loss_att 115.845680 loss_ctc 32.870556 loss_rnnt 14.917250 hw_loss 0.267179 history loss 10.087835 rank 0
2023-02-17 13:47:12,134 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 1
2023-02-17 13:47:12,280 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 5
2023-02-17 13:47:12,470 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 6
2023-02-17 13:47:12,717 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 2
2023-02-17 13:47:12,812 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 7
2023-02-17 13:47:15,675 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 3
2023-02-17 13:47:15,761 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 4
2023-02-17 13:47:15,889 DEBUG CV Batch 5/800 loss 15.645812 loss_att 13.065104 loss_ctc 24.279564 loss_rnnt 14.773222 hw_loss 0.445435 history loss 9.454622 rank 0
2023-02-17 13:47:26,287 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 1
2023-02-17 13:47:26,480 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 7
2023-02-17 13:47:26,726 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 5
2023-02-17 13:47:26,822 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 6
2023-02-17 13:47:27,089 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 2
2023-02-17 13:47:30,636 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 0
2023-02-17 13:47:30,858 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 3
2023-02-17 13:47:31,068 DEBUG CV Batch 5/900 loss 12.568536 loss_att 25.546392 loss_ctc 22.247673 loss_rnnt 8.517641 hw_loss 0.308948 history loss 9.227700 rank 4
2023-02-17 13:47:38,438 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 1
2023-02-17 13:47:38,635 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 7
2023-02-17 13:47:38,985 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 5
2023-02-17 13:47:39,009 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 6
2023-02-17 13:47:39,410 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 2
2023-02-17 13:47:43,208 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 3
2023-02-17 13:47:43,556 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 4
2023-02-17 13:47:43,718 DEBUG CV Batch 5/1000 loss 6.358245 loss_att 7.369959 loss_ctc 8.992768 loss_rnnt 5.572656 hw_loss 0.434956 history loss 8.968432 rank 0
2023-02-17 13:47:50,304 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 1
2023-02-17 13:47:50,458 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 7
2023-02-17 13:47:50,815 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 6
2023-02-17 13:47:50,910 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 5
2023-02-17 13:47:51,496 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 2
2023-02-17 13:47:55,188 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 3
2023-02-17 13:47:55,460 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 4
2023-02-17 13:47:55,668 DEBUG CV Batch 5/1100 loss 6.480443 loss_att 6.578595 loss_ctc 10.775622 loss_rnnt 5.649480 hw_loss 0.447452 history loss 8.943943 rank 0
2023-02-17 13:48:00,770 DEBUG CV Batch 5/1200 loss 12.955094 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 1
2023-02-17 13:48:00,773 DEBUG CV Batch 5/1200 loss 12.955094 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 7
2023-02-17 13:48:01,251 DEBUG CV Batch 5/1200 loss 12.955094 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 6
2023-02-17 13:48:01,379 DEBUG CV Batch 5/1200 loss 12.955095 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 5
2023-02-17 13:48:02,111 DEBUG CV Batch 5/1200 loss 12.955094 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 2
2023-02-17 13:48:05,761 DEBUG CV Batch 5/1200 loss 12.955095 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 3
2023-02-17 13:48:06,033 DEBUG CV Batch 5/1200 loss 12.955095 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 4
2023-02-17 13:48:06,239 DEBUG CV Batch 5/1200 loss 12.955095 loss_att 14.030888 loss_ctc 18.571844 loss_rnnt 11.836110 hw_loss 0.290486 history loss 9.360539 rank 0
2023-02-17 13:48:12,733 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 1
2023-02-17 13:48:12,753 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 7
2023-02-17 13:48:13,159 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 6
2023-02-17 13:48:13,353 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 5
2023-02-17 13:48:14,401 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 2
2023-02-17 13:48:17,778 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 3
2023-02-17 13:48:18,263 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 4
2023-02-17 13:48:18,286 DEBUG CV Batch 5/1300 loss 7.658388 loss_att 7.752675 loss_ctc 10.907322 loss_rnnt 6.984957 hw_loss 0.415091 history loss 9.698798 rank 0
2023-02-17 13:48:24,596 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 1
2023-02-17 13:48:24,773 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 6
2023-02-17 13:48:24,938 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 5
2023-02-17 13:48:25,111 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 7
2023-02-17 13:48:27,154 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 2
2023-02-17 13:48:29,794 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 3
2023-02-17 13:48:30,583 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 0
2023-02-17 13:48:31,268 DEBUG CV Batch 5/1400 loss 20.319225 loss_att 72.146828 loss_ctc 23.676704 loss_rnnt 9.305892 hw_loss 0.375280 history loss 10.044676 rank 4
2023-02-17 13:48:36,885 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 1
2023-02-17 13:48:37,269 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 6
2023-02-17 13:48:37,321 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 5
2023-02-17 13:48:37,822 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 7
2023-02-17 13:48:38,878 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 2
2023-02-17 13:48:42,398 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 3
2023-02-17 13:48:43,205 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 0
2023-02-17 13:48:43,654 DEBUG CV Batch 5/1500 loss 9.779518 loss_att 10.471215 loss_ctc 10.113771 loss_rnnt 9.428427 hw_loss 0.315346 history loss 9.827393 rank 4
2023-02-17 13:48:50,641 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 1
2023-02-17 13:48:50,916 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 5
2023-02-17 13:48:51,107 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 6
2023-02-17 13:48:51,557 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 7
2023-02-17 13:48:52,647 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 2
2023-02-17 13:48:56,174 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 3
2023-02-17 13:48:56,889 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 0
2023-02-17 13:48:57,835 DEBUG CV Batch 5/1600 loss 14.041278 loss_att 39.438202 loss_ctc 18.344234 loss_rnnt 8.250635 hw_loss 0.257869 history loss 9.736545 rank 4
2023-02-17 13:49:03,138 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 1
2023-02-17 13:49:03,420 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 5
2023-02-17 13:49:03,566 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 6
2023-02-17 13:49:03,953 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 7
2023-02-17 13:49:05,189 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 2
2023-02-17 13:49:08,704 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 3
2023-02-17 13:49:09,449 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 0
2023-02-17 13:49:10,381 DEBUG CV Batch 5/1700 loss 12.425949 loss_att 12.144834 loss_ctc 21.722755 loss_rnnt 11.012716 hw_loss 0.431030 history loss 9.625804 rank 4
2023-02-17 13:49:12,368 INFO Epoch 5 CV info cv_loss 9.582746205439818
2023-02-17 13:49:12,369 INFO Epoch 6 TRAIN info lr 0.0007066828985740602
2023-02-17 13:49:12,372 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:12,640 INFO Epoch 5 CV info cv_loss 9.58274620399256
2023-02-17 13:49:12,641 INFO Epoch 6 TRAIN info lr 0.0007069583355212467
2023-02-17 13:49:12,646 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:12,720 INFO Epoch 5 CV info cv_loss 9.582746205732715
2023-02-17 13:49:12,721 INFO Epoch 6 TRAIN info lr 0.0007071138523604271
2023-02-17 13:49:12,725 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:13,121 INFO Epoch 5 CV info cv_loss 9.582746204406062
2023-02-17 13:49:13,122 INFO Epoch 6 TRAIN info lr 0.0007068311707635063
2023-02-17 13:49:13,127 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:14,716 INFO Epoch 5 CV info cv_loss 9.582746204733418
2023-02-17 13:49:14,717 INFO Epoch 6 TRAIN info lr 0.0007062668231450268
2023-02-17 13:49:14,721 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:18,064 INFO Epoch 5 CV info cv_loss 9.58274620444052
2023-02-17 13:49:18,065 INFO Epoch 6 TRAIN info lr 0.0007067040746027092
2023-02-17 13:49:18,069 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:18,736 INFO Epoch 5 CV info cv_loss 9.582746205164149
2023-02-17 13:49:18,737 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/5.pt
2023-02-17 13:49:19,406 INFO Epoch 6 TRAIN info lr 0.0007066687822791302
2023-02-17 13:49:19,410 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:49:19,557 INFO Epoch 5 CV info cv_loss 9.582746205767174
2023-02-17 13:49:19,558 INFO Epoch 6 TRAIN info lr 0.0007072128710745472
2023-02-17 13:49:19,561 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 13:50:33,402 DEBUG TRAIN Batch 6/0 loss 11.849524 loss_att 11.435704 loss_ctc 16.995226 loss_rnnt 11.069178 hw_loss 0.331903 lr 0.00070695 rank 5
2023-02-17 13:50:33,404 DEBUG TRAIN Batch 6/0 loss 10.846258 loss_att 11.904134 loss_ctc 14.190491 loss_rnnt 10.001340 hw_loss 0.351460 lr 0.00070682 rank 7
2023-02-17 13:50:33,407 DEBUG TRAIN Batch 6/0 loss 9.280479 loss_att 8.536286 loss_ctc 15.131557 loss_rnnt 8.403617 hw_loss 0.460419 lr 0.00070626 rank 2
2023-02-17 13:50:33,407 DEBUG TRAIN Batch 6/0 loss 9.519458 loss_att 10.219197 loss_ctc 13.220644 loss_rnnt 8.694183 hw_loss 0.359691 lr 0.00070711 rank 6
2023-02-17 13:50:33,407 DEBUG TRAIN Batch 6/0 loss 12.085505 loss_att 11.951490 loss_ctc 14.499886 loss_rnnt 11.561038 hw_loss 0.430036 lr 0.00070666 rank 0
2023-02-17 13:50:33,407 DEBUG TRAIN Batch 6/0 loss 11.537657 loss_att 11.812659 loss_ctc 16.242653 loss_rnnt 10.598900 hw_loss 0.480793 lr 0.00070670 rank 3
2023-02-17 13:50:33,411 DEBUG TRAIN Batch 6/0 loss 12.145902 loss_att 12.482576 loss_ctc 18.561417 loss_rnnt 11.006063 hw_loss 0.407063 lr 0.00070668 rank 1
2023-02-17 13:50:33,427 DEBUG TRAIN Batch 6/0 loss 12.503906 loss_att 12.954249 loss_ctc 17.341206 loss_rnnt 11.561223 hw_loss 0.389328 lr 0.00070721 rank 4
2023-02-17 13:51:49,896 DEBUG TRAIN Batch 6/100 loss 18.920135 loss_att 21.642447 loss_ctc 30.973265 loss_rnnt 16.585590 hw_loss 0.343123 lr 0.00070597 rank 1
2023-02-17 13:51:49,899 DEBUG TRAIN Batch 6/100 loss 25.934477 loss_att 26.737934 loss_ctc 41.720432 loss_rnnt 23.488102 hw_loss 0.339163 lr 0.00070640 rank 6
2023-02-17 13:51:49,902 DEBUG TRAIN Batch 6/100 loss 19.338869 loss_att 23.132092 loss_ctc 28.348959 loss_rnnt 17.198267 hw_loss 0.338645 lr 0.00070650 rank 4
2023-02-17 13:51:49,902 DEBUG TRAIN Batch 6/100 loss 17.628725 loss_att 21.338495 loss_ctc 34.876720 loss_rnnt 14.351267 hw_loss 0.442072 lr 0.00070612 rank 7
2023-02-17 13:51:49,903 DEBUG TRAIN Batch 6/100 loss 17.094391 loss_att 22.270235 loss_ctc 27.513138 loss_rnnt 14.468665 hw_loss 0.377610 lr 0.00070556 rank 2
2023-02-17 13:51:49,903 DEBUG TRAIN Batch 6/100 loss 31.330200 loss_att 33.933674 loss_ctc 48.911545 loss_rnnt 28.326162 hw_loss 0.260932 lr 0.00070625 rank 5
2023-02-17 13:51:49,905 DEBUG TRAIN Batch 6/100 loss 25.393236 loss_att 29.783440 loss_ctc 44.591518 loss_rnnt 21.792410 hw_loss 0.305653 lr 0.00070599 rank 3
2023-02-17 13:51:49,907 DEBUG TRAIN Batch 6/100 loss 18.358061 loss_att 23.143276 loss_ctc 27.215893 loss_rnnt 16.102722 hw_loss 0.219847 lr 0.00070596 rank 0
2023-02-17 13:53:04,670 DEBUG TRAIN Batch 6/200 loss 11.871215 loss_att 19.632927 loss_ctc 22.461590 loss_rnnt 8.732532 hw_loss 0.326798 lr 0.00070529 rank 3
2023-02-17 13:53:04,671 DEBUG TRAIN Batch 6/200 loss 18.202768 loss_att 23.914532 loss_ctc 32.562649 loss_rnnt 14.964203 hw_loss 0.340428 lr 0.00070570 rank 6
2023-02-17 13:53:04,673 DEBUG TRAIN Batch 6/200 loss 12.174702 loss_att 15.267881 loss_ctc 21.241796 loss_rnnt 10.165512 hw_loss 0.340514 lr 0.00070554 rank 5
2023-02-17 13:53:04,674 DEBUG TRAIN Batch 6/200 loss 9.674666 loss_att 15.488481 loss_ctc 17.129700 loss_rnnt 7.349789 hw_loss 0.315207 lr 0.00070542 rank 7
2023-02-17 13:53:04,675 DEBUG TRAIN Batch 6/200 loss 31.759745 loss_att 41.493057 loss_ctc 52.818359 loss_rnnt 26.828421 hw_loss 0.331587 lr 0.00070485 rank 2
2023-02-17 13:53:04,676 DEBUG TRAIN Batch 6/200 loss 16.753946 loss_att 19.841316 loss_ctc 22.957455 loss_rnnt 15.165014 hw_loss 0.270606 lr 0.00070527 rank 1
2023-02-17 13:53:04,681 DEBUG TRAIN Batch 6/200 loss 28.730103 loss_att 31.391235 loss_ctc 47.580120 loss_rnnt 25.515213 hw_loss 0.317487 lr 0.00070525 rank 0
2023-02-17 13:53:04,684 DEBUG TRAIN Batch 6/200 loss 23.443594 loss_att 31.133129 loss_ctc 37.317730 loss_rnnt 19.895466 hw_loss 0.300630 lr 0.00070580 rank 4
2023-02-17 13:54:22,897 DEBUG TRAIN Batch 6/300 loss 25.663040 loss_att 27.436632 loss_ctc 40.020775 loss_rnnt 23.202297 hw_loss 0.359362 lr 0.00070484 rank 5
2023-02-17 13:54:22,901 DEBUG TRAIN Batch 6/300 loss 13.181069 loss_att 20.122759 loss_ctc 22.990334 loss_rnnt 10.258693 hw_loss 0.424005 lr 0.00070471 rank 7
2023-02-17 13:54:22,901 DEBUG TRAIN Batch 6/300 loss 22.533281 loss_att 27.003685 loss_ctc 35.882736 loss_rnnt 19.665281 hw_loss 0.363738 lr 0.00070455 rank 0
2023-02-17 13:54:22,902 DEBUG TRAIN Batch 6/300 loss 16.444023 loss_att 21.595924 loss_ctc 23.402775 loss_rnnt 14.262597 hw_loss 0.418518 lr 0.00070499 rank 6
2023-02-17 13:54:22,903 DEBUG TRAIN Batch 6/300 loss 18.144058 loss_att 24.022524 loss_ctc 26.018171 loss_rnnt 15.706274 hw_loss 0.397890 lr 0.00070509 rank 4
2023-02-17 13:54:22,904 DEBUG TRAIN Batch 6/300 loss 24.209000 loss_att 26.335087 loss_ctc 34.916111 loss_rnnt 22.188995 hw_loss 0.313443 lr 0.00070416 rank 2
2023-02-17 13:54:22,904 DEBUG TRAIN Batch 6/300 loss 18.379051 loss_att 22.769341 loss_ctc 36.209381 loss_rnnt 14.869269 hw_loss 0.476899 lr 0.00070459 rank 3
2023-02-17 13:54:22,907 DEBUG TRAIN Batch 6/300 loss 18.925200 loss_att 27.065170 loss_ctc 28.623909 loss_rnnt 15.840432 hw_loss 0.306768 lr 0.00070457 rank 1
2023-02-17 13:55:40,918 DEBUG TRAIN Batch 6/400 loss 37.119984 loss_att 34.660351 loss_ctc 49.731869 loss_rnnt 35.767277 hw_loss 0.305719 lr 0.00070402 rank 7
2023-02-17 13:55:40,918 DEBUG TRAIN Batch 6/400 loss 17.866306 loss_att 23.609505 loss_ctc 31.878637 loss_rnnt 14.638430 hw_loss 0.395482 lr 0.00070414 rank 5
2023-02-17 13:55:40,919 DEBUG TRAIN Batch 6/400 loss 26.298412 loss_att 28.949112 loss_ctc 48.282936 loss_rnnt 22.666775 hw_loss 0.319183 lr 0.00070387 rank 1
2023-02-17 13:55:40,921 DEBUG TRAIN Batch 6/400 loss 23.718960 loss_att 21.825447 loss_ctc 32.797577 loss_rnnt 22.629902 hw_loss 0.482391 lr 0.00070430 rank 6
2023-02-17 13:55:40,924 DEBUG TRAIN Batch 6/400 loss 18.528917 loss_att 21.132050 loss_ctc 30.059624 loss_rnnt 16.335733 hw_loss 0.253368 lr 0.00070439 rank 4
2023-02-17 13:55:40,924 DEBUG TRAIN Batch 6/400 loss 10.248775 loss_att 14.588221 loss_ctc 15.796934 loss_rnnt 8.438872 hw_loss 0.379237 lr 0.00070346 rank 2
2023-02-17 13:55:40,934 DEBUG TRAIN Batch 6/400 loss 17.481096 loss_att 21.172764 loss_ctc 25.253876 loss_rnnt 15.546926 hw_loss 0.299002 lr 0.00070386 rank 0
2023-02-17 13:55:40,963 DEBUG TRAIN Batch 6/400 loss 13.804722 loss_att 18.675877 loss_ctc 23.177578 loss_rnnt 11.409329 hw_loss 0.321464 lr 0.00070389 rank 3
2023-02-17 13:56:56,805 DEBUG TRAIN Batch 6/500 loss 21.537048 loss_att 21.649210 loss_ctc 26.288977 loss_rnnt 20.690199 hw_loss 0.357805 lr 0.00070360 rank 6
2023-02-17 13:56:56,806 DEBUG TRAIN Batch 6/500 loss 16.048952 loss_att 19.545135 loss_ctc 25.652090 loss_rnnt 13.929685 hw_loss 0.261774 lr 0.00070317 rank 1
2023-02-17 13:56:56,807 DEBUG TRAIN Batch 6/500 loss 12.757302 loss_att 15.633296 loss_ctc 23.519999 loss_rnnt 10.513420 hw_loss 0.438106 lr 0.00070276 rank 2
2023-02-17 13:56:56,808 DEBUG TRAIN Batch 6/500 loss 18.717400 loss_att 25.988731 loss_ctc 34.801750 loss_rnnt 14.971716 hw_loss 0.275323 lr 0.00070332 rank 7
2023-02-17 13:56:56,808 DEBUG TRAIN Batch 6/500 loss 17.648886 loss_att 23.458601 loss_ctc 33.954971 loss_rnnt 14.114765 hw_loss 0.371309 lr 0.00070319 rank 3
2023-02-17 13:56:56,809 DEBUG TRAIN Batch 6/500 loss 27.908840 loss_att 28.133162 loss_ctc 44.293835 loss_rnnt 25.488142 hw_loss 0.358438 lr 0.00070316 rank 0
2023-02-17 13:56:56,808 DEBUG TRAIN Batch 6/500 loss 9.736382 loss_att 12.622118 loss_ctc 19.942547 loss_rnnt 7.603745 hw_loss 0.365004 lr 0.00070344 rank 5
2023-02-17 13:56:56,814 DEBUG TRAIN Batch 6/500 loss 16.589634 loss_att 20.138000 loss_ctc 26.945990 loss_rnnt 14.346992 hw_loss 0.285224 lr 0.00070370 rank 4
2023-02-17 13:58:13,537 DEBUG TRAIN Batch 6/600 loss 10.008097 loss_att 9.278898 loss_ctc 14.514873 loss_rnnt 9.363473 hw_loss 0.355426 lr 0.00070290 rank 6
2023-02-17 13:58:13,540 DEBUG TRAIN Batch 6/600 loss 19.491943 loss_att 19.260708 loss_ctc 29.490450 loss_rnnt 17.976973 hw_loss 0.427656 lr 0.00070275 rank 5
2023-02-17 13:58:13,542 DEBUG TRAIN Batch 6/600 loss 17.288837 loss_att 17.957167 loss_ctc 26.542706 loss_rnnt 15.742547 hw_loss 0.335204 lr 0.00070250 rank 3
2023-02-17 13:58:13,544 DEBUG TRAIN Batch 6/600 loss 14.194172 loss_att 15.022491 loss_ctc 19.147114 loss_rnnt 13.177729 hw_loss 0.356974 lr 0.00070248 rank 1
2023-02-17 13:58:13,545 DEBUG TRAIN Batch 6/600 loss 15.299839 loss_att 16.656960 loss_ctc 23.070503 loss_rnnt 13.747711 hw_loss 0.458654 lr 0.00070246 rank 0
2023-02-17 13:58:13,546 DEBUG TRAIN Batch 6/600 loss 16.633821 loss_att 18.417931 loss_ctc 26.721592 loss_rnnt 14.727357 hw_loss 0.383638 lr 0.00070262 rank 7
2023-02-17 13:58:13,551 DEBUG TRAIN Batch 6/600 loss 13.446730 loss_att 15.686548 loss_ctc 21.966480 loss_rnnt 11.662908 hw_loss 0.374796 lr 0.00070207 rank 2
2023-02-17 13:58:13,590 DEBUG TRAIN Batch 6/600 loss 15.130962 loss_att 16.963963 loss_ctc 24.372074 loss_rnnt 13.317476 hw_loss 0.402633 lr 0.00070300 rank 4
2023-02-17 13:59:32,155 DEBUG TRAIN Batch 6/700 loss 11.503325 loss_att 13.705637 loss_ctc 15.476927 loss_rnnt 10.409487 hw_loss 0.231678 lr 0.00070179 rank 1
2023-02-17 13:59:32,157 DEBUG TRAIN Batch 6/700 loss 18.080317 loss_att 24.536743 loss_ctc 32.852516 loss_rnnt 14.715336 hw_loss 0.195127 lr 0.00070221 rank 6
2023-02-17 13:59:32,157 DEBUG TRAIN Batch 6/700 loss 18.159128 loss_att 22.820843 loss_ctc 33.486290 loss_rnnt 15.005939 hw_loss 0.332297 lr 0.00070138 rank 2
2023-02-17 13:59:32,157 DEBUG TRAIN Batch 6/700 loss 25.384140 loss_att 27.957119 loss_ctc 37.664299 loss_rnnt 23.072990 hw_loss 0.298501 lr 0.00070206 rank 5
2023-02-17 13:59:32,158 DEBUG TRAIN Batch 6/700 loss 11.313884 loss_att 18.402603 loss_ctc 18.021133 loss_rnnt 8.812146 hw_loss 0.355674 lr 0.00070181 rank 3
2023-02-17 13:59:32,160 DEBUG TRAIN Batch 6/700 loss 22.443756 loss_att 25.266220 loss_ctc 45.111328 loss_rnnt 18.639648 hw_loss 0.407383 lr 0.00070231 rank 4
2023-02-17 13:59:32,170 DEBUG TRAIN Batch 6/700 loss 44.303776 loss_att 51.900097 loss_ctc 50.767303 loss_rnnt 41.786613 hw_loss 0.255171 lr 0.00070177 rank 0
2023-02-17 13:59:32,185 DEBUG TRAIN Batch 6/700 loss 15.763248 loss_att 21.603632 loss_ctc 27.756634 loss_rnnt 12.797956 hw_loss 0.371433 lr 0.00070193 rank 7
2023-02-17 14:00:48,780 DEBUG TRAIN Batch 6/800 loss 6.640810 loss_att 11.255072 loss_ctc 12.214386 loss_rnnt 4.776865 hw_loss 0.371157 lr 0.00070152 rank 6
2023-02-17 14:00:48,781 DEBUG TRAIN Batch 6/800 loss 19.120598 loss_att 25.276888 loss_ctc 30.431728 loss_rnnt 16.212967 hw_loss 0.315420 lr 0.00070124 rank 7
2023-02-17 14:00:48,783 DEBUG TRAIN Batch 6/800 loss 21.031225 loss_att 29.853500 loss_ctc 34.203026 loss_rnnt 17.347488 hw_loss 0.305702 lr 0.00070110 rank 1
2023-02-17 14:00:48,785 DEBUG TRAIN Batch 6/800 loss 29.893824 loss_att 39.460922 loss_ctc 46.989693 loss_rnnt 25.511120 hw_loss 0.355945 lr 0.00070161 rank 4
2023-02-17 14:00:48,786 DEBUG TRAIN Batch 6/800 loss 25.712120 loss_att 29.348911 loss_ctc 34.686329 loss_rnnt 23.627350 hw_loss 0.301590 lr 0.00070112 rank 3
2023-02-17 14:00:48,787 DEBUG TRAIN Batch 6/800 loss 8.036836 loss_att 12.155643 loss_ctc 11.035450 loss_rnnt 6.583547 hw_loss 0.430710 lr 0.00070137 rank 5
2023-02-17 14:00:48,788 DEBUG TRAIN Batch 6/800 loss 19.790813 loss_att 26.859444 loss_ctc 36.755058 loss_rnnt 15.961970 hw_loss 0.287287 lr 0.00070069 rank 2
2023-02-17 14:00:48,789 DEBUG TRAIN Batch 6/800 loss 10.848260 loss_att 20.494696 loss_ctc 24.983376 loss_rnnt 6.915130 hw_loss 0.223428 lr 0.00070108 rank 0
2023-02-17 14:02:04,391 DEBUG TRAIN Batch 6/900 loss 11.957415 loss_att 15.849322 loss_ctc 19.253902 loss_rnnt 10.004428 hw_loss 0.378264 lr 0.00070041 rank 1
2023-02-17 14:02:04,397 DEBUG TRAIN Batch 6/900 loss 15.252235 loss_att 18.532814 loss_ctc 25.742998 loss_rnnt 13.036873 hw_loss 0.300896 lr 0.00070083 rank 6
2023-02-17 14:02:04,397 DEBUG TRAIN Batch 6/900 loss 21.415133 loss_att 25.507328 loss_ctc 28.986334 loss_rnnt 19.463659 hw_loss 0.231638 lr 0.00070068 rank 5
2023-02-17 14:02:04,399 DEBUG TRAIN Batch 6/900 loss 26.177313 loss_att 30.986166 loss_ctc 44.208172 loss_rnnt 22.605515 hw_loss 0.386084 lr 0.00070055 rank 7
2023-02-17 14:02:04,402 DEBUG TRAIN Batch 6/900 loss 34.522022 loss_att 37.487648 loss_ctc 48.778244 loss_rnnt 31.880857 hw_loss 0.276018 lr 0.00070043 rank 3
2023-02-17 14:02:04,405 DEBUG TRAIN Batch 6/900 loss 21.143684 loss_att 28.290314 loss_ctc 41.838364 loss_rnnt 16.779970 hw_loss 0.328306 lr 0.00070000 rank 2
2023-02-17 14:02:04,407 DEBUG TRAIN Batch 6/900 loss 16.073668 loss_att 20.991529 loss_ctc 24.702784 loss_rnnt 13.769491 hw_loss 0.318854 lr 0.00070039 rank 0
2023-02-17 14:02:04,407 DEBUG TRAIN Batch 6/900 loss 12.162569 loss_att 17.066723 loss_ctc 19.978039 loss_rnnt 9.937344 hw_loss 0.379371 lr 0.00070092 rank 4
2023-02-17 14:03:21,537 DEBUG TRAIN Batch 6/1000 loss 14.417597 loss_att 21.601358 loss_ctc 25.965490 loss_rnnt 11.245668 hw_loss 0.366480 lr 0.00069974 rank 3
2023-02-17 14:03:21,540 DEBUG TRAIN Batch 6/1000 loss 18.165907 loss_att 22.394524 loss_ctc 31.366501 loss_rnnt 15.382232 hw_loss 0.333509 lr 0.00070024 rank 4
2023-02-17 14:03:21,542 DEBUG TRAIN Batch 6/1000 loss 19.452736 loss_att 20.168589 loss_ctc 29.618492 loss_rnnt 17.771585 hw_loss 0.342274 lr 0.00069971 rank 0
2023-02-17 14:03:21,542 DEBUG TRAIN Batch 6/1000 loss 24.322498 loss_att 28.661974 loss_ctc 35.613358 loss_rnnt 21.783695 hw_loss 0.310228 lr 0.00069999 rank 5
2023-02-17 14:03:21,545 DEBUG TRAIN Batch 6/1000 loss 9.478254 loss_att 11.681293 loss_ctc 17.518684 loss_rnnt 7.821055 hw_loss 0.271001 lr 0.00069972 rank 1
2023-02-17 14:03:21,562 DEBUG TRAIN Batch 6/1000 loss 22.996267 loss_att 24.659754 loss_ctc 34.233829 loss_rnnt 20.966228 hw_loss 0.373125 lr 0.00069932 rank 2
2023-02-17 14:03:21,581 DEBUG TRAIN Batch 6/1000 loss 17.917131 loss_att 22.359440 loss_ctc 28.992813 loss_rnnt 15.377817 hw_loss 0.326430 lr 0.00069987 rank 7
2023-02-17 14:03:21,589 DEBUG TRAIN Batch 6/1000 loss 9.765548 loss_att 15.512515 loss_ctc 17.051922 loss_rnnt 7.428374 hw_loss 0.405494 lr 0.00070014 rank 6
2023-02-17 14:04:41,227 DEBUG TRAIN Batch 6/1100 loss 17.685316 loss_att 23.646761 loss_ctc 35.417843 loss_rnnt 13.922037 hw_loss 0.387475 lr 0.00069945 rank 6
2023-02-17 14:04:41,229 DEBUG TRAIN Batch 6/1100 loss 19.042616 loss_att 22.100121 loss_ctc 32.602406 loss_rnnt 16.474991 hw_loss 0.277785 lr 0.00069955 rank 4
2023-02-17 14:04:41,234 DEBUG TRAIN Batch 6/1100 loss 31.755379 loss_att 35.227112 loss_ctc 52.475082 loss_rnnt 28.116249 hw_loss 0.341539 lr 0.00069918 rank 7
2023-02-17 14:04:41,235 DEBUG TRAIN Batch 6/1100 loss 29.446924 loss_att 32.745628 loss_ctc 51.030994 loss_rnnt 25.711090 hw_loss 0.371655 lr 0.00069930 rank 5
2023-02-17 14:04:41,235 DEBUG TRAIN Batch 6/1100 loss 20.909876 loss_att 25.931229 loss_ctc 36.465160 loss_rnnt 17.592236 hw_loss 0.448746 lr 0.00069906 rank 3
2023-02-17 14:04:41,238 DEBUG TRAIN Batch 6/1100 loss 20.248730 loss_att 21.218887 loss_ctc 28.343462 loss_rnnt 18.831633 hw_loss 0.269563 lr 0.00069904 rank 1
2023-02-17 14:04:41,238 DEBUG TRAIN Batch 6/1100 loss 22.848301 loss_att 25.895090 loss_ctc 37.739063 loss_rnnt 20.116238 hw_loss 0.257381 lr 0.00069863 rank 2
2023-02-17 14:04:41,239 DEBUG TRAIN Batch 6/1100 loss 15.051660 loss_att 18.732410 loss_ctc 29.162020 loss_rnnt 12.244126 hw_loss 0.356253 lr 0.00069902 rank 0
2023-02-17 14:05:57,422 DEBUG TRAIN Batch 6/1200 loss 15.906763 loss_att 19.626820 loss_ctc 27.859724 loss_rnnt 13.380015 hw_loss 0.354390 lr 0.00069838 rank 3
2023-02-17 14:05:57,423 DEBUG TRAIN Batch 6/1200 loss 11.391644 loss_att 14.184538 loss_ctc 22.580740 loss_rnnt 9.129826 hw_loss 0.396299 lr 0.00069877 rank 6
2023-02-17 14:05:57,423 DEBUG TRAIN Batch 6/1200 loss 25.920092 loss_att 30.486492 loss_ctc 44.686443 loss_rnnt 22.277922 hw_loss 0.425076 lr 0.00069836 rank 1
2023-02-17 14:05:57,423 DEBUG TRAIN Batch 6/1200 loss 19.293676 loss_att 23.435358 loss_ctc 32.724876 loss_rnnt 16.449682 hw_loss 0.421556 lr 0.00069887 rank 4
2023-02-17 14:05:57,425 DEBUG TRAIN Batch 6/1200 loss 12.967945 loss_att 18.132607 loss_ctc 23.340647 loss_rnnt 10.385531 hw_loss 0.312104 lr 0.00069850 rank 7
2023-02-17 14:05:57,426 DEBUG TRAIN Batch 6/1200 loss 19.687096 loss_att 21.642094 loss_ctc 29.959396 loss_rnnt 17.716578 hw_loss 0.393520 lr 0.00069862 rank 5
2023-02-17 14:05:57,430 DEBUG TRAIN Batch 6/1200 loss 12.404479 loss_att 13.422539 loss_ctc 20.652082 loss_rnnt 10.895617 hw_loss 0.385441 lr 0.00069834 rank 0
2023-02-17 14:05:57,431 DEBUG TRAIN Batch 6/1200 loss 23.643625 loss_att 26.212341 loss_ctc 38.607967 loss_rnnt 20.974072 hw_loss 0.301063 lr 0.00069795 rank 2
2023-02-17 14:07:13,252 DEBUG TRAIN Batch 6/1300 loss 19.534781 loss_att 24.616360 loss_ctc 27.084318 loss_rnnt 17.354115 hw_loss 0.295772 lr 0.00069809 rank 6
2023-02-17 14:07:13,253 DEBUG TRAIN Batch 6/1300 loss 22.362833 loss_att 29.638725 loss_ctc 38.546280 loss_rnnt 18.544735 hw_loss 0.384613 lr 0.00069794 rank 5
2023-02-17 14:07:13,254 DEBUG TRAIN Batch 6/1300 loss 31.398668 loss_att 41.306320 loss_ctc 47.828918 loss_rnnt 27.092564 hw_loss 0.251011 lr 0.00069727 rank 2
2023-02-17 14:07:13,259 DEBUG TRAIN Batch 6/1300 loss 31.208597 loss_att 42.464333 loss_ctc 49.003155 loss_rnnt 26.379566 hw_loss 0.384886 lr 0.00069768 rank 1
2023-02-17 14:07:13,261 DEBUG TRAIN Batch 6/1300 loss 11.116025 loss_att 19.238741 loss_ctc 21.067478 loss_rnnt 8.011644 hw_loss 0.286829 lr 0.00069782 rank 7
2023-02-17 14:07:13,261 DEBUG TRAIN Batch 6/1300 loss 29.036259 loss_att 33.510315 loss_ctc 43.100616 loss_rnnt 26.094969 hw_loss 0.321053 lr 0.00069770 rank 3
2023-02-17 14:07:13,263 DEBUG TRAIN Batch 6/1300 loss 15.555922 loss_att 23.062378 loss_ctc 28.361799 loss_rnnt 12.239791 hw_loss 0.201355 lr 0.00069819 rank 4
2023-02-17 14:07:13,303 DEBUG TRAIN Batch 6/1300 loss 14.254332 loss_att 20.724480 loss_ctc 25.906034 loss_rnnt 11.214615 hw_loss 0.360237 lr 0.00069766 rank 0
2023-02-17 14:08:33,008 DEBUG TRAIN Batch 6/1400 loss 59.933632 loss_att 65.395996 loss_ctc 93.390076 loss_rnnt 54.191833 hw_loss 0.353373 lr 0.00069700 rank 1
2023-02-17 14:08:33,010 DEBUG TRAIN Batch 6/1400 loss 15.037387 loss_att 18.971092 loss_ctc 22.730061 loss_rnnt 12.937236 hw_loss 0.539475 lr 0.00069702 rank 3
2023-02-17 14:08:33,013 DEBUG TRAIN Batch 6/1400 loss 17.715599 loss_att 22.865976 loss_ctc 30.059433 loss_rnnt 14.865885 hw_loss 0.325861 lr 0.00069714 rank 7
2023-02-17 14:08:33,014 DEBUG TRAIN Batch 6/1400 loss 30.289379 loss_att 32.755623 loss_ctc 45.565411 loss_rnnt 27.607832 hw_loss 0.284053 lr 0.00069751 rank 4
2023-02-17 14:08:33,015 DEBUG TRAIN Batch 6/1400 loss 20.150749 loss_att 29.680302 loss_ctc 43.786144 loss_rnnt 14.986934 hw_loss 0.199723 lr 0.00069698 rank 0
2023-02-17 14:08:33,016 DEBUG TRAIN Batch 6/1400 loss 17.090134 loss_att 20.993124 loss_ctc 26.643017 loss_rnnt 14.900017 hw_loss 0.254625 lr 0.00069741 rank 6
2023-02-17 14:08:33,017 DEBUG TRAIN Batch 6/1400 loss 24.591190 loss_att 30.021238 loss_ctc 38.645699 loss_rnnt 21.451099 hw_loss 0.337771 lr 0.00069726 rank 5
2023-02-17 14:08:33,062 DEBUG TRAIN Batch 6/1400 loss 13.529218 loss_att 17.149338 loss_ctc 21.116388 loss_rnnt 11.621604 hw_loss 0.322437 lr 0.00069660 rank 2
2023-02-17 14:09:49,870 DEBUG TRAIN Batch 6/1500 loss 11.711971 loss_att 19.088514 loss_ctc 21.960724 loss_rnnt 8.657532 hw_loss 0.398680 lr 0.00069634 rank 3
2023-02-17 14:09:49,873 DEBUG TRAIN Batch 6/1500 loss 16.729912 loss_att 22.415836 loss_ctc 30.132973 loss_rnnt 13.648337 hw_loss 0.294965 lr 0.00069646 rank 7
2023-02-17 14:09:49,874 DEBUG TRAIN Batch 6/1500 loss 26.460648 loss_att 32.457413 loss_ctc 50.354950 loss_rnnt 21.902954 hw_loss 0.323310 lr 0.00069673 rank 6
2023-02-17 14:09:49,875 DEBUG TRAIN Batch 6/1500 loss 9.082391 loss_att 12.162872 loss_ctc 14.661545 loss_rnnt 7.549427 hw_loss 0.324338 lr 0.00069683 rank 4
2023-02-17 14:09:49,876 DEBUG TRAIN Batch 6/1500 loss 15.588049 loss_att 18.821148 loss_ctc 24.152622 loss_rnnt 13.630153 hw_loss 0.317500 lr 0.00069631 rank 0
2023-02-17 14:09:49,878 DEBUG TRAIN Batch 6/1500 loss 22.310417 loss_att 26.642063 loss_ctc 41.670723 loss_rnnt 18.640488 hw_loss 0.416673 lr 0.00069592 rank 2
2023-02-17 14:09:49,891 DEBUG TRAIN Batch 6/1500 loss 16.368229 loss_att 22.569649 loss_ctc 28.259521 loss_rnnt 13.387393 hw_loss 0.290709 lr 0.00069632 rank 1
2023-02-17 14:09:49,900 DEBUG TRAIN Batch 6/1500 loss 26.032061 loss_att 29.964638 loss_ctc 46.892742 loss_rnnt 22.318686 hw_loss 0.272693 lr 0.00069658 rank 5
2023-02-17 14:11:04,443 DEBUG TRAIN Batch 6/1600 loss 28.187431 loss_att 32.335392 loss_ctc 43.339241 loss_rnnt 25.174149 hw_loss 0.306470 lr 0.00069565 rank 1
2023-02-17 14:11:04,445 DEBUG TRAIN Batch 6/1600 loss 18.422913 loss_att 25.280659 loss_ctc 35.098095 loss_rnnt 14.641708 hw_loss 0.349309 lr 0.00069606 rank 6
2023-02-17 14:11:04,448 DEBUG TRAIN Batch 6/1600 loss 23.421402 loss_att 26.889378 loss_ctc 39.962292 loss_rnnt 20.291256 hw_loss 0.433308 lr 0.00069567 rank 3
2023-02-17 14:11:04,447 DEBUG TRAIN Batch 6/1600 loss 11.658992 loss_att 16.001472 loss_ctc 26.055603 loss_rnnt 8.650297 hw_loss 0.413722 lr 0.00069579 rank 7
2023-02-17 14:11:04,448 DEBUG TRAIN Batch 6/1600 loss 15.765906 loss_att 19.319485 loss_ctc 29.245665 loss_rnnt 13.071718 hw_loss 0.349071 lr 0.00069525 rank 2
2023-02-17 14:11:04,449 DEBUG TRAIN Batch 6/1600 loss 16.768288 loss_att 23.082655 loss_ctc 27.100441 loss_rnnt 13.932573 hw_loss 0.366041 lr 0.00069591 rank 5
2023-02-17 14:11:04,450 DEBUG TRAIN Batch 6/1600 loss 24.081720 loss_att 27.488195 loss_ctc 29.970066 loss_rnnt 22.449068 hw_loss 0.311702 lr 0.00069615 rank 4
2023-02-17 14:11:04,454 DEBUG TRAIN Batch 6/1600 loss 13.025068 loss_att 16.305517 loss_ctc 18.042454 loss_rnnt 11.521769 hw_loss 0.334172 lr 0.00069563 rank 0
2023-02-17 14:12:20,817 DEBUG TRAIN Batch 6/1700 loss 18.622868 loss_att 23.904972 loss_ctc 28.886198 loss_rnnt 15.968696 hw_loss 0.429948 lr 0.00069524 rank 5
2023-02-17 14:12:20,825 DEBUG TRAIN Batch 6/1700 loss 27.640980 loss_att 29.498402 loss_ctc 40.332138 loss_rnnt 25.406372 hw_loss 0.320564 lr 0.00069512 rank 7
2023-02-17 14:12:20,825 DEBUG TRAIN Batch 6/1700 loss 31.211559 loss_att 33.901882 loss_ctc 41.833755 loss_rnnt 29.069447 hw_loss 0.352039 lr 0.00069496 rank 0
2023-02-17 14:12:20,828 DEBUG TRAIN Batch 6/1700 loss 9.239276 loss_att 18.310976 loss_ctc 17.715374 loss_rnnt 6.151156 hw_loss 0.269314 lr 0.00069538 rank 6
2023-02-17 14:12:20,831 DEBUG TRAIN Batch 6/1700 loss 13.318536 loss_att 15.052020 loss_ctc 22.591278 loss_rnnt 11.579313 hw_loss 0.292802 lr 0.00069548 rank 4
2023-02-17 14:12:20,839 DEBUG TRAIN Batch 6/1700 loss 10.318993 loss_att 15.122528 loss_ctc 18.876627 loss_rnnt 8.005162 hw_loss 0.397696 lr 0.00069497 rank 1
2023-02-17 14:12:20,847 DEBUG TRAIN Batch 6/1700 loss 14.467892 loss_att 18.229151 loss_ctc 25.221134 loss_rnnt 12.092634 hw_loss 0.354823 lr 0.00069458 rank 2
2023-02-17 14:12:20,872 DEBUG TRAIN Batch 6/1700 loss 11.518580 loss_att 15.708372 loss_ctc 25.161648 loss_rnnt 8.664720 hw_loss 0.369050 lr 0.00069499 rank 3
2023-02-17 14:13:40,633 DEBUG TRAIN Batch 6/1800 loss 22.870987 loss_att 27.640560 loss_ctc 34.525841 loss_rnnt 20.198683 hw_loss 0.308271 lr 0.00069471 rank 6
2023-02-17 14:13:40,633 DEBUG TRAIN Batch 6/1800 loss 19.130363 loss_att 24.926479 loss_ctc 31.642647 loss_rnnt 16.090286 hw_loss 0.398529 lr 0.00069457 rank 5
2023-02-17 14:13:40,634 DEBUG TRAIN Batch 6/1800 loss 11.310846 loss_att 14.496206 loss_ctc 18.022947 loss_rnnt 9.579616 hw_loss 0.373524 lr 0.00069444 rank 7
2023-02-17 14:13:40,635 DEBUG TRAIN Batch 6/1800 loss 19.430962 loss_att 22.574041 loss_ctc 35.847080 loss_rnnt 16.434914 hw_loss 0.334906 lr 0.00069429 rank 0
2023-02-17 14:13:40,637 DEBUG TRAIN Batch 6/1800 loss 20.255342 loss_att 23.388420 loss_ctc 32.585751 loss_rnnt 17.795862 hw_loss 0.354017 lr 0.00069430 rank 1
2023-02-17 14:13:40,636 DEBUG TRAIN Batch 6/1800 loss 11.182563 loss_att 14.796326 loss_ctc 20.155310 loss_rnnt 9.061070 hw_loss 0.379448 lr 0.00069432 rank 3
2023-02-17 14:13:40,640 DEBUG TRAIN Batch 6/1800 loss 21.311306 loss_att 25.069405 loss_ctc 37.699234 loss_rnnt 18.173817 hw_loss 0.376525 lr 0.00069481 rank 4
2023-02-17 14:13:40,685 DEBUG TRAIN Batch 6/1800 loss 17.994598 loss_att 19.478962 loss_ctc 25.193514 loss_rnnt 16.557825 hw_loss 0.337583 lr 0.00069391 rank 2
2023-02-17 14:14:56,483 DEBUG TRAIN Batch 6/1900 loss 10.619229 loss_att 10.487032 loss_ctc 13.201935 loss_rnnt 10.067251 hw_loss 0.438855 lr 0.00069404 rank 6
2023-02-17 14:14:56,485 DEBUG TRAIN Batch 6/1900 loss 12.974257 loss_att 14.645887 loss_ctc 17.813177 loss_rnnt 11.773320 hw_loss 0.415166 lr 0.00069378 rank 7
2023-02-17 14:14:56,487 DEBUG TRAIN Batch 6/1900 loss 37.679623 loss_att 45.895927 loss_ctc 64.675003 loss_rnnt 32.299889 hw_loss 0.257045 lr 0.00069366 rank 3
2023-02-17 14:14:56,489 DEBUG TRAIN Batch 6/1900 loss 16.196077 loss_att 21.172943 loss_ctc 33.550667 loss_rnnt 12.687887 hw_loss 0.372888 lr 0.00069324 rank 2
2023-02-17 14:14:56,492 DEBUG TRAIN Batch 6/1900 loss 16.820549 loss_att 16.858725 loss_ctc 22.323370 loss_rnnt 15.843256 hw_loss 0.442398 lr 0.00069364 rank 1
2023-02-17 14:14:56,493 DEBUG TRAIN Batch 6/1900 loss 14.767904 loss_att 14.981074 loss_ctc 24.462980 loss_rnnt 13.216247 hw_loss 0.405652 lr 0.00069362 rank 0
2023-02-17 14:14:56,494 DEBUG TRAIN Batch 6/1900 loss 14.954414 loss_att 14.726973 loss_ctc 19.120712 loss_rnnt 14.169365 hw_loss 0.515684 lr 0.00069414 rank 4
2023-02-17 14:14:56,494 DEBUG TRAIN Batch 6/1900 loss 13.994630 loss_att 16.150095 loss_ctc 21.814831 loss_rnnt 12.326718 hw_loss 0.363981 lr 0.00069390 rank 5
2023-02-17 14:16:12,408 DEBUG TRAIN Batch 6/2000 loss 10.810331 loss_att 15.364384 loss_ctc 20.596672 loss_rnnt 8.452990 hw_loss 0.265660 lr 0.00069338 rank 6
2023-02-17 14:16:12,411 DEBUG TRAIN Batch 6/2000 loss 26.373745 loss_att 29.405815 loss_ctc 38.875290 loss_rnnt 23.972464 hw_loss 0.239990 lr 0.00069311 rank 7
2023-02-17 14:16:12,411 DEBUG TRAIN Batch 6/2000 loss 20.477724 loss_att 22.422693 loss_ctc 32.202778 loss_rnnt 18.316093 hw_loss 0.392430 lr 0.00069296 rank 0
2023-02-17 14:16:12,411 DEBUG TRAIN Batch 6/2000 loss 18.530005 loss_att 23.355532 loss_ctc 29.353361 loss_rnnt 15.939410 hw_loss 0.341953 lr 0.00069258 rank 2
2023-02-17 14:16:12,413 DEBUG TRAIN Batch 6/2000 loss 11.661043 loss_att 16.952965 loss_ctc 20.922146 loss_rnnt 9.217110 hw_loss 0.282628 lr 0.00069299 rank 3
2023-02-17 14:16:12,415 DEBUG TRAIN Batch 6/2000 loss 21.954576 loss_att 26.282410 loss_ctc 34.271263 loss_rnnt 19.286924 hw_loss 0.299742 lr 0.00069323 rank 5
2023-02-17 14:16:12,415 DEBUG TRAIN Batch 6/2000 loss 12.903346 loss_att 20.072048 loss_ctc 31.129789 loss_rnnt 8.854637 hw_loss 0.346455 lr 0.00069297 rank 1
2023-02-17 14:16:12,462 DEBUG TRAIN Batch 6/2000 loss 19.337067 loss_att 21.793819 loss_ctc 32.785347 loss_rnnt 16.907082 hw_loss 0.272871 lr 0.00069347 rank 4
2023-02-17 14:17:31,695 DEBUG TRAIN Batch 6/2100 loss 16.292690 loss_att 20.816357 loss_ctc 38.096901 loss_rnnt 12.281740 hw_loss 0.373102 lr 0.00069232 rank 3
2023-02-17 14:17:31,698 DEBUG TRAIN Batch 6/2100 loss 7.378536 loss_att 11.432181 loss_ctc 13.414302 loss_rnnt 5.545492 hw_loss 0.407901 lr 0.00069230 rank 1
2023-02-17 14:17:31,700 DEBUG TRAIN Batch 6/2100 loss 16.315775 loss_att 20.762497 loss_ctc 25.313591 loss_rnnt 14.022728 hw_loss 0.382491 lr 0.00069271 rank 6
2023-02-17 14:17:31,702 DEBUG TRAIN Batch 6/2100 loss 15.717898 loss_att 19.296288 loss_ctc 29.381222 loss_rnnt 13.059740 hw_loss 0.226318 lr 0.00069256 rank 5
2023-02-17 14:17:31,703 DEBUG TRAIN Batch 6/2100 loss 11.347975 loss_att 16.705006 loss_ctc 17.441826 loss_rnnt 9.294434 hw_loss 0.318040 lr 0.00069280 rank 4
2023-02-17 14:17:31,705 DEBUG TRAIN Batch 6/2100 loss 11.334950 loss_att 19.242805 loss_ctc 25.231384 loss_rnnt 7.720539 hw_loss 0.337465 lr 0.00069244 rank 7
2023-02-17 14:17:31,707 DEBUG TRAIN Batch 6/2100 loss 11.010067 loss_att 14.984552 loss_ctc 17.762861 loss_rnnt 9.104473 hw_loss 0.394359 lr 0.00069229 rank 0
2023-02-17 14:17:31,732 DEBUG TRAIN Batch 6/2100 loss 22.726883 loss_att 26.777864 loss_ctc 32.673382 loss_rnnt 20.457207 hw_loss 0.249899 lr 0.00069191 rank 2
2023-02-17 14:18:47,984 DEBUG TRAIN Batch 6/2200 loss 7.710938 loss_att 14.484633 loss_ctc 15.465342 loss_rnnt 5.162459 hw_loss 0.299661 lr 0.00069178 rank 7
2023-02-17 14:18:47,985 DEBUG TRAIN Batch 6/2200 loss 7.817490 loss_att 13.742613 loss_ctc 18.050995 loss_rnnt 5.084568 hw_loss 0.343931 lr 0.00069205 rank 6
2023-02-17 14:18:47,986 DEBUG TRAIN Batch 6/2200 loss 28.150108 loss_att 35.461586 loss_ctc 53.556225 loss_rnnt 23.132139 hw_loss 0.315356 lr 0.00069164 rank 1
2023-02-17 14:18:47,989 DEBUG TRAIN Batch 6/2200 loss 21.872465 loss_att 28.637693 loss_ctc 35.279263 loss_rnnt 18.540972 hw_loss 0.357890 lr 0.00069166 rank 3
2023-02-17 14:18:47,990 DEBUG TRAIN Batch 6/2200 loss 25.722858 loss_att 35.369839 loss_ctc 39.929604 loss_rnnt 21.745369 hw_loss 0.288485 lr 0.00069190 rank 5
2023-02-17 14:18:47,993 DEBUG TRAIN Batch 6/2200 loss 14.137598 loss_att 18.587757 loss_ctc 22.266712 loss_rnnt 11.996311 hw_loss 0.313826 lr 0.00069125 rank 2
2023-02-17 14:18:47,993 DEBUG TRAIN Batch 6/2200 loss 10.286270 loss_att 14.223263 loss_ctc 18.484451 loss_rnnt 8.212002 hw_loss 0.363336 lr 0.00069214 rank 4
2023-02-17 14:18:47,994 DEBUG TRAIN Batch 6/2200 loss 26.384706 loss_att 35.000900 loss_ctc 35.288513 loss_rnnt 23.276373 hw_loss 0.371105 lr 0.00069163 rank 0
2023-02-17 14:20:03,135 DEBUG TRAIN Batch 6/2300 loss 15.668576 loss_att 21.783733 loss_ctc 27.892853 loss_rnnt 12.595344 hw_loss 0.413057 lr 0.00069112 rank 7
2023-02-17 14:20:03,136 DEBUG TRAIN Batch 6/2300 loss 49.984596 loss_att 54.017891 loss_ctc 71.201942 loss_rnnt 46.241856 hw_loss 0.200818 lr 0.00069098 rank 1
2023-02-17 14:20:03,138 DEBUG TRAIN Batch 6/2300 loss 17.203588 loss_att 19.831257 loss_ctc 28.513447 loss_rnnt 14.981407 hw_loss 0.353752 lr 0.00069138 rank 6
2023-02-17 14:20:03,141 DEBUG TRAIN Batch 6/2300 loss 12.920715 loss_att 15.666867 loss_ctc 18.466499 loss_rnnt 11.468719 hw_loss 0.306240 lr 0.00069100 rank 3
2023-02-17 14:20:03,142 DEBUG TRAIN Batch 6/2300 loss 18.040653 loss_att 22.006062 loss_ctc 31.067255 loss_rnnt 15.299294 hw_loss 0.396370 lr 0.00069059 rank 2
2023-02-17 14:20:03,143 DEBUG TRAIN Batch 6/2300 loss 16.407185 loss_att 23.975243 loss_ctc 36.424412 loss_rnnt 11.992127 hw_loss 0.435899 lr 0.00069097 rank 0
2023-02-17 14:20:03,145 DEBUG TRAIN Batch 6/2300 loss 31.874445 loss_att 30.841125 loss_ctc 46.517334 loss_rnnt 29.955107 hw_loss 0.325532 lr 0.00069148 rank 4
2023-02-17 14:20:03,146 DEBUG TRAIN Batch 6/2300 loss 19.745724 loss_att 25.767120 loss_ctc 28.243456 loss_rnnt 17.242283 hw_loss 0.311492 lr 0.00069124 rank 5
2023-02-17 14:21:20,797 DEBUG TRAIN Batch 6/2400 loss 42.081635 loss_att 49.621216 loss_ctc 68.104279 loss_rnnt 36.949078 hw_loss 0.290535 lr 0.00069072 rank 6
2023-02-17 14:21:20,798 DEBUG TRAIN Batch 6/2400 loss 12.901441 loss_att 17.006680 loss_ctc 18.886513 loss_rnnt 11.061822 hw_loss 0.413555 lr 0.00069046 rank 7
2023-02-17 14:21:20,799 DEBUG TRAIN Batch 6/2400 loss 14.578320 loss_att 18.027538 loss_ctc 22.785278 loss_rnnt 12.600169 hw_loss 0.363833 lr 0.00068993 rank 2
2023-02-17 14:21:20,804 DEBUG TRAIN Batch 6/2400 loss 7.642452 loss_att 11.348434 loss_ctc 16.351215 loss_rnnt 5.562119 hw_loss 0.333688 lr 0.00069082 rank 4
2023-02-17 14:21:20,804 DEBUG TRAIN Batch 6/2400 loss 17.948368 loss_att 22.063961 loss_ctc 28.622173 loss_rnnt 15.512418 hw_loss 0.355605 lr 0.00069032 rank 1
2023-02-17 14:21:20,806 DEBUG TRAIN Batch 6/2400 loss 29.359922 loss_att 33.114857 loss_ctc 40.797173 loss_rnnt 26.904936 hw_loss 0.335690 lr 0.00069034 rank 3
2023-02-17 14:21:20,806 DEBUG TRAIN Batch 6/2400 loss 7.647367 loss_att 12.088737 loss_ctc 8.831989 loss_rnnt 6.384241 hw_loss 0.406691 lr 0.00069031 rank 0
2023-02-17 14:21:20,806 DEBUG TRAIN Batch 6/2400 loss 20.251089 loss_att 23.845539 loss_ctc 32.040787 loss_rnnt 17.748465 hw_loss 0.397076 lr 0.00069058 rank 5
2023-02-17 14:22:40,790 DEBUG TRAIN Batch 6/2500 loss 9.760678 loss_att 13.647661 loss_ctc 16.253986 loss_rnnt 7.933205 hw_loss 0.345566 lr 0.00069007 rank 6
2023-02-17 14:22:40,793 DEBUG TRAIN Batch 6/2500 loss 18.454411 loss_att 22.569016 loss_ctc 31.282370 loss_rnnt 15.739875 hw_loss 0.339788 lr 0.00068928 rank 2
2023-02-17 14:22:40,795 DEBUG TRAIN Batch 6/2500 loss 18.831036 loss_att 17.803507 loss_ctc 26.167154 loss_rnnt 17.812737 hw_loss 0.460607 lr 0.00068968 rank 3
2023-02-17 14:22:40,795 DEBUG TRAIN Batch 6/2500 loss 9.410989 loss_att 10.597527 loss_ctc 14.608380 loss_rnnt 8.262916 hw_loss 0.408337 lr 0.00068967 rank 1
2023-02-17 14:22:40,796 DEBUG TRAIN Batch 6/2500 loss 19.924398 loss_att 24.253008 loss_ctc 35.373032 loss_rnnt 16.790005 hw_loss 0.391604 lr 0.00068980 rank 7
2023-02-17 14:22:40,797 DEBUG TRAIN Batch 6/2500 loss 11.633799 loss_att 13.973552 loss_ctc 19.334999 loss_rnnt 9.971381 hw_loss 0.314323 lr 0.00069016 rank 4
2023-02-17 14:22:40,799 DEBUG TRAIN Batch 6/2500 loss 20.629486 loss_att 24.314711 loss_ctc 28.881464 loss_rnnt 18.610498 hw_loss 0.340650 lr 0.00068965 rank 0
2023-02-17 14:22:40,799 DEBUG TRAIN Batch 6/2500 loss 8.821264 loss_att 12.769901 loss_ctc 18.218510 loss_rnnt 6.608655 hw_loss 0.318592 lr 0.00068992 rank 5
2023-02-17 14:23:57,432 DEBUG TRAIN Batch 6/2600 loss 16.232119 loss_att 27.548412 loss_ctc 26.067425 loss_rnnt 12.420977 hw_loss 0.443454 lr 0.00068901 rank 1
2023-02-17 14:23:57,432 DEBUG TRAIN Batch 6/2600 loss 12.372432 loss_att 17.152584 loss_ctc 24.903950 loss_rnnt 9.560451 hw_loss 0.347030 lr 0.00068915 rank 7
2023-02-17 14:23:57,434 DEBUG TRAIN Batch 6/2600 loss 18.476576 loss_att 26.118576 loss_ctc 26.888794 loss_rnnt 15.710800 hw_loss 0.217024 lr 0.00068927 rank 5
2023-02-17 14:23:57,437 DEBUG TRAIN Batch 6/2600 loss 14.786366 loss_att 21.877504 loss_ctc 28.143848 loss_rnnt 11.416534 hw_loss 0.319887 lr 0.00068941 rank 6
2023-02-17 14:23:57,438 DEBUG TRAIN Batch 6/2600 loss 21.326773 loss_att 19.655209 loss_ctc 25.669600 loss_rnnt 20.787237 hw_loss 0.552765 lr 0.00068900 rank 0
2023-02-17 14:23:57,439 DEBUG TRAIN Batch 6/2600 loss 17.863335 loss_att 23.002018 loss_ctc 32.267204 loss_rnnt 14.740618 hw_loss 0.327118 lr 0.00068950 rank 4
2023-02-17 14:23:57,442 DEBUG TRAIN Batch 6/2600 loss 10.905913 loss_att 18.692436 loss_ctc 20.450649 loss_rnnt 7.902309 hw_loss 0.325628 lr 0.00068862 rank 2
2023-02-17 14:23:57,480 DEBUG TRAIN Batch 6/2600 loss 17.397787 loss_att 21.696751 loss_ctc 25.448807 loss_rnnt 15.278037 hw_loss 0.349663 lr 0.00068903 rank 3
2023-02-17 14:25:15,636 DEBUG TRAIN Batch 6/2700 loss 29.060871 loss_att 33.243542 loss_ctc 44.074387 loss_rnnt 26.095978 hw_loss 0.237291 lr 0.00068861 rank 5
2023-02-17 14:25:15,639 DEBUG TRAIN Batch 6/2700 loss 25.767458 loss_att 34.628635 loss_ctc 46.946892 loss_rnnt 21.010685 hw_loss 0.301148 lr 0.00068885 rank 4
2023-02-17 14:25:15,644 DEBUG TRAIN Batch 6/2700 loss 8.862329 loss_att 10.894350 loss_ctc 17.219284 loss_rnnt 7.178676 hw_loss 0.305604 lr 0.00068875 rank 6
2023-02-17 14:25:15,644 DEBUG TRAIN Batch 6/2700 loss 26.382620 loss_att 31.483076 loss_ctc 43.251900 loss_rnnt 22.969633 hw_loss 0.269362 lr 0.00068836 rank 1
2023-02-17 14:25:15,645 DEBUG TRAIN Batch 6/2700 loss 13.661807 loss_att 18.158123 loss_ctc 26.603798 loss_rnnt 10.877212 hw_loss 0.299499 lr 0.00068849 rank 7
2023-02-17 14:25:15,647 DEBUG TRAIN Batch 6/2700 loss 19.427988 loss_att 23.173483 loss_ctc 37.631927 loss_rnnt 16.099703 hw_loss 0.284987 lr 0.00068834 rank 0
2023-02-17 14:25:15,647 DEBUG TRAIN Batch 6/2700 loss 20.547022 loss_att 26.999504 loss_ctc 39.828880 loss_rnnt 16.517374 hw_loss 0.315443 lr 0.00068838 rank 3
2023-02-17 14:25:15,647 DEBUG TRAIN Batch 6/2700 loss 15.720596 loss_att 19.475517 loss_ctc 25.786709 loss_rnnt 13.497989 hw_loss 0.242767 lr 0.00068797 rank 2
2023-02-17 14:26:32,605 DEBUG TRAIN Batch 6/2800 loss 18.391380 loss_att 24.312878 loss_ctc 33.052704 loss_rnnt 15.098591 hw_loss 0.288085 lr 0.00068772 rank 3
2023-02-17 14:26:32,607 DEBUG TRAIN Batch 6/2800 loss 25.573111 loss_att 32.415165 loss_ctc 45.800613 loss_rnnt 21.348415 hw_loss 0.298657 lr 0.00068810 rank 6
2023-02-17 14:26:32,607 DEBUG TRAIN Batch 6/2800 loss 14.657118 loss_att 16.388969 loss_ctc 17.835587 loss_rnnt 13.703044 hw_loss 0.344826 lr 0.00068819 rank 4
2023-02-17 14:26:32,609 DEBUG TRAIN Batch 6/2800 loss 27.644377 loss_att 33.346893 loss_ctc 45.862160 loss_rnnt 23.875294 hw_loss 0.374142 lr 0.00068784 rank 7
2023-02-17 14:26:32,610 DEBUG TRAIN Batch 6/2800 loss 19.569113 loss_att 23.400965 loss_ctc 34.653881 loss_rnnt 16.610445 hw_loss 0.339365 lr 0.00068769 rank 0
2023-02-17 14:26:32,610 DEBUG TRAIN Batch 6/2800 loss 12.053134 loss_att 17.613350 loss_ctc 23.494812 loss_rnnt 9.272680 hw_loss 0.267852 lr 0.00068771 rank 1
2023-02-17 14:26:32,611 DEBUG TRAIN Batch 6/2800 loss 12.028034 loss_att 13.165941 loss_ctc 16.900572 loss_rnnt 10.973192 hw_loss 0.332978 lr 0.00068796 rank 5
2023-02-17 14:26:32,616 DEBUG TRAIN Batch 6/2800 loss 23.625212 loss_att 27.183641 loss_ctc 33.768845 loss_rnnt 21.425459 hw_loss 0.254216 lr 0.00068732 rank 2
2023-02-17 14:27:51,111 DEBUG TRAIN Batch 6/2900 loss 20.221586 loss_att 25.455446 loss_ctc 34.206306 loss_rnnt 17.137285 hw_loss 0.324182 lr 0.00068708 rank 3
2023-02-17 14:27:51,112 DEBUG TRAIN Batch 6/2900 loss 33.174358 loss_att 35.401093 loss_ctc 53.841408 loss_rnnt 29.763205 hw_loss 0.394129 lr 0.00068706 rank 1
2023-02-17 14:27:51,113 DEBUG TRAIN Batch 6/2900 loss 21.187132 loss_att 28.183773 loss_ctc 32.615150 loss_rnnt 18.092224 hw_loss 0.322207 lr 0.00068745 rank 6
2023-02-17 14:27:51,118 DEBUG TRAIN Batch 6/2900 loss 22.787060 loss_att 24.187531 loss_ctc 36.717682 loss_rnnt 20.480228 hw_loss 0.317474 lr 0.00068719 rank 7
2023-02-17 14:27:51,119 DEBUG TRAIN Batch 6/2900 loss 30.646284 loss_att 42.071682 loss_ctc 52.346016 loss_rnnt 25.316422 hw_loss 0.284031 lr 0.00068731 rank 5
2023-02-17 14:27:51,119 DEBUG TRAIN Batch 6/2900 loss 12.245099 loss_att 17.041096 loss_ctc 21.660091 loss_rnnt 9.845003 hw_loss 0.347931 lr 0.00068667 rank 2
2023-02-17 14:27:51,121 DEBUG TRAIN Batch 6/2900 loss 18.765106 loss_att 22.362202 loss_ctc 33.066021 loss_rnnt 15.933331 hw_loss 0.385436 lr 0.00068704 rank 0
2023-02-17 14:27:51,161 DEBUG TRAIN Batch 6/2900 loss 28.258514 loss_att 30.553127 loss_ctc 40.478806 loss_rnnt 25.970158 hw_loss 0.375115 lr 0.00068754 rank 4
2023-02-17 14:29:08,052 DEBUG TRAIN Batch 6/3000 loss 15.145160 loss_att 15.993399 loss_ctc 23.531914 loss_rnnt 13.674097 hw_loss 0.343464 lr 0.00068666 rank 5
2023-02-17 14:29:08,056 DEBUG TRAIN Batch 6/3000 loss 12.250637 loss_att 18.850433 loss_ctc 22.609430 loss_rnnt 9.351781 hw_loss 0.370732 lr 0.00068680 rank 6
2023-02-17 14:29:08,059 DEBUG TRAIN Batch 6/3000 loss 18.574940 loss_att 20.006437 loss_ctc 29.031385 loss_rnnt 16.700274 hw_loss 0.364074 lr 0.00068603 rank 2
2023-02-17 14:29:08,059 DEBUG TRAIN Batch 6/3000 loss 18.641569 loss_att 21.005447 loss_ctc 31.786026 loss_rnnt 16.241890 hw_loss 0.326829 lr 0.00068689 rank 4
2023-02-17 14:29:08,060 DEBUG TRAIN Batch 6/3000 loss 13.759033 loss_att 18.711288 loss_ctc 24.576118 loss_rnnt 11.134844 hw_loss 0.358987 lr 0.00068643 rank 3
2023-02-17 14:29:08,063 DEBUG TRAIN Batch 6/3000 loss 16.844492 loss_att 20.330902 loss_ctc 24.782681 loss_rnnt 14.898114 hw_loss 0.357507 lr 0.00068641 rank 1
2023-02-17 14:29:08,064 DEBUG TRAIN Batch 6/3000 loss 15.423925 loss_att 19.239628 loss_ctc 25.202742 loss_rnnt 13.167680 hw_loss 0.354867 lr 0.00068654 rank 7
2023-02-17 14:29:08,067 DEBUG TRAIN Batch 6/3000 loss 23.504353 loss_att 25.317942 loss_ctc 36.611824 loss_rnnt 21.173065 hw_loss 0.414201 lr 0.00068639 rank 0
2023-02-17 14:30:22,962 DEBUG TRAIN Batch 6/3100 loss 23.321854 loss_att 23.379045 loss_ctc 32.576141 loss_rnnt 21.884350 hw_loss 0.360301 lr 0.00068575 rank 0
2023-02-17 14:30:22,962 DEBUG TRAIN Batch 6/3100 loss 14.725348 loss_att 14.994250 loss_ctc 21.481764 loss_rnnt 13.589869 hw_loss 0.339079 lr 0.00068590 rank 7
2023-02-17 14:30:22,965 DEBUG TRAIN Batch 6/3100 loss 22.217899 loss_att 24.768852 loss_ctc 34.517761 loss_rnnt 19.872360 hw_loss 0.366309 lr 0.00068616 rank 6
2023-02-17 14:30:22,965 DEBUG TRAIN Batch 6/3100 loss 13.145627 loss_att 14.079775 loss_ctc 20.207270 loss_rnnt 11.841488 hw_loss 0.329545 lr 0.00068576 rank 1
2023-02-17 14:30:22,966 DEBUG TRAIN Batch 6/3100 loss 22.477091 loss_att 24.546173 loss_ctc 31.570124 loss_rnnt 20.660368 hw_loss 0.357189 lr 0.00068601 rank 5
2023-02-17 14:30:22,968 DEBUG TRAIN Batch 6/3100 loss 10.309626 loss_att 14.059896 loss_ctc 19.923443 loss_rnnt 8.101816 hw_loss 0.329838 lr 0.00068538 rank 2
2023-02-17 14:30:22,969 DEBUG TRAIN Batch 6/3100 loss 14.844692 loss_att 19.184000 loss_ctc 23.374207 loss_rnnt 12.692090 hw_loss 0.276507 lr 0.00068578 rank 3
2023-02-17 14:30:22,970 DEBUG TRAIN Batch 6/3100 loss 12.717644 loss_att 15.538722 loss_ctc 16.563477 loss_rnnt 11.453216 hw_loss 0.351442 lr 0.00068625 rank 4
2023-02-17 14:31:43,965 DEBUG TRAIN Batch 6/3200 loss 12.125065 loss_att 14.361090 loss_ctc 17.574921 loss_rnnt 10.780508 hw_loss 0.320072 lr 0.00068514 rank 3
2023-02-17 14:31:43,974 DEBUG TRAIN Batch 6/3200 loss 14.955973 loss_att 15.272148 loss_ctc 24.371025 loss_rnnt 13.475007 hw_loss 0.304481 lr 0.00068551 rank 6
2023-02-17 14:31:43,975 DEBUG TRAIN Batch 6/3200 loss 14.422769 loss_att 24.894041 loss_ctc 28.483717 loss_rnnt 10.249835 hw_loss 0.382287 lr 0.00068537 rank 5
2023-02-17 14:31:43,975 DEBUG TRAIN Batch 6/3200 loss 11.692827 loss_att 16.458431 loss_ctc 14.913040 loss_rnnt 10.119975 hw_loss 0.356941 lr 0.00068474 rank 2
2023-02-17 14:31:43,976 DEBUG TRAIN Batch 6/3200 loss 11.662267 loss_att 10.880645 loss_ctc 15.000974 loss_rnnt 11.131610 hw_loss 0.453412 lr 0.00068560 rank 4
2023-02-17 14:31:43,977 DEBUG TRAIN Batch 6/3200 loss 15.101052 loss_att 23.737568 loss_ctc 29.095184 loss_rnnt 11.308416 hw_loss 0.373966 lr 0.00068525 rank 7
2023-02-17 14:31:43,982 DEBUG TRAIN Batch 6/3200 loss 18.217419 loss_att 19.546421 loss_ctc 36.646805 loss_rnnt 15.314827 hw_loss 0.336636 lr 0.00068512 rank 1
2023-02-17 14:31:44,022 DEBUG TRAIN Batch 6/3200 loss 13.646718 loss_att 14.532243 loss_ctc 24.622881 loss_rnnt 11.783832 hw_loss 0.416800 lr 0.00068511 rank 0
2023-02-17 14:33:00,340 DEBUG TRAIN Batch 6/3300 loss 22.454222 loss_att 28.760662 loss_ctc 37.923233 loss_rnnt 18.883669 hw_loss 0.462611 lr 0.00068487 rank 6
2023-02-17 14:33:00,344 DEBUG TRAIN Batch 6/3300 loss 15.503087 loss_att 21.328350 loss_ctc 33.323540 loss_rnnt 11.773684 hw_loss 0.353043 lr 0.00068461 rank 7
2023-02-17 14:33:00,344 DEBUG TRAIN Batch 6/3300 loss 51.594910 loss_att 57.096413 loss_ctc 78.624893 loss_rnnt 46.756409 hw_loss 0.251632 lr 0.00068448 rank 1
2023-02-17 14:33:00,345 DEBUG TRAIN Batch 6/3300 loss 15.936315 loss_att 23.069454 loss_ctc 29.007694 loss_rnnt 12.523078 hw_loss 0.457047 lr 0.00068449 rank 3
2023-02-17 14:33:00,351 DEBUG TRAIN Batch 6/3300 loss 21.452360 loss_att 26.361757 loss_ctc 30.830450 loss_rnnt 19.093002 hw_loss 0.238249 lr 0.00068496 rank 4
2023-02-17 14:33:00,351 DEBUG TRAIN Batch 6/3300 loss 22.225864 loss_att 26.426676 loss_ctc 36.475689 loss_rnnt 19.302803 hw_loss 0.342978 lr 0.00068473 rank 5
2023-02-17 14:33:00,352 DEBUG TRAIN Batch 6/3300 loss 28.430477 loss_att 33.308414 loss_ctc 42.262943 loss_rnnt 25.449966 hw_loss 0.301112 lr 0.00068410 rank 2
2023-02-17 14:33:00,395 DEBUG TRAIN Batch 6/3300 loss 6.259715 loss_att 10.709645 loss_ctc 10.182838 loss_rnnt 4.684722 hw_loss 0.303607 lr 0.00068446 rank 0
2023-02-17 14:34:16,854 DEBUG TRAIN Batch 6/3400 loss 23.180109 loss_att 25.319452 loss_ctc 30.306799 loss_rnnt 21.610868 hw_loss 0.358398 lr 0.00068346 rank 2
2023-02-17 14:34:16,855 DEBUG TRAIN Batch 6/3400 loss 12.778270 loss_att 18.792025 loss_ctc 15.121879 loss_rnnt 11.058989 hw_loss 0.382592 lr 0.00068397 rank 7
2023-02-17 14:34:16,855 DEBUG TRAIN Batch 6/3400 loss 14.625854 loss_att 18.378017 loss_ctc 23.963434 loss_rnnt 12.453588 hw_loss 0.331544 lr 0.00068384 rank 1
2023-02-17 14:34:16,855 DEBUG TRAIN Batch 6/3400 loss 44.919159 loss_att 45.777027 loss_ctc 62.385571 loss_rnnt 42.230618 hw_loss 0.352718 lr 0.00068423 rank 6
2023-02-17 14:34:16,858 DEBUG TRAIN Batch 6/3400 loss 23.432161 loss_att 29.184175 loss_ctc 36.734055 loss_rnnt 20.280144 hw_loss 0.427555 lr 0.00068385 rank 3
2023-02-17 14:34:16,860 DEBUG TRAIN Batch 6/3400 loss 24.397659 loss_att 28.863958 loss_ctc 37.018768 loss_rnnt 21.660564 hw_loss 0.301913 lr 0.00068432 rank 4
2023-02-17 14:34:16,860 DEBUG TRAIN Batch 6/3400 loss 24.981052 loss_att 30.981859 loss_ctc 48.908855 loss_rnnt 20.401819 hw_loss 0.353802 lr 0.00068408 rank 5
2023-02-17 14:34:16,861 DEBUG TRAIN Batch 6/3400 loss 17.734695 loss_att 21.396828 loss_ctc 29.169615 loss_rnnt 15.264830 hw_loss 0.398972 lr 0.00068382 rank 0
2023-02-17 14:35:35,061 DEBUG TRAIN Batch 6/3500 loss 26.454378 loss_att 29.648235 loss_ctc 44.636406 loss_rnnt 23.209591 hw_loss 0.340779 lr 0.00068320 rank 1
2023-02-17 14:35:35,063 DEBUG TRAIN Batch 6/3500 loss 11.742476 loss_att 18.453669 loss_ctc 22.211494 loss_rnnt 8.813889 hw_loss 0.357150 lr 0.00068333 rank 7
2023-02-17 14:35:35,062 DEBUG TRAIN Batch 6/3500 loss 18.682116 loss_att 23.525688 loss_ctc 31.868481 loss_rnnt 15.795204 hw_loss 0.300028 lr 0.00068359 rank 6
2023-02-17 14:35:35,067 DEBUG TRAIN Batch 6/3500 loss 11.839102 loss_att 18.662052 loss_ctc 29.708422 loss_rnnt 7.912208 hw_loss 0.336991 lr 0.00068322 rank 3
2023-02-17 14:35:35,068 DEBUG TRAIN Batch 6/3500 loss 16.934708 loss_att 19.326801 loss_ctc 26.215017 loss_rnnt 15.042281 hw_loss 0.331188 lr 0.00068345 rank 5
2023-02-17 14:35:35,071 DEBUG TRAIN Batch 6/3500 loss 15.100631 loss_att 20.974190 loss_ctc 26.889824 loss_rnnt 12.151557 hw_loss 0.379630 lr 0.00068368 rank 4
2023-02-17 14:35:35,073 DEBUG TRAIN Batch 6/3500 loss 14.155383 loss_att 17.529587 loss_ctc 23.785698 loss_rnnt 12.037174 hw_loss 0.298736 lr 0.00068318 rank 0
2023-02-17 14:35:35,113 DEBUG TRAIN Batch 6/3500 loss 15.345305 loss_att 21.133665 loss_ctc 24.941093 loss_rnnt 12.650299 hw_loss 0.483557 lr 0.00068282 rank 2
2023-02-17 14:36:53,282 DEBUG TRAIN Batch 6/3600 loss 10.853436 loss_att 15.595809 loss_ctc 20.125301 loss_rnnt 8.488781 hw_loss 0.337370 lr 0.00068295 rank 6
2023-02-17 14:36:53,289 DEBUG TRAIN Batch 6/3600 loss 40.002110 loss_att 47.730286 loss_ctc 64.896255 loss_rnnt 34.950386 hw_loss 0.350378 lr 0.00068258 rank 3
2023-02-17 14:36:53,289 DEBUG TRAIN Batch 6/3600 loss 13.325899 loss_att 15.498378 loss_ctc 25.937479 loss_rnnt 11.018081 hw_loss 0.359585 lr 0.00068218 rank 2
2023-02-17 14:36:53,291 DEBUG TRAIN Batch 6/3600 loss 10.764094 loss_att 14.862661 loss_ctc 16.210985 loss_rnnt 9.014356 hw_loss 0.382073 lr 0.00068281 rank 5
2023-02-17 14:36:53,292 DEBUG TRAIN Batch 6/3600 loss 18.321825 loss_att 19.928097 loss_ctc 28.308447 loss_rnnt 16.499687 hw_loss 0.317500 lr 0.00068269 rank 7
2023-02-17 14:36:53,291 DEBUG TRAIN Batch 6/3600 loss 15.669525 loss_att 20.964972 loss_ctc 27.360260 loss_rnnt 12.804412 hw_loss 0.463612 lr 0.00068256 rank 1
2023-02-17 14:36:53,294 DEBUG TRAIN Batch 6/3600 loss 31.583294 loss_att 34.965874 loss_ctc 43.196831 loss_rnnt 29.188477 hw_loss 0.318432 lr 0.00068304 rank 4
2023-02-17 14:36:53,294 DEBUG TRAIN Batch 6/3600 loss 28.614567 loss_att 31.674076 loss_ctc 45.418472 loss_rnnt 25.642124 hw_loss 0.225033 lr 0.00068255 rank 0
2023-02-17 14:38:08,878 DEBUG TRAIN Batch 6/3700 loss 24.605303 loss_att 30.288563 loss_ctc 39.739464 loss_rnnt 21.272932 hw_loss 0.333436 lr 0.00068231 rank 6
2023-02-17 14:38:08,880 DEBUG TRAIN Batch 6/3700 loss 8.991914 loss_att 14.205984 loss_ctc 17.683102 loss_rnnt 6.609806 hw_loss 0.338380 lr 0.00068194 rank 3
2023-02-17 14:38:08,880 DEBUG TRAIN Batch 6/3700 loss 11.296595 loss_att 13.415793 loss_ctc 19.606009 loss_rnnt 9.621944 hw_loss 0.267916 lr 0.00068192 rank 1
2023-02-17 14:38:08,880 DEBUG TRAIN Batch 6/3700 loss 18.513113 loss_att 21.741829 loss_ctc 33.673187 loss_rnnt 15.657315 hw_loss 0.353832 lr 0.00068206 rank 7
2023-02-17 14:38:08,881 DEBUG TRAIN Batch 6/3700 loss 21.795820 loss_att 25.875671 loss_ctc 35.829731 loss_rnnt 18.931608 hw_loss 0.331974 lr 0.00068217 rank 5
2023-02-17 14:38:08,882 DEBUG TRAIN Batch 6/3700 loss 17.925041 loss_att 19.854130 loss_ctc 27.363895 loss_rnnt 16.145407 hw_loss 0.253698 lr 0.00068191 rank 0
2023-02-17 14:38:08,884 DEBUG TRAIN Batch 6/3700 loss 11.617457 loss_att 11.943862 loss_ctc 17.398643 loss_rnnt 10.553122 hw_loss 0.427933 lr 0.00068155 rank 2
2023-02-17 14:38:08,888 DEBUG TRAIN Batch 6/3700 loss 18.598316 loss_att 25.743778 loss_ctc 30.897175 loss_rnnt 15.313791 hw_loss 0.404222 lr 0.00068240 rank 4
2023-02-17 14:39:26,305 DEBUG TRAIN Batch 6/3800 loss 14.218801 loss_att 15.203026 loss_ctc 22.126711 loss_rnnt 12.780788 hw_loss 0.350211 lr 0.00068142 rank 7
2023-02-17 14:39:26,306 DEBUG TRAIN Batch 6/3800 loss 15.560514 loss_att 17.251596 loss_ctc 20.787006 loss_rnnt 14.377006 hw_loss 0.278301 lr 0.00068131 rank 3
2023-02-17 14:39:26,308 DEBUG TRAIN Batch 6/3800 loss 18.106163 loss_att 22.354898 loss_ctc 30.980453 loss_rnnt 15.364444 hw_loss 0.328874 lr 0.00068168 rank 6
2023-02-17 14:39:26,310 DEBUG TRAIN Batch 6/3800 loss 10.895380 loss_att 11.511395 loss_ctc 17.116547 loss_rnnt 9.663186 hw_loss 0.524065 lr 0.00068154 rank 5
2023-02-17 14:39:26,313 DEBUG TRAIN Batch 6/3800 loss 11.699454 loss_att 15.174463 loss_ctc 21.400457 loss_rnnt 9.531521 hw_loss 0.336494 lr 0.00068128 rank 0
2023-02-17 14:39:26,314 DEBUG TRAIN Batch 6/3800 loss 26.790619 loss_att 36.645576 loss_ctc 47.061096 loss_rnnt 21.949041 hw_loss 0.314731 lr 0.00068129 rank 1
2023-02-17 14:39:26,319 DEBUG TRAIN Batch 6/3800 loss 26.751720 loss_att 32.439072 loss_ctc 50.858700 loss_rnnt 22.217886 hw_loss 0.341432 lr 0.00068092 rank 2
2023-02-17 14:39:26,321 DEBUG TRAIN Batch 6/3800 loss 29.016647 loss_att 32.252434 loss_ctc 37.797920 loss_rnnt 27.053604 hw_loss 0.271963 lr 0.00068177 rank 4
2023-02-17 14:40:45,846 DEBUG TRAIN Batch 6/3900 loss 14.882223 loss_att 18.268400 loss_ctc 19.009636 loss_rnnt 13.452540 hw_loss 0.378986 lr 0.00068079 rank 7
2023-02-17 14:40:45,848 DEBUG TRAIN Batch 6/3900 loss 34.465393 loss_att 39.730564 loss_ctc 57.970490 loss_rnnt 30.149557 hw_loss 0.241481 lr 0.00068104 rank 6
2023-02-17 14:40:45,852 DEBUG TRAIN Batch 6/3900 loss 8.687713 loss_att 13.859694 loss_ctc 16.794147 loss_rnnt 6.406496 hw_loss 0.311181 lr 0.00068029 rank 2
2023-02-17 14:40:45,853 DEBUG TRAIN Batch 6/3900 loss 21.015333 loss_att 29.597008 loss_ctc 33.147713 loss_rnnt 17.546499 hw_loss 0.252841 lr 0.00068091 rank 5
2023-02-17 14:40:45,854 DEBUG TRAIN Batch 6/3900 loss 24.077084 loss_att 27.642138 loss_ctc 40.080269 loss_rnnt 21.057549 hw_loss 0.323932 lr 0.00068066 rank 1
2023-02-17 14:40:45,862 DEBUG TRAIN Batch 6/3900 loss 10.621213 loss_att 14.044002 loss_ctc 19.275398 loss_rnnt 8.604972 hw_loss 0.333359 lr 0.00068065 rank 0
2023-02-17 14:40:45,869 DEBUG TRAIN Batch 6/3900 loss 15.139314 loss_att 19.513851 loss_ctc 21.911093 loss_rnnt 13.193550 hw_loss 0.314910 lr 0.00068113 rank 4
2023-02-17 14:40:45,893 DEBUG TRAIN Batch 6/3900 loss 12.294184 loss_att 19.393303 loss_ctc 21.239607 loss_rnnt 9.498070 hw_loss 0.344188 lr 0.00068068 rank 3
2023-02-17 14:42:04,398 DEBUG TRAIN Batch 6/4000 loss 13.351224 loss_att 17.308620 loss_ctc 23.041840 loss_rnnt 11.048656 hw_loss 0.410636 lr 0.00068003 rank 1
2023-02-17 14:42:04,401 DEBUG TRAIN Batch 6/4000 loss 19.966068 loss_att 22.256767 loss_ctc 36.694214 loss_rnnt 17.095757 hw_loss 0.340788 lr 0.00068041 rank 6
2023-02-17 14:42:04,402 DEBUG TRAIN Batch 6/4000 loss 27.964354 loss_att 31.874722 loss_ctc 42.374458 loss_rnnt 25.035152 hw_loss 0.423341 lr 0.00068028 rank 5
2023-02-17 14:42:04,402 DEBUG TRAIN Batch 6/4000 loss 18.659117 loss_att 20.545084 loss_ctc 31.543486 loss_rnnt 16.392262 hw_loss 0.322025 lr 0.00068005 rank 3
2023-02-17 14:42:04,407 DEBUG TRAIN Batch 6/4000 loss 30.762350 loss_att 34.852436 loss_ctc 51.053661 loss_rnnt 27.063250 hw_loss 0.329201 lr 0.00067966 rank 2
2023-02-17 14:42:04,407 DEBUG TRAIN Batch 6/4000 loss 18.055328 loss_att 19.927774 loss_ctc 30.772238 loss_rnnt 15.815561 hw_loss 0.318164 lr 0.00068016 rank 7
2023-02-17 14:42:04,410 DEBUG TRAIN Batch 6/4000 loss 21.015377 loss_att 23.520226 loss_ctc 31.674114 loss_rnnt 18.927855 hw_loss 0.310103 lr 0.00068050 rank 4
2023-02-17 14:42:04,412 DEBUG TRAIN Batch 6/4000 loss 10.899739 loss_att 17.794138 loss_ctc 19.000526 loss_rnnt 8.263803 hw_loss 0.331782 lr 0.00068002 rank 0
2023-02-17 14:43:18,747 DEBUG TRAIN Batch 6/4100 loss 25.309628 loss_att 23.654701 loss_ctc 32.708397 loss_rnnt 24.479607 hw_loss 0.327196 lr 0.00067978 rank 6
2023-02-17 14:43:18,752 DEBUG TRAIN Batch 6/4100 loss 22.792889 loss_att 26.087967 loss_ctc 34.082840 loss_rnnt 20.421204 hw_loss 0.388769 lr 0.00067953 rank 7
2023-02-17 14:43:18,753 DEBUG TRAIN Batch 6/4100 loss 28.844503 loss_att 30.696182 loss_ctc 44.198742 loss_rnnt 26.220261 hw_loss 0.387515 lr 0.00067987 rank 4
2023-02-17 14:43:18,757 DEBUG TRAIN Batch 6/4100 loss 10.398640 loss_att 15.381998 loss_ctc 15.552498 loss_rnnt 8.499226 hw_loss 0.404176 lr 0.00067939 rank 0
2023-02-17 14:43:18,758 DEBUG TRAIN Batch 6/4100 loss 19.332008 loss_att 21.492964 loss_ctc 28.171322 loss_rnnt 17.566425 hw_loss 0.290279 lr 0.00067942 rank 3
2023-02-17 14:43:18,758 DEBUG TRAIN Batch 6/4100 loss 9.503903 loss_att 12.673487 loss_ctc 12.959275 loss_rnnt 8.233068 hw_loss 0.330380 lr 0.00067965 rank 5
2023-02-17 14:43:18,758 DEBUG TRAIN Batch 6/4100 loss 11.824116 loss_att 15.993118 loss_ctc 23.908604 loss_rnnt 9.183779 hw_loss 0.366135 lr 0.00067940 rank 1
2023-02-17 14:43:18,764 DEBUG TRAIN Batch 6/4100 loss 25.593536 loss_att 30.386726 loss_ctc 43.778706 loss_rnnt 22.016514 hw_loss 0.363180 lr 0.00067903 rank 2
2023-02-17 14:44:35,626 DEBUG TRAIN Batch 6/4200 loss 7.765021 loss_att 13.004939 loss_ctc 12.380630 loss_rnnt 5.937325 hw_loss 0.308059 lr 0.00067916 rank 6
2023-02-17 14:44:35,627 DEBUG TRAIN Batch 6/4200 loss 22.720800 loss_att 32.134277 loss_ctc 42.569557 loss_rnnt 18.005878 hw_loss 0.348239 lr 0.00067841 rank 2
2023-02-17 14:44:35,628 DEBUG TRAIN Batch 6/4200 loss 17.672539 loss_att 18.484989 loss_ctc 26.743084 loss_rnnt 16.074070 hw_loss 0.424822 lr 0.00067891 rank 7
2023-02-17 14:44:35,628 DEBUG TRAIN Batch 6/4200 loss 12.891749 loss_att 19.222134 loss_ctc 20.281616 loss_rnnt 10.439058 hw_loss 0.377435 lr 0.00067878 rank 1
2023-02-17 14:44:35,630 DEBUG TRAIN Batch 6/4200 loss 14.220891 loss_att 16.400467 loss_ctc 19.894157 loss_rnnt 12.848372 hw_loss 0.337818 lr 0.00067902 rank 5
2023-02-17 14:44:35,631 DEBUG TRAIN Batch 6/4200 loss 19.039913 loss_att 23.071693 loss_ctc 30.638454 loss_rnnt 16.474087 hw_loss 0.399372 lr 0.00067876 rank 0
2023-02-17 14:44:35,631 DEBUG TRAIN Batch 6/4200 loss 17.088173 loss_att 24.278515 loss_ctc 32.050793 loss_rnnt 13.477966 hw_loss 0.332103 lr 0.00067879 rank 3
2023-02-17 14:44:35,636 DEBUG TRAIN Batch 6/4200 loss 30.108694 loss_att 34.196716 loss_ctc 50.562038 loss_rnnt 26.383181 hw_loss 0.338993 lr 0.00067925 rank 4
2023-02-17 14:45:55,874 DEBUG TRAIN Batch 6/4300 loss 15.906698 loss_att 19.977295 loss_ctc 19.232143 loss_rnnt 14.479393 hw_loss 0.318362 lr 0.00067817 rank 3
2023-02-17 14:45:55,876 DEBUG TRAIN Batch 6/4300 loss 15.341925 loss_att 15.883047 loss_ctc 24.578230 loss_rnnt 13.854179 hw_loss 0.277523 lr 0.00067853 rank 6
2023-02-17 14:45:55,878 DEBUG TRAIN Batch 6/4300 loss 17.199713 loss_att 20.598768 loss_ctc 24.559126 loss_rnnt 15.398267 hw_loss 0.263211 lr 0.00067815 rank 1
2023-02-17 14:45:55,879 DEBUG TRAIN Batch 6/4300 loss 20.416862 loss_att 24.355663 loss_ctc 36.090221 loss_rnnt 17.361565 hw_loss 0.333294 lr 0.00067814 rank 0
2023-02-17 14:45:55,880 DEBUG TRAIN Batch 6/4300 loss 16.127613 loss_att 21.164030 loss_ctc 25.338598 loss_rnnt 13.712633 hw_loss 0.336685 lr 0.00067839 rank 5
2023-02-17 14:45:55,882 DEBUG TRAIN Batch 6/4300 loss 16.750134 loss_att 20.142635 loss_ctc 27.559353 loss_rnnt 14.438951 hw_loss 0.358975 lr 0.00067828 rank 7
2023-02-17 14:45:55,882 DEBUG TRAIN Batch 6/4300 loss 23.277458 loss_att 28.167994 loss_ctc 36.742210 loss_rnnt 20.342514 hw_loss 0.302881 lr 0.00067862 rank 4
2023-02-17 14:45:55,921 DEBUG TRAIN Batch 6/4300 loss 11.332510 loss_att 13.156128 loss_ctc 21.845879 loss_rnnt 9.328440 hw_loss 0.445433 lr 0.00067778 rank 2
2023-02-17 14:47:13,958 DEBUG TRAIN Batch 6/4400 loss 15.470289 loss_att 17.628716 loss_ctc 28.686171 loss_rnnt 13.063239 hw_loss 0.399840 lr 0.00067791 rank 6
2023-02-17 14:47:13,964 DEBUG TRAIN Batch 6/4400 loss 12.987940 loss_att 15.271158 loss_ctc 21.972851 loss_rnnt 11.149234 hw_loss 0.345140 lr 0.00067777 rank 5
2023-02-17 14:47:13,965 DEBUG TRAIN Batch 6/4400 loss 9.053997 loss_att 10.861815 loss_ctc 13.311444 loss_rnnt 7.959998 hw_loss 0.308954 lr 0.00067766 rank 7
2023-02-17 14:47:13,966 DEBUG TRAIN Batch 6/4400 loss 19.174448 loss_att 22.190533 loss_ctc 33.506454 loss_rnnt 16.492353 hw_loss 0.314894 lr 0.00067752 rank 0
2023-02-17 14:47:13,970 DEBUG TRAIN Batch 6/4400 loss 13.606506 loss_att 16.350895 loss_ctc 22.197481 loss_rnnt 11.662278 hw_loss 0.468535 lr 0.00067753 rank 1
2023-02-17 14:47:13,972 DEBUG TRAIN Batch 6/4400 loss 22.083588 loss_att 25.543257 loss_ctc 34.175953 loss_rnnt 19.615345 hw_loss 0.307484 lr 0.00067799 rank 4
2023-02-17 14:47:13,973 DEBUG TRAIN Batch 6/4400 loss 16.003054 loss_att 24.217857 loss_ctc 27.861378 loss_rnnt 12.629860 hw_loss 0.279604 lr 0.00067716 rank 2
2023-02-17 14:47:14,015 DEBUG TRAIN Batch 6/4400 loss 19.718868 loss_att 22.993301 loss_ctc 36.183125 loss_rnnt 16.710745 hw_loss 0.296256 lr 0.00067755 rank 3
2023-02-17 14:48:30,215 DEBUG TRAIN Batch 6/4500 loss 25.722832 loss_att 29.949492 loss_ctc 41.820122 loss_rnnt 22.634176 hw_loss 0.181909 lr 0.00067729 rank 6
2023-02-17 14:48:30,221 DEBUG TRAIN Batch 6/4500 loss 16.266155 loss_att 17.717422 loss_ctc 24.221367 loss_rnnt 14.695416 hw_loss 0.412105 lr 0.00067693 rank 3
2023-02-17 14:48:30,221 DEBUG TRAIN Batch 6/4500 loss 13.469740 loss_att 18.321037 loss_ctc 19.876911 loss_rnnt 11.476961 hw_loss 0.315430 lr 0.00067691 rank 1
2023-02-17 14:48:30,222 DEBUG TRAIN Batch 6/4500 loss 7.175421 loss_att 9.612339 loss_ctc 10.326834 loss_rnnt 6.081809 hw_loss 0.348826 lr 0.00067704 rank 7
2023-02-17 14:48:30,222 DEBUG TRAIN Batch 6/4500 loss 38.191589 loss_att 42.936367 loss_ctc 57.193802 loss_rnnt 34.531235 hw_loss 0.333316 lr 0.00067737 rank 4
2023-02-17 14:48:30,223 DEBUG TRAIN Batch 6/4500 loss 10.723145 loss_att 16.620985 loss_ctc 22.578428 loss_rnnt 7.771393 hw_loss 0.359022 lr 0.00067715 rank 5
2023-02-17 14:48:30,223 DEBUG TRAIN Batch 6/4500 loss 13.228762 loss_att 16.507832 loss_ctc 23.368238 loss_rnnt 11.019678 hw_loss 0.377507 lr 0.00067654 rank 2
2023-02-17 14:48:30,226 DEBUG TRAIN Batch 6/4500 loss 21.728130 loss_att 23.534302 loss_ctc 29.705532 loss_rnnt 20.153225 hw_loss 0.281283 lr 0.00067689 rank 0
2023-02-17 14:49:48,873 DEBUG TRAIN Batch 6/4600 loss 28.352602 loss_att 31.518387 loss_ctc 49.478783 loss_rnnt 24.756611 hw_loss 0.273768 lr 0.00067666 rank 6
2023-02-17 14:49:48,877 DEBUG TRAIN Batch 6/4600 loss 12.867506 loss_att 17.334887 loss_ctc 26.306412 loss_rnnt 10.001471 hw_loss 0.338822 lr 0.00067653 rank 5
2023-02-17 14:49:48,877 DEBUG TRAIN Batch 6/4600 loss 26.601053 loss_att 32.661819 loss_ctc 40.553661 loss_rnnt 23.339989 hw_loss 0.353556 lr 0.00067629 rank 1
2023-02-17 14:49:48,878 DEBUG TRAIN Batch 6/4600 loss 19.588560 loss_att 22.634058 loss_ctc 31.707052 loss_rnnt 17.218138 hw_loss 0.272857 lr 0.00067642 rank 7
2023-02-17 14:49:48,879 DEBUG TRAIN Batch 6/4600 loss 17.867340 loss_att 21.939138 loss_ctc 31.122932 loss_rnnt 15.152774 hw_loss 0.248990 lr 0.00067675 rank 4
2023-02-17 14:49:48,880 DEBUG TRAIN Batch 6/4600 loss 19.572420 loss_att 23.114456 loss_ctc 28.238775 loss_rnnt 17.531691 hw_loss 0.331518 lr 0.00067627 rank 0
2023-02-17 14:49:48,882 DEBUG TRAIN Batch 6/4600 loss 7.733775 loss_att 10.468511 loss_ctc 9.562653 loss_rnnt 6.764558 hw_loss 0.334534 lr 0.00067631 rank 3
2023-02-17 14:49:48,931 DEBUG TRAIN Batch 6/4600 loss 17.123415 loss_att 21.307072 loss_ctc 30.524063 loss_rnnt 14.301758 hw_loss 0.371575 lr 0.00067592 rank 2
2023-02-17 14:51:06,161 DEBUG TRAIN Batch 6/4700 loss 25.430578 loss_att 30.642979 loss_ctc 38.480427 loss_rnnt 22.459509 hw_loss 0.353641 lr 0.00067580 rank 7
2023-02-17 14:51:06,161 DEBUG TRAIN Batch 6/4700 loss 21.720860 loss_att 28.443508 loss_ctc 39.725128 loss_rnnt 17.820423 hw_loss 0.291264 lr 0.00067591 rank 5
2023-02-17 14:51:06,161 DEBUG TRAIN Batch 6/4700 loss 11.931647 loss_att 18.340870 loss_ctc 19.953785 loss_rnnt 9.413383 hw_loss 0.312750 lr 0.00067613 rank 4
2023-02-17 14:51:06,161 DEBUG TRAIN Batch 6/4700 loss 11.482524 loss_att 17.961674 loss_ctc 17.513779 loss_rnnt 9.226055 hw_loss 0.293384 lr 0.00067569 rank 3
2023-02-17 14:51:06,162 DEBUG TRAIN Batch 6/4700 loss 22.529751 loss_att 25.522053 loss_ctc 31.866535 loss_rnnt 20.484474 hw_loss 0.378583 lr 0.00067531 rank 2
2023-02-17 14:51:06,164 DEBUG TRAIN Batch 6/4700 loss 7.331163 loss_att 11.388809 loss_ctc 18.037941 loss_rnnt 4.913599 hw_loss 0.334620 lr 0.00067605 rank 6
2023-02-17 14:51:06,166 DEBUG TRAIN Batch 6/4700 loss 12.809873 loss_att 16.447447 loss_ctc 23.934902 loss_rnnt 10.409434 hw_loss 0.355476 lr 0.00067567 rank 1
2023-02-17 14:51:06,174 DEBUG TRAIN Batch 6/4700 loss 19.815014 loss_att 22.811399 loss_ctc 32.358780 loss_rnnt 17.372662 hw_loss 0.319825 lr 0.00067566 rank 0
2023-02-17 14:52:23,567 DEBUG TRAIN Batch 6/4800 loss 33.327515 loss_att 38.764004 loss_ctc 56.719307 loss_rnnt 28.955128 hw_loss 0.311596 lr 0.00067518 rank 7
2023-02-17 14:52:23,567 DEBUG TRAIN Batch 6/4800 loss 21.613319 loss_att 24.394163 loss_ctc 32.107071 loss_rnnt 19.484039 hw_loss 0.326146 lr 0.00067543 rank 6
2023-02-17 14:52:23,571 DEBUG TRAIN Batch 6/4800 loss 15.417251 loss_att 20.048931 loss_ctc 24.099522 loss_rnnt 13.150347 hw_loss 0.342997 lr 0.00067552 rank 4
2023-02-17 14:52:23,571 DEBUG TRAIN Batch 6/4800 loss 11.500611 loss_att 18.762611 loss_ctc 23.904068 loss_rnnt 8.197115 hw_loss 0.369939 lr 0.00067505 rank 1
2023-02-17 14:52:23,574 DEBUG TRAIN Batch 6/4800 loss 14.653871 loss_att 16.752119 loss_ctc 22.054623 loss_rnnt 13.079391 hw_loss 0.315115 lr 0.00067504 rank 0
2023-02-17 14:52:23,574 DEBUG TRAIN Batch 6/4800 loss 8.626834 loss_att 13.103449 loss_ctc 14.594067 loss_rnnt 6.716967 hw_loss 0.410462 lr 0.00067529 rank 5
2023-02-17 14:52:23,576 DEBUG TRAIN Batch 6/4800 loss 14.832929 loss_att 17.035912 loss_ctc 25.651245 loss_rnnt 12.722126 hw_loss 0.427057 lr 0.00067507 rank 3
2023-02-17 14:52:23,580 DEBUG TRAIN Batch 6/4800 loss 19.904936 loss_att 24.076492 loss_ctc 33.919533 loss_rnnt 16.996399 hw_loss 0.385526 lr 0.00067469 rank 2
2023-02-17 14:53:40,149 DEBUG TRAIN Batch 6/4900 loss 19.278589 loss_att 24.722570 loss_ctc 32.005608 loss_rnnt 16.359482 hw_loss 0.250076 lr 0.00067490 rank 4
2023-02-17 14:53:40,150 DEBUG TRAIN Batch 6/4900 loss 12.832336 loss_att 15.748030 loss_ctc 22.423817 loss_rnnt 10.752691 hw_loss 0.408080 lr 0.00067457 rank 7
2023-02-17 14:53:40,152 DEBUG TRAIN Batch 6/4900 loss 22.827541 loss_att 23.661762 loss_ctc 39.299606 loss_rnnt 20.293489 hw_loss 0.320496 lr 0.00067468 rank 5
2023-02-17 14:53:40,154 DEBUG TRAIN Batch 6/4900 loss 18.212191 loss_att 21.950613 loss_ctc 28.923901 loss_rnnt 15.903600 hw_loss 0.248771 lr 0.00067443 rank 0
2023-02-17 14:53:40,154 DEBUG TRAIN Batch 6/4900 loss 14.777550 loss_att 19.811134 loss_ctc 22.095797 loss_rnnt 12.618076 hw_loss 0.331858 lr 0.00067444 rank 1
2023-02-17 14:53:40,157 DEBUG TRAIN Batch 6/4900 loss 25.957390 loss_att 33.028221 loss_ctc 45.386063 loss_rnnt 21.811523 hw_loss 0.264768 lr 0.00067481 rank 6
2023-02-17 14:53:40,158 DEBUG TRAIN Batch 6/4900 loss 25.379358 loss_att 25.625725 loss_ctc 42.382412 loss_rnnt 22.939959 hw_loss 0.230720 lr 0.00067446 rank 3
2023-02-17 14:53:40,160 DEBUG TRAIN Batch 6/4900 loss 12.892925 loss_att 14.887049 loss_ctc 24.077223 loss_rnnt 10.823817 hw_loss 0.335707 lr 0.00067408 rank 2
2023-02-17 14:55:00,402 DEBUG TRAIN Batch 6/5000 loss 22.993906 loss_att 24.636654 loss_ctc 35.319679 loss_rnnt 20.800762 hw_loss 0.414670 lr 0.00067383 rank 1
2023-02-17 14:55:00,404 DEBUG TRAIN Batch 6/5000 loss 20.699533 loss_att 24.992525 loss_ctc 36.886879 loss_rnnt 17.541817 hw_loss 0.264011 lr 0.00067420 rank 6
2023-02-17 14:55:00,405 DEBUG TRAIN Batch 6/5000 loss 14.840688 loss_att 18.797268 loss_ctc 21.714443 loss_rnnt 12.926761 hw_loss 0.386458 lr 0.00067407 rank 5
2023-02-17 14:55:00,406 DEBUG TRAIN Batch 6/5000 loss 17.784512 loss_att 23.121538 loss_ctc 30.448601 loss_rnnt 14.779984 hw_loss 0.466084 lr 0.00067395 rank 7
2023-02-17 14:55:00,407 DEBUG TRAIN Batch 6/5000 loss 23.302261 loss_att 32.614506 loss_ctc 33.729424 loss_rnnt 19.896900 hw_loss 0.286169 lr 0.00067347 rank 2
2023-02-17 14:55:00,408 DEBUG TRAIN Batch 6/5000 loss 16.323200 loss_att 19.287577 loss_ctc 22.521627 loss_rnnt 14.742989 hw_loss 0.301649 lr 0.00067429 rank 4
2023-02-17 14:55:00,411 DEBUG TRAIN Batch 6/5000 loss 23.712448 loss_att 25.159557 loss_ctc 34.562027 loss_rnnt 21.817457 hw_loss 0.298046 lr 0.00067381 rank 0
2023-02-17 14:55:00,411 DEBUG TRAIN Batch 6/5000 loss 17.976780 loss_att 23.627968 loss_ctc 32.592445 loss_rnnt 14.710377 hw_loss 0.351389 lr 0.00067384 rank 3
2023-02-17 14:56:16,469 DEBUG TRAIN Batch 6/5100 loss 21.975313 loss_att 28.444857 loss_ctc 41.568886 loss_rnnt 17.851433 hw_loss 0.407801 lr 0.00067345 rank 5
2023-02-17 14:56:16,469 DEBUG TRAIN Batch 6/5100 loss 10.220296 loss_att 15.181279 loss_ctc 13.411075 loss_rnnt 8.640981 hw_loss 0.303151 lr 0.00067286 rank 2
2023-02-17 14:56:16,471 DEBUG TRAIN Batch 6/5100 loss 18.428679 loss_att 20.100683 loss_ctc 27.508537 loss_rnnt 16.649498 hw_loss 0.438995 lr 0.00067334 rank 7
2023-02-17 14:56:16,473 DEBUG TRAIN Batch 6/5100 loss 10.930275 loss_att 14.124661 loss_ctc 16.386280 loss_rnnt 9.342464 hw_loss 0.415249 lr 0.00067323 rank 3
2023-02-17 14:56:16,473 DEBUG TRAIN Batch 6/5100 loss 23.064795 loss_att 24.040541 loss_ctc 29.148365 loss_rnnt 21.875097 hw_loss 0.343887 lr 0.00067359 rank 6
2023-02-17 14:56:16,474 DEBUG TRAIN Batch 6/5100 loss 11.043667 loss_att 10.992491 loss_ctc 16.223034 loss_rnnt 10.108948 hw_loss 0.476950 lr 0.00067367 rank 4
2023-02-17 14:56:16,475 DEBUG TRAIN Batch 6/5100 loss 13.811642 loss_att 14.560179 loss_ctc 19.081415 loss_rnnt 12.710891 hw_loss 0.465766 lr 0.00067320 rank 0
2023-02-17 14:56:16,475 DEBUG TRAIN Batch 6/5100 loss 18.501730 loss_att 22.089285 loss_ctc 25.425131 loss_rnnt 16.611057 hw_loss 0.468824 lr 0.00067322 rank 1
2023-02-17 14:57:31,186 DEBUG TRAIN Batch 6/5200 loss 16.424143 loss_att 25.818232 loss_ctc 29.326488 loss_rnnt 12.658984 hw_loss 0.311303 lr 0.00067261 rank 1
2023-02-17 14:57:31,186 DEBUG TRAIN Batch 6/5200 loss 9.482508 loss_att 13.004770 loss_ctc 16.595785 loss_rnnt 7.645445 hw_loss 0.345323 lr 0.00067284 rank 5
2023-02-17 14:57:31,187 DEBUG TRAIN Batch 6/5200 loss 17.076887 loss_att 23.405598 loss_ctc 34.257317 loss_rnnt 13.319003 hw_loss 0.377659 lr 0.00067306 rank 4
2023-02-17 14:57:31,187 DEBUG TRAIN Batch 6/5200 loss 19.955997 loss_att 19.371382 loss_ctc 30.999290 loss_rnnt 18.431190 hw_loss 0.317422 lr 0.00067273 rank 7
2023-02-17 14:57:31,190 DEBUG TRAIN Batch 6/5200 loss 20.203714 loss_att 35.325401 loss_ctc 33.732315 loss_rnnt 15.227938 hw_loss 0.276792 lr 0.00067262 rank 3
2023-02-17 14:57:31,190 DEBUG TRAIN Batch 6/5200 loss 14.504676 loss_att 21.296005 loss_ctc 23.402374 loss_rnnt 11.842039 hw_loss 0.221270 lr 0.00067298 rank 6
2023-02-17 14:57:31,192 DEBUG TRAIN Batch 6/5200 loss 15.083359 loss_att 19.786768 loss_ctc 20.980312 loss_rnnt 13.115420 hw_loss 0.451869 lr 0.00067225 rank 2
2023-02-17 14:57:31,199 DEBUG TRAIN Batch 6/5200 loss 23.166325 loss_att 29.193483 loss_ctc 35.647102 loss_rnnt 20.048742 hw_loss 0.465090 lr 0.00067259 rank 0
2023-02-17 14:58:50,683 DEBUG TRAIN Batch 6/5300 loss 27.534834 loss_att 29.595558 loss_ctc 46.167290 loss_rnnt 24.487701 hw_loss 0.282483 lr 0.00067237 rank 6
2023-02-17 14:58:50,684 DEBUG TRAIN Batch 6/5300 loss 34.088417 loss_att 43.996456 loss_ctc 47.056252 loss_rnnt 30.177349 hw_loss 0.375778 lr 0.00067223 rank 5
2023-02-17 14:58:50,684 DEBUG TRAIN Batch 6/5300 loss 10.690782 loss_att 13.789333 loss_ctc 16.619198 loss_rnnt 9.087936 hw_loss 0.361274 lr 0.00067245 rank 4
2023-02-17 14:58:50,688 DEBUG TRAIN Batch 6/5300 loss 28.129599 loss_att 28.007404 loss_ctc 41.090282 loss_rnnt 26.229374 hw_loss 0.368570 lr 0.00067199 rank 0
2023-02-17 14:58:50,690 DEBUG TRAIN Batch 6/5300 loss 12.784720 loss_att 20.142935 loss_ctc 28.274151 loss_rnnt 9.044140 hw_loss 0.381899 lr 0.00067213 rank 7
2023-02-17 14:58:50,692 DEBUG TRAIN Batch 6/5300 loss 12.758071 loss_att 15.458508 loss_ctc 18.014809 loss_rnnt 11.338161 hw_loss 0.335484 lr 0.00067202 rank 3
2023-02-17 14:58:50,693 DEBUG TRAIN Batch 6/5300 loss 18.414423 loss_att 25.710373 loss_ctc 33.167915 loss_rnnt 14.861729 hw_loss 0.236947 lr 0.00067164 rank 2
2023-02-17 14:58:50,704 DEBUG TRAIN Batch 6/5300 loss 14.322632 loss_att 18.781803 loss_ctc 20.771431 loss_rnnt 12.376686 hw_loss 0.364261 lr 0.00067200 rank 1
2023-02-17 15:00:08,616 DEBUG TRAIN Batch 6/5400 loss 24.566341 loss_att 32.847298 loss_ctc 42.405411 loss_rnnt 20.401920 hw_loss 0.243161 lr 0.00067152 rank 7
2023-02-17 15:00:08,620 DEBUG TRAIN Batch 6/5400 loss 32.689659 loss_att 40.915741 loss_ctc 47.857643 loss_rnnt 28.844135 hw_loss 0.333572 lr 0.00067176 rank 6
2023-02-17 15:00:08,621 DEBUG TRAIN Batch 6/5400 loss 7.124727 loss_att 11.204988 loss_ctc 12.884171 loss_rnnt 5.363521 hw_loss 0.332305 lr 0.00067138 rank 0
2023-02-17 15:00:08,622 DEBUG TRAIN Batch 6/5400 loss 19.065933 loss_att 19.386026 loss_ctc 30.389296 loss_rnnt 17.326096 hw_loss 0.311320 lr 0.00067163 rank 5
2023-02-17 15:00:08,623 DEBUG TRAIN Batch 6/5400 loss 17.521374 loss_att 23.938139 loss_ctc 35.684479 loss_rnnt 13.667561 hw_loss 0.278837 lr 0.00067141 rank 3
2023-02-17 15:00:08,625 DEBUG TRAIN Batch 6/5400 loss 8.994703 loss_att 13.756855 loss_ctc 22.820412 loss_rnnt 6.013392 hw_loss 0.347723 lr 0.00067104 rank 2
2023-02-17 15:00:08,628 DEBUG TRAIN Batch 6/5400 loss 12.747757 loss_att 16.905708 loss_ctc 17.469957 loss_rnnt 11.053270 hw_loss 0.437379 lr 0.00067139 rank 1
2023-02-17 15:00:08,628 DEBUG TRAIN Batch 6/5400 loss 16.839806 loss_att 20.410698 loss_ctc 25.284363 loss_rnnt 14.803896 hw_loss 0.367101 lr 0.00067185 rank 4
2023-02-17 15:01:25,175 DEBUG TRAIN Batch 6/5500 loss 17.479332 loss_att 21.638245 loss_ctc 29.698317 loss_rnnt 14.818920 hw_loss 0.373935 lr 0.00067116 rank 6
2023-02-17 15:01:25,178 DEBUG TRAIN Batch 6/5500 loss 15.678529 loss_att 18.516171 loss_ctc 24.290691 loss_rnnt 13.787849 hw_loss 0.327867 lr 0.00067043 rank 2
2023-02-17 15:01:25,179 DEBUG TRAIN Batch 6/5500 loss 17.872793 loss_att 19.149193 loss_ctc 23.852877 loss_rnnt 16.612625 hw_loss 0.389141 lr 0.00067102 rank 5
2023-02-17 15:01:25,182 DEBUG TRAIN Batch 6/5500 loss 24.031183 loss_att 25.805515 loss_ctc 31.420734 loss_rnnt 22.556343 hw_loss 0.252569 lr 0.00067091 rank 7
2023-02-17 15:01:25,182 DEBUG TRAIN Batch 6/5500 loss 11.319580 loss_att 16.833992 loss_ctc 20.314388 loss_rnnt 8.852683 hw_loss 0.308827 lr 0.00067079 rank 1
2023-02-17 15:01:25,183 DEBUG TRAIN Batch 6/5500 loss 14.020393 loss_att 18.276533 loss_ctc 28.058601 loss_rnnt 11.118501 hw_loss 0.335443 lr 0.00067078 rank 0
2023-02-17 15:01:25,187 DEBUG TRAIN Batch 6/5500 loss 24.857113 loss_att 27.542759 loss_ctc 43.093193 loss_rnnt 21.675657 hw_loss 0.399089 lr 0.00067124 rank 4
2023-02-17 15:01:25,187 DEBUG TRAIN Batch 6/5500 loss 14.359262 loss_att 18.079216 loss_ctc 22.606615 loss_rnnt 12.359135 hw_loss 0.293417 lr 0.00067081 rank 3
2023-02-17 15:02:41,401 DEBUG TRAIN Batch 6/5600 loss 12.511190 loss_att 14.959394 loss_ctc 22.607101 loss_rnnt 10.424552 hw_loss 0.470391 lr 0.00067042 rank 5
2023-02-17 15:02:41,402 DEBUG TRAIN Batch 6/5600 loss 14.613749 loss_att 19.334427 loss_ctc 24.627464 loss_rnnt 12.203865 hw_loss 0.244846 lr 0.00067031 rank 7
2023-02-17 15:02:41,403 DEBUG TRAIN Batch 6/5600 loss 10.662519 loss_att 15.519648 loss_ctc 18.431917 loss_rnnt 8.473448 hw_loss 0.340735 lr 0.00067055 rank 6
2023-02-17 15:02:41,404 DEBUG TRAIN Batch 6/5600 loss 14.908184 loss_att 17.626589 loss_ctc 22.332441 loss_rnnt 13.144135 hw_loss 0.432125 lr 0.00067064 rank 4
2023-02-17 15:02:41,407 DEBUG TRAIN Batch 6/5600 loss 30.567791 loss_att 31.962856 loss_ctc 46.563065 loss_rnnt 27.985744 hw_loss 0.319366 lr 0.00067018 rank 1
2023-02-17 15:02:41,411 DEBUG TRAIN Batch 6/5600 loss 19.705784 loss_att 24.552534 loss_ctc 27.986311 loss_rnnt 17.466286 hw_loss 0.311393 lr 0.00067020 rank 3
2023-02-17 15:02:41,412 DEBUG TRAIN Batch 6/5600 loss 14.298043 loss_att 15.819903 loss_ctc 21.337952 loss_rnnt 12.818286 hw_loss 0.443871 lr 0.00066983 rank 2
2023-02-17 15:02:41,414 DEBUG TRAIN Batch 6/5600 loss 11.491890 loss_att 15.630946 loss_ctc 24.170305 loss_rnnt 8.792671 hw_loss 0.339287 lr 0.00067017 rank 0
2023-02-17 15:04:01,699 DEBUG TRAIN Batch 6/5700 loss 15.782277 loss_att 17.619522 loss_ctc 22.510559 loss_rnnt 14.336226 hw_loss 0.340313 lr 0.00066971 rank 7
2023-02-17 15:04:01,700 DEBUG TRAIN Batch 6/5700 loss 15.526534 loss_att 17.333946 loss_ctc 24.485802 loss_rnnt 13.772063 hw_loss 0.372034 lr 0.00067003 rank 4
2023-02-17 15:04:01,704 DEBUG TRAIN Batch 6/5700 loss 17.848160 loss_att 21.303879 loss_ctc 25.540218 loss_rnnt 15.955904 hw_loss 0.329071 lr 0.00066995 rank 6
2023-02-17 15:04:01,704 DEBUG TRAIN Batch 6/5700 loss 11.101171 loss_att 13.891858 loss_ctc 22.212769 loss_rnnt 8.891626 hw_loss 0.318488 lr 0.00066957 rank 0
2023-02-17 15:04:01,706 DEBUG TRAIN Batch 6/5700 loss 15.182076 loss_att 18.000217 loss_ctc 22.482283 loss_rnnt 13.467920 hw_loss 0.332183 lr 0.00066960 rank 3
2023-02-17 15:04:01,711 DEBUG TRAIN Batch 6/5700 loss 10.028671 loss_att 12.682843 loss_ctc 13.912746 loss_rnnt 8.815838 hw_loss 0.307730 lr 0.00066923 rank 2
2023-02-17 15:04:01,714 DEBUG TRAIN Batch 6/5700 loss 13.398856 loss_att 15.091757 loss_ctc 21.681572 loss_rnnt 11.732162 hw_loss 0.419535 lr 0.00066982 rank 5
2023-02-17 15:04:01,725 DEBUG TRAIN Batch 6/5700 loss 17.891687 loss_att 19.551191 loss_ctc 27.148348 loss_rnnt 16.103729 hw_loss 0.415938 lr 0.00066958 rank 1
2023-02-17 15:05:17,934 DEBUG TRAIN Batch 6/5800 loss 38.901154 loss_att 49.098583 loss_ctc 68.264999 loss_rnnt 32.739147 hw_loss 0.388764 lr 0.00066922 rank 5
2023-02-17 15:05:17,933 DEBUG TRAIN Batch 6/5800 loss 21.467699 loss_att 25.425877 loss_ctc 36.149597 loss_rnnt 18.574705 hw_loss 0.269576 lr 0.00066935 rank 6
2023-02-17 15:05:17,934 DEBUG TRAIN Batch 6/5800 loss 21.763205 loss_att 27.254421 loss_ctc 33.233353 loss_rnnt 18.946087 hw_loss 0.355354 lr 0.00066898 rank 1
2023-02-17 15:05:17,935 DEBUG TRAIN Batch 6/5800 loss 9.347431 loss_att 16.830534 loss_ctc 15.275890 loss_rnnt 6.910159 hw_loss 0.281609 lr 0.00066911 rank 7
2023-02-17 15:05:17,939 DEBUG TRAIN Batch 6/5800 loss 26.209869 loss_att 29.318230 loss_ctc 46.598335 loss_rnnt 22.680645 hw_loss 0.354548 lr 0.00066897 rank 0
2023-02-17 15:05:17,940 DEBUG TRAIN Batch 6/5800 loss 27.694193 loss_att 32.525616 loss_ctc 46.157776 loss_rnnt 24.094639 hw_loss 0.321477 lr 0.00066943 rank 4
2023-02-17 15:05:17,940 DEBUG TRAIN Batch 6/5800 loss 6.586529 loss_att 7.149214 loss_ctc 10.718691 loss_rnnt 5.684905 hw_loss 0.446500 lr 0.00066900 rank 3
2023-02-17 15:05:17,944 DEBUG TRAIN Batch 6/5800 loss 10.853692 loss_att 17.418982 loss_ctc 21.440832 loss_rnnt 8.024693 hw_loss 0.195603 lr 0.00066863 rank 2
2023-02-17 15:06:34,754 DEBUG TRAIN Batch 6/5900 loss 24.958261 loss_att 31.457972 loss_ctc 38.607395 loss_rnnt 21.639410 hw_loss 0.373171 lr 0.00066839 rank 1
2023-02-17 15:06:34,758 DEBUG TRAIN Batch 6/5900 loss 29.592672 loss_att 33.320091 loss_ctc 40.233070 loss_rnnt 27.250019 hw_loss 0.334587 lr 0.00066803 rank 2
2023-02-17 15:06:34,761 DEBUG TRAIN Batch 6/5900 loss 20.673437 loss_att 30.349201 loss_ctc 47.396088 loss_rnnt 14.977989 hw_loss 0.369890 lr 0.00066840 rank 3
2023-02-17 15:06:34,762 DEBUG TRAIN Batch 6/5900 loss 16.719185 loss_att 21.114061 loss_ctc 38.503773 loss_rnnt 12.753352 hw_loss 0.341709 lr 0.00066875 rank 6
2023-02-17 15:06:34,762 DEBUG TRAIN Batch 6/5900 loss 16.254370 loss_att 21.306355 loss_ctc 22.470022 loss_rnnt 14.208232 hw_loss 0.388104 lr 0.00066862 rank 5
2023-02-17 15:06:34,764 DEBUG TRAIN Batch 6/5900 loss 15.418875 loss_att 20.033367 loss_ctc 33.412075 loss_rnnt 11.913848 hw_loss 0.343192 lr 0.00066851 rank 7
2023-02-17 15:06:34,765 DEBUG TRAIN Batch 6/5900 loss 11.160447 loss_att 17.059534 loss_ctc 19.940849 loss_rnnt 8.607555 hw_loss 0.379411 lr 0.00066837 rank 0
2023-02-17 15:06:34,809 DEBUG TRAIN Batch 6/5900 loss 32.217236 loss_att 34.980377 loss_ctc 48.444901 loss_rnnt 29.293968 hw_loss 0.388034 lr 0.00066883 rank 4
2023-02-17 15:07:52,912 DEBUG TRAIN Batch 6/6000 loss 29.728064 loss_att 35.723133 loss_ctc 38.836784 loss_rnnt 27.121750 hw_loss 0.361507 lr 0.00066744 rank 2
2023-02-17 15:07:52,913 DEBUG TRAIN Batch 6/6000 loss 18.673401 loss_att 18.446281 loss_ctc 27.010517 loss_rnnt 17.418638 hw_loss 0.353571 lr 0.00066824 rank 4
2023-02-17 15:07:52,913 DEBUG TRAIN Batch 6/6000 loss 14.923981 loss_att 18.836565 loss_ctc 23.944386 loss_rnnt 12.823351 hw_loss 0.216361 lr 0.00066802 rank 5
2023-02-17 15:07:52,914 DEBUG TRAIN Batch 6/6000 loss 13.217683 loss_att 21.780312 loss_ctc 19.616692 loss_rnnt 10.458620 hw_loss 0.362504 lr 0.00066779 rank 1
2023-02-17 15:07:52,916 DEBUG TRAIN Batch 6/6000 loss 15.214354 loss_att 21.390854 loss_ctc 22.910461 loss_rnnt 12.780193 hw_loss 0.323834 lr 0.00066791 rank 7
2023-02-17 15:07:52,916 DEBUG TRAIN Batch 6/6000 loss 12.460048 loss_att 20.227169 loss_ctc 17.893976 loss_rnnt 10.030250 hw_loss 0.284720 lr 0.00066781 rank 3
2023-02-17 15:07:52,920 DEBUG TRAIN Batch 6/6000 loss 5.138795 loss_att 9.564146 loss_ctc 10.158504 loss_rnnt 3.430996 hw_loss 0.287690 lr 0.00066778 rank 0
2023-02-17 15:07:52,928 DEBUG TRAIN Batch 6/6000 loss 23.420359 loss_att 26.911903 loss_ctc 40.273258 loss_rnnt 20.307709 hw_loss 0.313664 lr 0.00066815 rank 6
2023-02-17 15:09:11,178 DEBUG TRAIN Batch 6/6100 loss 27.345367 loss_att 29.479759 loss_ctc 38.909142 loss_rnnt 25.250750 hw_loss 0.236065 lr 0.00066743 rank 5
2023-02-17 15:09:11,179 DEBUG TRAIN Batch 6/6100 loss 19.283367 loss_att 28.460747 loss_ctc 30.816633 loss_rnnt 15.712442 hw_loss 0.370649 lr 0.00066719 rank 1
2023-02-17 15:09:11,181 DEBUG TRAIN Batch 6/6100 loss 21.290960 loss_att 25.324789 loss_ctc 36.099541 loss_rnnt 18.328249 hw_loss 0.340246 lr 0.00066684 rank 2
2023-02-17 15:09:11,182 DEBUG TRAIN Batch 6/6100 loss 14.285283 loss_att 20.994488 loss_ctc 28.630753 loss_rnnt 10.867638 hw_loss 0.305765 lr 0.00066764 rank 4
2023-02-17 15:09:11,184 DEBUG TRAIN Batch 6/6100 loss 20.856516 loss_att 26.469549 loss_ctc 36.441589 loss_rnnt 17.499008 hw_loss 0.294170 lr 0.00066718 rank 0
2023-02-17 15:09:11,206 DEBUG TRAIN Batch 6/6100 loss 13.758186 loss_att 15.332335 loss_ctc 22.543041 loss_rnnt 12.065478 hw_loss 0.387308 lr 0.00066732 rank 7
2023-02-17 15:09:11,216 DEBUG TRAIN Batch 6/6100 loss 11.042915 loss_att 18.097488 loss_ctc 21.471214 loss_rnnt 8.061412 hw_loss 0.337780 lr 0.00066756 rank 6
2023-02-17 15:09:11,229 DEBUG TRAIN Batch 6/6100 loss 17.559191 loss_att 22.708660 loss_ctc 32.075111 loss_rnnt 14.428787 hw_loss 0.309478 lr 0.00066721 rank 3
2023-02-17 15:10:27,974 DEBUG TRAIN Batch 6/6200 loss 11.191191 loss_att 14.702076 loss_ctc 16.904869 loss_rnnt 9.498251 hw_loss 0.429260 lr 0.00066625 rank 2
2023-02-17 15:10:27,975 DEBUG TRAIN Batch 6/6200 loss 22.014915 loss_att 22.625568 loss_ctc 35.493950 loss_rnnt 19.920441 hw_loss 0.328389 lr 0.00066673 rank 7
2023-02-17 15:10:27,975 DEBUG TRAIN Batch 6/6200 loss 8.241965 loss_att 12.181627 loss_ctc 13.085352 loss_rnnt 6.598608 hw_loss 0.393076 lr 0.00066662 rank 3
2023-02-17 15:10:27,976 DEBUG TRAIN Batch 6/6200 loss 53.118504 loss_att 48.670536 loss_ctc 60.671677 loss_rnnt 52.813583 hw_loss 0.351415 lr 0.00066660 rank 1
2023-02-17 15:10:27,976 DEBUG TRAIN Batch 6/6200 loss 15.356530 loss_att 18.015335 loss_ctc 26.203787 loss_rnnt 13.185999 hw_loss 0.360880 lr 0.00066696 rank 6
2023-02-17 15:10:27,978 DEBUG TRAIN Batch 6/6200 loss 11.139142 loss_att 15.121086 loss_ctc 21.743826 loss_rnnt 8.710046 hw_loss 0.410156 lr 0.00066683 rank 5
2023-02-17 15:10:27,979 DEBUG TRAIN Batch 6/6200 loss 17.440378 loss_att 21.215866 loss_ctc 28.157370 loss_rnnt 15.070302 hw_loss 0.348835 lr 0.00066705 rank 4
2023-02-17 15:10:27,982 DEBUG TRAIN Batch 6/6200 loss 16.518261 loss_att 18.867258 loss_ctc 26.128353 loss_rnnt 14.566148 hw_loss 0.376812 lr 0.00066659 rank 0
2023-02-17 15:11:45,178 DEBUG TRAIN Batch 6/6300 loss 22.898685 loss_att 23.450577 loss_ctc 30.239851 loss_rnnt 21.599087 hw_loss 0.394499 lr 0.00066624 rank 5
2023-02-17 15:11:45,179 DEBUG TRAIN Batch 6/6300 loss 13.337241 loss_att 15.616725 loss_ctc 20.353210 loss_rnnt 11.762152 hw_loss 0.344494 lr 0.00066645 rank 4
2023-02-17 15:11:45,182 DEBUG TRAIN Batch 6/6300 loss 19.557623 loss_att 21.438988 loss_ctc 25.215611 loss_rnnt 18.129995 hw_loss 0.556796 lr 0.00066601 rank 1
2023-02-17 15:11:45,181 DEBUG TRAIN Batch 6/6300 loss 18.639389 loss_att 23.495564 loss_ctc 31.905170 loss_rnnt 15.684970 hw_loss 0.402028 lr 0.00066603 rank 3
2023-02-17 15:11:45,182 DEBUG TRAIN Batch 6/6300 loss 31.794624 loss_att 37.875122 loss_ctc 51.314297 loss_rnnt 27.851700 hw_loss 0.232876 lr 0.00066566 rank 2
2023-02-17 15:11:45,183 DEBUG TRAIN Batch 6/6300 loss 20.010878 loss_att 24.361641 loss_ctc 34.068626 loss_rnnt 17.058552 hw_loss 0.389638 lr 0.00066600 rank 0
2023-02-17 15:11:45,183 DEBUG TRAIN Batch 6/6300 loss 18.524797 loss_att 24.226967 loss_ctc 28.150696 loss_rnnt 15.878863 hw_loss 0.416340 lr 0.00066613 rank 7
2023-02-17 15:11:45,190 DEBUG TRAIN Batch 6/6300 loss 14.845158 loss_att 17.482910 loss_ctc 24.353228 loss_rnnt 12.845706 hw_loss 0.382799 lr 0.00066637 rank 6
2023-02-17 15:13:05,108 DEBUG TRAIN Batch 6/6400 loss 7.179527 loss_att 7.974504 loss_ctc 9.548000 loss_rnnt 6.489521 hw_loss 0.403525 lr 0.00066578 rank 6
2023-02-17 15:13:05,109 DEBUG TRAIN Batch 6/6400 loss 9.386642 loss_att 12.125091 loss_ctc 14.406935 loss_rnnt 7.958834 hw_loss 0.395147 lr 0.00066554 rank 7
2023-02-17 15:13:05,109 DEBUG TRAIN Batch 6/6400 loss 13.972853 loss_att 24.704304 loss_ctc 32.580727 loss_rnnt 9.157484 hw_loss 0.352555 lr 0.00066565 rank 5
2023-02-17 15:13:05,110 DEBUG TRAIN Batch 6/6400 loss 16.508764 loss_att 18.216930 loss_ctc 20.979757 loss_rnnt 15.389826 hw_loss 0.339700 lr 0.00066544 rank 3
2023-02-17 15:13:05,113 DEBUG TRAIN Batch 6/6400 loss 13.565641 loss_att 22.747818 loss_ctc 28.904463 loss_rnnt 9.489368 hw_loss 0.364990 lr 0.00066507 rank 2
2023-02-17 15:13:05,116 DEBUG TRAIN Batch 6/6400 loss 11.157184 loss_att 12.729406 loss_ctc 20.763128 loss_rnnt 9.386093 hw_loss 0.329722 lr 0.00066541 rank 0
2023-02-17 15:13:05,126 DEBUG TRAIN Batch 6/6400 loss 13.574295 loss_att 21.224854 loss_ctc 32.210293 loss_rnnt 9.353922 hw_loss 0.385239 lr 0.00066586 rank 4
2023-02-17 15:13:05,128 DEBUG TRAIN Batch 6/6400 loss 28.226242 loss_att 28.999033 loss_ctc 39.409992 loss_rnnt 26.400314 hw_loss 0.337878 lr 0.00066542 rank 1
2023-02-17 15:14:21,846 DEBUG TRAIN Batch 6/6500 loss 26.325539 loss_att 32.531719 loss_ctc 43.342693 loss_rnnt 22.709545 hw_loss 0.198385 lr 0.00066483 rank 1
2023-02-17 15:14:21,851 DEBUG TRAIN Batch 6/6500 loss 36.627350 loss_att 40.046227 loss_ctc 62.723495 loss_rnnt 32.308910 hw_loss 0.290958 lr 0.00066519 rank 6
2023-02-17 15:14:21,854 DEBUG TRAIN Batch 6/6500 loss 16.989923 loss_att 23.155479 loss_ctc 28.323462 loss_rnnt 14.105624 hw_loss 0.262592 lr 0.00066506 rank 5
2023-02-17 15:14:21,854 DEBUG TRAIN Batch 6/6500 loss 17.065105 loss_att 23.016768 loss_ctc 37.646572 loss_rnnt 12.884735 hw_loss 0.460952 lr 0.00066448 rank 2
2023-02-17 15:14:21,854 DEBUG TRAIN Batch 6/6500 loss 35.645741 loss_att 39.095959 loss_ctc 48.306938 loss_rnnt 33.115925 hw_loss 0.284267 lr 0.00066485 rank 3
2023-02-17 15:14:21,854 DEBUG TRAIN Batch 6/6500 loss 8.783902 loss_att 14.793655 loss_ctc 16.852930 loss_rnnt 6.292048 hw_loss 0.401311 lr 0.00066495 rank 7
2023-02-17 15:14:21,859 DEBUG TRAIN Batch 6/6500 loss 17.758198 loss_att 22.295799 loss_ctc 30.390053 loss_rnnt 15.029224 hw_loss 0.257257 lr 0.00066482 rank 0
2023-02-17 15:14:21,904 DEBUG TRAIN Batch 6/6500 loss 8.896331 loss_att 12.018205 loss_ctc 18.046896 loss_rnnt 6.858460 hw_loss 0.362664 lr 0.00066527 rank 4
2023-02-17 15:15:39,415 DEBUG TRAIN Batch 6/6600 loss 17.757313 loss_att 23.512348 loss_ctc 31.677145 loss_rnnt 14.592026 hw_loss 0.296819 lr 0.00066460 rank 6
2023-02-17 15:15:39,420 DEBUG TRAIN Batch 6/6600 loss 17.057911 loss_att 21.445648 loss_ctc 25.576595 loss_rnnt 14.842963 hw_loss 0.377949 lr 0.00066468 rank 4
2023-02-17 15:15:39,422 DEBUG TRAIN Batch 6/6600 loss 24.642572 loss_att 29.009918 loss_ctc 49.816402 loss_rnnt 20.213232 hw_loss 0.373800 lr 0.00066426 rank 3
2023-02-17 15:15:39,424 DEBUG TRAIN Batch 6/6600 loss 13.343685 loss_att 17.352983 loss_ctc 22.636452 loss_rnnt 11.131224 hw_loss 0.321688 lr 0.00066447 rank 5
2023-02-17 15:15:39,425 DEBUG TRAIN Batch 6/6600 loss 12.437173 loss_att 16.826920 loss_ctc 22.053816 loss_rnnt 10.060467 hw_loss 0.406009 lr 0.00066424 rank 1
2023-02-17 15:15:39,426 DEBUG TRAIN Batch 6/6600 loss 20.570658 loss_att 29.594975 loss_ctc 33.113194 loss_rnnt 16.971359 hw_loss 0.228932 lr 0.00066437 rank 7
2023-02-17 15:15:39,430 DEBUG TRAIN Batch 6/6600 loss 30.978947 loss_att 34.581749 loss_ctc 54.599667 loss_rnnt 26.947937 hw_loss 0.301916 lr 0.00066423 rank 0
2023-02-17 15:15:39,473 DEBUG TRAIN Batch 6/6600 loss 32.745808 loss_att 45.367775 loss_ctc 61.288239 loss_rnnt 26.288452 hw_loss 0.238694 lr 0.00066390 rank 2
2023-02-17 15:16:57,826 DEBUG TRAIN Batch 6/6700 loss 27.136330 loss_att 28.174381 loss_ctc 40.569050 loss_rnnt 25.002052 hw_loss 0.254314 lr 0.00066366 rank 1
2023-02-17 15:16:57,828 DEBUG TRAIN Batch 6/6700 loss 14.048955 loss_att 16.900919 loss_ctc 18.851528 loss_rnnt 12.632578 hw_loss 0.385576 lr 0.00066402 rank 6
2023-02-17 15:16:57,834 DEBUG TRAIN Batch 6/6700 loss 10.808397 loss_att 15.152174 loss_ctc 19.651096 loss_rnnt 8.573425 hw_loss 0.350980 lr 0.00066378 rank 7
2023-02-17 15:16:57,834 DEBUG TRAIN Batch 6/6700 loss 8.423515 loss_att 13.129172 loss_ctc 20.669682 loss_rnnt 5.648005 hw_loss 0.377920 lr 0.00066410 rank 4
2023-02-17 15:16:57,834 DEBUG TRAIN Batch 6/6700 loss 18.703941 loss_att 19.752163 loss_ctc 26.791775 loss_rnnt 17.215649 hw_loss 0.375509 lr 0.00066389 rank 5
2023-02-17 15:16:57,835 DEBUG TRAIN Batch 6/6700 loss 13.152722 loss_att 19.129383 loss_ctc 25.182846 loss_rnnt 10.173298 hw_loss 0.337641 lr 0.00066365 rank 0
2023-02-17 15:16:57,846 DEBUG TRAIN Batch 6/6700 loss 13.512211 loss_att 18.377131 loss_ctc 21.585096 loss_rnnt 11.315226 hw_loss 0.276778 lr 0.00066368 rank 3
2023-02-17 15:16:57,849 DEBUG TRAIN Batch 6/6700 loss 33.948692 loss_att 34.979897 loss_ctc 51.103836 loss_rnnt 31.254953 hw_loss 0.375273 lr 0.00066331 rank 2
2023-02-17 15:18:16,248 DEBUG TRAIN Batch 6/6800 loss 12.894570 loss_att 17.225569 loss_ctc 28.495037 loss_rnnt 9.747499 hw_loss 0.376514 lr 0.00066343 rank 6
2023-02-17 15:18:16,248 DEBUG TRAIN Batch 6/6800 loss 34.360420 loss_att 34.035561 loss_ctc 55.086311 loss_rnnt 31.498463 hw_loss 0.306523 lr 0.00066308 rank 1
2023-02-17 15:18:16,250 DEBUG TRAIN Batch 6/6800 loss 19.469275 loss_att 21.918472 loss_ctc 32.859642 loss_rnnt 17.024780 hw_loss 0.317384 lr 0.00066330 rank 5
2023-02-17 15:18:16,251 DEBUG TRAIN Batch 6/6800 loss 16.592201 loss_att 16.479343 loss_ctc 25.893763 loss_rnnt 15.137601 hw_loss 0.444307 lr 0.00066273 rank 2
2023-02-17 15:18:16,253 DEBUG TRAIN Batch 6/6800 loss 15.762911 loss_att 18.061163 loss_ctc 22.833080 loss_rnnt 14.226417 hw_loss 0.251541 lr 0.00066306 rank 0
2023-02-17 15:18:16,255 DEBUG TRAIN Batch 6/6800 loss 18.710642 loss_att 23.200920 loss_ctc 29.365189 loss_rnnt 16.228628 hw_loss 0.306289 lr 0.00066351 rank 4
2023-02-17 15:18:16,256 DEBUG TRAIN Batch 6/6800 loss 8.444399 loss_att 12.186703 loss_ctc 13.122849 loss_rnnt 6.898037 hw_loss 0.326449 lr 0.00066320 rank 7
2023-02-17 15:18:16,296 DEBUG TRAIN Batch 6/6800 loss 21.141586 loss_att 25.814772 loss_ctc 34.450272 loss_rnnt 18.245087 hw_loss 0.351323 lr 0.00066309 rank 3
2023-02-17 15:19:33,059 DEBUG TRAIN Batch 6/6900 loss 7.316284 loss_att 12.332184 loss_ctc 13.943266 loss_rnnt 5.264467 hw_loss 0.309447 lr 0.00066285 rank 6
2023-02-17 15:19:33,059 DEBUG TRAIN Batch 6/6900 loss 12.014003 loss_att 13.190304 loss_ctc 17.406338 loss_rnnt 10.849251 hw_loss 0.394714 lr 0.00066251 rank 3
2023-02-17 15:19:33,063 DEBUG TRAIN Batch 6/6900 loss 17.015408 loss_att 20.448084 loss_ctc 25.932737 loss_rnnt 14.973000 hw_loss 0.312931 lr 0.00066272 rank 5
2023-02-17 15:19:33,065 DEBUG TRAIN Batch 6/6900 loss 24.371038 loss_att 28.088812 loss_ctc 42.080929 loss_rnnt 21.053278 hw_loss 0.399162 lr 0.00066249 rank 1
2023-02-17 15:19:33,066 DEBUG TRAIN Batch 6/6900 loss 15.495152 loss_att 17.949835 loss_ctc 23.508526 loss_rnnt 13.758708 hw_loss 0.331982 lr 0.00066215 rank 2
2023-02-17 15:19:33,066 DEBUG TRAIN Batch 6/6900 loss 11.595462 loss_att 13.691990 loss_ctc 15.425945 loss_rnnt 10.448351 hw_loss 0.407015 lr 0.00066262 rank 7
2023-02-17 15:19:33,067 DEBUG TRAIN Batch 6/6900 loss 5.577053 loss_att 8.053273 loss_ctc 8.111770 loss_rnnt 4.518123 hw_loss 0.423231 lr 0.00066248 rank 0
2023-02-17 15:19:33,071 DEBUG TRAIN Batch 6/6900 loss 26.016479 loss_att 28.729528 loss_ctc 41.638592 loss_rnnt 23.251007 hw_loss 0.262338 lr 0.00066293 rank 4
2023-02-17 15:20:48,740 DEBUG TRAIN Batch 6/7000 loss 17.111893 loss_att 18.585171 loss_ctc 27.720051 loss_rnnt 15.204787 hw_loss 0.371302 lr 0.00066190 rank 0
2023-02-17 15:20:48,741 DEBUG TRAIN Batch 6/7000 loss 12.945227 loss_att 14.190282 loss_ctc 23.153879 loss_rnnt 11.143378 hw_loss 0.359408 lr 0.00066193 rank 3
2023-02-17 15:20:48,742 DEBUG TRAIN Batch 6/7000 loss 30.651180 loss_att 34.398232 loss_ctc 44.844658 loss_rnnt 27.798065 hw_loss 0.396075 lr 0.00066157 rank 2
2023-02-17 15:20:48,744 DEBUG TRAIN Batch 6/7000 loss 20.137903 loss_att 23.375385 loss_ctc 37.382690 loss_rnnt 16.978302 hw_loss 0.398998 lr 0.00066227 rank 6
2023-02-17 15:20:48,744 DEBUG TRAIN Batch 6/7000 loss 17.235819 loss_att 21.682295 loss_ctc 31.579367 loss_rnnt 14.235364 hw_loss 0.372532 lr 0.00066203 rank 7
2023-02-17 15:20:48,745 DEBUG TRAIN Batch 6/7000 loss 9.371470 loss_att 9.703997 loss_ctc 12.995326 loss_rnnt 8.542305 hw_loss 0.524023 lr 0.00066235 rank 4
2023-02-17 15:20:48,744 DEBUG TRAIN Batch 6/7000 loss 22.311033 loss_att 24.967846 loss_ctc 30.988943 loss_rnnt 20.457344 hw_loss 0.309887 lr 0.00066191 rank 1
2023-02-17 15:20:48,745 DEBUG TRAIN Batch 6/7000 loss 17.480715 loss_att 17.070129 loss_ctc 26.898651 loss_rnnt 16.101339 hw_loss 0.385812 lr 0.00066214 rank 5
2023-02-17 15:22:09,764 DEBUG TRAIN Batch 6/7100 loss 16.248478 loss_att 20.302942 loss_ctc 24.684868 loss_rnnt 14.158195 hw_loss 0.289757 lr 0.00066156 rank 5
2023-02-17 15:22:09,764 DEBUG TRAIN Batch 6/7100 loss 9.019178 loss_att 15.868567 loss_ctc 14.592186 loss_rnnt 6.745210 hw_loss 0.301918 lr 0.00066177 rank 4
2023-02-17 15:22:09,766 DEBUG TRAIN Batch 6/7100 loss 27.483011 loss_att 34.563358 loss_ctc 51.268005 loss_rnnt 22.746422 hw_loss 0.279728 lr 0.00066133 rank 1
2023-02-17 15:22:09,767 DEBUG TRAIN Batch 6/7100 loss 15.408314 loss_att 15.645805 loss_ctc 20.262650 loss_rnnt 14.456833 hw_loss 0.481382 lr 0.00066145 rank 7
2023-02-17 15:22:09,767 DEBUG TRAIN Batch 6/7100 loss 13.365746 loss_att 16.612114 loss_ctc 28.413284 loss_rnnt 10.540644 hw_loss 0.317796 lr 0.00066099 rank 2
2023-02-17 15:22:09,768 DEBUG TRAIN Batch 6/7100 loss 14.438233 loss_att 22.205622 loss_ctc 29.506638 loss_rnnt 10.713034 hw_loss 0.304877 lr 0.00066169 rank 6
2023-02-17 15:22:09,769 DEBUG TRAIN Batch 6/7100 loss 16.880766 loss_att 23.090345 loss_ctc 32.124924 loss_rnnt 13.377498 hw_loss 0.428997 lr 0.00066135 rank 3
2023-02-17 15:22:09,770 DEBUG TRAIN Batch 6/7100 loss 11.444415 loss_att 18.342203 loss_ctc 18.133316 loss_rnnt 9.033153 hw_loss 0.262222 lr 0.00066132 rank 0
2023-02-17 15:23:25,960 DEBUG TRAIN Batch 6/7200 loss 8.227987 loss_att 13.008676 loss_ctc 15.864319 loss_rnnt 6.060018 hw_loss 0.363101 lr 0.00066076 rank 1
2023-02-17 15:23:25,961 DEBUG TRAIN Batch 6/7200 loss 19.948004 loss_att 22.531265 loss_ctc 38.149837 loss_rnnt 16.870922 hw_loss 0.250344 lr 0.00066077 rank 3
2023-02-17 15:23:25,961 DEBUG TRAIN Batch 6/7200 loss 25.855917 loss_att 28.716915 loss_ctc 43.390099 loss_rnnt 22.800947 hw_loss 0.271651 lr 0.00066088 rank 7
2023-02-17 15:23:25,962 DEBUG TRAIN Batch 6/7200 loss 32.943344 loss_att 37.601299 loss_ctc 51.460751 loss_rnnt 29.387558 hw_loss 0.291014 lr 0.00066111 rank 6
2023-02-17 15:23:25,964 DEBUG TRAIN Batch 6/7200 loss 21.678276 loss_att 23.163269 loss_ctc 32.627224 loss_rnnt 19.810253 hw_loss 0.208432 lr 0.00066098 rank 5
2023-02-17 15:23:25,969 DEBUG TRAIN Batch 6/7200 loss 24.620825 loss_att 28.940224 loss_ctc 41.958725 loss_rnnt 21.347746 hw_loss 0.182773 lr 0.00066041 rank 2
2023-02-17 15:23:25,969 DEBUG TRAIN Batch 6/7200 loss 17.349985 loss_att 22.752632 loss_ctc 26.473541 loss_rnnt 14.918344 hw_loss 0.252440 lr 0.00066119 rank 4
2023-02-17 15:23:25,969 DEBUG TRAIN Batch 6/7200 loss 14.383798 loss_att 19.567047 loss_ctc 23.782692 loss_rnnt 11.914467 hw_loss 0.336555 lr 0.00066074 rank 0
2023-02-17 15:24:42,174 DEBUG TRAIN Batch 6/7300 loss 14.406649 loss_att 22.519152 loss_ctc 26.103323 loss_rnnt 11.060226 hw_loss 0.308184 lr 0.00066020 rank 3
2023-02-17 15:24:42,180 DEBUG TRAIN Batch 6/7300 loss 21.386354 loss_att 25.052231 loss_ctc 35.046215 loss_rnnt 18.676979 hw_loss 0.290407 lr 0.00066018 rank 1
2023-02-17 15:24:42,181 DEBUG TRAIN Batch 6/7300 loss 6.505712 loss_att 9.666984 loss_ctc 13.415739 loss_rnnt 4.754554 hw_loss 0.370438 lr 0.00066053 rank 6
2023-02-17 15:24:42,182 DEBUG TRAIN Batch 6/7300 loss 9.361111 loss_att 19.336016 loss_ctc 22.324242 loss_rnnt 5.475437 hw_loss 0.304264 lr 0.00066030 rank 7
2023-02-17 15:24:42,183 DEBUG TRAIN Batch 6/7300 loss 15.424311 loss_att 19.344526 loss_ctc 25.562262 loss_rnnt 13.114122 hw_loss 0.327031 lr 0.00066040 rank 5
2023-02-17 15:24:42,184 DEBUG TRAIN Batch 6/7300 loss 14.765090 loss_att 23.107887 loss_ctc 27.145710 loss_rnnt 11.305138 hw_loss 0.263707 lr 0.00066061 rank 4
2023-02-17 15:24:42,186 DEBUG TRAIN Batch 6/7300 loss 10.441338 loss_att 16.926189 loss_ctc 19.977655 loss_rnnt 7.671842 hw_loss 0.376905 lr 0.00066017 rank 0
2023-02-17 15:24:42,229 DEBUG TRAIN Batch 6/7300 loss 8.887277 loss_att 12.632263 loss_ctc 11.564045 loss_rnnt 7.570003 hw_loss 0.396327 lr 0.00065984 rank 2
2023-02-17 15:26:00,018 DEBUG TRAIN Batch 6/7400 loss 16.764868 loss_att 23.203861 loss_ctc 29.115654 loss_rnnt 13.681782 hw_loss 0.278470 lr 0.00065962 rank 3
2023-02-17 15:26:00,019 DEBUG TRAIN Batch 6/7400 loss 25.792995 loss_att 33.142624 loss_ctc 43.980949 loss_rnnt 21.727804 hw_loss 0.319139 lr 0.00065927 rank 2
2023-02-17 15:26:00,021 DEBUG TRAIN Batch 6/7400 loss 20.391569 loss_att 22.712543 loss_ctc 28.532722 loss_rnnt 18.657722 hw_loss 0.345309 lr 0.00065959 rank 0
2023-02-17 15:26:00,021 DEBUG TRAIN Batch 6/7400 loss 26.968014 loss_att 29.651827 loss_ctc 37.876144 loss_rnnt 24.798780 hw_loss 0.333848 lr 0.00065960 rank 1
2023-02-17 15:26:00,022 DEBUG TRAIN Batch 6/7400 loss 9.828088 loss_att 13.352003 loss_ctc 17.926159 loss_rnnt 7.876988 hw_loss 0.312326 lr 0.00065995 rank 6
2023-02-17 15:26:00,024 DEBUG TRAIN Batch 6/7400 loss 15.324176 loss_att 18.958921 loss_ctc 23.540905 loss_rnnt 13.302889 hw_loss 0.372699 lr 0.00065983 rank 5
2023-02-17 15:26:00,025 DEBUG TRAIN Batch 6/7400 loss 29.634123 loss_att 32.725563 loss_ctc 39.090824 loss_rnnt 27.570862 hw_loss 0.345147 lr 0.00066004 rank 4
2023-02-17 15:26:00,025 DEBUG TRAIN Batch 6/7400 loss 35.628548 loss_att 42.289101 loss_ctc 57.113617 loss_rnnt 31.313610 hw_loss 0.221536 lr 0.00065972 rank 7
2023-02-17 15:27:19,524 DEBUG TRAIN Batch 6/7500 loss 14.210648 loss_att 14.727312 loss_ctc 19.685862 loss_rnnt 13.195358 hw_loss 0.341115 lr 0.00065869 rank 2
2023-02-17 15:27:19,529 DEBUG TRAIN Batch 6/7500 loss 19.597536 loss_att 22.018019 loss_ctc 30.573147 loss_rnnt 17.426311 hw_loss 0.419457 lr 0.00065925 rank 5
2023-02-17 15:27:19,529 DEBUG TRAIN Batch 6/7500 loss 19.756144 loss_att 22.111557 loss_ctc 34.215313 loss_rnnt 17.186180 hw_loss 0.320604 lr 0.00065915 rank 7
2023-02-17 15:27:19,529 DEBUG TRAIN Batch 6/7500 loss 22.060749 loss_att 24.238605 loss_ctc 34.743919 loss_rnnt 19.775646 hw_loss 0.297080 lr 0.00065938 rank 6
2023-02-17 15:27:19,530 DEBUG TRAIN Batch 6/7500 loss 34.295559 loss_att 37.994713 loss_ctc 49.549187 loss_rnnt 31.359442 hw_loss 0.304628 lr 0.00065903 rank 1
2023-02-17 15:27:19,532 DEBUG TRAIN Batch 6/7500 loss 15.398187 loss_att 17.685843 loss_ctc 24.567711 loss_rnnt 13.533024 hw_loss 0.346930 lr 0.00065946 rank 4
2023-02-17 15:27:19,533 DEBUG TRAIN Batch 6/7500 loss 21.191753 loss_att 26.009874 loss_ctc 38.579208 loss_rnnt 17.731623 hw_loss 0.334080 lr 0.00065902 rank 0
2023-02-17 15:27:19,569 DEBUG TRAIN Batch 6/7500 loss 17.446484 loss_att 22.243542 loss_ctc 36.459705 loss_rnnt 13.769752 hw_loss 0.341675 lr 0.00065905 rank 3
2023-02-17 15:28:36,282 DEBUG TRAIN Batch 6/7600 loss 10.553245 loss_att 13.348583 loss_ctc 16.725698 loss_rnnt 8.941332 hw_loss 0.430971 lr 0.00065881 rank 6
2023-02-17 15:28:36,283 DEBUG TRAIN Batch 6/7600 loss 13.198910 loss_att 15.052060 loss_ctc 17.726778 loss_rnnt 12.023080 hw_loss 0.377784 lr 0.00065858 rank 7
2023-02-17 15:28:36,284 DEBUG TRAIN Batch 6/7600 loss 25.237623 loss_att 30.320316 loss_ctc 39.517067 loss_rnnt 22.126310 hw_loss 0.357844 lr 0.00065845 rank 0
2023-02-17 15:28:36,285 DEBUG TRAIN Batch 6/7600 loss 16.491650 loss_att 17.496662 loss_ctc 29.679617 loss_rnnt 14.323883 hw_loss 0.390690 lr 0.00065868 rank 5
2023-02-17 15:28:36,286 DEBUG TRAIN Batch 6/7600 loss 29.918283 loss_att 36.235714 loss_ctc 56.805950 loss_rnnt 24.915205 hw_loss 0.289819 lr 0.00065812 rank 2
2023-02-17 15:28:36,293 DEBUG TRAIN Batch 6/7600 loss 15.812807 loss_att 15.380247 loss_ctc 21.656973 loss_rnnt 14.943214 hw_loss 0.331654 lr 0.00065889 rank 4
2023-02-17 15:28:36,293 DEBUG TRAIN Batch 6/7600 loss 18.033016 loss_att 21.346003 loss_ctc 27.328659 loss_rnnt 15.924317 hw_loss 0.387526 lr 0.00065848 rank 3
2023-02-17 15:28:36,295 DEBUG TRAIN Batch 6/7600 loss 16.082865 loss_att 18.261826 loss_ctc 21.761658 loss_rnnt 14.696147 hw_loss 0.363288 lr 0.00065846 rank 1
2023-02-17 15:29:52,996 DEBUG TRAIN Batch 6/7700 loss 9.264945 loss_att 9.653874 loss_ctc 12.289241 loss_rnnt 8.503983 hw_loss 0.524879 lr 0.00065824 rank 6
2023-02-17 15:29:52,997 DEBUG TRAIN Batch 6/7700 loss 14.550762 loss_att 16.578955 loss_ctc 23.512037 loss_rnnt 12.776703 hw_loss 0.325469 lr 0.00065801 rank 7
2023-02-17 15:29:52,998 DEBUG TRAIN Batch 6/7700 loss 20.788956 loss_att 20.909565 loss_ctc 30.702538 loss_rnnt 19.299824 hw_loss 0.268497 lr 0.00065791 rank 3
2023-02-17 15:29:53,001 DEBUG TRAIN Batch 6/7700 loss 8.583970 loss_att 9.451296 loss_ctc 8.575071 loss_rnnt 8.273700 hw_loss 0.258735 lr 0.00065832 rank 4
2023-02-17 15:29:53,001 DEBUG TRAIN Batch 6/7700 loss 15.195574 loss_att 22.546318 loss_ctc 27.619522 loss_rnnt 11.902587 hw_loss 0.311835 lr 0.00065811 rank 5
2023-02-17 15:29:53,004 DEBUG TRAIN Batch 6/7700 loss 18.955755 loss_att 22.694988 loss_ctc 30.507261 loss_rnnt 16.457378 hw_loss 0.394367 lr 0.00065788 rank 0
2023-02-17 15:29:53,004 DEBUG TRAIN Batch 6/7700 loss 15.579409 loss_att 18.407280 loss_ctc 27.973822 loss_rnnt 13.161246 hw_loss 0.375000 lr 0.00065789 rank 1
2023-02-17 15:29:53,045 DEBUG TRAIN Batch 6/7700 loss 12.779666 loss_att 19.421703 loss_ctc 25.721703 loss_rnnt 9.503649 hw_loss 0.416256 lr 0.00065755 rank 2
2023-02-17 15:31:11,773 DEBUG TRAIN Batch 6/7800 loss 15.686729 loss_att 19.222847 loss_ctc 23.063877 loss_rnnt 13.793078 hw_loss 0.380263 lr 0.00065767 rank 6
2023-02-17 15:31:11,774 DEBUG TRAIN Batch 6/7800 loss 25.398746 loss_att 30.786816 loss_ctc 40.948185 loss_rnnt 22.059116 hw_loss 0.353916 lr 0.00065734 rank 3
2023-02-17 15:31:11,776 DEBUG TRAIN Batch 6/7800 loss 13.034972 loss_att 17.662573 loss_ctc 18.361460 loss_rnnt 11.247545 hw_loss 0.284451 lr 0.00065744 rank 7
2023-02-17 15:31:11,778 DEBUG TRAIN Batch 6/7800 loss 15.108505 loss_att 19.304977 loss_ctc 23.419691 loss_rnnt 12.968342 hw_loss 0.361333 lr 0.00065754 rank 5
2023-02-17 15:31:11,783 DEBUG TRAIN Batch 6/7800 loss 15.390881 loss_att 17.498270 loss_ctc 20.238745 loss_rnnt 14.171659 hw_loss 0.283805 lr 0.00065775 rank 4
2023-02-17 15:31:11,784 DEBUG TRAIN Batch 6/7800 loss 17.303726 loss_att 27.124044 loss_ctc 33.866405 loss_rnnt 12.953108 hw_loss 0.334118 lr 0.00065732 rank 1
2023-02-17 15:31:11,785 DEBUG TRAIN Batch 6/7800 loss 31.030647 loss_att 40.053940 loss_ctc 52.234585 loss_rnnt 26.171785 hw_loss 0.425645 lr 0.00065699 rank 2
2023-02-17 15:31:11,786 DEBUG TRAIN Batch 6/7800 loss 8.793735 loss_att 14.666647 loss_ctc 18.979757 loss_rnnt 6.072532 hw_loss 0.353408 lr 0.00065731 rank 0
2023-02-17 15:32:29,518 DEBUG TRAIN Batch 6/7900 loss 8.906583 loss_att 14.410783 loss_ctc 21.050991 loss_rnnt 6.043108 hw_loss 0.268838 lr 0.00065677 rank 3
2023-02-17 15:32:29,518 DEBUG TRAIN Batch 6/7900 loss 15.070143 loss_att 19.064484 loss_ctc 20.653904 loss_rnnt 13.402761 hw_loss 0.232522 lr 0.00065697 rank 5
2023-02-17 15:32:29,520 DEBUG TRAIN Batch 6/7900 loss 14.008123 loss_att 18.633043 loss_ctc 21.526884 loss_rnnt 11.905899 hw_loss 0.327636 lr 0.00065710 rank 6
2023-02-17 15:32:29,522 DEBUG TRAIN Batch 6/7900 loss 18.188797 loss_att 22.252335 loss_ctc 31.108101 loss_rnnt 15.514779 hw_loss 0.260129 lr 0.00065675 rank 1
2023-02-17 15:32:29,522 DEBUG TRAIN Batch 6/7900 loss 18.239124 loss_att 26.527361 loss_ctc 34.537132 loss_rnnt 14.223175 hw_loss 0.347316 lr 0.00065687 rank 7
2023-02-17 15:32:29,524 DEBUG TRAIN Batch 6/7900 loss 16.735922 loss_att 18.872993 loss_ctc 29.392004 loss_rnnt 14.450571 hw_loss 0.319611 lr 0.00065718 rank 4
2023-02-17 15:32:29,525 DEBUG TRAIN Batch 6/7900 loss 16.992855 loss_att 18.541502 loss_ctc 25.643398 loss_rnnt 15.329478 hw_loss 0.375452 lr 0.00065674 rank 0
2023-02-17 15:32:29,526 DEBUG TRAIN Batch 6/7900 loss 16.711073 loss_att 22.317915 loss_ctc 29.328241 loss_rnnt 13.709002 hw_loss 0.372024 lr 0.00065642 rank 2
2023-02-17 15:33:44,556 DEBUG TRAIN Batch 6/8000 loss 12.547005 loss_att 15.438221 loss_ctc 22.116013 loss_rnnt 10.499924 hw_loss 0.361820 lr 0.00065619 rank 1
2023-02-17 15:33:44,557 DEBUG TRAIN Batch 6/8000 loss 8.859993 loss_att 10.942257 loss_ctc 14.515011 loss_rnnt 7.494061 hw_loss 0.366517 lr 0.00065620 rank 3
2023-02-17 15:33:44,559 DEBUG TRAIN Batch 6/8000 loss 18.540567 loss_att 21.778809 loss_ctc 32.751251 loss_rnnt 15.800638 hw_loss 0.370353 lr 0.00065653 rank 6
2023-02-17 15:33:44,560 DEBUG TRAIN Batch 6/8000 loss 17.979588 loss_att 25.329466 loss_ctc 36.624992 loss_rnnt 13.854415 hw_loss 0.317143 lr 0.00065661 rank 4
2023-02-17 15:33:44,559 DEBUG TRAIN Batch 6/8000 loss 18.187263 loss_att 25.182697 loss_ctc 38.316662 loss_rnnt 13.909049 hw_loss 0.366015 lr 0.00065641 rank 5
2023-02-17 15:33:44,563 DEBUG TRAIN Batch 6/8000 loss 17.287172 loss_att 21.909063 loss_ctc 26.817446 loss_rnnt 14.927746 hw_loss 0.308145 lr 0.00065631 rank 7
2023-02-17 15:33:44,564 DEBUG TRAIN Batch 6/8000 loss 19.603937 loss_att 23.184008 loss_ctc 32.106030 loss_rnnt 17.045759 hw_loss 0.328536 lr 0.00065618 rank 0
2023-02-17 15:33:44,610 DEBUG TRAIN Batch 6/8000 loss 29.736351 loss_att 31.514069 loss_ctc 40.625561 loss_rnnt 27.771889 hw_loss 0.294425 lr 0.00065585 rank 2
2023-02-17 15:35:01,679 DEBUG TRAIN Batch 6/8100 loss 20.053478 loss_att 22.190178 loss_ctc 35.070923 loss_rnnt 17.450623 hw_loss 0.324729 lr 0.00065529 rank 2
2023-02-17 15:35:01,679 DEBUG TRAIN Batch 6/8100 loss 18.310806 loss_att 25.485666 loss_ctc 28.184155 loss_rnnt 15.395651 hw_loss 0.307008 lr 0.00065574 rank 7
2023-02-17 15:35:01,680 DEBUG TRAIN Batch 6/8100 loss 14.291678 loss_att 18.470640 loss_ctc 25.921543 loss_rnnt 11.740526 hw_loss 0.308830 lr 0.00065584 rank 5
2023-02-17 15:35:01,680 DEBUG TRAIN Batch 6/8100 loss 15.327195 loss_att 20.155972 loss_ctc 28.183004 loss_rnnt 12.401596 hw_loss 0.460757 lr 0.00065597 rank 6
2023-02-17 15:35:01,680 DEBUG TRAIN Batch 6/8100 loss 16.869432 loss_att 22.207542 loss_ctc 26.353027 loss_rnnt 14.324609 hw_loss 0.398854 lr 0.00065564 rank 3
2023-02-17 15:35:01,682 DEBUG TRAIN Batch 6/8100 loss 15.747254 loss_att 17.642082 loss_ctc 22.289520 loss_rnnt 14.289408 hw_loss 0.387332 lr 0.00065562 rank 1
2023-02-17 15:35:01,684 DEBUG TRAIN Batch 6/8100 loss 17.841337 loss_att 23.869165 loss_ctc 29.446709 loss_rnnt 14.899527 hw_loss 0.354118 lr 0.00065561 rank 0
2023-02-17 15:35:01,726 DEBUG TRAIN Batch 6/8100 loss 26.503359 loss_att 25.441938 loss_ctc 34.505173 loss_rnnt 25.468115 hw_loss 0.338666 lr 0.00065605 rank 4
2023-02-17 15:36:19,491 DEBUG TRAIN Batch 6/8200 loss 21.185810 loss_att 28.848852 loss_ctc 36.180428 loss_rnnt 17.465656 hw_loss 0.352995 lr 0.00065518 rank 7
2023-02-17 15:36:19,494 DEBUG TRAIN Batch 6/8200 loss 22.862587 loss_att 22.466629 loss_ctc 32.458801 loss_rnnt 21.444010 hw_loss 0.409259 lr 0.00065548 rank 4
2023-02-17 15:36:19,495 DEBUG TRAIN Batch 6/8200 loss 17.613304 loss_att 18.261415 loss_ctc 25.982803 loss_rnnt 16.208635 hw_loss 0.298338 lr 0.00065540 rank 6
2023-02-17 15:36:19,497 DEBUG TRAIN Batch 6/8200 loss 20.603683 loss_att 19.510098 loss_ctc 28.706629 loss_rnnt 19.483347 hw_loss 0.484991 lr 0.00065528 rank 5
2023-02-17 15:36:19,498 DEBUG TRAIN Batch 6/8200 loss 17.061399 loss_att 19.989950 loss_ctc 33.049492 loss_rnnt 14.125849 hw_loss 0.408928 lr 0.00065506 rank 1
2023-02-17 15:36:19,500 DEBUG TRAIN Batch 6/8200 loss 10.610365 loss_att 15.534973 loss_ctc 15.627085 loss_rnnt 8.822089 hw_loss 0.252109 lr 0.00065473 rank 2
2023-02-17 15:36:19,502 DEBUG TRAIN Batch 6/8200 loss 14.075474 loss_att 19.289892 loss_ctc 21.388474 loss_rnnt 11.882631 hw_loss 0.327922 lr 0.00065508 rank 3
2023-02-17 15:36:19,502 DEBUG TRAIN Batch 6/8200 loss 18.143847 loss_att 20.161198 loss_ctc 25.675831 loss_rnnt 16.580725 hw_loss 0.291352 lr 0.00065505 rank 0
2023-02-17 15:37:34,118 DEBUG TRAIN Batch 6/8300 loss 30.071543 loss_att 39.745293 loss_ctc 49.083065 loss_rnnt 25.443525 hw_loss 0.297000 lr 0.00065472 rank 5
2023-02-17 15:37:34,120 DEBUG TRAIN Batch 6/8300 loss 12.604458 loss_att 17.350170 loss_ctc 23.716742 loss_rnnt 9.965940 hw_loss 0.389507 lr 0.00065450 rank 1
2023-02-17 15:37:34,120 DEBUG TRAIN Batch 6/8300 loss 12.937686 loss_att 18.153830 loss_ctc 20.021988 loss_rnnt 10.818623 hw_loss 0.246112 lr 0.00065484 rank 6
2023-02-17 15:37:34,122 DEBUG TRAIN Batch 6/8300 loss 15.765449 loss_att 21.020535 loss_ctc 21.994659 loss_rnnt 13.722479 hw_loss 0.302610 lr 0.00065462 rank 7
2023-02-17 15:37:34,122 DEBUG TRAIN Batch 6/8300 loss 22.122873 loss_att 25.287064 loss_ctc 32.428822 loss_rnnt 19.953894 hw_loss 0.303777 lr 0.00065452 rank 3
2023-02-17 15:37:34,124 DEBUG TRAIN Batch 6/8300 loss 15.644175 loss_att 18.512627 loss_ctc 23.814005 loss_rnnt 13.788541 hw_loss 0.361186 lr 0.00065449 rank 0
2023-02-17 15:37:34,124 DEBUG TRAIN Batch 6/8300 loss 14.934155 loss_att 21.353491 loss_ctc 26.589682 loss_rnnt 11.891548 hw_loss 0.383754 lr 0.00065417 rank 2
2023-02-17 15:37:34,165 DEBUG TRAIN Batch 6/8300 loss 16.572830 loss_att 19.222294 loss_ctc 28.391180 loss_rnnt 14.300262 hw_loss 0.312929 lr 0.00065492 rank 4
2023-02-17 15:38:23,679 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 5
2023-02-17 15:38:23,679 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 7
2023-02-17 15:38:23,690 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 1
2023-02-17 15:38:23,690 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 4
2023-02-17 15:38:23,692 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 6
2023-02-17 15:38:23,703 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 3
2023-02-17 15:38:23,716 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 2
2023-02-17 15:38:23,723 DEBUG CV Batch 6/0 loss 2.583326 loss_att 2.750854 loss_ctc 4.506302 loss_rnnt 2.072746 hw_loss 0.413771 history loss 2.487647 rank 0
2023-02-17 15:38:34,831 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 7
2023-02-17 15:38:34,843 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 1
2023-02-17 15:38:34,897 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 2
2023-02-17 15:38:34,923 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 6
2023-02-17 15:38:34,929 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 5
2023-02-17 15:38:35,010 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 4
2023-02-17 15:38:35,139 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 0
2023-02-17 15:38:35,571 DEBUG CV Batch 6/100 loss 9.672562 loss_att 11.090580 loss_ctc 19.425518 loss_rnnt 7.866670 hw_loss 0.416051 history loss 4.975369 rank 3
2023-02-17 15:38:48,798 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 4
2023-02-17 15:38:48,980 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 1
2023-02-17 15:38:49,018 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 7
2023-02-17 15:38:49,040 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 6
2023-02-17 15:38:49,075 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 5
2023-02-17 15:38:49,084 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 3
2023-02-17 15:38:49,278 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 2
2023-02-17 15:38:49,513 DEBUG CV Batch 6/200 loss 13.964302 loss_att 21.217335 loss_ctc 19.498123 loss_rnnt 11.599609 hw_loss 0.330457 history loss 5.656411 rank 0
2023-02-17 15:39:00,919 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 7
2023-02-17 15:39:01,026 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 6
2023-02-17 15:39:01,072 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 1
2023-02-17 15:39:01,072 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 5
2023-02-17 15:39:01,187 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 3
2023-02-17 15:39:01,342 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 4
2023-02-17 15:39:01,425 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 2
2023-02-17 15:39:01,669 DEBUG CV Batch 6/300 loss 4.973388 loss_att 6.587571 loss_ctc 10.266918 loss_rnnt 3.742450 hw_loss 0.379308 history loss 5.842613 rank 0
2023-02-17 15:39:12,806 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 7
2023-02-17 15:39:12,984 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 1
2023-02-17 15:39:12,991 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 6
2023-02-17 15:39:13,080 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 5
2023-02-17 15:39:13,218 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 3
2023-02-17 15:39:13,545 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 2
2023-02-17 15:39:13,704 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 4
2023-02-17 15:39:13,765 DEBUG CV Batch 6/400 loss 26.380150 loss_att 103.003906 loss_ctc 23.832468 loss_rnnt 11.271759 hw_loss 0.231242 history loss 6.975133 rank 0
2023-02-17 15:39:23,302 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 7
2023-02-17 15:39:23,403 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 6
2023-02-17 15:39:23,415 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 1
2023-02-17 15:39:23,610 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 5
2023-02-17 15:39:23,800 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 3
2023-02-17 15:39:24,237 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 2
2023-02-17 15:39:24,264 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 4
2023-02-17 15:39:24,571 DEBUG CV Batch 6/500 loss 7.843289 loss_att 8.650822 loss_ctc 11.456540 loss_rnnt 7.010071 hw_loss 0.356148 history loss 7.751109 rank 0
2023-02-17 15:39:35,484 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 1
2023-02-17 15:39:35,587 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 6
2023-02-17 15:39:35,647 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 7
2023-02-17 15:39:35,998 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 5
2023-02-17 15:39:36,361 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 4
2023-02-17 15:39:36,747 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 3
2023-02-17 15:39:37,074 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 0
2023-02-17 15:39:38,016 DEBUG CV Batch 6/600 loss 6.663181 loss_att 7.261958 loss_ctc 10.093443 loss_rnnt 5.832460 hw_loss 0.475494 history loss 8.739571 rank 2
2023-02-17 15:39:48,429 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 1
2023-02-17 15:39:48,577 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 4
2023-02-17 15:39:48,642 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 6
2023-02-17 15:39:48,957 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 7
2023-02-17 15:39:49,141 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 5
2023-02-17 15:39:49,889 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 3
2023-02-17 15:39:50,167 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 0
2023-02-17 15:39:50,825 DEBUG CV Batch 6/700 loss 23.651611 loss_att 66.648788 loss_ctc 36.922298 loss_rnnt 13.118240 hw_loss 0.308456 history loss 9.443003 rank 2
2023-02-17 15:40:00,424 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 1
2023-02-17 15:40:00,454 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 6
2023-02-17 15:40:00,733 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 4
2023-02-17 15:40:01,020 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 7
2023-02-17 15:40:01,267 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 5
2023-02-17 15:40:02,212 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 0
2023-02-17 15:40:02,364 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 3
2023-02-17 15:40:02,872 DEBUG CV Batch 6/800 loss 12.855245 loss_att 12.656048 loss_ctc 23.276663 loss_rnnt 11.298930 hw_loss 0.387434 history loss 8.841871 rank 2
2023-02-17 15:40:14,189 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 1
2023-02-17 15:40:14,516 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 6
2023-02-17 15:40:14,967 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 4
2023-02-17 15:40:15,062 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 7
2023-02-17 15:40:15,343 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 5
2023-02-17 15:40:16,084 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 0
2023-02-17 15:40:16,303 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 3
2023-02-17 15:40:16,932 DEBUG CV Batch 6/900 loss 18.311607 loss_att 21.875587 loss_ctc 33.758316 loss_rnnt 15.409309 hw_loss 0.243642 history loss 8.628419 rank 2
2023-02-17 15:40:26,283 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 1
2023-02-17 15:40:26,709 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 6
2023-02-17 15:40:27,113 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 4
2023-02-17 15:40:27,248 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 7
2023-02-17 15:40:27,492 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 5
2023-02-17 15:40:28,483 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 0
2023-02-17 15:40:28,556 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 3
2023-02-17 15:40:29,273 DEBUG CV Batch 6/1000 loss 5.239396 loss_att 5.648523 loss_ctc 8.232418 loss_rnnt 4.558503 hw_loss 0.374994 history loss 8.396202 rank 2
2023-02-17 15:40:38,081 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 1
2023-02-17 15:40:38,519 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 6
2023-02-17 15:40:39,013 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 7
2023-02-17 15:40:39,388 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 5
2023-02-17 15:40:39,687 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 4
2023-02-17 15:40:40,496 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 0
2023-02-17 15:40:40,630 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 3
2023-02-17 15:40:41,196 DEBUG CV Batch 6/1100 loss 6.195836 loss_att 6.495296 loss_ctc 10.707207 loss_rnnt 5.310661 hw_loss 0.419561 history loss 8.389842 rank 2
2023-02-17 15:40:48,402 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 1
2023-02-17 15:40:48,906 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 6
2023-02-17 15:40:49,602 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 7
2023-02-17 15:40:49,787 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 5
2023-02-17 15:40:50,122 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 4
2023-02-17 15:40:51,106 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 3
2023-02-17 15:40:51,782 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 2
2023-02-17 15:40:51,948 DEBUG CV Batch 6/1200 loss 8.969168 loss_att 10.762411 loss_ctc 12.690536 loss_rnnt 7.913059 hw_loss 0.377398 history loss 8.736380 rank 0
2023-02-17 15:41:00,395 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 1
2023-02-17 15:41:00,913 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 6
2023-02-17 15:41:01,541 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 7
2023-02-17 15:41:01,775 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 5
2023-02-17 15:41:02,034 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 4
2023-02-17 15:41:03,085 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 3
2023-02-17 15:41:03,839 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 2
2023-02-17 15:41:03,988 DEBUG CV Batch 6/1300 loss 6.172675 loss_att 6.372304 loss_ctc 9.518429 loss_rnnt 5.495192 hw_loss 0.358981 history loss 9.065949 rank 0
2023-02-17 15:41:11,481 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 1
2023-02-17 15:41:12,334 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 6
2023-02-17 15:41:13,114 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 4
2023-02-17 15:41:13,306 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 5
2023-02-17 15:41:13,706 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 7
2023-02-17 15:41:14,641 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 3
2023-02-17 15:41:15,238 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 0
2023-02-17 15:41:16,261 DEBUG CV Batch 6/1400 loss 11.263944 loss_att 25.321224 loss_ctc 18.974209 loss_rnnt 7.247740 hw_loss 0.331335 history loss 9.408468 rank 2
2023-02-17 15:41:24,211 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 1
2023-02-17 15:41:24,788 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 6
2023-02-17 15:41:24,799 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 4
2023-02-17 15:41:25,731 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 5
2023-02-17 15:41:26,152 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 7
2023-02-17 15:41:26,852 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 3
2023-02-17 15:41:27,630 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 0
2023-02-17 15:41:28,711 DEBUG CV Batch 6/1500 loss 9.292958 loss_att 9.893650 loss_ctc 9.663589 loss_rnnt 8.939504 hw_loss 0.344807 history loss 9.205278 rank 2
2023-02-17 15:41:37,554 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 1
2023-02-17 15:41:38,144 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 4
2023-02-17 15:41:38,407 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 6
2023-02-17 15:41:39,443 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 5
2023-02-17 15:41:39,848 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 7
2023-02-17 15:41:40,530 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 3
2023-02-17 15:41:40,804 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 0
2023-02-17 15:41:43,177 DEBUG CV Batch 6/1600 loss 9.450026 loss_att 15.797766 loss_ctc 20.151503 loss_rnnt 6.589080 hw_loss 0.308502 history loss 9.122441 rank 2
2023-02-17 15:41:50,003 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 1
2023-02-17 15:41:50,754 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 4
2023-02-17 15:41:50,839 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 6
2023-02-17 15:41:51,967 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 5
2023-02-17 15:41:52,212 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 7
2023-02-17 15:41:53,075 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 3
2023-02-17 15:41:53,395 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 0
2023-02-17 15:41:55,610 DEBUG CV Batch 6/1700 loss 10.150037 loss_att 10.512934 loss_ctc 18.600847 loss_rnnt 8.758739 hw_loss 0.359892 history loss 9.023844 rank 2
2023-02-17 15:41:59,247 INFO Epoch 6 CV info cv_loss 8.987216988099519
2023-02-17 15:41:59,248 INFO Epoch 7 TRAIN info lr 0.0006542574975427367
2023-02-17 15:41:59,251 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:00,054 INFO Epoch 6 CV info cv_loss 8.987216987100222
2023-02-17 15:42:00,055 INFO Epoch 7 TRAIN info lr 0.0006546892119441996
2023-02-17 15:42:00,059 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:00,103 INFO Epoch 6 CV info cv_loss 8.987216986290447
2023-02-17 15:42:00,105 INFO Epoch 7 TRAIN info lr 0.0006546330968482318
2023-02-17 15:42:00,108 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:01,163 INFO Epoch 6 CV info cv_loss 8.987216987703246
2023-02-17 15:42:01,164 INFO Epoch 7 TRAIN info lr 0.0006544255963861218
2023-02-17 15:42:01,168 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:01,317 INFO Epoch 6 CV info cv_loss 8.98721698784108
2023-02-17 15:42:01,318 INFO Epoch 7 TRAIN info lr 0.0006545377342913616
2023-02-17 15:42:01,322 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:02,300 INFO Epoch 6 CV info cv_loss 8.987216988177051
2023-02-17 15:42:02,301 INFO Epoch 7 TRAIN info lr 0.0006544031757205658
2023-02-17 15:42:02,305 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:02,652 INFO Epoch 6 CV info cv_loss 8.987216986617803
2023-02-17 15:42:02,653 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/6.pt
2023-02-17 15:42:03,306 INFO Epoch 7 TRAIN info lr 0.0006543191187436711
2023-02-17 15:42:03,311 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:42:04,841 INFO Epoch 6 CV info cv_loss 8.987216987832465
2023-02-17 15:42:04,842 INFO Epoch 7 TRAIN info lr 0.0006538434069358356
2023-02-17 15:42:04,845 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 15:43:17,843 DEBUG TRAIN Batch 7/0 loss 6.589005 loss_att 6.983518 loss_ctc 9.020358 loss_rnnt 5.974836 hw_loss 0.395785 lr 0.00065463 rank 4
2023-02-17 15:43:17,844 DEBUG TRAIN Batch 7/0 loss 9.583849 loss_att 9.375092 loss_ctc 14.080727 loss_rnnt 8.813681 hw_loss 0.398131 lr 0.00065453 rank 7
2023-02-17 15:43:17,847 DEBUG TRAIN Batch 7/0 loss 14.146953 loss_att 13.582027 loss_ctc 18.805155 loss_rnnt 13.495070 hw_loss 0.269579 lr 0.00065468 rank 6
2023-02-17 15:43:17,847 DEBUG TRAIN Batch 7/0 loss 16.412239 loss_att 15.153459 loss_ctc 19.930740 loss_rnnt 16.000849 hw_loss 0.363769 lr 0.00065425 rank 1
2023-02-17 15:43:17,851 DEBUG TRAIN Batch 7/0 loss 12.073871 loss_att 12.031717 loss_ctc 16.026911 loss_rnnt 11.350161 hw_loss 0.384503 lr 0.00065442 rank 5
2023-02-17 15:43:17,861 DEBUG TRAIN Batch 7/0 loss 9.307568 loss_att 8.978113 loss_ctc 13.210245 loss_rnnt 8.632569 hw_loss 0.413496 lr 0.00065384 rank 2
2023-02-17 15:43:17,876 DEBUG TRAIN Batch 7/0 loss 11.750257 loss_att 11.658983 loss_ctc 15.289397 loss_rnnt 11.082661 hw_loss 0.401185 lr 0.00065431 rank 0
2023-02-17 15:43:17,892 DEBUG TRAIN Batch 7/0 loss 11.199219 loss_att 11.661399 loss_ctc 17.340799 loss_rnnt 10.082577 hw_loss 0.384989 lr 0.00065440 rank 3
2023-02-17 15:44:33,424 DEBUG TRAIN Batch 7/100 loss 14.323215 loss_att 20.257168 loss_ctc 26.277599 loss_rnnt 11.367729 hw_loss 0.327706 lr 0.00065412 rank 6
2023-02-17 15:44:33,428 DEBUG TRAIN Batch 7/100 loss 11.050989 loss_att 17.458282 loss_ctc 20.707115 loss_rnnt 8.326832 hw_loss 0.291029 lr 0.00065369 rank 1
2023-02-17 15:44:33,428 DEBUG TRAIN Batch 7/100 loss 19.088659 loss_att 24.373552 loss_ctc 29.744841 loss_rnnt 16.446651 hw_loss 0.307882 lr 0.00065397 rank 7
2023-02-17 15:44:33,432 DEBUG TRAIN Batch 7/100 loss 21.289330 loss_att 24.769981 loss_ctc 31.020836 loss_rnnt 19.103506 hw_loss 0.360298 lr 0.00065328 rank 2
2023-02-17 15:44:33,431 DEBUG TRAIN Batch 7/100 loss 32.721760 loss_att 35.223030 loss_ctc 50.900688 loss_rnnt 29.636778 hw_loss 0.301637 lr 0.00065407 rank 4
2023-02-17 15:44:33,432 DEBUG TRAIN Batch 7/100 loss 14.884857 loss_att 21.491470 loss_ctc 28.367239 loss_rnnt 11.617507 hw_loss 0.278207 lr 0.00065386 rank 5
2023-02-17 15:44:33,434 DEBUG TRAIN Batch 7/100 loss 24.321671 loss_att 26.600981 loss_ctc 33.406704 loss_rnnt 22.456406 hw_loss 0.371374 lr 0.00065375 rank 0
2023-02-17 15:44:33,480 DEBUG TRAIN Batch 7/100 loss 21.884151 loss_att 32.523071 loss_ctc 43.187191 loss_rnnt 16.732285 hw_loss 0.344389 lr 0.00065384 rank 3
2023-02-17 15:45:48,476 DEBUG TRAIN Batch 7/200 loss 14.125781 loss_att 17.626106 loss_ctc 29.897655 loss_rnnt 11.179075 hw_loss 0.269485 lr 0.00065356 rank 6
2023-02-17 15:45:48,479 DEBUG TRAIN Batch 7/200 loss 15.411255 loss_att 29.735868 loss_ctc 26.885916 loss_rnnt 10.879066 hw_loss 0.257457 lr 0.00065313 rank 1
2023-02-17 15:45:48,480 DEBUG TRAIN Batch 7/200 loss 40.374981 loss_att 39.742134 loss_ctc 57.931000 loss_rnnt 38.000683 hw_loss 0.300134 lr 0.00065341 rank 7
2023-02-17 15:45:48,480 DEBUG TRAIN Batch 7/200 loss 25.835888 loss_att 34.375198 loss_ctc 39.433197 loss_rnnt 22.125969 hw_loss 0.354527 lr 0.00065330 rank 5
2023-02-17 15:45:48,482 DEBUG TRAIN Batch 7/200 loss 10.512264 loss_att 13.106299 loss_ctc 13.404320 loss_rnnt 9.457774 hw_loss 0.281391 lr 0.00065272 rank 2
2023-02-17 15:45:48,483 DEBUG TRAIN Batch 7/200 loss 19.147667 loss_att 24.357323 loss_ctc 35.360451 loss_rnnt 15.783374 hw_loss 0.301234 lr 0.00065351 rank 4
2023-02-17 15:45:48,484 DEBUG TRAIN Batch 7/200 loss 12.292569 loss_att 21.563438 loss_ctc 29.395641 loss_rnnt 7.982971 hw_loss 0.328151 lr 0.00065320 rank 0
2023-02-17 15:45:48,486 DEBUG TRAIN Batch 7/200 loss 23.836161 loss_att 27.430241 loss_ctc 38.386696 loss_rnnt 21.032791 hw_loss 0.270904 lr 0.00065328 rank 3
2023-02-17 15:47:05,584 DEBUG TRAIN Batch 7/300 loss 15.616606 loss_att 21.490389 loss_ctc 23.358212 loss_rnnt 13.239691 hw_loss 0.318648 lr 0.00065286 rank 7
2023-02-17 15:47:05,588 DEBUG TRAIN Batch 7/300 loss 13.742188 loss_att 17.735680 loss_ctc 22.262184 loss_rnnt 11.654311 hw_loss 0.287210 lr 0.00065301 rank 6
2023-02-17 15:47:05,589 DEBUG TRAIN Batch 7/300 loss 21.142056 loss_att 27.017864 loss_ctc 36.456051 loss_rnnt 17.797886 hw_loss 0.238390 lr 0.00065274 rank 5
2023-02-17 15:47:05,593 DEBUG TRAIN Batch 7/300 loss 9.218048 loss_att 14.846428 loss_ctc 17.964985 loss_rnnt 6.766697 hw_loss 0.298908 lr 0.00065217 rank 2
2023-02-17 15:47:05,592 DEBUG TRAIN Batch 7/300 loss 14.769002 loss_att 18.799997 loss_ctc 22.545847 loss_rnnt 12.748609 hw_loss 0.332400 lr 0.00065264 rank 0
2023-02-17 15:47:05,593 DEBUG TRAIN Batch 7/300 loss 21.579586 loss_att 23.484669 loss_ctc 29.781139 loss_rnnt 19.936493 hw_loss 0.316003 lr 0.00065295 rank 4
2023-02-17 15:47:05,594 DEBUG TRAIN Batch 7/300 loss 14.873631 loss_att 16.608917 loss_ctc 20.041216 loss_rnnt 13.680737 hw_loss 0.294046 lr 0.00065272 rank 3
2023-02-17 15:47:05,594 DEBUG TRAIN Batch 7/300 loss 18.589401 loss_att 25.697071 loss_ctc 38.243908 loss_rnnt 14.391954 hw_loss 0.291207 lr 0.00065258 rank 1
2023-02-17 15:48:22,879 DEBUG TRAIN Batch 7/400 loss 17.852211 loss_att 21.727957 loss_ctc 26.777649 loss_rnnt 15.695955 hw_loss 0.358215 lr 0.00065245 rank 6
2023-02-17 15:48:22,881 DEBUG TRAIN Batch 7/400 loss 17.578493 loss_att 24.482946 loss_ctc 25.538086 loss_rnnt 14.948203 hw_loss 0.352727 lr 0.00065202 rank 1
2023-02-17 15:48:22,881 DEBUG TRAIN Batch 7/400 loss 8.243890 loss_att 11.678398 loss_ctc 15.807531 loss_rnnt 6.363588 hw_loss 0.346714 lr 0.00065230 rank 7
2023-02-17 15:48:22,882 DEBUG TRAIN Batch 7/400 loss 21.679346 loss_att 26.205677 loss_ctc 32.058495 loss_rnnt 19.231831 hw_loss 0.296933 lr 0.00065219 rank 5
2023-02-17 15:48:22,882 DEBUG TRAIN Batch 7/400 loss 13.113877 loss_att 15.744978 loss_ctc 20.341164 loss_rnnt 11.430186 hw_loss 0.363435 lr 0.00065239 rank 4
2023-02-17 15:48:22,883 DEBUG TRAIN Batch 7/400 loss 22.811323 loss_att 26.735075 loss_ctc 35.541679 loss_rnnt 20.154217 hw_loss 0.328077 lr 0.00065208 rank 0
2023-02-17 15:48:22,884 DEBUG TRAIN Batch 7/400 loss 20.496130 loss_att 25.245050 loss_ctc 31.618290 loss_rnnt 17.899408 hw_loss 0.307466 lr 0.00065161 rank 2
2023-02-17 15:48:22,923 DEBUG TRAIN Batch 7/400 loss 33.672684 loss_att 34.597473 loss_ctc 67.132156 loss_rnnt 28.855894 hw_loss 0.319814 lr 0.00065217 rank 3
2023-02-17 15:49:38,448 DEBUG TRAIN Batch 7/500 loss 7.171082 loss_att 11.428127 loss_ctc 17.136667 loss_rnnt 4.841043 hw_loss 0.281033 lr 0.00065190 rank 6
2023-02-17 15:49:38,452 DEBUG TRAIN Batch 7/500 loss 13.661966 loss_att 21.049620 loss_ctc 24.743429 loss_rnnt 10.577774 hw_loss 0.242124 lr 0.00065164 rank 5
2023-02-17 15:49:38,454 DEBUG TRAIN Batch 7/500 loss 24.620062 loss_att 28.052505 loss_ctc 38.100761 loss_rnnt 22.001589 hw_loss 0.252295 lr 0.00065147 rank 1
2023-02-17 15:49:38,455 DEBUG TRAIN Batch 7/500 loss 13.566759 loss_att 19.934166 loss_ctc 24.649473 loss_rnnt 10.612803 hw_loss 0.380213 lr 0.00065153 rank 0
2023-02-17 15:49:38,458 DEBUG TRAIN Batch 7/500 loss 18.122370 loss_att 19.664143 loss_ctc 28.619114 loss_rnnt 16.217251 hw_loss 0.369746 lr 0.00065175 rank 7
2023-02-17 15:49:38,458 DEBUG TRAIN Batch 7/500 loss 13.785147 loss_att 16.328808 loss_ctc 22.494905 loss_rnnt 11.965641 hw_loss 0.280261 lr 0.00065161 rank 3
2023-02-17 15:49:38,460 DEBUG TRAIN Batch 7/500 loss 10.343989 loss_att 13.435591 loss_ctc 15.594172 loss_rnnt 8.815693 hw_loss 0.393660 lr 0.00065106 rank 2
2023-02-17 15:49:38,462 DEBUG TRAIN Batch 7/500 loss 14.745228 loss_att 17.776352 loss_ctc 25.065451 loss_rnnt 12.627234 hw_loss 0.254511 lr 0.00065184 rank 4
2023-02-17 15:50:55,195 DEBUG TRAIN Batch 7/600 loss 13.730482 loss_att 17.111000 loss_ctc 21.841274 loss_rnnt 11.834690 hw_loss 0.259215 lr 0.00065108 rank 5
2023-02-17 15:50:55,197 DEBUG TRAIN Batch 7/600 loss 23.596455 loss_att 23.361897 loss_ctc 35.574257 loss_rnnt 21.865192 hw_loss 0.339621 lr 0.00065106 rank 3
2023-02-17 15:50:55,199 DEBUG TRAIN Batch 7/600 loss 13.621121 loss_att 15.610596 loss_ctc 21.147583 loss_rnnt 12.035578 hw_loss 0.345224 lr 0.00065092 rank 1
2023-02-17 15:50:55,200 DEBUG TRAIN Batch 7/600 loss 11.667432 loss_att 14.509266 loss_ctc 20.126976 loss_rnnt 9.788541 hw_loss 0.342348 lr 0.00065134 rank 6
2023-02-17 15:50:55,200 DEBUG TRAIN Batch 7/600 loss 21.770969 loss_att 21.048515 loss_ctc 28.780920 loss_rnnt 20.792871 hw_loss 0.352365 lr 0.00065119 rank 7
2023-02-17 15:50:55,202 DEBUG TRAIN Batch 7/600 loss 10.866079 loss_att 10.270719 loss_ctc 14.702992 loss_rnnt 10.237536 hw_loss 0.442552 lr 0.00065051 rank 2
2023-02-17 15:50:55,202 DEBUG TRAIN Batch 7/600 loss 12.177224 loss_att 13.299170 loss_ctc 17.230165 loss_rnnt 11.066630 hw_loss 0.398398 lr 0.00065098 rank 0
2023-02-17 15:50:55,242 DEBUG TRAIN Batch 7/600 loss 16.201126 loss_att 15.690159 loss_ctc 21.062674 loss_rnnt 15.486902 hw_loss 0.315394 lr 0.00065129 rank 4
2023-02-17 15:52:15,132 DEBUG TRAIN Batch 7/700 loss 24.126314 loss_att 26.615992 loss_ctc 44.843426 loss_rnnt 20.740522 hw_loss 0.235451 lr 0.00065079 rank 6
2023-02-17 15:52:15,132 DEBUG TRAIN Batch 7/700 loss 16.425161 loss_att 21.158981 loss_ctc 31.335617 loss_rnnt 13.327679 hw_loss 0.304981 lr 0.00064996 rank 2
2023-02-17 15:52:15,134 DEBUG TRAIN Batch 7/700 loss 18.156616 loss_att 24.308327 loss_ctc 31.980106 loss_rnnt 14.917128 hw_loss 0.311280 lr 0.00065051 rank 3
2023-02-17 15:52:15,135 DEBUG TRAIN Batch 7/700 loss 16.499838 loss_att 19.045958 loss_ctc 25.084015 loss_rnnt 14.719234 hw_loss 0.237795 lr 0.00065074 rank 4
2023-02-17 15:52:15,138 DEBUG TRAIN Batch 7/700 loss 15.124490 loss_att 17.649387 loss_ctc 25.514904 loss_rnnt 13.041212 hw_loss 0.361704 lr 0.00065043 rank 0
2023-02-17 15:52:15,151 DEBUG TRAIN Batch 7/700 loss 26.575764 loss_att 35.142830 loss_ctc 49.173672 loss_rnnt 21.688873 hw_loss 0.300790 lr 0.00065053 rank 5
2023-02-17 15:52:15,159 DEBUG TRAIN Batch 7/700 loss 9.881626 loss_att 15.188661 loss_ctc 20.553640 loss_rnnt 7.197439 hw_loss 0.374711 lr 0.00065037 rank 1
2023-02-17 15:52:15,165 DEBUG TRAIN Batch 7/700 loss 19.456705 loss_att 26.918148 loss_ctc 40.987377 loss_rnnt 14.966933 hw_loss 0.237612 lr 0.00065064 rank 7
2023-02-17 15:53:29,915 DEBUG TRAIN Batch 7/800 loss 12.363925 loss_att 18.399757 loss_ctc 25.079754 loss_rnnt 9.283758 hw_loss 0.332918 lr 0.00065009 rank 7
2023-02-17 15:53:29,920 DEBUG TRAIN Batch 7/800 loss 20.141897 loss_att 29.681768 loss_ctc 40.618668 loss_rnnt 15.346210 hw_loss 0.295273 lr 0.00065018 rank 4
2023-02-17 15:53:29,921 DEBUG TRAIN Batch 7/800 loss 19.705513 loss_att 24.846920 loss_ctc 38.497749 loss_rnnt 15.984547 hw_loss 0.350725 lr 0.00065024 rank 6
2023-02-17 15:53:29,921 DEBUG TRAIN Batch 7/800 loss 19.685410 loss_att 22.593193 loss_ctc 36.506054 loss_rnnt 16.680454 hw_loss 0.338712 lr 0.00064982 rank 1
2023-02-17 15:53:29,922 DEBUG TRAIN Batch 7/800 loss 11.012028 loss_att 18.987457 loss_ctc 18.163010 loss_rnnt 8.362645 hw_loss 0.189058 lr 0.00064998 rank 5
2023-02-17 15:53:29,924 DEBUG TRAIN Batch 7/800 loss 14.040871 loss_att 19.769634 loss_ctc 27.622320 loss_rnnt 10.919371 hw_loss 0.309166 lr 0.00064996 rank 3
2023-02-17 15:53:29,925 DEBUG TRAIN Batch 7/800 loss 18.328434 loss_att 23.330782 loss_ctc 29.544949 loss_rnnt 15.583260 hw_loss 0.467194 lr 0.00064941 rank 2
2023-02-17 15:53:29,928 DEBUG TRAIN Batch 7/800 loss 8.295488 loss_att 12.250250 loss_ctc 15.794350 loss_rnnt 6.318431 hw_loss 0.349232 lr 0.00064988 rank 0
2023-02-17 15:54:46,328 DEBUG TRAIN Batch 7/900 loss 16.022587 loss_att 17.305441 loss_ctc 22.272959 loss_rnnt 14.740384 hw_loss 0.360465 lr 0.00064954 rank 7
2023-02-17 15:54:46,331 DEBUG TRAIN Batch 7/900 loss 14.455607 loss_att 20.331734 loss_ctc 24.326508 loss_rnnt 11.808680 hw_loss 0.291716 lr 0.00064941 rank 3
2023-02-17 15:54:46,331 DEBUG TRAIN Batch 7/900 loss 14.986440 loss_att 18.514160 loss_ctc 24.339584 loss_rnnt 12.840696 hw_loss 0.362088 lr 0.00064969 rank 6
2023-02-17 15:54:46,331 DEBUG TRAIN Batch 7/900 loss 7.533521 loss_att 10.956687 loss_ctc 13.832157 loss_rnnt 5.832690 hw_loss 0.330711 lr 0.00064964 rank 4
2023-02-17 15:54:46,332 DEBUG TRAIN Batch 7/900 loss 19.662552 loss_att 23.879269 loss_ctc 35.092728 loss_rnnt 16.567575 hw_loss 0.364265 lr 0.00064943 rank 5
2023-02-17 15:54:46,333 DEBUG TRAIN Batch 7/900 loss 17.880928 loss_att 20.999413 loss_ctc 22.129366 loss_rnnt 16.502178 hw_loss 0.353612 lr 0.00064927 rank 1
2023-02-17 15:54:46,336 DEBUG TRAIN Batch 7/900 loss 21.531818 loss_att 23.220320 loss_ctc 29.531029 loss_rnnt 19.942760 hw_loss 0.346492 lr 0.00064933 rank 0
2023-02-17 15:54:46,340 DEBUG TRAIN Batch 7/900 loss 17.751169 loss_att 23.588829 loss_ctc 32.646294 loss_rnnt 14.465538 hw_loss 0.247657 lr 0.00064886 rank 2
2023-02-17 15:56:02,505 DEBUG TRAIN Batch 7/1000 loss 19.198099 loss_att 23.046904 loss_ctc 37.291656 loss_rnnt 15.822980 hw_loss 0.361655 lr 0.00064914 rank 6
2023-02-17 15:56:02,509 DEBUG TRAIN Batch 7/1000 loss 16.727190 loss_att 21.467123 loss_ctc 27.627884 loss_rnnt 14.172521 hw_loss 0.287355 lr 0.00064899 rank 7
2023-02-17 15:56:02,510 DEBUG TRAIN Batch 7/1000 loss 17.837950 loss_att 24.339882 loss_ctc 32.300797 loss_rnnt 14.482742 hw_loss 0.237078 lr 0.00064872 rank 1
2023-02-17 15:56:02,512 DEBUG TRAIN Batch 7/1000 loss 26.737286 loss_att 34.863064 loss_ctc 50.022518 loss_rnnt 21.865562 hw_loss 0.266003 lr 0.00064889 rank 5
2023-02-17 15:56:02,517 DEBUG TRAIN Batch 7/1000 loss 9.554242 loss_att 15.214817 loss_ctc 16.820816 loss_rnnt 7.260980 hw_loss 0.360505 lr 0.00064886 rank 3
2023-02-17 15:56:02,518 DEBUG TRAIN Batch 7/1000 loss 14.143247 loss_att 19.921101 loss_ctc 20.333332 loss_rnnt 12.031740 hw_loss 0.244859 lr 0.00064878 rank 0
2023-02-17 15:56:02,544 DEBUG TRAIN Batch 7/1000 loss 24.216341 loss_att 30.472572 loss_ctc 30.690489 loss_rnnt 21.983479 hw_loss 0.221985 lr 0.00064909 rank 4
2023-02-17 15:56:02,549 DEBUG TRAIN Batch 7/1000 loss 13.591114 loss_att 19.103035 loss_ctc 19.372826 loss_rnnt 11.530367 hw_loss 0.351500 lr 0.00064832 rank 2
2023-02-17 15:57:19,714 DEBUG TRAIN Batch 7/1100 loss 14.065109 loss_att 21.750126 loss_ctc 32.442490 loss_rnnt 9.966139 hw_loss 0.209344 lr 0.00064860 rank 6
2023-02-17 15:57:19,717 DEBUG TRAIN Batch 7/1100 loss 23.920815 loss_att 25.384670 loss_ctc 34.505699 loss_rnnt 22.019512 hw_loss 0.369775 lr 0.00064834 rank 5
2023-02-17 15:57:19,719 DEBUG TRAIN Batch 7/1100 loss 13.794809 loss_att 15.327637 loss_ctc 19.008049 loss_rnnt 12.595574 hw_loss 0.370446 lr 0.00064832 rank 3
2023-02-17 15:57:19,719 DEBUG TRAIN Batch 7/1100 loss 15.360082 loss_att 18.872898 loss_ctc 25.365208 loss_rnnt 13.165255 hw_loss 0.296712 lr 0.00064845 rank 7
2023-02-17 15:57:19,720 DEBUG TRAIN Batch 7/1100 loss 16.808613 loss_att 20.422737 loss_ctc 29.606344 loss_rnnt 14.209135 hw_loss 0.319292 lr 0.00064854 rank 4
2023-02-17 15:57:19,722 DEBUG TRAIN Batch 7/1100 loss 19.034819 loss_att 22.011311 loss_ctc 28.407398 loss_rnnt 16.950888 hw_loss 0.448040 lr 0.00064777 rank 2
2023-02-17 15:57:19,722 DEBUG TRAIN Batch 7/1100 loss 17.997982 loss_att 24.244774 loss_ctc 33.003197 loss_rnnt 14.606994 hw_loss 0.264253 lr 0.00064818 rank 1
2023-02-17 15:57:19,727 DEBUG TRAIN Batch 7/1100 loss 11.456672 loss_att 16.017139 loss_ctc 19.135906 loss_rnnt 9.301471 hw_loss 0.411019 lr 0.00064824 rank 0
2023-02-17 15:58:35,677 DEBUG TRAIN Batch 7/1200 loss 9.386848 loss_att 12.372942 loss_ctc 16.045982 loss_rnnt 7.674190 hw_loss 0.426663 lr 0.00064790 rank 7
2023-02-17 15:58:35,683 DEBUG TRAIN Batch 7/1200 loss 18.570650 loss_att 19.857405 loss_ctc 28.271244 loss_rnnt 16.843369 hw_loss 0.330971 lr 0.00064780 rank 5
2023-02-17 15:58:35,684 DEBUG TRAIN Batch 7/1200 loss 16.215065 loss_att 17.866449 loss_ctc 27.972151 loss_rnnt 14.084945 hw_loss 0.435433 lr 0.00064769 rank 0
2023-02-17 15:58:35,686 DEBUG TRAIN Batch 7/1200 loss 12.479209 loss_att 14.644237 loss_ctc 19.410053 loss_rnnt 10.965617 hw_loss 0.293388 lr 0.00064805 rank 6
2023-02-17 15:58:35,687 DEBUG TRAIN Batch 7/1200 loss 14.667050 loss_att 15.201872 loss_ctc 22.611185 loss_rnnt 13.270828 hw_loss 0.431322 lr 0.00064763 rank 1
2023-02-17 15:58:35,688 DEBUG TRAIN Batch 7/1200 loss 14.183802 loss_att 18.476433 loss_ctc 23.957680 loss_rnnt 11.839113 hw_loss 0.343081 lr 0.00064723 rank 2
2023-02-17 15:58:35,690 DEBUG TRAIN Batch 7/1200 loss 16.880301 loss_att 20.682144 loss_ctc 32.523716 loss_rnnt 13.839041 hw_loss 0.365813 lr 0.00064800 rank 4
2023-02-17 15:58:35,690 DEBUG TRAIN Batch 7/1200 loss 20.943722 loss_att 25.312469 loss_ctc 36.538853 loss_rnnt 17.852928 hw_loss 0.258172 lr 0.00064777 rank 3
2023-02-17 15:59:50,705 DEBUG TRAIN Batch 7/1300 loss 19.131697 loss_att 23.249750 loss_ctc 30.035233 loss_rnnt 16.688061 hw_loss 0.311659 lr 0.00064723 rank 3
2023-02-17 15:59:50,710 DEBUG TRAIN Batch 7/1300 loss 16.649099 loss_att 25.928905 loss_ctc 25.144377 loss_rnnt 13.547374 hw_loss 0.211986 lr 0.00064709 rank 1
2023-02-17 15:59:50,712 DEBUG TRAIN Batch 7/1300 loss 13.917608 loss_att 15.008230 loss_ctc 21.087852 loss_rnnt 12.631529 hw_loss 0.209856 lr 0.00064751 rank 6
2023-02-17 15:59:50,713 DEBUG TRAIN Batch 7/1300 loss 19.642897 loss_att 20.997169 loss_ctc 32.846214 loss_rnnt 17.540745 hw_loss 0.132852 lr 0.00064736 rank 7
2023-02-17 15:59:50,713 DEBUG TRAIN Batch 7/1300 loss 31.012596 loss_att 36.240337 loss_ctc 52.043098 loss_rnnt 26.951469 hw_loss 0.396584 lr 0.00064669 rank 2
2023-02-17 15:59:50,713 DEBUG TRAIN Batch 7/1300 loss 25.782282 loss_att 33.099892 loss_ctc 42.863075 loss_rnnt 21.860752 hw_loss 0.338557 lr 0.00064745 rank 4
2023-02-17 15:59:50,713 DEBUG TRAIN Batch 7/1300 loss 15.933539 loss_att 15.987523 loss_ctc 24.159224 loss_rnnt 14.637587 hw_loss 0.353247 lr 0.00064725 rank 5
2023-02-17 15:59:50,764 DEBUG TRAIN Batch 7/1300 loss 14.917011 loss_att 19.462475 loss_ctc 28.425243 loss_rnnt 12.039470 hw_loss 0.313783 lr 0.00064715 rank 0
2023-02-17 16:01:09,553 DEBUG TRAIN Batch 7/1400 loss 28.726484 loss_att 30.036415 loss_ctc 45.524567 loss_rnnt 26.063049 hw_loss 0.303196 lr 0.00064671 rank 5
2023-02-17 16:01:09,555 DEBUG TRAIN Batch 7/1400 loss 27.904184 loss_att 35.371632 loss_ctc 54.375515 loss_rnnt 22.721304 hw_loss 0.299771 lr 0.00064682 rank 7
2023-02-17 16:01:09,555 DEBUG TRAIN Batch 7/1400 loss 29.921482 loss_att 34.226803 loss_ctc 47.327118 loss_rnnt 26.487875 hw_loss 0.472109 lr 0.00064655 rank 1
2023-02-17 16:01:09,556 DEBUG TRAIN Batch 7/1400 loss 14.815615 loss_att 20.163296 loss_ctc 23.114141 loss_rnnt 12.468913 hw_loss 0.320052 lr 0.00064615 rank 2
2023-02-17 16:01:09,557 DEBUG TRAIN Batch 7/1400 loss 15.939131 loss_att 16.055531 loss_ctc 21.902737 loss_rnnt 14.844851 hw_loss 0.517224 lr 0.00064697 rank 6
2023-02-17 16:01:09,557 DEBUG TRAIN Batch 7/1400 loss 14.669320 loss_att 15.286594 loss_ctc 22.523468 loss_rnnt 13.332019 hw_loss 0.312427 lr 0.00064669 rank 3
2023-02-17 16:01:09,558 DEBUG TRAIN Batch 7/1400 loss 19.196732 loss_att 24.213093 loss_ctc 38.743736 loss_rnnt 15.391823 hw_loss 0.366317 lr 0.00064691 rank 4
2023-02-17 16:01:09,560 DEBUG TRAIN Batch 7/1400 loss 9.362278 loss_att 12.513828 loss_ctc 16.726654 loss_rnnt 7.571746 hw_loss 0.334323 lr 0.00064661 rank 0
2023-02-17 16:02:26,800 DEBUG TRAIN Batch 7/1500 loss 18.486277 loss_att 20.960659 loss_ctc 30.532576 loss_rnnt 16.203194 hw_loss 0.341311 lr 0.00064601 rank 1
2023-02-17 16:02:26,802 DEBUG TRAIN Batch 7/1500 loss 12.628510 loss_att 17.097649 loss_ctc 24.528088 loss_rnnt 9.984381 hw_loss 0.306919 lr 0.00064617 rank 5
2023-02-17 16:02:26,802 DEBUG TRAIN Batch 7/1500 loss 7.562723 loss_att 14.112372 loss_ctc 12.325870 loss_rnnt 5.409059 hw_loss 0.391217 lr 0.00064561 rank 2
2023-02-17 16:02:26,804 DEBUG TRAIN Batch 7/1500 loss 15.677759 loss_att 21.974581 loss_ctc 23.941862 loss_rnnt 13.163718 hw_loss 0.286493 lr 0.00064642 rank 6
2023-02-17 16:02:26,805 DEBUG TRAIN Batch 7/1500 loss 10.290189 loss_att 13.442152 loss_ctc 23.340378 loss_rnnt 7.711515 hw_loss 0.390479 lr 0.00064628 rank 7
2023-02-17 16:02:26,806 DEBUG TRAIN Batch 7/1500 loss 24.439724 loss_att 31.374174 loss_ctc 37.038818 loss_rnnt 21.212048 hw_loss 0.301706 lr 0.00064637 rank 4
2023-02-17 16:02:26,806 DEBUG TRAIN Batch 7/1500 loss 19.343565 loss_att 19.312134 loss_ctc 25.911285 loss_rnnt 18.314066 hw_loss 0.300165 lr 0.00064607 rank 0
2023-02-17 16:02:26,851 DEBUG TRAIN Batch 7/1500 loss 19.636763 loss_att 23.930546 loss_ctc 31.064070 loss_rnnt 17.088242 hw_loss 0.311481 lr 0.00064615 rank 3
2023-02-17 16:03:41,715 DEBUG TRAIN Batch 7/1600 loss 15.247381 loss_att 19.564682 loss_ctc 27.376812 loss_rnnt 12.590221 hw_loss 0.330829 lr 0.00064547 rank 1
2023-02-17 16:03:41,715 DEBUG TRAIN Batch 7/1600 loss 19.715847 loss_att 22.308687 loss_ctc 36.043327 loss_rnnt 16.776493 hw_loss 0.457099 lr 0.00064561 rank 3
2023-02-17 16:03:41,718 DEBUG TRAIN Batch 7/1600 loss 20.220907 loss_att 26.346821 loss_ctc 34.145771 loss_rnnt 16.949011 hw_loss 0.356374 lr 0.00064507 rank 2
2023-02-17 16:03:41,719 DEBUG TRAIN Batch 7/1600 loss 17.364973 loss_att 18.782907 loss_ctc 23.685631 loss_rnnt 16.070795 hw_loss 0.314693 lr 0.00064574 rank 7
2023-02-17 16:03:41,723 DEBUG TRAIN Batch 7/1600 loss 18.197983 loss_att 25.189287 loss_ctc 27.726198 loss_rnnt 15.374973 hw_loss 0.289350 lr 0.00064583 rank 4
2023-02-17 16:03:41,724 DEBUG TRAIN Batch 7/1600 loss 14.795825 loss_att 17.044138 loss_ctc 20.830154 loss_rnnt 13.377627 hw_loss 0.307422 lr 0.00064588 rank 6
2023-02-17 16:03:41,726 DEBUG TRAIN Batch 7/1600 loss 23.350639 loss_att 26.842623 loss_ctc 40.723251 loss_rnnt 20.172146 hw_loss 0.307026 lr 0.00064563 rank 5
2023-02-17 16:03:41,728 DEBUG TRAIN Batch 7/1600 loss 17.395790 loss_att 22.620277 loss_ctc 27.704784 loss_rnnt 14.828999 hw_loss 0.276302 lr 0.00064553 rank 0
2023-02-17 16:04:57,450 DEBUG TRAIN Batch 7/1700 loss 13.099609 loss_att 16.731726 loss_ctc 22.507959 loss_rnnt 10.887272 hw_loss 0.434001 lr 0.00064507 rank 3
2023-02-17 16:04:57,455 DEBUG TRAIN Batch 7/1700 loss 20.909861 loss_att 24.620447 loss_ctc 37.411873 loss_rnnt 17.762188 hw_loss 0.384913 lr 0.00064529 rank 4
2023-02-17 16:04:57,454 DEBUG TRAIN Batch 7/1700 loss 9.383636 loss_att 12.524950 loss_ctc 14.303893 loss_rnnt 7.892371 hw_loss 0.388066 lr 0.00064509 rank 5
2023-02-17 16:04:57,456 DEBUG TRAIN Batch 7/1700 loss 47.449589 loss_att 48.767311 loss_ctc 63.571072 loss_rnnt 44.893463 hw_loss 0.268214 lr 0.00064535 rank 6
2023-02-17 16:04:57,458 DEBUG TRAIN Batch 7/1700 loss 14.519043 loss_att 16.137556 loss_ctc 19.368462 loss_rnnt 13.427643 hw_loss 0.227080 lr 0.00064454 rank 2
2023-02-17 16:04:57,460 DEBUG TRAIN Batch 7/1700 loss 19.965416 loss_att 23.625473 loss_ctc 35.611141 loss_rnnt 16.939350 hw_loss 0.389921 lr 0.00064499 rank 0
2023-02-17 16:04:57,460 DEBUG TRAIN Batch 7/1700 loss 21.785904 loss_att 24.170963 loss_ctc 34.641373 loss_rnnt 19.394955 hw_loss 0.374764 lr 0.00064493 rank 1
2023-02-17 16:04:57,461 DEBUG TRAIN Batch 7/1700 loss 18.176018 loss_att 24.259655 loss_ctc 28.828737 loss_rnnt 15.346350 hw_loss 0.361083 lr 0.00064520 rank 7
2023-02-17 16:06:17,032 DEBUG TRAIN Batch 7/1800 loss 20.680986 loss_att 24.175095 loss_ctc 37.530567 loss_rnnt 17.514746 hw_loss 0.414013 lr 0.00064456 rank 5
2023-02-17 16:06:17,035 DEBUG TRAIN Batch 7/1800 loss 21.100176 loss_att 24.910364 loss_ctc 34.154804 loss_rnnt 18.454483 hw_loss 0.268196 lr 0.00064446 rank 0
2023-02-17 16:06:17,036 DEBUG TRAIN Batch 7/1800 loss 12.523719 loss_att 13.741575 loss_ctc 19.480217 loss_rnnt 11.159864 hw_loss 0.361407 lr 0.00064481 rank 6
2023-02-17 16:06:17,038 DEBUG TRAIN Batch 7/1800 loss 33.468258 loss_att 37.518063 loss_ctc 54.007130 loss_rnnt 29.745264 hw_loss 0.327211 lr 0.00064476 rank 4
2023-02-17 16:06:17,038 DEBUG TRAIN Batch 7/1800 loss 22.208780 loss_att 22.221386 loss_ctc 27.691927 loss_rnnt 21.274075 hw_loss 0.377059 lr 0.00064467 rank 7
2023-02-17 16:06:17,038 DEBUG TRAIN Batch 7/1800 loss 14.686672 loss_att 20.221087 loss_ctc 25.721354 loss_rnnt 11.933232 hw_loss 0.328622 lr 0.00064454 rank 3
2023-02-17 16:06:17,040 DEBUG TRAIN Batch 7/1800 loss 19.843071 loss_att 26.076794 loss_ctc 33.109795 loss_rnnt 16.676140 hw_loss 0.283668 lr 0.00064440 rank 1
2023-02-17 16:06:17,041 DEBUG TRAIN Batch 7/1800 loss 22.270155 loss_att 24.395260 loss_ctc 41.652657 loss_rnnt 19.049494 hw_loss 0.396198 lr 0.00064400 rank 2
2023-02-17 16:07:33,847 DEBUG TRAIN Batch 7/1900 loss 14.951445 loss_att 17.051632 loss_ctc 23.525684 loss_rnnt 13.177137 hw_loss 0.395697 lr 0.00064427 rank 6
2023-02-17 16:07:33,848 DEBUG TRAIN Batch 7/1900 loss 14.375104 loss_att 15.579620 loss_ctc 22.348185 loss_rnnt 12.892543 hw_loss 0.334836 lr 0.00064386 rank 1
2023-02-17 16:07:33,850 DEBUG TRAIN Batch 7/1900 loss 13.994039 loss_att 14.041651 loss_ctc 21.734762 loss_rnnt 12.745758 hw_loss 0.387490 lr 0.00064413 rank 7
2023-02-17 16:07:33,853 DEBUG TRAIN Batch 7/1900 loss 11.188171 loss_att 16.734358 loss_ctc 20.248032 loss_rnnt 8.688474 hw_loss 0.342148 lr 0.00064402 rank 5
2023-02-17 16:07:33,856 DEBUG TRAIN Batch 7/1900 loss 15.988441 loss_att 16.588491 loss_ctc 24.853796 loss_rnnt 14.443624 hw_loss 0.455172 lr 0.00064400 rank 3
2023-02-17 16:07:33,857 DEBUG TRAIN Batch 7/1900 loss 20.455059 loss_att 20.032261 loss_ctc 25.542843 loss_rnnt 19.604887 hw_loss 0.480677 lr 0.00064347 rank 2
2023-02-17 16:07:33,859 DEBUG TRAIN Batch 7/1900 loss 20.165134 loss_att 23.805012 loss_ctc 32.305363 loss_rnnt 17.622749 hw_loss 0.366963 lr 0.00064392 rank 0
2023-02-17 16:07:33,900 DEBUG TRAIN Batch 7/1900 loss 11.574142 loss_att 10.735441 loss_ctc 15.119100 loss_rnnt 11.038206 hw_loss 0.433156 lr 0.00064422 rank 4
2023-02-17 16:08:49,661 DEBUG TRAIN Batch 7/2000 loss 29.807795 loss_att 36.509193 loss_ctc 45.608490 loss_rnnt 26.222153 hw_loss 0.259880 lr 0.00064374 rank 6
2023-02-17 16:08:49,664 DEBUG TRAIN Batch 7/2000 loss 22.774721 loss_att 23.541998 loss_ctc 37.449120 loss_rnnt 20.480797 hw_loss 0.344780 lr 0.00064360 rank 7
2023-02-17 16:08:49,665 DEBUG TRAIN Batch 7/2000 loss 11.926762 loss_att 17.913883 loss_ctc 19.179808 loss_rnnt 9.617924 hw_loss 0.270639 lr 0.00064294 rank 2
2023-02-17 16:08:49,666 DEBUG TRAIN Batch 7/2000 loss 21.398809 loss_att 28.575375 loss_ctc 37.561321 loss_rnnt 17.632799 hw_loss 0.329427 lr 0.00064333 rank 1
2023-02-17 16:08:49,666 DEBUG TRAIN Batch 7/2000 loss 7.377621 loss_att 11.489801 loss_ctc 12.479479 loss_rnnt 5.624945 hw_loss 0.468735 lr 0.00064349 rank 5
2023-02-17 16:08:49,671 DEBUG TRAIN Batch 7/2000 loss 19.346823 loss_att 21.894707 loss_ctc 28.596478 loss_rnnt 17.438160 hw_loss 0.310873 lr 0.00064347 rank 3
2023-02-17 16:08:49,672 DEBUG TRAIN Batch 7/2000 loss 17.037958 loss_att 16.666950 loss_ctc 25.236797 loss_rnnt 15.846603 hw_loss 0.323208 lr 0.00064369 rank 4
2023-02-17 16:08:49,673 DEBUG TRAIN Batch 7/2000 loss 22.199448 loss_att 22.170483 loss_ctc 32.039284 loss_rnnt 20.709200 hw_loss 0.345116 lr 0.00064339 rank 0
2023-02-17 16:10:08,160 DEBUG TRAIN Batch 7/2100 loss 12.894409 loss_att 17.371761 loss_ctc 20.280746 loss_rnnt 10.798510 hw_loss 0.404221 lr 0.00064294 rank 3
2023-02-17 16:10:08,168 DEBUG TRAIN Batch 7/2100 loss 16.606401 loss_att 21.411850 loss_ctc 25.913298 loss_rnnt 14.203952 hw_loss 0.375828 lr 0.00064315 rank 4
2023-02-17 16:10:08,169 DEBUG TRAIN Batch 7/2100 loss 21.621590 loss_att 27.982979 loss_ctc 33.273739 loss_rnnt 18.651144 hw_loss 0.271029 lr 0.00064296 rank 5
2023-02-17 16:10:08,171 DEBUG TRAIN Batch 7/2100 loss 15.941847 loss_att 17.774759 loss_ctc 24.269123 loss_rnnt 14.293915 hw_loss 0.320711 lr 0.00064306 rank 7
2023-02-17 16:10:08,174 DEBUG TRAIN Batch 7/2100 loss 8.744032 loss_att 15.288267 loss_ctc 18.743374 loss_rnnt 5.946712 hw_loss 0.291052 lr 0.00064241 rank 2
2023-02-17 16:10:08,195 DEBUG TRAIN Batch 7/2100 loss 11.958974 loss_att 18.028698 loss_ctc 23.406548 loss_rnnt 9.107843 hw_loss 0.207829 lr 0.00064286 rank 0
2023-02-17 16:10:08,196 DEBUG TRAIN Batch 7/2100 loss 11.551139 loss_att 19.110222 loss_ctc 25.312164 loss_rnnt 8.071596 hw_loss 0.249232 lr 0.00064280 rank 1
2023-02-17 16:10:08,206 DEBUG TRAIN Batch 7/2100 loss 12.746892 loss_att 15.010933 loss_ctc 20.345831 loss_rnnt 11.163043 hw_loss 0.220966 lr 0.00064321 rank 6
2023-02-17 16:11:25,718 DEBUG TRAIN Batch 7/2200 loss 14.426259 loss_att 18.556953 loss_ctc 23.525717 loss_rnnt 12.187129 hw_loss 0.374497 lr 0.00064253 rank 7
2023-02-17 16:11:25,719 DEBUG TRAIN Batch 7/2200 loss 11.246541 loss_att 15.386547 loss_ctc 20.645632 loss_rnnt 8.940482 hw_loss 0.421587 lr 0.00064227 rank 1
2023-02-17 16:11:25,719 DEBUG TRAIN Batch 7/2200 loss 14.135592 loss_att 19.297266 loss_ctc 22.705328 loss_rnnt 11.809669 hw_loss 0.283044 lr 0.00064243 rank 5
2023-02-17 16:11:25,720 DEBUG TRAIN Batch 7/2200 loss 22.088867 loss_att 24.735338 loss_ctc 36.241493 loss_rnnt 19.505257 hw_loss 0.313686 lr 0.00064188 rank 2
2023-02-17 16:11:25,721 DEBUG TRAIN Batch 7/2200 loss 12.442557 loss_att 17.521885 loss_ctc 22.357620 loss_rnnt 9.874403 hw_loss 0.431776 lr 0.00064262 rank 4
2023-02-17 16:11:25,721 DEBUG TRAIN Batch 7/2200 loss 9.231245 loss_att 13.247047 loss_ctc 14.910376 loss_rnnt 7.479531 hw_loss 0.358753 lr 0.00064268 rank 6
2023-02-17 16:11:25,723 DEBUG TRAIN Batch 7/2200 loss 17.323927 loss_att 19.423300 loss_ctc 27.574230 loss_rnnt 15.332867 hw_loss 0.383395 lr 0.00064241 rank 3
2023-02-17 16:11:25,724 DEBUG TRAIN Batch 7/2200 loss 18.786814 loss_att 22.316492 loss_ctc 31.340687 loss_rnnt 16.198820 hw_loss 0.390392 lr 0.00064233 rank 0
2023-02-17 16:12:43,106 DEBUG TRAIN Batch 7/2300 loss 24.307632 loss_att 28.112160 loss_ctc 34.814167 loss_rnnt 21.965961 hw_loss 0.337299 lr 0.00064190 rank 5
2023-02-17 16:12:43,107 DEBUG TRAIN Batch 7/2300 loss 16.421106 loss_att 18.857185 loss_ctc 26.302727 loss_rnnt 14.443711 hw_loss 0.323681 lr 0.00064200 rank 7
2023-02-17 16:12:43,107 DEBUG TRAIN Batch 7/2300 loss 13.149593 loss_att 15.991047 loss_ctc 20.867798 loss_rnnt 11.360039 hw_loss 0.360319 lr 0.00064188 rank 3
2023-02-17 16:12:43,109 DEBUG TRAIN Batch 7/2300 loss 18.054764 loss_att 25.352198 loss_ctc 36.301659 loss_rnnt 13.988017 hw_loss 0.326893 lr 0.00064209 rank 4
2023-02-17 16:12:43,109 DEBUG TRAIN Batch 7/2300 loss 32.209068 loss_att 33.419983 loss_ctc 57.414391 loss_rnnt 28.421825 hw_loss 0.345656 lr 0.00064215 rank 6
2023-02-17 16:12:43,110 DEBUG TRAIN Batch 7/2300 loss 12.117332 loss_att 22.079241 loss_ctc 20.683523 loss_rnnt 8.823769 hw_loss 0.298167 lr 0.00064135 rank 2
2023-02-17 16:12:43,110 DEBUG TRAIN Batch 7/2300 loss 24.004574 loss_att 29.034351 loss_ctc 37.032742 loss_rnnt 21.091434 hw_loss 0.318928 lr 0.00064174 rank 1
2023-02-17 16:12:43,157 DEBUG TRAIN Batch 7/2300 loss 15.568484 loss_att 16.087137 loss_ctc 23.433413 loss_rnnt 14.251710 hw_loss 0.308224 lr 0.00064180 rank 0
2023-02-17 16:13:59,909 DEBUG TRAIN Batch 7/2400 loss 20.183735 loss_att 23.210964 loss_ctc 22.652319 loss_rnnt 19.123720 hw_loss 0.235167 lr 0.00064135 rank 3
2023-02-17 16:13:59,915 DEBUG TRAIN Batch 7/2400 loss 12.874063 loss_att 15.319590 loss_ctc 24.278412 loss_rnnt 10.710739 hw_loss 0.288072 lr 0.00064156 rank 4
2023-02-17 16:13:59,915 DEBUG TRAIN Batch 7/2400 loss 13.048885 loss_att 16.398972 loss_ctc 21.457256 loss_rnnt 11.078943 hw_loss 0.335266 lr 0.00064137 rank 5
2023-02-17 16:13:59,916 DEBUG TRAIN Batch 7/2400 loss 18.087744 loss_att 24.875053 loss_ctc 31.351604 loss_rnnt 14.835213 hw_loss 0.237290 lr 0.00064082 rank 2
2023-02-17 16:13:59,916 DEBUG TRAIN Batch 7/2400 loss 13.461581 loss_att 15.055162 loss_ctc 18.159060 loss_rnnt 12.310190 hw_loss 0.386893 lr 0.00064162 rank 6
2023-02-17 16:13:59,920 DEBUG TRAIN Batch 7/2400 loss 6.591183 loss_att 9.874947 loss_ctc 10.081782 loss_rnnt 5.261880 hw_loss 0.388382 lr 0.00064147 rank 7
2023-02-17 16:13:59,923 DEBUG TRAIN Batch 7/2400 loss 9.827920 loss_att 12.763477 loss_ctc 12.903941 loss_rnnt 8.651313 hw_loss 0.336298 lr 0.00064121 rank 1
2023-02-17 16:13:59,970 DEBUG TRAIN Batch 7/2400 loss 17.735020 loss_att 18.918373 loss_ctc 23.670851 loss_rnnt 16.462061 hw_loss 0.459079 lr 0.00064127 rank 0
2023-02-17 16:15:19,413 DEBUG TRAIN Batch 7/2500 loss 13.791153 loss_att 16.516094 loss_ctc 24.259893 loss_rnnt 11.687991 hw_loss 0.304391 lr 0.00064068 rank 1
2023-02-17 16:15:19,413 DEBUG TRAIN Batch 7/2500 loss 12.820900 loss_att 18.025341 loss_ctc 19.711021 loss_rnnt 10.657391 hw_loss 0.382386 lr 0.00064095 rank 7
2023-02-17 16:15:19,413 DEBUG TRAIN Batch 7/2500 loss 28.343416 loss_att 31.483431 loss_ctc 38.841991 loss_rnnt 26.105621 hw_loss 0.393713 lr 0.00064109 rank 6
2023-02-17 16:15:19,414 DEBUG TRAIN Batch 7/2500 loss 14.446208 loss_att 14.089085 loss_ctc 20.271339 loss_rnnt 13.522318 hw_loss 0.409930 lr 0.00064084 rank 5
2023-02-17 16:15:19,417 DEBUG TRAIN Batch 7/2500 loss 19.958591 loss_att 21.252056 loss_ctc 30.130381 loss_rnnt 18.166344 hw_loss 0.332466 lr 0.00064104 rank 4
2023-02-17 16:15:19,434 DEBUG TRAIN Batch 7/2500 loss 11.936529 loss_att 12.801128 loss_ctc 17.369171 loss_rnnt 10.869423 hw_loss 0.318436 lr 0.00064029 rank 2
2023-02-17 16:15:19,463 DEBUG TRAIN Batch 7/2500 loss 17.373625 loss_att 17.844929 loss_ctc 25.556135 loss_rnnt 16.009947 hw_loss 0.334532 lr 0.00064082 rank 3
2023-02-17 16:15:19,488 DEBUG TRAIN Batch 7/2500 loss 12.337281 loss_att 18.096252 loss_ctc 21.202723 loss_rnnt 9.880298 hw_loss 0.230868 lr 0.00064074 rank 0
2023-02-17 16:16:36,551 DEBUG TRAIN Batch 7/2600 loss 11.006469 loss_att 17.359192 loss_ctc 20.169821 loss_rnnt 8.348300 hw_loss 0.310958 lr 0.00064051 rank 4
2023-02-17 16:16:36,552 DEBUG TRAIN Batch 7/2600 loss 24.193253 loss_att 26.900242 loss_ctc 35.332600 loss_rnnt 22.007908 hw_loss 0.297560 lr 0.00064032 rank 5
2023-02-17 16:16:36,553 DEBUG TRAIN Batch 7/2600 loss 14.337369 loss_att 14.695586 loss_ctc 17.225599 loss_rnnt 13.663736 hw_loss 0.406671 lr 0.00064042 rank 7
2023-02-17 16:16:36,555 DEBUG TRAIN Batch 7/2600 loss 18.656033 loss_att 21.962414 loss_ctc 26.292389 loss_rnnt 16.736015 hw_loss 0.451051 lr 0.00063977 rank 2
2023-02-17 16:16:36,555 DEBUG TRAIN Batch 7/2600 loss 7.267419 loss_att 13.183128 loss_ctc 13.336050 loss_rnnt 5.106073 hw_loss 0.316974 lr 0.00064016 rank 1
2023-02-17 16:16:36,556 DEBUG TRAIN Batch 7/2600 loss 12.133247 loss_att 14.832052 loss_ctc 22.045425 loss_rnnt 10.100564 hw_loss 0.321184 lr 0.00064056 rank 6
2023-02-17 16:16:36,560 DEBUG TRAIN Batch 7/2600 loss 18.897781 loss_att 23.804432 loss_ctc 31.131668 loss_rnnt 16.182180 hw_loss 0.193288 lr 0.00064022 rank 0
2023-02-17 16:16:36,603 DEBUG TRAIN Batch 7/2600 loss 12.190839 loss_att 18.520580 loss_ctc 20.015251 loss_rnnt 9.704814 hw_loss 0.331542 lr 0.00064029 rank 3
2023-02-17 16:17:52,482 DEBUG TRAIN Batch 7/2700 loss 12.872455 loss_att 16.169153 loss_ctc 24.715534 loss_rnnt 10.447899 hw_loss 0.349009 lr 0.00063999 rank 4
2023-02-17 16:17:52,483 DEBUG TRAIN Batch 7/2700 loss 20.841944 loss_att 27.867910 loss_ctc 39.155083 loss_rnnt 16.834606 hw_loss 0.300734 lr 0.00063977 rank 3
2023-02-17 16:17:52,484 DEBUG TRAIN Batch 7/2700 loss 10.726134 loss_att 14.510880 loss_ctc 16.447001 loss_rnnt 9.069872 hw_loss 0.255996 lr 0.00063979 rank 5
2023-02-17 16:17:52,484 DEBUG TRAIN Batch 7/2700 loss 33.863750 loss_att 41.173401 loss_ctc 57.543407 loss_rnnt 29.077826 hw_loss 0.312574 lr 0.00063990 rank 7
2023-02-17 16:17:52,485 DEBUG TRAIN Batch 7/2700 loss 13.303111 loss_att 17.788233 loss_ctc 25.116550 loss_rnnt 10.652570 hw_loss 0.334485 lr 0.00064004 rank 6
2023-02-17 16:17:52,487 DEBUG TRAIN Batch 7/2700 loss 11.247826 loss_att 15.468400 loss_ctc 19.236275 loss_rnnt 9.155054 hw_loss 0.344119 lr 0.00063925 rank 2
2023-02-17 16:17:52,487 DEBUG TRAIN Batch 7/2700 loss 18.275885 loss_att 26.141666 loss_ctc 39.775230 loss_rnnt 13.669458 hw_loss 0.312543 lr 0.00063963 rank 1
2023-02-17 16:17:52,499 DEBUG TRAIN Batch 7/2700 loss 19.223366 loss_att 20.525614 loss_ctc 31.403677 loss_rnnt 17.110601 hw_loss 0.428012 lr 0.00063969 rank 0
2023-02-17 16:19:10,619 DEBUG TRAIN Batch 7/2800 loss 15.512716 loss_att 17.871212 loss_ctc 27.963024 loss_rnnt 13.230465 hw_loss 0.282207 lr 0.00063951 rank 6
2023-02-17 16:19:10,623 DEBUG TRAIN Batch 7/2800 loss 12.934144 loss_att 16.084682 loss_ctc 22.619461 loss_rnnt 10.796523 hw_loss 0.405257 lr 0.00063937 rank 7
2023-02-17 16:19:10,624 DEBUG TRAIN Batch 7/2800 loss 17.373644 loss_att 22.233906 loss_ctc 25.718596 loss_rnnt 15.116940 hw_loss 0.322487 lr 0.00063917 rank 0
2023-02-17 16:19:10,624 DEBUG TRAIN Batch 7/2800 loss 19.413836 loss_att 26.228739 loss_ctc 30.650259 loss_rnnt 16.390966 hw_loss 0.303187 lr 0.00063927 rank 5
2023-02-17 16:19:10,626 DEBUG TRAIN Batch 7/2800 loss 25.099983 loss_att 30.087330 loss_ctc 42.053329 loss_rnnt 21.666214 hw_loss 0.329721 lr 0.00063946 rank 4
2023-02-17 16:19:10,626 DEBUG TRAIN Batch 7/2800 loss 9.118377 loss_att 11.229426 loss_ctc 14.317001 loss_rnnt 7.767164 hw_loss 0.442224 lr 0.00063925 rank 3
2023-02-17 16:19:10,627 DEBUG TRAIN Batch 7/2800 loss 26.586504 loss_att 27.551382 loss_ctc 39.413193 loss_rnnt 24.487123 hw_loss 0.367837 lr 0.00063873 rank 2
2023-02-17 16:19:10,628 DEBUG TRAIN Batch 7/2800 loss 18.572306 loss_att 21.816349 loss_ctc 31.339470 loss_rnnt 16.044430 hw_loss 0.331460 lr 0.00063911 rank 1
2023-02-17 16:20:27,467 DEBUG TRAIN Batch 7/2900 loss 14.817621 loss_att 20.254847 loss_ctc 22.975058 loss_rnnt 12.471885 hw_loss 0.319940 lr 0.00063899 rank 6
2023-02-17 16:20:27,468 DEBUG TRAIN Batch 7/2900 loss 24.335217 loss_att 26.989006 loss_ctc 42.255486 loss_rnnt 21.206305 hw_loss 0.391473 lr 0.00063859 rank 1
2023-02-17 16:20:27,474 DEBUG TRAIN Batch 7/2900 loss 15.770022 loss_att 17.368364 loss_ctc 26.393221 loss_rnnt 13.821701 hw_loss 0.397922 lr 0.00063820 rank 2
2023-02-17 16:20:27,475 DEBUG TRAIN Batch 7/2900 loss 17.754862 loss_att 22.946951 loss_ctc 33.155609 loss_rnnt 14.502267 hw_loss 0.301395 lr 0.00063875 rank 5
2023-02-17 16:20:27,475 DEBUG TRAIN Batch 7/2900 loss 25.022549 loss_att 30.514437 loss_ctc 41.694893 loss_rnnt 21.528728 hw_loss 0.323370 lr 0.00063873 rank 3
2023-02-17 16:20:27,476 DEBUG TRAIN Batch 7/2900 loss 11.882359 loss_att 15.717440 loss_ctc 19.433319 loss_rnnt 9.982000 hw_loss 0.237274 lr 0.00063885 rank 7
2023-02-17 16:20:27,478 DEBUG TRAIN Batch 7/2900 loss 11.654497 loss_att 14.367210 loss_ctc 21.152155 loss_rnnt 9.677660 hw_loss 0.314887 lr 0.00063894 rank 4
2023-02-17 16:20:27,482 DEBUG TRAIN Batch 7/2900 loss 15.257005 loss_att 18.735926 loss_ctc 25.941561 loss_rnnt 12.990263 hw_loss 0.274408 lr 0.00063865 rank 0
2023-02-17 16:21:43,164 DEBUG TRAIN Batch 7/3000 loss 10.844744 loss_att 15.877020 loss_ctc 17.576899 loss_rnnt 8.733942 hw_loss 0.387611 lr 0.00063823 rank 5
2023-02-17 16:21:43,167 DEBUG TRAIN Batch 7/3000 loss 20.699358 loss_att 25.780296 loss_ctc 38.061111 loss_rnnt 17.233280 hw_loss 0.253103 lr 0.00063847 rank 6
2023-02-17 16:21:43,169 DEBUG TRAIN Batch 7/3000 loss 10.322517 loss_att 15.816833 loss_ctc 20.401199 loss_rnnt 7.756514 hw_loss 0.231218 lr 0.00063833 rank 7
2023-02-17 16:21:43,170 DEBUG TRAIN Batch 7/3000 loss 18.019463 loss_att 19.990454 loss_ctc 33.540916 loss_rnnt 15.387886 hw_loss 0.314721 lr 0.00063820 rank 3
2023-02-17 16:21:43,170 DEBUG TRAIN Batch 7/3000 loss 22.494835 loss_att 28.111334 loss_ctc 36.566608 loss_rnnt 19.342052 hw_loss 0.287337 lr 0.00063807 rank 1
2023-02-17 16:21:43,177 DEBUG TRAIN Batch 7/3000 loss 28.761459 loss_att 34.077881 loss_ctc 48.780788 loss_rnnt 24.796421 hw_loss 0.435953 lr 0.00063813 rank 0
2023-02-17 16:21:43,177 DEBUG TRAIN Batch 7/3000 loss 21.740295 loss_att 23.801161 loss_ctc 36.231041 loss_rnnt 19.181561 hw_loss 0.402118 lr 0.00063842 rank 4
2023-02-17 16:21:43,180 DEBUG TRAIN Batch 7/3000 loss 26.801512 loss_att 29.234440 loss_ctc 38.511490 loss_rnnt 24.601004 hw_loss 0.286114 lr 0.00063769 rank 2
2023-02-17 16:22:59,579 DEBUG TRAIN Batch 7/3100 loss 11.814523 loss_att 14.951603 loss_ctc 24.814861 loss_rnnt 9.267029 hw_loss 0.350062 lr 0.00063781 rank 7
2023-02-17 16:22:59,580 DEBUG TRAIN Batch 7/3100 loss 14.975718 loss_att 15.339511 loss_ctc 23.796087 loss_rnnt 13.544921 hw_loss 0.341232 lr 0.00063717 rank 2
2023-02-17 16:22:59,580 DEBUG TRAIN Batch 7/3100 loss 24.083113 loss_att 26.001692 loss_ctc 35.563251 loss_rnnt 21.968925 hw_loss 0.374600 lr 0.00063795 rank 6
2023-02-17 16:22:59,583 DEBUG TRAIN Batch 7/3100 loss 15.638409 loss_att 15.867502 loss_ctc 22.871290 loss_rnnt 14.449094 hw_loss 0.335831 lr 0.00063769 rank 3
2023-02-17 16:22:59,584 DEBUG TRAIN Batch 7/3100 loss 8.202851 loss_att 9.621416 loss_ctc 12.311549 loss_rnnt 7.180970 hw_loss 0.356891 lr 0.00063790 rank 4
2023-02-17 16:22:59,585 DEBUG TRAIN Batch 7/3100 loss 12.637955 loss_att 16.733677 loss_ctc 25.071852 loss_rnnt 10.029373 hw_loss 0.246720 lr 0.00063755 rank 1
2023-02-17 16:22:59,587 DEBUG TRAIN Batch 7/3100 loss 16.651224 loss_att 18.249037 loss_ctc 26.390400 loss_rnnt 14.859725 hw_loss 0.325087 lr 0.00063771 rank 5
2023-02-17 16:22:59,634 DEBUG TRAIN Batch 7/3100 loss 12.589445 loss_att 12.759890 loss_ctc 17.118328 loss_rnnt 11.756557 hw_loss 0.365528 lr 0.00063761 rank 0
2023-02-17 16:24:21,471 DEBUG TRAIN Batch 7/3200 loss 11.797032 loss_att 17.879257 loss_ctc 19.486843 loss_rnnt 9.398304 hw_loss 0.294327 lr 0.00063738 rank 4
2023-02-17 16:24:21,471 DEBUG TRAIN Batch 7/3200 loss 18.805674 loss_att 24.372807 loss_ctc 31.649466 loss_rnnt 15.843367 hw_loss 0.255702 lr 0.00063717 rank 3
2023-02-17 16:24:21,475 DEBUG TRAIN Batch 7/3200 loss 14.436499 loss_att 19.802834 loss_ctc 30.289238 loss_rnnt 11.056888 hw_loss 0.361209 lr 0.00063719 rank 5
2023-02-17 16:24:21,476 DEBUG TRAIN Batch 7/3200 loss 18.793396 loss_att 20.045113 loss_ctc 30.014435 loss_rnnt 16.904224 hw_loss 0.267546 lr 0.00063729 rank 7
2023-02-17 16:24:21,478 DEBUG TRAIN Batch 7/3200 loss 10.160564 loss_att 9.649303 loss_ctc 13.946661 loss_rnnt 9.467578 hw_loss 0.544550 lr 0.00063703 rank 1
2023-02-17 16:24:21,479 DEBUG TRAIN Batch 7/3200 loss 18.830009 loss_att 24.223314 loss_ctc 31.845032 loss_rnnt 15.806575 hw_loss 0.392692 lr 0.00063665 rank 2
2023-02-17 16:24:21,480 DEBUG TRAIN Batch 7/3200 loss 12.180866 loss_att 17.181936 loss_ctc 20.759050 loss_rnnt 9.912435 hw_loss 0.233361 lr 0.00063709 rank 0
2023-02-17 16:24:21,489 DEBUG TRAIN Batch 7/3200 loss 15.129207 loss_att 22.941339 loss_ctc 23.320354 loss_rnnt 12.306414 hw_loss 0.315398 lr 0.00063743 rank 6
2023-02-17 16:25:38,336 DEBUG TRAIN Batch 7/3300 loss 22.909046 loss_att 28.016247 loss_ctc 42.794308 loss_rnnt 19.064156 hw_loss 0.322657 lr 0.00063691 rank 6
2023-02-17 16:25:38,340 DEBUG TRAIN Batch 7/3300 loss 12.269089 loss_att 18.417767 loss_ctc 20.103603 loss_rnnt 9.811025 hw_loss 0.344486 lr 0.00063677 rank 7
2023-02-17 16:25:38,342 DEBUG TRAIN Batch 7/3300 loss 12.717498 loss_att 23.804436 loss_ctc 22.111366 loss_rnnt 9.055721 hw_loss 0.359762 lr 0.00063667 rank 5
2023-02-17 16:25:38,345 DEBUG TRAIN Batch 7/3300 loss 26.159590 loss_att 25.617872 loss_ctc 31.927010 loss_rnnt 25.285416 hw_loss 0.400365 lr 0.00063665 rank 3
2023-02-17 16:25:38,345 DEBUG TRAIN Batch 7/3300 loss 25.791615 loss_att 26.936661 loss_ctc 36.396919 loss_rnnt 23.975027 hw_loss 0.325383 lr 0.00063652 rank 1
2023-02-17 16:25:38,346 DEBUG TRAIN Batch 7/3300 loss 10.914853 loss_att 19.564262 loss_ctc 22.306801 loss_rnnt 7.524530 hw_loss 0.265338 lr 0.00063614 rank 2
2023-02-17 16:25:38,349 DEBUG TRAIN Batch 7/3300 loss 20.231016 loss_att 23.926081 loss_ctc 30.105820 loss_rnnt 17.955017 hw_loss 0.413145 lr 0.00063657 rank 0
2023-02-17 16:25:38,395 DEBUG TRAIN Batch 7/3300 loss 10.771652 loss_att 19.122412 loss_ctc 30.891361 loss_rnnt 6.234835 hw_loss 0.345071 lr 0.00063686 rank 4
2023-02-17 16:26:56,344 DEBUG TRAIN Batch 7/3400 loss 20.333496 loss_att 24.054380 loss_ctc 38.370262 loss_rnnt 16.996563 hw_loss 0.352228 lr 0.00063640 rank 6
2023-02-17 16:26:56,346 DEBUG TRAIN Batch 7/3400 loss 20.901197 loss_att 27.258024 loss_ctc 34.371857 loss_rnnt 17.685192 hw_loss 0.278533 lr 0.00063562 rank 2
2023-02-17 16:26:56,347 DEBUG TRAIN Batch 7/3400 loss 10.005643 loss_att 12.514984 loss_ctc 19.135254 loss_rnnt 8.148857 hw_loss 0.258066 lr 0.00063616 rank 5
2023-02-17 16:26:56,348 DEBUG TRAIN Batch 7/3400 loss 11.293418 loss_att 16.610302 loss_ctc 23.826385 loss_rnnt 8.375086 hw_loss 0.344797 lr 0.00063614 rank 3
2023-02-17 16:26:56,348 DEBUG TRAIN Batch 7/3400 loss 15.152287 loss_att 15.392220 loss_ctc 22.026470 loss_rnnt 14.004470 hw_loss 0.343638 lr 0.00063600 rank 1
2023-02-17 16:26:56,350 DEBUG TRAIN Batch 7/3400 loss 16.148205 loss_att 18.811417 loss_ctc 20.007902 loss_rnnt 14.966815 hw_loss 0.251475 lr 0.00063606 rank 0
2023-02-17 16:26:56,350 DEBUG TRAIN Batch 7/3400 loss 22.091549 loss_att 28.383942 loss_ctc 41.251797 loss_rnnt 18.181000 hw_loss 0.182574 lr 0.00063626 rank 7
2023-02-17 16:26:56,387 DEBUG TRAIN Batch 7/3400 loss 12.037918 loss_att 16.756617 loss_ctc 18.181286 loss_rnnt 10.103809 hw_loss 0.321100 lr 0.00063635 rank 4
2023-02-17 16:28:13,985 DEBUG TRAIN Batch 7/3500 loss 22.699770 loss_att 28.625845 loss_ctc 33.752163 loss_rnnt 19.845295 hw_loss 0.366764 lr 0.00063554 rank 0
2023-02-17 16:28:13,990 DEBUG TRAIN Batch 7/3500 loss 21.678373 loss_att 23.178801 loss_ctc 36.173828 loss_rnnt 19.282402 hw_loss 0.305927 lr 0.00063562 rank 3
2023-02-17 16:28:13,994 DEBUG TRAIN Batch 7/3500 loss 18.322445 loss_att 22.108355 loss_ctc 30.151390 loss_rnnt 15.845491 hw_loss 0.267334 lr 0.00063583 rank 4
2023-02-17 16:28:13,994 DEBUG TRAIN Batch 7/3500 loss 22.180162 loss_att 27.701645 loss_ctc 41.054291 loss_rnnt 18.391609 hw_loss 0.314452 lr 0.00063549 rank 1
2023-02-17 16:28:13,995 DEBUG TRAIN Batch 7/3500 loss 21.014711 loss_att 28.378408 loss_ctc 39.265770 loss_rnnt 16.952297 hw_loss 0.292873 lr 0.00063564 rank 5
2023-02-17 16:28:13,995 DEBUG TRAIN Batch 7/3500 loss 17.885580 loss_att 22.990295 loss_ctc 34.008816 loss_rnnt 14.591206 hw_loss 0.231871 lr 0.00063574 rank 7
2023-02-17 16:28:13,995 DEBUG TRAIN Batch 7/3500 loss 25.461014 loss_att 30.545717 loss_ctc 42.479179 loss_rnnt 22.003485 hw_loss 0.321565 lr 0.00063588 rank 6
2023-02-17 16:28:14,036 DEBUG TRAIN Batch 7/3500 loss 4.872599 loss_att 8.004308 loss_ctc 7.184541 loss_rnnt 3.781670 hw_loss 0.293116 lr 0.00063511 rank 2
2023-02-17 16:29:32,742 DEBUG TRAIN Batch 7/3600 loss 9.355615 loss_att 14.711340 loss_ctc 16.320446 loss_rnnt 7.178160 hw_loss 0.333120 lr 0.00063523 rank 7
2023-02-17 16:29:32,743 DEBUG TRAIN Batch 7/3600 loss 14.553272 loss_att 14.739052 loss_ctc 23.219208 loss_rnnt 13.201942 hw_loss 0.297594 lr 0.00063513 rank 5
2023-02-17 16:29:32,743 DEBUG TRAIN Batch 7/3600 loss 7.905835 loss_att 9.676637 loss_ctc 15.687675 loss_rnnt 6.335882 hw_loss 0.334151 lr 0.00063503 rank 0
2023-02-17 16:29:32,744 DEBUG TRAIN Batch 7/3600 loss 11.386512 loss_att 14.469206 loss_ctc 17.693455 loss_rnnt 9.779417 hw_loss 0.280555 lr 0.00063532 rank 4
2023-02-17 16:29:32,745 DEBUG TRAIN Batch 7/3600 loss 16.729250 loss_att 19.664650 loss_ctc 23.868822 loss_rnnt 15.025657 hw_loss 0.308569 lr 0.00063537 rank 6
2023-02-17 16:29:32,746 DEBUG TRAIN Batch 7/3600 loss 29.377604 loss_att 34.932835 loss_ctc 47.273964 loss_rnnt 25.735037 hw_loss 0.272511 lr 0.00063498 rank 1
2023-02-17 16:29:32,748 DEBUG TRAIN Batch 7/3600 loss 19.460907 loss_att 23.315331 loss_ctc 33.472969 loss_rnnt 16.659847 hw_loss 0.303562 lr 0.00063460 rank 2
2023-02-17 16:29:32,784 DEBUG TRAIN Batch 7/3600 loss 14.262368 loss_att 18.004902 loss_ctc 29.311716 loss_rnnt 11.349073 hw_loss 0.296644 lr 0.00063511 rank 3
2023-02-17 16:30:49,759 DEBUG TRAIN Batch 7/3700 loss 27.205135 loss_att 30.721298 loss_ctc 41.271393 loss_rnnt 24.441719 hw_loss 0.346281 lr 0.00063472 rank 7
2023-02-17 16:30:49,764 DEBUG TRAIN Batch 7/3700 loss 11.842422 loss_att 14.995487 loss_ctc 20.062737 loss_rnnt 9.941397 hw_loss 0.326945 lr 0.00063486 rank 6
2023-02-17 16:30:49,767 DEBUG TRAIN Batch 7/3700 loss 13.833274 loss_att 18.078766 loss_ctc 27.465466 loss_rnnt 10.929751 hw_loss 0.443998 lr 0.00063446 rank 1
2023-02-17 16:30:49,768 DEBUG TRAIN Batch 7/3700 loss 27.646948 loss_att 33.417587 loss_ctc 46.037746 loss_rnnt 23.895935 hw_loss 0.271458 lr 0.00063409 rank 2
2023-02-17 16:30:49,768 DEBUG TRAIN Batch 7/3700 loss 20.922979 loss_att 22.480831 loss_ctc 40.010578 loss_rnnt 17.902924 hw_loss 0.306507 lr 0.00063462 rank 5
2023-02-17 16:30:49,770 DEBUG TRAIN Batch 7/3700 loss 20.795874 loss_att 23.727367 loss_ctc 41.665901 loss_rnnt 17.244892 hw_loss 0.341272 lr 0.00063481 rank 4
2023-02-17 16:30:49,770 DEBUG TRAIN Batch 7/3700 loss 9.945938 loss_att 12.847363 loss_ctc 16.868299 loss_rnnt 8.240598 hw_loss 0.378888 lr 0.00063460 rank 3
2023-02-17 16:30:49,815 DEBUG TRAIN Batch 7/3700 loss 23.814877 loss_att 25.648573 loss_ctc 35.699379 loss_rnnt 21.657238 hw_loss 0.386809 lr 0.00063452 rank 0
2023-02-17 16:32:06,772 DEBUG TRAIN Batch 7/3800 loss 38.022221 loss_att 40.819698 loss_ctc 62.924530 loss_rnnt 33.920738 hw_loss 0.415648 lr 0.00063435 rank 6
2023-02-17 16:32:06,772 DEBUG TRAIN Batch 7/3800 loss 11.241588 loss_att 15.745141 loss_ctc 18.920805 loss_rnnt 9.105863 hw_loss 0.395846 lr 0.00063421 rank 7
2023-02-17 16:32:06,776 DEBUG TRAIN Batch 7/3800 loss 12.098631 loss_att 11.399916 loss_ctc 16.606974 loss_rnnt 11.417130 hw_loss 0.412748 lr 0.00063411 rank 5
2023-02-17 16:32:06,777 DEBUG TRAIN Batch 7/3800 loss 7.782515 loss_att 8.175758 loss_ctc 10.976474 loss_rnnt 7.015980 hw_loss 0.491296 lr 0.00063358 rank 2
2023-02-17 16:32:06,778 DEBUG TRAIN Batch 7/3800 loss 24.651613 loss_att 24.563808 loss_ctc 43.230125 loss_rnnt 22.061148 hw_loss 0.245417 lr 0.00063401 rank 0
2023-02-17 16:32:06,778 DEBUG TRAIN Batch 7/3800 loss 13.983503 loss_att 15.561405 loss_ctc 20.047352 loss_rnnt 12.644337 hw_loss 0.403261 lr 0.00063395 rank 1
2023-02-17 16:32:06,781 DEBUG TRAIN Batch 7/3800 loss 17.082281 loss_att 16.929420 loss_ctc 22.604849 loss_rnnt 16.138050 hw_loss 0.447114 lr 0.00063430 rank 4
2023-02-17 16:32:06,825 DEBUG TRAIN Batch 7/3800 loss 11.660320 loss_att 12.681341 loss_ctc 16.477745 loss_rnnt 10.592683 hw_loss 0.414580 lr 0.00063409 rank 3
2023-02-17 16:33:26,041 DEBUG TRAIN Batch 7/3900 loss 9.476833 loss_att 13.596843 loss_ctc 15.078569 loss_rnnt 7.725036 hw_loss 0.339183 lr 0.00063384 rank 6
2023-02-17 16:33:26,044 DEBUG TRAIN Batch 7/3900 loss 22.818420 loss_att 31.408955 loss_ctc 35.355453 loss_rnnt 19.218796 hw_loss 0.393588 lr 0.00063370 rank 7
2023-02-17 16:33:26,045 DEBUG TRAIN Batch 7/3900 loss 18.499823 loss_att 22.085730 loss_ctc 42.613270 loss_rnnt 14.358623 hw_loss 0.391669 lr 0.00063379 rank 4
2023-02-17 16:33:26,050 DEBUG TRAIN Batch 7/3900 loss 19.967667 loss_att 24.378332 loss_ctc 37.449120 loss_rnnt 16.526358 hw_loss 0.428096 lr 0.00063358 rank 3
2023-02-17 16:33:26,051 DEBUG TRAIN Batch 7/3900 loss 20.047392 loss_att 28.186405 loss_ctc 28.385855 loss_rnnt 17.176561 hw_loss 0.246059 lr 0.00063350 rank 0
2023-02-17 16:33:26,052 DEBUG TRAIN Batch 7/3900 loss 14.316337 loss_att 17.452641 loss_ctc 21.236542 loss_rnnt 12.607225 hw_loss 0.298416 lr 0.00063307 rank 2
2023-02-17 16:33:26,057 DEBUG TRAIN Batch 7/3900 loss 13.585629 loss_att 19.054806 loss_ctc 25.663633 loss_rnnt 10.756201 hw_loss 0.234734 lr 0.00063360 rank 5
2023-02-17 16:33:26,086 DEBUG TRAIN Batch 7/3900 loss 22.014771 loss_att 27.054321 loss_ctc 32.749935 loss_rnnt 19.395527 hw_loss 0.337459 lr 0.00063344 rank 1
2023-02-17 16:34:42,672 DEBUG TRAIN Batch 7/4000 loss 34.628391 loss_att 38.625313 loss_ctc 50.716179 loss_rnnt 31.520674 hw_loss 0.306172 lr 0.00063309 rank 5
2023-02-17 16:34:42,678 DEBUG TRAIN Batch 7/4000 loss 29.408894 loss_att 31.856506 loss_ctc 44.163719 loss_rnnt 26.761389 hw_loss 0.357510 lr 0.00063299 rank 0
2023-02-17 16:34:42,680 DEBUG TRAIN Batch 7/4000 loss 33.278282 loss_att 32.054794 loss_ctc 56.725842 loss_rnnt 30.214371 hw_loss 0.341751 lr 0.00063319 rank 7
2023-02-17 16:34:42,680 DEBUG TRAIN Batch 7/4000 loss 9.026673 loss_att 14.286396 loss_ctc 17.013844 loss_rnnt 6.762784 hw_loss 0.275602 lr 0.00063333 rank 6
2023-02-17 16:34:42,681 DEBUG TRAIN Batch 7/4000 loss 7.115819 loss_att 10.294206 loss_ctc 14.282972 loss_rnnt 5.383321 hw_loss 0.264750 lr 0.00063328 rank 4
2023-02-17 16:34:42,681 DEBUG TRAIN Batch 7/4000 loss 16.181749 loss_att 17.872887 loss_ctc 26.606848 loss_rnnt 14.259811 hw_loss 0.363184 lr 0.00063307 rank 3
2023-02-17 16:34:42,682 DEBUG TRAIN Batch 7/4000 loss 35.987469 loss_att 37.145981 loss_ctc 58.570564 loss_rnnt 32.549362 hw_loss 0.366236 lr 0.00063294 rank 1
2023-02-17 16:34:42,684 DEBUG TRAIN Batch 7/4000 loss 22.911533 loss_att 28.985142 loss_ctc 33.898544 loss_rnnt 20.039761 hw_loss 0.360224 lr 0.00063256 rank 2
2023-02-17 16:35:59,193 DEBUG TRAIN Batch 7/4100 loss 13.516362 loss_att 17.181812 loss_ctc 25.224712 loss_rnnt 11.057167 hw_loss 0.309361 lr 0.00063206 rank 2
2023-02-17 16:35:59,195 DEBUG TRAIN Batch 7/4100 loss 12.625443 loss_att 17.855474 loss_ctc 18.491385 loss_rnnt 10.667287 hw_loss 0.243792 lr 0.00063282 rank 6
2023-02-17 16:35:59,196 DEBUG TRAIN Batch 7/4100 loss 27.799660 loss_att 30.584757 loss_ctc 45.652161 loss_rnnt 24.661642 hw_loss 0.376248 lr 0.00063243 rank 1
2023-02-17 16:35:59,197 DEBUG TRAIN Batch 7/4100 loss 16.721970 loss_att 22.646080 loss_ctc 30.856327 loss_rnnt 13.525295 hw_loss 0.238633 lr 0.00063258 rank 5
2023-02-17 16:35:59,197 DEBUG TRAIN Batch 7/4100 loss 18.191418 loss_att 20.937914 loss_ctc 31.695431 loss_rnnt 15.667521 hw_loss 0.326366 lr 0.00063249 rank 0
2023-02-17 16:35:59,198 DEBUG TRAIN Batch 7/4100 loss 11.362324 loss_att 14.065359 loss_ctc 18.389467 loss_rnnt 9.719931 hw_loss 0.309061 lr 0.00063268 rank 7
2023-02-17 16:35:59,200 DEBUG TRAIN Batch 7/4100 loss 11.829105 loss_att 13.741625 loss_ctc 20.211319 loss_rnnt 10.138967 hw_loss 0.356261 lr 0.00063277 rank 4
2023-02-17 16:35:59,201 DEBUG TRAIN Batch 7/4100 loss 8.953487 loss_att 12.990909 loss_ctc 14.418696 loss_rnnt 7.227259 hw_loss 0.356342 lr 0.00063256 rank 3
2023-02-17 16:37:17,760 DEBUG TRAIN Batch 7/4200 loss 12.895343 loss_att 15.522691 loss_ctc 17.943443 loss_rnnt 11.501758 hw_loss 0.365693 lr 0.00063231 rank 6
2023-02-17 16:37:17,760 DEBUG TRAIN Batch 7/4200 loss 10.886702 loss_att 14.295870 loss_ctc 17.523241 loss_rnnt 9.137636 hw_loss 0.341925 lr 0.00063206 rank 3
2023-02-17 16:37:17,762 DEBUG TRAIN Batch 7/4200 loss 12.538836 loss_att 17.337912 loss_ctc 18.190584 loss_rnnt 10.703574 hw_loss 0.228527 lr 0.00063226 rank 4
2023-02-17 16:37:17,765 DEBUG TRAIN Batch 7/4200 loss 27.943048 loss_att 34.585567 loss_ctc 44.603447 loss_rnnt 24.214531 hw_loss 0.334927 lr 0.00063208 rank 5
2023-02-17 16:37:17,767 DEBUG TRAIN Batch 7/4200 loss 18.874741 loss_att 23.590759 loss_ctc 31.766510 loss_rnnt 16.060217 hw_loss 0.285783 lr 0.00063192 rank 1
2023-02-17 16:37:17,769 DEBUG TRAIN Batch 7/4200 loss 20.718597 loss_att 21.522421 loss_ctc 30.018263 loss_rnnt 19.156786 hw_loss 0.302047 lr 0.00063218 rank 7
2023-02-17 16:37:17,770 DEBUG TRAIN Batch 7/4200 loss 18.976950 loss_att 23.050579 loss_ctc 36.968964 loss_rnnt 15.579900 hw_loss 0.343851 lr 0.00063198 rank 0
2023-02-17 16:37:17,814 DEBUG TRAIN Batch 7/4200 loss 29.251263 loss_att 30.980602 loss_ctc 43.779179 loss_rnnt 26.770031 hw_loss 0.371827 lr 0.00063155 rank 2
2023-02-17 16:38:37,240 DEBUG TRAIN Batch 7/4300 loss 27.041479 loss_att 35.894051 loss_ctc 40.255898 loss_rnnt 23.346397 hw_loss 0.304958 lr 0.00063142 rank 1
2023-02-17 16:38:37,240 DEBUG TRAIN Batch 7/4300 loss 18.182993 loss_att 18.958416 loss_ctc 28.757374 loss_rnnt 16.413645 hw_loss 0.383149 lr 0.00063167 rank 7
2023-02-17 16:38:37,244 DEBUG TRAIN Batch 7/4300 loss 17.921751 loss_att 23.720636 loss_ctc 32.359848 loss_rnnt 14.620906 hw_loss 0.404980 lr 0.00063148 rank 0
2023-02-17 16:38:37,246 DEBUG TRAIN Batch 7/4300 loss 23.386028 loss_att 25.073412 loss_ctc 37.816677 loss_rnnt 21.018547 hw_loss 0.198598 lr 0.00063181 rank 6
2023-02-17 16:38:37,247 DEBUG TRAIN Batch 7/4300 loss 20.838781 loss_att 23.826693 loss_ctc 30.075102 loss_rnnt 18.807770 hw_loss 0.378599 lr 0.00063176 rank 4
2023-02-17 16:38:37,247 DEBUG TRAIN Batch 7/4300 loss 14.673013 loss_att 18.403616 loss_ctc 21.955921 loss_rnnt 12.773178 hw_loss 0.342486 lr 0.00063155 rank 3
2023-02-17 16:38:37,251 DEBUG TRAIN Batch 7/4300 loss 18.531563 loss_att 21.281361 loss_ctc 31.813797 loss_rnnt 16.026367 hw_loss 0.345511 lr 0.00063157 rank 5
2023-02-17 16:38:37,252 DEBUG TRAIN Batch 7/4300 loss 16.199127 loss_att 20.498110 loss_ctc 29.285137 loss_rnnt 13.391388 hw_loss 0.380887 lr 0.00063105 rank 2
2023-02-17 16:39:52,724 DEBUG TRAIN Batch 7/4400 loss 12.247399 loss_att 13.451749 loss_ctc 20.723053 loss_rnnt 10.687695 hw_loss 0.353900 lr 0.00063117 rank 7
2023-02-17 16:39:52,726 DEBUG TRAIN Batch 7/4400 loss 15.308113 loss_att 16.933708 loss_ctc 18.812687 loss_rnnt 14.277542 hw_loss 0.446580 lr 0.00063105 rank 3
2023-02-17 16:39:52,726 DEBUG TRAIN Batch 7/4400 loss 34.237370 loss_att 38.687542 loss_ctc 57.506359 loss_rnnt 30.073109 hw_loss 0.321927 lr 0.00063131 rank 6
2023-02-17 16:39:52,729 DEBUG TRAIN Batch 7/4400 loss 10.106707 loss_att 11.752657 loss_ctc 21.316854 loss_rnnt 8.108234 hw_loss 0.327367 lr 0.00063107 rank 5
2023-02-17 16:39:52,729 DEBUG TRAIN Batch 7/4400 loss 13.690349 loss_att 17.503929 loss_ctc 26.296400 loss_rnnt 11.051509 hw_loss 0.366219 lr 0.00063055 rank 2
2023-02-17 16:39:52,732 DEBUG TRAIN Batch 7/4400 loss 20.968855 loss_att 24.317806 loss_ctc 36.205139 loss_rnnt 18.123039 hw_loss 0.270975 lr 0.00063092 rank 1
2023-02-17 16:39:52,734 DEBUG TRAIN Batch 7/4400 loss 10.151752 loss_att 10.343872 loss_ctc 17.946081 loss_rnnt 8.844410 hw_loss 0.430639 lr 0.00063097 rank 0
2023-02-17 16:39:52,775 DEBUG TRAIN Batch 7/4400 loss 11.698969 loss_att 12.644186 loss_ctc 20.793808 loss_rnnt 10.133208 hw_loss 0.307634 lr 0.00063125 rank 4
2023-02-17 16:41:08,982 DEBUG TRAIN Batch 7/4500 loss 10.599677 loss_att 10.455973 loss_ctc 15.430237 loss_rnnt 9.839055 hw_loss 0.272415 lr 0.00063067 rank 7
2023-02-17 16:41:08,983 DEBUG TRAIN Batch 7/4500 loss 30.717934 loss_att 31.029739 loss_ctc 38.700188 loss_rnnt 29.416365 hw_loss 0.327952 lr 0.00063075 rank 4
2023-02-17 16:41:08,984 DEBUG TRAIN Batch 7/4500 loss 10.435313 loss_att 14.532084 loss_ctc 17.980642 loss_rnnt 8.416195 hw_loss 0.363225 lr 0.00063055 rank 3
2023-02-17 16:41:08,983 DEBUG TRAIN Batch 7/4500 loss 14.856927 loss_att 20.425365 loss_ctc 28.371862 loss_rnnt 11.785130 hw_loss 0.292724 lr 0.00063080 rank 6
2023-02-17 16:41:08,986 DEBUG TRAIN Batch 7/4500 loss 27.638182 loss_att 28.120247 loss_ctc 45.820629 loss_rnnt 24.935051 hw_loss 0.341984 lr 0.00063057 rank 5
2023-02-17 16:41:08,987 DEBUG TRAIN Batch 7/4500 loss 10.806865 loss_att 15.466934 loss_ctc 23.552328 loss_rnnt 7.958274 hw_loss 0.407216 lr 0.00063005 rank 2
2023-02-17 16:41:08,987 DEBUG TRAIN Batch 7/4500 loss 14.763806 loss_att 16.543171 loss_ctc 25.462606 loss_rnnt 12.915413 hw_loss 0.123775 lr 0.00063042 rank 1
2023-02-17 16:41:08,994 DEBUG TRAIN Batch 7/4500 loss 10.848236 loss_att 17.310179 loss_ctc 19.810938 loss_rnnt 8.165590 hw_loss 0.366056 lr 0.00063047 rank 0
2023-02-17 16:42:28,527 DEBUG TRAIN Batch 7/4600 loss 6.799264 loss_att 8.967659 loss_ctc 9.944985 loss_rnnt 5.719349 hw_loss 0.425262 lr 0.00063007 rank 5
2023-02-17 16:42:28,532 DEBUG TRAIN Batch 7/4600 loss 20.342644 loss_att 24.760176 loss_ctc 36.881081 loss_rnnt 17.078976 hw_loss 0.328196 lr 0.00062992 rank 1
2023-02-17 16:42:28,534 DEBUG TRAIN Batch 7/4600 loss 16.061146 loss_att 16.516975 loss_ctc 22.985088 loss_rnnt 14.885077 hw_loss 0.303206 lr 0.00063017 rank 7
2023-02-17 16:42:28,534 DEBUG TRAIN Batch 7/4600 loss 11.857821 loss_att 14.441704 loss_ctc 22.732346 loss_rnnt 9.761943 hw_loss 0.242186 lr 0.00063030 rank 6
2023-02-17 16:42:28,535 DEBUG TRAIN Batch 7/4600 loss 12.807017 loss_att 17.205379 loss_ctc 21.425217 loss_rnnt 10.614882 hw_loss 0.306317 lr 0.00062955 rank 2
2023-02-17 16:42:28,540 DEBUG TRAIN Batch 7/4600 loss 27.316120 loss_att 34.881546 loss_ctc 49.541988 loss_rnnt 22.659008 hw_loss 0.338583 lr 0.00063025 rank 4
2023-02-17 16:42:28,540 DEBUG TRAIN Batch 7/4600 loss 14.464762 loss_att 18.668104 loss_ctc 30.735388 loss_rnnt 11.265097 hw_loss 0.355462 lr 0.00063005 rank 3
2023-02-17 16:42:28,541 DEBUG TRAIN Batch 7/4600 loss 22.375898 loss_att 23.409267 loss_ctc 40.471607 loss_rnnt 19.517128 hw_loss 0.448754 lr 0.00062997 rank 0
2023-02-17 16:43:45,787 DEBUG TRAIN Batch 7/4700 loss 17.366661 loss_att 23.055988 loss_ctc 30.612049 loss_rnnt 14.257850 hw_loss 0.384178 lr 0.00062942 rank 1
2023-02-17 16:43:45,789 DEBUG TRAIN Batch 7/4700 loss 19.367615 loss_att 23.343262 loss_ctc 33.357697 loss_rnnt 16.539825 hw_loss 0.313718 lr 0.00062957 rank 5
2023-02-17 16:43:45,793 DEBUG TRAIN Batch 7/4700 loss 16.777340 loss_att 21.573626 loss_ctc 26.178335 loss_rnnt 14.345637 hw_loss 0.410586 lr 0.00062967 rank 7
2023-02-17 16:43:45,794 DEBUG TRAIN Batch 7/4700 loss 19.461884 loss_att 26.114685 loss_ctc 31.285879 loss_rnnt 16.424072 hw_loss 0.245096 lr 0.00062955 rank 3
2023-02-17 16:43:45,795 DEBUG TRAIN Batch 7/4700 loss 13.716356 loss_att 16.562887 loss_ctc 25.377367 loss_rnnt 11.427507 hw_loss 0.308888 lr 0.00062975 rank 4
2023-02-17 16:43:45,796 DEBUG TRAIN Batch 7/4700 loss 15.250916 loss_att 18.374840 loss_ctc 24.617790 loss_rnnt 13.203423 hw_loss 0.325864 lr 0.00062980 rank 6
2023-02-17 16:43:45,797 DEBUG TRAIN Batch 7/4700 loss 11.027703 loss_att 13.987540 loss_ctc 19.851295 loss_rnnt 9.089808 hw_loss 0.317717 lr 0.00062947 rank 0
2023-02-17 16:43:45,801 DEBUG TRAIN Batch 7/4700 loss 23.732094 loss_att 30.794979 loss_ctc 41.401642 loss_rnnt 19.768204 hw_loss 0.366323 lr 0.00062905 rank 2
2023-02-17 16:45:00,390 DEBUG TRAIN Batch 7/4800 loss 16.395426 loss_att 21.758766 loss_ctc 30.776901 loss_rnnt 13.219002 hw_loss 0.349174 lr 0.00062905 rank 3
2023-02-17 16:45:00,393 DEBUG TRAIN Batch 7/4800 loss 12.912041 loss_att 16.514019 loss_ctc 23.372208 loss_rnnt 10.590183 hw_loss 0.387696 lr 0.00062892 rank 1
2023-02-17 16:45:00,393 DEBUG TRAIN Batch 7/4800 loss 14.337407 loss_att 19.287762 loss_ctc 23.738529 loss_rnnt 11.932756 hw_loss 0.302056 lr 0.00062925 rank 4
2023-02-17 16:45:00,396 DEBUG TRAIN Batch 7/4800 loss 12.838659 loss_att 16.430031 loss_ctc 19.856091 loss_rnnt 10.980448 hw_loss 0.383023 lr 0.00062855 rank 2
2023-02-17 16:45:00,397 DEBUG TRAIN Batch 7/4800 loss 9.232799 loss_att 15.725874 loss_ctc 20.925762 loss_rnnt 6.240026 hw_loss 0.253306 lr 0.00062930 rank 6
2023-02-17 16:45:00,397 DEBUG TRAIN Batch 7/4800 loss 15.939892 loss_att 17.948265 loss_ctc 28.105183 loss_rnnt 13.682788 hw_loss 0.437607 lr 0.00062897 rank 0
2023-02-17 16:45:00,398 DEBUG TRAIN Batch 7/4800 loss 6.321210 loss_att 11.747795 loss_ctc 11.844384 loss_rnnt 4.315063 hw_loss 0.345761 lr 0.00062907 rank 5
2023-02-17 16:45:00,401 DEBUG TRAIN Batch 7/4800 loss 22.291962 loss_att 23.772421 loss_ctc 29.777233 loss_rnnt 20.849827 hw_loss 0.277513 lr 0.00062917 rank 7
2023-02-17 16:46:16,449 DEBUG TRAIN Batch 7/4900 loss 26.665636 loss_att 33.196121 loss_ctc 49.005013 loss_rnnt 22.238344 hw_loss 0.267396 lr 0.00062867 rank 7
2023-02-17 16:46:16,453 DEBUG TRAIN Batch 7/4900 loss 16.019350 loss_att 18.678917 loss_ctc 24.302420 loss_rnnt 14.242755 hw_loss 0.263008 lr 0.00062880 rank 6
2023-02-17 16:46:16,453 DEBUG TRAIN Batch 7/4900 loss 20.441851 loss_att 27.602688 loss_ctc 37.148308 loss_rnnt 16.634550 hw_loss 0.276761 lr 0.00062842 rank 1
2023-02-17 16:46:16,454 DEBUG TRAIN Batch 7/4900 loss 29.980234 loss_att 35.204037 loss_ctc 50.007950 loss_rnnt 26.073631 hw_loss 0.359027 lr 0.00062875 rank 4
2023-02-17 16:46:16,454 DEBUG TRAIN Batch 7/4900 loss 16.314978 loss_att 21.633673 loss_ctc 29.587738 loss_rnnt 13.320095 hw_loss 0.302704 lr 0.00062855 rank 3
2023-02-17 16:46:16,456 DEBUG TRAIN Batch 7/4900 loss 19.395788 loss_att 21.880751 loss_ctc 26.320793 loss_rnnt 17.732992 hw_loss 0.454632 lr 0.00062857 rank 5
2023-02-17 16:46:16,463 DEBUG TRAIN Batch 7/4900 loss 14.963604 loss_att 19.159750 loss_ctc 28.136425 loss_rnnt 12.155769 hw_loss 0.397929 lr 0.00062805 rank 2
2023-02-17 16:46:16,500 DEBUG TRAIN Batch 7/4900 loss 18.429035 loss_att 24.402874 loss_ctc 34.481850 loss_rnnt 14.935730 hw_loss 0.296553 lr 0.00062848 rank 0
2023-02-17 16:47:36,140 DEBUG TRAIN Batch 7/5000 loss 15.690361 loss_att 18.815977 loss_ctc 27.290272 loss_rnnt 13.354876 hw_loss 0.306950 lr 0.00062807 rank 5
2023-02-17 16:47:36,142 DEBUG TRAIN Batch 7/5000 loss 20.331347 loss_att 22.348736 loss_ctc 28.503059 loss_rnnt 18.673737 hw_loss 0.308570 lr 0.00062826 rank 4
2023-02-17 16:47:36,145 DEBUG TRAIN Batch 7/5000 loss 11.733953 loss_att 12.778534 loss_ctc 17.111008 loss_rnnt 10.569036 hw_loss 0.448237 lr 0.00062831 rank 6
2023-02-17 16:47:36,148 DEBUG TRAIN Batch 7/5000 loss 12.260128 loss_att 14.781390 loss_ctc 19.338251 loss_rnnt 10.678189 hw_loss 0.251129 lr 0.00062805 rank 3
2023-02-17 16:47:36,149 DEBUG TRAIN Batch 7/5000 loss 5.221345 loss_att 8.799541 loss_ctc 9.082673 loss_rnnt 3.769972 hw_loss 0.414169 lr 0.00062817 rank 7
2023-02-17 16:47:36,149 DEBUG TRAIN Batch 7/5000 loss 21.333117 loss_att 27.895102 loss_ctc 35.175804 loss_rnnt 17.950668 hw_loss 0.420675 lr 0.00062793 rank 1
2023-02-17 16:47:36,152 DEBUG TRAIN Batch 7/5000 loss 15.200236 loss_att 19.221598 loss_ctc 30.426252 loss_rnnt 12.203443 hw_loss 0.304472 lr 0.00062756 rank 2
2023-02-17 16:47:36,153 DEBUG TRAIN Batch 7/5000 loss 35.208942 loss_att 34.844620 loss_ctc 47.000595 loss_rnnt 33.489754 hw_loss 0.412189 lr 0.00062798 rank 0
2023-02-17 16:48:54,254 DEBUG TRAIN Batch 7/5100 loss 18.318476 loss_att 18.308479 loss_ctc 22.582203 loss_rnnt 17.563202 hw_loss 0.353950 lr 0.00062743 rank 1
2023-02-17 16:48:54,257 DEBUG TRAIN Batch 7/5100 loss 11.474912 loss_att 12.667705 loss_ctc 20.743107 loss_rnnt 9.739326 hw_loss 0.489875 lr 0.00062758 rank 5
2023-02-17 16:48:54,257 DEBUG TRAIN Batch 7/5100 loss 14.846605 loss_att 18.100262 loss_ctc 23.400375 loss_rnnt 12.877299 hw_loss 0.333887 lr 0.00062781 rank 6
2023-02-17 16:48:54,258 DEBUG TRAIN Batch 7/5100 loss 9.835073 loss_att 11.101553 loss_ctc 16.746050 loss_rnnt 8.385530 hw_loss 0.515221 lr 0.00062776 rank 4
2023-02-17 16:48:54,261 DEBUG TRAIN Batch 7/5100 loss 13.007860 loss_att 16.811245 loss_ctc 21.501968 loss_rnnt 11.002246 hw_loss 0.210731 lr 0.00062749 rank 0
2023-02-17 16:48:54,261 DEBUG TRAIN Batch 7/5100 loss 20.791742 loss_att 20.733137 loss_ctc 30.543133 loss_rnnt 19.284191 hw_loss 0.410789 lr 0.00062756 rank 3
2023-02-17 16:48:54,262 DEBUG TRAIN Batch 7/5100 loss 10.838766 loss_att 13.411516 loss_ctc 21.018793 loss_rnnt 8.746292 hw_loss 0.413601 lr 0.00062768 rank 7
2023-02-17 16:48:54,265 DEBUG TRAIN Batch 7/5100 loss 7.500454 loss_att 9.427740 loss_ctc 12.100051 loss_rnnt 6.252867 hw_loss 0.466592 lr 0.00062707 rank 2
2023-02-17 16:50:10,618 DEBUG TRAIN Batch 7/5200 loss 18.929398 loss_att 23.748159 loss_ctc 31.246115 loss_rnnt 16.120705 hw_loss 0.380082 lr 0.00062727 rank 4
2023-02-17 16:50:10,619 DEBUG TRAIN Batch 7/5200 loss 21.162851 loss_att 23.352396 loss_ctc 36.762989 loss_rnnt 18.446972 hw_loss 0.371162 lr 0.00062718 rank 7
2023-02-17 16:50:10,622 DEBUG TRAIN Batch 7/5200 loss 4.233078 loss_att 7.817650 loss_ctc 6.637859 loss_rnnt 2.998999 hw_loss 0.368489 lr 0.00062709 rank 5
2023-02-17 16:50:10,625 DEBUG TRAIN Batch 7/5200 loss 25.791929 loss_att 29.366035 loss_ctc 37.335762 loss_rnnt 23.371283 hw_loss 0.312469 lr 0.00062694 rank 1
2023-02-17 16:50:10,625 DEBUG TRAIN Batch 7/5200 loss 15.900612 loss_att 21.426853 loss_ctc 24.508688 loss_rnnt 13.464954 hw_loss 0.342499 lr 0.00062732 rank 6
2023-02-17 16:50:10,628 DEBUG TRAIN Batch 7/5200 loss 15.054430 loss_att 18.038437 loss_ctc 18.561296 loss_rnnt 13.842773 hw_loss 0.276136 lr 0.00062657 rank 2
2023-02-17 16:50:10,629 DEBUG TRAIN Batch 7/5200 loss 30.203119 loss_att 35.830894 loss_ctc 50.390469 loss_rnnt 26.272846 hw_loss 0.212008 lr 0.00062699 rank 0
2023-02-17 16:50:10,673 DEBUG TRAIN Batch 7/5200 loss 25.820688 loss_att 25.325939 loss_ctc 35.414772 loss_rnnt 24.463257 hw_loss 0.332196 lr 0.00062707 rank 3
2023-02-17 16:51:29,986 DEBUG TRAIN Batch 7/5300 loss 7.614000 loss_att 13.448439 loss_ctc 18.938894 loss_rnnt 4.775583 hw_loss 0.302893 lr 0.00062669 rank 7
2023-02-17 16:51:29,987 DEBUG TRAIN Batch 7/5300 loss 17.308170 loss_att 22.479664 loss_ctc 33.376053 loss_rnnt 13.946716 hw_loss 0.346446 lr 0.00062650 rank 0
2023-02-17 16:51:29,991 DEBUG TRAIN Batch 7/5300 loss 13.885327 loss_att 18.943974 loss_ctc 21.036871 loss_rnnt 11.794020 hw_loss 0.236323 lr 0.00062659 rank 5
2023-02-17 16:51:29,991 DEBUG TRAIN Batch 7/5300 loss 6.968808 loss_att 10.104687 loss_ctc 12.586020 loss_rnnt 5.446223 hw_loss 0.274589 lr 0.00062678 rank 4
2023-02-17 16:51:29,994 DEBUG TRAIN Batch 7/5300 loss 21.536934 loss_att 24.435123 loss_ctc 29.833344 loss_rnnt 19.641415 hw_loss 0.393178 lr 0.00062645 rank 1
2023-02-17 16:51:29,997 DEBUG TRAIN Batch 7/5300 loss 13.507252 loss_att 15.869534 loss_ctc 20.118950 loss_rnnt 12.005344 hw_loss 0.277294 lr 0.00062682 rank 6
2023-02-17 16:51:30,032 DEBUG TRAIN Batch 7/5300 loss 20.531893 loss_att 26.542982 loss_ctc 39.684196 loss_rnnt 16.597778 hw_loss 0.334230 lr 0.00062657 rank 3
2023-02-17 16:51:30,038 DEBUG TRAIN Batch 7/5300 loss 18.636887 loss_att 21.306761 loss_ctc 30.495060 loss_rnnt 16.386787 hw_loss 0.253189 lr 0.00062608 rank 2
2023-02-17 16:52:48,593 DEBUG TRAIN Batch 7/5400 loss 16.556812 loss_att 22.513044 loss_ctc 31.223577 loss_rnnt 13.196220 hw_loss 0.400829 lr 0.00062628 rank 4
2023-02-17 16:52:48,593 DEBUG TRAIN Batch 7/5400 loss 14.417733 loss_att 19.340153 loss_ctc 23.349804 loss_rnnt 12.012422 hw_loss 0.431032 lr 0.00062633 rank 6
2023-02-17 16:52:48,598 DEBUG TRAIN Batch 7/5400 loss 10.834004 loss_att 13.609118 loss_ctc 16.074057 loss_rnnt 9.419749 hw_loss 0.301049 lr 0.00062610 rank 5
2023-02-17 16:52:48,598 DEBUG TRAIN Batch 7/5400 loss 15.799070 loss_att 20.639313 loss_ctc 29.550777 loss_rnnt 12.820968 hw_loss 0.330926 lr 0.00062620 rank 7
2023-02-17 16:52:48,599 DEBUG TRAIN Batch 7/5400 loss 14.012114 loss_att 18.487980 loss_ctc 24.321770 loss_rnnt 11.546903 hw_loss 0.366407 lr 0.00062559 rank 2
2023-02-17 16:52:48,603 DEBUG TRAIN Batch 7/5400 loss 15.881732 loss_att 18.467537 loss_ctc 25.218405 loss_rnnt 13.973198 hw_loss 0.274656 lr 0.00062595 rank 1
2023-02-17 16:52:48,602 DEBUG TRAIN Batch 7/5400 loss 12.316089 loss_att 19.771210 loss_ctc 28.951496 loss_rnnt 8.426125 hw_loss 0.339164 lr 0.00062608 rank 3
2023-02-17 16:52:48,611 DEBUG TRAIN Batch 7/5400 loss 10.176472 loss_att 13.673596 loss_ctc 17.543037 loss_rnnt 8.308910 hw_loss 0.348614 lr 0.00062601 rank 0
2023-02-17 16:54:05,126 DEBUG TRAIN Batch 7/5500 loss 16.000080 loss_att 22.568405 loss_ctc 25.113792 loss_rnnt 13.334602 hw_loss 0.256221 lr 0.00062561 rank 5
2023-02-17 16:54:05,133 DEBUG TRAIN Batch 7/5500 loss 13.667333 loss_att 18.089918 loss_ctc 24.801624 loss_rnnt 11.111973 hw_loss 0.349257 lr 0.00062546 rank 1
2023-02-17 16:54:05,133 DEBUG TRAIN Batch 7/5500 loss 24.393911 loss_att 28.816177 loss_ctc 37.382843 loss_rnnt 21.637522 hw_loss 0.262648 lr 0.00062571 rank 7
2023-02-17 16:54:05,133 DEBUG TRAIN Batch 7/5500 loss 18.729614 loss_att 22.534307 loss_ctc 28.543303 loss_rnnt 16.479280 hw_loss 0.339192 lr 0.00062584 rank 6
2023-02-17 16:54:05,134 DEBUG TRAIN Batch 7/5500 loss 18.546959 loss_att 20.664585 loss_ctc 28.864208 loss_rnnt 16.593063 hw_loss 0.290132 lr 0.00062559 rank 3
2023-02-17 16:54:05,145 DEBUG TRAIN Batch 7/5500 loss 17.209377 loss_att 23.829178 loss_ctc 33.023071 loss_rnnt 13.630604 hw_loss 0.274353 lr 0.00062510 rank 2
2023-02-17 16:54:05,153 DEBUG TRAIN Batch 7/5500 loss 25.405607 loss_att 29.852785 loss_ctc 37.679153 loss_rnnt 22.668030 hw_loss 0.396875 lr 0.00062579 rank 4
2023-02-17 16:54:05,167 DEBUG TRAIN Batch 7/5500 loss 17.677954 loss_att 19.011341 loss_ctc 29.024136 loss_rnnt 15.696892 hw_loss 0.377924 lr 0.00062552 rank 0
2023-02-17 16:55:21,099 DEBUG TRAIN Batch 7/5600 loss 11.741492 loss_att 11.842645 loss_ctc 16.998638 loss_rnnt 10.853868 hw_loss 0.312076 lr 0.00062535 rank 6
2023-02-17 16:55:21,103 DEBUG TRAIN Batch 7/5600 loss 23.019617 loss_att 24.834427 loss_ctc 31.021236 loss_rnnt 21.450321 hw_loss 0.261469 lr 0.00062512 rank 5
2023-02-17 16:55:21,105 DEBUG TRAIN Batch 7/5600 loss 13.402825 loss_att 17.200090 loss_ctc 23.848392 loss_rnnt 11.069040 hw_loss 0.340478 lr 0.00062510 rank 3
2023-02-17 16:55:21,105 DEBUG TRAIN Batch 7/5600 loss 19.176313 loss_att 24.386887 loss_ctc 31.505146 loss_rnnt 16.332352 hw_loss 0.296256 lr 0.00062522 rank 7
2023-02-17 16:55:21,106 DEBUG TRAIN Batch 7/5600 loss 10.039450 loss_att 12.044409 loss_ctc 15.328779 loss_rnnt 8.704881 hw_loss 0.428125 lr 0.00062498 rank 1
2023-02-17 16:55:21,109 DEBUG TRAIN Batch 7/5600 loss 10.115488 loss_att 15.170823 loss_ctc 21.046783 loss_rnnt 7.436539 hw_loss 0.394456 lr 0.00062530 rank 4
2023-02-17 16:55:21,111 DEBUG TRAIN Batch 7/5600 loss 18.237398 loss_att 21.042088 loss_ctc 29.954487 loss_rnnt 15.938420 hw_loss 0.329551 lr 0.00062503 rank 0
2023-02-17 16:55:21,158 DEBUG TRAIN Batch 7/5600 loss 14.775188 loss_att 17.556456 loss_ctc 26.312311 loss_rnnt 12.486229 hw_loss 0.364542 lr 0.00062461 rank 2
2023-02-17 16:56:41,940 DEBUG TRAIN Batch 7/5700 loss 15.055532 loss_att 19.036005 loss_ctc 28.381201 loss_rnnt 12.299973 hw_loss 0.342578 lr 0.00062461 rank 3
2023-02-17 16:56:41,946 DEBUG TRAIN Batch 7/5700 loss 19.774130 loss_att 23.550636 loss_ctc 34.675037 loss_rnnt 16.840805 hw_loss 0.358569 lr 0.00062413 rank 2
2023-02-17 16:56:41,945 DEBUG TRAIN Batch 7/5700 loss 8.945336 loss_att 13.181194 loss_ctc 17.564346 loss_rnnt 6.879016 hw_loss 0.131151 lr 0.00062454 rank 0
2023-02-17 16:56:41,946 DEBUG TRAIN Batch 7/5700 loss 14.146055 loss_att 16.637398 loss_ctc 23.713219 loss_rnnt 12.180473 hw_loss 0.359422 lr 0.00062463 rank 5
2023-02-17 16:56:41,946 DEBUG TRAIN Batch 7/5700 loss 17.029228 loss_att 22.771288 loss_ctc 26.902000 loss_rnnt 14.416824 hw_loss 0.276796 lr 0.00062486 rank 6
2023-02-17 16:56:41,947 DEBUG TRAIN Batch 7/5700 loss 15.753612 loss_att 19.955444 loss_ctc 24.158895 loss_rnnt 13.594722 hw_loss 0.370910 lr 0.00062473 rank 7
2023-02-17 16:56:41,949 DEBUG TRAIN Batch 7/5700 loss 23.720295 loss_att 24.463638 loss_ctc 30.861225 loss_rnnt 22.406931 hw_loss 0.398569 lr 0.00062481 rank 4
2023-02-17 16:56:41,949 DEBUG TRAIN Batch 7/5700 loss 34.492447 loss_att 35.489784 loss_ctc 48.507397 loss_rnnt 32.251221 hw_loss 0.324557 lr 0.00062449 rank 1
2023-02-17 16:57:57,877 DEBUG TRAIN Batch 7/5800 loss 12.194982 loss_att 21.523607 loss_ctc 23.005627 loss_rnnt 8.769217 hw_loss 0.222413 lr 0.00062413 rank 3
2023-02-17 16:57:57,878 DEBUG TRAIN Batch 7/5800 loss 18.927784 loss_att 20.963661 loss_ctc 29.748287 loss_rnnt 16.906265 hw_loss 0.321769 lr 0.00062438 rank 6
2023-02-17 16:57:57,879 DEBUG TRAIN Batch 7/5800 loss 10.605984 loss_att 17.634411 loss_ctc 20.854572 loss_rnnt 7.671552 hw_loss 0.304251 lr 0.00062433 rank 4
2023-02-17 16:57:57,880 DEBUG TRAIN Batch 7/5800 loss 8.505351 loss_att 15.257006 loss_ctc 11.988333 loss_rnnt 6.476331 hw_loss 0.401796 lr 0.00062415 rank 5
2023-02-17 16:57:57,882 DEBUG TRAIN Batch 7/5800 loss 26.363064 loss_att 33.889927 loss_ctc 42.516384 loss_rnnt 22.551970 hw_loss 0.284902 lr 0.00062364 rank 2
2023-02-17 16:57:57,883 DEBUG TRAIN Batch 7/5800 loss 14.986571 loss_att 19.623226 loss_ctc 29.663887 loss_rnnt 11.945949 hw_loss 0.293093 lr 0.00062400 rank 1
2023-02-17 16:57:57,883 DEBUG TRAIN Batch 7/5800 loss 7.278722 loss_att 12.410506 loss_ctc 15.060885 loss_rnnt 5.098868 hw_loss 0.217267 lr 0.00062424 rank 7
2023-02-17 16:57:57,886 DEBUG TRAIN Batch 7/5800 loss 19.919910 loss_att 20.811256 loss_ctc 30.227032 loss_rnnt 18.153965 hw_loss 0.400114 lr 0.00062405 rank 0
2023-02-17 16:59:13,851 DEBUG TRAIN Batch 7/5900 loss 31.196068 loss_att 37.494652 loss_ctc 44.804977 loss_rnnt 27.945805 hw_loss 0.330048 lr 0.00062389 rank 6
2023-02-17 16:59:13,852 DEBUG TRAIN Batch 7/5900 loss 22.465288 loss_att 31.042267 loss_ctc 40.482151 loss_rnnt 18.205860 hw_loss 0.265845 lr 0.00062366 rank 5
2023-02-17 16:59:13,859 DEBUG TRAIN Batch 7/5900 loss 17.962929 loss_att 22.949846 loss_ctc 33.205994 loss_rnnt 14.773672 hw_loss 0.298995 lr 0.00062384 rank 4
2023-02-17 16:59:13,860 DEBUG TRAIN Batch 7/5900 loss 18.488483 loss_att 23.308033 loss_ctc 42.798409 loss_rnnt 14.150156 hw_loss 0.249553 lr 0.00062357 rank 0
2023-02-17 16:59:13,861 DEBUG TRAIN Batch 7/5900 loss 10.044836 loss_att 14.596152 loss_ctc 20.740719 loss_rnnt 7.578416 hw_loss 0.243823 lr 0.00062376 rank 7
2023-02-17 16:59:13,864 DEBUG TRAIN Batch 7/5900 loss 22.302996 loss_att 25.721529 loss_ctc 44.049713 loss_rnnt 18.510056 hw_loss 0.393131 lr 0.00062352 rank 1
2023-02-17 16:59:13,866 DEBUG TRAIN Batch 7/5900 loss 18.832621 loss_att 25.012224 loss_ctc 38.014038 loss_rnnt 14.905993 hw_loss 0.249719 lr 0.00062316 rank 2
2023-02-17 16:59:13,867 DEBUG TRAIN Batch 7/5900 loss 21.560833 loss_att 25.671684 loss_ctc 42.674286 loss_rnnt 17.733265 hw_loss 0.356758 lr 0.00062364 rank 3
2023-02-17 17:00:31,217 DEBUG TRAIN Batch 7/6000 loss 13.476654 loss_att 19.321194 loss_ctc 18.006958 loss_rnnt 11.549768 hw_loss 0.288633 lr 0.00062327 rank 7
2023-02-17 17:00:31,221 DEBUG TRAIN Batch 7/6000 loss 25.554689 loss_att 32.329475 loss_ctc 33.359653 loss_rnnt 22.995773 hw_loss 0.306184 lr 0.00062316 rank 3
2023-02-17 17:00:31,221 DEBUG TRAIN Batch 7/6000 loss 16.798370 loss_att 22.464012 loss_ctc 29.561008 loss_rnnt 13.728294 hw_loss 0.441118 lr 0.00062318 rank 5
2023-02-17 17:00:31,224 DEBUG TRAIN Batch 7/6000 loss 10.585096 loss_att 13.014591 loss_ctc 15.143250 loss_rnnt 9.318996 hw_loss 0.323337 lr 0.00062267 rank 2
2023-02-17 17:00:31,224 DEBUG TRAIN Batch 7/6000 loss 30.394867 loss_att 34.783035 loss_ctc 46.011681 loss_rnnt 27.271324 hw_loss 0.306878 lr 0.00062340 rank 6
2023-02-17 17:00:31,225 DEBUG TRAIN Batch 7/6000 loss 14.312400 loss_att 19.431917 loss_ctc 24.133404 loss_rnnt 11.864066 hw_loss 0.215557 lr 0.00062303 rank 1
2023-02-17 17:00:31,230 DEBUG TRAIN Batch 7/6000 loss 13.251504 loss_att 18.483511 loss_ctc 19.539021 loss_rnnt 11.240230 hw_loss 0.237258 lr 0.00062336 rank 4
2023-02-17 17:00:31,230 DEBUG TRAIN Batch 7/6000 loss 18.816027 loss_att 20.555462 loss_ctc 28.018583 loss_rnnt 17.019283 hw_loss 0.415968 lr 0.00062309 rank 0
2023-02-17 17:01:48,785 DEBUG TRAIN Batch 7/6100 loss 23.820438 loss_att 29.815388 loss_ctc 41.871010 loss_rnnt 20.076530 hw_loss 0.259073 lr 0.00062267 rank 3
2023-02-17 17:01:48,789 DEBUG TRAIN Batch 7/6100 loss 23.471628 loss_att 26.103039 loss_ctc 30.912031 loss_rnnt 21.767735 hw_loss 0.347914 lr 0.00062292 rank 6
2023-02-17 17:01:48,789 DEBUG TRAIN Batch 7/6100 loss 21.613386 loss_att 25.289627 loss_ctc 36.336205 loss_rnnt 18.730400 hw_loss 0.346303 lr 0.00062219 rank 2
2023-02-17 17:01:48,791 DEBUG TRAIN Batch 7/6100 loss 15.396612 loss_att 21.064995 loss_ctc 22.477459 loss_rnnt 13.180383 hw_loss 0.259574 lr 0.00062287 rank 4
2023-02-17 17:01:48,791 DEBUG TRAIN Batch 7/6100 loss 8.498608 loss_att 12.119642 loss_ctc 17.506931 loss_rnnt 6.437212 hw_loss 0.255149 lr 0.00062255 rank 1
2023-02-17 17:01:48,794 DEBUG TRAIN Batch 7/6100 loss 19.852324 loss_att 22.598326 loss_ctc 31.320244 loss_rnnt 17.583385 hw_loss 0.357530 lr 0.00062269 rank 5
2023-02-17 17:01:48,794 DEBUG TRAIN Batch 7/6100 loss 6.880798 loss_att 10.627577 loss_ctc 10.078329 loss_rnnt 5.554056 hw_loss 0.283217 lr 0.00062279 rank 7
2023-02-17 17:01:48,798 DEBUG TRAIN Batch 7/6100 loss 21.200272 loss_att 27.468119 loss_ctc 40.518387 loss_rnnt 17.183140 hw_loss 0.352148 lr 0.00062260 rank 0
2023-02-17 17:03:04,758 DEBUG TRAIN Batch 7/6200 loss 21.570292 loss_att 24.467644 loss_ctc 40.416119 loss_rnnt 18.317520 hw_loss 0.300981 lr 0.00062207 rank 1
2023-02-17 17:03:04,761 DEBUG TRAIN Batch 7/6200 loss 17.921671 loss_att 19.840090 loss_ctc 30.729982 loss_rnnt 15.596586 hw_loss 0.438052 lr 0.00062244 rank 6
2023-02-17 17:03:04,763 DEBUG TRAIN Batch 7/6200 loss 17.794024 loss_att 20.993309 loss_ctc 28.841335 loss_rnnt 15.547722 hw_loss 0.250255 lr 0.00062212 rank 0
2023-02-17 17:03:04,764 DEBUG TRAIN Batch 7/6200 loss 26.441637 loss_att 33.064434 loss_ctc 45.921867 loss_rnnt 22.331614 hw_loss 0.352688 lr 0.00062221 rank 5
2023-02-17 17:03:04,765 DEBUG TRAIN Batch 7/6200 loss 26.426802 loss_att 28.471325 loss_ctc 46.988396 loss_rnnt 23.057617 hw_loss 0.410126 lr 0.00062231 rank 7
2023-02-17 17:03:04,765 DEBUG TRAIN Batch 7/6200 loss 13.582187 loss_att 16.978348 loss_ctc 19.490623 loss_rnnt 11.933285 hw_loss 0.341022 lr 0.00062219 rank 3
2023-02-17 17:03:04,767 DEBUG TRAIN Batch 7/6200 loss 12.605962 loss_att 17.090630 loss_ctc 30.659229 loss_rnnt 9.067743 hw_loss 0.439091 lr 0.00062239 rank 4
2023-02-17 17:03:04,768 DEBUG TRAIN Batch 7/6200 loss 16.269682 loss_att 17.870953 loss_ctc 23.638773 loss_rnnt 14.781294 hw_loss 0.347982 lr 0.00062171 rank 2
2023-02-17 17:04:20,208 DEBUG TRAIN Batch 7/6300 loss 23.773520 loss_att 31.554241 loss_ctc 35.424576 loss_rnnt 20.530445 hw_loss 0.250232 lr 0.00062196 rank 6
2023-02-17 17:04:20,209 DEBUG TRAIN Batch 7/6300 loss 14.692885 loss_att 19.497311 loss_ctc 25.211414 loss_rnnt 12.159751 hw_loss 0.318334 lr 0.00062183 rank 7
2023-02-17 17:04:20,211 DEBUG TRAIN Batch 7/6300 loss 9.791745 loss_att 15.966784 loss_ctc 18.297184 loss_rnnt 7.238578 hw_loss 0.345190 lr 0.00062173 rank 5
2023-02-17 17:04:20,214 DEBUG TRAIN Batch 7/6300 loss 10.757946 loss_att 14.354533 loss_ctc 17.005077 loss_rnnt 8.984635 hw_loss 0.414452 lr 0.00062191 rank 4
2023-02-17 17:04:20,215 DEBUG TRAIN Batch 7/6300 loss 18.137308 loss_att 19.276718 loss_ctc 26.264950 loss_rnnt 16.597149 hw_loss 0.428611 lr 0.00062164 rank 0
2023-02-17 17:04:20,219 DEBUG TRAIN Batch 7/6300 loss 18.469002 loss_att 20.844809 loss_ctc 31.945107 loss_rnnt 15.976917 hw_loss 0.412701 lr 0.00062159 rank 1
2023-02-17 17:04:20,223 DEBUG TRAIN Batch 7/6300 loss 22.520145 loss_att 22.337402 loss_ctc 29.761799 loss_rnnt 21.411541 hw_loss 0.336753 lr 0.00062123 rank 2
2023-02-17 17:04:20,223 DEBUG TRAIN Batch 7/6300 loss 14.995813 loss_att 19.397095 loss_ctc 27.981544 loss_rnnt 12.250594 hw_loss 0.250373 lr 0.00062171 rank 3
2023-02-17 17:05:40,127 DEBUG TRAIN Batch 7/6400 loss 9.768955 loss_att 13.695247 loss_ctc 12.856646 loss_rnnt 8.421296 hw_loss 0.282577 lr 0.00062148 rank 6
2023-02-17 17:05:40,130 DEBUG TRAIN Batch 7/6400 loss 13.623547 loss_att 14.337655 loss_ctc 18.861244 loss_rnnt 12.582576 hw_loss 0.374605 lr 0.00062125 rank 5
2023-02-17 17:05:40,132 DEBUG TRAIN Batch 7/6400 loss 14.291846 loss_att 15.239200 loss_ctc 22.058592 loss_rnnt 12.879040 hw_loss 0.352067 lr 0.00062111 rank 1
2023-02-17 17:05:40,132 DEBUG TRAIN Batch 7/6400 loss 17.690784 loss_att 23.505568 loss_ctc 28.790325 loss_rnnt 14.949131 hw_loss 0.185168 lr 0.00062075 rank 2
2023-02-17 17:05:40,132 DEBUG TRAIN Batch 7/6400 loss 4.790897 loss_att 10.190467 loss_ctc 10.464375 loss_rnnt 2.801761 hw_loss 0.286422 lr 0.00062143 rank 4
2023-02-17 17:05:40,133 DEBUG TRAIN Batch 7/6400 loss 11.302363 loss_att 13.851118 loss_ctc 22.046753 loss_rnnt 9.193617 hw_loss 0.312021 lr 0.00062116 rank 0
2023-02-17 17:05:40,134 DEBUG TRAIN Batch 7/6400 loss 22.110136 loss_att 21.771515 loss_ctc 29.578278 loss_rnnt 20.984287 hw_loss 0.370912 lr 0.00062135 rank 7
2023-02-17 17:05:40,138 DEBUG TRAIN Batch 7/6400 loss 11.029177 loss_att 12.975534 loss_ctc 16.264242 loss_rnnt 9.771794 hw_loss 0.318942 lr 0.00062123 rank 3
2023-02-17 17:06:55,920 DEBUG TRAIN Batch 7/6500 loss 10.369427 loss_att 15.278560 loss_ctc 22.698376 loss_rnnt 7.551873 hw_loss 0.359750 lr 0.00062100 rank 6
2023-02-17 17:06:55,923 DEBUG TRAIN Batch 7/6500 loss 14.442734 loss_att 17.574707 loss_ctc 18.700148 loss_rnnt 13.106964 hw_loss 0.265723 lr 0.00062027 rank 2
2023-02-17 17:06:55,925 DEBUG TRAIN Batch 7/6500 loss 9.083354 loss_att 12.007212 loss_ctc 16.717560 loss_rnnt 7.318213 hw_loss 0.304641 lr 0.00062087 rank 7
2023-02-17 17:06:55,927 DEBUG TRAIN Batch 7/6500 loss 8.783098 loss_att 15.480873 loss_ctc 18.987911 loss_rnnt 5.938216 hw_loss 0.271285 lr 0.00062095 rank 4
2023-02-17 17:06:55,927 DEBUG TRAIN Batch 7/6500 loss 13.324246 loss_att 16.156357 loss_ctc 17.815516 loss_rnnt 12.023008 hw_loss 0.254965 lr 0.00062068 rank 0
2023-02-17 17:06:55,929 DEBUG TRAIN Batch 7/6500 loss 18.927910 loss_att 20.662285 loss_ctc 27.224052 loss_rnnt 17.281553 hw_loss 0.362491 lr 0.00062077 rank 5
2023-02-17 17:06:55,929 DEBUG TRAIN Batch 7/6500 loss 32.453930 loss_att 33.698601 loss_ctc 53.500793 loss_rnnt 29.231939 hw_loss 0.312771 lr 0.00062075 rank 3
2023-02-17 17:06:55,931 DEBUG TRAIN Batch 7/6500 loss 24.618587 loss_att 28.208206 loss_ctc 35.213825 loss_rnnt 22.287647 hw_loss 0.375592 lr 0.00062063 rank 1
2023-02-17 17:08:11,744 DEBUG TRAIN Batch 7/6600 loss 26.396351 loss_att 31.077171 loss_ctc 46.081108 loss_rnnt 22.674915 hw_loss 0.301196 lr 0.00062027 rank 3
2023-02-17 17:08:11,747 DEBUG TRAIN Batch 7/6600 loss 21.403564 loss_att 23.487144 loss_ctc 30.306519 loss_rnnt 19.663446 hw_loss 0.255641 lr 0.00062052 rank 6
2023-02-17 17:08:11,750 DEBUG TRAIN Batch 7/6600 loss 10.674070 loss_att 15.185087 loss_ctc 17.701340 loss_rnnt 8.725206 hw_loss 0.205670 lr 0.00062015 rank 1
2023-02-17 17:08:11,752 DEBUG TRAIN Batch 7/6600 loss 13.184720 loss_att 17.687851 loss_ctc 23.684269 loss_rnnt 10.687403 hw_loss 0.368906 lr 0.00062020 rank 0
2023-02-17 17:08:11,752 DEBUG TRAIN Batch 7/6600 loss 27.380203 loss_att 29.735180 loss_ctc 43.080151 loss_rnnt 24.646465 hw_loss 0.317656 lr 0.00061980 rank 2
2023-02-17 17:08:11,753 DEBUG TRAIN Batch 7/6600 loss 15.070694 loss_att 18.077297 loss_ctc 28.285976 loss_rnnt 12.522139 hw_loss 0.347244 lr 0.00062029 rank 5
2023-02-17 17:08:11,755 DEBUG TRAIN Batch 7/6600 loss 24.613232 loss_att 33.289230 loss_ctc 44.396683 loss_rnnt 20.050535 hw_loss 0.355690 lr 0.00062039 rank 7
2023-02-17 17:08:11,756 DEBUG TRAIN Batch 7/6600 loss 28.532568 loss_att 32.631577 loss_ctc 45.534023 loss_rnnt 25.346352 hw_loss 0.186663 lr 0.00062047 rank 4
2023-02-17 17:09:30,808 DEBUG TRAIN Batch 7/6700 loss 9.887379 loss_att 11.577159 loss_ctc 19.122711 loss_rnnt 8.136694 hw_loss 0.340032 lr 0.00061991 rank 7
2023-02-17 17:09:30,810 DEBUG TRAIN Batch 7/6700 loss 11.788803 loss_att 14.709688 loss_ctc 22.494194 loss_rnnt 9.612583 hw_loss 0.308734 lr 0.00061982 rank 5
2023-02-17 17:09:30,811 DEBUG TRAIN Batch 7/6700 loss 19.631157 loss_att 24.591406 loss_ctc 37.784431 loss_rnnt 16.083632 hw_loss 0.253202 lr 0.00061980 rank 3
2023-02-17 17:09:30,813 DEBUG TRAIN Batch 7/6700 loss 9.796919 loss_att 13.445036 loss_ctc 18.205784 loss_rnnt 7.703320 hw_loss 0.455237 lr 0.00062004 rank 6
2023-02-17 17:09:30,813 DEBUG TRAIN Batch 7/6700 loss 14.017488 loss_att 18.567816 loss_ctc 21.626225 loss_rnnt 11.919132 hw_loss 0.325859 lr 0.00061967 rank 1
2023-02-17 17:09:30,814 DEBUG TRAIN Batch 7/6700 loss 22.169310 loss_att 27.651163 loss_ctc 40.809471 loss_rnnt 18.373966 hw_loss 0.400532 lr 0.00061932 rank 2
2023-02-17 17:09:30,816 DEBUG TRAIN Batch 7/6700 loss 12.895479 loss_att 18.074570 loss_ctc 21.708153 loss_rnnt 10.538015 hw_loss 0.274917 lr 0.00061999 rank 4
2023-02-17 17:09:30,859 DEBUG TRAIN Batch 7/6700 loss 10.140125 loss_att 15.402074 loss_ctc 19.258926 loss_rnnt 7.727458 hw_loss 0.270820 lr 0.00061973 rank 0
2023-02-17 17:10:49,672 DEBUG TRAIN Batch 7/6800 loss 19.966146 loss_att 20.879436 loss_ctc 28.387989 loss_rnnt 18.485638 hw_loss 0.328008 lr 0.00061944 rank 7
2023-02-17 17:10:49,673 DEBUG TRAIN Batch 7/6800 loss 13.500357 loss_att 17.476725 loss_ctc 23.964081 loss_rnnt 11.112621 hw_loss 0.369934 lr 0.00061885 rank 2
2023-02-17 17:10:49,675 DEBUG TRAIN Batch 7/6800 loss 15.162159 loss_att 16.047455 loss_ctc 23.116226 loss_rnnt 13.791453 hw_loss 0.249572 lr 0.00061920 rank 1
2023-02-17 17:10:49,677 DEBUG TRAIN Batch 7/6800 loss 31.967474 loss_att 36.551765 loss_ctc 48.282669 loss_rnnt 28.654770 hw_loss 0.413412 lr 0.00061932 rank 3
2023-02-17 17:10:49,676 DEBUG TRAIN Batch 7/6800 loss 23.046890 loss_att 24.419106 loss_ctc 30.239090 loss_rnnt 21.638494 hw_loss 0.328113 lr 0.00061956 rank 6
2023-02-17 17:10:49,677 DEBUG TRAIN Batch 7/6800 loss 26.233799 loss_att 28.968044 loss_ctc 42.008659 loss_rnnt 23.404175 hw_loss 0.336483 lr 0.00061934 rank 5
2023-02-17 17:10:49,680 DEBUG TRAIN Batch 7/6800 loss 6.856361 loss_att 9.737223 loss_ctc 14.627581 loss_rnnt 5.072226 hw_loss 0.322124 lr 0.00061925 rank 0
2023-02-17 17:10:49,731 DEBUG TRAIN Batch 7/6800 loss 19.105284 loss_att 21.790672 loss_ctc 27.311527 loss_rnnt 17.294336 hw_loss 0.336942 lr 0.00061952 rank 4
2023-02-17 17:12:05,743 DEBUG TRAIN Batch 7/6900 loss 14.483573 loss_att 20.677654 loss_ctc 24.339157 loss_rnnt 11.772297 hw_loss 0.296965 lr 0.00061909 rank 6
2023-02-17 17:12:05,744 DEBUG TRAIN Batch 7/6900 loss 5.731836 loss_att 11.215305 loss_ctc 12.046320 loss_rnnt 3.623038 hw_loss 0.319073 lr 0.00061887 rank 5
2023-02-17 17:12:05,746 DEBUG TRAIN Batch 7/6900 loss 15.172987 loss_att 18.419485 loss_ctc 22.913696 loss_rnnt 13.310760 hw_loss 0.339059 lr 0.00061896 rank 7
2023-02-17 17:12:05,746 DEBUG TRAIN Batch 7/6900 loss 11.082282 loss_att 13.371487 loss_ctc 19.861107 loss_rnnt 9.235476 hw_loss 0.409604 lr 0.00061878 rank 0
2023-02-17 17:12:05,747 DEBUG TRAIN Batch 7/6900 loss 20.772217 loss_att 22.821739 loss_ctc 34.635906 loss_rnnt 18.377991 hw_loss 0.254679 lr 0.00061904 rank 4
2023-02-17 17:12:05,748 DEBUG TRAIN Batch 7/6900 loss 13.588208 loss_att 16.079096 loss_ctc 19.412403 loss_rnnt 12.105500 hw_loss 0.389946 lr 0.00061885 rank 3
2023-02-17 17:12:05,748 DEBUG TRAIN Batch 7/6900 loss 13.987005 loss_att 16.363461 loss_ctc 23.417524 loss_rnnt 12.079324 hw_loss 0.328101 lr 0.00061837 rank 2
2023-02-17 17:12:05,749 DEBUG TRAIN Batch 7/6900 loss 7.132300 loss_att 11.877489 loss_ctc 13.201654 loss_rnnt 5.144352 hw_loss 0.430617 lr 0.00061872 rank 1
2023-02-17 17:13:22,620 DEBUG TRAIN Batch 7/7000 loss 18.320587 loss_att 21.562168 loss_ctc 34.226192 loss_rnnt 15.375124 hw_loss 0.330748 lr 0.00061825 rank 1
2023-02-17 17:13:22,621 DEBUG TRAIN Batch 7/7000 loss 12.003274 loss_att 13.810776 loss_ctc 18.285227 loss_rnnt 10.638981 hw_loss 0.309748 lr 0.00061849 rank 7
2023-02-17 17:13:22,624 DEBUG TRAIN Batch 7/7000 loss 11.641960 loss_att 15.068479 loss_ctc 19.044149 loss_rnnt 9.802675 hw_loss 0.313167 lr 0.00061839 rank 5
2023-02-17 17:13:22,624 DEBUG TRAIN Batch 7/7000 loss 19.530706 loss_att 22.681868 loss_ctc 34.756912 loss_rnnt 16.679226 hw_loss 0.358287 lr 0.00061837 rank 3
2023-02-17 17:13:22,626 DEBUG TRAIN Batch 7/7000 loss 8.182955 loss_att 11.276516 loss_ctc 18.532515 loss_rnnt 5.966291 hw_loss 0.408769 lr 0.00061857 rank 4
2023-02-17 17:13:22,627 DEBUG TRAIN Batch 7/7000 loss 11.820196 loss_att 18.720335 loss_ctc 22.539593 loss_rnnt 8.847129 hw_loss 0.307100 lr 0.00061861 rank 6
2023-02-17 17:13:22,635 DEBUG TRAIN Batch 7/7000 loss 15.247488 loss_att 22.609928 loss_ctc 28.860672 loss_rnnt 11.763476 hw_loss 0.368311 lr 0.00061830 rank 0
2023-02-17 17:13:22,670 DEBUG TRAIN Batch 7/7000 loss 19.529100 loss_att 24.697853 loss_ctc 26.932215 loss_rnnt 17.340460 hw_loss 0.314641 lr 0.00061790 rank 2
2023-02-17 17:14:43,402 DEBUG TRAIN Batch 7/7100 loss 14.651372 loss_att 17.894476 loss_ctc 25.495239 loss_rnnt 12.380648 hw_loss 0.330473 lr 0.00061801 rank 7
2023-02-17 17:14:43,408 DEBUG TRAIN Batch 7/7100 loss 6.167668 loss_att 10.560243 loss_ctc 11.595879 loss_rnnt 4.342528 hw_loss 0.417869 lr 0.00061783 rank 0
2023-02-17 17:14:43,408 DEBUG TRAIN Batch 7/7100 loss 12.769277 loss_att 16.207981 loss_ctc 22.767843 loss_rnnt 10.606292 hw_loss 0.266442 lr 0.00061792 rank 5
2023-02-17 17:14:43,410 DEBUG TRAIN Batch 7/7100 loss 12.918699 loss_att 19.655901 loss_ctc 21.422691 loss_rnnt 10.197131 hw_loss 0.450492 lr 0.00061809 rank 4
2023-02-17 17:14:43,411 DEBUG TRAIN Batch 7/7100 loss 10.832549 loss_att 10.852483 loss_ctc 16.166916 loss_rnnt 9.925136 hw_loss 0.360333 lr 0.00061790 rank 3
2023-02-17 17:14:43,412 DEBUG TRAIN Batch 7/7100 loss 16.629801 loss_att 22.206711 loss_ctc 27.019100 loss_rnnt 14.000354 hw_loss 0.241545 lr 0.00061743 rank 2
2023-02-17 17:14:43,412 DEBUG TRAIN Batch 7/7100 loss 12.373949 loss_att 13.178010 loss_ctc 23.935085 loss_rnnt 10.416587 hw_loss 0.478247 lr 0.00061814 rank 6
2023-02-17 17:14:43,423 DEBUG TRAIN Batch 7/7100 loss 12.711957 loss_att 13.304905 loss_ctc 17.723242 loss_rnnt 11.720163 hw_loss 0.384433 lr 0.00061778 rank 1
2023-02-17 17:16:01,040 DEBUG TRAIN Batch 7/7200 loss 18.695271 loss_att 21.203350 loss_ctc 30.696615 loss_rnnt 16.394474 hw_loss 0.373129 lr 0.00061767 rank 6
2023-02-17 17:16:01,041 DEBUG TRAIN Batch 7/7200 loss 15.702500 loss_att 23.564255 loss_ctc 26.187773 loss_rnnt 12.607272 hw_loss 0.234076 lr 0.00061754 rank 7
2023-02-17 17:16:01,042 DEBUG TRAIN Batch 7/7200 loss 20.225273 loss_att 26.431465 loss_ctc 32.735611 loss_rnnt 17.149260 hw_loss 0.312621 lr 0.00061745 rank 5
2023-02-17 17:16:01,043 DEBUG TRAIN Batch 7/7200 loss 17.475443 loss_att 18.540863 loss_ctc 26.351286 loss_rnnt 15.902464 hw_loss 0.330844 lr 0.00061731 rank 1
2023-02-17 17:16:01,044 DEBUG TRAIN Batch 7/7200 loss 11.248493 loss_att 15.945448 loss_ctc 21.932491 loss_rnnt 8.660369 hw_loss 0.420376 lr 0.00061696 rank 2
2023-02-17 17:16:01,049 DEBUG TRAIN Batch 7/7200 loss 15.156230 loss_att 21.125425 loss_ctc 22.338913 loss_rnnt 12.841993 hw_loss 0.305075 lr 0.00061762 rank 4
2023-02-17 17:16:01,049 DEBUG TRAIN Batch 7/7200 loss 7.858691 loss_att 10.706987 loss_ctc 14.820265 loss_rnnt 6.175296 hw_loss 0.347862 lr 0.00061736 rank 0
2023-02-17 17:16:01,093 DEBUG TRAIN Batch 7/7200 loss 14.685294 loss_att 18.656418 loss_ctc 33.832779 loss_rnnt 11.182810 hw_loss 0.291114 lr 0.00061743 rank 3
2023-02-17 17:17:17,723 DEBUG TRAIN Batch 7/7300 loss 12.227258 loss_att 12.871408 loss_ctc 17.025206 loss_rnnt 11.249087 hw_loss 0.393028 lr 0.00061715 rank 4
2023-02-17 17:17:17,723 DEBUG TRAIN Batch 7/7300 loss 9.092352 loss_att 13.227721 loss_ctc 15.540759 loss_rnnt 7.247365 hw_loss 0.296483 lr 0.00061720 rank 6
2023-02-17 17:17:17,725 DEBUG TRAIN Batch 7/7300 loss 26.345144 loss_att 31.128269 loss_ctc 49.691525 loss_rnnt 22.135845 hw_loss 0.262171 lr 0.00061707 rank 7
2023-02-17 17:17:17,727 DEBUG TRAIN Batch 7/7300 loss 12.729059 loss_att 17.910288 loss_ctc 14.925520 loss_rnnt 11.198038 hw_loss 0.378587 lr 0.00061684 rank 1
2023-02-17 17:17:17,729 DEBUG TRAIN Batch 7/7300 loss 13.034094 loss_att 15.731600 loss_ctc 25.185797 loss_rnnt 10.728671 hw_loss 0.273179 lr 0.00061696 rank 3
2023-02-17 17:17:17,731 DEBUG TRAIN Batch 7/7300 loss 8.835648 loss_att 13.997087 loss_ctc 19.866032 loss_rnnt 6.102669 hw_loss 0.431199 lr 0.00061698 rank 5
2023-02-17 17:17:17,732 DEBUG TRAIN Batch 7/7300 loss 28.009644 loss_att 31.633186 loss_ctc 40.737484 loss_rnnt 25.458179 hw_loss 0.243204 lr 0.00061649 rank 2
2023-02-17 17:17:17,736 DEBUG TRAIN Batch 7/7300 loss 32.472141 loss_att 37.237663 loss_ctc 46.366543 loss_rnnt 29.458237 hw_loss 0.390408 lr 0.00061689 rank 0
2023-02-17 17:18:34,583 DEBUG TRAIN Batch 7/7400 loss 19.784840 loss_att 21.922571 loss_ctc 27.365747 loss_rnnt 18.232723 hw_loss 0.213340 lr 0.00061651 rank 5
2023-02-17 17:18:34,584 DEBUG TRAIN Batch 7/7400 loss 12.022329 loss_att 15.008059 loss_ctc 16.330456 loss_rnnt 10.659220 hw_loss 0.359150 lr 0.00061649 rank 3
2023-02-17 17:18:34,589 DEBUG TRAIN Batch 7/7400 loss 19.994934 loss_att 24.998421 loss_ctc 36.582661 loss_rnnt 16.584791 hw_loss 0.370780 lr 0.00061660 rank 7
2023-02-17 17:18:34,590 DEBUG TRAIN Batch 7/7400 loss 31.159361 loss_att 34.451481 loss_ctc 46.129589 loss_rnnt 28.337002 hw_loss 0.314821 lr 0.00061637 rank 1
2023-02-17 17:18:34,590 DEBUG TRAIN Batch 7/7400 loss 25.608244 loss_att 26.665434 loss_ctc 44.418049 loss_rnnt 22.753326 hw_loss 0.254072 lr 0.00061673 rank 6
2023-02-17 17:18:34,590 DEBUG TRAIN Batch 7/7400 loss 28.361870 loss_att 30.122946 loss_ctc 42.937668 loss_rnnt 25.893272 hw_loss 0.324265 lr 0.00061668 rank 4
2023-02-17 17:18:34,591 DEBUG TRAIN Batch 7/7400 loss 20.333010 loss_att 23.813375 loss_ctc 31.797676 loss_rnnt 17.919222 hw_loss 0.354550 lr 0.00061602 rank 2
2023-02-17 17:18:34,593 DEBUG TRAIN Batch 7/7400 loss 25.086208 loss_att 26.978127 loss_ctc 40.991882 loss_rnnt 22.411264 hw_loss 0.329633 lr 0.00061642 rank 0
2023-02-17 17:19:54,221 DEBUG TRAIN Batch 7/7500 loss 20.263336 loss_att 21.092533 loss_ctc 27.578709 loss_rnnt 18.940281 hw_loss 0.340938 lr 0.00061604 rank 5
2023-02-17 17:19:54,221 DEBUG TRAIN Batch 7/7500 loss 9.006340 loss_att 12.390860 loss_ctc 14.365778 loss_rnnt 7.451277 hw_loss 0.306690 lr 0.00061613 rank 7
2023-02-17 17:19:54,222 DEBUG TRAIN Batch 7/7500 loss 9.629684 loss_att 11.619177 loss_ctc 15.165987 loss_rnnt 8.264483 hw_loss 0.429617 lr 0.00061626 rank 6
2023-02-17 17:19:54,224 DEBUG TRAIN Batch 7/7500 loss 13.060161 loss_att 15.878050 loss_ctc 22.622311 loss_rnnt 11.017353 hw_loss 0.383017 lr 0.00061621 rank 4
2023-02-17 17:19:54,228 DEBUG TRAIN Batch 7/7500 loss 9.249780 loss_att 10.687607 loss_ctc 16.872341 loss_rnnt 7.757108 hw_loss 0.353935 lr 0.00061602 rank 3
2023-02-17 17:19:54,228 DEBUG TRAIN Batch 7/7500 loss 26.998920 loss_att 32.147789 loss_ctc 46.118206 loss_rnnt 23.246223 hw_loss 0.325659 lr 0.00061556 rank 2
2023-02-17 17:19:54,229 DEBUG TRAIN Batch 7/7500 loss 22.929012 loss_att 26.559082 loss_ctc 36.674316 loss_rnnt 20.217281 hw_loss 0.286893 lr 0.00061590 rank 1
2023-02-17 17:19:54,230 DEBUG TRAIN Batch 7/7500 loss 12.621146 loss_att 14.432491 loss_ctc 21.021988 loss_rnnt 10.920435 hw_loss 0.409367 lr 0.00061595 rank 0
2023-02-17 17:21:11,178 DEBUG TRAIN Batch 7/7600 loss 12.656356 loss_att 15.529272 loss_ctc 18.298315 loss_rnnt 11.119151 hw_loss 0.394428 lr 0.00061579 rank 6
2023-02-17 17:21:11,184 DEBUG TRAIN Batch 7/7600 loss 14.747303 loss_att 18.918392 loss_ctc 23.984390 loss_rnnt 12.484781 hw_loss 0.368795 lr 0.00061543 rank 1
2023-02-17 17:21:11,188 DEBUG TRAIN Batch 7/7600 loss 20.267002 loss_att 25.178080 loss_ctc 36.398109 loss_rnnt 16.933638 hw_loss 0.375626 lr 0.00061567 rank 7
2023-02-17 17:21:11,191 DEBUG TRAIN Batch 7/7600 loss 8.228982 loss_att 12.063954 loss_ctc 13.431285 loss_rnnt 6.530942 hw_loss 0.445135 lr 0.00061557 rank 5
2023-02-17 17:21:11,192 DEBUG TRAIN Batch 7/7600 loss 16.583742 loss_att 18.867048 loss_ctc 27.519882 loss_rnnt 14.515479 hw_loss 0.287719 lr 0.00061556 rank 3
2023-02-17 17:21:11,193 DEBUG TRAIN Batch 7/7600 loss 12.577403 loss_att 15.132808 loss_ctc 19.243179 loss_rnnt 10.987002 hw_loss 0.357280 lr 0.00061575 rank 4
2023-02-17 17:21:11,193 DEBUG TRAIN Batch 7/7600 loss 23.646589 loss_att 25.898327 loss_ctc 36.362392 loss_rnnt 21.303204 hw_loss 0.370499 lr 0.00061509 rank 2
2023-02-17 17:21:11,197 DEBUG TRAIN Batch 7/7600 loss 9.298632 loss_att 10.712420 loss_ctc 16.014149 loss_rnnt 7.875656 hw_loss 0.459030 lr 0.00061549 rank 0
2023-02-17 17:22:28,959 DEBUG TRAIN Batch 7/7700 loss 19.338776 loss_att 20.987053 loss_ctc 32.308426 loss_rnnt 17.055403 hw_loss 0.420807 lr 0.00061509 rank 3
2023-02-17 17:22:28,962 DEBUG TRAIN Batch 7/7700 loss 12.867758 loss_att 13.342900 loss_ctc 17.313175 loss_rnnt 11.988901 hw_loss 0.358321 lr 0.00061511 rank 5
2023-02-17 17:22:28,965 DEBUG TRAIN Batch 7/7700 loss 17.754768 loss_att 22.011322 loss_ctc 25.248554 loss_rnnt 15.677635 hw_loss 0.424973 lr 0.00061533 rank 6
2023-02-17 17:22:28,966 DEBUG TRAIN Batch 7/7700 loss 16.121140 loss_att 19.063528 loss_ctc 20.671288 loss_rnnt 14.782593 hw_loss 0.268841 lr 0.00061502 rank 0
2023-02-17 17:22:28,967 DEBUG TRAIN Batch 7/7700 loss 12.068572 loss_att 12.931110 loss_ctc 21.982880 loss_rnnt 10.429922 hw_loss 0.270439 lr 0.00061462 rank 2
2023-02-17 17:22:28,968 DEBUG TRAIN Batch 7/7700 loss 11.758065 loss_att 17.025242 loss_ctc 21.936405 loss_rnnt 9.138234 hw_loss 0.392406 lr 0.00061528 rank 4
2023-02-17 17:22:28,971 DEBUG TRAIN Batch 7/7700 loss 17.932045 loss_att 18.946346 loss_ctc 25.798830 loss_rnnt 16.475580 hw_loss 0.383810 lr 0.00061497 rank 1
2023-02-17 17:22:28,974 DEBUG TRAIN Batch 7/7700 loss 10.155016 loss_att 15.749319 loss_ctc 13.032725 loss_rnnt 8.494140 hw_loss 0.296852 lr 0.00061520 rank 7
2023-02-17 17:23:47,952 DEBUG TRAIN Batch 7/7800 loss 18.766626 loss_att 23.738659 loss_ctc 27.784033 loss_rnnt 16.400686 hw_loss 0.317271 lr 0.00061464 rank 5
2023-02-17 17:23:47,957 DEBUG TRAIN Batch 7/7800 loss 13.415505 loss_att 18.556501 loss_ctc 20.902325 loss_rnnt 11.234543 hw_loss 0.289725 lr 0.00061474 rank 7
2023-02-17 17:23:47,958 DEBUG TRAIN Batch 7/7800 loss 13.573376 loss_att 18.230690 loss_ctc 20.746925 loss_rnnt 11.502716 hw_loss 0.342605 lr 0.00061486 rank 6
2023-02-17 17:23:47,960 DEBUG TRAIN Batch 7/7800 loss 15.927187 loss_att 23.952751 loss_ctc 28.623203 loss_rnnt 12.461562 hw_loss 0.314454 lr 0.00061455 rank 0
2023-02-17 17:23:47,961 DEBUG TRAIN Batch 7/7800 loss 8.532043 loss_att 12.638712 loss_ctc 14.247287 loss_rnnt 6.787834 hw_loss 0.301579 lr 0.00061462 rank 3
2023-02-17 17:23:47,963 DEBUG TRAIN Batch 7/7800 loss 24.892567 loss_att 29.805573 loss_ctc 29.875519 loss_rnnt 23.107738 hw_loss 0.258437 lr 0.00061416 rank 2
2023-02-17 17:23:47,964 DEBUG TRAIN Batch 7/7800 loss 18.543316 loss_att 23.455803 loss_ctc 38.023769 loss_rnnt 14.822158 hw_loss 0.264874 lr 0.00061482 rank 4
2023-02-17 17:23:47,967 DEBUG TRAIN Batch 7/7800 loss 24.678055 loss_att 31.275991 loss_ctc 40.595493 loss_rnnt 21.042347 hw_loss 0.363367 lr 0.00061450 rank 1
2023-02-17 17:25:06,922 DEBUG TRAIN Batch 7/7900 loss 9.034711 loss_att 14.644099 loss_ctc 19.209988 loss_rnnt 6.396085 hw_loss 0.300083 lr 0.00061440 rank 6
2023-02-17 17:25:06,922 DEBUG TRAIN Batch 7/7900 loss 18.648611 loss_att 24.916473 loss_ctc 29.355902 loss_rnnt 15.826313 hw_loss 0.264540 lr 0.00061427 rank 7
2023-02-17 17:25:06,925 DEBUG TRAIN Batch 7/7900 loss 12.443492 loss_att 15.515159 loss_ctc 22.934174 loss_rnnt 10.214698 hw_loss 0.404440 lr 0.00061416 rank 3
2023-02-17 17:25:06,926 DEBUG TRAIN Batch 7/7900 loss 7.481056 loss_att 12.482847 loss_ctc 11.789263 loss_rnnt 5.784089 hw_loss 0.229088 lr 0.00061370 rank 2
2023-02-17 17:25:06,927 DEBUG TRAIN Batch 7/7900 loss 34.078037 loss_att 38.667786 loss_ctc 59.662140 loss_rnnt 29.642262 hw_loss 0.199903 lr 0.00061418 rank 5
2023-02-17 17:25:06,928 DEBUG TRAIN Batch 7/7900 loss 21.901043 loss_att 24.785278 loss_ctc 34.151997 loss_rnnt 19.517782 hw_loss 0.324288 lr 0.00061409 rank 0
2023-02-17 17:25:06,928 DEBUG TRAIN Batch 7/7900 loss 9.580640 loss_att 11.600409 loss_ctc 16.979778 loss_rnnt 7.976819 hw_loss 0.399963 lr 0.00061404 rank 1
2023-02-17 17:25:06,970 DEBUG TRAIN Batch 7/7900 loss 14.101172 loss_att 15.973107 loss_ctc 24.561926 loss_rnnt 12.195150 hw_loss 0.256628 lr 0.00061435 rank 4
2023-02-17 17:26:23,635 DEBUG TRAIN Batch 7/8000 loss 15.051691 loss_att 19.822222 loss_ctc 24.494314 loss_rnnt 12.672270 hw_loss 0.311810 lr 0.00061358 rank 1
2023-02-17 17:26:23,635 DEBUG TRAIN Batch 7/8000 loss 18.261578 loss_att 24.219564 loss_ctc 35.120224 loss_rnnt 14.688883 hw_loss 0.249897 lr 0.00061372 rank 5
2023-02-17 17:26:23,636 DEBUG TRAIN Batch 7/8000 loss 20.189077 loss_att 26.457638 loss_ctc 37.899345 loss_rnnt 16.396360 hw_loss 0.333069 lr 0.00061389 rank 4
2023-02-17 17:26:23,638 DEBUG TRAIN Batch 7/8000 loss 25.984871 loss_att 29.007896 loss_ctc 41.334579 loss_rnnt 23.134403 hw_loss 0.373561 lr 0.00061393 rank 6
2023-02-17 17:26:23,640 DEBUG TRAIN Batch 7/8000 loss 14.937273 loss_att 21.165043 loss_ctc 28.945303 loss_rnnt 11.628601 hw_loss 0.366340 lr 0.00061381 rank 7
2023-02-17 17:26:23,640 DEBUG TRAIN Batch 7/8000 loss 31.600159 loss_att 35.993958 loss_ctc 51.878658 loss_rnnt 27.844301 hw_loss 0.324933 lr 0.00061363 rank 0
2023-02-17 17:26:23,646 DEBUG TRAIN Batch 7/8000 loss 12.503188 loss_att 16.488691 loss_ctc 21.509697 loss_rnnt 10.359043 hw_loss 0.274081 lr 0.00061324 rank 2
2023-02-17 17:26:23,688 DEBUG TRAIN Batch 7/8000 loss 10.077343 loss_att 13.947500 loss_ctc 22.754770 loss_rnnt 7.458445 hw_loss 0.289767 lr 0.00061370 rank 3
2023-02-17 17:27:41,574 DEBUG TRAIN Batch 7/8100 loss 5.423043 loss_att 7.656386 loss_ctc 10.209311 loss_rnnt 4.146104 hw_loss 0.360189 lr 0.00061347 rank 6
2023-02-17 17:27:41,579 DEBUG TRAIN Batch 7/8100 loss 13.159267 loss_att 16.529781 loss_ctc 27.178745 loss_rnnt 10.475943 hw_loss 0.262422 lr 0.00061324 rank 3
2023-02-17 17:27:41,582 DEBUG TRAIN Batch 7/8100 loss 14.646978 loss_att 19.156563 loss_ctc 23.282625 loss_rnnt 12.429638 hw_loss 0.307509 lr 0.00061335 rank 7
2023-02-17 17:27:41,588 DEBUG TRAIN Batch 7/8100 loss 15.372416 loss_att 18.068779 loss_ctc 25.950188 loss_rnnt 13.214713 hw_loss 0.390116 lr 0.00061278 rank 2
2023-02-17 17:27:41,590 DEBUG TRAIN Batch 7/8100 loss 9.948344 loss_att 14.083361 loss_ctc 16.260666 loss_rnnt 8.097851 hw_loss 0.340961 lr 0.00061325 rank 5
2023-02-17 17:27:41,592 DEBUG TRAIN Batch 7/8100 loss 16.694672 loss_att 18.749607 loss_ctc 25.048199 loss_rnnt 15.005550 hw_loss 0.308120 lr 0.00061317 rank 0
2023-02-17 17:27:41,607 DEBUG TRAIN Batch 7/8100 loss 12.196203 loss_att 17.725269 loss_ctc 19.018017 loss_rnnt 10.043421 hw_loss 0.257614 lr 0.00061312 rank 1
2023-02-17 17:27:41,626 DEBUG TRAIN Batch 7/8100 loss 22.598810 loss_att 24.059832 loss_ctc 31.117195 loss_rnnt 20.960358 hw_loss 0.394619 lr 0.00061343 rank 4
2023-02-17 17:28:59,308 DEBUG TRAIN Batch 7/8200 loss 10.887448 loss_att 15.146787 loss_ctc 21.286207 loss_rnnt 8.483437 hw_loss 0.310579 lr 0.00061301 rank 6
2023-02-17 17:28:59,310 DEBUG TRAIN Batch 7/8200 loss 11.494823 loss_att 14.995045 loss_ctc 25.644905 loss_rnnt 8.769955 hw_loss 0.259023 lr 0.00061266 rank 1
2023-02-17 17:28:59,311 DEBUG TRAIN Batch 7/8200 loss 29.983891 loss_att 31.133343 loss_ctc 43.097305 loss_rnnt 27.850653 hw_loss 0.290424 lr 0.00061296 rank 4
2023-02-17 17:28:59,312 DEBUG TRAIN Batch 7/8200 loss 8.834876 loss_att 11.540186 loss_ctc 15.913341 loss_rnnt 7.176659 hw_loss 0.325050 lr 0.00061289 rank 7
2023-02-17 17:28:59,313 DEBUG TRAIN Batch 7/8200 loss 23.692019 loss_att 23.746292 loss_ctc 33.566036 loss_rnnt 22.124445 hw_loss 0.450339 lr 0.00061271 rank 0
2023-02-17 17:28:59,314 DEBUG TRAIN Batch 7/8200 loss 13.073048 loss_att 17.325991 loss_ctc 22.870413 loss_rnnt 10.782837 hw_loss 0.249949 lr 0.00061232 rank 2
2023-02-17 17:28:59,315 DEBUG TRAIN Batch 7/8200 loss 10.647182 loss_att 13.056002 loss_ctc 16.971899 loss_rnnt 9.097461 hw_loss 0.421240 lr 0.00061279 rank 5
2023-02-17 17:28:59,356 DEBUG TRAIN Batch 7/8200 loss 22.654387 loss_att 30.254948 loss_ctc 38.046921 loss_rnnt 18.915783 hw_loss 0.311537 lr 0.00061278 rank 3
2023-02-17 17:30:16,017 DEBUG TRAIN Batch 7/8300 loss 8.877614 loss_att 11.986574 loss_ctc 16.453184 loss_rnnt 7.066150 hw_loss 0.336742 lr 0.00061233 rank 5
2023-02-17 17:30:16,018 DEBUG TRAIN Batch 7/8300 loss 11.604534 loss_att 15.040494 loss_ctc 17.485714 loss_rnnt 9.887844 hw_loss 0.460014 lr 0.00061250 rank 4
2023-02-17 17:30:16,019 DEBUG TRAIN Batch 7/8300 loss 10.630933 loss_att 16.798283 loss_ctc 20.520199 loss_rnnt 7.888958 hw_loss 0.356134 lr 0.00061255 rank 6
2023-02-17 17:30:16,019 DEBUG TRAIN Batch 7/8300 loss 15.709966 loss_att 19.015022 loss_ctc 27.512838 loss_rnnt 13.270855 hw_loss 0.383218 lr 0.00061243 rank 7
2023-02-17 17:30:16,022 DEBUG TRAIN Batch 7/8300 loss 20.651485 loss_att 23.563564 loss_ctc 30.497566 loss_rnnt 18.614937 hw_loss 0.264980 lr 0.00061220 rank 1
2023-02-17 17:30:16,022 DEBUG TRAIN Batch 7/8300 loss 32.526855 loss_att 35.637039 loss_ctc 47.707634 loss_rnnt 29.692829 hw_loss 0.352289 lr 0.00061232 rank 3
2023-02-17 17:30:16,025 DEBUG TRAIN Batch 7/8300 loss 23.519703 loss_att 28.954168 loss_ctc 34.700512 loss_rnnt 20.743122 hw_loss 0.372961 lr 0.00061225 rank 0
2023-02-17 17:30:16,026 DEBUG TRAIN Batch 7/8300 loss 13.754983 loss_att 17.396479 loss_ctc 23.457985 loss_rnnt 11.511299 hw_loss 0.415594 lr 0.00061186 rank 2
2023-02-17 17:31:11,504 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 3
2023-02-17 17:31:11,505 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 4
2023-02-17 17:31:11,507 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 7
2023-02-17 17:31:11,507 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 1
2023-02-17 17:31:11,508 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 5
2023-02-17 17:31:11,514 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 2
2023-02-17 17:31:11,524 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 0
2023-02-17 17:31:11,529 DEBUG CV Batch 7/0 loss 2.733085 loss_att 2.785747 loss_ctc 4.672821 loss_rnnt 2.250897 hw_loss 0.399420 history loss 2.631859 rank 6
2023-02-17 17:31:22,622 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 1
2023-02-17 17:31:22,629 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 7
2023-02-17 17:31:22,637 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 6
2023-02-17 17:31:22,664 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 5
2023-02-17 17:31:22,699 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 2
2023-02-17 17:31:22,843 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 4
2023-02-17 17:31:22,863 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 0
2023-02-17 17:31:22,865 DEBUG CV Batch 7/100 loss 9.829378 loss_att 10.798455 loss_ctc 17.359289 loss_rnnt 8.405002 hw_loss 0.424827 history loss 4.649959 rank 3
2023-02-17 17:31:36,509 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 6
2023-02-17 17:31:36,564 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 1
2023-02-17 17:31:36,669 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 0
2023-02-17 17:31:36,672 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 7
2023-02-17 17:31:36,745 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 5
2023-02-17 17:31:36,812 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 2
2023-02-17 17:31:36,950 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 3
2023-02-17 17:31:37,341 DEBUG CV Batch 7/200 loss 12.457731 loss_att 16.513420 loss_ctc 19.634895 loss_rnnt 10.508469 hw_loss 0.339695 history loss 5.394143 rank 4
2023-02-17 17:31:48,520 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 6
2023-02-17 17:31:48,599 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 1
2023-02-17 17:31:48,724 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 7
2023-02-17 17:31:48,761 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 5
2023-02-17 17:31:48,778 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 0
2023-02-17 17:31:48,950 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 2
2023-02-17 17:31:49,147 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 3
2023-02-17 17:31:49,445 DEBUG CV Batch 7/300 loss 5.074862 loss_att 6.082009 loss_ctc 10.539789 loss_rnnt 3.936267 hw_loss 0.390954 history loss 5.545484 rank 4
2023-02-17 17:32:00,526 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 6
2023-02-17 17:32:00,552 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 1
2023-02-17 17:32:00,620 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 7
2023-02-17 17:32:00,769 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 5
2023-02-17 17:32:00,848 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 0
2023-02-17 17:32:01,018 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 2
2023-02-17 17:32:01,265 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 3
2023-02-17 17:32:01,496 DEBUG CV Batch 7/400 loss 21.804304 loss_att 93.298851 loss_ctc 20.131544 loss_rnnt 7.591403 hw_loss 0.256925 history loss 6.621804 rank 4
2023-02-17 17:32:10,975 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 6
2023-02-17 17:32:10,984 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 1
2023-02-17 17:32:11,063 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 7
2023-02-17 17:32:11,311 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 5
2023-02-17 17:32:11,491 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 0
2023-02-17 17:32:11,524 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 2
2023-02-17 17:32:11,945 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 3
2023-02-17 17:32:12,606 DEBUG CV Batch 7/500 loss 9.754145 loss_att 10.007759 loss_ctc 13.043594 loss_rnnt 9.078962 hw_loss 0.348502 history loss 7.503909 rank 4
2023-02-17 17:32:22,998 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 1
2023-02-17 17:32:23,028 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 6
2023-02-17 17:32:23,118 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 7
2023-02-17 17:32:23,511 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 5
2023-02-17 17:32:23,665 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 2
2023-02-17 17:32:23,679 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 0
2023-02-17 17:32:24,632 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 3
2023-02-17 17:32:25,033 DEBUG CV Batch 7/600 loss 6.415009 loss_att 7.284039 loss_ctc 8.924481 loss_rnnt 5.653019 hw_loss 0.475475 history loss 8.472398 rank 4
2023-02-17 17:32:34,852 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 1
2023-02-17 17:32:35,021 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 6
2023-02-17 17:32:35,185 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 7
2023-02-17 17:32:35,535 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 5
2023-02-17 17:32:35,949 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 0
2023-02-17 17:32:36,055 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 2
2023-02-17 17:32:36,574 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 4
2023-02-17 17:32:36,605 DEBUG CV Batch 7/700 loss 19.763226 loss_att 66.306458 loss_ctc 28.745506 loss_rnnt 9.112153 hw_loss 0.271478 history loss 9.171686 rank 3
2023-02-17 17:32:46,791 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 1
2023-02-17 17:32:46,987 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 6
2023-02-17 17:32:47,322 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 7
2023-02-17 17:32:47,692 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 5
2023-02-17 17:32:47,880 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 2
2023-02-17 17:32:48,475 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 0
2023-02-17 17:32:48,683 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 4
2023-02-17 17:32:48,797 DEBUG CV Batch 7/800 loss 12.158882 loss_att 11.456472 loss_ctc 20.466072 loss_rnnt 10.984942 hw_loss 0.387744 history loss 8.563030 rank 3
2023-02-17 17:33:00,867 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 1
2023-02-17 17:33:00,877 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 6
2023-02-17 17:33:01,202 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 7
2023-02-17 17:33:01,394 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 5
2023-02-17 17:33:01,797 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 2
2023-02-17 17:33:02,295 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 4
2023-02-17 17:33:02,785 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 3
2023-02-17 17:33:03,191 DEBUG CV Batch 7/900 loss 14.846345 loss_att 20.911510 loss_ctc 30.039766 loss_rnnt 11.493087 hw_loss 0.214568 history loss 8.362166 rank 0
2023-02-17 17:33:12,978 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 6
2023-02-17 17:33:13,051 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 1
2023-02-17 17:33:13,354 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 7
2023-02-17 17:33:13,589 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 5
2023-02-17 17:33:14,110 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 2
2023-02-17 17:33:14,906 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 4
2023-02-17 17:33:15,219 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 3
2023-02-17 17:33:15,581 DEBUG CV Batch 7/1000 loss 6.182084 loss_att 6.988792 loss_ctc 9.765799 loss_rnnt 5.332015 hw_loss 0.395435 history loss 8.114800 rank 0
2023-02-17 17:33:24,830 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 6
2023-02-17 17:33:24,850 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 1
2023-02-17 17:33:25,166 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 7
2023-02-17 17:33:25,460 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 5
2023-02-17 17:33:26,060 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 2
2023-02-17 17:33:27,156 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 4
2023-02-17 17:33:27,295 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 3
2023-02-17 17:33:27,631 DEBUG CV Batch 7/1100 loss 6.147287 loss_att 5.939837 loss_ctc 9.959055 loss_rnnt 5.454618 hw_loss 0.423607 history loss 8.103684 rank 0
2023-02-17 17:33:35,202 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 1
2023-02-17 17:33:35,264 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 6
2023-02-17 17:33:35,620 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 7
2023-02-17 17:33:35,975 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 5
2023-02-17 17:33:36,565 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 2
2023-02-17 17:33:37,675 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 4
2023-02-17 17:33:38,287 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 0
2023-02-17 17:33:39,316 DEBUG CV Batch 7/1200 loss 9.475572 loss_att 10.769100 loss_ctc 12.409484 loss_rnnt 8.662652 hw_loss 0.305671 history loss 8.468716 rank 3
2023-02-17 17:33:47,139 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 1
2023-02-17 17:33:47,177 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 6
2023-02-17 17:33:47,547 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 7
2023-02-17 17:33:47,897 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 5
2023-02-17 17:33:48,670 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 2
2023-02-17 17:33:49,735 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 4
2023-02-17 17:33:50,450 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 0
2023-02-17 17:33:52,672 DEBUG CV Batch 7/1300 loss 6.459040 loss_att 6.456447 loss_ctc 8.856558 loss_rnnt 5.940257 hw_loss 0.374311 history loss 8.801395 rank 3
2023-02-17 17:33:58,344 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 1
2023-02-17 17:33:58,975 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 7
2023-02-17 17:33:59,081 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 6
2023-02-17 17:33:59,584 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 5
2023-02-17 17:34:00,753 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 2
2023-02-17 17:34:01,166 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 4
2023-02-17 17:34:02,081 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 0
2023-02-17 17:34:05,292 DEBUG CV Batch 7/1400 loss 12.782377 loss_att 24.611349 loss_ctc 17.381884 loss_rnnt 9.671185 hw_loss 0.247741 history loss 9.136527 rank 3
2023-02-17 17:34:10,732 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 1
2023-02-17 17:34:11,475 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 6
2023-02-17 17:34:11,508 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 7
2023-02-17 17:34:12,040 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 5
2023-02-17 17:34:13,680 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 4
2023-02-17 17:34:13,920 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 2
2023-02-17 17:34:14,530 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 0
2023-02-17 17:34:17,368 DEBUG CV Batch 7/1500 loss 7.883873 loss_att 8.688221 loss_ctc 7.548771 loss_rnnt 7.594556 hw_loss 0.324615 history loss 8.924812 rank 3
2023-02-17 17:34:24,386 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 1
2023-02-17 17:34:25,201 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 6
2023-02-17 17:34:25,295 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 7
2023-02-17 17:34:25,921 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 5
2023-02-17 17:34:27,607 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 4
2023-02-17 17:34:27,713 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 2
2023-02-17 17:34:28,510 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 0
2023-02-17 17:34:30,676 DEBUG CV Batch 7/1600 loss 12.503221 loss_att 19.574703 loss_ctc 18.048702 loss_rnnt 10.162559 hw_loss 0.350564 history loss 8.848797 rank 3
2023-02-17 17:34:36,893 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 1
2023-02-17 17:34:37,657 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 6
2023-02-17 17:34:37,762 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 7
2023-02-17 17:34:38,429 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 5
2023-02-17 17:34:40,135 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 4
2023-02-17 17:34:40,245 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 2
2023-02-17 17:34:41,150 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 0
2023-02-17 17:34:43,241 DEBUG CV Batch 7/1700 loss 10.733680 loss_att 10.604197 loss_ctc 23.720623 loss_rnnt 8.806607 hw_loss 0.415080 history loss 8.740172 rank 3
2023-02-17 17:34:46,159 INFO Epoch 7 CV info cv_loss 8.69560457310988
2023-02-17 17:34:46,160 INFO Epoch 8 TRAIN info lr 0.0006121322209319782
2023-02-17 17:34:46,165 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:46,867 INFO Epoch 7 CV info cv_loss 8.69560457242071
2023-02-17 17:34:46,868 INFO Epoch 8 TRAIN info lr 0.0006122423480171885
2023-02-17 17:34:46,872 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:46,916 INFO Epoch 7 CV info cv_loss 8.695604570628866
2023-02-17 17:34:46,917 INFO Epoch 8 TRAIN info lr 0.0006122469379357199
2023-02-17 17:34:46,920 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:47,552 INFO Epoch 7 CV info cv_loss 8.695604572558544
2023-02-17 17:34:47,553 INFO Epoch 8 TRAIN info lr 0.0006121872770454996
2023-02-17 17:34:47,557 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:49,301 INFO Epoch 7 CV info cv_loss 8.695604573264944
2023-02-17 17:34:49,302 INFO Epoch 8 TRAIN info lr 0.0006123479422679626
2023-02-17 17:34:49,304 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:49,453 INFO Epoch 7 CV info cv_loss 8.695604572799754
2023-02-17 17:34:49,454 INFO Epoch 8 TRAIN info lr 0.000611779297310135
2023-02-17 17:34:49,457 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:50,406 INFO Epoch 7 CV info cv_loss 8.695604572386252
2023-02-17 17:34:50,406 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/7.pt
2023-02-17 17:34:51,055 INFO Epoch 8 TRAIN info lr 0.0006120175683878219
2023-02-17 17:34:51,060 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:34:52,398 INFO Epoch 7 CV info cv_loss 8.695604570870076
2023-02-17 17:34:52,399 INFO Epoch 8 TRAIN info lr 0.0006122193999728761
2023-02-17 17:34:52,404 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 17:36:06,265 DEBUG TRAIN Batch 8/0 loss 11.402411 loss_att 11.135638 loss_ctc 16.363214 loss_rnnt 10.614363 hw_loss 0.337429 lr 0.00061224 rank 7
2023-02-17 17:36:06,267 DEBUG TRAIN Batch 8/0 loss 9.399105 loss_att 10.260480 loss_ctc 12.276041 loss_rnnt 8.617958 hw_loss 0.422402 lr 0.00061224 rank 6
2023-02-17 17:36:06,267 DEBUG TRAIN Batch 8/0 loss 10.987424 loss_att 11.101162 loss_ctc 15.244163 loss_rnnt 10.214978 hw_loss 0.341498 lr 0.00061234 rank 4
2023-02-17 17:36:06,268 DEBUG TRAIN Batch 8/0 loss 15.198709 loss_att 14.270823 loss_ctc 21.280375 loss_rnnt 14.374601 hw_loss 0.372741 lr 0.00061213 rank 1
2023-02-17 17:36:06,271 DEBUG TRAIN Batch 8/0 loss 13.311388 loss_att 12.878065 loss_ctc 16.047825 loss_rnnt 12.813369 hw_loss 0.412173 lr 0.00061201 rank 0
2023-02-17 17:36:06,273 DEBUG TRAIN Batch 8/0 loss 12.996441 loss_att 13.327879 loss_ctc 17.275339 loss_rnnt 12.145646 hw_loss 0.401225 lr 0.00061218 rank 5
2023-02-17 17:36:06,275 DEBUG TRAIN Batch 8/0 loss 13.455183 loss_att 12.620779 loss_ctc 18.233341 loss_rnnt 12.755210 hw_loss 0.430813 lr 0.00061177 rank 2
2023-02-17 17:36:06,287 DEBUG TRAIN Batch 8/0 loss 9.329992 loss_att 8.359746 loss_ctc 13.417901 loss_rnnt 8.799715 hw_loss 0.336133 lr 0.00061221 rank 3
2023-02-17 17:37:20,739 DEBUG TRAIN Batch 8/100 loss 12.863923 loss_att 19.972496 loss_ctc 22.976677 loss_rnnt 9.927333 hw_loss 0.312205 lr 0.00061188 rank 4
2023-02-17 17:37:20,739 DEBUG TRAIN Batch 8/100 loss 10.437478 loss_att 16.573860 loss_ctc 17.486683 loss_rnnt 8.072444 hw_loss 0.370995 lr 0.00061156 rank 0
2023-02-17 17:37:20,740 DEBUG TRAIN Batch 8/100 loss 13.557135 loss_att 16.219618 loss_ctc 17.318398 loss_rnnt 12.291678 hw_loss 0.433983 lr 0.00061178 rank 6
2023-02-17 17:37:20,741 DEBUG TRAIN Batch 8/100 loss 8.906063 loss_att 14.134935 loss_ctc 13.859805 loss_rnnt 7.018266 hw_loss 0.340358 lr 0.00061178 rank 7
2023-02-17 17:37:20,745 DEBUG TRAIN Batch 8/100 loss 25.449821 loss_att 29.469849 loss_ctc 39.917984 loss_rnnt 22.500212 hw_loss 0.405970 lr 0.00061172 rank 5
2023-02-17 17:37:20,745 DEBUG TRAIN Batch 8/100 loss 23.527685 loss_att 29.213139 loss_ctc 42.135239 loss_rnnt 19.711803 hw_loss 0.370844 lr 0.00061176 rank 3
2023-02-17 17:37:20,747 DEBUG TRAIN Batch 8/100 loss 9.636766 loss_att 14.811856 loss_ctc 14.726684 loss_rnnt 7.786138 hw_loss 0.256788 lr 0.00061167 rank 1
2023-02-17 17:37:20,747 DEBUG TRAIN Batch 8/100 loss 21.907272 loss_att 25.922256 loss_ctc 41.949036 loss_rnnt 18.280199 hw_loss 0.284702 lr 0.00061132 rank 2
2023-02-17 17:38:37,594 DEBUG TRAIN Batch 8/200 loss 17.599072 loss_att 18.812008 loss_ctc 24.579626 loss_rnnt 16.238791 hw_loss 0.350540 lr 0.00061132 rank 6
2023-02-17 17:38:37,599 DEBUG TRAIN Batch 8/200 loss 18.995319 loss_att 24.984772 loss_ctc 36.601467 loss_rnnt 15.317316 hw_loss 0.248671 lr 0.00061127 rank 5
2023-02-17 17:38:37,599 DEBUG TRAIN Batch 8/200 loss 20.994408 loss_att 30.301390 loss_ctc 38.969494 loss_rnnt 16.582596 hw_loss 0.288258 lr 0.00061121 rank 1
2023-02-17 17:38:37,600 DEBUG TRAIN Batch 8/200 loss 17.803930 loss_att 21.846445 loss_ctc 33.440014 loss_rnnt 14.784309 hw_loss 0.236828 lr 0.00061133 rank 7
2023-02-17 17:38:37,601 DEBUG TRAIN Batch 8/200 loss 20.085606 loss_att 29.295547 loss_ctc 36.038837 loss_rnnt 15.960281 hw_loss 0.292947 lr 0.00061143 rank 4
2023-02-17 17:38:37,603 DEBUG TRAIN Batch 8/200 loss 5.968286 loss_att 10.549388 loss_ctc 9.920372 loss_rnnt 4.350412 hw_loss 0.327577 lr 0.00061086 rank 2
2023-02-17 17:38:37,604 DEBUG TRAIN Batch 8/200 loss 26.482428 loss_att 34.691059 loss_ctc 39.230698 loss_rnnt 22.930134 hw_loss 0.395246 lr 0.00061130 rank 3
2023-02-17 17:38:37,605 DEBUG TRAIN Batch 8/200 loss 11.174954 loss_att 16.063190 loss_ctc 23.954779 loss_rnnt 8.325966 hw_loss 0.313810 lr 0.00061110 rank 0
2023-02-17 17:39:54,916 DEBUG TRAIN Batch 8/300 loss 7.629071 loss_att 11.255329 loss_ctc 13.715855 loss_rnnt 5.883920 hw_loss 0.390615 lr 0.00061087 rank 6
2023-02-17 17:39:54,916 DEBUG TRAIN Batch 8/300 loss 13.638122 loss_att 18.640011 loss_ctc 22.040600 loss_rnnt 11.359451 hw_loss 0.296181 lr 0.00061097 rank 4
2023-02-17 17:39:54,917 DEBUG TRAIN Batch 8/300 loss 12.091661 loss_att 18.333691 loss_ctc 21.713238 loss_rnnt 9.417730 hw_loss 0.267469 lr 0.00061081 rank 5
2023-02-17 17:39:54,918 DEBUG TRAIN Batch 8/300 loss 14.433502 loss_att 16.628689 loss_ctc 22.743361 loss_rnnt 12.671476 hw_loss 0.403139 lr 0.00061087 rank 7
2023-02-17 17:39:54,923 DEBUG TRAIN Batch 8/300 loss 22.151461 loss_att 25.286484 loss_ctc 37.186287 loss_rnnt 19.372047 hw_loss 0.277060 lr 0.00061084 rank 3
2023-02-17 17:39:54,923 DEBUG TRAIN Batch 8/300 loss 20.085188 loss_att 27.440687 loss_ctc 33.561413 loss_rnnt 16.663345 hw_loss 0.288589 lr 0.00061041 rank 2
2023-02-17 17:39:54,924 DEBUG TRAIN Batch 8/300 loss 24.560556 loss_att 27.730579 loss_ctc 40.091438 loss_rnnt 21.686485 hw_loss 0.317407 lr 0.00061076 rank 1
2023-02-17 17:39:54,927 DEBUG TRAIN Batch 8/300 loss 15.788026 loss_att 19.312269 loss_ctc 26.091337 loss_rnnt 13.543941 hw_loss 0.310237 lr 0.00061064 rank 0
2023-02-17 17:41:13,648 DEBUG TRAIN Batch 8/400 loss 24.773865 loss_att 31.951031 loss_ctc 46.555031 loss_rnnt 20.245483 hw_loss 0.353983 lr 0.00061019 rank 0
2023-02-17 17:41:13,650 DEBUG TRAIN Batch 8/400 loss 15.490747 loss_att 17.025949 loss_ctc 22.529284 loss_rnnt 14.050022 hw_loss 0.366025 lr 0.00060995 rank 2
2023-02-17 17:41:13,653 DEBUG TRAIN Batch 8/400 loss 13.423315 loss_att 15.710513 loss_ctc 22.237711 loss_rnnt 11.622876 hw_loss 0.314523 lr 0.00061041 rank 6
2023-02-17 17:41:13,653 DEBUG TRAIN Batch 8/400 loss 13.304594 loss_att 22.208279 loss_ctc 26.066170 loss_rnnt 9.631111 hw_loss 0.358506 lr 0.00061030 rank 1
2023-02-17 17:41:13,654 DEBUG TRAIN Batch 8/400 loss 10.818123 loss_att 17.741117 loss_ctc 21.808075 loss_rnnt 7.789634 hw_loss 0.334806 lr 0.00061036 rank 5
2023-02-17 17:41:13,654 DEBUG TRAIN Batch 8/400 loss 30.408379 loss_att 32.931973 loss_ctc 44.059929 loss_rnnt 27.894026 hw_loss 0.355178 lr 0.00061039 rank 3
2023-02-17 17:41:13,656 DEBUG TRAIN Batch 8/400 loss 23.147350 loss_att 27.063505 loss_ctc 41.384098 loss_rnnt 19.775215 hw_loss 0.295005 lr 0.00061041 rank 7
2023-02-17 17:41:13,700 DEBUG TRAIN Batch 8/400 loss 10.426460 loss_att 10.976645 loss_ctc 14.185881 loss_rnnt 9.608241 hw_loss 0.387988 lr 0.00061051 rank 4
2023-02-17 17:42:31,533 DEBUG TRAIN Batch 8/500 loss 10.968928 loss_att 13.843224 loss_ctc 18.912889 loss_rnnt 9.169403 hw_loss 0.310259 lr 0.00060993 rank 3
2023-02-17 17:42:31,538 DEBUG TRAIN Batch 8/500 loss 18.407755 loss_att 22.786694 loss_ctc 31.675720 loss_rnnt 15.581339 hw_loss 0.340432 lr 0.00060985 rank 1
2023-02-17 17:42:31,539 DEBUG TRAIN Batch 8/500 loss 28.523674 loss_att 35.195721 loss_ctc 42.469604 loss_rnnt 25.207977 hw_loss 0.228431 lr 0.00060996 rank 7
2023-02-17 17:42:31,540 DEBUG TRAIN Batch 8/500 loss 18.325813 loss_att 22.968788 loss_ctc 30.328636 loss_rnnt 15.636456 hw_loss 0.300724 lr 0.00060990 rank 5
2023-02-17 17:42:31,540 DEBUG TRAIN Batch 8/500 loss 7.043529 loss_att 9.340441 loss_ctc 8.747505 loss_rnnt 6.127645 hw_loss 0.429946 lr 0.00060950 rank 2
2023-02-17 17:42:31,540 DEBUG TRAIN Batch 8/500 loss 16.107786 loss_att 18.667244 loss_ctc 24.638624 loss_rnnt 14.267748 hw_loss 0.357566 lr 0.00060996 rank 6
2023-02-17 17:42:31,546 DEBUG TRAIN Batch 8/500 loss 12.938808 loss_att 17.388308 loss_ctc 28.038652 loss_rnnt 9.859711 hw_loss 0.329784 lr 0.00061006 rank 4
2023-02-17 17:42:31,545 DEBUG TRAIN Batch 8/500 loss 15.251966 loss_att 19.004850 loss_ctc 21.526865 loss_rnnt 13.478255 hw_loss 0.349655 lr 0.00060973 rank 0
2023-02-17 17:43:49,134 DEBUG TRAIN Batch 8/600 loss 19.564262 loss_att 19.087435 loss_ctc 25.718979 loss_rnnt 18.666546 hw_loss 0.323346 lr 0.00060939 rank 1
2023-02-17 17:43:49,138 DEBUG TRAIN Batch 8/600 loss 26.332266 loss_att 28.632914 loss_ctc 44.575275 loss_rnnt 23.253422 hw_loss 0.349336 lr 0.00060928 rank 0
2023-02-17 17:43:49,139 DEBUG TRAIN Batch 8/600 loss 12.820540 loss_att 17.657799 loss_ctc 19.552485 loss_rnnt 10.781880 hw_loss 0.325528 lr 0.00060945 rank 5
2023-02-17 17:43:49,140 DEBUG TRAIN Batch 8/600 loss 11.461587 loss_att 12.610895 loss_ctc 18.591820 loss_rnnt 10.072613 hw_loss 0.390777 lr 0.00060950 rank 6
2023-02-17 17:43:49,141 DEBUG TRAIN Batch 8/600 loss 14.092283 loss_att 14.001122 loss_ctc 19.706900 loss_rnnt 13.132783 hw_loss 0.429595 lr 0.00060951 rank 7
2023-02-17 17:43:49,145 DEBUG TRAIN Batch 8/600 loss 10.365273 loss_att 13.787396 loss_ctc 17.033398 loss_rnnt 8.526516 hw_loss 0.497343 lr 0.00060948 rank 3
2023-02-17 17:43:49,146 DEBUG TRAIN Batch 8/600 loss 12.158937 loss_att 13.986069 loss_ctc 18.203154 loss_rnnt 10.841059 hw_loss 0.274792 lr 0.00060905 rank 2
2023-02-17 17:43:49,184 DEBUG TRAIN Batch 8/600 loss 9.185945 loss_att 10.221360 loss_ctc 15.796261 loss_rnnt 7.879663 hw_loss 0.408419 lr 0.00060961 rank 4
2023-02-17 17:45:09,221 DEBUG TRAIN Batch 8/700 loss 13.940949 loss_att 17.646872 loss_ctc 24.989555 loss_rnnt 11.536212 hw_loss 0.357010 lr 0.00060915 rank 4
2023-02-17 17:45:09,221 DEBUG TRAIN Batch 8/700 loss 6.290200 loss_att 10.051011 loss_ctc 12.926524 loss_rnnt 4.442671 hw_loss 0.394730 lr 0.00060903 rank 3
2023-02-17 17:45:09,221 DEBUG TRAIN Batch 8/700 loss 13.139042 loss_att 18.810242 loss_ctc 22.146252 loss_rnnt 10.577585 hw_loss 0.424231 lr 0.00060900 rank 5
2023-02-17 17:45:09,227 DEBUG TRAIN Batch 8/700 loss 27.051237 loss_att 35.046345 loss_ctc 42.677731 loss_rnnt 23.141968 hw_loss 0.425093 lr 0.00060883 rank 0
2023-02-17 17:45:09,228 DEBUG TRAIN Batch 8/700 loss 8.354118 loss_att 12.369871 loss_ctc 15.197482 loss_rnnt 6.467959 hw_loss 0.319801 lr 0.00060905 rank 7
2023-02-17 17:45:09,250 DEBUG TRAIN Batch 8/700 loss 12.074408 loss_att 18.703686 loss_ctc 23.462530 loss_rnnt 9.083107 hw_loss 0.275680 lr 0.00060894 rank 1
2023-02-17 17:45:09,251 DEBUG TRAIN Batch 8/700 loss 6.997030 loss_att 10.470530 loss_ctc 11.203731 loss_rnnt 5.576822 hw_loss 0.308652 lr 0.00060905 rank 6
2023-02-17 17:45:09,271 DEBUG TRAIN Batch 8/700 loss 18.996086 loss_att 24.920053 loss_ctc 30.617605 loss_rnnt 16.080761 hw_loss 0.339363 lr 0.00060859 rank 2
2023-02-17 17:46:26,081 DEBUG TRAIN Batch 8/800 loss 16.049154 loss_att 20.440641 loss_ctc 28.706112 loss_rnnt 13.359363 hw_loss 0.232314 lr 0.00060814 rank 2
2023-02-17 17:46:26,084 DEBUG TRAIN Batch 8/800 loss 13.265953 loss_att 18.826885 loss_ctc 19.738295 loss_rnnt 11.116657 hw_loss 0.326496 lr 0.00060870 rank 4
2023-02-17 17:46:26,083 DEBUG TRAIN Batch 8/800 loss 15.408614 loss_att 24.850695 loss_ctc 28.836363 loss_rnnt 11.540771 hw_loss 0.354487 lr 0.00060860 rank 6
2023-02-17 17:46:26,085 DEBUG TRAIN Batch 8/800 loss 25.798637 loss_att 32.426460 loss_ctc 39.171478 loss_rnnt 22.496243 hw_loss 0.363346 lr 0.00060860 rank 7
2023-02-17 17:46:26,086 DEBUG TRAIN Batch 8/800 loss 16.756681 loss_att 19.477699 loss_ctc 28.964077 loss_rnnt 14.411350 hw_loss 0.325263 lr 0.00060849 rank 1
2023-02-17 17:46:26,087 DEBUG TRAIN Batch 8/800 loss 8.037058 loss_att 13.180483 loss_ctc 16.048321 loss_rnnt 5.689472 hw_loss 0.470122 lr 0.00060854 rank 5
2023-02-17 17:46:26,090 DEBUG TRAIN Batch 8/800 loss 9.356115 loss_att 13.851814 loss_ctc 17.757582 loss_rnnt 7.139007 hw_loss 0.370824 lr 0.00060838 rank 0
2023-02-17 17:46:26,129 DEBUG TRAIN Batch 8/800 loss 14.035489 loss_att 16.912598 loss_ctc 21.895798 loss_rnnt 12.205877 hw_loss 0.386529 lr 0.00060858 rank 3
2023-02-17 17:47:41,558 DEBUG TRAIN Batch 8/900 loss 18.909782 loss_att 24.451429 loss_ctc 27.474777 loss_rnnt 16.456360 hw_loss 0.380802 lr 0.00060815 rank 6
2023-02-17 17:47:41,562 DEBUG TRAIN Batch 8/900 loss 11.889248 loss_att 18.804770 loss_ctc 22.054073 loss_rnnt 8.992619 hw_loss 0.296655 lr 0.00060809 rank 5
2023-02-17 17:47:41,564 DEBUG TRAIN Batch 8/900 loss 14.470058 loss_att 22.119795 loss_ctc 20.786203 loss_rnnt 11.853306 hw_loss 0.458725 lr 0.00060804 rank 1
2023-02-17 17:47:41,564 DEBUG TRAIN Batch 8/900 loss 13.169229 loss_att 17.548609 loss_ctc 21.485098 loss_rnnt 11.009368 hw_loss 0.328505 lr 0.00060769 rank 2
2023-02-17 17:47:41,565 DEBUG TRAIN Batch 8/900 loss 17.981251 loss_att 22.807941 loss_ctc 31.314430 loss_rnnt 15.059011 hw_loss 0.335894 lr 0.00060813 rank 3
2023-02-17 17:47:41,565 DEBUG TRAIN Batch 8/900 loss 10.297962 loss_att 14.834064 loss_ctc 20.278221 loss_rnnt 7.878336 hw_loss 0.340695 lr 0.00060825 rank 4
2023-02-17 17:47:41,566 DEBUG TRAIN Batch 8/900 loss 27.574306 loss_att 31.904636 loss_ctc 44.674629 loss_rnnt 24.295052 hw_loss 0.249650 lr 0.00060815 rank 7
2023-02-17 17:47:41,566 DEBUG TRAIN Batch 8/900 loss 19.483669 loss_att 20.246872 loss_ctc 35.008522 loss_rnnt 17.057068 hw_loss 0.382461 lr 0.00060793 rank 0
2023-02-17 17:48:59,896 DEBUG TRAIN Batch 8/1000 loss 22.940378 loss_att 27.477219 loss_ctc 32.211853 loss_rnnt 20.641350 hw_loss 0.291496 lr 0.00060770 rank 6
2023-02-17 17:48:59,898 DEBUG TRAIN Batch 8/1000 loss 12.942637 loss_att 16.059595 loss_ctc 22.799393 loss_rnnt 10.852736 hw_loss 0.285516 lr 0.00060725 rank 2
2023-02-17 17:48:59,900 DEBUG TRAIN Batch 8/1000 loss 15.505086 loss_att 20.885298 loss_ctc 26.601145 loss_rnnt 12.756327 hw_loss 0.362326 lr 0.00060770 rank 7
2023-02-17 17:48:59,903 DEBUG TRAIN Batch 8/1000 loss 20.506657 loss_att 24.835075 loss_ctc 36.102833 loss_rnnt 17.410992 hw_loss 0.282167 lr 0.00060759 rank 1
2023-02-17 17:48:59,904 DEBUG TRAIN Batch 8/1000 loss 14.707192 loss_att 20.029436 loss_ctc 22.413280 loss_rnnt 12.460199 hw_loss 0.290746 lr 0.00060765 rank 5
2023-02-17 17:48:59,904 DEBUG TRAIN Batch 8/1000 loss 16.258068 loss_att 17.284393 loss_ctc 21.300884 loss_rnnt 15.208742 hw_loss 0.321909 lr 0.00060780 rank 4
2023-02-17 17:48:59,904 DEBUG TRAIN Batch 8/1000 loss 15.686958 loss_att 19.973639 loss_ctc 29.541180 loss_rnnt 12.848581 hw_loss 0.250895 lr 0.00060748 rank 0
2023-02-17 17:48:59,948 DEBUG TRAIN Batch 8/1000 loss 16.176424 loss_att 20.152958 loss_ctc 26.944948 loss_rnnt 13.792875 hw_loss 0.285821 lr 0.00060768 rank 3
2023-02-17 17:50:19,664 DEBUG TRAIN Batch 8/1100 loss 16.670422 loss_att 18.172535 loss_ctc 22.418674 loss_rnnt 15.368714 hw_loss 0.440343 lr 0.00060720 rank 5
2023-02-17 17:50:19,665 DEBUG TRAIN Batch 8/1100 loss 12.279120 loss_att 16.379328 loss_ctc 22.382013 loss_rnnt 9.944843 hw_loss 0.313471 lr 0.00060725 rank 6
2023-02-17 17:50:19,666 DEBUG TRAIN Batch 8/1100 loss 17.013155 loss_att 22.248867 loss_ctc 31.112637 loss_rnnt 13.873349 hw_loss 0.398872 lr 0.00060726 rank 7
2023-02-17 17:50:19,667 DEBUG TRAIN Batch 8/1100 loss 14.779184 loss_att 19.035307 loss_ctc 26.644539 loss_rnnt 12.154788 hw_loss 0.358360 lr 0.00060723 rank 3
2023-02-17 17:50:19,669 DEBUG TRAIN Batch 8/1100 loss 9.955361 loss_att 12.432365 loss_ctc 13.252393 loss_rnnt 8.832952 hw_loss 0.351381 lr 0.00060735 rank 4
2023-02-17 17:50:19,672 DEBUG TRAIN Batch 8/1100 loss 13.210096 loss_att 18.949596 loss_ctc 21.725611 loss_rnnt 10.771294 hw_loss 0.291563 lr 0.00060714 rank 1
2023-02-17 17:50:19,677 DEBUG TRAIN Batch 8/1100 loss 7.092811 loss_att 9.311218 loss_ctc 12.969553 loss_rnnt 5.703426 hw_loss 0.304009 lr 0.00060680 rank 2
2023-02-17 17:50:19,714 DEBUG TRAIN Batch 8/1100 loss 19.489674 loss_att 22.760572 loss_ctc 31.314619 loss_rnnt 17.105637 hw_loss 0.287249 lr 0.00060703 rank 0
2023-02-17 17:51:36,676 DEBUG TRAIN Batch 8/1200 loss 10.184043 loss_att 11.991521 loss_ctc 17.005789 loss_rnnt 8.709659 hw_loss 0.381231 lr 0.00060678 rank 3
2023-02-17 17:51:36,680 DEBUG TRAIN Batch 8/1200 loss 11.852514 loss_att 13.192163 loss_ctc 19.239058 loss_rnnt 10.364703 hw_loss 0.440643 lr 0.00060681 rank 7
2023-02-17 17:51:36,681 DEBUG TRAIN Batch 8/1200 loss 12.514413 loss_att 15.494398 loss_ctc 21.359938 loss_rnnt 10.558466 hw_loss 0.338527 lr 0.00060675 rank 5
2023-02-17 17:51:36,684 DEBUG TRAIN Batch 8/1200 loss 11.119117 loss_att 13.199994 loss_ctc 17.149647 loss_rnnt 9.682910 hw_loss 0.404925 lr 0.00060670 rank 1
2023-02-17 17:51:36,684 DEBUG TRAIN Batch 8/1200 loss 18.900562 loss_att 20.395613 loss_ctc 27.871439 loss_rnnt 17.215040 hw_loss 0.356996 lr 0.00060691 rank 4
2023-02-17 17:51:36,683 DEBUG TRAIN Batch 8/1200 loss 15.862298 loss_att 15.702028 loss_ctc 21.934484 loss_rnnt 14.890665 hw_loss 0.363863 lr 0.00060680 rank 6
2023-02-17 17:51:36,686 DEBUG TRAIN Batch 8/1200 loss 30.183760 loss_att 33.272938 loss_ctc 47.210953 loss_rnnt 27.140415 hw_loss 0.291029 lr 0.00060658 rank 0
2023-02-17 17:51:36,687 DEBUG TRAIN Batch 8/1200 loss 11.172185 loss_att 14.867483 loss_ctc 22.351240 loss_rnnt 8.769653 hw_loss 0.324245 lr 0.00060635 rank 2
2023-02-17 17:52:53,592 DEBUG TRAIN Batch 8/1300 loss 15.850203 loss_att 14.770826 loss_ctc 22.425852 loss_rnnt 14.956691 hw_loss 0.436190 lr 0.00060636 rank 6
2023-02-17 17:52:53,593 DEBUG TRAIN Batch 8/1300 loss 53.210247 loss_att 57.225025 loss_ctc 72.979858 loss_rnnt 49.581802 hw_loss 0.355386 lr 0.00060625 rank 1
2023-02-17 17:52:53,598 DEBUG TRAIN Batch 8/1300 loss 8.794538 loss_att 13.391562 loss_ctc 18.077488 loss_rnnt 6.481895 hw_loss 0.291585 lr 0.00060636 rank 7
2023-02-17 17:52:53,598 DEBUG TRAIN Batch 8/1300 loss 17.601225 loss_att 21.976128 loss_ctc 37.130619 loss_rnnt 13.965557 hw_loss 0.293936 lr 0.00060633 rank 3
2023-02-17 17:52:53,599 DEBUG TRAIN Batch 8/1300 loss 14.803661 loss_att 15.207615 loss_ctc 22.304501 loss_rnnt 13.492024 hw_loss 0.432627 lr 0.00060630 rank 5
2023-02-17 17:52:53,600 DEBUG TRAIN Batch 8/1300 loss 21.149513 loss_att 22.748367 loss_ctc 32.155560 loss_rnnt 19.166098 hw_loss 0.367822 lr 0.00060646 rank 4
2023-02-17 17:52:53,602 DEBUG TRAIN Batch 8/1300 loss 11.966510 loss_att 12.648939 loss_ctc 18.176298 loss_rnnt 10.777684 hw_loss 0.420690 lr 0.00060614 rank 0
2023-02-17 17:52:53,604 DEBUG TRAIN Batch 8/1300 loss 21.878370 loss_att 21.087923 loss_ctc 31.643726 loss_rnnt 20.485785 hw_loss 0.466176 lr 0.00060591 rank 2
2023-02-17 17:54:12,739 DEBUG TRAIN Batch 8/1400 loss 16.197021 loss_att 21.321301 loss_ctc 31.463186 loss_rnnt 12.915859 hw_loss 0.414031 lr 0.00060580 rank 1
2023-02-17 17:54:12,740 DEBUG TRAIN Batch 8/1400 loss 19.019680 loss_att 19.245770 loss_ctc 29.851425 loss_rnnt 17.403437 hw_loss 0.237735 lr 0.00060586 rank 5
2023-02-17 17:54:12,741 DEBUG TRAIN Batch 8/1400 loss 15.275402 loss_att 22.378725 loss_ctc 31.668568 loss_rnnt 11.504111 hw_loss 0.309133 lr 0.00060589 rank 3
2023-02-17 17:54:12,741 DEBUG TRAIN Batch 8/1400 loss 10.087122 loss_att 14.744763 loss_ctc 15.931116 loss_rnnt 8.202250 hw_loss 0.326521 lr 0.00060591 rank 6
2023-02-17 17:54:12,743 DEBUG TRAIN Batch 8/1400 loss 22.816704 loss_att 27.336691 loss_ctc 40.220543 loss_rnnt 19.355083 hw_loss 0.444586 lr 0.00060592 rank 7
2023-02-17 17:54:12,744 DEBUG TRAIN Batch 8/1400 loss 11.448428 loss_att 15.430492 loss_ctc 19.521713 loss_rnnt 9.355812 hw_loss 0.412059 lr 0.00060569 rank 0
2023-02-17 17:54:12,746 DEBUG TRAIN Batch 8/1400 loss 12.338757 loss_att 16.112034 loss_ctc 20.882107 loss_rnnt 10.269861 hw_loss 0.328363 lr 0.00060601 rank 4
2023-02-17 17:54:12,764 DEBUG TRAIN Batch 8/1400 loss 9.728999 loss_att 17.046949 loss_ctc 19.801998 loss_rnnt 6.789819 hw_loss 0.248482 lr 0.00060546 rank 2
2023-02-17 17:55:29,009 DEBUG TRAIN Batch 8/1500 loss 16.416708 loss_att 19.771559 loss_ctc 20.174652 loss_rnnt 15.056330 hw_loss 0.353148 lr 0.00060547 rank 6
2023-02-17 17:55:29,013 DEBUG TRAIN Batch 8/1500 loss 6.950701 loss_att 11.335207 loss_ctc 15.066811 loss_rnnt 4.820458 hw_loss 0.320988 lr 0.00060541 rank 5
2023-02-17 17:55:29,015 DEBUG TRAIN Batch 8/1500 loss 13.059661 loss_att 14.585716 loss_ctc 20.648653 loss_rnnt 11.570162 hw_loss 0.323291 lr 0.00060525 rank 0
2023-02-17 17:55:29,017 DEBUG TRAIN Batch 8/1500 loss 12.289846 loss_att 18.253712 loss_ctc 20.438770 loss_rnnt 9.837931 hw_loss 0.323660 lr 0.00060544 rank 3
2023-02-17 17:55:29,018 DEBUG TRAIN Batch 8/1500 loss 7.672989 loss_att 11.198565 loss_ctc 15.193477 loss_rnnt 5.770929 hw_loss 0.364149 lr 0.00060536 rank 1
2023-02-17 17:55:29,020 DEBUG TRAIN Batch 8/1500 loss 5.824780 loss_att 8.891062 loss_ctc 11.027957 loss_rnnt 4.332391 hw_loss 0.347580 lr 0.00060557 rank 4
2023-02-17 17:55:29,022 DEBUG TRAIN Batch 8/1500 loss 24.875469 loss_att 29.259340 loss_ctc 40.781189 loss_rnnt 21.707520 hw_loss 0.319525 lr 0.00060547 rank 7
2023-02-17 17:55:29,059 DEBUG TRAIN Batch 8/1500 loss 13.445688 loss_att 15.851385 loss_ctc 25.336174 loss_rnnt 11.203670 hw_loss 0.329027 lr 0.00060502 rank 2
2023-02-17 17:56:46,161 DEBUG TRAIN Batch 8/1600 loss 9.562901 loss_att 15.447580 loss_ctc 13.867239 loss_rnnt 7.650488 hw_loss 0.302934 lr 0.00060492 rank 1
2023-02-17 17:56:46,161 DEBUG TRAIN Batch 8/1600 loss 17.502687 loss_att 21.106064 loss_ctc 27.705685 loss_rnnt 15.291990 hw_loss 0.243041 lr 0.00060513 rank 4
2023-02-17 17:56:46,162 DEBUG TRAIN Batch 8/1600 loss 10.311810 loss_att 13.721699 loss_ctc 18.600580 loss_rnnt 8.362448 hw_loss 0.304151 lr 0.00060500 rank 3
2023-02-17 17:56:46,162 DEBUG TRAIN Batch 8/1600 loss 9.847236 loss_att 14.288522 loss_ctc 17.141525 loss_rnnt 7.845212 hw_loss 0.264740 lr 0.00060502 rank 6
2023-02-17 17:56:46,164 DEBUG TRAIN Batch 8/1600 loss 15.984217 loss_att 19.470512 loss_ctc 24.266037 loss_rnnt 13.998001 hw_loss 0.346340 lr 0.00060458 rank 2
2023-02-17 17:56:46,166 DEBUG TRAIN Batch 8/1600 loss 12.943636 loss_att 17.223318 loss_ctc 22.996075 loss_rnnt 10.566406 hw_loss 0.339315 lr 0.00060481 rank 0
2023-02-17 17:56:46,193 DEBUG TRAIN Batch 8/1600 loss 21.792774 loss_att 26.507236 loss_ctc 38.835274 loss_rnnt 18.411591 hw_loss 0.311172 lr 0.00060497 rank 5
2023-02-17 17:56:46,204 DEBUG TRAIN Batch 8/1600 loss 10.946800 loss_att 13.142130 loss_ctc 18.708672 loss_rnnt 9.314205 hw_loss 0.297401 lr 0.00060503 rank 7
2023-02-17 17:58:03,434 DEBUG TRAIN Batch 8/1700 loss 11.091540 loss_att 13.821152 loss_ctc 20.085403 loss_rnnt 9.192251 hw_loss 0.289096 lr 0.00060448 rank 1
2023-02-17 17:58:03,436 DEBUG TRAIN Batch 8/1700 loss 15.737569 loss_att 20.059811 loss_ctc 23.116179 loss_rnnt 13.650273 hw_loss 0.448186 lr 0.00060456 rank 3
2023-02-17 17:58:03,439 DEBUG TRAIN Batch 8/1700 loss 11.944026 loss_att 19.545998 loss_ctc 25.132334 loss_rnnt 8.527445 hw_loss 0.258271 lr 0.00060459 rank 7
2023-02-17 17:58:03,440 DEBUG TRAIN Batch 8/1700 loss 23.642834 loss_att 27.921700 loss_ctc 42.831310 loss_rnnt 20.070190 hw_loss 0.297011 lr 0.00060453 rank 5
2023-02-17 17:58:03,440 DEBUG TRAIN Batch 8/1700 loss 19.272894 loss_att 21.148754 loss_ctc 31.940083 loss_rnnt 17.050106 hw_loss 0.297480 lr 0.00060458 rank 6
2023-02-17 17:58:03,441 DEBUG TRAIN Batch 8/1700 loss 9.970815 loss_att 13.289947 loss_ctc 17.575632 loss_rnnt 8.071267 hw_loss 0.415774 lr 0.00060414 rank 2
2023-02-17 17:58:03,442 DEBUG TRAIN Batch 8/1700 loss 22.037640 loss_att 22.538736 loss_ctc 28.872974 loss_rnnt 20.861042 hw_loss 0.309376 lr 0.00060468 rank 4
2023-02-17 17:58:03,493 DEBUG TRAIN Batch 8/1700 loss 20.208099 loss_att 22.919714 loss_ctc 36.038269 loss_rnnt 17.368006 hw_loss 0.350778 lr 0.00060436 rank 0
2023-02-17 17:59:22,931 DEBUG TRAIN Batch 8/1800 loss 13.000391 loss_att 14.174661 loss_ctc 21.290823 loss_rnnt 11.464653 hw_loss 0.366549 lr 0.00060412 rank 3
2023-02-17 17:59:22,932 DEBUG TRAIN Batch 8/1800 loss 18.581268 loss_att 21.862946 loss_ctc 28.421698 loss_rnnt 16.408314 hw_loss 0.383555 lr 0.00060414 rank 7
2023-02-17 17:59:22,934 DEBUG TRAIN Batch 8/1800 loss 13.257224 loss_att 18.183416 loss_ctc 20.928217 loss_rnnt 11.100826 hw_loss 0.278174 lr 0.00060414 rank 6
2023-02-17 17:59:22,934 DEBUG TRAIN Batch 8/1800 loss 21.028566 loss_att 22.627104 loss_ctc 36.199986 loss_rnnt 18.499546 hw_loss 0.349606 lr 0.00060409 rank 5
2023-02-17 17:59:22,937 DEBUG TRAIN Batch 8/1800 loss 9.567819 loss_att 14.088726 loss_ctc 19.139149 loss_rnnt 7.156982 hw_loss 0.432146 lr 0.00060424 rank 4
2023-02-17 17:59:22,937 DEBUG TRAIN Batch 8/1800 loss 14.556591 loss_att 18.408976 loss_ctc 24.137814 loss_rnnt 12.292916 hw_loss 0.404439 lr 0.00060369 rank 2
2023-02-17 17:59:22,937 DEBUG TRAIN Batch 8/1800 loss 12.131671 loss_att 15.094293 loss_ctc 20.389206 loss_rnnt 10.294167 hw_loss 0.269955 lr 0.00060403 rank 1
2023-02-17 17:59:22,938 DEBUG TRAIN Batch 8/1800 loss 6.093872 loss_att 9.400129 loss_ctc 13.002750 loss_rnnt 4.369720 hw_loss 0.265719 lr 0.00060392 rank 0
2023-02-17 18:00:38,705 DEBUG TRAIN Batch 8/1900 loss 11.090000 loss_att 13.860664 loss_ctc 15.456417 loss_rnnt 9.756489 hw_loss 0.369730 lr 0.00060365 rank 5
2023-02-17 18:00:38,708 DEBUG TRAIN Batch 8/1900 loss 13.395308 loss_att 17.744061 loss_ctc 23.625601 loss_rnnt 10.978737 hw_loss 0.342714 lr 0.00060370 rank 7
2023-02-17 18:00:38,711 DEBUG TRAIN Batch 8/1900 loss 15.915616 loss_att 20.630686 loss_ctc 25.329788 loss_rnnt 13.567218 hw_loss 0.281550 lr 0.00060368 rank 3
2023-02-17 18:00:38,713 DEBUG TRAIN Batch 8/1900 loss 14.286926 loss_att 14.283380 loss_ctc 18.805029 loss_rnnt 13.484505 hw_loss 0.376343 lr 0.00060370 rank 6
2023-02-17 18:00:38,716 DEBUG TRAIN Batch 8/1900 loss 23.804262 loss_att 28.918049 loss_ctc 41.832901 loss_rnnt 20.247093 hw_loss 0.244858 lr 0.00060359 rank 1
2023-02-17 18:00:38,718 DEBUG TRAIN Batch 8/1900 loss 17.131914 loss_att 18.172474 loss_ctc 28.443056 loss_rnnt 15.230570 hw_loss 0.347024 lr 0.00060326 rank 2
2023-02-17 18:00:38,718 DEBUG TRAIN Batch 8/1900 loss 19.514917 loss_att 19.696987 loss_ctc 29.019623 loss_rnnt 18.015905 hw_loss 0.366197 lr 0.00060348 rank 0
2023-02-17 18:00:38,721 DEBUG TRAIN Batch 8/1900 loss 18.264698 loss_att 19.541775 loss_ctc 27.890148 loss_rnnt 16.517487 hw_loss 0.390755 lr 0.00060380 rank 4
2023-02-17 18:01:53,444 DEBUG TRAIN Batch 8/2000 loss 11.884910 loss_att 14.736919 loss_ctc 21.108435 loss_rnnt 9.911372 hw_loss 0.324998 lr 0.00060326 rank 7
2023-02-17 18:01:53,444 DEBUG TRAIN Batch 8/2000 loss 9.825331 loss_att 14.783363 loss_ctc 22.368658 loss_rnnt 7.013641 hw_loss 0.276822 lr 0.00060321 rank 5
2023-02-17 18:01:53,444 DEBUG TRAIN Batch 8/2000 loss 30.418602 loss_att 34.744698 loss_ctc 55.462036 loss_rnnt 26.034811 hw_loss 0.336464 lr 0.00060324 rank 3
2023-02-17 18:01:53,446 DEBUG TRAIN Batch 8/2000 loss 12.515147 loss_att 19.147938 loss_ctc 20.138121 loss_rnnt 10.001407 hw_loss 0.320226 lr 0.00060326 rank 6
2023-02-17 18:01:53,448 DEBUG TRAIN Batch 8/2000 loss 9.276280 loss_att 12.603112 loss_ctc 14.864296 loss_rnnt 7.635270 hw_loss 0.432326 lr 0.00060315 rank 1
2023-02-17 18:01:53,451 DEBUG TRAIN Batch 8/2000 loss 9.997323 loss_att 11.807726 loss_ctc 17.277000 loss_rnnt 8.485819 hw_loss 0.335249 lr 0.00060282 rank 2
2023-02-17 18:01:53,451 DEBUG TRAIN Batch 8/2000 loss 8.707919 loss_att 12.175837 loss_ctc 17.111837 loss_rnnt 6.732828 hw_loss 0.301848 lr 0.00060304 rank 0
2023-02-17 18:01:53,453 DEBUG TRAIN Batch 8/2000 loss 21.327209 loss_att 22.478117 loss_ctc 30.891699 loss_rnnt 19.636648 hw_loss 0.347088 lr 0.00060336 rank 4
2023-02-17 18:03:12,177 DEBUG TRAIN Batch 8/2100 loss 14.515807 loss_att 16.004318 loss_ctc 24.645752 loss_rnnt 12.633675 hw_loss 0.438320 lr 0.00060283 rank 7
2023-02-17 18:03:12,179 DEBUG TRAIN Batch 8/2100 loss 10.399899 loss_att 15.335014 loss_ctc 22.769524 loss_rnnt 7.634031 hw_loss 0.242929 lr 0.00060277 rank 5
2023-02-17 18:03:12,182 DEBUG TRAIN Batch 8/2100 loss 12.758293 loss_att 19.205196 loss_ctc 23.193090 loss_rnnt 9.929871 hw_loss 0.277004 lr 0.00060282 rank 6
2023-02-17 18:03:12,189 DEBUG TRAIN Batch 8/2100 loss 20.723839 loss_att 24.698303 loss_ctc 32.824478 loss_rnnt 18.121809 hw_loss 0.363220 lr 0.00060238 rank 2
2023-02-17 18:03:12,190 DEBUG TRAIN Batch 8/2100 loss 21.931875 loss_att 22.332026 loss_ctc 39.205551 loss_rnnt 19.344198 hw_loss 0.383419 lr 0.00060280 rank 3
2023-02-17 18:03:12,190 DEBUG TRAIN Batch 8/2100 loss 16.899597 loss_att 21.484413 loss_ctc 26.857054 loss_rnnt 14.443663 hw_loss 0.396208 lr 0.00060272 rank 1
2023-02-17 18:03:12,193 DEBUG TRAIN Batch 8/2100 loss 15.953887 loss_att 24.973783 loss_ctc 28.993292 loss_rnnt 12.152584 hw_loss 0.485129 lr 0.00060292 rank 4
2023-02-17 18:03:12,195 DEBUG TRAIN Batch 8/2100 loss 13.870190 loss_att 19.197203 loss_ctc 24.487030 loss_rnnt 11.185045 hw_loss 0.382805 lr 0.00060261 rank 0
2023-02-17 18:04:29,043 DEBUG TRAIN Batch 8/2200 loss 14.005922 loss_att 15.894583 loss_ctc 23.178837 loss_rnnt 12.259567 hw_loss 0.272939 lr 0.00060236 rank 3
2023-02-17 18:04:29,044 DEBUG TRAIN Batch 8/2200 loss 14.898402 loss_att 21.426126 loss_ctc 27.114090 loss_rnnt 11.783022 hw_loss 0.339520 lr 0.00060228 rank 1
2023-02-17 18:04:29,044 DEBUG TRAIN Batch 8/2200 loss 13.076056 loss_att 16.497395 loss_ctc 16.397171 loss_rnnt 11.769674 hw_loss 0.336187 lr 0.00060239 rank 7
2023-02-17 18:04:29,044 DEBUG TRAIN Batch 8/2200 loss 20.127350 loss_att 26.507984 loss_ctc 31.391598 loss_rnnt 17.198931 hw_loss 0.281980 lr 0.00060238 rank 6
2023-02-17 18:04:29,046 DEBUG TRAIN Batch 8/2200 loss 14.203825 loss_att 18.958473 loss_ctc 24.670975 loss_rnnt 11.631031 hw_loss 0.424209 lr 0.00060194 rank 2
2023-02-17 18:04:29,048 DEBUG TRAIN Batch 8/2200 loss 16.093082 loss_att 21.650293 loss_ctc 27.340450 loss_rnnt 13.352558 hw_loss 0.242688 lr 0.00060248 rank 4
2023-02-17 18:04:29,048 DEBUG TRAIN Batch 8/2200 loss 12.362324 loss_att 15.078287 loss_ctc 17.456406 loss_rnnt 10.984416 hw_loss 0.291571 lr 0.00060233 rank 5
2023-02-17 18:04:29,050 DEBUG TRAIN Batch 8/2200 loss 10.836530 loss_att 17.602308 loss_ctc 21.985889 loss_rnnt 7.795066 hw_loss 0.378237 lr 0.00060217 rank 0
2023-02-17 18:05:44,182 DEBUG TRAIN Batch 8/2300 loss 16.544304 loss_att 19.693220 loss_ctc 31.508263 loss_rnnt 13.773817 hw_loss 0.272832 lr 0.00060195 rank 7
2023-02-17 18:05:44,182 DEBUG TRAIN Batch 8/2300 loss 13.833897 loss_att 15.979017 loss_ctc 26.551168 loss_rnnt 11.500147 hw_loss 0.392042 lr 0.00060195 rank 6
2023-02-17 18:05:44,186 DEBUG TRAIN Batch 8/2300 loss 22.783033 loss_att 26.907322 loss_ctc 37.829594 loss_rnnt 19.810255 hw_loss 0.265712 lr 0.00060189 rank 5
2023-02-17 18:05:44,186 DEBUG TRAIN Batch 8/2300 loss 10.324604 loss_att 15.166685 loss_ctc 19.464014 loss_rnnt 8.010514 hw_loss 0.238285 lr 0.00060184 rank 1
2023-02-17 18:05:44,188 DEBUG TRAIN Batch 8/2300 loss 15.407808 loss_att 18.793386 loss_ctc 27.823666 loss_rnnt 12.951493 hw_loss 0.232034 lr 0.00060151 rank 2
2023-02-17 18:05:44,189 DEBUG TRAIN Batch 8/2300 loss 16.381702 loss_att 19.944050 loss_ctc 24.777931 loss_rnnt 14.371895 hw_loss 0.333453 lr 0.00060192 rank 3
2023-02-17 18:05:44,189 DEBUG TRAIN Batch 8/2300 loss 19.913727 loss_att 20.112267 loss_ctc 29.158112 loss_rnnt 18.448734 hw_loss 0.361314 lr 0.00060205 rank 4
2023-02-17 18:05:44,238 DEBUG TRAIN Batch 8/2300 loss 16.933908 loss_att 23.071426 loss_ctc 29.349621 loss_rnnt 13.856232 hw_loss 0.365148 lr 0.00060173 rank 0
2023-02-17 18:07:00,735 DEBUG TRAIN Batch 8/2400 loss 16.308977 loss_att 18.019560 loss_ctc 27.927439 loss_rnnt 14.222183 hw_loss 0.366655 lr 0.00060151 rank 6
2023-02-17 18:07:00,736 DEBUG TRAIN Batch 8/2400 loss 22.562952 loss_att 29.635994 loss_ctc 40.372444 loss_rnnt 18.577251 hw_loss 0.368429 lr 0.00060130 rank 0
2023-02-17 18:07:00,739 DEBUG TRAIN Batch 8/2400 loss 17.132679 loss_att 22.921303 loss_ctc 30.456381 loss_rnnt 14.001854 hw_loss 0.368640 lr 0.00060141 rank 1
2023-02-17 18:07:00,740 DEBUG TRAIN Batch 8/2400 loss 14.779912 loss_att 16.229130 loss_ctc 24.666056 loss_rnnt 12.948949 hw_loss 0.418060 lr 0.00060149 rank 3
2023-02-17 18:07:00,740 DEBUG TRAIN Batch 8/2400 loss 11.253247 loss_att 13.968584 loss_ctc 20.657696 loss_rnnt 9.259475 hw_loss 0.368961 lr 0.00060107 rank 2
2023-02-17 18:07:00,743 DEBUG TRAIN Batch 8/2400 loss 13.828694 loss_att 18.324924 loss_ctc 24.385712 loss_rnnt 11.382964 hw_loss 0.260403 lr 0.00060152 rank 7
2023-02-17 18:07:00,743 DEBUG TRAIN Batch 8/2400 loss 8.524563 loss_att 12.990013 loss_ctc 15.224458 loss_rnnt 6.527884 hw_loss 0.394256 lr 0.00060146 rank 5
2023-02-17 18:07:00,746 DEBUG TRAIN Batch 8/2400 loss 9.307004 loss_att 16.636936 loss_ctc 22.450457 loss_rnnt 5.948038 hw_loss 0.263475 lr 0.00060161 rank 4
2023-02-17 18:08:22,012 DEBUG TRAIN Batch 8/2500 loss 17.347227 loss_att 22.344709 loss_ctc 25.367691 loss_rnnt 15.120214 hw_loss 0.296478 lr 0.00060108 rank 6
2023-02-17 18:08:22,015 DEBUG TRAIN Batch 8/2500 loss 10.040134 loss_att 10.898898 loss_ctc 17.734539 loss_rnnt 8.660760 hw_loss 0.340691 lr 0.00060097 rank 1
2023-02-17 18:08:22,016 DEBUG TRAIN Batch 8/2500 loss 5.346975 loss_att 9.411564 loss_ctc 8.722754 loss_rnnt 3.893195 hw_loss 0.357672 lr 0.00060118 rank 4
2023-02-17 18:08:22,017 DEBUG TRAIN Batch 8/2500 loss 14.016038 loss_att 15.419697 loss_ctc 23.683792 loss_rnnt 12.297508 hw_loss 0.278930 lr 0.00060105 rank 3
2023-02-17 18:08:22,024 DEBUG TRAIN Batch 8/2500 loss 8.568680 loss_att 13.035660 loss_ctc 12.720264 loss_rnnt 6.900866 hw_loss 0.414138 lr 0.00060064 rank 2
2023-02-17 18:08:22,024 DEBUG TRAIN Batch 8/2500 loss 9.705713 loss_att 13.917767 loss_ctc 17.651476 loss_rnnt 7.608350 hw_loss 0.366594 lr 0.00060086 rank 0
2023-02-17 18:08:22,046 DEBUG TRAIN Batch 8/2500 loss 19.413517 loss_att 19.603149 loss_ctc 26.214355 loss_rnnt 18.227413 hw_loss 0.452622 lr 0.00060102 rank 5
2023-02-17 18:08:22,052 DEBUG TRAIN Batch 8/2500 loss 16.886463 loss_att 18.489933 loss_ctc 25.384691 loss_rnnt 15.235126 hw_loss 0.370397 lr 0.00060108 rank 7
2023-02-17 18:09:39,194 DEBUG TRAIN Batch 8/2600 loss 13.616121 loss_att 17.467825 loss_ctc 26.826797 loss_rnnt 10.979706 hw_loss 0.196219 lr 0.00060062 rank 3
2023-02-17 18:09:39,198 DEBUG TRAIN Batch 8/2600 loss 5.999961 loss_att 10.446767 loss_ctc 14.220938 loss_rnnt 3.823918 hw_loss 0.357284 lr 0.00060059 rank 5
2023-02-17 18:09:39,200 DEBUG TRAIN Batch 8/2600 loss 15.523070 loss_att 16.122393 loss_ctc 23.508389 loss_rnnt 14.125457 hw_loss 0.399452 lr 0.00060074 rank 4
2023-02-17 18:09:39,203 DEBUG TRAIN Batch 8/2600 loss 24.407438 loss_att 31.968857 loss_ctc 39.354630 loss_rnnt 20.740456 hw_loss 0.303266 lr 0.00060065 rank 7
2023-02-17 18:09:39,204 DEBUG TRAIN Batch 8/2600 loss 19.160992 loss_att 20.169094 loss_ctc 32.733200 loss_rnnt 16.981125 hw_loss 0.316163 lr 0.00060021 rank 2
2023-02-17 18:09:39,205 DEBUG TRAIN Batch 8/2600 loss 28.980371 loss_att 32.844223 loss_ctc 53.451397 loss_rnnt 24.787870 hw_loss 0.294235 lr 0.00060064 rank 6
2023-02-17 18:09:39,210 DEBUG TRAIN Batch 8/2600 loss 31.648130 loss_att 40.016788 loss_ctc 46.873283 loss_rnnt 27.791618 hw_loss 0.286420 lr 0.00060054 rank 1
2023-02-17 18:09:39,211 DEBUG TRAIN Batch 8/2600 loss 11.724347 loss_att 13.305269 loss_ctc 17.513163 loss_rnnt 10.472974 hw_loss 0.306275 lr 0.00060043 rank 0
2023-02-17 18:10:56,164 DEBUG TRAIN Batch 8/2700 loss 12.126972 loss_att 17.466602 loss_ctc 19.798645 loss_rnnt 9.905919 hw_loss 0.244196 lr 0.00060011 rank 1
2023-02-17 18:10:56,166 DEBUG TRAIN Batch 8/2700 loss 24.182165 loss_att 28.790092 loss_ctc 39.531265 loss_rnnt 21.056335 hw_loss 0.295682 lr 0.00060021 rank 7
2023-02-17 18:10:56,169 DEBUG TRAIN Batch 8/2700 loss 15.442634 loss_att 24.739374 loss_ctc 31.000463 loss_rnnt 11.363154 hw_loss 0.273289 lr 0.00060021 rank 6
2023-02-17 18:10:56,170 DEBUG TRAIN Batch 8/2700 loss 10.790258 loss_att 16.700056 loss_ctc 18.105780 loss_rnnt 8.472301 hw_loss 0.301116 lr 0.00060016 rank 5
2023-02-17 18:10:56,171 DEBUG TRAIN Batch 8/2700 loss 24.333244 loss_att 30.082714 loss_ctc 44.977184 loss_rnnt 20.278091 hw_loss 0.286372 lr 0.00060019 rank 3
2023-02-17 18:10:56,177 DEBUG TRAIN Batch 8/2700 loss 15.095878 loss_att 21.150187 loss_ctc 27.138088 loss_rnnt 12.100665 hw_loss 0.335105 lr 0.00060031 rank 4
2023-02-17 18:10:56,177 DEBUG TRAIN Batch 8/2700 loss 14.911327 loss_att 17.238256 loss_ctc 26.213146 loss_rnnt 12.736481 hw_loss 0.379783 lr 0.00059977 rank 2
2023-02-17 18:10:56,178 DEBUG TRAIN Batch 8/2700 loss 13.324490 loss_att 18.416344 loss_ctc 24.458858 loss_rnnt 10.635139 hw_loss 0.349493 lr 0.00060000 rank 0
2023-02-17 18:12:15,519 DEBUG TRAIN Batch 8/2800 loss 18.605028 loss_att 21.650915 loss_ctc 25.993786 loss_rnnt 16.815464 hw_loss 0.366035 lr 0.00059978 rank 7
2023-02-17 18:12:15,520 DEBUG TRAIN Batch 8/2800 loss 15.470840 loss_att 18.891083 loss_ctc 27.077026 loss_rnnt 13.038241 hw_loss 0.376987 lr 0.00059978 rank 6
2023-02-17 18:12:15,520 DEBUG TRAIN Batch 8/2800 loss 19.020292 loss_att 24.194462 loss_ctc 40.236282 loss_rnnt 14.991783 hw_loss 0.309144 lr 0.00059976 rank 3
2023-02-17 18:12:15,521 DEBUG TRAIN Batch 8/2800 loss 10.910546 loss_att 13.581864 loss_ctc 13.833251 loss_rnnt 9.814611 hw_loss 0.322456 lr 0.00059988 rank 4
2023-02-17 18:12:15,521 DEBUG TRAIN Batch 8/2800 loss 15.702106 loss_att 17.167953 loss_ctc 23.895473 loss_rnnt 14.154409 hw_loss 0.303897 lr 0.00059967 rank 1
2023-02-17 18:12:15,525 DEBUG TRAIN Batch 8/2800 loss 12.551755 loss_att 14.833405 loss_ctc 19.111485 loss_rnnt 11.071980 hw_loss 0.279028 lr 0.00059957 rank 0
2023-02-17 18:12:15,525 DEBUG TRAIN Batch 8/2800 loss 24.584978 loss_att 32.297981 loss_ctc 37.313385 loss_rnnt 21.171997 hw_loss 0.324858 lr 0.00059934 rank 2
2023-02-17 18:12:15,526 DEBUG TRAIN Batch 8/2800 loss 12.101566 loss_att 15.581457 loss_ctc 15.876966 loss_rnnt 10.723014 hw_loss 0.335974 lr 0.00059973 rank 5
2023-02-17 18:13:33,972 DEBUG TRAIN Batch 8/2900 loss 24.102510 loss_att 28.361494 loss_ctc 38.110165 loss_rnnt 21.195633 hw_loss 0.351363 lr 0.00059935 rank 6
2023-02-17 18:13:33,975 DEBUG TRAIN Batch 8/2900 loss 16.552622 loss_att 21.184511 loss_ctc 28.320259 loss_rnnt 13.832386 hw_loss 0.421573 lr 0.00059929 rank 5
2023-02-17 18:13:33,980 DEBUG TRAIN Batch 8/2900 loss 11.234591 loss_att 18.325359 loss_ctc 18.098564 loss_rnnt 8.717141 hw_loss 0.345187 lr 0.00059914 rank 0
2023-02-17 18:13:33,981 DEBUG TRAIN Batch 8/2900 loss 20.164551 loss_att 27.534182 loss_ctc 32.844765 loss_rnnt 16.857332 hw_loss 0.267366 lr 0.00059945 rank 4
2023-02-17 18:13:33,981 DEBUG TRAIN Batch 8/2900 loss 17.563093 loss_att 22.010889 loss_ctc 31.219952 loss_rnnt 14.649388 hw_loss 0.381057 lr 0.00059935 rank 7
2023-02-17 18:13:33,982 DEBUG TRAIN Batch 8/2900 loss 15.487101 loss_att 18.630398 loss_ctc 22.525787 loss_rnnt 13.764032 hw_loss 0.292345 lr 0.00059924 rank 1
2023-02-17 18:13:33,984 DEBUG TRAIN Batch 8/2900 loss 4.236443 loss_att 8.040882 loss_ctc 9.426466 loss_rnnt 2.605569 hw_loss 0.333719 lr 0.00059891 rank 2
2023-02-17 18:13:34,022 DEBUG TRAIN Batch 8/2900 loss 24.571934 loss_att 29.498085 loss_ctc 38.664280 loss_rnnt 21.500614 hw_loss 0.388326 lr 0.00059932 rank 3
2023-02-17 18:14:49,252 DEBUG TRAIN Batch 8/3000 loss 18.546019 loss_att 23.711887 loss_ctc 30.710615 loss_rnnt 15.736513 hw_loss 0.289470 lr 0.00059881 rank 1
2023-02-17 18:14:49,252 DEBUG TRAIN Batch 8/3000 loss 7.786269 loss_att 13.059823 loss_ctc 12.573956 loss_rnnt 5.908090 hw_loss 0.347081 lr 0.00059892 rank 7
2023-02-17 18:14:49,256 DEBUG TRAIN Batch 8/3000 loss 17.308258 loss_att 27.607014 loss_ctc 26.420626 loss_rnnt 13.897552 hw_loss 0.254945 lr 0.00059892 rank 6
2023-02-17 18:14:49,257 DEBUG TRAIN Batch 8/3000 loss 16.261158 loss_att 25.203304 loss_ctc 27.609972 loss_rnnt 12.811172 hw_loss 0.278210 lr 0.00059848 rank 2
2023-02-17 18:14:49,257 DEBUG TRAIN Batch 8/3000 loss 21.387783 loss_att 24.993774 loss_ctc 37.745480 loss_rnnt 18.291403 hw_loss 0.364048 lr 0.00059902 rank 4
2023-02-17 18:14:49,257 DEBUG TRAIN Batch 8/3000 loss 20.158978 loss_att 27.593201 loss_ctc 42.100826 loss_rnnt 15.567140 hw_loss 0.336399 lr 0.00059871 rank 0
2023-02-17 18:14:49,259 DEBUG TRAIN Batch 8/3000 loss 8.194575 loss_att 11.161990 loss_ctc 13.275908 loss_rnnt 6.710406 hw_loss 0.399705 lr 0.00059886 rank 5
2023-02-17 18:14:49,260 DEBUG TRAIN Batch 8/3000 loss 11.837341 loss_att 16.246107 loss_ctc 21.268103 loss_rnnt 9.494023 hw_loss 0.382742 lr 0.00059889 rank 3
2023-02-17 18:16:07,431 DEBUG TRAIN Batch 8/3100 loss 15.617320 loss_att 17.230286 loss_ctc 22.946381 loss_rnnt 14.063960 hw_loss 0.475423 lr 0.00059849 rank 7
2023-02-17 18:16:07,437 DEBUG TRAIN Batch 8/3100 loss 17.205620 loss_att 20.819197 loss_ctc 27.284723 loss_rnnt 14.929946 hw_loss 0.392022 lr 0.00059844 rank 5
2023-02-17 18:16:07,437 DEBUG TRAIN Batch 8/3100 loss 18.038252 loss_att 23.073267 loss_ctc 31.910046 loss_rnnt 15.012147 hw_loss 0.317865 lr 0.00059828 rank 0
2023-02-17 18:16:07,437 DEBUG TRAIN Batch 8/3100 loss 19.861990 loss_att 22.025852 loss_ctc 25.990290 loss_rnnt 18.448896 hw_loss 0.306024 lr 0.00059849 rank 6
2023-02-17 18:16:07,438 DEBUG TRAIN Batch 8/3100 loss 14.523810 loss_att 15.656229 loss_ctc 26.295288 loss_rnnt 12.594673 hw_loss 0.249604 lr 0.00059805 rank 2
2023-02-17 18:16:07,440 DEBUG TRAIN Batch 8/3100 loss 15.468058 loss_att 17.518774 loss_ctc 25.144293 loss_rnnt 13.509846 hw_loss 0.483570 lr 0.00059838 rank 1
2023-02-17 18:16:07,442 DEBUG TRAIN Batch 8/3100 loss 8.812325 loss_att 12.813957 loss_ctc 19.400177 loss_rnnt 6.465524 hw_loss 0.252678 lr 0.00059859 rank 4
2023-02-17 18:16:07,485 DEBUG TRAIN Batch 8/3100 loss 15.772128 loss_att 18.075321 loss_ctc 21.730753 loss_rnnt 14.278961 hw_loss 0.446333 lr 0.00059847 rank 3
2023-02-17 18:17:27,369 DEBUG TRAIN Batch 8/3200 loss 13.663230 loss_att 13.410423 loss_ctc 19.554983 loss_rnnt 12.721761 hw_loss 0.387117 lr 0.00059806 rank 6
2023-02-17 18:17:27,372 DEBUG TRAIN Batch 8/3200 loss 13.537189 loss_att 16.330595 loss_ctc 24.620359 loss_rnnt 11.291104 hw_loss 0.393089 lr 0.00059763 rank 2
2023-02-17 18:17:27,372 DEBUG TRAIN Batch 8/3200 loss 14.639641 loss_att 16.237900 loss_ctc 19.079481 loss_rnnt 13.567508 hw_loss 0.300940 lr 0.00059801 rank 5
2023-02-17 18:17:27,373 DEBUG TRAIN Batch 8/3200 loss 9.761518 loss_att 15.738396 loss_ctc 21.635971 loss_rnnt 6.774080 hw_loss 0.391502 lr 0.00059804 rank 3
2023-02-17 18:17:27,375 DEBUG TRAIN Batch 8/3200 loss 17.449999 loss_att 17.823938 loss_ctc 30.578970 loss_rnnt 15.409338 hw_loss 0.403766 lr 0.00059816 rank 4
2023-02-17 18:17:27,375 DEBUG TRAIN Batch 8/3200 loss 16.829790 loss_att 18.962648 loss_ctc 31.810982 loss_rnnt 14.247546 hw_loss 0.296588 lr 0.00059796 rank 1
2023-02-17 18:17:27,376 DEBUG TRAIN Batch 8/3200 loss 17.006252 loss_att 17.208935 loss_ctc 20.997122 loss_rnnt 16.278934 hw_loss 0.289997 lr 0.00059806 rank 7
2023-02-17 18:17:27,376 DEBUG TRAIN Batch 8/3200 loss 18.570868 loss_att 21.445377 loss_ctc 27.720549 loss_rnnt 16.573233 hw_loss 0.380207 lr 0.00059785 rank 0
2023-02-17 18:18:44,033 DEBUG TRAIN Batch 8/3300 loss 16.179455 loss_att 19.774380 loss_ctc 22.764431 loss_rnnt 14.431849 hw_loss 0.282420 lr 0.00059763 rank 6
2023-02-17 18:18:44,035 DEBUG TRAIN Batch 8/3300 loss 16.207056 loss_att 17.677357 loss_ctc 30.852688 loss_rnnt 13.766136 hw_loss 0.363955 lr 0.00059764 rank 7
2023-02-17 18:18:44,034 DEBUG TRAIN Batch 8/3300 loss 16.653696 loss_att 21.057535 loss_ctc 25.343689 loss_rnnt 14.375129 hw_loss 0.448375 lr 0.00059758 rank 5
2023-02-17 18:18:44,037 DEBUG TRAIN Batch 8/3300 loss 8.268066 loss_att 8.996885 loss_ctc 13.504419 loss_rnnt 7.226384 hw_loss 0.370760 lr 0.00059720 rank 2
2023-02-17 18:18:44,037 DEBUG TRAIN Batch 8/3300 loss 19.741983 loss_att 20.929276 loss_ctc 24.566006 loss_rnnt 18.735731 hw_loss 0.235488 lr 0.00059753 rank 1
2023-02-17 18:18:44,039 DEBUG TRAIN Batch 8/3300 loss 14.203888 loss_att 20.041079 loss_ctc 29.574032 loss_rnnt 10.808661 hw_loss 0.334570 lr 0.00059773 rank 4
2023-02-17 18:18:44,042 DEBUG TRAIN Batch 8/3300 loss 13.534181 loss_att 16.305653 loss_ctc 25.641495 loss_rnnt 11.191555 hw_loss 0.326292 lr 0.00059761 rank 3
2023-02-17 18:18:44,046 DEBUG TRAIN Batch 8/3300 loss 13.963477 loss_att 17.283573 loss_ctc 24.663425 loss_rnnt 11.729515 hw_loss 0.268654 lr 0.00059742 rank 0
2023-02-17 18:19:59,893 DEBUG TRAIN Batch 8/3400 loss 12.910980 loss_att 14.878528 loss_ctc 22.751287 loss_rnnt 11.074112 hw_loss 0.246220 lr 0.00059720 rank 6
2023-02-17 18:19:59,906 DEBUG TRAIN Batch 8/3400 loss 18.181551 loss_att 20.715958 loss_ctc 29.333958 loss_rnnt 15.991735 hw_loss 0.367401 lr 0.00059710 rank 1
2023-02-17 18:19:59,908 DEBUG TRAIN Batch 8/3400 loss 11.776989 loss_att 18.023798 loss_ctc 23.953421 loss_rnnt 8.689245 hw_loss 0.402857 lr 0.00059715 rank 5
2023-02-17 18:19:59,909 DEBUG TRAIN Batch 8/3400 loss 16.196945 loss_att 17.028923 loss_ctc 19.720051 loss_rnnt 15.315657 hw_loss 0.459652 lr 0.00059730 rank 4
2023-02-17 18:19:59,910 DEBUG TRAIN Batch 8/3400 loss 8.765306 loss_att 13.887564 loss_ctc 17.525049 loss_rnnt 6.335956 hw_loss 0.444249 lr 0.00059718 rank 3
2023-02-17 18:19:59,910 DEBUG TRAIN Batch 8/3400 loss 7.733605 loss_att 9.931744 loss_ctc 14.976607 loss_rnnt 6.137624 hw_loss 0.357412 lr 0.00059721 rank 7
2023-02-17 18:19:59,910 DEBUG TRAIN Batch 8/3400 loss 9.462277 loss_att 14.093152 loss_ctc 16.658651 loss_rnnt 7.342734 hw_loss 0.438473 lr 0.00059678 rank 2
2023-02-17 18:19:59,912 DEBUG TRAIN Batch 8/3400 loss 14.240277 loss_att 20.619703 loss_ctc 32.296829 loss_rnnt 10.400091 hw_loss 0.293927 lr 0.00059700 rank 0
2023-02-17 18:21:17,699 DEBUG TRAIN Batch 8/3500 loss 14.462831 loss_att 18.719908 loss_ctc 23.388876 loss_rnnt 12.307135 hw_loss 0.214016 lr 0.00059678 rank 6
2023-02-17 18:21:17,699 DEBUG TRAIN Batch 8/3500 loss 14.004213 loss_att 16.079214 loss_ctc 19.325516 loss_rnnt 12.715166 hw_loss 0.308511 lr 0.00059673 rank 5
2023-02-17 18:21:17,699 DEBUG TRAIN Batch 8/3500 loss 20.417130 loss_att 22.024368 loss_ctc 29.634827 loss_rnnt 18.721159 hw_loss 0.272810 lr 0.00059657 rank 0
2023-02-17 18:21:17,699 DEBUG TRAIN Batch 8/3500 loss 26.088856 loss_att 33.091667 loss_ctc 43.957985 loss_rnnt 22.061092 hw_loss 0.458723 lr 0.00059678 rank 7
2023-02-17 18:21:17,699 DEBUG TRAIN Batch 8/3500 loss 13.937346 loss_att 19.773636 loss_ctc 20.896675 loss_rnnt 11.657295 hw_loss 0.346654 lr 0.00059688 rank 4
2023-02-17 18:21:17,700 DEBUG TRAIN Batch 8/3500 loss 10.210382 loss_att 13.683966 loss_ctc 16.583160 loss_rnnt 8.515636 hw_loss 0.281858 lr 0.00059676 rank 3
2023-02-17 18:21:17,701 DEBUG TRAIN Batch 8/3500 loss 7.475451 loss_att 11.391792 loss_ctc 11.683581 loss_rnnt 5.971688 hw_loss 0.298893 lr 0.00059635 rank 2
2023-02-17 18:21:17,705 DEBUG TRAIN Batch 8/3500 loss 16.660376 loss_att 20.874693 loss_ctc 30.268814 loss_rnnt 13.837559 hw_loss 0.310304 lr 0.00059668 rank 1
2023-02-17 18:22:35,379 DEBUG TRAIN Batch 8/3600 loss 12.416674 loss_att 15.212238 loss_ctc 20.236496 loss_rnnt 10.677303 hw_loss 0.258027 lr 0.00059630 rank 5
2023-02-17 18:22:35,383 DEBUG TRAIN Batch 8/3600 loss 14.536814 loss_att 15.690643 loss_ctc 22.108166 loss_rnnt 13.113234 hw_loss 0.343691 lr 0.00059635 rank 6
2023-02-17 18:22:35,384 DEBUG TRAIN Batch 8/3600 loss 16.001511 loss_att 24.415310 loss_ctc 28.832060 loss_rnnt 12.360922 hw_loss 0.463291 lr 0.00059625 rank 1
2023-02-17 18:22:35,384 DEBUG TRAIN Batch 8/3600 loss 14.865914 loss_att 17.236805 loss_ctc 24.985846 loss_rnnt 12.908310 hw_loss 0.251442 lr 0.00059645 rank 4
2023-02-17 18:22:35,384 DEBUG TRAIN Batch 8/3600 loss 14.773390 loss_att 15.071354 loss_ctc 22.405621 loss_rnnt 13.531348 hw_loss 0.309032 lr 0.00059636 rank 7
2023-02-17 18:22:35,386 DEBUG TRAIN Batch 8/3600 loss 12.273179 loss_att 16.902283 loss_ctc 21.084396 loss_rnnt 10.005857 hw_loss 0.312510 lr 0.00059633 rank 3
2023-02-17 18:22:35,388 DEBUG TRAIN Batch 8/3600 loss 7.760616 loss_att 11.706812 loss_ctc 12.958694 loss_rnnt 6.086954 hw_loss 0.358774 lr 0.00059593 rank 2
2023-02-17 18:22:35,389 DEBUG TRAIN Batch 8/3600 loss 12.484220 loss_att 15.903593 loss_ctc 19.900333 loss_rnnt 10.660535 hw_loss 0.283114 lr 0.00059615 rank 0
2023-02-17 18:23:51,146 DEBUG TRAIN Batch 8/3700 loss 23.971977 loss_att 30.401180 loss_ctc 35.084579 loss_rnnt 21.096584 hw_loss 0.202261 lr 0.00059593 rank 6
2023-02-17 18:23:51,154 DEBUG TRAIN Batch 8/3700 loss 7.958978 loss_att 11.848442 loss_ctc 15.559278 loss_rnnt 5.981975 hw_loss 0.348257 lr 0.00059594 rank 7
2023-02-17 18:23:51,155 DEBUG TRAIN Batch 8/3700 loss 17.649412 loss_att 16.401104 loss_ctc 23.645126 loss_rnnt 16.861374 hw_loss 0.446760 lr 0.00059591 rank 3
2023-02-17 18:23:51,155 DEBUG TRAIN Batch 8/3700 loss 14.284837 loss_att 19.824162 loss_ctc 26.617722 loss_rnnt 11.312500 hw_loss 0.412665 lr 0.00059588 rank 5
2023-02-17 18:23:51,158 DEBUG TRAIN Batch 8/3700 loss 6.570159 loss_att 11.820211 loss_ctc 12.368941 loss_rnnt 4.551316 hw_loss 0.366866 lr 0.00059572 rank 0
2023-02-17 18:23:51,160 DEBUG TRAIN Batch 8/3700 loss 14.425817 loss_att 14.257594 loss_ctc 21.081509 loss_rnnt 13.388754 hw_loss 0.343653 lr 0.00059583 rank 1
2023-02-17 18:23:51,161 DEBUG TRAIN Batch 8/3700 loss 15.168092 loss_att 17.771172 loss_ctc 22.677307 loss_rnnt 13.506220 hw_loss 0.262549 lr 0.00059603 rank 4
2023-02-17 18:23:51,197 DEBUG TRAIN Batch 8/3700 loss 24.820555 loss_att 28.138159 loss_ctc 39.548828 loss_rnnt 22.021299 hw_loss 0.322431 lr 0.00059550 rank 2
2023-02-17 18:25:08,705 DEBUG TRAIN Batch 8/3800 loss 13.874473 loss_att 19.447872 loss_ctc 23.492168 loss_rnnt 11.330235 hw_loss 0.275995 lr 0.00059541 rank 1
2023-02-17 18:25:08,706 DEBUG TRAIN Batch 8/3800 loss 9.412913 loss_att 13.504604 loss_ctc 20.605371 loss_rnnt 6.936195 hw_loss 0.311347 lr 0.00059549 rank 3
2023-02-17 18:25:08,706 DEBUG TRAIN Batch 8/3800 loss 19.136703 loss_att 22.160492 loss_ctc 38.041348 loss_rnnt 15.806580 hw_loss 0.383900 lr 0.00059561 rank 4
2023-02-17 18:25:08,706 DEBUG TRAIN Batch 8/3800 loss 8.871234 loss_att 11.881544 loss_ctc 18.560923 loss_rnnt 6.784400 hw_loss 0.361527 lr 0.00059551 rank 7
2023-02-17 18:25:08,706 DEBUG TRAIN Batch 8/3800 loss 15.036345 loss_att 15.169136 loss_ctc 19.539497 loss_rnnt 14.192517 hw_loss 0.406589 lr 0.00059546 rank 5
2023-02-17 18:25:08,706 DEBUG TRAIN Batch 8/3800 loss 14.489678 loss_att 17.723936 loss_ctc 27.773878 loss_rnnt 11.860826 hw_loss 0.395199 lr 0.00059551 rank 6
2023-02-17 18:25:08,711 DEBUG TRAIN Batch 8/3800 loss 12.296092 loss_att 17.820723 loss_ctc 24.695189 loss_rnnt 9.367043 hw_loss 0.320455 lr 0.00059508 rank 2
2023-02-17 18:25:08,715 DEBUG TRAIN Batch 8/3800 loss 15.902348 loss_att 17.303074 loss_ctc 27.873129 loss_rnnt 13.825167 hw_loss 0.376745 lr 0.00059530 rank 0
2023-02-17 18:26:28,968 DEBUG TRAIN Batch 8/3900 loss 9.899317 loss_att 13.632174 loss_ctc 18.628147 loss_rnnt 7.902877 hw_loss 0.161295 lr 0.00059509 rank 7
2023-02-17 18:26:28,971 DEBUG TRAIN Batch 8/3900 loss 8.591736 loss_att 12.512695 loss_ctc 16.090221 loss_rnnt 6.574102 hw_loss 0.438082 lr 0.00059466 rank 2
2023-02-17 18:26:28,972 DEBUG TRAIN Batch 8/3900 loss 21.876137 loss_att 25.980270 loss_ctc 31.460804 loss_rnnt 19.615963 hw_loss 0.302605 lr 0.00059499 rank 1
2023-02-17 18:26:28,973 DEBUG TRAIN Batch 8/3900 loss 19.554953 loss_att 19.363535 loss_ctc 26.046589 loss_rnnt 18.514965 hw_loss 0.398852 lr 0.00059488 rank 0
2023-02-17 18:26:28,974 DEBUG TRAIN Batch 8/3900 loss 22.412981 loss_att 24.765158 loss_ctc 34.701668 loss_rnnt 20.100626 hw_loss 0.381429 lr 0.00059504 rank 5
2023-02-17 18:26:28,974 DEBUG TRAIN Batch 8/3900 loss 14.578652 loss_att 19.822109 loss_ctc 31.093201 loss_rnnt 11.152405 hw_loss 0.329282 lr 0.00059507 rank 3
2023-02-17 18:26:28,975 DEBUG TRAIN Batch 8/3900 loss 5.071019 loss_att 6.114642 loss_ctc 7.450731 loss_rnnt 4.375232 hw_loss 0.318313 lr 0.00059509 rank 6
2023-02-17 18:26:28,985 DEBUG TRAIN Batch 8/3900 loss 6.542748 loss_att 6.910407 loss_ctc 9.834165 loss_rnnt 5.762191 hw_loss 0.502818 lr 0.00059518 rank 4
2023-02-17 18:27:44,681 DEBUG TRAIN Batch 8/4000 loss 3.712641 loss_att 9.574854 loss_ctc 6.164043 loss_rnnt 2.069663 hw_loss 0.269404 lr 0.00059467 rank 6
2023-02-17 18:27:44,686 DEBUG TRAIN Batch 8/4000 loss 15.853284 loss_att 21.709763 loss_ctc 28.763081 loss_rnnt 12.812725 hw_loss 0.277417 lr 0.00059467 rank 7
2023-02-17 18:27:44,690 DEBUG TRAIN Batch 8/4000 loss 22.794909 loss_att 23.041780 loss_ctc 28.385845 loss_rnnt 21.876554 hw_loss 0.231605 lr 0.00059461 rank 5
2023-02-17 18:27:44,692 DEBUG TRAIN Batch 8/4000 loss 14.340311 loss_att 16.624033 loss_ctc 23.945839 loss_rnnt 12.430797 hw_loss 0.322562 lr 0.00059424 rank 2
2023-02-17 18:27:44,694 DEBUG TRAIN Batch 8/4000 loss 15.864962 loss_att 18.633728 loss_ctc 28.072279 loss_rnnt 13.458576 hw_loss 0.421856 lr 0.00059464 rank 3
2023-02-17 18:27:44,694 DEBUG TRAIN Batch 8/4000 loss 2.804047 loss_att 6.688210 loss_ctc 3.548119 loss_rnnt 1.809233 hw_loss 0.222696 lr 0.00059446 rank 0
2023-02-17 18:27:44,694 DEBUG TRAIN Batch 8/4000 loss 16.132160 loss_att 18.350384 loss_ctc 21.104851 loss_rnnt 14.889010 hw_loss 0.255900 lr 0.00059456 rank 1
2023-02-17 18:27:44,695 DEBUG TRAIN Batch 8/4000 loss 15.780792 loss_att 17.311911 loss_ctc 21.484024 loss_rnnt 14.502928 hw_loss 0.396020 lr 0.00059476 rank 4
2023-02-17 18:29:00,817 DEBUG TRAIN Batch 8/4100 loss 8.290649 loss_att 10.514480 loss_ctc 12.492449 loss_rnnt 7.106981 hw_loss 0.334989 lr 0.00059425 rank 7
2023-02-17 18:29:00,819 DEBUG TRAIN Batch 8/4100 loss 27.753286 loss_att 32.850079 loss_ctc 43.206261 loss_rnnt 24.526464 hw_loss 0.275751 lr 0.00059425 rank 6
2023-02-17 18:29:00,824 DEBUG TRAIN Batch 8/4100 loss 22.005552 loss_att 25.516623 loss_ctc 36.845814 loss_rnnt 19.208496 hw_loss 0.217762 lr 0.00059414 rank 1
2023-02-17 18:29:00,826 DEBUG TRAIN Batch 8/4100 loss 16.171730 loss_att 19.088863 loss_ctc 20.100845 loss_rnnt 14.891777 hw_loss 0.323711 lr 0.00059404 rank 0
2023-02-17 18:29:00,828 DEBUG TRAIN Batch 8/4100 loss 8.920876 loss_att 12.875865 loss_ctc 16.398014 loss_rnnt 7.006957 hw_loss 0.236191 lr 0.00059422 rank 3
2023-02-17 18:29:00,829 DEBUG TRAIN Batch 8/4100 loss 28.605396 loss_att 32.101070 loss_ctc 48.437817 loss_rnnt 25.120491 hw_loss 0.265212 lr 0.00059419 rank 5
2023-02-17 18:29:00,830 DEBUG TRAIN Batch 8/4100 loss 18.842295 loss_att 23.155632 loss_ctc 31.905478 loss_rnnt 16.075727 hw_loss 0.304017 lr 0.00059382 rank 2
2023-02-17 18:29:00,875 DEBUG TRAIN Batch 8/4100 loss 21.744659 loss_att 31.531954 loss_ctc 38.515144 loss_rnnt 17.429836 hw_loss 0.227435 lr 0.00059434 rank 4
2023-02-17 18:30:19,148 DEBUG TRAIN Batch 8/4200 loss 14.466246 loss_att 18.651548 loss_ctc 23.326668 loss_rnnt 12.306436 hw_loss 0.265048 lr 0.00059373 rank 1
2023-02-17 18:30:19,148 DEBUG TRAIN Batch 8/4200 loss 9.800773 loss_att 15.703943 loss_ctc 17.870731 loss_rnnt 7.372231 hw_loss 0.322336 lr 0.00059340 rank 2
2023-02-17 18:30:19,148 DEBUG TRAIN Batch 8/4200 loss 14.793787 loss_att 17.710932 loss_ctc 21.153812 loss_rnnt 13.260153 hw_loss 0.191630 lr 0.00059383 rank 6
2023-02-17 18:30:19,150 DEBUG TRAIN Batch 8/4200 loss 27.275381 loss_att 27.310696 loss_ctc 40.608692 loss_rnnt 25.281731 hw_loss 0.391525 lr 0.00059380 rank 3
2023-02-17 18:30:19,150 DEBUG TRAIN Batch 8/4200 loss 25.754856 loss_att 27.312534 loss_ctc 35.888283 loss_rnnt 23.909634 hw_loss 0.342308 lr 0.00059392 rank 4
2023-02-17 18:30:19,181 DEBUG TRAIN Batch 8/4200 loss 17.682829 loss_att 21.261585 loss_ctc 28.268747 loss_rnnt 15.410682 hw_loss 0.271765 lr 0.00059383 rank 7
2023-02-17 18:30:19,187 DEBUG TRAIN Batch 8/4200 loss 9.530457 loss_att 12.720251 loss_ctc 18.420921 loss_rnnt 7.524150 hw_loss 0.343037 lr 0.00059378 rank 5
2023-02-17 18:30:19,197 DEBUG TRAIN Batch 8/4200 loss 11.103440 loss_att 14.264763 loss_ctc 24.579449 loss_rnnt 8.512014 hw_loss 0.304428 lr 0.00059362 rank 0
2023-02-17 18:31:38,260 DEBUG TRAIN Batch 8/4300 loss 14.725484 loss_att 17.599268 loss_ctc 25.064205 loss_rnnt 12.628813 hw_loss 0.268909 lr 0.00059341 rank 6
2023-02-17 18:31:38,261 DEBUG TRAIN Batch 8/4300 loss 16.153934 loss_att 20.243645 loss_ctc 25.764463 loss_rnnt 13.831263 hw_loss 0.418735 lr 0.00059331 rank 1
2023-02-17 18:31:38,264 DEBUG TRAIN Batch 8/4300 loss 21.050062 loss_att 25.734896 loss_ctc 33.283443 loss_rnnt 18.328430 hw_loss 0.287901 lr 0.00059299 rank 2
2023-02-17 18:31:38,264 DEBUG TRAIN Batch 8/4300 loss 9.185289 loss_att 12.168606 loss_ctc 13.630878 loss_rnnt 7.805311 hw_loss 0.357317 lr 0.00059341 rank 7
2023-02-17 18:31:38,266 DEBUG TRAIN Batch 8/4300 loss 22.061260 loss_att 23.945602 loss_ctc 36.325531 loss_rnnt 19.590496 hw_loss 0.359990 lr 0.00059336 rank 5
2023-02-17 18:31:38,267 DEBUG TRAIN Batch 8/4300 loss 17.831497 loss_att 22.949265 loss_ctc 26.976637 loss_rnnt 15.452715 hw_loss 0.254766 lr 0.00059350 rank 4
2023-02-17 18:31:38,268 DEBUG TRAIN Batch 8/4300 loss 20.974741 loss_att 26.679810 loss_ctc 28.255898 loss_rnnt 18.684788 hw_loss 0.333970 lr 0.00059320 rank 0
2023-02-17 18:31:38,307 DEBUG TRAIN Batch 8/4300 loss 22.752518 loss_att 24.408861 loss_ctc 29.799408 loss_rnnt 21.326714 hw_loss 0.290532 lr 0.00059339 rank 3
2023-02-17 18:32:54,775 DEBUG TRAIN Batch 8/4400 loss 16.793547 loss_att 22.168007 loss_ctc 31.230280 loss_rnnt 13.606104 hw_loss 0.351847 lr 0.00059294 rank 5
2023-02-17 18:32:54,777 DEBUG TRAIN Batch 8/4400 loss 8.280210 loss_att 12.164472 loss_ctc 16.107754 loss_rnnt 6.281735 hw_loss 0.333658 lr 0.00059309 rank 4
2023-02-17 18:32:54,777 DEBUG TRAIN Batch 8/4400 loss 23.588064 loss_att 24.222584 loss_ctc 29.480633 loss_rnnt 22.433918 hw_loss 0.452935 lr 0.00059257 rank 2
2023-02-17 18:32:54,777 DEBUG TRAIN Batch 8/4400 loss 11.965972 loss_att 12.648548 loss_ctc 15.211310 loss_rnnt 11.198478 hw_loss 0.371752 lr 0.00059299 rank 6
2023-02-17 18:32:54,778 DEBUG TRAIN Batch 8/4400 loss 11.731774 loss_att 12.269943 loss_ctc 16.503294 loss_rnnt 10.815852 hw_loss 0.322663 lr 0.00059299 rank 7
2023-02-17 18:32:54,780 DEBUG TRAIN Batch 8/4400 loss 13.678994 loss_att 19.974670 loss_ctc 32.045494 loss_rnnt 9.852275 hw_loss 0.222595 lr 0.00059289 rank 1
2023-02-17 18:32:54,781 DEBUG TRAIN Batch 8/4400 loss 31.324970 loss_att 35.321110 loss_ctc 43.403908 loss_rnnt 28.742010 hw_loss 0.324766 lr 0.00059297 rank 3
2023-02-17 18:32:54,783 DEBUG TRAIN Batch 8/4400 loss 13.315381 loss_att 17.538303 loss_ctc 19.991863 loss_rnnt 11.443494 hw_loss 0.257071 lr 0.00059279 rank 0
2023-02-17 18:34:09,895 DEBUG TRAIN Batch 8/4500 loss 17.345331 loss_att 20.861620 loss_ctc 30.815588 loss_rnnt 14.692717 hw_loss 0.287480 lr 0.00059255 rank 3
2023-02-17 18:34:09,899 DEBUG TRAIN Batch 8/4500 loss 26.869799 loss_att 31.780552 loss_ctc 47.940929 loss_rnnt 22.846691 hw_loss 0.434015 lr 0.00059258 rank 7
2023-02-17 18:34:09,902 DEBUG TRAIN Batch 8/4500 loss 13.436646 loss_att 13.863685 loss_ctc 18.861492 loss_rnnt 12.511901 hw_loss 0.217544 lr 0.00059252 rank 5
2023-02-17 18:34:09,903 DEBUG TRAIN Batch 8/4500 loss 8.903875 loss_att 8.559389 loss_ctc 11.628141 loss_rnnt 8.441356 hw_loss 0.315341 lr 0.00059257 rank 6
2023-02-17 18:34:09,903 DEBUG TRAIN Batch 8/4500 loss 8.106308 loss_att 11.520323 loss_ctc 12.582547 loss_rnnt 6.648840 hw_loss 0.333435 lr 0.00059215 rank 2
2023-02-17 18:34:09,905 DEBUG TRAIN Batch 8/4500 loss 24.207935 loss_att 24.325178 loss_ctc 42.794159 loss_rnnt 21.558294 hw_loss 0.277558 lr 0.00059247 rank 1
2023-02-17 18:34:09,908 DEBUG TRAIN Batch 8/4500 loss 12.703814 loss_att 15.486027 loss_ctc 22.466925 loss_rnnt 10.672401 hw_loss 0.324789 lr 0.00059237 rank 0
2023-02-17 18:34:09,910 DEBUG TRAIN Batch 8/4500 loss 15.088309 loss_att 16.256821 loss_ctc 27.300087 loss_rnnt 13.017666 hw_loss 0.391320 lr 0.00059267 rank 4
2023-02-17 18:35:29,800 DEBUG TRAIN Batch 8/4600 loss 13.353922 loss_att 17.697931 loss_ctc 22.460968 loss_rnnt 11.098617 hw_loss 0.322932 lr 0.00059216 rank 6
2023-02-17 18:35:29,804 DEBUG TRAIN Batch 8/4600 loss 15.746959 loss_att 19.577969 loss_ctc 21.074818 loss_rnnt 14.070320 hw_loss 0.375104 lr 0.00059195 rank 0
2023-02-17 18:35:29,806 DEBUG TRAIN Batch 8/4600 loss 15.365859 loss_att 17.466068 loss_ctc 20.462944 loss_rnnt 14.096697 hw_loss 0.317830 lr 0.00059214 rank 3
2023-02-17 18:35:29,806 DEBUG TRAIN Batch 8/4600 loss 15.226239 loss_att 16.753223 loss_ctc 26.855553 loss_rnnt 13.189981 hw_loss 0.338035 lr 0.00059206 rank 1
2023-02-17 18:35:29,814 DEBUG TRAIN Batch 8/4600 loss 6.808780 loss_att 14.206241 loss_ctc 10.711432 loss_rnnt 4.638262 hw_loss 0.320010 lr 0.00059225 rank 4
2023-02-17 18:35:29,820 DEBUG TRAIN Batch 8/4600 loss 17.752632 loss_att 22.344307 loss_ctc 33.777447 loss_rnnt 14.515834 hw_loss 0.340914 lr 0.00059174 rank 2
2023-02-17 18:35:29,826 DEBUG TRAIN Batch 8/4600 loss 9.640457 loss_att 13.824224 loss_ctc 15.646433 loss_rnnt 7.778252 hw_loss 0.421228 lr 0.00059211 rank 5
2023-02-17 18:35:29,828 DEBUG TRAIN Batch 8/4600 loss 15.249718 loss_att 21.343159 loss_ctc 28.795853 loss_rnnt 12.063694 hw_loss 0.302220 lr 0.00059216 rank 7
2023-02-17 18:36:46,420 DEBUG TRAIN Batch 8/4700 loss 9.091662 loss_att 13.365273 loss_ctc 14.840423 loss_rnnt 7.291026 hw_loss 0.336400 lr 0.00059174 rank 6
2023-02-17 18:36:46,423 DEBUG TRAIN Batch 8/4700 loss 19.368530 loss_att 22.341774 loss_ctc 29.110092 loss_rnnt 17.294445 hw_loss 0.338549 lr 0.00059175 rank 7
2023-02-17 18:36:46,423 DEBUG TRAIN Batch 8/4700 loss 14.593008 loss_att 16.951754 loss_ctc 22.669783 loss_rnnt 12.881982 hw_loss 0.304451 lr 0.00059172 rank 3
2023-02-17 18:36:46,423 DEBUG TRAIN Batch 8/4700 loss 21.019352 loss_att 25.371510 loss_ctc 26.482870 loss_rnnt 19.246765 hw_loss 0.325662 lr 0.00059164 rank 1
2023-02-17 18:36:46,425 DEBUG TRAIN Batch 8/4700 loss 11.233515 loss_att 14.509365 loss_ctc 18.236252 loss_rnnt 9.438566 hw_loss 0.386400 lr 0.00059184 rank 4
2023-02-17 18:36:46,426 DEBUG TRAIN Batch 8/4700 loss 13.433197 loss_att 18.479836 loss_ctc 24.631163 loss_rnnt 10.757908 hw_loss 0.324184 lr 0.00059154 rank 0
2023-02-17 18:36:46,426 DEBUG TRAIN Batch 8/4700 loss 11.657337 loss_att 16.870333 loss_ctc 19.924051 loss_rnnt 9.371557 hw_loss 0.264285 lr 0.00059169 rank 5
2023-02-17 18:36:46,429 DEBUG TRAIN Batch 8/4700 loss 31.970459 loss_att 38.329533 loss_ctc 49.928413 loss_rnnt 28.166815 hw_loss 0.257690 lr 0.00059132 rank 2
2023-02-17 18:38:02,796 DEBUG TRAIN Batch 8/4800 loss 17.160194 loss_att 21.585732 loss_ctc 31.199011 loss_rnnt 14.212193 hw_loss 0.358220 lr 0.00059131 rank 3
2023-02-17 18:38:02,796 DEBUG TRAIN Batch 8/4800 loss 11.507016 loss_att 15.109800 loss_ctc 17.287037 loss_rnnt 9.798923 hw_loss 0.406625 lr 0.00059133 rank 6
2023-02-17 18:38:02,797 DEBUG TRAIN Batch 8/4800 loss 28.555828 loss_att 30.496090 loss_ctc 45.788593 loss_rnnt 25.678108 hw_loss 0.359934 lr 0.00059128 rank 5
2023-02-17 18:38:02,798 DEBUG TRAIN Batch 8/4800 loss 9.694343 loss_att 13.898005 loss_ctc 19.576284 loss_rnnt 7.369367 hw_loss 0.312472 lr 0.00059142 rank 4
2023-02-17 18:38:02,798 DEBUG TRAIN Batch 8/4800 loss 13.650185 loss_att 15.792103 loss_ctc 26.288189 loss_rnnt 11.328541 hw_loss 0.390360 lr 0.00059133 rank 7
2023-02-17 18:38:02,802 DEBUG TRAIN Batch 8/4800 loss 14.553462 loss_att 19.796949 loss_ctc 24.177273 loss_rnnt 12.044544 hw_loss 0.331960 lr 0.00059123 rank 1
2023-02-17 18:38:02,805 DEBUG TRAIN Batch 8/4800 loss 13.734860 loss_att 17.327209 loss_ctc 19.048021 loss_rnnt 12.175138 hw_loss 0.249059 lr 0.00059091 rank 2
2023-02-17 18:38:02,846 DEBUG TRAIN Batch 8/4800 loss 14.347817 loss_att 19.185326 loss_ctc 25.441799 loss_rnnt 11.756367 hw_loss 0.271409 lr 0.00059113 rank 0
2023-02-17 18:39:19,710 DEBUG TRAIN Batch 8/4900 loss 15.473053 loss_att 21.962654 loss_ctc 23.952507 loss_rnnt 12.894867 hw_loss 0.280636 lr 0.00059050 rank 2
2023-02-17 18:39:19,710 DEBUG TRAIN Batch 8/4900 loss 12.759533 loss_att 15.639174 loss_ctc 16.864134 loss_rnnt 11.463182 hw_loss 0.324641 lr 0.00059092 rank 6
2023-02-17 18:39:19,711 DEBUG TRAIN Batch 8/4900 loss 13.559718 loss_att 17.815739 loss_ctc 21.712341 loss_rnnt 11.440606 hw_loss 0.339171 lr 0.00059092 rank 7
2023-02-17 18:39:19,713 DEBUG TRAIN Batch 8/4900 loss 10.472193 loss_att 12.084479 loss_ctc 16.663687 loss_rnnt 9.128932 hw_loss 0.366135 lr 0.00059090 rank 3
2023-02-17 18:39:19,714 DEBUG TRAIN Batch 8/4900 loss 16.976295 loss_att 18.643688 loss_ctc 30.547209 loss_rnnt 14.648767 hw_loss 0.346111 lr 0.00059082 rank 1
2023-02-17 18:39:19,714 DEBUG TRAIN Batch 8/4900 loss 17.030737 loss_att 18.303661 loss_ctc 28.633898 loss_rnnt 15.096849 hw_loss 0.247904 lr 0.00059071 rank 0
2023-02-17 18:39:19,716 DEBUG TRAIN Batch 8/4900 loss 7.382847 loss_att 12.530493 loss_ctc 12.081057 loss_rnnt 5.583542 hw_loss 0.268779 lr 0.00059087 rank 5
2023-02-17 18:39:19,723 DEBUG TRAIN Batch 8/4900 loss 17.077908 loss_att 20.831100 loss_ctc 31.295181 loss_rnnt 14.255308 hw_loss 0.330612 lr 0.00059101 rank 4
2023-02-17 18:40:38,674 DEBUG TRAIN Batch 8/5000 loss 11.488480 loss_att 16.827921 loss_ctc 18.169369 loss_rnnt 9.361620 hw_loss 0.315348 lr 0.00059060 rank 4
2023-02-17 18:40:38,674 DEBUG TRAIN Batch 8/5000 loss 10.370914 loss_att 13.531667 loss_ctc 17.658758 loss_rnnt 8.593272 hw_loss 0.325837 lr 0.00059045 rank 5
2023-02-17 18:40:38,676 DEBUG TRAIN Batch 8/5000 loss 15.334562 loss_att 21.055946 loss_ctc 26.030355 loss_rnnt 12.582876 hw_loss 0.339945 lr 0.00059040 rank 1
2023-02-17 18:40:38,678 DEBUG TRAIN Batch 8/5000 loss 22.893244 loss_att 22.745630 loss_ctc 34.374969 loss_rnnt 21.237141 hw_loss 0.290115 lr 0.00059030 rank 0
2023-02-17 18:40:38,679 DEBUG TRAIN Batch 8/5000 loss 17.839533 loss_att 23.162334 loss_ctc 29.350780 loss_rnnt 15.046325 hw_loss 0.363405 lr 0.00059009 rank 2
2023-02-17 18:40:38,679 DEBUG TRAIN Batch 8/5000 loss 16.898306 loss_att 20.627712 loss_ctc 29.053436 loss_rnnt 14.364011 hw_loss 0.314493 lr 0.00059050 rank 6
2023-02-17 18:40:38,680 DEBUG TRAIN Batch 8/5000 loss 11.149643 loss_att 12.467993 loss_ctc 17.780355 loss_rnnt 9.814058 hw_loss 0.352161 lr 0.00059051 rank 7
2023-02-17 18:40:38,723 DEBUG TRAIN Batch 8/5000 loss 13.994420 loss_att 15.730711 loss_ctc 19.583851 loss_rnnt 12.706642 hw_loss 0.366116 lr 0.00059048 rank 3
2023-02-17 18:41:55,738 DEBUG TRAIN Batch 8/5100 loss 7.824604 loss_att 9.832750 loss_ctc 12.839200 loss_rnnt 6.591043 hw_loss 0.306221 lr 0.00059009 rank 6
2023-02-17 18:41:55,740 DEBUG TRAIN Batch 8/5100 loss 10.244869 loss_att 18.983482 loss_ctc 17.686459 loss_rnnt 7.345998 hw_loss 0.298007 lr 0.00059007 rank 3
2023-02-17 18:41:55,740 DEBUG TRAIN Batch 8/5100 loss 10.848864 loss_att 11.442763 loss_ctc 15.783362 loss_rnnt 9.806320 hw_loss 0.498430 lr 0.00059004 rank 5
2023-02-17 18:41:55,742 DEBUG TRAIN Batch 8/5100 loss 22.452242 loss_att 23.279266 loss_ctc 32.937256 loss_rnnt 20.677088 hw_loss 0.397025 lr 0.00059010 rank 7
2023-02-17 18:41:55,744 DEBUG TRAIN Batch 8/5100 loss 11.008320 loss_att 15.646418 loss_ctc 21.569756 loss_rnnt 8.476246 hw_loss 0.367991 lr 0.00058968 rank 2
2023-02-17 18:41:55,745 DEBUG TRAIN Batch 8/5100 loss 5.534787 loss_att 8.584372 loss_ctc 11.117795 loss_rnnt 3.980980 hw_loss 0.374042 lr 0.00058989 rank 0
2023-02-17 18:41:55,748 DEBUG TRAIN Batch 8/5100 loss 6.249314 loss_att 10.585749 loss_ctc 13.661530 loss_rnnt 4.272507 hw_loss 0.227297 lr 0.00058999 rank 1
2023-02-17 18:41:55,785 DEBUG TRAIN Batch 8/5100 loss 9.668089 loss_att 11.686533 loss_ctc 18.079107 loss_rnnt 7.995683 hw_loss 0.276088 lr 0.00059019 rank 4
2023-02-17 18:43:13,435 DEBUG TRAIN Batch 8/5200 loss 20.920801 loss_att 24.088753 loss_ctc 32.300163 loss_rnnt 18.660568 hw_loss 0.205111 lr 0.00058968 rank 6
2023-02-17 18:43:13,441 DEBUG TRAIN Batch 8/5200 loss 18.591757 loss_att 14.953413 loss_ctc 22.968519 loss_rnnt 18.584290 hw_loss 0.284188 lr 0.00058969 rank 7
2023-02-17 18:43:13,441 DEBUG TRAIN Batch 8/5200 loss 17.397783 loss_att 21.053141 loss_ctc 31.288128 loss_rnnt 14.650381 hw_loss 0.308036 lr 0.00058963 rank 5
2023-02-17 18:43:13,442 DEBUG TRAIN Batch 8/5200 loss 11.051768 loss_att 14.997862 loss_ctc 19.627865 loss_rnnt 8.887148 hw_loss 0.434854 lr 0.00058958 rank 1
2023-02-17 18:43:13,442 DEBUG TRAIN Batch 8/5200 loss 9.295527 loss_att 14.180713 loss_ctc 17.777594 loss_rnnt 7.035519 hw_loss 0.285055 lr 0.00058978 rank 4
2023-02-17 18:43:13,446 DEBUG TRAIN Batch 8/5200 loss 16.177704 loss_att 19.731993 loss_ctc 27.014114 loss_rnnt 13.844017 hw_loss 0.333700 lr 0.00058948 rank 0
2023-02-17 18:43:13,461 DEBUG TRAIN Batch 8/5200 loss 8.399928 loss_att 9.902493 loss_ctc 14.333898 loss_rnnt 7.120692 hw_loss 0.351612 lr 0.00058927 rank 2
2023-02-17 18:43:13,468 DEBUG TRAIN Batch 8/5200 loss 18.463455 loss_att 21.316866 loss_ctc 32.944695 loss_rnnt 15.771921 hw_loss 0.356285 lr 0.00058966 rank 3
2023-02-17 18:44:31,928 DEBUG TRAIN Batch 8/5300 loss 16.783630 loss_att 20.516611 loss_ctc 22.619980 loss_rnnt 15.121355 hw_loss 0.257805 lr 0.00058925 rank 3
2023-02-17 18:44:31,929 DEBUG TRAIN Batch 8/5300 loss 13.815720 loss_att 19.679890 loss_ctc 29.582836 loss_rnnt 10.361166 hw_loss 0.336445 lr 0.00058928 rank 7
2023-02-17 18:44:31,930 DEBUG TRAIN Batch 8/5300 loss 17.940321 loss_att 25.845968 loss_ctc 34.088573 loss_rnnt 13.979305 hw_loss 0.425226 lr 0.00058922 rank 5
2023-02-17 18:44:31,931 DEBUG TRAIN Batch 8/5300 loss 21.246948 loss_att 24.530712 loss_ctc 26.410614 loss_rnnt 19.736935 hw_loss 0.308947 lr 0.00058907 rank 0
2023-02-17 18:44:31,931 DEBUG TRAIN Batch 8/5300 loss 12.857496 loss_att 16.418095 loss_ctc 20.895615 loss_rnnt 10.871141 hw_loss 0.379659 lr 0.00058937 rank 4
2023-02-17 18:44:31,931 DEBUG TRAIN Batch 8/5300 loss 8.921283 loss_att 11.598563 loss_ctc 14.479774 loss_rnnt 7.481569 hw_loss 0.305861 lr 0.00058917 rank 1
2023-02-17 18:44:31,934 DEBUG TRAIN Batch 8/5300 loss 23.298986 loss_att 23.164711 loss_ctc 42.418194 loss_rnnt 20.551968 hw_loss 0.421208 lr 0.00058927 rank 6
2023-02-17 18:44:31,938 DEBUG TRAIN Batch 8/5300 loss 22.617662 loss_att 24.252838 loss_ctc 35.149612 loss_rnnt 20.472672 hw_loss 0.275677 lr 0.00058886 rank 2
2023-02-17 18:45:49,569 DEBUG TRAIN Batch 8/5400 loss 11.104690 loss_att 16.643654 loss_ctc 22.278042 loss_rnnt 8.311562 hw_loss 0.366666 lr 0.00058877 rank 1
2023-02-17 18:45:49,569 DEBUG TRAIN Batch 8/5400 loss 11.831442 loss_att 11.988148 loss_ctc 16.874603 loss_rnnt 11.001375 hw_loss 0.236818 lr 0.00058886 rank 6
2023-02-17 18:45:49,570 DEBUG TRAIN Batch 8/5400 loss 15.659298 loss_att 19.763863 loss_ctc 25.534227 loss_rnnt 13.291008 hw_loss 0.432600 lr 0.00058884 rank 3
2023-02-17 18:45:49,570 DEBUG TRAIN Batch 8/5400 loss 26.518171 loss_att 27.951363 loss_ctc 38.767830 loss_rnnt 24.425865 hw_loss 0.323209 lr 0.00058881 rank 5
2023-02-17 18:45:49,573 DEBUG TRAIN Batch 8/5400 loss 14.379179 loss_att 18.573074 loss_ctc 23.851276 loss_rnnt 12.076152 hw_loss 0.377440 lr 0.00058887 rank 7
2023-02-17 18:45:49,573 DEBUG TRAIN Batch 8/5400 loss 15.685464 loss_att 22.113981 loss_ctc 31.678173 loss_rnnt 12.098558 hw_loss 0.316573 lr 0.00058866 rank 0
2023-02-17 18:45:49,577 DEBUG TRAIN Batch 8/5400 loss 15.012488 loss_att 21.923731 loss_ctc 31.164125 loss_rnnt 11.317453 hw_loss 0.298562 lr 0.00058896 rank 4
2023-02-17 18:45:49,621 DEBUG TRAIN Batch 8/5400 loss 15.518417 loss_att 20.326462 loss_ctc 16.737036 loss_rnnt 14.277674 hw_loss 0.218724 lr 0.00058845 rank 2
2023-02-17 18:47:07,359 DEBUG TRAIN Batch 8/5500 loss 21.225086 loss_att 23.536909 loss_ctc 29.080898 loss_rnnt 19.548195 hw_loss 0.313284 lr 0.00058846 rank 6
2023-02-17 18:47:07,361 DEBUG TRAIN Batch 8/5500 loss 9.890236 loss_att 14.383022 loss_ctc 17.310005 loss_rnnt 7.878926 hw_loss 0.231466 lr 0.00058843 rank 3
2023-02-17 18:47:07,364 DEBUG TRAIN Batch 8/5500 loss 15.239601 loss_att 18.797710 loss_ctc 31.296526 loss_rnnt 12.243870 hw_loss 0.268473 lr 0.00058804 rank 2
2023-02-17 18:47:07,364 DEBUG TRAIN Batch 8/5500 loss 21.780661 loss_att 26.067135 loss_ctc 35.200115 loss_rnnt 18.973677 hw_loss 0.300803 lr 0.00058836 rank 1
2023-02-17 18:47:07,367 DEBUG TRAIN Batch 8/5500 loss 10.548948 loss_att 15.096039 loss_ctc 22.412155 loss_rnnt 7.882140 hw_loss 0.329305 lr 0.00058841 rank 5
2023-02-17 18:47:07,369 DEBUG TRAIN Batch 8/5500 loss 16.007603 loss_att 21.101467 loss_ctc 19.889284 loss_rnnt 14.328107 hw_loss 0.268434 lr 0.00058826 rank 0
2023-02-17 18:47:07,369 DEBUG TRAIN Batch 8/5500 loss 18.698709 loss_att 20.404308 loss_ctc 28.718800 loss_rnnt 16.859196 hw_loss 0.304464 lr 0.00058855 rank 4
2023-02-17 18:47:07,370 DEBUG TRAIN Batch 8/5500 loss 16.157717 loss_att 21.218069 loss_ctc 29.139650 loss_rnnt 13.222149 hw_loss 0.361076 lr 0.00058846 rank 7
2023-02-17 18:48:25,741 DEBUG TRAIN Batch 8/5600 loss 13.881208 loss_att 14.701618 loss_ctc 19.428913 loss_rnnt 12.822126 hw_loss 0.291199 lr 0.00058795 rank 1
2023-02-17 18:48:25,742 DEBUG TRAIN Batch 8/5600 loss 6.881573 loss_att 9.482392 loss_ctc 11.099659 loss_rnnt 5.621505 hw_loss 0.332799 lr 0.00058803 rank 3
2023-02-17 18:48:25,743 DEBUG TRAIN Batch 8/5600 loss 6.635747 loss_att 9.407353 loss_ctc 13.372076 loss_rnnt 5.069450 hw_loss 0.213374 lr 0.00058805 rank 7
2023-02-17 18:48:25,743 DEBUG TRAIN Batch 8/5600 loss 5.306499 loss_att 8.890265 loss_ctc 7.267321 loss_rnnt 4.132107 hw_loss 0.367867 lr 0.00058805 rank 6
2023-02-17 18:48:25,744 DEBUG TRAIN Batch 8/5600 loss 10.777338 loss_att 14.289042 loss_ctc 16.645714 loss_rnnt 9.040839 hw_loss 0.471953 lr 0.00058800 rank 5
2023-02-17 18:48:25,747 DEBUG TRAIN Batch 8/5600 loss 18.464863 loss_att 20.157990 loss_ctc 30.324715 loss_rnnt 16.405117 hw_loss 0.262139 lr 0.00058814 rank 4
2023-02-17 18:48:25,748 DEBUG TRAIN Batch 8/5600 loss 20.601954 loss_att 23.507328 loss_ctc 34.596634 loss_rnnt 17.958061 hw_loss 0.369111 lr 0.00058764 rank 2
2023-02-17 18:48:25,791 DEBUG TRAIN Batch 8/5600 loss 14.581034 loss_att 18.366512 loss_ctc 23.618176 loss_rnnt 12.416321 hw_loss 0.379995 lr 0.00058785 rank 0
2023-02-17 18:49:45,983 DEBUG TRAIN Batch 8/5700 loss 12.397464 loss_att 14.325596 loss_ctc 21.233356 loss_rnnt 10.735741 hw_loss 0.183710 lr 0.00058762 rank 3
2023-02-17 18:49:45,984 DEBUG TRAIN Batch 8/5700 loss 5.397739 loss_att 7.401992 loss_ctc 8.899039 loss_rnnt 4.377668 hw_loss 0.285713 lr 0.00058774 rank 4
2023-02-17 18:49:45,986 DEBUG TRAIN Batch 8/5700 loss 4.327439 loss_att 7.236294 loss_ctc 8.485825 loss_rnnt 2.987133 hw_loss 0.382656 lr 0.00058765 rank 7
2023-02-17 18:49:45,990 DEBUG TRAIN Batch 8/5700 loss 16.954056 loss_att 18.686590 loss_ctc 25.152233 loss_rnnt 15.343904 hw_loss 0.319784 lr 0.00058723 rank 2
2023-02-17 18:49:45,992 DEBUG TRAIN Batch 8/5700 loss 12.160226 loss_att 17.045547 loss_ctc 21.886065 loss_rnnt 9.667959 hw_loss 0.409544 lr 0.00058744 rank 0
2023-02-17 18:49:45,992 DEBUG TRAIN Batch 8/5700 loss 12.773783 loss_att 14.655327 loss_ctc 22.545107 loss_rnnt 10.891836 hw_loss 0.380239 lr 0.00058764 rank 6
2023-02-17 18:49:46,008 DEBUG TRAIN Batch 8/5700 loss 13.231023 loss_att 18.467911 loss_ctc 29.982422 loss_rnnt 9.841179 hw_loss 0.204274 lr 0.00058754 rank 1
2023-02-17 18:49:46,052 DEBUG TRAIN Batch 8/5700 loss 15.682206 loss_att 22.054617 loss_ctc 33.700859 loss_rnnt 11.842612 hw_loss 0.304922 lr 0.00058759 rank 5
2023-02-17 18:51:01,152 DEBUG TRAIN Batch 8/5800 loss 18.122034 loss_att 27.724159 loss_ctc 29.953674 loss_rnnt 14.470564 hw_loss 0.287800 lr 0.00058724 rank 6
2023-02-17 18:51:01,153 DEBUG TRAIN Batch 8/5800 loss 20.219536 loss_att 29.225748 loss_ctc 32.868549 loss_rnnt 16.527287 hw_loss 0.383387 lr 0.00058722 rank 3
2023-02-17 18:51:01,153 DEBUG TRAIN Batch 8/5800 loss 27.337618 loss_att 30.739475 loss_ctc 51.031517 loss_rnnt 23.363667 hw_loss 0.251985 lr 0.00058724 rank 7
2023-02-17 18:51:01,153 DEBUG TRAIN Batch 8/5800 loss 22.846960 loss_att 26.757681 loss_ctc 41.853943 loss_rnnt 19.354935 hw_loss 0.329282 lr 0.00058719 rank 5
2023-02-17 18:51:01,156 DEBUG TRAIN Batch 8/5800 loss 16.721479 loss_att 18.654646 loss_ctc 24.721584 loss_rnnt 15.124995 hw_loss 0.268445 lr 0.00058714 rank 1
2023-02-17 18:51:01,157 DEBUG TRAIN Batch 8/5800 loss 15.270700 loss_att 18.676857 loss_ctc 28.023293 loss_rnnt 12.715277 hw_loss 0.325964 lr 0.00058733 rank 4
2023-02-17 18:51:01,161 DEBUG TRAIN Batch 8/5800 loss 9.040406 loss_att 11.158910 loss_ctc 12.866643 loss_rnnt 7.871822 hw_loss 0.440098 lr 0.00058683 rank 2
2023-02-17 18:51:01,166 DEBUG TRAIN Batch 8/5800 loss 17.928535 loss_att 19.632719 loss_ctc 29.691380 loss_rnnt 15.818855 hw_loss 0.375871 lr 0.00058704 rank 0
2023-02-17 18:52:18,757 DEBUG TRAIN Batch 8/5900 loss 7.740936 loss_att 11.227779 loss_ctc 12.223333 loss_rnnt 6.326630 hw_loss 0.223660 lr 0.00058678 rank 5
2023-02-17 18:52:18,763 DEBUG TRAIN Batch 8/5900 loss 6.838427 loss_att 12.744968 loss_ctc 14.711380 loss_rnnt 4.463261 hw_loss 0.270247 lr 0.00058692 rank 4
2023-02-17 18:52:18,764 DEBUG TRAIN Batch 8/5900 loss 20.034830 loss_att 22.577518 loss_ctc 35.890400 loss_rnnt 17.245642 hw_loss 0.312327 lr 0.00058673 rank 1
2023-02-17 18:52:18,765 DEBUG TRAIN Batch 8/5900 loss 11.270529 loss_att 12.811594 loss_ctc 17.286665 loss_rnnt 10.000029 hw_loss 0.300252 lr 0.00058681 rank 3
2023-02-17 18:52:18,766 DEBUG TRAIN Batch 8/5900 loss 25.718872 loss_att 34.762978 loss_ctc 39.541199 loss_rnnt 21.941397 hw_loss 0.235645 lr 0.00058684 rank 7
2023-02-17 18:52:18,768 DEBUG TRAIN Batch 8/5900 loss 9.904688 loss_att 15.057195 loss_ctc 17.205044 loss_rnnt 7.750687 hw_loss 0.281472 lr 0.00058683 rank 6
2023-02-17 18:52:18,768 DEBUG TRAIN Batch 8/5900 loss 8.599953 loss_att 11.038622 loss_ctc 13.362328 loss_rnnt 7.303069 hw_loss 0.326563 lr 0.00058642 rank 2
2023-02-17 18:52:18,772 DEBUG TRAIN Batch 8/5900 loss 23.445354 loss_att 30.163322 loss_ctc 37.165867 loss_rnnt 20.119553 hw_loss 0.286512 lr 0.00058663 rank 0
2023-02-17 18:53:37,209 DEBUG TRAIN Batch 8/6000 loss 10.346790 loss_att 13.298208 loss_ctc 18.852634 loss_rnnt 8.432251 hw_loss 0.356519 lr 0.00058643 rank 6
2023-02-17 18:53:37,212 DEBUG TRAIN Batch 8/6000 loss 14.080850 loss_att 18.691963 loss_ctc 24.597380 loss_rnnt 11.562956 hw_loss 0.362752 lr 0.00058652 rank 4
2023-02-17 18:53:37,218 DEBUG TRAIN Batch 8/6000 loss 6.293604 loss_att 11.184518 loss_ctc 8.638493 loss_rnnt 4.845258 hw_loss 0.295334 lr 0.00058641 rank 3
2023-02-17 18:53:37,219 DEBUG TRAIN Batch 8/6000 loss 21.704447 loss_att 24.491751 loss_ctc 38.825211 loss_rnnt 18.692980 hw_loss 0.321068 lr 0.00058633 rank 1
2023-02-17 18:53:37,219 DEBUG TRAIN Batch 8/6000 loss 8.590734 loss_att 11.806043 loss_ctc 18.206104 loss_rnnt 6.508209 hw_loss 0.295150 lr 0.00058638 rank 5
2023-02-17 18:53:37,220 DEBUG TRAIN Batch 8/6000 loss 17.208195 loss_att 18.673157 loss_ctc 17.651533 loss_rnnt 16.699354 hw_loss 0.293883 lr 0.00058643 rank 7
2023-02-17 18:53:37,246 DEBUG TRAIN Batch 8/6000 loss 15.895432 loss_att 22.179199 loss_ctc 29.076946 loss_rnnt 12.737965 hw_loss 0.268459 lr 0.00058623 rank 0
2023-02-17 18:53:37,280 DEBUG TRAIN Batch 8/6000 loss 8.076739 loss_att 10.627155 loss_ctc 13.343638 loss_rnnt 6.679607 hw_loss 0.346492 lr 0.00058602 rank 2
2023-02-17 18:54:56,275 DEBUG TRAIN Batch 8/6100 loss 16.315289 loss_att 19.243750 loss_ctc 28.416695 loss_rnnt 13.954800 hw_loss 0.302393 lr 0.00058603 rank 7
2023-02-17 18:54:56,278 DEBUG TRAIN Batch 8/6100 loss 12.045389 loss_att 15.964065 loss_ctc 24.079411 loss_rnnt 9.560013 hw_loss 0.182073 lr 0.00058603 rank 6
2023-02-17 18:54:56,279 DEBUG TRAIN Batch 8/6100 loss 21.260408 loss_att 24.426035 loss_ctc 36.618828 loss_rnnt 18.395657 hw_loss 0.344693 lr 0.00058598 rank 5
2023-02-17 18:54:56,280 DEBUG TRAIN Batch 8/6100 loss 10.225292 loss_att 15.231108 loss_ctc 15.667927 loss_rnnt 8.367344 hw_loss 0.245814 lr 0.00058601 rank 3
2023-02-17 18:54:56,282 DEBUG TRAIN Batch 8/6100 loss 16.205521 loss_att 22.072943 loss_ctc 25.480042 loss_rnnt 13.644041 hw_loss 0.283859 lr 0.00058562 rank 2
2023-02-17 18:54:56,284 DEBUG TRAIN Batch 8/6100 loss 10.774130 loss_att 16.689159 loss_ctc 19.081625 loss_rnnt 8.276113 hw_loss 0.388772 lr 0.00058593 rank 1
2023-02-17 18:54:56,284 DEBUG TRAIN Batch 8/6100 loss 14.565065 loss_att 17.281324 loss_ctc 25.666876 loss_rnnt 12.431418 hw_loss 0.206536 lr 0.00058583 rank 0
2023-02-17 18:54:56,319 DEBUG TRAIN Batch 8/6100 loss 11.364872 loss_att 15.596573 loss_ctc 18.948025 loss_rnnt 9.366810 hw_loss 0.263691 lr 0.00058612 rank 4
2023-02-17 18:56:12,195 DEBUG TRAIN Batch 8/6200 loss 12.024838 loss_att 15.288820 loss_ctc 19.743793 loss_rnnt 10.157653 hw_loss 0.347239 lr 0.00058557 rank 5
2023-02-17 18:56:12,197 DEBUG TRAIN Batch 8/6200 loss 9.992474 loss_att 13.348001 loss_ctc 16.404644 loss_rnnt 8.292091 hw_loss 0.326851 lr 0.00058562 rank 6
2023-02-17 18:56:12,197 DEBUG TRAIN Batch 8/6200 loss 4.537069 loss_att 10.048970 loss_ctc 7.169176 loss_rnnt 2.989662 hw_loss 0.176399 lr 0.00058563 rank 7
2023-02-17 18:56:12,198 DEBUG TRAIN Batch 8/6200 loss 10.936577 loss_att 16.326637 loss_ctc 18.227926 loss_rnnt 8.721281 hw_loss 0.309569 lr 0.00058560 rank 3
2023-02-17 18:56:12,200 DEBUG TRAIN Batch 8/6200 loss 15.040613 loss_att 19.707611 loss_ctc 29.410275 loss_rnnt 11.947768 hw_loss 0.456545 lr 0.00058522 rank 2
2023-02-17 18:56:12,200 DEBUG TRAIN Batch 8/6200 loss 15.998818 loss_att 17.710827 loss_ctc 26.141943 loss_rnnt 14.137286 hw_loss 0.312587 lr 0.00058553 rank 1
2023-02-17 18:56:12,202 DEBUG TRAIN Batch 8/6200 loss 16.783316 loss_att 21.992403 loss_ctc 29.068970 loss_rnnt 13.957788 hw_loss 0.273043 lr 0.00058572 rank 4
2023-02-17 18:56:12,202 DEBUG TRAIN Batch 8/6200 loss 13.618958 loss_att 18.690548 loss_ctc 23.309521 loss_rnnt 11.073398 hw_loss 0.448439 lr 0.00058543 rank 0
2023-02-17 18:57:28,744 DEBUG TRAIN Batch 8/6300 loss 11.064715 loss_att 15.060214 loss_ctc 16.721464 loss_rnnt 9.341208 hw_loss 0.319078 lr 0.00058531 rank 4
2023-02-17 18:57:28,748 DEBUG TRAIN Batch 8/6300 loss 15.480938 loss_att 20.182423 loss_ctc 24.324829 loss_rnnt 13.149906 hw_loss 0.396654 lr 0.00058522 rank 6
2023-02-17 18:57:28,749 DEBUG TRAIN Batch 8/6300 loss 12.564119 loss_att 14.289837 loss_ctc 21.695736 loss_rnnt 10.765526 hw_loss 0.442315 lr 0.00058482 rank 2
2023-02-17 18:57:28,749 DEBUG TRAIN Batch 8/6300 loss 27.425852 loss_att 30.742153 loss_ctc 43.176170 loss_rnnt 24.508390 hw_loss 0.289046 lr 0.00058513 rank 1
2023-02-17 18:57:28,751 DEBUG TRAIN Batch 8/6300 loss 25.456959 loss_att 30.167110 loss_ctc 41.988853 loss_rnnt 22.060062 hw_loss 0.469901 lr 0.00058523 rank 7
2023-02-17 18:57:28,751 DEBUG TRAIN Batch 8/6300 loss 11.291779 loss_att 13.415016 loss_ctc 19.788177 loss_rnnt 9.538363 hw_loss 0.367340 lr 0.00058517 rank 5
2023-02-17 18:57:28,751 DEBUG TRAIN Batch 8/6300 loss 17.015059 loss_att 20.366976 loss_ctc 30.903934 loss_rnnt 14.307182 hw_loss 0.348081 lr 0.00058503 rank 0
2023-02-17 18:57:28,752 DEBUG TRAIN Batch 8/6300 loss 10.653966 loss_att 12.860619 loss_ctc 15.176536 loss_rnnt 9.423368 hw_loss 0.349233 lr 0.00058520 rank 3
2023-02-17 18:58:48,701 DEBUG TRAIN Batch 8/6400 loss 17.256340 loss_att 23.709564 loss_ctc 33.744972 loss_rnnt 13.555185 hw_loss 0.397546 lr 0.00058463 rank 0
2023-02-17 18:58:48,702 DEBUG TRAIN Batch 8/6400 loss 9.929007 loss_att 11.582108 loss_ctc 15.866687 loss_rnnt 8.596144 hw_loss 0.394786 lr 0.00058491 rank 4
2023-02-17 18:58:48,704 DEBUG TRAIN Batch 8/6400 loss 13.027738 loss_att 15.478099 loss_ctc 20.507160 loss_rnnt 11.356094 hw_loss 0.345590 lr 0.00058442 rank 2
2023-02-17 18:58:48,705 DEBUG TRAIN Batch 8/6400 loss 10.502767 loss_att 11.848389 loss_ctc 15.437712 loss_rnnt 9.371914 hw_loss 0.382005 lr 0.00058482 rank 6
2023-02-17 18:58:48,707 DEBUG TRAIN Batch 8/6400 loss 16.924282 loss_att 19.476458 loss_ctc 27.173786 loss_rnnt 14.828876 hw_loss 0.409440 lr 0.00058483 rank 7
2023-02-17 18:58:48,707 DEBUG TRAIN Batch 8/6400 loss 8.459117 loss_att 14.928027 loss_ctc 16.632246 loss_rnnt 5.887468 hw_loss 0.352717 lr 0.00058473 rank 1
2023-02-17 18:58:48,711 DEBUG TRAIN Batch 8/6400 loss 12.032953 loss_att 16.574772 loss_ctc 31.628403 loss_rnnt 8.275020 hw_loss 0.444083 lr 0.00058477 rank 5
2023-02-17 18:58:48,722 DEBUG TRAIN Batch 8/6400 loss 16.139648 loss_att 22.044930 loss_ctc 30.072737 loss_rnnt 12.955862 hw_loss 0.271850 lr 0.00058480 rank 3
2023-02-17 19:00:04,874 DEBUG TRAIN Batch 8/6500 loss 15.730785 loss_att 20.555534 loss_ctc 25.519821 loss_rnnt 13.203232 hw_loss 0.482624 lr 0.00058442 rank 6
2023-02-17 19:00:04,875 DEBUG TRAIN Batch 8/6500 loss 18.226009 loss_att 21.778063 loss_ctc 20.057407 loss_rnnt 17.133663 hw_loss 0.258280 lr 0.00058437 rank 5
2023-02-17 19:00:04,878 DEBUG TRAIN Batch 8/6500 loss 17.390371 loss_att 20.834177 loss_ctc 26.107803 loss_rnnt 15.369917 hw_loss 0.317569 lr 0.00058443 rank 7
2023-02-17 19:00:04,879 DEBUG TRAIN Batch 8/6500 loss 19.691170 loss_att 19.544245 loss_ctc 27.454636 loss_rnnt 18.590200 hw_loss 0.178545 lr 0.00058433 rank 1
2023-02-17 19:00:04,881 DEBUG TRAIN Batch 8/6500 loss 24.970503 loss_att 31.049637 loss_ctc 38.466682 loss_rnnt 21.762550 hw_loss 0.361190 lr 0.00058451 rank 4
2023-02-17 19:00:04,881 DEBUG TRAIN Batch 8/6500 loss 13.715801 loss_att 16.004463 loss_ctc 25.409071 loss_rnnt 11.521759 hw_loss 0.332265 lr 0.00058440 rank 3
2023-02-17 19:00:04,882 DEBUG TRAIN Batch 8/6500 loss 14.627900 loss_att 19.763874 loss_ctc 27.064110 loss_rnnt 11.746855 hw_loss 0.366916 lr 0.00058402 rank 2
2023-02-17 19:00:04,882 DEBUG TRAIN Batch 8/6500 loss 14.023674 loss_att 14.829901 loss_ctc 20.747181 loss_rnnt 12.773828 hw_loss 0.360248 lr 0.00058423 rank 0
2023-02-17 19:01:21,445 DEBUG TRAIN Batch 8/6600 loss 7.295301 loss_att 11.585855 loss_ctc 13.793882 loss_rnnt 5.448263 hw_loss 0.229594 lr 0.00058402 rank 6
2023-02-17 19:01:21,447 DEBUG TRAIN Batch 8/6600 loss 11.487964 loss_att 18.814556 loss_ctc 24.516464 loss_rnnt 8.132952 hw_loss 0.286050 lr 0.00058403 rank 7
2023-02-17 19:01:21,447 DEBUG TRAIN Batch 8/6600 loss 6.663156 loss_att 9.667675 loss_ctc 12.996131 loss_rnnt 5.063204 hw_loss 0.289969 lr 0.00058362 rank 2
2023-02-17 19:01:21,448 DEBUG TRAIN Batch 8/6600 loss 8.011673 loss_att 9.823032 loss_ctc 13.598536 loss_rnnt 6.697735 hw_loss 0.387659 lr 0.00058398 rank 5
2023-02-17 19:01:21,450 DEBUG TRAIN Batch 8/6600 loss 43.408714 loss_att 49.752209 loss_ctc 61.519802 loss_rnnt 39.538296 hw_loss 0.350453 lr 0.00058411 rank 4
2023-02-17 19:01:21,452 DEBUG TRAIN Batch 8/6600 loss 15.028189 loss_att 16.419382 loss_ctc 24.247540 loss_rnnt 13.408916 hw_loss 0.209599 lr 0.00058400 rank 3
2023-02-17 19:01:21,454 DEBUG TRAIN Batch 8/6600 loss 9.418936 loss_att 11.618059 loss_ctc 18.215935 loss_rnnt 7.650839 hw_loss 0.291259 lr 0.00058393 rank 1
2023-02-17 19:01:21,457 DEBUG TRAIN Batch 8/6600 loss 18.998323 loss_att 21.019445 loss_ctc 31.212536 loss_rnnt 16.819059 hw_loss 0.274644 lr 0.00058383 rank 0
2023-02-17 19:02:40,815 DEBUG TRAIN Batch 8/6700 loss 8.019314 loss_att 11.444386 loss_ctc 14.924209 loss_rnnt 6.218279 hw_loss 0.366315 lr 0.00058353 rank 1
2023-02-17 19:02:40,816 DEBUG TRAIN Batch 8/6700 loss 6.249496 loss_att 8.792298 loss_ctc 9.041198 loss_rnnt 5.195348 hw_loss 0.325053 lr 0.00058363 rank 7
2023-02-17 19:02:40,818 DEBUG TRAIN Batch 8/6700 loss 16.569687 loss_att 19.849665 loss_ctc 26.502838 loss_rnnt 14.388206 hw_loss 0.376997 lr 0.00058372 rank 4
2023-02-17 19:02:40,819 DEBUG TRAIN Batch 8/6700 loss 13.672606 loss_att 16.044300 loss_ctc 21.595428 loss_rnnt 11.993218 hw_loss 0.278761 lr 0.00058358 rank 5
2023-02-17 19:02:40,819 DEBUG TRAIN Batch 8/6700 loss 12.927915 loss_att 14.460062 loss_ctc 20.042568 loss_rnnt 11.554637 hw_loss 0.221677 lr 0.00058362 rank 6
2023-02-17 19:02:40,822 DEBUG TRAIN Batch 8/6700 loss 19.216780 loss_att 26.340904 loss_ctc 35.489277 loss_rnnt 15.511996 hw_loss 0.206797 lr 0.00058343 rank 0
2023-02-17 19:02:40,823 DEBUG TRAIN Batch 8/6700 loss 12.187264 loss_att 17.623806 loss_ctc 25.062103 loss_rnnt 9.155167 hw_loss 0.427771 lr 0.00058322 rank 2
2023-02-17 19:02:40,864 DEBUG TRAIN Batch 8/6700 loss 16.839649 loss_att 18.638031 loss_ctc 25.141323 loss_rnnt 15.260498 hw_loss 0.211097 lr 0.00058361 rank 3
2023-02-17 19:03:58,090 DEBUG TRAIN Batch 8/6800 loss 20.302784 loss_att 22.796116 loss_ctc 29.587326 loss_rnnt 18.408379 hw_loss 0.295870 lr 0.00058283 rank 2
2023-02-17 19:03:58,092 DEBUG TRAIN Batch 8/6800 loss 19.328243 loss_att 22.877586 loss_ctc 31.535118 loss_rnnt 16.781061 hw_loss 0.393243 lr 0.00058323 rank 6
2023-02-17 19:03:58,092 DEBUG TRAIN Batch 8/6800 loss 11.570318 loss_att 17.433636 loss_ctc 20.701424 loss_rnnt 8.997931 hw_loss 0.341706 lr 0.00058321 rank 3
2023-02-17 19:03:58,093 DEBUG TRAIN Batch 8/6800 loss 12.801973 loss_att 17.488066 loss_ctc 21.305328 loss_rnnt 10.565211 hw_loss 0.310806 lr 0.00058323 rank 7
2023-02-17 19:03:58,093 DEBUG TRAIN Batch 8/6800 loss 17.847616 loss_att 20.711613 loss_ctc 26.643843 loss_rnnt 15.901893 hw_loss 0.375177 lr 0.00058313 rank 1
2023-02-17 19:03:58,096 DEBUG TRAIN Batch 8/6800 loss 30.543631 loss_att 34.575012 loss_ctc 48.290382 loss_rnnt 27.185646 hw_loss 0.347762 lr 0.00058332 rank 4
2023-02-17 19:03:58,097 DEBUG TRAIN Batch 8/6800 loss 11.196445 loss_att 15.198341 loss_ctc 20.539694 loss_rnnt 8.980482 hw_loss 0.318407 lr 0.00058303 rank 0
2023-02-17 19:03:58,099 DEBUG TRAIN Batch 8/6800 loss 11.631557 loss_att 15.327480 loss_ctc 19.045359 loss_rnnt 9.738581 hw_loss 0.309910 lr 0.00058318 rank 5
2023-02-17 19:05:13,416 DEBUG TRAIN Batch 8/6900 loss 18.885508 loss_att 24.871712 loss_ctc 30.520176 loss_rnnt 15.971172 hw_loss 0.310886 lr 0.00058243 rank 2
2023-02-17 19:05:13,417 DEBUG TRAIN Batch 8/6900 loss 14.793790 loss_att 14.821813 loss_ctc 22.258053 loss_rnnt 13.655093 hw_loss 0.258481 lr 0.00058274 rank 1
2023-02-17 19:05:13,417 DEBUG TRAIN Batch 8/6900 loss 18.246763 loss_att 21.194199 loss_ctc 31.324385 loss_rnnt 15.744584 hw_loss 0.316893 lr 0.00058283 rank 6
2023-02-17 19:05:13,418 DEBUG TRAIN Batch 8/6900 loss 11.357463 loss_att 14.105413 loss_ctc 18.547791 loss_rnnt 9.705259 hw_loss 0.269817 lr 0.00058278 rank 5
2023-02-17 19:05:13,420 DEBUG TRAIN Batch 8/6900 loss 17.173969 loss_att 22.699028 loss_ctc 28.829742 loss_rnnt 14.383685 hw_loss 0.245944 lr 0.00058284 rank 7
2023-02-17 19:05:13,420 DEBUG TRAIN Batch 8/6900 loss 5.931954 loss_att 9.895063 loss_ctc 12.625605 loss_rnnt 4.032588 hw_loss 0.401733 lr 0.00058292 rank 4
2023-02-17 19:05:13,421 DEBUG TRAIN Batch 8/6900 loss 13.614069 loss_att 20.582577 loss_ctc 27.911274 loss_rnnt 10.140705 hw_loss 0.325065 lr 0.00058264 rank 0
2023-02-17 19:05:13,422 DEBUG TRAIN Batch 8/6900 loss 8.977960 loss_att 11.839561 loss_ctc 14.847531 loss_rnnt 7.395743 hw_loss 0.426160 lr 0.00058281 rank 3
2023-02-17 19:06:30,353 DEBUG TRAIN Batch 8/7000 loss 15.878590 loss_att 19.512432 loss_ctc 24.447271 loss_rnnt 13.830556 hw_loss 0.335201 lr 0.00058244 rank 7
2023-02-17 19:06:30,356 DEBUG TRAIN Batch 8/7000 loss 10.490282 loss_att 13.940411 loss_ctc 16.808939 loss_rnnt 8.811546 hw_loss 0.274167 lr 0.00058239 rank 5
2023-02-17 19:06:30,359 DEBUG TRAIN Batch 8/7000 loss 21.631832 loss_att 26.636189 loss_ctc 37.899395 loss_rnnt 18.246168 hw_loss 0.404596 lr 0.00058253 rank 4
2023-02-17 19:06:30,362 DEBUG TRAIN Batch 8/7000 loss 12.293133 loss_att 14.474829 loss_ctc 15.653061 loss_rnnt 11.133321 hw_loss 0.516528 lr 0.00058244 rank 6
2023-02-17 19:06:30,362 DEBUG TRAIN Batch 8/7000 loss 14.970446 loss_att 19.476364 loss_ctc 28.308914 loss_rnnt 12.145146 hw_loss 0.273099 lr 0.00058204 rank 2
2023-02-17 19:06:30,363 DEBUG TRAIN Batch 8/7000 loss 22.106670 loss_att 26.730816 loss_ctc 39.897186 loss_rnnt 18.641491 hw_loss 0.315530 lr 0.00058234 rank 1
2023-02-17 19:06:30,365 DEBUG TRAIN Batch 8/7000 loss 11.836344 loss_att 16.720600 loss_ctc 19.213675 loss_rnnt 9.706308 hw_loss 0.317887 lr 0.00058242 rank 3
2023-02-17 19:06:30,368 DEBUG TRAIN Batch 8/7000 loss 18.294086 loss_att 18.583761 loss_ctc 26.103909 loss_rnnt 17.011234 hw_loss 0.344267 lr 0.00058224 rank 0
2023-02-17 19:07:50,449 DEBUG TRAIN Batch 8/7100 loss 9.847383 loss_att 12.911609 loss_ctc 24.899887 loss_rnnt 7.086793 hw_loss 0.263897 lr 0.00058199 rank 5
2023-02-17 19:07:50,451 DEBUG TRAIN Batch 8/7100 loss 23.922218 loss_att 30.634071 loss_ctc 44.740128 loss_rnnt 19.649843 hw_loss 0.289276 lr 0.00058204 rank 6
2023-02-17 19:07:50,456 DEBUG TRAIN Batch 8/7100 loss 8.062289 loss_att 8.198441 loss_ctc 11.910922 loss_rnnt 7.309431 hw_loss 0.398394 lr 0.00058164 rank 2
2023-02-17 19:07:50,457 DEBUG TRAIN Batch 8/7100 loss 17.545481 loss_att 20.764677 loss_ctc 25.780323 loss_rnnt 15.593284 hw_loss 0.394459 lr 0.00058213 rank 4
2023-02-17 19:07:50,459 DEBUG TRAIN Batch 8/7100 loss 25.199039 loss_att 26.692923 loss_ctc 40.915565 loss_rnnt 22.609411 hw_loss 0.366218 lr 0.00058195 rank 1
2023-02-17 19:07:50,459 DEBUG TRAIN Batch 8/7100 loss 11.209925 loss_att 11.562425 loss_ctc 13.965527 loss_rnnt 10.532879 hw_loss 0.448371 lr 0.00058204 rank 7
2023-02-17 19:07:50,461 DEBUG TRAIN Batch 8/7100 loss 15.423612 loss_att 23.537230 loss_ctc 29.819082 loss_rnnt 11.697047 hw_loss 0.345833 lr 0.00058202 rank 3
2023-02-17 19:07:50,463 DEBUG TRAIN Batch 8/7100 loss 9.142121 loss_att 11.395201 loss_ctc 17.414093 loss_rnnt 7.373201 hw_loss 0.403828 lr 0.00058185 rank 0
2023-02-17 19:09:07,025 DEBUG TRAIN Batch 8/7200 loss 11.989850 loss_att 16.222301 loss_ctc 21.839926 loss_rnnt 9.670433 hw_loss 0.299218 lr 0.00058145 rank 0
2023-02-17 19:09:07,027 DEBUG TRAIN Batch 8/7200 loss 8.561481 loss_att 13.066551 loss_ctc 16.340864 loss_rnnt 6.414165 hw_loss 0.391972 lr 0.00058165 rank 6
2023-02-17 19:09:07,028 DEBUG TRAIN Batch 8/7200 loss 14.918398 loss_att 17.760265 loss_ctc 31.216694 loss_rnnt 12.044844 hw_loss 0.247640 lr 0.00058174 rank 4
2023-02-17 19:09:07,028 DEBUG TRAIN Batch 8/7200 loss 18.322895 loss_att 20.605799 loss_ctc 29.170156 loss_rnnt 16.196144 hw_loss 0.419751 lr 0.00058165 rank 7
2023-02-17 19:09:07,030 DEBUG TRAIN Batch 8/7200 loss 26.589869 loss_att 30.038160 loss_ctc 45.421978 loss_rnnt 23.234653 hw_loss 0.289888 lr 0.00058160 rank 5
2023-02-17 19:09:07,029 DEBUG TRAIN Batch 8/7200 loss 14.316838 loss_att 18.727489 loss_ctc 28.550146 loss_rnnt 11.441206 hw_loss 0.179489 lr 0.00058125 rank 2
2023-02-17 19:09:07,030 DEBUG TRAIN Batch 8/7200 loss 11.959101 loss_att 14.968193 loss_ctc 21.324553 loss_rnnt 9.904446 hw_loss 0.382707 lr 0.00058163 rank 3
2023-02-17 19:09:07,031 DEBUG TRAIN Batch 8/7200 loss 17.595387 loss_att 21.787779 loss_ctc 28.834539 loss_rnnt 15.051052 hw_loss 0.388687 lr 0.00058155 rank 1
2023-02-17 19:10:23,906 DEBUG TRAIN Batch 8/7300 loss 10.112592 loss_att 12.342443 loss_ctc 18.283075 loss_rnnt 8.435326 hw_loss 0.266058 lr 0.00058121 rank 5
2023-02-17 19:10:23,907 DEBUG TRAIN Batch 8/7300 loss 7.561858 loss_att 11.404606 loss_ctc 19.846189 loss_rnnt 4.986227 hw_loss 0.317193 lr 0.00058126 rank 7
2023-02-17 19:10:23,909 DEBUG TRAIN Batch 8/7300 loss 14.823915 loss_att 26.139799 loss_ctc 25.742857 loss_rnnt 10.961841 hw_loss 0.268199 lr 0.00058086 rank 2
2023-02-17 19:10:23,909 DEBUG TRAIN Batch 8/7300 loss 7.113446 loss_att 11.641241 loss_ctc 13.635353 loss_rnnt 5.206826 hw_loss 0.246513 lr 0.00058125 rank 6
2023-02-17 19:10:23,913 DEBUG TRAIN Batch 8/7300 loss 14.909094 loss_att 19.695118 loss_ctc 23.841682 loss_rnnt 12.560706 hw_loss 0.375322 lr 0.00058106 rank 0
2023-02-17 19:10:23,913 DEBUG TRAIN Batch 8/7300 loss 5.605205 loss_att 11.218648 loss_ctc 10.061195 loss_rnnt 3.651912 hw_loss 0.443385 lr 0.00058123 rank 3
2023-02-17 19:10:23,913 DEBUG TRAIN Batch 8/7300 loss 12.358994 loss_att 14.567847 loss_ctc 21.078918 loss_rnnt 10.518244 hw_loss 0.443106 lr 0.00058116 rank 1
2023-02-17 19:10:23,956 DEBUG TRAIN Batch 8/7300 loss 10.259185 loss_att 15.498182 loss_ctc 15.669060 loss_rnnt 8.271840 hw_loss 0.409176 lr 0.00058134 rank 4
2023-02-17 19:11:41,551 DEBUG TRAIN Batch 8/7400 loss 15.242409 loss_att 19.467165 loss_ctc 33.636787 loss_rnnt 11.752555 hw_loss 0.360599 lr 0.00058081 rank 5
2023-02-17 19:11:41,557 DEBUG TRAIN Batch 8/7400 loss 14.767358 loss_att 15.874799 loss_ctc 21.923388 loss_rnnt 13.414940 hw_loss 0.331484 lr 0.00058067 rank 0
2023-02-17 19:11:41,557 DEBUG TRAIN Batch 8/7400 loss 27.348711 loss_att 29.788197 loss_ctc 41.259716 loss_rnnt 24.856411 hw_loss 0.280505 lr 0.00058087 rank 7
2023-02-17 19:11:41,558 DEBUG TRAIN Batch 8/7400 loss 24.343218 loss_att 25.885427 loss_ctc 35.076271 loss_rnnt 22.448277 hw_loss 0.291419 lr 0.00058086 rank 6
2023-02-17 19:11:41,558 DEBUG TRAIN Batch 8/7400 loss 7.946805 loss_att 13.051172 loss_ctc 18.299648 loss_rnnt 5.331461 hw_loss 0.401422 lr 0.00058047 rank 2
2023-02-17 19:11:41,560 DEBUG TRAIN Batch 8/7400 loss 9.816924 loss_att 13.858248 loss_ctc 19.580845 loss_rnnt 7.507152 hw_loss 0.374347 lr 0.00058077 rank 1
2023-02-17 19:11:41,570 DEBUG TRAIN Batch 8/7400 loss 10.899532 loss_att 14.405572 loss_ctc 20.259417 loss_rnnt 8.776066 hw_loss 0.326763 lr 0.00058095 rank 4
2023-02-17 19:11:41,605 DEBUG TRAIN Batch 8/7400 loss 10.874678 loss_att 14.590603 loss_ctc 18.684439 loss_rnnt 8.918877 hw_loss 0.321216 lr 0.00058084 rank 3
2023-02-17 19:13:01,276 DEBUG TRAIN Batch 8/7500 loss 16.572475 loss_att 19.026781 loss_ctc 27.476618 loss_rnnt 14.476719 hw_loss 0.283146 lr 0.00058008 rank 2
2023-02-17 19:13:01,276 DEBUG TRAIN Batch 8/7500 loss 9.594225 loss_att 15.558678 loss_ctc 17.332472 loss_rnnt 7.178476 hw_loss 0.358297 lr 0.00058056 rank 4
2023-02-17 19:13:01,277 DEBUG TRAIN Batch 8/7500 loss 26.778811 loss_att 28.504967 loss_ctc 37.077953 loss_rnnt 24.865871 hw_loss 0.364668 lr 0.00058038 rank 1
2023-02-17 19:13:01,279 DEBUG TRAIN Batch 8/7500 loss 10.542208 loss_att 13.422455 loss_ctc 18.401546 loss_rnnt 8.735300 hw_loss 0.343023 lr 0.00058047 rank 6
2023-02-17 19:13:01,280 DEBUG TRAIN Batch 8/7500 loss 13.315673 loss_att 17.184788 loss_ctc 19.798052 loss_rnnt 11.544085 hw_loss 0.250213 lr 0.00058028 rank 0
2023-02-17 19:13:01,281 DEBUG TRAIN Batch 8/7500 loss 13.938754 loss_att 15.724686 loss_ctc 27.341774 loss_rnnt 11.626062 hw_loss 0.315816 lr 0.00058047 rank 7
2023-02-17 19:13:01,282 DEBUG TRAIN Batch 8/7500 loss 11.398737 loss_att 15.151802 loss_ctc 20.061934 loss_rnnt 9.270328 hw_loss 0.417571 lr 0.00058042 rank 5
2023-02-17 19:13:01,329 DEBUG TRAIN Batch 8/7500 loss 17.943100 loss_att 19.283192 loss_ctc 26.804184 loss_rnnt 16.286310 hw_loss 0.388673 lr 0.00058045 rank 3
2023-02-17 19:14:18,931 DEBUG TRAIN Batch 8/7600 loss 11.649173 loss_att 12.391254 loss_ctc 16.963039 loss_rnnt 10.616201 hw_loss 0.330073 lr 0.00058003 rank 5
2023-02-17 19:14:18,931 DEBUG TRAIN Batch 8/7600 loss 16.005369 loss_att 22.171888 loss_ctc 27.718653 loss_rnnt 13.046535 hw_loss 0.307048 lr 0.00058008 rank 6
2023-02-17 19:14:18,938 DEBUG TRAIN Batch 8/7600 loss 8.299453 loss_att 10.800193 loss_ctc 12.685756 loss_rnnt 7.016007 hw_loss 0.372106 lr 0.00058017 rank 4
2023-02-17 19:14:18,939 DEBUG TRAIN Batch 8/7600 loss 9.391398 loss_att 17.249187 loss_ctc 14.102985 loss_rnnt 7.112554 hw_loss 0.148265 lr 0.00058006 rank 3
2023-02-17 19:14:18,939 DEBUG TRAIN Batch 8/7600 loss 6.480171 loss_att 9.179163 loss_ctc 12.354988 loss_rnnt 4.905137 hw_loss 0.472363 lr 0.00057989 rank 0
2023-02-17 19:14:18,941 DEBUG TRAIN Batch 8/7600 loss 10.007405 loss_att 13.341997 loss_ctc 15.006743 loss_rnnt 8.511028 hw_loss 0.305400 lr 0.00058008 rank 7
2023-02-17 19:14:18,942 DEBUG TRAIN Batch 8/7600 loss 23.559559 loss_att 25.602161 loss_ctc 32.877850 loss_rnnt 21.755039 hw_loss 0.287925 lr 0.00057969 rank 2
2023-02-17 19:14:18,943 DEBUG TRAIN Batch 8/7600 loss 14.251715 loss_att 19.697369 loss_ctc 23.659920 loss_rnnt 11.767767 hw_loss 0.263233 lr 0.00057999 rank 1
2023-02-17 19:15:35,720 DEBUG TRAIN Batch 8/7700 loss 6.305822 loss_att 8.705951 loss_ctc 11.534652 loss_rnnt 4.954798 hw_loss 0.325915 lr 0.00057978 rank 4
2023-02-17 19:15:35,721 DEBUG TRAIN Batch 8/7700 loss 11.173975 loss_att 14.808575 loss_ctc 20.152121 loss_rnnt 9.029541 hw_loss 0.413303 lr 0.00057964 rank 5
2023-02-17 19:15:35,722 DEBUG TRAIN Batch 8/7700 loss 24.360151 loss_att 34.318817 loss_ctc 45.589500 loss_rnnt 19.392496 hw_loss 0.272510 lr 0.00057967 rank 3
2023-02-17 19:15:35,723 DEBUG TRAIN Batch 8/7700 loss 19.597361 loss_att 18.327887 loss_ctc 28.191605 loss_rnnt 18.518661 hw_loss 0.350052 lr 0.00057969 rank 6
2023-02-17 19:15:35,726 DEBUG TRAIN Batch 8/7700 loss 7.891027 loss_att 11.148042 loss_ctc 16.847120 loss_rnnt 5.866813 hw_loss 0.334998 lr 0.00057960 rank 1
2023-02-17 19:15:35,727 DEBUG TRAIN Batch 8/7700 loss 20.049511 loss_att 21.011972 loss_ctc 29.767401 loss_rnnt 18.460052 hw_loss 0.189839 lr 0.00057969 rank 7
2023-02-17 19:15:35,728 DEBUG TRAIN Batch 8/7700 loss 14.152765 loss_att 16.135090 loss_ctc 23.303474 loss_rnnt 12.366623 hw_loss 0.317969 lr 0.00057930 rank 2
2023-02-17 19:15:35,729 DEBUG TRAIN Batch 8/7700 loss 8.125942 loss_att 10.938364 loss_ctc 18.257090 loss_rnnt 5.983023 hw_loss 0.430528 lr 0.00057950 rank 0
2023-02-17 19:16:55,760 DEBUG TRAIN Batch 8/7800 loss 16.716396 loss_att 22.684711 loss_ctc 26.901882 loss_rnnt 13.925574 hw_loss 0.448304 lr 0.00057939 rank 4
2023-02-17 19:16:55,761 DEBUG TRAIN Batch 8/7800 loss 12.809758 loss_att 16.699923 loss_ctc 27.592941 loss_rnnt 9.867554 hw_loss 0.362024 lr 0.00057930 rank 7
2023-02-17 19:16:55,764 DEBUG TRAIN Batch 8/7800 loss 17.013172 loss_att 22.785604 loss_ctc 30.315029 loss_rnnt 13.830618 hw_loss 0.477165 lr 0.00057928 rank 3
2023-02-17 19:16:55,765 DEBUG TRAIN Batch 8/7800 loss 11.632673 loss_att 12.587074 loss_ctc 17.764004 loss_rnnt 10.457010 hw_loss 0.313635 lr 0.00057921 rank 1
2023-02-17 19:16:55,770 DEBUG TRAIN Batch 8/7800 loss 15.264132 loss_att 22.237328 loss_ctc 27.104910 loss_rnnt 12.058064 hw_loss 0.436234 lr 0.00057925 rank 5
2023-02-17 19:16:55,778 DEBUG TRAIN Batch 8/7800 loss 13.152408 loss_att 14.296122 loss_ctc 22.526638 loss_rnnt 11.479556 hw_loss 0.364148 lr 0.00057930 rank 6
2023-02-17 19:16:55,779 DEBUG TRAIN Batch 8/7800 loss 14.288300 loss_att 19.086409 loss_ctc 22.849300 loss_rnnt 12.049563 hw_loss 0.258087 lr 0.00057891 rank 2
2023-02-17 19:16:55,797 DEBUG TRAIN Batch 8/7800 loss 16.602562 loss_att 22.199585 loss_ctc 28.021362 loss_rnnt 13.833736 hw_loss 0.237960 lr 0.00057911 rank 0
2023-02-17 19:18:12,941 DEBUG TRAIN Batch 8/7900 loss 28.234743 loss_att 28.285955 loss_ctc 41.222160 loss_rnnt 26.340282 hw_loss 0.286060 lr 0.00057892 rank 7
2023-02-17 19:18:12,942 DEBUG TRAIN Batch 8/7900 loss 16.393379 loss_att 20.874237 loss_ctc 30.770603 loss_rnnt 13.409014 hw_loss 0.321054 lr 0.00057882 rank 1
2023-02-17 19:18:12,942 DEBUG TRAIN Batch 8/7900 loss 20.127304 loss_att 26.068359 loss_ctc 32.236526 loss_rnnt 17.198235 hw_loss 0.236805 lr 0.00057889 rank 3
2023-02-17 19:18:12,944 DEBUG TRAIN Batch 8/7900 loss 14.610131 loss_att 17.377251 loss_ctc 19.541914 loss_rnnt 13.235339 hw_loss 0.307121 lr 0.00057900 rank 4
2023-02-17 19:18:12,944 DEBUG TRAIN Batch 8/7900 loss 17.274376 loss_att 21.664532 loss_ctc 28.222118 loss_rnnt 14.801672 hw_loss 0.253077 lr 0.00057887 rank 5
2023-02-17 19:18:12,945 DEBUG TRAIN Batch 8/7900 loss 10.003693 loss_att 15.628531 loss_ctc 21.770163 loss_rnnt 7.143563 hw_loss 0.311809 lr 0.00057891 rank 6
2023-02-17 19:18:12,945 DEBUG TRAIN Batch 8/7900 loss 9.751657 loss_att 14.704355 loss_ctc 16.310280 loss_rnnt 7.780971 hw_loss 0.198119 lr 0.00057852 rank 2
2023-02-17 19:18:12,951 DEBUG TRAIN Batch 8/7900 loss 13.647983 loss_att 18.189196 loss_ctc 23.151754 loss_rnnt 11.319706 hw_loss 0.286622 lr 0.00057872 rank 0
2023-02-17 19:19:29,450 DEBUG TRAIN Batch 8/8000 loss 17.148399 loss_att 17.900801 loss_ctc 24.543076 loss_rnnt 15.830830 hw_loss 0.339621 lr 0.00057843 rank 1
2023-02-17 19:19:29,453 DEBUG TRAIN Batch 8/8000 loss 6.834793 loss_att 11.721760 loss_ctc 14.427646 loss_rnnt 4.701083 hw_loss 0.269879 lr 0.00057833 rank 0
2023-02-17 19:19:29,455 DEBUG TRAIN Batch 8/8000 loss 21.301867 loss_att 28.288448 loss_ctc 33.738747 loss_rnnt 18.064049 hw_loss 0.341720 lr 0.00057853 rank 7
2023-02-17 19:19:29,456 DEBUG TRAIN Batch 8/8000 loss 9.027953 loss_att 11.238789 loss_ctc 15.833745 loss_rnnt 7.517195 hw_loss 0.302159 lr 0.00057852 rank 6
2023-02-17 19:19:29,459 DEBUG TRAIN Batch 8/8000 loss 10.209946 loss_att 14.913080 loss_ctc 19.394697 loss_rnnt 7.879788 hw_loss 0.309182 lr 0.00057813 rank 2
2023-02-17 19:19:29,459 DEBUG TRAIN Batch 8/8000 loss 12.840796 loss_att 18.671747 loss_ctc 23.760647 loss_rnnt 10.033537 hw_loss 0.347043 lr 0.00057848 rank 5
2023-02-17 19:19:29,460 DEBUG TRAIN Batch 8/8000 loss 7.994536 loss_att 12.430140 loss_ctc 11.468568 loss_rnnt 6.504826 hw_loss 0.261347 lr 0.00057850 rank 3
2023-02-17 19:19:29,461 DEBUG TRAIN Batch 8/8000 loss 17.451731 loss_att 22.629797 loss_ctc 27.652931 loss_rnnt 14.886653 hw_loss 0.317443 lr 0.00057861 rank 4
2023-02-17 19:20:46,372 DEBUG TRAIN Batch 8/8100 loss 18.163399 loss_att 20.188293 loss_ctc 29.714943 loss_rnnt 16.060299 hw_loss 0.296085 lr 0.00057814 rank 6
2023-02-17 19:20:46,378 DEBUG TRAIN Batch 8/8100 loss 23.395226 loss_att 24.105915 loss_ctc 35.510231 loss_rnnt 21.430279 hw_loss 0.389020 lr 0.00057809 rank 5
2023-02-17 19:20:46,380 DEBUG TRAIN Batch 8/8100 loss 17.984486 loss_att 22.555990 loss_ctc 28.453512 loss_rnnt 15.510889 hw_loss 0.306419 lr 0.00057795 rank 0
2023-02-17 19:20:46,384 DEBUG TRAIN Batch 8/8100 loss 18.155659 loss_att 19.697073 loss_ctc 25.024857 loss_rnnt 16.758213 hw_loss 0.324880 lr 0.00057823 rank 4
2023-02-17 19:20:46,385 DEBUG TRAIN Batch 8/8100 loss 21.647734 loss_att 22.553158 loss_ctc 34.869514 loss_rnnt 19.515598 hw_loss 0.352774 lr 0.00057804 rank 1
2023-02-17 19:20:46,385 DEBUG TRAIN Batch 8/8100 loss 24.365957 loss_att 25.373198 loss_ctc 39.421867 loss_rnnt 21.980915 hw_loss 0.330264 lr 0.00057812 rank 3
2023-02-17 19:20:46,385 DEBUG TRAIN Batch 8/8100 loss 12.055250 loss_att 14.667976 loss_ctc 19.714684 loss_rnnt 10.334070 hw_loss 0.332583 lr 0.00057814 rank 7
2023-02-17 19:20:46,386 DEBUG TRAIN Batch 8/8100 loss 19.622982 loss_att 21.781624 loss_ctc 33.757935 loss_rnnt 17.139488 hw_loss 0.313321 lr 0.00057775 rank 2
2023-02-17 19:22:02,204 DEBUG TRAIN Batch 8/8200 loss 25.164467 loss_att 28.142723 loss_ctc 38.711090 loss_rnnt 22.586409 hw_loss 0.330355 lr 0.00057766 rank 1
2023-02-17 19:22:02,205 DEBUG TRAIN Batch 8/8200 loss 19.429085 loss_att 20.588514 loss_ctc 27.688709 loss_rnnt 17.920351 hw_loss 0.329179 lr 0.00057775 rank 7
2023-02-17 19:22:02,205 DEBUG TRAIN Batch 8/8200 loss 8.246970 loss_att 11.549628 loss_ctc 10.265255 loss_rnnt 7.148963 hw_loss 0.315698 lr 0.00057775 rank 6
2023-02-17 19:22:02,207 DEBUG TRAIN Batch 8/8200 loss 10.927114 loss_att 13.778727 loss_ctc 17.018082 loss_rnnt 9.343927 hw_loss 0.376377 lr 0.00057770 rank 5
2023-02-17 19:22:02,208 DEBUG TRAIN Batch 8/8200 loss 20.462883 loss_att 21.718082 loss_ctc 33.981300 loss_rnnt 18.240538 hw_loss 0.316590 lr 0.00057784 rank 4
2023-02-17 19:22:02,211 DEBUG TRAIN Batch 8/8200 loss 33.622841 loss_att 34.074409 loss_ctc 47.379696 loss_rnnt 31.519800 hw_loss 0.334648 lr 0.00057756 rank 0
2023-02-17 19:22:02,214 DEBUG TRAIN Batch 8/8200 loss 10.945907 loss_att 13.267595 loss_ctc 19.134872 loss_rnnt 9.206754 hw_loss 0.343036 lr 0.00057736 rank 2
2023-02-17 19:22:02,215 DEBUG TRAIN Batch 8/8200 loss 14.970851 loss_att 15.770327 loss_ctc 20.357609 loss_rnnt 13.914989 hw_loss 0.333250 lr 0.00057773 rank 3
2023-02-17 19:23:16,171 DEBUG TRAIN Batch 8/8300 loss 10.414477 loss_att 14.868925 loss_ctc 19.328505 loss_rnnt 8.186930 hw_loss 0.277727 lr 0.00057732 rank 5
2023-02-17 19:23:16,172 DEBUG TRAIN Batch 8/8300 loss 12.235106 loss_att 17.329258 loss_ctc 26.650740 loss_rnnt 9.150443 hw_loss 0.269525 lr 0.00057745 rank 4
2023-02-17 19:23:16,173 DEBUG TRAIN Batch 8/8300 loss 10.522920 loss_att 13.391826 loss_ctc 20.379328 loss_rnnt 8.449286 hw_loss 0.348123 lr 0.00057698 rank 2
2023-02-17 19:23:16,173 DEBUG TRAIN Batch 8/8300 loss 8.174188 loss_att 13.252573 loss_ctc 15.166251 loss_rnnt 6.095528 hw_loss 0.245075 lr 0.00057735 rank 3
2023-02-17 19:23:16,173 DEBUG TRAIN Batch 8/8300 loss 11.507994 loss_att 15.104712 loss_ctc 20.527481 loss_rnnt 9.379786 hw_loss 0.386749 lr 0.00057737 rank 7
2023-02-17 19:23:16,174 DEBUG TRAIN Batch 8/8300 loss 18.544977 loss_att 18.451622 loss_ctc 28.979624 loss_rnnt 16.997597 hw_loss 0.327684 lr 0.00057737 rank 6
2023-02-17 19:23:16,178 DEBUG TRAIN Batch 8/8300 loss 16.863844 loss_att 20.362568 loss_ctc 26.521675 loss_rnnt 14.721565 hw_loss 0.290292 lr 0.00057727 rank 1
2023-02-17 19:23:16,178 DEBUG TRAIN Batch 8/8300 loss 9.750982 loss_att 12.196212 loss_ctc 15.068369 loss_rnnt 8.369856 hw_loss 0.343303 lr 0.00057718 rank 0
2023-02-17 19:24:13,209 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 4
2023-02-17 19:24:13,210 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 3
2023-02-17 19:24:13,210 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 1
2023-02-17 19:24:13,212 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 7
2023-02-17 19:24:13,212 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 2
2023-02-17 19:24:13,214 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 0
2023-02-17 19:24:13,215 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 5
2023-02-17 19:24:13,224 DEBUG CV Batch 8/0 loss 2.192708 loss_att 2.313591 loss_ctc 3.477846 loss_rnnt 1.781522 hw_loss 0.404359 history loss 2.111497 rank 6
2023-02-17 19:24:24,288 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 7
2023-02-17 19:24:24,331 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 1
2023-02-17 19:24:24,392 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 5
2023-02-17 19:24:24,417 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 4
2023-02-17 19:24:24,479 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 3
2023-02-17 19:24:24,499 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 6
2023-02-17 19:24:24,506 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 2
2023-02-17 19:24:24,549 DEBUG CV Batch 8/100 loss 8.691180 loss_att 9.909798 loss_ctc 17.211498 loss_rnnt 7.129509 hw_loss 0.341072 history loss 4.413082 rank 0
2023-02-17 19:24:38,066 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 1
2023-02-17 19:24:38,101 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 7
2023-02-17 19:24:38,203 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 6
2023-02-17 19:24:38,254 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 4
2023-02-17 19:24:38,349 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 3
2023-02-17 19:24:38,352 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 5
2023-02-17 19:24:38,661 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 0
2023-02-17 19:24:39,766 DEBUG CV Batch 8/200 loss 8.826951 loss_att 16.403006 loss_ctc 12.386835 loss_rnnt 6.687248 hw_loss 0.280952 history loss 5.053714 rank 2
2023-02-17 19:24:50,013 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 7
2023-02-17 19:24:50,080 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 1
2023-02-17 19:24:50,169 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 6
2023-02-17 19:24:50,363 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 4
2023-02-17 19:24:50,424 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 5
2023-02-17 19:24:50,516 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 3
2023-02-17 19:24:50,799 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 0
2023-02-17 19:24:51,997 DEBUG CV Batch 8/300 loss 7.022058 loss_att 7.313903 loss_ctc 13.005994 loss_rnnt 5.949777 hw_loss 0.405101 history loss 5.239153 rank 2
2023-02-17 19:25:01,924 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 7
2023-02-17 19:25:02,091 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 6
2023-02-17 19:25:02,135 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 1
2023-02-17 19:25:02,397 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 5
2023-02-17 19:25:02,426 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 4
2023-02-17 19:25:02,614 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 3
2023-02-17 19:25:02,963 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 0
2023-02-17 19:25:04,992 DEBUG CV Batch 8/400 loss 21.624367 loss_att 83.269073 loss_ctc 23.161448 loss_rnnt 8.933987 hw_loss 0.293428 history loss 6.232337 rank 2
2023-02-17 19:25:12,350 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 7
2023-02-17 19:25:12,512 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 6
2023-02-17 19:25:12,634 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 1
2023-02-17 19:25:12,994 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 5
2023-02-17 19:25:13,101 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 3
2023-02-17 19:25:13,108 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 4
2023-02-17 19:25:14,136 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 0
2023-02-17 19:25:15,606 DEBUG CV Batch 8/500 loss 8.975051 loss_att 8.824903 loss_ctc 12.711780 loss_rnnt 8.341131 hw_loss 0.310724 history loss 7.014940 rank 2
2023-02-17 19:25:24,581 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 6
2023-02-17 19:25:24,710 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 1
2023-02-17 19:25:25,024 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 5
2023-02-17 19:25:25,239 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 7
2023-02-17 19:25:25,259 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 3
2023-02-17 19:25:25,443 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 4
2023-02-17 19:25:26,604 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 0
2023-02-17 19:25:27,817 DEBUG CV Batch 8/600 loss 7.476358 loss_att 8.519691 loss_ctc 11.123649 loss_rnnt 6.552477 hw_loss 0.429205 history loss 7.978752 rank 2
2023-02-17 19:25:36,969 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 6
2023-02-17 19:25:36,998 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 1
2023-02-17 19:25:37,412 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 5
2023-02-17 19:25:37,678 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 3
2023-02-17 19:25:37,678 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 7
2023-02-17 19:25:38,136 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 4
2023-02-17 19:25:38,146 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 0
2023-02-17 19:25:39,296 DEBUG CV Batch 8/700 loss 26.584614 loss_att 96.882500 loss_ctc 33.663197 loss_rnnt 11.465211 hw_loss 0.217523 history loss 8.632364 rank 2
2023-02-17 19:25:49,030 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 1
2023-02-17 19:25:49,054 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 6
2023-02-17 19:25:49,448 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 5
2023-02-17 19:25:49,760 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 7
2023-02-17 19:25:49,979 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 3
2023-02-17 19:25:50,037 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 4
2023-02-17 19:25:50,261 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 0
2023-02-17 19:25:52,316 DEBUG CV Batch 8/800 loss 12.057064 loss_att 12.081613 loss_ctc 21.911560 loss_rnnt 10.603789 hw_loss 0.252062 history loss 8.067140 rank 2
2023-02-17 19:26:03,037 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 1
2023-02-17 19:26:03,254 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 6
2023-02-17 19:26:03,469 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 5
2023-02-17 19:26:03,640 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 7
2023-02-17 19:26:04,018 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 0
2023-02-17 19:26:04,099 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 4
2023-02-17 19:26:04,247 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 3
2023-02-17 19:26:06,220 DEBUG CV Batch 8/900 loss 15.071017 loss_att 21.436903 loss_ctc 24.725971 loss_rnnt 12.390984 hw_loss 0.224115 history loss 7.864706 rank 2
2023-02-17 19:26:15,169 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 1
2023-02-17 19:26:15,446 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 6
2023-02-17 19:26:15,682 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 5
2023-02-17 19:26:15,701 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 7
2023-02-17 19:26:16,241 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 0
2023-02-17 19:26:16,377 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 4
2023-02-17 19:26:16,609 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 3
2023-02-17 19:26:18,516 DEBUG CV Batch 8/1000 loss 5.150796 loss_att 6.142025 loss_ctc 7.507251 loss_rnnt 4.407710 hw_loss 0.432463 history loss 7.634952 rank 2
2023-02-17 19:26:27,022 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 1
2023-02-17 19:26:27,361 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 6
2023-02-17 19:26:27,448 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 7
2023-02-17 19:26:27,556 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 5
2023-02-17 19:26:28,291 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 0
2023-02-17 19:26:28,313 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 4
2023-02-17 19:26:28,689 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 3
2023-02-17 19:26:30,509 DEBUG CV Batch 8/1100 loss 6.819024 loss_att 6.388420 loss_ctc 11.005088 loss_rnnt 6.130155 hw_loss 0.406590 history loss 7.623110 rank 2
2023-02-17 19:26:37,474 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 1
2023-02-17 19:26:37,777 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 6
2023-02-17 19:26:37,913 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 7
2023-02-17 19:26:38,055 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 5
2023-02-17 19:26:38,796 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 4
2023-02-17 19:26:38,820 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 0
2023-02-17 19:26:39,374 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 3
2023-02-17 19:26:41,647 DEBUG CV Batch 8/1200 loss 15.723918 loss_att 15.175109 loss_ctc 21.685940 loss_rnnt 14.847157 hw_loss 0.359223 history loss 7.953676 rank 2
2023-02-17 19:26:49,388 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 1
2023-02-17 19:26:49,769 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 6
2023-02-17 19:26:49,846 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 7
2023-02-17 19:26:49,958 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 5
2023-02-17 19:26:50,744 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 4
2023-02-17 19:26:50,996 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 0
2023-02-17 19:26:51,585 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 3
2023-02-17 19:26:53,907 DEBUG CV Batch 8/1300 loss 7.023684 loss_att 7.255644 loss_ctc 10.267870 loss_rnnt 6.342190 hw_loss 0.379771 history loss 8.265198 rank 2
2023-02-17 19:27:00,728 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 1
2023-02-17 19:27:01,028 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 5
2023-02-17 19:27:01,337 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 6
2023-02-17 19:27:01,587 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 7
2023-02-17 19:27:02,893 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 4
2023-02-17 19:27:02,949 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 0
2023-02-17 19:27:03,620 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 3
2023-02-17 19:27:05,129 DEBUG CV Batch 8/1400 loss 10.331675 loss_att 37.040859 loss_ctc 10.897226 loss_rnnt 4.760878 hw_loss 0.287909 history loss 8.590792 rank 2
2023-02-17 19:27:13,359 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 1
2023-02-17 19:27:13,363 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 5
2023-02-17 19:27:13,893 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 7
2023-02-17 19:27:14,110 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 6
2023-02-17 19:27:14,880 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 0
2023-02-17 19:27:16,017 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 4
2023-02-17 19:27:16,797 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 3
2023-02-17 19:27:17,338 DEBUG CV Batch 8/1500 loss 7.722501 loss_att 8.511031 loss_ctc 8.714970 loss_rnnt 7.241910 hw_loss 0.357291 history loss 8.389012 rank 2
2023-02-17 19:27:27,210 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 5
2023-02-17 19:27:27,333 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 1
2023-02-17 19:27:27,579 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 7
2023-02-17 19:27:27,923 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 6
2023-02-17 19:27:28,871 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 0
2023-02-17 19:27:30,011 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 4
2023-02-17 19:27:31,154 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 2
2023-02-17 19:27:31,352 DEBUG CV Batch 8/1600 loss 5.846880 loss_att 12.713140 loss_ctc 13.246220 loss_rnnt 3.352666 hw_loss 0.251968 history loss 8.305038 rank 3
2023-02-17 19:27:39,728 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 5
2023-02-17 19:27:39,754 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 1
2023-02-17 19:27:39,979 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 7
2023-02-17 19:27:40,378 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 6
2023-02-17 19:27:41,515 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 0
2023-02-17 19:27:42,643 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 4
2023-02-17 19:27:43,790 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 2
2023-02-17 19:27:44,793 DEBUG CV Batch 8/1700 loss 9.251242 loss_att 9.164766 loss_ctc 15.517015 loss_rnnt 8.214261 hw_loss 0.410323 history loss 8.202400 rank 3
2023-02-17 19:27:48,915 INFO Epoch 8 CV info cv_loss 8.168085110344316
2023-02-17 19:27:48,917 INFO Epoch 9 TRAIN info lr 0.0005771540701539412
2023-02-17 19:27:48,921 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:48,968 INFO Epoch 8 CV info cv_loss 8.16808510982744
2023-02-17 19:27:48,969 INFO Epoch 9 TRAIN info lr 0.0005770080126081596
2023-02-17 19:27:48,973 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:49,150 INFO Epoch 8 CV info cv_loss 8.16808511211893
2023-02-17 19:27:49,151 INFO Epoch 9 TRAIN info lr 0.0005773194796384112
2023-02-17 19:27:49,154 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:49,495 INFO Epoch 8 CV info cv_loss 8.168085112584121
2023-02-17 19:27:49,495 INFO Epoch 9 TRAIN info lr 0.0005772925428215177
2023-02-17 19:27:49,499 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:50,854 INFO Epoch 8 CV info cv_loss 8.168085110111722
2023-02-17 19:27:50,855 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_51_class_both/8.pt
2023-02-17 19:27:51,523 INFO Epoch 9 TRAIN info lr 0.0005771040906178584
2023-02-17 19:27:51,528 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:51,751 INFO Epoch 8 CV info cv_loss 8.168085111627898
2023-02-17 19:27:51,752 INFO Epoch 9 TRAIN info lr 0.0005772694571220775
2023-02-17 19:27:51,755 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:52,930 INFO Epoch 8 CV info cv_loss 8.168085110103107
2023-02-17 19:27:52,931 INFO Epoch 9 TRAIN info lr 0.0005769081423641781
2023-02-17 19:27:52,933 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:27:54,726 INFO Epoch 8 CV info cv_loss 8.168085112463517
2023-02-17 19:27:54,727 INFO Epoch 9 TRAIN info lr 0.0005771502251132183
2023-02-17 19:27:54,729 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 19:29:06,407 DEBUG TRAIN Batch 9/0 loss 11.314838 loss_att 10.720041 loss_ctc 14.576686 loss_rnnt 10.822961 hw_loss 0.329858 lr 0.00057700 rank 1
2023-02-17 19:29:06,407 DEBUG TRAIN Batch 9/0 loss 11.078805 loss_att 11.388346 loss_ctc 16.145031 loss_rnnt 10.109003 hw_loss 0.435744 lr 0.00057732 rank 7
2023-02-17 19:29:06,410 DEBUG TRAIN Batch 9/0 loss 9.544295 loss_att 9.003297 loss_ctc 12.475281 loss_rnnt 9.106026 hw_loss 0.291883 lr 0.00057729 rank 6
2023-02-17 19:29:06,410 DEBUG TRAIN Batch 9/0 loss 14.178325 loss_att 13.429962 loss_ctc 18.358664 loss_rnnt 13.612215 hw_loss 0.297006 lr 0.00057710 rank 0
2023-02-17 19:29:06,412 DEBUG TRAIN Batch 9/0 loss 10.949956 loss_att 10.276977 loss_ctc 15.572039 loss_rnnt 10.259421 hw_loss 0.391597 lr 0.00057727 rank 4
2023-02-17 19:29:06,416 DEBUG TRAIN Batch 9/0 loss 12.898830 loss_att 11.242917 loss_ctc 16.238651 loss_rnnt 12.545721 hw_loss 0.448094 lr 0.00057715 rank 5
2023-02-17 19:29:06,422 DEBUG TRAIN Batch 9/0 loss 13.602839 loss_att 12.780053 loss_ctc 16.836416 loss_rnnt 13.149988 hw_loss 0.349242 lr 0.00057690 rank 2
2023-02-17 19:29:06,426 DEBUG TRAIN Batch 9/0 loss 12.140537 loss_att 11.575025 loss_ctc 16.996252 loss_rnnt 11.407703 hw_loss 0.372203 lr 0.00057715 rank 3
2023-02-17 19:30:21,553 DEBUG TRAIN Batch 9/100 loss 12.542194 loss_att 20.020531 loss_ctc 27.770317 loss_rnnt 8.909344 hw_loss 0.200187 lr 0.00057677 rank 5
2023-02-17 19:30:21,554 DEBUG TRAIN Batch 9/100 loss 13.901367 loss_att 18.697422 loss_ctc 28.294752 loss_rnnt 10.843557 hw_loss 0.336528 lr 0.00057690 rank 6
2023-02-17 19:30:21,554 DEBUG TRAIN Batch 9/100 loss 14.422287 loss_att 17.586308 loss_ctc 24.647001 loss_rnnt 12.259874 hw_loss 0.311834 lr 0.00057652 rank 2
2023-02-17 19:30:21,555 DEBUG TRAIN Batch 9/100 loss 14.650490 loss_att 17.990911 loss_ctc 26.537441 loss_rnnt 12.274103 hw_loss 0.231329 lr 0.00057688 rank 4
2023-02-17 19:30:21,555 DEBUG TRAIN Batch 9/100 loss 17.047815 loss_att 24.884176 loss_ctc 31.204466 loss_rnnt 13.440631 hw_loss 0.285672 lr 0.00057672 rank 0
2023-02-17 19:30:21,555 DEBUG TRAIN Batch 9/100 loss 13.372826 loss_att 16.303852 loss_ctc 26.040571 loss_rnnt 10.914099 hw_loss 0.344042 lr 0.00057693 rank 7
2023-02-17 19:30:21,558 DEBUG TRAIN Batch 9/100 loss 16.232136 loss_att 20.307661 loss_ctc 32.301495 loss_rnnt 13.100708 hw_loss 0.325765 lr 0.00057676 rank 3
2023-02-17 19:30:21,559 DEBUG TRAIN Batch 9/100 loss 8.148990 loss_att 12.924667 loss_ctc 15.049360 loss_rnnt 6.049777 hw_loss 0.420052 lr 0.00057662 rank 1
2023-02-17 19:31:37,378 DEBUG TRAIN Batch 9/200 loss 11.028082 loss_att 15.585798 loss_ctc 17.570049 loss_rnnt 9.034468 hw_loss 0.393390 lr 0.00057638 rank 3
2023-02-17 19:31:37,378 DEBUG TRAIN Batch 9/200 loss 17.752550 loss_att 21.148800 loss_ctc 26.925726 loss_rnnt 15.661701 hw_loss 0.353456 lr 0.00057624 rank 1
2023-02-17 19:31:37,379 DEBUG TRAIN Batch 9/200 loss 18.485374 loss_att 20.883879 loss_ctc 34.220558 loss_rnnt 15.735191 hw_loss 0.323360 lr 0.00057614 rank 2
2023-02-17 19:31:37,379 DEBUG TRAIN Batch 9/200 loss 15.133472 loss_att 19.893583 loss_ctc 23.999044 loss_rnnt 12.833162 hw_loss 0.311647 lr 0.00057633 rank 0
2023-02-17 19:31:37,379 DEBUG TRAIN Batch 9/200 loss 8.030114 loss_att 13.506271 loss_ctc 22.679953 loss_rnnt 4.888807 hw_loss 0.173933 lr 0.00057650 rank 4
2023-02-17 19:31:37,380 DEBUG TRAIN Batch 9/200 loss 17.889511 loss_att 21.814026 loss_ctc 23.561575 loss_rnnt 16.181810 hw_loss 0.312228 lr 0.00057652 rank 6
2023-02-17 19:31:37,382 DEBUG TRAIN Batch 9/200 loss 13.971098 loss_att 19.219437 loss_ctc 31.441170 loss_rnnt 10.415882 hw_loss 0.330385 lr 0.00057638 rank 5
2023-02-17 19:31:37,382 DEBUG TRAIN Batch 9/200 loss 17.517401 loss_att 21.491169 loss_ctc 26.321287 loss_rnnt 15.377022 hw_loss 0.322073 lr 0.00057655 rank 7
2023-02-17 19:32:53,786 DEBUG TRAIN Batch 9/300 loss 12.293039 loss_att 14.666335 loss_ctc 20.428942 loss_rnnt 10.567186 hw_loss 0.312013 lr 0.00057614 rank 6
2023-02-17 19:32:53,789 DEBUG TRAIN Batch 9/300 loss 3.629868 loss_att 6.266123 loss_ctc 5.210236 loss_rnnt 2.740843 hw_loss 0.283234 lr 0.00057600 rank 5
2023-02-17 19:32:53,791 DEBUG TRAIN Batch 9/300 loss 6.778916 loss_att 10.841631 loss_ctc 10.895213 loss_rnnt 5.293633 hw_loss 0.232315 lr 0.00057600 rank 3
2023-02-17 19:32:53,791 DEBUG TRAIN Batch 9/300 loss 19.042639 loss_att 25.597988 loss_ctc 35.055878 loss_rnnt 15.475599 hw_loss 0.226634 lr 0.00057585 rank 1
2023-02-17 19:32:53,792 DEBUG TRAIN Batch 9/300 loss 18.127222 loss_att 22.557936 loss_ctc 38.425728 loss_rnnt 14.363046 hw_loss 0.321683 lr 0.00057576 rank 2
2023-02-17 19:32:53,794 DEBUG TRAIN Batch 9/300 loss 20.015722 loss_att 25.250076 loss_ctc 31.348186 loss_rnnt 17.321884 hw_loss 0.254945 lr 0.00057616 rank 7
2023-02-17 19:32:53,795 DEBUG TRAIN Batch 9/300 loss 5.778511 loss_att 8.999218 loss_ctc 8.724681 loss_rnnt 4.580252 hw_loss 0.302429 lr 0.00057595 rank 0
2023-02-17 19:32:53,837 DEBUG TRAIN Batch 9/300 loss 19.496939 loss_att 24.209276 loss_ctc 33.794689 loss_rnnt 16.512764 hw_loss 0.253763 lr 0.00057611 rank 4
2023-02-17 19:34:12,848 DEBUG TRAIN Batch 9/400 loss 12.885032 loss_att 14.597840 loss_ctc 17.519016 loss_rnnt 11.743832 hw_loss 0.338951 lr 0.00057562 rank 5
2023-02-17 19:34:12,849 DEBUG TRAIN Batch 9/400 loss 15.337976 loss_att 22.585257 loss_ctc 28.708401 loss_rnnt 11.942677 hw_loss 0.305848 lr 0.00057578 rank 7
2023-02-17 19:34:12,853 DEBUG TRAIN Batch 9/400 loss 23.050625 loss_att 30.291775 loss_ctc 43.582329 loss_rnnt 18.736685 hw_loss 0.240285 lr 0.00057576 rank 6
2023-02-17 19:34:12,854 DEBUG TRAIN Batch 9/400 loss 13.622050 loss_att 19.304646 loss_ctc 19.698887 loss_rnnt 11.485055 hw_loss 0.356684 lr 0.00057573 rank 4
2023-02-17 19:34:12,856 DEBUG TRAIN Batch 9/400 loss 12.867879 loss_att 13.076270 loss_ctc 19.152121 loss_rnnt 11.827394 hw_loss 0.301702 lr 0.00057561 rank 3
2023-02-17 19:34:12,857 DEBUG TRAIN Batch 9/400 loss 4.026807 loss_att 7.206744 loss_ctc 8.508649 loss_rnnt 2.662065 hw_loss 0.245955 lr 0.00057557 rank 0
2023-02-17 19:34:12,858 DEBUG TRAIN Batch 9/400 loss 13.226986 loss_att 16.375467 loss_ctc 25.952345 loss_rnnt 10.701876 hw_loss 0.372557 lr 0.00057547 rank 1
2023-02-17 19:34:12,859 DEBUG TRAIN Batch 9/400 loss 14.482342 loss_att 16.893238 loss_ctc 24.404564 loss_rnnt 12.518291 hw_loss 0.297952 lr 0.00057537 rank 2
2023-02-17 19:35:27,711 DEBUG TRAIN Batch 9/500 loss 11.520888 loss_att 17.039667 loss_ctc 27.067709 loss_rnnt 8.195845 hw_loss 0.278208 lr 0.00057537 rank 6
2023-02-17 19:35:27,711 DEBUG TRAIN Batch 9/500 loss 14.386774 loss_att 18.029570 loss_ctc 20.338238 loss_rnnt 12.683581 hw_loss 0.339573 lr 0.00057523 rank 3
2023-02-17 19:35:27,712 DEBUG TRAIN Batch 9/500 loss 7.225417 loss_att 11.921131 loss_ctc 11.459859 loss_rnnt 5.536803 hw_loss 0.346647 lr 0.00057524 rank 5
2023-02-17 19:35:27,713 DEBUG TRAIN Batch 9/500 loss 20.203320 loss_att 25.100121 loss_ctc 35.135117 loss_rnnt 17.055277 hw_loss 0.333329 lr 0.00057519 rank 0
2023-02-17 19:35:27,716 DEBUG TRAIN Batch 9/500 loss 16.384993 loss_att 17.708454 loss_ctc 23.214678 loss_rnnt 15.021561 hw_loss 0.352714 lr 0.00057540 rank 7
2023-02-17 19:35:27,715 DEBUG TRAIN Batch 9/500 loss 13.282332 loss_att 18.221678 loss_ctc 23.662388 loss_rnnt 10.723779 hw_loss 0.350020 lr 0.00057499 rank 2
2023-02-17 19:35:27,717 DEBUG TRAIN Batch 9/500 loss 14.091850 loss_att 18.609249 loss_ctc 20.093403 loss_rnnt 12.257299 hw_loss 0.245371 lr 0.00057535 rank 4
2023-02-17 19:35:27,717 DEBUG TRAIN Batch 9/500 loss 16.547129 loss_att 19.463037 loss_ctc 29.893049 loss_rnnt 14.052292 hw_loss 0.247871 lr 0.00057509 rank 1
2023-02-17 19:36:42,135 DEBUG TRAIN Batch 9/600 loss 14.396771 loss_att 15.565181 loss_ctc 21.025476 loss_rnnt 13.048826 hw_loss 0.432070 lr 0.00057485 rank 3
2023-02-17 19:36:42,138 DEBUG TRAIN Batch 9/600 loss 11.633569 loss_att 12.958520 loss_ctc 14.606744 loss_rnnt 10.833807 hw_loss 0.259405 lr 0.00057461 rank 2
2023-02-17 19:36:42,138 DEBUG TRAIN Batch 9/600 loss 13.650286 loss_att 17.174049 loss_ctc 28.044622 loss_rnnt 10.828827 hw_loss 0.370241 lr 0.00057497 rank 4
2023-02-17 19:36:42,139 DEBUG TRAIN Batch 9/600 loss 24.682785 loss_att 28.272848 loss_ctc 44.675385 loss_rnnt 21.228123 hw_loss 0.133070 lr 0.00057486 rank 5
2023-02-17 19:36:42,139 DEBUG TRAIN Batch 9/600 loss 6.158011 loss_att 7.021131 loss_ctc 10.065846 loss_rnnt 5.273646 hw_loss 0.357556 lr 0.00057502 rank 7
2023-02-17 19:36:42,139 DEBUG TRAIN Batch 9/600 loss 6.479532 loss_att 8.314991 loss_ctc 10.919882 loss_rnnt 5.317385 hw_loss 0.380640 lr 0.00057481 rank 0
2023-02-17 19:36:42,140 DEBUG TRAIN Batch 9/600 loss 12.228395 loss_att 12.500502 loss_ctc 18.008696 loss_rnnt 11.204066 hw_loss 0.373499 lr 0.00057499 rank 6
2023-02-17 19:36:42,142 DEBUG TRAIN Batch 9/600 loss 15.209015 loss_att 15.195805 loss_ctc 19.953480 loss_rnnt 14.322199 hw_loss 0.481614 lr 0.00057471 rank 1
2023-02-17 19:38:01,697 DEBUG TRAIN Batch 9/700 loss 20.577047 loss_att 23.624641 loss_ctc 32.693787 loss_rnnt 18.193687 hw_loss 0.296768 lr 0.00057464 rank 7
2023-02-17 19:38:01,700 DEBUG TRAIN Batch 9/700 loss 13.916242 loss_att 22.426407 loss_ctc 22.867256 loss_rnnt 10.829220 hw_loss 0.359100 lr 0.00057459 rank 4
2023-02-17 19:38:01,699 DEBUG TRAIN Batch 9/700 loss 9.307426 loss_att 13.202354 loss_ctc 16.312902 loss_rnnt 7.416761 hw_loss 0.333031 lr 0.00057461 rank 6
2023-02-17 19:38:01,699 DEBUG TRAIN Batch 9/700 loss 14.179066 loss_att 22.176493 loss_ctc 33.501114 loss_rnnt 9.910069 hw_loss 0.174820 lr 0.00057448 rank 5
2023-02-17 19:38:01,701 DEBUG TRAIN Batch 9/700 loss 15.373643 loss_att 20.126369 loss_ctc 28.694134 loss_rnnt 12.413010 hw_loss 0.438793 lr 0.00057433 rank 1
2023-02-17 19:38:01,701 DEBUG TRAIN Batch 9/700 loss 16.237381 loss_att 24.391563 loss_ctc 24.266037 loss_rnnt 13.376658 hw_loss 0.298874 lr 0.00057447 rank 3
2023-02-17 19:38:01,704 DEBUG TRAIN Batch 9/700 loss 13.522871 loss_att 18.020676 loss_ctc 21.204536 loss_rnnt 11.454323 hw_loss 0.271436 lr 0.00057443 rank 0
2023-02-17 19:38:01,721 DEBUG TRAIN Batch 9/700 loss 15.584449 loss_att 17.334173 loss_ctc 33.159897 loss_rnnt 12.721487 hw_loss 0.318046 lr 0.00057423 rank 2
2023-02-17 19:39:16,921 DEBUG TRAIN Batch 9/800 loss 13.313023 loss_att 14.910809 loss_ctc 22.661373 loss_rnnt 11.583181 hw_loss 0.307198 lr 0.00057395 rank 1
2023-02-17 19:39:16,922 DEBUG TRAIN Batch 9/800 loss 18.497232 loss_att 22.524933 loss_ctc 34.126564 loss_rnnt 15.413161 hw_loss 0.364916 lr 0.00057405 rank 0
2023-02-17 19:39:16,926 DEBUG TRAIN Batch 9/800 loss 9.539786 loss_att 13.720301 loss_ctc 15.628859 loss_rnnt 7.713396 hw_loss 0.334522 lr 0.00057423 rank 6
2023-02-17 19:39:16,926 DEBUG TRAIN Batch 9/800 loss 11.304494 loss_att 15.602263 loss_ctc 20.002291 loss_rnnt 9.059592 hw_loss 0.423078 lr 0.00057386 rank 2
2023-02-17 19:39:16,926 DEBUG TRAIN Batch 9/800 loss 9.772237 loss_att 13.385607 loss_ctc 14.188938 loss_rnnt 8.259003 hw_loss 0.378126 lr 0.00057410 rank 5
2023-02-17 19:39:16,929 DEBUG TRAIN Batch 9/800 loss 6.796971 loss_att 9.596961 loss_ctc 11.623131 loss_rnnt 5.422434 hw_loss 0.320721 lr 0.00057426 rank 7
2023-02-17 19:39:16,934 DEBUG TRAIN Batch 9/800 loss 13.874935 loss_att 17.823816 loss_ctc 26.614630 loss_rnnt 11.185246 hw_loss 0.377414 lr 0.00057421 rank 4
2023-02-17 19:39:16,969 DEBUG TRAIN Batch 9/800 loss 19.070648 loss_att 20.936323 loss_ctc 40.479622 loss_rnnt 15.736008 hw_loss 0.200576 lr 0.00057409 rank 3
2023-02-17 19:40:34,176 DEBUG TRAIN Batch 9/900 loss 15.451490 loss_att 20.730335 loss_ctc 27.840740 loss_rnnt 12.570325 hw_loss 0.325306 lr 0.00057372 rank 5
2023-02-17 19:40:34,176 DEBUG TRAIN Batch 9/900 loss 14.766523 loss_att 17.784672 loss_ctc 27.561804 loss_rnnt 12.294705 hw_loss 0.304032 lr 0.00057388 rank 7
2023-02-17 19:40:34,180 DEBUG TRAIN Batch 9/900 loss 14.239323 loss_att 18.623329 loss_ctc 20.632725 loss_rnnt 12.357668 hw_loss 0.285751 lr 0.00057386 rank 6
2023-02-17 19:40:34,184 DEBUG TRAIN Batch 9/900 loss 12.362886 loss_att 12.982389 loss_ctc 17.593689 loss_rnnt 11.366882 hw_loss 0.327495 lr 0.00057358 rank 1
2023-02-17 19:40:34,186 DEBUG TRAIN Batch 9/900 loss 12.156109 loss_att 14.097992 loss_ctc 19.897766 loss_rnnt 10.574512 hw_loss 0.301871 lr 0.00057372 rank 3
2023-02-17 19:40:34,189 DEBUG TRAIN Batch 9/900 loss 22.131197 loss_att 29.196987 loss_ctc 33.078850 loss_rnnt 19.128817 hw_loss 0.242875 lr 0.00057383 rank 4
2023-02-17 19:40:34,189 DEBUG TRAIN Batch 9/900 loss 19.111675 loss_att 21.303507 loss_ctc 30.270868 loss_rnnt 16.998070 hw_loss 0.351277 lr 0.00057348 rank 2
2023-02-17 19:40:34,194 DEBUG TRAIN Batch 9/900 loss 13.285939 loss_att 14.392475 loss_ctc 23.250435 loss_rnnt 11.534612 hw_loss 0.377663 lr 0.00057367 rank 0
2023-02-17 19:41:50,956 DEBUG TRAIN Batch 9/1000 loss 17.014894 loss_att 21.754189 loss_ctc 28.796062 loss_rnnt 14.336271 hw_loss 0.299891 lr 0.00057348 rank 6
2023-02-17 19:41:50,960 DEBUG TRAIN Batch 9/1000 loss 16.433718 loss_att 20.189871 loss_ctc 27.418205 loss_rnnt 14.043233 hw_loss 0.327475 lr 0.00057346 rank 4
2023-02-17 19:41:50,966 DEBUG TRAIN Batch 9/1000 loss 6.896868 loss_att 9.095341 loss_ctc 11.787186 loss_rnnt 5.621730 hw_loss 0.343877 lr 0.00057320 rank 1
2023-02-17 19:41:50,966 DEBUG TRAIN Batch 9/1000 loss 20.333639 loss_att 23.442930 loss_ctc 31.748085 loss_rnnt 18.032391 hw_loss 0.295250 lr 0.00057334 rank 5
2023-02-17 19:41:50,967 DEBUG TRAIN Batch 9/1000 loss 10.219854 loss_att 16.825924 loss_ctc 24.287973 loss_rnnt 6.895497 hw_loss 0.238862 lr 0.00057351 rank 7
2023-02-17 19:41:50,967 DEBUG TRAIN Batch 9/1000 loss 11.727391 loss_att 15.314486 loss_ctc 22.505293 loss_rnnt 9.445944 hw_loss 0.238078 lr 0.00057310 rank 2
2023-02-17 19:41:50,969 DEBUG TRAIN Batch 9/1000 loss 10.016514 loss_att 15.329729 loss_ctc 20.735950 loss_rnnt 7.408535 hw_loss 0.217643 lr 0.00057329 rank 0
2023-02-17 19:41:50,971 DEBUG TRAIN Batch 9/1000 loss 32.775402 loss_att 38.502724 loss_ctc 68.434174 loss_rnnt 26.716482 hw_loss 0.298034 lr 0.00057334 rank 3
2023-02-17 19:43:08,540 DEBUG TRAIN Batch 9/1100 loss 12.344725 loss_att 16.887110 loss_ctc 19.584482 loss_rnnt 10.318995 hw_loss 0.284910 lr 0.00057296 rank 3
2023-02-17 19:43:08,540 DEBUG TRAIN Batch 9/1100 loss 9.406647 loss_att 15.115928 loss_ctc 17.890789 loss_rnnt 6.923333 hw_loss 0.394196 lr 0.00057313 rank 7
2023-02-17 19:43:08,542 DEBUG TRAIN Batch 9/1100 loss 13.779403 loss_att 18.575150 loss_ctc 22.973087 loss_rnnt 11.418453 hw_loss 0.329953 lr 0.00057273 rank 2
2023-02-17 19:43:08,545 DEBUG TRAIN Batch 9/1100 loss 18.769720 loss_att 23.453920 loss_ctc 35.343170 loss_rnnt 15.428334 hw_loss 0.365160 lr 0.00057282 rank 1
2023-02-17 19:43:08,545 DEBUG TRAIN Batch 9/1100 loss 20.654018 loss_att 21.306664 loss_ctc 35.922920 loss_rnnt 18.339020 hw_loss 0.278653 lr 0.00057310 rank 6
2023-02-17 19:43:08,545 DEBUG TRAIN Batch 9/1100 loss 5.394563 loss_att 8.542095 loss_ctc 7.524519 loss_rnnt 4.329342 hw_loss 0.284477 lr 0.00057297 rank 5
2023-02-17 19:43:08,549 DEBUG TRAIN Batch 9/1100 loss 16.679319 loss_att 19.304298 loss_ctc 31.661814 loss_rnnt 13.974427 hw_loss 0.341681 lr 0.00057308 rank 4
2023-02-17 19:43:08,553 DEBUG TRAIN Batch 9/1100 loss 17.988859 loss_att 19.196320 loss_ctc 32.300842 loss_rnnt 15.685312 hw_loss 0.288360 lr 0.00057292 rank 0
2023-02-17 19:44:24,957 DEBUG TRAIN Batch 9/1200 loss 15.032627 loss_att 17.741322 loss_ctc 23.540321 loss_rnnt 13.169422 hw_loss 0.350824 lr 0.00057275 rank 7
2023-02-17 19:44:24,962 DEBUG TRAIN Batch 9/1200 loss 17.679216 loss_att 21.135303 loss_ctc 31.050817 loss_rnnt 15.035794 hw_loss 0.317482 lr 0.00057273 rank 6
2023-02-17 19:44:24,963 DEBUG TRAIN Batch 9/1200 loss 18.863726 loss_att 21.707329 loss_ctc 29.485142 loss_rnnt 16.728367 hw_loss 0.282094 lr 0.00057270 rank 4
2023-02-17 19:44:24,965 DEBUG TRAIN Batch 9/1200 loss 14.146540 loss_att 16.003445 loss_ctc 17.252930 loss_rnnt 13.147719 hw_loss 0.399852 lr 0.00057245 rank 1
2023-02-17 19:44:24,967 DEBUG TRAIN Batch 9/1200 loss 10.387761 loss_att 13.347809 loss_ctc 16.044325 loss_rnnt 8.818374 hw_loss 0.418444 lr 0.00057259 rank 3
2023-02-17 19:44:24,968 DEBUG TRAIN Batch 9/1200 loss 13.317357 loss_att 14.690749 loss_ctc 19.145201 loss_rnnt 12.070056 hw_loss 0.366707 lr 0.00057254 rank 0
2023-02-17 19:44:24,970 DEBUG TRAIN Batch 9/1200 loss 12.138257 loss_att 14.923523 loss_ctc 19.941498 loss_rnnt 10.321413 hw_loss 0.411299 lr 0.00057259 rank 5
2023-02-17 19:44:25,009 DEBUG TRAIN Batch 9/1200 loss 11.018278 loss_att 13.360536 loss_ctc 20.613735 loss_rnnt 9.085166 hw_loss 0.347374 lr 0.00057235 rank 2
2023-02-17 19:45:41,730 DEBUG TRAIN Batch 9/1300 loss 12.857458 loss_att 18.011288 loss_ctc 19.315519 loss_rnnt 10.804125 hw_loss 0.302796 lr 0.00057238 rank 7
2023-02-17 19:45:41,734 DEBUG TRAIN Batch 9/1300 loss 19.691402 loss_att 22.680857 loss_ctc 40.093582 loss_rnnt 16.203203 hw_loss 0.318778 lr 0.00057217 rank 0
2023-02-17 19:45:41,735 DEBUG TRAIN Batch 9/1300 loss 10.619282 loss_att 16.695196 loss_ctc 15.688800 loss_rnnt 8.539143 hw_loss 0.354414 lr 0.00057222 rank 5
2023-02-17 19:45:41,736 DEBUG TRAIN Batch 9/1300 loss 14.390941 loss_att 23.532574 loss_ctc 32.467880 loss_rnnt 10.005725 hw_loss 0.274932 lr 0.00057207 rank 1
2023-02-17 19:45:41,738 DEBUG TRAIN Batch 9/1300 loss 11.061728 loss_att 14.792130 loss_ctc 21.326036 loss_rnnt 8.749895 hw_loss 0.369711 lr 0.00057221 rank 3
2023-02-17 19:45:41,738 DEBUG TRAIN Batch 9/1300 loss 13.797275 loss_att 15.141289 loss_ctc 19.099407 loss_rnnt 12.656158 hw_loss 0.310054 lr 0.00057235 rank 6
2023-02-17 19:45:41,739 DEBUG TRAIN Batch 9/1300 loss 13.346103 loss_att 15.732592 loss_ctc 21.619013 loss_rnnt 11.548234 hw_loss 0.407840 lr 0.00057233 rank 4
2023-02-17 19:45:41,739 DEBUG TRAIN Batch 9/1300 loss 16.423046 loss_att 18.526625 loss_ctc 31.741505 loss_rnnt 13.798691 hw_loss 0.302212 lr 0.00057198 rank 2
2023-02-17 19:47:01,122 DEBUG TRAIN Batch 9/1400 loss 17.431154 loss_att 18.255621 loss_ctc 22.350876 loss_rnnt 16.431187 hw_loss 0.335830 lr 0.00057200 rank 7
2023-02-17 19:47:01,124 DEBUG TRAIN Batch 9/1400 loss 16.679409 loss_att 22.539822 loss_ctc 30.699774 loss_rnnt 13.474764 hw_loss 0.305965 lr 0.00057170 rank 1
2023-02-17 19:47:01,126 DEBUG TRAIN Batch 9/1400 loss 8.806558 loss_att 14.833076 loss_ctc 15.646605 loss_rnnt 6.516910 hw_loss 0.323132 lr 0.00057184 rank 3
2023-02-17 19:47:01,125 DEBUG TRAIN Batch 9/1400 loss 15.555597 loss_att 20.441444 loss_ctc 21.982922 loss_rnnt 13.574162 hw_loss 0.276168 lr 0.00057179 rank 0
2023-02-17 19:47:01,126 DEBUG TRAIN Batch 9/1400 loss 11.466903 loss_att 14.262139 loss_ctc 19.308659 loss_rnnt 9.660749 hw_loss 0.377884 lr 0.00057198 rank 6
2023-02-17 19:47:01,129 DEBUG TRAIN Batch 9/1400 loss 11.306627 loss_att 16.135586 loss_ctc 20.295126 loss_rnnt 8.945129 hw_loss 0.369826 lr 0.00057184 rank 5
2023-02-17 19:47:01,137 DEBUG TRAIN Batch 9/1400 loss 26.737658 loss_att 28.096123 loss_ctc 43.478485 loss_rnnt 24.035788 hw_loss 0.371375 lr 0.00057195 rank 4
2023-02-17 19:47:01,142 DEBUG TRAIN Batch 9/1400 loss 12.483303 loss_att 15.553433 loss_ctc 17.020147 loss_rnnt 11.124927 hw_loss 0.261446 lr 0.00057160 rank 2
2023-02-17 19:48:19,197 DEBUG TRAIN Batch 9/1500 loss 8.466847 loss_att 11.858793 loss_ctc 15.606331 loss_rnnt 6.628847 hw_loss 0.389401 lr 0.00057160 rank 6
2023-02-17 19:48:19,202 DEBUG TRAIN Batch 9/1500 loss 10.108933 loss_att 12.500587 loss_ctc 14.241500 loss_rnnt 8.961180 hw_loss 0.222026 lr 0.00057147 rank 5
2023-02-17 19:48:19,203 DEBUG TRAIN Batch 9/1500 loss 22.636580 loss_att 26.785080 loss_ctc 41.423042 loss_rnnt 19.139454 hw_loss 0.304810 lr 0.00057133 rank 1
2023-02-17 19:48:19,206 DEBUG TRAIN Batch 9/1500 loss 16.863968 loss_att 24.166315 loss_ctc 34.221825 loss_rnnt 12.951889 hw_loss 0.257306 lr 0.00057163 rank 7
2023-02-17 19:48:19,208 DEBUG TRAIN Batch 9/1500 loss 32.364635 loss_att 38.949238 loss_ctc 57.255005 loss_rnnt 27.476416 hw_loss 0.473596 lr 0.00057146 rank 3
2023-02-17 19:48:19,209 DEBUG TRAIN Batch 9/1500 loss 14.837124 loss_att 17.145063 loss_ctc 24.157482 loss_rnnt 12.977800 hw_loss 0.290661 lr 0.00057158 rank 4
2023-02-17 19:48:19,210 DEBUG TRAIN Batch 9/1500 loss 11.454741 loss_att 16.410408 loss_ctc 22.200241 loss_rnnt 8.900105 hw_loss 0.245194 lr 0.00057142 rank 0
2023-02-17 19:48:19,215 DEBUG TRAIN Batch 9/1500 loss 15.840346 loss_att 20.052437 loss_ctc 28.976624 loss_rnnt 13.077060 hw_loss 0.317559 lr 0.00057123 rank 2
2023-02-17 19:49:34,841 DEBUG TRAIN Batch 9/1600 loss 15.561651 loss_att 18.328758 loss_ctc 25.707308 loss_rnnt 13.513449 hw_loss 0.266299 lr 0.00057126 rank 7
2023-02-17 19:49:34,844 DEBUG TRAIN Batch 9/1600 loss 7.643906 loss_att 12.952974 loss_ctc 16.605503 loss_rnnt 5.256564 hw_loss 0.244967 lr 0.00057109 rank 3
2023-02-17 19:49:34,844 DEBUG TRAIN Batch 9/1600 loss 14.183106 loss_att 17.065893 loss_ctc 26.211346 loss_rnnt 11.834609 hw_loss 0.315328 lr 0.00057123 rank 6
2023-02-17 19:49:34,844 DEBUG TRAIN Batch 9/1600 loss 12.257976 loss_att 13.785152 loss_ctc 12.858685 loss_rnnt 11.709638 hw_loss 0.305265 lr 0.00057109 rank 5
2023-02-17 19:49:34,846 DEBUG TRAIN Batch 9/1600 loss 19.991158 loss_att 24.596210 loss_ctc 33.566593 loss_rnnt 17.107639 hw_loss 0.285839 lr 0.00057121 rank 4
2023-02-17 19:49:34,847 DEBUG TRAIN Batch 9/1600 loss 14.860044 loss_att 23.338667 loss_ctc 29.206995 loss_rnnt 11.098735 hw_loss 0.286230 lr 0.00057095 rank 1
2023-02-17 19:49:34,848 DEBUG TRAIN Batch 9/1600 loss 10.446416 loss_att 12.696542 loss_ctc 14.952826 loss_rnnt 9.289101 hw_loss 0.199566 lr 0.00057086 rank 2
2023-02-17 19:49:34,849 DEBUG TRAIN Batch 9/1600 loss 20.670519 loss_att 25.009892 loss_ctc 34.632671 loss_rnnt 17.765871 hw_loss 0.328414 lr 0.00057105 rank 0
2023-02-17 19:50:51,952 DEBUG TRAIN Batch 9/1700 loss 9.457653 loss_att 15.378523 loss_ctc 15.084458 loss_rnnt 7.412802 hw_loss 0.207070 lr 0.00057086 rank 6
2023-02-17 19:50:51,953 DEBUG TRAIN Batch 9/1700 loss 17.280481 loss_att 20.674433 loss_ctc 30.461285 loss_rnnt 14.661943 hw_loss 0.341825 lr 0.00057088 rank 7
2023-02-17 19:50:51,955 DEBUG TRAIN Batch 9/1700 loss 15.919516 loss_att 18.140125 loss_ctc 28.985100 loss_rnnt 13.535098 hw_loss 0.371660 lr 0.00057058 rank 1
2023-02-17 19:50:51,956 DEBUG TRAIN Batch 9/1700 loss 11.758851 loss_att 14.022401 loss_ctc 15.778902 loss_rnnt 10.606801 hw_loss 0.306248 lr 0.00057067 rank 0
2023-02-17 19:50:51,957 DEBUG TRAIN Batch 9/1700 loss 11.853128 loss_att 16.934553 loss_ctc 21.303625 loss_rnnt 9.390024 hw_loss 0.350164 lr 0.00057072 rank 5
2023-02-17 19:50:51,960 DEBUG TRAIN Batch 9/1700 loss 18.659943 loss_att 24.795994 loss_ctc 27.572548 loss_rnnt 16.084572 hw_loss 0.299650 lr 0.00057072 rank 3
2023-02-17 19:50:51,965 DEBUG TRAIN Batch 9/1700 loss 15.022394 loss_att 18.222334 loss_ctc 25.095301 loss_rnnt 12.883796 hw_loss 0.291666 lr 0.00057048 rank 2
2023-02-17 19:50:52,002 DEBUG TRAIN Batch 9/1700 loss 18.758286 loss_att 25.001318 loss_ctc 30.896866 loss_rnnt 15.717709 hw_loss 0.325299 lr 0.00057083 rank 4
2023-02-17 19:52:11,591 DEBUG TRAIN Batch 9/1800 loss 13.303102 loss_att 13.292965 loss_ctc 21.563152 loss_rnnt 12.024954 hw_loss 0.335319 lr 0.00057051 rank 7
2023-02-17 19:52:11,594 DEBUG TRAIN Batch 9/1800 loss 26.985664 loss_att 32.143978 loss_ctc 40.127243 loss_rnnt 23.975681 hw_loss 0.423958 lr 0.00057046 rank 4
2023-02-17 19:52:11,597 DEBUG TRAIN Batch 9/1800 loss 15.753098 loss_att 20.695522 loss_ctc 24.057705 loss_rnnt 13.504132 hw_loss 0.287250 lr 0.00057035 rank 5
2023-02-17 19:52:11,597 DEBUG TRAIN Batch 9/1800 loss 13.307840 loss_att 16.400887 loss_ctc 27.726795 loss_rnnt 10.605968 hw_loss 0.301380 lr 0.00057048 rank 6
2023-02-17 19:52:11,598 DEBUG TRAIN Batch 9/1800 loss 20.879587 loss_att 22.732054 loss_ctc 32.240952 loss_rnnt 18.809879 hw_loss 0.345681 lr 0.00057035 rank 3
2023-02-17 19:52:11,599 DEBUG TRAIN Batch 9/1800 loss 10.829586 loss_att 13.476494 loss_ctc 14.885268 loss_rnnt 9.558242 hw_loss 0.377262 lr 0.00057011 rank 2
2023-02-17 19:52:11,599 DEBUG TRAIN Batch 9/1800 loss 14.275464 loss_att 18.162527 loss_ctc 28.445690 loss_rnnt 11.398562 hw_loss 0.393985 lr 0.00057021 rank 1
2023-02-17 19:52:11,649 DEBUG TRAIN Batch 9/1800 loss 8.921743 loss_att 9.949892 loss_ctc 14.342473 loss_rnnt 7.823358 hw_loss 0.318735 lr 0.00057030 rank 0
2023-02-17 19:53:29,546 DEBUG TRAIN Batch 9/1900 loss 18.781988 loss_att 19.490761 loss_ctc 27.514721 loss_rnnt 17.315083 hw_loss 0.301474 lr 0.00056984 rank 1
2023-02-17 19:53:29,547 DEBUG TRAIN Batch 9/1900 loss 9.531486 loss_att 11.048985 loss_ctc 16.395237 loss_rnnt 8.078969 hw_loss 0.438467 lr 0.00057009 rank 4
2023-02-17 19:53:29,548 DEBUG TRAIN Batch 9/1900 loss 12.944066 loss_att 20.868513 loss_ctc 28.961126 loss_rnnt 9.117264 hw_loss 0.199321 lr 0.00057014 rank 7
2023-02-17 19:53:29,549 DEBUG TRAIN Batch 9/1900 loss 5.634751 loss_att 7.871766 loss_ctc 11.117115 loss_rnnt 4.249181 hw_loss 0.388471 lr 0.00057011 rank 6
2023-02-17 19:53:29,548 DEBUG TRAIN Batch 9/1900 loss 13.568520 loss_att 14.961927 loss_ctc 18.536264 loss_rnnt 12.450673 hw_loss 0.331497 lr 0.00056998 rank 5
2023-02-17 19:53:29,550 DEBUG TRAIN Batch 9/1900 loss 8.961264 loss_att 10.596138 loss_ctc 12.110186 loss_rnnt 8.015434 hw_loss 0.373120 lr 0.00056974 rank 2
2023-02-17 19:53:29,555 DEBUG TRAIN Batch 9/1900 loss 13.589247 loss_att 14.273907 loss_ctc 20.117626 loss_rnnt 12.361350 hw_loss 0.413464 lr 0.00056993 rank 0
2023-02-17 19:53:29,600 DEBUG TRAIN Batch 9/1900 loss 22.398844 loss_att 33.870361 loss_ctc 42.493176 loss_rnnt 17.268347 hw_loss 0.294280 lr 0.00056998 rank 3
2023-02-17 19:54:46,750 DEBUG TRAIN Batch 9/2000 loss 18.700409 loss_att 21.512033 loss_ctc 38.449783 loss_rnnt 15.287665 hw_loss 0.407190 lr 0.00056974 rank 6
2023-02-17 19:54:46,751 DEBUG TRAIN Batch 9/2000 loss 20.139994 loss_att 23.831621 loss_ctc 37.805893 loss_rnnt 16.820784 hw_loss 0.422682 lr 0.00056977 rank 7
2023-02-17 19:54:46,753 DEBUG TRAIN Batch 9/2000 loss 19.510607 loss_att 21.529844 loss_ctc 34.469772 loss_rnnt 16.918631 hw_loss 0.362948 lr 0.00056956 rank 0
2023-02-17 19:54:46,755 DEBUG TRAIN Batch 9/2000 loss 18.006432 loss_att 26.598270 loss_ctc 37.616257 loss_rnnt 13.487151 hw_loss 0.349254 lr 0.00056961 rank 3
2023-02-17 19:54:46,754 DEBUG TRAIN Batch 9/2000 loss 7.349658 loss_att 12.407656 loss_ctc 21.652029 loss_rnnt 4.222611 hw_loss 0.390870 lr 0.00056947 rank 1
2023-02-17 19:54:46,755 DEBUG TRAIN Batch 9/2000 loss 14.241389 loss_att 20.238312 loss_ctc 19.231304 loss_rnnt 12.183523 hw_loss 0.362177 lr 0.00056961 rank 5
2023-02-17 19:54:46,757 DEBUG TRAIN Batch 9/2000 loss 10.407819 loss_att 17.332859 loss_ctc 13.613430 loss_rnnt 8.433623 hw_loss 0.303323 lr 0.00056937 rank 2
2023-02-17 19:54:46,797 DEBUG TRAIN Batch 9/2000 loss 12.809156 loss_att 19.995951 loss_ctc 16.290739 loss_rnnt 10.766623 hw_loss 0.264309 lr 0.00056972 rank 4
2023-02-17 19:56:04,953 DEBUG TRAIN Batch 9/2100 loss 11.761436 loss_att 19.085960 loss_ctc 26.768986 loss_rnnt 8.138121 hw_loss 0.295134 lr 0.00056919 rank 0
2023-02-17 19:56:04,954 DEBUG TRAIN Batch 9/2100 loss 10.404884 loss_att 15.000553 loss_ctc 16.338667 loss_rnnt 8.507102 hw_loss 0.351521 lr 0.00056924 rank 5
2023-02-17 19:56:04,954 DEBUG TRAIN Batch 9/2100 loss 10.182988 loss_att 11.742229 loss_ctc 16.502846 loss_rnnt 8.906937 hw_loss 0.227915 lr 0.00056935 rank 4
2023-02-17 19:56:04,958 DEBUG TRAIN Batch 9/2100 loss 6.588487 loss_att 11.281635 loss_ctc 15.382132 loss_rnnt 4.331986 hw_loss 0.272596 lr 0.00056910 rank 1
2023-02-17 19:56:04,958 DEBUG TRAIN Batch 9/2100 loss 21.225779 loss_att 29.072037 loss_ctc 37.669464 loss_rnnt 17.329983 hw_loss 0.251353 lr 0.00056937 rank 6
2023-02-17 19:56:04,958 DEBUG TRAIN Batch 9/2100 loss 15.601874 loss_att 20.137148 loss_ctc 22.217070 loss_rnnt 13.637144 hw_loss 0.329341 lr 0.00056924 rank 3
2023-02-17 19:56:04,959 DEBUG TRAIN Batch 9/2100 loss 19.062843 loss_att 22.468105 loss_ctc 28.846466 loss_rnnt 16.909107 hw_loss 0.315378 lr 0.00056940 rank 7
2023-02-17 19:56:04,965 DEBUG TRAIN Batch 9/2100 loss 25.381233 loss_att 26.707600 loss_ctc 40.065907 loss_rnnt 22.979019 hw_loss 0.335593 lr 0.00056901 rank 2
2023-02-17 19:57:23,486 DEBUG TRAIN Batch 9/2200 loss 10.075788 loss_att 13.223434 loss_ctc 15.599824 loss_rnnt 8.537073 hw_loss 0.323712 lr 0.00056903 rank 7
2023-02-17 19:57:23,488 DEBUG TRAIN Batch 9/2200 loss 13.354995 loss_att 20.082340 loss_ctc 29.059437 loss_rnnt 9.752292 hw_loss 0.306204 lr 0.00056898 rank 4
2023-02-17 19:57:23,489 DEBUG TRAIN Batch 9/2200 loss 12.291327 loss_att 20.036396 loss_ctc 23.116875 loss_rnnt 9.136486 hw_loss 0.304537 lr 0.00056901 rank 6
2023-02-17 19:57:23,490 DEBUG TRAIN Batch 9/2200 loss 6.896054 loss_att 10.830693 loss_ctc 14.519691 loss_rnnt 4.928990 hw_loss 0.306846 lr 0.00056887 rank 5
2023-02-17 19:57:23,497 DEBUG TRAIN Batch 9/2200 loss 27.145164 loss_att 31.598217 loss_ctc 46.498425 loss_rnnt 23.520912 hw_loss 0.287263 lr 0.00056873 rank 1
2023-02-17 19:57:23,498 DEBUG TRAIN Batch 9/2200 loss 17.884418 loss_att 21.409893 loss_ctc 36.060341 loss_rnnt 14.548865 hw_loss 0.388127 lr 0.00056864 rank 2
2023-02-17 19:57:23,499 DEBUG TRAIN Batch 9/2200 loss 18.050608 loss_att 19.598673 loss_ctc 30.113247 loss_rnnt 15.987581 hw_loss 0.271988 lr 0.00056882 rank 0
2023-02-17 19:57:23,543 DEBUG TRAIN Batch 9/2200 loss 4.664707 loss_att 7.077900 loss_ctc 6.592314 loss_rnnt 3.724469 hw_loss 0.376097 lr 0.00056887 rank 3
2023-02-17 19:58:39,687 DEBUG TRAIN Batch 9/2300 loss 11.838647 loss_att 12.638073 loss_ctc 15.009643 loss_rnnt 11.084919 hw_loss 0.320706 lr 0.00056864 rank 6
2023-02-17 19:58:39,693 DEBUG TRAIN Batch 9/2300 loss 11.113031 loss_att 15.278854 loss_ctc 20.871960 loss_rnnt 8.811216 hw_loss 0.313987 lr 0.00056827 rank 2
2023-02-17 19:58:39,693 DEBUG TRAIN Batch 9/2300 loss 14.278231 loss_att 14.833337 loss_ctc 24.563065 loss_rnnt 12.607486 hw_loss 0.353274 lr 0.00056850 rank 5
2023-02-17 19:58:39,694 DEBUG TRAIN Batch 9/2300 loss 19.491964 loss_att 25.690163 loss_ctc 32.236824 loss_rnnt 16.361130 hw_loss 0.359773 lr 0.00056846 rank 0
2023-02-17 19:58:39,696 DEBUG TRAIN Batch 9/2300 loss 17.943480 loss_att 24.403982 loss_ctc 30.327896 loss_rnnt 14.856483 hw_loss 0.269329 lr 0.00056837 rank 1
2023-02-17 19:58:39,697 DEBUG TRAIN Batch 9/2300 loss 10.622507 loss_att 15.626392 loss_ctc 20.132524 loss_rnnt 8.185925 hw_loss 0.314632 lr 0.00056866 rank 7
2023-02-17 19:58:39,699 DEBUG TRAIN Batch 9/2300 loss 19.439905 loss_att 21.939713 loss_ctc 32.478443 loss_rnnt 17.041693 hw_loss 0.299588 lr 0.00056862 rank 4
2023-02-17 19:58:39,713 DEBUG TRAIN Batch 9/2300 loss 10.603410 loss_att 15.860682 loss_ctc 20.286472 loss_rnnt 8.107717 hw_loss 0.287182 lr 0.00056850 rank 3
2023-02-17 19:59:56,465 DEBUG TRAIN Batch 9/2400 loss 23.115133 loss_att 23.921730 loss_ctc 31.559757 loss_rnnt 21.652473 hw_loss 0.328862 lr 0.00056814 rank 5
2023-02-17 19:59:56,467 DEBUG TRAIN Batch 9/2400 loss 6.057285 loss_att 7.782328 loss_ctc 9.005681 loss_rnnt 5.160889 hw_loss 0.296753 lr 0.00056825 rank 4
2023-02-17 19:59:56,469 DEBUG TRAIN Batch 9/2400 loss 14.318201 loss_att 18.053272 loss_ctc 25.357397 loss_rnnt 11.954588 hw_loss 0.271325 lr 0.00056830 rank 7
2023-02-17 19:59:56,473 DEBUG TRAIN Batch 9/2400 loss 23.385738 loss_att 25.101530 loss_ctc 38.951019 loss_rnnt 20.786259 hw_loss 0.339278 lr 0.00056827 rank 6
2023-02-17 19:59:56,476 DEBUG TRAIN Batch 9/2400 loss 18.588556 loss_att 23.289518 loss_ctc 29.919373 loss_rnnt 15.971478 hw_loss 0.311454 lr 0.00056809 rank 0
2023-02-17 19:59:56,477 DEBUG TRAIN Batch 9/2400 loss 23.133123 loss_att 29.971794 loss_ctc 30.177765 loss_rnnt 20.669327 hw_loss 0.293958 lr 0.00056800 rank 1
2023-02-17 19:59:56,499 DEBUG TRAIN Batch 9/2400 loss 16.644880 loss_att 19.736809 loss_ctc 30.302368 loss_rnnt 13.985552 hw_loss 0.412392 lr 0.00056813 rank 3
2023-02-17 19:59:56,524 DEBUG TRAIN Batch 9/2400 loss 9.466262 loss_att 13.732164 loss_ctc 22.619678 loss_rnnt 6.653645 hw_loss 0.385591 lr 0.00056790 rank 2
2023-02-17 20:01:16,879 DEBUG TRAIN Batch 9/2500 loss 15.513098 loss_att 18.021553 loss_ctc 30.547979 loss_rnnt 12.810626 hw_loss 0.367745 lr 0.00056790 rank 6
2023-02-17 20:01:16,880 DEBUG TRAIN Batch 9/2500 loss 14.700729 loss_att 15.946396 loss_ctc 24.634558 loss_rnnt 12.957435 hw_loss 0.318097 lr 0.00056763 rank 1
2023-02-17 20:01:16,883 DEBUG TRAIN Batch 9/2500 loss 26.787014 loss_att 29.347214 loss_ctc 42.638134 loss_rnnt 23.998333 hw_loss 0.305925 lr 0.00056788 rank 4
2023-02-17 20:01:16,883 DEBUG TRAIN Batch 9/2500 loss 19.895729 loss_att 19.358660 loss_ctc 24.310699 loss_rnnt 19.179256 hw_loss 0.441048 lr 0.00056754 rank 2
2023-02-17 20:01:16,884 DEBUG TRAIN Batch 9/2500 loss 13.860698 loss_att 16.334955 loss_ctc 22.485533 loss_rnnt 12.038423 hw_loss 0.332711 lr 0.00056777 rank 5
2023-02-17 20:01:16,904 DEBUG TRAIN Batch 9/2500 loss 8.301852 loss_att 11.223589 loss_ctc 17.006433 loss_rnnt 6.359799 hw_loss 0.369551 lr 0.00056772 rank 0
2023-02-17 20:01:16,907 DEBUG TRAIN Batch 9/2500 loss 12.772787 loss_att 14.184793 loss_ctc 17.849842 loss_rnnt 11.619325 hw_loss 0.363976 lr 0.00056777 rank 3
2023-02-17 20:01:16,951 DEBUG TRAIN Batch 9/2500 loss 10.592252 loss_att 15.287057 loss_ctc 15.535651 loss_rnnt 8.861663 hw_loss 0.248453 lr 0.00056793 rank 7
2023-02-17 20:02:32,669 DEBUG TRAIN Batch 9/2600 loss 13.593126 loss_att 18.529226 loss_ctc 24.277540 loss_rnnt 11.003200 hw_loss 0.333973 lr 0.00056736 rank 0
2023-02-17 20:02:32,668 DEBUG TRAIN Batch 9/2600 loss 14.279282 loss_att 21.850012 loss_ctc 33.719170 loss_rnnt 10.038036 hw_loss 0.253338 lr 0.00056741 rank 5
2023-02-17 20:02:32,671 DEBUG TRAIN Batch 9/2600 loss 17.431515 loss_att 24.644432 loss_ctc 24.015226 loss_rnnt 14.969146 hw_loss 0.266169 lr 0.00056740 rank 3
2023-02-17 20:02:32,671 DEBUG TRAIN Batch 9/2600 loss 13.985170 loss_att 17.829525 loss_ctc 24.354519 loss_rnnt 11.607343 hw_loss 0.424458 lr 0.00056752 rank 4
2023-02-17 20:02:32,673 DEBUG TRAIN Batch 9/2600 loss 8.787840 loss_att 12.135769 loss_ctc 23.097563 loss_rnnt 6.030849 hw_loss 0.336452 lr 0.00056717 rank 2
2023-02-17 20:02:32,675 DEBUG TRAIN Batch 9/2600 loss 10.852844 loss_att 11.825803 loss_ctc 14.354124 loss_rnnt 9.963780 hw_loss 0.426814 lr 0.00056727 rank 1
2023-02-17 20:02:32,676 DEBUG TRAIN Batch 9/2600 loss 17.441174 loss_att 19.889235 loss_ctc 27.734692 loss_rnnt 15.415229 hw_loss 0.307245 lr 0.00056754 rank 6
2023-02-17 20:02:32,675 DEBUG TRAIN Batch 9/2600 loss 19.628490 loss_att 27.535263 loss_ctc 28.474400 loss_rnnt 16.686050 hw_loss 0.340556 lr 0.00056756 rank 7
2023-02-17 20:03:48,686 DEBUG TRAIN Batch 9/2700 loss 9.551749 loss_att 14.528905 loss_ctc 17.594511 loss_rnnt 7.317277 hw_loss 0.312511 lr 0.00056717 rank 6
2023-02-17 20:03:48,688 DEBUG TRAIN Batch 9/2700 loss 15.128639 loss_att 19.539228 loss_ctc 25.451445 loss_rnnt 12.699400 hw_loss 0.320152 lr 0.00056715 rank 4
2023-02-17 20:03:48,689 DEBUG TRAIN Batch 9/2700 loss 8.077609 loss_att 12.070720 loss_ctc 13.530119 loss_rnnt 6.358059 hw_loss 0.363613 lr 0.00056720 rank 7
2023-02-17 20:03:48,689 DEBUG TRAIN Batch 9/2700 loss 8.957559 loss_att 14.918288 loss_ctc 12.951385 loss_rnnt 7.111711 hw_loss 0.227235 lr 0.00056704 rank 5
2023-02-17 20:03:48,690 DEBUG TRAIN Batch 9/2700 loss 15.994513 loss_att 19.207453 loss_ctc 23.238724 loss_rnnt 14.230505 hw_loss 0.291609 lr 0.00056681 rank 2
2023-02-17 20:03:48,691 DEBUG TRAIN Batch 9/2700 loss 6.465929 loss_att 10.053636 loss_ctc 12.811766 loss_rnnt 4.786036 hw_loss 0.217951 lr 0.00056690 rank 1
2023-02-17 20:03:48,695 DEBUG TRAIN Batch 9/2700 loss 12.246167 loss_att 18.082882 loss_ctc 23.145508 loss_rnnt 9.426218 hw_loss 0.373799 lr 0.00056704 rank 3
2023-02-17 20:03:48,696 DEBUG TRAIN Batch 9/2700 loss 8.869904 loss_att 13.344584 loss_ctc 20.039745 loss_rnnt 6.306478 hw_loss 0.335956 lr 0.00056699 rank 0
2023-02-17 20:05:07,031 DEBUG TRAIN Batch 9/2800 loss 13.046142 loss_att 17.639141 loss_ctc 25.835369 loss_rnnt 10.273449 hw_loss 0.279117 lr 0.00056681 rank 6
2023-02-17 20:05:07,038 DEBUG TRAIN Batch 9/2800 loss 24.150404 loss_att 27.573641 loss_ctc 41.683456 loss_rnnt 20.981705 hw_loss 0.274331 lr 0.00056667 rank 3
2023-02-17 20:05:07,039 DEBUG TRAIN Batch 9/2800 loss 11.919990 loss_att 13.124591 loss_ctc 14.803427 loss_rnnt 11.124775 hw_loss 0.318441 lr 0.00056654 rank 1
2023-02-17 20:05:07,039 DEBUG TRAIN Batch 9/2800 loss 20.765781 loss_att 22.501465 loss_ctc 32.432381 loss_rnnt 18.680883 hw_loss 0.341654 lr 0.00056668 rank 5
2023-02-17 20:05:07,039 DEBUG TRAIN Batch 9/2800 loss 17.736620 loss_att 23.031395 loss_ctc 30.799026 loss_rnnt 14.743990 hw_loss 0.360036 lr 0.00056644 rank 2
2023-02-17 20:05:07,040 DEBUG TRAIN Batch 9/2800 loss 22.014057 loss_att 24.700792 loss_ctc 37.233109 loss_rnnt 19.223904 hw_loss 0.419249 lr 0.00056679 rank 4
2023-02-17 20:05:07,040 DEBUG TRAIN Batch 9/2800 loss 13.460517 loss_att 18.507303 loss_ctc 22.631302 loss_rnnt 11.092400 hw_loss 0.254979 lr 0.00056683 rank 7
2023-02-17 20:05:07,042 DEBUG TRAIN Batch 9/2800 loss 6.626938 loss_att 8.123282 loss_ctc 9.160022 loss_rnnt 5.801865 hw_loss 0.352613 lr 0.00056663 rank 0
2023-02-17 20:06:23,873 DEBUG TRAIN Batch 9/2900 loss 17.064224 loss_att 19.630823 loss_ctc 22.535706 loss_rnnt 15.696415 hw_loss 0.234301 lr 0.00056644 rank 6
2023-02-17 20:06:23,878 DEBUG TRAIN Batch 9/2900 loss 16.595385 loss_att 21.111574 loss_ctc 30.102425 loss_rnnt 13.722992 hw_loss 0.315406 lr 0.00056647 rank 7
2023-02-17 20:06:23,881 DEBUG TRAIN Batch 9/2900 loss 7.712008 loss_att 12.427133 loss_ctc 11.604795 loss_rnnt 6.099028 hw_loss 0.282969 lr 0.00056617 rank 1
2023-02-17 20:06:23,883 DEBUG TRAIN Batch 9/2900 loss 17.557373 loss_att 19.763964 loss_ctc 32.238747 loss_rnnt 15.024268 hw_loss 0.251757 lr 0.00056631 rank 5
2023-02-17 20:06:23,883 DEBUG TRAIN Batch 9/2900 loss 19.839062 loss_att 23.528347 loss_ctc 28.083839 loss_rnnt 17.830189 hw_loss 0.321956 lr 0.00056631 rank 3
2023-02-17 20:06:23,886 DEBUG TRAIN Batch 9/2900 loss 8.819214 loss_att 10.854899 loss_ctc 16.896885 loss_rnnt 7.201088 hw_loss 0.251185 lr 0.00056608 rank 2
2023-02-17 20:06:23,885 DEBUG TRAIN Batch 9/2900 loss 30.605886 loss_att 39.704994 loss_ctc 47.898216 loss_rnnt 26.274115 hw_loss 0.386823 lr 0.00056642 rank 4
2023-02-17 20:06:23,931 DEBUG TRAIN Batch 9/2900 loss 21.938915 loss_att 27.487564 loss_ctc 33.749344 loss_rnnt 19.068789 hw_loss 0.348134 lr 0.00056627 rank 0
2023-02-17 20:07:39,704 DEBUG TRAIN Batch 9/3000 loss 11.633276 loss_att 17.405807 loss_ctc 20.890640 loss_rnnt 9.021252 hw_loss 0.418503 lr 0.00056595 rank 5
2023-02-17 20:07:39,705 DEBUG TRAIN Batch 9/3000 loss 18.864796 loss_att 21.324697 loss_ctc 34.437943 loss_rnnt 16.097179 hw_loss 0.373529 lr 0.00056611 rank 7
2023-02-17 20:07:39,706 DEBUG TRAIN Batch 9/3000 loss 17.076828 loss_att 21.949749 loss_ctc 31.590675 loss_rnnt 13.981583 hw_loss 0.347775 lr 0.00056595 rank 3
2023-02-17 20:07:39,706 DEBUG TRAIN Batch 9/3000 loss 16.461540 loss_att 24.241667 loss_ctc 22.526886 loss_rnnt 13.960735 hw_loss 0.255127 lr 0.00056581 rank 1
2023-02-17 20:07:39,708 DEBUG TRAIN Batch 9/3000 loss 20.160433 loss_att 20.885731 loss_ctc 37.773201 loss_rnnt 17.493637 hw_loss 0.325059 lr 0.00056590 rank 0
2023-02-17 20:07:39,711 DEBUG TRAIN Batch 9/3000 loss 17.541914 loss_att 20.388981 loss_ctc 29.601427 loss_rnnt 15.194691 hw_loss 0.318511 lr 0.00056608 rank 6
2023-02-17 20:07:39,712 DEBUG TRAIN Batch 9/3000 loss 24.684364 loss_att 28.005791 loss_ctc 35.104256 loss_rnnt 22.441288 hw_loss 0.355257 lr 0.00056606 rank 4
2023-02-17 20:07:39,713 DEBUG TRAIN Batch 9/3000 loss 10.863428 loss_att 15.484652 loss_ctc 21.649710 loss_rnnt 8.376297 hw_loss 0.233842 lr 0.00056572 rank 2
2023-02-17 20:08:55,743 DEBUG TRAIN Batch 9/3100 loss 13.155185 loss_att 14.247679 loss_ctc 17.857447 loss_rnnt 12.129087 hw_loss 0.338683 lr 0.00056559 rank 5
2023-02-17 20:08:55,744 DEBUG TRAIN Batch 9/3100 loss 21.343201 loss_att 25.513367 loss_ctc 38.020409 loss_rnnt 18.106430 hw_loss 0.335830 lr 0.00056570 rank 4
2023-02-17 20:08:55,744 DEBUG TRAIN Batch 9/3100 loss 16.441959 loss_att 17.003546 loss_ctc 26.633070 loss_rnnt 14.815269 hw_loss 0.291672 lr 0.00056554 rank 0
2023-02-17 20:08:55,745 DEBUG TRAIN Batch 9/3100 loss 13.380347 loss_att 14.039434 loss_ctc 23.855839 loss_rnnt 11.697505 hw_loss 0.289300 lr 0.00056572 rank 6
2023-02-17 20:08:55,748 DEBUG TRAIN Batch 9/3100 loss 13.517775 loss_att 14.861767 loss_ctc 23.906588 loss_rnnt 11.695933 hw_loss 0.314751 lr 0.00056558 rank 3
2023-02-17 20:08:55,748 DEBUG TRAIN Batch 9/3100 loss 21.497643 loss_att 22.974869 loss_ctc 32.751598 loss_rnnt 19.500546 hw_loss 0.377109 lr 0.00056574 rank 7
2023-02-17 20:08:55,749 DEBUG TRAIN Batch 9/3100 loss 24.280716 loss_att 28.063679 loss_ctc 38.390297 loss_rnnt 21.445629 hw_loss 0.369788 lr 0.00056545 rank 1
2023-02-17 20:08:55,754 DEBUG TRAIN Batch 9/3100 loss 13.379201 loss_att 16.965948 loss_ctc 25.922468 loss_rnnt 10.749736 hw_loss 0.449401 lr 0.00056536 rank 2
2023-02-17 20:10:15,914 DEBUG TRAIN Batch 9/3200 loss 22.355360 loss_att 23.233732 loss_ctc 33.783394 loss_rnnt 20.473047 hw_loss 0.342936 lr 0.00056523 rank 5
2023-02-17 20:10:15,919 DEBUG TRAIN Batch 9/3200 loss 14.852221 loss_att 19.814056 loss_ctc 33.523758 loss_rnnt 11.214760 hw_loss 0.291664 lr 0.00056522 rank 3
2023-02-17 20:10:15,920 DEBUG TRAIN Batch 9/3200 loss 5.601631 loss_att 8.410967 loss_ctc 9.541904 loss_rnnt 4.375935 hw_loss 0.259611 lr 0.00056533 rank 4
2023-02-17 20:10:15,921 DEBUG TRAIN Batch 9/3200 loss 8.399648 loss_att 9.789047 loss_ctc 13.278404 loss_rnnt 7.254307 hw_loss 0.406800 lr 0.00056518 rank 0
2023-02-17 20:10:15,922 DEBUG TRAIN Batch 9/3200 loss 15.822646 loss_att 17.576008 loss_ctc 21.124151 loss_rnnt 14.589446 hw_loss 0.329363 lr 0.00056538 rank 7
2023-02-17 20:10:15,926 DEBUG TRAIN Batch 9/3200 loss 11.753631 loss_att 17.413357 loss_ctc 23.736835 loss_rnnt 8.819358 hw_loss 0.383563 lr 0.00056500 rank 2
2023-02-17 20:10:15,928 DEBUG TRAIN Batch 9/3200 loss 16.209806 loss_att 18.249872 loss_ctc 29.262159 loss_rnnt 13.856264 hw_loss 0.384778 lr 0.00056509 rank 1
2023-02-17 20:10:15,936 DEBUG TRAIN Batch 9/3200 loss 16.425858 loss_att 21.283030 loss_ctc 25.722397 loss_rnnt 14.039370 hw_loss 0.329089 lr 0.00056536 rank 6
2023-02-17 20:11:31,257 DEBUG TRAIN Batch 9/3300 loss 9.683719 loss_att 10.644402 loss_ctc 12.728842 loss_rnnt 8.945605 hw_loss 0.262425 lr 0.00056487 rank 5
2023-02-17 20:11:31,257 DEBUG TRAIN Batch 9/3300 loss 8.758724 loss_att 11.702976 loss_ctc 13.680525 loss_rnnt 7.316153 hw_loss 0.370276 lr 0.00056497 rank 4
2023-02-17 20:11:31,260 DEBUG TRAIN Batch 9/3300 loss 18.952831 loss_att 22.455299 loss_ctc 34.060577 loss_rnnt 16.010462 hw_loss 0.426579 lr 0.00056502 rank 7
2023-02-17 20:11:31,260 DEBUG TRAIN Batch 9/3300 loss 17.276909 loss_att 19.302860 loss_ctc 33.202637 loss_rnnt 14.590075 hw_loss 0.296649 lr 0.00056500 rank 6
2023-02-17 20:11:31,261 DEBUG TRAIN Batch 9/3300 loss 12.275918 loss_att 13.399608 loss_ctc 20.463383 loss_rnnt 10.775522 hw_loss 0.344992 lr 0.00056486 rank 3
2023-02-17 20:11:31,261 DEBUG TRAIN Batch 9/3300 loss 12.732293 loss_att 16.468121 loss_ctc 22.991112 loss_rnnt 10.456451 hw_loss 0.301564 lr 0.00056473 rank 1
2023-02-17 20:11:31,262 DEBUG TRAIN Batch 9/3300 loss 9.551246 loss_att 13.280892 loss_ctc 20.133011 loss_rnnt 7.198712 hw_loss 0.366942 lr 0.00056482 rank 0
2023-02-17 20:11:31,264 DEBUG TRAIN Batch 9/3300 loss 22.070362 loss_att 30.397356 loss_ctc 25.828060 loss_rnnt 19.724949 hw_loss 0.335596 lr 0.00056463 rank 2
2023-02-17 20:12:47,107 DEBUG TRAIN Batch 9/3400 loss 23.032776 loss_att 26.780960 loss_ctc 39.016823 loss_rnnt 19.970558 hw_loss 0.340081 lr 0.00056451 rank 5
2023-02-17 20:12:47,108 DEBUG TRAIN Batch 9/3400 loss 11.339252 loss_att 15.270620 loss_ctc 23.599747 loss_rnnt 8.743189 hw_loss 0.328234 lr 0.00056428 rank 2
2023-02-17 20:12:47,108 DEBUG TRAIN Batch 9/3400 loss 19.523716 loss_att 24.163998 loss_ctc 32.559830 loss_rnnt 16.697945 hw_loss 0.299185 lr 0.00056466 rank 7
2023-02-17 20:12:47,109 DEBUG TRAIN Batch 9/3400 loss 16.765770 loss_att 20.847885 loss_ctc 26.686100 loss_rnnt 14.424262 hw_loss 0.379449 lr 0.00056463 rank 6
2023-02-17 20:12:47,109 DEBUG TRAIN Batch 9/3400 loss 6.661294 loss_att 10.297088 loss_ctc 14.106438 loss_rnnt 4.791990 hw_loss 0.280236 lr 0.00056437 rank 1
2023-02-17 20:12:47,110 DEBUG TRAIN Batch 9/3400 loss 11.491834 loss_att 16.223982 loss_ctc 17.164341 loss_rnnt 9.627199 hw_loss 0.303508 lr 0.00056450 rank 3
2023-02-17 20:12:47,131 DEBUG TRAIN Batch 9/3400 loss 21.127687 loss_att 25.161476 loss_ctc 34.803417 loss_rnnt 18.302868 hw_loss 0.364931 lr 0.00056446 rank 0
2023-02-17 20:12:47,140 DEBUG TRAIN Batch 9/3400 loss 8.171743 loss_att 15.819582 loss_ctc 11.517776 loss_rnnt 5.987972 hw_loss 0.390125 lr 0.00056461 rank 4
2023-02-17 20:14:05,390 DEBUG TRAIN Batch 9/3500 loss 10.200344 loss_att 18.420822 loss_ctc 18.394379 loss_rnnt 7.298959 hw_loss 0.308910 lr 0.00056428 rank 6
2023-02-17 20:14:05,391 DEBUG TRAIN Batch 9/3500 loss 9.750498 loss_att 12.874669 loss_ctc 16.100012 loss_rnnt 8.123144 hw_loss 0.292346 lr 0.00056401 rank 1
2023-02-17 20:14:05,395 DEBUG TRAIN Batch 9/3500 loss 21.989862 loss_att 24.654690 loss_ctc 31.361572 loss_rnnt 20.033588 hw_loss 0.325775 lr 0.00056414 rank 3
2023-02-17 20:14:05,397 DEBUG TRAIN Batch 9/3500 loss 4.899362 loss_att 9.133000 loss_ctc 8.206104 loss_rnnt 3.455862 hw_loss 0.292260 lr 0.00056410 rank 0
2023-02-17 20:14:05,400 DEBUG TRAIN Batch 9/3500 loss 17.206463 loss_att 22.157946 loss_ctc 35.099846 loss_rnnt 13.671865 hw_loss 0.297215 lr 0.00056415 rank 5
2023-02-17 20:14:05,400 DEBUG TRAIN Batch 9/3500 loss 18.719618 loss_att 21.432898 loss_ctc 30.158537 loss_rnnt 16.467627 hw_loss 0.345275 lr 0.00056430 rank 7
2023-02-17 20:14:05,401 DEBUG TRAIN Batch 9/3500 loss 11.479904 loss_att 15.281574 loss_ctc 18.876350 loss_rnnt 9.567276 hw_loss 0.311439 lr 0.00056425 rank 4
2023-02-17 20:14:05,404 DEBUG TRAIN Batch 9/3500 loss 10.316946 loss_att 11.822856 loss_ctc 14.960363 loss_rnnt 9.223899 hw_loss 0.323892 lr 0.00056392 rank 2
2023-02-17 20:15:22,719 DEBUG TRAIN Batch 9/3600 loss 22.568007 loss_att 24.759886 loss_ctc 39.722198 loss_rnnt 19.753246 hw_loss 0.167170 lr 0.00056389 rank 4
2023-02-17 20:15:22,720 DEBUG TRAIN Batch 9/3600 loss 19.034325 loss_att 21.342522 loss_ctc 29.202345 loss_rnnt 17.110329 hw_loss 0.199914 lr 0.00056379 rank 5
2023-02-17 20:15:22,721 DEBUG TRAIN Batch 9/3600 loss 10.946675 loss_att 14.406157 loss_ctc 20.314812 loss_rnnt 8.844988 hw_loss 0.301325 lr 0.00056374 rank 0
2023-02-17 20:15:22,723 DEBUG TRAIN Batch 9/3600 loss 7.096203 loss_att 9.754977 loss_ctc 16.572653 loss_rnnt 5.161099 hw_loss 0.262168 lr 0.00056394 rank 7
2023-02-17 20:15:22,724 DEBUG TRAIN Batch 9/3600 loss 17.665390 loss_att 22.951914 loss_ctc 35.646683 loss_rnnt 14.016459 hw_loss 0.363976 lr 0.00056378 rank 3
2023-02-17 20:15:22,724 DEBUG TRAIN Batch 9/3600 loss 20.719137 loss_att 22.473595 loss_ctc 36.184456 loss_rnnt 18.125977 hw_loss 0.337920 lr 0.00056392 rank 6
2023-02-17 20:15:22,727 DEBUG TRAIN Batch 9/3600 loss 14.562492 loss_att 20.854143 loss_ctc 29.234770 loss_rnnt 11.180462 hw_loss 0.313869 lr 0.00056365 rank 1
2023-02-17 20:15:22,727 DEBUG TRAIN Batch 9/3600 loss 11.503251 loss_att 18.065048 loss_ctc 19.628447 loss_rnnt 8.963514 hw_loss 0.270035 lr 0.00056356 rank 2
2023-02-17 20:16:39,149 DEBUG TRAIN Batch 9/3700 loss 10.796371 loss_att 14.350115 loss_ctc 17.167408 loss_rnnt 8.995617 hw_loss 0.451003 lr 0.00056354 rank 4
2023-02-17 20:16:39,151 DEBUG TRAIN Batch 9/3700 loss 26.907064 loss_att 29.079613 loss_ctc 44.046627 loss_rnnt 23.973545 hw_loss 0.400749 lr 0.00056356 rank 6
2023-02-17 20:16:39,151 DEBUG TRAIN Batch 9/3700 loss 11.631560 loss_att 15.545647 loss_ctc 20.267296 loss_rnnt 9.490245 hw_loss 0.388251 lr 0.00056343 rank 3
2023-02-17 20:16:39,153 DEBUG TRAIN Batch 9/3700 loss 27.781096 loss_att 29.451544 loss_ctc 40.934875 loss_rnnt 25.484030 hw_loss 0.392135 lr 0.00056358 rank 7
2023-02-17 20:16:39,154 DEBUG TRAIN Batch 9/3700 loss 10.899708 loss_att 13.241547 loss_ctc 19.474394 loss_rnnt 9.121628 hw_loss 0.312040 lr 0.00056320 rank 2
2023-02-17 20:16:39,153 DEBUG TRAIN Batch 9/3700 loss 17.837025 loss_att 21.126322 loss_ctc 29.305851 loss_rnnt 15.506874 hw_loss 0.268340 lr 0.00056329 rank 1
2023-02-17 20:16:39,156 DEBUG TRAIN Batch 9/3700 loss 10.714092 loss_att 17.744440 loss_ctc 25.218510 loss_rnnt 7.139923 hw_loss 0.439080 lr 0.00056338 rank 0
2023-02-17 20:16:39,156 DEBUG TRAIN Batch 9/3700 loss 13.822903 loss_att 14.898015 loss_ctc 19.317265 loss_rnnt 12.691263 hw_loss 0.345065 lr 0.00056343 rank 5
2023-02-17 20:17:56,234 DEBUG TRAIN Batch 9/3800 loss 9.324227 loss_att 14.605387 loss_ctc 17.860901 loss_rnnt 6.995719 hw_loss 0.251350 lr 0.00056320 rank 6
2023-02-17 20:17:56,238 DEBUG TRAIN Batch 9/3800 loss 11.900337 loss_att 13.937537 loss_ctc 19.798044 loss_rnnt 10.263662 hw_loss 0.330388 lr 0.00056303 rank 0
2023-02-17 20:17:56,240 DEBUG TRAIN Batch 9/3800 loss 18.407959 loss_att 22.744505 loss_ctc 32.532562 loss_rnnt 15.437611 hw_loss 0.412043 lr 0.00056307 rank 3
2023-02-17 20:17:56,245 DEBUG TRAIN Batch 9/3800 loss 4.182123 loss_att 9.823532 loss_ctc 9.889396 loss_rnnt 2.139629 hw_loss 0.287329 lr 0.00056323 rank 7
2023-02-17 20:17:56,246 DEBUG TRAIN Batch 9/3800 loss 15.223852 loss_att 14.570366 loss_ctc 20.708649 loss_rnnt 14.398207 hw_loss 0.421943 lr 0.00056307 rank 5
2023-02-17 20:17:56,247 DEBUG TRAIN Batch 9/3800 loss 11.145885 loss_att 14.317616 loss_ctc 16.445145 loss_rnnt 9.627131 hw_loss 0.333453 lr 0.00056284 rank 2
2023-02-17 20:17:56,249 DEBUG TRAIN Batch 9/3800 loss 12.725460 loss_att 15.552240 loss_ctc 20.343695 loss_rnnt 10.979535 hw_loss 0.309008 lr 0.00056294 rank 1
2023-02-17 20:17:56,289 DEBUG TRAIN Batch 9/3800 loss 12.709523 loss_att 13.493032 loss_ctc 18.747873 loss_rnnt 11.555900 hw_loss 0.359643 lr 0.00056318 rank 4
2023-02-17 20:19:16,326 DEBUG TRAIN Batch 9/3900 loss 6.447362 loss_att 12.850355 loss_ctc 12.366226 loss_rnnt 4.235084 hw_loss 0.267183 lr 0.00056258 rank 1
2023-02-17 20:19:16,326 DEBUG TRAIN Batch 9/3900 loss 8.271880 loss_att 11.911434 loss_ctc 12.932696 loss_rnnt 6.776494 hw_loss 0.273813 lr 0.00056284 rank 6
2023-02-17 20:19:16,329 DEBUG TRAIN Batch 9/3900 loss 21.559856 loss_att 23.147898 loss_ctc 34.183147 loss_rnnt 19.457298 hw_loss 0.190958 lr 0.00056271 rank 5
2023-02-17 20:19:16,331 DEBUG TRAIN Batch 9/3900 loss 14.988925 loss_att 16.070301 loss_ctc 24.199211 loss_rnnt 13.356910 hw_loss 0.351941 lr 0.00056282 rank 4
2023-02-17 20:19:16,334 DEBUG TRAIN Batch 9/3900 loss 6.561256 loss_att 14.913779 loss_ctc 10.659241 loss_rnnt 4.211661 hw_loss 0.248798 lr 0.00056287 rank 7
2023-02-17 20:19:16,338 DEBUG TRAIN Batch 9/3900 loss 20.661177 loss_att 26.675650 loss_ctc 40.461224 loss_rnnt 16.652554 hw_loss 0.310732 lr 0.00056271 rank 3
2023-02-17 20:19:16,343 DEBUG TRAIN Batch 9/3900 loss 12.969933 loss_att 19.571400 loss_ctc 22.590813 loss_rnnt 10.157629 hw_loss 0.392298 lr 0.00056249 rank 2
2023-02-17 20:19:16,347 DEBUG TRAIN Batch 9/3900 loss 9.519104 loss_att 11.006902 loss_ctc 15.310766 loss_rnnt 8.328323 hw_loss 0.226875 lr 0.00056267 rank 0
2023-02-17 20:20:31,921 DEBUG TRAIN Batch 9/4000 loss 8.970578 loss_att 12.495926 loss_ctc 16.488518 loss_rnnt 7.102765 hw_loss 0.300658 lr 0.00056236 rank 5
2023-02-17 20:20:31,923 DEBUG TRAIN Batch 9/4000 loss 9.720339 loss_att 14.459660 loss_ctc 15.117266 loss_rnnt 7.831859 hw_loss 0.414423 lr 0.00056251 rank 7
2023-02-17 20:20:31,925 DEBUG TRAIN Batch 9/4000 loss 12.510700 loss_att 13.976543 loss_ctc 16.711733 loss_rnnt 11.478324 hw_loss 0.335757 lr 0.00056247 rank 4
2023-02-17 20:20:31,926 DEBUG TRAIN Batch 9/4000 loss 8.295198 loss_att 15.495864 loss_ctc 17.421019 loss_rnnt 5.517234 hw_loss 0.226978 lr 0.00056249 rank 6
2023-02-17 20:20:31,927 DEBUG TRAIN Batch 9/4000 loss 10.681027 loss_att 14.273469 loss_ctc 22.496483 loss_rnnt 8.236572 hw_loss 0.282324 lr 0.00056236 rank 3
2023-02-17 20:20:31,927 DEBUG TRAIN Batch 9/4000 loss 11.989137 loss_att 14.686620 loss_ctc 13.500328 loss_rnnt 11.023504 hw_loss 0.421208 lr 0.00056222 rank 1
2023-02-17 20:20:31,929 DEBUG TRAIN Batch 9/4000 loss 23.214014 loss_att 28.314720 loss_ctc 35.131371 loss_rnnt 20.415829 hw_loss 0.354493 lr 0.00056213 rank 2
2023-02-17 20:20:31,932 DEBUG TRAIN Batch 9/4000 loss 8.966263 loss_att 12.914801 loss_ctc 16.261986 loss_rnnt 7.010291 hw_loss 0.362814 lr 0.00056231 rank 0
2023-02-17 20:21:47,794 DEBUG TRAIN Batch 9/4100 loss 17.632534 loss_att 17.538216 loss_ctc 31.896978 loss_rnnt 15.557933 hw_loss 0.359134 lr 0.00056213 rank 6
2023-02-17 20:21:47,795 DEBUG TRAIN Batch 9/4100 loss 12.517230 loss_att 18.636484 loss_ctc 23.840282 loss_rnnt 9.594234 hw_loss 0.355136 lr 0.00056196 rank 0
2023-02-17 20:21:47,796 DEBUG TRAIN Batch 9/4100 loss 10.765084 loss_att 16.169525 loss_ctc 17.752018 loss_rnnt 8.568579 hw_loss 0.345050 lr 0.00056187 rank 1
2023-02-17 20:21:47,797 DEBUG TRAIN Batch 9/4100 loss 16.582111 loss_att 19.229561 loss_ctc 20.381290 loss_rnnt 15.372414 hw_loss 0.325593 lr 0.00056200 rank 5
2023-02-17 20:21:47,798 DEBUG TRAIN Batch 9/4100 loss 10.266744 loss_att 18.070324 loss_ctc 20.750599 loss_rnnt 7.137108 hw_loss 0.320762 lr 0.00056216 rank 7
2023-02-17 20:21:47,799 DEBUG TRAIN Batch 9/4100 loss 18.276114 loss_att 25.029072 loss_ctc 27.399300 loss_rnnt 15.513053 hw_loss 0.367585 lr 0.00056200 rank 3
2023-02-17 20:21:47,800 DEBUG TRAIN Batch 9/4100 loss 10.181825 loss_att 14.345751 loss_ctc 14.734161 loss_rnnt 8.563110 hw_loss 0.335534 lr 0.00056178 rank 2
2023-02-17 20:21:47,800 DEBUG TRAIN Batch 9/4100 loss 17.318596 loss_att 18.783684 loss_ctc 25.727219 loss_rnnt 15.705867 hw_loss 0.372303 lr 0.00056211 rank 4
2023-02-17 20:23:06,004 DEBUG TRAIN Batch 9/4200 loss 11.817174 loss_att 14.589899 loss_ctc 16.777302 loss_rnnt 10.400830 hw_loss 0.375838 lr 0.00056178 rank 6
2023-02-17 20:23:06,008 DEBUG TRAIN Batch 9/4200 loss 11.575097 loss_att 14.841850 loss_ctc 14.695036 loss_rnnt 10.337826 hw_loss 0.314867 lr 0.00056151 rank 1
2023-02-17 20:23:06,011 DEBUG TRAIN Batch 9/4200 loss 6.651310 loss_att 10.191711 loss_ctc 13.126469 loss_rnnt 4.948942 hw_loss 0.245500 lr 0.00056165 rank 5
2023-02-17 20:23:06,014 DEBUG TRAIN Batch 9/4200 loss 10.908885 loss_att 15.957724 loss_ctc 18.032106 loss_rnnt 8.802654 hw_loss 0.275063 lr 0.00056180 rank 7
2023-02-17 20:23:06,016 DEBUG TRAIN Batch 9/4200 loss 18.504965 loss_att 20.635807 loss_ctc 27.161798 loss_rnnt 16.771839 hw_loss 0.286334 lr 0.00056142 rank 2
2023-02-17 20:23:06,039 DEBUG TRAIN Batch 9/4200 loss 30.543077 loss_att 31.575333 loss_ctc 46.784653 loss_rnnt 28.050283 hw_loss 0.226503 lr 0.00056176 rank 4
2023-02-17 20:23:06,041 DEBUG TRAIN Batch 9/4200 loss 14.219945 loss_att 17.393253 loss_ctc 25.686554 loss_rnnt 11.927769 hw_loss 0.241187 lr 0.00056165 rank 3
2023-02-17 20:23:06,062 DEBUG TRAIN Batch 9/4200 loss 22.723696 loss_att 26.706507 loss_ctc 38.148556 loss_rnnt 19.677357 hw_loss 0.362120 lr 0.00056160 rank 0
2023-02-17 20:24:25,591 DEBUG TRAIN Batch 9/4300 loss 13.931455 loss_att 16.492548 loss_ctc 21.562468 loss_rnnt 12.206476 hw_loss 0.366170 lr 0.00056142 rank 6
2023-02-17 20:24:25,594 DEBUG TRAIN Batch 9/4300 loss 20.693848 loss_att 24.831337 loss_ctc 39.134926 loss_rnnt 17.308592 hw_loss 0.185530 lr 0.00056140 rank 4
2023-02-17 20:24:25,596 DEBUG TRAIN Batch 9/4300 loss 15.870849 loss_att 19.641350 loss_ctc 25.247692 loss_rnnt 13.710187 hw_loss 0.293094 lr 0.00056145 rank 7
2023-02-17 20:24:25,597 DEBUG TRAIN Batch 9/4300 loss 15.186378 loss_att 18.445488 loss_ctc 27.096508 loss_rnnt 12.749586 hw_loss 0.369285 lr 0.00056129 rank 5
2023-02-17 20:24:25,598 DEBUG TRAIN Batch 9/4300 loss 14.755163 loss_att 19.351803 loss_ctc 24.271219 loss_rnnt 12.346786 hw_loss 0.412956 lr 0.00056125 rank 0
2023-02-17 20:24:25,599 DEBUG TRAIN Batch 9/4300 loss 20.395443 loss_att 23.346657 loss_ctc 27.518248 loss_rnnt 18.688641 hw_loss 0.312848 lr 0.00056116 rank 1
2023-02-17 20:24:25,601 DEBUG TRAIN Batch 9/4300 loss 28.442867 loss_att 32.290573 loss_ctc 45.679359 loss_rnnt 25.241505 hw_loss 0.250541 lr 0.00056107 rank 2
2023-02-17 20:24:25,646 DEBUG TRAIN Batch 9/4300 loss 14.950701 loss_att 20.133333 loss_ctc 27.172506 loss_rnnt 12.135345 hw_loss 0.279854 lr 0.00056129 rank 3
2023-02-17 20:25:41,384 DEBUG TRAIN Batch 9/4400 loss 11.317553 loss_att 14.066251 loss_ctc 18.265465 loss_rnnt 9.669734 hw_loss 0.321919 lr 0.00056081 rank 1
2023-02-17 20:25:41,387 DEBUG TRAIN Batch 9/4400 loss 9.720757 loss_att 9.272032 loss_ctc 13.409781 loss_rnnt 9.057456 hw_loss 0.489709 lr 0.00056109 rank 7
2023-02-17 20:25:41,388 DEBUG TRAIN Batch 9/4400 loss 9.497296 loss_att 12.285034 loss_ctc 13.942472 loss_rnnt 8.120154 hw_loss 0.425446 lr 0.00056094 rank 5
2023-02-17 20:25:41,388 DEBUG TRAIN Batch 9/4400 loss 9.137771 loss_att 10.440048 loss_ctc 13.815059 loss_rnnt 8.036812 hw_loss 0.406621 lr 0.00056107 rank 6
2023-02-17 20:25:41,389 DEBUG TRAIN Batch 9/4400 loss 6.545537 loss_att 9.759648 loss_ctc 11.631243 loss_rnnt 5.018600 hw_loss 0.386287 lr 0.00056105 rank 4
2023-02-17 20:25:41,389 DEBUG TRAIN Batch 9/4400 loss 9.357552 loss_att 9.176174 loss_ctc 12.614337 loss_rnnt 8.717379 hw_loss 0.454145 lr 0.00056094 rank 3
2023-02-17 20:25:41,390 DEBUG TRAIN Batch 9/4400 loss 12.952745 loss_att 15.822245 loss_ctc 21.387190 loss_rnnt 11.071215 hw_loss 0.343194 lr 0.00056090 rank 0
2023-02-17 20:25:41,390 DEBUG TRAIN Batch 9/4400 loss 11.316682 loss_att 12.708757 loss_ctc 15.291493 loss_rnnt 10.282906 hw_loss 0.422599 lr 0.00056072 rank 2
2023-02-17 20:26:57,611 DEBUG TRAIN Batch 9/4500 loss 13.384444 loss_att 14.190518 loss_ctc 17.043522 loss_rnnt 12.506222 hw_loss 0.429619 lr 0.00056046 rank 1
2023-02-17 20:26:57,613 DEBUG TRAIN Batch 9/4500 loss 6.224709 loss_att 9.032610 loss_ctc 7.893634 loss_rnnt 5.249379 hw_loss 0.358549 lr 0.00056059 rank 5
2023-02-17 20:26:57,614 DEBUG TRAIN Batch 9/4500 loss 18.232431 loss_att 25.040785 loss_ctc 33.334110 loss_rnnt 14.654516 hw_loss 0.380040 lr 0.00056036 rank 2
2023-02-17 20:26:57,614 DEBUG TRAIN Batch 9/4500 loss 9.953050 loss_att 13.769160 loss_ctc 18.643143 loss_rnnt 7.838842 hw_loss 0.360576 lr 0.00056074 rank 7
2023-02-17 20:26:57,616 DEBUG TRAIN Batch 9/4500 loss 19.971281 loss_att 22.255014 loss_ctc 27.865112 loss_rnnt 18.293583 hw_loss 0.315824 lr 0.00056069 rank 4
2023-02-17 20:26:57,617 DEBUG TRAIN Batch 9/4500 loss 24.041052 loss_att 27.123287 loss_ctc 31.674385 loss_rnnt 22.228512 hw_loss 0.334344 lr 0.00056072 rank 6
2023-02-17 20:26:57,618 DEBUG TRAIN Batch 9/4500 loss 9.650451 loss_att 11.888412 loss_ctc 12.373648 loss_rnnt 8.692657 hw_loss 0.275827 lr 0.00056059 rank 3
2023-02-17 20:26:57,620 DEBUG TRAIN Batch 9/4500 loss 11.381088 loss_att 12.587950 loss_ctc 19.279354 loss_rnnt 9.875925 hw_loss 0.395042 lr 0.00056054 rank 0
2023-02-17 20:28:16,373 DEBUG TRAIN Batch 9/4600 loss 20.989954 loss_att 24.053703 loss_ctc 34.888737 loss_rnnt 18.374962 hw_loss 0.279506 lr 0.00056036 rank 6
2023-02-17 20:28:16,378 DEBUG TRAIN Batch 9/4600 loss 12.271732 loss_att 17.495188 loss_ctc 22.927994 loss_rnnt 9.599068 hw_loss 0.388386 lr 0.00056039 rank 7
2023-02-17 20:28:16,379 DEBUG TRAIN Batch 9/4600 loss 9.431859 loss_att 11.416644 loss_ctc 13.991888 loss_rnnt 8.238867 hw_loss 0.352560 lr 0.00056010 rank 1
2023-02-17 20:28:16,381 DEBUG TRAIN Batch 9/4600 loss 5.338072 loss_att 9.945471 loss_ctc 9.992144 loss_rnnt 3.642332 hw_loss 0.288221 lr 0.00056001 rank 2
2023-02-17 20:28:16,382 DEBUG TRAIN Batch 9/4600 loss 8.825144 loss_att 10.880001 loss_ctc 13.031416 loss_rnnt 7.709175 hw_loss 0.270303 lr 0.00056034 rank 4
2023-02-17 20:28:16,382 DEBUG TRAIN Batch 9/4600 loss 24.830109 loss_att 33.824654 loss_ctc 45.079037 loss_rnnt 20.105278 hw_loss 0.423869 lr 0.00056024 rank 5
2023-02-17 20:28:16,386 DEBUG TRAIN Batch 9/4600 loss 10.284268 loss_att 16.270954 loss_ctc 20.311989 loss_rnnt 7.547585 hw_loss 0.379342 lr 0.00056019 rank 0
2023-02-17 20:28:16,422 DEBUG TRAIN Batch 9/4600 loss 16.831985 loss_att 23.168388 loss_ctc 34.040398 loss_rnnt 13.121107 hw_loss 0.279644 lr 0.00056023 rank 3
2023-02-17 20:29:32,780 DEBUG TRAIN Batch 9/4700 loss 8.243560 loss_att 12.333212 loss_ctc 14.501412 loss_rnnt 6.465199 hw_loss 0.236342 lr 0.00056001 rank 6
2023-02-17 20:29:32,783 DEBUG TRAIN Batch 9/4700 loss 6.986255 loss_att 10.674738 loss_ctc 12.643359 loss_rnnt 5.321381 hw_loss 0.324182 lr 0.00055975 rank 1
2023-02-17 20:29:32,783 DEBUG TRAIN Batch 9/4700 loss 25.632278 loss_att 32.619137 loss_ctc 44.252388 loss_rnnt 21.589579 hw_loss 0.304955 lr 0.00055988 rank 3
2023-02-17 20:29:32,785 DEBUG TRAIN Batch 9/4700 loss 15.377040 loss_att 17.432022 loss_ctc 22.803310 loss_rnnt 13.781809 hw_loss 0.363873 lr 0.00056004 rank 7
2023-02-17 20:29:32,788 DEBUG TRAIN Batch 9/4700 loss 11.290231 loss_att 17.562929 loss_ctc 21.680012 loss_rnnt 8.445683 hw_loss 0.383820 lr 0.00055989 rank 5
2023-02-17 20:29:32,788 DEBUG TRAIN Batch 9/4700 loss 15.635508 loss_att 19.475866 loss_ctc 29.130613 loss_rnnt 12.938922 hw_loss 0.242188 lr 0.00055999 rank 4
2023-02-17 20:29:32,819 DEBUG TRAIN Batch 9/4700 loss 18.867929 loss_att 25.117496 loss_ctc 39.426422 loss_rnnt 14.720836 hw_loss 0.292594 lr 0.00055984 rank 0
2023-02-17 20:29:32,825 DEBUG TRAIN Batch 9/4700 loss 18.556019 loss_att 23.146460 loss_ctc 30.123550 loss_rnnt 15.959081 hw_loss 0.255963 lr 0.00055966 rank 2
2023-02-17 20:30:49,220 DEBUG TRAIN Batch 9/4800 loss 8.733727 loss_att 12.622036 loss_ctc 14.834249 loss_rnnt 6.981169 hw_loss 0.302799 lr 0.00055953 rank 5
2023-02-17 20:30:49,224 DEBUG TRAIN Batch 9/4800 loss 8.764245 loss_att 13.055153 loss_ctc 15.431385 loss_rnnt 6.820442 hw_loss 0.368755 lr 0.00055964 rank 4
2023-02-17 20:30:49,225 DEBUG TRAIN Batch 9/4800 loss 11.131017 loss_att 15.479777 loss_ctc 18.784855 loss_rnnt 9.060822 hw_loss 0.337370 lr 0.00055969 rank 7
2023-02-17 20:30:49,228 DEBUG TRAIN Batch 9/4800 loss 20.584967 loss_att 25.312250 loss_ctc 37.151978 loss_rnnt 17.263939 hw_loss 0.312444 lr 0.00055966 rank 6
2023-02-17 20:30:49,229 DEBUG TRAIN Batch 9/4800 loss 16.735048 loss_att 20.287470 loss_ctc 28.066803 loss_rnnt 14.272985 hw_loss 0.451268 lr 0.00055940 rank 1
2023-02-17 20:30:49,230 DEBUG TRAIN Batch 9/4800 loss 15.725441 loss_att 15.975651 loss_ctc 22.197462 loss_rnnt 14.664625 hw_loss 0.277196 lr 0.00055931 rank 2
2023-02-17 20:30:49,231 DEBUG TRAIN Batch 9/4800 loss 12.554694 loss_att 13.921726 loss_ctc 18.160732 loss_rnnt 11.363851 hw_loss 0.318684 lr 0.00055953 rank 3
2023-02-17 20:30:49,231 DEBUG TRAIN Batch 9/4800 loss 10.560856 loss_att 13.462839 loss_ctc 19.722208 loss_rnnt 8.543133 hw_loss 0.404649 lr 0.00055949 rank 0
2023-02-17 20:32:06,719 DEBUG TRAIN Batch 9/4900 loss 18.190992 loss_att 20.520212 loss_ctc 30.494341 loss_rnnt 15.934856 hw_loss 0.280964 lr 0.00055931 rank 6
2023-02-17 20:32:06,722 DEBUG TRAIN Batch 9/4900 loss 7.530513 loss_att 11.526636 loss_ctc 15.814936 loss_rnnt 5.409113 hw_loss 0.407973 lr 0.00055929 rank 4
2023-02-17 20:32:06,723 DEBUG TRAIN Batch 9/4900 loss 11.066605 loss_att 12.904677 loss_ctc 15.476642 loss_rnnt 9.927996 hw_loss 0.343106 lr 0.00055918 rank 3
2023-02-17 20:32:06,723 DEBUG TRAIN Batch 9/4900 loss 22.025915 loss_att 26.604609 loss_ctc 34.152390 loss_rnnt 19.370838 hw_loss 0.229638 lr 0.00055914 rank 0
2023-02-17 20:32:06,723 DEBUG TRAIN Batch 9/4900 loss 20.691019 loss_att 22.850677 loss_ctc 36.450226 loss_rnnt 18.017033 hw_loss 0.264050 lr 0.00055918 rank 5
2023-02-17 20:32:06,723 DEBUG TRAIN Batch 9/4900 loss 16.636122 loss_att 21.754379 loss_ctc 30.888687 loss_rnnt 13.543403 hw_loss 0.316358 lr 0.00055934 rank 7
2023-02-17 20:32:06,726 DEBUG TRAIN Batch 9/4900 loss 9.295426 loss_att 11.631470 loss_ctc 16.101093 loss_rnnt 7.725335 hw_loss 0.366487 lr 0.00055896 rank 2
2023-02-17 20:32:06,728 DEBUG TRAIN Batch 9/4900 loss 13.311190 loss_att 20.799307 loss_ctc 25.804634 loss_rnnt 9.938459 hw_loss 0.392465 lr 0.00055905 rank 1
2023-02-17 20:33:27,382 DEBUG TRAIN Batch 9/5000 loss 12.764848 loss_att 17.470894 loss_ctc 29.400539 loss_rnnt 9.383426 hw_loss 0.416476 lr 0.00055896 rank 6
2023-02-17 20:33:27,383 DEBUG TRAIN Batch 9/5000 loss 17.621000 loss_att 21.842588 loss_ctc 29.179516 loss_rnnt 15.024681 hw_loss 0.395373 lr 0.00055861 rank 2
2023-02-17 20:33:27,384 DEBUG TRAIN Batch 9/5000 loss 10.128372 loss_att 9.570303 loss_ctc 17.474220 loss_rnnt 9.087831 hw_loss 0.323829 lr 0.00055883 rank 3
2023-02-17 20:33:27,384 DEBUG TRAIN Batch 9/5000 loss 16.632000 loss_att 19.213978 loss_ctc 27.387552 loss_rnnt 14.528421 hw_loss 0.287081 lr 0.00055884 rank 5
2023-02-17 20:33:27,385 DEBUG TRAIN Batch 9/5000 loss 7.677935 loss_att 9.634427 loss_ctc 12.982353 loss_rnnt 6.394896 hw_loss 0.345910 lr 0.00055899 rank 7
2023-02-17 20:33:27,386 DEBUG TRAIN Batch 9/5000 loss 5.945449 loss_att 9.759068 loss_ctc 12.602682 loss_rnnt 4.074563 hw_loss 0.413495 lr 0.00055870 rank 1
2023-02-17 20:33:27,395 DEBUG TRAIN Batch 9/5000 loss 12.525191 loss_att 17.557114 loss_ctc 18.581177 loss_rnnt 10.524019 hw_loss 0.351231 lr 0.00055879 rank 0
2023-02-17 20:33:27,435 DEBUG TRAIN Batch 9/5000 loss 10.614349 loss_att 15.470424 loss_ctc 14.776587 loss_rnnt 8.892851 hw_loss 0.366224 lr 0.00055894 rank 4
2023-02-17 20:34:42,950 DEBUG TRAIN Batch 9/5100 loss 9.376139 loss_att 12.408017 loss_ctc 18.995165 loss_rnnt 7.323223 hw_loss 0.307506 lr 0.00055864 rank 7
2023-02-17 20:34:42,952 DEBUG TRAIN Batch 9/5100 loss 16.603527 loss_att 21.228380 loss_ctc 32.824780 loss_rnnt 13.305096 hw_loss 0.394928 lr 0.00055861 rank 6
2023-02-17 20:34:42,955 DEBUG TRAIN Batch 9/5100 loss 11.928163 loss_att 16.379873 loss_ctc 19.609671 loss_rnnt 9.779265 hw_loss 0.439413 lr 0.00055835 rank 1
2023-02-17 20:34:42,956 DEBUG TRAIN Batch 9/5100 loss 8.690797 loss_att 13.705644 loss_ctc 14.540550 loss_rnnt 6.679451 hw_loss 0.428268 lr 0.00055849 rank 5
2023-02-17 20:34:42,956 DEBUG TRAIN Batch 9/5100 loss 9.072842 loss_att 11.246317 loss_ctc 11.519485 loss_rnnt 8.124060 hw_loss 0.352252 lr 0.00055844 rank 0
2023-02-17 20:34:42,957 DEBUG TRAIN Batch 9/5100 loss 8.632069 loss_att 13.041412 loss_ctc 22.395576 loss_rnnt 5.730680 hw_loss 0.345722 lr 0.00055848 rank 3
2023-02-17 20:34:42,958 DEBUG TRAIN Batch 9/5100 loss 15.571196 loss_att 21.945496 loss_ctc 27.742989 loss_rnnt 12.539191 hw_loss 0.251700 lr 0.00055859 rank 4
2023-02-17 20:34:42,964 DEBUG TRAIN Batch 9/5100 loss 24.225023 loss_att 26.153080 loss_ctc 30.763281 loss_rnnt 22.813622 hw_loss 0.288792 lr 0.00055826 rank 2
2023-02-17 20:35:58,588 DEBUG TRAIN Batch 9/5200 loss 22.178045 loss_att 25.987316 loss_ctc 34.297173 loss_rnnt 19.635077 hw_loss 0.309807 lr 0.00055829 rank 7
2023-02-17 20:35:58,590 DEBUG TRAIN Batch 9/5200 loss 28.009314 loss_att 26.408897 loss_ctc 48.509644 loss_rnnt 25.442640 hw_loss 0.287583 lr 0.00055814 rank 5
2023-02-17 20:35:58,593 DEBUG TRAIN Batch 9/5200 loss 10.444927 loss_att 12.929777 loss_ctc 16.159336 loss_rnnt 9.055217 hw_loss 0.245287 lr 0.00055792 rank 2
2023-02-17 20:35:58,595 DEBUG TRAIN Batch 9/5200 loss 13.066540 loss_att 16.503042 loss_ctc 22.030109 loss_rnnt 10.978278 hw_loss 0.385910 lr 0.00055824 rank 4
2023-02-17 20:35:58,595 DEBUG TRAIN Batch 9/5200 loss 6.123963 loss_att 9.478463 loss_ctc 10.299645 loss_rnnt 4.736637 hw_loss 0.299378 lr 0.00055826 rank 6
2023-02-17 20:35:58,596 DEBUG TRAIN Batch 9/5200 loss 24.438528 loss_att 27.772644 loss_ctc 41.048096 loss_rnnt 21.394287 hw_loss 0.305267 lr 0.00055814 rank 3
2023-02-17 20:35:58,596 DEBUG TRAIN Batch 9/5200 loss 9.698627 loss_att 16.848787 loss_ctc 18.781521 loss_rnnt 6.862876 hw_loss 0.365000 lr 0.00055809 rank 0
2023-02-17 20:35:58,602 DEBUG TRAIN Batch 9/5200 loss 14.114096 loss_att 18.547218 loss_ctc 24.932461 loss_rnnt 11.698868 hw_loss 0.161541 lr 0.00055801 rank 1
2023-02-17 20:37:16,999 DEBUG TRAIN Batch 9/5300 loss 15.424762 loss_att 18.565140 loss_ctc 26.856491 loss_rnnt 13.120996 hw_loss 0.283986 lr 0.00055794 rank 7
2023-02-17 20:37:17,001 DEBUG TRAIN Batch 9/5300 loss 40.333607 loss_att 44.388268 loss_ctc 60.418350 loss_rnnt 36.700775 hw_loss 0.269873 lr 0.00055792 rank 6
2023-02-17 20:37:17,002 DEBUG TRAIN Batch 9/5300 loss 12.223930 loss_att 16.682426 loss_ctc 22.712080 loss_rnnt 9.738659 hw_loss 0.365911 lr 0.00055775 rank 0
2023-02-17 20:37:17,003 DEBUG TRAIN Batch 9/5300 loss 17.869106 loss_att 21.131163 loss_ctc 34.194511 loss_rnnt 14.848956 hw_loss 0.358161 lr 0.00055779 rank 5
2023-02-17 20:37:17,008 DEBUG TRAIN Batch 9/5300 loss 6.354365 loss_att 11.234276 loss_ctc 10.274818 loss_rnnt 4.713439 hw_loss 0.266657 lr 0.00055766 rank 1
2023-02-17 20:37:17,015 DEBUG TRAIN Batch 9/5300 loss 18.898636 loss_att 22.153591 loss_ctc 31.786743 loss_rnnt 16.368935 hw_loss 0.300554 lr 0.00055757 rank 2
2023-02-17 20:37:17,021 DEBUG TRAIN Batch 9/5300 loss 9.867827 loss_att 12.729950 loss_ctc 16.666477 loss_rnnt 8.275646 hw_loss 0.212382 lr 0.00055790 rank 4
2023-02-17 20:37:17,065 DEBUG TRAIN Batch 9/5300 loss 8.425733 loss_att 15.099829 loss_ctc 19.753525 loss_rnnt 5.479984 hw_loss 0.188545 lr 0.00055779 rank 3
2023-02-17 20:38:35,337 DEBUG TRAIN Batch 9/5400 loss 15.994740 loss_att 20.990602 loss_ctc 27.997423 loss_rnnt 13.187115 hw_loss 0.390178 lr 0.00055757 rank 6
2023-02-17 20:38:35,345 DEBUG TRAIN Batch 9/5400 loss 14.642067 loss_att 17.947603 loss_ctc 24.900743 loss_rnnt 12.418003 hw_loss 0.365874 lr 0.00055759 rank 7
2023-02-17 20:38:35,346 DEBUG TRAIN Batch 9/5400 loss 13.119914 loss_att 17.006376 loss_ctc 22.172523 loss_rnnt 10.942325 hw_loss 0.362403 lr 0.00055731 rank 1
2023-02-17 20:38:35,347 DEBUG TRAIN Batch 9/5400 loss 18.522905 loss_att 20.101820 loss_ctc 28.195778 loss_rnnt 16.757978 hw_loss 0.298924 lr 0.00055722 rank 2
2023-02-17 20:38:35,347 DEBUG TRAIN Batch 9/5400 loss 14.100294 loss_att 17.660307 loss_ctc 25.996666 loss_rnnt 11.596023 hw_loss 0.386411 lr 0.00055740 rank 0
2023-02-17 20:38:35,350 DEBUG TRAIN Batch 9/5400 loss 9.426270 loss_att 14.433954 loss_ctc 18.033306 loss_rnnt 7.106048 hw_loss 0.320779 lr 0.00055744 rank 5
2023-02-17 20:38:35,354 DEBUG TRAIN Batch 9/5400 loss 11.624434 loss_att 17.292324 loss_ctc 20.342159 loss_rnnt 9.133556 hw_loss 0.365507 lr 0.00055744 rank 3
2023-02-17 20:38:35,392 DEBUG TRAIN Batch 9/5400 loss 16.884739 loss_att 18.813110 loss_ctc 27.324642 loss_rnnt 14.909483 hw_loss 0.370491 lr 0.00055755 rank 4
2023-02-17 20:39:52,559 DEBUG TRAIN Batch 9/5500 loss 33.839970 loss_att 34.020355 loss_ctc 43.034534 loss_rnnt 32.423592 hw_loss 0.289423 lr 0.00055709 rank 3
2023-02-17 20:39:52,562 DEBUG TRAIN Batch 9/5500 loss 15.701244 loss_att 19.761055 loss_ctc 26.292734 loss_rnnt 13.324928 hw_loss 0.285292 lr 0.00055688 rank 2
2023-02-17 20:39:52,565 DEBUG TRAIN Batch 9/5500 loss 18.000914 loss_att 22.088312 loss_ctc 29.367016 loss_rnnt 15.494491 hw_loss 0.325239 lr 0.00055722 rank 6
2023-02-17 20:39:52,564 DEBUG TRAIN Batch 9/5500 loss 15.445555 loss_att 21.766380 loss_ctc 18.070801 loss_rnnt 13.701733 hw_loss 0.243044 lr 0.00055705 rank 0
2023-02-17 20:39:52,564 DEBUG TRAIN Batch 9/5500 loss 7.184072 loss_att 10.381061 loss_ctc 8.804003 loss_rnnt 6.157851 hw_loss 0.320314 lr 0.00055697 rank 1
2023-02-17 20:39:52,566 DEBUG TRAIN Batch 9/5500 loss 8.581175 loss_att 13.205078 loss_ctc 16.420712 loss_rnnt 6.424421 hw_loss 0.350065 lr 0.00055725 rank 7
2023-02-17 20:39:52,567 DEBUG TRAIN Batch 9/5500 loss 11.044211 loss_att 17.484585 loss_ctc 21.030361 loss_rnnt 8.232988 hw_loss 0.359365 lr 0.00055710 rank 5
2023-02-17 20:39:52,569 DEBUG TRAIN Batch 9/5500 loss 10.405931 loss_att 14.531591 loss_ctc 16.099575 loss_rnnt 8.667469 hw_loss 0.289082 lr 0.00055720 rank 4
2023-02-17 20:41:09,017 DEBUG TRAIN Batch 9/5600 loss 13.488674 loss_att 16.149828 loss_ctc 20.924171 loss_rnnt 11.743786 hw_loss 0.414859 lr 0.00055675 rank 3
2023-02-17 20:41:09,021 DEBUG TRAIN Batch 9/5600 loss 12.405478 loss_att 15.603806 loss_ctc 23.729116 loss_rnnt 10.087585 hw_loss 0.315769 lr 0.00055662 rank 1
2023-02-17 20:41:09,024 DEBUG TRAIN Batch 9/5600 loss 10.724327 loss_att 13.494003 loss_ctc 15.982866 loss_rnnt 9.292509 hw_loss 0.331394 lr 0.00055686 rank 4
2023-02-17 20:41:09,026 DEBUG TRAIN Batch 9/5600 loss 23.090769 loss_att 26.198231 loss_ctc 42.860825 loss_rnnt 19.614515 hw_loss 0.410160 lr 0.00055675 rank 5
2023-02-17 20:41:09,028 DEBUG TRAIN Batch 9/5600 loss 11.688127 loss_att 14.038995 loss_ctc 19.906483 loss_rnnt 9.962818 hw_loss 0.298788 lr 0.00055690 rank 7
2023-02-17 20:41:09,028 DEBUG TRAIN Batch 9/5600 loss 19.161983 loss_att 18.833872 loss_ctc 27.289543 loss_rnnt 17.938429 hw_loss 0.385320 lr 0.00055688 rank 6
2023-02-17 20:41:09,032 DEBUG TRAIN Batch 9/5600 loss 8.647890 loss_att 11.105467 loss_ctc 16.318684 loss_rnnt 6.977245 hw_loss 0.293169 lr 0.00055653 rank 2
2023-02-17 20:41:09,032 DEBUG TRAIN Batch 9/5600 loss 13.302665 loss_att 16.149017 loss_ctc 21.570568 loss_rnnt 11.482548 hw_loss 0.278361 lr 0.00055671 rank 0
2023-02-17 20:42:29,412 DEBUG TRAIN Batch 9/5700 loss 16.548059 loss_att 20.783636 loss_ctc 28.351061 loss_rnnt 13.935979 hw_loss 0.358556 lr 0.00055628 rank 1
2023-02-17 20:42:29,412 DEBUG TRAIN Batch 9/5700 loss 9.372480 loss_att 14.752096 loss_ctc 17.207687 loss_rnnt 7.079516 hw_loss 0.323149 lr 0.00055653 rank 6
2023-02-17 20:42:29,413 DEBUG TRAIN Batch 9/5700 loss 23.096521 loss_att 44.058437 loss_ctc 41.623783 loss_rnnt 16.290850 hw_loss 0.268099 lr 0.00055619 rank 2
2023-02-17 20:42:29,412 DEBUG TRAIN Batch 9/5700 loss 10.709656 loss_att 12.324488 loss_ctc 16.319366 loss_rnnt 9.448808 hw_loss 0.356099 lr 0.00055651 rank 4
2023-02-17 20:42:29,413 DEBUG TRAIN Batch 9/5700 loss 17.892677 loss_att 21.682503 loss_ctc 23.774792 loss_rnnt 16.250280 hw_loss 0.187780 lr 0.00055641 rank 5
2023-02-17 20:42:29,416 DEBUG TRAIN Batch 9/5700 loss 11.934494 loss_att 10.288952 loss_ctc 17.062744 loss_rnnt 11.373833 hw_loss 0.386258 lr 0.00055640 rank 3
2023-02-17 20:42:29,417 DEBUG TRAIN Batch 9/5700 loss 23.466957 loss_att 28.433472 loss_ctc 37.059998 loss_rnnt 20.417482 hw_loss 0.457064 lr 0.00055636 rank 0
2023-02-17 20:42:29,463 DEBUG TRAIN Batch 9/5700 loss 19.734207 loss_att 21.268875 loss_ctc 27.675581 loss_rnnt 18.239222 hw_loss 0.242253 lr 0.00055656 rank 7
2023-02-17 20:43:47,227 DEBUG TRAIN Batch 9/5800 loss 22.520184 loss_att 24.010176 loss_ctc 33.673382 loss_rnnt 20.577101 hw_loss 0.296237 lr 0.00055621 rank 7
2023-02-17 20:43:47,228 DEBUG TRAIN Batch 9/5800 loss 21.442244 loss_att 20.950560 loss_ctc 30.539848 loss_rnnt 20.162140 hw_loss 0.310174 lr 0.00055619 rank 6
2023-02-17 20:43:47,230 DEBUG TRAIN Batch 9/5800 loss 9.745184 loss_att 10.976048 loss_ctc 12.897816 loss_rnnt 8.863317 hw_loss 0.403766 lr 0.00055593 rank 1
2023-02-17 20:43:47,232 DEBUG TRAIN Batch 9/5800 loss 12.465513 loss_att 13.664402 loss_ctc 18.313164 loss_rnnt 11.237558 hw_loss 0.390917 lr 0.00055617 rank 4
2023-02-17 20:43:47,235 DEBUG TRAIN Batch 9/5800 loss 14.468473 loss_att 21.053133 loss_ctc 24.664740 loss_rnnt 11.619708 hw_loss 0.323119 lr 0.00055606 rank 3
2023-02-17 20:43:47,236 DEBUG TRAIN Batch 9/5800 loss 17.342396 loss_att 15.619839 loss_ctc 23.598465 loss_rnnt 16.627434 hw_loss 0.422495 lr 0.00055602 rank 0
2023-02-17 20:43:47,237 DEBUG TRAIN Batch 9/5800 loss 12.924241 loss_att 17.618210 loss_ctc 22.736160 loss_rnnt 10.574959 hw_loss 0.191686 lr 0.00055606 rank 5
2023-02-17 20:43:47,284 DEBUG TRAIN Batch 9/5800 loss 19.307274 loss_att 24.175114 loss_ctc 29.869246 loss_rnnt 16.772987 hw_loss 0.285856 lr 0.00055584 rank 2
2023-02-17 20:45:06,046 DEBUG TRAIN Batch 9/5900 loss 6.119704 loss_att 9.011473 loss_ctc 10.594153 loss_rnnt 4.797390 hw_loss 0.276314 lr 0.00055572 rank 5
2023-02-17 20:45:06,049 DEBUG TRAIN Batch 9/5900 loss 11.164666 loss_att 13.561270 loss_ctc 20.227249 loss_rnnt 9.314472 hw_loss 0.304740 lr 0.00055584 rank 6
2023-02-17 20:45:06,054 DEBUG TRAIN Batch 9/5900 loss 39.379116 loss_att 43.798733 loss_ctc 62.588768 loss_rnnt 35.222778 hw_loss 0.333364 lr 0.00055587 rank 7
2023-02-17 20:45:06,057 DEBUG TRAIN Batch 9/5900 loss 15.326637 loss_att 16.963236 loss_ctc 30.663412 loss_rnnt 12.734659 hw_loss 0.412039 lr 0.00055582 rank 4
2023-02-17 20:45:06,058 DEBUG TRAIN Batch 9/5900 loss 12.999883 loss_att 16.923491 loss_ctc 24.088867 loss_rnnt 10.544638 hw_loss 0.359986 lr 0.00055550 rank 2
2023-02-17 20:45:06,059 DEBUG TRAIN Batch 9/5900 loss 10.575953 loss_att 13.175940 loss_ctc 20.863007 loss_rnnt 8.479919 hw_loss 0.383306 lr 0.00055568 rank 0
2023-02-17 20:45:06,066 DEBUG TRAIN Batch 9/5900 loss 16.764189 loss_att 21.933350 loss_ctc 30.846661 loss_rnnt 13.661247 hw_loss 0.358963 lr 0.00055559 rank 1
2023-02-17 20:45:06,104 DEBUG TRAIN Batch 9/5900 loss 13.720491 loss_att 16.056524 loss_ctc 20.172842 loss_rnnt 12.240134 hw_loss 0.286571 lr 0.00055572 rank 3
2023-02-17 20:46:25,205 DEBUG TRAIN Batch 9/6000 loss 10.753870 loss_att 12.785955 loss_ctc 15.851118 loss_rnnt 9.491647 hw_loss 0.330324 lr 0.00055552 rank 7
2023-02-17 20:46:25,209 DEBUG TRAIN Batch 9/6000 loss 20.094837 loss_att 23.731056 loss_ctc 36.319454 loss_rnnt 17.022545 hw_loss 0.340815 lr 0.00055537 rank 3
2023-02-17 20:46:25,209 DEBUG TRAIN Batch 9/6000 loss 16.384228 loss_att 22.674541 loss_ctc 25.418455 loss_rnnt 13.749975 hw_loss 0.321796 lr 0.00055533 rank 0
2023-02-17 20:46:25,211 DEBUG TRAIN Batch 9/6000 loss 15.028296 loss_att 18.838970 loss_ctc 30.677788 loss_rnnt 12.031364 hw_loss 0.277873 lr 0.00055538 rank 5
2023-02-17 20:46:25,213 DEBUG TRAIN Batch 9/6000 loss 17.594145 loss_att 19.009207 loss_ctc 26.739803 loss_rnnt 15.842806 hw_loss 0.466694 lr 0.00055550 rank 6
2023-02-17 20:46:25,215 DEBUG TRAIN Batch 9/6000 loss 13.025765 loss_att 15.675032 loss_ctc 22.353176 loss_rnnt 11.055863 hw_loss 0.368235 lr 0.00055516 rank 2
2023-02-17 20:46:25,215 DEBUG TRAIN Batch 9/6000 loss 12.203225 loss_att 16.889065 loss_ctc 22.869949 loss_rnnt 9.688480 hw_loss 0.291277 lr 0.00055548 rank 4
2023-02-17 20:46:25,216 DEBUG TRAIN Batch 9/6000 loss 7.015928 loss_att 9.256690 loss_ctc 10.317564 loss_rnnt 5.972759 hw_loss 0.290246 lr 0.00055525 rank 1
2023-02-17 20:47:44,981 DEBUG TRAIN Batch 9/6100 loss 13.369635 loss_att 16.167675 loss_ctc 20.991432 loss_rnnt 11.582689 hw_loss 0.395808 lr 0.00055516 rank 6
2023-02-17 20:47:44,981 DEBUG TRAIN Batch 9/6100 loss 21.931583 loss_att 28.231735 loss_ctc 44.438698 loss_rnnt 17.489693 hw_loss 0.339209 lr 0.00055514 rank 4
2023-02-17 20:47:44,981 DEBUG TRAIN Batch 9/6100 loss 10.993085 loss_att 17.164698 loss_ctc 18.739677 loss_rnnt 8.607255 hw_loss 0.222428 lr 0.00055504 rank 5
2023-02-17 20:47:44,982 DEBUG TRAIN Batch 9/6100 loss 15.384950 loss_att 18.794533 loss_ctc 25.642853 loss_rnnt 13.172659 hw_loss 0.304977 lr 0.00055503 rank 3
2023-02-17 20:47:44,982 DEBUG TRAIN Batch 9/6100 loss 22.242472 loss_att 29.052776 loss_ctc 37.337601 loss_rnnt 18.699005 hw_loss 0.316346 lr 0.00055518 rank 7
2023-02-17 20:47:44,982 DEBUG TRAIN Batch 9/6100 loss 5.745337 loss_att 12.118802 loss_ctc 13.160239 loss_rnnt 3.308875 hw_loss 0.324591 lr 0.00055491 rank 1
2023-02-17 20:47:44,984 DEBUG TRAIN Batch 9/6100 loss 15.420746 loss_att 21.234983 loss_ctc 28.086365 loss_rnnt 12.410665 hw_loss 0.297157 lr 0.00055482 rank 2
2023-02-17 20:47:44,986 DEBUG TRAIN Batch 9/6100 loss 15.772446 loss_att 20.405640 loss_ctc 24.469635 loss_rnnt 13.487783 hw_loss 0.371998 lr 0.00055499 rank 0
2023-02-17 20:49:01,786 DEBUG TRAIN Batch 9/6200 loss 16.175665 loss_att 17.981031 loss_ctc 24.616051 loss_rnnt 14.480221 hw_loss 0.391850 lr 0.00055469 rank 3
2023-02-17 20:49:01,786 DEBUG TRAIN Batch 9/6200 loss 11.625555 loss_att 14.878160 loss_ctc 20.460718 loss_rnnt 9.575907 hw_loss 0.414573 lr 0.00055482 rank 6
2023-02-17 20:49:01,792 DEBUG TRAIN Batch 9/6200 loss 16.338720 loss_att 22.194973 loss_ctc 35.628983 loss_rnnt 12.459225 hw_loss 0.255395 lr 0.00055448 rank 2
2023-02-17 20:49:01,794 DEBUG TRAIN Batch 9/6200 loss 19.003654 loss_att 22.683882 loss_ctc 31.250803 loss_rnnt 16.374985 hw_loss 0.486883 lr 0.00055484 rank 7
2023-02-17 20:49:01,795 DEBUG TRAIN Batch 9/6200 loss 12.290436 loss_att 15.274403 loss_ctc 16.509443 loss_rnnt 10.942526 hw_loss 0.353592 lr 0.00055456 rank 1
2023-02-17 20:49:01,795 DEBUG TRAIN Batch 9/6200 loss 13.471185 loss_att 15.710445 loss_ctc 20.748491 loss_rnnt 11.766582 hw_loss 0.537083 lr 0.00055469 rank 5
2023-02-17 20:49:01,798 DEBUG TRAIN Batch 9/6200 loss 12.600317 loss_att 15.369808 loss_ctc 17.898512 loss_rnnt 11.144901 hw_loss 0.365797 lr 0.00055465 rank 0
2023-02-17 20:49:01,841 DEBUG TRAIN Batch 9/6200 loss 19.510294 loss_att 20.774845 loss_ctc 29.326553 loss_rnnt 17.792564 hw_loss 0.292471 lr 0.00055480 rank 4
2023-02-17 20:50:20,959 DEBUG TRAIN Batch 9/6300 loss 13.778151 loss_att 14.100489 loss_ctc 19.458511 loss_rnnt 12.722931 hw_loss 0.437570 lr 0.00055448 rank 6
2023-02-17 20:50:20,965 DEBUG TRAIN Batch 9/6300 loss 17.253344 loss_att 20.822685 loss_ctc 32.973293 loss_rnnt 14.257726 hw_loss 0.348292 lr 0.00055422 rank 1
2023-02-17 20:50:20,967 DEBUG TRAIN Batch 9/6300 loss 6.941349 loss_att 8.234962 loss_ctc 10.712653 loss_rnnt 6.018245 hw_loss 0.302890 lr 0.00055435 rank 5
2023-02-17 20:50:20,968 DEBUG TRAIN Batch 9/6300 loss 11.287765 loss_att 11.996489 loss_ctc 18.067413 loss_rnnt 10.072280 hw_loss 0.318349 lr 0.00055435 rank 3
2023-02-17 20:50:20,970 DEBUG TRAIN Batch 9/6300 loss 11.061144 loss_att 15.676646 loss_ctc 20.185959 loss_rnnt 8.721926 hw_loss 0.374016 lr 0.00055445 rank 4
2023-02-17 20:50:20,970 DEBUG TRAIN Batch 9/6300 loss 10.972939 loss_att 14.259712 loss_ctc 14.115782 loss_rnnt 9.697255 hw_loss 0.373654 lr 0.00055431 rank 0
2023-02-17 20:50:20,971 DEBUG TRAIN Batch 9/6300 loss 5.264613 loss_att 11.198400 loss_ctc 10.880015 loss_rnnt 3.149814 hw_loss 0.336227 lr 0.00055450 rank 7
2023-02-17 20:50:20,978 DEBUG TRAIN Batch 9/6300 loss 15.825957 loss_att 20.109854 loss_ctc 23.934628 loss_rnnt 13.757154 hw_loss 0.245376 lr 0.00055413 rank 2
2023-02-17 20:51:42,323 DEBUG TRAIN Batch 9/6400 loss 8.007848 loss_att 11.425392 loss_ctc 14.998947 loss_rnnt 6.211371 hw_loss 0.339039 lr 0.00055416 rank 7
2023-02-17 20:51:42,322 DEBUG TRAIN Batch 9/6400 loss 10.830933 loss_att 13.703187 loss_ctc 16.964951 loss_rnnt 9.297626 hw_loss 0.264352 lr 0.00055413 rank 6
2023-02-17 20:51:42,323 DEBUG TRAIN Batch 9/6400 loss 18.248268 loss_att 20.979168 loss_ctc 27.667622 loss_rnnt 16.286240 hw_loss 0.299880 lr 0.00055388 rank 1
2023-02-17 20:51:42,326 DEBUG TRAIN Batch 9/6400 loss 8.519033 loss_att 11.171923 loss_ctc 12.006910 loss_rnnt 7.346375 hw_loss 0.331929 lr 0.00055379 rank 2
2023-02-17 20:51:42,329 DEBUG TRAIN Batch 9/6400 loss 12.106877 loss_att 20.996195 loss_ctc 31.316372 loss_rnnt 7.597157 hw_loss 0.319858 lr 0.00055397 rank 0
2023-02-17 20:51:42,329 DEBUG TRAIN Batch 9/6400 loss 14.331672 loss_att 17.922396 loss_ctc 21.759298 loss_rnnt 12.371215 hw_loss 0.472428 lr 0.00055411 rank 4
2023-02-17 20:51:42,334 DEBUG TRAIN Batch 9/6400 loss 19.087309 loss_att 19.538736 loss_ctc 28.438423 loss_rnnt 17.608393 hw_loss 0.265904 lr 0.00055401 rank 3
2023-02-17 20:51:42,346 DEBUG TRAIN Batch 9/6400 loss 12.131846 loss_att 16.470421 loss_ctc 17.161926 loss_rnnt 10.468250 hw_loss 0.234759 lr 0.00055401 rank 5
2023-02-17 20:52:58,957 DEBUG TRAIN Batch 9/6500 loss 16.617548 loss_att 19.126350 loss_ctc 20.990543 loss_rnnt 15.288559 hw_loss 0.457805 lr 0.00055382 rank 7
2023-02-17 20:52:58,960 DEBUG TRAIN Batch 9/6500 loss 15.507165 loss_att 15.023251 loss_ctc 19.608536 loss_rnnt 14.841190 hw_loss 0.404828 lr 0.00055346 rank 2
2023-02-17 20:52:58,960 DEBUG TRAIN Batch 9/6500 loss 15.747154 loss_att 19.106623 loss_ctc 28.496181 loss_rnnt 13.198966 hw_loss 0.330797 lr 0.00055354 rank 1
2023-02-17 20:52:58,960 DEBUG TRAIN Batch 9/6500 loss 21.064188 loss_att 23.375334 loss_ctc 33.654640 loss_rnnt 18.729044 hw_loss 0.364105 lr 0.00055379 rank 6
2023-02-17 20:52:58,961 DEBUG TRAIN Batch 9/6500 loss 15.470233 loss_att 20.992859 loss_ctc 22.039062 loss_rnnt 13.275645 hw_loss 0.401661 lr 0.00055377 rank 4
2023-02-17 20:52:58,962 DEBUG TRAIN Batch 9/6500 loss 11.190325 loss_att 15.669417 loss_ctc 24.667990 loss_rnnt 8.350718 hw_loss 0.275188 lr 0.00055367 rank 5
2023-02-17 20:52:58,963 DEBUG TRAIN Batch 9/6500 loss 21.551323 loss_att 21.704512 loss_ctc 36.439865 loss_rnnt 19.375824 hw_loss 0.299481 lr 0.00055367 rank 3
2023-02-17 20:52:58,965 DEBUG TRAIN Batch 9/6500 loss 6.676760 loss_att 10.906239 loss_ctc 11.566531 loss_rnnt 5.010777 hw_loss 0.315221 lr 0.00055363 rank 0
2023-02-17 20:54:16,202 DEBUG TRAIN Batch 9/6600 loss 15.727645 loss_att 17.190987 loss_ctc 23.622200 loss_rnnt 14.176360 hw_loss 0.386269 lr 0.00055333 rank 3
2023-02-17 20:54:16,203 DEBUG TRAIN Batch 9/6600 loss 13.352479 loss_att 17.061710 loss_ctc 26.825943 loss_rnnt 10.715295 hw_loss 0.185391 lr 0.00055348 rank 7
2023-02-17 20:54:16,206 DEBUG TRAIN Batch 9/6600 loss 12.840681 loss_att 16.039454 loss_ctc 25.406597 loss_rnnt 10.334639 hw_loss 0.357810 lr 0.00055333 rank 5
2023-02-17 20:54:16,209 DEBUG TRAIN Batch 9/6600 loss 17.810160 loss_att 23.503056 loss_ctc 42.640564 loss_rnnt 13.187393 hw_loss 0.325246 lr 0.00055312 rank 2
2023-02-17 20:54:16,209 DEBUG TRAIN Batch 9/6600 loss 16.199192 loss_att 22.991125 loss_ctc 30.148769 loss_rnnt 12.807989 hw_loss 0.324135 lr 0.00055343 rank 4
2023-02-17 20:54:16,209 DEBUG TRAIN Batch 9/6600 loss 20.814775 loss_att 27.765196 loss_ctc 33.349426 loss_rnnt 17.613111 hw_loss 0.263052 lr 0.00055346 rank 6
2023-02-17 20:54:16,210 DEBUG TRAIN Batch 9/6600 loss 23.050011 loss_att 25.208984 loss_ctc 31.731068 loss_rnnt 21.311844 hw_loss 0.279183 lr 0.00055329 rank 0
2023-02-17 20:54:16,214 DEBUG TRAIN Batch 9/6600 loss 24.189774 loss_att 30.325096 loss_ctc 55.861366 loss_rnnt 18.504257 hw_loss 0.441699 lr 0.00055320 rank 1
2023-02-17 20:55:34,869 DEBUG TRAIN Batch 9/6700 loss 10.467703 loss_att 16.914936 loss_ctc 17.996857 loss_rnnt 8.019119 hw_loss 0.291095 lr 0.00055312 rank 6
2023-02-17 20:55:34,873 DEBUG TRAIN Batch 9/6700 loss 19.452929 loss_att 23.039484 loss_ctc 34.549404 loss_rnnt 16.614182 hw_loss 0.203570 lr 0.00055287 rank 1
2023-02-17 20:55:34,876 DEBUG TRAIN Batch 9/6700 loss 20.052671 loss_att 22.429710 loss_ctc 34.653435 loss_rnnt 17.483261 hw_loss 0.276064 lr 0.00055278 rank 2
2023-02-17 20:55:34,877 DEBUG TRAIN Batch 9/6700 loss 12.084051 loss_att 16.066515 loss_ctc 17.141636 loss_rnnt 10.420821 hw_loss 0.360736 lr 0.00055299 rank 3
2023-02-17 20:55:34,878 DEBUG TRAIN Batch 9/6700 loss 14.884037 loss_att 19.577812 loss_ctc 23.935745 loss_rnnt 12.531009 hw_loss 0.388834 lr 0.00055299 rank 5
2023-02-17 20:55:34,878 DEBUG TRAIN Batch 9/6700 loss 14.359190 loss_att 15.987394 loss_ctc 22.962086 loss_rnnt 12.721260 hw_loss 0.309817 lr 0.00055295 rank 0
2023-02-17 20:55:34,880 DEBUG TRAIN Batch 9/6700 loss 17.020924 loss_att 21.733473 loss_ctc 25.766592 loss_rnnt 14.699553 hw_loss 0.398952 lr 0.00055314 rank 7
2023-02-17 20:55:34,898 DEBUG TRAIN Batch 9/6700 loss 12.709539 loss_att 20.376606 loss_ctc 20.364141 loss_rnnt 10.042477 hw_loss 0.211941 lr 0.00055310 rank 4
2023-02-17 20:56:54,285 DEBUG TRAIN Batch 9/6800 loss 18.746370 loss_att 20.249889 loss_ctc 31.655861 loss_rnnt 16.560892 hw_loss 0.306578 lr 0.00055278 rank 6
2023-02-17 20:56:54,287 DEBUG TRAIN Batch 9/6800 loss 27.489378 loss_att 31.667068 loss_ctc 47.930038 loss_rnnt 23.734270 hw_loss 0.364025 lr 0.00055253 rank 1
2023-02-17 20:56:54,287 DEBUG TRAIN Batch 9/6800 loss 14.835105 loss_att 19.332233 loss_ctc 29.906565 loss_rnnt 11.819178 hw_loss 0.200575 lr 0.00055280 rank 7
2023-02-17 20:56:54,289 DEBUG TRAIN Batch 9/6800 loss 11.274421 loss_att 15.593060 loss_ctc 19.982124 loss_rnnt 9.086954 hw_loss 0.305083 lr 0.00055266 rank 5
2023-02-17 20:56:54,291 DEBUG TRAIN Batch 9/6800 loss 18.388372 loss_att 18.005922 loss_ctc 23.488216 loss_rnnt 17.572353 hw_loss 0.398494 lr 0.00055276 rank 4
2023-02-17 20:56:54,293 DEBUG TRAIN Batch 9/6800 loss 7.707792 loss_att 9.163223 loss_ctc 11.312098 loss_rnnt 6.777387 hw_loss 0.297646 lr 0.00055265 rank 3
2023-02-17 20:56:54,295 DEBUG TRAIN Batch 9/6800 loss 14.618275 loss_att 19.335684 loss_ctc 26.342991 loss_rnnt 11.960392 hw_loss 0.283321 lr 0.00055261 rank 0
2023-02-17 20:56:54,295 DEBUG TRAIN Batch 9/6800 loss 17.759991 loss_att 21.391705 loss_ctc 29.081955 loss_rnnt 15.374477 hw_loss 0.280451 lr 0.00055244 rank 2
2023-02-17 20:58:11,985 DEBUG TRAIN Batch 9/6900 loss 11.468136 loss_att 12.623881 loss_ctc 19.013235 loss_rnnt 10.036990 hw_loss 0.363719 lr 0.00055244 rank 6
2023-02-17 20:58:11,990 DEBUG TRAIN Batch 9/6900 loss 13.026104 loss_att 14.523119 loss_ctc 15.756553 loss_rnnt 12.140561 hw_loss 0.416400 lr 0.00055232 rank 5
2023-02-17 20:58:11,992 DEBUG TRAIN Batch 9/6900 loss 20.903728 loss_att 25.256361 loss_ctc 35.003010 loss_rnnt 17.941662 hw_loss 0.396819 lr 0.00055219 rank 1
2023-02-17 20:58:11,997 DEBUG TRAIN Batch 9/6900 loss 14.895309 loss_att 16.685089 loss_ctc 21.997572 loss_rnnt 13.481240 hw_loss 0.204645 lr 0.00055242 rank 4
2023-02-17 20:58:12,000 DEBUG TRAIN Batch 9/6900 loss 14.298947 loss_att 15.987124 loss_ctc 24.293921 loss_rnnt 12.443995 hw_loss 0.346227 lr 0.00055210 rank 2
2023-02-17 20:58:12,000 DEBUG TRAIN Batch 9/6900 loss 11.504699 loss_att 14.035645 loss_ctc 21.052038 loss_rnnt 9.531107 hw_loss 0.364544 lr 0.00055232 rank 3
2023-02-17 20:58:12,001 DEBUG TRAIN Batch 9/6900 loss 10.117608 loss_att 10.232752 loss_ctc 12.504444 loss_rnnt 9.565825 hw_loss 0.394704 lr 0.00055246 rank 7
2023-02-17 20:58:12,004 DEBUG TRAIN Batch 9/6900 loss 20.678736 loss_att 25.172703 loss_ctc 32.812214 loss_rnnt 18.003729 hw_loss 0.297030 lr 0.00055228 rank 0
run_2_16_rnnt_bias_both.sh: line 166: 12077 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.1.1]:13223: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:40845
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:65230
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:29614
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:44967
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:61139
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:52956
