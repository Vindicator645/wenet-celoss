/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_17_rnnt_bias_1word_22.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_17_rnnt_bias_loss_2_class_1word_22/ddp_init
2023-02-17 22:15:03,866 INFO training on multiple gpus, this gpu 4
2023-02-17 22:15:03,868 INFO training on multiple gpus, this gpu 6
2023-02-17 22:15:03,869 INFO training on multiple gpus, this gpu 7
2023-02-17 22:15:03,870 INFO training on multiple gpus, this gpu 0
2023-02-17 22:15:03,871 INFO training on multiple gpus, this gpu 2
2023-02-17 22:15:03,871 INFO training on multiple gpus, this gpu 5
2023-02-17 22:15:03,871 INFO training on multiple gpus, this gpu 3
2023-02-17 22:15:03,936 INFO training on multiple gpus, this gpu 1
2023-02-17 22:18:17,185 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-17 22:18:17,426 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-17 22:18:20,578 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-17 22:18:27,755 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:30,046 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:30,957 INFO Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:37,874 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:40,144 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:47,919 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:50,222 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=3, timeout=0:30:00)
2023-02-17 22:18:51,808 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-17 22:18:58,063 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=4, timeout=0:30:00)
2023-02-17 22:19:00,406 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=4, timeout=0:30:00)
2023-02-17 22:19:08,224 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=4, timeout=0:30:00)
2023-02-17 22:19:10,071 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-17 22:19:10,559 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=5, timeout=0:30:00)
2023-02-17 22:19:13,186 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-17 22:19:20,813 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:20,971 INFO Waiting in store based barrier to initialize process group for rank: 6, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:30,980 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:31,131 INFO Waiting in store based barrier to initialize process group for rank: 6, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:31,560 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:41,287 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:41,710 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:42,594 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:51,879 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:19:52,793 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:02,205 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:02,924 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:12,581 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:13,031 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:22,595 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=6, timeout=0:30:00)
2023-02-17 22:20:25,542 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-17 22:20:26,177 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-17 22:20:26,265 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-17 22:20:26,543 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:20:26,655 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:20:27,793 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:20:28,084 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:20:54,484 INFO Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-17 22:20:54,485 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:21:24,648 INFO Waiting in store based barrier to initialize process group for rank: 6, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-17 22:21:24,648 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:21:54,841 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-17 22:21:54,841 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:22:25,743 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-17 22:22:25,743 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 22:22:35,469 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,473 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,562 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word_22/init.pt
2023-02-17 22:22:35,569 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,571 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,578 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,580 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,637 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,648 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,731 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,734 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,815 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,817 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:35,824 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:35,826 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:22:51,873 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 22:22:51,878 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 22:24:08,648 DEBUG TRAIN Batch 0/0 loss 517.754211 loss_att 69.357887 loss_ctc 492.922699 loss_rnnt 610.052551 hw_loss 1.297155 lr 0.00000004 rank 5
2023-02-17 22:24:08,652 DEBUG TRAIN Batch 0/0 loss 466.948914 loss_att 71.576172 loss_ctc 451.671722 loss_rnnt 547.539001 hw_loss 0.977672 lr 0.00000004 rank 7
2023-02-17 22:24:08,658 DEBUG TRAIN Batch 0/0 loss 492.116577 loss_att 68.548515 loss_ctc 472.269623 loss_rnnt 579.232422 hw_loss 0.457572 lr 0.00000004 rank 2
2023-02-17 22:24:08,660 DEBUG TRAIN Batch 0/0 loss 474.698395 loss_att 67.741440 loss_ctc 442.884186 loss_rnnt 559.768494 hw_loss 1.055975 lr 0.00000004 rank 6
2023-02-17 22:24:08,661 DEBUG TRAIN Batch 0/0 loss 469.624664 loss_att 65.000450 loss_ctc 427.637665 loss_rnnt 555.351990 hw_loss 1.492136 lr 0.00000004 rank 3
2023-02-17 22:24:08,678 DEBUG TRAIN Batch 0/0 loss 483.948792 loss_att 74.567070 loss_ctc 452.600677 loss_rnnt 569.613708 hw_loss 0.733510 lr 0.00000004 rank 0
2023-02-17 22:24:08,692 DEBUG TRAIN Batch 0/0 loss 504.408508 loss_att 63.932541 loss_ctc 485.483490 loss_rnnt 594.489990 hw_loss 1.007027 lr 0.00000004 rank 4
2023-02-17 22:24:08,707 DEBUG TRAIN Batch 0/0 loss 519.760559 loss_att 69.445923 loss_ctc 479.665039 loss_rnnt 614.488831 hw_loss 1.276359 lr 0.00000004 rank 1
2023-02-17 22:25:06,576 DEBUG TRAIN Batch 0/100 loss 2471.095459 loss_att 437.775391 loss_ctc 2537.080078 loss_rnnt 2868.610352 hw_loss 0.658372 lr 0.00000204 rank 0
2023-02-17 22:25:06,576 DEBUG TRAIN Batch 0/100 loss 2377.349365 loss_att 376.080841 loss_ctc 2581.349365 loss_rnnt 2750.090332 hw_loss 0.585869 lr 0.00000204 rank 5
2023-02-17 22:25:06,579 DEBUG TRAIN Batch 0/100 loss 2285.471436 loss_att 312.745972 loss_ctc 2624.320068 loss_rnnt 2634.270020 hw_loss 1.062324 lr 0.00000204 rank 1
2023-02-17 22:25:06,579 DEBUG TRAIN Batch 0/100 loss 2391.750488 loss_att 351.827057 loss_ctc 2680.354980 loss_rnnt 2760.918457 hw_loss 0.629605 lr 0.00000204 rank 7
2023-02-17 22:25:06,582 DEBUG TRAIN Batch 0/100 loss 2402.937012 loss_att 395.598633 loss_ctc 2648.581787 loss_rnnt 2771.125488 hw_loss 0.986911 lr 0.00000204 rank 4
2023-02-17 22:25:06,587 DEBUG TRAIN Batch 0/100 loss 2460.792725 loss_att 324.213013 loss_ctc 2596.748047 loss_rnnt 2869.535156 hw_loss 0.836722 lr 0.00000204 rank 2
2023-02-17 22:25:06,588 DEBUG TRAIN Batch 0/100 loss 2540.874023 loss_att 416.781006 loss_ctc 2622.207275 loss_rnnt 2954.364990 hw_loss 0.906263 lr 0.00000204 rank 6
2023-02-17 22:25:06,590 DEBUG TRAIN Batch 0/100 loss 2436.263184 loss_att 410.416779 loss_ctc 2605.567871 loss_rnnt 2818.400879 hw_loss 0.857450 lr 0.00000204 rank 3
2023-02-17 22:26:06,655 DEBUG TRAIN Batch 0/200 loss 1981.610229 loss_att 406.883240 loss_ctc 2947.397705 loss_rnnt 2167.270996 hw_loss 0.961815 lr 0.00000404 rank 2
2023-02-17 22:26:06,656 DEBUG TRAIN Batch 0/200 loss 1978.632446 loss_att 415.785980 loss_ctc 3027.692383 loss_rnnt 2150.948486 hw_loss 0.709605 lr 0.00000404 rank 0
2023-02-17 22:26:06,658 DEBUG TRAIN Batch 0/200 loss 1833.155029 loss_att 398.887848 loss_ctc 3066.164307 loss_rnnt 1955.261475 hw_loss 0.648357 lr 0.00000404 rank 6
2023-02-17 22:26:06,658 DEBUG TRAIN Batch 0/200 loss 2106.369385 loss_att 529.704346 loss_ctc 2924.122070 loss_rnnt 2312.282959 hw_loss 0.723430 lr 0.00000404 rank 7
2023-02-17 22:26:06,658 DEBUG TRAIN Batch 0/200 loss 1884.497070 loss_att 509.806396 loss_ctc 2805.201172 loss_rnnt 2036.328857 hw_loss 0.648357 lr 0.00000404 rank 1
2023-02-17 22:26:06,658 DEBUG TRAIN Batch 0/200 loss 1933.784302 loss_att 346.489380 loss_ctc 2886.579590 loss_rnnt 2123.841064 hw_loss 0.680199 lr 0.00000404 rank 5
2023-02-17 22:26:06,659 DEBUG TRAIN Batch 0/200 loss 1982.052002 loss_att 392.651917 loss_ctc 2980.291260 loss_rnnt 2166.386963 hw_loss 0.836823 lr 0.00000404 rank 4
2023-02-17 22:26:06,662 DEBUG TRAIN Batch 0/200 loss 1860.568726 loss_att 447.701538 loss_ctc 3049.895264 loss_rnnt 1984.219360 hw_loss 0.648357 lr 0.00000404 rank 3
2023-02-17 22:27:04,476 DEBUG TRAIN Batch 0/300 loss 793.562622 loss_att 265.019287 loss_ctc 1690.849609 loss_rnnt 779.526489 hw_loss 0.199745 lr 0.00000604 rank 0
2023-02-17 22:27:04,481 DEBUG TRAIN Batch 0/300 loss 665.097229 loss_att 200.844849 loss_ctc 1441.735107 loss_rnnt 654.142578 hw_loss 0.475182 lr 0.00000604 rank 2
2023-02-17 22:27:04,481 DEBUG TRAIN Batch 0/300 loss 842.273621 loss_att 260.169434 loss_ctc 1569.706787 loss_rnnt 861.495544 hw_loss 0.389647 lr 0.00000604 rank 5
2023-02-17 22:27:04,481 DEBUG TRAIN Batch 0/300 loss 472.505188 loss_att 199.941147 loss_ctc 1116.113281 loss_rnnt 441.064026 hw_loss 0.261584 lr 0.00000604 rank 7
2023-02-17 22:27:04,482 DEBUG TRAIN Batch 0/300 loss 432.943939 loss_att 132.931091 loss_ctc 943.323425 loss_rnnt 424.631561 hw_loss 0.495689 lr 0.00000604 rank 1
2023-02-17 22:27:04,485 DEBUG TRAIN Batch 0/300 loss 803.208252 loss_att 260.765350 loss_ctc 1585.472412 loss_rnnt 807.010254 hw_loss 0.721224 lr 0.00000604 rank 3
2023-02-17 22:27:04,487 DEBUG TRAIN Batch 0/300 loss 543.497009 loss_att 196.509384 loss_ctc 1402.324097 loss_rnnt 498.244812 hw_loss 0.261584 lr 0.00000604 rank 6
2023-02-17 22:27:04,491 DEBUG TRAIN Batch 0/300 loss 549.081360 loss_att 154.330688 loss_ctc 1058.152710 loss_rnnt 560.017029 hw_loss 0.259403 lr 0.00000604 rank 4
2023-02-17 22:28:03,185 DEBUG TRAIN Batch 0/400 loss 825.808777 loss_att 442.697571 loss_ctc 2293.382812 loss_rnnt 706.607788 hw_loss 0.275151 lr 0.00000804 rank 6
2023-02-17 22:28:03,186 DEBUG TRAIN Batch 0/400 loss 719.003845 loss_att 339.447693 loss_ctc 2202.729492 loss_rnnt 596.954102 hw_loss 0.245337 lr 0.00000804 rank 4
2023-02-17 22:28:03,187 DEBUG TRAIN Batch 0/400 loss 795.847290 loss_att 397.891113 loss_ctc 2144.307373 loss_rnnt 695.513916 hw_loss 0.243611 lr 0.00000804 rank 3
2023-02-17 22:28:03,187 DEBUG TRAIN Batch 0/400 loss 685.269287 loss_att 311.075623 loss_ctc 2163.997803 loss_rnnt 562.806763 hw_loss 0.257583 lr 0.00000804 rank 1
2023-02-17 22:28:03,191 DEBUG TRAIN Batch 0/400 loss 735.228516 loss_att 352.776123 loss_ctc 2260.156250 loss_rnnt 608.255493 hw_loss 0.262044 lr 0.00000804 rank 7
2023-02-17 22:28:03,190 DEBUG TRAIN Batch 0/400 loss 787.748291 loss_att 359.586304 loss_ctc 2288.265137 loss_rnnt 673.194702 hw_loss 0.219584 lr 0.00000804 rank 2
2023-02-17 22:28:03,192 DEBUG TRAIN Batch 0/400 loss 746.529968 loss_att 421.428894 loss_ctc 2157.617432 loss_rnnt 623.370728 hw_loss 0.064527 lr 0.00000804 rank 0
2023-02-17 22:28:03,193 DEBUG TRAIN Batch 0/400 loss 782.159180 loss_att 382.918854 loss_ctc 2286.437012 loss_rnnt 661.345032 hw_loss 0.172314 lr 0.00000804 rank 5
2023-02-17 22:29:01,172 DEBUG TRAIN Batch 0/500 loss 501.009491 loss_att 271.812836 loss_ctc 1917.356201 loss_rnnt 357.986267 hw_loss 0.030580 lr 0.00001004 rank 3
2023-02-17 22:29:01,175 DEBUG TRAIN Batch 0/500 loss 521.006897 loss_att 342.740570 loss_ctc 1426.602783 loss_rnnt 435.856598 hw_loss 0.107669 lr 0.00001004 rank 4
2023-02-17 22:29:01,177 DEBUG TRAIN Batch 0/500 loss 554.134644 loss_att 379.798859 loss_ctc 1437.534790 loss_rnnt 471.125336 hw_loss 0.168239 lr 0.00001004 rank 5
2023-02-17 22:29:01,179 DEBUG TRAIN Batch 0/500 loss 632.889160 loss_att 429.387756 loss_ctc 1967.557983 loss_rnnt 495.617371 hw_loss 0.030580 lr 0.00001004 rank 6
2023-02-17 22:29:01,181 DEBUG TRAIN Batch 0/500 loss 594.613464 loss_att 441.094269 loss_ctc 1460.979858 loss_rnnt 509.785583 hw_loss 0.030580 lr 0.00001004 rank 1
2023-02-17 22:29:01,182 DEBUG TRAIN Batch 0/500 loss 639.095093 loss_att 490.567017 loss_ctc 1541.725708 loss_rnnt 548.433655 hw_loss 0.030580 lr 0.00001004 rank 2
2023-02-17 22:29:01,181 DEBUG TRAIN Batch 0/500 loss 635.140137 loss_att 480.245972 loss_ctc 1495.587158 loss_rnnt 551.376465 hw_loss 0.030580 lr 0.00001004 rank 7
2023-02-17 22:29:01,183 DEBUG TRAIN Batch 0/500 loss 512.963257 loss_att 293.956268 loss_ctc 1895.693848 loss_rnnt 372.384277 hw_loss 0.030580 lr 0.00001004 rank 0
2023-02-17 22:29:58,380 DEBUG TRAIN Batch 0/600 loss 275.368011 loss_att 211.512726 loss_ctc 765.370605 loss_rnnt 222.714722 hw_loss 0.169972 lr 0.00001204 rank 5
2023-02-17 22:29:58,381 DEBUG TRAIN Batch 0/600 loss 369.982300 loss_att 302.567749 loss_ctc 819.819153 loss_rnnt 323.364136 hw_loss 0.230323 lr 0.00001204 rank 0
2023-02-17 22:29:58,386 DEBUG TRAIN Batch 0/600 loss 264.903473 loss_att 208.500687 loss_ctc 660.940063 loss_rnnt 223.263977 hw_loss 0.215919 lr 0.00001204 rank 2
2023-02-17 22:29:58,386 DEBUG TRAIN Batch 0/600 loss 385.737488 loss_att 258.588623 loss_ctc 1422.515137 loss_rnnt 272.853882 hw_loss 0.143237 lr 0.00001204 rank 3
2023-02-17 22:29:58,388 DEBUG TRAIN Batch 0/600 loss 247.440140 loss_att 185.601196 loss_ctc 715.920532 loss_rnnt 197.255707 hw_loss 0.165311 lr 0.00001204 rank 7
2023-02-17 22:29:58,389 DEBUG TRAIN Batch 0/600 loss 306.302460 loss_att 245.224762 loss_ctc 753.619873 loss_rnnt 258.806305 hw_loss 0.130016 lr 0.00001204 rank 6
2023-02-17 22:29:58,394 DEBUG TRAIN Batch 0/600 loss 247.839264 loss_att 146.863037 loss_ctc 1105.686401 loss_rnnt 153.590851 hw_loss 0.120104 lr 0.00001204 rank 1
2023-02-17 22:29:58,445 DEBUG TRAIN Batch 0/600 loss 275.349365 loss_att 204.747940 loss_ctc 845.813843 loss_rnnt 213.218674 hw_loss 0.354485 lr 0.00001204 rank 4
2023-02-17 22:30:56,892 DEBUG TRAIN Batch 0/700 loss 387.350220 loss_att 355.736145 loss_ctc 557.468811 loss_rnnt 370.892914 hw_loss 0.183078 lr 0.00001404 rank 5
2023-02-17 22:30:56,895 DEBUG TRAIN Batch 0/700 loss 362.933472 loss_att 338.062683 loss_ctc 531.549255 loss_rnnt 345.418671 hw_loss 0.012797 lr 0.00001404 rank 1
2023-02-17 22:30:56,902 DEBUG TRAIN Batch 0/700 loss 363.769745 loss_att 334.876099 loss_ctc 537.565002 loss_rnnt 346.271271 hw_loss 0.195951 lr 0.00001404 rank 2
2023-02-17 22:30:56,903 DEBUG TRAIN Batch 0/700 loss 385.885437 loss_att 350.483643 loss_ctc 538.237244 loss_rnnt 372.442688 hw_loss 0.392860 lr 0.00001404 rank 6
2023-02-17 22:30:56,904 DEBUG TRAIN Batch 0/700 loss 398.612549 loss_att 361.108185 loss_ctc 559.223938 loss_rnnt 384.650330 hw_loss 0.090492 lr 0.00001404 rank 7
2023-02-17 22:30:56,905 DEBUG TRAIN Batch 0/700 loss 430.825775 loss_att 354.502838 loss_ctc 1116.124756 loss_rnnt 354.625549 hw_loss 0.171714 lr 0.00001404 rank 4
2023-02-17 22:30:56,906 DEBUG TRAIN Batch 0/700 loss 389.460205 loss_att 360.318359 loss_ctc 554.473022 loss_rnnt 373.280029 hw_loss 0.012797 lr 0.00001404 rank 0
2023-02-17 22:30:56,907 DEBUG TRAIN Batch 0/700 loss 379.148529 loss_att 348.151611 loss_ctc 536.023682 loss_rnnt 364.353790 hw_loss 0.145177 lr 0.00001404 rank 3
2023-02-17 22:31:55,878 DEBUG TRAIN Batch 0/800 loss 242.368347 loss_att 209.834335 loss_ctc 348.596283 loss_rnnt 234.541977 hw_loss 0.317706 lr 0.00001604 rank 0
2023-02-17 22:31:55,883 DEBUG TRAIN Batch 0/800 loss 420.030365 loss_att 389.602844 loss_ctc 577.596924 loss_rnnt 405.103912 hw_loss 0.005734 lr 0.00001604 rank 5
2023-02-17 22:31:55,885 DEBUG TRAIN Batch 0/800 loss 335.434692 loss_att 297.576660 loss_ctc 448.458801 loss_rnnt 327.933350 hw_loss 0.005734 lr 0.00001604 rank 6
2023-02-17 22:31:55,885 DEBUG TRAIN Batch 0/800 loss 384.455078 loss_att 347.226868 loss_ctc 527.048645 loss_rnnt 372.885162 hw_loss 0.005734 lr 0.00001604 rank 1
2023-02-17 22:31:55,885 DEBUG TRAIN Batch 0/800 loss 356.268158 loss_att 331.173828 loss_ctc 480.813721 loss_rnnt 344.561554 hw_loss 0.223843 lr 0.00001604 rank 7
2023-02-17 22:31:55,889 DEBUG TRAIN Batch 0/800 loss 358.733307 loss_att 323.619110 loss_ctc 502.225464 loss_rnnt 346.620819 hw_loss 0.005734 lr 0.00001604 rank 2
2023-02-17 22:31:55,890 DEBUG TRAIN Batch 0/800 loss 352.264496 loss_att 322.950317 loss_ctc 497.121674 loss_rnnt 338.726013 hw_loss 0.163196 lr 0.00001604 rank 3
2023-02-17 22:31:55,890 DEBUG TRAIN Batch 0/800 loss 309.619507 loss_att 279.771057 loss_ctc 415.806152 loss_rnnt 301.427887 hw_loss 0.005734 lr 0.00001604 rank 4
2023-02-17 22:33:16,569 DEBUG TRAIN Batch 0/900 loss 220.862518 loss_att 196.669647 loss_ctc 330.989929 loss_rnnt 211.014450 hw_loss 0.005614 lr 0.00001804 rank 1
2023-02-17 22:33:16,571 DEBUG TRAIN Batch 0/900 loss 250.515305 loss_att 221.545258 loss_ctc 369.690796 loss_rnnt 240.267273 hw_loss 0.284916 lr 0.00001804 rank 2
2023-02-17 22:33:16,574 DEBUG TRAIN Batch 0/900 loss 301.499603 loss_att 240.849838 loss_ctc 659.264832 loss_rnnt 265.808197 hw_loss 0.223777 lr 0.00001804 rank 4
2023-02-17 22:33:16,575 DEBUG TRAIN Batch 0/900 loss 242.795456 loss_att 206.771439 loss_ctc 351.761780 loss_rnnt 235.334442 hw_loss 0.256835 lr 0.00001804 rank 5
2023-02-17 22:33:16,577 DEBUG TRAIN Batch 0/900 loss 289.154144 loss_att 257.260559 loss_ctc 422.378418 loss_rnnt 277.689758 hw_loss 0.149803 lr 0.00001804 rank 0
2023-02-17 22:33:16,580 DEBUG TRAIN Batch 0/900 loss 251.095261 loss_att 181.422104 loss_ctc 779.657837 loss_rnnt 194.525925 hw_loss 0.054298 lr 0.00001804 rank 7
2023-02-17 22:33:16,581 DEBUG TRAIN Batch 0/900 loss 261.211365 loss_att 230.883316 loss_ctc 395.311401 loss_rnnt 249.306915 hw_loss 0.168858 lr 0.00001804 rank 3
2023-02-17 22:33:16,586 DEBUG TRAIN Batch 0/900 loss 273.110565 loss_att 234.593964 loss_ctc 413.662537 loss_rnnt 262.070618 hw_loss 0.005614 lr 0.00001804 rank 6
2023-02-17 22:34:15,989 DEBUG TRAIN Batch 0/1000 loss 378.049408 loss_att 262.314758 loss_ctc 1148.718994 loss_rnnt 298.357666 hw_loss 0.155068 lr 0.00002004 rank 7
2023-02-17 22:34:15,991 DEBUG TRAIN Batch 0/1000 loss 299.215424 loss_att 269.565613 loss_ctc 396.975342 loss_rnnt 292.108368 hw_loss 0.004393 lr 0.00002004 rank 4
2023-02-17 22:34:15,991 DEBUG TRAIN Batch 0/1000 loss 305.485016 loss_att 267.815094 loss_ctc 387.991150 loss_rnnt 301.965668 hw_loss 0.098485 lr 0.00002004 rank 5
2023-02-17 22:34:15,993 DEBUG TRAIN Batch 0/1000 loss 263.373535 loss_att 232.101135 loss_ctc 348.916138 loss_rnnt 258.095886 hw_loss 0.237119 lr 0.00002004 rank 1
2023-02-17 22:34:15,993 DEBUG TRAIN Batch 0/1000 loss 346.703369 loss_att 307.217712 loss_ctc 423.406036 loss_rnnt 344.304840 hw_loss 0.128606 lr 0.00002004 rank 2
2023-02-17 22:34:15,999 DEBUG TRAIN Batch 0/1000 loss 304.729919 loss_att 269.833649 loss_ctc 396.965271 loss_rnnt 299.298981 hw_loss 0.210275 lr 0.00002004 rank 3
2023-02-17 22:34:16,001 DEBUG TRAIN Batch 0/1000 loss 304.933807 loss_att 262.736298 loss_ctc 434.581055 loss_rnnt 295.947296 hw_loss 0.261884 lr 0.00002004 rank 6
2023-02-17 22:34:16,053 DEBUG TRAIN Batch 0/1000 loss 399.830383 loss_att 354.717957 loss_ctc 492.796082 loss_rnnt 396.455078 hw_loss 0.004393 lr 0.00002004 rank 0
2023-02-17 22:35:12,719 DEBUG TRAIN Batch 0/1100 loss 411.727936 loss_att 351.136230 loss_ctc 589.240540 loss_rnnt 400.124268 hw_loss 0.100622 lr 0.00002204 rank 7
2023-02-17 22:35:12,721 DEBUG TRAIN Batch 0/1100 loss 294.202301 loss_att 254.042969 loss_ctc 414.097931 loss_rnnt 286.175720 hw_loss 0.135666 lr 0.00002204 rank 4
2023-02-17 22:35:12,722 DEBUG TRAIN Batch 0/1100 loss 346.974731 loss_att 300.022797 loss_ctc 449.527649 loss_rnnt 342.546478 hw_loss 0.271788 lr 0.00002204 rank 0
2023-02-17 22:35:12,723 DEBUG TRAIN Batch 0/1100 loss 393.090515 loss_att 343.679840 loss_ctc 517.270142 loss_rnnt 386.413025 hw_loss 0.004420 lr 0.00002204 rank 5
2023-02-17 22:35:12,724 DEBUG TRAIN Batch 0/1100 loss 469.253601 loss_att 271.634277 loss_ctc 1986.939575 loss_rnnt 306.288849 hw_loss 0.244674 lr 0.00002204 rank 1
2023-02-17 22:35:12,726 DEBUG TRAIN Batch 0/1100 loss 391.574921 loss_att 341.589966 loss_ctc 531.817139 loss_rnnt 382.870605 hw_loss 0.004420 lr 0.00002204 rank 6
2023-02-17 22:35:12,727 DEBUG TRAIN Batch 0/1100 loss 623.355835 loss_att 407.111115 loss_ctc 2126.426758 loss_rnnt 466.073517 hw_loss 0.228375 lr 0.00002204 rank 3
2023-02-17 22:35:12,728 DEBUG TRAIN Batch 0/1100 loss 350.802155 loss_att 303.896912 loss_ctc 448.324493 loss_rnnt 347.089569 hw_loss 0.169948 lr 0.00002204 rank 2
2023-02-17 22:36:10,158 DEBUG TRAIN Batch 0/1200 loss 295.653351 loss_att 255.489594 loss_ctc 385.222656 loss_rnnt 291.631226 hw_loss 0.210532 lr 0.00002404 rank 6
2023-02-17 22:36:10,158 DEBUG TRAIN Batch 0/1200 loss 318.906189 loss_att 276.337585 loss_ctc 406.112183 loss_rnnt 315.757568 hw_loss 0.065453 lr 0.00002404 rank 5
2023-02-17 22:36:10,161 DEBUG TRAIN Batch 0/1200 loss 336.919647 loss_att 266.593445 loss_ctc 691.959412 loss_rnnt 303.588837 hw_loss 0.107673 lr 0.00002404 rank 0
2023-02-17 22:36:10,163 DEBUG TRAIN Batch 0/1200 loss 301.001770 loss_att 262.052246 loss_ctc 395.001343 loss_rnnt 296.202515 hw_loss 0.104840 lr 0.00002404 rank 1
2023-02-17 22:36:10,164 DEBUG TRAIN Batch 0/1200 loss 260.953339 loss_att 225.913696 loss_ctc 350.184998 loss_rnnt 255.956909 hw_loss 0.200305 lr 0.00002404 rank 3
2023-02-17 22:36:10,164 DEBUG TRAIN Batch 0/1200 loss 278.746979 loss_att 217.485199 loss_ctc 615.743286 loss_rnnt 245.947021 hw_loss 0.224006 lr 0.00002404 rank 4
2023-02-17 22:36:10,165 DEBUG TRAIN Batch 0/1200 loss 211.474304 loss_att 181.098526 loss_ctc 291.977966 loss_rnnt 206.718964 hw_loss 0.181267 lr 0.00002404 rank 7
2023-02-17 22:36:10,221 DEBUG TRAIN Batch 0/1200 loss 234.170563 loss_att 202.635834 loss_ctc 315.144196 loss_rnnt 229.569214 hw_loss 0.209660 lr 0.00002404 rank 2
2023-02-17 22:37:10,012 DEBUG TRAIN Batch 0/1300 loss 319.560059 loss_att 277.388977 loss_ctc 390.507751 loss_rnnt 318.533356 hw_loss 0.002275 lr 0.00002604 rank 6
2023-02-17 22:37:10,016 DEBUG TRAIN Batch 0/1300 loss 493.320587 loss_att 225.910767 loss_ctc 2658.449707 loss_rnnt 257.840820 hw_loss 0.520943 lr 0.00002604 rank 5
2023-02-17 22:37:10,016 DEBUG TRAIN Batch 0/1300 loss 418.379639 loss_att 363.644440 loss_ctc 470.877350 loss_rnnt 422.284790 hw_loss 0.079123 lr 0.00002604 rank 0
2023-02-17 22:37:10,016 DEBUG TRAIN Batch 0/1300 loss 290.216156 loss_att 252.471375 loss_ctc 362.611084 loss_rnnt 288.043030 hw_loss 0.130139 lr 0.00002604 rank 1
2023-02-17 22:37:10,016 DEBUG TRAIN Batch 0/1300 loss 385.913757 loss_att 337.823975 loss_ctc 458.855225 loss_rnnt 385.701904 hw_loss 0.195489 lr 0.00002604 rank 3
2023-02-17 22:37:10,018 DEBUG TRAIN Batch 0/1300 loss 290.593933 loss_att 250.968613 loss_ctc 386.342651 loss_rnnt 285.751312 hw_loss 0.002275 lr 0.00002604 rank 7
2023-02-17 22:37:10,019 DEBUG TRAIN Batch 0/1300 loss 417.546967 loss_att 360.346588 loss_ctc 551.073914 loss_rnnt 411.126404 hw_loss 0.106965 lr 0.00002604 rank 4
2023-02-17 22:37:10,027 DEBUG TRAIN Batch 0/1300 loss 303.970886 loss_att 262.288025 loss_ctc 392.610474 loss_rnnt 300.439026 hw_loss 0.093386 lr 0.00002604 rank 2
2023-02-17 22:38:07,910 DEBUG TRAIN Batch 0/1400 loss 342.260834 loss_att 297.043030 loss_ctc 414.290070 loss_rnnt 341.699432 hw_loss 0.002013 lr 0.00002804 rank 5
2023-02-17 22:38:07,911 DEBUG TRAIN Batch 0/1400 loss 264.081268 loss_att 227.757721 loss_ctc 358.453064 loss_rnnt 258.700012 hw_loss 0.118253 lr 0.00002804 rank 2
2023-02-17 22:38:07,912 DEBUG TRAIN Batch 0/1400 loss 360.602173 loss_att 313.366943 loss_ctc 450.859833 loss_rnnt 357.931396 hw_loss 0.156497 lr 0.00002804 rank 3
2023-02-17 22:38:07,913 DEBUG TRAIN Batch 0/1400 loss 303.429291 loss_att 261.768799 loss_ctc 372.069397 loss_rnnt 302.506287 hw_loss 0.193273 lr 0.00002804 rank 1
2023-02-17 22:38:07,914 DEBUG TRAIN Batch 0/1400 loss 398.727844 loss_att 339.295441 loss_ctc 560.043945 loss_rnnt 389.009247 hw_loss 0.180495 lr 0.00002804 rank 0
2023-02-17 22:38:07,915 DEBUG TRAIN Batch 0/1400 loss 397.515778 loss_att 228.085541 loss_ctc 1671.420532 loss_rnnt 261.546783 hw_loss 0.002013 lr 0.00002804 rank 7
2023-02-17 22:38:07,917 DEBUG TRAIN Batch 0/1400 loss 354.189972 loss_att 307.409454 loss_ctc 422.223267 loss_rnnt 354.391357 hw_loss 0.156799 lr 0.00002804 rank 4
2023-02-17 22:38:07,922 DEBUG TRAIN Batch 0/1400 loss 324.983307 loss_att 282.992157 loss_ctc 403.456116 loss_rnnt 322.829865 hw_loss 0.166182 lr 0.00002804 rank 6
2023-02-17 22:39:05,694 DEBUG TRAIN Batch 0/1500 loss 309.140594 loss_att 269.779266 loss_ctc 359.495300 loss_rnnt 310.227020 hw_loss 0.134784 lr 0.00003004 rank 0
2023-02-17 22:39:05,694 DEBUG TRAIN Batch 0/1500 loss 287.405212 loss_att 247.803680 loss_ctc 338.862732 loss_rnnt 288.369232 hw_loss 0.178667 lr 0.00003004 rank 2
2023-02-17 22:39:05,697 DEBUG TRAIN Batch 0/1500 loss 227.748764 loss_att 195.724380 loss_ctc 272.841034 loss_rnnt 228.016937 hw_loss 0.233280 lr 0.00003004 rank 7
2023-02-17 22:39:05,703 DEBUG TRAIN Batch 0/1500 loss 308.465820 loss_att 267.856750 loss_ctc 362.144348 loss_rnnt 309.282959 hw_loss 0.276653 lr 0.00003004 rank 5
2023-02-17 22:39:05,704 DEBUG TRAIN Batch 0/1500 loss 227.503586 loss_att 197.911896 loss_ctc 271.219666 loss_rnnt 227.488541 hw_loss 0.196092 lr 0.00003004 rank 4
2023-02-17 22:39:05,705 DEBUG TRAIN Batch 0/1500 loss 320.101349 loss_att 278.977661 loss_ctc 383.282349 loss_rnnt 319.900452 hw_loss 0.002858 lr 0.00003004 rank 6
2023-02-17 22:39:05,708 DEBUG TRAIN Batch 0/1500 loss 310.922302 loss_att 271.618408 loss_ctc 353.973267 loss_rnnt 313.041443 hw_loss 0.002858 lr 0.00003004 rank 1
2023-02-17 22:39:05,712 DEBUG TRAIN Batch 0/1500 loss 349.719971 loss_att 305.308380 loss_ctc 400.662842 loss_rnnt 351.657196 hw_loss 0.286327 lr 0.00003004 rank 3
2023-02-17 22:40:06,100 DEBUG TRAIN Batch 0/1600 loss 363.354767 loss_att 318.620361 loss_ctc 417.774658 loss_rnnt 365.044250 hw_loss 0.002612 lr 0.00003204 rank 5
2023-02-17 22:40:06,104 DEBUG TRAIN Batch 0/1600 loss 396.907898 loss_att 346.034149 loss_ctc 451.826904 loss_rnnt 399.685150 hw_loss 0.140584 lr 0.00003204 rank 4
2023-02-17 22:40:06,104 DEBUG TRAIN Batch 0/1600 loss 317.640900 loss_att 245.027527 loss_ctc 713.557617 loss_rnnt 279.314850 hw_loss 0.112180 lr 0.00003204 rank 1
2023-02-17 22:40:06,105 DEBUG TRAIN Batch 0/1600 loss 343.621887 loss_att 300.758301 loss_ctc 393.618408 loss_rnnt 345.527008 hw_loss 0.002612 lr 0.00003204 rank 2
2023-02-17 22:40:06,106 DEBUG TRAIN Batch 0/1600 loss 368.905670 loss_att 319.980804 loss_ctc 432.264587 loss_rnnt 370.209534 hw_loss 0.062277 lr 0.00003204 rank 7
2023-02-17 22:40:06,107 DEBUG TRAIN Batch 0/1600 loss 444.174683 loss_att 320.200134 loss_ctc 1197.085693 loss_rnnt 368.526367 hw_loss 0.103310 lr 0.00003204 rank 0
2023-02-17 22:40:06,112 DEBUG TRAIN Batch 0/1600 loss 233.844498 loss_att 201.143707 loss_ctc 280.759094 loss_rnnt 234.016617 hw_loss 0.211395 lr 0.00003204 rank 3
2023-02-17 22:40:06,169 DEBUG TRAIN Batch 0/1600 loss 318.632202 loss_att 152.868744 loss_ctc 1631.983398 loss_rnnt 176.525391 hw_loss 0.273751 lr 0.00003204 rank 6
2023-02-17 22:41:26,111 DEBUG TRAIN Batch 0/1700 loss 275.685059 loss_att 239.466766 loss_ctc 301.794373 loss_rnnt 279.344482 hw_loss 0.193132 lr 0.00003404 rank 1
2023-02-17 22:41:26,114 DEBUG TRAIN Batch 0/1700 loss 342.840851 loss_att 296.697876 loss_ctc 410.707550 loss_rnnt 342.933838 hw_loss 0.162601 lr 0.00003404 rank 0
2023-02-17 22:41:26,115 DEBUG TRAIN Batch 0/1700 loss 435.209076 loss_att 275.882172 loss_ctc 1569.191406 loss_rnnt 315.876343 hw_loss 0.000873 lr 0.00003404 rank 5
2023-02-17 22:41:26,118 DEBUG TRAIN Batch 0/1700 loss 230.013336 loss_att 197.735397 loss_ctc 260.952576 loss_rnnt 232.343216 hw_loss 0.000873 lr 0.00003404 rank 7
2023-02-17 22:41:26,119 DEBUG TRAIN Batch 0/1700 loss 360.443146 loss_att 313.899567 loss_ctc 402.975647 loss_rnnt 364.080414 hw_loss 0.000873 lr 0.00003404 rank 4
2023-02-17 22:41:26,124 DEBUG TRAIN Batch 0/1700 loss 309.428162 loss_att 271.122559 loss_ctc 356.985809 loss_rnnt 310.747803 hw_loss 0.000873 lr 0.00003404 rank 3
2023-02-17 22:41:26,160 DEBUG TRAIN Batch 0/1700 loss 302.630463 loss_att 262.727417 loss_ctc 359.033356 loss_rnnt 303.090240 hw_loss 0.000873 lr 0.00003404 rank 2
2023-02-17 22:41:26,163 DEBUG TRAIN Batch 0/1700 loss 73.613846 loss_att 60.261440 loss_ctc 130.178268 loss_rnnt 68.582581 hw_loss 0.299657 lr 0.00003404 rank 6
2023-02-17 22:42:25,142 DEBUG TRAIN Batch 0/1800 loss 281.345062 loss_att 250.028519 loss_ctc 291.789520 loss_rnnt 286.168457 hw_loss 0.088705 lr 0.00003604 rank 5
2023-02-17 22:42:25,147 DEBUG TRAIN Batch 0/1800 loss 329.771667 loss_att 289.821472 loss_ctc 343.135742 loss_rnnt 335.954773 hw_loss 0.047015 lr 0.00003604 rank 3
2023-02-17 22:42:25,147 DEBUG TRAIN Batch 0/1800 loss 278.603027 loss_att 247.377106 loss_ctc 286.978668 loss_rnnt 283.536346 hw_loss 0.365913 lr 0.00003604 rank 1
2023-02-17 22:42:25,147 DEBUG TRAIN Batch 0/1800 loss 281.872223 loss_att 250.092712 loss_ctc 293.237610 loss_rnnt 286.712250 hw_loss 0.000891 lr 0.00003604 rank 0
2023-02-17 22:42:25,148 DEBUG TRAIN Batch 0/1800 loss 319.455536 loss_att 282.667023 loss_ctc 328.634033 loss_rnnt 325.456512 hw_loss 0.249245 lr 0.00003604 rank 2
2023-02-17 22:42:25,152 DEBUG TRAIN Batch 0/1800 loss 217.738174 loss_att 189.149948 loss_ctc 221.940155 loss_rnnt 222.895081 hw_loss 0.000891 lr 0.00003604 rank 7
2023-02-17 22:42:25,154 DEBUG TRAIN Batch 0/1800 loss 263.068817 loss_att 231.333115 loss_ctc 271.654419 loss_rnnt 268.219055 hw_loss 0.097779 lr 0.00003604 rank 6
2023-02-17 22:42:25,207 DEBUG TRAIN Batch 0/1800 loss 327.072357 loss_att 285.678925 loss_ctc 337.037048 loss_rnnt 333.976685 hw_loss 0.085784 lr 0.00003604 rank 4
2023-02-17 22:43:26,281 DEBUG TRAIN Batch 0/1900 loss 252.035522 loss_att 223.014008 loss_ctc 261.025757 loss_rnnt 256.640564 hw_loss 0.001080 lr 0.00003804 rank 5
2023-02-17 22:43:26,281 DEBUG TRAIN Batch 0/1900 loss 278.854797 loss_att 247.368317 loss_ctc 285.094849 loss_rnnt 284.319489 hw_loss 0.001080 lr 0.00003804 rank 6
2023-02-17 22:43:26,284 DEBUG TRAIN Batch 0/1900 loss 309.488678 loss_att 271.739624 loss_ctc 317.174164 loss_rnnt 315.727875 hw_loss 0.536078 lr 0.00003804 rank 4
2023-02-17 22:43:26,289 DEBUG TRAIN Batch 0/1900 loss 325.028351 loss_att 286.720123 loss_ctc 333.331818 loss_rnnt 331.462311 hw_loss 0.226019 lr 0.00003804 rank 1
2023-02-17 22:43:26,290 DEBUG TRAIN Batch 0/1900 loss 302.185730 loss_att 269.919373 loss_ctc 311.489349 loss_rnnt 307.343262 hw_loss 0.103581 lr 0.00003804 rank 7
2023-02-17 22:43:26,291 DEBUG TRAIN Batch 0/1900 loss 280.636810 loss_att 243.670898 loss_ctc 286.393524 loss_rnnt 287.261841 hw_loss 0.001080 lr 0.00003804 rank 3
2023-02-17 22:43:26,298 DEBUG TRAIN Batch 0/1900 loss 323.825989 loss_att 286.889221 loss_ctc 330.711365 loss_rnnt 330.294739 hw_loss 0.001080 lr 0.00003804 rank 2
2023-02-17 22:43:26,299 DEBUG TRAIN Batch 0/1900 loss 304.372131 loss_att 270.979950 loss_ctc 312.441956 loss_rnnt 309.918365 hw_loss 0.105389 lr 0.00003804 rank 0
2023-02-17 22:44:22,728 DEBUG TRAIN Batch 0/2000 loss 346.646332 loss_att 304.905518 loss_ctc 357.803345 loss_rnnt 353.340607 hw_loss 0.311734 lr 0.00004004 rank 1
2023-02-17 22:44:22,731 DEBUG TRAIN Batch 0/2000 loss 437.148560 loss_att 384.036652 loss_ctc 454.749084 loss_rnnt 445.424011 hw_loss 0.000377 lr 0.00004004 rank 5
2023-02-17 22:44:22,733 DEBUG TRAIN Batch 0/2000 loss 399.395874 loss_att 351.885681 loss_ctc 412.867126 loss_rnnt 407.023438 hw_loss 0.146815 lr 0.00004004 rank 7
2023-02-17 22:44:22,734 DEBUG TRAIN Batch 0/2000 loss 473.745605 loss_att 416.327789 loss_ctc 486.985779 loss_rnnt 483.463623 hw_loss 0.000377 lr 0.00004004 rank 2
2023-02-17 22:44:22,735 DEBUG TRAIN Batch 0/2000 loss 150.213760 loss_att 132.951447 loss_ctc 154.671097 loss_rnnt 152.928925 hw_loss 0.268088 lr 0.00004004 rank 0
2023-02-17 22:44:22,735 DEBUG TRAIN Batch 0/2000 loss 177.747543 loss_att 158.324951 loss_ctc 181.405060 loss_rnnt 181.068207 hw_loss 0.142856 lr 0.00004004 rank 6
2023-02-17 22:44:22,736 DEBUG TRAIN Batch 0/2000 loss 354.919769 loss_att 312.028687 loss_ctc 366.418060 loss_rnnt 361.964691 hw_loss 0.000377 lr 0.00004004 rank 3
2023-02-17 22:44:22,740 DEBUG TRAIN Batch 0/2000 loss 356.808258 loss_att 314.445831 loss_ctc 368.379578 loss_rnnt 363.737732 hw_loss 0.000377 lr 0.00004004 rank 4
2023-02-17 22:45:20,902 DEBUG TRAIN Batch 0/2100 loss 277.356812 loss_att 243.001343 loss_ctc 284.651886 loss_rnnt 283.254852 hw_loss 0.000688 lr 0.00004204 rank 0
2023-02-17 22:45:20,902 DEBUG TRAIN Batch 0/2100 loss 217.077316 loss_att 192.107178 loss_ctc 225.060928 loss_rnnt 220.930939 hw_loss 0.142318 lr 0.00004204 rank 5
2023-02-17 22:45:20,907 DEBUG TRAIN Batch 0/2100 loss 300.085632 loss_att 264.995697 loss_ctc 308.846802 loss_rnnt 305.907013 hw_loss 0.053365 lr 0.00004204 rank 4
2023-02-17 22:45:20,908 DEBUG TRAIN Batch 0/2100 loss 247.742432 loss_att 218.945251 loss_ctc 256.626923 loss_rnnt 252.316895 hw_loss 0.000688 lr 0.00004204 rank 1
2023-02-17 22:45:20,910 DEBUG TRAIN Batch 0/2100 loss 256.372986 loss_att 224.048981 loss_ctc 265.654297 loss_rnnt 261.517151 hw_loss 0.155881 lr 0.00004204 rank 3
2023-02-17 22:45:20,911 DEBUG TRAIN Batch 0/2100 loss 339.411469 loss_att 295.111877 loss_ctc 349.985474 loss_rnnt 346.700989 hw_loss 0.301010 lr 0.00004204 rank 2
2023-02-17 22:45:20,912 DEBUG TRAIN Batch 0/2100 loss 285.982666 loss_att 252.725189 loss_ctc 296.523438 loss_rnnt 291.158142 hw_loss 0.132375 lr 0.00004204 rank 6
2023-02-17 22:45:20,964 DEBUG TRAIN Batch 0/2100 loss 325.972076 loss_att 288.862854 loss_ctc 337.070618 loss_rnnt 331.868927 hw_loss 0.084694 lr 0.00004204 rank 7
2023-02-17 22:46:20,914 DEBUG TRAIN Batch 0/2200 loss 272.010590 loss_att 242.109406 loss_ctc 284.295746 loss_rnnt 276.228119 hw_loss 0.233762 lr 0.00004404 rank 0
2023-02-17 22:46:20,915 DEBUG TRAIN Batch 0/2200 loss 329.282043 loss_att 288.307922 loss_ctc 343.725067 loss_rnnt 335.550537 hw_loss 0.001176 lr 0.00004404 rank 5
2023-02-17 22:46:20,918 DEBUG TRAIN Batch 0/2200 loss 344.924286 loss_att 308.274231 loss_ctc 355.497070 loss_rnnt 350.721497 hw_loss 0.230803 lr 0.00004404 rank 2
2023-02-17 22:46:20,920 DEBUG TRAIN Batch 0/2200 loss 314.747101 loss_att 273.833344 loss_ctc 322.682953 loss_rnnt 321.871124 hw_loss 0.001176 lr 0.00004404 rank 4
2023-02-17 22:46:20,921 DEBUG TRAIN Batch 0/2200 loss 265.903961 loss_att 232.245941 loss_ctc 275.479980 loss_rnnt 271.225128 hw_loss 0.250586 lr 0.00004404 rank 1
2023-02-17 22:46:20,931 DEBUG TRAIN Batch 0/2200 loss 288.278961 loss_att 251.764343 loss_ctc 298.057526 loss_rnnt 294.181183 hw_loss 0.181687 lr 0.00004404 rank 3
2023-02-17 22:46:20,931 DEBUG TRAIN Batch 0/2200 loss 233.504196 loss_att 205.620911 loss_ctc 244.093857 loss_rnnt 237.668259 hw_loss 0.001176 lr 0.00004404 rank 7
2023-02-17 22:46:20,936 DEBUG TRAIN Batch 0/2200 loss 332.022980 loss_att 291.667236 loss_ctc 344.895355 loss_rnnt 338.293640 hw_loss 0.157784 lr 0.00004404 rank 6
2023-02-17 22:47:18,424 DEBUG TRAIN Batch 0/2300 loss 286.349609 loss_att 255.177521 loss_ctc 299.710480 loss_rnnt 290.802307 hw_loss 0.000426 lr 0.00004604 rank 2
2023-02-17 22:47:18,424 DEBUG TRAIN Batch 0/2300 loss 116.579117 loss_att 103.364502 loss_ctc 120.390259 loss_rnnt 118.668533 hw_loss 0.085030 lr 0.00004604 rank 4
2023-02-17 22:47:18,425 DEBUG TRAIN Batch 0/2300 loss 201.800064 loss_att 179.030655 loss_ctc 210.491425 loss_rnnt 205.194855 hw_loss 0.000426 lr 0.00004604 rank 5
2023-02-17 22:47:18,429 DEBUG TRAIN Batch 0/2300 loss 250.140778 loss_att 218.768494 loss_ctc 257.233459 loss_rnnt 255.405090 hw_loss 0.120842 lr 0.00004604 rank 6
2023-02-17 22:47:18,429 DEBUG TRAIN Batch 0/2300 loss 182.910950 loss_att 161.611130 loss_ctc 190.787460 loss_rnnt 185.958786 hw_loss 0.303597 lr 0.00004604 rank 3
2023-02-17 22:47:18,470 DEBUG TRAIN Batch 0/2300 loss 213.138351 loss_att 187.534271 loss_ctc 223.592972 loss_rnnt 216.734604 hw_loss 0.244885 lr 0.00004604 rank 7
2023-02-17 22:47:18,475 DEBUG TRAIN Batch 0/2300 loss 197.772064 loss_att 174.674530 loss_ctc 202.076340 loss_rnnt 201.685791 hw_loss 0.247220 lr 0.00004604 rank 0
2023-02-17 22:47:18,483 DEBUG TRAIN Batch 0/2300 loss 375.214020 loss_att 337.444183 loss_ctc 391.099030 loss_rnnt 380.649750 hw_loss 0.000426 lr 0.00004604 rank 1
2023-02-17 22:48:17,123 DEBUG TRAIN Batch 0/2400 loss 358.528198 loss_att 316.102417 loss_ctc 379.209106 loss_rnnt 364.255676 hw_loss 0.000354 lr 0.00004804 rank 0
2023-02-17 22:48:17,124 DEBUG TRAIN Batch 0/2400 loss 360.286713 loss_att 314.781128 loss_ctc 381.682190 loss_rnnt 366.437775 hw_loss 0.182511 lr 0.00004804 rank 5
2023-02-17 22:48:17,124 DEBUG TRAIN Batch 0/2400 loss 302.932983 loss_att 267.973663 loss_ctc 317.424622 loss_rnnt 307.965790 hw_loss 0.050384 lr 0.00004804 rank 4
2023-02-17 22:48:17,125 DEBUG TRAIN Batch 0/2400 loss 331.463348 loss_att 295.194611 loss_ctc 347.684265 loss_rnnt 336.506592 hw_loss 0.089449 lr 0.00004804 rank 3
2023-02-17 22:48:17,125 DEBUG TRAIN Batch 0/2400 loss 249.639084 loss_att 220.079010 loss_ctc 259.267273 loss_rnnt 254.148651 hw_loss 0.222547 lr 0.00004804 rank 2
2023-02-17 22:48:17,129 DEBUG TRAIN Batch 0/2400 loss 342.396484 loss_att 300.895996 loss_ctc 360.999573 loss_rnnt 348.175873 hw_loss 0.075631 lr 0.00004804 rank 7
2023-02-17 22:48:17,140 DEBUG TRAIN Batch 0/2400 loss 297.125305 loss_att 263.326538 loss_ctc 308.534271 loss_rnnt 302.295471 hw_loss 0.128153 lr 0.00004804 rank 6
2023-02-17 22:48:17,178 DEBUG TRAIN Batch 0/2400 loss 212.690842 loss_att 188.039871 loss_ctc 221.496216 loss_rnnt 216.333084 hw_loss 0.213540 lr 0.00004804 rank 1
2023-02-17 22:49:18,882 DEBUG TRAIN Batch 0/2500 loss 333.632050 loss_att 297.313873 loss_ctc 350.160461 loss_rnnt 338.612091 hw_loss 0.149583 lr 0.00005004 rank 1
2023-02-17 22:49:18,884 DEBUG TRAIN Batch 0/2500 loss 223.439301 loss_att 196.851639 loss_ctc 233.241379 loss_rnnt 227.382278 hw_loss 0.126745 lr 0.00005004 rank 3
2023-02-17 22:49:18,885 DEBUG TRAIN Batch 0/2500 loss 371.163849 loss_att 331.142303 loss_ctc 400.213776 loss_rnnt 375.252167 hw_loss 0.079979 lr 0.00005004 rank 5
2023-02-17 22:49:18,886 DEBUG TRAIN Batch 0/2500 loss 339.700989 loss_att 303.495514 loss_ctc 356.939026 loss_rnnt 344.565582 hw_loss 0.146390 lr 0.00005004 rank 6
2023-02-17 22:49:18,887 DEBUG TRAIN Batch 0/2500 loss 262.734772 loss_att 234.321686 loss_ctc 281.185364 loss_rnnt 265.956940 hw_loss 0.000687 lr 0.00005004 rank 2
2023-02-17 22:49:18,888 DEBUG TRAIN Batch 0/2500 loss 276.371185 loss_att 239.442566 loss_ctc 305.225189 loss_rnnt 279.909332 hw_loss 0.000687 lr 0.00005004 rank 4
2023-02-17 22:49:18,892 DEBUG TRAIN Batch 0/2500 loss 351.407104 loss_att 307.294586 loss_ctc 375.944214 loss_rnnt 356.957642 hw_loss 0.000687 lr 0.00005004 rank 0
2023-02-17 22:49:18,892 DEBUG TRAIN Batch 0/2500 loss 269.017914 loss_att 241.118835 loss_ctc 283.718781 loss_rnnt 272.637268 hw_loss 0.000687 lr 0.00005004 rank 7
2023-02-17 22:50:38,714 DEBUG TRAIN Batch 0/2600 loss 295.177277 loss_att 258.521057 loss_ctc 311.943604 loss_rnnt 300.194885 hw_loss 0.146481 lr 0.00005204 rank 5
2023-02-17 22:50:38,716 DEBUG TRAIN Batch 0/2600 loss 249.563095 loss_att 221.141312 loss_ctc 264.325989 loss_rnnt 253.209824 hw_loss 0.129819 lr 0.00005204 rank 0
2023-02-17 22:50:38,722 DEBUG TRAIN Batch 0/2600 loss 203.911606 loss_att 181.179688 loss_ctc 212.865631 loss_rnnt 207.143677 hw_loss 0.225848 lr 0.00005204 rank 4
2023-02-17 22:50:38,725 DEBUG TRAIN Batch 0/2600 loss 304.607574 loss_att 267.958649 loss_ctc 320.705750 loss_rnnt 309.740265 hw_loss 0.094974 lr 0.00005204 rank 2
2023-02-17 22:50:38,728 DEBUG TRAIN Batch 0/2600 loss 211.597672 loss_att 187.030975 loss_ctc 221.667282 loss_rnnt 215.168259 hw_loss 0.000248 lr 0.00005204 rank 6
2023-02-17 22:50:38,729 DEBUG TRAIN Batch 0/2600 loss 389.657196 loss_att 344.097595 loss_ctc 403.173248 loss_rnnt 396.705902 hw_loss 0.489464 lr 0.00005204 rank 7
2023-02-17 22:50:38,730 DEBUG TRAIN Batch 0/2600 loss 183.100128 loss_att 163.849640 loss_ctc 193.172882 loss_rnnt 185.569748 hw_loss 0.070200 lr 0.00005204 rank 3
2023-02-17 22:50:38,778 DEBUG TRAIN Batch 0/2600 loss 93.239365 loss_att 83.555923 loss_ctc 97.868340 loss_rnnt 94.444016 hw_loss 0.215332 lr 0.00005204 rank 1
2023-02-17 22:51:37,514 DEBUG TRAIN Batch 0/2700 loss 373.432556 loss_att 332.283417 loss_ctc 393.631470 loss_rnnt 378.921356 hw_loss 0.089690 lr 0.00005404 rank 5
2023-02-17 22:51:37,515 DEBUG TRAIN Batch 0/2700 loss 312.739624 loss_att 276.105957 loss_ctc 327.746796 loss_rnnt 317.950043 hw_loss 0.216299 lr 0.00005404 rank 4
2023-02-17 22:51:37,516 DEBUG TRAIN Batch 0/2700 loss 282.970337 loss_att 249.871094 loss_ctc 293.266571 loss_rnnt 288.217194 hw_loss 0.000284 lr 0.00005404 rank 0
2023-02-17 22:51:37,521 DEBUG TRAIN Batch 0/2700 loss 280.025421 loss_att 244.928833 loss_ctc 289.458618 loss_rnnt 285.624420 hw_loss 0.304770 lr 0.00005404 rank 7
2023-02-17 22:51:37,523 DEBUG TRAIN Batch 0/2700 loss 316.151001 loss_att 281.821594 loss_ctc 332.513336 loss_rnnt 320.764557 hw_loss 0.132449 lr 0.00005404 rank 3
2023-02-17 22:51:37,525 DEBUG TRAIN Batch 0/2700 loss 284.162354 loss_att 255.300659 loss_ctc 295.829773 loss_rnnt 288.299744 hw_loss 0.148726 lr 0.00005404 rank 1
2023-02-17 22:51:37,526 DEBUG TRAIN Batch 0/2700 loss 315.842072 loss_att 279.257446 loss_ctc 338.680237 loss_rnnt 320.113770 hw_loss 0.000284 lr 0.00005404 rank 6
2023-02-17 22:51:37,530 DEBUG TRAIN Batch 0/2700 loss 257.318787 loss_att 228.462677 loss_ctc 270.852051 loss_rnnt 261.247192 hw_loss 0.071926 lr 0.00005404 rank 2
2023-02-17 22:52:37,290 DEBUG TRAIN Batch 0/2800 loss 362.757874 loss_att 320.055054 loss_ctc 384.705688 loss_rnnt 368.266876 hw_loss 0.197172 lr 0.00005604 rank 4
2023-02-17 22:52:37,291 DEBUG TRAIN Batch 0/2800 loss 340.461945 loss_att 299.900452 loss_ctc 351.516510 loss_rnnt 347.100189 hw_loss 0.000264 lr 0.00005604 rank 1
2023-02-17 22:52:37,294 DEBUG TRAIN Batch 0/2800 loss 327.659729 loss_att 284.158630 loss_ctc 341.369659 loss_rnnt 334.424683 hw_loss 0.201143 lr 0.00005604 rank 5
2023-02-17 22:52:37,297 DEBUG TRAIN Batch 0/2800 loss 305.093201 loss_att 274.530762 loss_ctc 324.062012 loss_rnnt 308.461273 hw_loss 0.403594 lr 0.00005604 rank 0
2023-02-17 22:52:37,297 DEBUG TRAIN Batch 0/2800 loss 401.465942 loss_att 359.023407 loss_ctc 424.994049 loss_rnnt 406.714996 hw_loss 0.191929 lr 0.00005604 rank 7
2023-02-17 22:52:37,299 DEBUG TRAIN Batch 0/2800 loss 363.604034 loss_att 322.971954 loss_ctc 380.890961 loss_rnnt 369.337708 hw_loss 0.164589 lr 0.00005604 rank 6
2023-02-17 22:52:37,301 DEBUG TRAIN Batch 0/2800 loss 362.417908 loss_att 324.369202 loss_ctc 388.141449 loss_rnnt 366.552979 hw_loss 0.084113 lr 0.00005604 rank 3
2023-02-17 22:52:37,355 DEBUG TRAIN Batch 0/2800 loss 364.021240 loss_att 321.690247 loss_ctc 372.513428 loss_rnnt 371.267792 hw_loss 0.163808 lr 0.00005604 rank 2
2023-02-17 22:53:36,535 DEBUG TRAIN Batch 0/2900 loss 286.214050 loss_att 257.383484 loss_ctc 306.737457 loss_rnnt 289.113342 hw_loss 0.244467 lr 0.00005804 rank 5
2023-02-17 22:53:36,541 DEBUG TRAIN Batch 0/2900 loss 251.400131 loss_att 223.415070 loss_ctc 264.649841 loss_rnnt 255.230438 hw_loss 0.000198 lr 0.00005804 rank 0
2023-02-17 22:53:36,543 DEBUG TRAIN Batch 0/2900 loss 244.523834 loss_att 219.006134 loss_ctc 258.626526 loss_rnnt 247.671539 hw_loss 0.141477 lr 0.00005804 rank 6
2023-02-17 22:53:36,544 DEBUG TRAIN Batch 0/2900 loss 254.923325 loss_att 224.912308 loss_ctc 266.673859 loss_rnnt 259.358673 hw_loss 0.000198 lr 0.00005804 rank 4
2023-02-17 22:53:36,547 DEBUG TRAIN Batch 0/2900 loss 125.690529 loss_att 112.772774 loss_ctc 132.840500 loss_rnnt 127.301712 hw_loss 0.035688 lr 0.00005804 rank 2
2023-02-17 22:53:36,548 DEBUG TRAIN Batch 0/2900 loss 280.773407 loss_att 249.927765 loss_ctc 301.585754 loss_rnnt 284.167419 hw_loss 0.000198 lr 0.00005804 rank 3
2023-02-17 22:53:36,589 DEBUG TRAIN Batch 0/2900 loss 495.422852 loss_att 443.101105 loss_ctc 515.435852 loss_rnnt 503.173859 hw_loss 0.084216 lr 0.00005804 rank 7
2023-02-17 22:53:36,594 DEBUG TRAIN Batch 0/2900 loss 99.035431 loss_att 89.385605 loss_ctc 104.241432 loss_rnnt 100.155983 hw_loss 0.216136 lr 0.00005804 rank 1
2023-02-17 22:54:36,187 DEBUG TRAIN Batch 0/3000 loss 309.539429 loss_att 274.631226 loss_ctc 332.712219 loss_rnnt 313.431183 hw_loss 0.000270 lr 0.00006004 rank 4
2023-02-17 22:54:36,190 DEBUG TRAIN Batch 0/3000 loss 316.875854 loss_att 282.425781 loss_ctc 331.908142 loss_rnnt 321.761414 hw_loss 0.000270 lr 0.00006004 rank 1
2023-02-17 22:54:36,193 DEBUG TRAIN Batch 0/3000 loss 297.528015 loss_att 263.012543 loss_ctc 313.795898 loss_rnnt 302.261932 hw_loss 0.000270 lr 0.00006004 rank 5
2023-02-17 22:54:36,194 DEBUG TRAIN Batch 0/3000 loss 243.602814 loss_att 215.881104 loss_ctc 259.424072 loss_rnnt 247.005096 hw_loss 0.061036 lr 0.00006004 rank 0
2023-02-17 22:54:36,197 DEBUG TRAIN Batch 0/3000 loss 280.410522 loss_att 251.307770 loss_ctc 296.135254 loss_rnnt 284.134277 hw_loss 0.000270 lr 0.00006004 rank 2
2023-02-17 22:54:36,199 DEBUG TRAIN Batch 0/3000 loss 263.738831 loss_att 234.739746 loss_ctc 278.427612 loss_rnnt 267.489441 hw_loss 0.170090 lr 0.00006004 rank 3
2023-02-17 22:54:36,200 DEBUG TRAIN Batch 0/3000 loss 325.533112 loss_att 287.312988 loss_ctc 344.934814 loss_rnnt 330.528229 hw_loss 0.116304 lr 0.00006004 rank 6
2023-02-17 22:54:36,254 DEBUG TRAIN Batch 0/3000 loss 285.478546 loss_att 254.409027 loss_ctc 303.784027 loss_rnnt 289.199860 hw_loss 0.097218 lr 0.00006004 rank 7
2023-02-17 22:55:34,069 DEBUG TRAIN Batch 0/3100 loss 300.142670 loss_att 267.989929 loss_ctc 319.386475 loss_rnnt 304.007172 hw_loss 0.000358 lr 0.00006204 rank 2
2023-02-17 22:55:34,069 DEBUG TRAIN Batch 0/3100 loss 322.549377 loss_att 283.629974 loss_ctc 345.898682 loss_rnnt 327.219818 hw_loss 0.000358 lr 0.00006204 rank 0
2023-02-17 22:55:34,070 DEBUG TRAIN Batch 0/3100 loss 352.958923 loss_att 314.013489 loss_ctc 378.258636 loss_rnnt 357.374512 hw_loss 0.000358 lr 0.00006204 rank 7
2023-02-17 22:55:34,071 DEBUG TRAIN Batch 0/3100 loss 395.677734 loss_att 347.012634 loss_ctc 426.896912 loss_rnnt 401.247955 hw_loss 0.000358 lr 0.00006204 rank 5
2023-02-17 22:55:34,072 DEBUG TRAIN Batch 0/3100 loss 271.414917 loss_att 246.459717 loss_ctc 282.014099 loss_rnnt 274.992523 hw_loss 0.000358 lr 0.00006204 rank 3
2023-02-17 22:55:34,074 DEBUG TRAIN Batch 0/3100 loss 342.212006 loss_att 306.346832 loss_ctc 373.881989 loss_rnnt 345.162201 hw_loss 0.000358 lr 0.00006204 rank 4
2023-02-17 22:55:34,074 DEBUG TRAIN Batch 0/3100 loss 295.252625 loss_att 269.513428 loss_ctc 319.233063 loss_rnnt 297.145477 hw_loss 0.108056 lr 0.00006204 rank 6
2023-02-17 22:55:34,125 DEBUG TRAIN Batch 0/3100 loss 309.108002 loss_att 271.998047 loss_ctc 330.470184 loss_rnnt 313.665039 hw_loss 0.031216 lr 0.00006204 rank 1
2023-02-17 22:56:32,514 DEBUG TRAIN Batch 0/3200 loss 154.244904 loss_att 136.733154 loss_ctc 164.458572 loss_rnnt 156.294846 hw_loss 0.169826 lr 0.00006404 rank 2
2023-02-17 22:56:32,520 DEBUG TRAIN Batch 0/3200 loss 288.798889 loss_att 257.737366 loss_ctc 311.518982 loss_rnnt 291.981689 hw_loss 0.000301 lr 0.00006404 rank 3
2023-02-17 22:56:32,522 DEBUG TRAIN Batch 0/3200 loss 293.218353 loss_att 256.895752 loss_ctc 304.512787 loss_rnnt 298.874756 hw_loss 0.191562 lr 0.00006404 rank 4
2023-02-17 22:56:32,523 DEBUG TRAIN Batch 0/3200 loss 237.003632 loss_att 212.677139 loss_ctc 254.971344 loss_rnnt 239.415466 hw_loss 0.108354 lr 0.00006404 rank 0
2023-02-17 22:56:32,525 DEBUG TRAIN Batch 0/3200 loss 312.163940 loss_att 277.005127 loss_ctc 332.070923 loss_rnnt 316.500183 hw_loss 0.077344 lr 0.00006404 rank 5
2023-02-17 22:56:32,524 DEBUG TRAIN Batch 0/3200 loss 278.977966 loss_att 246.082504 loss_ctc 296.025146 loss_rnnt 283.283936 hw_loss 0.000301 lr 0.00006404 rank 6
2023-02-17 22:56:32,526 DEBUG TRAIN Batch 0/3200 loss 190.119553 loss_att 169.458282 loss_ctc 201.746643 loss_rnnt 192.500427 hw_loss 0.377030 lr 0.00006404 rank 7
2023-02-17 22:56:32,574 DEBUG TRAIN Batch 0/3200 loss 222.931686 loss_att 197.592743 loss_ctc 236.970428 loss_rnnt 225.966766 hw_loss 0.301663 lr 0.00006404 rank 1
2023-02-17 22:57:32,502 DEBUG TRAIN Batch 0/3300 loss 303.806976 loss_att 270.296509 loss_ctc 333.084473 loss_rnnt 306.532654 hw_loss 0.136338 lr 0.00006604 rank 1
2023-02-17 22:57:32,502 DEBUG TRAIN Batch 0/3300 loss 328.995026 loss_att 290.825562 loss_ctc 352.356781 loss_rnnt 333.513855 hw_loss 0.000302 lr 0.00006604 rank 6
2023-02-17 22:57:32,504 DEBUG TRAIN Batch 0/3300 loss 264.175720 loss_att 235.979019 loss_ctc 280.105164 loss_rnnt 267.690979 hw_loss 0.000302 lr 0.00006604 rank 5
2023-02-17 22:57:32,504 DEBUG TRAIN Batch 0/3300 loss 324.616058 loss_att 290.209534 loss_ctc 344.867249 loss_rnnt 328.797028 hw_loss 0.000302 lr 0.00006604 rank 0
2023-02-17 22:57:32,505 DEBUG TRAIN Batch 0/3300 loss 294.664062 loss_att 265.838806 loss_ctc 313.064697 loss_rnnt 297.881897 hw_loss 0.175890 lr 0.00006604 rank 7
2023-02-17 22:57:32,506 DEBUG TRAIN Batch 0/3300 loss 300.320374 loss_att 265.907257 loss_ctc 324.502197 loss_rnnt 303.906067 hw_loss 0.136326 lr 0.00006604 rank 3
2023-02-17 22:57:32,507 DEBUG TRAIN Batch 0/3300 loss 390.733978 loss_att 348.334198 loss_ctc 428.886780 loss_rnnt 394.126740 hw_loss 0.000302 lr 0.00006604 rank 2
2023-02-17 22:57:32,568 DEBUG TRAIN Batch 0/3300 loss 235.440186 loss_att 210.524200 loss_ctc 256.905396 loss_rnnt 237.561203 hw_loss 0.000302 lr 0.00006604 rank 4
2023-02-17 22:58:32,686 DEBUG TRAIN Batch 0/3400 loss 355.065155 loss_att 315.377777 loss_ctc 385.298706 loss_rnnt 358.971375 hw_loss 0.000223 lr 0.00006804 rank 5
2023-02-17 22:58:32,686 DEBUG TRAIN Batch 0/3400 loss 377.993988 loss_att 336.705566 loss_ctc 411.524872 loss_rnnt 381.704803 hw_loss 0.142646 lr 0.00006804 rank 1
2023-02-17 22:58:32,688 DEBUG TRAIN Batch 0/3400 loss 396.671600 loss_att 350.479340 loss_ctc 418.876404 loss_rnnt 402.949310 hw_loss 0.000223 lr 0.00006804 rank 4
2023-02-17 22:58:32,690 DEBUG TRAIN Batch 0/3400 loss 258.616455 loss_att 238.059845 loss_ctc 282.962860 loss_rnnt 259.363129 hw_loss 0.222171 lr 0.00006804 rank 2
2023-02-17 22:58:32,693 DEBUG TRAIN Batch 0/3400 loss 279.533203 loss_att 248.652649 loss_ctc 288.314056 loss_rnnt 284.538422 hw_loss 0.000223 lr 0.00006804 rank 0
2023-02-17 22:58:32,694 DEBUG TRAIN Batch 0/3400 loss 471.193054 loss_att 412.778748 loss_ctc 513.219788 loss_rnnt 477.193787 hw_loss 0.147305 lr 0.00006804 rank 6
2023-02-17 22:58:32,701 DEBUG TRAIN Batch 0/3400 loss 278.297333 loss_att 251.206604 loss_ctc 293.823669 loss_rnnt 281.588135 hw_loss 0.107208 lr 0.00006804 rank 3
2023-02-17 22:58:32,754 DEBUG TRAIN Batch 0/3400 loss 230.516998 loss_att 201.113800 loss_ctc 248.487411 loss_rnnt 233.809433 hw_loss 0.360314 lr 0.00006804 rank 7
2023-02-17 22:59:51,989 DEBUG TRAIN Batch 0/3500 loss 281.313049 loss_att 251.833832 loss_ctc 298.987244 loss_rnnt 284.731445 hw_loss 0.226654 lr 0.00007004 rank 0
2023-02-17 22:59:51,990 DEBUG TRAIN Batch 0/3500 loss 341.345184 loss_att 305.673859 loss_ctc 375.737000 loss_rnnt 343.867188 hw_loss 0.050023 lr 0.00007004 rank 6
2023-02-17 22:59:51,994 DEBUG TRAIN Batch 0/3500 loss 292.253448 loss_att 261.767670 loss_ctc 316.137634 loss_rnnt 295.080109 hw_loss 0.161035 lr 0.00007004 rank 3
2023-02-17 22:59:51,996 DEBUG TRAIN Batch 0/3500 loss 302.578186 loss_att 271.687256 loss_ctc 329.009491 loss_rnnt 305.165588 hw_loss 0.124886 lr 0.00007004 rank 4
2023-02-17 22:59:51,996 DEBUG TRAIN Batch 0/3500 loss 298.575256 loss_att 268.245880 loss_ctc 325.294983 loss_rnnt 301.027222 hw_loss 0.096095 lr 0.00007004 rank 5
2023-02-17 22:59:51,997 DEBUG TRAIN Batch 0/3500 loss 249.396393 loss_att 227.631805 loss_ctc 266.847778 loss_rnnt 251.422394 hw_loss 0.000092 lr 0.00007004 rank 2
2023-02-17 22:59:52,005 DEBUG TRAIN Batch 0/3500 loss 163.903427 loss_att 148.750610 loss_ctc 181.058411 loss_rnnt 164.584167 hw_loss 0.117197 lr 0.00007004 rank 7
2023-02-17 22:59:52,053 DEBUG TRAIN Batch 0/3500 loss 225.683762 loss_att 201.208466 loss_ctc 245.306122 loss_rnnt 227.924225 hw_loss 0.071766 lr 0.00007004 rank 1
2023-02-17 23:00:52,283 DEBUG TRAIN Batch 0/3600 loss 317.161346 loss_att 278.754761 loss_ctc 345.137390 loss_rnnt 321.040314 hw_loss 0.135393 lr 0.00007204 rank 1
2023-02-17 23:00:52,291 DEBUG TRAIN Batch 0/3600 loss 303.235657 loss_att 273.082306 loss_ctc 327.176727 loss_rnnt 305.972778 hw_loss 0.190142 lr 0.00007204 rank 6
2023-02-17 23:00:52,291 DEBUG TRAIN Batch 0/3600 loss 296.498810 loss_att 259.811951 loss_ctc 310.792206 loss_rnnt 301.871185 hw_loss 0.111016 lr 0.00007204 rank 5
2023-02-17 23:00:52,293 DEBUG TRAIN Batch 0/3600 loss 326.154938 loss_att 287.590149 loss_ctc 350.994202 loss_rnnt 330.555908 hw_loss 0.000143 lr 0.00007204 rank 7
2023-02-17 23:00:52,297 DEBUG TRAIN Batch 0/3600 loss 255.886520 loss_att 221.318726 loss_ctc 269.458099 loss_rnnt 260.990448 hw_loss 0.000143 lr 0.00007204 rank 0
2023-02-17 23:00:52,297 DEBUG TRAIN Batch 0/3600 loss 316.428741 loss_att 280.875610 loss_ctc 352.800995 loss_rnnt 318.591614 hw_loss 0.183925 lr 0.00007204 rank 3
2023-02-17 23:00:52,299 DEBUG TRAIN Batch 0/3600 loss 231.470673 loss_att 205.970413 loss_ctc 250.158813 loss_rnnt 233.985733 hw_loss 0.174776 lr 0.00007204 rank 2
2023-02-17 23:00:52,303 DEBUG TRAIN Batch 0/3600 loss 387.632538 loss_att 346.430450 loss_ctc 414.517578 loss_rnnt 392.250549 hw_loss 0.070731 lr 0.00007204 rank 4
2023-02-17 23:01:50,456 DEBUG TRAIN Batch 0/3700 loss 90.873085 loss_att 81.871361 loss_ctc 98.161064 loss_rnnt 91.614838 hw_loss 0.162874 lr 0.00007404 rank 5
2023-02-17 23:01:50,460 DEBUG TRAIN Batch 0/3700 loss 278.022797 loss_att 252.487244 loss_ctc 302.190613 loss_rnnt 279.907471 hw_loss 0.000151 lr 0.00007404 rank 3
2023-02-17 23:01:50,460 DEBUG TRAIN Batch 0/3700 loss 306.704010 loss_att 272.332886 loss_ctc 335.339966 loss_rnnt 309.760010 hw_loss 0.000151 lr 0.00007404 rank 1
2023-02-17 23:01:50,463 DEBUG TRAIN Batch 0/3700 loss 347.795349 loss_att 315.387482 loss_ctc 373.460815 loss_rnnt 350.854767 hw_loss 0.000151 lr 0.00007404 rank 0
2023-02-17 23:01:50,465 DEBUG TRAIN Batch 0/3700 loss 274.910339 loss_att 248.368805 loss_ctc 304.009430 loss_rnnt 276.338715 hw_loss 0.000151 lr 0.00007404 rank 2
2023-02-17 23:01:50,465 DEBUG TRAIN Batch 0/3700 loss 351.106842 loss_att 312.041565 loss_ctc 375.968262 loss_rnnt 355.505432 hw_loss 0.186704 lr 0.00007404 rank 4
2023-02-17 23:01:50,473 DEBUG TRAIN Batch 0/3700 loss 333.966431 loss_att 295.066772 loss_ctc 379.693573 loss_rnnt 335.540100 hw_loss 0.204972 lr 0.00007404 rank 6
2023-02-17 23:01:50,522 DEBUG TRAIN Batch 0/3700 loss 329.337189 loss_att 295.228271 loss_ctc 342.497498 loss_rnnt 334.337189 hw_loss 0.125742 lr 0.00007404 rank 7
2023-02-17 23:02:48,359 DEBUG TRAIN Batch 0/3800 loss 394.285004 loss_att 350.714172 loss_ctc 431.962891 loss_rnnt 397.941132 hw_loss 0.064427 lr 0.00007604 rank 5
2023-02-17 23:02:48,364 DEBUG TRAIN Batch 0/3800 loss 271.147461 loss_att 241.348785 loss_ctc 294.315430 loss_rnnt 273.846222 hw_loss 0.322235 lr 0.00007604 rank 0
2023-02-17 23:02:48,364 DEBUG TRAIN Batch 0/3800 loss 212.096375 loss_att 190.253601 loss_ctc 227.580139 loss_rnnt 214.302246 hw_loss 0.184083 lr 0.00007604 rank 1
2023-02-17 23:02:48,365 DEBUG TRAIN Batch 0/3800 loss 296.332520 loss_att 264.026794 loss_ctc 329.637573 loss_rnnt 298.308929 hw_loss 0.082726 lr 0.00007604 rank 3
2023-02-17 23:02:48,368 DEBUG TRAIN Batch 0/3800 loss 271.935303 loss_att 246.138138 loss_ctc 290.439240 loss_rnnt 274.627533 hw_loss 0.000036 lr 0.00007604 rank 6
2023-02-17 23:02:48,374 DEBUG TRAIN Batch 0/3800 loss 246.916061 loss_att 218.889862 loss_ctc 274.041992 loss_rnnt 248.869751 hw_loss 0.065162 lr 0.00007604 rank 7
2023-02-17 23:02:48,374 DEBUG TRAIN Batch 0/3800 loss 272.428040 loss_att 245.084839 loss_ctc 294.870789 loss_rnnt 274.904297 hw_loss 0.000036 lr 0.00007604 rank 2
2023-02-17 23:02:48,420 DEBUG TRAIN Batch 0/3800 loss 227.160202 loss_att 202.907150 loss_ctc 265.869415 loss_rnnt 226.849548 hw_loss 0.000036 lr 0.00007604 rank 4
2023-02-17 23:03:49,137 DEBUG TRAIN Batch 0/3900 loss 276.677765 loss_att 245.841232 loss_ctc 328.041290 loss_rnnt 275.944244 hw_loss 0.098087 lr 0.00007804 rank 0
2023-02-17 23:03:49,138 DEBUG TRAIN Batch 0/3900 loss 343.844940 loss_att 310.087799 loss_ctc 359.794373 loss_rnnt 348.386414 hw_loss 0.156336 lr 0.00007804 rank 6
2023-02-17 23:03:49,138 DEBUG TRAIN Batch 0/3900 loss 291.180389 loss_att 258.494690 loss_ctc 315.063324 loss_rnnt 294.533081 hw_loss 0.000042 lr 0.00007804 rank 5
2023-02-17 23:03:49,141 DEBUG TRAIN Batch 0/3900 loss 302.774017 loss_att 269.070984 loss_ctc 342.667694 loss_rnnt 304.139526 hw_loss 0.104919 lr 0.00007804 rank 1
2023-02-17 23:03:49,141 DEBUG TRAIN Batch 0/3900 loss 229.702377 loss_att 208.352448 loss_ctc 251.458267 loss_rnnt 231.071548 hw_loss 0.000042 lr 0.00007804 rank 2
2023-02-17 23:03:49,143 DEBUG TRAIN Batch 0/3900 loss 375.003479 loss_att 334.521393 loss_ctc 407.653900 loss_rnnt 378.662506 hw_loss 0.157479 lr 0.00007804 rank 4
2023-02-17 23:03:49,143 DEBUG TRAIN Batch 0/3900 loss 310.711761 loss_att 276.658752 loss_ctc 341.955444 loss_rnnt 313.218597 hw_loss 0.258653 lr 0.00007804 rank 7
2023-02-17 23:03:49,145 DEBUG TRAIN Batch 0/3900 loss 223.277634 loss_att 196.696991 loss_ctc 250.537842 loss_rnnt 224.959045 hw_loss 0.000042 lr 0.00007804 rank 3
2023-02-17 23:04:46,333 DEBUG TRAIN Batch 0/4000 loss 185.597046 loss_att 164.959824 loss_ctc 200.597717 loss_rnnt 187.603210 hw_loss 0.227251 lr 0.00008004 rank 5
2023-02-17 23:04:46,336 DEBUG TRAIN Batch 0/4000 loss 501.138062 loss_att 448.618347 loss_ctc 544.190247 loss_rnnt 505.840210 hw_loss 0.115248 lr 0.00008004 rank 4
2023-02-17 23:04:46,339 DEBUG TRAIN Batch 0/4000 loss 365.023621 loss_att 324.085419 loss_ctc 391.502014 loss_rnnt 369.680725 hw_loss 0.000063 lr 0.00008004 rank 2
2023-02-17 23:04:46,341 DEBUG TRAIN Batch 0/4000 loss 88.489441 loss_att 81.383759 loss_ctc 98.869736 loss_rnnt 88.437828 hw_loss 0.166339 lr 0.00008004 rank 6
2023-02-17 23:04:46,342 DEBUG TRAIN Batch 0/4000 loss 255.635895 loss_att 224.760895 loss_ctc 279.037659 loss_rnnt 258.690643 hw_loss 0.000063 lr 0.00008004 rank 7
2023-02-17 23:04:46,347 DEBUG TRAIN Batch 0/4000 loss 103.051888 loss_att 91.429863 loss_ctc 109.279900 loss_rnnt 104.442375 hw_loss 0.194113 lr 0.00008004 rank 0
2023-02-17 23:04:46,347 DEBUG TRAIN Batch 0/4000 loss 66.374985 loss_att 61.046440 loss_ctc 69.933182 loss_rnnt 66.844810 hw_loss 0.227737 lr 0.00008004 rank 3
2023-02-17 23:04:46,349 DEBUG TRAIN Batch 0/4000 loss 369.714294 loss_att 326.158997 loss_ctc 400.484100 loss_rnnt 374.196106 hw_loss 0.237389 lr 0.00008004 rank 1
2023-02-17 23:05:44,665 DEBUG TRAIN Batch 0/4100 loss 361.249603 loss_att 326.241821 loss_ctc 398.364410 loss_rnnt 363.302490 hw_loss 0.000051 lr 0.00008204 rank 5
2023-02-17 23:05:44,668 DEBUG TRAIN Batch 0/4100 loss 200.341751 loss_att 180.326660 loss_ctc 216.157990 loss_rnnt 202.136688 hw_loss 0.186101 lr 0.00008204 rank 1
2023-02-17 23:05:44,672 DEBUG TRAIN Batch 0/4100 loss 258.556458 loss_att 228.788361 loss_ctc 281.060913 loss_rnnt 261.394745 hw_loss 0.215152 lr 0.00008204 rank 7
2023-02-17 23:05:44,673 DEBUG TRAIN Batch 0/4100 loss 292.657928 loss_att 259.583435 loss_ctc 318.114716 loss_rnnt 295.821838 hw_loss 0.106441 lr 0.00008204 rank 4
2023-02-17 23:05:44,675 DEBUG TRAIN Batch 0/4100 loss 296.151306 loss_att 260.572723 loss_ctc 329.381409 loss_rnnt 298.732666 hw_loss 0.194379 lr 0.00008204 rank 6
2023-02-17 23:05:44,678 DEBUG TRAIN Batch 0/4100 loss 318.503265 loss_att 280.863342 loss_ctc 347.890442 loss_rnnt 322.046356 hw_loss 0.124884 lr 0.00008204 rank 3
2023-02-17 23:05:44,683 DEBUG TRAIN Batch 0/4100 loss 353.959229 loss_att 318.748230 loss_ctc 389.653534 loss_rnnt 356.188171 hw_loss 0.101288 lr 0.00008204 rank 0
2023-02-17 23:05:44,732 DEBUG TRAIN Batch 0/4100 loss 229.202866 loss_att 204.630188 loss_ctc 250.250610 loss_rnnt 231.258514 hw_loss 0.098491 lr 0.00008204 rank 2
2023-02-17 23:06:46,581 DEBUG TRAIN Batch 0/4200 loss 249.336975 loss_att 221.921097 loss_ctc 274.027191 loss_rnnt 251.528076 hw_loss 0.000067 lr 0.00008404 rank 3
2023-02-17 23:06:46,582 DEBUG TRAIN Batch 0/4200 loss 279.962677 loss_att 248.192169 loss_ctc 303.711609 loss_rnnt 282.925964 hw_loss 0.420513 lr 0.00008404 rank 0
2023-02-17 23:06:46,582 DEBUG TRAIN Batch 0/4200 loss 260.670898 loss_att 235.678680 loss_ctc 290.375122 loss_rnnt 261.708740 hw_loss 0.000067 lr 0.00008404 rank 5
2023-02-17 23:06:46,584 DEBUG TRAIN Batch 0/4200 loss 299.075531 loss_att 267.049561 loss_ctc 323.398590 loss_rnnt 302.237640 hw_loss 0.000067 lr 0.00008404 rank 4
2023-02-17 23:06:46,585 DEBUG TRAIN Batch 0/4200 loss 319.565247 loss_att 284.671875 loss_ctc 370.535645 loss_rnnt 319.642456 hw_loss 0.197662 lr 0.00008404 rank 2
2023-02-17 23:06:46,589 DEBUG TRAIN Batch 0/4200 loss 266.256287 loss_att 239.202377 loss_ctc 282.678131 loss_rnnt 269.477448 hw_loss 0.000067 lr 0.00008404 rank 6
2023-02-17 23:06:46,596 DEBUG TRAIN Batch 0/4200 loss 334.465881 loss_att 298.516418 loss_ctc 355.995056 loss_rnnt 338.731873 hw_loss 0.100002 lr 0.00008404 rank 1
2023-02-17 23:06:46,599 DEBUG TRAIN Batch 0/4200 loss 287.254211 loss_att 258.132324 loss_ctc 311.703400 loss_rnnt 289.762817 hw_loss 0.104853 lr 0.00008404 rank 7
2023-02-17 23:08:04,734 DEBUG TRAIN Batch 0/4300 loss 145.083221 loss_att 130.756165 loss_ctc 165.531754 loss_rnnt 145.222137 hw_loss 0.000035 lr 0.00008604 rank 0
2023-02-17 23:08:04,734 DEBUG TRAIN Batch 0/4300 loss 231.091873 loss_att 208.802704 loss_ctc 260.164917 loss_rnnt 231.497131 hw_loss 0.330324 lr 0.00008604 rank 5
2023-02-17 23:08:04,739 DEBUG TRAIN Batch 0/4300 loss 323.809082 loss_att 290.837280 loss_ctc 356.506042 loss_rnnt 326.043823 hw_loss 0.000035 lr 0.00008604 rank 1
2023-02-17 23:08:04,740 DEBUG TRAIN Batch 0/4300 loss 282.913391 loss_att 256.405945 loss_ctc 313.068634 loss_rnnt 284.194153 hw_loss 0.000035 lr 0.00008604 rank 2
2023-02-17 23:08:04,743 DEBUG TRAIN Batch 0/4300 loss 163.468872 loss_att 147.083054 loss_ctc 178.278183 loss_rnnt 164.662582 hw_loss 0.204159 lr 0.00008604 rank 4
2023-02-17 23:08:04,743 DEBUG TRAIN Batch 0/4300 loss 329.808105 loss_att 295.103638 loss_ctc 349.983521 loss_rnnt 333.853546 hw_loss 0.385150 lr 0.00008604 rank 7
2023-02-17 23:08:04,751 DEBUG TRAIN Batch 0/4300 loss 206.136978 loss_att 184.342529 loss_ctc 229.953812 loss_rnnt 207.320282 hw_loss 0.000035 lr 0.00008604 rank 3
2023-02-17 23:08:04,795 DEBUG TRAIN Batch 0/4300 loss 207.605347 loss_att 187.075150 loss_ctc 229.775726 loss_rnnt 208.684494 hw_loss 0.132833 lr 0.00008604 rank 6
2023-02-17 23:09:06,012 DEBUG TRAIN Batch 0/4400 loss 274.106293 loss_att 244.047180 loss_ctc 306.765198 loss_rnnt 275.733032 hw_loss 0.057263 lr 0.00008804 rank 5
2023-02-17 23:09:06,014 DEBUG TRAIN Batch 0/4400 loss 329.018707 loss_att 293.635834 loss_ctc 355.526855 loss_rnnt 332.560852 hw_loss 0.000025 lr 0.00008804 rank 6
2023-02-17 23:09:06,014 DEBUG TRAIN Batch 0/4400 loss 304.150879 loss_att 271.828613 loss_ctc 322.961243 loss_rnnt 308.057037 hw_loss 0.094226 lr 0.00008804 rank 4
2023-02-17 23:09:06,014 DEBUG TRAIN Batch 0/4400 loss 263.977905 loss_att 238.534912 loss_ctc 292.569763 loss_rnnt 265.190826 hw_loss 0.118938 lr 0.00008804 rank 2
2023-02-17 23:09:06,015 DEBUG TRAIN Batch 0/4400 loss 284.290833 loss_att 251.612061 loss_ctc 322.656342 loss_rnnt 285.606995 hw_loss 0.195285 lr 0.00008804 rank 7
2023-02-17 23:09:06,017 DEBUG TRAIN Batch 0/4400 loss 266.937744 loss_att 238.636627 loss_ctc 290.010925 loss_rnnt 269.421753 hw_loss 0.187097 lr 0.00008804 rank 3
2023-02-17 23:09:06,052 DEBUG TRAIN Batch 0/4400 loss 327.787476 loss_att 295.327850 loss_ctc 361.036530 loss_rnnt 329.763489 hw_loss 0.154993 lr 0.00008804 rank 0
2023-02-17 23:09:06,057 DEBUG TRAIN Batch 0/4400 loss 257.258057 loss_att 228.930084 loss_ctc 275.351807 loss_rnnt 260.426971 hw_loss 0.157817 lr 0.00008804 rank 1
2023-02-17 23:10:07,191 DEBUG TRAIN Batch 0/4500 loss 267.738586 loss_att 241.031372 loss_ctc 303.927643 loss_rnnt 268.143127 hw_loss 0.209418 lr 0.00009004 rank 5
2023-02-17 23:10:07,202 DEBUG TRAIN Batch 0/4500 loss 214.781113 loss_att 189.731659 loss_ctc 239.689804 loss_rnnt 216.469818 hw_loss 0.000074 lr 0.00009004 rank 6
2023-02-17 23:10:07,208 DEBUG TRAIN Batch 0/4500 loss 259.934021 loss_att 229.054901 loss_ctc 289.729523 loss_rnnt 262.049866 hw_loss 0.163573 lr 0.00009004 rank 3
2023-02-17 23:10:07,210 DEBUG TRAIN Batch 0/4500 loss 290.617340 loss_att 261.625793 loss_ctc 325.366364 loss_rnnt 291.680176 hw_loss 0.191692 lr 0.00009004 rank 1
2023-02-17 23:10:07,210 DEBUG TRAIN Batch 0/4500 loss 339.312042 loss_att 296.606873 loss_ctc 359.432770 loss_rnnt 345.019012 hw_loss 0.283760 lr 0.00009004 rank 7
2023-02-17 23:10:07,212 DEBUG TRAIN Batch 0/4500 loss 213.302185 loss_att 191.731094 loss_ctc 240.544754 loss_rnnt 213.841660 hw_loss 0.267028 lr 0.00009004 rank 2
2023-02-17 23:10:07,217 DEBUG TRAIN Batch 0/4500 loss 301.413025 loss_att 263.393585 loss_ctc 345.182434 loss_rnnt 303.180969 hw_loss 0.000074 lr 0.00009004 rank 4
2023-02-17 23:10:07,268 DEBUG TRAIN Batch 0/4500 loss 275.934845 loss_att 242.779816 loss_ctc 298.480408 loss_rnnt 279.501129 hw_loss 0.109953 lr 0.00009004 rank 0
2023-02-17 23:11:05,928 DEBUG TRAIN Batch 0/4600 loss 294.070496 loss_att 263.331146 loss_ctc 310.187866 loss_rnnt 298.069336 hw_loss 0.000043 lr 0.00009204 rank 1
2023-02-17 23:11:05,929 DEBUG TRAIN Batch 0/4600 loss 198.810501 loss_att 176.947815 loss_ctc 222.049835 loss_rnnt 200.084442 hw_loss 0.000043 lr 0.00009204 rank 5
2023-02-17 23:11:05,931 DEBUG TRAIN Batch 0/4600 loss 205.308807 loss_att 183.432434 loss_ctc 229.096802 loss_rnnt 206.352844 hw_loss 0.299056 lr 0.00009204 rank 3
2023-02-17 23:11:05,933 DEBUG TRAIN Batch 0/4600 loss 115.140312 loss_att 104.906647 loss_ctc 127.655861 loss_rnnt 115.420815 hw_loss 0.182786 lr 0.00009204 rank 2
2023-02-17 23:11:05,935 DEBUG TRAIN Batch 0/4600 loss 272.310699 loss_att 247.430374 loss_ctc 304.610626 loss_rnnt 272.980072 hw_loss 0.000043 lr 0.00009204 rank 7
2023-02-17 23:11:05,935 DEBUG TRAIN Batch 0/4600 loss 214.436264 loss_att 191.146683 loss_ctc 238.748444 loss_rnnt 215.723312 hw_loss 0.242334 lr 0.00009204 rank 4
2023-02-17 23:11:05,938 DEBUG TRAIN Batch 0/4600 loss 175.415314 loss_att 156.517548 loss_ctc 191.281601 loss_rnnt 177.079346 hw_loss 0.000043 lr 0.00009204 rank 6
2023-02-17 23:11:05,990 DEBUG TRAIN Batch 0/4600 loss 256.899475 loss_att 233.879547 loss_ctc 282.677490 loss_rnnt 258.066345 hw_loss 0.000043 lr 0.00009204 rank 0
2023-02-17 23:12:05,323 DEBUG TRAIN Batch 0/4700 loss 281.811188 loss_att 252.977264 loss_ctc 305.993774 loss_rnnt 284.295502 hw_loss 0.109044 lr 0.00009404 rank 5
2023-02-17 23:12:05,325 DEBUG TRAIN Batch 0/4700 loss 321.252808 loss_att 284.218323 loss_ctc 351.180176 loss_rnnt 324.526337 hw_loss 0.268229 lr 0.00009404 rank 0
2023-02-17 23:12:05,325 DEBUG TRAIN Batch 0/4700 loss 312.383240 loss_att 281.388580 loss_ctc 346.488037 loss_rnnt 313.963013 hw_loss 0.134735 lr 0.00009404 rank 6
2023-02-17 23:12:05,328 DEBUG TRAIN Batch 0/4700 loss 268.583557 loss_att 239.422287 loss_ctc 303.912750 loss_rnnt 269.705261 hw_loss 0.000038 lr 0.00009404 rank 2
2023-02-17 23:12:05,329 DEBUG TRAIN Batch 0/4700 loss 250.039642 loss_att 219.533447 loss_ctc 275.338745 loss_rnnt 252.686249 hw_loss 0.152658 lr 0.00009404 rank 1
2023-02-17 23:12:05,331 DEBUG TRAIN Batch 0/4700 loss 258.881897 loss_att 232.824478 loss_ctc 283.250732 loss_rnnt 260.810303 hw_loss 0.063533 lr 0.00009404 rank 3
2023-02-17 23:12:05,333 DEBUG TRAIN Batch 0/4700 loss 227.724335 loss_att 201.724045 loss_ctc 246.161346 loss_rnnt 230.466095 hw_loss 0.000038 lr 0.00009404 rank 7
2023-02-17 23:12:05,334 DEBUG TRAIN Batch 0/4700 loss 265.639282 loss_att 238.392334 loss_ctc 284.290283 loss_rnnt 268.541443 hw_loss 0.113328 lr 0.00009404 rank 4
2023-02-17 23:13:06,668 DEBUG TRAIN Batch 0/4800 loss 372.429047 loss_att 325.275238 loss_ctc 415.896210 loss_rnnt 376.064087 hw_loss 0.000117 lr 0.00009604 rank 5
2023-02-17 23:13:06,669 DEBUG TRAIN Batch 0/4800 loss 212.490204 loss_att 189.510925 loss_ctc 229.783829 loss_rnnt 214.728424 hw_loss 0.097168 lr 0.00009604 rank 7
2023-02-17 23:13:06,672 DEBUG TRAIN Batch 0/4800 loss 361.956970 loss_att 325.523560 loss_ctc 402.465851 loss_rnnt 363.842377 hw_loss 0.000117 lr 0.00009604 rank 2
2023-02-17 23:13:06,674 DEBUG TRAIN Batch 0/4800 loss 342.489014 loss_att 303.369019 loss_ctc 381.045349 loss_rnnt 345.172119 hw_loss 0.000117 lr 0.00009604 rank 3
2023-02-17 23:13:06,685 DEBUG TRAIN Batch 0/4800 loss 321.069824 loss_att 284.452087 loss_ctc 360.616241 loss_rnnt 323.041779 hw_loss 0.147610 lr 0.00009604 rank 4
2023-02-17 23:13:06,709 DEBUG TRAIN Batch 0/4800 loss 395.243713 loss_att 347.583832 loss_ctc 446.246948 loss_rnnt 397.975159 hw_loss 0.000117 lr 0.00009604 rank 0
2023-02-17 23:13:06,710 DEBUG TRAIN Batch 0/4800 loss 257.439697 loss_att 227.578217 loss_ctc 280.367035 loss_rnnt 260.288086 hw_loss 0.125485 lr 0.00009604 rank 1
2023-02-17 23:13:06,735 DEBUG TRAIN Batch 0/4800 loss 356.503967 loss_att 316.236816 loss_ctc 411.717346 loss_rnnt 357.124573 hw_loss 0.133205 lr 0.00009604 rank 6
2023-02-17 23:14:04,718 DEBUG TRAIN Batch 0/4900 loss 252.987503 loss_att 224.088928 loss_ctc 278.192993 loss_rnnt 255.406433 hw_loss 0.000093 lr 0.00009804 rank 5
2023-02-17 23:14:04,719 DEBUG TRAIN Batch 0/4900 loss 238.069809 loss_att 211.890579 loss_ctc 262.948059 loss_rnnt 239.893707 hw_loss 0.177826 lr 0.00009804 rank 4
2023-02-17 23:14:04,722 DEBUG TRAIN Batch 0/4900 loss 239.553558 loss_att 211.642075 loss_ctc 260.662659 loss_rnnt 242.282639 hw_loss 0.072533 lr 0.00009804 rank 3
2023-02-17 23:14:04,723 DEBUG TRAIN Batch 0/4900 loss 129.774231 loss_att 115.902390 loss_ctc 142.697388 loss_rnnt 130.773834 hw_loss 0.096911 lr 0.00009804 rank 7
2023-02-17 23:14:04,723 DEBUG TRAIN Batch 0/4900 loss 287.706146 loss_att 255.084839 loss_ctc 320.339050 loss_rnnt 289.797302 hw_loss 0.153847 lr 0.00009804 rank 1
2023-02-17 23:14:04,726 DEBUG TRAIN Batch 0/4900 loss 188.877106 loss_att 167.804779 loss_ctc 209.611053 loss_rnnt 190.235626 hw_loss 0.171381 lr 0.00009804 rank 6
2023-02-17 23:14:04,728 DEBUG TRAIN Batch 0/4900 loss 215.069214 loss_att 193.175476 loss_ctc 238.084991 loss_rnnt 216.340637 hw_loss 0.072287 lr 0.00009804 rank 0
2023-02-17 23:14:04,736 DEBUG TRAIN Batch 0/4900 loss 156.672241 loss_att 139.507050 loss_ctc 180.322891 loss_rnnt 156.951813 hw_loss 0.000093 lr 0.00009804 rank 2
2023-02-17 23:15:04,779 DEBUG TRAIN Batch 0/5000 loss 270.788727 loss_att 244.166168 loss_ctc 307.423706 loss_rnnt 271.194489 hw_loss 0.063854 lr 0.00010004 rank 0
2023-02-17 23:15:04,780 DEBUG TRAIN Batch 0/5000 loss 222.976700 loss_att 198.039902 loss_ctc 250.510162 loss_rnnt 224.253601 hw_loss 0.073728 lr 0.00010004 rank 1
2023-02-17 23:15:04,780 DEBUG TRAIN Batch 0/5000 loss 271.234741 loss_att 240.147339 loss_ctc 296.074951 loss_rnnt 274.056030 hw_loss 0.157781 lr 0.00010004 rank 5
2023-02-17 23:15:04,782 DEBUG TRAIN Batch 0/5000 loss 318.429199 loss_att 288.955597 loss_ctc 351.534119 loss_rnnt 319.828918 hw_loss 0.151870 lr 0.00010004 rank 2
2023-02-17 23:15:04,783 DEBUG TRAIN Batch 0/5000 loss 224.428345 loss_att 197.921829 loss_ctc 246.173767 loss_rnnt 226.830215 hw_loss 0.000064 lr 0.00010004 rank 6
2023-02-17 23:15:04,792 DEBUG TRAIN Batch 0/5000 loss 361.112488 loss_att 327.061310 loss_ctc 413.982178 loss_rnnt 360.873352 hw_loss 0.000064 lr 0.00010004 rank 3
2023-02-17 23:15:04,795 DEBUG TRAIN Batch 0/5000 loss 278.778168 loss_att 246.994720 loss_ctc 313.443420 loss_rnnt 280.474365 hw_loss 0.072124 lr 0.00010004 rank 7
2023-02-17 23:15:04,844 DEBUG TRAIN Batch 0/5000 loss 292.353333 loss_att 260.188751 loss_ctc 324.831787 loss_rnnt 294.455719 hw_loss 0.000064 lr 0.00010004 rank 4
2023-02-17 23:16:07,116 DEBUG TRAIN Batch 0/5100 loss 392.445648 loss_att 354.738373 loss_ctc 440.960144 loss_rnnt 393.441803 hw_loss 0.143843 lr 0.00010204 rank 0
2023-02-17 23:16:07,117 DEBUG TRAIN Batch 0/5100 loss 299.267303 loss_att 260.056641 loss_ctc 332.368500 loss_rnnt 302.628662 hw_loss 0.126098 lr 0.00010204 rank 7
2023-02-17 23:16:07,120 DEBUG TRAIN Batch 0/5100 loss 420.827209 loss_att 366.656311 loss_ctc 456.747559 loss_rnnt 426.872009 hw_loss 0.000099 lr 0.00010204 rank 2
2023-02-17 23:16:07,120 DEBUG TRAIN Batch 0/5100 loss 285.079956 loss_att 251.931915 loss_ctc 298.508881 loss_rnnt 289.919006 hw_loss 0.000099 lr 0.00010204 rank 4
2023-02-17 23:16:07,123 DEBUG TRAIN Batch 0/5100 loss 313.691559 loss_att 276.349182 loss_ctc 339.433350 loss_rnnt 317.727753 hw_loss 0.000099 lr 0.00010204 rank 5
2023-02-17 23:16:07,124 DEBUG TRAIN Batch 0/5100 loss 248.217850 loss_att 220.939392 loss_ctc 275.161011 loss_rnnt 249.984039 hw_loss 0.181996 lr 0.00010204 rank 3
2023-02-17 23:16:07,151 DEBUG TRAIN Batch 0/5100 loss 340.143097 loss_att 304.412781 loss_ctc 371.577850 loss_rnnt 343.097809 hw_loss 0.000099 lr 0.00010204 rank 1
2023-02-17 23:16:07,208 DEBUG TRAIN Batch 0/5100 loss 305.354706 loss_att 268.161346 loss_ctc 338.578461 loss_rnnt 308.312805 hw_loss 0.095153 lr 0.00010204 rank 6
2023-02-17 23:17:25,356 DEBUG TRAIN Batch 0/5200 loss 322.374481 loss_att 289.610565 loss_ctc 358.063202 loss_rnnt 323.975708 hw_loss 0.362002 lr 0.00010404 rank 5
2023-02-17 23:17:25,357 DEBUG TRAIN Batch 0/5200 loss 164.497482 loss_att 142.039078 loss_ctc 190.057541 loss_rnnt 165.581131 hw_loss 0.000043 lr 0.00010404 rank 1
2023-02-17 23:17:25,359 DEBUG TRAIN Batch 0/5200 loss 206.594315 loss_att 189.174652 loss_ctc 234.483032 loss_rnnt 206.308990 hw_loss 0.095150 lr 0.00010404 rank 2
2023-02-17 23:17:25,367 DEBUG TRAIN Batch 0/5200 loss 266.991760 loss_att 239.079315 loss_ctc 295.246521 loss_rnnt 268.692139 hw_loss 0.215243 lr 0.00010404 rank 3
2023-02-17 23:17:25,368 DEBUG TRAIN Batch 0/5200 loss 189.850037 loss_att 168.533600 loss_ctc 214.686432 loss_rnnt 190.733749 hw_loss 0.127610 lr 0.00010404 rank 7
2023-02-17 23:17:25,369 DEBUG TRAIN Batch 0/5200 loss 229.560043 loss_att 203.967072 loss_ctc 250.549255 loss_rnnt 231.880035 hw_loss 0.000043 lr 0.00010404 rank 0
2023-02-17 23:17:25,402 DEBUG TRAIN Batch 0/5200 loss 234.869568 loss_att 207.410324 loss_ctc 253.787048 loss_rnnt 237.777466 hw_loss 0.115519 lr 0.00010404 rank 4
2023-02-17 23:17:25,404 DEBUG TRAIN Batch 0/5200 loss 296.751526 loss_att 267.240295 loss_ctc 333.543091 loss_rnnt 297.682953 hw_loss 0.122372 lr 0.00010404 rank 6
2023-02-17 23:18:28,122 DEBUG TRAIN Batch 0/5300 loss 327.106323 loss_att 293.728424 loss_ctc 363.685089 loss_rnnt 328.878387 hw_loss 0.049476 lr 0.00010604 rank 1
2023-02-17 23:18:28,123 DEBUG TRAIN Batch 0/5300 loss 321.526764 loss_att 284.994751 loss_ctc 337.164368 loss_rnnt 326.690491 hw_loss 0.108081 lr 0.00010604 rank 4
2023-02-17 23:18:28,126 DEBUG TRAIN Batch 0/5300 loss 279.347687 loss_att 248.090240 loss_ctc 309.881165 loss_rnnt 281.446594 hw_loss 0.152753 lr 0.00010604 rank 0
2023-02-17 23:18:28,126 DEBUG TRAIN Batch 0/5300 loss 227.376450 loss_att 202.479218 loss_ctc 247.971268 loss_rnnt 229.609909 hw_loss 0.000055 lr 0.00010604 rank 5
2023-02-17 23:18:28,128 DEBUG TRAIN Batch 0/5300 loss 267.229279 loss_att 237.328033 loss_ctc 298.149231 loss_rnnt 269.035034 hw_loss 0.097233 lr 0.00010604 rank 7
2023-02-17 23:18:28,132 DEBUG TRAIN Batch 0/5300 loss 353.529297 loss_att 313.370361 loss_ctc 414.592194 loss_rnnt 353.354889 hw_loss 0.120877 lr 0.00010604 rank 3
2023-02-17 23:18:28,132 DEBUG TRAIN Batch 0/5300 loss 262.024139 loss_att 231.146912 loss_ctc 303.028961 loss_rnnt 262.655396 hw_loss 0.144104 lr 0.00010604 rank 6
2023-02-17 23:18:28,190 DEBUG TRAIN Batch 0/5300 loss 310.522125 loss_att 276.616730 loss_ctc 342.696289 loss_rnnt 312.944275 hw_loss 0.129425 lr 0.00010604 rank 2
2023-02-17 23:19:26,585 DEBUG TRAIN Batch 0/5400 loss 308.088043 loss_att 270.157837 loss_ctc 330.592865 loss_rnnt 312.620667 hw_loss 0.098920 lr 0.00010804 rank 5
2023-02-17 23:19:26,585 DEBUG TRAIN Batch 0/5400 loss 282.165924 loss_att 253.110687 loss_ctc 307.702698 loss_rnnt 284.563354 hw_loss 0.016349 lr 0.00010804 rank 0
2023-02-17 23:19:26,585 DEBUG TRAIN Batch 0/5400 loss 254.764481 loss_att 226.860138 loss_ctc 287.280884 loss_rnnt 256.009766 hw_loss 0.000117 lr 0.00010804 rank 1
2023-02-17 23:19:26,588 DEBUG TRAIN Batch 0/5400 loss 309.444672 loss_att 277.142090 loss_ctc 359.667267 loss_rnnt 309.208740 hw_loss 0.000117 lr 0.00010804 rank 4
2023-02-17 23:19:26,590 DEBUG TRAIN Batch 0/5400 loss 274.632812 loss_att 243.991119 loss_ctc 285.732666 loss_rnnt 279.270874 hw_loss 0.019300 lr 0.00010804 rank 7
2023-02-17 23:19:26,593 DEBUG TRAIN Batch 0/5400 loss 291.688416 loss_att 261.850586 loss_ctc 326.788605 loss_rnnt 292.975861 hw_loss 0.000117 lr 0.00010804 rank 6
2023-02-17 23:19:26,602 DEBUG TRAIN Batch 0/5400 loss 418.872986 loss_att 373.937317 loss_ctc 463.026184 loss_rnnt 421.938965 hw_loss 0.063828 lr 0.00010804 rank 3
2023-02-17 23:19:26,603 DEBUG TRAIN Batch 0/5400 loss 289.569672 loss_att 261.330688 loss_ctc 326.967865 loss_rnnt 290.141052 hw_loss 0.168716 lr 0.00010804 rank 2
2023-02-17 23:20:24,394 DEBUG TRAIN Batch 0/5500 loss 225.463379 loss_att 203.983932 loss_ctc 248.530258 loss_rnnt 226.632202 hw_loss 0.096495 lr 0.00011004 rank 7
2023-02-17 23:20:24,394 DEBUG TRAIN Batch 0/5500 loss 238.286194 loss_att 218.682037 loss_ctc 271.368561 loss_rnnt 237.767334 hw_loss 0.053842 lr 0.00011004 rank 4
2023-02-17 23:20:24,395 DEBUG TRAIN Batch 0/5500 loss 290.323547 loss_att 253.672302 loss_ctc 326.772461 loss_rnnt 292.669647 hw_loss 0.233068 lr 0.00011004 rank 5
2023-02-17 23:20:24,398 DEBUG TRAIN Batch 0/5500 loss 305.034912 loss_att 276.214478 loss_ctc 351.028473 loss_rnnt 304.666534 hw_loss 0.000035 lr 0.00011004 rank 1
2023-02-17 23:20:24,401 DEBUG TRAIN Batch 0/5500 loss 249.689835 loss_att 221.223785 loss_ctc 285.032410 loss_rnnt 250.623322 hw_loss 0.088855 lr 0.00011004 rank 2
2023-02-17 23:20:24,401 DEBUG TRAIN Batch 0/5500 loss 284.581970 loss_att 253.657684 loss_ctc 308.917572 loss_rnnt 287.415863 hw_loss 0.199201 lr 0.00011004 rank 3
2023-02-17 23:20:24,406 DEBUG TRAIN Batch 0/5500 loss 242.275604 loss_att 214.321518 loss_ctc 264.224213 loss_rnnt 244.791046 hw_loss 0.279152 lr 0.00011004 rank 0
2023-02-17 23:20:24,406 DEBUG TRAIN Batch 0/5500 loss 257.766937 loss_att 232.084946 loss_ctc 282.557770 loss_rnnt 259.562500 hw_loss 0.066301 lr 0.00011004 rank 6
2023-02-17 23:21:25,719 DEBUG TRAIN Batch 0/5600 loss 232.638275 loss_att 211.976059 loss_ctc 268.994049 loss_rnnt 231.855103 hw_loss 0.127869 lr 0.00011204 rank 5
2023-02-17 23:21:25,721 DEBUG TRAIN Batch 0/5600 loss 269.034668 loss_att 243.720856 loss_ctc 301.965668 loss_rnnt 269.706604 hw_loss 0.000049 lr 0.00011204 rank 2
2023-02-17 23:21:25,724 DEBUG TRAIN Batch 0/5600 loss 228.061218 loss_att 205.284271 loss_ctc 258.255188 loss_rnnt 228.519592 hw_loss 0.133406 lr 0.00011204 rank 1
2023-02-17 23:21:25,725 DEBUG TRAIN Batch 0/5600 loss 238.116119 loss_att 213.822281 loss_ctc 258.962036 loss_rnnt 240.128403 hw_loss 0.125641 lr 0.00011204 rank 3
2023-02-17 23:21:25,727 DEBUG TRAIN Batch 0/5600 loss 196.220322 loss_att 184.657654 loss_ctc 206.265610 loss_rnnt 197.052231 hw_loss 0.264855 lr 0.00011204 rank 4
2023-02-17 23:21:25,729 DEBUG TRAIN Batch 0/5600 loss 322.005096 loss_att 289.193451 loss_ctc 352.839355 loss_rnnt 324.400330 hw_loss 0.104764 lr 0.00011204 rank 7
2023-02-17 23:21:25,731 DEBUG TRAIN Batch 0/5600 loss 185.552826 loss_att 168.984772 loss_ctc 206.979538 loss_rnnt 186.009521 hw_loss 0.000049 lr 0.00011204 rank 0
2023-02-17 23:21:25,780 DEBUG TRAIN Batch 0/5600 loss 207.401627 loss_att 186.317841 loss_ctc 234.925339 loss_rnnt 207.948517 hw_loss 0.000049 lr 0.00011204 rank 6
2023-02-17 23:22:25,032 DEBUG TRAIN Batch 0/5700 loss 276.496216 loss_att 253.319733 loss_ctc 320.946472 loss_rnnt 275.198181 hw_loss 0.012458 lr 0.00011404 rank 5
2023-02-17 23:22:25,038 DEBUG TRAIN Batch 0/5700 loss 275.974487 loss_att 247.409637 loss_ctc 308.828369 loss_rnnt 277.306885 hw_loss 0.000074 lr 0.00011404 rank 0
2023-02-17 23:22:25,041 DEBUG TRAIN Batch 0/5700 loss 353.560211 loss_att 319.415039 loss_ctc 397.072723 loss_rnnt 354.490723 hw_loss 0.181562 lr 0.00011404 rank 4
2023-02-17 23:22:25,041 DEBUG TRAIN Batch 0/5700 loss 249.817032 loss_att 223.103577 loss_ctc 268.264435 loss_rnnt 252.641144 hw_loss 0.110421 lr 0.00011404 rank 6
2023-02-17 23:22:25,042 DEBUG TRAIN Batch 0/5700 loss 318.224731 loss_att 281.937317 loss_ctc 357.902039 loss_rnnt 320.191895 hw_loss 0.000074 lr 0.00011404 rank 7
2023-02-17 23:22:25,046 DEBUG TRAIN Batch 0/5700 loss 256.534546 loss_att 234.379028 loss_ctc 291.304169 loss_rnnt 256.244385 hw_loss 0.159990 lr 0.00011404 rank 3
2023-02-17 23:22:25,072 DEBUG TRAIN Batch 0/5700 loss 226.836945 loss_att 202.928726 loss_ctc 248.865997 loss_rnnt 228.503082 hw_loss 0.334273 lr 0.00011404 rank 2
2023-02-17 23:22:25,088 DEBUG TRAIN Batch 0/5700 loss 223.463562 loss_att 205.666260 loss_ctc 244.171448 loss_rnnt 224.247223 hw_loss 0.027608 lr 0.00011404 rank 1
2023-02-17 23:23:24,246 DEBUG TRAIN Batch 0/5800 loss 250.304001 loss_att 228.528076 loss_ctc 280.312073 loss_rnnt 250.554230 hw_loss 0.194732 lr 0.00011604 rank 0
2023-02-17 23:23:24,247 DEBUG TRAIN Batch 0/5800 loss 263.468018 loss_att 239.054047 loss_ctc 289.604797 loss_rnnt 264.835266 hw_loss 0.057468 lr 0.00011604 rank 5
2023-02-17 23:23:24,248 DEBUG TRAIN Batch 0/5800 loss 295.652588 loss_att 263.599731 loss_ctc 316.609589 loss_rnnt 299.246216 hw_loss 0.042576 lr 0.00011604 rank 6
2023-02-17 23:23:24,249 DEBUG TRAIN Batch 0/5800 loss 244.112228 loss_att 222.717194 loss_ctc 280.811127 loss_rnnt 243.402313 hw_loss 0.179522 lr 0.00011604 rank 7
2023-02-17 23:23:24,250 DEBUG TRAIN Batch 0/5800 loss 264.145172 loss_att 240.950745 loss_ctc 295.792145 loss_rnnt 264.564423 hw_loss 0.000040 lr 0.00011604 rank 3
2023-02-17 23:23:24,251 DEBUG TRAIN Batch 0/5800 loss 247.336807 loss_att 224.555817 loss_ctc 265.517334 loss_rnnt 249.468903 hw_loss 0.000040 lr 0.00011604 rank 2
2023-02-17 23:23:24,254 DEBUG TRAIN Batch 0/5800 loss 293.417725 loss_att 262.409302 loss_ctc 324.651825 loss_rnnt 295.387085 hw_loss 0.127087 lr 0.00011604 rank 4
2023-02-17 23:23:24,254 DEBUG TRAIN Batch 0/5800 loss 334.443054 loss_att 303.213074 loss_ctc 381.165009 loss_rnnt 334.459442 hw_loss 0.000040 lr 0.00011604 rank 1
2023-02-17 23:24:25,686 DEBUG TRAIN Batch 0/5900 loss 268.749084 loss_att 247.964020 loss_ctc 291.303040 loss_rnnt 269.824097 hw_loss 0.140233 lr 0.00011804 rank 1
2023-02-17 23:24:25,693 DEBUG TRAIN Batch 0/5900 loss 303.696686 loss_att 277.714722 loss_ctc 345.908203 loss_rnnt 303.216248 hw_loss 0.091160 lr 0.00011804 rank 7
2023-02-17 23:24:25,693 DEBUG TRAIN Batch 0/5900 loss 247.158936 loss_att 223.522583 loss_ctc 268.044312 loss_rnnt 249.049881 hw_loss 0.096752 lr 0.00011804 rank 5
2023-02-17 23:24:25,693 DEBUG TRAIN Batch 0/5900 loss 258.366364 loss_att 234.447906 loss_ctc 299.977325 loss_rnnt 257.516113 hw_loss 0.160885 lr 0.00011804 rank 3
2023-02-17 23:24:25,696 DEBUG TRAIN Batch 0/5900 loss 313.467346 loss_att 282.507690 loss_ctc 349.679688 loss_rnnt 314.715912 hw_loss 0.215725 lr 0.00011804 rank 2
2023-02-17 23:24:25,698 DEBUG TRAIN Batch 0/5900 loss 286.084564 loss_att 259.481323 loss_ctc 326.237579 loss_rnnt 285.961578 hw_loss 0.168557 lr 0.00011804 rank 6
2023-02-17 23:24:25,698 DEBUG TRAIN Batch 0/5900 loss 174.094971 loss_att 153.877823 loss_ctc 191.170578 loss_rnnt 175.756836 hw_loss 0.196488 lr 0.00011804 rank 4
2023-02-17 23:24:25,698 DEBUG TRAIN Batch 0/5900 loss 343.042908 loss_att 312.114380 loss_ctc 376.571136 loss_rnnt 344.651215 hw_loss 0.200612 lr 0.00011804 rank 0
2023-02-17 23:25:47,895 DEBUG TRAIN Batch 0/6000 loss 83.678383 loss_att 77.223488 loss_ctc 92.274475 loss_rnnt 83.729813 hw_loss 0.175118 lr 0.00012004 rank 5
2023-02-17 23:25:47,900 DEBUG TRAIN Batch 0/6000 loss 262.848175 loss_att 239.810791 loss_ctc 282.492798 loss_rnnt 264.836304 hw_loss 0.000061 lr 0.00012004 rank 2
2023-02-17 23:25:47,902 DEBUG TRAIN Batch 0/6000 loss 434.067932 loss_att 385.097107 loss_ctc 458.554535 loss_rnnt 440.597198 hw_loss 0.000061 lr 0.00012004 rank 1
2023-02-17 23:25:47,902 DEBUG TRAIN Batch 0/6000 loss 288.458984 loss_att 261.612610 loss_ctc 314.520996 loss_rnnt 290.218140 hw_loss 0.253438 lr 0.00012004 rank 3
2023-02-17 23:25:47,905 DEBUG TRAIN Batch 0/6000 loss 282.774445 loss_att 259.269745 loss_ctc 304.350586 loss_rnnt 284.598511 hw_loss 0.000061 lr 0.00012004 rank 7
2023-02-17 23:25:47,906 DEBUG TRAIN Batch 0/6000 loss 285.076263 loss_att 255.181793 loss_ctc 333.511658 loss_rnnt 284.494446 hw_loss 0.192483 lr 0.00012004 rank 6
2023-02-17 23:25:47,935 DEBUG TRAIN Batch 0/6000 loss 356.575775 loss_att 315.889160 loss_ctc 392.833771 loss_rnnt 359.878662 hw_loss 0.000061 lr 0.00012004 rank 4
2023-02-17 23:25:47,975 DEBUG TRAIN Batch 0/6000 loss 318.661438 loss_att 290.606537 loss_ctc 338.566498 loss_rnnt 321.572906 hw_loss 0.085293 lr 0.00012004 rank 0
2023-02-17 23:26:47,253 DEBUG TRAIN Batch 0/6100 loss 251.752228 loss_att 223.600983 loss_ctc 281.831726 loss_rnnt 253.318695 hw_loss 0.099772 lr 0.00012204 rank 3
2023-02-17 23:26:47,254 DEBUG TRAIN Batch 0/6100 loss 272.687744 loss_att 245.837646 loss_ctc 302.925842 loss_rnnt 273.886993 hw_loss 0.260619 lr 0.00012204 rank 2
2023-02-17 23:26:47,255 DEBUG TRAIN Batch 0/6100 loss 209.378326 loss_att 189.440857 loss_ctc 224.137756 loss_rnnt 211.225494 hw_loss 0.323267 lr 0.00012204 rank 6
2023-02-17 23:26:47,255 DEBUG TRAIN Batch 0/6100 loss 218.759720 loss_att 196.776276 loss_ctc 241.905914 loss_rnnt 220.036072 hw_loss 0.064047 lr 0.00012204 rank 5
2023-02-17 23:26:47,259 DEBUG TRAIN Batch 0/6100 loss 247.210922 loss_att 233.003723 loss_ctc 277.093262 loss_rnnt 246.018173 hw_loss 0.093518 lr 0.00012204 rank 0
2023-02-17 23:26:47,260 DEBUG TRAIN Batch 0/6100 loss 285.327881 loss_att 257.198486 loss_ctc 307.766113 loss_rnnt 287.808777 hw_loss 0.287355 lr 0.00012204 rank 4
2023-02-17 23:26:47,269 DEBUG TRAIN Batch 0/6100 loss 235.910828 loss_att 213.922318 loss_ctc 279.157135 loss_rnnt 234.452728 hw_loss 0.168083 lr 0.00012204 rank 1
2023-02-17 23:26:47,322 DEBUG TRAIN Batch 0/6100 loss 287.856812 loss_att 262.833893 loss_ctc 308.920441 loss_rnnt 290.023132 hw_loss 0.055848 lr 0.00012204 rank 7
2023-02-17 23:28:09,036 DEBUG TRAIN Batch 0/6200 loss 220.514343 loss_att 203.841492 loss_ctc 252.055923 loss_rnnt 219.643311 hw_loss 0.000064 lr 0.00012404 rank 5
2023-02-17 23:28:09,042 DEBUG TRAIN Batch 0/6200 loss 242.227020 loss_att 223.549072 loss_ctc 268.360291 loss_rnnt 242.478134 hw_loss 0.000064 lr 0.00012404 rank 4
2023-02-17 23:28:09,043 DEBUG TRAIN Batch 0/6200 loss 210.104324 loss_att 195.611450 loss_ctc 234.297516 loss_rnnt 209.659760 hw_loss 0.220104 lr 0.00012404 rank 1
2023-02-17 23:28:09,045 DEBUG TRAIN Batch 0/6200 loss 277.000458 loss_att 257.395630 loss_ctc 299.979980 loss_rnnt 277.857452 hw_loss 0.000064 lr 0.00012404 rank 3
2023-02-17 23:28:09,045 DEBUG TRAIN Batch 0/6200 loss 301.703674 loss_att 284.786407 loss_ctc 337.702179 loss_rnnt 300.287292 hw_loss 0.000064 lr 0.00012404 rank 0
2023-02-17 23:28:09,045 DEBUG TRAIN Batch 0/6200 loss 240.030914 loss_att 227.527161 loss_ctc 262.787506 loss_rnnt 239.497421 hw_loss 0.000064 lr 0.00012404 rank 7
2023-02-17 23:28:09,047 DEBUG TRAIN Batch 0/6200 loss 269.657501 loss_att 243.534058 loss_ctc 281.047974 loss_rnnt 273.363403 hw_loss 0.000064 lr 0.00012404 rank 2
2023-02-17 23:28:09,101 DEBUG TRAIN Batch 0/6200 loss 370.944855 loss_att 332.384277 loss_ctc 400.497681 loss_rnnt 374.716553 hw_loss 0.000064 lr 0.00012404 rank 6
2023-02-17 23:29:08,473 DEBUG TRAIN Batch 0/6300 loss 53.829922 loss_att 50.230812 loss_ctc 57.307335 loss_rnnt 53.986084 hw_loss 0.187513 lr 0.00012604 rank 5
2023-02-17 23:29:08,478 DEBUG TRAIN Batch 0/6300 loss 347.232422 loss_att 322.613525 loss_ctc 378.313354 loss_rnnt 348.012054 hw_loss 0.000044 lr 0.00012604 rank 3
2023-02-17 23:29:08,480 DEBUG TRAIN Batch 0/6300 loss 311.724457 loss_att 285.929382 loss_ctc 349.109314 loss_rnnt 311.824921 hw_loss 0.138650 lr 0.00012604 rank 0
2023-02-17 23:29:08,482 DEBUG TRAIN Batch 0/6300 loss 201.974640 loss_att 191.410614 loss_ctc 228.250168 loss_rnnt 200.525696 hw_loss 0.109403 lr 0.00012604 rank 6
2023-02-17 23:29:08,482 DEBUG TRAIN Batch 0/6300 loss 330.976349 loss_att 299.412109 loss_ctc 350.914825 loss_rnnt 334.583405 hw_loss 0.088717 lr 0.00012604 rank 1
2023-02-17 23:29:08,485 DEBUG TRAIN Batch 0/6300 loss 251.229996 loss_att 234.970551 loss_ctc 282.962891 loss_rnnt 250.250793 hw_loss 0.000044 lr 0.00012604 rank 7
2023-02-17 23:29:08,525 DEBUG TRAIN Batch 0/6300 loss 304.983490 loss_att 276.481964 loss_ctc 335.056732 loss_rnnt 306.673981 hw_loss 0.000044 lr 0.00012604 rank 2
2023-02-17 23:29:09,021 DEBUG TRAIN Batch 0/6300 loss 136.700729 loss_att 123.831161 loss_ctc 152.914795 loss_rnnt 137.025787 hw_loss 0.163094 lr 0.00012604 rank 4
2023-02-17 23:30:09,990 DEBUG TRAIN Batch 0/6400 loss 244.854126 loss_att 226.245911 loss_ctc 270.186615 loss_rnnt 245.179749 hw_loss 0.034422 lr 0.00012804 rank 5
2023-02-17 23:30:09,994 DEBUG TRAIN Batch 0/6400 loss 263.373138 loss_att 239.025284 loss_ctc 290.011292 loss_rnnt 264.629333 hw_loss 0.115502 lr 0.00012804 rank 0
2023-02-17 23:30:09,995 DEBUG TRAIN Batch 0/6400 loss 70.070229 loss_att 65.259270 loss_ctc 75.857155 loss_rnnt 70.145065 hw_loss 0.217053 lr 0.00012804 rank 1
2023-02-17 23:30:09,995 DEBUG TRAIN Batch 0/6400 loss 273.404572 loss_att 255.372696 loss_ctc 306.141663 loss_rnnt 272.610535 hw_loss 0.066532 lr 0.00012804 rank 2
2023-02-17 23:30:09,999 DEBUG TRAIN Batch 0/6400 loss 266.656494 loss_att 248.923508 loss_ctc 291.354279 loss_rnnt 266.861084 hw_loss 0.091837 lr 0.00012804 rank 6
2023-02-17 23:30:09,999 DEBUG TRAIN Batch 0/6400 loss 300.954803 loss_att 277.198425 loss_ctc 318.684082 loss_rnnt 303.285706 hw_loss 0.105814 lr 0.00012804 rank 4
2023-02-17 23:30:10,001 DEBUG TRAIN Batch 0/6400 loss 323.030731 loss_att 297.870422 loss_ctc 357.633362 loss_rnnt 323.390381 hw_loss 0.110081 lr 0.00012804 rank 3
2023-02-17 23:30:10,002 DEBUG TRAIN Batch 0/6400 loss 215.587418 loss_att 201.581436 loss_ctc 234.263000 loss_rnnt 215.847336 hw_loss 0.096020 lr 0.00012804 rank 7
2023-02-17 23:31:10,405 DEBUG TRAIN Batch 0/6500 loss 268.427216 loss_att 243.247330 loss_ctc 295.321350 loss_rnnt 269.818573 hw_loss 0.110135 lr 0.00013004 rank 1
2023-02-17 23:31:10,405 DEBUG TRAIN Batch 0/6500 loss 231.645508 loss_att 212.377213 loss_ctc 253.345627 loss_rnnt 232.484680 hw_loss 0.227138 lr 0.00013004 rank 0
2023-02-17 23:31:10,407 DEBUG TRAIN Batch 0/6500 loss 319.906677 loss_att 287.378448 loss_ctc 352.256012 loss_rnnt 322.098999 hw_loss 0.000084 lr 0.00013004 rank 6
2023-02-17 23:31:10,408 DEBUG TRAIN Batch 0/6500 loss 291.755188 loss_att 267.058258 loss_ctc 307.954407 loss_rnnt 294.412262 hw_loss 0.229478 lr 0.00013004 rank 7
2023-02-17 23:31:10,410 DEBUG TRAIN Batch 0/6500 loss 208.151184 loss_att 193.121643 loss_ctc 233.567764 loss_rnnt 207.768173 hw_loss 0.000084 lr 0.00013004 rank 5
2023-02-17 23:31:10,411 DEBUG TRAIN Batch 0/6500 loss 272.290924 loss_att 249.973175 loss_ctc 291.515839 loss_rnnt 274.159668 hw_loss 0.059022 lr 0.00013004 rank 4
2023-02-17 23:31:10,415 DEBUG TRAIN Batch 0/6500 loss 282.696106 loss_att 252.871918 loss_ctc 309.065338 loss_rnnt 284.967987 hw_loss 0.331979 lr 0.00013004 rank 3
2023-02-17 23:31:10,418 DEBUG TRAIN Batch 0/6500 loss 255.681259 loss_att 244.232452 loss_ctc 288.223816 loss_rnnt 253.631958 hw_loss 0.000084 lr 0.00013004 rank 2
2023-02-17 23:32:09,382 DEBUG TRAIN Batch 0/6600 loss 282.725037 loss_att 261.529480 loss_ctc 290.785889 loss_rnnt 285.766876 hw_loss 0.229630 lr 0.00013204 rank 1
2023-02-17 23:32:09,387 DEBUG TRAIN Batch 0/6600 loss 246.765533 loss_att 228.378662 loss_ctc 258.278564 loss_rnnt 248.872467 hw_loss 0.066302 lr 0.00013204 rank 0
2023-02-17 23:32:09,388 DEBUG TRAIN Batch 0/6600 loss 189.402618 loss_att 177.277115 loss_ctc 204.263489 loss_rnnt 189.807190 hw_loss 0.073295 lr 0.00013204 rank 4
2023-02-17 23:32:09,388 DEBUG TRAIN Batch 0/6600 loss 256.461914 loss_att 231.436401 loss_ctc 285.855316 loss_rnnt 257.410278 hw_loss 0.257993 lr 0.00013204 rank 7
2023-02-17 23:32:09,388 DEBUG TRAIN Batch 0/6600 loss 343.475983 loss_att 311.152130 loss_ctc 360.376984 loss_rnnt 347.624176 hw_loss 0.118357 lr 0.00013204 rank 6
2023-02-17 23:32:09,394 DEBUG TRAIN Batch 0/6600 loss 94.468552 loss_att 87.312073 loss_ctc 107.587669 loss_rnnt 94.071854 hw_loss 0.147706 lr 0.00013204 rank 5
2023-02-17 23:32:09,394 DEBUG TRAIN Batch 0/6600 loss 59.591721 loss_att 56.429199 loss_ctc 62.463875 loss_rnnt 59.710564 hw_loss 0.245076 lr 0.00013204 rank 2
2023-02-17 23:32:09,397 DEBUG TRAIN Batch 0/6600 loss 40.702255 loss_att 39.724224 loss_ctc 42.799152 loss_rnnt 40.414032 hw_loss 0.382959 lr 0.00013204 rank 3
2023-02-17 23:33:07,735 DEBUG TRAIN Batch 0/6700 loss 254.657501 loss_att 236.431671 loss_ctc 264.566956 loss_rnnt 256.981384 hw_loss 0.000051 lr 0.00013404 rank 7
2023-02-17 23:33:07,735 DEBUG TRAIN Batch 0/6700 loss 291.390411 loss_att 265.455048 loss_ctc 315.217743 loss_rnnt 293.400452 hw_loss 0.000051 lr 0.00013404 rank 0
2023-02-17 23:33:07,738 DEBUG TRAIN Batch 0/6700 loss 311.653320 loss_att 289.432434 loss_ctc 332.572449 loss_rnnt 313.259521 hw_loss 0.091508 lr 0.00013404 rank 3
2023-02-17 23:33:07,739 DEBUG TRAIN Batch 0/6700 loss 222.514664 loss_att 210.557495 loss_ctc 246.153107 loss_rnnt 221.726288 hw_loss 0.052543 lr 0.00013404 rank 2
2023-02-17 23:33:07,742 DEBUG TRAIN Batch 0/6700 loss 276.931152 loss_att 256.365417 loss_ctc 301.111389 loss_rnnt 277.778473 hw_loss 0.078374 lr 0.00013404 rank 5
2023-02-17 23:33:07,742 DEBUG TRAIN Batch 0/6700 loss 249.154648 loss_att 239.952850 loss_ctc 275.702362 loss_rnnt 247.455276 hw_loss 0.000051 lr 0.00013404 rank 6
2023-02-17 23:33:07,746 DEBUG TRAIN Batch 0/6700 loss 296.657166 loss_att 278.119598 loss_ctc 316.780090 loss_rnnt 297.681580 hw_loss 0.000051 lr 0.00013404 rank 4
2023-02-17 23:33:07,747 DEBUG TRAIN Batch 0/6700 loss 103.204849 loss_att 97.273491 loss_ctc 106.895950 loss_rnnt 103.803368 hw_loss 0.179259 lr 0.00013404 rank 1
2023-02-17 23:34:07,490 DEBUG TRAIN Batch 0/6800 loss 352.789948 loss_att 330.957825 loss_ctc 380.382782 loss_rnnt 353.477295 hw_loss 0.000087 lr 0.00013604 rank 1
2023-02-17 23:34:07,492 DEBUG TRAIN Batch 0/6800 loss 354.352661 loss_att 327.070892 loss_ctc 385.338806 loss_rnnt 355.606415 hw_loss 0.133360 lr 0.00013604 rank 5
2023-02-17 23:34:07,492 DEBUG TRAIN Batch 0/6800 loss 296.561249 loss_att 282.176392 loss_ctc 315.774170 loss_rnnt 296.802826 hw_loss 0.138183 lr 0.00013604 rank 6
2023-02-17 23:34:07,493 DEBUG TRAIN Batch 0/6800 loss 246.151810 loss_att 234.624954 loss_ctc 259.835022 loss_rnnt 246.632706 hw_loss 0.000087 lr 0.00013604 rank 3
2023-02-17 23:34:07,493 DEBUG TRAIN Batch 0/6800 loss 208.692398 loss_att 196.125809 loss_ctc 220.982162 loss_rnnt 209.508362 hw_loss 0.110094 lr 0.00013604 rank 0
2023-02-17 23:34:07,494 DEBUG TRAIN Batch 0/6800 loss 291.390137 loss_att 284.234863 loss_ctc 311.817383 loss_rnnt 290.063019 hw_loss 0.064806 lr 0.00013604 rank 2
2023-02-17 23:34:07,497 DEBUG TRAIN Batch 0/6800 loss 297.045044 loss_att 279.875336 loss_ctc 326.574158 loss_rnnt 296.541748 hw_loss 0.000087 lr 0.00013604 rank 7
2023-02-17 23:34:07,525 DEBUG TRAIN Batch 0/6800 loss 375.247345 loss_att 343.482910 loss_ctc 416.195648 loss_rnnt 376.006134 hw_loss 0.251817 lr 0.00013604 rank 4
2023-02-17 23:35:28,294 DEBUG TRAIN Batch 0/6900 loss 220.186127 loss_att 218.268860 loss_ctc 238.256805 loss_rnnt 218.160126 hw_loss 0.000072 lr 0.00013804 rank 4
2023-02-17 23:35:28,293 DEBUG TRAIN Batch 0/6900 loss 166.190689 loss_att 157.002472 loss_ctc 179.368134 loss_rnnt 166.187027 hw_loss 0.158075 lr 0.00013804 rank 7
2023-02-17 23:35:28,293 DEBUG TRAIN Batch 0/6900 loss 255.543427 loss_att 242.550476 loss_ctc 274.478912 loss_rnnt 255.517395 hw_loss 0.187292 lr 0.00013804 rank 6
2023-02-17 23:35:28,294 DEBUG TRAIN Batch 0/6900 loss 147.334885 loss_att 139.087875 loss_ctc 155.507858 loss_rnnt 147.840744 hw_loss 0.100903 lr 0.00013804 rank 5
2023-02-17 23:35:28,294 DEBUG TRAIN Batch 0/6900 loss 83.872467 loss_att 80.060760 loss_ctc 91.552101 loss_rnnt 83.518303 hw_loss 0.173531 lr 0.00013804 rank 2
2023-02-17 23:35:28,301 DEBUG TRAIN Batch 0/6900 loss 166.680695 loss_att 155.094131 loss_ctc 182.558029 loss_rnnt 166.822723 hw_loss 0.109315 lr 0.00013804 rank 3
2023-02-17 23:35:28,307 DEBUG TRAIN Batch 0/6900 loss 324.344940 loss_att 305.304718 loss_ctc 351.197968 loss_rnnt 324.572510 hw_loss 0.000072 lr 0.00013804 rank 0
2023-02-17 23:35:28,312 DEBUG TRAIN Batch 0/6900 loss 310.745880 loss_att 298.514496 loss_ctc 337.659576 loss_rnnt 309.558502 hw_loss 0.084613 lr 0.00013804 rank 1
2023-02-17 23:36:44,051 DEBUG TRAIN Batch 0/7000 loss 242.469620 loss_att 228.122650 loss_ctc 263.484741 loss_rnnt 242.489380 hw_loss 0.089283 lr 0.00014004 rank 5
2023-02-17 23:36:44,055 DEBUG TRAIN Batch 0/7000 loss 270.139404 loss_att 257.748505 loss_ctc 281.082214 loss_rnnt 271.058014 hw_loss 0.188436 lr 0.00014004 rank 3
2023-02-17 23:36:44,056 DEBUG TRAIN Batch 0/7000 loss 297.749542 loss_att 292.138489 loss_ctc 319.539032 loss_rnnt 295.966431 hw_loss 0.000059 lr 0.00014004 rank 4
2023-02-17 23:36:44,057 DEBUG TRAIN Batch 0/7000 loss 249.789642 loss_att 231.334839 loss_ctc 266.417511 loss_rnnt 251.192932 hw_loss 0.132390 lr 0.00014004 rank 7
2023-02-17 23:36:44,062 DEBUG TRAIN Batch 0/7000 loss 225.394043 loss_att 214.788177 loss_ctc 235.531769 loss_rnnt 226.067764 hw_loss 0.179537 lr 0.00014004 rank 0
2023-02-17 23:36:44,065 DEBUG TRAIN Batch 0/7000 loss 290.195312 loss_att 275.829468 loss_ctc 321.622009 loss_rnnt 288.840393 hw_loss 0.070956 lr 0.00014004 rank 6
2023-02-17 23:36:44,067 DEBUG TRAIN Batch 0/7000 loss 185.620956 loss_att 178.389084 loss_ctc 203.087830 loss_rnnt 184.587082 hw_loss 0.283760 lr 0.00014004 rank 1
2023-02-17 23:36:44,070 DEBUG TRAIN Batch 0/7000 loss 309.686554 loss_att 283.074951 loss_ctc 328.361206 loss_rnnt 312.518860 hw_loss 0.000059 lr 0.00014004 rank 2
2023-02-17 23:37:44,526 DEBUG TRAIN Batch 0/7100 loss 311.422791 loss_att 288.586426 loss_ctc 336.758118 loss_rnnt 312.571594 hw_loss 0.075824 lr 0.00014204 rank 1
2023-02-17 23:37:44,528 DEBUG TRAIN Batch 0/7100 loss 237.283752 loss_att 234.050293 loss_ctc 249.320007 loss_rnnt 236.229187 hw_loss 0.180784 lr 0.00014204 rank 6
2023-02-17 23:37:44,532 DEBUG TRAIN Batch 0/7100 loss 311.541595 loss_att 304.212769 loss_ctc 311.422852 loss_rnnt 312.936432 hw_loss 0.162686 lr 0.00014204 rank 4
2023-02-17 23:37:44,532 DEBUG TRAIN Batch 0/7100 loss 281.799286 loss_att 275.421143 loss_ctc 304.758820 loss_rnnt 279.935944 hw_loss 0.145634 lr 0.00014204 rank 0
2023-02-17 23:37:44,534 DEBUG TRAIN Batch 0/7100 loss 307.818420 loss_att 298.501129 loss_ctc 318.213043 loss_rnnt 308.071167 hw_loss 0.421390 lr 0.00014204 rank 5
2023-02-17 23:37:44,534 DEBUG TRAIN Batch 0/7100 loss 216.021790 loss_att 219.655228 loss_ctc 231.631882 loss_rnnt 213.213730 hw_loss 0.000044 lr 0.00014204 rank 3
2023-02-17 23:37:44,535 DEBUG TRAIN Batch 0/7100 loss 278.903839 loss_att 286.446869 loss_ctc 290.255829 loss_rnnt 275.803314 hw_loss 0.146888 lr 0.00014204 rank 2
2023-02-17 23:37:44,536 DEBUG TRAIN Batch 0/7100 loss 338.006012 loss_att 316.132294 loss_ctc 359.104218 loss_rnnt 339.451294 hw_loss 0.218147 lr 0.00014204 rank 7
2023-02-17 23:38:42,890 DEBUG TRAIN Batch 0/7200 loss 216.246155 loss_att 199.737289 loss_ctc 239.885437 loss_rnnt 216.271790 hw_loss 0.232918 lr 0.00014404 rank 5
2023-02-17 23:38:42,891 DEBUG TRAIN Batch 0/7200 loss 147.341507 loss_att 141.547089 loss_ctc 153.024750 loss_rnnt 147.538666 hw_loss 0.382435 lr 0.00014404 rank 2
2023-02-17 23:38:42,894 DEBUG TRAIN Batch 0/7200 loss 224.557083 loss_att 219.220459 loss_ctc 232.199417 loss_rnnt 224.573715 hw_loss 0.059478 lr 0.00014404 rank 3
2023-02-17 23:38:42,901 DEBUG TRAIN Batch 0/7200 loss 124.334885 loss_att 116.318039 loss_ctc 134.130890 loss_rnnt 124.487076 hw_loss 0.271956 lr 0.00014404 rank 0
2023-02-17 23:38:42,901 DEBUG TRAIN Batch 0/7200 loss 287.582153 loss_att 291.575439 loss_ctc 294.221222 loss_rnnt 285.898224 hw_loss 0.000045 lr 0.00014404 rank 1
2023-02-17 23:38:42,902 DEBUG TRAIN Batch 0/7200 loss 58.264492 loss_att 56.470993 loss_ctc 61.827045 loss_rnnt 57.978802 hw_loss 0.317590 lr 0.00014404 rank 6
2023-02-17 23:38:42,902 DEBUG TRAIN Batch 0/7200 loss 208.094162 loss_att 205.652115 loss_ctc 213.532684 loss_rnnt 207.779984 hw_loss 0.145203 lr 0.00014404 rank 4
2023-02-17 23:38:42,910 DEBUG TRAIN Batch 0/7200 loss 172.163132 loss_att 171.121918 loss_ctc 180.715591 loss_rnnt 171.169189 hw_loss 0.115948 lr 0.00014404 rank 7
2023-02-17 23:39:42,616 DEBUG TRAIN Batch 0/7300 loss 191.059052 loss_att 197.382065 loss_ctc 208.115265 loss_rnnt 187.488495 hw_loss 0.059623 lr 0.00014604 rank 3
2023-02-17 23:39:42,617 DEBUG TRAIN Batch 0/7300 loss 344.798309 loss_att 348.632446 loss_ctc 365.757690 loss_rnnt 341.236877 hw_loss 0.000082 lr 0.00014604 rank 5
2023-02-17 23:39:42,617 DEBUG TRAIN Batch 0/7300 loss 216.630783 loss_att 207.969345 loss_ctc 232.117828 loss_rnnt 216.221191 hw_loss 0.144277 lr 0.00014604 rank 1
2023-02-17 23:39:42,619 DEBUG TRAIN Batch 0/7300 loss 258.828278 loss_att 252.028992 loss_ctc 264.872528 loss_rnnt 259.325806 hw_loss 0.105808 lr 0.00014604 rank 7
2023-02-17 23:39:42,621 DEBUG TRAIN Batch 0/7300 loss 201.157684 loss_att 204.582901 loss_ctc 212.455292 loss_rnnt 198.929749 hw_loss 0.068495 lr 0.00014604 rank 6
2023-02-17 23:39:42,662 DEBUG TRAIN Batch 0/7300 loss 225.201630 loss_att 226.560669 loss_ctc 226.995865 loss_rnnt 224.578217 hw_loss 0.210739 lr 0.00014604 rank 4
2023-02-17 23:39:42,669 DEBUG TRAIN Batch 0/7300 loss 284.839844 loss_att 273.972870 loss_ctc 307.628845 loss_rnnt 283.905518 hw_loss 0.129670 lr 0.00014604 rank 2
2023-02-17 23:39:42,677 DEBUG TRAIN Batch 0/7300 loss 260.987915 loss_att 257.724945 loss_ctc 273.726440 loss_rnnt 259.845703 hw_loss 0.180607 lr 0.00014604 rank 0
2023-02-17 23:40:43,753 DEBUG TRAIN Batch 0/7400 loss 270.150513 loss_att 271.247681 loss_ctc 290.834351 loss_rnnt 267.173187 hw_loss 0.000063 lr 0.00014804 rank 6
2023-02-17 23:40:43,755 DEBUG TRAIN Batch 0/7400 loss 258.914124 loss_att 263.446472 loss_ctc 274.053619 loss_rnnt 255.884598 hw_loss 0.195853 lr 0.00014804 rank 5
2023-02-17 23:40:43,757 DEBUG TRAIN Batch 0/7400 loss 247.090744 loss_att 244.525604 loss_ctc 240.357544 loss_rnnt 248.362244 hw_loss 0.261193 lr 0.00014804 rank 0
2023-02-17 23:40:43,758 DEBUG TRAIN Batch 0/7400 loss 273.945984 loss_att 275.914612 loss_ctc 281.152863 loss_rnnt 272.535278 hw_loss 0.105135 lr 0.00014804 rank 2
2023-02-17 23:40:43,762 DEBUG TRAIN Batch 0/7400 loss 306.959106 loss_att 301.555786 loss_ctc 316.873138 loss_rnnt 306.717865 hw_loss 0.000063 lr 0.00014804 rank 4
2023-02-17 23:40:43,763 DEBUG TRAIN Batch 0/7400 loss 253.069351 loss_att 245.972809 loss_ctc 262.986816 loss_rnnt 253.101257 hw_loss 0.122034 lr 0.00014804 rank 1
2023-02-17 23:40:43,763 DEBUG TRAIN Batch 0/7400 loss 222.734283 loss_att 245.256256 loss_ctc 227.613693 loss_rnnt 217.579254 hw_loss 0.000063 lr 0.00014804 rank 3
2023-02-17 23:40:43,820 DEBUG TRAIN Batch 0/7400 loss 184.009918 loss_att 192.111420 loss_ctc 193.972366 loss_rnnt 181.002487 hw_loss 0.110264 lr 0.00014804 rank 7
2023-02-17 23:41:41,786 DEBUG TRAIN Batch 0/7500 loss 202.462814 loss_att 208.896713 loss_ctc 198.717163 loss_rnnt 201.675385 hw_loss 0.000154 lr 0.00015004 rank 5
2023-02-17 23:41:41,787 DEBUG TRAIN Batch 0/7500 loss 232.447769 loss_att 237.748001 loss_ctc 237.594543 loss_rnnt 230.701401 hw_loss 0.000154 lr 0.00015004 rank 4
2023-02-17 23:41:41,789 DEBUG TRAIN Batch 0/7500 loss 209.729614 loss_att 210.415710 loss_ctc 221.137726 loss_rnnt 208.008118 hw_loss 0.118498 lr 0.00015004 rank 3
2023-02-17 23:41:41,789 DEBUG TRAIN Batch 0/7500 loss 198.351807 loss_att 203.212769 loss_ctc 200.666443 loss_rnnt 197.018509 hw_loss 0.098376 lr 0.00015004 rank 7
2023-02-17 23:41:41,789 DEBUG TRAIN Batch 0/7500 loss 314.280762 loss_att 306.599487 loss_ctc 332.598877 loss_rnnt 313.332458 hw_loss 0.079007 lr 0.00015004 rank 1
2023-02-17 23:41:41,791 DEBUG TRAIN Batch 0/7500 loss 115.080643 loss_att 115.853592 loss_ctc 122.387085 loss_rnnt 113.933853 hw_loss 0.033769 lr 0.00015004 rank 0
2023-02-17 23:41:41,791 DEBUG TRAIN Batch 0/7500 loss 137.044876 loss_att 134.602631 loss_ctc 144.227203 loss_rnnt 136.493530 hw_loss 0.154023 lr 0.00015004 rank 6
2023-02-17 23:41:41,847 DEBUG TRAIN Batch 0/7500 loss 154.602142 loss_att 153.841476 loss_ctc 160.722260 loss_rnnt 153.899139 hw_loss 0.073375 lr 0.00015004 rank 2
2023-02-17 23:42:41,693 DEBUG TRAIN Batch 0/7600 loss 303.022430 loss_att 301.436401 loss_ctc 316.925049 loss_rnnt 301.435028 hw_loss 0.095535 lr 0.00015204 rank 7
2023-02-17 23:42:41,697 DEBUG TRAIN Batch 0/7600 loss 275.571136 loss_att 289.072876 loss_ctc 270.789154 loss_rnnt 273.508362 hw_loss 0.000077 lr 0.00015204 rank 0
2023-02-17 23:42:41,698 DEBUG TRAIN Batch 0/7600 loss 348.816589 loss_att 333.016968 loss_ctc 356.212555 loss_rnnt 350.990356 hw_loss 0.000077 lr 0.00015204 rank 4
2023-02-17 23:42:41,698 DEBUG TRAIN Batch 0/7600 loss 243.356766 loss_att 244.689941 loss_ctc 246.283478 loss_rnnt 242.699875 hw_loss 0.000077 lr 0.00015204 rank 3
2023-02-17 23:42:41,699 DEBUG TRAIN Batch 0/7600 loss 213.847137 loss_att 218.420044 loss_ctc 214.927032 loss_rnnt 212.788513 hw_loss 0.000077 lr 0.00015204 rank 1
2023-02-17 23:42:41,701 DEBUG TRAIN Batch 0/7600 loss 312.107880 loss_att 304.496521 loss_ctc 331.792786 loss_rnnt 310.931763 hw_loss 0.138226 lr 0.00015204 rank 6
2023-02-17 23:42:41,738 DEBUG TRAIN Batch 0/7600 loss 251.513687 loss_att 255.610001 loss_ctc 253.066010 loss_rnnt 250.487411 hw_loss 0.000077 lr 0.00015204 rank 5
2023-02-17 23:42:41,752 DEBUG TRAIN Batch 0/7600 loss 205.076630 loss_att 227.138229 loss_ctc 212.296234 loss_rnnt 199.653015 hw_loss 0.091236 lr 0.00015204 rank 2
2023-02-17 23:43:43,482 DEBUG TRAIN Batch 0/7700 loss 243.048187 loss_att 259.414551 loss_ctc 241.652359 loss_rnnt 239.913544 hw_loss 0.089022 lr 0.00015404 rank 0
2023-02-17 23:43:43,488 DEBUG TRAIN Batch 0/7700 loss 249.285965 loss_att 247.839966 loss_ctc 268.610931 loss_rnnt 246.998444 hw_loss 0.000074 lr 0.00015404 rank 5
2023-02-17 23:43:43,489 DEBUG TRAIN Batch 0/7700 loss 224.916733 loss_att 251.382492 loss_ctc 236.855759 loss_rnnt 217.970291 hw_loss 0.115151 lr 0.00015404 rank 3
2023-02-17 23:43:43,492 DEBUG TRAIN Batch 0/7700 loss 228.879288 loss_att 240.246399 loss_ctc 238.689026 loss_rnnt 225.297867 hw_loss 0.000074 lr 0.00015404 rank 2
2023-02-17 23:43:43,493 DEBUG TRAIN Batch 0/7700 loss 196.537415 loss_att 195.324661 loss_ctc 199.843140 loss_rnnt 196.339157 hw_loss 0.000074 lr 0.00015404 rank 6
2023-02-17 23:43:43,494 DEBUG TRAIN Batch 0/7700 loss 217.771637 loss_att 234.158722 loss_ctc 216.009735 loss_rnnt 214.729111 hw_loss 0.000074 lr 0.00015404 rank 7
2023-02-17 23:43:43,523 DEBUG TRAIN Batch 0/7700 loss 335.380402 loss_att 317.278809 loss_ctc 354.539978 loss_rnnt 336.446045 hw_loss 0.000074 lr 0.00015404 rank 4
2023-02-17 23:43:43,531 DEBUG TRAIN Batch 0/7700 loss 212.248810 loss_att 220.002930 loss_ctc 220.914215 loss_rnnt 209.479736 hw_loss 0.117871 lr 0.00015404 rank 1
2023-02-17 23:45:01,357 DEBUG TRAIN Batch 0/7800 loss 273.779816 loss_att 282.861755 loss_ctc 281.872681 loss_rnnt 270.884308 hw_loss 0.000040 lr 0.00015604 rank 1
2023-02-17 23:45:01,358 DEBUG TRAIN Batch 0/7800 loss 205.216492 loss_att 221.446625 loss_ctc 208.616257 loss_rnnt 201.465729 hw_loss 0.096395 lr 0.00015604 rank 3
2023-02-17 23:45:01,362 DEBUG TRAIN Batch 0/7800 loss 205.524857 loss_att 207.587326 loss_ctc 202.814255 loss_rnnt 205.430267 hw_loss 0.081580 lr 0.00015604 rank 5
2023-02-17 23:45:01,368 DEBUG TRAIN Batch 0/7800 loss 88.044556 loss_att 94.023705 loss_ctc 89.657257 loss_rnnt 86.555618 hw_loss 0.146384 lr 0.00015604 rank 2
2023-02-17 23:45:01,373 DEBUG TRAIN Batch 0/7800 loss 169.994400 loss_att 169.959351 loss_ctc 182.604309 loss_rnnt 168.262604 hw_loss 0.107775 lr 0.00015604 rank 7
2023-02-17 23:45:01,396 DEBUG TRAIN Batch 0/7800 loss 153.337723 loss_att 154.135284 loss_ctc 156.056442 loss_rnnt 152.798721 hw_loss 0.031853 lr 0.00015604 rank 0
2023-02-17 23:45:01,406 DEBUG TRAIN Batch 0/7800 loss 111.887520 loss_att 118.521133 loss_ctc 117.990593 loss_rnnt 109.660172 hw_loss 0.162900 lr 0.00015604 rank 6
2023-02-17 23:45:01,425 DEBUG TRAIN Batch 0/7800 loss 182.069778 loss_att 190.747238 loss_ctc 192.647110 loss_rnnt 178.748993 hw_loss 0.328102 lr 0.00015604 rank 4
2023-02-17 23:46:16,403 DEBUG TRAIN Batch 0/7900 loss 190.699463 loss_att 213.083344 loss_ctc 197.306519 loss_rnnt 185.341690 hw_loss 0.000058 lr 0.00015804 rank 6
2023-02-17 23:46:16,404 DEBUG TRAIN Batch 0/7900 loss 199.661041 loss_att 211.968140 loss_ctc 205.225952 loss_rnnt 196.344177 hw_loss 0.212690 lr 0.00015804 rank 1
2023-02-17 23:46:16,407 DEBUG TRAIN Batch 0/7900 loss 269.901276 loss_att 283.994995 loss_ctc 271.708191 loss_rnnt 266.841553 hw_loss 0.000058 lr 0.00015804 rank 5
2023-02-17 23:46:16,408 DEBUG TRAIN Batch 0/7900 loss 245.211761 loss_att 280.429321 loss_ctc 247.013062 loss_rnnt 237.928009 hw_loss 0.000058 lr 0.00015804 rank 4
2023-02-17 23:46:16,413 DEBUG TRAIN Batch 0/7900 loss 217.722061 loss_att 221.910904 loss_ctc 220.531250 loss_rnnt 216.422180 hw_loss 0.164135 lr 0.00015804 rank 2
2023-02-17 23:46:16,414 DEBUG TRAIN Batch 0/7900 loss 189.594589 loss_att 206.959778 loss_ctc 188.332764 loss_rnnt 186.240448 hw_loss 0.092491 lr 0.00015804 rank 0
2023-02-17 23:46:16,415 DEBUG TRAIN Batch 0/7900 loss 221.707108 loss_att 245.487701 loss_ctc 218.197479 loss_rnnt 217.391800 hw_loss 0.050837 lr 0.00015804 rank 7
2023-02-17 23:46:16,416 DEBUG TRAIN Batch 0/7900 loss 250.062805 loss_att 262.242462 loss_ctc 259.257416 loss_rnnt 246.333389 hw_loss 0.126636 lr 0.00015804 rank 3
2023-02-17 23:47:15,170 DEBUG TRAIN Batch 0/8000 loss 185.131989 loss_att 206.362335 loss_ctc 176.745514 loss_rnnt 181.893936 hw_loss 0.206617 lr 0.00016004 rank 2
2023-02-17 23:47:15,172 DEBUG TRAIN Batch 0/8000 loss 228.742432 loss_att 268.435974 loss_ctc 224.409851 loss_rnnt 221.381378 hw_loss 0.000045 lr 0.00016004 rank 5
2023-02-17 23:47:15,171 DEBUG TRAIN Batch 0/8000 loss 153.054779 loss_att 171.224426 loss_ctc 148.600967 loss_rnnt 150.014664 hw_loss 0.000045 lr 0.00016004 rank 7
2023-02-17 23:47:15,173 DEBUG TRAIN Batch 0/8000 loss 203.673141 loss_att 224.749939 loss_ctc 200.855789 loss_rnnt 199.763794 hw_loss 0.130584 lr 0.00016004 rank 3
2023-02-17 23:47:15,174 DEBUG TRAIN Batch 0/8000 loss 258.093414 loss_att 286.113861 loss_ctc 257.970642 loss_rnnt 252.447144 hw_loss 0.109777 lr 0.00016004 rank 4
2023-02-17 23:47:15,175 DEBUG TRAIN Batch 0/8000 loss 253.737534 loss_att 299.630981 loss_ctc 241.875900 loss_rnnt 246.140381 hw_loss 0.000045 lr 0.00016004 rank 6
2023-02-17 23:47:15,181 DEBUG TRAIN Batch 0/8000 loss 191.872742 loss_att 203.012680 loss_ctc 212.108597 loss_rnnt 186.872971 hw_loss 0.138114 lr 0.00016004 rank 1
2023-02-17 23:47:15,237 DEBUG TRAIN Batch 0/8000 loss 216.233459 loss_att 239.743439 loss_ctc 221.998260 loss_rnnt 210.707932 hw_loss 0.102912 lr 0.00016004 rank 0
2023-02-17 23:48:12,894 DEBUG TRAIN Batch 0/8100 loss 245.413330 loss_att 279.600830 loss_ctc 242.794586 loss_rnnt 238.869370 hw_loss 0.104309 lr 0.00016204 rank 1
2023-02-17 23:48:12,897 DEBUG TRAIN Batch 0/8100 loss 137.174225 loss_att 156.546936 loss_ctc 134.422028 loss_rnnt 133.627930 hw_loss 0.072597 lr 0.00016204 rank 0
2023-02-17 23:48:12,898 DEBUG TRAIN Batch 0/8100 loss 213.269211 loss_att 228.819931 loss_ctc 213.211838 loss_rnnt 210.141357 hw_loss 0.047554 lr 0.00016204 rank 6
2023-02-17 23:48:12,900 DEBUG TRAIN Batch 0/8100 loss 194.951111 loss_att 212.030365 loss_ctc 205.068039 loss_rnnt 190.066956 hw_loss 0.223864 lr 0.00016204 rank 5
2023-02-17 23:48:12,910 DEBUG TRAIN Batch 0/8100 loss 252.039642 loss_att 260.819366 loss_ctc 251.001770 loss_rnnt 250.298691 hw_loss 0.231365 lr 0.00016204 rank 7
2023-02-17 23:48:12,918 DEBUG TRAIN Batch 0/8100 loss 147.497925 loss_att 162.421997 loss_ctc 150.979340 loss_rnnt 143.987228 hw_loss 0.115686 lr 0.00016204 rank 2
2023-02-17 23:48:12,925 DEBUG TRAIN Batch 0/8100 loss 229.147141 loss_att 237.175385 loss_ctc 227.951233 loss_rnnt 227.631851 hw_loss 0.129534 lr 0.00016204 rank 3
2023-02-17 23:48:12,946 DEBUG TRAIN Batch 0/8100 loss 219.907257 loss_att 241.622375 loss_ctc 225.870789 loss_rnnt 214.742828 hw_loss 0.049196 lr 0.00016204 rank 4
2023-02-17 23:49:14,339 DEBUG TRAIN Batch 0/8200 loss 258.915192 loss_att 284.587219 loss_ctc 266.393951 loss_rnnt 252.716797 hw_loss 0.125275 lr 0.00016404 rank 0
2023-02-17 23:49:14,340 DEBUG TRAIN Batch 0/8200 loss 334.328918 loss_att 348.840851 loss_ctc 351.371490 loss_rnnt 329.057404 hw_loss 0.181432 lr 0.00016404 rank 6
2023-02-17 23:49:14,343 DEBUG TRAIN Batch 0/8200 loss 241.127121 loss_att 284.868561 loss_ctc 245.371490 loss_rnnt 231.705902 hw_loss 0.200670 lr 0.00016404 rank 4
2023-02-17 23:49:14,345 DEBUG TRAIN Batch 0/8200 loss 213.495880 loss_att 240.274261 loss_ctc 215.459656 loss_rnnt 207.878357 hw_loss 0.000046 lr 0.00016404 rank 5
2023-02-17 23:49:14,345 DEBUG TRAIN Batch 0/8200 loss 219.149277 loss_att 250.741302 loss_ctc 228.056503 loss_rnnt 211.643219 hw_loss 0.000046 lr 0.00016404 rank 3
2023-02-17 23:49:14,355 DEBUG TRAIN Batch 0/8200 loss 242.371475 loss_att 266.354065 loss_ctc 236.007355 loss_rnnt 238.328278 hw_loss 0.178532 lr 0.00016404 rank 2
2023-02-17 23:49:14,356 DEBUG TRAIN Batch 0/8200 loss 224.688660 loss_att 240.326630 loss_ctc 229.141037 loss_rnnt 220.967392 hw_loss 0.000046 lr 0.00016404 rank 7
2023-02-17 23:49:14,404 DEBUG TRAIN Batch 0/8200 loss 208.269379 loss_att 218.810730 loss_ctc 215.197174 loss_rnnt 205.141861 hw_loss 0.179141 lr 0.00016404 rank 1
2023-02-17 23:50:13,878 DEBUG TRAIN Batch 0/8300 loss 175.078812 loss_att 219.860275 loss_ctc 165.591217 loss_rnnt 167.339584 hw_loss 0.089901 lr 0.00016604 rank 1
2023-02-17 23:50:13,879 DEBUG TRAIN Batch 0/8300 loss 228.986069 loss_att 255.062408 loss_ctc 212.277374 loss_rnnt 225.811935 hw_loss 0.350069 lr 0.00016604 rank 5
2023-02-17 23:50:13,881 DEBUG TRAIN Batch 0/8300 loss 228.599472 loss_att 273.481689 loss_ctc 239.407486 loss_rnnt 218.146881 hw_loss 0.065728 lr 0.00016604 rank 2
2023-02-17 23:50:13,882 DEBUG TRAIN Batch 0/8300 loss 253.932327 loss_att 284.010101 loss_ctc 260.642822 loss_rnnt 246.952789 hw_loss 0.129796 lr 0.00016604 rank 4
2023-02-17 23:50:13,885 DEBUG TRAIN Batch 0/8300 loss 214.378036 loss_att 251.308533 loss_ctc 214.350067 loss_rnnt 206.938751 hw_loss 0.106713 lr 0.00016604 rank 3
2023-02-17 23:50:13,886 DEBUG TRAIN Batch 0/8300 loss 220.781433 loss_att 249.327103 loss_ctc 213.982010 loss_rnnt 215.971817 hw_loss 0.013251 lr 0.00016604 rank 6
2023-02-17 23:50:13,918 DEBUG TRAIN Batch 0/8300 loss 168.305038 loss_att 196.849197 loss_ctc 168.019852 loss_rnnt 162.507019 hw_loss 0.238540 lr 0.00016604 rank 0
2023-02-17 23:50:13,973 DEBUG TRAIN Batch 0/8300 loss 236.989578 loss_att 263.682373 loss_ctc 228.156830 loss_rnnt 232.731689 hw_loss 0.181940 lr 0.00016604 rank 7
2023-02-17 23:51:13,517 DEBUG TRAIN Batch 0/8400 loss 162.296539 loss_att 181.112732 loss_ctc 169.997986 loss_rnnt 157.471527 hw_loss 0.065477 lr 0.00016804 rank 5
2023-02-17 23:51:13,517 DEBUG TRAIN Batch 0/8400 loss 177.343903 loss_att 207.423035 loss_ctc 176.483994 loss_rnnt 171.291092 hw_loss 0.284282 lr 0.00016804 rank 0
2023-02-17 23:51:13,521 DEBUG TRAIN Batch 0/8400 loss 203.739120 loss_att 232.827560 loss_ctc 201.529144 loss_rnnt 198.184570 hw_loss 0.059129 lr 0.00016804 rank 7
2023-02-17 23:51:13,527 DEBUG TRAIN Batch 0/8400 loss 159.014938 loss_att 180.926849 loss_ctc 172.670547 loss_rnnt 152.811798 hw_loss 0.000042 lr 0.00016804 rank 2
2023-02-17 23:51:13,527 DEBUG TRAIN Batch 0/8400 loss 245.554474 loss_att 269.377441 loss_ctc 244.831757 loss_rnnt 240.866638 hw_loss 0.036769 lr 0.00016804 rank 4
2023-02-17 23:51:13,532 DEBUG TRAIN Batch 0/8400 loss 218.761261 loss_att 241.232819 loss_ctc 236.289719 loss_rnnt 211.929810 hw_loss 0.000042 lr 0.00016804 rank 3
2023-02-17 23:51:13,538 DEBUG TRAIN Batch 0/8400 loss 222.685547 loss_att 237.942352 loss_ctc 209.755493 loss_rnnt 221.191788 hw_loss 0.311961 lr 0.00016804 rank 1
2023-02-17 23:51:13,581 DEBUG TRAIN Batch 0/8400 loss 234.637238 loss_att 252.419861 loss_ctc 238.796097 loss_rnnt 230.526184 hw_loss 0.000042 lr 0.00016804 rank 6
2023-02-17 23:52:14,561 DEBUG TRAIN Batch 0/8500 loss 162.603928 loss_att 176.985199 loss_ctc 164.365845 loss_rnnt 159.492722 hw_loss 0.000078 lr 0.00017004 rank 1
2023-02-17 23:52:14,561 DEBUG TRAIN Batch 0/8500 loss 139.320953 loss_att 187.299850 loss_ctc 135.602142 loss_rnnt 130.220978 hw_loss 0.000078 lr 0.00017004 rank 7
2023-02-17 23:52:14,561 DEBUG TRAIN Batch 0/8500 loss 215.563553 loss_att 233.263779 loss_ctc 222.666031 loss_rnnt 211.009766 hw_loss 0.125178 lr 0.00017004 rank 5
2023-02-17 23:52:14,565 DEBUG TRAIN Batch 0/8500 loss 178.740723 loss_att 202.655182 loss_ctc 185.840286 loss_rnnt 172.940231 hw_loss 0.133096 lr 0.00017004 rank 2
2023-02-17 23:52:14,567 DEBUG TRAIN Batch 0/8500 loss 205.722351 loss_att 218.165527 loss_ctc 211.726501 loss_rnnt 202.433121 hw_loss 0.000078 lr 0.00017004 rank 4
2023-02-17 23:52:14,567 DEBUG TRAIN Batch 0/8500 loss 244.670059 loss_att 286.163635 loss_ctc 240.315277 loss_rnnt 236.951950 hw_loss 0.000078 lr 0.00017004 rank 6
2023-02-17 23:52:14,570 DEBUG TRAIN Batch 0/8500 loss 172.681610 loss_att 209.601639 loss_ctc 173.689621 loss_rnnt 165.163162 hw_loss 0.000078 lr 0.00017004 rank 3
2023-02-17 23:52:14,622 DEBUG TRAIN Batch 0/8500 loss 205.062927 loss_att 222.325821 loss_ctc 212.864212 loss_rnnt 200.570129 hw_loss 0.000078 lr 0.00017004 rank 0
2023-02-17 23:53:16,336 DEBUG TRAIN Batch 0/8600 loss 193.456772 loss_att 264.331177 loss_ctc 182.452881 loss_rnnt 180.706909 hw_loss 0.079062 lr 0.00017204 rank 4
2023-02-17 23:53:16,339 DEBUG TRAIN Batch 0/8600 loss 199.233902 loss_att 235.275726 loss_ctc 205.174622 loss_rnnt 191.233444 hw_loss 0.000041 lr 0.00017204 rank 6
2023-02-17 23:53:16,339 DEBUG TRAIN Batch 0/8600 loss 256.818665 loss_att 305.832825 loss_ctc 248.856201 loss_rnnt 248.016876 hw_loss 0.113711 lr 0.00017204 rank 2
2023-02-17 23:53:16,339 DEBUG TRAIN Batch 0/8600 loss 238.070023 loss_att 277.210480 loss_ctc 241.177353 loss_rnnt 229.827591 hw_loss 0.000041 lr 0.00017204 rank 0
2023-02-17 23:53:16,339 DEBUG TRAIN Batch 0/8600 loss 161.480530 loss_att 185.370255 loss_ctc 177.307312 loss_rnnt 154.434448 hw_loss 0.296057 lr 0.00017204 rank 1
2023-02-17 23:53:16,341 DEBUG TRAIN Batch 0/8600 loss 248.029160 loss_att 295.449738 loss_ctc 266.394165 loss_rnnt 235.954163 hw_loss 0.266630 lr 0.00017204 rank 5
2023-02-17 23:53:16,346 DEBUG TRAIN Batch 0/8600 loss 171.257324 loss_att 225.440674 loss_ctc 160.186630 loss_rnnt 161.896729 hw_loss 0.000041 lr 0.00017204 rank 7
2023-02-17 23:53:16,350 DEBUG TRAIN Batch 0/8600 loss 331.571136 loss_att 325.551025 loss_ctc 339.284760 loss_rnnt 331.746643 hw_loss 0.000041 lr 0.00017204 rank 3
2023-02-17 23:54:33,137 DEBUG TRAIN Batch 0/8700 loss 227.162430 loss_att 254.110596 loss_ctc 226.989410 loss_rnnt 221.795822 hw_loss 0.000072 lr 0.00017404 rank 1
2023-02-17 23:54:33,138 DEBUG TRAIN Batch 0/8700 loss 190.958542 loss_att 227.381271 loss_ctc 199.629272 loss_rnnt 182.456055 hw_loss 0.115960 lr 0.00017404 rank 4
2023-02-17 23:54:33,141 DEBUG TRAIN Batch 0/8700 loss 125.327957 loss_att 152.991745 loss_ctc 122.669807 loss_rnnt 119.981308 hw_loss 0.315582 lr 0.00017404 rank 6
2023-02-17 23:54:33,142 DEBUG TRAIN Batch 0/8700 loss 185.417099 loss_att 210.023926 loss_ctc 192.246033 loss_rnnt 179.537903 hw_loss 0.088687 lr 0.00017404 rank 0
2023-02-17 23:54:33,143 DEBUG TRAIN Batch 0/8700 loss 185.570984 loss_att 223.244232 loss_ctc 183.941223 loss_rnnt 178.153503 hw_loss 0.187762 lr 0.00017404 rank 5
2023-02-17 23:54:33,144 DEBUG TRAIN Batch 0/8700 loss 244.833817 loss_att 267.368164 loss_ctc 244.055786 loss_rnnt 240.354492 hw_loss 0.142861 lr 0.00017404 rank 2
2023-02-17 23:54:33,146 DEBUG TRAIN Batch 0/8700 loss 223.279175 loss_att 247.730042 loss_ctc 218.406509 loss_rnnt 219.038635 hw_loss 0.000072 lr 0.00017404 rank 7
2023-02-17 23:54:33,148 DEBUG TRAIN Batch 0/8700 loss 184.845642 loss_att 240.138062 loss_ctc 179.729889 loss_rnnt 174.469223 hw_loss 0.000072 lr 0.00017404 rank 3
2023-02-17 23:55:53,426 DEBUG TRAIN Batch 0/8800 loss 170.045090 loss_att 210.158203 loss_ctc 165.758987 loss_rnnt 162.371735 hw_loss 0.416657 lr 0.00017604 rank 5
2023-02-17 23:55:53,428 DEBUG TRAIN Batch 0/8800 loss 220.203369 loss_att 272.278076 loss_ctc 220.354080 loss_rnnt 209.768311 hw_loss 0.000091 lr 0.00017604 rank 7
2023-02-17 23:55:53,431 DEBUG TRAIN Batch 0/8800 loss 306.032990 loss_att 323.508423 loss_ctc 327.617310 loss_rnnt 299.659943 hw_loss 0.000091 lr 0.00017604 rank 2
2023-02-17 23:55:53,433 DEBUG TRAIN Batch 0/8800 loss 180.569305 loss_att 206.890289 loss_ctc 181.241318 loss_rnnt 175.194977 hw_loss 0.038492 lr 0.00017604 rank 4
2023-02-17 23:55:53,434 DEBUG TRAIN Batch 0/8800 loss 285.688477 loss_att 316.092773 loss_ctc 309.090088 loss_rnnt 276.487366 hw_loss 0.000091 lr 0.00017604 rank 0
2023-02-17 23:55:53,433 DEBUG TRAIN Batch 0/8800 loss 162.563019 loss_att 216.446198 loss_ctc 151.059814 loss_rnnt 153.320084 hw_loss 0.000091 lr 0.00017604 rank 6
2023-02-17 23:55:53,434 DEBUG TRAIN Batch 0/8800 loss 216.393768 loss_att 242.222977 loss_ctc 216.436386 loss_rnnt 211.080933 hw_loss 0.264935 lr 0.00017604 rank 1
2023-02-17 23:55:53,438 DEBUG TRAIN Batch 0/8800 loss 163.594696 loss_att 198.836853 loss_ctc 179.444366 loss_rnnt 154.293457 hw_loss 0.261571 lr 0.00017604 rank 3
2023-02-17 23:56:51,784 DEBUG TRAIN Batch 0/8900 loss 100.704208 loss_att 121.553223 loss_ctc 98.795570 loss_rnnt 96.679741 hw_loss 0.204661 lr 0.00017804 rank 7
2023-02-17 23:56:51,784 DEBUG TRAIN Batch 0/8900 loss 190.453201 loss_att 230.872879 loss_ctc 186.199753 loss_rnnt 182.936371 hw_loss 0.000049 lr 0.00017804 rank 6
2023-02-17 23:56:51,787 DEBUG TRAIN Batch 0/8900 loss 143.499832 loss_att 176.605011 loss_ctc 143.944916 loss_rnnt 136.751312 hw_loss 0.127760 lr 0.00017804 rank 3
2023-02-17 23:56:51,787 DEBUG TRAIN Batch 0/8900 loss 143.603775 loss_att 189.718262 loss_ctc 143.681198 loss_rnnt 134.370544 hw_loss 0.000049 lr 0.00017804 rank 1
2023-02-17 23:56:51,788 DEBUG TRAIN Batch 0/8900 loss 175.041748 loss_att 207.208588 loss_ctc 174.803055 loss_rnnt 168.627075 hw_loss 0.024649 lr 0.00017804 rank 0
2023-02-17 23:56:51,789 DEBUG TRAIN Batch 0/8900 loss 273.246857 loss_att 313.006104 loss_ctc 285.454987 loss_rnnt 263.627655 hw_loss 0.074249 lr 0.00017804 rank 2
2023-02-17 23:56:51,792 DEBUG TRAIN Batch 0/8900 loss 325.403290 loss_att 382.121460 loss_ctc 320.231750 loss_rnnt 314.728027 hw_loss 0.039737 lr 0.00017804 rank 4
2023-02-17 23:56:51,802 DEBUG TRAIN Batch 0/8900 loss 191.095535 loss_att 207.876953 loss_ctc 190.197571 loss_rnnt 187.858948 hw_loss 0.000049 lr 0.00017804 rank 5
2023-02-17 23:57:50,411 DEBUG TRAIN Batch 0/9000 loss 195.472244 loss_att 247.870789 loss_ctc 192.318359 loss_rnnt 185.348785 hw_loss 0.120453 lr 0.00018004 rank 5
2023-02-17 23:57:50,411 DEBUG TRAIN Batch 0/9000 loss 204.721542 loss_att 238.328705 loss_ctc 205.394531 loss_rnnt 197.910339 hw_loss 0.000050 lr 0.00018004 rank 4
2023-02-17 23:57:50,411 DEBUG TRAIN Batch 0/9000 loss 156.490402 loss_att 195.511246 loss_ctc 161.330734 loss_rnnt 147.901855 hw_loss 0.260610 lr 0.00018004 rank 7
2023-02-17 23:57:50,413 DEBUG TRAIN Batch 0/9000 loss 56.255455 loss_att 63.070641 loss_ctc 54.821533 loss_rnnt 54.962193 hw_loss 0.227638 lr 0.00018004 rank 1
2023-02-17 23:57:50,419 DEBUG TRAIN Batch 0/9000 loss 143.479660 loss_att 189.848190 loss_ctc 130.292694 loss_rnnt 135.964203 hw_loss 0.000050 lr 0.00018004 rank 0
2023-02-17 23:57:50,419 DEBUG TRAIN Batch 0/9000 loss 202.293503 loss_att 241.902832 loss_ctc 198.659363 loss_rnnt 194.856171 hw_loss 0.000050 lr 0.00018004 rank 6
2023-02-17 23:57:50,421 DEBUG TRAIN Batch 0/9000 loss 176.614182 loss_att 234.075134 loss_ctc 168.739349 loss_rnnt 166.087906 hw_loss 0.157616 lr 0.00018004 rank 3
2023-02-17 23:57:50,422 DEBUG TRAIN Batch 0/9000 loss 180.059433 loss_att 213.491898 loss_ctc 176.124329 loss_rnnt 173.768799 hw_loss 0.241543 lr 0.00018004 rank 2
2023-02-17 23:58:52,961 DEBUG TRAIN Batch 0/9100 loss 270.156586 loss_att 322.954163 loss_ctc 250.775879 loss_rnnt 262.181152 hw_loss 0.000033 lr 0.00018204 rank 5
2023-02-17 23:58:52,962 DEBUG TRAIN Batch 0/9100 loss 222.922043 loss_att 247.423203 loss_ctc 217.223938 loss_rnnt 218.781540 hw_loss 0.000033 lr 0.00018204 rank 4
2023-02-17 23:58:52,962 DEBUG TRAIN Batch 0/9100 loss 212.949936 loss_att 246.190155 loss_ctc 230.006927 loss_rnnt 203.944336 hw_loss 0.156165 lr 0.00018204 rank 6
2023-02-17 23:58:52,963 DEBUG TRAIN Batch 0/9100 loss 213.248535 loss_att 248.310486 loss_ctc 221.871597 loss_rnnt 205.055267 hw_loss 0.058357 lr 0.00018204 rank 1
2023-02-17 23:58:52,964 DEBUG TRAIN Batch 0/9100 loss 239.585281 loss_att 254.484177 loss_ctc 244.562851 loss_rnnt 235.900558 hw_loss 0.077418 lr 0.00018204 rank 2
2023-02-17 23:58:52,965 DEBUG TRAIN Batch 0/9100 loss 210.817886 loss_att 260.083740 loss_ctc 233.919479 loss_rnnt 197.799286 hw_loss 0.159813 lr 0.00018204 rank 3
2023-02-17 23:58:52,968 DEBUG TRAIN Batch 0/9100 loss 145.303131 loss_att 161.265137 loss_ctc 144.664703 loss_rnnt 142.048691 hw_loss 0.275943 lr 0.00018204 rank 0
2023-02-17 23:58:53,022 DEBUG TRAIN Batch 0/9100 loss 246.385986 loss_att 302.535187 loss_ctc 236.414215 loss_rnnt 236.449112 hw_loss 0.068625 lr 0.00018204 rank 7
2023-02-17 23:59:51,653 DEBUG TRAIN Batch 0/9200 loss 267.522034 loss_att 300.549133 loss_ctc 271.068878 loss_rnnt 260.443665 hw_loss 0.000042 lr 0.00018404 rank 5
2023-02-17 23:59:51,662 DEBUG TRAIN Batch 0/9200 loss 146.389969 loss_att 232.218094 loss_ctc 128.823547 loss_rnnt 131.522903 hw_loss 0.081805 lr 0.00018404 rank 0
2023-02-17 23:59:51,661 DEBUG TRAIN Batch 0/9200 loss 212.617905 loss_att 250.342499 loss_ctc 218.969299 loss_rnnt 204.145630 hw_loss 0.150966 lr 0.00018404 rank 2
2023-02-17 23:59:51,663 DEBUG TRAIN Batch 0/9200 loss 118.680756 loss_att 130.847046 loss_ctc 122.173706 loss_rnnt 115.777847 hw_loss 0.007335 lr 0.00018404 rank 4
2023-02-17 23:59:51,663 DEBUG TRAIN Batch 0/9200 loss 127.653137 loss_att 142.259781 loss_ctc 132.338562 loss_rnnt 124.053085 hw_loss 0.101267 lr 0.00018404 rank 7
2023-02-17 23:59:51,666 DEBUG TRAIN Batch 0/9200 loss 130.282455 loss_att 146.433746 loss_ctc 135.568237 loss_rnnt 126.335899 hw_loss 0.021633 lr 0.00018404 rank 3
2023-02-17 23:59:51,666 DEBUG TRAIN Batch 0/9200 loss 232.458267 loss_att 266.071655 loss_ctc 231.859894 loss_rnnt 225.815353 hw_loss 0.000042 lr 0.00018404 rank 6
2023-02-17 23:59:51,669 DEBUG TRAIN Batch 0/9200 loss 256.761475 loss_att 295.912628 loss_ctc 256.857086 loss_rnnt 248.849792 hw_loss 0.128771 lr 0.00018404 rank 1
2023-02-18 00:00:50,606 DEBUG TRAIN Batch 0/9300 loss 175.150955 loss_att 210.789459 loss_ctc 176.263641 loss_rnnt 167.789154 hw_loss 0.160760 lr 0.00018604 rank 5
2023-02-18 00:00:50,607 DEBUG TRAIN Batch 0/9300 loss 199.839981 loss_att 247.421509 loss_ctc 202.355743 loss_rnnt 189.988159 hw_loss 0.000102 lr 0.00018604 rank 4
2023-02-18 00:00:50,607 DEBUG TRAIN Batch 0/9300 loss 185.412003 loss_att 260.219604 loss_ctc 177.513153 loss_rnnt 171.503601 hw_loss 0.000102 lr 0.00018604 rank 7
2023-02-18 00:00:50,610 DEBUG TRAIN Batch 0/9300 loss 161.440384 loss_att 213.140289 loss_ctc 165.033829 loss_rnnt 150.562943 hw_loss 0.109354 lr 0.00018604 rank 0
2023-02-18 00:00:50,610 DEBUG TRAIN Batch 0/9300 loss 146.007111 loss_att 182.807892 loss_ctc 146.110016 loss_rnnt 138.609085 hw_loss 0.045275 lr 0.00018604 rank 1
2023-02-18 00:00:50,610 DEBUG TRAIN Batch 0/9300 loss 158.634048 loss_att 194.502579 loss_ctc 155.471725 loss_rnnt 151.881912 hw_loss 0.000102 lr 0.00018604 rank 6
2023-02-18 00:00:50,611 DEBUG TRAIN Batch 0/9300 loss 166.266861 loss_att 204.048965 loss_ctc 164.205948 loss_rnnt 158.955872 hw_loss 0.055057 lr 0.00018604 rank 2
2023-02-18 00:00:50,616 DEBUG TRAIN Batch 0/9300 loss 220.400131 loss_att 261.327454 loss_ctc 221.075409 loss_rnnt 212.124542 hw_loss 0.000102 lr 0.00018604 rank 3
2023-02-18 00:01:51,294 DEBUG TRAIN Batch 0/9400 loss 148.980835 loss_att 202.237106 loss_ctc 148.809097 loss_rnnt 138.276428 hw_loss 0.142599 lr 0.00018804 rank 1
2023-02-18 00:01:51,296 DEBUG TRAIN Batch 0/9400 loss 250.838303 loss_att 309.142395 loss_ctc 263.909790 loss_rnnt 237.434616 hw_loss 0.000053 lr 0.00018804 rank 7
2023-02-18 00:01:51,299 DEBUG TRAIN Batch 0/9400 loss 166.028076 loss_att 241.442490 loss_ctc 158.826263 loss_rnnt 151.843567 hw_loss 0.116021 lr 0.00018804 rank 0
2023-02-18 00:01:51,301 DEBUG TRAIN Batch 0/9400 loss 169.616348 loss_att 202.856293 loss_ctc 185.896133 loss_rnnt 160.797684 hw_loss 0.000053 lr 0.00018804 rank 5
2023-02-18 00:01:51,302 DEBUG TRAIN Batch 0/9400 loss 278.232819 loss_att 335.942505 loss_ctc 291.169250 loss_rnnt 264.899719 hw_loss 0.124268 lr 0.00018804 rank 3
2023-02-18 00:01:51,303 DEBUG TRAIN Batch 0/9400 loss 180.804993 loss_att 239.107269 loss_ctc 190.134094 loss_rnnt 167.900635 hw_loss 0.000053 lr 0.00018804 rank 2
2023-02-18 00:01:51,305 DEBUG TRAIN Batch 0/9400 loss 371.898956 loss_att 366.292816 loss_ctc 407.289642 loss_rnnt 368.137634 hw_loss 0.307086 lr 0.00018804 rank 6
2023-02-18 00:01:51,364 DEBUG TRAIN Batch 0/9400 loss 235.531937 loss_att 288.710510 loss_ctc 261.311981 loss_rnnt 221.379822 hw_loss 0.148223 lr 0.00018804 rank 4
2023-02-18 00:03:09,494 DEBUG TRAIN Batch 0/9500 loss 293.321503 loss_att 356.862244 loss_ctc 292.198822 loss_rnnt 280.763000 hw_loss 0.000066 lr 0.00019004 rank 5
2023-02-18 00:03:09,498 DEBUG TRAIN Batch 0/9500 loss 263.037048 loss_att 294.795532 loss_ctc 262.994507 loss_rnnt 256.690979 hw_loss 0.000066 lr 0.00019004 rank 1
2023-02-18 00:03:09,505 DEBUG TRAIN Batch 0/9500 loss 107.804825 loss_att 135.875504 loss_ctc 112.486450 loss_rnnt 101.521187 hw_loss 0.084915 lr 0.00019004 rank 0
2023-02-18 00:03:09,505 DEBUG TRAIN Batch 0/9500 loss 136.666321 loss_att 176.530350 loss_ctc 138.309570 loss_rnnt 128.474365 hw_loss 0.000066 lr 0.00019004 rank 3
2023-02-18 00:03:09,507 DEBUG TRAIN Batch 0/9500 loss 139.466919 loss_att 167.537552 loss_ctc 146.160202 loss_rnnt 132.895752 hw_loss 0.121128 lr 0.00019004 rank 7
2023-02-18 00:03:09,509 DEBUG TRAIN Batch 0/9500 loss 144.925537 loss_att 212.030411 loss_ctc 130.899475 loss_rnnt 133.211716 hw_loss 0.305595 lr 0.00019004 rank 6
2023-02-18 00:03:09,512 DEBUG TRAIN Batch 0/9500 loss 131.833496 loss_att 159.290512 loss_ctc 129.676163 loss_rnnt 126.556870 hw_loss 0.136625 lr 0.00019004 rank 4
2023-02-18 00:03:09,563 DEBUG TRAIN Batch 0/9500 loss 317.502960 loss_att 375.134125 loss_ctc 323.544373 loss_rnnt 305.171173 hw_loss 0.000066 lr 0.00019004 rank 2
2023-02-18 00:04:11,425 DEBUG TRAIN Batch 0/9600 loss 223.106903 loss_att 260.519287 loss_ctc 215.749924 loss_rnnt 216.475830 hw_loss 0.242855 lr 0.00019204 rank 5
2023-02-18 00:04:11,426 DEBUG TRAIN Batch 0/9600 loss 157.626434 loss_att 215.368225 loss_ctc 162.157593 loss_rnnt 145.434860 hw_loss 0.073234 lr 0.00019204 rank 1
2023-02-18 00:04:11,429 DEBUG TRAIN Batch 0/9600 loss 145.034836 loss_att 204.108459 loss_ctc 130.782745 loss_rnnt 135.043533 hw_loss 0.144097 lr 0.00019204 rank 0
2023-02-18 00:04:11,430 DEBUG TRAIN Batch 0/9600 loss 177.437149 loss_att 232.574188 loss_ctc 177.467346 loss_rnnt 166.298569 hw_loss 0.200931 lr 0.00019204 rank 3
2023-02-18 00:04:11,432 DEBUG TRAIN Batch 0/9600 loss 151.640854 loss_att 195.548096 loss_ctc 151.582550 loss_rnnt 142.791290 hw_loss 0.142305 lr 0.00019204 rank 7
2023-02-18 00:04:11,433 DEBUG TRAIN Batch 0/9600 loss 158.288651 loss_att 226.355209 loss_ctc 147.112320 loss_rnnt 146.165466 hw_loss 0.000069 lr 0.00019204 rank 4
2023-02-18 00:04:11,468 DEBUG TRAIN Batch 0/9600 loss 134.284317 loss_att 193.714783 loss_ctc 131.178162 loss_rnnt 122.812332 hw_loss 0.000069 lr 0.00019204 rank 6
2023-02-18 00:04:11,469 DEBUG TRAIN Batch 0/9600 loss 168.792755 loss_att 208.332642 loss_ctc 178.772186 loss_rnnt 159.511368 hw_loss 0.080286 lr 0.00019204 rank 2
2023-02-18 00:05:11,801 DEBUG TRAIN Batch 0/9700 loss 179.897308 loss_att 253.029297 loss_ctc 178.435120 loss_rnnt 165.465866 hw_loss 0.000051 lr 0.00019404 rank 4
2023-02-18 00:05:11,804 DEBUG TRAIN Batch 0/9700 loss 212.270355 loss_att 254.487427 loss_ctc 221.895508 loss_rnnt 202.461258 hw_loss 0.154397 lr 0.00019404 rank 5
2023-02-18 00:05:11,804 DEBUG TRAIN Batch 0/9700 loss 147.476837 loss_att 201.017181 loss_ctc 143.279785 loss_rnnt 137.322632 hw_loss 0.010739 lr 0.00019404 rank 6
2023-02-18 00:05:11,808 DEBUG TRAIN Batch 0/9700 loss 194.592117 loss_att 256.994781 loss_ctc 199.186737 loss_rnnt 181.498947 hw_loss 0.000051 lr 0.00019404 rank 3
2023-02-18 00:05:11,809 DEBUG TRAIN Batch 0/9700 loss 233.870132 loss_att 265.223785 loss_ctc 242.366974 loss_rnnt 226.466476 hw_loss 0.000051 lr 0.00019404 rank 1
2023-02-18 00:05:11,810 DEBUG TRAIN Batch 0/9700 loss 159.528839 loss_att 222.740555 loss_ctc 155.368088 loss_rnnt 147.441238 hw_loss 0.000051 lr 0.00019404 rank 2
2023-02-18 00:05:11,855 DEBUG TRAIN Batch 0/9700 loss 193.729538 loss_att 257.308716 loss_ctc 180.294296 loss_rnnt 182.805038 hw_loss 0.000051 lr 0.00019404 rank 0
2023-02-18 00:05:11,898 DEBUG TRAIN Batch 0/9700 loss 240.598251 loss_att 266.832123 loss_ctc 236.731033 loss_rnnt 235.867096 hw_loss 0.000051 lr 0.00019404 rank 7
2023-02-18 00:06:10,708 DEBUG TRAIN Batch 0/9800 loss 237.675018 loss_att 308.277527 loss_ctc 227.574112 loss_rnnt 224.897415 hw_loss 0.007255 lr 0.00019604 rank 5
2023-02-18 00:06:10,716 DEBUG TRAIN Batch 0/9800 loss 76.250923 loss_att 106.859299 loss_ctc 77.552902 loss_rnnt 69.894608 hw_loss 0.114458 lr 0.00019604 rank 6
2023-02-18 00:06:10,716 DEBUG TRAIN Batch 0/9800 loss 193.784805 loss_att 275.086670 loss_ctc 187.501663 loss_rnnt 178.321915 hw_loss 0.075483 lr 0.00019604 rank 1
2023-02-18 00:06:10,717 DEBUG TRAIN Batch 0/9800 loss 99.878700 loss_att 150.330185 loss_ctc 85.388237 loss_rnnt 91.626274 hw_loss 0.176608 lr 0.00019604 rank 0
2023-02-18 00:06:10,720 DEBUG TRAIN Batch 0/9800 loss 36.154251 loss_att 43.729313 loss_ctc 37.838589 loss_rnnt 34.277737 hw_loss 0.256731 lr 0.00019604 rank 2
2023-02-18 00:06:10,722 DEBUG TRAIN Batch 0/9800 loss 185.228394 loss_att 210.808746 loss_ctc 189.962677 loss_rnnt 179.340668 hw_loss 0.263313 lr 0.00019604 rank 4
2023-02-18 00:06:10,722 DEBUG TRAIN Batch 0/9800 loss 131.032578 loss_att 178.896790 loss_ctc 129.732147 loss_rnnt 121.565201 hw_loss 0.127321 lr 0.00019604 rank 3
2023-02-18 00:06:10,777 DEBUG TRAIN Batch 0/9800 loss 175.729080 loss_att 218.406494 loss_ctc 183.156448 loss_rnnt 166.069336 hw_loss 0.251177 lr 0.00019604 rank 7
2023-02-18 00:07:10,078 DEBUG TRAIN Batch 0/9900 loss 157.331863 loss_att 224.939392 loss_ctc 150.522034 loss_rnnt 144.641251 hw_loss 0.144541 lr 0.00019804 rank 5
2023-02-18 00:07:10,078 DEBUG TRAIN Batch 0/9900 loss 239.509186 loss_att 321.492249 loss_ctc 236.688721 loss_rnnt 223.459854 hw_loss 0.053958 lr 0.00019804 rank 0
2023-02-18 00:07:10,078 DEBUG TRAIN Batch 0/9900 loss 220.523590 loss_att 275.479065 loss_ctc 224.788055 loss_rnnt 208.901794 hw_loss 0.116420 lr 0.00019804 rank 7
2023-02-18 00:07:10,079 DEBUG TRAIN Batch 0/9900 loss 196.723480 loss_att 247.260529 loss_ctc 189.017975 loss_rnnt 187.643448 hw_loss 0.000043 lr 0.00019804 rank 2
2023-02-18 00:07:10,081 DEBUG TRAIN Batch 0/9900 loss 164.942902 loss_att 241.995087 loss_ctc 157.897125 loss_rnnt 150.394501 hw_loss 0.145118 lr 0.00019804 rank 6
2023-02-18 00:07:10,083 DEBUG TRAIN Batch 0/9900 loss 199.295822 loss_att 258.522400 loss_ctc 193.694275 loss_rnnt 188.117554 hw_loss 0.149656 lr 0.00019804 rank 1
2023-02-18 00:07:10,082 DEBUG TRAIN Batch 0/9900 loss 217.098038 loss_att 273.999939 loss_ctc 218.151733 loss_rnnt 205.510834 hw_loss 0.124382 lr 0.00019804 rank 4
2023-02-18 00:07:10,086 DEBUG TRAIN Batch 0/9900 loss 161.841187 loss_att 210.603302 loss_ctc 169.170044 loss_rnnt 150.982208 hw_loss 0.242593 lr 0.00019804 rank 3
2023-02-18 00:08:10,676 DEBUG TRAIN Batch 0/10000 loss 138.696136 loss_att 212.952881 loss_ctc 130.993210 loss_rnnt 124.871773 hw_loss 0.000109 lr 0.00020004 rank 1
2023-02-18 00:08:10,677 DEBUG TRAIN Batch 0/10000 loss 211.375977 loss_att 297.826080 loss_ctc 209.970169 loss_rnnt 194.219543 hw_loss 0.100938 lr 0.00020004 rank 5
2023-02-18 00:08:10,677 DEBUG TRAIN Batch 0/10000 loss 240.225616 loss_att 315.411194 loss_ctc 232.631851 loss_rnnt 226.200943 hw_loss 0.000109 lr 0.00020004 rank 3
2023-02-18 00:08:10,678 DEBUG TRAIN Batch 0/10000 loss 219.070023 loss_att 297.885010 loss_ctc 229.186462 loss_rnnt 201.924942 hw_loss 0.062293 lr 0.00020004 rank 0
2023-02-18 00:08:10,679 DEBUG TRAIN Batch 0/10000 loss 111.379723 loss_att 165.729889 loss_ctc 123.492867 loss_rnnt 98.713120 hw_loss 0.340299 lr 0.00020004 rank 6
2023-02-18 00:08:10,680 DEBUG TRAIN Batch 0/10000 loss 213.927628 loss_att 279.613495 loss_ctc 208.587219 loss_rnnt 201.400650 hw_loss 0.190984 lr 0.00020004 rank 2
2023-02-18 00:08:10,683 DEBUG TRAIN Batch 0/10000 loss 179.542038 loss_att 263.446075 loss_ctc 189.294128 loss_rnnt 161.460907 hw_loss 0.000109 lr 0.00020004 rank 7
2023-02-18 00:08:10,686 DEBUG TRAIN Batch 0/10000 loss 154.243088 loss_att 221.101257 loss_ctc 152.547592 loss_rnnt 141.097443 hw_loss 0.000109 lr 0.00020004 rank 4
2023-02-18 00:09:08,817 DEBUG TRAIN Batch 0/10100 loss 266.477264 loss_att 321.384094 loss_ctc 263.786163 loss_rnnt 255.783051 hw_loss 0.134358 lr 0.00020204 rank 5
2023-02-18 00:09:08,821 DEBUG TRAIN Batch 0/10100 loss 112.229073 loss_att 176.337555 loss_ctc 105.017426 loss_rnnt 100.328354 hw_loss 0.076075 lr 0.00020204 rank 4
2023-02-18 00:09:08,822 DEBUG TRAIN Batch 0/10100 loss 116.964600 loss_att 170.663849 loss_ctc 114.373619 loss_rnnt 106.440964 hw_loss 0.242356 lr 0.00020204 rank 0
2023-02-18 00:09:08,822 DEBUG TRAIN Batch 0/10100 loss 56.855480 loss_att 73.721680 loss_ctc 58.035248 loss_rnnt 53.210869 hw_loss 0.213884 lr 0.00020204 rank 6
2023-02-18 00:09:08,822 DEBUG TRAIN Batch 0/10100 loss 187.578125 loss_att 258.811279 loss_ctc 191.217377 loss_rnnt 172.846207 hw_loss 0.000067 lr 0.00020204 rank 1
2023-02-18 00:09:08,824 DEBUG TRAIN Batch 0/10100 loss 191.013763 loss_att 259.988281 loss_ctc 192.418411 loss_rnnt 177.005661 hw_loss 0.048591 lr 0.00020204 rank 3
2023-02-18 00:09:08,830 DEBUG TRAIN Batch 0/10100 loss 154.269958 loss_att 218.377975 loss_ctc 155.017197 loss_rnnt 141.348694 hw_loss 0.000067 lr 0.00020204 rank 7
2023-02-18 00:09:08,830 DEBUG TRAIN Batch 0/10100 loss 64.111382 loss_att 84.955971 loss_ctc 63.834011 loss_rnnt 59.845894 hw_loss 0.250414 lr 0.00020204 rank 2
2023-02-18 00:10:08,954 DEBUG TRAIN Batch 0/10200 loss 117.276733 loss_att 192.462418 loss_ctc 110.903885 loss_rnnt 103.029266 hw_loss 0.112589 lr 0.00020404 rank 1
2023-02-18 00:10:08,959 DEBUG TRAIN Batch 0/10200 loss 182.665024 loss_att 222.033142 loss_ctc 172.863464 loss_rnnt 176.081268 hw_loss 0.031903 lr 0.00020404 rank 5
2023-02-18 00:10:08,960 DEBUG TRAIN Batch 0/10200 loss 176.313248 loss_att 227.340958 loss_ctc 179.896164 loss_rnnt 165.579483 hw_loss 0.094698 lr 0.00020404 rank 3
2023-02-18 00:10:08,963 DEBUG TRAIN Batch 0/10200 loss 191.745071 loss_att 253.605499 loss_ctc 182.888184 loss_rnnt 180.464600 hw_loss 0.167432 lr 0.00020404 rank 6
2023-02-18 00:10:08,963 DEBUG TRAIN Batch 0/10200 loss 145.374268 loss_att 210.425568 loss_ctc 144.829498 loss_rnnt 132.333282 hw_loss 0.193778 lr 0.00020404 rank 0
2023-02-18 00:10:08,965 DEBUG TRAIN Batch 0/10200 loss 169.013550 loss_att 226.732422 loss_ctc 168.853424 loss_rnnt 157.450592 hw_loss 0.075984 lr 0.00020404 rank 2
2023-02-18 00:10:08,965 DEBUG TRAIN Batch 0/10200 loss 126.899361 loss_att 209.980072 loss_ctc 117.323380 loss_rnnt 111.559982 hw_loss 0.000068 lr 0.00020404 rank 4
2023-02-18 00:10:09,018 DEBUG TRAIN Batch 0/10200 loss 146.726593 loss_att 221.490936 loss_ctc 144.172852 loss_rnnt 132.114197 hw_loss 0.000068 lr 0.00020404 rank 7
2023-02-18 00:11:10,067 DEBUG TRAIN Batch 0/10300 loss 90.091217 loss_att 177.323624 loss_ctc 82.801903 loss_rnnt 73.616615 hw_loss 0.000041 lr 0.00020604 rank 5
2023-02-18 00:11:10,070 DEBUG TRAIN Batch 0/10300 loss 133.052460 loss_att 214.255920 loss_ctc 136.936493 loss_rnnt 116.205627 hw_loss 0.165465 lr 0.00020604 rank 2
2023-02-18 00:11:10,072 DEBUG TRAIN Batch 0/10300 loss 187.230270 loss_att 253.582062 loss_ctc 192.100052 loss_rnnt 173.310577 hw_loss 0.000041 lr 0.00020604 rank 4
2023-02-18 00:11:10,077 DEBUG TRAIN Batch 0/10300 loss 183.928329 loss_att 272.117493 loss_ctc 183.284119 loss_rnnt 166.376358 hw_loss 0.000041 lr 0.00020604 rank 1
2023-02-18 00:11:10,078 DEBUG TRAIN Batch 0/10300 loss 170.785828 loss_att 221.327911 loss_ctc 173.611191 loss_rnnt 160.300674 hw_loss 0.000041 lr 0.00020604 rank 0
2023-02-18 00:11:10,083 DEBUG TRAIN Batch 0/10300 loss 224.881104 loss_att 295.874695 loss_ctc 232.817169 loss_rnnt 209.598953 hw_loss 0.047396 lr 0.00020604 rank 6
2023-02-18 00:11:10,087 DEBUG TRAIN Batch 0/10300 loss 202.150803 loss_att 267.070923 loss_ctc 209.695663 loss_rnnt 188.160767 hw_loss 0.000041 lr 0.00020604 rank 3
2023-02-18 00:11:10,142 DEBUG TRAIN Batch 0/10300 loss 214.920700 loss_att 296.025726 loss_ctc 217.242508 loss_rnnt 198.390106 hw_loss 0.000041 lr 0.00020604 rank 7
2023-02-18 00:12:27,254 DEBUG TRAIN Batch 0/10400 loss 116.654076 loss_att 193.347504 loss_ctc 99.534409 loss_rnnt 103.593536 hw_loss 0.008395 lr 0.00020804 rank 5
2023-02-18 00:12:27,261 DEBUG TRAIN Batch 0/10400 loss 128.225800 loss_att 187.402542 loss_ctc 124.112694 loss_rnnt 116.853973 hw_loss 0.159167 lr 0.00020804 rank 4
2023-02-18 00:12:27,261 DEBUG TRAIN Batch 0/10400 loss 170.375519 loss_att 255.436386 loss_ctc 159.810669 loss_rnnt 154.771942 hw_loss 0.000068 lr 0.00020804 rank 1
2023-02-18 00:12:27,268 DEBUG TRAIN Batch 0/10400 loss 112.901230 loss_att 142.039917 loss_ctc 114.255737 loss_rnnt 106.806839 hw_loss 0.161326 lr 0.00020804 rank 2
2023-02-18 00:12:27,272 DEBUG TRAIN Batch 0/10400 loss 147.495544 loss_att 220.936371 loss_ctc 145.572495 loss_rnnt 133.063736 hw_loss 0.000068 lr 0.00020804 rank 3
2023-02-18 00:12:27,274 DEBUG TRAIN Batch 0/10400 loss 140.932510 loss_att 181.667908 loss_ctc 151.449799 loss_rnnt 131.383072 hw_loss 0.000068 lr 0.00020804 rank 7
2023-02-18 00:12:27,276 DEBUG TRAIN Batch 0/10400 loss 168.790268 loss_att 226.936691 loss_ctc 160.965088 loss_rnnt 158.131531 hw_loss 0.136546 lr 0.00020804 rank 0
2023-02-18 00:12:27,324 DEBUG TRAIN Batch 0/10400 loss 85.333443 loss_att 139.703644 loss_ctc 85.401840 loss_rnnt 74.367035 hw_loss 0.156079 lr 0.00020804 rank 6
2023-02-18 00:13:45,356 DEBUG TRAIN Batch 0/10500 loss 184.892319 loss_att 267.893372 loss_ctc 191.825333 loss_rnnt 167.285889 hw_loss 0.153410 lr 0.00021004 rank 0
2023-02-18 00:13:45,361 DEBUG TRAIN Batch 0/10500 loss 183.729050 loss_att 264.741272 loss_ctc 191.689880 loss_rnnt 166.465088 hw_loss 0.000107 lr 0.00021004 rank 3
2023-02-18 00:13:45,362 DEBUG TRAIN Batch 0/10500 loss 76.852798 loss_att 141.497818 loss_ctc 71.841507 loss_rnnt 64.591896 hw_loss 0.000107 lr 0.00021004 rank 4
2023-02-18 00:13:45,364 DEBUG TRAIN Batch 0/10500 loss 145.536499 loss_att 221.245224 loss_ctc 146.290863 loss_rnnt 130.266281 hw_loss 0.052283 lr 0.00021004 rank 7
2023-02-18 00:13:45,365 DEBUG TRAIN Batch 0/10500 loss 137.045212 loss_att 210.337524 loss_ctc 138.561096 loss_rnnt 122.138535 hw_loss 0.086440 lr 0.00021004 rank 5
2023-02-18 00:13:45,367 DEBUG TRAIN Batch 0/10500 loss 172.673691 loss_att 237.182510 loss_ctc 174.872269 loss_rnnt 159.396500 hw_loss 0.154275 lr 0.00021004 rank 1
2023-02-18 00:13:45,371 DEBUG TRAIN Batch 0/10500 loss 166.341171 loss_att 240.472748 loss_ctc 161.455383 loss_rnnt 152.065262 hw_loss 0.189381 lr 0.00021004 rank 2
2023-02-18 00:13:45,376 DEBUG TRAIN Batch 0/10500 loss 155.807602 loss_att 225.652634 loss_ctc 155.032410 loss_rnnt 141.868652 hw_loss 0.137425 lr 0.00021004 rank 6
2023-02-18 00:14:44,866 DEBUG TRAIN Batch 0/10600 loss 172.499054 loss_att 250.628479 loss_ctc 163.797928 loss_rnnt 157.975266 hw_loss 0.108865 lr 0.00021204 rank 5
2023-02-18 00:14:44,868 DEBUG TRAIN Batch 0/10600 loss 160.795227 loss_att 239.026642 loss_ctc 168.771942 loss_rnnt 144.085327 hw_loss 0.000063 lr 0.00021204 rank 3
2023-02-18 00:14:44,871 DEBUG TRAIN Batch 0/10600 loss 134.654160 loss_att 238.825287 loss_ctc 128.739975 loss_rnnt 114.608444 hw_loss 0.000063 lr 0.00021204 rank 6
2023-02-18 00:14:44,873 DEBUG TRAIN Batch 0/10600 loss 190.467834 loss_att 278.751801 loss_ctc 179.189621 loss_rnnt 174.244171 hw_loss 0.132395 lr 0.00021204 rank 2
2023-02-18 00:14:44,874 DEBUG TRAIN Batch 0/10600 loss 112.281128 loss_att 198.833694 loss_ctc 107.183136 loss_rnnt 95.548439 hw_loss 0.191068 lr 0.00021204 rank 4
2023-02-18 00:14:44,875 DEBUG TRAIN Batch 0/10600 loss 244.685196 loss_att 325.420227 loss_ctc 252.056915 loss_rnnt 227.555267 hw_loss 0.000063 lr 0.00021204 rank 0
2023-02-18 00:14:44,875 DEBUG TRAIN Batch 0/10600 loss 191.347916 loss_att 297.148407 loss_ctc 171.396698 loss_rnnt 172.847931 hw_loss 0.000063 lr 0.00021204 rank 1
2023-02-18 00:14:44,880 DEBUG TRAIN Batch 0/10600 loss 211.430283 loss_att 275.552155 loss_ctc 213.524979 loss_rnnt 198.209747 hw_loss 0.219121 lr 0.00021204 rank 7
2023-02-18 00:15:42,747 DEBUG TRAIN Batch 0/10700 loss 167.014908 loss_att 231.485611 loss_ctc 159.235733 loss_rnnt 155.140472 hw_loss 0.032828 lr 0.00021404 rank 0
2023-02-18 00:15:42,748 DEBUG TRAIN Batch 0/10700 loss 132.366974 loss_att 181.643723 loss_ctc 144.761475 loss_rnnt 120.770927 hw_loss 0.165158 lr 0.00021404 rank 7
2023-02-18 00:15:42,748 DEBUG TRAIN Batch 0/10700 loss 49.094093 loss_att 59.476215 loss_ctc 53.995323 loss_rnnt 46.196674 hw_loss 0.314057 lr 0.00021404 rank 5
2023-02-18 00:15:42,748 DEBUG TRAIN Batch 0/10700 loss 189.458252 loss_att 283.889343 loss_ctc 190.186218 loss_rnnt 170.440094 hw_loss 0.065381 lr 0.00021404 rank 1
2023-02-18 00:15:42,749 DEBUG TRAIN Batch 0/10700 loss 211.175323 loss_att 282.515381 loss_ctc 213.430740 loss_rnnt 196.606522 hw_loss 0.000110 lr 0.00021404 rank 3
2023-02-18 00:15:42,751 DEBUG TRAIN Batch 0/10700 loss 122.500038 loss_att 203.480591 loss_ctc 117.535034 loss_rnnt 106.914169 hw_loss 0.097059 lr 0.00021404 rank 2
2023-02-18 00:15:42,752 DEBUG TRAIN Batch 0/10700 loss 144.889679 loss_att 200.244461 loss_ctc 142.693115 loss_rnnt 134.046494 hw_loss 0.122087 lr 0.00021404 rank 4
2023-02-18 00:15:42,754 DEBUG TRAIN Batch 0/10700 loss 115.798149 loss_att 163.046280 loss_ctc 114.857971 loss_rnnt 106.356064 hw_loss 0.220932 lr 0.00021404 rank 6
2023-02-18 00:16:43,438 DEBUG TRAIN Batch 0/10800 loss 132.642731 loss_att 247.073883 loss_ctc 136.961075 loss_rnnt 109.180695 hw_loss 0.000034 lr 0.00021604 rank 0
2023-02-18 00:16:43,440 DEBUG TRAIN Batch 0/10800 loss 216.269653 loss_att 276.074707 loss_ctc 215.839172 loss_rnnt 204.265915 hw_loss 0.187705 lr 0.00021604 rank 7
2023-02-18 00:16:43,441 DEBUG TRAIN Batch 0/10800 loss 177.404678 loss_att 258.438782 loss_ctc 184.786865 loss_rnnt 160.128860 hw_loss 0.158814 lr 0.00021604 rank 4
2023-02-18 00:16:43,443 DEBUG TRAIN Batch 0/10800 loss 141.339798 loss_att 223.847351 loss_ctc 135.798462 loss_rnnt 125.544060 hw_loss 0.061986 lr 0.00021604 rank 6
2023-02-18 00:16:43,443 DEBUG TRAIN Batch 0/10800 loss 161.107513 loss_att 224.642334 loss_ctc 155.950256 loss_rnnt 149.088165 hw_loss 0.000034 lr 0.00021604 rank 1
2023-02-18 00:16:43,444 DEBUG TRAIN Batch 0/10800 loss 133.210831 loss_att 210.140671 loss_ctc 129.896942 loss_rnnt 118.266693 hw_loss 0.000034 lr 0.00021604 rank 3
2023-02-18 00:16:43,444 DEBUG TRAIN Batch 0/10800 loss 187.392212 loss_att 294.221313 loss_ctc 169.372086 loss_rnnt 168.338577 hw_loss 0.169662 lr 0.00021604 rank 5
2023-02-18 00:16:43,449 DEBUG TRAIN Batch 0/10800 loss 153.063507 loss_att 241.003082 loss_ctc 152.000854 loss_rnnt 135.558075 hw_loss 0.111001 lr 0.00021604 rank 2
2023-02-18 00:17:43,121 DEBUG TRAIN Batch 0/10900 loss 165.913208 loss_att 271.331360 loss_ctc 151.628723 loss_rnnt 146.676086 hw_loss 0.108916 lr 0.00021804 rank 5
2023-02-18 00:17:43,121 DEBUG TRAIN Batch 0/10900 loss 170.078781 loss_att 217.694168 loss_ctc 186.116348 loss_rnnt 158.417343 hw_loss 0.000041 lr 0.00021804 rank 1
2023-02-18 00:17:43,122 DEBUG TRAIN Batch 0/10900 loss 167.083115 loss_att 217.007370 loss_ctc 178.432510 loss_rnnt 155.584991 hw_loss 0.000041 lr 0.00021804 rank 4
2023-02-18 00:17:43,122 DEBUG TRAIN Batch 0/10900 loss 177.494705 loss_att 286.479340 loss_ctc 171.600433 loss_rnnt 156.483643 hw_loss 0.000041 lr 0.00021804 rank 6
2023-02-18 00:17:43,124 DEBUG TRAIN Batch 0/10900 loss 114.281021 loss_att 213.141357 loss_ctc 92.090416 loss_rnnt 97.467667 hw_loss 0.000041 lr 0.00021804 rank 0
2023-02-18 00:17:43,127 DEBUG TRAIN Batch 0/10900 loss 168.162308 loss_att 257.443848 loss_ctc 172.222351 loss_rnnt 149.735245 hw_loss 0.055158 lr 0.00021804 rank 2
2023-02-18 00:17:43,129 DEBUG TRAIN Batch 0/10900 loss 140.315414 loss_att 204.963867 loss_ctc 146.590469 loss_rnnt 126.549019 hw_loss 0.000041 lr 0.00021804 rank 7
2023-02-18 00:17:43,134 DEBUG TRAIN Batch 0/10900 loss 66.863647 loss_att 88.424271 loss_ctc 65.123672 loss_rnnt 62.563560 hw_loss 0.412418 lr 0.00021804 rank 3
2023-02-18 00:18:41,281 DEBUG TRAIN Batch 0/11000 loss 66.743813 loss_att 89.769699 loss_ctc 69.111710 loss_rnnt 61.738914 hw_loss 0.157501 lr 0.00022004 rank 5
2023-02-18 00:18:41,288 DEBUG TRAIN Batch 0/11000 loss 141.022614 loss_att 215.349350 loss_ctc 139.025421 loss_rnnt 126.423515 hw_loss 0.000063 lr 0.00022004 rank 0
2023-02-18 00:18:41,289 DEBUG TRAIN Batch 0/11000 loss 197.989365 loss_att 296.591492 loss_ctc 197.163055 loss_rnnt 178.327286 hw_loss 0.097158 lr 0.00022004 rank 7
2023-02-18 00:18:41,290 DEBUG TRAIN Batch 0/11000 loss 130.489929 loss_att 183.986191 loss_ctc 134.279938 loss_rnnt 119.224823 hw_loss 0.113483 lr 0.00022004 rank 6
2023-02-18 00:18:41,290 DEBUG TRAIN Batch 0/11000 loss 134.092621 loss_att 196.038010 loss_ctc 133.009308 loss_rnnt 121.814590 hw_loss 0.062589 lr 0.00022004 rank 3
2023-02-18 00:18:41,290 DEBUG TRAIN Batch 0/11000 loss 185.555740 loss_att 239.216278 loss_ctc 187.609695 loss_rnnt 174.549713 hw_loss 0.000063 lr 0.00022004 rank 1
2023-02-18 00:18:41,292 DEBUG TRAIN Batch 0/11000 loss 170.779129 loss_att 255.647797 loss_ctc 162.614502 loss_rnnt 154.849091 hw_loss 0.084233 lr 0.00022004 rank 4
2023-02-18 00:18:41,292 DEBUG TRAIN Batch 0/11000 loss 151.422531 loss_att 213.153320 loss_ctc 145.321686 loss_rnnt 139.773468 hw_loss 0.218156 lr 0.00022004 rank 2
2023-02-18 00:19:42,350 DEBUG TRAIN Batch 0/11100 loss 196.744049 loss_att 284.762360 loss_ctc 224.671738 loss_rnnt 175.416641 hw_loss 0.000075 lr 0.00022204 rank 0
2023-02-18 00:19:42,350 DEBUG TRAIN Batch 0/11100 loss 131.919571 loss_att 210.240540 loss_ctc 128.999466 loss_rnnt 116.644684 hw_loss 0.000075 lr 0.00022204 rank 1
2023-02-18 00:19:42,351 DEBUG TRAIN Batch 0/11100 loss 185.624252 loss_att 275.313232 loss_ctc 186.751251 loss_rnnt 167.450806 hw_loss 0.160109 lr 0.00022204 rank 5
2023-02-18 00:19:42,354 DEBUG TRAIN Batch 0/11100 loss 100.245201 loss_att 208.145523 loss_ctc 98.976158 loss_rnnt 78.834297 hw_loss 0.000075 lr 0.00022204 rank 4
2023-02-18 00:19:42,355 DEBUG TRAIN Batch 0/11100 loss 119.744774 loss_att 202.170898 loss_ctc 109.312263 loss_rnnt 104.536011 hw_loss 0.214753 lr 0.00022204 rank 3
2023-02-18 00:19:42,354 DEBUG TRAIN Batch 0/11100 loss 101.297066 loss_att 202.690750 loss_ctc 92.715889 loss_rnnt 82.117828 hw_loss 0.083706 lr 0.00022204 rank 2
2023-02-18 00:19:42,365 DEBUG TRAIN Batch 0/11100 loss 161.205780 loss_att 220.575272 loss_ctc 150.818619 loss_rnnt 150.596588 hw_loss 0.225495 lr 0.00022204 rank 7
2023-02-18 00:19:42,415 DEBUG TRAIN Batch 0/11100 loss 133.074966 loss_att 237.477966 loss_ctc 107.867195 loss_rnnt 115.486313 hw_loss 0.129538 lr 0.00022204 rank 6
2023-02-18 00:21:00,603 DEBUG TRAIN Batch 0/11200 loss 146.496201 loss_att 198.822418 loss_ctc 140.860474 loss_rnnt 136.782379 hw_loss 0.000055 lr 0.00022404 rank 5
2023-02-18 00:21:00,606 DEBUG TRAIN Batch 0/11200 loss 237.702896 loss_att 241.270111 loss_ctc 237.677902 loss_rnnt 236.992767 hw_loss 0.000055 lr 0.00022404 rank 2
2023-02-18 00:21:00,607 DEBUG TRAIN Batch 0/11200 loss 105.208664 loss_att 164.175934 loss_ctc 98.870010 loss_rnnt 94.248680 hw_loss 0.021910 lr 0.00022404 rank 3
2023-02-18 00:21:00,608 DEBUG TRAIN Batch 0/11200 loss 140.053833 loss_att 250.925629 loss_ctc 131.581009 loss_rnnt 119.009155 hw_loss 0.000055 lr 0.00022404 rank 6
2023-02-18 00:21:00,609 DEBUG TRAIN Batch 0/11200 loss 145.686829 loss_att 230.385223 loss_ctc 146.009445 loss_rnnt 128.608154 hw_loss 0.179984 lr 0.00022404 rank 1
2023-02-18 00:21:00,613 DEBUG TRAIN Batch 0/11200 loss 235.530991 loss_att 329.727112 loss_ctc 225.578171 loss_rnnt 218.018784 hw_loss 0.000055 lr 0.00022404 rank 4
2023-02-18 00:21:00,617 DEBUG TRAIN Batch 0/11200 loss 132.257446 loss_att 238.082062 loss_ctc 132.653915 loss_rnnt 111.034042 hw_loss 0.010547 lr 0.00022404 rank 7
2023-02-18 00:21:00,619 DEBUG TRAIN Batch 0/11200 loss 46.119961 loss_att 64.867355 loss_ctc 48.096096 loss_rnnt 42.087658 hw_loss 0.036264 lr 0.00022404 rank 0
2023-02-18 00:22:17,612 DEBUG TRAIN Batch 0/11300 loss 93.708267 loss_att 125.832657 loss_ctc 100.256683 loss_rnnt 86.318810 hw_loss 0.171480 lr 0.00022604 rank 1
2023-02-18 00:22:17,618 DEBUG TRAIN Batch 0/11300 loss 91.660408 loss_att 141.044434 loss_ctc 90.889816 loss_rnnt 81.777496 hw_loss 0.204102 lr 0.00022604 rank 5
2023-02-18 00:22:17,618 DEBUG TRAIN Batch 0/11300 loss 113.921585 loss_att 176.596725 loss_ctc 110.465752 loss_rnnt 101.847290 hw_loss 0.000084 lr 0.00022604 rank 6
2023-02-18 00:22:17,619 DEBUG TRAIN Batch 0/11300 loss 178.371063 loss_att 249.028839 loss_ctc 180.430664 loss_rnnt 163.883179 hw_loss 0.153222 lr 0.00022604 rank 0
2023-02-18 00:22:17,619 DEBUG TRAIN Batch 0/11300 loss 102.014732 loss_att 172.644226 loss_ctc 97.665253 loss_rnnt 88.439278 hw_loss 0.055270 lr 0.00022604 rank 2
2023-02-18 00:22:17,623 DEBUG TRAIN Batch 0/11300 loss 95.436882 loss_att 178.339325 loss_ctc 98.530045 loss_rnnt 78.299805 hw_loss 0.270318 lr 0.00022604 rank 4
2023-02-18 00:22:17,624 DEBUG TRAIN Batch 0/11300 loss 118.829384 loss_att 232.053253 loss_ctc 109.160873 loss_rnnt 97.414505 hw_loss 0.111057 lr 0.00022604 rank 3
2023-02-18 00:22:17,625 DEBUG TRAIN Batch 0/11300 loss 107.125763 loss_att 187.242584 loss_ctc 105.217621 loss_rnnt 91.281181 hw_loss 0.141826 lr 0.00022604 rank 7
2023-02-18 00:23:18,790 DEBUG TRAIN Batch 0/11400 loss 193.600174 loss_att 284.231964 loss_ctc 197.315994 loss_rnnt 174.886688 hw_loss 0.171926 lr 0.00022804 rank 6
2023-02-18 00:23:18,797 DEBUG TRAIN Batch 0/11400 loss 135.050690 loss_att 217.948883 loss_ctc 141.256042 loss_rnnt 117.590256 hw_loss 0.100181 lr 0.00022804 rank 7
2023-02-18 00:23:18,799 DEBUG TRAIN Batch 0/11400 loss 109.925972 loss_att 200.944336 loss_ctc 102.338303 loss_rnnt 92.733948 hw_loss 0.000054 lr 0.00022804 rank 0
2023-02-18 00:23:18,801 DEBUG TRAIN Batch 0/11400 loss 198.304245 loss_att 270.833069 loss_ctc 211.739822 loss_rnnt 182.007050 hw_loss 0.000054 lr 0.00022804 rank 2
2023-02-18 00:23:18,801 DEBUG TRAIN Batch 0/11400 loss 123.834137 loss_att 213.732376 loss_ctc 110.982346 loss_rnnt 107.536301 hw_loss 0.059548 lr 0.00022804 rank 1
2023-02-18 00:23:18,802 DEBUG TRAIN Batch 0/11400 loss 121.727661 loss_att 179.910294 loss_ctc 121.087769 loss_rnnt 110.071922 hw_loss 0.196006 lr 0.00022804 rank 4
2023-02-18 00:23:18,804 DEBUG TRAIN Batch 0/11400 loss 148.585953 loss_att 224.345306 loss_ctc 133.384964 loss_rnnt 135.460861 hw_loss 0.000054 lr 0.00022804 rank 3
2023-02-18 00:23:18,805 DEBUG TRAIN Batch 0/11400 loss 164.032440 loss_att 258.224304 loss_ctc 169.685516 loss_rnnt 144.440308 hw_loss 0.000054 lr 0.00022804 rank 5
2023-02-18 00:24:17,769 DEBUG TRAIN Batch 0/11500 loss 130.180771 loss_att 196.927872 loss_ctc 144.987427 loss_rnnt 114.857101 hw_loss 0.000088 lr 0.00023004 rank 3
2023-02-18 00:24:17,775 DEBUG TRAIN Batch 0/11500 loss 112.187599 loss_att 237.315735 loss_ctc 89.961540 loss_rnnt 90.125389 hw_loss 0.000088 lr 0.00023004 rank 6
2023-02-18 00:24:17,775 DEBUG TRAIN Batch 0/11500 loss 184.490540 loss_att 258.208374 loss_ctc 183.022339 loss_rnnt 169.939301 hw_loss 0.006420 lr 0.00023004 rank 5
2023-02-18 00:24:17,777 DEBUG TRAIN Batch 0/11500 loss 118.890182 loss_att 174.314957 loss_ctc 131.232880 loss_rnnt 106.092758 hw_loss 0.125201 lr 0.00023004 rank 0
2023-02-18 00:24:17,782 DEBUG TRAIN Batch 0/11500 loss 93.787064 loss_att 188.715317 loss_ctc 83.042137 loss_rnnt 76.234024 hw_loss 0.000088 lr 0.00023004 rank 2
2023-02-18 00:24:17,798 DEBUG TRAIN Batch 0/11500 loss 146.028198 loss_att 265.380798 loss_ctc 142.569153 loss_rnnt 122.618851 hw_loss 0.000088 lr 0.00023004 rank 4
2023-02-18 00:24:17,805 DEBUG TRAIN Batch 0/11500 loss 234.895615 loss_att 305.916809 loss_ctc 248.646454 loss_rnnt 218.830048 hw_loss 0.052302 lr 0.00023004 rank 1
2023-02-18 00:24:17,839 DEBUG TRAIN Batch 0/11500 loss 47.554199 loss_att 63.566467 loss_ctc 49.893383 loss_rnnt 43.988445 hw_loss 0.096398 lr 0.00023004 rank 7
2023-02-18 00:25:15,638 DEBUG TRAIN Batch 0/11600 loss 101.607788 loss_att 207.435364 loss_ctc 94.765137 loss_rnnt 81.260742 hw_loss 0.176036 lr 0.00023204 rank 7
2023-02-18 00:25:15,640 DEBUG TRAIN Batch 0/11600 loss 154.229965 loss_att 202.895355 loss_ctc 171.525116 loss_rnnt 142.190826 hw_loss 0.000073 lr 0.00023204 rank 5
2023-02-18 00:25:15,641 DEBUG TRAIN Batch 0/11600 loss 80.707848 loss_att 167.847687 loss_ctc 78.341019 loss_rnnt 63.551083 hw_loss 0.083196 lr 0.00023204 rank 2
2023-02-18 00:25:15,643 DEBUG TRAIN Batch 0/11600 loss 164.044708 loss_att 245.345200 loss_ctc 171.592499 loss_rnnt 146.756287 hw_loss 0.041170 lr 0.00023204 rank 4
2023-02-18 00:25:15,643 DEBUG TRAIN Batch 0/11600 loss 107.658325 loss_att 200.185669 loss_ctc 98.186958 loss_rnnt 90.374062 hw_loss 0.078092 lr 0.00023204 rank 6
2023-02-18 00:25:15,645 DEBUG TRAIN Batch 0/11600 loss 107.634819 loss_att 187.827454 loss_ctc 102.226608 loss_rnnt 92.265335 hw_loss 0.097589 lr 0.00023204 rank 1
2023-02-18 00:25:15,648 DEBUG TRAIN Batch 0/11600 loss 140.071518 loss_att 239.178284 loss_ctc 131.250885 loss_rnnt 121.346558 hw_loss 0.149418 lr 0.00023204 rank 3
2023-02-18 00:25:15,701 DEBUG TRAIN Batch 0/11600 loss 132.026031 loss_att 217.751099 loss_ctc 131.594360 loss_rnnt 114.938522 hw_loss 0.000073 lr 0.00023204 rank 0
2023-02-18 00:26:16,331 DEBUG TRAIN Batch 0/11700 loss 139.297577 loss_att 213.664948 loss_ctc 145.579483 loss_rnnt 123.586479 hw_loss 0.000062 lr 0.00023404 rank 0
2023-02-18 00:26:16,334 DEBUG TRAIN Batch 0/11700 loss 127.352417 loss_att 232.325958 loss_ctc 120.919525 loss_rnnt 107.200180 hw_loss 0.028606 lr 0.00023404 rank 7
2023-02-18 00:26:16,334 DEBUG TRAIN Batch 0/11700 loss 156.282303 loss_att 229.152344 loss_ctc 163.466431 loss_rnnt 140.707123 hw_loss 0.081171 lr 0.00023404 rank 5
2023-02-18 00:26:16,338 DEBUG TRAIN Batch 0/11700 loss 154.657761 loss_att 224.722260 loss_ctc 154.349731 loss_rnnt 140.685883 hw_loss 0.000062 lr 0.00023404 rank 2
2023-02-18 00:26:16,340 DEBUG TRAIN Batch 0/11700 loss 144.775360 loss_att 201.088150 loss_ctc 149.992310 loss_rnnt 132.817169 hw_loss 0.000062 lr 0.00023404 rank 4
2023-02-18 00:26:16,342 DEBUG TRAIN Batch 0/11700 loss 108.848732 loss_att 210.476334 loss_ctc 102.551651 loss_rnnt 89.282776 hw_loss 0.150094 lr 0.00023404 rank 6
2023-02-18 00:26:16,343 DEBUG TRAIN Batch 0/11700 loss 110.873894 loss_att 213.779007 loss_ctc 105.203896 loss_rnnt 91.048836 hw_loss 0.000062 lr 0.00023404 rank 1
2023-02-18 00:26:16,345 DEBUG TRAIN Batch 0/11700 loss 155.664536 loss_att 273.486328 loss_ctc 147.932617 loss_rnnt 133.040100 hw_loss 0.170614 lr 0.00023404 rank 3
2023-02-18 00:27:13,691 DEBUG TRAIN Batch 0/11800 loss 72.788483 loss_att 125.542175 loss_ctc 74.387901 loss_rnnt 61.984951 hw_loss 0.074140 lr 0.00023604 rank 4
2023-02-18 00:27:13,693 DEBUG TRAIN Batch 0/11800 loss 107.691475 loss_att 182.476349 loss_ctc 105.708511 loss_rnnt 92.927895 hw_loss 0.133114 lr 0.00023604 rank 0
2023-02-18 00:27:13,693 DEBUG TRAIN Batch 0/11800 loss 133.960052 loss_att 227.578400 loss_ctc 112.059219 loss_rnnt 118.156448 hw_loss 0.000080 lr 0.00023604 rank 1
2023-02-18 00:27:13,693 DEBUG TRAIN Batch 0/11800 loss 174.504959 loss_att 274.868896 loss_ctc 183.711761 loss_rnnt 153.204575 hw_loss 0.000080 lr 0.00023604 rank 5
2023-02-18 00:27:13,694 DEBUG TRAIN Batch 0/11800 loss 134.730728 loss_att 219.020004 loss_ctc 138.654190 loss_rnnt 117.286087 hw_loss 0.119376 lr 0.00023604 rank 3
2023-02-18 00:27:13,697 DEBUG TRAIN Batch 0/11800 loss 108.177292 loss_att 149.685852 loss_ctc 117.443993 loss_rnnt 98.580193 hw_loss 0.112166 lr 0.00023604 rank 7
2023-02-18 00:27:13,701 DEBUG TRAIN Batch 0/11800 loss 143.916107 loss_att 281.800903 loss_ctc 146.769363 loss_rnnt 115.870468 hw_loss 0.165470 lr 0.00023604 rank 2
2023-02-18 00:27:13,703 DEBUG TRAIN Batch 0/11800 loss 157.869095 loss_att 264.027466 loss_ctc 157.056946 loss_rnnt 136.676666 hw_loss 0.129447 lr 0.00023604 rank 6
2023-02-18 00:28:13,633 DEBUG TRAIN Batch 0/11900 loss 134.076263 loss_att 204.735535 loss_ctc 146.695892 loss_rnnt 118.223450 hw_loss 0.071889 lr 0.00023804 rank 1
2023-02-18 00:28:13,636 DEBUG TRAIN Batch 0/11900 loss 172.770874 loss_att 250.733124 loss_ctc 179.354004 loss_rnnt 156.223267 hw_loss 0.145150 lr 0.00023804 rank 3
2023-02-18 00:28:13,636 DEBUG TRAIN Batch 0/11900 loss 179.118332 loss_att 297.364258 loss_ctc 184.884842 loss_rnnt 154.589783 hw_loss 0.207174 lr 0.00023804 rank 0
2023-02-18 00:28:13,636 DEBUG TRAIN Batch 0/11900 loss 152.466934 loss_att 240.988831 loss_ctc 154.397293 loss_rnnt 134.505127 hw_loss 0.000065 lr 0.00023804 rank 4
2023-02-18 00:28:13,637 DEBUG TRAIN Batch 0/11900 loss 117.684715 loss_att 203.631210 loss_ctc 113.435593 loss_rnnt 101.012657 hw_loss 0.092453 lr 0.00023804 rank 5
2023-02-18 00:28:13,638 DEBUG TRAIN Batch 0/11900 loss 107.858948 loss_att 178.779205 loss_ctc 111.011673 loss_rnnt 93.254509 hw_loss 0.000065 lr 0.00023804 rank 6
2023-02-18 00:28:13,638 DEBUG TRAIN Batch 0/11900 loss 189.941406 loss_att 278.601074 loss_ctc 188.928787 loss_rnnt 172.270462 hw_loss 0.138767 lr 0.00023804 rank 7
2023-02-18 00:28:13,642 DEBUG TRAIN Batch 0/11900 loss 153.481689 loss_att 247.020172 loss_ctc 158.604599 loss_rnnt 134.034760 hw_loss 0.105326 lr 0.00023804 rank 2
2023-02-18 00:29:15,811 DEBUG TRAIN Batch 0/12000 loss 165.503006 loss_att 256.894653 loss_ctc 172.251465 loss_rnnt 146.253647 hw_loss 0.133576 lr 0.00024004 rank 5
2023-02-18 00:29:15,811 DEBUG TRAIN Batch 0/12000 loss 133.958359 loss_att 275.361572 loss_ctc 124.367287 loss_rnnt 106.911621 hw_loss 0.084196 lr 0.00024004 rank 0
2023-02-18 00:29:15,818 DEBUG TRAIN Batch 0/12000 loss 211.233673 loss_att 244.477997 loss_ctc 240.528564 loss_rnnt 200.498932 hw_loss 0.337283 lr 0.00024004 rank 7
2023-02-18 00:29:15,819 DEBUG TRAIN Batch 0/12000 loss 123.209000 loss_att 203.924103 loss_ctc 122.647873 loss_rnnt 107.140770 hw_loss 0.000053 lr 0.00024004 rank 1
2023-02-18 00:29:15,820 DEBUG TRAIN Batch 0/12000 loss 114.641693 loss_att 244.155884 loss_ctc 119.129547 loss_rnnt 88.140442 hw_loss 0.000053 lr 0.00024004 rank 3
2023-02-18 00:29:15,821 DEBUG TRAIN Batch 0/12000 loss 143.322067 loss_att 247.948639 loss_ctc 140.135117 loss_rnnt 122.765633 hw_loss 0.105102 lr 0.00024004 rank 2
2023-02-18 00:29:15,823 DEBUG TRAIN Batch 0/12000 loss 122.769516 loss_att 245.440308 loss_ctc 123.317177 loss_rnnt 98.121056 hw_loss 0.077409 lr 0.00024004 rank 4
2023-02-18 00:29:15,825 DEBUG TRAIN Batch 0/12000 loss 102.048126 loss_att 192.950302 loss_ctc 94.218582 loss_rnnt 84.753632 hw_loss 0.296250 lr 0.00024004 rank 6
2023-02-18 00:30:33,750 DEBUG TRAIN Batch 0/12100 loss 73.228539 loss_att 181.915878 loss_ctc 53.709484 loss_rnnt 53.985394 hw_loss 0.202907 lr 0.00024204 rank 5
2023-02-18 00:30:33,754 DEBUG TRAIN Batch 0/12100 loss 132.562973 loss_att 232.437668 loss_ctc 138.663040 loss_rnnt 111.735237 hw_loss 0.073976 lr 0.00024204 rank 1
2023-02-18 00:30:33,756 DEBUG TRAIN Batch 0/12100 loss 89.647820 loss_att 159.416046 loss_ctc 86.269775 loss_rnnt 76.063507 hw_loss 0.152011 lr 0.00024204 rank 3
2023-02-18 00:30:33,756 DEBUG TRAIN Batch 0/12100 loss 112.709541 loss_att 167.215134 loss_ctc 111.737442 loss_rnnt 101.929283 hw_loss 0.016427 lr 0.00024204 rank 4
2023-02-18 00:30:33,757 DEBUG TRAIN Batch 0/12100 loss 118.790710 loss_att 204.476685 loss_ctc 123.007141 loss_rnnt 100.994247 hw_loss 0.182029 lr 0.00024204 rank 0
2023-02-18 00:30:33,761 DEBUG TRAIN Batch 0/12100 loss 202.995728 loss_att 342.100739 loss_ctc 214.819641 loss_rnnt 173.598145 hw_loss 0.000129 lr 0.00024204 rank 2
2023-02-18 00:30:33,766 DEBUG TRAIN Batch 0/12100 loss 96.377922 loss_att 166.377670 loss_ctc 99.495529 loss_rnnt 81.869720 hw_loss 0.173577 lr 0.00024204 rank 7
2023-02-18 00:30:33,772 DEBUG TRAIN Batch 0/12100 loss 147.640274 loss_att 265.647034 loss_ctc 142.750137 loss_rnnt 124.690880 hw_loss 0.000129 lr 0.00024204 rank 6
2023-02-18 00:31:51,224 DEBUG TRAIN Batch 0/12200 loss 116.092514 loss_att 181.561584 loss_ctc 113.211243 loss_rnnt 103.382843 hw_loss 0.000051 lr 0.00024404 rank 5
2023-02-18 00:31:51,224 DEBUG TRAIN Batch 0/12200 loss 140.208649 loss_att 230.095139 loss_ctc 136.458969 loss_rnnt 122.680481 hw_loss 0.095286 lr 0.00024404 rank 2
2023-02-18 00:31:51,226 DEBUG TRAIN Batch 0/12200 loss 115.116478 loss_att 192.369019 loss_ctc 102.440620 loss_rnnt 101.203514 hw_loss 0.286062 lr 0.00024404 rank 3
2023-02-18 00:31:51,227 DEBUG TRAIN Batch 0/12200 loss 78.057739 loss_att 173.883041 loss_ctc 72.289818 loss_rnnt 59.633965 hw_loss 0.052073 lr 0.00024404 rank 4
2023-02-18 00:31:51,228 DEBUG TRAIN Batch 0/12200 loss 177.606186 loss_att 270.118347 loss_ctc 162.819305 loss_rnnt 160.984558 hw_loss 0.170184 lr 0.00024404 rank 6
2023-02-18 00:31:51,233 DEBUG TRAIN Batch 0/12200 loss 145.930328 loss_att 259.229370 loss_ctc 135.643005 loss_rnnt 124.560577 hw_loss 0.152982 lr 0.00024404 rank 7
2023-02-18 00:31:51,234 DEBUG TRAIN Batch 0/12200 loss 185.062424 loss_att 273.678345 loss_ctc 189.282867 loss_rnnt 166.765060 hw_loss 0.021464 lr 0.00024404 rank 1
2023-02-18 00:31:51,285 DEBUG TRAIN Batch 0/12200 loss 114.963783 loss_att 210.663773 loss_ctc 107.624290 loss_rnnt 96.802353 hw_loss 0.000051 lr 0.00024404 rank 0
2023-02-18 00:32:52,428 DEBUG TRAIN Batch 0/12300 loss 124.581421 loss_att 228.369446 loss_ctc 108.442505 loss_rnnt 105.931030 hw_loss 0.083691 lr 0.00024604 rank 5
2023-02-18 00:32:52,433 DEBUG TRAIN Batch 0/12300 loss 107.547585 loss_att 209.309265 loss_ctc 97.808273 loss_rnnt 88.493790 hw_loss 0.000044 lr 0.00024604 rank 3
2023-02-18 00:32:52,437 DEBUG TRAIN Batch 0/12300 loss 162.276978 loss_att 206.596207 loss_ctc 178.309998 loss_rnnt 151.183441 hw_loss 0.172413 lr 0.00024604 rank 1
2023-02-18 00:32:52,437 DEBUG TRAIN Batch 0/12300 loss 130.083313 loss_att 256.506622 loss_ctc 125.233559 loss_rnnt 105.332626 hw_loss 0.211274 lr 0.00024604 rank 4
2023-02-18 00:32:52,438 DEBUG TRAIN Batch 0/12300 loss 106.034904 loss_att 230.759430 loss_ctc 102.895325 loss_rnnt 81.400955 hw_loss 0.201864 lr 0.00024604 rank 0
2023-02-18 00:32:52,438 DEBUG TRAIN Batch 0/12300 loss 114.310776 loss_att 221.954132 loss_ctc 99.990807 loss_rnnt 94.691406 hw_loss 0.000044 lr 0.00024604 rank 6
2023-02-18 00:32:52,439 DEBUG TRAIN Batch 0/12300 loss 178.315155 loss_att 302.868744 loss_ctc 186.872375 loss_rnnt 152.263443 hw_loss 0.000044 lr 0.00024604 rank 7
2023-02-18 00:32:52,446 DEBUG TRAIN Batch 0/12300 loss 161.329697 loss_att 297.079803 loss_ctc 152.475555 loss_rnnt 135.360199 hw_loss 0.000044 lr 0.00024604 rank 2
2023-02-18 00:33:50,969 DEBUG TRAIN Batch 0/12400 loss 142.473328 loss_att 226.554382 loss_ctc 144.059662 loss_rnnt 125.438919 hw_loss 0.012495 lr 0.00024804 rank 7
2023-02-18 00:33:50,971 DEBUG TRAIN Batch 0/12400 loss 107.688042 loss_att 183.552124 loss_ctc 105.458893 loss_rnnt 92.727592 hw_loss 0.159089 lr 0.00024804 rank 4
2023-02-18 00:33:50,971 DEBUG TRAIN Batch 0/12400 loss 141.625900 loss_att 237.010513 loss_ctc 152.473846 loss_rnnt 121.102554 hw_loss 0.000047 lr 0.00024804 rank 1
2023-02-18 00:33:50,972 DEBUG TRAIN Batch 0/12400 loss 92.024986 loss_att 138.454224 loss_ctc 90.897774 loss_rnnt 82.765968 hw_loss 0.231487 lr 0.00024804 rank 6
2023-02-18 00:33:50,973 DEBUG TRAIN Batch 0/12400 loss 99.760124 loss_att 176.130539 loss_ctc 100.484131 loss_rnnt 84.279518 hw_loss 0.206194 lr 0.00024804 rank 0
2023-02-18 00:33:50,973 DEBUG TRAIN Batch 0/12400 loss 189.372330 loss_att 267.453278 loss_ctc 186.151505 loss_rnnt 174.132721 hw_loss 0.099125 lr 0.00024804 rank 5
2023-02-18 00:33:50,978 DEBUG TRAIN Batch 0/12400 loss 54.838524 loss_att 94.409645 loss_ctc 59.219547 loss_rnnt 46.211887 hw_loss 0.240520 lr 0.00024804 rank 2
2023-02-18 00:33:50,983 DEBUG TRAIN Batch 0/12400 loss 131.471466 loss_att 236.102890 loss_ctc 136.694290 loss_rnnt 109.840210 hw_loss 0.016100 lr 0.00024804 rank 3
2023-02-18 00:34:52,239 DEBUG TRAIN Batch 0/12500 loss 97.676521 loss_att 207.498169 loss_ctc 80.981728 loss_rnnt 77.913948 hw_loss 0.045401 lr 0.00025004 rank 5
2023-02-18 00:34:52,239 DEBUG TRAIN Batch 0/12500 loss 257.482208 loss_att 383.175171 loss_ctc 273.040680 loss_rnnt 230.190582 hw_loss 0.147336 lr 0.00025004 rank 0
2023-02-18 00:34:52,240 DEBUG TRAIN Batch 0/12500 loss 86.565788 loss_att 177.645203 loss_ctc 96.500076 loss_rnnt 67.015244 hw_loss 0.018902 lr 0.00025004 rank 3
2023-02-18 00:34:52,239 DEBUG TRAIN Batch 0/12500 loss 147.875793 loss_att 216.793472 loss_ctc 160.609299 loss_rnnt 132.394409 hw_loss 0.000093 lr 0.00025004 rank 1
2023-02-18 00:34:52,246 DEBUG TRAIN Batch 0/12500 loss 169.308044 loss_att 252.205612 loss_ctc 175.324310 loss_rnnt 151.916199 hw_loss 0.019066 lr 0.00025004 rank 6
2023-02-18 00:34:52,249 DEBUG TRAIN Batch 0/12500 loss 125.236053 loss_att 191.526733 loss_ctc 135.045105 loss_rnnt 110.626808 hw_loss 0.081056 lr 0.00025004 rank 2
2023-02-18 00:34:52,254 DEBUG TRAIN Batch 0/12500 loss 149.649979 loss_att 235.374130 loss_ctc 142.553802 loss_rnnt 133.359497 hw_loss 0.172163 lr 0.00025004 rank 7
2023-02-18 00:34:52,303 DEBUG TRAIN Batch 0/12500 loss 191.713638 loss_att 296.538177 loss_ctc 197.397308 loss_rnnt 169.921112 hw_loss 0.130831 lr 0.00025004 rank 4
2023-02-18 00:35:52,031 DEBUG TRAIN Batch 0/12600 loss 109.729774 loss_att 208.018616 loss_ctc 105.013573 loss_rnnt 90.621033 hw_loss 0.149639 lr 0.00025204 rank 5
2023-02-18 00:35:52,034 DEBUG TRAIN Batch 0/12600 loss 150.674652 loss_att 259.512756 loss_ctc 160.888275 loss_rnnt 127.414360 hw_loss 0.245375 lr 0.00025204 rank 1
2023-02-18 00:35:52,039 DEBUG TRAIN Batch 0/12600 loss 166.597855 loss_att 263.199432 loss_ctc 175.113190 loss_rnnt 146.142120 hw_loss 0.000070 lr 0.00025204 rank 6
2023-02-18 00:35:52,039 DEBUG TRAIN Batch 0/12600 loss 162.894470 loss_att 306.100403 loss_ctc 160.809296 loss_rnnt 134.515839 hw_loss 0.028984 lr 0.00025204 rank 3
2023-02-18 00:35:52,044 DEBUG TRAIN Batch 0/12600 loss 128.136063 loss_att 214.447189 loss_ctc 127.340950 loss_rnnt 110.979805 hw_loss 0.000070 lr 0.00025204 rank 7
2023-02-18 00:35:52,044 DEBUG TRAIN Batch 0/12600 loss 146.990677 loss_att 273.848419 loss_ctc 141.305069 loss_rnnt 122.345589 hw_loss 0.059262 lr 0.00025204 rank 2
2023-02-18 00:35:52,044 DEBUG TRAIN Batch 0/12600 loss 185.416595 loss_att 312.100616 loss_ctc 188.037842 loss_rnnt 159.572968 hw_loss 0.294999 lr 0.00025204 rank 0
2023-02-18 00:35:52,095 DEBUG TRAIN Batch 0/12600 loss 191.710922 loss_att 248.973785 loss_ctc 197.363785 loss_rnnt 179.504608 hw_loss 0.000070 lr 0.00025204 rank 4
2023-02-18 00:36:49,493 DEBUG TRAIN Batch 0/12700 loss 105.854828 loss_att 169.413254 loss_ctc 102.589043 loss_rnnt 93.524719 hw_loss 0.101002 lr 0.00025404 rank 6
2023-02-18 00:36:49,492 DEBUG TRAIN Batch 0/12700 loss 104.405998 loss_att 207.266342 loss_ctc 100.451973 loss_rnnt 84.289871 hw_loss 0.133605 lr 0.00025404 rank 5
2023-02-18 00:36:49,495 DEBUG TRAIN Batch 0/12700 loss 187.532318 loss_att 265.317139 loss_ctc 207.670685 loss_rnnt 169.290176 hw_loss 0.000068 lr 0.00025404 rank 1
2023-02-18 00:36:49,497 DEBUG TRAIN Batch 0/12700 loss 153.881989 loss_att 260.692322 loss_ctc 150.257950 loss_rnnt 132.888947 hw_loss 0.214079 lr 0.00025404 rank 3
2023-02-18 00:36:49,500 DEBUG TRAIN Batch 0/12700 loss 93.047272 loss_att 186.842972 loss_ctc 81.169380 loss_rnnt 75.847832 hw_loss 0.045050 lr 0.00025404 rank 0
2023-02-18 00:36:49,502 DEBUG TRAIN Batch 0/12700 loss 113.001152 loss_att 162.608551 loss_ctc 113.064011 loss_rnnt 102.985214 hw_loss 0.161393 lr 0.00025404 rank 2
2023-02-18 00:36:49,502 DEBUG TRAIN Batch 0/12700 loss 114.688461 loss_att 190.755264 loss_ctc 127.602020 loss_rnnt 97.753250 hw_loss 0.000068 lr 0.00025404 rank 7
2023-02-18 00:36:49,505 DEBUG TRAIN Batch 0/12700 loss 118.958679 loss_att 213.989700 loss_ctc 114.601120 loss_rnnt 100.492096 hw_loss 0.077587 lr 0.00025404 rank 4
2023-02-18 00:37:50,670 DEBUG TRAIN Batch 0/12800 loss 93.389160 loss_att 200.209503 loss_ctc 83.611389 loss_rnnt 73.328751 hw_loss 0.000085 lr 0.00025604 rank 1
2023-02-18 00:37:50,671 DEBUG TRAIN Batch 0/12800 loss 131.574966 loss_att 231.565063 loss_ctc 124.432213 loss_rnnt 112.529266 hw_loss 0.000085 lr 0.00025604 rank 7
2023-02-18 00:37:50,673 DEBUG TRAIN Batch 0/12800 loss 141.783829 loss_att 228.981995 loss_ctc 141.652237 loss_rnnt 124.361694 hw_loss 0.000085 lr 0.00025604 rank 6
2023-02-18 00:37:50,674 DEBUG TRAIN Batch 0/12800 loss 199.026169 loss_att 245.759903 loss_ctc 220.920822 loss_rnnt 186.598572 hw_loss 0.302917 lr 0.00025604 rank 0
2023-02-18 00:37:50,674 DEBUG TRAIN Batch 0/12800 loss 100.317444 loss_att 179.626221 loss_ctc 104.521339 loss_rnnt 83.822762 hw_loss 0.135757 lr 0.00025604 rank 5
2023-02-18 00:37:50,678 DEBUG TRAIN Batch 0/12800 loss 133.939621 loss_att 228.514282 loss_ctc 133.534698 loss_rnnt 115.078629 hw_loss 0.000085 lr 0.00025604 rank 4
2023-02-18 00:37:50,678 DEBUG TRAIN Batch 0/12800 loss 99.976753 loss_att 231.966431 loss_ctc 95.580460 loss_rnnt 74.145615 hw_loss 0.036314 lr 0.00025604 rank 2
2023-02-18 00:37:50,678 DEBUG TRAIN Batch 0/12800 loss 102.732773 loss_att 180.972397 loss_ctc 95.375656 loss_rnnt 88.065750 hw_loss 0.000085 lr 0.00025604 rank 3
2023-02-18 00:39:08,765 DEBUG TRAIN Batch 0/12900 loss 121.854195 loss_att 260.009003 loss_ctc 125.261307 loss_rnnt 93.768898 hw_loss 0.000092 lr 0.00025804 rank 0
2023-02-18 00:39:08,765 DEBUG TRAIN Batch 0/12900 loss 131.164078 loss_att 296.218079 loss_ctc 110.254997 loss_rnnt 100.927338 hw_loss 0.025899 lr 0.00025804 rank 6
2023-02-18 00:39:08,768 DEBUG TRAIN Batch 0/12900 loss 64.297173 loss_att 148.763397 loss_ctc 52.711983 loss_rnnt 48.861332 hw_loss 0.163668 lr 0.00025804 rank 5
2023-02-18 00:39:08,770 DEBUG TRAIN Batch 0/12900 loss 49.755180 loss_att 67.824944 loss_ctc 51.420673 loss_rnnt 45.886417 hw_loss 0.061399 lr 0.00025804 rank 3
2023-02-18 00:39:08,771 DEBUG TRAIN Batch 0/12900 loss 130.391953 loss_att 226.590622 loss_ctc 135.463516 loss_rnnt 110.445496 hw_loss 0.057223 lr 0.00025804 rank 2
2023-02-18 00:39:08,785 DEBUG TRAIN Batch 0/12900 loss 93.189613 loss_att 220.916382 loss_ctc 69.629333 loss_rnnt 70.658524 hw_loss 0.238327 lr 0.00025804 rank 1
2023-02-18 00:39:08,803 DEBUG TRAIN Batch 0/12900 loss 145.234268 loss_att 250.039825 loss_ctc 154.832031 loss_rnnt 122.966614 hw_loss 0.050335 lr 0.00025804 rank 4
2023-02-18 00:39:08,851 DEBUG TRAIN Batch 0/12900 loss 103.572151 loss_att 185.801880 loss_ctc 93.997452 loss_rnnt 88.402779 hw_loss 0.000092 lr 0.00025804 rank 7
2023-02-18 00:40:25,183 DEBUG TRAIN Batch 0/13000 loss 29.536793 loss_att 36.319099 loss_ctc 30.057417 loss_rnnt 27.935175 hw_loss 0.329513 lr 0.00026004 rank 5
2023-02-18 00:40:25,195 DEBUG TRAIN Batch 0/13000 loss 137.516357 loss_att 253.810883 loss_ctc 137.520599 loss_rnnt 114.176537 hw_loss 0.150690 lr 0.00026004 rank 7
2023-02-18 00:40:25,197 DEBUG TRAIN Batch 0/13000 loss 123.734047 loss_att 194.760101 loss_ctc 119.234009 loss_rnnt 110.059296 hw_loss 0.130409 lr 0.00026004 rank 4
2023-02-18 00:40:25,200 DEBUG TRAIN Batch 0/13000 loss 138.191269 loss_att 246.234192 loss_ctc 138.360184 loss_rnnt 116.509216 hw_loss 0.095505 lr 0.00026004 rank 3
2023-02-18 00:40:25,205 DEBUG TRAIN Batch 0/13000 loss 173.007339 loss_att 278.305725 loss_ctc 169.429611 loss_rnnt 152.424576 hw_loss 0.000204 lr 0.00026004 rank 1
2023-02-18 00:40:25,225 DEBUG TRAIN Batch 0/13000 loss 62.845524 loss_att 121.764816 loss_ctc 61.775558 loss_rnnt 51.112411 hw_loss 0.172335 lr 0.00026004 rank 6
2023-02-18 00:40:25,238 DEBUG TRAIN Batch 0/13000 loss 114.352341 loss_att 219.185989 loss_ctc 121.815506 loss_rnnt 92.390411 hw_loss 0.000204 lr 0.00026004 rank 0
2023-02-18 00:40:25,269 DEBUG TRAIN Batch 0/13000 loss 99.167007 loss_att 167.692703 loss_ctc 92.845840 loss_rnnt 86.233276 hw_loss 0.133899 lr 0.00026004 rank 2
2023-02-18 00:41:27,086 DEBUG TRAIN Batch 0/13100 loss 128.369827 loss_att 221.522079 loss_ctc 124.419556 loss_rnnt 110.222389 hw_loss 0.081893 lr 0.00026204 rank 5
2023-02-18 00:41:27,089 DEBUG TRAIN Batch 0/13100 loss 108.546555 loss_att 199.861755 loss_ctc 111.778801 loss_rnnt 89.852509 hw_loss 0.000084 lr 0.00026204 rank 2
2023-02-18 00:41:27,089 DEBUG TRAIN Batch 0/13100 loss 152.503403 loss_att 297.493744 loss_ctc 139.466400 loss_rnnt 125.243561 hw_loss 0.000084 lr 0.00026204 rank 0
2023-02-18 00:41:27,094 DEBUG TRAIN Batch 0/13100 loss 108.740067 loss_att 214.998413 loss_ctc 108.159874 loss_rnnt 87.462143 hw_loss 0.194277 lr 0.00026204 rank 1
2023-02-18 00:41:27,094 DEBUG TRAIN Batch 0/13100 loss 132.002731 loss_att 213.688492 loss_ctc 133.494446 loss_rnnt 115.431534 hw_loss 0.065895 lr 0.00026204 rank 6
2023-02-18 00:41:27,093 DEBUG TRAIN Batch 0/13100 loss 156.811203 loss_att 210.655594 loss_ctc 161.200531 loss_rnnt 145.368576 hw_loss 0.165945 lr 0.00026204 rank 4
2023-02-18 00:41:27,096 DEBUG TRAIN Batch 0/13100 loss 222.982346 loss_att 284.179169 loss_ctc 247.063202 loss_rnnt 207.532166 hw_loss 0.000084 lr 0.00026204 rank 3
2023-02-18 00:41:27,100 DEBUG TRAIN Batch 0/13100 loss 119.425850 loss_att 241.758148 loss_ctc 125.343323 loss_rnnt 94.170349 hw_loss 0.000084 lr 0.00026204 rank 7
2023-02-18 00:42:26,629 DEBUG TRAIN Batch 0/13200 loss 101.181694 loss_att 180.098114 loss_ctc 96.576202 loss_rnnt 85.885361 hw_loss 0.238340 lr 0.00026404 rank 1
2023-02-18 00:42:26,634 DEBUG TRAIN Batch 0/13200 loss 64.175629 loss_att 95.844658 loss_ctc 69.605278 loss_rnnt 57.000427 hw_loss 0.220204 lr 0.00026404 rank 0
2023-02-18 00:42:26,635 DEBUG TRAIN Batch 0/13200 loss 188.235062 loss_att 269.753204 loss_ctc 189.202286 loss_rnnt 171.802429 hw_loss 0.000087 lr 0.00026404 rank 5
2023-02-18 00:42:26,635 DEBUG TRAIN Batch 0/13200 loss 63.250504 loss_att 123.448860 loss_ctc 61.935791 loss_rnnt 51.313927 hw_loss 0.135375 lr 0.00026404 rank 3
2023-02-18 00:42:26,639 DEBUG TRAIN Batch 0/13200 loss 121.290779 loss_att 215.166748 loss_ctc 123.078636 loss_rnnt 102.201340 hw_loss 0.142264 lr 0.00026404 rank 6
2023-02-18 00:42:26,640 DEBUG TRAIN Batch 0/13200 loss 62.950077 loss_att 170.022888 loss_ctc 45.844521 loss_rnnt 43.816200 hw_loss 0.000087 lr 0.00026404 rank 2
2023-02-18 00:42:26,647 DEBUG TRAIN Batch 0/13200 loss 124.977448 loss_att 272.639130 loss_ctc 129.898361 loss_rnnt 94.729080 hw_loss 0.112296 lr 0.00026404 rank 4
2023-02-18 00:42:26,651 DEBUG TRAIN Batch 0/13200 loss 139.734177 loss_att 265.808105 loss_ctc 141.583405 loss_rnnt 114.197861 hw_loss 0.140585 lr 0.00026404 rank 7
2023-02-18 00:43:25,690 DEBUG TRAIN Batch 0/13300 loss 67.903580 loss_att 106.396820 loss_ctc 76.528717 loss_rnnt 58.990852 hw_loss 0.120097 lr 0.00026604 rank 5
2023-02-18 00:43:25,690 DEBUG TRAIN Batch 0/13300 loss 100.783882 loss_att 192.938812 loss_ctc 111.244156 loss_rnnt 80.950684 hw_loss 0.014095 lr 0.00026604 rank 4
2023-02-18 00:43:25,694 DEBUG TRAIN Batch 0/13300 loss 115.379868 loss_att 218.617523 loss_ctc 115.281097 loss_rnnt 94.729202 hw_loss 0.030561 lr 0.00026604 rank 0
2023-02-18 00:43:25,694 DEBUG TRAIN Batch 0/13300 loss 60.303585 loss_att 91.020844 loss_ctc 61.612450 loss_rnnt 53.944080 hw_loss 0.077888 lr 0.00026604 rank 1
2023-02-18 00:43:25,696 DEBUG TRAIN Batch 0/13300 loss 91.527298 loss_att 173.496170 loss_ctc 87.693474 loss_rnnt 75.634537 hw_loss 0.019049 lr 0.00026604 rank 6
2023-02-18 00:43:25,696 DEBUG TRAIN Batch 0/13300 loss 144.032623 loss_att 251.633209 loss_ctc 152.431183 loss_rnnt 121.392677 hw_loss 0.000055 lr 0.00026604 rank 3
2023-02-18 00:43:25,698 DEBUG TRAIN Batch 0/13300 loss 106.195419 loss_att 204.006485 loss_ctc 107.122429 loss_rnnt 86.442764 hw_loss 0.125321 lr 0.00026604 rank 7
2023-02-18 00:43:25,756 DEBUG TRAIN Batch 0/13300 loss 112.713196 loss_att 215.150879 loss_ctc 109.493454 loss_rnnt 92.611710 hw_loss 0.081053 lr 0.00026604 rank 2
2023-02-18 00:44:26,022 DEBUG TRAIN Batch 0/13400 loss 101.878258 loss_att 193.089905 loss_ctc 91.486221 loss_rnnt 84.996284 hw_loss 0.047342 lr 0.00026804 rank 6
2023-02-18 00:44:26,027 DEBUG TRAIN Batch 0/13400 loss 121.265549 loss_att 245.558716 loss_ctc 113.525345 loss_rnnt 97.363800 hw_loss 0.140897 lr 0.00026804 rank 5
2023-02-18 00:44:26,028 DEBUG TRAIN Batch 0/13400 loss 92.858528 loss_att 152.094025 loss_ctc 89.790337 loss_rnnt 81.420494 hw_loss 0.000060 lr 0.00026804 rank 0
2023-02-18 00:44:26,028 DEBUG TRAIN Batch 0/13400 loss 104.999641 loss_att 208.647003 loss_ctc 114.179321 loss_rnnt 82.993149 hw_loss 0.099485 lr 0.00026804 rank 7
2023-02-18 00:44:26,033 DEBUG TRAIN Batch 0/13400 loss 78.197220 loss_att 188.463516 loss_ctc 69.458214 loss_rnnt 57.309128 hw_loss 0.000060 lr 0.00026804 rank 3
2023-02-18 00:44:26,034 DEBUG TRAIN Batch 0/13400 loss 141.658508 loss_att 247.912857 loss_ctc 133.266479 loss_rnnt 121.526535 hw_loss 0.000060 lr 0.00026804 rank 4
2023-02-18 00:44:26,036 DEBUG TRAIN Batch 0/13400 loss 126.722435 loss_att 261.279968 loss_ctc 120.820908 loss_rnnt 100.597763 hw_loss 0.000060 lr 0.00026804 rank 2
2023-02-18 00:44:26,087 DEBUG TRAIN Batch 0/13400 loss 157.301239 loss_att 263.836548 loss_ctc 154.791595 loss_rnnt 136.238419 hw_loss 0.169431 lr 0.00026804 rank 1
2023-02-18 00:45:25,988 DEBUG TRAIN Batch 0/13500 loss 127.053520 loss_att 250.992249 loss_ctc 128.840240 loss_rnnt 102.027519 hw_loss 0.000051 lr 0.00027004 rank 2
2023-02-18 00:45:25,989 DEBUG TRAIN Batch 0/13500 loss 39.697704 loss_att 64.268974 loss_ctc 41.581245 loss_rnnt 34.423004 hw_loss 0.204955 lr 0.00027004 rank 4
2023-02-18 00:45:25,990 DEBUG TRAIN Batch 0/13500 loss 151.305328 loss_att 266.425110 loss_ctc 145.492706 loss_rnnt 129.055511 hw_loss 0.001643 lr 0.00027004 rank 1
2023-02-18 00:45:25,990 DEBUG TRAIN Batch 0/13500 loss 159.229477 loss_att 258.554779 loss_ctc 165.582245 loss_rnnt 138.517349 hw_loss 0.000051 lr 0.00027004 rank 5
2023-02-18 00:45:25,991 DEBUG TRAIN Batch 0/13500 loss 139.351166 loss_att 295.714233 loss_ctc 131.470032 loss_rnnt 108.977493 hw_loss 0.284770 lr 0.00027004 rank 7
2023-02-18 00:45:25,993 DEBUG TRAIN Batch 0/13500 loss 73.449608 loss_att 128.080200 loss_ctc 69.926720 loss_rnnt 62.898659 hw_loss 0.177264 lr 0.00027004 rank 0
2023-02-18 00:45:25,995 DEBUG TRAIN Batch 0/13500 loss 66.935478 loss_att 122.914383 loss_ctc 67.796959 loss_rnnt 55.512177 hw_loss 0.211227 lr 0.00027004 rank 3
2023-02-18 00:45:25,996 DEBUG TRAIN Batch 0/13500 loss 181.427353 loss_att 270.744965 loss_ctc 185.525665 loss_rnnt 163.017365 hw_loss 0.000051 lr 0.00027004 rank 6
2023-02-18 00:46:25,331 DEBUG TRAIN Batch 0/13600 loss 68.722656 loss_att 128.802734 loss_ctc 64.953270 loss_rnnt 57.166759 hw_loss 0.079607 lr 0.00027204 rank 5
2023-02-18 00:46:25,334 DEBUG TRAIN Batch 0/13600 loss 122.648743 loss_att 204.589844 loss_ctc 123.274536 loss_rnnt 106.177010 hw_loss 0.000130 lr 0.00027204 rank 0
2023-02-18 00:46:25,334 DEBUG TRAIN Batch 0/13600 loss 138.410294 loss_att 230.015869 loss_ctc 145.081329 loss_rnnt 119.199646 hw_loss 0.000130 lr 0.00027204 rank 2
2023-02-18 00:46:25,338 DEBUG TRAIN Batch 0/13600 loss 79.122849 loss_att 135.321228 loss_ctc 81.732536 loss_rnnt 67.389450 hw_loss 0.273304 lr 0.00027204 rank 1
2023-02-18 00:46:25,339 DEBUG TRAIN Batch 0/13600 loss 157.214096 loss_att 232.131714 loss_ctc 165.921997 loss_rnnt 141.001953 hw_loss 0.126670 lr 0.00027204 rank 6
2023-02-18 00:46:25,339 DEBUG TRAIN Batch 0/13600 loss 108.221382 loss_att 233.819977 loss_ctc 101.507904 loss_rnnt 83.996719 hw_loss 0.000130 lr 0.00027204 rank 4
2023-02-18 00:46:25,339 DEBUG TRAIN Batch 0/13600 loss 88.898170 loss_att 182.295105 loss_ctc 90.988121 loss_rnnt 69.900253 hw_loss 0.074755 lr 0.00027204 rank 3
2023-02-18 00:46:25,340 DEBUG TRAIN Batch 0/13600 loss 116.069672 loss_att 199.720840 loss_ctc 118.922012 loss_rnnt 98.842087 hw_loss 0.219439 lr 0.00027204 rank 7
2023-02-18 00:47:26,860 DEBUG TRAIN Batch 0/13700 loss 122.947289 loss_att 246.971771 loss_ctc 128.497055 loss_rnnt 97.402374 hw_loss 0.000083 lr 0.00027404 rank 4
2023-02-18 00:47:26,866 DEBUG TRAIN Batch 0/13700 loss 76.649521 loss_att 183.996246 loss_ctc 76.355988 loss_rnnt 55.219269 hw_loss 0.000083 lr 0.00027404 rank 5
2023-02-18 00:47:26,870 DEBUG TRAIN Batch 0/13700 loss 188.882629 loss_att 314.437439 loss_ctc 202.733170 loss_rnnt 161.921417 hw_loss 0.006577 lr 0.00027404 rank 7
2023-02-18 00:47:26,871 DEBUG TRAIN Batch 0/13700 loss 175.924805 loss_att 306.895355 loss_ctc 179.335617 loss_rnnt 149.204758 hw_loss 0.133448 lr 0.00027404 rank 6
2023-02-18 00:47:26,874 DEBUG TRAIN Batch 0/13700 loss 123.901512 loss_att 248.056442 loss_ctc 129.505295 loss_rnnt 98.229652 hw_loss 0.175681 lr 0.00027404 rank 2
2023-02-18 00:47:26,881 DEBUG TRAIN Batch 0/13700 loss 102.235916 loss_att 222.209778 loss_ctc 94.741035 loss_rnnt 79.132835 hw_loss 0.201795 lr 0.00027404 rank 3
2023-02-18 00:47:26,920 DEBUG TRAIN Batch 0/13700 loss 124.685089 loss_att 255.293152 loss_ctc 120.903671 loss_rnnt 99.067619 hw_loss 0.000083 lr 0.00027404 rank 1
2023-02-18 00:47:26,921 DEBUG TRAIN Batch 0/13700 loss 89.425507 loss_att 226.361084 loss_ctc 76.342033 loss_rnnt 63.715462 hw_loss 0.126339 lr 0.00027404 rank 0
2023-02-18 00:48:45,732 DEBUG TRAIN Batch 0/13800 loss 79.240303 loss_att 118.869217 loss_ctc 93.443199 loss_rnnt 69.357758 hw_loss 0.118206 lr 0.00027604 rank 4
2023-02-18 00:48:45,733 DEBUG TRAIN Batch 0/13800 loss 109.849449 loss_att 172.369019 loss_ctc 110.865814 loss_rnnt 97.209976 hw_loss 0.000075 lr 0.00027604 rank 5
2023-02-18 00:48:45,736 DEBUG TRAIN Batch 0/13800 loss 148.528168 loss_att 226.965912 loss_ctc 144.827026 loss_rnnt 133.334061 hw_loss 0.000075 lr 0.00027604 rank 3
2023-02-18 00:48:45,736 DEBUG TRAIN Batch 0/13800 loss 118.958389 loss_att 265.335632 loss_ctc 102.095947 loss_rnnt 91.845085 hw_loss 0.161599 lr 0.00027604 rank 1
2023-02-18 00:48:45,744 DEBUG TRAIN Batch 0/13800 loss 37.634747 loss_att 57.175529 loss_ctc 39.929745 loss_rnnt 33.374363 hw_loss 0.086667 lr 0.00027604 rank 7
2023-02-18 00:48:45,751 DEBUG TRAIN Batch 0/13800 loss 146.862244 loss_att 259.254639 loss_ctc 154.724915 loss_rnnt 123.335381 hw_loss 0.000075 lr 0.00027604 rank 6
2023-02-18 00:48:45,761 DEBUG TRAIN Batch 0/13800 loss 76.741699 loss_att 173.523758 loss_ctc 69.537056 loss_rnnt 58.345871 hw_loss 0.000075 lr 0.00027604 rank 0
2023-02-18 00:48:45,783 DEBUG TRAIN Batch 0/13800 loss 117.647102 loss_att 266.798828 loss_ctc 103.085770 loss_rnnt 89.688507 hw_loss 0.130789 lr 0.00027604 rank 2
2023-02-18 00:50:02,280 DEBUG TRAIN Batch 0/13900 loss 65.465660 loss_att 146.745575 loss_ctc 60.411156 loss_rnnt 49.799767 hw_loss 0.157221 lr 0.00027804 rank 1
2023-02-18 00:50:02,279 DEBUG TRAIN Batch 0/13900 loss 117.631996 loss_att 237.424210 loss_ctc 121.049988 loss_rnnt 93.173126 hw_loss 0.083817 lr 0.00027804 rank 6
2023-02-18 00:50:02,283 DEBUG TRAIN Batch 0/13900 loss 82.442284 loss_att 186.458557 loss_ctc 74.597130 loss_rnnt 62.634224 hw_loss 0.095288 lr 0.00027804 rank 5
2023-02-18 00:50:02,284 DEBUG TRAIN Batch 0/13900 loss 111.645508 loss_att 206.779724 loss_ctc 102.049767 loss_rnnt 93.815140 hw_loss 0.155519 lr 0.00027804 rank 4
2023-02-18 00:50:02,286 DEBUG TRAIN Batch 0/13900 loss 105.104950 loss_att 221.106277 loss_ctc 105.210846 loss_rnnt 81.863495 hw_loss 0.050735 lr 0.00027804 rank 2
2023-02-18 00:50:02,287 DEBUG TRAIN Batch 0/13900 loss 107.591179 loss_att 217.126358 loss_ctc 107.166519 loss_rnnt 85.740723 hw_loss 0.000076 lr 0.00027804 rank 7
2023-02-18 00:50:02,290 DEBUG TRAIN Batch 0/13900 loss 124.201210 loss_att 228.911667 loss_ctc 124.620514 loss_rnnt 103.198013 hw_loss 0.009754 lr 0.00027804 rank 3
2023-02-18 00:50:02,297 DEBUG TRAIN Batch 0/13900 loss 160.891617 loss_att 311.362610 loss_ctc 161.199310 loss_rnnt 130.728027 hw_loss 0.053160 lr 0.00027804 rank 0
2023-02-18 00:51:03,167 DEBUG TRAIN Batch 0/14000 loss 137.905090 loss_att 259.341187 loss_ctc 128.047653 loss_rnnt 114.914009 hw_loss 0.034105 lr 0.00028004 rank 1
2023-02-18 00:51:03,167 DEBUG TRAIN Batch 0/14000 loss 116.986298 loss_att 237.065094 loss_ctc 113.650238 loss_rnnt 93.339417 hw_loss 0.142367 lr 0.00028004 rank 0
2023-02-18 00:51:03,167 DEBUG TRAIN Batch 0/14000 loss 82.321060 loss_att 163.015869 loss_ctc 90.662537 loss_rnnt 65.069855 hw_loss 0.000072 lr 0.00028004 rank 7
2023-02-18 00:51:03,171 DEBUG TRAIN Batch 0/14000 loss 112.916550 loss_att 214.782104 loss_ctc 125.397911 loss_rnnt 90.879211 hw_loss 0.000072 lr 0.00028004 rank 4
2023-02-18 00:51:03,172 DEBUG TRAIN Batch 0/14000 loss 199.883041 loss_att 296.849182 loss_ctc 186.736176 loss_rnnt 182.162338 hw_loss 0.150764 lr 0.00028004 rank 3
2023-02-18 00:51:03,172 DEBUG TRAIN Batch 0/14000 loss 118.371902 loss_att 247.995178 loss_ctc 133.366409 loss_rnnt 90.447937 hw_loss 0.000072 lr 0.00028004 rank 6
2023-02-18 00:51:03,172 DEBUG TRAIN Batch 0/14000 loss 161.932892 loss_att 278.298950 loss_ctc 166.301071 loss_rnnt 138.076721 hw_loss 0.001009 lr 0.00028004 rank 5
2023-02-18 00:51:03,174 DEBUG TRAIN Batch 0/14000 loss 97.006706 loss_att 241.987869 loss_ctc 83.018013 loss_rnnt 69.819855 hw_loss 0.104586 lr 0.00028004 rank 2
2023-02-18 00:52:01,405 DEBUG TRAIN Batch 0/14100 loss 52.608635 loss_att 86.346199 loss_ctc 51.675232 loss_rnnt 45.916782 hw_loss 0.128987 lr 0.00028204 rank 7
2023-02-18 00:52:01,405 DEBUG TRAIN Batch 0/14100 loss 74.594940 loss_att 166.876404 loss_ctc 69.854103 loss_rnnt 56.770729 hw_loss 0.000055 lr 0.00028204 rank 4
2023-02-18 00:52:01,407 DEBUG TRAIN Batch 0/14100 loss 71.904419 loss_att 192.241089 loss_ctc 57.698303 loss_rnnt 49.731201 hw_loss 0.000055 lr 0.00028204 rank 5
2023-02-18 00:52:01,409 DEBUG TRAIN Batch 0/14100 loss 123.185532 loss_att 225.304108 loss_ctc 112.581856 loss_rnnt 104.169662 hw_loss 0.011219 lr 0.00028204 rank 2
2023-02-18 00:52:01,411 DEBUG TRAIN Batch 0/14100 loss 143.529526 loss_att 270.101257 loss_ctc 150.949036 loss_rnnt 117.222717 hw_loss 0.006001 lr 0.00028204 rank 1
2023-02-18 00:52:01,411 DEBUG TRAIN Batch 0/14100 loss 119.420784 loss_att 202.348007 loss_ctc 119.882309 loss_rnnt 102.741982 hw_loss 0.059668 lr 0.00028204 rank 0
2023-02-18 00:52:01,410 DEBUG TRAIN Batch 0/14100 loss 80.110329 loss_att 213.585144 loss_ctc 56.048046 loss_rnnt 56.623638 hw_loss 0.000055 lr 0.00028204 rank 6
2023-02-18 00:52:01,413 DEBUG TRAIN Batch 0/14100 loss 104.216347 loss_att 189.061371 loss_ctc 114.968903 loss_rnnt 85.780151 hw_loss 0.062847 lr 0.00028204 rank 3
2023-02-18 00:53:01,167 DEBUG TRAIN Batch 0/14200 loss 112.512779 loss_att 205.454102 loss_ctc 120.649307 loss_rnnt 92.766655 hw_loss 0.136860 lr 0.00028404 rank 1
2023-02-18 00:53:01,169 DEBUG TRAIN Batch 0/14200 loss 163.086395 loss_att 217.012543 loss_ctc 161.018845 loss_rnnt 152.576797 hw_loss 0.000078 lr 0.00028404 rank 5
2023-02-18 00:53:01,169 DEBUG TRAIN Batch 0/14200 loss 71.029724 loss_att 177.379105 loss_ctc 70.073578 loss_rnnt 49.887287 hw_loss 0.000078 lr 0.00028404 rank 2
2023-02-18 00:53:01,169 DEBUG TRAIN Batch 0/14200 loss 111.724953 loss_att 239.107468 loss_ctc 114.360214 loss_rnnt 85.868927 hw_loss 0.052775 lr 0.00028404 rank 4
2023-02-18 00:53:01,172 DEBUG TRAIN Batch 0/14200 loss 131.222397 loss_att 260.640564 loss_ctc 124.964935 loss_rnnt 106.173050 hw_loss 0.000078 lr 0.00028404 rank 6
2023-02-18 00:53:01,174 DEBUG TRAIN Batch 0/14200 loss 85.399155 loss_att 220.068604 loss_ctc 80.477966 loss_rnnt 59.121384 hw_loss 0.000078 lr 0.00028404 rank 3
2023-02-18 00:53:01,175 DEBUG TRAIN Batch 0/14200 loss 107.112129 loss_att 211.366364 loss_ctc 102.810783 loss_rnnt 86.764053 hw_loss 0.132620 lr 0.00028404 rank 7
2023-02-18 00:53:01,179 DEBUG TRAIN Batch 0/14200 loss 129.559738 loss_att 267.332397 loss_ctc 119.499634 loss_rnnt 103.236618 hw_loss 0.206125 lr 0.00028404 rank 0
2023-02-18 00:54:01,772 DEBUG TRAIN Batch 0/14300 loss 77.081306 loss_att 212.854889 loss_ctc 68.552170 loss_rnnt 51.063782 hw_loss 0.000064 lr 0.00028604 rank 1
2023-02-18 00:54:01,772 DEBUG TRAIN Batch 0/14300 loss 143.041138 loss_att 281.879944 loss_ctc 142.000031 loss_rnnt 115.407654 hw_loss 0.008489 lr 0.00028604 rank 2
2023-02-18 00:54:01,773 DEBUG TRAIN Batch 0/14300 loss 130.121582 loss_att 261.313843 loss_ctc 121.890877 loss_rnnt 104.980507 hw_loss 0.000064 lr 0.00028604 rank 5
2023-02-18 00:54:01,776 DEBUG TRAIN Batch 0/14300 loss 88.134079 loss_att 203.375061 loss_ctc 85.759109 loss_rnnt 65.311996 hw_loss 0.169769 lr 0.00028604 rank 7
2023-02-18 00:54:01,776 DEBUG TRAIN Batch 0/14300 loss 109.625099 loss_att 235.589172 loss_ctc 111.710747 loss_rnnt 84.154167 hw_loss 0.000064 lr 0.00028604 rank 6
2023-02-18 00:54:01,776 DEBUG TRAIN Batch 0/14300 loss 93.520927 loss_att 216.196106 loss_ctc 82.485603 loss_rnnt 70.432426 hw_loss 0.046575 lr 0.00028604 rank 3
2023-02-18 00:54:01,777 DEBUG TRAIN Batch 0/14300 loss 72.541161 loss_att 197.819641 loss_ctc 66.436295 loss_rnnt 48.299419 hw_loss 0.000064 lr 0.00028604 rank 0
2023-02-18 00:54:01,778 DEBUG TRAIN Batch 0/14300 loss 148.242249 loss_att 250.029388 loss_ctc 160.535278 loss_rnnt 126.245697 hw_loss 0.000064 lr 0.00028604 rank 4
2023-02-18 00:55:00,461 DEBUG TRAIN Batch 0/14400 loss 108.583885 loss_att 246.234131 loss_ctc 93.998344 loss_rnnt 82.998489 hw_loss 0.000148 lr 0.00028804 rank 1
2023-02-18 00:55:00,464 DEBUG TRAIN Batch 0/14400 loss 82.902893 loss_att 161.336258 loss_ctc 86.777206 loss_rnnt 66.675407 hw_loss 0.045455 lr 0.00028804 rank 4
2023-02-18 00:55:00,472 DEBUG TRAIN Batch 0/14400 loss 64.029861 loss_att 171.694519 loss_ctc 55.190468 loss_rnnt 43.675430 hw_loss 0.000148 lr 0.00028804 rank 0
2023-02-18 00:55:00,472 DEBUG TRAIN Batch 0/14400 loss 74.742722 loss_att 157.245911 loss_ctc 74.947006 loss_rnnt 58.214767 hw_loss 0.000148 lr 0.00028804 rank 5
2023-02-18 00:55:00,477 DEBUG TRAIN Batch 0/14400 loss 53.339809 loss_att 94.338219 loss_ctc 55.837433 loss_rnnt 44.790207 hw_loss 0.031691 lr 0.00028804 rank 6
2023-02-18 00:55:00,479 DEBUG TRAIN Batch 0/14400 loss 67.349907 loss_att 164.317780 loss_ctc 56.749229 loss_rnnt 49.369675 hw_loss 0.000148 lr 0.00028804 rank 3
2023-02-18 00:55:00,482 DEBUG TRAIN Batch 0/14400 loss 67.601517 loss_att 99.329132 loss_ctc 77.380798 loss_rnnt 59.907391 hw_loss 0.083803 lr 0.00028804 rank 2
2023-02-18 00:55:00,525 DEBUG TRAIN Batch 0/14400 loss 121.172516 loss_att 206.933792 loss_ctc 117.813911 loss_rnnt 104.467987 hw_loss 0.000148 lr 0.00028804 rank 7
2023-02-18 00:56:00,814 DEBUG TRAIN Batch 0/14500 loss 87.537315 loss_att 203.951431 loss_ctc 74.059029 loss_rnnt 65.979782 hw_loss 0.134649 lr 0.00029004 rank 4
2023-02-18 00:56:00,815 DEBUG TRAIN Batch 0/14500 loss 107.175179 loss_att 192.227066 loss_ctc 109.672119 loss_rnnt 89.831841 hw_loss 0.000068 lr 0.00029004 rank 5
2023-02-18 00:56:00,816 DEBUG TRAIN Batch 0/14500 loss 136.832123 loss_att 262.192230 loss_ctc 136.392685 loss_rnnt 111.818665 hw_loss 0.000068 lr 0.00029004 rank 2
2023-02-18 00:56:00,817 DEBUG TRAIN Batch 0/14500 loss 90.832809 loss_att 185.757736 loss_ctc 86.981209 loss_rnnt 72.361328 hw_loss 0.000068 lr 0.00029004 rank 1
2023-02-18 00:56:00,819 DEBUG TRAIN Batch 0/14500 loss 109.035088 loss_att 209.563049 loss_ctc 105.135445 loss_rnnt 89.426437 hw_loss 0.043169 lr 0.00029004 rank 3
2023-02-18 00:56:00,821 DEBUG TRAIN Batch 0/14500 loss 111.949585 loss_att 246.818497 loss_ctc 107.775887 loss_rnnt 85.532242 hw_loss 0.000068 lr 0.00029004 rank 0
2023-02-18 00:56:00,824 DEBUG TRAIN Batch 0/14500 loss 169.074234 loss_att 275.503296 loss_ctc 170.729828 loss_rnnt 147.534988 hw_loss 0.061271 lr 0.00029004 rank 7
2023-02-18 00:56:00,826 DEBUG TRAIN Batch 0/14500 loss 113.661270 loss_att 246.100327 loss_ctc 113.035202 loss_rnnt 87.256897 hw_loss 0.000068 lr 0.00029004 rank 6
2023-02-18 00:57:03,116 DEBUG TRAIN Batch 0/14600 loss 103.352707 loss_att 217.328751 loss_ctc 107.143265 loss_rnnt 80.052048 hw_loss 0.000083 lr 0.00029204 rank 6
2023-02-18 00:57:03,120 DEBUG TRAIN Batch 0/14600 loss 111.366776 loss_att 229.199463 loss_ctc 107.172859 loss_rnnt 88.359383 hw_loss 0.000083 lr 0.00029204 rank 0
2023-02-18 00:57:03,125 DEBUG TRAIN Batch 0/14600 loss 81.872345 loss_att 220.299210 loss_ctc 77.853195 loss_rnnt 54.722809 hw_loss 0.000083 lr 0.00029204 rank 5
2023-02-18 00:57:03,128 DEBUG TRAIN Batch 0/14600 loss 137.665253 loss_att 270.848999 loss_ctc 146.410233 loss_rnnt 109.793762 hw_loss 0.128881 lr 0.00029204 rank 4
2023-02-18 00:57:03,129 DEBUG TRAIN Batch 0/14600 loss 108.078773 loss_att 219.698013 loss_ctc 111.631432 loss_rnnt 85.281189 hw_loss 0.000083 lr 0.00029204 rank 7
2023-02-18 00:57:03,131 DEBUG TRAIN Batch 0/14600 loss 135.805313 loss_att 281.176086 loss_ctc 139.819855 loss_rnnt 106.152390 hw_loss 0.081547 lr 0.00029204 rank 3
2023-02-18 00:57:03,131 DEBUG TRAIN Batch 0/14600 loss 138.785065 loss_att 261.971832 loss_ctc 141.532898 loss_rnnt 113.781281 hw_loss 0.000083 lr 0.00029204 rank 1
2023-02-18 00:57:03,186 DEBUG TRAIN Batch 0/14600 loss 123.279976 loss_att 212.084671 loss_ctc 129.237061 loss_rnnt 104.619720 hw_loss 0.196927 lr 0.00029204 rank 2
2023-02-18 00:58:22,350 DEBUG TRAIN Batch 0/14700 loss 81.440689 loss_att 212.404602 loss_ctc 71.043159 loss_rnnt 56.542580 hw_loss 0.171847 lr 0.00029404 rank 5
2023-02-18 00:58:22,358 DEBUG TRAIN Batch 0/14700 loss 84.477081 loss_att 177.471588 loss_ctc 85.941788 loss_rnnt 65.582870 hw_loss 0.187538 lr 0.00029404 rank 7
2023-02-18 00:58:22,358 DEBUG TRAIN Batch 0/14700 loss 104.471230 loss_att 191.342758 loss_ctc 108.805542 loss_rnnt 86.444481 hw_loss 0.139757 lr 0.00029404 rank 2
2023-02-18 00:58:22,362 DEBUG TRAIN Batch 0/14700 loss 107.243187 loss_att 218.511627 loss_ctc 94.931145 loss_rnnt 86.631027 hw_loss 0.000128 lr 0.00029404 rank 4
2023-02-18 00:58:22,362 DEBUG TRAIN Batch 0/14700 loss 103.042206 loss_att 204.147705 loss_ctc 99.668701 loss_rnnt 83.220978 hw_loss 0.093622 lr 0.00029404 rank 0
2023-02-18 00:58:22,362 DEBUG TRAIN Batch 0/14700 loss 111.346039 loss_att 210.200684 loss_ctc 113.714470 loss_rnnt 91.191162 hw_loss 0.127788 lr 0.00029404 rank 3
2023-02-18 00:58:22,367 DEBUG TRAIN Batch 0/14700 loss 102.423721 loss_att 170.821579 loss_ctc 103.839500 loss_rnnt 88.473831 hw_loss 0.152884 lr 0.00029404 rank 6
2023-02-18 00:58:22,416 DEBUG TRAIN Batch 0/14700 loss 145.936539 loss_att 286.645355 loss_ctc 143.265945 loss_rnnt 118.150787 hw_loss 0.000128 lr 0.00029404 rank 1
2023-02-18 00:59:39,348 DEBUG TRAIN Batch 0/14800 loss 93.840233 loss_att 216.024078 loss_ctc 89.973770 loss_rnnt 69.918961 hw_loss 0.000061 lr 0.00029604 rank 1
2023-02-18 00:59:39,349 DEBUG TRAIN Batch 0/14800 loss 71.186310 loss_att 190.224228 loss_ctc 60.997551 loss_rnnt 48.737190 hw_loss 0.000061 lr 0.00029604 rank 3
2023-02-18 00:59:39,351 DEBUG TRAIN Batch 0/14800 loss 107.722923 loss_att 204.825974 loss_ctc 114.985336 loss_rnnt 87.333954 hw_loss 0.000061 lr 0.00029604 rank 5
2023-02-18 00:59:39,352 DEBUG TRAIN Batch 0/14800 loss 142.111938 loss_att 228.601196 loss_ctc 150.419693 loss_rnnt 123.706345 hw_loss 0.000061 lr 0.00029604 rank 2
2023-02-18 00:59:39,355 DEBUG TRAIN Batch 0/14800 loss 99.254845 loss_att 208.720459 loss_ctc 105.169159 loss_rnnt 76.502151 hw_loss 0.133113 lr 0.00029604 rank 6
2023-02-18 00:59:39,355 DEBUG TRAIN Batch 0/14800 loss 122.946106 loss_att 233.092667 loss_ctc 126.985886 loss_rnnt 100.269112 hw_loss 0.204460 lr 0.00029604 rank 4
2023-02-18 00:59:39,359 DEBUG TRAIN Batch 0/14800 loss 125.286362 loss_att 233.303589 loss_ctc 132.899017 loss_rnnt 102.606796 hw_loss 0.114556 lr 0.00029604 rank 7
2023-02-18 00:59:39,367 DEBUG TRAIN Batch 0/14800 loss 79.403221 loss_att 241.668625 loss_ctc 74.677734 loss_rnnt 47.481247 hw_loss 0.185550 lr 0.00029604 rank 0
2023-02-18 01:00:38,670 DEBUG TRAIN Batch 0/14900 loss 139.131302 loss_att 279.533539 loss_ctc 139.473572 loss_rnnt 111.005180 hw_loss 0.000066 lr 0.00029804 rank 1
2023-02-18 01:00:38,676 DEBUG TRAIN Batch 0/14900 loss 76.877449 loss_att 199.707916 loss_ctc 64.777176 loss_rnnt 53.924690 hw_loss 0.000066 lr 0.00029804 rank 0
2023-02-18 01:00:38,676 DEBUG TRAIN Batch 0/14900 loss 107.680626 loss_att 214.570999 loss_ctc 107.175209 loss_rnnt 86.369919 hw_loss 0.000066 lr 0.00029804 rank 5
2023-02-18 01:00:38,677 DEBUG TRAIN Batch 0/14900 loss 83.282051 loss_att 227.375336 loss_ctc 65.506439 loss_rnnt 56.833443 hw_loss 0.000066 lr 0.00029804 rank 3
2023-02-18 01:00:38,678 DEBUG TRAIN Batch 0/14900 loss 73.091629 loss_att 170.939789 loss_ctc 72.946815 loss_rnnt 53.369305 hw_loss 0.322499 lr 0.00029804 rank 2
2023-02-18 01:00:38,680 DEBUG TRAIN Batch 0/14900 loss 165.146042 loss_att 283.160828 loss_ctc 168.881210 loss_rnnt 141.041397 hw_loss 0.006849 lr 0.00029804 rank 6
2023-02-18 01:00:38,680 DEBUG TRAIN Batch 0/14900 loss 169.889496 loss_att 227.032928 loss_ctc 189.712738 loss_rnnt 155.817673 hw_loss 0.000066 lr 0.00029804 rank 7
2023-02-18 01:00:38,683 DEBUG TRAIN Batch 0/14900 loss 88.772102 loss_att 233.334320 loss_ctc 73.200546 loss_rnnt 61.935837 hw_loss 0.000066 lr 0.00029804 rank 4
2023-02-18 01:01:36,652 DEBUG TRAIN Batch 0/15000 loss 111.458717 loss_att 210.355225 loss_ctc 117.662910 loss_rnnt 90.742615 hw_loss 0.205461 lr 0.00030004 rank 4
2023-02-18 01:01:36,655 DEBUG TRAIN Batch 0/15000 loss 106.030502 loss_att 211.251160 loss_ctc 105.033920 loss_rnnt 84.977921 hw_loss 0.264989 lr 0.00030004 rank 7
2023-02-18 01:01:36,657 DEBUG TRAIN Batch 0/15000 loss 93.567375 loss_att 153.801605 loss_ctc 104.136314 loss_rnnt 80.111298 hw_loss 0.000069 lr 0.00030004 rank 6
2023-02-18 01:01:36,657 DEBUG TRAIN Batch 0/15000 loss 119.890762 loss_att 271.922668 loss_ctc 136.192932 loss_rnnt 87.302719 hw_loss 0.015077 lr 0.00030004 rank 5
2023-02-18 01:01:36,657 DEBUG TRAIN Batch 0/15000 loss 104.863174 loss_att 215.428589 loss_ctc 108.744278 loss_rnnt 82.210464 hw_loss 0.041541 lr 0.00030004 rank 0
2023-02-18 01:01:36,659 DEBUG TRAIN Batch 0/15000 loss 72.911331 loss_att 105.322182 loss_ctc 80.305817 loss_rnnt 65.400490 hw_loss 0.080129 lr 0.00030004 rank 1
2023-02-18 01:01:36,662 DEBUG TRAIN Batch 0/15000 loss 103.030975 loss_att 204.956818 loss_ctc 114.226746 loss_rnnt 81.113815 hw_loss 0.073544 lr 0.00030004 rank 2
2023-02-18 01:01:36,667 DEBUG TRAIN Batch 0/15000 loss 60.105190 loss_att 152.877182 loss_ctc 57.423668 loss_rnnt 41.908295 hw_loss 0.000069 lr 0.00030004 rank 3
2023-02-18 01:02:37,751 DEBUG TRAIN Batch 0/15100 loss 94.717270 loss_att 198.478058 loss_ctc 89.689606 loss_rnnt 74.531372 hw_loss 0.195165 lr 0.00030204 rank 1
2023-02-18 01:02:37,752 DEBUG TRAIN Batch 0/15100 loss 116.759407 loss_att 236.721741 loss_ctc 110.806320 loss_rnnt 93.550682 hw_loss 0.018745 lr 0.00030204 rank 6
2023-02-18 01:02:37,753 DEBUG TRAIN Batch 0/15100 loss 115.477402 loss_att 213.746918 loss_ctc 119.791733 loss_rnnt 95.136002 hw_loss 0.210458 lr 0.00030204 rank 5
2023-02-18 01:02:37,754 DEBUG TRAIN Batch 0/15100 loss 112.470734 loss_att 223.967407 loss_ctc 109.442703 loss_rnnt 90.472473 hw_loss 0.192495 lr 0.00030204 rank 2
2023-02-18 01:02:37,758 DEBUG TRAIN Batch 0/15100 loss 103.884514 loss_att 214.701599 loss_ctc 107.809021 loss_rnnt 81.136513 hw_loss 0.114959 lr 0.00030204 rank 3
2023-02-18 01:02:37,760 DEBUG TRAIN Batch 0/15100 loss 65.572899 loss_att 175.455383 loss_ctc 59.823544 loss_rnnt 44.362953 hw_loss 0.000066 lr 0.00030204 rank 4
2023-02-18 01:02:37,760 DEBUG TRAIN Batch 0/15100 loss 90.485481 loss_att 240.019012 loss_ctc 80.181496 loss_rnnt 61.952606 hw_loss 0.000066 lr 0.00030204 rank 7
2023-02-18 01:02:37,814 DEBUG TRAIN Batch 0/15100 loss 75.603859 loss_att 152.031311 loss_ctc 67.294258 loss_rnnt 61.426277 hw_loss 0.000066 lr 0.00030204 rank 0
2023-02-18 01:03:37,631 DEBUG TRAIN Batch 0/15200 loss 141.272720 loss_att 274.513428 loss_ctc 157.418182 loss_rnnt 112.471802 hw_loss 0.000078 lr 0.00030404 rank 4
2023-02-18 01:03:37,633 DEBUG TRAIN Batch 0/15200 loss 183.269882 loss_att 299.534912 loss_ctc 199.855255 loss_rnnt 157.805466 hw_loss 0.000078 lr 0.00030404 rank 5
2023-02-18 01:03:37,633 DEBUG TRAIN Batch 0/15200 loss 187.567581 loss_att 348.354401 loss_ctc 174.692368 loss_rnnt 157.030090 hw_loss 0.181539 lr 0.00030404 rank 6
2023-02-18 01:03:37,634 DEBUG TRAIN Batch 0/15200 loss 88.424156 loss_att 236.569626 loss_ctc 71.271614 loss_rnnt 61.017498 hw_loss 0.121066 lr 0.00030404 rank 2
2023-02-18 01:03:37,634 DEBUG TRAIN Batch 0/15200 loss 91.820969 loss_att 205.799850 loss_ctc 92.387032 loss_rnnt 68.949677 hw_loss 0.000078 lr 0.00030404 rank 1
2023-02-18 01:03:37,635 DEBUG TRAIN Batch 0/15200 loss 50.817307 loss_att 95.191833 loss_ctc 50.023151 loss_rnnt 42.015766 hw_loss 0.060977 lr 0.00030404 rank 3
2023-02-18 01:03:37,635 DEBUG TRAIN Batch 0/15200 loss 100.118454 loss_att 213.705170 loss_ctc 98.607780 loss_rnnt 77.602486 hw_loss 0.000078 lr 0.00030404 rank 7
2023-02-18 01:03:37,692 DEBUG TRAIN Batch 0/15200 loss 71.041237 loss_att 179.162613 loss_ctc 57.291023 loss_rnnt 51.027191 hw_loss 0.418371 lr 0.00030404 rank 0
2023-02-18 01:04:36,108 DEBUG TRAIN Batch 0/15300 loss 126.982262 loss_att 270.817749 loss_ctc 123.525444 loss_rnnt 98.610779 hw_loss 0.122406 lr 0.00030604 rank 5
2023-02-18 01:04:36,109 DEBUG TRAIN Batch 0/15300 loss 146.587418 loss_att 220.956482 loss_ctc 150.557343 loss_rnnt 131.151886 hw_loss 0.060738 lr 0.00030604 rank 0
2023-02-18 01:04:36,112 DEBUG TRAIN Batch 0/15300 loss 52.983238 loss_att 98.437012 loss_ctc 56.223633 loss_rnnt 43.460384 hw_loss 0.000088 lr 0.00030604 rank 1
2023-02-18 01:04:36,116 DEBUG TRAIN Batch 0/15300 loss 78.534790 loss_att 189.814850 loss_ctc 72.957306 loss_rnnt 57.022392 hw_loss 0.000088 lr 0.00030604 rank 4
2023-02-18 01:04:36,120 DEBUG TRAIN Batch 0/15300 loss 146.025848 loss_att 240.959427 loss_ctc 141.607086 loss_rnnt 127.494904 hw_loss 0.250130 lr 0.00030604 rank 7
2023-02-18 01:04:36,122 DEBUG TRAIN Batch 0/15300 loss 56.281136 loss_att 135.756256 loss_ctc 51.367470 loss_rnnt 41.000896 hw_loss 0.075703 lr 0.00030604 rank 6
2023-02-18 01:04:36,122 DEBUG TRAIN Batch 0/15300 loss 95.133202 loss_att 240.552979 loss_ctc 91.475548 loss_rnnt 66.463585 hw_loss 0.137533 lr 0.00030604 rank 3
2023-02-18 01:04:36,125 DEBUG TRAIN Batch 0/15300 loss 96.225380 loss_att 191.514175 loss_ctc 101.359009 loss_rnnt 76.458778 hw_loss 0.045663 lr 0.00030604 rank 2
2023-02-18 01:05:37,767 DEBUG TRAIN Batch 0/15400 loss 88.340164 loss_att 194.684250 loss_ctc 85.450523 loss_rnnt 67.456589 hw_loss 0.000068 lr 0.00030804 rank 5
2023-02-18 01:05:37,768 DEBUG TRAIN Batch 0/15400 loss 133.771622 loss_att 223.222275 loss_ctc 142.140182 loss_rnnt 114.681480 hw_loss 0.157908 lr 0.00030804 rank 4
2023-02-18 01:05:37,770 DEBUG TRAIN Batch 0/15400 loss 131.505188 loss_att 224.675186 loss_ctc 146.105865 loss_rnnt 110.924393 hw_loss 0.000068 lr 0.00030804 rank 7
2023-02-18 01:05:37,772 DEBUG TRAIN Batch 0/15400 loss 122.827530 loss_att 234.664093 loss_ctc 126.024918 loss_rnnt 99.968658 hw_loss 0.122320 lr 0.00030804 rank 6
2023-02-18 01:05:37,776 DEBUG TRAIN Batch 0/15400 loss 122.354507 loss_att 207.237671 loss_ctc 144.774551 loss_rnnt 102.388489 hw_loss 0.000068 lr 0.00030804 rank 0
2023-02-18 01:05:37,776 DEBUG TRAIN Batch 0/15400 loss 101.461090 loss_att 215.018539 loss_ctc 95.099998 loss_rnnt 79.554466 hw_loss 0.081138 lr 0.00030804 rank 1
2023-02-18 01:05:37,777 DEBUG TRAIN Batch 0/15400 loss 90.647079 loss_att 155.263550 loss_ctc 106.352318 loss_rnnt 75.629707 hw_loss 0.000068 lr 0.00030804 rank 3
2023-02-18 01:05:37,782 DEBUG TRAIN Batch 0/15400 loss 89.008865 loss_att 177.018188 loss_ctc 87.568413 loss_rnnt 71.534531 hw_loss 0.120994 lr 0.00030804 rank 2
2023-02-18 01:06:53,690 DEBUG TRAIN Batch 0/15500 loss 88.992378 loss_att 216.606079 loss_ctc 82.150658 loss_rnnt 64.381821 hw_loss 0.000090 lr 0.00031004 rank 1
2023-02-18 01:06:53,694 DEBUG TRAIN Batch 0/15500 loss 126.565079 loss_att 253.991486 loss_ctc 129.441101 loss_rnnt 100.696281 hw_loss 0.000090 lr 0.00031004 rank 6
2023-02-18 01:06:53,694 DEBUG TRAIN Batch 0/15500 loss 55.378536 loss_att 182.607849 loss_ctc 49.981495 loss_rnnt 30.652235 hw_loss 0.000090 lr 0.00031004 rank 5
2023-02-18 01:06:53,695 DEBUG TRAIN Batch 0/15500 loss 79.869652 loss_att 162.762131 loss_ctc 82.224655 loss_rnnt 62.945400 hw_loss 0.059556 lr 0.00031004 rank 3
2023-02-18 01:06:53,697 DEBUG TRAIN Batch 0/15500 loss 44.781227 loss_att 57.033241 loss_ctc 52.825638 loss_rnnt 41.109390 hw_loss 0.279082 lr 0.00031004 rank 4
2023-02-18 01:06:53,700 DEBUG TRAIN Batch 0/15500 loss 157.061600 loss_att 304.751343 loss_ctc 161.183838 loss_rnnt 126.973976 hw_loss 0.000090 lr 0.00031004 rank 7
2023-02-18 01:06:53,741 DEBUG TRAIN Batch 0/15500 loss 126.206543 loss_att 221.259705 loss_ctc 123.326302 loss_rnnt 107.579895 hw_loss 0.000090 lr 0.00031004 rank 2
2023-02-18 01:06:53,744 DEBUG TRAIN Batch 0/15500 loss 138.884872 loss_att 203.979263 loss_ctc 166.489853 loss_rnnt 122.101456 hw_loss 0.157257 lr 0.00031004 rank 0
2023-02-18 01:08:11,582 DEBUG TRAIN Batch 0/15600 loss 50.994068 loss_att 85.498322 loss_ctc 59.250648 loss_rnnt 42.882286 hw_loss 0.206359 lr 0.00031204 rank 5
2023-02-18 01:08:11,584 DEBUG TRAIN Batch 0/15600 loss 80.892891 loss_att 204.084137 loss_ctc 76.104782 loss_rnnt 56.842262 hw_loss 0.095237 lr 0.00031204 rank 3
2023-02-18 01:08:11,585 DEBUG TRAIN Batch 0/15600 loss 59.943905 loss_att 134.859634 loss_ctc 58.108082 loss_rnnt 45.134148 hw_loss 0.133859 lr 0.00031204 rank 1
2023-02-18 01:08:11,588 DEBUG TRAIN Batch 0/15600 loss 110.800797 loss_att 207.012024 loss_ctc 103.612213 loss_rnnt 92.511978 hw_loss 0.009460 lr 0.00031204 rank 0
2023-02-18 01:08:11,592 DEBUG TRAIN Batch 0/15600 loss 106.076218 loss_att 208.686478 loss_ctc 104.182709 loss_rnnt 85.806595 hw_loss 0.000061 lr 0.00031204 rank 4
2023-02-18 01:08:11,592 DEBUG TRAIN Batch 0/15600 loss 112.666023 loss_att 217.827118 loss_ctc 128.238541 loss_rnnt 89.494492 hw_loss 0.118088 lr 0.00031204 rank 7
2023-02-18 01:08:11,597 DEBUG TRAIN Batch 0/15600 loss 116.073631 loss_att 182.678604 loss_ctc 118.226860 loss_rnnt 102.463165 hw_loss 0.004444 lr 0.00031204 rank 2
2023-02-18 01:08:11,647 DEBUG TRAIN Batch 0/15600 loss 81.725113 loss_att 187.920029 loss_ctc 74.411659 loss_rnnt 61.434685 hw_loss 0.049828 lr 0.00031204 rank 6
2023-02-18 01:09:13,062 DEBUG TRAIN Batch 0/15700 loss 81.625954 loss_att 191.731033 loss_ctc 76.064590 loss_rnnt 60.346397 hw_loss 0.000104 lr 0.00031404 rank 5
2023-02-18 01:09:13,069 DEBUG TRAIN Batch 0/15700 loss 110.583702 loss_att 200.255814 loss_ctc 119.448395 loss_rnnt 91.355659 hw_loss 0.209346 lr 0.00031404 rank 1
2023-02-18 01:09:13,071 DEBUG TRAIN Batch 0/15700 loss 111.395454 loss_att 225.204224 loss_ctc 123.542801 loss_rnnt 87.014008 hw_loss 0.000104 lr 0.00031404 rank 6
2023-02-18 01:09:13,071 DEBUG TRAIN Batch 0/15700 loss 77.382179 loss_att 181.132355 loss_ctc 62.493225 loss_rnnt 58.611584 hw_loss 0.010793 lr 0.00031404 rank 0
2023-02-18 01:09:13,075 DEBUG TRAIN Batch 0/15700 loss 79.738815 loss_att 183.025604 loss_ctc 71.144470 loss_rnnt 60.133408 hw_loss 0.176163 lr 0.00031404 rank 7
2023-02-18 01:09:13,077 DEBUG TRAIN Batch 0/15700 loss 139.347916 loss_att 250.980118 loss_ctc 134.109314 loss_rnnt 117.719887 hw_loss 0.000104 lr 0.00031404 rank 3
2023-02-18 01:09:13,081 DEBUG TRAIN Batch 0/15700 loss 101.802834 loss_att 247.205109 loss_ctc 99.834686 loss_rnnt 72.943100 hw_loss 0.078191 lr 0.00031404 rank 4
2023-02-18 01:09:13,131 DEBUG TRAIN Batch 0/15700 loss 78.532486 loss_att 205.839752 loss_ctc 89.491684 loss_rnnt 51.523674 hw_loss 0.161502 lr 0.00031404 rank 2
2023-02-18 01:10:11,859 DEBUG TRAIN Batch 0/15800 loss 62.520500 loss_att 158.301895 loss_ctc 53.320595 loss_rnnt 44.590805 hw_loss 0.000137 lr 0.00031604 rank 6
2023-02-18 01:10:11,859 DEBUG TRAIN Batch 0/15800 loss 35.232410 loss_att 57.127998 loss_ctc 36.986893 loss_rnnt 30.550362 hw_loss 0.129380 lr 0.00031604 rank 7
2023-02-18 01:10:11,861 DEBUG TRAIN Batch 0/15800 loss 93.994270 loss_att 218.506378 loss_ctc 106.362289 loss_rnnt 67.334488 hw_loss 0.203029 lr 0.00031604 rank 5
2023-02-18 01:10:11,861 DEBUG TRAIN Batch 0/15800 loss 107.115196 loss_att 199.807907 loss_ctc 117.076035 loss_rnnt 87.204216 hw_loss 0.083097 lr 0.00031604 rank 2
2023-02-18 01:10:11,862 DEBUG TRAIN Batch 0/15800 loss 37.663322 loss_att 64.967804 loss_ctc 40.620235 loss_rnnt 31.767548 hw_loss 0.076167 lr 0.00031604 rank 4
2023-02-18 01:10:11,864 DEBUG TRAIN Batch 0/15800 loss 67.240494 loss_att 142.754868 loss_ctc 62.993282 loss_rnnt 52.703842 hw_loss 0.000137 lr 0.00031604 rank 3
2023-02-18 01:10:11,862 DEBUG TRAIN Batch 0/15800 loss 78.962196 loss_att 185.475128 loss_ctc 72.963142 loss_rnnt 58.459412 hw_loss 0.000137 lr 0.00031604 rank 1
2023-02-18 01:10:11,867 DEBUG TRAIN Batch 0/15800 loss 46.383896 loss_att 70.700027 loss_ctc 51.532848 loss_rnnt 40.768627 hw_loss 0.122839 lr 0.00031604 rank 0
2023-02-18 01:11:10,874 DEBUG TRAIN Batch 0/15900 loss 51.648674 loss_att 114.488739 loss_ctc 45.533447 loss_rnnt 39.867264 hw_loss 0.053923 lr 0.00031804 rank 5
2023-02-18 01:11:10,873 DEBUG TRAIN Batch 0/15900 loss 78.736305 loss_att 163.089279 loss_ctc 79.024948 loss_rnnt 61.827175 hw_loss 0.000087 lr 0.00031804 rank 1
2023-02-18 01:11:10,876 DEBUG TRAIN Batch 0/15900 loss 108.316376 loss_att 202.456360 loss_ctc 107.443016 loss_rnnt 89.540817 hw_loss 0.120030 lr 0.00031804 rank 0
2023-02-18 01:11:10,879 DEBUG TRAIN Batch 0/15900 loss 90.850716 loss_att 193.205109 loss_ctc 97.892288 loss_rnnt 69.398651 hw_loss 0.079328 lr 0.00031804 rank 4
2023-02-18 01:11:10,879 DEBUG TRAIN Batch 0/15900 loss 76.997528 loss_att 188.712906 loss_ctc 68.498138 loss_rnnt 55.787655 hw_loss 0.000087 lr 0.00031804 rank 7
2023-02-18 01:11:10,882 DEBUG TRAIN Batch 0/15900 loss 81.990028 loss_att 170.524063 loss_ctc 81.673714 loss_rnnt 64.298622 hw_loss 0.050191 lr 0.00031804 rank 6
2023-02-18 01:11:10,885 DEBUG TRAIN Batch 0/15900 loss 123.587471 loss_att 264.069458 loss_ctc 126.870758 loss_rnnt 95.011719 hw_loss 0.077979 lr 0.00031804 rank 3
2023-02-18 01:11:10,944 DEBUG TRAIN Batch 0/15900 loss 108.095871 loss_att 217.448669 loss_ctc 105.120056 loss_rnnt 86.575638 hw_loss 0.087067 lr 0.00031804 rank 2
2023-02-18 01:12:11,991 DEBUG TRAIN Batch 0/16000 loss 64.374115 loss_att 180.125305 loss_ctc 55.907055 loss_rnnt 42.352772 hw_loss 0.000075 lr 0.00032004 rank 4
2023-02-18 01:12:11,992 DEBUG TRAIN Batch 0/16000 loss 162.157639 loss_att 239.982285 loss_ctc 172.502060 loss_rnnt 145.149078 hw_loss 0.120684 lr 0.00032004 rank 5
2023-02-18 01:12:11,993 DEBUG TRAIN Batch 0/16000 loss 91.029968 loss_att 197.411667 loss_ctc 88.886818 loss_rnnt 70.039337 hw_loss 0.000075 lr 0.00032004 rank 1
2023-02-18 01:12:11,994 DEBUG TRAIN Batch 0/16000 loss 195.018311 loss_att 289.078522 loss_ctc 202.106720 loss_rnnt 175.261093 hw_loss 0.000075 lr 0.00032004 rank 3
2023-02-18 01:12:11,994 DEBUG TRAIN Batch 0/16000 loss 98.119148 loss_att 257.497742 loss_ctc 85.036903 loss_rnnt 67.987686 hw_loss 0.000075 lr 0.00032004 rank 0
2023-02-18 01:12:11,997 DEBUG TRAIN Batch 0/16000 loss 110.411049 loss_att 226.769852 loss_ctc 113.584717 loss_rnnt 86.663483 hw_loss 0.098710 lr 0.00032004 rank 6
2023-02-18 01:12:12,003 DEBUG TRAIN Batch 0/16000 loss 152.514664 loss_att 281.144592 loss_ctc 155.639328 loss_rnnt 126.291168 hw_loss 0.151619 lr 0.00032004 rank 7
2023-02-18 01:12:12,055 DEBUG TRAIN Batch 0/16000 loss 165.838425 loss_att 313.852081 loss_ctc 167.016068 loss_rnnt 136.078629 hw_loss 0.000075 lr 0.00032004 rank 2
2023-02-18 01:13:10,652 DEBUG TRAIN Batch 0/16100 loss 113.692924 loss_att 222.329880 loss_ctc 121.191582 loss_rnnt 90.965675 hw_loss 0.000057 lr 0.00032204 rank 5
2023-02-18 01:13:10,654 DEBUG TRAIN Batch 0/16100 loss 138.763123 loss_att 294.347626 loss_ctc 125.896721 loss_rnnt 109.330376 hw_loss 0.058835 lr 0.00032204 rank 2
2023-02-18 01:13:10,657 DEBUG TRAIN Batch 0/16100 loss 72.133446 loss_att 172.651825 loss_ctc 71.275543 loss_rnnt 52.144119 hw_loss 0.000057 lr 0.00032204 rank 3
2023-02-18 01:13:10,661 DEBUG TRAIN Batch 0/16100 loss 71.011940 loss_att 178.264709 loss_ctc 67.638176 loss_rnnt 50.011192 hw_loss 0.000057 lr 0.00032204 rank 1
2023-02-18 01:13:10,661 DEBUG TRAIN Batch 0/16100 loss 91.083199 loss_att 166.814911 loss_ctc 96.530731 loss_rnnt 75.079544 hw_loss 0.245572 lr 0.00032204 rank 4
2023-02-18 01:13:10,664 DEBUG TRAIN Batch 0/16100 loss 52.923935 loss_att 104.429932 loss_ctc 55.116146 loss_rnnt 42.216232 hw_loss 0.214135 lr 0.00032204 rank 0
2023-02-18 01:13:10,665 DEBUG TRAIN Batch 0/16100 loss 73.372787 loss_att 150.807907 loss_ctc 69.725769 loss_rnnt 58.322403 hw_loss 0.093065 lr 0.00032204 rank 7
2023-02-18 01:13:10,715 DEBUG TRAIN Batch 0/16100 loss 75.091232 loss_att 188.398300 loss_ctc 78.721992 loss_rnnt 51.884426 hw_loss 0.114927 lr 0.00032204 rank 6
2023-02-18 01:14:09,414 DEBUG TRAIN Batch 0/16200 loss 89.952873 loss_att 190.574753 loss_ctc 87.705887 loss_rnnt 70.126846 hw_loss 0.002350 lr 0.00032404 rank 7
2023-02-18 01:14:09,416 DEBUG TRAIN Batch 0/16200 loss 90.618668 loss_att 207.618927 loss_ctc 91.991974 loss_rnnt 66.997841 hw_loss 0.070634 lr 0.00032404 rank 4
2023-02-18 01:14:09,421 DEBUG TRAIN Batch 0/16200 loss 82.598984 loss_att 165.338989 loss_ctc 77.679489 loss_rnnt 66.558937 hw_loss 0.277450 lr 0.00032404 rank 5
2023-02-18 01:14:09,420 DEBUG TRAIN Batch 0/16200 loss 101.508781 loss_att 186.652481 loss_ctc 115.633774 loss_rnnt 82.596664 hw_loss 0.000075 lr 0.00032404 rank 3
2023-02-18 01:14:09,421 DEBUG TRAIN Batch 0/16200 loss 69.265671 loss_att 145.461136 loss_ctc 81.238556 loss_rnnt 52.345200 hw_loss 0.159351 lr 0.00032404 rank 1
2023-02-18 01:14:09,428 DEBUG TRAIN Batch 0/16200 loss 95.800972 loss_att 207.855621 loss_ctc 89.934029 loss_rnnt 74.103996 hw_loss 0.128069 lr 0.00032404 rank 6
2023-02-18 01:14:09,430 DEBUG TRAIN Batch 0/16200 loss 110.024323 loss_att 222.746658 loss_ctc 109.515610 loss_rnnt 87.518204 hw_loss 0.055276 lr 0.00032404 rank 2
2023-02-18 01:14:09,488 DEBUG TRAIN Batch 0/16200 loss 99.630348 loss_att 223.586731 loss_ctc 94.974945 loss_rnnt 75.459747 hw_loss 0.000075 lr 0.00032404 rank 0
2023-02-18 01:15:11,994 DEBUG TRAIN Batch 0/16300 loss 153.286499 loss_att 250.833206 loss_ctc 156.345047 loss_rnnt 133.320618 hw_loss 0.091378 lr 0.00032604 rank 4
2023-02-18 01:15:12,000 DEBUG TRAIN Batch 0/16300 loss 98.997925 loss_att 194.690582 loss_ctc 108.614281 loss_rnnt 78.577164 hw_loss 0.000075 lr 0.00032604 rank 3
2023-02-18 01:15:12,002 DEBUG TRAIN Batch 0/16300 loss 124.455627 loss_att 226.376892 loss_ctc 130.891251 loss_rnnt 103.122261 hw_loss 0.170675 lr 0.00032604 rank 5
2023-02-18 01:15:12,003 DEBUG TRAIN Batch 0/16300 loss 86.279495 loss_att 182.855469 loss_ctc 81.067070 loss_rnnt 67.659241 hw_loss 0.000075 lr 0.00032604 rank 6
2023-02-18 01:15:12,006 DEBUG TRAIN Batch 0/16300 loss 122.014412 loss_att 220.184402 loss_ctc 142.007263 loss_rnnt 99.689949 hw_loss 0.046415 lr 0.00032604 rank 2
2023-02-18 01:15:12,017 DEBUG TRAIN Batch 0/16300 loss 76.932892 loss_att 219.530029 loss_ctc 66.098305 loss_rnnt 49.827793 hw_loss 0.056775 lr 0.00032604 rank 0
2023-02-18 01:15:12,040 DEBUG TRAIN Batch 0/16300 loss 114.635101 loss_att 216.861084 loss_ctc 114.410454 loss_rnnt 94.219803 hw_loss 0.000075 lr 0.00032604 rank 7
2023-02-18 01:15:12,044 DEBUG TRAIN Batch 0/16300 loss 106.211281 loss_att 202.505127 loss_ctc 93.159683 loss_rnnt 88.600662 hw_loss 0.172587 lr 0.00032604 rank 1
2023-02-18 01:16:30,420 DEBUG TRAIN Batch 0/16400 loss 127.868980 loss_att 234.845917 loss_ctc 129.958511 loss_rnnt 106.079552 hw_loss 0.216448 lr 0.00032804 rank 5
2023-02-18 01:16:30,423 DEBUG TRAIN Batch 0/16400 loss 97.152672 loss_att 225.589905 loss_ctc 89.745605 loss_rnnt 72.375702 hw_loss 0.144636 lr 0.00032804 rank 1
2023-02-18 01:16:30,423 DEBUG TRAIN Batch 0/16400 loss 72.398300 loss_att 139.637573 loss_ctc 73.263031 loss_rnnt 58.794098 hw_loss 0.076976 lr 0.00032804 rank 2
2023-02-18 01:16:30,425 DEBUG TRAIN Batch 0/16400 loss 96.388672 loss_att 247.839081 loss_ctc 93.461212 loss_rnnt 66.409325 hw_loss 0.149222 lr 0.00032804 rank 6
2023-02-18 01:16:30,431 DEBUG TRAIN Batch 0/16400 loss 73.468338 loss_att 141.639832 loss_ctc 79.540817 loss_rnnt 58.932102 hw_loss 0.172997 lr 0.00032804 rank 0
2023-02-18 01:16:30,433 DEBUG TRAIN Batch 0/16400 loss 120.890022 loss_att 198.279266 loss_ctc 143.013367 loss_rnnt 102.356392 hw_loss 0.198747 lr 0.00032804 rank 4
2023-02-18 01:16:30,434 DEBUG TRAIN Batch 0/16400 loss 78.453079 loss_att 163.704895 loss_ctc 72.813736 loss_rnnt 62.154602 hw_loss 0.000066 lr 0.00032804 rank 3
2023-02-18 01:16:30,492 DEBUG TRAIN Batch 0/16400 loss 104.569107 loss_att 190.368927 loss_ctc 100.296112 loss_rnnt 87.956482 hw_loss 0.042001 lr 0.00032804 rank 7
2023-02-18 01:17:48,737 DEBUG TRAIN Batch 0/16500 loss 81.245285 loss_att 157.458099 loss_ctc 96.340988 loss_rnnt 63.989914 hw_loss 0.000102 lr 0.00033004 rank 5
2023-02-18 01:17:48,741 DEBUG TRAIN Batch 0/16500 loss 99.485176 loss_att 215.174164 loss_ctc 107.009201 loss_rnnt 75.296104 hw_loss 0.090131 lr 0.00033004 rank 0
2023-02-18 01:17:48,743 DEBUG TRAIN Batch 0/16500 loss 88.783096 loss_att 185.457413 loss_ctc 99.630173 loss_rnnt 67.972015 hw_loss 0.056135 lr 0.00033004 rank 4
2023-02-18 01:17:48,744 DEBUG TRAIN Batch 0/16500 loss 67.264114 loss_att 153.388428 loss_ctc 66.143677 loss_rnnt 50.131672 hw_loss 0.106818 lr 0.00033004 rank 1
2023-02-18 01:17:48,746 DEBUG TRAIN Batch 0/16500 loss 81.263763 loss_att 160.367569 loss_ctc 89.919716 loss_rnnt 64.197678 hw_loss 0.171001 lr 0.00033004 rank 7
2023-02-18 01:17:48,749 DEBUG TRAIN Batch 0/16500 loss 130.891235 loss_att 248.181137 loss_ctc 130.856659 loss_rnnt 107.380318 hw_loss 0.107870 lr 0.00033004 rank 3
2023-02-18 01:17:48,750 DEBUG TRAIN Batch 0/16500 loss 84.085075 loss_att 186.097076 loss_ctc 94.000824 loss_rnnt 62.360531 hw_loss 0.000102 lr 0.00033004 rank 6
2023-02-18 01:17:48,800 DEBUG TRAIN Batch 0/16500 loss 90.647285 loss_att 206.802490 loss_ctc 90.334244 loss_rnnt 67.391708 hw_loss 0.124270 lr 0.00033004 rank 2
2023-02-18 01:18:48,389 DEBUG TRAIN Batch 0/16600 loss 78.317764 loss_att 153.821426 loss_ctc 88.511261 loss_rnnt 61.857849 hw_loss 0.000077 lr 0.00033204 rank 0
2023-02-18 01:18:48,392 DEBUG TRAIN Batch 0/16600 loss 94.133659 loss_att 193.001053 loss_ctc 107.018761 loss_rnnt 72.630440 hw_loss 0.021992 lr 0.00033204 rank 3
2023-02-18 01:18:48,392 DEBUG TRAIN Batch 0/16600 loss 114.716545 loss_att 225.516571 loss_ctc 119.625130 loss_rnnt 91.849121 hw_loss 0.099283 lr 0.00033204 rank 5
2023-02-18 01:18:48,397 DEBUG TRAIN Batch 0/16600 loss 125.974243 loss_att 225.587708 loss_ctc 148.563599 loss_rnnt 103.039597 hw_loss 0.000077 lr 0.00033204 rank 7
2023-02-18 01:18:48,399 DEBUG TRAIN Batch 0/16600 loss 102.355293 loss_att 219.711914 loss_ctc 96.188553 loss_rnnt 79.588417 hw_loss 0.220825 lr 0.00033204 rank 4
2023-02-18 01:18:48,404 DEBUG TRAIN Batch 0/16600 loss 85.188232 loss_att 178.114288 loss_ctc 78.324509 loss_rnnt 67.518143 hw_loss 0.000077 lr 0.00033204 rank 6
2023-02-18 01:18:48,405 DEBUG TRAIN Batch 0/16600 loss 41.000526 loss_att 161.784622 loss_ctc 33.542591 loss_rnnt 17.793091 hw_loss 0.084382 lr 0.00033204 rank 2
2023-02-18 01:18:48,456 DEBUG TRAIN Batch 0/16600 loss 49.477169 loss_att 162.862961 loss_ctc 39.730553 loss_rnnt 28.004354 hw_loss 0.178508 lr 0.00033204 rank 1
2023-02-18 01:19:47,703 DEBUG TRAIN Batch 0/16700 loss 68.548561 loss_att 174.661331 loss_ctc 66.296638 loss_rnnt 47.626221 hw_loss 0.000069 lr 0.00033404 rank 1
2023-02-18 01:19:47,705 DEBUG TRAIN Batch 0/16700 loss 62.814434 loss_att 214.321121 loss_ctc 45.855011 loss_rnnt 34.774315 hw_loss 0.000069 lr 0.00033404 rank 5
2023-02-18 01:19:47,710 DEBUG TRAIN Batch 0/16700 loss 46.994686 loss_att 98.783165 loss_ctc 47.815487 loss_rnnt 36.446213 hw_loss 0.152516 lr 0.00033404 rank 2
2023-02-18 01:19:47,711 DEBUG TRAIN Batch 0/16700 loss 111.873238 loss_att 193.233093 loss_ctc 108.280075 loss_rnnt 95.965805 hw_loss 0.214781 lr 0.00033404 rank 7
2023-02-18 01:19:47,712 DEBUG TRAIN Batch 0/16700 loss 103.227707 loss_att 224.824036 loss_ctc 95.604271 loss_rnnt 79.797897 hw_loss 0.238114 lr 0.00033404 rank 3
2023-02-18 01:19:47,713 DEBUG TRAIN Batch 0/16700 loss 102.680229 loss_att 197.748428 loss_ctc 113.640991 loss_rnnt 82.063950 hw_loss 0.264767 lr 0.00033404 rank 4
2023-02-18 01:19:47,716 DEBUG TRAIN Batch 0/16700 loss 44.205826 loss_att 73.845413 loss_ctc 49.284317 loss_rnnt 37.499611 hw_loss 0.189686 lr 0.00033404 rank 6
2023-02-18 01:19:47,718 DEBUG TRAIN Batch 0/16700 loss 76.169106 loss_att 157.224915 loss_ctc 71.251511 loss_rnnt 60.613583 hw_loss 0.000069 lr 0.00033404 rank 0
2023-02-18 01:20:48,920 DEBUG TRAIN Batch 0/16800 loss 101.040115 loss_att 223.721252 loss_ctc 91.167297 loss_rnnt 77.753159 hw_loss 0.125837 lr 0.00033604 rank 0
2023-02-18 01:20:48,920 DEBUG TRAIN Batch 0/16800 loss 103.171165 loss_att 229.903381 loss_ctc 104.791206 loss_rnnt 77.573776 hw_loss 0.065513 lr 0.00033604 rank 5
2023-02-18 01:20:48,921 DEBUG TRAIN Batch 0/16800 loss 129.238800 loss_att 262.610657 loss_ctc 127.639679 loss_rnnt 102.777588 hw_loss 0.000085 lr 0.00033604 rank 1
2023-02-18 01:20:48,925 DEBUG TRAIN Batch 0/16800 loss 76.511284 loss_att 141.470993 loss_ctc 73.125450 loss_rnnt 63.884476 hw_loss 0.161834 lr 0.00033604 rank 3
2023-02-18 01:20:48,927 DEBUG TRAIN Batch 0/16800 loss 121.079567 loss_att 233.748962 loss_ctc 132.551788 loss_rnnt 96.971672 hw_loss 0.083214 lr 0.00033604 rank 4
2023-02-18 01:20:48,931 DEBUG TRAIN Batch 0/16800 loss 97.244278 loss_att 210.602203 loss_ctc 99.442825 loss_rnnt 74.279510 hw_loss 0.000085 lr 0.00033604 rank 2
2023-02-18 01:20:48,931 DEBUG TRAIN Batch 0/16800 loss 82.754349 loss_att 202.881485 loss_ctc 74.660156 loss_rnnt 59.808105 hw_loss 0.000085 lr 0.00033604 rank 7
2023-02-18 01:20:48,982 DEBUG TRAIN Batch 0/16800 loss 83.840523 loss_att 175.059937 loss_ctc 90.044479 loss_rnnt 64.690933 hw_loss 0.147213 lr 0.00033604 rank 6
2023-02-18 01:21:49,159 DEBUG TRAIN Batch 0/16900 loss 97.193047 loss_att 226.713791 loss_ctc 97.554924 loss_rnnt 71.240623 hw_loss 0.000061 lr 0.00033804 rank 6
2023-02-18 01:21:49,158 DEBUG TRAIN Batch 0/16900 loss 48.460659 loss_att 145.574875 loss_ctc 40.937111 loss_rnnt 29.972387 hw_loss 0.128569 lr 0.00033804 rank 2
2023-02-18 01:21:49,159 DEBUG TRAIN Batch 0/16900 loss 43.912140 loss_att 162.659683 loss_ctc 35.574741 loss_rnnt 21.274254 hw_loss 0.000061 lr 0.00033804 rank 5
2023-02-18 01:21:49,161 DEBUG TRAIN Batch 0/16900 loss 80.155411 loss_att 208.040680 loss_ctc 78.110100 loss_rnnt 54.795540 hw_loss 0.104098 lr 0.00033804 rank 0
2023-02-18 01:21:49,161 DEBUG TRAIN Batch 0/16900 loss 145.668228 loss_att 294.464600 loss_ctc 148.380783 loss_rnnt 115.547234 hw_loss 0.000061 lr 0.00033804 rank 7
2023-02-18 01:21:49,164 DEBUG TRAIN Batch 0/16900 loss 98.338654 loss_att 224.223969 loss_ctc 106.478706 loss_rnnt 72.018227 hw_loss 0.108770 lr 0.00033804 rank 1
2023-02-18 01:21:49,167 DEBUG TRAIN Batch 0/16900 loss 81.336113 loss_att 231.146591 loss_ctc 64.464485 loss_rnnt 53.582016 hw_loss 0.077909 lr 0.00033804 rank 4
2023-02-18 01:21:49,174 DEBUG TRAIN Batch 0/16900 loss 121.150642 loss_att 242.054932 loss_ctc 125.733757 loss_rnnt 96.254288 hw_loss 0.195753 lr 0.00033804 rank 3
2023-02-18 01:22:47,302 DEBUG TRAIN Batch 0/17000 loss 26.221104 loss_att 33.370388 loss_ctc 27.209820 loss_rnnt 24.532249 hw_loss 0.238440 lr 0.00034004 rank 5
2023-02-18 01:22:47,307 DEBUG TRAIN Batch 0/17000 loss 82.178017 loss_att 157.336823 loss_ctc 84.286293 loss_rnnt 66.865112 hw_loss 0.000053 lr 0.00034004 rank 2
2023-02-18 01:22:47,308 DEBUG TRAIN Batch 0/17000 loss 87.885811 loss_att 186.319901 loss_ctc 92.932663 loss_rnnt 67.459152 hw_loss 0.125482 lr 0.00034004 rank 4
2023-02-18 01:22:47,312 DEBUG TRAIN Batch 0/17000 loss 114.928894 loss_att 186.316025 loss_ctc 110.132248 loss_rnnt 101.291000 hw_loss 0.000053 lr 0.00034004 rank 3
2023-02-18 01:22:47,313 DEBUG TRAIN Batch 0/17000 loss 139.032608 loss_att 256.827911 loss_ctc 145.916519 loss_rnnt 114.440132 hw_loss 0.216675 lr 0.00034004 rank 1
2023-02-18 01:22:47,313 DEBUG TRAIN Batch 0/17000 loss 90.519226 loss_att 176.121338 loss_ctc 95.026527 loss_rnnt 72.715042 hw_loss 0.155221 lr 0.00034004 rank 7
2023-02-18 01:22:47,314 DEBUG TRAIN Batch 0/17000 loss 136.636826 loss_att 237.228088 loss_ctc 149.633514 loss_rnnt 114.695595 hw_loss 0.168922 lr 0.00034004 rank 0
2023-02-18 01:22:47,317 DEBUG TRAIN Batch 0/17000 loss 41.664276 loss_att 80.015808 loss_ctc 47.531212 loss_rnnt 33.172520 hw_loss 0.073476 lr 0.00034004 rank 6
2023-02-18 01:23:49,195 DEBUG TRAIN Batch 0/17100 loss 96.165672 loss_att 204.537140 loss_ctc 97.519806 loss_rnnt 74.291031 hw_loss 0.037124 lr 0.00034204 rank 1
2023-02-18 01:23:49,197 DEBUG TRAIN Batch 0/17100 loss 74.627739 loss_att 186.439362 loss_ctc 68.754272 loss_rnnt 53.045685 hw_loss 0.005366 lr 0.00034204 rank 2
2023-02-18 01:23:49,199 DEBUG TRAIN Batch 0/17100 loss 83.772079 loss_att 185.203186 loss_ctc 80.218521 loss_rnnt 63.929718 hw_loss 0.056159 lr 0.00034204 rank 5
2023-02-18 01:23:49,201 DEBUG TRAIN Batch 0/17100 loss 101.442513 loss_att 219.736298 loss_ctc 97.922966 loss_rnnt 78.140236 hw_loss 0.211463 lr 0.00034204 rank 6
2023-02-18 01:23:49,200 DEBUG TRAIN Batch 0/17100 loss 86.148758 loss_att 185.215744 loss_ctc 102.231987 loss_rnnt 64.190887 hw_loss 0.000071 lr 0.00034204 rank 7
2023-02-18 01:23:49,201 DEBUG TRAIN Batch 0/17100 loss 162.714996 loss_att 246.162521 loss_ctc 173.386765 loss_rnnt 144.578583 hw_loss 0.044996 lr 0.00034204 rank 4
2023-02-18 01:23:49,202 DEBUG TRAIN Batch 0/17100 loss 115.656525 loss_att 243.949295 loss_ctc 136.720596 loss_rnnt 87.189384 hw_loss 0.000071 lr 0.00034204 rank 0
2023-02-18 01:23:49,204 DEBUG TRAIN Batch 0/17100 loss 77.977158 loss_att 213.556000 loss_ctc 73.629089 loss_rnnt 51.441093 hw_loss 0.000071 lr 0.00034204 rank 3
2023-02-18 01:24:50,686 DEBUG TRAIN Batch 0/17200 loss 63.995800 loss_att 162.146378 loss_ctc 71.139771 loss_rnnt 43.413109 hw_loss 0.000091 lr 0.00034404 rank 5
2023-02-18 01:24:50,686 DEBUG TRAIN Batch 0/17200 loss 94.363457 loss_att 242.828522 loss_ctc 87.365837 loss_rnnt 65.558723 hw_loss 0.083866 lr 0.00034404 rank 7
2023-02-18 01:24:50,690 DEBUG TRAIN Batch 0/17200 loss 90.557442 loss_att 207.794540 loss_ctc 98.737862 loss_rnnt 66.019241 hw_loss 0.000091 lr 0.00034404 rank 0
2023-02-18 01:24:50,691 DEBUG TRAIN Batch 0/17200 loss 98.049797 loss_att 219.047913 loss_ctc 103.413933 loss_rnnt 73.134895 hw_loss 0.000091 lr 0.00034404 rank 4
2023-02-18 01:24:50,693 DEBUG TRAIN Batch 0/17200 loss 96.806183 loss_att 219.174500 loss_ctc 75.393814 loss_rnnt 75.177612 hw_loss 0.018540 lr 0.00034404 rank 1
2023-02-18 01:24:50,693 DEBUG TRAIN Batch 0/17200 loss 80.328209 loss_att 207.518906 loss_ctc 80.081490 loss_rnnt 54.862343 hw_loss 0.113651 lr 0.00034404 rank 2
2023-02-18 01:24:50,694 DEBUG TRAIN Batch 0/17200 loss 87.611595 loss_att 251.594101 loss_ctc 84.751122 loss_rnnt 55.196434 hw_loss 0.000091 lr 0.00034404 rank 3
2023-02-18 01:24:50,697 DEBUG TRAIN Batch 0/17200 loss 93.641693 loss_att 195.592987 loss_ctc 97.533333 loss_rnnt 72.732498 hw_loss 0.000091 lr 0.00034404 rank 6
2023-02-18 01:26:09,312 DEBUG TRAIN Batch 0/17300 loss 106.429550 loss_att 210.870544 loss_ctc 102.423508 loss_rnnt 86.075432 hw_loss 0.000111 lr 0.00034604 rank 1
2023-02-18 01:26:09,313 DEBUG TRAIN Batch 0/17300 loss 100.634033 loss_att 202.346466 loss_ctc 117.926086 loss_rnnt 77.928978 hw_loss 0.106813 lr 0.00034604 rank 7
2023-02-18 01:26:09,314 DEBUG TRAIN Batch 0/17300 loss 53.330608 loss_att 120.964729 loss_ctc 46.159832 loss_rnnt 40.557537 hw_loss 0.379413 lr 0.00034604 rank 6
2023-02-18 01:26:09,316 DEBUG TRAIN Batch 0/17300 loss 79.168602 loss_att 160.334518 loss_ctc 75.544090 loss_rnnt 63.409573 hw_loss 0.017088 lr 0.00034604 rank 3
2023-02-18 01:26:09,322 DEBUG TRAIN Batch 0/17300 loss 106.586136 loss_att 191.839371 loss_ctc 114.726883 loss_rnnt 88.380722 hw_loss 0.130007 lr 0.00034604 rank 2
2023-02-18 01:26:09,353 DEBUG TRAIN Batch 0/17300 loss 53.038464 loss_att 103.141418 loss_ctc 52.182487 loss_rnnt 43.131935 hw_loss 0.000111 lr 0.00034604 rank 5
2023-02-18 01:26:09,365 DEBUG TRAIN Batch 0/17300 loss 106.644089 loss_att 213.834366 loss_ctc 115.204750 loss_rnnt 84.014534 hw_loss 0.093873 lr 0.00034604 rank 4
2023-02-18 01:26:09,377 DEBUG TRAIN Batch 0/17300 loss 82.951897 loss_att 168.499451 loss_ctc 87.561111 loss_rnnt 65.155685 hw_loss 0.135248 lr 0.00034604 rank 0
2023-02-18 01:27:25,076 DEBUG TRAIN Batch 0/17400 loss 50.852840 loss_att 143.910080 loss_ctc 49.201347 loss_rnnt 32.418671 hw_loss 0.080475 lr 0.00034804 rank 4
2023-02-18 01:27:25,077 DEBUG TRAIN Batch 0/17400 loss 108.785110 loss_att 189.518295 loss_ctc 105.859131 loss_rnnt 92.952393 hw_loss 0.142896 lr 0.00034804 rank 6
2023-02-18 01:27:25,079 DEBUG TRAIN Batch 0/17400 loss 117.765244 loss_att 248.881393 loss_ctc 117.745804 loss_rnnt 91.544525 hw_loss 0.000154 lr 0.00034804 rank 5
2023-02-18 01:27:25,081 DEBUG TRAIN Batch 0/17400 loss 51.909885 loss_att 136.650604 loss_ctc 43.029598 loss_rnnt 36.117229 hw_loss 0.053526 lr 0.00034804 rank 3
2023-02-18 01:27:25,081 DEBUG TRAIN Batch 0/17400 loss 109.003441 loss_att 216.270828 loss_ctc 108.558350 loss_rnnt 87.544937 hw_loss 0.120684 lr 0.00034804 rank 2
2023-02-18 01:27:25,082 DEBUG TRAIN Batch 0/17400 loss 86.508591 loss_att 199.969345 loss_ctc 87.199646 loss_rnnt 63.680908 hw_loss 0.081363 lr 0.00034804 rank 0
2023-02-18 01:27:25,094 DEBUG TRAIN Batch 0/17400 loss 96.262474 loss_att 265.407074 loss_ctc 88.506058 loss_rnnt 63.408676 hw_loss 0.110751 lr 0.00034804 rank 7
2023-02-18 01:27:25,138 DEBUG TRAIN Batch 0/17400 loss 76.502457 loss_att 169.080994 loss_ctc 72.063606 loss_rnnt 58.578510 hw_loss 0.000154 lr 0.00034804 rank 1
2023-02-18 01:28:24,111 DEBUG TRAIN Batch 0/17500 loss 96.988770 loss_att 192.173584 loss_ctc 100.257408 loss_rnnt 77.459496 hw_loss 0.105939 lr 0.00035004 rank 6
2023-02-18 01:28:24,112 DEBUG TRAIN Batch 0/17500 loss 40.479244 loss_att 68.287254 loss_ctc 41.428001 loss_rnnt 34.682732 hw_loss 0.203269 lr 0.00035004 rank 4
2023-02-18 01:28:24,113 DEBUG TRAIN Batch 0/17500 loss 101.458328 loss_att 250.091309 loss_ctc 112.075638 loss_rnnt 70.316055 hw_loss 0.000054 lr 0.00035004 rank 0
2023-02-18 01:28:24,114 DEBUG TRAIN Batch 0/17500 loss 63.695873 loss_att 114.911087 loss_ctc 72.013138 loss_rnnt 52.294380 hw_loss 0.092770 lr 0.00035004 rank 3
2023-02-18 01:28:24,116 DEBUG TRAIN Batch 0/17500 loss 100.752312 loss_att 185.444794 loss_ctc 114.655441 loss_rnnt 81.960037 hw_loss 0.000054 lr 0.00035004 rank 5
2023-02-18 01:28:24,118 DEBUG TRAIN Batch 0/17500 loss 104.718109 loss_att 216.448761 loss_ctc 100.504303 loss_rnnt 82.858612 hw_loss 0.141010 lr 0.00035004 rank 2
2023-02-18 01:28:24,119 DEBUG TRAIN Batch 0/17500 loss 75.691177 loss_att 204.578430 loss_ctc 74.193245 loss_rnnt 50.113426 hw_loss 0.000054 lr 0.00035004 rank 7
2023-02-18 01:28:24,182 DEBUG TRAIN Batch 0/17500 loss 60.907761 loss_att 159.431091 loss_ctc 56.998142 loss_rnnt 41.724342 hw_loss 0.000054 lr 0.00035004 rank 1
2023-02-18 01:29:22,518 DEBUG TRAIN Batch 0/17600 loss 46.045097 loss_att 75.992096 loss_ctc 52.374413 loss_rnnt 39.170181 hw_loss 0.078014 lr 0.00035204 rank 1
2023-02-18 01:29:22,525 DEBUG TRAIN Batch 0/17600 loss 62.285511 loss_att 142.108047 loss_ctc 61.549652 loss_rnnt 46.372089 hw_loss 0.088181 lr 0.00035204 rank 5
2023-02-18 01:29:22,525 DEBUG TRAIN Batch 0/17600 loss 79.629341 loss_att 190.360779 loss_ctc 90.662323 loss_rnnt 56.011948 hw_loss 0.000077 lr 0.00035204 rank 0
2023-02-18 01:29:22,527 DEBUG TRAIN Batch 0/17600 loss 102.003311 loss_att 193.773193 loss_ctc 98.526001 loss_rnnt 84.086838 hw_loss 0.049027 lr 0.00035204 rank 7
2023-02-18 01:29:22,529 DEBUG TRAIN Batch 0/17600 loss 68.827995 loss_att 152.612228 loss_ctc 63.047859 loss_rnnt 52.794113 hw_loss 0.089482 lr 0.00035204 rank 6
2023-02-18 01:29:22,532 DEBUG TRAIN Batch 0/17600 loss 87.733719 loss_att 196.534073 loss_ctc 93.698090 loss_rnnt 65.093597 hw_loss 0.158999 lr 0.00035204 rank 4
2023-02-18 01:29:22,536 DEBUG TRAIN Batch 0/17600 loss 90.066216 loss_att 214.043945 loss_ctc 89.774139 loss_rnnt 65.287987 hw_loss 0.040549 lr 0.00035204 rank 3
2023-02-18 01:29:22,582 DEBUG TRAIN Batch 0/17600 loss 65.889015 loss_att 140.311523 loss_ctc 72.660431 loss_rnnt 50.101616 hw_loss 0.000077 lr 0.00035204 rank 2
2023-02-18 01:30:24,684 DEBUG TRAIN Batch 0/17700 loss 96.159210 loss_att 203.124756 loss_ctc 97.238480 loss_rnnt 74.576904 hw_loss 0.084928 lr 0.00035404 rank 1
2023-02-18 01:30:24,690 DEBUG TRAIN Batch 0/17700 loss 117.979965 loss_att 202.605713 loss_ctc 121.780609 loss_rnnt 100.547989 hw_loss 0.000110 lr 0.00035404 rank 6
2023-02-18 01:30:24,692 DEBUG TRAIN Batch 0/17700 loss 71.698082 loss_att 184.609009 loss_ctc 73.801376 loss_rnnt 48.751190 hw_loss 0.157977 lr 0.00035404 rank 5
2023-02-18 01:30:24,693 DEBUG TRAIN Batch 0/17700 loss 40.561111 loss_att 142.030457 loss_ctc 22.443890 loss_rnnt 22.606989 hw_loss 0.142275 lr 0.00035404 rank 3
2023-02-18 01:30:24,694 DEBUG TRAIN Batch 0/17700 loss 40.188362 loss_att 144.460968 loss_ctc 41.831799 loss_rnnt 19.114658 hw_loss 0.000110 lr 0.00035404 rank 4
2023-02-18 01:30:24,701 DEBUG TRAIN Batch 0/17700 loss 38.198280 loss_att 120.345520 loss_ctc 38.671173 loss_rnnt 21.705717 hw_loss 0.000110 lr 0.00035404 rank 0
2023-02-18 01:30:24,708 DEBUG TRAIN Batch 0/17700 loss 82.955559 loss_att 188.864761 loss_ctc 89.724289 loss_rnnt 60.817635 hw_loss 0.100459 lr 0.00035404 rank 2
2023-02-18 01:30:24,714 DEBUG TRAIN Batch 0/17700 loss 78.088402 loss_att 221.892792 loss_ctc 83.203751 loss_rnnt 48.569626 hw_loss 0.142216 lr 0.00035404 rank 7
2023-02-18 01:31:23,827 DEBUG TRAIN Batch 0/17800 loss 44.413200 loss_att 84.378716 loss_ctc 44.404552 loss_rnnt 36.399597 hw_loss 0.040601 lr 0.00035604 rank 4
2023-02-18 01:31:23,835 DEBUG TRAIN Batch 0/17800 loss 82.429855 loss_att 207.218979 loss_ctc 74.333649 loss_rnnt 58.479469 hw_loss 0.135091 lr 0.00035604 rank 0
2023-02-18 01:31:23,836 DEBUG TRAIN Batch 0/17800 loss 70.638687 loss_att 121.823387 loss_ctc 76.268356 loss_rnnt 59.651093 hw_loss 0.000059 lr 0.00035604 rank 3
2023-02-18 01:31:23,836 DEBUG TRAIN Batch 0/17800 loss 179.126831 loss_att 255.364014 loss_ctc 177.845001 loss_rnnt 164.050262 hw_loss 0.000059 lr 0.00035604 rank 5
2023-02-18 01:31:23,837 DEBUG TRAIN Batch 0/17800 loss 68.845963 loss_att 180.261047 loss_ctc 71.405869 loss_rnnt 46.221592 hw_loss 0.000059 lr 0.00035604 rank 1
2023-02-18 01:31:23,837 DEBUG TRAIN Batch 0/17800 loss 112.754707 loss_att 135.867889 loss_ctc 121.653488 loss_rnnt 106.945541 hw_loss 0.000059 lr 0.00035604 rank 6
2023-02-18 01:31:23,838 DEBUG TRAIN Batch 0/17800 loss 163.444962 loss_att 280.133667 loss_ctc 189.450363 loss_rnnt 136.529404 hw_loss 0.207081 lr 0.00035604 rank 7
2023-02-18 01:31:23,839 DEBUG TRAIN Batch 0/17800 loss 79.446472 loss_att 192.053284 loss_ctc 85.166290 loss_rnnt 56.162437 hw_loss 0.000059 lr 0.00035604 rank 2
2023-02-18 01:32:22,752 DEBUG TRAIN Batch 0/17900 loss 104.225449 loss_att 171.254395 loss_ctc 119.127205 loss_rnnt 88.832703 hw_loss 0.000115 lr 0.00035804 rank 5
2023-02-18 01:32:22,754 DEBUG TRAIN Batch 0/17900 loss 84.454338 loss_att 176.120605 loss_ctc 91.494102 loss_rnnt 65.182388 hw_loss 0.000115 lr 0.00035804 rank 0
2023-02-18 01:32:22,755 DEBUG TRAIN Batch 0/17900 loss 95.531807 loss_att 208.884140 loss_ctc 93.887733 loss_rnnt 73.018021 hw_loss 0.117246 lr 0.00035804 rank 7
2023-02-18 01:32:22,755 DEBUG TRAIN Batch 0/17900 loss 63.624996 loss_att 160.164429 loss_ctc 65.405304 loss_rnnt 44.006035 hw_loss 0.138184 lr 0.00035804 rank 2
2023-02-18 01:32:22,755 DEBUG TRAIN Batch 0/17900 loss 54.017254 loss_att 113.547165 loss_ctc 55.502029 loss_rnnt 41.774105 hw_loss 0.260988 lr 0.00035804 rank 1
2023-02-18 01:32:22,757 DEBUG TRAIN Batch 0/17900 loss 66.721725 loss_att 169.056305 loss_ctc 70.132576 loss_rnnt 45.682228 hw_loss 0.220876 lr 0.00035804 rank 4
2023-02-18 01:32:22,758 DEBUG TRAIN Batch 0/17900 loss 73.986877 loss_att 174.271088 loss_ctc 71.684265 loss_rnnt 54.197937 hw_loss 0.073344 lr 0.00035804 rank 3
2023-02-18 01:32:22,759 DEBUG TRAIN Batch 0/17900 loss 60.035004 loss_att 159.168747 loss_ctc 57.985138 loss_rnnt 40.450428 hw_loss 0.058388 lr 0.00035804 rank 6
2023-02-18 01:33:25,651 DEBUG TRAIN Batch 0/18000 loss 105.662941 loss_att 236.344498 loss_ctc 124.674973 loss_rnnt 76.875290 hw_loss 0.218258 lr 0.00036004 rank 5
2023-02-18 01:33:25,654 DEBUG TRAIN Batch 0/18000 loss 115.405663 loss_att 212.623077 loss_ctc 119.296494 loss_rnnt 95.385170 hw_loss 0.109174 lr 0.00036004 rank 1
2023-02-18 01:33:25,659 DEBUG TRAIN Batch 0/18000 loss 80.485062 loss_att 185.028900 loss_ctc 79.312889 loss_rnnt 59.732498 hw_loss 0.000152 lr 0.00036004 rank 6
2023-02-18 01:33:25,659 DEBUG TRAIN Batch 0/18000 loss 62.719574 loss_att 164.035782 loss_ctc 50.693237 loss_rnnt 44.059765 hw_loss 0.000152 lr 0.00036004 rank 0
2023-02-18 01:33:25,660 DEBUG TRAIN Batch 0/18000 loss 82.619408 loss_att 204.628525 loss_ctc 75.546616 loss_rnnt 59.160538 hw_loss 0.000152 lr 0.00036004 rank 3
2023-02-18 01:33:25,660 DEBUG TRAIN Batch 0/18000 loss 111.698410 loss_att 171.532944 loss_ctc 132.068192 loss_rnnt 97.015450 hw_loss 0.000152 lr 0.00036004 rank 7
2023-02-18 01:33:25,668 DEBUG TRAIN Batch 0/18000 loss 44.269547 loss_att 107.091141 loss_ctc 43.462067 loss_rnnt 31.795959 hw_loss 0.031748 lr 0.00036004 rank 2
2023-02-18 01:33:25,726 DEBUG TRAIN Batch 0/18000 loss 90.238785 loss_att 198.829300 loss_ctc 106.582611 loss_rnnt 66.303955 hw_loss 0.070395 lr 0.00036004 rank 4
2023-02-18 01:34:44,899 DEBUG TRAIN Batch 0/18100 loss 49.106056 loss_att 69.556969 loss_ctc 55.891285 loss_rnnt 43.959682 hw_loss 0.284039 lr 0.00036204 rank 0
2023-02-18 01:34:44,901 DEBUG TRAIN Batch 0/18100 loss 89.542732 loss_att 217.068314 loss_ctc 90.126945 loss_rnnt 63.946487 hw_loss 0.024811 lr 0.00036204 rank 5
2023-02-18 01:34:44,902 DEBUG TRAIN Batch 0/18100 loss 152.729584 loss_att 261.807068 loss_ctc 170.936920 loss_rnnt 128.486404 hw_loss 0.000067 lr 0.00036204 rank 1
2023-02-18 01:34:44,908 DEBUG TRAIN Batch 0/18100 loss 58.105656 loss_att 143.204346 loss_ctc 54.214123 loss_rnnt 41.525986 hw_loss 0.147756 lr 0.00036204 rank 3
2023-02-18 01:34:44,915 DEBUG TRAIN Batch 0/18100 loss 78.379837 loss_att 138.988480 loss_ctc 88.857483 loss_rnnt 64.824020 hw_loss 0.069512 lr 0.00036204 rank 4
2023-02-18 01:34:44,915 DEBUG TRAIN Batch 0/18100 loss 112.302155 loss_att 212.519043 loss_ctc 136.751221 loss_rnnt 88.998848 hw_loss 0.000067 lr 0.00036204 rank 6
2023-02-18 01:34:44,946 DEBUG TRAIN Batch 0/18100 loss 118.033264 loss_att 274.658936 loss_ctc 118.523079 loss_rnnt 86.642769 hw_loss 0.000067 lr 0.00036204 rank 7
2023-02-18 01:34:44,973 DEBUG TRAIN Batch 0/18100 loss 89.079414 loss_att 230.682983 loss_ctc 77.132393 loss_rnnt 62.258926 hw_loss 0.173837 lr 0.00036204 rank 2
2023-02-18 01:36:03,501 DEBUG TRAIN Batch 0/18200 loss 95.756180 loss_att 178.850800 loss_ctc 102.616486 loss_rnnt 78.218384 hw_loss 0.007797 lr 0.00036404 rank 5
2023-02-18 01:36:03,505 DEBUG TRAIN Batch 0/18200 loss 87.376617 loss_att 196.206467 loss_ctc 85.812820 loss_rnnt 65.720490 hw_loss 0.184987 lr 0.00036404 rank 2
2023-02-18 01:36:03,505 DEBUG TRAIN Batch 0/18200 loss 110.186211 loss_att 184.123505 loss_ctc 128.249359 loss_rnnt 92.943130 hw_loss 0.088476 lr 0.00036404 rank 1
2023-02-18 01:36:03,506 DEBUG TRAIN Batch 0/18200 loss 85.823105 loss_att 185.507446 loss_ctc 89.636002 loss_rnnt 65.330460 hw_loss 0.088838 lr 0.00036404 rank 7
2023-02-18 01:36:03,507 DEBUG TRAIN Batch 0/18200 loss 101.668655 loss_att 202.454590 loss_ctc 112.687775 loss_rnnt 80.004852 hw_loss 0.070129 lr 0.00036404 rank 3
2023-02-18 01:36:03,513 DEBUG TRAIN Batch 0/18200 loss 86.100967 loss_att 189.739609 loss_ctc 80.407768 loss_rnnt 66.128082 hw_loss 0.007947 lr 0.00036404 rank 6
2023-02-18 01:36:03,516 DEBUG TRAIN Batch 0/18200 loss 132.411331 loss_att 245.486496 loss_ctc 139.600769 loss_rnnt 108.774445 hw_loss 0.118587 lr 0.00036404 rank 0
2023-02-18 01:36:03,563 DEBUG TRAIN Batch 0/18200 loss 107.881866 loss_att 212.586365 loss_ctc 114.539230 loss_rnnt 86.017868 hw_loss 0.066480 lr 0.00036404 rank 4
2023-02-18 01:37:05,383 DEBUG TRAIN Batch 0/18300 loss 48.070801 loss_att 146.880112 loss_ctc 42.327335 loss_rnnt 29.074673 hw_loss 0.000115 lr 0.00036604 rank 6
2023-02-18 01:37:05,390 DEBUG TRAIN Batch 0/18300 loss 91.994675 loss_att 219.240005 loss_ctc 97.222107 loss_rnnt 65.848557 hw_loss 0.000115 lr 0.00036604 rank 4
2023-02-18 01:37:05,390 DEBUG TRAIN Batch 0/18300 loss 107.275032 loss_att 204.712128 loss_ctc 121.255737 loss_rnnt 85.923462 hw_loss 0.000115 lr 0.00036604 rank 0
2023-02-18 01:37:05,390 DEBUG TRAIN Batch 0/18300 loss 180.155991 loss_att 318.682922 loss_ctc 170.546600 loss_rnnt 153.642029 hw_loss 0.168426 lr 0.00036604 rank 2
2023-02-18 01:37:05,394 DEBUG TRAIN Batch 0/18300 loss 86.777512 loss_att 165.607239 loss_ctc 90.938255 loss_rnnt 70.456741 hw_loss 0.000115 lr 0.00036604 rank 5
2023-02-18 01:37:05,399 DEBUG TRAIN Batch 0/18300 loss 131.883057 loss_att 287.667175 loss_ctc 138.471298 loss_rnnt 99.847733 hw_loss 0.000115 lr 0.00036604 rank 3
2023-02-18 01:37:05,423 DEBUG TRAIN Batch 0/18300 loss 90.124146 loss_att 210.256195 loss_ctc 95.845032 loss_rnnt 65.282135 hw_loss 0.099027 lr 0.00036604 rank 1
2023-02-18 01:37:05,429 DEBUG TRAIN Batch 0/18300 loss 102.650093 loss_att 205.288055 loss_ctc 112.879990 loss_rnnt 80.752190 hw_loss 0.011849 lr 0.00036604 rank 7
2023-02-18 01:38:04,156 DEBUG TRAIN Batch 0/18400 loss 64.213486 loss_att 97.928802 loss_ctc 67.760040 loss_rnnt 56.917377 hw_loss 0.150315 lr 0.00036804 rank 0
2023-02-18 01:38:04,159 DEBUG TRAIN Batch 0/18400 loss 89.861931 loss_att 198.852753 loss_ctc 85.840607 loss_rnnt 68.599899 hw_loss 0.000077 lr 0.00036804 rank 5
2023-02-18 01:38:04,159 DEBUG TRAIN Batch 0/18400 loss 113.850456 loss_att 207.783722 loss_ctc 140.357986 loss_rnnt 91.525497 hw_loss 0.007454 lr 0.00036804 rank 3
2023-02-18 01:38:04,160 DEBUG TRAIN Batch 0/18400 loss 32.520893 loss_att 58.118134 loss_ctc 38.776123 loss_rnnt 26.472202 hw_loss 0.178523 lr 0.00036804 rank 7
2023-02-18 01:38:04,161 DEBUG TRAIN Batch 0/18400 loss 64.981987 loss_att 144.316727 loss_ctc 61.637375 loss_rnnt 49.533890 hw_loss 0.050792 lr 0.00036804 rank 4
2023-02-18 01:38:04,164 DEBUG TRAIN Batch 0/18400 loss 120.270386 loss_att 202.294327 loss_ctc 115.704895 loss_rnnt 104.474289 hw_loss 0.000077 lr 0.00036804 rank 6
2023-02-18 01:38:04,164 DEBUG TRAIN Batch 0/18400 loss 88.660454 loss_att 200.519989 loss_ctc 74.121162 loss_rnnt 68.222900 hw_loss 0.007907 lr 0.00036804 rank 1
2023-02-18 01:38:04,169 DEBUG TRAIN Batch 0/18400 loss 80.505951 loss_att 203.984818 loss_ctc 80.361832 loss_rnnt 55.829353 hw_loss 0.000077 lr 0.00036804 rank 2
2023-02-18 01:39:04,048 DEBUG TRAIN Batch 0/18500 loss 64.733513 loss_att 149.203156 loss_ctc 64.496719 loss_rnnt 47.834675 hw_loss 0.068397 lr 0.00037004 rank 6
2023-02-18 01:39:04,049 DEBUG TRAIN Batch 0/18500 loss 45.148468 loss_att 103.112473 loss_ctc 47.855431 loss_rnnt 33.101974 hw_loss 0.173935 lr 0.00037004 rank 1
2023-02-18 01:39:04,049 DEBUG TRAIN Batch 0/18500 loss 81.852112 loss_att 189.982239 loss_ctc 84.132996 loss_rnnt 59.896324 hw_loss 0.048078 lr 0.00037004 rank 5
2023-02-18 01:39:04,052 DEBUG TRAIN Batch 0/18500 loss 75.993134 loss_att 175.841339 loss_ctc 80.261406 loss_rnnt 55.454342 hw_loss 0.000084 lr 0.00037004 rank 4
2023-02-18 01:39:04,057 DEBUG TRAIN Batch 0/18500 loss 68.311707 loss_att 163.551880 loss_ctc 69.970291 loss_rnnt 49.042480 hw_loss 0.000084 lr 0.00037004 rank 0
2023-02-18 01:39:04,056 DEBUG TRAIN Batch 0/18500 loss 68.192238 loss_att 152.688782 loss_ctc 64.724258 loss_rnnt 51.732773 hw_loss 0.042277 lr 0.00037004 rank 7
2023-02-18 01:39:04,056 DEBUG TRAIN Batch 0/18500 loss 57.836098 loss_att 156.149765 loss_ctc 58.985546 loss_rnnt 37.990658 hw_loss 0.055212 lr 0.00037004 rank 2
2023-02-18 01:39:04,059 DEBUG TRAIN Batch 0/18500 loss 68.141930 loss_att 168.095932 loss_ctc 59.647068 loss_rnnt 49.081345 hw_loss 0.379554 lr 0.00037004 rank 3
2023-02-18 01:40:04,460 DEBUG TRAIN Batch 0/18600 loss 128.896881 loss_att 221.748138 loss_ctc 151.804764 loss_rnnt 107.267143 hw_loss 0.009584 lr 0.00037204 rank 0
2023-02-18 01:40:04,465 DEBUG TRAIN Batch 0/18600 loss 56.702309 loss_att 125.612320 loss_ctc 61.188316 loss_rnnt 42.165897 hw_loss 0.293009 lr 0.00037204 rank 1
2023-02-18 01:40:04,466 DEBUG TRAIN Batch 0/18600 loss 102.891861 loss_att 224.533722 loss_ctc 110.957230 loss_rnnt 77.488060 hw_loss 0.000075 lr 0.00037204 rank 7
2023-02-18 01:40:04,468 DEBUG TRAIN Batch 0/18600 loss 76.260590 loss_att 182.858063 loss_ctc 87.258179 loss_rnnt 53.359703 hw_loss 0.215716 lr 0.00037204 rank 4
2023-02-18 01:40:04,469 DEBUG TRAIN Batch 0/18600 loss 61.565773 loss_att 186.451782 loss_ctc 74.258705 loss_rnnt 34.896141 hw_loss 0.000075 lr 0.00037204 rank 5
2023-02-18 01:40:04,469 DEBUG TRAIN Batch 0/18600 loss 131.836899 loss_att 203.258774 loss_ctc 160.674133 loss_rnnt 113.667618 hw_loss 0.074876 lr 0.00037204 rank 6
2023-02-18 01:40:04,471 DEBUG TRAIN Batch 0/18600 loss 121.652451 loss_att 225.533966 loss_ctc 144.254211 loss_rnnt 97.862534 hw_loss 0.000075 lr 0.00037204 rank 3
2023-02-18 01:40:04,525 DEBUG TRAIN Batch 0/18600 loss 164.654480 loss_att 295.186859 loss_ctc 188.112000 loss_rnnt 135.412537 hw_loss 0.014609 lr 0.00037204 rank 2
2023-02-18 01:41:03,336 DEBUG TRAIN Batch 0/18700 loss 32.543846 loss_att 58.645840 loss_ctc 35.916637 loss_rnnt 26.740608 hw_loss 0.249627 lr 0.00037404 rank 2
2023-02-18 01:41:03,340 DEBUG TRAIN Batch 0/18700 loss 85.771896 loss_att 153.555389 loss_ctc 71.983299 loss_rnnt 74.052460 hw_loss 0.002284 lr 0.00037404 rank 1
2023-02-18 01:41:03,342 DEBUG TRAIN Batch 0/18700 loss 97.693321 loss_att 158.649292 loss_ctc 112.252304 loss_rnnt 83.516022 hw_loss 0.084210 lr 0.00037404 rank 4
2023-02-18 01:41:03,344 DEBUG TRAIN Batch 0/18700 loss 85.451385 loss_att 174.950989 loss_ctc 93.034424 loss_rnnt 66.540321 hw_loss 0.000119 lr 0.00037404 rank 3
2023-02-18 01:41:03,344 DEBUG TRAIN Batch 0/18700 loss 23.677778 loss_att 35.060696 loss_ctc 25.801569 loss_rnnt 21.046448 hw_loss 0.134204 lr 0.00037404 rank 5
2023-02-18 01:41:03,346 DEBUG TRAIN Batch 0/18700 loss 44.592213 loss_att 75.989182 loss_ctc 50.632046 loss_rnnt 37.419739 hw_loss 0.164567 lr 0.00037404 rank 6
2023-02-18 01:41:03,354 DEBUG TRAIN Batch 0/18700 loss 70.201408 loss_att 130.171722 loss_ctc 75.037766 loss_rnnt 57.498390 hw_loss 0.120208 lr 0.00037404 rank 7
2023-02-18 01:41:03,399 DEBUG TRAIN Batch 0/18700 loss 90.826912 loss_att 151.971710 loss_ctc 112.389290 loss_rnnt 75.698601 hw_loss 0.045681 lr 0.00037404 rank 0
2023-02-18 01:42:03,588 DEBUG TRAIN Batch 0/18800 loss 40.334034 loss_att 87.252800 loss_ctc 42.866940 loss_rnnt 30.437866 hw_loss 0.327548 lr 0.00037604 rank 1
2023-02-18 01:42:03,589 DEBUG TRAIN Batch 0/18800 loss 86.650528 loss_att 195.041382 loss_ctc 88.266136 loss_rnnt 64.754532 hw_loss 0.004516 lr 0.00037604 rank 4
2023-02-18 01:42:03,594 DEBUG TRAIN Batch 0/18800 loss 103.187134 loss_att 184.340714 loss_ctc 102.012825 loss_rnnt 87.062897 hw_loss 0.093914 lr 0.00037604 rank 5
2023-02-18 01:42:03,594 DEBUG TRAIN Batch 0/18800 loss 60.765137 loss_att 146.946777 loss_ctc 59.404984 loss_rnnt 43.710136 hw_loss 0.000057 lr 0.00037604 rank 6
2023-02-18 01:42:03,598 DEBUG TRAIN Batch 0/18800 loss 113.561951 loss_att 202.159637 loss_ctc 122.812630 loss_rnnt 94.544891 hw_loss 0.120183 lr 0.00037604 rank 2
2023-02-18 01:42:03,599 DEBUG TRAIN Batch 0/18800 loss 71.870796 loss_att 163.770325 loss_ctc 72.870117 loss_rnnt 53.357613 hw_loss 0.000057 lr 0.00037604 rank 7
2023-02-18 01:42:03,603 DEBUG TRAIN Batch 0/18800 loss 58.091808 loss_att 165.225372 loss_ctc 52.562912 loss_rnnt 37.340210 hw_loss 0.116380 lr 0.00037604 rank 3
2023-02-18 01:42:03,659 DEBUG TRAIN Batch 0/18800 loss 61.075115 loss_att 174.623154 loss_ctc 65.335594 loss_rnnt 37.737270 hw_loss 0.112826 lr 0.00037604 rank 0
2023-02-18 01:43:05,606 DEBUG TRAIN Batch 0/18900 loss 55.402016 loss_att 125.984047 loss_ctc 58.304264 loss_rnnt 40.886127 hw_loss 0.023461 lr 0.00037804 rank 1
2023-02-18 01:43:05,606 DEBUG TRAIN Batch 0/18900 loss 55.797642 loss_att 128.711517 loss_ctc 68.317993 loss_rnnt 39.544762 hw_loss 0.001352 lr 0.00037804 rank 4
2023-02-18 01:43:05,609 DEBUG TRAIN Batch 0/18900 loss 52.317829 loss_att 134.747498 loss_ctc 49.588669 loss_rnnt 36.195717 hw_loss 0.000119 lr 0.00037804 rank 5
2023-02-18 01:43:05,609 DEBUG TRAIN Batch 0/18900 loss 116.325272 loss_att 244.957672 loss_ctc 134.370667 loss_rnnt 88.071693 hw_loss 0.226958 lr 0.00037804 rank 6
2023-02-18 01:43:05,610 DEBUG TRAIN Batch 0/18900 loss 57.921936 loss_att 173.097794 loss_ctc 60.673584 loss_rnnt 34.414986 hw_loss 0.196666 lr 0.00037804 rank 2
2023-02-18 01:43:05,612 DEBUG TRAIN Batch 0/18900 loss 127.597626 loss_att 215.766617 loss_ctc 124.288399 loss_rnnt 110.383369 hw_loss 0.040672 lr 0.00037804 rank 7
2023-02-18 01:43:05,611 DEBUG TRAIN Batch 0/18900 loss 72.967880 loss_att 193.575592 loss_ctc 65.532448 loss_rnnt 49.837666 hw_loss 0.000119 lr 0.00037804 rank 0
2023-02-18 01:43:05,617 DEBUG TRAIN Batch 0/18900 loss 101.536339 loss_att 189.544113 loss_ctc 107.714729 loss_rnnt 82.939049 hw_loss 0.322416 lr 0.00037804 rank 3
2023-02-18 01:44:24,288 DEBUG TRAIN Batch 0/19000 loss 72.631897 loss_att 143.210144 loss_ctc 74.569710 loss_rnnt 58.251492 hw_loss 0.011979 lr 0.00038004 rank 5
2023-02-18 01:44:24,288 DEBUG TRAIN Batch 0/19000 loss 86.610565 loss_att 187.609802 loss_ctc 92.348305 loss_rnnt 65.572021 hw_loss 0.138115 lr 0.00038004 rank 4
2023-02-18 01:44:24,289 DEBUG TRAIN Batch 0/19000 loss 40.183998 loss_att 87.608276 loss_ctc 41.749168 loss_rnnt 30.417320 hw_loss 0.137120 lr 0.00038004 rank 6
2023-02-18 01:44:24,290 DEBUG TRAIN Batch 0/19000 loss 38.842186 loss_att 96.452484 loss_ctc 45.409683 loss_rnnt 26.444424 hw_loss 0.000068 lr 0.00038004 rank 0
2023-02-18 01:44:24,295 DEBUG TRAIN Batch 0/19000 loss 66.958427 loss_att 157.766861 loss_ctc 63.961468 loss_rnnt 49.151085 hw_loss 0.084831 lr 0.00038004 rank 3
2023-02-18 01:44:24,297 DEBUG TRAIN Batch 0/19000 loss 107.627159 loss_att 188.824219 loss_ctc 112.016739 loss_rnnt 90.802429 hw_loss 0.000068 lr 0.00038004 rank 1
2023-02-18 01:44:24,296 DEBUG TRAIN Batch 0/19000 loss 47.778675 loss_att 102.797012 loss_ctc 53.745758 loss_rnnt 35.912605 hw_loss 0.125235 lr 0.00038004 rank 2
2023-02-18 01:44:24,299 DEBUG TRAIN Batch 0/19000 loss 62.722439 loss_att 111.602196 loss_ctc 69.830902 loss_rnnt 51.890114 hw_loss 0.203583 lr 0.00038004 rank 7
2023-02-18 01:45:45,062 DEBUG TRAIN Batch 0/19100 loss 141.506668 loss_att 251.181366 loss_ctc 161.765350 loss_rnnt 116.817307 hw_loss 0.099851 lr 0.00038204 rank 5
2023-02-18 01:45:45,065 DEBUG TRAIN Batch 0/19100 loss 88.414871 loss_att 172.107407 loss_ctc 87.816956 loss_rnnt 71.756027 hw_loss 0.000123 lr 0.00038204 rank 0
2023-02-18 01:45:45,068 DEBUG TRAIN Batch 0/19100 loss 57.295162 loss_att 128.038483 loss_ctc 58.873260 loss_rnnt 42.849297 hw_loss 0.162736 lr 0.00038204 rank 3
2023-02-18 01:45:45,069 DEBUG TRAIN Batch 0/19100 loss 41.486771 loss_att 121.635529 loss_ctc 41.952805 loss_rnnt 25.324492 hw_loss 0.131978 lr 0.00038204 rank 7
2023-02-18 01:45:45,070 DEBUG TRAIN Batch 0/19100 loss 51.792202 loss_att 119.977524 loss_ctc 61.448006 loss_rnnt 36.812679 hw_loss 0.103156 lr 0.00038204 rank 2
2023-02-18 01:45:45,070 DEBUG TRAIN Batch 0/19100 loss 66.107788 loss_att 154.593719 loss_ctc 69.578033 loss_rnnt 47.881752 hw_loss 0.124029 lr 0.00038204 rank 6
2023-02-18 01:45:45,071 DEBUG TRAIN Batch 0/19100 loss 79.698471 loss_att 143.542511 loss_ctc 88.039978 loss_rnnt 65.797371 hw_loss 0.037668 lr 0.00038204 rank 4
2023-02-18 01:45:45,122 DEBUG TRAIN Batch 0/19100 loss 75.624504 loss_att 155.844513 loss_ctc 87.536339 loss_rnnt 57.916748 hw_loss 0.141589 lr 0.00038204 rank 1
2023-02-18 01:46:46,151 DEBUG TRAIN Batch 0/19200 loss 94.359238 loss_att 185.224655 loss_ctc 84.171494 loss_rnnt 77.544472 hw_loss 0.000071 lr 0.00038404 rank 4
2023-02-18 01:46:46,151 DEBUG TRAIN Batch 0/19200 loss 103.380775 loss_att 216.246796 loss_ctc 107.009575 loss_rnnt 80.251640 hw_loss 0.135171 lr 0.00038404 rank 2
2023-02-18 01:46:46,156 DEBUG TRAIN Batch 0/19200 loss 70.640274 loss_att 150.672043 loss_ctc 79.934334 loss_rnnt 53.394661 hw_loss 0.000071 lr 0.00038404 rank 7
2023-02-18 01:46:46,157 DEBUG TRAIN Batch 0/19200 loss 52.606094 loss_att 118.116806 loss_ctc 52.407253 loss_rnnt 39.498085 hw_loss 0.060713 lr 0.00038404 rank 5
2023-02-18 01:46:46,164 DEBUG TRAIN Batch 0/19200 loss 163.479599 loss_att 266.527039 loss_ctc 178.522293 loss_rnnt 140.853012 hw_loss 0.021384 lr 0.00038404 rank 6
2023-02-18 01:46:46,163 DEBUG TRAIN Batch 0/19200 loss 37.522266 loss_att 56.864315 loss_ctc 41.403847 loss_rnnt 33.034657 hw_loss 0.190597 lr 0.00038404 rank 3
2023-02-18 01:46:46,168 DEBUG TRAIN Batch 0/19200 loss 34.249596 loss_att 83.754379 loss_ctc 36.245361 loss_rnnt 23.978619 hw_loss 0.194847 lr 0.00038404 rank 0
2023-02-18 01:46:46,215 DEBUG TRAIN Batch 0/19200 loss 125.469620 loss_att 246.201019 loss_ctc 140.176758 loss_rnnt 99.362350 hw_loss 0.000071 lr 0.00038404 rank 1
2023-02-18 01:47:44,092 DEBUG TRAIN Batch 0/19300 loss 50.772049 loss_att 132.276016 loss_ctc 44.779083 loss_rnnt 35.270226 hw_loss 0.000175 lr 0.00038604 rank 5
2023-02-18 01:47:44,098 DEBUG TRAIN Batch 0/19300 loss 79.699196 loss_att 164.625397 loss_ctc 82.504807 loss_rnnt 62.288212 hw_loss 0.096860 lr 0.00038604 rank 7
2023-02-18 01:47:44,098 DEBUG TRAIN Batch 0/19300 loss 59.734447 loss_att 124.446228 loss_ctc 55.525757 loss_rnnt 47.297672 hw_loss 0.104205 lr 0.00038604 rank 0
2023-02-18 01:47:44,101 DEBUG TRAIN Batch 0/19300 loss 74.572113 loss_att 155.175568 loss_ctc 74.601967 loss_rnnt 58.447350 hw_loss 0.000175 lr 0.00038604 rank 6
2023-02-18 01:47:44,103 DEBUG TRAIN Batch 0/19300 loss 68.402786 loss_att 132.527939 loss_ctc 71.508385 loss_rnnt 55.121281 hw_loss 0.079495 lr 0.00038604 rank 4
2023-02-18 01:47:44,103 DEBUG TRAIN Batch 0/19300 loss 64.415947 loss_att 152.103714 loss_ctc 67.707611 loss_rnnt 46.439415 hw_loss 0.000175 lr 0.00038604 rank 1
2023-02-18 01:47:44,104 DEBUG TRAIN Batch 0/19300 loss 47.125313 loss_att 118.244720 loss_ctc 48.709641 loss_rnnt 32.642448 hw_loss 0.089514 lr 0.00038604 rank 2
2023-02-18 01:47:44,106 DEBUG TRAIN Batch 0/19300 loss 67.137321 loss_att 163.956329 loss_ctc 68.610344 loss_rnnt 47.577026 hw_loss 0.000175 lr 0.00038604 rank 3
2023-02-18 01:48:44,481 DEBUG TRAIN Batch 0/19400 loss 63.303799 loss_att 147.366989 loss_ctc 65.413589 loss_rnnt 46.145599 hw_loss 0.120480 lr 0.00038804 rank 5
2023-02-18 01:48:44,484 DEBUG TRAIN Batch 0/19400 loss 69.981003 loss_att 179.210327 loss_ctc 74.575668 loss_rnnt 47.433510 hw_loss 0.166887 lr 0.00038804 rank 6
2023-02-18 01:48:44,488 DEBUG TRAIN Batch 0/19400 loss 43.789902 loss_att 145.417297 loss_ctc 39.608273 loss_rnnt 24.021791 hw_loss 0.000347 lr 0.00038804 rank 0
2023-02-18 01:48:44,488 DEBUG TRAIN Batch 0/19400 loss 66.953255 loss_att 171.811890 loss_ctc 70.124191 loss_rnnt 45.558548 hw_loss 0.000347 lr 0.00038804 rank 7
2023-02-18 01:48:44,492 DEBUG TRAIN Batch 0/19400 loss 102.945511 loss_att 212.482498 loss_ctc 112.746872 loss_rnnt 79.731079 hw_loss 0.000347 lr 0.00038804 rank 1
2023-02-18 01:48:44,495 DEBUG TRAIN Batch 0/19400 loss 54.765919 loss_att 141.015152 loss_ctc 52.906723 loss_rnnt 37.713230 hw_loss 0.095125 lr 0.00038804 rank 2
2023-02-18 01:48:44,499 DEBUG TRAIN Batch 0/19400 loss 117.506004 loss_att 236.732025 loss_ctc 129.328949 loss_rnnt 92.037903 hw_loss 0.087205 lr 0.00038804 rank 3
2023-02-18 01:48:44,557 DEBUG TRAIN Batch 0/19400 loss 114.455658 loss_att 243.225784 loss_ctc 129.871735 loss_rnnt 86.645973 hw_loss 0.000347 lr 0.00038804 rank 4
2023-02-18 01:49:45,198 DEBUG TRAIN Batch 0/19500 loss 155.263733 loss_att 273.449280 loss_ctc 178.981415 loss_rnnt 128.464218 hw_loss 0.000083 lr 0.00039004 rank 5
2023-02-18 01:49:45,202 DEBUG TRAIN Batch 0/19500 loss 49.954708 loss_att 152.406006 loss_ctc 52.423622 loss_rnnt 29.135210 hw_loss 0.000083 lr 0.00039004 rank 0
2023-02-18 01:49:45,207 DEBUG TRAIN Batch 0/19500 loss 102.732315 loss_att 245.344666 loss_ctc 120.531693 loss_rnnt 71.836563 hw_loss 0.000083 lr 0.00039004 rank 7
2023-02-18 01:49:45,208 DEBUG TRAIN Batch 0/19500 loss 69.392395 loss_att 153.562866 loss_ctc 66.494881 loss_rnnt 52.944588 hw_loss 0.000083 lr 0.00039004 rank 2
2023-02-18 01:49:45,208 DEBUG TRAIN Batch 0/19500 loss 82.092430 loss_att 175.242981 loss_ctc 103.182098 loss_rnnt 60.614311 hw_loss 0.067602 lr 0.00039004 rank 4
2023-02-18 01:49:45,208 DEBUG TRAIN Batch 0/19500 loss 100.742989 loss_att 164.882141 loss_ctc 108.859688 loss_rnnt 86.829025 hw_loss 0.007320 lr 0.00039004 rank 1
2023-02-18 01:49:45,209 DEBUG TRAIN Batch 0/19500 loss 48.484882 loss_att 87.773102 loss_ctc 54.320679 loss_rnnt 39.763657 hw_loss 0.160269 lr 0.00039004 rank 3
2023-02-18 01:49:45,219 DEBUG TRAIN Batch 0/19500 loss 164.681671 loss_att 255.476593 loss_ctc 188.232880 loss_rnnt 143.243835 hw_loss 0.260042 lr 0.00039004 rank 6
2023-02-18 01:50:45,880 DEBUG TRAIN Batch 0/19600 loss 70.368370 loss_att 142.600006 loss_ctc 78.494308 loss_rnnt 54.838165 hw_loss 0.000776 lr 0.00039204 rank 5
2023-02-18 01:50:45,882 DEBUG TRAIN Batch 0/19600 loss 55.907356 loss_att 139.585312 loss_ctc 55.358849 loss_rnnt 39.165283 hw_loss 0.149277 lr 0.00039204 rank 4
2023-02-18 01:50:45,883 DEBUG TRAIN Batch 0/19600 loss 68.357704 loss_att 152.320831 loss_ctc 62.622658 loss_rnnt 52.329334 hw_loss 0.000776 lr 0.00039204 rank 3
2023-02-18 01:50:45,883 DEBUG TRAIN Batch 0/19600 loss 53.267033 loss_att 170.925751 loss_ctc 58.894768 loss_rnnt 28.984507 hw_loss 0.000776 lr 0.00039204 rank 1
2023-02-18 01:50:45,885 DEBUG TRAIN Batch 0/19600 loss 82.422585 loss_att 163.685715 loss_ctc 95.226700 loss_rnnt 64.462326 hw_loss 0.000776 lr 0.00039204 rank 0
2023-02-18 01:50:45,890 DEBUG TRAIN Batch 0/19600 loss 72.333557 loss_att 135.951248 loss_ctc 81.810181 loss_rnnt 58.304142 hw_loss 0.079339 lr 0.00039204 rank 6
2023-02-18 01:50:45,890 DEBUG TRAIN Batch 0/19600 loss 64.285065 loss_att 150.776932 loss_ctc 78.263275 loss_rnnt 45.122505 hw_loss 0.000776 lr 0.00039204 rank 2
2023-02-18 01:50:45,890 DEBUG TRAIN Batch 0/19600 loss 69.138329 loss_att 154.294083 loss_ctc 71.904602 loss_rnnt 51.725479 hw_loss 0.024117 lr 0.00039204 rank 7
2023-02-18 01:51:48,258 DEBUG TRAIN Batch 0/19700 loss 77.010109 loss_att 138.328781 loss_ctc 87.420059 loss_rnnt 63.293072 hw_loss 0.122477 lr 0.00039404 rank 0
2023-02-18 01:51:48,259 DEBUG TRAIN Batch 0/19700 loss 96.856659 loss_att 159.370163 loss_ctc 98.269157 loss_rnnt 84.165581 hw_loss 0.000101 lr 0.00039404 rank 6
2023-02-18 01:51:48,261 DEBUG TRAIN Batch 0/19700 loss 100.257568 loss_att 209.925842 loss_ctc 103.321915 loss_rnnt 77.915283 hw_loss 0.000101 lr 0.00039404 rank 5
2023-02-18 01:51:48,262 DEBUG TRAIN Batch 0/19700 loss 110.199074 loss_att 207.131027 loss_ctc 119.703537 loss_rnnt 89.545372 hw_loss 0.000101 lr 0.00039404 rank 4
2023-02-18 01:51:48,262 DEBUG TRAIN Batch 0/19700 loss 68.265694 loss_att 117.527107 loss_ctc 75.371849 loss_rnnt 57.412994 hw_loss 0.099231 lr 0.00039404 rank 1
2023-02-18 01:51:48,267 DEBUG TRAIN Batch 0/19700 loss 81.816818 loss_att 161.104950 loss_ctc 90.387047 loss_rnnt 64.739670 hw_loss 0.144045 lr 0.00039404 rank 7
2023-02-18 01:51:48,272 DEBUG TRAIN Batch 0/19700 loss 72.056213 loss_att 170.685135 loss_ctc 97.235443 loss_rnnt 48.929676 hw_loss 0.081593 lr 0.00039404 rank 3
2023-02-18 01:51:48,280 DEBUG TRAIN Batch 0/19700 loss 98.249008 loss_att 187.569397 loss_ctc 107.380463 loss_rnnt 79.106453 hw_loss 0.114283 lr 0.00039404 rank 2
2023-02-18 01:52:50,050 DEBUG TRAIN Batch 0/19800 loss 64.163452 loss_att 172.789337 loss_ctc 82.853279 loss_rnnt 39.740540 hw_loss 0.385795 lr 0.00039604 rank 5
2023-02-18 01:52:50,051 DEBUG TRAIN Batch 0/19800 loss 69.590569 loss_att 160.466599 loss_ctc 70.820923 loss_rnnt 51.186005 hw_loss 0.122453 lr 0.00039604 rank 1
2023-02-18 01:52:50,051 DEBUG TRAIN Batch 0/19800 loss 38.566338 loss_att 57.335327 loss_ctc 43.949753 loss_rnnt 33.996384 hw_loss 0.184441 lr 0.00039604 rank 4
2023-02-18 01:52:50,054 DEBUG TRAIN Batch 0/19800 loss 46.477287 loss_att 138.498962 loss_ctc 41.759808 loss_rnnt 28.701897 hw_loss 0.000096 lr 0.00039604 rank 2
2023-02-18 01:52:50,058 DEBUG TRAIN Batch 0/19800 loss 86.309250 loss_att 163.825165 loss_ctc 83.938118 loss_rnnt 71.122162 hw_loss 0.000096 lr 0.00039604 rank 6
2023-02-18 01:52:50,060 DEBUG TRAIN Batch 0/19800 loss 33.199406 loss_att 80.343292 loss_ctc 34.366966 loss_rnnt 23.560532 hw_loss 0.102032 lr 0.00039604 rank 3
2023-02-18 01:52:50,061 DEBUG TRAIN Batch 0/19800 loss 139.523407 loss_att 250.450165 loss_ctc 177.843323 loss_rnnt 112.228668 hw_loss 0.000096 lr 0.00039604 rank 7
2023-02-18 01:52:50,120 DEBUG TRAIN Batch 0/19800 loss 70.645126 loss_att 152.558685 loss_ctc 66.059700 loss_rnnt 54.873753 hw_loss 0.000096 lr 0.00039604 rank 0
2023-02-18 01:53:48,599 DEBUG TRAIN Batch 0/19900 loss 98.602951 loss_att 194.143372 loss_ctc 109.107712 loss_rnnt 77.999634 hw_loss 0.177357 lr 0.00039804 rank 5
2023-02-18 01:53:48,607 DEBUG TRAIN Batch 0/19900 loss 63.051052 loss_att 155.902679 loss_ctc 68.294968 loss_rnnt 43.781380 hw_loss 0.000290 lr 0.00039804 rank 0
2023-02-18 01:53:48,609 DEBUG TRAIN Batch 0/19900 loss 80.977753 loss_att 139.684326 loss_ctc 91.485062 loss_rnnt 67.773552 hw_loss 0.116089 lr 0.00039804 rank 7
2023-02-18 01:53:48,608 DEBUG TRAIN Batch 0/19900 loss 61.603874 loss_att 164.534363 loss_ctc 63.272141 loss_rnnt 40.795193 hw_loss 0.000290 lr 0.00039804 rank 3
2023-02-18 01:53:48,609 DEBUG TRAIN Batch 0/19900 loss 90.486931 loss_att 180.876907 loss_ctc 127.910034 loss_rnnt 67.387390 hw_loss 0.059615 lr 0.00039804 rank 1
2023-02-18 01:53:48,612 DEBUG TRAIN Batch 0/19900 loss 66.642601 loss_att 147.151047 loss_ctc 74.939774 loss_rnnt 49.430344 hw_loss 0.008036 lr 0.00039804 rank 2
2023-02-18 01:53:48,613 DEBUG TRAIN Batch 0/19900 loss 70.264236 loss_att 148.324432 loss_ctc 74.783279 loss_rnnt 54.040791 hw_loss 0.016630 lr 0.00039804 rank 4
2023-02-18 01:53:48,667 DEBUG TRAIN Batch 0/19900 loss 45.629520 loss_att 107.620872 loss_ctc 46.413399 loss_rnnt 33.020561 hw_loss 0.199072 lr 0.00039804 rank 6
2023-02-18 01:54:49,736 DEBUG TRAIN Batch 0/20000 loss 155.680801 loss_att 269.746918 loss_ctc 187.165497 loss_rnnt 128.669571 hw_loss 0.000077 lr 0.00040004 rank 4
2023-02-18 01:54:49,737 DEBUG TRAIN Batch 0/20000 loss 99.515625 loss_att 174.043304 loss_ctc 124.031258 loss_rnnt 81.212662 hw_loss 0.241255 lr 0.00040004 rank 5
2023-02-18 01:54:49,738 DEBUG TRAIN Batch 0/20000 loss 38.089432 loss_att 120.092667 loss_ctc 41.787514 loss_rnnt 21.104694 hw_loss 0.170650 lr 0.00040004 rank 7
2023-02-18 01:54:49,738 DEBUG TRAIN Batch 0/20000 loss 113.545029 loss_att 224.574463 loss_ctc 155.977356 loss_rnnt 85.610748 hw_loss 0.132654 lr 0.00040004 rank 0
2023-02-18 01:54:49,739 DEBUG TRAIN Batch 0/20000 loss 64.877075 loss_att 164.025269 loss_ctc 67.915611 loss_rnnt 44.642250 hw_loss 0.000077 lr 0.00040004 rank 6
2023-02-18 01:54:49,739 DEBUG TRAIN Batch 0/20000 loss 50.702160 loss_att 147.608246 loss_ctc 60.894291 loss_rnnt 29.857821 hw_loss 0.195322 lr 0.00040004 rank 2
2023-02-18 01:54:49,747 DEBUG TRAIN Batch 0/20000 loss 90.740913 loss_att 171.396332 loss_ctc 116.378555 loss_rnnt 71.066391 hw_loss 0.234525 lr 0.00040004 rank 3
2023-02-18 01:54:49,811 DEBUG TRAIN Batch 0/20000 loss 67.462929 loss_att 133.697617 loss_ctc 67.353333 loss_rnnt 54.189949 hw_loss 0.076220 lr 0.00040004 rank 1
2023-02-18 01:55:50,009 DEBUG TRAIN Batch 0/20100 loss 106.797997 loss_att 182.490356 loss_ctc 116.164528 loss_rnnt 90.410606 hw_loss 0.000100 lr 0.00040204 rank 0
2023-02-18 01:55:50,009 DEBUG TRAIN Batch 0/20100 loss 73.199562 loss_att 134.365295 loss_ctc 84.059875 loss_rnnt 59.518326 hw_loss 0.000100 lr 0.00040204 rank 7
2023-02-18 01:55:50,010 DEBUG TRAIN Batch 0/20100 loss 44.771317 loss_att 120.309311 loss_ctc 39.108158 loss_rnnt 30.418749 hw_loss 0.000100 lr 0.00040204 rank 1
2023-02-18 01:55:50,013 DEBUG TRAIN Batch 0/20100 loss 83.916084 loss_att 190.770584 loss_ctc 93.931610 loss_rnnt 61.185875 hw_loss 0.044820 lr 0.00040204 rank 6
2023-02-18 01:55:50,015 DEBUG TRAIN Batch 0/20100 loss 97.646324 loss_att 177.309540 loss_ctc 125.207550 loss_rnnt 78.023659 hw_loss 0.028486 lr 0.00040204 rank 4
2023-02-18 01:55:50,020 DEBUG TRAIN Batch 0/20100 loss 66.521362 loss_att 178.222092 loss_ctc 69.062729 loss_rnnt 43.810081 hw_loss 0.060549 lr 0.00040204 rank 3
2023-02-18 01:55:50,021 DEBUG TRAIN Batch 0/20100 loss 75.134857 loss_att 197.893188 loss_ctc 74.093536 loss_rnnt 50.686638 hw_loss 0.066373 lr 0.00040204 rank 5
2023-02-18 01:55:50,077 DEBUG TRAIN Batch 0/20100 loss 63.742897 loss_att 156.471497 loss_ctc 54.223492 loss_rnnt 46.419350 hw_loss 0.088274 lr 0.00040204 rank 2
2023-02-18 01:56:50,010 DEBUG TRAIN Batch 0/20200 loss 46.205315 loss_att 142.852493 loss_ctc 34.842934 loss_rnnt 28.294813 hw_loss 0.180088 lr 0.00040404 rank 5
2023-02-18 01:56:50,015 DEBUG TRAIN Batch 0/20200 loss 69.142128 loss_att 222.573242 loss_ctc 62.223518 loss_rnnt 39.302750 hw_loss 0.141805 lr 0.00040404 rank 1
2023-02-18 01:56:50,016 DEBUG TRAIN Batch 0/20200 loss 154.100403 loss_att 267.877014 loss_ctc 185.168396 loss_rnnt 127.202637 hw_loss 0.000095 lr 0.00040404 rank 2
2023-02-18 01:56:50,027 DEBUG TRAIN Batch 0/20200 loss 155.353760 loss_att 265.019714 loss_ctc 182.382538 loss_rnnt 129.816681 hw_loss 0.000095 lr 0.00040404 rank 6
2023-02-18 01:56:50,076 DEBUG TRAIN Batch 0/20200 loss 66.999542 loss_att 123.226173 loss_ctc 73.923988 loss_rnnt 54.708996 hw_loss 0.228666 lr 0.00040404 rank 7
2023-02-18 01:57:49,558 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 6
2023-02-18 01:57:49,560 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 2
2023-02-18 01:57:49,638 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 5
2023-02-18 01:57:49,779 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 7
2023-02-18 01:57:49,798 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 1
2023-02-18 01:57:49,854 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 3
2023-02-18 01:57:49,983 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 0
2023-02-18 01:57:50,005 DEBUG CV Batch 0/0 loss 13.386022 loss_att 19.974915 loss_ctc 16.537119 loss_rnnt 11.394004 hw_loss 0.476425 history loss 12.493620 rank 4
2023-02-18 01:57:59,950 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 2
2023-02-18 01:57:59,959 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 6
2023-02-18 01:58:00,143 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 5
2023-02-18 01:58:00,408 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 1
2023-02-18 01:58:00,663 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 0
2023-02-18 01:58:00,699 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 3
2023-02-18 01:58:01,063 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 7
2023-02-18 01:58:01,769 DEBUG CV Batch 0/100 loss 120.571762 loss_att 229.993973 loss_ctc 143.817413 loss_rnnt 95.531174 hw_loss 0.106352 history loss 33.888778 rank 4
2023-02-18 01:58:12,602 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 6
2023-02-18 01:58:12,623 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 2
2023-02-18 01:58:12,710 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 5
2023-02-18 01:58:13,018 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 1
2023-02-18 01:58:13,357 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 0
2023-02-18 01:58:13,367 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 3
2023-02-18 01:58:14,057 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 7
2023-02-18 01:58:14,686 DEBUG CV Batch 0/200 loss 40.377106 loss_att 66.216331 loss_ctc 42.832470 loss_rnnt 34.871574 hw_loss 0.019319 history loss 33.892183 rank 4
2023-02-18 01:58:26,070 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 6
2023-02-18 01:58:26,276 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 5
2023-02-18 01:58:26,369 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 2
2023-02-18 01:58:26,617 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 1
2023-02-18 01:58:26,988 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 0
2023-02-18 01:58:27,213 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 3
2023-02-18 01:58:27,792 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 7
2023-02-18 01:58:28,660 DEBUG CV Batch 0/300 loss 21.086290 loss_att 32.318474 loss_ctc 23.463455 loss_rnnt 18.413868 hw_loss 0.204436 history loss 37.278936 rank 4
2023-02-18 01:58:36,745 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 5
2023-02-18 01:58:37,090 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 1
2023-02-18 01:58:37,137 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 2
2023-02-18 01:58:37,144 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 6
2023-02-18 01:58:37,629 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 0
2023-02-18 01:58:37,931 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 3
2023-02-18 01:58:38,546 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 7
2023-02-18 01:58:39,309 DEBUG CV Batch 0/400 loss 91.441261 loss_att 197.190308 loss_ctc 89.936798 loss_rnnt 70.436737 hw_loss 0.103701 history loss 37.553634 rank 4
2023-02-18 01:58:52,005 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 5
2023-02-18 01:58:52,760 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 2
2023-02-18 01:58:53,161 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 1
2023-02-18 01:58:53,230 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 0
2023-02-18 01:58:53,244 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 6
2023-02-18 01:58:53,515 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 3
2023-02-18 01:58:54,410 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 7
2023-02-18 01:58:54,901 DEBUG CV Batch 0/500 loss 36.132366 loss_att 71.390121 loss_ctc 37.085266 loss_rnnt 28.926189 hw_loss 0.051696 history loss 37.576008 rank 4
2023-02-18 01:59:04,828 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 5
2023-02-18 01:59:05,795 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 2
2023-02-18 01:59:06,215 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 6
2023-02-18 01:59:06,370 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 1
2023-02-18 01:59:06,547 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 3
2023-02-18 01:59:06,565 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 0
2023-02-18 01:59:07,672 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 7
2023-02-18 01:59:08,148 DEBUG CV Batch 0/600 loss 108.008659 loss_att 289.707977 loss_ctc 107.071892 loss_rnnt 71.753883 hw_loss 0.074629 history loss 40.120455 rank 4
2023-02-18 01:59:16,057 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 5
2023-02-18 01:59:17,218 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 2
2023-02-18 01:59:17,724 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 6
2023-02-18 01:59:17,882 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 1
2023-02-18 01:59:17,987 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 3
2023-02-18 01:59:18,156 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 0
2023-02-18 01:59:19,330 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 7
2023-02-18 01:59:19,797 DEBUG CV Batch 0/700 loss 90.956261 loss_att 157.734848 loss_ctc 97.757500 loss_rnnt 76.598206 hw_loss 0.179082 history loss 39.507468 rank 4
2023-02-18 01:59:27,224 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 5
2023-02-18 01:59:28,562 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 2
2023-02-18 01:59:29,171 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 6
2023-02-18 01:59:29,318 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 3
2023-02-18 01:59:29,420 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 1
2023-02-18 01:59:29,654 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 0
2023-02-18 01:59:31,023 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 7
2023-02-18 01:59:31,417 DEBUG CV Batch 0/800 loss 51.891682 loss_att 97.868141 loss_ctc 64.477295 loss_rnnt 40.957558 hw_loss 0.113903 history loss 39.332906 rank 4
2023-02-18 01:59:39,406 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 5
2023-02-18 01:59:41,249 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 2
2023-02-18 01:59:41,885 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 3
2023-02-18 01:59:41,906 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 6
2023-02-18 01:59:42,122 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 0
2023-02-18 01:59:42,157 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 1
2023-02-18 01:59:43,934 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 7
2023-02-18 01:59:44,225 DEBUG CV Batch 0/900 loss 29.564400 loss_att 48.709377 loss_ctc 32.965042 loss_rnnt 25.135544 hw_loss 0.274570 history loss 40.169851 rank 4
2023-02-18 01:59:50,104 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 5
2023-02-18 01:59:52,294 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 2
2023-02-18 01:59:53,015 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 3
2023-02-18 01:59:53,243 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 0
2023-02-18 01:59:53,246 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 6
2023-02-18 01:59:53,404 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 1
2023-02-18 01:59:55,127 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 7
2023-02-18 01:59:55,382 DEBUG CV Batch 0/1000 loss 170.358566 loss_att 258.614838 loss_ctc 195.197693 loss_rnnt 149.334930 hw_loss 0.113406 history loss 42.101086 rank 4
2023-02-18 02:00:00,782 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 5
2023-02-18 02:00:03,421 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 2
2023-02-18 02:00:04,374 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 3
2023-02-18 02:00:04,576 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 0
2023-02-18 02:00:04,627 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 1
2023-02-18 02:00:04,776 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 6
2023-02-18 02:00:06,565 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 7
2023-02-18 02:00:06,797 DEBUG CV Batch 0/1100 loss 59.150944 loss_att 123.344498 loss_ctc 64.982986 loss_rnnt 45.509621 hw_loss 0.046878 history loss 41.879568 rank 4
2023-02-18 02:00:12,640 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 5
2023-02-18 02:00:15,503 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 2
2023-02-18 02:00:16,491 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 3
2023-02-18 02:00:16,741 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 6
2023-02-18 02:00:16,750 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 0
2023-02-18 02:00:16,824 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 1
2023-02-18 02:00:18,697 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 7
2023-02-18 02:00:18,931 DEBUG CV Batch 0/1200 loss 54.118542 loss_att 87.259308 loss_ctc 63.371914 loss_rnnt 46.202644 hw_loss 0.101169 history loss 42.369060 rank 4
2023-02-18 02:00:25,580 DEBUG CV Batch 0/1300 loss 19.456367 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 5
2023-02-18 02:00:29,288 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 2
2023-02-18 02:00:29,601 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 3
2023-02-18 02:00:29,846 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 6
2023-02-18 02:00:30,111 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 1
2023-02-18 02:00:30,365 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 0
2023-02-18 02:00:31,918 DEBUG CV Batch 0/1300 loss 19.456367 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 7
2023-02-18 02:00:31,991 DEBUG CV Batch 0/1300 loss 19.456366 loss_att 27.412636 loss_ctc 23.705585 loss_rnnt 17.189987 hw_loss 0.203555 history loss 43.821274 rank 4
2023-02-18 02:00:36,247 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 5
2023-02-18 02:00:40,077 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 2
2023-02-18 02:00:40,404 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 3
2023-02-18 02:00:40,695 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 6
2023-02-18 02:00:41,169 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 1
2023-02-18 02:00:41,204 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 0
2023-02-18 02:00:42,724 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 7
2023-02-18 02:00:42,978 DEBUG CV Batch 0/1400 loss 327.089233 loss_att 457.485992 loss_ctc 395.710724 loss_rnnt 291.860291 hw_loss 0.000139 history loss 44.715409 rank 4
2023-02-18 02:00:45,966 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 5
2023-02-18 02:00:50,190 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 2
2023-02-18 02:00:50,683 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 3
2023-02-18 02:00:50,755 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 6
2023-02-18 02:00:51,261 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 1
2023-02-18 02:00:51,429 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 0
2023-02-18 02:00:53,038 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 7
2023-02-18 02:00:53,960 DEBUG CV Batch 0/1500 loss 61.326042 loss_att 122.358162 loss_ctc 62.318375 loss_rnnt 48.890167 hw_loss 0.182130 history loss 44.279233 rank 4
2023-02-18 02:00:57,892 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 5
2023-02-18 02:01:02,391 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 2
2023-02-18 02:01:02,927 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 3
2023-02-18 02:01:03,029 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 6
2023-02-18 02:01:03,640 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 1
2023-02-18 02:01:04,153 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 0
2023-02-18 02:01:05,233 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 7
2023-02-18 02:01:06,270 DEBUG CV Batch 0/1600 loss 65.783257 loss_att 116.208687 loss_ctc 68.402313 loss_rnnt 55.267357 hw_loss 0.153000 history loss 43.584007 rank 4
2023-02-18 02:01:10,819 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 5
2023-02-18 02:01:15,476 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 2
2023-02-18 02:01:16,079 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 3
2023-02-18 02:01:16,799 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 1
2023-02-18 02:01:17,267 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 6
2023-02-18 02:01:17,271 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 0
2023-02-18 02:01:18,307 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 7
2023-02-18 02:01:19,243 DEBUG CV Batch 0/1700 loss 24.718153 loss_att 51.434898 loss_ctc 25.278856 loss_rnnt 19.242125 hw_loss 0.108602 history loss 43.348717 rank 4
2023-02-18 02:01:22,256 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 5
2023-02-18 02:01:27,431 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 2
2023-02-18 02:01:28,004 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 3
2023-02-18 02:01:28,757 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 1
2023-02-18 02:01:29,216 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 0
2023-02-18 02:01:29,956 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 6
2023-02-18 02:01:30,405 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 7
2023-02-18 02:01:31,241 DEBUG CV Batch 0/1800 loss 156.956421 loss_att 344.066162 loss_ctc 193.693939 loss_rnnt 114.604370 hw_loss 0.059555 history loss 43.677160 rank 4
2023-02-18 02:01:33,305 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 5
2023-02-18 02:01:39,284 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 3
2023-02-18 02:01:39,522 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 2
2023-02-18 02:01:40,115 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 1
2023-02-18 02:01:40,626 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 0
2023-02-18 02:01:41,113 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 6
2023-02-18 02:01:42,003 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 7
2023-02-18 02:01:43,689 DEBUG CV Batch 0/1900 loss 37.366833 loss_att 74.820778 loss_ctc 35.938507 loss_rnnt 29.956154 hw_loss 0.206875 history loss 42.868144 rank 4
2023-02-18 02:01:49,236 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 5
2023-02-18 02:01:55,077 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 3
2023-02-18 02:01:55,554 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 2
2023-02-18 02:01:56,253 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 1
2023-02-18 02:01:56,640 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 0
2023-02-18 02:01:56,783 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 6
2023-02-18 02:01:58,446 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 7
2023-02-18 02:01:59,721 DEBUG CV Batch 0/2000 loss 36.482395 loss_att 63.342484 loss_ctc 39.128036 loss_rnnt 30.757549 hw_loss 0.000139 history loss 42.885783 rank 4
2023-02-18 02:01:59,933 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 5
2023-02-18 02:02:05,982 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 3
2023-02-18 02:02:06,752 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 2
2023-02-18 02:02:07,136 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 1
2023-02-18 02:02:07,609 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 0
2023-02-18 02:02:07,619 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 6
2023-02-18 02:02:09,661 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 7
2023-02-18 02:02:10,741 DEBUG CV Batch 0/2100 loss 118.959557 loss_att 261.684021 loss_ctc 128.258331 loss_rnnt 89.174744 hw_loss 0.000139 history loss 43.202771 rank 4
2023-02-18 02:02:12,966 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 5
2023-02-18 02:02:19,071 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 3
2023-02-18 02:02:19,716 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 2
2023-02-18 02:02:20,438 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 1
2023-02-18 02:02:20,553 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 6
2023-02-18 02:02:20,599 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 0
2023-02-18 02:02:23,559 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 7
2023-02-18 02:02:24,347 DEBUG CV Batch 0/2200 loss 53.229446 loss_att 109.207047 loss_ctc 58.461788 loss_rnnt 41.311371 hw_loss 0.046704 history loss 42.897090 rank 4
2023-02-18 02:02:24,627 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 5
2023-02-18 02:02:30,614 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 3
2023-02-18 02:02:31,617 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 2
2023-02-18 02:02:32,257 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 1
2023-02-18 02:02:32,260 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 6
2023-02-18 02:02:32,398 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 0
2023-02-18 02:02:36,240 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 4
2023-02-18 02:02:36,499 DEBUG CV Batch 0/2300 loss 25.170752 loss_att 55.705299 loss_ctc 27.184038 loss_rnnt 18.670952 hw_loss 0.233345 history loss 42.690053 rank 7
2023-02-18 02:02:36,744 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 5
2023-02-18 02:02:42,784 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 3
2023-02-18 02:02:43,916 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 2
2023-02-18 02:02:44,469 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 6
2023-02-18 02:02:44,709 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 0
2023-02-18 02:02:44,735 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 1
2023-02-18 02:02:46,651 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 5
2023-02-18 02:02:48,677 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 4
2023-02-18 02:02:49,096 DEBUG CV Batch 0/2400 loss 16.147715 loss_att 25.863684 loss_ctc 17.176109 loss_rnnt 13.991658 hw_loss 0.142015 history loss 42.919160 rank 7
2023-02-18 02:02:53,065 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 3
2023-02-18 02:02:53,898 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 2
2023-02-18 02:02:54,516 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 6
2023-02-18 02:02:54,960 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 1
2023-02-18 02:02:55,363 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 0
2023-02-18 02:02:57,994 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 5
2023-02-18 02:02:58,907 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 4
2023-02-18 02:02:59,729 DEBUG CV Batch 0/2500 loss 146.176392 loss_att 232.741669 loss_ctc 160.998260 loss_rnnt 126.887009 hw_loss 0.000139 history loss 43.278391 rank 7
2023-02-18 02:03:04,463 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 3
2023-02-18 02:03:05,467 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 2
2023-02-18 02:03:05,798 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 6
2023-02-18 02:03:06,374 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 1
2023-02-18 02:03:07,170 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 0
2023-02-18 02:03:10,299 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 4
2023-02-18 02:03:10,600 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 5
2023-02-18 02:03:11,411 DEBUG CV Batch 0/2600 loss 54.248608 loss_att 96.621719 loss_ctc 64.619263 loss_rnnt 44.391155 hw_loss 0.000139 history loss 43.297177 rank 7
2023-02-18 02:03:17,380 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 3
2023-02-18 02:03:18,300 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 2
2023-02-18 02:03:18,755 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 6
2023-02-18 02:03:19,349 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 1
2023-02-18 02:03:20,166 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 0
2023-02-18 02:03:21,023 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 5
2023-02-18 02:03:23,224 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 4
2023-02-18 02:03:24,219 DEBUG CV Batch 0/2700 loss 45.122261 loss_att 65.709877 loss_ctc 54.682644 loss_rnnt 39.604240 hw_loss 0.235844 history loss 43.484671 rank 7
2023-02-18 02:03:28,038 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 3
2023-02-18 02:03:28,953 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 2
2023-02-18 02:03:29,941 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 6
2023-02-18 02:03:30,051 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 1
2023-02-18 02:03:30,824 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 0
2023-02-18 02:03:32,138 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 5
2023-02-18 02:03:33,601 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 4
2023-02-18 02:03:34,857 DEBUG CV Batch 0/2800 loss 91.430550 loss_att 188.136536 loss_ctc 87.084618 loss_rnnt 72.668739 hw_loss 0.000139 history loss 44.026324 rank 7
2023-02-18 02:03:39,282 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 3
2023-02-18 02:03:40,408 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 2
2023-02-18 02:03:41,199 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 6
2023-02-18 02:03:41,429 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 1
2023-02-18 02:03:43,070 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 0
2023-02-18 02:03:43,286 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 5
2023-02-18 02:03:44,846 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 4
2023-02-18 02:03:46,431 DEBUG CV Batch 0/2900 loss 120.885536 loss_att 196.449005 loss_ctc 132.495346 loss_rnnt 104.146835 hw_loss 0.146305 history loss 44.229282 rank 7
2023-02-18 02:03:50,591 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 3
2023-02-18 02:03:51,899 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 2
2023-02-18 02:03:52,524 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 6
2023-02-18 02:03:52,726 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 1
2023-02-18 02:03:54,888 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 0
2023-02-18 02:03:55,469 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 5
2023-02-18 02:03:56,044 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 4
2023-02-18 02:03:58,041 DEBUG CV Batch 0/3000 loss 46.050346 loss_att 84.094734 loss_ctc 52.593864 loss_rnnt 37.532509 hw_loss 0.068415 history loss 44.212732 rank 7
2023-02-18 02:04:02,953 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 3
2023-02-18 02:04:04,290 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 2
2023-02-18 02:04:04,881 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 6
2023-02-18 02:04:05,377 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 1
2023-02-18 02:04:07,296 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 0
2023-02-18 02:04:08,143 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 5
2023-02-18 02:04:08,370 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 4
2023-02-18 02:04:10,521 DEBUG CV Batch 0/3100 loss 44.996929 loss_att 76.204102 loss_ctc 55.609520 loss_rnnt 37.340408 hw_loss 0.000139 history loss 43.980907 rank 7
2023-02-18 02:04:15,527 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 3
2023-02-18 02:04:17,374 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 2
2023-02-18 02:04:17,666 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 6
2023-02-18 02:04:18,564 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 1
2023-02-18 02:04:18,604 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 5
2023-02-18 02:04:20,065 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 0
2023-02-18 02:04:21,194 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 4
2023-02-18 02:04:23,205 DEBUG CV Batch 0/3200 loss 16.267036 loss_att 33.566753 loss_ctc 18.991812 loss_rnnt 12.384187 hw_loss 0.111750 history loss 44.051106 rank 7
2023-02-18 02:04:26,049 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 3
2023-02-18 02:04:27,879 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 2
2023-02-18 02:04:28,190 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 6
2023-02-18 02:04:29,266 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 1
2023-02-18 02:04:30,976 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 0
2023-02-18 02:04:31,148 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 5
2023-02-18 02:04:31,993 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 4
2023-02-18 02:04:33,794 DEBUG CV Batch 0/3300 loss 99.314476 loss_att 196.386139 loss_ctc 104.632019 loss_rnnt 79.150131 hw_loss 0.076876 history loss 44.024232 rank 7
2023-02-18 02:04:38,886 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 3
2023-02-18 02:04:40,634 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 2
2023-02-18 02:04:40,823 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 6
2023-02-18 02:04:42,092 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 1
2023-02-18 02:04:43,730 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 0
2023-02-18 02:04:44,891 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 4
2023-02-18 02:04:46,756 DEBUG CV Batch 0/3400 loss 41.978291 loss_att 80.380745 loss_ctc 39.876842 loss_rnnt 34.548943 hw_loss 0.054467 history loss 43.695849 rank 7
2023-02-18 02:04:46,771 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 5
2023-02-18 02:04:54,692 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 3
2023-02-18 02:04:56,403 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 2
2023-02-18 02:04:56,488 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 6
2023-02-18 02:04:57,192 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 5
2023-02-18 02:04:58,041 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 1
2023-02-18 02:04:59,495 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 0
2023-02-18 02:05:00,653 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 4
2023-02-18 02:05:02,576 DEBUG CV Batch 0/3500 loss 212.910706 loss_att 468.471497 loss_ctc 248.378525 loss_rnnt 157.005188 hw_loss 0.120609 history loss 44.091389 rank 7
2023-02-18 02:05:05,284 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 3
2023-02-18 02:05:06,875 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 2
2023-02-18 02:05:07,025 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 6
2023-02-18 02:05:08,661 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 1
2023-02-18 02:05:10,023 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 0
2023-02-18 02:05:10,857 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 5
2023-02-18 02:05:11,359 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 4
2023-02-18 02:05:13,594 DEBUG CV Batch 0/3600 loss 83.784676 loss_att 169.683823 loss_ctc 95.240746 loss_rnnt 64.989120 hw_loss 0.165458 history loss 43.771799 rank 7
2023-02-18 02:05:19,076 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 3
2023-02-18 02:05:20,803 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 6
2023-02-18 02:05:20,931 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 2
2023-02-18 02:05:21,819 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 5
2023-02-18 02:05:22,581 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 1
2023-02-18 02:05:24,490 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 0
2023-02-18 02:05:25,241 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 4
2023-02-18 02:05:27,851 DEBUG CV Batch 0/3700 loss 40.971241 loss_att 75.996422 loss_ctc 47.583279 loss_rnnt 32.995953 hw_loss 0.166217 history loss 43.666893 rank 7
2023-02-18 02:05:30,379 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 3
2023-02-18 02:05:30,430 INFO Epoch 0 CV info cv_loss 43.81612081051493
2023-02-18 02:05:30,430 INFO Epoch 1 TRAIN info lr 0.00040427999999999997
2023-02-18 02:05:30,433 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:32,074 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 6
2023-02-18 02:05:32,414 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 2
2023-02-18 02:05:34,049 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 1
2023-02-18 02:05:36,199 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 0
2023-02-18 02:05:36,625 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 4
2023-02-18 02:05:39,112 INFO Epoch 0 CV info cv_loss 43.81612080913659
2023-02-18 02:05:39,113 INFO Epoch 1 TRAIN info lr 0.00040303999999999996
2023-02-18 02:05:39,117 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:39,179 DEBUG CV Batch 0/3800 loss 25.137352 loss_att 55.263245 loss_ctc 29.771523 loss_rnnt 18.380722 hw_loss 0.212929 history loss 43.714273 rank 7
2023-02-18 02:05:40,646 INFO Epoch 0 CV info cv_loss 43.81612080913659
2023-02-18 02:05:40,646 INFO Epoch 1 TRAIN info lr 0.00040436
2023-02-18 02:05:40,650 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:41,148 INFO Epoch 0 CV info cv_loss 43.81612080913659
2023-02-18 02:05:41,148 INFO Epoch 1 TRAIN info lr 0.00040444
2023-02-18 02:05:41,152 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:42,984 INFO Epoch 0 CV info cv_loss 43.81612080996359
2023-02-18 02:05:42,986 INFO Epoch 1 TRAIN info lr 0.00040599999999999995
2023-02-18 02:05:42,991 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:45,401 INFO Epoch 0 CV info cv_loss 43.81612080913659
2023-02-18 02:05:45,402 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word_22/0.pt
2023-02-18 02:05:45,476 INFO Epoch 0 CV info cv_loss 43.81612080913659
2023-02-18 02:05:45,477 INFO Epoch 1 TRAIN info lr 0.00040332
2023-02-18 02:05:45,483 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:47,031 INFO Epoch 1 TRAIN info lr 0.00040396
2023-02-18 02:05:47,035 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:05:48,264 INFO Epoch 0 CV info cv_loss 43.81612081051493
2023-02-18 02:05:48,264 INFO Epoch 1 TRAIN info lr 0.00040448
2023-02-18 02:05:48,269 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 02:07:07,119 DEBUG TRAIN Batch 1/0 loss 27.096632 loss_att 33.283653 loss_ctc 31.565609 loss_rnnt 25.168406 hw_loss 0.178045 lr 0.00040448 rank 2
2023-02-18 02:07:07,122 DEBUG TRAIN Batch 1/0 loss 25.905710 loss_att 29.867872 loss_ctc 28.008078 loss_rnnt 24.713762 hw_loss 0.223497 lr 0.00040604 rank 1
2023-02-18 02:07:07,124 DEBUG TRAIN Batch 1/0 loss 25.707575 loss_att 33.407364 loss_ctc 29.023743 loss_rnnt 23.674391 hw_loss 0.095760 lr 0.00040432 rank 5
2023-02-18 02:07:07,126 DEBUG TRAIN Batch 1/0 loss 27.264877 loss_att 34.995789 loss_ctc 29.342371 loss_rnnt 25.302036 hw_loss 0.261858 lr 0.00040308 rank 3
2023-02-18 02:07:07,130 DEBUG TRAIN Batch 1/0 loss 24.831190 loss_att 31.198240 loss_ctc 29.058437 loss_rnnt 22.876047 hw_loss 0.221439 lr 0.00040452 rank 7
2023-02-18 02:07:07,159 DEBUG TRAIN Batch 1/0 loss 25.384422 loss_att 31.621437 loss_ctc 30.045517 loss_rnnt 23.441263 hw_loss 0.139269 lr 0.00040336 rank 4
2023-02-18 02:07:07,169 DEBUG TRAIN Batch 1/0 loss 27.004044 loss_att 31.430065 loss_ctc 31.629700 loss_rnnt 25.384047 hw_loss 0.221326 lr 0.00040440 rank 6
2023-02-18 02:07:07,218 DEBUG TRAIN Batch 1/0 loss 23.618628 loss_att 30.080334 loss_ctc 27.032639 loss_rnnt 21.752882 hw_loss 0.221631 lr 0.00040400 rank 0
2023-02-18 02:08:04,586 DEBUG TRAIN Batch 1/100 loss 107.096916 loss_att 214.644287 loss_ctc 116.190025 loss_rnnt 84.374969 hw_loss 0.000098 lr 0.00040536 rank 4
2023-02-18 02:08:04,594 DEBUG TRAIN Batch 1/100 loss 83.264259 loss_att 163.491791 loss_ctc 101.026588 loss_rnnt 64.817833 hw_loss 0.061131 lr 0.00040508 rank 3
2023-02-18 02:08:04,596 DEBUG TRAIN Batch 1/100 loss 69.901840 loss_att 149.127838 loss_ctc 75.739929 loss_rnnt 53.245720 hw_loss 0.060964 lr 0.00040600 rank 0
2023-02-18 02:08:04,598 DEBUG TRAIN Batch 1/100 loss 88.193321 loss_att 146.915344 loss_ctc 100.005943 loss_rnnt 74.873856 hw_loss 0.000092 lr 0.00040652 rank 7
2023-02-18 02:08:04,598 DEBUG TRAIN Batch 1/100 loss 75.796204 loss_att 134.209259 loss_ctc 84.595444 loss_rnnt 62.940292 hw_loss 0.000123 lr 0.00040804 rank 1
2023-02-18 02:08:04,598 DEBUG TRAIN Batch 1/100 loss 79.915154 loss_att 149.890320 loss_ctc 86.912750 loss_rnnt 64.966591 hw_loss 0.038458 lr 0.00040632 rank 5
2023-02-18 02:08:04,600 DEBUG TRAIN Batch 1/100 loss 77.278809 loss_att 153.948669 loss_ctc 87.930130 loss_rnnt 60.412323 hw_loss 0.210633 lr 0.00040640 rank 6
2023-02-18 02:08:04,663 DEBUG TRAIN Batch 1/100 loss 86.038727 loss_att 183.514740 loss_ctc 96.638657 loss_rnnt 65.130142 hw_loss 0.000087 lr 0.00040648 rank 2
2023-02-18 02:09:04,828 DEBUG TRAIN Batch 1/200 loss 85.783501 loss_att 164.820084 loss_ctc 66.029861 loss_rnnt 72.515305 hw_loss 0.177558 lr 0.00040848 rank 2
2023-02-18 02:09:04,830 DEBUG TRAIN Batch 1/200 loss 97.162109 loss_att 203.660553 loss_ctc 84.021278 loss_rnnt 77.608589 hw_loss 0.011141 lr 0.00040708 rank 3
2023-02-18 02:09:04,831 DEBUG TRAIN Batch 1/200 loss 79.075798 loss_att 181.439941 loss_ctc 87.599075 loss_rnnt 57.421654 hw_loss 0.084158 lr 0.00040736 rank 4
2023-02-18 02:09:04,832 DEBUG TRAIN Batch 1/200 loss 70.667282 loss_att 144.454788 loss_ctc 67.368874 loss_rnnt 56.349419 hw_loss 0.000287 lr 0.00040852 rank 7
2023-02-18 02:09:04,831 DEBUG TRAIN Batch 1/200 loss 111.364037 loss_att 193.844482 loss_ctc 132.567856 loss_rnnt 91.999374 hw_loss 0.077621 lr 0.00041004 rank 1
2023-02-18 02:09:04,834 DEBUG TRAIN Batch 1/200 loss 68.421982 loss_att 144.867538 loss_ctc 75.414932 loss_rnnt 52.200272 hw_loss 0.000389 lr 0.00040800 rank 0
2023-02-18 02:09:04,837 DEBUG TRAIN Batch 1/200 loss 65.002960 loss_att 162.954178 loss_ctc 68.333199 loss_rnnt 44.968544 hw_loss 0.000253 lr 0.00040832 rank 5
2023-02-18 02:09:04,839 DEBUG TRAIN Batch 1/200 loss 90.368500 loss_att 185.240845 loss_ctc 98.232132 loss_rnnt 70.196396 hw_loss 0.279657 lr 0.00040840 rank 6
2023-02-18 02:10:02,509 DEBUG TRAIN Batch 1/300 loss 51.052486 loss_att 76.217987 loss_ctc 53.324047 loss_rnnt 45.639858 hw_loss 0.143732 lr 0.00041040 rank 6
2023-02-18 02:10:02,510 DEBUG TRAIN Batch 1/300 loss 47.974442 loss_att 85.482895 loss_ctc 53.114204 loss_rnnt 39.753944 hw_loss 0.062828 lr 0.00041052 rank 7
2023-02-18 02:10:02,514 DEBUG TRAIN Batch 1/300 loss 51.244717 loss_att 87.132736 loss_ctc 56.368431 loss_rnnt 43.278576 hw_loss 0.197572 lr 0.00040936 rank 4
2023-02-18 02:10:02,515 DEBUG TRAIN Batch 1/300 loss 75.853470 loss_att 128.482391 loss_ctc 93.769867 loss_rnnt 62.875866 hw_loss 0.118057 lr 0.00041000 rank 0
2023-02-18 02:10:02,515 DEBUG TRAIN Batch 1/300 loss 66.404099 loss_att 111.649673 loss_ctc 76.877213 loss_rnnt 55.904343 hw_loss 0.101665 lr 0.00041032 rank 5
2023-02-18 02:10:02,515 DEBUG TRAIN Batch 1/300 loss 55.739346 loss_att 93.862488 loss_ctc 59.027946 loss_rnnt 47.587257 hw_loss 0.166840 lr 0.00041048 rank 2
2023-02-18 02:10:02,517 DEBUG TRAIN Batch 1/300 loss 96.977386 loss_att 187.870941 loss_ctc 106.797318 loss_rnnt 77.413620 hw_loss 0.142010 lr 0.00040908 rank 3
2023-02-18 02:10:02,519 DEBUG TRAIN Batch 1/300 loss 47.015694 loss_att 92.594818 loss_ctc 50.366081 loss_rnnt 37.357716 hw_loss 0.178925 lr 0.00041204 rank 1
2023-02-18 02:11:00,151 DEBUG TRAIN Batch 1/400 loss 70.614647 loss_att 143.920914 loss_ctc 79.043961 loss_rnnt 54.820213 hw_loss 0.017379 lr 0.00041232 rank 5
2023-02-18 02:11:00,155 DEBUG TRAIN Batch 1/400 loss 70.993774 loss_att 161.769440 loss_ctc 71.516701 loss_rnnt 52.768711 hw_loss 0.000389 lr 0.00041200 rank 0
2023-02-18 02:11:00,156 DEBUG TRAIN Batch 1/400 loss 72.360184 loss_att 150.296692 loss_ctc 69.013054 loss_rnnt 57.147285 hw_loss 0.134767 lr 0.00041248 rank 2
2023-02-18 02:11:00,157 DEBUG TRAIN Batch 1/400 loss 59.717083 loss_att 141.842407 loss_ctc 67.694229 loss_rnnt 42.220119 hw_loss 0.015515 lr 0.00041404 rank 1
2023-02-18 02:11:00,161 DEBUG TRAIN Batch 1/400 loss 84.451546 loss_att 160.775192 loss_ctc 97.867874 loss_rnnt 67.397766 hw_loss 0.000380 lr 0.00041108 rank 3
2023-02-18 02:11:00,161 DEBUG TRAIN Batch 1/400 loss 100.423317 loss_att 206.807953 loss_ctc 114.008621 loss_rnnt 77.314331 hw_loss 0.038777 lr 0.00041136 rank 4
2023-02-18 02:11:00,160 DEBUG TRAIN Batch 1/400 loss 83.995903 loss_att 163.601059 loss_ctc 91.606422 loss_rnnt 66.998184 hw_loss 0.116154 lr 0.00041240 rank 6
2023-02-18 02:11:00,161 DEBUG TRAIN Batch 1/400 loss 109.434601 loss_att 199.662048 loss_ctc 119.680565 loss_rnnt 89.965401 hw_loss 0.107965 lr 0.00041252 rank 7
2023-02-18 02:11:58,941 DEBUG TRAIN Batch 1/500 loss 50.951939 loss_att 126.255737 loss_ctc 55.254112 loss_rnnt 35.226845 hw_loss 0.170077 lr 0.00041432 rank 5
2023-02-18 02:11:58,942 DEBUG TRAIN Batch 1/500 loss 85.293373 loss_att 171.475632 loss_ctc 96.058434 loss_rnnt 66.621483 hw_loss 0.000198 lr 0.00041440 rank 6
2023-02-18 02:11:58,944 DEBUG TRAIN Batch 1/500 loss 89.591621 loss_att 167.941574 loss_ctc 107.588837 loss_rnnt 71.521965 hw_loss 0.000078 lr 0.00041336 rank 4
2023-02-18 02:11:58,946 DEBUG TRAIN Batch 1/500 loss 93.139732 loss_att 206.013443 loss_ctc 112.593895 loss_rnnt 67.970688 hw_loss 0.000789 lr 0.00041308 rank 3
2023-02-18 02:11:58,949 DEBUG TRAIN Batch 1/500 loss 75.973549 loss_att 160.768707 loss_ctc 89.094261 loss_rnnt 57.264942 hw_loss 0.000258 lr 0.00041452 rank 7
2023-02-18 02:11:58,952 DEBUG TRAIN Batch 1/500 loss 145.584732 loss_att 262.779327 loss_ctc 176.544815 loss_rnnt 118.017700 hw_loss 0.000204 lr 0.00041448 rank 2
2023-02-18 02:11:58,979 DEBUG TRAIN Batch 1/500 loss 88.802559 loss_att 175.363297 loss_ctc 97.325592 loss_rnnt 70.353760 hw_loss 0.000463 lr 0.00041400 rank 0
2023-02-18 02:11:58,984 DEBUG TRAIN Batch 1/500 loss 73.008766 loss_att 156.658875 loss_ctc 75.733536 loss_rnnt 55.755081 hw_loss 0.300681 lr 0.00041604 rank 1
2023-02-18 02:12:58,150 DEBUG TRAIN Batch 1/600 loss 64.623650 loss_att 140.845520 loss_ctc 71.332367 loss_rnnt 48.394783 hw_loss 0.168748 lr 0.00041600 rank 0
2023-02-18 02:12:58,151 DEBUG TRAIN Batch 1/600 loss 81.269058 loss_att 131.571823 loss_ctc 92.418991 loss_rnnt 69.693321 hw_loss 0.053491 lr 0.00041648 rank 2
2023-02-18 02:12:58,151 DEBUG TRAIN Batch 1/600 loss 65.923470 loss_att 137.276840 loss_ctc 71.599121 loss_rnnt 50.829922 hw_loss 0.123985 lr 0.00041632 rank 5
2023-02-18 02:12:58,151 DEBUG TRAIN Batch 1/600 loss 54.366943 loss_att 95.769310 loss_ctc 62.753326 loss_rnnt 44.896385 hw_loss 0.134811 lr 0.00041536 rank 4
2023-02-18 02:12:58,152 DEBUG TRAIN Batch 1/600 loss 63.710922 loss_att 146.545670 loss_ctc 59.814102 loss_rnnt 47.628014 hw_loss 0.066627 lr 0.00041804 rank 1
2023-02-18 02:12:58,154 DEBUG TRAIN Batch 1/600 loss 57.904781 loss_att 91.218727 loss_ctc 66.634010 loss_rnnt 49.967262 hw_loss 0.207796 lr 0.00041640 rank 6
2023-02-18 02:12:58,155 DEBUG TRAIN Batch 1/600 loss 25.909039 loss_att 32.706070 loss_ctc 30.399090 loss_rnnt 23.849983 hw_loss 0.189329 lr 0.00041508 rank 3
2023-02-18 02:12:58,220 DEBUG TRAIN Batch 1/600 loss 59.361485 loss_att 116.801231 loss_ctc 61.795082 loss_rnnt 47.496479 hw_loss 0.098580 lr 0.00041652 rank 7
2023-02-18 02:13:57,571 DEBUG TRAIN Batch 1/700 loss 61.602482 loss_att 138.098541 loss_ctc 61.817120 loss_rnnt 46.235580 hw_loss 0.073255 lr 0.00041840 rank 6
2023-02-18 02:13:57,572 DEBUG TRAIN Batch 1/700 loss 80.077179 loss_att 181.154541 loss_ctc 87.252014 loss_rnnt 58.904980 hw_loss 0.000151 lr 0.00041832 rank 5
2023-02-18 02:13:57,572 DEBUG TRAIN Batch 1/700 loss 99.922630 loss_att 199.537567 loss_ctc 98.820297 loss_rnnt 80.120430 hw_loss 0.049108 lr 0.00042004 rank 1
2023-02-18 02:13:57,582 DEBUG TRAIN Batch 1/700 loss 42.384468 loss_att 105.892693 loss_ctc 39.241127 loss_rnnt 30.072113 hw_loss 0.055914 lr 0.00041708 rank 3
2023-02-18 02:13:57,583 DEBUG TRAIN Batch 1/700 loss 84.342201 loss_att 162.685135 loss_ctc 85.938889 loss_rnnt 68.400177 hw_loss 0.113526 lr 0.00041736 rank 4
2023-02-18 02:13:57,589 DEBUG TRAIN Batch 1/700 loss 83.964645 loss_att 167.445984 loss_ctc 105.528610 loss_rnnt 64.295395 hw_loss 0.183355 lr 0.00041848 rank 2
2023-02-18 02:13:57,600 DEBUG TRAIN Batch 1/700 loss 78.091637 loss_att 171.116180 loss_ctc 88.982780 loss_rnnt 57.992317 hw_loss 0.079237 lr 0.00041852 rank 7
2023-02-18 02:13:57,612 DEBUG TRAIN Batch 1/700 loss 99.811264 loss_att 164.990494 loss_ctc 109.711624 loss_rnnt 85.395340 hw_loss 0.112556 lr 0.00041800 rank 0
2023-02-18 02:14:58,548 DEBUG TRAIN Batch 1/800 loss 118.568077 loss_att 208.566193 loss_ctc 143.680649 loss_rnnt 97.148132 hw_loss 0.134970 lr 0.00042032 rank 5
2023-02-18 02:14:58,550 DEBUG TRAIN Batch 1/800 loss 37.640011 loss_att 98.698685 loss_ctc 37.540859 loss_rnnt 25.441450 hw_loss 0.000085 lr 0.00042052 rank 7
2023-02-18 02:14:58,552 DEBUG TRAIN Batch 1/800 loss 68.340218 loss_att 150.169922 loss_ctc 79.228104 loss_rnnt 50.460659 hw_loss 0.116065 lr 0.00042040 rank 6
2023-02-18 02:14:58,554 DEBUG TRAIN Batch 1/800 loss 54.160454 loss_att 148.711151 loss_ctc 66.992195 loss_rnnt 33.539322 hw_loss 0.000175 lr 0.00042000 rank 0
2023-02-18 02:14:58,557 DEBUG TRAIN Batch 1/800 loss 62.600231 loss_att 157.043747 loss_ctc 70.177437 loss_rnnt 42.677341 hw_loss 0.044797 lr 0.00041936 rank 4
2023-02-18 02:14:58,561 DEBUG TRAIN Batch 1/800 loss 86.719940 loss_att 191.059555 loss_ctc 82.844055 loss_rnnt 66.303589 hw_loss 0.122285 lr 0.00041908 rank 3
2023-02-18 02:14:58,561 DEBUG TRAIN Batch 1/800 loss 57.430477 loss_att 166.250580 loss_ctc 52.962326 loss_rnnt 36.163651 hw_loss 0.184797 lr 0.00042204 rank 1
2023-02-18 02:14:58,566 DEBUG TRAIN Batch 1/800 loss 52.281830 loss_att 111.777222 loss_ctc 51.472065 loss_rnnt 40.490677 hw_loss 0.000077 lr 0.00042048 rank 2
2023-02-18 02:16:20,833 DEBUG TRAIN Batch 1/900 loss 53.227932 loss_att 108.418915 loss_ctc 62.321793 loss_rnnt 40.930389 hw_loss 0.087805 lr 0.00042200 rank 0
2023-02-18 02:16:20,833 DEBUG TRAIN Batch 1/900 loss 64.484314 loss_att 106.718307 loss_ctc 71.651550 loss_rnnt 55.053509 hw_loss 0.053212 lr 0.00042136 rank 4
2023-02-18 02:16:20,834 DEBUG TRAIN Batch 1/900 loss 52.176323 loss_att 107.142975 loss_ctc 62.284435 loss_rnnt 39.835190 hw_loss 0.000096 lr 0.00042232 rank 5
2023-02-18 02:16:20,836 DEBUG TRAIN Batch 1/900 loss 62.404129 loss_att 131.762070 loss_ctc 70.071396 loss_rnnt 47.510197 hw_loss 0.000074 lr 0.00042252 rank 7
2023-02-18 02:16:20,837 DEBUG TRAIN Batch 1/900 loss 72.508911 loss_att 120.803360 loss_ctc 87.659843 loss_rnnt 60.792603 hw_loss 0.069917 lr 0.00042240 rank 6
2023-02-18 02:16:20,840 DEBUG TRAIN Batch 1/900 loss 93.310997 loss_att 151.584198 loss_ctc 106.625992 loss_rnnt 79.850685 hw_loss 0.056875 lr 0.00042248 rank 2
2023-02-18 02:16:20,842 DEBUG TRAIN Batch 1/900 loss 38.810635 loss_att 81.523071 loss_ctc 46.494896 loss_rnnt 29.158300 hw_loss 0.159891 lr 0.00042108 rank 3
2023-02-18 02:16:20,902 DEBUG TRAIN Batch 1/900 loss 56.031460 loss_att 124.947655 loss_ctc 53.048935 loss_rnnt 42.532856 hw_loss 0.211935 lr 0.00042404 rank 1
2023-02-18 02:17:21,823 DEBUG TRAIN Batch 1/1000 loss 41.793819 loss_att 96.528717 loss_ctc 46.423038 loss_rnnt 30.229481 hw_loss 0.000234 lr 0.00042604 rank 1
2023-02-18 02:17:21,826 DEBUG TRAIN Batch 1/1000 loss 72.765869 loss_att 136.862152 loss_ctc 78.194122 loss_rnnt 59.222698 hw_loss 0.000267 lr 0.00042400 rank 0
2023-02-18 02:17:21,827 DEBUG TRAIN Batch 1/1000 loss 87.420288 loss_att 177.135315 loss_ctc 97.839859 loss_rnnt 68.056396 hw_loss 0.059269 lr 0.00042432 rank 5
2023-02-18 02:17:21,829 DEBUG TRAIN Batch 1/1000 loss 84.713562 loss_att 165.731873 loss_ctc 85.709129 loss_rnnt 68.309822 hw_loss 0.126242 lr 0.00042448 rank 2
2023-02-18 02:17:21,829 DEBUG TRAIN Batch 1/1000 loss 83.692268 loss_att 169.648315 loss_ctc 100.416885 loss_rnnt 64.228752 hw_loss 0.079422 lr 0.00042440 rank 6
2023-02-18 02:17:21,830 DEBUG TRAIN Batch 1/1000 loss 72.384598 loss_att 155.641541 loss_ctc 78.823944 loss_rnnt 54.759750 hw_loss 0.215392 lr 0.00042452 rank 7
2023-02-18 02:17:21,831 DEBUG TRAIN Batch 1/1000 loss 78.447449 loss_att 184.642715 loss_ctc 94.301193 loss_rnnt 55.039925 hw_loss 0.102447 lr 0.00042308 rank 3
2023-02-18 02:17:21,893 DEBUG TRAIN Batch 1/1000 loss 84.179497 loss_att 174.310913 loss_ctc 95.028656 loss_rnnt 64.705627 hw_loss 0.001932 lr 0.00042336 rank 4
2023-02-18 02:18:20,547 DEBUG TRAIN Batch 1/1100 loss 102.250801 loss_att 183.564255 loss_ctc 102.080254 loss_rnnt 85.862061 hw_loss 0.278978 lr 0.00042632 rank 5
2023-02-18 02:18:20,549 DEBUG TRAIN Batch 1/1100 loss 59.079388 loss_att 99.799698 loss_ctc 65.533943 loss_rnnt 50.009365 hw_loss 0.122535 lr 0.00042652 rank 7
2023-02-18 02:18:20,552 DEBUG TRAIN Batch 1/1100 loss 108.941559 loss_att 212.606293 loss_ctc 112.711357 loss_rnnt 87.628250 hw_loss 0.145729 lr 0.00042508 rank 3
2023-02-18 02:18:20,555 DEBUG TRAIN Batch 1/1100 loss 66.915092 loss_att 146.968521 loss_ctc 79.562874 loss_rnnt 49.153217 hw_loss 0.121532 lr 0.00042600 rank 0
2023-02-18 02:18:20,558 DEBUG TRAIN Batch 1/1100 loss 115.291786 loss_att 200.082733 loss_ctc 138.876144 loss_rnnt 95.068016 hw_loss 0.226876 lr 0.00042640 rank 6
2023-02-18 02:18:20,561 DEBUG TRAIN Batch 1/1100 loss 122.670074 loss_att 228.874847 loss_ctc 150.239624 loss_rnnt 97.563110 hw_loss 0.356377 lr 0.00042804 rank 1
2023-02-18 02:18:20,563 DEBUG TRAIN Batch 1/1100 loss 112.321106 loss_att 240.457550 loss_ctc 130.890366 loss_rnnt 84.133026 hw_loss 0.159171 lr 0.00042648 rank 2
2023-02-18 02:18:20,610 DEBUG TRAIN Batch 1/1100 loss 102.883919 loss_att 181.752670 loss_ctc 131.999420 loss_rnnt 83.228058 hw_loss 0.000075 lr 0.00042536 rank 4
2023-02-18 02:19:19,023 DEBUG TRAIN Batch 1/1200 loss 64.844742 loss_att 113.584183 loss_ctc 79.360023 loss_rnnt 53.046783 hw_loss 0.215080 lr 0.00042736 rank 4
2023-02-18 02:19:19,025 DEBUG TRAIN Batch 1/1200 loss 48.289013 loss_att 86.726326 loss_ctc 48.018280 loss_rnnt 40.570412 hw_loss 0.126070 lr 0.00042840 rank 6
2023-02-18 02:19:19,025 DEBUG TRAIN Batch 1/1200 loss 67.185890 loss_att 135.846924 loss_ctc 80.037270 loss_rnnt 51.695686 hw_loss 0.083393 lr 0.00042832 rank 5
2023-02-18 02:19:19,026 DEBUG TRAIN Batch 1/1200 loss 42.127403 loss_att 103.590057 loss_ctc 44.783310 loss_rnnt 29.437576 hw_loss 0.080945 lr 0.00042800 rank 0
2023-02-18 02:19:19,026 DEBUG TRAIN Batch 1/1200 loss 67.726601 loss_att 144.503662 loss_ctc 69.301270 loss_rnnt 52.156708 hw_loss 0.008492 lr 0.00042708 rank 3
2023-02-18 02:19:19,028 DEBUG TRAIN Batch 1/1200 loss 91.438103 loss_att 153.769989 loss_ctc 103.293274 loss_rnnt 77.352463 hw_loss 0.072333 lr 0.00042852 rank 7
2023-02-18 02:19:19,033 DEBUG TRAIN Batch 1/1200 loss 68.454063 loss_att 139.715637 loss_ctc 76.502892 loss_rnnt 53.128510 hw_loss 0.000105 lr 0.00042848 rank 2
2023-02-18 02:19:19,085 DEBUG TRAIN Batch 1/1200 loss 50.919209 loss_att 106.151985 loss_ctc 53.588642 loss_rnnt 39.470436 hw_loss 0.086793 lr 0.00043004 rank 1
2023-02-18 02:20:21,219 DEBUG TRAIN Batch 1/1300 loss 81.218689 loss_att 158.554703 loss_ctc 94.110458 loss_rnnt 63.939610 hw_loss 0.174323 lr 0.00043040 rank 6
2023-02-18 02:20:21,219 DEBUG TRAIN Batch 1/1300 loss 107.791931 loss_att 197.951721 loss_ctc 119.210060 loss_rnnt 88.189926 hw_loss 0.089325 lr 0.00043000 rank 0
2023-02-18 02:20:21,220 DEBUG TRAIN Batch 1/1300 loss 112.540504 loss_att 186.564484 loss_ctc 117.574768 loss_rnnt 96.961365 hw_loss 0.193331 lr 0.00043032 rank 5
2023-02-18 02:20:21,223 DEBUG TRAIN Batch 1/1300 loss 74.854431 loss_att 162.913010 loss_ctc 88.161911 loss_rnnt 55.468288 hw_loss 0.000168 lr 0.00043204 rank 1
2023-02-18 02:20:21,224 DEBUG TRAIN Batch 1/1300 loss 77.080368 loss_att 169.849152 loss_ctc 84.701637 loss_rnnt 57.497910 hw_loss 0.023500 lr 0.00042936 rank 4
2023-02-18 02:20:21,224 DEBUG TRAIN Batch 1/1300 loss 56.343914 loss_att 122.033607 loss_ctc 61.919788 loss_rnnt 42.462234 hw_loss 0.000544 lr 0.00042908 rank 3
2023-02-18 02:20:21,226 DEBUG TRAIN Batch 1/1300 loss 95.992706 loss_att 204.796143 loss_ctc 117.660545 loss_rnnt 71.342911 hw_loss 0.000109 lr 0.00043052 rank 7
2023-02-18 02:20:21,228 DEBUG TRAIN Batch 1/1300 loss 61.885113 loss_att 122.786209 loss_ctc 61.880806 loss_rnnt 49.678936 hw_loss 0.049753 lr 0.00043048 rank 2
2023-02-18 02:21:18,320 DEBUG TRAIN Batch 1/1400 loss 106.189926 loss_att 211.602524 loss_ctc 116.942902 loss_rnnt 83.633438 hw_loss 0.075435 lr 0.00043108 rank 3
2023-02-18 02:21:18,322 DEBUG TRAIN Batch 1/1400 loss 41.360294 loss_att 97.261597 loss_ctc 34.307861 loss_rnnt 31.120325 hw_loss 0.000054 lr 0.00043252 rank 7
2023-02-18 02:21:18,321 DEBUG TRAIN Batch 1/1400 loss 83.961700 loss_att 130.854950 loss_ctc 65.709908 loss_rnnt 77.016571 hw_loss 0.000103 lr 0.00043136 rank 4
2023-02-18 02:21:18,322 DEBUG TRAIN Batch 1/1400 loss 80.204430 loss_att 168.373718 loss_ctc 77.903946 loss_rnnt 62.877228 hw_loss 0.000132 lr 0.00043232 rank 5
2023-02-18 02:21:18,325 DEBUG TRAIN Batch 1/1400 loss 31.970219 loss_att 96.319809 loss_ctc 33.499985 loss_rnnt 18.896299 hw_loss 0.000058 lr 0.00043248 rank 2
2023-02-18 02:21:18,329 DEBUG TRAIN Batch 1/1400 loss 64.518387 loss_att 136.222397 loss_ctc 76.580711 loss_rnnt 48.569221 hw_loss 0.000086 lr 0.00043404 rank 1
2023-02-18 02:21:18,328 DEBUG TRAIN Batch 1/1400 loss 92.574837 loss_att 155.625610 loss_ctc 113.921852 loss_rnnt 77.118355 hw_loss 0.000091 lr 0.00043200 rank 0
2023-02-18 02:21:18,334 DEBUG TRAIN Batch 1/1400 loss 56.709484 loss_att 128.231964 loss_ctc 66.554893 loss_rnnt 41.092232 hw_loss 0.000066 lr 0.00043240 rank 6
2023-02-18 02:22:17,067 DEBUG TRAIN Batch 1/1500 loss 95.333366 loss_att 186.576797 loss_ctc 116.065300 loss_rnnt 74.255096 hw_loss 0.122499 lr 0.00043604 rank 1
2023-02-18 02:22:17,070 DEBUG TRAIN Batch 1/1500 loss 91.558678 loss_att 161.056778 loss_ctc 105.377457 loss_rnnt 75.763191 hw_loss 0.100054 lr 0.00043432 rank 5
2023-02-18 02:22:17,071 DEBUG TRAIN Batch 1/1500 loss 50.683990 loss_att 116.313599 loss_ctc 60.026421 loss_rnnt 36.214108 hw_loss 0.184318 lr 0.00043336 rank 4
2023-02-18 02:22:17,070 DEBUG TRAIN Batch 1/1500 loss 105.763374 loss_att 179.493835 loss_ctc 131.797729 loss_rnnt 87.545898 hw_loss 0.000245 lr 0.00043400 rank 0
2023-02-18 02:22:17,073 DEBUG TRAIN Batch 1/1500 loss 54.026737 loss_att 113.742477 loss_ctc 55.723816 loss_rnnt 41.811279 hw_loss 0.086318 lr 0.00043308 rank 3
2023-02-18 02:22:17,077 DEBUG TRAIN Batch 1/1500 loss 93.697083 loss_att 155.917999 loss_ctc 108.989227 loss_rnnt 79.149467 hw_loss 0.120890 lr 0.00043448 rank 2
2023-02-18 02:22:17,080 DEBUG TRAIN Batch 1/1500 loss 68.874405 loss_att 147.525681 loss_ctc 83.896774 loss_rnnt 51.049988 hw_loss 0.170956 lr 0.00043440 rank 6
2023-02-18 02:22:17,081 DEBUG TRAIN Batch 1/1500 loss 65.963188 loss_att 124.735275 loss_ctc 77.699600 loss_rnnt 52.626472 hw_loss 0.032706 lr 0.00043452 rank 7
2023-02-18 02:23:19,096 DEBUG TRAIN Batch 1/1600 loss 145.786911 loss_att 217.724014 loss_ctc 151.777237 loss_rnnt 130.600708 hw_loss 0.000131 lr 0.00043632 rank 5
2023-02-18 02:23:19,099 DEBUG TRAIN Batch 1/1600 loss 61.639965 loss_att 145.004715 loss_ctc 70.079353 loss_rnnt 43.680103 hw_loss 0.303106 lr 0.00043652 rank 7
2023-02-18 02:23:19,099 DEBUG TRAIN Batch 1/1600 loss 92.080063 loss_att 171.156357 loss_ctc 115.429100 loss_rnnt 73.079994 hw_loss 0.134260 lr 0.00043648 rank 2
2023-02-18 02:23:19,100 DEBUG TRAIN Batch 1/1600 loss 54.639816 loss_att 144.592712 loss_ctc 58.266701 loss_rnnt 36.108913 hw_loss 0.106380 lr 0.00043640 rank 6
2023-02-18 02:23:19,105 DEBUG TRAIN Batch 1/1600 loss 56.900764 loss_att 150.829224 loss_ctc 55.574764 loss_rnnt 38.291672 hw_loss 0.000384 lr 0.00043600 rank 0
2023-02-18 02:23:19,106 DEBUG TRAIN Batch 1/1600 loss 48.471382 loss_att 93.687714 loss_ctc 45.395706 loss_rnnt 39.707153 hw_loss 0.245722 lr 0.00043508 rank 3
2023-02-18 02:23:19,107 DEBUG TRAIN Batch 1/1600 loss 74.528252 loss_att 153.011673 loss_ctc 74.101257 loss_rnnt 58.888405 hw_loss 0.000170 lr 0.00043536 rank 4
2023-02-18 02:23:19,157 DEBUG TRAIN Batch 1/1600 loss 78.230011 loss_att 176.116745 loss_ctc 76.466042 loss_rnnt 58.887787 hw_loss 0.000132 lr 0.00043804 rank 1
2023-02-18 02:24:19,094 DEBUG TRAIN Batch 1/1700 loss 47.028679 loss_att 150.042679 loss_ctc 45.770603 loss_rnnt 26.536596 hw_loss 0.106928 lr 0.00043832 rank 5
2023-02-18 02:24:19,095 DEBUG TRAIN Batch 1/1700 loss 76.057220 loss_att 141.187729 loss_ctc 94.402237 loss_rnnt 60.520164 hw_loss 0.121793 lr 0.00043840 rank 6
2023-02-18 02:24:19,101 DEBUG TRAIN Batch 1/1700 loss 43.799206 loss_att 98.251175 loss_ctc 52.860352 loss_rnnt 31.621998 hw_loss 0.147487 lr 0.00044004 rank 1
2023-02-18 02:24:19,101 DEBUG TRAIN Batch 1/1700 loss 89.901161 loss_att 166.483047 loss_ctc 109.924561 loss_rnnt 71.822266 hw_loss 0.173876 lr 0.00043800 rank 0
2023-02-18 02:24:19,103 DEBUG TRAIN Batch 1/1700 loss 69.273109 loss_att 182.568542 loss_ctc 78.790306 loss_rnnt 45.244743 hw_loss 0.188109 lr 0.00043852 rank 7
2023-02-18 02:24:19,105 DEBUG TRAIN Batch 1/1700 loss 53.928864 loss_att 124.827713 loss_ctc 72.180115 loss_rnnt 37.270283 hw_loss 0.084954 lr 0.00043708 rank 3
2023-02-18 02:24:19,134 DEBUG TRAIN Batch 1/1700 loss 61.501415 loss_att 129.022186 loss_ctc 66.551895 loss_rnnt 47.323814 hw_loss 0.000100 lr 0.00043848 rank 2
2023-02-18 02:24:19,180 DEBUG TRAIN Batch 1/1700 loss 75.685905 loss_att 133.124664 loss_ctc 64.987541 loss_rnnt 65.504585 hw_loss 0.225023 lr 0.00043736 rank 4
2023-02-18 02:25:38,364 DEBUG TRAIN Batch 1/1800 loss 63.947758 loss_att 137.276413 loss_ctc 79.880760 loss_rnnt 47.064632 hw_loss 0.174361 lr 0.00044032 rank 5
2023-02-18 02:25:38,365 DEBUG TRAIN Batch 1/1800 loss 33.181225 loss_att 95.751938 loss_ctc 32.756081 loss_rnnt 20.706606 hw_loss 0.032180 lr 0.00044000 rank 0
2023-02-18 02:25:38,366 DEBUG TRAIN Batch 1/1800 loss 58.814709 loss_att 123.293854 loss_ctc 65.175476 loss_rnnt 44.991016 hw_loss 0.149550 lr 0.00043936 rank 4
2023-02-18 02:25:38,367 DEBUG TRAIN Batch 1/1800 loss 66.420601 loss_att 128.743652 loss_ctc 84.193130 loss_rnnt 51.578835 hw_loss 0.014036 lr 0.00044040 rank 6
2023-02-18 02:25:38,368 DEBUG TRAIN Batch 1/1800 loss 69.200325 loss_att 140.599365 loss_ctc 76.397202 loss_rnnt 53.900021 hw_loss 0.114217 lr 0.00044204 rank 1
2023-02-18 02:25:38,370 DEBUG TRAIN Batch 1/1800 loss 47.849339 loss_att 109.885452 loss_ctc 54.371590 loss_rnnt 34.572449 hw_loss 0.000071 lr 0.00044052 rank 7
2023-02-18 02:25:38,374 DEBUG TRAIN Batch 1/1800 loss 79.736526 loss_att 145.568253 loss_ctc 92.783226 loss_rnnt 64.800034 hw_loss 0.057330 lr 0.00043908 rank 3
2023-02-18 02:25:38,379 DEBUG TRAIN Batch 1/1800 loss 61.327122 loss_att 117.716629 loss_ctc 66.024620 loss_rnnt 49.341930 hw_loss 0.151795 lr 0.00044048 rank 2
2023-02-18 02:26:39,281 DEBUG TRAIN Batch 1/1900 loss 95.298714 loss_att 188.614441 loss_ctc 116.817986 loss_rnnt 73.696930 hw_loss 0.130120 lr 0.00044404 rank 1
2023-02-18 02:26:39,283 DEBUG TRAIN Batch 1/1900 loss 48.050419 loss_att 125.226196 loss_ctc 55.171715 loss_rnnt 31.561472 hw_loss 0.195530 lr 0.00044232 rank 5
2023-02-18 02:26:39,285 DEBUG TRAIN Batch 1/1900 loss 64.357788 loss_att 137.085876 loss_ctc 88.777519 loss_rnnt 46.503506 hw_loss 0.098824 lr 0.00044240 rank 6
2023-02-18 02:26:39,287 DEBUG TRAIN Batch 1/1900 loss 93.081032 loss_att 183.580658 loss_ctc 100.201897 loss_rnnt 74.031586 hw_loss 0.000135 lr 0.00044248 rank 2
2023-02-18 02:26:39,288 DEBUG TRAIN Batch 1/1900 loss 65.949722 loss_att 150.624329 loss_ctc 81.387962 loss_rnnt 46.831974 hw_loss 0.233234 lr 0.00044200 rank 0
2023-02-18 02:26:39,289 DEBUG TRAIN Batch 1/1900 loss 55.151794 loss_att 133.604797 loss_ctc 67.095467 loss_rnnt 37.615608 hw_loss 0.474546 lr 0.00044252 rank 7
2023-02-18 02:26:39,290 DEBUG TRAIN Batch 1/1900 loss 128.352203 loss_att 191.711670 loss_ctc 144.946548 loss_rnnt 113.461746 hw_loss 0.011229 lr 0.00044136 rank 4
2023-02-18 02:26:39,290 DEBUG TRAIN Batch 1/1900 loss 125.122208 loss_att 183.574753 loss_ctc 132.006683 loss_rnnt 112.513611 hw_loss 0.000287 lr 0.00044108 rank 3
2023-02-18 02:27:37,244 DEBUG TRAIN Batch 1/2000 loss 34.563354 loss_att 43.063942 loss_ctc 43.275414 loss_rnnt 31.622131 hw_loss 0.149057 lr 0.00044432 rank 5
2023-02-18 02:27:37,252 DEBUG TRAIN Batch 1/2000 loss 91.579674 loss_att 187.753296 loss_ctc 103.619156 loss_rnnt 70.739624 hw_loss 0.000109 lr 0.00044440 rank 6
2023-02-18 02:27:37,253 DEBUG TRAIN Batch 1/2000 loss 40.041515 loss_att 86.255669 loss_ctc 41.343121 loss_rnnt 30.625092 hw_loss 0.000084 lr 0.00044452 rank 7
2023-02-18 02:27:37,254 DEBUG TRAIN Batch 1/2000 loss 44.263721 loss_att 61.089634 loss_ctc 53.647728 loss_rnnt 39.525742 hw_loss 0.227997 lr 0.00044604 rank 1
2023-02-18 02:27:37,254 DEBUG TRAIN Batch 1/2000 loss 42.189442 loss_att 113.878906 loss_ctc 47.539738 loss_rnnt 27.093290 hw_loss 0.084159 lr 0.00044400 rank 0
2023-02-18 02:27:37,260 DEBUG TRAIN Batch 1/2000 loss 29.933094 loss_att 84.738785 loss_ctc 27.051996 loss_rnnt 19.356049 hw_loss 0.000094 lr 0.00044448 rank 2
2023-02-18 02:27:37,262 DEBUG TRAIN Batch 1/2000 loss 108.800117 loss_att 192.727509 loss_ctc 134.821091 loss_rnnt 88.536880 hw_loss 0.015544 lr 0.00044308 rank 3
2023-02-18 02:27:37,318 DEBUG TRAIN Batch 1/2000 loss 77.032120 loss_att 179.164886 loss_ctc 81.766960 loss_rnnt 55.909042 hw_loss 0.122256 lr 0.00044336 rank 4
2023-02-18 02:28:36,310 DEBUG TRAIN Batch 1/2100 loss 55.873245 loss_att 114.200157 loss_ctc 59.382339 loss_rnnt 43.678871 hw_loss 0.114582 lr 0.00044632 rank 5
2023-02-18 02:28:36,311 DEBUG TRAIN Batch 1/2100 loss 67.205330 loss_att 125.801544 loss_ctc 78.546165 loss_rnnt 53.973877 hw_loss 0.000181 lr 0.00044600 rank 0
2023-02-18 02:28:36,313 DEBUG TRAIN Batch 1/2100 loss 67.632881 loss_att 141.712204 loss_ctc 76.927879 loss_rnnt 51.504581 hw_loss 0.137059 lr 0.00044640 rank 6
2023-02-18 02:28:36,315 DEBUG TRAIN Batch 1/2100 loss 65.338409 loss_att 145.615494 loss_ctc 73.053185 loss_rnnt 48.202957 hw_loss 0.096383 lr 0.00044804 rank 1
2023-02-18 02:28:36,316 DEBUG TRAIN Batch 1/2100 loss 38.496140 loss_att 94.531555 loss_ctc 37.638100 loss_rnnt 27.336668 hw_loss 0.125239 lr 0.00044648 rank 2
2023-02-18 02:28:36,318 DEBUG TRAIN Batch 1/2100 loss 60.511436 loss_att 122.281738 loss_ctc 57.778378 loss_rnnt 48.432995 hw_loss 0.166469 lr 0.00044536 rank 4
2023-02-18 02:28:36,324 DEBUG TRAIN Batch 1/2100 loss 70.909164 loss_att 146.638336 loss_ctc 83.668167 loss_rnnt 54.044228 hw_loss 0.033574 lr 0.00044508 rank 3
2023-02-18 02:28:36,375 DEBUG TRAIN Batch 1/2100 loss 66.650536 loss_att 114.904419 loss_ctc 84.046082 loss_rnnt 54.578785 hw_loss 0.190430 lr 0.00044652 rank 7
2023-02-18 02:29:37,641 DEBUG TRAIN Batch 1/2200 loss 70.549767 loss_att 149.808502 loss_ctc 85.934677 loss_rnnt 52.578896 hw_loss 0.127134 lr 0.00044736 rank 4
2023-02-18 02:29:37,641 DEBUG TRAIN Batch 1/2200 loss 75.464676 loss_att 183.995605 loss_ctc 83.047356 loss_rnnt 52.657745 hw_loss 0.168232 lr 0.00044852 rank 7
2023-02-18 02:29:37,643 DEBUG TRAIN Batch 1/2200 loss 53.240894 loss_att 143.903412 loss_ctc 54.231941 loss_rnnt 34.900444 hw_loss 0.142133 lr 0.00044832 rank 5
2023-02-18 02:29:37,643 DEBUG TRAIN Batch 1/2200 loss 89.779465 loss_att 182.806549 loss_ctc 87.353676 loss_rnnt 71.408562 hw_loss 0.166750 lr 0.00045004 rank 1
2023-02-18 02:29:37,646 DEBUG TRAIN Batch 1/2200 loss 86.169777 loss_att 180.119690 loss_ctc 87.889488 loss_rnnt 67.150429 hw_loss 0.000136 lr 0.00044708 rank 3
2023-02-18 02:29:37,647 DEBUG TRAIN Batch 1/2200 loss 74.921822 loss_att 137.077057 loss_ctc 82.434669 loss_rnnt 61.489006 hw_loss 0.000087 lr 0.00044848 rank 2
2023-02-18 02:29:37,648 DEBUG TRAIN Batch 1/2200 loss 91.464149 loss_att 203.687012 loss_ctc 98.340065 loss_rnnt 68.102646 hw_loss 0.000262 lr 0.00044800 rank 0
2023-02-18 02:29:37,650 DEBUG TRAIN Batch 1/2200 loss 68.450066 loss_att 156.098953 loss_ctc 85.276100 loss_rnnt 48.640915 hw_loss 0.067304 lr 0.00044840 rank 6
2023-02-18 02:30:35,028 DEBUG TRAIN Batch 1/2300 loss 35.157700 loss_att 69.081406 loss_ctc 39.801140 loss_rnnt 27.718590 hw_loss 0.066080 lr 0.00045032 rank 5
2023-02-18 02:30:35,028 DEBUG TRAIN Batch 1/2300 loss 21.991802 loss_att 40.157917 loss_ctc 26.621164 loss_rnnt 17.691175 hw_loss 0.094042 lr 0.00045000 rank 0
2023-02-18 02:30:35,029 DEBUG TRAIN Batch 1/2300 loss 191.674362 loss_att 311.469086 loss_ctc 211.482025 loss_rnnt 165.027573 hw_loss 0.087773 lr 0.00045040 rank 6
2023-02-18 02:30:35,030 DEBUG TRAIN Batch 1/2300 loss 43.185097 loss_att 82.146454 loss_ctc 51.550186 loss_rnnt 34.205791 hw_loss 0.134418 lr 0.00045204 rank 1
2023-02-18 02:30:35,031 DEBUG TRAIN Batch 1/2300 loss 28.493896 loss_att 40.523346 loss_ctc 33.971214 loss_rnnt 25.284023 hw_loss 0.138138 lr 0.00044936 rank 4
2023-02-18 02:30:35,033 DEBUG TRAIN Batch 1/2300 loss 72.963776 loss_att 123.538231 loss_ctc 83.186859 loss_rnnt 61.477943 hw_loss 0.014724 lr 0.00044908 rank 3
2023-02-18 02:30:35,044 DEBUG TRAIN Batch 1/2300 loss 28.042484 loss_att 41.834023 loss_ctc 32.497658 loss_rnnt 24.625128 hw_loss 0.121926 lr 0.00045052 rank 7
2023-02-18 02:30:35,092 DEBUG TRAIN Batch 1/2300 loss 51.104534 loss_att 114.217216 loss_ctc 61.948265 loss_rnnt 36.968582 hw_loss 0.126715 lr 0.00045048 rank 2
2023-02-18 02:31:34,338 DEBUG TRAIN Batch 1/2400 loss 121.245735 loss_att 188.202011 loss_ctc 147.061874 loss_rnnt 104.359451 hw_loss 0.099151 lr 0.00045108 rank 3
2023-02-18 02:31:34,339 DEBUG TRAIN Batch 1/2400 loss 62.677128 loss_att 138.581100 loss_ctc 73.787094 loss_rnnt 45.977131 hw_loss 0.071011 lr 0.00045404 rank 1
2023-02-18 02:31:34,341 DEBUG TRAIN Batch 1/2400 loss 64.577393 loss_att 129.916611 loss_ctc 74.565430 loss_rnnt 50.174976 hw_loss 0.005305 lr 0.00045240 rank 6
2023-02-18 02:31:34,343 DEBUG TRAIN Batch 1/2400 loss 65.522469 loss_att 143.990341 loss_ctc 70.084007 loss_rnnt 49.146553 hw_loss 0.138998 lr 0.00045252 rank 7
2023-02-18 02:31:34,343 DEBUG TRAIN Batch 1/2400 loss 63.301308 loss_att 148.415802 loss_ctc 69.758957 loss_rnnt 45.400059 hw_loss 0.032492 lr 0.00045232 rank 5
2023-02-18 02:31:34,344 DEBUG TRAIN Batch 1/2400 loss 51.941441 loss_att 99.558426 loss_ctc 60.332516 loss_rnnt 41.222168 hw_loss 0.144491 lr 0.00045136 rank 4
2023-02-18 02:31:34,345 DEBUG TRAIN Batch 1/2400 loss 90.678238 loss_att 150.496796 loss_ctc 94.521637 loss_rnnt 78.201988 hw_loss 0.000147 lr 0.00045200 rank 0
2023-02-18 02:31:34,355 DEBUG TRAIN Batch 1/2400 loss 63.302917 loss_att 139.925964 loss_ctc 69.220596 loss_rnnt 47.189247 hw_loss 0.000065 lr 0.00045248 rank 2
2023-02-18 02:32:34,845 DEBUG TRAIN Batch 1/2500 loss 73.905151 loss_att 141.773621 loss_ctc 88.642036 loss_rnnt 58.288414 hw_loss 0.146492 lr 0.00045400 rank 0
2023-02-18 02:32:34,846 DEBUG TRAIN Batch 1/2500 loss 57.587132 loss_att 132.189423 loss_ctc 56.451344 loss_rnnt 42.731274 hw_loss 0.162819 lr 0.00045336 rank 4
2023-02-18 02:32:34,848 DEBUG TRAIN Batch 1/2500 loss 168.588791 loss_att 251.508377 loss_ctc 176.209152 loss_rnnt 150.970825 hw_loss 0.033748 lr 0.00045452 rank 7
2023-02-18 02:32:34,849 DEBUG TRAIN Batch 1/2500 loss 68.055984 loss_att 124.294189 loss_ctc 70.989777 loss_rnnt 56.367603 hw_loss 0.092936 lr 0.00045432 rank 5
2023-02-18 02:32:34,850 DEBUG TRAIN Batch 1/2500 loss 54.375206 loss_att 134.935150 loss_ctc 61.666210 loss_rnnt 37.227058 hw_loss 0.120045 lr 0.00045448 rank 2
2023-02-18 02:32:34,850 DEBUG TRAIN Batch 1/2500 loss 76.899666 loss_att 201.602585 loss_ctc 81.836349 loss_rnnt 51.288334 hw_loss 0.023485 lr 0.00045604 rank 1
2023-02-18 02:32:34,851 DEBUG TRAIN Batch 1/2500 loss 48.527412 loss_att 127.004601 loss_ctc 55.472565 loss_rnnt 31.905643 hw_loss 0.000570 lr 0.00045308 rank 3
2023-02-18 02:32:34,855 DEBUG TRAIN Batch 1/2500 loss 85.816589 loss_att 192.305923 loss_ctc 110.506599 loss_rnnt 61.123436 hw_loss 0.193671 lr 0.00045440 rank 6
2023-02-18 02:33:55,704 DEBUG TRAIN Batch 1/2600 loss 43.357582 loss_att 87.118576 loss_ctc 48.961952 loss_rnnt 33.786072 hw_loss 0.135113 lr 0.00045804 rank 1
2023-02-18 02:33:55,708 DEBUG TRAIN Batch 1/2600 loss 71.422920 loss_att 107.654877 loss_ctc 92.527710 loss_rnnt 61.175888 hw_loss 0.350006 lr 0.00045652 rank 7
2023-02-18 02:33:55,709 DEBUG TRAIN Batch 1/2600 loss 73.896423 loss_att 136.234482 loss_ctc 81.907166 loss_rnnt 60.259460 hw_loss 0.189856 lr 0.00045632 rank 5
2023-02-18 02:33:55,710 DEBUG TRAIN Batch 1/2600 loss 71.049347 loss_att 106.340904 loss_ctc 87.417221 loss_rnnt 61.709229 hw_loss 0.186419 lr 0.00045536 rank 4
2023-02-18 02:33:55,711 DEBUG TRAIN Batch 1/2600 loss 24.695635 loss_att 31.007881 loss_ctc 28.153198 loss_rnnt 22.903715 hw_loss 0.128371 lr 0.00045508 rank 3
2023-02-18 02:33:55,714 DEBUG TRAIN Batch 1/2600 loss 36.627983 loss_att 59.592148 loss_ctc 43.362377 loss_rnnt 31.056049 hw_loss 0.152202 lr 0.00045600 rank 0
2023-02-18 02:33:55,715 DEBUG TRAIN Batch 1/2600 loss 24.669655 loss_att 28.067991 loss_ctc 28.736456 loss_rnnt 23.303453 hw_loss 0.270548 lr 0.00045640 rank 6
2023-02-18 02:33:55,773 DEBUG TRAIN Batch 1/2600 loss 66.126160 loss_att 147.332413 loss_ctc 78.296844 loss_rnnt 48.251369 hw_loss 0.020214 lr 0.00045648 rank 2
2023-02-18 02:34:55,590 DEBUG TRAIN Batch 1/2700 loss 52.841412 loss_att 103.093033 loss_ctc 64.383224 loss_rnnt 41.236935 hw_loss 0.028581 lr 0.00045832 rank 5
2023-02-18 02:34:55,590 DEBUG TRAIN Batch 1/2700 loss 46.719013 loss_att 100.096344 loss_ctc 55.083366 loss_rnnt 34.894173 hw_loss 0.063980 lr 0.00046004 rank 1
2023-02-18 02:34:55,593 DEBUG TRAIN Batch 1/2700 loss 57.844837 loss_att 127.957672 loss_ctc 65.361267 loss_rnnt 42.783871 hw_loss 0.067893 lr 0.00045848 rank 2
2023-02-18 02:34:55,597 DEBUG TRAIN Batch 1/2700 loss 71.273003 loss_att 137.422333 loss_ctc 82.492928 loss_rnnt 56.508553 hw_loss 0.072338 lr 0.00045736 rank 4
2023-02-18 02:34:55,597 DEBUG TRAIN Batch 1/2700 loss 65.946648 loss_att 135.616577 loss_ctc 77.031265 loss_rnnt 50.458057 hw_loss 0.143719 lr 0.00045852 rank 7
2023-02-18 02:34:55,597 DEBUG TRAIN Batch 1/2700 loss 77.387848 loss_att 165.543808 loss_ctc 82.676926 loss_rnnt 59.051357 hw_loss 0.000170 lr 0.00045708 rank 3
2023-02-18 02:34:55,598 DEBUG TRAIN Batch 1/2700 loss 70.378311 loss_att 137.373108 loss_ctc 81.638168 loss_rnnt 55.477222 hw_loss 0.001520 lr 0.00045840 rank 6
2023-02-18 02:34:55,660 DEBUG TRAIN Batch 1/2700 loss 67.231377 loss_att 147.278305 loss_ctc 70.688530 loss_rnnt 50.708801 hw_loss 0.097951 lr 0.00045800 rank 0
2023-02-18 02:35:56,049 DEBUG TRAIN Batch 1/2800 loss 63.720238 loss_att 127.563583 loss_ctc 70.426514 loss_rnnt 50.057327 hw_loss 0.000137 lr 0.00046032 rank 5
2023-02-18 02:35:56,049 DEBUG TRAIN Batch 1/2800 loss 140.049606 loss_att 220.352112 loss_ctc 158.388504 loss_rnnt 121.458183 hw_loss 0.160767 lr 0.00045936 rank 4
2023-02-18 02:35:56,050 DEBUG TRAIN Batch 1/2800 loss 66.433014 loss_att 107.047646 loss_ctc 82.097023 loss_rnnt 56.221458 hw_loss 0.000190 lr 0.00046052 rank 7
2023-02-18 02:35:56,050 DEBUG TRAIN Batch 1/2800 loss 71.338692 loss_att 183.233551 loss_ctc 97.356575 loss_rnnt 45.301327 hw_loss 0.355017 lr 0.00045908 rank 3
2023-02-18 02:35:56,052 DEBUG TRAIN Batch 1/2800 loss 23.962896 loss_att 80.459549 loss_ctc 28.655603 loss_rnnt 12.020050 hw_loss 0.033414 lr 0.00046040 rank 6
2023-02-18 02:35:56,053 DEBUG TRAIN Batch 1/2800 loss 43.536243 loss_att 96.747673 loss_ctc 48.281799 loss_rnnt 32.254932 hw_loss 0.011779 lr 0.00046000 rank 0
2023-02-18 02:35:56,056 DEBUG TRAIN Batch 1/2800 loss 63.347504 loss_att 143.527466 loss_ctc 70.093971 loss_rnnt 46.334869 hw_loss 0.144585 lr 0.00046204 rank 1
2023-02-18 02:35:56,061 DEBUG TRAIN Batch 1/2800 loss 56.096710 loss_att 98.667618 loss_ctc 60.430737 loss_rnnt 46.786163 hw_loss 0.409674 lr 0.00046048 rank 2
2023-02-18 02:36:54,988 DEBUG TRAIN Batch 1/2900 loss 66.398621 loss_att 111.743881 loss_ctc 80.155731 loss_rnnt 55.495224 hw_loss 0.000116 lr 0.00046232 rank 5
2023-02-18 02:36:54,992 DEBUG TRAIN Batch 1/2900 loss 75.948441 loss_att 123.735168 loss_ctc 89.811440 loss_rnnt 64.515106 hw_loss 0.051718 lr 0.00046404 rank 1
2023-02-18 02:36:54,997 DEBUG TRAIN Batch 1/2900 loss 46.957249 loss_att 71.956924 loss_ctc 54.447823 loss_rnnt 40.927898 hw_loss 0.057516 lr 0.00046108 rank 3
2023-02-18 02:36:54,998 DEBUG TRAIN Batch 1/2900 loss 52.314667 loss_att 87.421951 loss_ctc 62.214245 loss_rnnt 43.973198 hw_loss 0.000131 lr 0.00046240 rank 6
2023-02-18 02:36:55,000 DEBUG TRAIN Batch 1/2900 loss 54.358456 loss_att 109.168930 loss_ctc 77.734619 loss_rnnt 40.248810 hw_loss 0.057621 lr 0.00046252 rank 7
2023-02-18 02:36:55,026 DEBUG TRAIN Batch 1/2900 loss 44.531639 loss_att 63.430138 loss_ctc 54.919365 loss_rnnt 39.238480 hw_loss 0.240807 lr 0.00046248 rank 2
2023-02-18 02:36:55,032 DEBUG TRAIN Batch 1/2900 loss 68.988274 loss_att 124.398628 loss_ctc 80.329163 loss_rnnt 56.384838 hw_loss 0.017339 lr 0.00046136 rank 4
2023-02-18 02:36:55,055 DEBUG TRAIN Batch 1/2900 loss 46.461624 loss_att 70.200264 loss_ctc 55.549126 loss_rnnt 40.451515 hw_loss 0.095090 lr 0.00046200 rank 0
2023-02-18 02:37:55,779 DEBUG TRAIN Batch 1/3000 loss 74.028244 loss_att 127.200111 loss_ctc 79.804474 loss_rnnt 62.623650 hw_loss 0.000115 lr 0.00046336 rank 4
2023-02-18 02:37:55,783 DEBUG TRAIN Batch 1/3000 loss 64.943588 loss_att 123.188087 loss_ctc 70.677986 loss_rnnt 52.485664 hw_loss 0.083308 lr 0.00046432 rank 5
2023-02-18 02:37:55,785 DEBUG TRAIN Batch 1/3000 loss 77.264824 loss_att 157.660538 loss_ctc 83.136292 loss_rnnt 60.402763 hw_loss 0.000097 lr 0.00046440 rank 6
2023-02-18 02:37:55,786 DEBUG TRAIN Batch 1/3000 loss 60.205826 loss_att 121.718140 loss_ctc 70.907547 loss_rnnt 46.448158 hw_loss 0.053074 lr 0.00046400 rank 0
2023-02-18 02:37:55,789 DEBUG TRAIN Batch 1/3000 loss 48.175442 loss_att 108.837517 loss_ctc 52.670258 loss_rnnt 35.443672 hw_loss 0.000085 lr 0.00046448 rank 2
2023-02-18 02:37:55,790 DEBUG TRAIN Batch 1/3000 loss 85.489685 loss_att 171.973999 loss_ctc 101.651161 loss_rnnt 65.991394 hw_loss 0.087311 lr 0.00046604 rank 1
2023-02-18 02:37:55,793 DEBUG TRAIN Batch 1/3000 loss 93.308647 loss_att 171.918640 loss_ctc 103.451439 loss_rnnt 76.149055 hw_loss 0.159774 lr 0.00046308 rank 3
2023-02-18 02:37:55,799 DEBUG TRAIN Batch 1/3000 loss 74.948730 loss_att 126.583496 loss_ctc 84.609711 loss_rnnt 63.333599 hw_loss 0.000083 lr 0.00046452 rank 7
2023-02-18 02:38:55,873 DEBUG TRAIN Batch 1/3100 loss 94.299164 loss_att 164.039352 loss_ctc 107.659760 loss_rnnt 78.569687 hw_loss 0.000063 lr 0.00046804 rank 1
2023-02-18 02:38:55,874 DEBUG TRAIN Batch 1/3100 loss 76.343887 loss_att 176.431671 loss_ctc 69.963432 loss_rnnt 57.128754 hw_loss 0.090558 lr 0.00046640 rank 6
2023-02-18 02:38:55,874 DEBUG TRAIN Batch 1/3100 loss 47.064095 loss_att 123.969177 loss_ctc 57.339947 loss_rnnt 30.183407 hw_loss 0.242918 lr 0.00046632 rank 5
2023-02-18 02:38:55,875 DEBUG TRAIN Batch 1/3100 loss 101.972626 loss_att 189.015488 loss_ctc 108.555992 loss_rnnt 83.477806 hw_loss 0.390873 lr 0.00046600 rank 0
2023-02-18 02:38:55,875 DEBUG TRAIN Batch 1/3100 loss 73.493935 loss_att 155.855286 loss_ctc 79.191406 loss_rnnt 56.193329 hw_loss 0.128757 lr 0.00046536 rank 4
2023-02-18 02:38:55,878 DEBUG TRAIN Batch 1/3100 loss 76.860153 loss_att 137.649689 loss_ctc 92.633148 loss_rnnt 62.581326 hw_loss 0.033478 lr 0.00046648 rank 2
2023-02-18 02:38:55,880 DEBUG TRAIN Batch 1/3100 loss 104.040009 loss_att 227.541046 loss_ctc 118.508942 loss_rnnt 77.349213 hw_loss 0.115132 lr 0.00046508 rank 3
2023-02-18 02:38:55,935 DEBUG TRAIN Batch 1/3100 loss 77.514435 loss_att 133.174255 loss_ctc 92.917404 loss_rnnt 64.245071 hw_loss 0.156887 lr 0.00046652 rank 7
2023-02-18 02:39:55,179 DEBUG TRAIN Batch 1/3200 loss 50.728031 loss_att 111.700790 loss_ctc 54.932404 loss_rnnt 37.916130 hw_loss 0.106441 lr 0.00046708 rank 3
2023-02-18 02:39:55,180 DEBUG TRAIN Batch 1/3200 loss 80.319710 loss_att 161.626907 loss_ctc 102.697220 loss_rnnt 61.071213 hw_loss 0.006354 lr 0.00046736 rank 4
2023-02-18 02:39:55,181 DEBUG TRAIN Batch 1/3200 loss 76.811493 loss_att 141.778763 loss_ctc 81.357643 loss_rnnt 63.167313 hw_loss 0.083573 lr 0.00046832 rank 5
2023-02-18 02:39:55,183 DEBUG TRAIN Batch 1/3200 loss 57.142925 loss_att 129.708878 loss_ctc 69.696289 loss_rnnt 40.955883 hw_loss 0.000135 lr 0.00047004 rank 1
2023-02-18 02:39:55,183 DEBUG TRAIN Batch 1/3200 loss 36.683716 loss_att 77.079262 loss_ctc 45.507462 loss_rnnt 27.388542 hw_loss 0.074180 lr 0.00046800 rank 0
2023-02-18 02:39:55,186 DEBUG TRAIN Batch 1/3200 loss 58.173149 loss_att 91.555954 loss_ctc 71.545906 loss_rnnt 49.674751 hw_loss 0.072751 lr 0.00046848 rank 2
2023-02-18 02:39:55,191 DEBUG TRAIN Batch 1/3200 loss 71.083054 loss_att 136.084625 loss_ctc 81.436783 loss_rnnt 56.702168 hw_loss 0.000143 lr 0.00046852 rank 7
2023-02-18 02:39:55,190 DEBUG TRAIN Batch 1/3200 loss 63.564659 loss_att 109.165359 loss_ctc 72.365913 loss_rnnt 53.164848 hw_loss 0.199071 lr 0.00046840 rank 6
2023-02-18 02:40:56,823 DEBUG TRAIN Batch 1/3300 loss 60.763073 loss_att 123.612381 loss_ctc 69.711800 loss_rnnt 46.998333 hw_loss 0.003213 lr 0.00047032 rank 5
2023-02-18 02:40:56,829 DEBUG TRAIN Batch 1/3300 loss 35.896740 loss_att 89.710388 loss_ctc 36.512245 loss_rnnt 24.870474 hw_loss 0.340250 lr 0.00046936 rank 4
2023-02-18 02:40:56,829 DEBUG TRAIN Batch 1/3300 loss 83.819305 loss_att 148.839035 loss_ctc 99.139664 loss_rnnt 68.772583 hw_loss 0.000112 lr 0.00047040 rank 6
2023-02-18 02:40:56,831 DEBUG TRAIN Batch 1/3300 loss 59.186321 loss_att 145.820099 loss_ctc 60.603561 loss_rnnt 41.624725 hw_loss 0.086011 lr 0.00047204 rank 1
2023-02-18 02:40:56,833 DEBUG TRAIN Batch 1/3300 loss 29.823246 loss_att 87.643860 loss_ctc 35.314285 loss_rnnt 17.526918 hw_loss 0.000117 lr 0.00047052 rank 7
2023-02-18 02:40:56,834 DEBUG TRAIN Batch 1/3300 loss 37.200939 loss_att 102.973236 loss_ctc 37.055958 loss_rnnt 24.029423 hw_loss 0.068224 lr 0.00046908 rank 3
2023-02-18 02:40:56,836 DEBUG TRAIN Batch 1/3300 loss 41.962929 loss_att 114.574615 loss_ctc 42.807865 loss_rnnt 27.327789 hw_loss 0.000263 lr 0.00047000 rank 0
2023-02-18 02:40:56,839 DEBUG TRAIN Batch 1/3300 loss 74.571213 loss_att 147.944672 loss_ctc 74.312500 loss_rnnt 59.856651 hw_loss 0.139422 lr 0.00047048 rank 2
2023-02-18 02:41:57,125 DEBUG TRAIN Batch 1/3400 loss 54.790627 loss_att 134.408096 loss_ctc 58.301483 loss_rnnt 38.323021 hw_loss 0.142502 lr 0.00047240 rank 6
2023-02-18 02:41:57,129 DEBUG TRAIN Batch 1/3400 loss 65.176590 loss_att 128.938446 loss_ctc 75.956657 loss_rnnt 50.816185 hw_loss 0.320046 lr 0.00047200 rank 0
2023-02-18 02:41:57,130 DEBUG TRAIN Batch 1/3400 loss 95.704628 loss_att 187.021591 loss_ctc 130.164536 loss_rnnt 72.846550 hw_loss 0.000062 lr 0.00047232 rank 5
2023-02-18 02:41:57,130 DEBUG TRAIN Batch 1/3400 loss 58.832092 loss_att 137.352722 loss_ctc 85.629158 loss_rnnt 39.554989 hw_loss 0.000072 lr 0.00047252 rank 7
2023-02-18 02:41:57,130 DEBUG TRAIN Batch 1/3400 loss 89.999901 loss_att 197.168335 loss_ctc 106.199669 loss_rnnt 66.406204 hw_loss 0.000080 lr 0.00047108 rank 3
2023-02-18 02:41:57,163 DEBUG TRAIN Batch 1/3400 loss 58.678658 loss_att 143.108109 loss_ctc 74.922554 loss_rnnt 39.626877 hw_loss 0.000070 lr 0.00047404 rank 1
2023-02-18 02:41:57,170 DEBUG TRAIN Batch 1/3400 loss 23.451136 loss_att 78.565102 loss_ctc 20.526047 loss_rnnt 12.818320 hw_loss 0.000064 lr 0.00047248 rank 2
2023-02-18 02:41:57,186 DEBUG TRAIN Batch 1/3400 loss 79.988205 loss_att 169.110840 loss_ctc 100.203880 loss_rnnt 59.468224 hw_loss 0.000067 lr 0.00047136 rank 4
2023-02-18 02:43:17,683 DEBUG TRAIN Batch 1/3500 loss 54.895004 loss_att 100.725029 loss_ctc 60.501369 loss_rnnt 44.981445 hw_loss 0.000078 lr 0.00047432 rank 5
2023-02-18 02:43:17,684 DEBUG TRAIN Batch 1/3500 loss 51.016953 loss_att 96.897667 loss_ctc 65.897690 loss_rnnt 39.758587 hw_loss 0.183987 lr 0.00047448 rank 2
2023-02-18 02:43:17,685 DEBUG TRAIN Batch 1/3500 loss 81.309067 loss_att 118.169350 loss_ctc 84.592407 loss_rnnt 73.416336 hw_loss 0.155431 lr 0.00047440 rank 6
2023-02-18 02:43:17,687 DEBUG TRAIN Batch 1/3500 loss 73.788956 loss_att 104.397346 loss_ctc 89.758133 loss_rnnt 65.487015 hw_loss 0.095684 lr 0.00047400 rank 0
2023-02-18 02:43:17,688 DEBUG TRAIN Batch 1/3500 loss 58.508667 loss_att 116.876678 loss_ctc 75.984573 loss_rnnt 44.454773 hw_loss 0.094073 lr 0.00047336 rank 4
2023-02-18 02:43:17,689 DEBUG TRAIN Batch 1/3500 loss 67.315514 loss_att 133.458771 loss_ctc 76.881187 loss_rnnt 52.805553 hw_loss 0.011045 lr 0.00047452 rank 7
2023-02-18 02:43:17,693 DEBUG TRAIN Batch 1/3500 loss 65.036934 loss_att 119.093658 loss_ctc 74.552612 loss_rnnt 52.956783 hw_loss 0.000093 lr 0.00047308 rank 3
2023-02-18 02:43:17,697 DEBUG TRAIN Batch 1/3500 loss 69.027946 loss_att 134.491425 loss_ctc 77.318680 loss_rnnt 54.809685 hw_loss 0.037746 lr 0.00047604 rank 1
2023-02-18 02:44:17,930 DEBUG TRAIN Batch 1/3600 loss 57.982689 loss_att 145.842422 loss_ctc 60.412113 loss_rnnt 40.081554 hw_loss 0.009877 lr 0.00047632 rank 5
2023-02-18 02:44:17,932 DEBUG TRAIN Batch 1/3600 loss 61.551624 loss_att 147.960220 loss_ctc 72.361557 loss_rnnt 42.771416 hw_loss 0.107186 lr 0.00047640 rank 6
2023-02-18 02:44:17,935 DEBUG TRAIN Batch 1/3600 loss 54.294102 loss_att 131.389191 loss_ctc 73.930862 loss_rnnt 36.225220 hw_loss 0.059308 lr 0.00047536 rank 4
2023-02-18 02:44:17,936 DEBUG TRAIN Batch 1/3600 loss 32.107193 loss_att 86.471603 loss_ctc 32.359898 loss_rnnt 21.065929 hw_loss 0.252533 lr 0.00047652 rank 7
2023-02-18 02:44:17,937 DEBUG TRAIN Batch 1/3600 loss 63.381897 loss_att 141.308441 loss_ctc 72.957458 loss_rnnt 46.519794 hw_loss 0.000097 lr 0.00047648 rank 2
2023-02-18 02:44:17,946 DEBUG TRAIN Batch 1/3600 loss 78.908020 loss_att 151.765594 loss_ctc 99.959129 loss_rnnt 61.529587 hw_loss 0.000195 lr 0.00047508 rank 3
2023-02-18 02:44:17,952 DEBUG TRAIN Batch 1/3600 loss 54.338039 loss_att 126.305405 loss_ctc 60.565086 loss_rnnt 39.039242 hw_loss 0.140722 lr 0.00047600 rank 0
2023-02-18 02:44:17,953 DEBUG TRAIN Batch 1/3600 loss 48.421738 loss_att 98.278076 loss_ctc 56.782902 loss_rnnt 37.296444 hw_loss 0.073508 lr 0.00047804 rank 1
2023-02-18 02:45:16,197 DEBUG TRAIN Batch 1/3700 loss 99.955551 loss_att 191.710815 loss_ctc 140.266052 loss_rnnt 76.229706 hw_loss 0.000118 lr 0.00047840 rank 6
2023-02-18 02:45:16,202 DEBUG TRAIN Batch 1/3700 loss 79.189224 loss_att 169.797318 loss_ctc 107.314430 loss_rnnt 57.231529 hw_loss 0.161344 lr 0.00047832 rank 5
2023-02-18 02:45:16,202 DEBUG TRAIN Batch 1/3700 loss 46.758141 loss_att 113.354614 loss_ctc 57.208801 loss_rnnt 32.045364 hw_loss 0.000113 lr 0.00047848 rank 2
2023-02-18 02:45:16,203 DEBUG TRAIN Batch 1/3700 loss 46.441292 loss_att 128.624313 loss_ctc 63.485405 loss_rnnt 27.673912 hw_loss 0.109174 lr 0.00047852 rank 7
2023-02-18 02:45:16,203 DEBUG TRAIN Batch 1/3700 loss 28.105795 loss_att 103.591461 loss_ctc 31.185160 loss_rnnt 12.597939 hw_loss 0.000264 lr 0.00047800 rank 0
2023-02-18 02:45:16,203 DEBUG TRAIN Batch 1/3700 loss 58.003841 loss_att 134.993683 loss_ctc 70.095741 loss_rnnt 40.947105 hw_loss 0.087215 lr 0.00047708 rank 3
2023-02-18 02:45:16,206 DEBUG TRAIN Batch 1/3700 loss 28.394724 loss_att 43.060368 loss_ctc 34.851048 loss_rnnt 24.542526 hw_loss 0.109170 lr 0.00048004 rank 1
2023-02-18 02:45:16,258 DEBUG TRAIN Batch 1/3700 loss 67.827904 loss_att 137.796646 loss_ctc 86.157036 loss_rnnt 51.349102 hw_loss 0.077195 lr 0.00047736 rank 4
2023-02-18 02:46:14,631 DEBUG TRAIN Batch 1/3800 loss 29.941626 loss_att 64.671677 loss_ctc 36.638439 loss_rnnt 22.025263 hw_loss 0.145208 lr 0.00048032 rank 5
2023-02-18 02:46:14,635 DEBUG TRAIN Batch 1/3800 loss 54.426788 loss_att 112.667404 loss_ctc 69.225937 loss_rnnt 40.805389 hw_loss 0.000103 lr 0.00048048 rank 2
2023-02-18 02:46:14,636 DEBUG TRAIN Batch 1/3800 loss 59.180420 loss_att 129.331116 loss_ctc 67.152924 loss_rnnt 44.087242 hw_loss 0.000075 lr 0.00047936 rank 4
2023-02-18 02:46:14,637 DEBUG TRAIN Batch 1/3800 loss 65.434082 loss_att 148.422791 loss_ctc 78.731369 loss_rnnt 47.029194 hw_loss 0.064070 lr 0.00047908 rank 3
2023-02-18 02:46:14,639 DEBUG TRAIN Batch 1/3800 loss 132.853058 loss_att 219.727142 loss_ctc 143.909912 loss_rnnt 114.003906 hw_loss 0.000152 lr 0.00048204 rank 1
2023-02-18 02:46:14,641 DEBUG TRAIN Batch 1/3800 loss 44.799023 loss_att 89.705566 loss_ctc 46.927208 loss_rnnt 35.426674 hw_loss 0.201157 lr 0.00048040 rank 6
2023-02-18 02:46:14,647 DEBUG TRAIN Batch 1/3800 loss 67.741829 loss_att 115.886887 loss_ctc 80.096901 loss_rnnt 56.415947 hw_loss 0.092875 lr 0.00048052 rank 7
2023-02-18 02:46:14,705 DEBUG TRAIN Batch 1/3800 loss 61.774670 loss_att 118.919144 loss_ctc 78.291779 loss_rnnt 48.143433 hw_loss 0.000118 lr 0.00048000 rank 0
2023-02-18 02:47:17,494 DEBUG TRAIN Batch 1/3900 loss 73.064438 loss_att 146.651337 loss_ctc 83.093033 loss_rnnt 57.009872 hw_loss 0.000080 lr 0.00048232 rank 5
2023-02-18 02:47:17,495 DEBUG TRAIN Batch 1/3900 loss 83.840126 loss_att 169.475937 loss_ctc 112.994942 loss_rnnt 62.813072 hw_loss 0.023600 lr 0.00048404 rank 1
2023-02-18 02:47:17,500 DEBUG TRAIN Batch 1/3900 loss 89.319832 loss_att 155.212845 loss_ctc 97.548553 loss_rnnt 74.958031 hw_loss 0.161314 lr 0.00048252 rank 7
2023-02-18 02:47:17,500 DEBUG TRAIN Batch 1/3900 loss 89.154358 loss_att 165.874252 loss_ctc 104.298889 loss_rnnt 71.791016 hw_loss 0.000172 lr 0.00048108 rank 3
2023-02-18 02:47:17,500 DEBUG TRAIN Batch 1/3900 loss 92.286278 loss_att 166.275101 loss_ctc 112.287491 loss_rnnt 74.752304 hw_loss 0.130082 lr 0.00048200 rank 0
2023-02-18 02:47:17,501 DEBUG TRAIN Batch 1/3900 loss 71.244080 loss_att 127.976471 loss_ctc 76.648994 loss_rnnt 59.176903 hw_loss 0.000075 lr 0.00048240 rank 6
2023-02-18 02:47:17,552 DEBUG TRAIN Batch 1/3900 loss 84.194916 loss_att 162.161407 loss_ctc 98.414825 loss_rnnt 66.543922 hw_loss 0.303198 lr 0.00048136 rank 4
2023-02-18 02:47:17,596 DEBUG TRAIN Batch 1/3900 loss 72.158775 loss_att 155.289185 loss_ctc 80.810570 loss_rnnt 54.311237 hw_loss 0.127271 lr 0.00048248 rank 2
2023-02-18 02:48:15,544 DEBUG TRAIN Batch 1/4000 loss 52.524319 loss_att 140.836731 loss_ctc 57.793194 loss_rnnt 34.159252 hw_loss 0.000129 lr 0.00048308 rank 3
2023-02-18 02:48:15,544 DEBUG TRAIN Batch 1/4000 loss 149.201920 loss_att 199.711838 loss_ctc 162.843994 loss_rnnt 137.275620 hw_loss 0.010065 lr 0.00048400 rank 0
2023-02-18 02:48:15,544 DEBUG TRAIN Batch 1/4000 loss 49.096951 loss_att 69.234741 loss_ctc 55.077892 loss_rnnt 44.195736 hw_loss 0.142872 lr 0.00048336 rank 4
2023-02-18 02:48:15,546 DEBUG TRAIN Batch 1/4000 loss 27.981403 loss_att 77.817833 loss_ctc 24.978386 loss_rnnt 18.414452 hw_loss 0.000126 lr 0.00048432 rank 5
2023-02-18 02:48:15,546 DEBUG TRAIN Batch 1/4000 loss 38.255939 loss_att 54.571617 loss_ctc 47.946960 loss_rnnt 33.609909 hw_loss 0.170167 lr 0.00048452 rank 7
2023-02-18 02:48:15,548 DEBUG TRAIN Batch 1/4000 loss 54.190495 loss_att 77.769691 loss_ctc 61.096001 loss_rnnt 48.546448 hw_loss 0.014013 lr 0.00048604 rank 1
2023-02-18 02:48:15,550 DEBUG TRAIN Batch 1/4000 loss 85.033707 loss_att 157.659821 loss_ctc 81.820412 loss_rnnt 70.936844 hw_loss 0.000149 lr 0.00048440 rank 6
2023-02-18 02:48:15,604 DEBUG TRAIN Batch 1/4000 loss 47.177395 loss_att 87.336700 loss_ctc 59.497402 loss_rnnt 37.399254 hw_loss 0.194273 lr 0.00048448 rank 2
2023-02-18 02:49:15,348 DEBUG TRAIN Batch 1/4100 loss 83.410728 loss_att 165.919067 loss_ctc 91.130264 loss_rnnt 65.879715 hw_loss 0.000139 lr 0.00048632 rank 5
2023-02-18 02:49:15,353 DEBUG TRAIN Batch 1/4100 loss 38.811298 loss_att 83.335052 loss_ctc 47.719257 loss_rnnt 28.670656 hw_loss 0.090300 lr 0.00048600 rank 0
2023-02-18 02:49:15,353 DEBUG TRAIN Batch 1/4100 loss 55.003250 loss_att 114.114899 loss_ctc 63.424606 loss_rnnt 42.013786 hw_loss 0.083048 lr 0.00048640 rank 6
2023-02-18 02:49:15,353 DEBUG TRAIN Batch 1/4100 loss 51.764256 loss_att 114.384964 loss_ctc 63.046448 loss_rnnt 37.681400 hw_loss 0.102031 lr 0.00048804 rank 1
2023-02-18 02:49:15,356 DEBUG TRAIN Batch 1/4100 loss 59.257576 loss_att 143.169067 loss_ctc 66.934937 loss_rnnt 41.372303 hw_loss 0.148738 lr 0.00048536 rank 4
2023-02-18 02:49:15,356 DEBUG TRAIN Batch 1/4100 loss 42.714714 loss_att 79.213448 loss_ctc 46.555630 loss_rnnt 34.854630 hw_loss 0.090406 lr 0.00048648 rank 2
2023-02-18 02:49:15,359 DEBUG TRAIN Batch 1/4100 loss 41.919804 loss_att 92.299377 loss_ctc 49.682175 loss_rnnt 30.712112 hw_loss 0.181486 lr 0.00048652 rank 7
2023-02-18 02:49:15,360 DEBUG TRAIN Batch 1/4100 loss 51.817863 loss_att 109.894608 loss_ctc 63.418587 loss_rnnt 38.643379 hw_loss 0.023193 lr 0.00048508 rank 3
2023-02-18 02:50:16,735 DEBUG TRAIN Batch 1/4200 loss 127.380226 loss_att 237.955231 loss_ctc 130.632935 loss_rnnt 104.831413 hw_loss 0.000209 lr 0.00048832 rank 5
2023-02-18 02:50:16,737 DEBUG TRAIN Batch 1/4200 loss 73.244957 loss_att 167.749420 loss_ctc 81.313370 loss_rnnt 53.242516 hw_loss 0.048322 lr 0.00048800 rank 0
2023-02-18 02:50:16,740 DEBUG TRAIN Batch 1/4200 loss 70.932487 loss_att 145.654480 loss_ctc 92.589073 loss_rnnt 53.087685 hw_loss 0.024115 lr 0.00048736 rank 4
2023-02-18 02:50:16,741 DEBUG TRAIN Batch 1/4200 loss 76.853485 loss_att 146.263885 loss_ctc 83.355339 loss_rnnt 62.104221 hw_loss 0.000502 lr 0.00048852 rank 7
2023-02-18 02:50:16,741 DEBUG TRAIN Batch 1/4200 loss 52.320084 loss_att 119.108215 loss_ctc 51.498936 loss_rnnt 39.071831 hw_loss 0.000207 lr 0.00048708 rank 3
2023-02-18 02:50:16,742 DEBUG TRAIN Batch 1/4200 loss 41.118641 loss_att 89.598633 loss_ctc 42.000862 loss_rnnt 31.304728 hw_loss 0.000531 lr 0.00049004 rank 1
2023-02-18 02:50:16,747 DEBUG TRAIN Batch 1/4200 loss 55.630348 loss_att 121.024811 loss_ctc 52.902611 loss_rnnt 42.837650 hw_loss 0.145321 lr 0.00048840 rank 6
2023-02-18 02:50:16,751 DEBUG TRAIN Batch 1/4200 loss 85.958305 loss_att 146.296188 loss_ctc 102.621628 loss_rnnt 71.668732 hw_loss 0.000424 lr 0.00048848 rank 2
2023-02-18 02:51:35,653 DEBUG TRAIN Batch 1/4300 loss 64.099243 loss_att 131.100403 loss_ctc 66.924744 loss_rnnt 50.322227 hw_loss 0.000101 lr 0.00049000 rank 0
2023-02-18 02:51:35,654 DEBUG TRAIN Batch 1/4300 loss 120.617470 loss_att 162.304062 loss_ctc 154.696411 loss_rnnt 107.736244 hw_loss 0.000084 lr 0.00049048 rank 2
2023-02-18 02:51:35,655 DEBUG TRAIN Batch 1/4300 loss 35.697884 loss_att 78.537140 loss_ctc 38.723549 loss_rnnt 26.726574 hw_loss 0.000077 lr 0.00049032 rank 5
2023-02-18 02:51:35,656 DEBUG TRAIN Batch 1/4300 loss 46.977043 loss_att 77.941185 loss_ctc 56.256115 loss_rnnt 39.428070 hw_loss 0.223005 lr 0.00048936 rank 4
2023-02-18 02:51:35,659 DEBUG TRAIN Batch 1/4300 loss 51.744354 loss_att 92.961700 loss_ctc 59.435486 loss_rnnt 42.417320 hw_loss 0.108900 lr 0.00049204 rank 1
2023-02-18 02:51:35,662 DEBUG TRAIN Batch 1/4300 loss 114.192688 loss_att 177.415741 loss_ctc 126.826195 loss_rnnt 99.863564 hw_loss 0.000081 lr 0.00049040 rank 6
2023-02-18 02:51:35,663 DEBUG TRAIN Batch 1/4300 loss 50.340832 loss_att 96.592094 loss_ctc 53.221252 loss_rnnt 40.658859 hw_loss 0.089370 lr 0.00048908 rank 3
2023-02-18 02:51:35,714 DEBUG TRAIN Batch 1/4300 loss 50.154736 loss_att 94.624916 loss_ctc 56.304993 loss_rnnt 40.351028 hw_loss 0.168063 lr 0.00049052 rank 7
2023-02-18 02:52:37,288 DEBUG TRAIN Batch 1/4400 loss 65.388924 loss_att 119.329086 loss_ctc 81.695305 loss_rnnt 52.426651 hw_loss 0.000086 lr 0.00049232 rank 5
2023-02-18 02:52:37,290 DEBUG TRAIN Batch 1/4400 loss 78.091454 loss_att 148.882660 loss_ctc 92.348419 loss_rnnt 61.931770 hw_loss 0.188472 lr 0.00049252 rank 7
2023-02-18 02:52:37,294 DEBUG TRAIN Batch 1/4400 loss 66.095306 loss_att 135.824554 loss_ctc 72.644020 loss_rnnt 51.276260 hw_loss 0.000075 lr 0.00049136 rank 4
2023-02-18 02:52:37,295 DEBUG TRAIN Batch 1/4400 loss 46.755089 loss_att 94.819260 loss_ctc 50.447472 loss_rnnt 36.638046 hw_loss 0.022300 lr 0.00049200 rank 0
2023-02-18 02:52:37,298 DEBUG TRAIN Batch 1/4400 loss 64.693619 loss_att 126.351837 loss_ctc 84.320137 loss_rnnt 49.658058 hw_loss 0.163217 lr 0.00049404 rank 1
2023-02-18 02:52:37,302 DEBUG TRAIN Batch 1/4400 loss 54.809971 loss_att 98.001228 loss_ctc 57.541260 loss_rnnt 45.807510 hw_loss 0.000063 lr 0.00049248 rank 2
2023-02-18 02:52:37,301 DEBUG TRAIN Batch 1/4400 loss 34.054382 loss_att 66.915878 loss_ctc 37.264320 loss_rnnt 27.005999 hw_loss 0.090171 lr 0.00049108 rank 3
2023-02-18 02:52:37,353 DEBUG TRAIN Batch 1/4400 loss 51.226986 loss_att 106.793694 loss_ctc 59.701759 loss_rnnt 38.918243 hw_loss 0.122680 lr 0.00049240 rank 6
2023-02-18 02:53:39,280 DEBUG TRAIN Batch 1/4500 loss 49.334553 loss_att 125.106552 loss_ctc 61.154613 loss_rnnt 32.494995 hw_loss 0.204652 lr 0.00049432 rank 5
2023-02-18 02:53:39,282 DEBUG TRAIN Batch 1/4500 loss 57.753536 loss_att 133.393265 loss_ctc 74.417656 loss_rnnt 40.303398 hw_loss 0.188072 lr 0.00049336 rank 4
2023-02-18 02:53:39,282 DEBUG TRAIN Batch 1/4500 loss 82.634857 loss_att 160.988647 loss_ctc 95.105247 loss_rnnt 65.298935 hw_loss 0.004574 lr 0.00049440 rank 6
2023-02-18 02:53:39,283 DEBUG TRAIN Batch 1/4500 loss 65.485748 loss_att 134.632370 loss_ctc 86.805000 loss_rnnt 48.727303 hw_loss 0.162270 lr 0.00049604 rank 1
2023-02-18 02:53:39,285 DEBUG TRAIN Batch 1/4500 loss 44.065086 loss_att 75.331169 loss_ctc 53.909271 loss_rnnt 36.396088 hw_loss 0.193545 lr 0.00049400 rank 0
2023-02-18 02:53:39,286 DEBUG TRAIN Batch 1/4500 loss 67.399277 loss_att 129.991837 loss_ctc 78.745163 loss_rnnt 53.367897 hw_loss 0.000154 lr 0.00049448 rank 2
2023-02-18 02:53:39,292 DEBUG TRAIN Batch 1/4500 loss 129.316711 loss_att 202.926971 loss_ctc 159.027267 loss_rnnt 110.578560 hw_loss 0.102520 lr 0.00049452 rank 7
2023-02-18 02:53:39,291 DEBUG TRAIN Batch 1/4500 loss 41.576298 loss_att 115.832336 loss_ctc 44.796314 loss_rnnt 26.215126 hw_loss 0.151178 lr 0.00049308 rank 3
2023-02-18 02:54:37,418 DEBUG TRAIN Batch 1/4600 loss 46.341908 loss_att 91.757248 loss_ctc 56.072510 loss_rnnt 35.898689 hw_loss 0.117630 lr 0.00049804 rank 1
2023-02-18 02:54:37,418 DEBUG TRAIN Batch 1/4600 loss 78.742355 loss_att 163.891037 loss_ctc 86.708191 loss_rnnt 60.582066 hw_loss 0.128331 lr 0.00049600 rank 0
2023-02-18 02:54:37,420 DEBUG TRAIN Batch 1/4600 loss 79.912903 loss_att 169.788116 loss_ctc 103.105865 loss_rnnt 58.813538 hw_loss 0.059877 lr 0.00049640 rank 6
2023-02-18 02:54:37,421 DEBUG TRAIN Batch 1/4600 loss 64.224754 loss_att 129.799408 loss_ctc 80.983627 loss_rnnt 48.813118 hw_loss 0.116588 lr 0.00049536 rank 4
2023-02-18 02:54:37,423 DEBUG TRAIN Batch 1/4600 loss 44.198521 loss_att 90.175613 loss_ctc 53.903389 loss_rnnt 33.656803 hw_loss 0.098089 lr 0.00049632 rank 5
2023-02-18 02:54:37,426 DEBUG TRAIN Batch 1/4600 loss 26.037703 loss_att 62.519352 loss_ctc 31.064247 loss_rnnt 17.980265 hw_loss 0.170439 lr 0.00049652 rank 7
2023-02-18 02:54:37,427 DEBUG TRAIN Batch 1/4600 loss 80.322372 loss_att 162.519196 loss_ctc 109.264519 loss_rnnt 59.990555 hw_loss 0.062807 lr 0.00049648 rank 2
2023-02-18 02:54:37,433 DEBUG TRAIN Batch 1/4600 loss 48.453320 loss_att 131.414337 loss_ctc 55.797230 loss_rnnt 30.837828 hw_loss 0.082691 lr 0.00049508 rank 3
2023-02-18 02:55:37,871 DEBUG TRAIN Batch 1/4700 loss 91.487930 loss_att 167.335419 loss_ctc 105.795639 loss_rnnt 74.410660 hw_loss 0.000151 lr 0.00049736 rank 4
2023-02-18 02:55:37,871 DEBUG TRAIN Batch 1/4700 loss 27.066860 loss_att 68.277679 loss_ctc 33.564743 loss_rnnt 17.874205 hw_loss 0.157698 lr 0.00049800 rank 0
2023-02-18 02:55:37,872 DEBUG TRAIN Batch 1/4700 loss 57.029236 loss_att 117.329269 loss_ctc 62.234604 loss_rnnt 44.217091 hw_loss 0.108905 lr 0.00049832 rank 5
2023-02-18 02:55:37,873 DEBUG TRAIN Batch 1/4700 loss 59.023987 loss_att 116.431793 loss_ctc 65.375931 loss_rnnt 46.657375 hw_loss 0.071484 lr 0.00050004 rank 1
2023-02-18 02:55:37,875 DEBUG TRAIN Batch 1/4700 loss 73.978256 loss_att 122.401169 loss_ctc 93.524567 loss_rnnt 61.657604 hw_loss 0.056055 lr 0.00049852 rank 7
2023-02-18 02:55:37,879 DEBUG TRAIN Batch 1/4700 loss 84.714516 loss_att 146.960144 loss_ctc 96.752670 loss_rnnt 70.629578 hw_loss 0.057619 lr 0.00049848 rank 2
2023-02-18 02:55:37,882 DEBUG TRAIN Batch 1/4700 loss 114.204536 loss_att 180.200256 loss_ctc 135.811203 loss_rnnt 98.018272 hw_loss 0.199186 lr 0.00049708 rank 3
2023-02-18 02:55:37,881 DEBUG TRAIN Batch 1/4700 loss 87.718208 loss_att 153.041260 loss_ctc 101.161972 loss_rnnt 72.752655 hw_loss 0.203320 lr 0.00049840 rank 6
2023-02-18 02:56:39,662 DEBUG TRAIN Batch 1/4800 loss 93.303795 loss_att 205.996521 loss_ctc 104.914589 loss_rnnt 69.216988 hw_loss 0.000287 lr 0.00050000 rank 0
2023-02-18 02:56:39,662 DEBUG TRAIN Batch 1/4800 loss 41.424988 loss_att 104.414124 loss_ctc 44.378529 loss_rnnt 28.433311 hw_loss 0.000078 lr 0.00050032 rank 5
2023-02-18 02:56:39,662 DEBUG TRAIN Batch 1/4800 loss 53.283096 loss_att 109.167633 loss_ctc 61.440346 loss_rnnt 41.018402 hw_loss 0.000288 lr 0.00050204 rank 1
2023-02-18 02:56:39,663 DEBUG TRAIN Batch 1/4800 loss 72.578468 loss_att 128.463593 loss_ctc 73.359688 loss_rnnt 61.208702 hw_loss 0.166089 lr 0.00050052 rank 7
2023-02-18 02:56:39,665 DEBUG TRAIN Batch 1/4800 loss 29.746941 loss_att 93.387115 loss_ctc 39.510044 loss_rnnt 15.675375 hw_loss 0.078342 lr 0.00049936 rank 4
2023-02-18 02:56:39,667 DEBUG TRAIN Batch 1/4800 loss 57.969082 loss_att 116.952728 loss_ctc 68.026741 loss_rnnt 44.831261 hw_loss 0.000141 lr 0.00050040 rank 6
2023-02-18 02:56:39,668 DEBUG TRAIN Batch 1/4800 loss 50.640285 loss_att 93.059158 loss_ctc 56.690945 loss_rnnt 41.291908 hw_loss 0.108458 lr 0.00049908 rank 3
2023-02-18 02:56:39,726 DEBUG TRAIN Batch 1/4800 loss 63.881901 loss_att 130.949585 loss_ctc 77.536659 loss_rnnt 48.513523 hw_loss 0.251627 lr 0.00050048 rank 2
2023-02-18 02:57:38,251 DEBUG TRAIN Batch 1/4900 loss 54.841370 loss_att 149.842422 loss_ctc 52.130466 loss_rnnt 36.171257 hw_loss 0.058793 lr 0.00050240 rank 6
2023-02-18 02:57:38,256 DEBUG TRAIN Batch 1/4900 loss 51.310287 loss_att 89.336945 loss_ctc 64.948349 loss_rnnt 41.886478 hw_loss 0.000128 lr 0.00050232 rank 5
2023-02-18 02:57:38,257 DEBUG TRAIN Batch 1/4900 loss 87.806259 loss_att 144.070709 loss_ctc 107.219849 loss_rnnt 73.899284 hw_loss 0.123023 lr 0.00050404 rank 1
2023-02-18 02:57:38,258 DEBUG TRAIN Batch 1/4900 loss 76.336395 loss_att 151.662125 loss_ctc 89.801651 loss_rnnt 59.475800 hw_loss 0.000157 lr 0.00050108 rank 3
2023-02-18 02:57:38,260 DEBUG TRAIN Batch 1/4900 loss 59.476955 loss_att 100.424576 loss_ctc 59.650780 loss_rnnt 51.264191 hw_loss 0.000125 lr 0.00050200 rank 0
2023-02-18 02:57:38,262 DEBUG TRAIN Batch 1/4900 loss 61.554955 loss_att 143.411896 loss_ctc 78.136360 loss_rnnt 42.972679 hw_loss 0.000061 lr 0.00050248 rank 2
2023-02-18 02:57:38,262 DEBUG TRAIN Batch 1/4900 loss 29.510723 loss_att 84.322563 loss_ctc 34.897926 loss_rnnt 17.768356 hw_loss 0.115699 lr 0.00050136 rank 4
2023-02-18 02:57:38,316 DEBUG TRAIN Batch 1/4900 loss 47.578888 loss_att 88.322052 loss_ctc 52.605347 loss_rnnt 38.712280 hw_loss 0.089586 lr 0.00050252 rank 7
2023-02-18 02:58:39,309 DEBUG TRAIN Batch 1/5000 loss 59.577999 loss_att 124.964928 loss_ctc 61.197453 loss_rnnt 46.183723 hw_loss 0.189302 lr 0.00050604 rank 1
2023-02-18 02:58:39,314 DEBUG TRAIN Batch 1/5000 loss 66.948257 loss_att 122.722816 loss_ctc 83.112930 loss_rnnt 53.593399 hw_loss 0.083731 lr 0.00050308 rank 3
2023-02-18 02:58:39,315 DEBUG TRAIN Batch 1/5000 loss 36.800037 loss_att 63.583466 loss_ctc 37.935020 loss_rnnt 31.190414 hw_loss 0.190508 lr 0.00050432 rank 5
2023-02-18 02:58:39,317 DEBUG TRAIN Batch 1/5000 loss 57.560204 loss_att 101.011513 loss_ctc 61.467644 loss_rnnt 48.348404 hw_loss 0.001026 lr 0.00050400 rank 0
2023-02-18 02:58:39,318 DEBUG TRAIN Batch 1/5000 loss 103.380951 loss_att 150.900879 loss_ctc 136.536224 loss_rnnt 89.340088 hw_loss 0.217811 lr 0.00050336 rank 4
2023-02-18 02:58:39,318 DEBUG TRAIN Batch 1/5000 loss 96.615631 loss_att 181.248825 loss_ctc 127.347588 loss_rnnt 75.552917 hw_loss 0.072147 lr 0.00050452 rank 7
2023-02-18 02:58:39,324 DEBUG TRAIN Batch 1/5000 loss 40.521000 loss_att 81.729500 loss_ctc 49.790401 loss_rnnt 31.043343 hw_loss 0.000065 lr 0.00050440 rank 6
2023-02-18 02:58:39,378 DEBUG TRAIN Batch 1/5000 loss 72.042244 loss_att 123.971436 loss_ctc 88.139175 loss_rnnt 59.494011 hw_loss 0.030248 lr 0.00050448 rank 2
2023-02-18 02:59:40,144 DEBUG TRAIN Batch 1/5100 loss 59.316345 loss_att 115.925049 loss_ctc 68.549347 loss_rnnt 46.763458 hw_loss 0.000155 lr 0.00050508 rank 3
2023-02-18 02:59:40,144 DEBUG TRAIN Batch 1/5100 loss 49.554279 loss_att 106.994499 loss_ctc 64.712128 loss_rnnt 35.928967 hw_loss 0.217912 lr 0.00050632 rank 5
2023-02-18 02:59:40,148 DEBUG TRAIN Batch 1/5100 loss 76.263962 loss_att 149.040329 loss_ctc 83.452576 loss_rnnt 60.750160 hw_loss 0.000085 lr 0.00050536 rank 4
2023-02-18 02:59:40,149 DEBUG TRAIN Batch 1/5100 loss 42.219048 loss_att 104.215408 loss_ctc 50.157547 loss_rnnt 28.685493 hw_loss 0.142153 lr 0.00050640 rank 6
2023-02-18 02:59:40,150 DEBUG TRAIN Batch 1/5100 loss 135.545273 loss_att 252.297028 loss_ctc 160.137314 loss_rnnt 108.876022 hw_loss 0.074940 lr 0.00050648 rank 2
2023-02-18 02:59:40,153 DEBUG TRAIN Batch 1/5100 loss 60.764908 loss_att 113.150665 loss_ctc 57.536415 loss_rnnt 50.718090 hw_loss 0.000248 lr 0.00050600 rank 0
2023-02-18 02:59:40,157 DEBUG TRAIN Batch 1/5100 loss 42.105762 loss_att 96.788971 loss_ctc 48.256721 loss_rnnt 30.308931 hw_loss 0.075114 lr 0.00050652 rank 7
2023-02-18 02:59:40,160 DEBUG TRAIN Batch 1/5100 loss 42.861183 loss_att 135.622040 loss_ctc 42.875782 loss_rnnt 24.301321 hw_loss 0.010771 lr 0.00050804 rank 1
2023-02-18 03:01:00,005 DEBUG TRAIN Batch 1/5200 loss 47.238712 loss_att 87.858994 loss_ctc 47.212864 loss_rnnt 39.057617 hw_loss 0.113410 lr 0.00050852 rank 7
2023-02-18 03:01:00,007 DEBUG TRAIN Batch 1/5200 loss 91.227928 loss_att 145.152954 loss_ctc 101.575951 loss_rnnt 78.978790 hw_loss 0.158260 lr 0.00050800 rank 0
2023-02-18 03:01:00,008 DEBUG TRAIN Batch 1/5200 loss 83.974350 loss_att 136.948334 loss_ctc 104.386192 loss_rnnt 70.625015 hw_loss 0.061790 lr 0.00050832 rank 5
2023-02-18 03:01:00,009 DEBUG TRAIN Batch 1/5200 loss 41.577118 loss_att 54.515335 loss_ctc 47.933491 loss_rnnt 38.133087 hw_loss 0.016628 lr 0.00050840 rank 6
2023-02-18 03:01:00,010 DEBUG TRAIN Batch 1/5200 loss 36.810764 loss_att 76.362747 loss_ctc 54.873032 loss_rnnt 26.491989 hw_loss 0.000141 lr 0.00051004 rank 1
2023-02-18 03:01:00,011 DEBUG TRAIN Batch 1/5200 loss 69.085121 loss_att 119.776443 loss_ctc 80.308136 loss_rnnt 57.450367 hw_loss 0.000143 lr 0.00050736 rank 4
2023-02-18 03:01:00,012 DEBUG TRAIN Batch 1/5200 loss 18.564692 loss_att 29.427017 loss_ctc 26.222883 loss_rnnt 15.342044 hw_loss 0.054546 lr 0.00050708 rank 3
2023-02-18 03:01:00,071 DEBUG TRAIN Batch 1/5200 loss 43.368416 loss_att 64.771629 loss_ctc 53.832912 loss_rnnt 37.604187 hw_loss 0.165592 lr 0.00050848 rank 2
2023-02-18 03:02:01,490 DEBUG TRAIN Batch 1/5300 loss 30.159756 loss_att 66.375298 loss_ctc 25.372213 loss_rnnt 23.474134 hw_loss 0.151598 lr 0.00050936 rank 4
2023-02-18 03:02:01,492 DEBUG TRAIN Batch 1/5300 loss 60.457535 loss_att 115.489288 loss_ctc 82.385864 loss_rnnt 46.527359 hw_loss 0.000083 lr 0.00051032 rank 5
2023-02-18 03:02:01,493 DEBUG TRAIN Batch 1/5300 loss 74.505600 loss_att 110.281113 loss_ctc 87.309906 loss_rnnt 65.643120 hw_loss 0.000275 lr 0.00050908 rank 3
2023-02-18 03:02:01,493 DEBUG TRAIN Batch 1/5300 loss 24.400314 loss_att 66.969254 loss_ctc 19.415232 loss_rnnt 16.551067 hw_loss 0.000257 lr 0.00051204 rank 1
2023-02-18 03:02:01,494 DEBUG TRAIN Batch 1/5300 loss 91.012405 loss_att 168.776367 loss_ctc 111.741898 loss_rnnt 72.607750 hw_loss 0.164877 lr 0.00051048 rank 2
2023-02-18 03:02:01,494 DEBUG TRAIN Batch 1/5300 loss 55.438862 loss_att 108.181564 loss_ctc 56.311436 loss_rnnt 44.750786 hw_loss 0.043499 lr 0.00051000 rank 0
2023-02-18 03:02:01,502 DEBUG TRAIN Batch 1/5300 loss 54.902359 loss_att 116.851028 loss_ctc 69.996895 loss_rnnt 40.413582 hw_loss 0.162069 lr 0.00051052 rank 7
2023-02-18 03:02:01,503 DEBUG TRAIN Batch 1/5300 loss 75.065147 loss_att 133.610306 loss_ctc 87.404358 loss_rnnt 61.687199 hw_loss 0.044425 lr 0.00051040 rank 6
2023-02-18 03:03:01,476 DEBUG TRAIN Batch 1/5400 loss 67.037643 loss_att 128.614487 loss_ctc 82.119133 loss_rnnt 52.711380 hw_loss 0.000057 lr 0.00051240 rank 6
2023-02-18 03:03:01,481 DEBUG TRAIN Batch 1/5400 loss 42.415787 loss_att 103.146889 loss_ctc 46.633469 loss_rnnt 29.707170 hw_loss 0.000068 lr 0.00051136 rank 4
2023-02-18 03:03:01,481 DEBUG TRAIN Batch 1/5400 loss 53.930408 loss_att 107.860077 loss_ctc 56.132175 loss_rnnt 42.850872 hw_loss 0.000064 lr 0.00051232 rank 5
2023-02-18 03:03:01,485 DEBUG TRAIN Batch 1/5400 loss 70.673882 loss_att 136.874313 loss_ctc 85.490982 loss_rnnt 55.458153 hw_loss 0.000060 lr 0.00051252 rank 7
2023-02-18 03:03:01,489 DEBUG TRAIN Batch 1/5400 loss 89.560791 loss_att 150.766556 loss_ctc 106.130653 loss_rnnt 75.110268 hw_loss 0.000086 lr 0.00051404 rank 1
2023-02-18 03:03:01,491 DEBUG TRAIN Batch 1/5400 loss 26.791676 loss_att 74.602158 loss_ctc 29.857094 loss_rnnt 16.748199 hw_loss 0.136234 lr 0.00051248 rank 2
2023-02-18 03:03:01,492 DEBUG TRAIN Batch 1/5400 loss 74.079071 loss_att 128.637833 loss_ctc 92.806046 loss_rnnt 60.670338 hw_loss 0.000104 lr 0.00051200 rank 0
2023-02-18 03:03:01,492 DEBUG TRAIN Batch 1/5400 loss 39.194664 loss_att 99.189316 loss_ctc 42.210690 loss_rnnt 26.752382 hw_loss 0.077273 lr 0.00051108 rank 3
2023-02-18 03:04:02,005 DEBUG TRAIN Batch 1/5500 loss 85.344467 loss_att 138.578659 loss_ctc 103.233353 loss_rnnt 72.226753 hw_loss 0.160679 lr 0.00051432 rank 5
2023-02-18 03:04:02,008 DEBUG TRAIN Batch 1/5500 loss 63.607738 loss_att 134.422333 loss_ctc 69.582169 loss_rnnt 48.597622 hw_loss 0.094885 lr 0.00051336 rank 4
2023-02-18 03:04:02,011 DEBUG TRAIN Batch 1/5500 loss 49.529438 loss_att 85.628090 loss_ctc 54.094955 loss_rnnt 41.700886 hw_loss 0.000157 lr 0.00051604 rank 1
2023-02-18 03:04:02,012 DEBUG TRAIN Batch 1/5500 loss 47.188511 loss_att 81.781410 loss_ctc 52.527042 loss_rnnt 39.512058 hw_loss 0.086371 lr 0.00051452 rank 7
2023-02-18 03:04:02,014 DEBUG TRAIN Batch 1/5500 loss 51.571262 loss_att 85.472107 loss_ctc 62.993832 loss_rnnt 43.268021 hw_loss 0.000123 lr 0.00051448 rank 2
2023-02-18 03:04:02,017 DEBUG TRAIN Batch 1/5500 loss 64.354019 loss_att 90.363045 loss_ctc 85.903114 loss_rnnt 56.254539 hw_loss 0.045881 lr 0.00051440 rank 6
2023-02-18 03:04:02,021 DEBUG TRAIN Batch 1/5500 loss 48.346455 loss_att 73.063141 loss_ctc 60.015015 loss_rnnt 41.819717 hw_loss 0.051724 lr 0.00051308 rank 3
2023-02-18 03:04:02,077 DEBUG TRAIN Batch 1/5500 loss 55.024151 loss_att 70.147263 loss_ctc 67.199432 loss_rnnt 50.367046 hw_loss 0.017084 lr 0.00051400 rank 0
2023-02-18 03:05:03,174 DEBUG TRAIN Batch 1/5600 loss 27.274412 loss_att 69.836952 loss_ctc 23.516544 loss_rnnt 19.194283 hw_loss 0.128761 lr 0.00051632 rank 5
2023-02-18 03:05:03,174 DEBUG TRAIN Batch 1/5600 loss 80.047272 loss_att 131.162659 loss_ctc 115.827141 loss_rnnt 65.053528 hw_loss 0.000036 lr 0.00051536 rank 4
2023-02-18 03:05:03,175 DEBUG TRAIN Batch 1/5600 loss 56.454159 loss_att 110.302200 loss_ctc 69.926376 loss_rnnt 43.875603 hw_loss 0.023727 lr 0.00051648 rank 2
2023-02-18 03:05:03,178 DEBUG TRAIN Batch 1/5600 loss 82.815979 loss_att 124.581833 loss_ctc 91.684692 loss_rnnt 73.280289 hw_loss 0.000040 lr 0.00051640 rank 6
2023-02-18 03:05:03,179 DEBUG TRAIN Batch 1/5600 loss 41.959415 loss_att 75.343094 loss_ctc 39.974636 loss_rnnt 35.466324 hw_loss 0.151862 lr 0.00051652 rank 7
2023-02-18 03:05:03,181 DEBUG TRAIN Batch 1/5600 loss 89.314095 loss_att 148.371246 loss_ctc 104.950798 loss_rnnt 75.351250 hw_loss 0.124719 lr 0.00051508 rank 3
2023-02-18 03:05:03,193 DEBUG TRAIN Batch 1/5600 loss 57.447315 loss_att 126.831253 loss_ctc 61.221291 loss_rnnt 43.067307 hw_loss 0.000052 lr 0.00051804 rank 1
2023-02-18 03:05:03,194 DEBUG TRAIN Batch 1/5600 loss 57.334297 loss_att 116.194733 loss_ctc 61.414661 loss_rnnt 44.981224 hw_loss 0.069268 lr 0.00051600 rank 0
2023-02-18 03:06:03,471 DEBUG TRAIN Batch 1/5700 loss 87.766663 loss_att 152.835281 loss_ctc 88.822594 loss_rnnt 74.522781 hw_loss 0.167561 lr 0.00051840 rank 6
2023-02-18 03:06:03,472 DEBUG TRAIN Batch 1/5700 loss 159.373901 loss_att 244.199066 loss_ctc 187.480225 loss_rnnt 138.661316 hw_loss 0.000072 lr 0.00051800 rank 0
2023-02-18 03:06:03,472 DEBUG TRAIN Batch 1/5700 loss 61.394936 loss_att 121.945732 loss_ctc 74.735802 loss_rnnt 47.418701 hw_loss 0.163665 lr 0.00051832 rank 5
2023-02-18 03:06:03,472 DEBUG TRAIN Batch 1/5700 loss 89.824188 loss_att 160.400772 loss_ctc 90.224182 loss_rnnt 75.655464 hw_loss 0.000140 lr 0.00052004 rank 1
2023-02-18 03:06:03,477 DEBUG TRAIN Batch 1/5700 loss 42.223721 loss_att 96.446106 loss_ctc 58.073284 loss_rnnt 29.263277 hw_loss 0.005044 lr 0.00051708 rank 3
2023-02-18 03:06:03,481 DEBUG TRAIN Batch 1/5700 loss 50.654194 loss_att 147.802628 loss_ctc 43.519562 loss_rnnt 32.114727 hw_loss 0.114496 lr 0.00051852 rank 7
2023-02-18 03:06:03,482 DEBUG TRAIN Batch 1/5700 loss 49.409649 loss_att 115.047279 loss_ctc 69.269844 loss_rnnt 33.634048 hw_loss 0.000090 lr 0.00051848 rank 2
2023-02-18 03:06:03,535 DEBUG TRAIN Batch 1/5700 loss 41.653038 loss_att 95.114761 loss_ctc 44.214970 loss_rnnt 30.615070 hw_loss 0.007549 lr 0.00051736 rank 4
2023-02-18 03:07:01,996 DEBUG TRAIN Batch 1/5800 loss 72.943497 loss_att 111.684982 loss_ctc 97.654312 loss_rnnt 61.868916 hw_loss 0.059075 lr 0.00052048 rank 2
2023-02-18 03:07:01,999 DEBUG TRAIN Batch 1/5800 loss 62.013309 loss_att 90.863235 loss_ctc 72.367416 loss_rnnt 54.809391 hw_loss 0.100100 lr 0.00052204 rank 1
2023-02-18 03:07:02,000 DEBUG TRAIN Batch 1/5800 loss 64.995461 loss_att 111.065247 loss_ctc 77.408852 loss_rnnt 54.126308 hw_loss 0.000139 lr 0.00052052 rank 7
2023-02-18 03:07:02,001 DEBUG TRAIN Batch 1/5800 loss 54.541626 loss_att 70.397392 loss_ctc 69.803802 loss_rnnt 49.335438 hw_loss 0.000144 lr 0.00052040 rank 6
2023-02-18 03:07:02,002 DEBUG TRAIN Batch 1/5800 loss 53.206512 loss_att 99.297028 loss_ctc 64.210350 loss_rnnt 42.456715 hw_loss 0.120961 lr 0.00052032 rank 5
2023-02-18 03:07:02,002 DEBUG TRAIN Batch 1/5800 loss 44.655190 loss_att 90.892593 loss_ctc 60.511848 loss_rnnt 33.287952 hw_loss 0.010378 lr 0.00052000 rank 0
2023-02-18 03:07:02,008 DEBUG TRAIN Batch 1/5800 loss 55.120544 loss_att 107.407928 loss_ctc 71.435478 loss_rnnt 42.487671 hw_loss 0.000129 lr 0.00051908 rank 3
2023-02-18 03:07:02,009 DEBUG TRAIN Batch 1/5800 loss 49.787548 loss_att 105.142784 loss_ctc 63.748436 loss_rnnt 36.705933 hw_loss 0.279584 lr 0.00051936 rank 4
2023-02-18 03:08:05,825 DEBUG TRAIN Batch 1/5900 loss 55.449902 loss_att 78.302917 loss_ctc 70.555756 loss_rnnt 48.865158 hw_loss 0.000050 lr 0.00052232 rank 5
2023-02-18 03:08:05,828 DEBUG TRAIN Batch 1/5900 loss 58.093277 loss_att 108.933121 loss_ctc 62.652073 loss_rnnt 47.218487 hw_loss 0.185597 lr 0.00052252 rank 7
2023-02-18 03:08:05,830 DEBUG TRAIN Batch 1/5900 loss 86.775383 loss_att 154.210480 loss_ctc 95.585876 loss_rnnt 72.113586 hw_loss 0.000096 lr 0.00052200 rank 0
2023-02-18 03:08:05,833 DEBUG TRAIN Batch 1/5900 loss 55.670486 loss_att 118.477753 loss_ctc 70.549583 loss_rnnt 41.100273 hw_loss 0.046644 lr 0.00052240 rank 6
2023-02-18 03:08:05,835 DEBUG TRAIN Batch 1/5900 loss 51.854351 loss_att 108.749489 loss_ctc 63.283733 loss_rnnt 38.905579 hw_loss 0.085928 lr 0.00052248 rank 2
2023-02-18 03:08:05,838 DEBUG TRAIN Batch 1/5900 loss 46.922039 loss_att 101.598938 loss_ctc 59.492550 loss_rnnt 34.254929 hw_loss 0.104358 lr 0.00052108 rank 3
2023-02-18 03:08:05,838 DEBUG TRAIN Batch 1/5900 loss 42.878895 loss_att 122.303635 loss_ctc 38.894497 loss_rnnt 27.525156 hw_loss 0.000074 lr 0.00052404 rank 1
2023-02-18 03:08:05,905 DEBUG TRAIN Batch 1/5900 loss 32.675293 loss_att 69.462486 loss_ctc 39.454777 loss_rnnt 24.411095 hw_loss 0.005306 lr 0.00052136 rank 4
2023-02-18 03:09:25,023 DEBUG TRAIN Batch 1/6000 loss 19.060341 loss_att 26.529224 loss_ctc 22.364552 loss_rnnt 17.008648 hw_loss 0.220043 lr 0.00052604 rank 1
2023-02-18 03:09:25,031 DEBUG TRAIN Batch 1/6000 loss 82.652702 loss_att 159.648590 loss_ctc 84.929695 loss_rnnt 66.946121 hw_loss 0.007131 lr 0.00052336 rank 4
2023-02-18 03:09:25,035 DEBUG TRAIN Batch 1/6000 loss 41.261395 loss_att 107.575760 loss_ctc 52.065392 loss_rnnt 26.425831 hw_loss 0.247798 lr 0.00052440 rank 6
2023-02-18 03:09:25,035 DEBUG TRAIN Batch 1/6000 loss 78.019814 loss_att 143.760513 loss_ctc 90.324936 loss_rnnt 63.201569 hw_loss 0.055167 lr 0.00052448 rank 2
2023-02-18 03:09:25,038 DEBUG TRAIN Batch 1/6000 loss 30.942701 loss_att 42.481487 loss_ctc 38.795788 loss_rnnt 27.524073 hw_loss 0.119610 lr 0.00052452 rank 7
2023-02-18 03:09:25,038 DEBUG TRAIN Batch 1/6000 loss 58.010830 loss_att 119.548447 loss_ctc 79.077995 loss_rnnt 42.834297 hw_loss 0.112602 lr 0.00052432 rank 5
2023-02-18 03:09:25,039 DEBUG TRAIN Batch 1/6000 loss 88.886841 loss_att 149.410629 loss_ctc 103.227867 loss_rnnt 74.869904 hw_loss 0.000067 lr 0.00052400 rank 0
2023-02-18 03:09:25,042 DEBUG TRAIN Batch 1/6000 loss 36.112217 loss_att 63.877117 loss_ctc 39.545273 loss_rnnt 30.101414 hw_loss 0.000157 lr 0.00052308 rank 3
2023-02-18 03:10:27,524 DEBUG TRAIN Batch 1/6100 loss 64.588005 loss_att 108.402542 loss_ctc 75.903847 loss_rnnt 54.316254 hw_loss 0.000119 lr 0.00052804 rank 1
2023-02-18 03:10:27,528 DEBUG TRAIN Batch 1/6100 loss 51.716476 loss_att 82.823921 loss_ctc 61.422485 loss_rnnt 44.200748 hw_loss 0.000193 lr 0.00052508 rank 3
2023-02-18 03:10:27,528 DEBUG TRAIN Batch 1/6100 loss 28.883499 loss_att 78.067421 loss_ctc 30.545021 loss_rnnt 18.825155 hw_loss 0.000045 lr 0.00052536 rank 4
2023-02-18 03:10:27,529 DEBUG TRAIN Batch 1/6100 loss 54.380447 loss_att 111.802559 loss_ctc 61.432728 loss_rnnt 41.920876 hw_loss 0.065328 lr 0.00052632 rank 5
2023-02-18 03:10:27,530 DEBUG TRAIN Batch 1/6100 loss 77.338448 loss_att 139.189285 loss_ctc 87.725624 loss_rnnt 63.583290 hw_loss 0.000061 lr 0.00052652 rank 7
2023-02-18 03:10:27,530 DEBUG TRAIN Batch 1/6100 loss 48.900970 loss_att 68.167252 loss_ctc 53.202248 loss_rnnt 44.418816 hw_loss 0.103873 lr 0.00052640 rank 6
2023-02-18 03:10:27,530 DEBUG TRAIN Batch 1/6100 loss 42.300556 loss_att 92.525070 loss_ctc 55.457832 loss_rnnt 30.468544 hw_loss 0.061498 lr 0.00052648 rank 2
2023-02-18 03:10:27,533 DEBUG TRAIN Batch 1/6100 loss 54.077751 loss_att 91.453125 loss_ctc 75.559410 loss_rnnt 43.708992 hw_loss 0.055241 lr 0.00052600 rank 0
2023-02-18 03:11:28,796 DEBUG TRAIN Batch 1/6200 loss 63.097515 loss_att 121.281181 loss_ctc 72.708145 loss_rnnt 50.179226 hw_loss 0.000261 lr 0.00052840 rank 6
2023-02-18 03:11:28,798 DEBUG TRAIN Batch 1/6200 loss 129.347534 loss_att 174.162216 loss_ctc 148.000443 loss_rnnt 117.896606 hw_loss 0.001744 lr 0.00052832 rank 5
2023-02-18 03:11:28,800 DEBUG TRAIN Batch 1/6200 loss 69.959389 loss_att 132.573441 loss_ctc 89.642517 loss_rnnt 54.642731 hw_loss 0.317665 lr 0.00052852 rank 7
2023-02-18 03:11:28,801 DEBUG TRAIN Batch 1/6200 loss 48.562489 loss_att 104.714478 loss_ctc 63.199970 loss_rnnt 35.380402 hw_loss 0.000047 lr 0.00053004 rank 1
2023-02-18 03:11:28,803 DEBUG TRAIN Batch 1/6200 loss 40.045094 loss_att 73.141464 loss_ctc 55.034348 loss_rnnt 31.427231 hw_loss 0.000042 lr 0.00052800 rank 0
2023-02-18 03:11:28,806 DEBUG TRAIN Batch 1/6200 loss 85.120819 loss_att 168.266495 loss_ctc 97.127129 loss_rnnt 66.800018 hw_loss 0.170290 lr 0.00052708 rank 3
2023-02-18 03:11:28,806 DEBUG TRAIN Batch 1/6200 loss 56.703701 loss_att 104.185791 loss_ctc 63.745262 loss_rnnt 46.240372 hw_loss 0.052567 lr 0.00052848 rank 2
2023-02-18 03:11:28,858 DEBUG TRAIN Batch 1/6200 loss 51.857288 loss_att 91.391663 loss_ctc 62.361252 loss_rnnt 42.549858 hw_loss 0.000049 lr 0.00052736 rank 4
2023-02-18 03:12:27,589 DEBUG TRAIN Batch 1/6300 loss 52.069935 loss_att 113.491257 loss_ctc 66.547272 loss_rnnt 37.848934 hw_loss 0.012049 lr 0.00053040 rank 6
2023-02-18 03:12:27,590 DEBUG TRAIN Batch 1/6300 loss 51.812313 loss_att 76.989014 loss_ctc 55.269436 loss_rnnt 46.315765 hw_loss 0.000488 lr 0.00053204 rank 1
2023-02-18 03:12:27,591 DEBUG TRAIN Batch 1/6300 loss 20.600056 loss_att 27.744587 loss_ctc 25.628696 loss_rnnt 18.468767 hw_loss 0.059803 lr 0.00052936 rank 4
2023-02-18 03:12:27,594 DEBUG TRAIN Batch 1/6300 loss 46.317486 loss_att 92.356331 loss_ctc 61.005798 loss_rnnt 35.110439 hw_loss 0.076571 lr 0.00053052 rank 7
2023-02-18 03:12:27,597 DEBUG TRAIN Batch 1/6300 loss 66.784157 loss_att 95.386917 loss_ctc 67.518967 loss_rnnt 60.859573 hw_loss 0.198864 lr 0.00053000 rank 0
2023-02-18 03:12:27,604 DEBUG TRAIN Batch 1/6300 loss 22.604881 loss_att 32.194946 loss_ctc 26.789795 loss_rnnt 20.081118 hw_loss 0.089552 lr 0.00053032 rank 5
2023-02-18 03:12:27,605 DEBUG TRAIN Batch 1/6300 loss 72.328453 loss_att 127.557587 loss_ctc 84.996445 loss_rnnt 59.593437 hw_loss 0.000235 lr 0.00052908 rank 3
2023-02-18 03:12:27,655 DEBUG TRAIN Batch 1/6300 loss 46.189758 loss_att 113.513092 loss_ctc 60.651508 loss_rnnt 30.796673 hw_loss 0.000347 lr 0.00053048 rank 2
2023-02-18 03:13:28,172 DEBUG TRAIN Batch 1/6400 loss 59.461681 loss_att 101.888435 loss_ctc 77.531639 loss_rnnt 48.480118 hw_loss 0.162913 lr 0.00053200 rank 0
2023-02-18 03:13:28,178 DEBUG TRAIN Batch 1/6400 loss 43.370453 loss_att 89.336311 loss_ctc 54.739594 loss_rnnt 32.661366 hw_loss 0.000055 lr 0.00053232 rank 5
2023-02-18 03:13:28,178 DEBUG TRAIN Batch 1/6400 loss 44.735172 loss_att 69.073776 loss_ctc 47.863693 loss_rnnt 39.448265 hw_loss 0.003845 lr 0.00053240 rank 6
2023-02-18 03:13:28,180 DEBUG TRAIN Batch 1/6400 loss 84.062126 loss_att 162.543549 loss_ctc 105.290802 loss_rnnt 65.514603 hw_loss 0.038906 lr 0.00053252 rank 7
2023-02-18 03:13:28,181 DEBUG TRAIN Batch 1/6400 loss 73.007004 loss_att 145.509033 loss_ctc 96.752953 loss_rnnt 55.318256 hw_loss 0.041664 lr 0.00053136 rank 4
2023-02-18 03:13:28,181 DEBUG TRAIN Batch 1/6400 loss 30.291718 loss_att 59.808853 loss_ctc 34.271332 loss_rnnt 23.843666 hw_loss 0.026266 lr 0.00053108 rank 3
2023-02-18 03:13:28,188 DEBUG TRAIN Batch 1/6400 loss 64.575851 loss_att 135.245758 loss_ctc 72.458786 loss_rnnt 49.390778 hw_loss 0.000061 lr 0.00053248 rank 2
2023-02-18 03:13:28,191 DEBUG TRAIN Batch 1/6400 loss 71.429939 loss_att 137.502167 loss_ctc 87.840935 loss_rnnt 56.003113 hw_loss 0.045447 lr 0.00053404 rank 1
2023-02-18 03:14:29,234 DEBUG TRAIN Batch 1/6500 loss 83.286705 loss_att 170.964218 loss_ctc 109.831047 loss_rnnt 62.211899 hw_loss 0.000090 lr 0.00053432 rank 5
2023-02-18 03:14:29,236 DEBUG TRAIN Batch 1/6500 loss 55.548775 loss_att 105.778564 loss_ctc 72.959755 loss_rnnt 43.164738 hw_loss 0.031143 lr 0.00053400 rank 0
2023-02-18 03:14:29,237 DEBUG TRAIN Batch 1/6500 loss 52.102974 loss_att 95.547531 loss_ctc 53.645851 loss_rnnt 43.111023 hw_loss 0.182476 lr 0.00053448 rank 2
2023-02-18 03:14:29,239 DEBUG TRAIN Batch 1/6500 loss 38.463902 loss_att 85.790009 loss_ctc 52.255943 loss_rnnt 27.159689 hw_loss 0.000101 lr 0.00053336 rank 4
2023-02-18 03:14:29,239 DEBUG TRAIN Batch 1/6500 loss 78.726791 loss_att 138.129578 loss_ctc 94.507385 loss_rnnt 64.735695 hw_loss 0.012110 lr 0.00053604 rank 1
2023-02-18 03:14:29,242 DEBUG TRAIN Batch 1/6500 loss 58.177757 loss_att 141.720398 loss_ctc 64.319725 loss_rnnt 40.642326 hw_loss 0.014944 lr 0.00053308 rank 3
2023-02-18 03:14:29,248 DEBUG TRAIN Batch 1/6500 loss 21.667648 loss_att 61.486183 loss_ctc 22.350779 loss_rnnt 13.549312 hw_loss 0.119149 lr 0.00053452 rank 7
2023-02-18 03:14:29,250 DEBUG TRAIN Batch 1/6500 loss 36.429325 loss_att 82.968422 loss_ctc 46.032612 loss_rnnt 25.800817 hw_loss 0.075465 lr 0.00053440 rank 6
2023-02-18 03:15:28,218 DEBUG TRAIN Batch 1/6600 loss 63.439754 loss_att 103.723953 loss_ctc 82.747269 loss_rnnt 52.773018 hw_loss 0.066670 lr 0.00053632 rank 5
2023-02-18 03:15:28,223 DEBUG TRAIN Batch 1/6600 loss 39.818531 loss_att 103.877289 loss_ctc 39.063049 loss_rnnt 27.107426 hw_loss 0.000149 lr 0.00053600 rank 0
2023-02-18 03:15:28,224 DEBUG TRAIN Batch 1/6600 loss 50.897717 loss_att 82.785873 loss_ctc 65.236115 loss_rnnt 42.542320 hw_loss 0.123711 lr 0.00053804 rank 1
2023-02-18 03:15:28,226 DEBUG TRAIN Batch 1/6600 loss 93.105019 loss_att 166.969498 loss_ctc 102.182762 loss_rnnt 77.121719 hw_loss 0.000070 lr 0.00053648 rank 2
2023-02-18 03:15:28,228 DEBUG TRAIN Batch 1/6600 loss 38.092609 loss_att 106.826630 loss_ctc 43.158611 loss_rnnt 23.670294 hw_loss 0.000083 lr 0.00053508 rank 3
2023-02-18 03:15:28,233 DEBUG TRAIN Batch 1/6600 loss 33.855309 loss_att 46.358261 loss_ctc 42.435532 loss_rnnt 30.194635 hw_loss 0.030096 lr 0.00053536 rank 4
2023-02-18 03:15:28,235 DEBUG TRAIN Batch 1/6600 loss 63.383324 loss_att 142.309875 loss_ctc 89.922958 loss_rnnt 44.059357 hw_loss 0.000072 lr 0.00053640 rank 6
2023-02-18 03:15:28,284 DEBUG TRAIN Batch 1/6600 loss 55.555862 loss_att 98.672592 loss_ctc 65.745903 loss_rnnt 45.573807 hw_loss 0.000079 lr 0.00053652 rank 7
2023-02-18 03:16:27,726 DEBUG TRAIN Batch 1/6700 loss 52.110748 loss_att 112.373199 loss_ctc 74.397682 loss_rnnt 36.976803 hw_loss 0.205993 lr 0.00053736 rank 4
2023-02-18 03:16:27,731 DEBUG TRAIN Batch 1/6700 loss 58.981487 loss_att 93.933769 loss_ctc 68.460114 loss_rnnt 50.645901 hw_loss 0.152466 lr 0.00053832 rank 5
2023-02-18 03:16:27,733 DEBUG TRAIN Batch 1/6700 loss 27.012983 loss_att 61.906929 loss_ctc 36.131660 loss_rnnt 18.775103 hw_loss 0.081127 lr 0.00053708 rank 3
2023-02-18 03:16:27,733 DEBUG TRAIN Batch 1/6700 loss 59.738541 loss_att 100.172859 loss_ctc 81.567581 loss_rnnt 48.699272 hw_loss 0.078508 lr 0.00053800 rank 0
2023-02-18 03:16:27,734 DEBUG TRAIN Batch 1/6700 loss 62.764088 loss_att 105.486626 loss_ctc 71.750519 loss_rnnt 53.014290 hw_loss 0.013300 lr 0.00053852 rank 7
2023-02-18 03:16:27,738 DEBUG TRAIN Batch 1/6700 loss 84.307961 loss_att 134.705688 loss_ctc 108.456596 loss_rnnt 70.909035 hw_loss 0.186682 lr 0.00053848 rank 2
2023-02-18 03:16:27,742 DEBUG TRAIN Batch 1/6700 loss 61.070511 loss_att 117.844131 loss_ctc 77.396164 loss_rnnt 47.501217 hw_loss 0.070906 lr 0.00054004 rank 1
2023-02-18 03:16:27,749 DEBUG TRAIN Batch 1/6700 loss 55.826153 loss_att 101.991737 loss_ctc 66.608109 loss_rnnt 45.153305 hw_loss 0.003991 lr 0.00053840 rank 6
2023-02-18 03:17:29,335 DEBUG TRAIN Batch 1/6800 loss 44.614578 loss_att 127.309387 loss_ctc 59.365883 loss_rnnt 26.014252 hw_loss 0.177226 lr 0.00054032 rank 5
2023-02-18 03:17:29,336 DEBUG TRAIN Batch 1/6800 loss 71.965744 loss_att 134.512909 loss_ctc 81.854294 loss_rnnt 58.137772 hw_loss 0.000113 lr 0.00054052 rank 7
2023-02-18 03:17:29,338 DEBUG TRAIN Batch 1/6800 loss 95.649796 loss_att 140.129578 loss_ctc 111.720200 loss_rnnt 84.611023 hw_loss 0.000170 lr 0.00053908 rank 3
2023-02-18 03:17:29,338 DEBUG TRAIN Batch 1/6800 loss 47.385483 loss_att 111.352577 loss_ctc 63.966595 loss_rnnt 32.301826 hw_loss 0.148921 lr 0.00054040 rank 6
2023-02-18 03:17:29,339 DEBUG TRAIN Batch 1/6800 loss 118.167641 loss_att 157.366211 loss_ctc 148.439560 loss_rnnt 106.291611 hw_loss 0.000113 lr 0.00054048 rank 2
2023-02-18 03:17:29,340 DEBUG TRAIN Batch 1/6800 loss 52.931473 loss_att 121.600159 loss_ctc 59.427509 loss_rnnt 38.331497 hw_loss 0.000178 lr 0.00054204 rank 1
2023-02-18 03:17:29,342 DEBUG TRAIN Batch 1/6800 loss 93.261589 loss_att 133.279053 loss_ctc 111.497215 loss_rnnt 82.826630 hw_loss 0.000093 lr 0.00053936 rank 4
2023-02-18 03:17:29,343 DEBUG TRAIN Batch 1/6800 loss 42.145226 loss_att 89.495438 loss_ctc 54.528641 loss_rnnt 30.953463 hw_loss 0.132369 lr 0.00054000 rank 0
2023-02-18 03:18:52,972 DEBUG TRAIN Batch 1/6900 loss 48.382401 loss_att 108.248734 loss_ctc 60.832626 loss_rnnt 34.722515 hw_loss 0.049854 lr 0.00054404 rank 1
2023-02-18 03:18:52,975 DEBUG TRAIN Batch 1/6900 loss 72.888969 loss_att 99.089142 loss_ctc 84.657494 loss_rnnt 66.043747 hw_loss 0.067603 lr 0.00054136 rank 4
2023-02-18 03:18:52,975 DEBUG TRAIN Batch 1/6900 loss 46.942749 loss_att 95.419319 loss_ctc 56.509018 loss_rnnt 35.971851 hw_loss 0.000157 lr 0.00054232 rank 5
2023-02-18 03:18:52,977 DEBUG TRAIN Batch 1/6900 loss 177.585358 loss_att 229.675537 loss_ctc 202.420197 loss_rnnt 163.855957 hw_loss 0.000092 lr 0.00054248 rank 2
2023-02-18 03:18:52,978 DEBUG TRAIN Batch 1/6900 loss 78.573166 loss_att 139.470444 loss_ctc 83.079956 loss_rnnt 65.657562 hw_loss 0.253578 lr 0.00054108 rank 3
2023-02-18 03:18:52,981 DEBUG TRAIN Batch 1/6900 loss 58.797504 loss_att 100.747910 loss_ctc 78.384369 loss_rnnt 47.702110 hw_loss 0.175752 lr 0.00054252 rank 7
2023-02-18 03:18:52,980 DEBUG TRAIN Batch 1/6900 loss 80.252861 loss_att 151.523346 loss_ctc 109.640068 loss_rnnt 62.072845 hw_loss 0.014286 lr 0.00054240 rank 6
2023-02-18 03:18:52,981 DEBUG TRAIN Batch 1/6900 loss 40.192047 loss_att 87.814888 loss_ctc 42.947720 loss_rnnt 30.224815 hw_loss 0.141080 lr 0.00054200 rank 0
2023-02-18 03:19:55,705 DEBUG TRAIN Batch 1/7000 loss 55.464138 loss_att 121.361771 loss_ctc 77.943352 loss_rnnt 39.261581 hw_loss 0.048381 lr 0.00054432 rank 5
2023-02-18 03:19:55,707 DEBUG TRAIN Batch 1/7000 loss 40.173504 loss_att 71.936218 loss_ctc 51.873497 loss_rnnt 32.218651 hw_loss 0.079337 lr 0.00054448 rank 2
2023-02-18 03:19:55,707 DEBUG TRAIN Batch 1/7000 loss 79.755219 loss_att 120.590027 loss_ctc 91.385208 loss_rnnt 69.962921 hw_loss 0.140007 lr 0.00054604 rank 1
2023-02-18 03:19:55,707 DEBUG TRAIN Batch 1/7000 loss 20.002136 loss_att 48.047379 loss_ctc 26.819422 loss_rnnt 13.435889 hw_loss 0.090426 lr 0.00054440 rank 6
2023-02-18 03:19:55,708 DEBUG TRAIN Batch 1/7000 loss 36.608978 loss_att 66.460922 loss_ctc 51.935074 loss_rnnt 28.595070 hw_loss 0.000073 lr 0.00054308 rank 3
2023-02-18 03:19:55,714 DEBUG TRAIN Batch 1/7000 loss 77.903831 loss_att 122.938156 loss_ctc 97.176521 loss_rnnt 66.327148 hw_loss 0.000245 lr 0.00054400 rank 0
2023-02-18 03:19:55,716 DEBUG TRAIN Batch 1/7000 loss 53.264339 loss_att 102.857407 loss_ctc 69.007027 loss_rnnt 41.191433 hw_loss 0.103625 lr 0.00054452 rank 7
2023-02-18 03:19:55,766 DEBUG TRAIN Batch 1/7000 loss 48.588127 loss_att 81.116936 loss_ctc 67.637016 loss_rnnt 39.506714 hw_loss 0.067119 lr 0.00054336 rank 4
2023-02-18 03:20:56,686 DEBUG TRAIN Batch 1/7100 loss 45.804031 loss_att 122.615372 loss_ctc 59.124779 loss_rnnt 28.607399 hw_loss 0.109242 lr 0.00054804 rank 1
2023-02-18 03:20:56,686 DEBUG TRAIN Batch 1/7100 loss 34.277168 loss_att 77.792267 loss_ctc 42.492016 loss_rnnt 24.478781 hw_loss 0.000094 lr 0.00054640 rank 6
2023-02-18 03:20:56,692 DEBUG TRAIN Batch 1/7100 loss 31.966454 loss_att 69.057365 loss_ctc 36.388535 loss_rnnt 23.958612 hw_loss 0.000087 lr 0.00054652 rank 7
2023-02-18 03:20:56,697 DEBUG TRAIN Batch 1/7100 loss 59.280788 loss_att 110.260864 loss_ctc 86.257935 loss_rnnt 45.481842 hw_loss 0.011201 lr 0.00054632 rank 5
2023-02-18 03:20:56,698 DEBUG TRAIN Batch 1/7100 loss 40.230946 loss_att 87.010307 loss_ctc 68.744698 loss_rnnt 27.073181 hw_loss 0.000105 lr 0.00054600 rank 0
2023-02-18 03:20:56,697 DEBUG TRAIN Batch 1/7100 loss 112.579109 loss_att 185.950531 loss_ctc 132.364578 loss_rnnt 95.266685 hw_loss 0.000153 lr 0.00054508 rank 3
2023-02-18 03:20:56,699 DEBUG TRAIN Batch 1/7100 loss 67.767426 loss_att 126.074692 loss_ctc 80.295959 loss_rnnt 54.338280 hw_loss 0.182309 lr 0.00054648 rank 2
2023-02-18 03:20:56,702 DEBUG TRAIN Batch 1/7100 loss 47.318382 loss_att 103.869698 loss_ctc 55.704479 loss_rnnt 34.773834 hw_loss 0.217760 lr 0.00054536 rank 4
2023-02-18 03:21:55,671 DEBUG TRAIN Batch 1/7200 loss 25.770744 loss_att 35.635803 loss_ctc 35.014309 loss_rnnt 22.539358 hw_loss 0.048561 lr 0.00054840 rank 6
2023-02-18 03:21:55,675 DEBUG TRAIN Batch 1/7200 loss 43.881786 loss_att 79.771118 loss_ctc 52.349594 loss_rnnt 35.574844 hw_loss 0.000070 lr 0.00054832 rank 5
2023-02-18 03:21:55,676 DEBUG TRAIN Batch 1/7200 loss 28.185289 loss_att 53.503532 loss_ctc 35.562630 loss_rnnt 22.137962 hw_loss 0.000064 lr 0.00054736 rank 4
2023-02-18 03:21:55,677 DEBUG TRAIN Batch 1/7200 loss 60.155251 loss_att 89.964523 loss_ctc 73.632401 loss_rnnt 52.396381 hw_loss 0.000113 lr 0.00054800 rank 0
2023-02-18 03:21:55,678 DEBUG TRAIN Batch 1/7200 loss 30.465828 loss_att 41.657059 loss_ctc 37.613503 loss_rnnt 27.200993 hw_loss 0.137929 lr 0.00054708 rank 3
2023-02-18 03:21:55,683 DEBUG TRAIN Batch 1/7200 loss 97.492043 loss_att 138.396118 loss_ctc 115.478752 loss_rnnt 86.873245 hw_loss 0.074512 lr 0.00055004 rank 1
2023-02-18 03:21:55,706 DEBUG TRAIN Batch 1/7200 loss 49.602451 loss_att 85.931122 loss_ctc 71.750557 loss_rnnt 39.383598 hw_loss 0.000074 lr 0.00054848 rank 2
2023-02-18 03:21:55,713 DEBUG TRAIN Batch 1/7200 loss 82.228371 loss_att 131.665558 loss_ctc 108.014824 loss_rnnt 68.763062 hw_loss 0.261899 lr 0.00054852 rank 7
2023-02-18 03:22:57,192 DEBUG TRAIN Batch 1/7300 loss 51.135498 loss_att 95.569489 loss_ctc 65.608368 loss_rnnt 40.204178 hw_loss 0.215267 lr 0.00055032 rank 5
2023-02-18 03:22:57,192 DEBUG TRAIN Batch 1/7300 loss 47.850922 loss_att 92.381783 loss_ctc 57.604519 loss_rnnt 37.641048 hw_loss 0.006041 lr 0.00055040 rank 6
2023-02-18 03:22:57,192 DEBUG TRAIN Batch 1/7300 loss 85.383812 loss_att 131.561859 loss_ctc 103.834770 loss_rnnt 73.626579 hw_loss 0.115306 lr 0.00055048 rank 2
2023-02-18 03:22:57,195 DEBUG TRAIN Batch 1/7300 loss 103.721535 loss_att 202.390976 loss_ctc 114.547806 loss_rnnt 82.483398 hw_loss 0.113891 lr 0.00055204 rank 1
2023-02-18 03:22:57,198 DEBUG TRAIN Batch 1/7300 loss 69.460876 loss_att 113.193115 loss_ctc 88.415939 loss_rnnt 58.187012 hw_loss 0.000124 lr 0.00055000 rank 0
2023-02-18 03:22:57,220 DEBUG TRAIN Batch 1/7300 loss 32.713955 loss_att 65.760620 loss_ctc 44.292816 loss_rnnt 24.540815 hw_loss 0.037419 lr 0.00054908 rank 3
2023-02-18 03:22:57,231 DEBUG TRAIN Batch 1/7300 loss 63.999012 loss_att 134.301071 loss_ctc 91.464401 loss_rnnt 46.212456 hw_loss 0.120180 lr 0.00054936 rank 4
2023-02-18 03:22:57,257 DEBUG TRAIN Batch 1/7300 loss 61.365646 loss_att 118.558334 loss_ctc 77.667923 loss_rnnt 47.672775 hw_loss 0.151312 lr 0.00055052 rank 7
2023-02-18 03:23:58,911 DEBUG TRAIN Batch 1/7400 loss 74.160904 loss_att 130.101257 loss_ctc 93.168877 loss_rnnt 60.345646 hw_loss 0.173990 lr 0.00055232 rank 5
2023-02-18 03:23:58,912 DEBUG TRAIN Batch 1/7400 loss 61.575047 loss_att 124.777710 loss_ctc 68.533264 loss_rnnt 47.961300 hw_loss 0.085232 lr 0.00055136 rank 4
2023-02-18 03:23:58,913 DEBUG TRAIN Batch 1/7400 loss 26.350016 loss_att 57.437397 loss_ctc 28.634138 loss_rnnt 19.827797 hw_loss 0.000362 lr 0.00055240 rank 6
2023-02-18 03:23:58,913 DEBUG TRAIN Batch 1/7400 loss 81.233925 loss_att 138.967346 loss_ctc 104.370064 loss_rnnt 66.602203 hw_loss 0.000391 lr 0.00055252 rank 7
2023-02-18 03:23:58,915 DEBUG TRAIN Batch 1/7400 loss 34.208145 loss_att 77.741425 loss_ctc 39.206177 loss_rnnt 24.834902 hw_loss 0.000343 lr 0.00055404 rank 1
2023-02-18 03:23:58,920 DEBUG TRAIN Batch 1/7400 loss 58.004913 loss_att 107.615288 loss_ctc 65.026657 loss_rnnt 47.146526 hw_loss 0.000155 lr 0.00055108 rank 3
2023-02-18 03:23:58,921 DEBUG TRAIN Batch 1/7400 loss 31.628464 loss_att 76.492393 loss_ctc 34.325176 loss_rnnt 22.295925 hw_loss 0.000358 lr 0.00055248 rank 2
2023-02-18 03:23:58,979 DEBUG TRAIN Batch 1/7400 loss 144.446396 loss_att 229.076736 loss_ctc 178.949097 loss_rnnt 122.919601 hw_loss 0.000704 lr 0.00055200 rank 0
2023-02-18 03:24:57,577 DEBUG TRAIN Batch 1/7500 loss 55.256439 loss_att 116.883247 loss_ctc 81.478111 loss_rnnt 39.433395 hw_loss 0.002726 lr 0.00055448 rank 2
2023-02-18 03:24:57,579 DEBUG TRAIN Batch 1/7500 loss 50.341312 loss_att 94.539276 loss_ctc 67.606010 loss_rnnt 39.149052 hw_loss 0.095075 lr 0.00055336 rank 4
2023-02-18 03:24:57,582 DEBUG TRAIN Batch 1/7500 loss 43.288937 loss_att 76.030006 loss_ctc 52.942265 loss_rnnt 35.414967 hw_loss 0.072448 lr 0.00055604 rank 1
2023-02-18 03:24:57,583 DEBUG TRAIN Batch 1/7500 loss 48.031284 loss_att 94.632462 loss_ctc 60.628876 loss_rnnt 37.021576 hw_loss 0.018354 lr 0.00055432 rank 5
2023-02-18 03:24:57,585 DEBUG TRAIN Batch 1/7500 loss 17.332329 loss_att 28.043232 loss_ctc 20.932547 loss_rnnt 14.543375 hw_loss 0.312645 lr 0.00055440 rank 6
2023-02-18 03:24:57,587 DEBUG TRAIN Batch 1/7500 loss 92.762383 loss_att 138.864395 loss_ctc 110.321152 loss_rnnt 81.200722 hw_loss 0.000175 lr 0.00055400 rank 0
2023-02-18 03:24:57,587 DEBUG TRAIN Batch 1/7500 loss 50.224991 loss_att 71.793526 loss_ctc 60.437981 loss_rnnt 44.537308 hw_loss 0.022963 lr 0.00055308 rank 3
2023-02-18 03:24:57,594 DEBUG TRAIN Batch 1/7500 loss 86.364517 loss_att 160.617065 loss_ctc 120.096298 loss_rnnt 67.002579 hw_loss 0.025985 lr 0.00055452 rank 7
2023-02-18 03:25:57,942 DEBUG TRAIN Batch 1/7600 loss 75.337166 loss_att 145.950165 loss_ctc 87.297394 loss_rnnt 59.578121 hw_loss 0.078268 lr 0.00055632 rank 5
2023-02-18 03:25:57,945 DEBUG TRAIN Batch 1/7600 loss 42.832546 loss_att 97.142395 loss_ctc 50.047894 loss_rnnt 30.992493 hw_loss 0.030073 lr 0.00055508 rank 3
2023-02-18 03:25:57,950 DEBUG TRAIN Batch 1/7600 loss 46.153526 loss_att 107.787666 loss_ctc 50.729454 loss_rnnt 33.216507 hw_loss 0.000137 lr 0.00055652 rank 7
2023-02-18 03:25:57,953 DEBUG TRAIN Batch 1/7600 loss 44.497665 loss_att 81.295532 loss_ctc 65.594131 loss_rnnt 34.291656 hw_loss 0.062946 lr 0.00055600 rank 0
2023-02-18 03:25:57,961 DEBUG TRAIN Batch 1/7600 loss 58.191040 loss_att 106.465218 loss_ctc 74.589966 loss_rnnt 46.349613 hw_loss 0.000126 lr 0.00055648 rank 2
2023-02-18 03:25:57,986 DEBUG TRAIN Batch 1/7600 loss 25.463367 loss_att 61.883369 loss_ctc 32.893486 loss_rnnt 17.075142 hw_loss 0.212887 lr 0.00055536 rank 4
2023-02-18 03:25:57,992 DEBUG TRAIN Batch 1/7600 loss 53.409954 loss_att 95.806969 loss_ctc 72.206421 loss_rnnt 42.334732 hw_loss 0.168048 lr 0.00055640 rank 6
2023-02-18 03:25:58,005 DEBUG TRAIN Batch 1/7600 loss 60.489815 loss_att 114.922501 loss_ctc 70.976875 loss_rnnt 48.141392 hw_loss 0.119270 lr 0.00055804 rank 1
2023-02-18 03:27:13,551 DEBUG TRAIN Batch 1/7700 loss 37.843754 loss_att 70.207756 loss_ctc 39.573112 loss_rnnt 31.052919 hw_loss 0.163966 lr 0.00055840 rank 6
2023-02-18 03:27:13,551 DEBUG TRAIN Batch 1/7700 loss 102.951294 loss_att 177.174332 loss_ctc 119.759102 loss_rnnt 85.865608 hw_loss 0.000049 lr 0.00055832 rank 5
2023-02-18 03:27:13,552 DEBUG TRAIN Batch 1/7700 loss 99.832626 loss_att 162.564209 loss_ctc 132.141647 loss_rnnt 82.944725 hw_loss 0.063219 lr 0.00055708 rank 3
2023-02-18 03:27:13,554 DEBUG TRAIN Batch 1/7700 loss 33.791817 loss_att 41.976868 loss_ctc 40.747814 loss_rnnt 31.157938 hw_loss 0.130133 lr 0.00055852 rank 7
2023-02-18 03:27:13,557 DEBUG TRAIN Batch 1/7700 loss 45.534271 loss_att 87.065224 loss_ctc 65.697205 loss_rnnt 34.485958 hw_loss 0.100749 lr 0.00055848 rank 2
2023-02-18 03:27:13,593 DEBUG TRAIN Batch 1/7700 loss 54.545029 loss_att 113.837128 loss_ctc 56.059376 loss_rnnt 42.484642 hw_loss 0.000109 lr 0.00055800 rank 0
2023-02-18 03:27:13,604 DEBUG TRAIN Batch 1/7700 loss 85.168503 loss_att 119.335205 loss_ctc 110.236275 loss_rnnt 74.978539 hw_loss 0.026717 lr 0.00055736 rank 4
2023-02-18 03:27:13,617 DEBUG TRAIN Batch 1/7700 loss 62.688034 loss_att 117.596039 loss_ctc 89.988075 loss_rnnt 48.066376 hw_loss 0.000091 lr 0.00056004 rank 1
2023-02-18 03:28:20,270 DEBUG TRAIN Batch 1/7800 loss 44.814144 loss_att 67.793365 loss_ctc 57.640434 loss_rnnt 38.475258 hw_loss 0.061629 lr 0.00056040 rank 6
2023-02-18 03:28:20,271 DEBUG TRAIN Batch 1/7800 loss 32.082592 loss_att 61.725338 loss_ctc 39.767704 loss_rnnt 25.129318 hw_loss 0.000074 lr 0.00056032 rank 5
2023-02-18 03:28:20,273 DEBUG TRAIN Batch 1/7800 loss 62.123390 loss_att 111.854401 loss_ctc 78.453217 loss_rnnt 49.999817 hw_loss 0.000116 lr 0.00056052 rank 7
2023-02-18 03:28:20,275 DEBUG TRAIN Batch 1/7800 loss 33.734951 loss_att 47.209499 loss_ctc 39.605404 loss_rnnt 30.177742 hw_loss 0.149197 lr 0.00056000 rank 0
2023-02-18 03:28:20,278 DEBUG TRAIN Batch 1/7800 loss 51.228397 loss_att 89.537003 loss_ctc 59.351830 loss_rnnt 42.449829 hw_loss 0.063233 lr 0.00056204 rank 1
2023-02-18 03:28:20,281 DEBUG TRAIN Batch 1/7800 loss 50.676395 loss_att 92.462616 loss_ctc 57.710548 loss_rnnt 41.313377 hw_loss 0.127288 lr 0.00055908 rank 3
2023-02-18 03:28:20,281 DEBUG TRAIN Batch 1/7800 loss 31.389608 loss_att 39.837173 loss_ctc 36.109383 loss_rnnt 28.965546 hw_loss 0.197332 lr 0.00056048 rank 2
2023-02-18 03:28:20,287 DEBUG TRAIN Batch 1/7800 loss 43.573399 loss_att 80.338562 loss_ctc 60.412060 loss_rnnt 33.889011 hw_loss 0.161617 lr 0.00055936 rank 4
2023-02-18 03:29:20,095 DEBUG TRAIN Batch 1/7900 loss 40.188156 loss_att 83.777885 loss_ctc 53.140339 loss_rnnt 29.738628 hw_loss 0.008669 lr 0.00056108 rank 3
2023-02-18 03:29:20,095 DEBUG TRAIN Batch 1/7900 loss 56.948639 loss_att 101.384979 loss_ctc 66.483101 loss_rnnt 46.779491 hw_loss 0.019907 lr 0.00056248 rank 2
2023-02-18 03:29:20,097 DEBUG TRAIN Batch 1/7900 loss 45.459118 loss_att 82.659050 loss_ctc 59.165318 loss_rnnt 36.191589 hw_loss 0.000095 lr 0.00056232 rank 5
2023-02-18 03:29:20,097 DEBUG TRAIN Batch 1/7900 loss 69.928970 loss_att 108.690643 loss_ctc 74.339867 loss_rnnt 61.582138 hw_loss 0.011966 lr 0.00056136 rank 4
2023-02-18 03:29:20,099 DEBUG TRAIN Batch 1/7900 loss 62.359451 loss_att 103.192963 loss_ctc 85.585541 loss_rnnt 51.057205 hw_loss 0.072616 lr 0.00056404 rank 1
2023-02-18 03:29:20,134 DEBUG TRAIN Batch 1/7900 loss 48.769852 loss_att 90.439323 loss_ctc 62.707191 loss_rnnt 38.525848 hw_loss 0.097120 lr 0.00056240 rank 6
2023-02-18 03:29:20,146 DEBUG TRAIN Batch 1/7900 loss 36.200119 loss_att 60.355198 loss_ctc 54.835018 loss_rnnt 28.687914 hw_loss 0.368504 lr 0.00056252 rank 7
2023-02-18 03:29:20,160 DEBUG TRAIN Batch 1/7900 loss 72.349442 loss_att 110.114540 loss_ctc 76.968307 loss_rnnt 64.160759 hw_loss 0.037149 lr 0.00056200 rank 0
2023-02-18 03:30:19,940 DEBUG TRAIN Batch 1/8000 loss 35.732407 loss_att 48.608276 loss_ctc 43.547256 loss_rnnt 31.977352 hw_loss 0.258569 lr 0.00056432 rank 5
2023-02-18 03:30:19,942 DEBUG TRAIN Batch 1/8000 loss 39.521255 loss_att 63.061581 loss_ctc 50.284027 loss_rnnt 33.378021 hw_loss 0.000246 lr 0.00056308 rank 3
2023-02-18 03:30:19,942 DEBUG TRAIN Batch 1/8000 loss 36.399857 loss_att 55.643585 loss_ctc 50.791515 loss_rnnt 30.520658 hw_loss 0.209180 lr 0.00056452 rank 7
2023-02-18 03:30:19,946 DEBUG TRAIN Batch 1/8000 loss 54.187122 loss_att 106.167061 loss_ctc 97.295631 loss_rnnt 38.043274 hw_loss 0.000110 lr 0.00056440 rank 6
2023-02-18 03:30:19,947 DEBUG TRAIN Batch 1/8000 loss 17.448910 loss_att 20.135098 loss_ctc 19.063938 loss_rnnt 16.608038 hw_loss 0.165559 lr 0.00056604 rank 1
2023-02-18 03:30:19,948 DEBUG TRAIN Batch 1/8000 loss 21.928738 loss_att 31.827026 loss_ctc 25.807255 loss_rnnt 19.380907 hw_loss 0.095695 lr 0.00056336 rank 4
2023-02-18 03:30:19,949 DEBUG TRAIN Batch 1/8000 loss 74.692612 loss_att 131.517944 loss_ctc 86.882225 loss_rnnt 61.622459 hw_loss 0.149634 lr 0.00056400 rank 0
2023-02-18 03:30:20,011 DEBUG TRAIN Batch 1/8000 loss 75.205170 loss_att 125.377014 loss_ctc 105.660156 loss_rnnt 61.107094 hw_loss 0.005691 lr 0.00056448 rank 2
2023-02-18 03:31:18,898 DEBUG TRAIN Batch 1/8100 loss 67.844810 loss_att 110.511070 loss_ctc 75.197433 loss_rnnt 58.330692 hw_loss 0.000957 lr 0.00056536 rank 4
2023-02-18 03:31:18,902 DEBUG TRAIN Batch 1/8100 loss 39.380486 loss_att 77.351501 loss_ctc 55.059822 loss_rnnt 29.673351 hw_loss 0.041916 lr 0.00056632 rank 5
2023-02-18 03:31:18,903 DEBUG TRAIN Batch 1/8100 loss 37.658596 loss_att 69.709297 loss_ctc 50.792595 loss_rnnt 29.426600 hw_loss 0.132478 lr 0.00056508 rank 3
2023-02-18 03:31:18,907 DEBUG TRAIN Batch 1/8100 loss 39.248310 loss_att 60.144226 loss_ctc 48.221386 loss_rnnt 33.774609 hw_loss 0.183954 lr 0.00056648 rank 2
2023-02-18 03:31:18,907 DEBUG TRAIN Batch 1/8100 loss 19.730227 loss_att 30.923426 loss_ctc 28.174637 loss_rnnt 16.316626 hw_loss 0.091947 lr 0.00056600 rank 0
2023-02-18 03:31:18,933 DEBUG TRAIN Batch 1/8100 loss 46.218143 loss_att 70.968300 loss_ctc 59.571590 loss_rnnt 39.428085 hw_loss 0.111694 lr 0.00056804 rank 1
2023-02-18 03:31:18,941 DEBUG TRAIN Batch 1/8100 loss 71.631790 loss_att 125.253639 loss_ctc 91.969299 loss_rnnt 58.195702 hw_loss 0.000095 lr 0.00056652 rank 7
2023-02-18 03:31:18,960 DEBUG TRAIN Batch 1/8100 loss 64.785004 loss_att 106.043037 loss_ctc 89.844170 loss_rnnt 53.153305 hw_loss 0.072871 lr 0.00056640 rank 6
2023-02-18 03:32:19,638 DEBUG TRAIN Batch 1/8200 loss 74.763580 loss_att 125.705811 loss_ctc 88.573685 loss_rnnt 62.733639 hw_loss 0.000272 lr 0.00057004 rank 1
2023-02-18 03:32:19,639 DEBUG TRAIN Batch 1/8200 loss 23.148653 loss_att 56.197151 loss_ctc 36.456776 loss_rnnt 14.764451 hw_loss 0.000156 lr 0.00056832 rank 5
2023-02-18 03:32:19,640 DEBUG TRAIN Batch 1/8200 loss 43.674618 loss_att 89.190514 loss_ctc 53.916328 loss_rnnt 33.173717 hw_loss 0.060304 lr 0.00056800 rank 0
2023-02-18 03:32:19,645 DEBUG TRAIN Batch 1/8200 loss 58.163857 loss_att 118.238602 loss_ctc 91.243301 loss_rnnt 41.732834 hw_loss 0.010270 lr 0.00056852 rank 7
2023-02-18 03:32:19,647 DEBUG TRAIN Batch 1/8200 loss 126.088432 loss_att 176.543945 loss_ctc 156.127335 loss_rnnt 111.992058 hw_loss 0.000166 lr 0.00056736 rank 4
2023-02-18 03:32:19,646 DEBUG TRAIN Batch 1/8200 loss 47.028687 loss_att 81.497902 loss_ctc 65.563759 loss_rnnt 37.644318 hw_loss 0.035967 lr 0.00056840 rank 6
2023-02-18 03:32:19,652 DEBUG TRAIN Batch 1/8200 loss 23.596426 loss_att 54.303028 loss_ctc 39.125809 loss_rnnt 15.329530 hw_loss 0.103107 lr 0.00056708 rank 3
2023-02-18 03:32:19,699 DEBUG TRAIN Batch 1/8200 loss 43.020588 loss_att 76.781677 loss_ctc 49.937801 loss_rnnt 35.311745 hw_loss 0.064366 lr 0.00056848 rank 2
2023-02-18 03:33:19,264 DEBUG TRAIN Batch 1/8300 loss 19.998505 loss_att 34.607018 loss_ctc 24.159460 loss_rnnt 16.521942 hw_loss 0.000123 lr 0.00057204 rank 1
2023-02-18 03:33:19,268 DEBUG TRAIN Batch 1/8300 loss 47.299747 loss_att 84.432175 loss_ctc 58.460827 loss_rnnt 38.381805 hw_loss 0.006208 lr 0.00056908 rank 3
2023-02-18 03:33:19,269 DEBUG TRAIN Batch 1/8300 loss 51.690151 loss_att 90.675850 loss_ctc 68.123436 loss_rnnt 41.656544 hw_loss 0.085052 lr 0.00057032 rank 5
2023-02-18 03:33:19,269 DEBUG TRAIN Batch 1/8300 loss 71.728790 loss_att 111.944305 loss_ctc 92.738564 loss_rnnt 60.814751 hw_loss 0.130563 lr 0.00057000 rank 0
2023-02-18 03:33:19,272 DEBUG TRAIN Batch 1/8300 loss 23.518225 loss_att 60.461739 loss_ctc 30.873503 loss_rnnt 15.135061 hw_loss 0.025790 lr 0.00057040 rank 6
2023-02-18 03:33:19,276 DEBUG TRAIN Batch 1/8300 loss 37.710270 loss_att 47.135052 loss_ctc 53.974205 loss_rnnt 33.650208 hw_loss 0.012340 lr 0.00056936 rank 4
2023-02-18 03:33:19,277 DEBUG TRAIN Batch 1/8300 loss 42.476654 loss_att 60.606812 loss_ctc 55.375519 loss_rnnt 37.047089 hw_loss 0.156905 lr 0.00057052 rank 7
2023-02-18 03:33:19,286 DEBUG TRAIN Batch 1/8300 loss 48.277920 loss_att 86.816605 loss_ctc 62.543339 loss_rnnt 38.597973 hw_loss 0.131538 lr 0.00057048 rank 2
2023-02-18 03:34:17,942 DEBUG TRAIN Batch 1/8400 loss 73.250084 loss_att 107.935059 loss_ctc 85.546402 loss_rnnt 64.638809 hw_loss 0.065199 lr 0.00057404 rank 1
2023-02-18 03:34:17,944 DEBUG TRAIN Batch 1/8400 loss 58.494690 loss_att 87.607941 loss_ctc 73.468094 loss_rnnt 50.636452 hw_loss 0.073374 lr 0.00057252 rank 7
2023-02-18 03:34:17,945 DEBUG TRAIN Batch 1/8400 loss 77.256996 loss_att 97.127960 loss_ctc 98.572548 loss_rnnt 70.421242 hw_loss 0.036535 lr 0.00057232 rank 5
2023-02-18 03:34:17,945 DEBUG TRAIN Batch 1/8400 loss 93.786514 loss_att 167.286469 loss_ctc 114.655167 loss_rnnt 76.304016 hw_loss 0.000034 lr 0.00057136 rank 4
2023-02-18 03:34:17,946 DEBUG TRAIN Batch 1/8400 loss 53.223015 loss_att 91.118011 loss_ctc 70.489899 loss_rnnt 43.311928 hw_loss 0.055938 lr 0.00057108 rank 3
2023-02-18 03:34:17,947 DEBUG TRAIN Batch 1/8400 loss 39.288010 loss_att 54.580612 loss_ctc 52.315384 loss_rnnt 34.419430 hw_loss 0.137027 lr 0.00057200 rank 0
2023-02-18 03:34:17,947 DEBUG TRAIN Batch 1/8400 loss 31.603998 loss_att 53.468407 loss_ctc 47.839027 loss_rnnt 25.066425 hw_loss 0.000034 lr 0.00057240 rank 6
2023-02-18 03:34:17,950 DEBUG TRAIN Batch 1/8400 loss 40.053478 loss_att 59.324066 loss_ctc 53.045044 loss_rnnt 34.407623 hw_loss 0.111623 lr 0.00057248 rank 2
2023-02-18 03:35:18,820 DEBUG TRAIN Batch 1/8500 loss 67.820412 loss_att 104.417343 loss_ctc 87.164322 loss_rnnt 57.886482 hw_loss 0.066286 lr 0.00057400 rank 0
2023-02-18 03:35:18,820 DEBUG TRAIN Batch 1/8500 loss 37.220417 loss_att 69.179108 loss_ctc 50.262573 loss_rnnt 29.055075 hw_loss 0.064969 lr 0.00057440 rank 6
2023-02-18 03:35:18,824 DEBUG TRAIN Batch 1/8500 loss 64.223404 loss_att 100.170746 loss_ctc 95.781830 loss_rnnt 52.826031 hw_loss 0.000215 lr 0.00057448 rank 2
2023-02-18 03:35:18,826 DEBUG TRAIN Batch 1/8500 loss 64.839043 loss_att 86.022980 loss_ctc 84.644165 loss_rnnt 57.961536 hw_loss 0.000051 lr 0.00057432 rank 5
2023-02-18 03:35:18,826 DEBUG TRAIN Batch 1/8500 loss 48.001156 loss_att 79.034561 loss_ctc 60.011177 loss_rnnt 40.193077 hw_loss 0.000103 lr 0.00057308 rank 3
2023-02-18 03:35:18,829 DEBUG TRAIN Batch 1/8500 loss 95.251221 loss_att 151.370987 loss_ctc 113.214790 loss_rnnt 81.628525 hw_loss 0.006746 lr 0.00057604 rank 1
2023-02-18 03:35:18,834 DEBUG TRAIN Batch 1/8500 loss 71.617126 loss_att 118.093353 loss_ctc 89.521873 loss_rnnt 59.900230 hw_loss 0.064411 lr 0.00057336 rank 4
2023-02-18 03:35:18,880 DEBUG TRAIN Batch 1/8500 loss 37.122070 loss_att 76.243950 loss_ctc 67.798561 loss_rnnt 25.207466 hw_loss 0.000056 lr 0.00057452 rank 7
2023-02-18 03:36:37,532 DEBUG TRAIN Batch 1/8600 loss 43.688385 loss_att 73.385994 loss_ctc 50.647015 loss_rnnt 36.821022 hw_loss 0.000040 lr 0.00057632 rank 5
2023-02-18 03:36:37,532 DEBUG TRAIN Batch 1/8600 loss 36.804127 loss_att 72.820961 loss_ctc 62.169792 loss_rnnt 26.218649 hw_loss 0.000036 lr 0.00057640 rank 6
2023-02-18 03:36:37,533 DEBUG TRAIN Batch 1/8600 loss 106.160591 loss_att 150.333771 loss_ctc 118.508301 loss_rnnt 95.584244 hw_loss 0.178780 lr 0.00057648 rank 2
2023-02-18 03:36:37,535 DEBUG TRAIN Batch 1/8600 loss 33.173801 loss_att 69.766098 loss_ctc 51.027164 loss_rnnt 23.474854 hw_loss 0.000075 lr 0.00057508 rank 3
2023-02-18 03:36:37,540 DEBUG TRAIN Batch 1/8600 loss 49.486485 loss_att 70.386017 loss_ctc 69.744751 loss_rnnt 42.545502 hw_loss 0.112452 lr 0.00057536 rank 4
2023-02-18 03:36:37,552 DEBUG TRAIN Batch 1/8600 loss 34.317131 loss_att 59.125668 loss_ctc 43.170704 loss_rnnt 28.117992 hw_loss 0.106790 lr 0.00057652 rank 7
2023-02-18 03:36:37,559 DEBUG TRAIN Batch 1/8600 loss 59.990318 loss_att 89.584816 loss_ctc 65.542358 loss_rnnt 53.331116 hw_loss 0.000062 lr 0.00057600 rank 0
2023-02-18 03:36:37,595 DEBUG TRAIN Batch 1/8600 loss 61.653934 loss_att 87.284424 loss_ctc 78.823410 loss_rnnt 54.219353 hw_loss 0.036050 lr 0.00057804 rank 1
2023-02-18 03:37:56,532 DEBUG TRAIN Batch 1/8700 loss 42.976315 loss_att 67.511200 loss_ctc 52.740967 loss_rnnt 36.767288 hw_loss 0.000177 lr 0.00057852 rank 7
2023-02-18 03:37:56,535 DEBUG TRAIN Batch 1/8700 loss 35.363441 loss_att 64.687988 loss_ctc 48.696274 loss_rnnt 27.720745 hw_loss 0.000144 lr 0.00057832 rank 5
2023-02-18 03:37:56,536 DEBUG TRAIN Batch 1/8700 loss 42.458057 loss_att 75.428497 loss_ctc 63.060139 loss_rnnt 33.061802 hw_loss 0.103536 lr 0.00057736 rank 4
2023-02-18 03:37:56,540 DEBUG TRAIN Batch 1/8700 loss 49.690201 loss_att 75.073128 loss_ctc 62.189312 loss_rnnt 42.946861 hw_loss 0.000383 lr 0.00057800 rank 0
2023-02-18 03:37:56,540 DEBUG TRAIN Batch 1/8700 loss 72.136688 loss_att 103.311684 loss_ctc 84.536835 loss_rnnt 64.184189 hw_loss 0.120281 lr 0.00057840 rank 6
2023-02-18 03:37:56,542 DEBUG TRAIN Batch 1/8700 loss 44.875675 loss_att 84.931084 loss_ctc 58.473793 loss_rnnt 34.971786 hw_loss 0.149488 lr 0.00058004 rank 1
2023-02-18 03:37:56,543 DEBUG TRAIN Batch 1/8700 loss 35.534908 loss_att 69.496948 loss_ctc 49.057388 loss_rnnt 26.800819 hw_loss 0.260030 lr 0.00057708 rank 3
2023-02-18 03:37:56,604 DEBUG TRAIN Batch 1/8700 loss 41.555691 loss_att 62.433277 loss_ctc 58.572056 loss_rnnt 35.087036 hw_loss 0.045534 lr 0.00057848 rank 2
2023-02-18 03:38:57,485 DEBUG TRAIN Batch 1/8800 loss 38.387917 loss_att 67.218010 loss_ctc 50.950306 loss_rnnt 30.874235 hw_loss 0.136270 lr 0.00058032 rank 5
2023-02-18 03:38:57,486 DEBUG TRAIN Batch 1/8800 loss 17.858839 loss_att 51.245407 loss_ctc 26.872158 loss_rnnt 9.979650 hw_loss 0.000186 lr 0.00058204 rank 1
2023-02-18 03:38:57,490 DEBUG TRAIN Batch 1/8800 loss 40.544418 loss_att 78.425110 loss_ctc 47.077583 loss_rnnt 32.097176 hw_loss 0.000031 lr 0.00057936 rank 4
2023-02-18 03:38:57,492 DEBUG TRAIN Batch 1/8800 loss 42.776394 loss_att 80.206352 loss_ctc 53.320564 loss_rnnt 33.843571 hw_loss 0.076759 lr 0.00058040 rank 6
2023-02-18 03:38:57,492 DEBUG TRAIN Batch 1/8800 loss 37.676769 loss_att 76.587784 loss_ctc 47.618843 loss_rnnt 28.497461 hw_loss 0.134045 lr 0.00058000 rank 0
2023-02-18 03:38:57,493 DEBUG TRAIN Batch 1/8800 loss 57.648819 loss_att 84.466118 loss_ctc 72.199951 loss_rnnt 50.267075 hw_loss 0.146502 lr 0.00057908 rank 3
2023-02-18 03:38:57,532 DEBUG TRAIN Batch 1/8800 loss 58.291813 loss_att 110.956078 loss_ctc 62.636810 loss_rnnt 47.179588 hw_loss 0.000089 lr 0.00058052 rank 7
2023-02-18 03:38:57,533 DEBUG TRAIN Batch 1/8800 loss 61.514301 loss_att 91.690102 loss_ctc 80.169144 loss_rnnt 52.988522 hw_loss 0.006199 lr 0.00058048 rank 2
2023-02-18 03:39:56,276 DEBUG TRAIN Batch 1/8900 loss 74.082146 loss_att 103.826218 loss_ctc 99.375671 loss_rnnt 64.720139 hw_loss 0.076346 lr 0.00058232 rank 5
2023-02-18 03:39:56,277 DEBUG TRAIN Batch 1/8900 loss 37.538677 loss_att 59.886471 loss_ctc 48.446083 loss_rnnt 31.551439 hw_loss 0.118796 lr 0.00058404 rank 1
2023-02-18 03:39:56,278 DEBUG TRAIN Batch 1/8900 loss 94.714508 loss_att 143.868591 loss_ctc 107.890106 loss_rnnt 83.022964 hw_loss 0.194948 lr 0.00058240 rank 6
2023-02-18 03:39:56,280 DEBUG TRAIN Batch 1/8900 loss 56.197094 loss_att 84.050491 loss_ctc 76.627029 loss_rnnt 47.902290 hw_loss 0.000241 lr 0.00058136 rank 4
2023-02-18 03:39:56,280 DEBUG TRAIN Batch 1/8900 loss 67.232079 loss_att 99.221649 loss_ctc 77.739861 loss_rnnt 59.328346 hw_loss 0.196454 lr 0.00058252 rank 7
2023-02-18 03:39:56,284 DEBUG TRAIN Batch 1/8900 loss 27.400724 loss_att 35.870373 loss_ctc 33.134384 loss_rnnt 24.792480 hw_loss 0.280925 lr 0.00058108 rank 3
2023-02-18 03:39:56,290 DEBUG TRAIN Batch 1/8900 loss 23.589981 loss_att 62.029976 loss_ctc 23.186783 loss_rnnt 15.944098 hw_loss 0.021832 lr 0.00058248 rank 2
2023-02-18 03:39:56,296 DEBUG TRAIN Batch 1/8900 loss 36.729107 loss_att 65.212059 loss_ctc 51.597042 loss_rnnt 29.049839 hw_loss 0.000531 lr 0.00058200 rank 0
2023-02-18 03:40:57,401 DEBUG TRAIN Batch 1/9000 loss 45.381420 loss_att 81.741203 loss_ctc 67.720596 loss_rnnt 35.085072 hw_loss 0.085935 lr 0.00058308 rank 3
2023-02-18 03:40:57,406 DEBUG TRAIN Batch 1/9000 loss 77.470009 loss_att 105.411789 loss_ctc 100.853722 loss_rnnt 68.763809 hw_loss 0.000028 lr 0.00058336 rank 4
2023-02-18 03:40:57,408 DEBUG TRAIN Batch 1/9000 loss 36.796879 loss_att 47.476494 loss_ctc 49.597179 loss_rnnt 32.869606 hw_loss 0.158704 lr 0.00058400 rank 0
2023-02-18 03:40:57,408 DEBUG TRAIN Batch 1/9000 loss 36.554981 loss_att 75.866623 loss_ctc 50.148163 loss_rnnt 26.841900 hw_loss 0.071871 lr 0.00058440 rank 6
2023-02-18 03:40:57,409 DEBUG TRAIN Batch 1/9000 loss 54.108295 loss_att 82.076675 loss_ctc 70.939972 loss_rnnt 46.220387 hw_loss 0.093767 lr 0.00058604 rank 1
2023-02-18 03:40:57,412 DEBUG TRAIN Batch 1/9000 loss 44.350445 loss_att 68.682114 loss_ctc 54.886578 loss_rnnt 38.007248 hw_loss 0.135078 lr 0.00058452 rank 7
2023-02-18 03:40:57,413 DEBUG TRAIN Batch 1/9000 loss 35.745407 loss_att 67.795486 loss_ctc 46.638939 loss_rnnt 27.815033 hw_loss 0.127285 lr 0.00058432 rank 5
2023-02-18 03:40:57,465 DEBUG TRAIN Batch 1/9000 loss 54.504879 loss_att 84.563797 loss_ctc 64.928490 loss_rnnt 47.103260 hw_loss 0.000038 lr 0.00058448 rank 2
2023-02-18 03:41:58,472 DEBUG TRAIN Batch 1/9100 loss 50.129818 loss_att 77.161591 loss_ctc 75.971169 loss_rnnt 41.242859 hw_loss 0.065790 lr 0.00058632 rank 5
2023-02-18 03:41:58,475 DEBUG TRAIN Batch 1/9100 loss 52.946659 loss_att 100.984795 loss_ctc 78.532562 loss_rnnt 39.926552 hw_loss 0.001917 lr 0.00058600 rank 0
2023-02-18 03:41:58,477 DEBUG TRAIN Batch 1/9100 loss 72.616875 loss_att 111.467575 loss_ctc 105.364998 loss_rnnt 60.415852 hw_loss 0.120867 lr 0.00058652 rank 7
2023-02-18 03:41:58,478 DEBUG TRAIN Batch 1/9100 loss 50.907990 loss_att 71.742485 loss_ctc 67.511269 loss_rnnt 44.482498 hw_loss 0.084035 lr 0.00058640 rank 6
2023-02-18 03:41:58,478 DEBUG TRAIN Batch 1/9100 loss 30.982395 loss_att 64.062416 loss_ctc 36.404755 loss_rnnt 23.643391 hw_loss 0.000030 lr 0.00058536 rank 4
2023-02-18 03:41:58,483 DEBUG TRAIN Batch 1/9100 loss 63.595795 loss_att 115.631561 loss_ctc 82.851837 loss_rnnt 50.526867 hw_loss 0.176824 lr 0.00058804 rank 1
2023-02-18 03:41:58,484 DEBUG TRAIN Batch 1/9100 loss 57.233795 loss_att 85.741653 loss_ctc 68.333778 loss_rnnt 50.052197 hw_loss 0.000062 lr 0.00058508 rank 3
2023-02-18 03:41:58,485 DEBUG TRAIN Batch 1/9100 loss 58.866879 loss_att 94.133034 loss_ctc 66.896324 loss_rnnt 50.670540 hw_loss 0.135955 lr 0.00058648 rank 2
2023-02-18 03:42:55,552 DEBUG TRAIN Batch 1/9200 loss 38.304684 loss_att 60.347893 loss_ctc 48.626904 loss_rnnt 32.506065 hw_loss 0.025658 lr 0.00058840 rank 6
2023-02-18 03:42:55,553 DEBUG TRAIN Batch 1/9200 loss 31.147961 loss_att 56.359417 loss_ctc 37.732048 loss_rnnt 25.186977 hw_loss 0.076527 lr 0.00058832 rank 5
2023-02-18 03:42:55,554 DEBUG TRAIN Batch 1/9200 loss 48.895298 loss_att 74.661797 loss_ctc 70.276215 loss_rnnt 40.781158 hw_loss 0.206350 lr 0.00059004 rank 1
2023-02-18 03:42:55,557 DEBUG TRAIN Batch 1/9200 loss 45.432011 loss_att 79.033447 loss_ctc 54.025543 loss_rnnt 37.565891 hw_loss 0.000044 lr 0.00058736 rank 4
2023-02-18 03:42:55,561 DEBUG TRAIN Batch 1/9200 loss 39.137669 loss_att 59.130341 loss_ctc 46.316826 loss_rnnt 34.181885 hw_loss 0.000051 lr 0.00058852 rank 7
2023-02-18 03:42:55,563 DEBUG TRAIN Batch 1/9200 loss 39.326191 loss_att 58.619598 loss_ctc 55.809830 loss_rnnt 33.269489 hw_loss 0.000370 lr 0.00058708 rank 3
2023-02-18 03:42:55,568 DEBUG TRAIN Batch 1/9200 loss 48.070473 loss_att 71.380203 loss_ctc 64.632103 loss_rnnt 41.151722 hw_loss 0.091098 lr 0.00058848 rank 2
2023-02-18 03:42:55,569 DEBUG TRAIN Batch 1/9200 loss 66.693550 loss_att 112.478149 loss_ctc 67.394287 loss_rnnt 57.373844 hw_loss 0.130018 lr 0.00058800 rank 0
2023-02-18 03:43:56,900 DEBUG TRAIN Batch 1/9300 loss 52.651844 loss_att 74.557426 loss_ctc 61.363583 loss_rnnt 47.018211 hw_loss 0.170524 lr 0.00058936 rank 4
2023-02-18 03:43:56,903 DEBUG TRAIN Batch 1/9300 loss 100.795578 loss_att 142.481949 loss_ctc 112.504074 loss_rnnt 90.897102 hw_loss 0.000131 lr 0.00059204 rank 1
2023-02-18 03:43:56,905 DEBUG TRAIN Batch 1/9300 loss 149.971146 loss_att 192.140045 loss_ctc 180.427246 loss_rnnt 137.476501 hw_loss 0.000068 lr 0.00059032 rank 5
2023-02-18 03:43:56,905 DEBUG TRAIN Batch 1/9300 loss 36.105003 loss_att 75.856499 loss_ctc 61.146240 loss_rnnt 24.815817 hw_loss 0.000108 lr 0.00059052 rank 7
2023-02-18 03:43:56,909 DEBUG TRAIN Batch 1/9300 loss 35.917000 loss_att 71.167099 loss_ctc 47.711376 loss_rnnt 27.279224 hw_loss 0.028449 lr 0.00059000 rank 0
2023-02-18 03:43:56,909 DEBUG TRAIN Batch 1/9300 loss 73.661880 loss_att 117.979614 loss_ctc 87.746788 loss_rnnt 62.866161 hw_loss 0.101587 lr 0.00058908 rank 3
2023-02-18 03:43:56,910 DEBUG TRAIN Batch 1/9300 loss 40.353828 loss_att 73.582947 loss_ctc 60.534779 loss_rnnt 30.968201 hw_loss 0.091895 lr 0.00059040 rank 6
2023-02-18 03:43:56,972 DEBUG TRAIN Batch 1/9300 loss 42.269619 loss_att 62.700386 loss_ctc 53.081337 loss_rnnt 36.678734 hw_loss 0.118435 lr 0.00059048 rank 2
2023-02-18 03:44:59,432 DEBUG TRAIN Batch 1/9400 loss 23.602966 loss_att 45.164932 loss_ctc 25.818785 loss_rnnt 18.995108 hw_loss 0.000041 lr 0.00059252 rank 7
2023-02-18 03:44:59,439 DEBUG TRAIN Batch 1/9400 loss 95.284973 loss_att 164.299988 loss_ctc 130.155457 loss_rnnt 76.832550 hw_loss 0.000035 lr 0.00059136 rank 4
2023-02-18 03:44:59,438 DEBUG TRAIN Batch 1/9400 loss 74.378342 loss_att 116.754211 loss_ctc 86.167755 loss_rnnt 64.240097 hw_loss 0.170896 lr 0.00059404 rank 1
2023-02-18 03:44:59,441 DEBUG TRAIN Batch 1/9400 loss 36.504311 loss_att 67.477890 loss_ctc 46.167728 loss_rnnt 29.021116 hw_loss 0.000041 lr 0.00059248 rank 2
2023-02-18 03:44:59,442 DEBUG TRAIN Batch 1/9400 loss 57.307026 loss_att 106.568130 loss_ctc 76.852440 loss_rnnt 44.848724 hw_loss 0.000045 lr 0.00059232 rank 5
2023-02-18 03:44:59,442 DEBUG TRAIN Batch 1/9400 loss 93.019928 loss_att 128.126770 loss_ctc 99.248604 loss_rnnt 85.168015 hw_loss 0.000097 lr 0.00059108 rank 3
2023-02-18 03:44:59,448 DEBUG TRAIN Batch 1/9400 loss 63.256920 loss_att 114.354378 loss_ctc 86.483124 loss_rnnt 49.914368 hw_loss 0.049192 lr 0.00059200 rank 0
2023-02-18 03:44:59,501 DEBUG TRAIN Batch 1/9400 loss 38.641956 loss_att 61.537628 loss_ctc 49.887939 loss_rnnt 32.440353 hw_loss 0.230623 lr 0.00059240 rank 6
2023-02-18 03:46:18,531 DEBUG TRAIN Batch 1/9500 loss 57.297478 loss_att 89.323944 loss_ctc 80.670273 loss_rnnt 47.674561 hw_loss 0.189836 lr 0.00059452 rank 7
2023-02-18 03:46:18,536 DEBUG TRAIN Batch 1/9500 loss 33.896278 loss_att 56.360226 loss_ctc 43.040680 loss_rnnt 28.184135 hw_loss 0.000190 lr 0.00059432 rank 5
2023-02-18 03:46:18,536 DEBUG TRAIN Batch 1/9500 loss 104.029533 loss_att 147.457199 loss_ctc 128.851852 loss_rnnt 91.903030 hw_loss 0.246227 lr 0.00059448 rank 2
2023-02-18 03:46:18,536 DEBUG TRAIN Batch 1/9500 loss 67.894859 loss_att 100.975082 loss_ctc 85.393494 loss_rnnt 58.867802 hw_loss 0.145997 lr 0.00059400 rank 0
2023-02-18 03:46:18,541 DEBUG TRAIN Batch 1/9500 loss 50.301620 loss_att 67.914551 loss_ctc 64.309311 loss_rnnt 44.911247 hw_loss 0.000181 lr 0.00059308 rank 3
2023-02-18 03:46:18,541 DEBUG TRAIN Batch 1/9500 loss 119.411957 loss_att 169.343475 loss_ctc 129.963608 loss_rnnt 108.018661 hw_loss 0.000218 lr 0.00059440 rank 6
2023-02-18 03:46:18,542 DEBUG TRAIN Batch 1/9500 loss 80.352509 loss_att 97.438629 loss_ctc 102.591187 loss_rnnt 73.851707 hw_loss 0.222054 lr 0.00059604 rank 1
2023-02-18 03:46:18,544 DEBUG TRAIN Batch 1/9500 loss 53.521244 loss_att 78.239655 loss_ctc 70.594856 loss_rnnt 46.256290 hw_loss 0.083974 lr 0.00059336 rank 4
2023-02-18 03:47:40,644 DEBUG TRAIN Batch 1/9600 loss 57.287716 loss_att 94.612236 loss_ctc 62.251820 loss_rnnt 49.018711 hw_loss 0.266653 lr 0.00059632 rank 5
2023-02-18 03:47:40,645 DEBUG TRAIN Batch 1/9600 loss 46.732906 loss_att 73.454071 loss_ctc 61.062851 loss_rnnt 39.477875 hw_loss 0.000257 lr 0.00059536 rank 4
2023-02-18 03:47:40,648 DEBUG TRAIN Batch 1/9600 loss 34.127537 loss_att 56.750916 loss_ctc 51.226475 loss_rnnt 27.271111 hw_loss 0.097293 lr 0.00059600 rank 0
2023-02-18 03:47:40,649 DEBUG TRAIN Batch 1/9600 loss 60.500584 loss_att 83.002075 loss_ctc 87.346924 loss_rnnt 52.283325 hw_loss 0.257709 lr 0.00059640 rank 6
2023-02-18 03:47:40,649 DEBUG TRAIN Batch 1/9600 loss 53.832142 loss_att 83.909904 loss_ctc 70.805870 loss_rnnt 45.494499 hw_loss 0.110496 lr 0.00059652 rank 7
2023-02-18 03:47:40,653 DEBUG TRAIN Batch 1/9600 loss 50.154476 loss_att 74.760071 loss_ctc 66.469582 loss_rnnt 43.057934 hw_loss 0.000145 lr 0.00059508 rank 3
2023-02-18 03:47:40,658 DEBUG TRAIN Batch 1/9600 loss 48.700520 loss_att 72.354210 loss_ctc 55.785107 loss_rnnt 42.975647 hw_loss 0.092858 lr 0.00059804 rank 1
2023-02-18 03:47:40,674 DEBUG TRAIN Batch 1/9600 loss 59.441750 loss_att 83.650177 loss_ctc 74.987061 loss_rnnt 52.481373 hw_loss 0.086211 lr 0.00059648 rank 2
2023-02-18 03:48:40,464 DEBUG TRAIN Batch 1/9700 loss 26.220219 loss_att 62.849800 loss_ctc 34.755924 loss_rnnt 17.756144 hw_loss 0.000125 lr 0.00059736 rank 4
2023-02-18 03:48:40,464 DEBUG TRAIN Batch 1/9700 loss 123.983025 loss_att 128.276016 loss_ctc 144.737030 loss_rnnt 120.308472 hw_loss 0.091426 lr 0.00059832 rank 5
2023-02-18 03:48:40,468 DEBUG TRAIN Batch 1/9700 loss 24.751982 loss_att 37.298805 loss_ctc 35.438141 loss_rnnt 20.817690 hw_loss 0.000197 lr 0.00059840 rank 6
2023-02-18 03:48:40,470 DEBUG TRAIN Batch 1/9700 loss 38.466171 loss_att 62.768169 loss_ctc 63.123260 loss_rnnt 30.317991 hw_loss 0.000313 lr 0.00060004 rank 1
2023-02-18 03:48:40,471 DEBUG TRAIN Batch 1/9700 loss 27.263309 loss_att 58.020706 loss_ctc 46.673519 loss_rnnt 18.523699 hw_loss 0.000192 lr 0.00059848 rank 2
2023-02-18 03:48:40,474 DEBUG TRAIN Batch 1/9700 loss 70.858704 loss_att 97.170395 loss_ctc 98.828224 loss_rnnt 61.792583 hw_loss 0.139714 lr 0.00059800 rank 0
2023-02-18 03:48:40,473 DEBUG TRAIN Batch 1/9700 loss 60.273983 loss_att 85.095200 loss_ctc 81.667282 loss_rnnt 52.453678 hw_loss 0.006782 lr 0.00059708 rank 3
2023-02-18 03:48:40,530 DEBUG TRAIN Batch 1/9700 loss 20.672581 loss_att 29.237360 loss_ctc 26.715714 loss_rnnt 18.153763 hw_loss 0.000205 lr 0.00059852 rank 7
2023-02-18 03:49:39,049 DEBUG TRAIN Batch 1/9800 loss 58.590664 loss_att 94.288170 loss_ctc 79.985092 loss_rnnt 48.553123 hw_loss 0.085217 lr 0.00060204 rank 1
2023-02-18 03:49:39,051 DEBUG TRAIN Batch 1/9800 loss 50.753216 loss_att 78.848289 loss_ctc 62.818909 loss_rnnt 43.494122 hw_loss 0.058728 lr 0.00060032 rank 5
2023-02-18 03:49:39,052 DEBUG TRAIN Batch 1/9800 loss 34.817863 loss_att 62.773743 loss_ctc 45.271893 loss_rnnt 27.832748 hw_loss 0.000138 lr 0.00060052 rank 7
2023-02-18 03:49:39,055 DEBUG TRAIN Batch 1/9800 loss 31.074169 loss_att 72.886528 loss_ctc 39.356106 loss_rnnt 21.607374 hw_loss 0.000125 lr 0.00060040 rank 6
2023-02-18 03:49:39,056 DEBUG TRAIN Batch 1/9800 loss 32.242752 loss_att 63.369720 loss_ctc 45.692146 loss_rnnt 24.195765 hw_loss 0.053141 lr 0.00059908 rank 3
2023-02-18 03:49:39,056 DEBUG TRAIN Batch 1/9800 loss 43.273239 loss_att 74.192978 loss_ctc 63.857666 loss_rnnt 34.344635 hw_loss 0.000125 lr 0.00060048 rank 2
2023-02-18 03:49:39,059 DEBUG TRAIN Batch 1/9800 loss 21.655588 loss_att 42.514603 loss_ctc 27.018353 loss_rnnt 16.768690 hw_loss 0.000112 lr 0.00059936 rank 4
2023-02-18 03:49:39,060 DEBUG TRAIN Batch 1/9800 loss 59.485901 loss_att 111.482330 loss_ctc 96.910431 loss_rnnt 44.096550 hw_loss 0.000242 lr 0.00060000 rank 0
2023-02-18 03:50:40,324 DEBUG TRAIN Batch 1/9900 loss 35.143738 loss_att 55.883099 loss_ctc 47.620762 loss_rnnt 29.332174 hw_loss 0.000164 lr 0.00060232 rank 5
2023-02-18 03:50:40,325 DEBUG TRAIN Batch 1/9900 loss 40.795021 loss_att 72.290436 loss_ctc 47.586716 loss_rnnt 33.527996 hw_loss 0.116968 lr 0.00060240 rank 6
2023-02-18 03:50:40,325 DEBUG TRAIN Batch 1/9900 loss 57.973003 loss_att 91.601608 loss_ctc 91.404922 loss_rnnt 46.786972 hw_loss 0.005099 lr 0.00060136 rank 4
2023-02-18 03:50:40,327 DEBUG TRAIN Batch 1/9900 loss 63.332691 loss_att 101.647255 loss_ctc 82.868408 loss_rnnt 52.977676 hw_loss 0.163759 lr 0.00060404 rank 1
2023-02-18 03:50:40,331 DEBUG TRAIN Batch 1/9900 loss 73.510757 loss_att 104.013535 loss_ctc 103.474442 loss_rnnt 63.414898 hw_loss 0.000284 lr 0.00060200 rank 0
2023-02-18 03:50:40,332 DEBUG TRAIN Batch 1/9900 loss 24.329750 loss_att 41.827007 loss_ctc 37.185612 loss_rnnt 19.006014 hw_loss 0.206566 lr 0.00060252 rank 7
2023-02-18 03:50:40,333 DEBUG TRAIN Batch 1/9900 loss 47.639709 loss_att 72.764435 loss_ctc 60.512531 loss_rnnt 40.898327 hw_loss 0.000115 lr 0.00060108 rank 3
2023-02-18 03:50:40,390 DEBUG TRAIN Batch 1/9900 loss 33.344009 loss_att 57.150101 loss_ctc 40.490421 loss_rnnt 27.554268 hw_loss 0.141882 lr 0.00060248 rank 2
2023-02-18 03:51:41,851 DEBUG TRAIN Batch 1/10000 loss 53.352806 loss_att 86.709824 loss_ctc 61.738316 loss_rnnt 45.563309 hw_loss 0.000047 lr 0.00060336 rank 4
2023-02-18 03:51:41,852 DEBUG TRAIN Batch 1/10000 loss 56.660801 loss_att 105.552124 loss_ctc 65.039764 loss_rnnt 45.765297 hw_loss 0.000082 lr 0.00060308 rank 3
2023-02-18 03:51:41,852 DEBUG TRAIN Batch 1/10000 loss 42.974625 loss_att 56.258713 loss_ctc 54.305462 loss_rnnt 38.716682 hw_loss 0.169391 lr 0.00060452 rank 7
2023-02-18 03:51:41,852 DEBUG TRAIN Batch 1/10000 loss 37.802296 loss_att 86.440804 loss_ctc 58.789421 loss_rnnt 25.276283 hw_loss 0.000052 lr 0.00060432 rank 5
2023-02-18 03:51:41,854 DEBUG TRAIN Batch 1/10000 loss 69.095970 loss_att 96.760269 loss_ctc 89.195908 loss_rnnt 60.883072 hw_loss 0.000088 lr 0.00060604 rank 1
2023-02-18 03:51:41,855 DEBUG TRAIN Batch 1/10000 loss 40.014828 loss_att 57.195343 loss_ctc 53.910473 loss_rnnt 34.667633 hw_loss 0.109379 lr 0.00060400 rank 0
2023-02-18 03:51:41,858 DEBUG TRAIN Batch 1/10000 loss 25.415230 loss_att 58.797966 loss_ctc 35.606930 loss_rnnt 17.239700 hw_loss 0.262660 lr 0.00060448 rank 2
2023-02-18 03:51:41,862 DEBUG TRAIN Batch 1/10000 loss 44.952015 loss_att 83.948059 loss_ctc 56.060368 loss_rnnt 35.671658 hw_loss 0.000065 lr 0.00060440 rank 6
2023-02-18 03:52:41,368 DEBUG TRAIN Batch 1/10100 loss 49.128307 loss_att 82.335251 loss_ctc 55.325771 loss_rnnt 41.595013 hw_loss 0.122959 lr 0.00060632 rank 5
2023-02-18 03:52:41,371 DEBUG TRAIN Batch 1/10100 loss 40.491001 loss_att 54.918697 loss_ctc 60.511654 loss_rnnt 34.909397 hw_loss 0.049953 lr 0.00060648 rank 2
2023-02-18 03:52:41,371 DEBUG TRAIN Batch 1/10100 loss 60.647766 loss_att 93.963516 loss_ctc 76.505920 loss_rnnt 51.785046 hw_loss 0.159651 lr 0.00060536 rank 4
2023-02-18 03:52:41,375 DEBUG TRAIN Batch 1/10100 loss 77.932213 loss_att 105.108772 loss_ctc 102.309937 loss_rnnt 69.218124 hw_loss 0.053279 lr 0.00060508 rank 3
2023-02-18 03:52:41,376 DEBUG TRAIN Batch 1/10100 loss 44.446751 loss_att 76.633270 loss_ctc 57.720406 loss_rnnt 36.239235 hw_loss 0.000734 lr 0.00060652 rank 7
2023-02-18 03:52:41,393 DEBUG TRAIN Batch 1/10100 loss 39.298496 loss_att 64.152184 loss_ctc 48.012512 loss_rnnt 33.165443 hw_loss 0.000832 lr 0.00060804 rank 1
2023-02-18 03:52:41,405 DEBUG TRAIN Batch 1/10100 loss 41.650673 loss_att 97.380989 loss_ctc 56.081692 loss_rnnt 28.580042 hw_loss 0.000802 lr 0.00060640 rank 6
2023-02-18 03:52:41,432 DEBUG TRAIN Batch 1/10100 loss 38.240822 loss_att 75.599335 loss_ctc 57.303246 loss_rnnt 28.227079 hw_loss 0.000716 lr 0.00060600 rank 0
2023-02-18 03:53:42,763 DEBUG TRAIN Batch 1/10200 loss 25.977526 loss_att 49.759033 loss_ctc 41.356228 loss_rnnt 19.170668 hw_loss 0.000118 lr 0.00060832 rank 5
2023-02-18 03:53:42,768 DEBUG TRAIN Batch 1/10200 loss 46.396755 loss_att 68.948486 loss_ctc 55.712364 loss_rnnt 40.639610 hw_loss 0.008838 lr 0.00061004 rank 1
2023-02-18 03:53:42,769 DEBUG TRAIN Batch 1/10200 loss 84.074272 loss_att 147.748672 loss_ctc 109.680031 loss_rnnt 67.925217 hw_loss 0.000132 lr 0.00060852 rank 7
2023-02-18 03:53:42,771 DEBUG TRAIN Batch 1/10200 loss 58.554504 loss_att 70.542847 loss_ctc 88.481018 loss_rnnt 52.061802 hw_loss 0.196553 lr 0.00060800 rank 0
2023-02-18 03:53:42,772 DEBUG TRAIN Batch 1/10200 loss 23.566446 loss_att 36.966156 loss_ctc 33.823246 loss_rnnt 19.518883 hw_loss 0.000090 lr 0.00060708 rank 3
2023-02-18 03:53:42,775 DEBUG TRAIN Batch 1/10200 loss 91.621193 loss_att 129.864944 loss_ctc 130.216385 loss_rnnt 78.800201 hw_loss 0.049144 lr 0.00060840 rank 6
2023-02-18 03:53:42,778 DEBUG TRAIN Batch 1/10200 loss 109.128639 loss_att 157.298218 loss_ctc 128.763916 loss_rnnt 96.876617 hw_loss 0.000141 lr 0.00060736 rank 4
2023-02-18 03:53:42,784 DEBUG TRAIN Batch 1/10200 loss 47.842205 loss_att 79.166695 loss_ctc 58.456734 loss_rnnt 40.161972 hw_loss 0.000127 lr 0.00060848 rank 2
2023-02-18 03:55:08,939 DEBUG TRAIN Batch 1/10300 loss 27.878101 loss_att 49.088600 loss_ctc 44.320946 loss_rnnt 21.347294 hw_loss 0.180612 lr 0.00061052 rank 7
2023-02-18 03:55:08,940 DEBUG TRAIN Batch 1/10300 loss 23.471977 loss_att 53.201157 loss_ctc 32.545364 loss_rnnt 16.316158 hw_loss 0.000373 lr 0.00061048 rank 2
2023-02-18 03:55:08,941 DEBUG TRAIN Batch 1/10300 loss 63.811325 loss_att 72.774033 loss_ctc 81.278015 loss_rnnt 59.648632 hw_loss 0.077359 lr 0.00061204 rank 1
2023-02-18 03:55:08,943 DEBUG TRAIN Batch 1/10300 loss 76.726349 loss_att 110.661133 loss_ctc 94.851494 loss_rnnt 67.522316 hw_loss 0.000719 lr 0.00061000 rank 0
2023-02-18 03:55:08,943 DEBUG TRAIN Batch 1/10300 loss 35.212009 loss_att 66.318352 loss_ctc 36.543621 loss_rnnt 28.813000 hw_loss 0.000363 lr 0.00061040 rank 6
2023-02-18 03:55:08,944 DEBUG TRAIN Batch 1/10300 loss 42.534515 loss_att 70.097816 loss_ctc 67.954086 loss_rnnt 33.628616 hw_loss 0.007419 lr 0.00061032 rank 5
2023-02-18 03:55:08,947 DEBUG TRAIN Batch 1/10300 loss 59.436062 loss_att 115.354797 loss_ctc 75.501495 loss_rnnt 46.110146 hw_loss 0.000196 lr 0.00060908 rank 3
2023-02-18 03:55:08,948 DEBUG TRAIN Batch 1/10300 loss 74.753487 loss_att 117.453445 loss_ctc 107.825127 loss_rnnt 61.803738 hw_loss 0.000376 lr 0.00060936 rank 4
2023-02-18 03:56:31,517 DEBUG TRAIN Batch 1/10400 loss 58.778915 loss_att 94.643089 loss_ctc 91.565460 loss_rnnt 47.208725 hw_loss 0.048400 lr 0.00061404 rank 1
2023-02-18 03:56:31,518 DEBUG TRAIN Batch 1/10400 loss 55.948936 loss_att 90.568123 loss_ctc 81.515503 loss_rnnt 45.599243 hw_loss 0.031838 lr 0.00061252 rank 7
2023-02-18 03:56:31,518 DEBUG TRAIN Batch 1/10400 loss 48.169243 loss_att 75.689117 loss_ctc 60.291847 loss_rnnt 41.018700 hw_loss 0.056664 lr 0.00061136 rank 4
2023-02-18 03:56:31,521 DEBUG TRAIN Batch 1/10400 loss 67.605087 loss_att 101.323204 loss_ctc 82.202507 loss_rnnt 58.910431 hw_loss 0.008825 lr 0.00061232 rank 5
2023-02-18 03:56:31,521 DEBUG TRAIN Batch 1/10400 loss 32.810814 loss_att 46.224865 loss_ctc 47.739338 loss_rnnt 28.095295 hw_loss 0.079197 lr 0.00061240 rank 6
2023-02-18 03:56:31,522 DEBUG TRAIN Batch 1/10400 loss 56.499161 loss_att 104.574989 loss_ctc 82.491722 loss_rnnt 43.335129 hw_loss 0.155980 lr 0.00061200 rank 0
2023-02-18 03:56:31,529 DEBUG TRAIN Batch 1/10400 loss 68.669533 loss_att 90.457161 loss_ctc 83.626930 loss_rnnt 62.281292 hw_loss 0.068244 lr 0.00061108 rank 3
2023-02-18 03:56:31,579 DEBUG TRAIN Batch 1/10400 loss 45.702553 loss_att 69.525620 loss_ctc 65.981308 loss_rnnt 38.234077 hw_loss 0.000051 lr 0.00061248 rank 2
2023-02-18 03:57:33,900 DEBUG TRAIN Batch 1/10500 loss 58.570152 loss_att 101.841339 loss_ctc 83.571144 loss_rnnt 46.582405 hw_loss 0.000075 lr 0.00061452 rank 7
2023-02-18 03:57:33,900 DEBUG TRAIN Batch 1/10500 loss 58.227173 loss_att 81.857162 loss_ctc 71.627022 loss_rnnt 51.714478 hw_loss 0.000096 lr 0.00061308 rank 3
2023-02-18 03:57:33,901 DEBUG TRAIN Batch 1/10500 loss 44.296745 loss_att 86.504921 loss_ctc 61.767857 loss_rnnt 33.525597 hw_loss 0.000055 lr 0.00061432 rank 5
2023-02-18 03:57:33,904 DEBUG TRAIN Batch 1/10500 loss 57.484024 loss_att 87.095505 loss_ctc 73.346489 loss_rnnt 49.370262 hw_loss 0.143381 lr 0.00061448 rank 2
2023-02-18 03:57:33,905 DEBUG TRAIN Batch 1/10500 loss 68.473167 loss_att 89.998764 loss_ctc 81.626373 loss_rnnt 62.302288 hw_loss 0.209998 lr 0.00061400 rank 0
2023-02-18 03:57:33,909 DEBUG TRAIN Batch 1/10500 loss 42.538502 loss_att 74.893890 loss_ctc 60.258675 loss_rnnt 33.704693 hw_loss 0.000078 lr 0.00061440 rank 6
2023-02-18 03:57:33,910 DEBUG TRAIN Batch 1/10500 loss 52.374146 loss_att 93.327431 loss_ctc 70.766090 loss_rnnt 41.731201 hw_loss 0.000044 lr 0.00061336 rank 4
2023-02-18 03:57:33,961 DEBUG TRAIN Batch 1/10500 loss 38.622425 loss_att 55.208481 loss_ctc 53.345699 loss_rnnt 33.296593 hw_loss 0.085343 lr 0.00061604 rank 1
2023-02-18 03:58:33,317 DEBUG TRAIN Batch 1/10600 loss 17.107143 loss_att 23.696711 loss_ctc 20.499611 loss_rnnt 15.303706 hw_loss 0.062241 lr 0.00061804 rank 1
2023-02-18 03:58:33,319 DEBUG TRAIN Batch 1/10600 loss 22.979208 loss_att 26.651037 loss_ctc 24.458017 loss_rnnt 21.898994 hw_loss 0.278759 lr 0.00061632 rank 5
2023-02-18 03:58:33,322 DEBUG TRAIN Batch 1/10600 loss 60.986900 loss_att 82.631248 loss_ctc 79.681923 loss_rnnt 54.088066 hw_loss 0.144935 lr 0.00061600 rank 0
2023-02-18 03:58:33,324 DEBUG TRAIN Batch 1/10600 loss 20.148611 loss_att 34.463242 loss_ctc 24.654709 loss_rnnt 16.588257 hw_loss 0.181150 lr 0.00061536 rank 4
2023-02-18 03:58:33,327 DEBUG TRAIN Batch 1/10600 loss 41.896130 loss_att 80.294609 loss_ctc 74.423874 loss_rnnt 29.874081 hw_loss 0.009976 lr 0.00061648 rank 2
2023-02-18 03:58:33,328 DEBUG TRAIN Batch 1/10600 loss 44.057022 loss_att 72.701279 loss_ctc 67.741539 loss_rnnt 35.083855 hw_loss 0.161960 lr 0.00061640 rank 6
2023-02-18 03:58:33,330 DEBUG TRAIN Batch 1/10600 loss 78.941254 loss_att 105.565529 loss_ctc 116.600189 loss_rnnt 68.595154 hw_loss 0.000089 lr 0.00061508 rank 3
2023-02-18 03:58:33,337 DEBUG TRAIN Batch 1/10600 loss 32.328171 loss_att 52.661102 loss_ctc 45.870888 loss_rnnt 26.455853 hw_loss 0.000070 lr 0.00061652 rank 7
2023-02-18 03:59:32,100 DEBUG TRAIN Batch 1/10700 loss 117.240852 loss_att 178.883331 loss_ctc 148.545105 loss_rnnt 100.637268 hw_loss 0.189731 lr 0.00061832 rank 5
2023-02-18 03:59:32,101 DEBUG TRAIN Batch 1/10700 loss 33.167660 loss_att 51.991764 loss_ctc 42.279800 loss_rnnt 28.187836 hw_loss 0.000095 lr 0.00061736 rank 4
2023-02-18 03:59:32,101 DEBUG TRAIN Batch 1/10700 loss 44.774406 loss_att 68.066353 loss_ctc 64.621544 loss_rnnt 37.469658 hw_loss 0.000131 lr 0.00061852 rank 7
2023-02-18 03:59:32,102 DEBUG TRAIN Batch 1/10700 loss 24.429165 loss_att 31.366596 loss_ctc 37.237514 loss_rnnt 21.291267 hw_loss 0.079937 lr 0.00061800 rank 0
2023-02-18 03:59:32,102 DEBUG TRAIN Batch 1/10700 loss 37.448845 loss_att 66.536606 loss_ctc 46.890083 loss_rnnt 30.279758 hw_loss 0.173818 lr 0.00062004 rank 1
2023-02-18 03:59:32,106 DEBUG TRAIN Batch 1/10700 loss 47.543022 loss_att 79.938797 loss_ctc 56.953957 loss_rnnt 39.756359 hw_loss 0.098836 lr 0.00061848 rank 2
2023-02-18 03:59:32,108 DEBUG TRAIN Batch 1/10700 loss 53.787434 loss_att 89.299370 loss_ctc 66.644073 loss_rnnt 44.933090 hw_loss 0.070760 lr 0.00061708 rank 3
2023-02-18 03:59:32,110 DEBUG TRAIN Batch 1/10700 loss 32.008759 loss_att 44.877220 loss_ctc 41.835236 loss_rnnt 28.124802 hw_loss 0.000125 lr 0.00061840 rank 6
2023-02-18 04:00:32,388 DEBUG TRAIN Batch 1/10800 loss 85.063362 loss_att 138.726349 loss_ctc 105.049004 loss_rnnt 71.665993 hw_loss 0.000038 lr 0.00061936 rank 4
2023-02-18 04:00:32,388 DEBUG TRAIN Batch 1/10800 loss 37.636509 loss_att 70.263870 loss_ctc 50.592373 loss_rnnt 29.377028 hw_loss 0.012305 lr 0.00062204 rank 1
2023-02-18 04:00:32,389 DEBUG TRAIN Batch 1/10800 loss 41.343616 loss_att 67.537773 loss_ctc 62.719967 loss_rnnt 33.254578 hw_loss 0.000045 lr 0.00062052 rank 7
2023-02-18 04:00:32,389 DEBUG TRAIN Batch 1/10800 loss 47.107216 loss_att 83.989304 loss_ctc 61.688034 loss_rnnt 37.786667 hw_loss 0.000036 lr 0.00062032 rank 5
2023-02-18 04:00:32,390 DEBUG TRAIN Batch 1/10800 loss 33.422501 loss_att 65.160194 loss_ctc 49.217342 loss_rnnt 24.968678 hw_loss 0.000573 lr 0.00062048 rank 2
2023-02-18 04:00:32,391 DEBUG TRAIN Batch 1/10800 loss 35.364014 loss_att 67.975021 loss_ctc 52.134937 loss_rnnt 26.574196 hw_loss 0.059049 lr 0.00062040 rank 6
2023-02-18 04:00:32,391 DEBUG TRAIN Batch 1/10800 loss 34.410702 loss_att 61.660137 loss_ctc 46.332443 loss_rnnt 27.245680 hw_loss 0.235447 lr 0.00062000 rank 0
2023-02-18 04:00:32,407 DEBUG TRAIN Batch 1/10800 loss 41.162968 loss_att 75.545769 loss_ctc 59.531101 loss_rnnt 31.837297 hw_loss 0.000045 lr 0.00061908 rank 3
2023-02-18 04:01:31,500 DEBUG TRAIN Batch 1/10900 loss 31.296070 loss_att 43.259945 loss_ctc 44.881828 loss_rnnt 27.004511 hw_loss 0.163778 lr 0.00062232 rank 5
2023-02-18 04:01:31,500 DEBUG TRAIN Batch 1/10900 loss 42.422142 loss_att 59.345432 loss_ctc 56.575592 loss_rnnt 37.069160 hw_loss 0.152247 lr 0.00062108 rank 3
2023-02-18 04:01:31,502 DEBUG TRAIN Batch 1/10900 loss 38.512859 loss_att 53.131458 loss_ctc 46.776947 loss_rnnt 34.453850 hw_loss 0.062648 lr 0.00062136 rank 4
2023-02-18 04:01:31,503 DEBUG TRAIN Batch 1/10900 loss 68.729439 loss_att 99.292519 loss_ctc 87.732040 loss_rnnt 60.024208 hw_loss 0.110484 lr 0.00062252 rank 7
2023-02-18 04:01:31,503 DEBUG TRAIN Batch 1/10900 loss 59.376308 loss_att 90.799072 loss_ctc 80.672943 loss_rnnt 50.252167 hw_loss 0.000069 lr 0.00062404 rank 1
2023-02-18 04:01:31,504 DEBUG TRAIN Batch 1/10900 loss 59.499832 loss_att 82.550117 loss_ctc 68.558914 loss_rnnt 53.651535 hw_loss 0.056925 lr 0.00062200 rank 0
2023-02-18 04:01:31,505 DEBUG TRAIN Batch 1/10900 loss 21.817600 loss_att 51.946098 loss_ctc 29.849623 loss_rnnt 14.719660 hw_loss 0.002442 lr 0.00062240 rank 6
2023-02-18 04:01:31,507 DEBUG TRAIN Batch 1/10900 loss 32.412674 loss_att 52.477036 loss_ctc 47.579746 loss_rnnt 26.336012 hw_loss 0.077836 lr 0.00062248 rank 2
2023-02-18 04:02:31,023 DEBUG TRAIN Batch 1/11000 loss 39.960041 loss_att 67.970406 loss_ctc 52.935974 loss_rnnt 32.627785 hw_loss 0.000104 lr 0.00062432 rank 5
2023-02-18 04:02:31,023 DEBUG TRAIN Batch 1/11000 loss 56.494709 loss_att 79.946457 loss_ctc 82.109138 loss_rnnt 48.388832 hw_loss 0.000496 lr 0.00062336 rank 4
2023-02-18 04:02:31,028 DEBUG TRAIN Batch 1/11000 loss 25.042208 loss_att 38.830963 loss_ctc 38.918060 loss_rnnt 20.434290 hw_loss 0.000099 lr 0.00062308 rank 3
2023-02-18 04:02:31,030 DEBUG TRAIN Batch 1/11000 loss 44.996853 loss_att 77.429527 loss_ctc 56.480328 loss_rnnt 36.946171 hw_loss 0.061905 lr 0.00062452 rank 7
2023-02-18 04:02:31,030 DEBUG TRAIN Batch 1/11000 loss 77.828064 loss_att 99.526245 loss_ctc 98.032623 loss_rnnt 70.794403 hw_loss 0.000156 lr 0.00062448 rank 2
2023-02-18 04:02:31,031 DEBUG TRAIN Batch 1/11000 loss 41.396439 loss_att 65.910347 loss_ctc 62.963211 loss_rnnt 33.618000 hw_loss 0.000149 lr 0.00062440 rank 6
2023-02-18 04:02:31,032 DEBUG TRAIN Batch 1/11000 loss 47.959263 loss_att 73.337120 loss_ctc 59.871193 loss_rnnt 41.295349 hw_loss 0.000166 lr 0.00062604 rank 1
2023-02-18 04:02:31,088 DEBUG TRAIN Batch 1/11000 loss 38.740337 loss_att 56.726013 loss_ctc 52.334869 loss_rnnt 33.256798 hw_loss 0.138368 lr 0.00062400 rank 0
2023-02-18 04:03:31,531 DEBUG TRAIN Batch 1/11100 loss 28.201786 loss_att 64.494308 loss_ctc 40.261642 loss_rnnt 19.335279 hw_loss 0.000039 lr 0.00062536 rank 4
2023-02-18 04:03:31,538 DEBUG TRAIN Batch 1/11100 loss 69.992416 loss_att 89.934998 loss_ctc 90.233063 loss_rnnt 63.286243 hw_loss 0.035440 lr 0.00062600 rank 0
2023-02-18 04:03:31,538 DEBUG TRAIN Batch 1/11100 loss 47.670883 loss_att 81.592834 loss_ctc 63.469296 loss_rnnt 38.716785 hw_loss 0.118589 lr 0.00062648 rank 2
2023-02-18 04:03:31,539 DEBUG TRAIN Batch 1/11100 loss 33.055981 loss_att 62.499649 loss_ctc 49.840893 loss_rnnt 24.928244 hw_loss 0.001904 lr 0.00062632 rank 5
2023-02-18 04:03:31,541 DEBUG TRAIN Batch 1/11100 loss 39.245296 loss_att 72.710709 loss_ctc 57.152367 loss_rnnt 30.103880 hw_loss 0.113859 lr 0.00062640 rank 6
2023-02-18 04:03:31,541 DEBUG TRAIN Batch 1/11100 loss 39.948883 loss_att 71.502594 loss_ctc 49.852112 loss_rnnt 32.258904 hw_loss 0.110262 lr 0.00062508 rank 3
2023-02-18 04:03:31,564 DEBUG TRAIN Batch 1/11100 loss 67.447479 loss_att 106.145065 loss_ctc 89.718346 loss_rnnt 56.738480 hw_loss 0.000058 lr 0.00062652 rank 7
2023-02-18 04:03:31,611 DEBUG TRAIN Batch 1/11100 loss 68.820900 loss_att 94.516418 loss_ctc 85.001488 loss_rnnt 61.524330 hw_loss 0.000099 lr 0.00062804 rank 1
2023-02-18 04:04:53,422 DEBUG TRAIN Batch 1/11200 loss 127.892891 loss_att 169.536682 loss_ctc 172.550842 loss_rnnt 113.609612 hw_loss 0.000221 lr 0.00062800 rank 0
2023-02-18 04:04:53,424 DEBUG TRAIN Batch 1/11200 loss 40.675964 loss_att 59.465427 loss_ctc 69.129105 loss_rnnt 33.004974 hw_loss 0.223768 lr 0.00063004 rank 1
2023-02-18 04:04:53,426 DEBUG TRAIN Batch 1/11200 loss 29.597410 loss_att 40.311119 loss_ctc 37.188011 loss_rnnt 26.367046 hw_loss 0.141645 lr 0.00062852 rank 7
2023-02-18 04:04:53,428 DEBUG TRAIN Batch 1/11200 loss 52.817020 loss_att 96.517303 loss_ctc 86.865875 loss_rnnt 39.468971 hw_loss 0.127773 lr 0.00062840 rank 6
2023-02-18 04:04:53,430 DEBUG TRAIN Batch 1/11200 loss 28.885012 loss_att 40.520428 loss_ctc 41.095200 loss_rnnt 24.873259 hw_loss 0.106211 lr 0.00062832 rank 5
2023-02-18 04:04:53,433 DEBUG TRAIN Batch 1/11200 loss 82.739738 loss_att 108.559494 loss_ctc 102.429047 loss_rnnt 74.879868 hw_loss 0.132545 lr 0.00062736 rank 4
2023-02-18 04:04:53,440 DEBUG TRAIN Batch 1/11200 loss 38.875797 loss_att 48.975117 loss_ctc 47.720093 loss_rnnt 35.676624 hw_loss 0.000123 lr 0.00062708 rank 3
2023-02-18 04:04:53,454 DEBUG TRAIN Batch 1/11200 loss 87.182350 loss_att 134.224091 loss_ctc 109.359192 loss_rnnt 74.817024 hw_loss 0.000114 lr 0.00062848 rank 2
2023-02-18 04:06:14,141 DEBUG TRAIN Batch 1/11300 loss 71.301773 loss_att 99.340271 loss_ctc 83.741730 loss_rnnt 64.035370 hw_loss 0.000077 lr 0.00062936 rank 4
2023-02-18 04:06:14,143 DEBUG TRAIN Batch 1/11300 loss 38.487804 loss_att 69.235474 loss_ctc 59.534981 loss_rnnt 29.531948 hw_loss 0.000061 lr 0.00063032 rank 5
2023-02-18 04:06:14,148 DEBUG TRAIN Batch 1/11300 loss 39.948528 loss_att 59.345936 loss_ctc 48.486115 loss_rnnt 34.927502 hw_loss 0.006003 lr 0.00063052 rank 7
2023-02-18 04:06:14,149 DEBUG TRAIN Batch 1/11300 loss 66.073921 loss_att 100.418152 loss_ctc 87.149155 loss_rnnt 56.370552 hw_loss 0.045916 lr 0.00062908 rank 3
2023-02-18 04:06:14,152 DEBUG TRAIN Batch 1/11300 loss 33.579468 loss_att 50.938438 loss_ctc 45.889797 loss_rnnt 28.423485 hw_loss 0.080272 lr 0.00063040 rank 6
2023-02-18 04:06:14,153 DEBUG TRAIN Batch 1/11300 loss 62.294983 loss_att 92.270065 loss_ctc 84.674683 loss_rnnt 53.279064 hw_loss 0.069267 lr 0.00063048 rank 2
2023-02-18 04:06:14,164 DEBUG TRAIN Batch 1/11300 loss 35.094376 loss_att 57.266541 loss_ctc 42.683914 loss_rnnt 29.609745 hw_loss 0.071736 lr 0.00063000 rank 0
2023-02-18 04:06:14,206 DEBUG TRAIN Batch 1/11300 loss 21.424088 loss_att 46.966053 loss_ctc 31.460096 loss_rnnt 14.977523 hw_loss 0.000070 lr 0.00063204 rank 1
2023-02-18 04:07:14,877 DEBUG TRAIN Batch 1/11400 loss 36.607758 loss_att 55.208542 loss_ctc 59.876602 loss_rnnt 29.785025 hw_loss 0.000124 lr 0.00063232 rank 5
2023-02-18 04:07:14,877 DEBUG TRAIN Batch 1/11400 loss 44.963177 loss_att 76.464615 loss_ctc 55.823822 loss_rnnt 37.140396 hw_loss 0.139510 lr 0.00063248 rank 2
2023-02-18 04:07:14,878 DEBUG TRAIN Batch 1/11400 loss 22.518076 loss_att 45.609669 loss_ctc 44.848774 loss_rnnt 14.878181 hw_loss 0.082777 lr 0.00063404 rank 1
2023-02-18 04:07:14,878 DEBUG TRAIN Batch 1/11400 loss 45.463459 loss_att 81.896378 loss_ctc 63.281700 loss_rnnt 35.746933 hw_loss 0.101574 lr 0.00063136 rank 4
2023-02-18 04:07:14,879 DEBUG TRAIN Batch 1/11400 loss 68.584053 loss_att 93.098610 loss_ctc 91.712234 loss_rnnt 60.597260 hw_loss 0.000229 lr 0.00063252 rank 7
2023-02-18 04:07:14,881 DEBUG TRAIN Batch 1/11400 loss 49.816101 loss_att 85.853630 loss_ctc 69.315994 loss_rnnt 39.978325 hw_loss 0.056779 lr 0.00063200 rank 0
2023-02-18 04:07:14,883 DEBUG TRAIN Batch 1/11400 loss 78.423119 loss_att 127.464737 loss_ctc 112.137726 loss_rnnt 64.119415 hw_loss 0.000198 lr 0.00063240 rank 6
2023-02-18 04:07:14,889 DEBUG TRAIN Batch 1/11400 loss 67.985710 loss_att 98.596802 loss_ctc 87.344070 loss_rnnt 59.282322 hw_loss 0.000098 lr 0.00063108 rank 3
2023-02-18 04:08:13,406 DEBUG TRAIN Batch 1/11500 loss 61.308731 loss_att 70.600052 loss_ctc 79.912582 loss_rnnt 56.871269 hw_loss 0.185040 lr 0.00063432 rank 5
2023-02-18 04:08:13,408 DEBUG TRAIN Batch 1/11500 loss 29.963091 loss_att 51.091919 loss_ctc 38.756531 loss_rnnt 24.493910 hw_loss 0.133049 lr 0.00063440 rank 6
2023-02-18 04:08:13,407 DEBUG TRAIN Batch 1/11500 loss 45.139294 loss_att 63.261250 loss_ctc 57.952621 loss_rnnt 39.751709 hw_loss 0.102654 lr 0.00063308 rank 3
2023-02-18 04:08:13,408 DEBUG TRAIN Batch 1/11500 loss 76.264320 loss_att 104.619362 loss_ctc 98.864372 loss_rnnt 67.550980 hw_loss 0.054364 lr 0.00063604 rank 1
2023-02-18 04:08:13,409 DEBUG TRAIN Batch 1/11500 loss 48.798363 loss_att 58.821301 loss_ctc 59.600811 loss_rnnt 45.297203 hw_loss 0.105453 lr 0.00063452 rank 7
2023-02-18 04:08:13,411 DEBUG TRAIN Batch 1/11500 loss 29.135365 loss_att 48.448528 loss_ctc 45.108978 loss_rnnt 23.142891 hw_loss 0.000048 lr 0.00063336 rank 4
2023-02-18 04:08:13,415 DEBUG TRAIN Batch 1/11500 loss 52.662548 loss_att 80.187935 loss_ctc 79.307243 loss_rnnt 43.529266 hw_loss 0.141703 lr 0.00063400 rank 0
2023-02-18 04:08:13,416 DEBUG TRAIN Batch 1/11500 loss 27.683119 loss_att 37.148861 loss_ctc 38.696312 loss_rnnt 24.222460 hw_loss 0.185782 lr 0.00063448 rank 2
2023-02-18 04:09:13,510 DEBUG TRAIN Batch 1/11600 loss 56.332306 loss_att 79.767365 loss_ctc 74.629814 loss_rnnt 49.195789 hw_loss 0.018445 lr 0.00063632 rank 5
2023-02-18 04:09:13,515 DEBUG TRAIN Batch 1/11600 loss 51.389950 loss_att 65.711617 loss_ctc 62.644115 loss_rnnt 47.025002 hw_loss 0.000109 lr 0.00063600 rank 0
2023-02-18 04:09:13,514 DEBUG TRAIN Batch 1/11600 loss 26.359470 loss_att 46.726418 loss_ctc 34.193008 loss_rnnt 21.177189 hw_loss 0.120786 lr 0.00063804 rank 1
2023-02-18 04:09:13,514 DEBUG TRAIN Batch 1/11600 loss 37.089764 loss_att 74.347794 loss_ctc 52.776939 loss_rnnt 27.514856 hw_loss 0.059398 lr 0.00063536 rank 4
2023-02-18 04:09:13,515 DEBUG TRAIN Batch 1/11600 loss 68.601128 loss_att 95.305359 loss_ctc 80.239952 loss_rnnt 61.640823 hw_loss 0.126788 lr 0.00063652 rank 7
2023-02-18 04:09:13,516 DEBUG TRAIN Batch 1/11600 loss 27.021667 loss_att 34.917870 loss_ctc 35.869171 loss_rnnt 24.196617 hw_loss 0.124014 lr 0.00063640 rank 6
2023-02-18 04:09:13,526 DEBUG TRAIN Batch 1/11600 loss 64.332611 loss_att 94.639900 loss_ctc 90.889008 loss_rnnt 54.728142 hw_loss 0.004051 lr 0.00063508 rank 3
2023-02-18 04:09:13,579 DEBUG TRAIN Batch 1/11600 loss 62.974159 loss_att 86.748634 loss_ctc 79.123779 loss_rnnt 55.992134 hw_loss 0.138462 lr 0.00063648 rank 2
2023-02-18 04:10:13,990 DEBUG TRAIN Batch 1/11700 loss 58.380299 loss_att 73.270279 loss_ctc 74.620941 loss_rnnt 53.236805 hw_loss 0.000147 lr 0.00064004 rank 1
2023-02-18 04:10:13,993 DEBUG TRAIN Batch 1/11700 loss 56.165653 loss_att 88.235245 loss_ctc 67.247513 loss_rnnt 48.274071 hw_loss 0.000166 lr 0.00063848 rank 2
2023-02-18 04:10:13,996 DEBUG TRAIN Batch 1/11700 loss 57.279495 loss_att 71.736984 loss_ctc 80.676880 loss_rnnt 51.268307 hw_loss 0.000081 lr 0.00063708 rank 3
2023-02-18 04:10:13,997 DEBUG TRAIN Batch 1/11700 loss 34.230373 loss_att 63.963058 loss_ctc 45.640308 loss_rnnt 26.761866 hw_loss 0.001207 lr 0.00063800 rank 0
2023-02-18 04:10:13,997 DEBUG TRAIN Batch 1/11700 loss 32.180698 loss_att 56.173172 loss_ctc 45.732346 loss_rnnt 25.575264 hw_loss 0.000103 lr 0.00063832 rank 5
2023-02-18 04:10:14,001 DEBUG TRAIN Batch 1/11700 loss 112.621521 loss_att 142.142776 loss_ctc 145.096283 loss_rnnt 102.387230 hw_loss 0.000142 lr 0.00063736 rank 4
2023-02-18 04:10:14,002 DEBUG TRAIN Batch 1/11700 loss 63.882782 loss_att 105.791565 loss_ctc 91.328102 loss_rnnt 51.758415 hw_loss 0.156055 lr 0.00063840 rank 6
2023-02-18 04:10:14,057 DEBUG TRAIN Batch 1/11700 loss 80.624847 loss_att 105.208344 loss_ctc 100.639221 loss_rnnt 73.039474 hw_loss 0.000178 lr 0.00063852 rank 7
2023-02-18 04:11:12,143 DEBUG TRAIN Batch 1/11800 loss 32.697231 loss_att 49.024437 loss_ctc 43.168491 loss_rnnt 28.035528 hw_loss 0.000184 lr 0.00064000 rank 0
2023-02-18 04:11:12,146 DEBUG TRAIN Batch 1/11800 loss 43.234028 loss_att 85.383080 loss_ctc 51.972649 loss_rnnt 33.541016 hw_loss 0.183836 lr 0.00064040 rank 6
2023-02-18 04:11:12,146 DEBUG TRAIN Batch 1/11800 loss 80.017250 loss_att 98.285194 loss_ctc 105.814117 loss_rnnt 72.893997 hw_loss 0.056393 lr 0.00064052 rank 7
2023-02-18 04:11:12,147 DEBUG TRAIN Batch 1/11800 loss 39.448185 loss_att 49.111736 loss_ctc 56.735142 loss_rnnt 35.088867 hw_loss 0.228145 lr 0.00063908 rank 3
2023-02-18 04:11:12,148 DEBUG TRAIN Batch 1/11800 loss 41.810417 loss_att 56.963486 loss_ctc 63.968353 loss_rnnt 35.753441 hw_loss 0.134942 lr 0.00064204 rank 1
2023-02-18 04:11:12,148 DEBUG TRAIN Batch 1/11800 loss 60.982445 loss_att 86.489655 loss_ctc 83.362473 loss_rnnt 52.804817 hw_loss 0.172833 lr 0.00064032 rank 5
2023-02-18 04:11:12,150 DEBUG TRAIN Batch 1/11800 loss 47.879440 loss_att 63.813484 loss_ctc 63.143536 loss_rnnt 42.554314 hw_loss 0.193326 lr 0.00064048 rank 2
2023-02-18 04:11:12,205 DEBUG TRAIN Batch 1/11800 loss 48.217709 loss_att 61.437439 loss_ctc 61.844444 loss_rnnt 43.736561 hw_loss 0.038061 lr 0.00063936 rank 4
2023-02-18 04:12:11,899 DEBUG TRAIN Batch 1/11900 loss 49.121323 loss_att 77.140839 loss_ctc 57.960587 loss_rnnt 42.236275 hw_loss 0.192332 lr 0.00064232 rank 5
2023-02-18 04:12:11,904 DEBUG TRAIN Batch 1/11900 loss 37.910206 loss_att 54.748482 loss_ctc 48.655991 loss_rnnt 33.086407 hw_loss 0.043823 lr 0.00064200 rank 0
2023-02-18 04:12:11,904 DEBUG TRAIN Batch 1/11900 loss 61.795521 loss_att 87.972816 loss_ctc 68.536209 loss_rnnt 55.558781 hw_loss 0.192222 lr 0.00064404 rank 1
2023-02-18 04:12:11,906 DEBUG TRAIN Batch 1/11900 loss 78.500412 loss_att 115.228676 loss_ctc 107.570015 loss_rnnt 67.174438 hw_loss 0.195688 lr 0.00064252 rank 7
2023-02-18 04:12:11,906 DEBUG TRAIN Batch 1/11900 loss 44.124672 loss_att 72.310753 loss_ctc 65.083481 loss_rnnt 35.628445 hw_loss 0.120942 lr 0.00064136 rank 4
2023-02-18 04:12:11,908 DEBUG TRAIN Batch 1/11900 loss 52.654148 loss_att 80.452522 loss_ctc 83.875908 loss_rnnt 42.909077 hw_loss 0.042171 lr 0.00064240 rank 6
2023-02-18 04:12:11,912 DEBUG TRAIN Batch 1/11900 loss 38.985653 loss_att 62.010086 loss_ctc 46.381386 loss_rnnt 33.394646 hw_loss 0.000034 lr 0.00064108 rank 3
2023-02-18 04:12:11,968 DEBUG TRAIN Batch 1/11900 loss 55.805038 loss_att 81.966606 loss_ctc 70.606743 loss_rnnt 48.593567 hw_loss 0.010502 lr 0.00064248 rank 2
2023-02-18 04:13:14,229 DEBUG TRAIN Batch 1/12000 loss 36.307350 loss_att 53.858139 loss_ctc 55.611031 loss_rnnt 30.168140 hw_loss 0.103554 lr 0.00064336 rank 4
2023-02-18 04:13:14,229 DEBUG TRAIN Batch 1/12000 loss 38.420368 loss_att 64.658897 loss_ctc 44.930016 loss_rnnt 32.304665 hw_loss 0.000085 lr 0.00064308 rank 3
2023-02-18 04:13:14,230 DEBUG TRAIN Batch 1/12000 loss 20.503210 loss_att 52.151936 loss_ctc 18.389214 loss_rnnt 14.330071 hw_loss 0.234862 lr 0.00064432 rank 5
2023-02-18 04:13:14,232 DEBUG TRAIN Batch 1/12000 loss 57.549683 loss_att 95.261993 loss_ctc 60.674263 loss_rnnt 49.590580 hw_loss 0.000053 lr 0.00064604 rank 1
2023-02-18 04:13:14,233 DEBUG TRAIN Batch 1/12000 loss 53.821594 loss_att 98.498795 loss_ctc 66.670227 loss_rnnt 43.171753 hw_loss 0.002346 lr 0.00064400 rank 0
2023-02-18 04:13:14,235 DEBUG TRAIN Batch 1/12000 loss 48.760437 loss_att 90.359970 loss_ctc 77.176109 loss_rnnt 36.591927 hw_loss 0.112207 lr 0.00064440 rank 6
2023-02-18 04:13:14,235 DEBUG TRAIN Batch 1/12000 loss 29.911882 loss_att 65.775116 loss_ctc 45.410294 loss_rnnt 20.672764 hw_loss 0.000028 lr 0.00064452 rank 7
2023-02-18 04:13:14,291 DEBUG TRAIN Batch 1/12000 loss 82.657288 loss_att 108.777031 loss_ctc 111.482719 loss_rnnt 73.513763 hw_loss 0.142865 lr 0.00064448 rank 2
2023-02-18 04:14:33,124 DEBUG TRAIN Batch 1/12100 loss 43.954166 loss_att 73.879456 loss_ctc 58.444290 loss_rnnt 35.972801 hw_loss 0.120555 lr 0.00064632 rank 5
2023-02-18 04:14:33,125 DEBUG TRAIN Batch 1/12100 loss 65.562744 loss_att 83.349319 loss_ctc 92.197289 loss_rnnt 58.357372 hw_loss 0.181465 lr 0.00064536 rank 4
2023-02-18 04:14:33,126 DEBUG TRAIN Batch 1/12100 loss 30.531731 loss_att 44.927704 loss_ctc 45.545673 loss_rnnt 25.590622 hw_loss 0.112603 lr 0.00064652 rank 7
2023-02-18 04:14:33,132 DEBUG TRAIN Batch 1/12100 loss 41.108513 loss_att 53.762779 loss_ctc 60.818428 loss_rnnt 35.898365 hw_loss 0.096189 lr 0.00064648 rank 2
2023-02-18 04:14:33,133 DEBUG TRAIN Batch 1/12100 loss 31.535225 loss_att 64.258881 loss_ctc 49.853882 loss_rnnt 22.547989 hw_loss 0.000032 lr 0.00064640 rank 6
2023-02-18 04:14:33,135 DEBUG TRAIN Batch 1/12100 loss 30.575380 loss_att 61.466110 loss_ctc 33.809334 loss_rnnt 23.918941 hw_loss 0.088310 lr 0.00064804 rank 1
2023-02-18 04:14:33,138 DEBUG TRAIN Batch 1/12100 loss 41.826385 loss_att 67.038353 loss_ctc 55.954365 loss_rnnt 34.899773 hw_loss 0.000912 lr 0.00064508 rank 3
2023-02-18 04:14:33,140 DEBUG TRAIN Batch 1/12100 loss 66.087662 loss_att 82.437378 loss_ctc 85.167740 loss_rnnt 60.100052 hw_loss 0.325606 lr 0.00064600 rank 0
2023-02-18 04:15:48,473 DEBUG TRAIN Batch 1/12200 loss 54.617393 loss_att 76.713074 loss_ctc 85.780998 loss_rnnt 46.043030 hw_loss 0.000145 lr 0.00064800 rank 0
2023-02-18 04:15:48,474 DEBUG TRAIN Batch 1/12200 loss 55.008778 loss_att 83.083275 loss_ctc 83.618835 loss_rnnt 45.579155 hw_loss 0.000082 lr 0.00065004 rank 1
2023-02-18 04:15:48,477 DEBUG TRAIN Batch 1/12200 loss 42.234108 loss_att 61.594887 loss_ctc 57.581940 loss_rnnt 36.233982 hw_loss 0.152990 lr 0.00064840 rank 6
2023-02-18 04:15:48,478 DEBUG TRAIN Batch 1/12200 loss 47.123997 loss_att 63.160339 loss_ctc 68.148468 loss_rnnt 41.113430 hw_loss 0.000069 lr 0.00064832 rank 5
2023-02-18 04:15:48,480 DEBUG TRAIN Batch 1/12200 loss 86.408989 loss_att 118.152061 loss_ctc 108.530434 loss_rnnt 77.110802 hw_loss 0.000105 lr 0.00064852 rank 7
2023-02-18 04:15:48,485 DEBUG TRAIN Batch 1/12200 loss 48.753990 loss_att 71.059143 loss_ctc 75.579567 loss_rnnt 40.716171 hw_loss 0.000088 lr 0.00064736 rank 4
2023-02-18 04:15:48,486 DEBUG TRAIN Batch 1/12200 loss 31.645481 loss_att 54.321907 loss_ctc 44.327236 loss_rnnt 25.419266 hw_loss 0.000052 lr 0.00064708 rank 3
2023-02-18 04:15:48,486 DEBUG TRAIN Batch 1/12200 loss 18.162804 loss_att 30.849518 loss_ctc 27.250622 loss_rnnt 14.364307 hw_loss 0.092708 lr 0.00064848 rank 2
2023-02-18 04:16:48,362 DEBUG TRAIN Batch 1/12300 loss 21.342434 loss_att 42.131805 loss_ctc 17.421968 loss_rnnt 17.634478 hw_loss 0.136519 lr 0.00065032 rank 5
2023-02-18 04:16:48,365 DEBUG TRAIN Batch 1/12300 loss 24.262167 loss_att 38.491711 loss_ctc 43.750336 loss_rnnt 18.716032 hw_loss 0.190884 lr 0.00064936 rank 4
2023-02-18 04:16:48,365 DEBUG TRAIN Batch 1/12300 loss 37.250191 loss_att 66.010826 loss_ctc 56.877556 loss_rnnt 28.881027 hw_loss 0.000096 lr 0.00065000 rank 0
2023-02-18 04:16:48,367 DEBUG TRAIN Batch 1/12300 loss 39.148079 loss_att 61.121532 loss_ctc 50.048183 loss_rnnt 33.299999 hw_loss 0.000075 lr 0.00065040 rank 6
2023-02-18 04:16:48,368 DEBUG TRAIN Batch 1/12300 loss 27.824223 loss_att 32.269016 loss_ctc 35.724201 loss_rnnt 25.838171 hw_loss 0.082055 lr 0.00065052 rank 7
2023-02-18 04:16:48,370 DEBUG TRAIN Batch 1/12300 loss 21.246868 loss_att 25.653532 loss_ctc 28.342245 loss_rnnt 19.338209 hw_loss 0.152395 lr 0.00065204 rank 1
2023-02-18 04:16:48,372 DEBUG TRAIN Batch 1/12300 loss 13.852256 loss_att 32.156708 loss_ctc 31.446447 loss_rnnt 7.845426 hw_loss 0.000089 lr 0.00064908 rank 3
2023-02-18 04:16:48,375 DEBUG TRAIN Batch 1/12300 loss 57.564552 loss_att 94.563652 loss_ctc 84.982918 loss_rnnt 46.508911 hw_loss 0.000072 lr 0.00065048 rank 2
2023-02-18 04:17:46,724 DEBUG TRAIN Batch 1/12400 loss 43.181660 loss_att 64.807816 loss_ctc 57.611099 loss_rnnt 36.865536 hw_loss 0.125567 lr 0.00065232 rank 5
2023-02-18 04:17:46,730 DEBUG TRAIN Batch 1/12400 loss 51.308704 loss_att 76.131256 loss_ctc 73.252762 loss_rnnt 43.418266 hw_loss 0.000101 lr 0.00065404 rank 1
2023-02-18 04:17:46,735 DEBUG TRAIN Batch 1/12400 loss 33.116676 loss_att 43.764648 loss_ctc 45.239529 loss_rnnt 29.318882 hw_loss 0.097163 lr 0.00065240 rank 6
2023-02-18 04:17:46,735 DEBUG TRAIN Batch 1/12400 loss 34.506939 loss_att 56.256779 loss_ctc 50.002819 loss_rnnt 28.090816 hw_loss 0.000071 lr 0.00065136 rank 4
2023-02-18 04:17:46,740 DEBUG TRAIN Batch 1/12400 loss 43.029163 loss_att 79.543922 loss_ctc 67.718544 loss_rnnt 32.427711 hw_loss 0.012344 lr 0.00065200 rank 0
2023-02-18 04:17:46,745 DEBUG TRAIN Batch 1/12400 loss 40.684296 loss_att 56.746536 loss_ctc 53.660255 loss_rnnt 35.694023 hw_loss 0.089430 lr 0.00065252 rank 7
2023-02-18 04:17:46,746 DEBUG TRAIN Batch 1/12400 loss 23.723261 loss_att 41.880856 loss_ctc 36.736809 loss_rnnt 18.334185 hw_loss 0.042026 lr 0.00065248 rank 2
2023-02-18 04:17:46,749 DEBUG TRAIN Batch 1/12400 loss 64.102203 loss_att 92.491135 loss_ctc 80.653633 loss_rnnt 56.118736 hw_loss 0.185298 lr 0.00065108 rank 3
2023-02-18 04:18:48,901 DEBUG TRAIN Batch 1/12500 loss 94.273064 loss_att 129.423615 loss_ctc 119.757744 loss_rnnt 83.844986 hw_loss 0.000026 lr 0.00065336 rank 4
2023-02-18 04:18:48,903 DEBUG TRAIN Batch 1/12500 loss 23.167763 loss_att 38.895439 loss_ctc 38.651402 loss_rnnt 17.899433 hw_loss 0.109329 lr 0.00065448 rank 2
2023-02-18 04:18:48,903 DEBUG TRAIN Batch 1/12500 loss 32.308056 loss_att 50.803112 loss_ctc 42.567879 loss_rnnt 27.241043 hw_loss 0.000051 lr 0.00065604 rank 1
2023-02-18 04:18:48,907 DEBUG TRAIN Batch 1/12500 loss 58.927731 loss_att 91.758377 loss_ctc 92.666649 loss_rnnt 47.863060 hw_loss 0.000035 lr 0.00065432 rank 5
2023-02-18 04:18:48,907 DEBUG TRAIN Batch 1/12500 loss 37.473869 loss_att 54.718063 loss_ctc 49.967278 loss_rnnt 32.353050 hw_loss 0.011604 lr 0.00065308 rank 3
2023-02-18 04:18:48,913 DEBUG TRAIN Batch 1/12500 loss 39.627995 loss_att 68.833008 loss_ctc 56.030594 loss_rnnt 31.579416 hw_loss 0.038555 lr 0.00065440 rank 6
2023-02-18 04:18:48,915 DEBUG TRAIN Batch 1/12500 loss 52.394028 loss_att 73.183548 loss_ctc 64.966408 loss_rnnt 46.449310 hw_loss 0.207184 lr 0.00065400 rank 0
2023-02-18 04:18:48,966 DEBUG TRAIN Batch 1/12500 loss 21.589684 loss_att 46.249043 loss_ctc 27.622761 loss_rnnt 15.740081 hw_loss 0.212473 lr 0.00065452 rank 7
2023-02-18 04:19:50,539 DEBUG TRAIN Batch 1/12600 loss 68.184769 loss_att 94.180061 loss_ctc 98.185760 loss_rnnt 58.937935 hw_loss 0.089315 lr 0.00065600 rank 0
2023-02-18 04:19:50,542 DEBUG TRAIN Batch 1/12600 loss 66.744812 loss_att 83.293427 loss_ctc 81.279129 loss_rnnt 61.437908 hw_loss 0.111117 lr 0.00065632 rank 5
2023-02-18 04:19:50,544 DEBUG TRAIN Batch 1/12600 loss 38.160820 loss_att 55.045609 loss_ctc 45.907616 loss_rnnt 33.678566 hw_loss 0.135728 lr 0.00065648 rank 2
2023-02-18 04:19:50,545 DEBUG TRAIN Batch 1/12600 loss 45.698696 loss_att 87.122528 loss_ctc 56.010281 loss_rnnt 36.039001 hw_loss 0.000092 lr 0.00065640 rank 6
2023-02-18 04:19:50,548 DEBUG TRAIN Batch 1/12600 loss 57.929203 loss_att 61.717918 loss_ctc 71.944092 loss_rnnt 55.259460 hw_loss 0.081286 lr 0.00065508 rank 3
2023-02-18 04:19:50,552 DEBUG TRAIN Batch 1/12600 loss 20.157282 loss_att 30.994410 loss_ctc 29.639141 loss_rnnt 16.590668 hw_loss 0.253015 lr 0.00065652 rank 7
2023-02-18 04:19:50,592 DEBUG TRAIN Batch 1/12600 loss 103.988007 loss_att 128.212570 loss_ctc 137.675095 loss_rnnt 94.651451 hw_loss 0.000053 lr 0.00065536 rank 4
2023-02-18 04:19:50,635 DEBUG TRAIN Batch 1/12600 loss 19.126829 loss_att 23.654881 loss_ctc 22.436543 loss_rnnt 17.620579 hw_loss 0.298770 lr 0.00065804 rank 1
2023-02-18 04:20:48,873 DEBUG TRAIN Batch 1/12700 loss 74.159355 loss_att 95.721931 loss_ctc 94.511909 loss_rnnt 67.133087 hw_loss 0.000159 lr 0.00065848 rank 2
2023-02-18 04:20:48,874 DEBUG TRAIN Batch 1/12700 loss 29.493942 loss_att 52.484306 loss_ctc 44.001740 loss_rnnt 22.879417 hw_loss 0.153897 lr 0.00065736 rank 4
2023-02-18 04:20:48,874 DEBUG TRAIN Batch 1/12700 loss 28.198107 loss_att 40.914978 loss_ctc 40.669060 loss_rnnt 23.971157 hw_loss 0.038967 lr 0.00065832 rank 5
2023-02-18 04:20:48,875 DEBUG TRAIN Batch 1/12700 loss 41.386570 loss_att 56.875626 loss_ctc 53.496658 loss_rnnt 36.650192 hw_loss 0.044785 lr 0.00065708 rank 3
2023-02-18 04:20:48,876 DEBUG TRAIN Batch 1/12700 loss 40.736168 loss_att 69.194962 loss_ctc 54.310276 loss_rnnt 33.234413 hw_loss 0.000207 lr 0.00065800 rank 0
2023-02-18 04:20:48,876 DEBUG TRAIN Batch 1/12700 loss 43.577744 loss_att 59.768272 loss_ctc 64.063774 loss_rnnt 37.591301 hw_loss 0.031622 lr 0.00066004 rank 1
2023-02-18 04:20:48,877 DEBUG TRAIN Batch 1/12700 loss 30.795479 loss_att 44.709984 loss_ctc 32.901459 loss_rnnt 27.697554 hw_loss 0.064178 lr 0.00065852 rank 7
2023-02-18 04:20:48,878 DEBUG TRAIN Batch 1/12700 loss 16.478300 loss_att 26.498047 loss_ctc 21.248718 loss_rnnt 13.838218 hw_loss 0.000145 lr 0.00065840 rank 6
2023-02-18 04:21:50,053 DEBUG TRAIN Batch 1/12800 loss 56.197750 loss_att 76.471268 loss_ctc 69.257401 loss_rnnt 50.401733 hw_loss 0.000048 lr 0.00066032 rank 5
2023-02-18 04:21:50,055 DEBUG TRAIN Batch 1/12800 loss 85.611382 loss_att 144.073761 loss_ctc 95.534508 loss_rnnt 72.595764 hw_loss 0.000091 lr 0.00065908 rank 3
2023-02-18 04:21:50,057 DEBUG TRAIN Batch 1/12800 loss 65.608376 loss_att 92.115936 loss_ctc 102.826294 loss_rnnt 55.344452 hw_loss 0.000038 lr 0.00065936 rank 4
2023-02-18 04:21:50,057 DEBUG TRAIN Batch 1/12800 loss 67.941444 loss_att 72.129799 loss_ctc 82.039955 loss_rnnt 65.223709 hw_loss 0.000498 lr 0.00066052 rank 7
2023-02-18 04:21:50,058 DEBUG TRAIN Batch 1/12800 loss 62.097359 loss_att 75.089645 loss_ctc 85.224503 loss_rnnt 56.415253 hw_loss 0.000062 lr 0.00066048 rank 2
2023-02-18 04:21:50,061 DEBUG TRAIN Batch 1/12800 loss 25.593918 loss_att 41.980484 loss_ctc 44.595284 loss_rnnt 19.751966 hw_loss 0.058354 lr 0.00066040 rank 6
2023-02-18 04:21:50,062 DEBUG TRAIN Batch 1/12800 loss 63.114208 loss_att 94.770485 loss_ctc 96.834808 loss_rnnt 52.254112 hw_loss 0.061434 lr 0.00066000 rank 0
2023-02-18 04:21:50,062 DEBUG TRAIN Batch 1/12800 loss 21.084030 loss_att 50.425667 loss_ctc 29.765915 loss_rnnt 14.016710 hw_loss 0.077639 lr 0.00066204 rank 1
2023-02-18 04:23:10,143 DEBUG TRAIN Batch 1/12900 loss 74.804825 loss_att 96.196999 loss_ctc 102.798958 loss_rnnt 66.714355 hw_loss 0.149045 lr 0.00066252 rank 7
2023-02-18 04:23:10,145 DEBUG TRAIN Batch 1/12900 loss 45.237675 loss_att 79.011681 loss_ctc 60.393078 loss_rnnt 36.462124 hw_loss 0.000058 lr 0.00066200 rank 0
2023-02-18 04:23:10,145 DEBUG TRAIN Batch 1/12900 loss 41.714676 loss_att 47.923409 loss_ctc 57.932087 loss_rnnt 38.189125 hw_loss 0.227785 lr 0.00066404 rank 1
2023-02-18 04:23:10,147 DEBUG TRAIN Batch 1/12900 loss 26.168577 loss_att 41.328087 loss_ctc 31.261225 loss_rnnt 22.449278 hw_loss 0.015706 lr 0.00066136 rank 4
2023-02-18 04:23:10,147 DEBUG TRAIN Batch 1/12900 loss 36.695568 loss_att 50.609150 loss_ctc 57.945908 loss_rnnt 30.828171 hw_loss 0.471195 lr 0.00066240 rank 6
2023-02-18 04:23:10,148 DEBUG TRAIN Batch 1/12900 loss 53.993530 loss_att 66.065514 loss_ctc 60.092804 loss_rnnt 50.720104 hw_loss 0.085860 lr 0.00066232 rank 5
2023-02-18 04:23:10,149 DEBUG TRAIN Batch 1/12900 loss 58.051346 loss_att 94.220192 loss_ctc 83.793755 loss_rnnt 47.385227 hw_loss 0.000048 lr 0.00066248 rank 2
2023-02-18 04:23:10,152 DEBUG TRAIN Batch 1/12900 loss 19.399202 loss_att 43.092678 loss_ctc 45.896221 loss_rnnt 11.127537 hw_loss 0.000067 lr 0.00066108 rank 3
2023-02-18 04:24:27,008 DEBUG TRAIN Batch 1/13000 loss 47.241138 loss_att 67.454010 loss_ctc 65.568436 loss_rnnt 40.750908 hw_loss 0.007525 lr 0.00066432 rank 5
2023-02-18 04:24:27,009 DEBUG TRAIN Batch 1/13000 loss 67.802292 loss_att 99.621201 loss_ctc 91.077721 loss_rnnt 58.319267 hw_loss 0.029716 lr 0.00066452 rank 7
2023-02-18 04:24:27,012 DEBUG TRAIN Batch 1/13000 loss 65.424484 loss_att 86.612274 loss_ctc 89.374100 loss_rnnt 57.961098 hw_loss 0.061017 lr 0.00066336 rank 4
2023-02-18 04:24:27,012 DEBUG TRAIN Batch 1/13000 loss 64.584740 loss_att 81.738007 loss_ctc 86.265282 loss_rnnt 58.263294 hw_loss 0.000097 lr 0.00066604 rank 1
2023-02-18 04:24:27,014 DEBUG TRAIN Batch 1/13000 loss 44.941269 loss_att 47.048321 loss_ctc 59.123985 loss_rnnt 42.535568 hw_loss 0.174868 lr 0.00066400 rank 0
2023-02-18 04:24:27,015 DEBUG TRAIN Batch 1/13000 loss 26.753197 loss_att 52.826687 loss_ctc 36.909153 loss_rnnt 20.184317 hw_loss 0.000100 lr 0.00066440 rank 6
2023-02-18 04:24:27,019 DEBUG TRAIN Batch 1/13000 loss 80.086327 loss_att 103.213196 loss_ctc 116.550751 loss_rnnt 70.524765 hw_loss 0.139264 lr 0.00066308 rank 3
2023-02-18 04:24:27,079 DEBUG TRAIN Batch 1/13000 loss 38.981174 loss_att 59.608978 loss_ctc 53.025524 loss_rnnt 32.982979 hw_loss 0.000103 lr 0.00066448 rank 2
2023-02-18 04:25:27,124 DEBUG TRAIN Batch 1/13100 loss 27.279003 loss_att 43.948891 loss_ctc 42.079311 loss_rnnt 21.863560 hw_loss 0.202665 lr 0.00066640 rank 6
2023-02-18 04:25:27,126 DEBUG TRAIN Batch 1/13100 loss 48.580540 loss_att 79.551651 loss_ctc 69.761307 loss_rnnt 39.513977 hw_loss 0.090446 lr 0.00066804 rank 1
2023-02-18 04:25:27,127 DEBUG TRAIN Batch 1/13100 loss 44.645607 loss_att 63.502869 loss_ctc 53.729225 loss_rnnt 39.662884 hw_loss 0.000228 lr 0.00066600 rank 0
2023-02-18 04:25:27,126 DEBUG TRAIN Batch 1/13100 loss 45.338726 loss_att 73.979645 loss_ctc 82.920624 loss_rnnt 34.501610 hw_loss 0.183775 lr 0.00066632 rank 5
2023-02-18 04:25:27,132 DEBUG TRAIN Batch 1/13100 loss 26.682070 loss_att 50.103424 loss_ctc 38.655144 loss_rnnt 20.272871 hw_loss 0.240970 lr 0.00066648 rank 2
2023-02-18 04:25:27,134 DEBUG TRAIN Batch 1/13100 loss 54.263168 loss_att 84.274139 loss_ctc 71.772141 loss_rnnt 45.926300 hw_loss 0.000266 lr 0.00066508 rank 3
2023-02-18 04:25:27,139 DEBUG TRAIN Batch 1/13100 loss 68.959023 loss_att 111.215912 loss_ctc 99.234489 loss_rnnt 56.442154 hw_loss 0.053916 lr 0.00066536 rank 4
2023-02-18 04:25:27,188 DEBUG TRAIN Batch 1/13100 loss 46.522556 loss_att 84.921860 loss_ctc 61.399837 loss_rnnt 36.858948 hw_loss 0.000200 lr 0.00066652 rank 7
2023-02-18 04:26:25,180 DEBUG TRAIN Batch 1/13200 loss 39.480221 loss_att 44.303986 loss_ctc 61.213604 loss_rnnt 35.517288 hw_loss 0.188233 lr 0.00066832 rank 5
2023-02-18 04:26:25,185 DEBUG TRAIN Batch 1/13200 loss 31.389544 loss_att 38.238064 loss_ctc 46.574474 loss_rnnt 27.828972 hw_loss 0.311644 lr 0.00066736 rank 4
2023-02-18 04:26:25,186 DEBUG TRAIN Batch 1/13200 loss 41.057087 loss_att 55.630806 loss_ctc 52.294815 loss_rnnt 36.580772 hw_loss 0.118516 lr 0.00066852 rank 7
2023-02-18 04:26:25,185 DEBUG TRAIN Batch 1/13200 loss 38.580765 loss_att 72.870361 loss_ctc 58.098465 loss_rnnt 29.120449 hw_loss 0.000066 lr 0.00066848 rank 2
2023-02-18 04:26:25,194 DEBUG TRAIN Batch 1/13200 loss 25.929762 loss_att 57.343040 loss_ctc 39.259350 loss_rnnt 17.789526 hw_loss 0.150560 lr 0.00066800 rank 0
2023-02-18 04:26:25,196 DEBUG TRAIN Batch 1/13200 loss 17.925676 loss_att 21.828527 loss_ctc 25.134893 loss_rnnt 16.078718 hw_loss 0.197171 lr 0.00066708 rank 3
2023-02-18 04:26:25,225 DEBUG TRAIN Batch 1/13200 loss 44.288715 loss_att 53.777119 loss_ctc 54.187988 loss_rnnt 41.006470 hw_loss 0.121239 lr 0.00067004 rank 1
2023-02-18 04:26:25,274 DEBUG TRAIN Batch 1/13200 loss 19.506058 loss_att 40.757313 loss_ctc 28.957531 loss_rnnt 13.886903 hw_loss 0.203822 lr 0.00066840 rank 6
2023-02-18 04:27:24,305 DEBUG TRAIN Batch 1/13300 loss 24.928671 loss_att 46.384026 loss_ctc 40.151855 loss_rnnt 18.607809 hw_loss 0.000062 lr 0.00067032 rank 5
2023-02-18 04:27:24,308 DEBUG TRAIN Batch 1/13300 loss 31.455076 loss_att 43.066956 loss_ctc 38.768276 loss_rnnt 28.157566 hw_loss 0.000078 lr 0.00067040 rank 6
2023-02-18 04:27:24,312 DEBUG TRAIN Batch 1/13300 loss 47.470707 loss_att 60.018543 loss_ctc 54.085178 loss_rnnt 44.036030 hw_loss 0.080967 lr 0.00066908 rank 3
2023-02-18 04:27:24,313 DEBUG TRAIN Batch 1/13300 loss 108.781555 loss_att 128.542923 loss_ctc 152.299500 loss_rnnt 99.026848 hw_loss 0.000086 lr 0.00067052 rank 7
2023-02-18 04:27:24,313 DEBUG TRAIN Batch 1/13300 loss 47.971909 loss_att 71.168159 loss_ctc 75.387100 loss_rnnt 39.598431 hw_loss 0.147875 lr 0.00066936 rank 4
2023-02-18 04:27:24,316 DEBUG TRAIN Batch 1/13300 loss 43.398331 loss_att 58.342617 loss_ctc 63.131172 loss_rnnt 37.778381 hw_loss 0.000083 lr 0.00067048 rank 2
2023-02-18 04:27:24,319 DEBUG TRAIN Batch 1/13300 loss 115.375122 loss_att 151.116882 loss_ctc 159.310150 loss_rnnt 102.319260 hw_loss 0.092836 lr 0.00067204 rank 1
2023-02-18 04:27:24,369 DEBUG TRAIN Batch 1/13300 loss 25.513170 loss_att 42.157810 loss_ctc 42.031094 loss_rnnt 19.905933 hw_loss 0.142350 lr 0.00067000 rank 0
2023-02-18 04:28:23,444 DEBUG TRAIN Batch 1/13400 loss 46.835880 loss_att 77.653168 loss_ctc 70.917168 loss_rnnt 37.461559 hw_loss 0.000051 lr 0.00067252 rank 7
2023-02-18 04:28:23,445 DEBUG TRAIN Batch 1/13400 loss 33.591835 loss_att 50.886955 loss_ctc 49.271088 loss_rnnt 28.042200 hw_loss 0.000077 lr 0.00067404 rank 1
2023-02-18 04:28:23,448 DEBUG TRAIN Batch 1/13400 loss 41.695457 loss_att 65.803352 loss_ctc 66.503365 loss_rnnt 33.566128 hw_loss 0.000061 lr 0.00067232 rank 5
2023-02-18 04:28:23,449 DEBUG TRAIN Batch 1/13400 loss 23.454988 loss_att 40.786880 loss_ctc 50.683174 loss_rnnt 16.358154 hw_loss 0.000056 lr 0.00067200 rank 0
2023-02-18 04:28:23,453 DEBUG TRAIN Batch 1/13400 loss 70.440414 loss_att 88.689865 loss_ctc 95.964073 loss_rnnt 63.387341 hw_loss 0.000051 lr 0.00067248 rank 2
2023-02-18 04:28:23,454 DEBUG TRAIN Batch 1/13400 loss 83.281288 loss_att 122.023361 loss_ctc 92.945526 loss_rnnt 74.221115 hw_loss 0.043490 lr 0.00067240 rank 6
2023-02-18 04:28:23,464 DEBUG TRAIN Batch 1/13400 loss 85.089188 loss_att 111.854385 loss_ctc 104.293236 loss_rnnt 77.086700 hw_loss 0.166695 lr 0.00067108 rank 3
2023-02-18 04:28:23,512 DEBUG TRAIN Batch 1/13400 loss 32.542801 loss_att 58.468967 loss_ctc 39.837177 loss_rnnt 26.378252 hw_loss 0.012627 lr 0.00067136 rank 4
2023-02-18 04:29:22,358 DEBUG TRAIN Batch 1/13500 loss 55.120426 loss_att 86.755081 loss_ctc 82.995209 loss_rnnt 44.976410 hw_loss 0.188333 lr 0.00067400 rank 0
2023-02-18 04:29:22,359 DEBUG TRAIN Batch 1/13500 loss 25.566113 loss_att 40.920326 loss_ctc 41.306442 loss_rnnt 20.329206 hw_loss 0.126283 lr 0.00067336 rank 4
2023-02-18 04:29:22,362 DEBUG TRAIN Batch 1/13500 loss 41.426060 loss_att 50.757984 loss_ctc 55.928917 loss_rnnt 37.625916 hw_loss 0.000090 lr 0.00067604 rank 1
2023-02-18 04:29:22,362 DEBUG TRAIN Batch 1/13500 loss 26.284346 loss_att 44.634277 loss_ctc 41.458138 loss_rnnt 20.574318 hw_loss 0.031627 lr 0.00067452 rank 7
2023-02-18 04:29:22,364 DEBUG TRAIN Batch 1/13500 loss 55.758915 loss_att 110.944237 loss_ctc 65.593445 loss_rnnt 43.410530 hw_loss 0.000090 lr 0.00067448 rank 2
2023-02-18 04:29:22,370 DEBUG TRAIN Batch 1/13500 loss 31.621256 loss_att 42.791786 loss_ctc 43.025055 loss_rnnt 27.866608 hw_loss 0.000068 lr 0.00067432 rank 5
2023-02-18 04:29:22,370 DEBUG TRAIN Batch 1/13500 loss 28.388550 loss_att 36.269386 loss_ctc 38.264969 loss_rnnt 25.452084 hw_loss 0.081460 lr 0.00067308 rank 3
2023-02-18 04:29:22,427 DEBUG TRAIN Batch 1/13500 loss 38.899807 loss_att 62.193840 loss_ctc 48.579296 loss_rnnt 32.950352 hw_loss 0.000084 lr 0.00067440 rank 6
2023-02-18 04:30:21,849 DEBUG TRAIN Batch 1/13600 loss 45.974495 loss_att 68.823090 loss_ctc 67.083923 loss_rnnt 38.562843 hw_loss 0.051274 lr 0.00067508 rank 3
2023-02-18 04:30:21,850 DEBUG TRAIN Batch 1/13600 loss 60.469448 loss_att 71.686401 loss_ctc 69.517197 loss_rnnt 56.938030 hw_loss 0.153112 lr 0.00067804 rank 1
2023-02-18 04:30:21,852 DEBUG TRAIN Batch 1/13600 loss 82.896584 loss_att 116.333603 loss_ctc 96.593132 loss_rnnt 74.301285 hw_loss 0.153154 lr 0.00067632 rank 5
2023-02-18 04:30:21,853 DEBUG TRAIN Batch 1/13600 loss 38.266319 loss_att 63.192364 loss_ctc 47.478729 loss_rnnt 32.052742 hw_loss 0.000083 lr 0.00067640 rank 6
2023-02-18 04:30:21,853 DEBUG TRAIN Batch 1/13600 loss 89.076164 loss_att 119.649246 loss_ctc 124.804276 loss_rnnt 78.121536 hw_loss 0.143001 lr 0.00067536 rank 4
2023-02-18 04:30:21,858 DEBUG TRAIN Batch 1/13600 loss 25.224575 loss_att 39.551586 loss_ctc 38.506279 loss_rnnt 20.588232 hw_loss 0.000087 lr 0.00067648 rank 2
2023-02-18 04:30:21,859 DEBUG TRAIN Batch 1/13600 loss 33.169983 loss_att 44.634514 loss_ctc 52.308575 loss_rnnt 28.274382 hw_loss 0.095402 lr 0.00067652 rank 7
2023-02-18 04:30:21,922 DEBUG TRAIN Batch 1/13600 loss 13.252834 loss_att 26.603054 loss_ctc 23.306721 loss_rnnt 9.213693 hw_loss 0.053584 lr 0.00067600 rank 0
2023-02-18 04:31:22,761 DEBUG TRAIN Batch 1/13700 loss 46.511444 loss_att 74.208649 loss_ctc 56.461380 loss_rnnt 39.601913 hw_loss 0.081433 lr 0.00067832 rank 5
2023-02-18 04:31:22,761 DEBUG TRAIN Batch 1/13700 loss 28.604361 loss_att 41.556789 loss_ctc 34.771851 loss_rnnt 25.159267 hw_loss 0.060519 lr 0.00067800 rank 0
2023-02-18 04:31:22,765 DEBUG TRAIN Batch 1/13700 loss 95.954422 loss_att 130.740128 loss_ctc 142.482040 loss_rnnt 82.642761 hw_loss 0.282829 lr 0.00068004 rank 1
2023-02-18 04:31:22,765 DEBUG TRAIN Batch 1/13700 loss 53.276356 loss_att 73.207489 loss_ctc 74.355095 loss_rnnt 46.446373 hw_loss 0.062364 lr 0.00067736 rank 4
2023-02-18 04:31:22,770 DEBUG TRAIN Batch 1/13700 loss 38.498737 loss_att 57.449406 loss_ctc 62.519535 loss_rnnt 31.273415 hw_loss 0.435778 lr 0.00067840 rank 6
2023-02-18 04:31:22,770 DEBUG TRAIN Batch 1/13700 loss 30.654442 loss_att 48.132675 loss_ctc 52.521446 loss_rnnt 24.171230 hw_loss 0.134937 lr 0.00067708 rank 3
2023-02-18 04:31:22,776 DEBUG TRAIN Batch 1/13700 loss 52.761173 loss_att 75.644073 loss_ctc 82.129974 loss_rnnt 44.268684 hw_loss 0.000125 lr 0.00067852 rank 7
2023-02-18 04:31:22,808 DEBUG TRAIN Batch 1/13700 loss 57.965332 loss_att 100.010139 loss_ctc 60.188786 loss_rnnt 49.239117 hw_loss 0.038984 lr 0.00067848 rank 2
2023-02-18 04:32:47,301 DEBUG TRAIN Batch 1/13800 loss 54.981968 loss_att 78.996048 loss_ctc 97.572929 loss_rnnt 44.500320 hw_loss 0.000071 lr 0.00068000 rank 0
2023-02-18 04:32:47,301 DEBUG TRAIN Batch 1/13800 loss 47.348190 loss_att 64.002571 loss_ctc 71.579880 loss_rnnt 40.786201 hw_loss 0.000415 lr 0.00068204 rank 1
2023-02-18 04:32:47,306 DEBUG TRAIN Batch 1/13800 loss 59.066219 loss_att 84.726959 loss_ctc 81.226097 loss_rnnt 50.930107 hw_loss 0.092459 lr 0.00068040 rank 6
2023-02-18 04:32:47,305 DEBUG TRAIN Batch 1/13800 loss 71.443695 loss_att 110.015686 loss_ctc 96.088898 loss_rnnt 60.441177 hw_loss 0.003922 lr 0.00067936 rank 4
2023-02-18 04:32:47,328 DEBUG TRAIN Batch 1/13800 loss 17.081123 loss_att 27.276684 loss_ctc 32.364017 loss_rnnt 13.004265 hw_loss 0.000053 lr 0.00068032 rank 5
2023-02-18 04:32:47,330 DEBUG TRAIN Batch 1/13800 loss 36.172157 loss_att 63.344563 loss_ctc 54.244598 loss_rnnt 28.219696 hw_loss 0.203100 lr 0.00067908 rank 3
2023-02-18 04:32:47,346 DEBUG TRAIN Batch 1/13800 loss 53.739014 loss_att 78.048248 loss_ctc 75.241104 loss_rnnt 46.010193 hw_loss 0.000059 lr 0.00068048 rank 2
2023-02-18 04:32:47,356 DEBUG TRAIN Batch 1/13800 loss 52.492851 loss_att 73.885559 loss_ctc 60.601624 loss_rnnt 47.098167 hw_loss 0.065572 lr 0.00068052 rank 7
2023-02-18 04:34:02,663 DEBUG TRAIN Batch 1/13900 loss 63.658138 loss_att 88.372528 loss_ctc 90.563683 loss_rnnt 55.073738 hw_loss 0.101460 lr 0.00068232 rank 5
2023-02-18 04:34:02,666 DEBUG TRAIN Batch 1/13900 loss 72.325325 loss_att 86.983124 loss_ctc 96.671577 loss_rnnt 66.123062 hw_loss 0.046008 lr 0.00068200 rank 0
2023-02-18 04:34:02,668 DEBUG TRAIN Batch 1/13900 loss 45.473774 loss_att 73.733681 loss_ctc 74.437256 loss_rnnt 35.959969 hw_loss 0.000051 lr 0.00068252 rank 7
2023-02-18 04:34:02,668 DEBUG TRAIN Batch 1/13900 loss 51.877701 loss_att 65.642807 loss_ctc 74.905266 loss_rnnt 46.054310 hw_loss 0.000051 lr 0.00068240 rank 6
2023-02-18 04:34:02,669 DEBUG TRAIN Batch 1/13900 loss 36.762836 loss_att 53.650341 loss_ctc 54.523338 loss_rnnt 30.955357 hw_loss 0.116084 lr 0.00068404 rank 1
2023-02-18 04:34:02,677 DEBUG TRAIN Batch 1/13900 loss 54.066502 loss_att 94.533928 loss_ctc 88.872772 loss_rnnt 41.278061 hw_loss 0.101458 lr 0.00068136 rank 4
2023-02-18 04:34:02,677 DEBUG TRAIN Batch 1/13900 loss 42.712360 loss_att 65.181061 loss_ctc 52.521721 loss_rnnt 36.910633 hw_loss 0.000137 lr 0.00068108 rank 3
2023-02-18 04:34:02,687 DEBUG TRAIN Batch 1/13900 loss 32.763157 loss_att 55.193329 loss_ctc 56.847946 loss_rnnt 25.026100 hw_loss 0.074471 lr 0.00068248 rank 2
2023-02-18 04:35:02,736 DEBUG TRAIN Batch 1/14000 loss 30.593628 loss_att 57.097382 loss_ctc 55.773838 loss_rnnt 21.935495 hw_loss 0.000035 lr 0.00068432 rank 5
2023-02-18 04:35:02,736 DEBUG TRAIN Batch 1/14000 loss 41.302410 loss_att 75.455780 loss_ctc 77.521149 loss_rnnt 29.572628 hw_loss 0.131144 lr 0.00068440 rank 6
2023-02-18 04:35:02,739 DEBUG TRAIN Batch 1/14000 loss 51.500851 loss_att 76.204453 loss_ctc 71.504959 loss_rnnt 43.892891 hw_loss 0.000039 lr 0.00068336 rank 4
2023-02-18 04:35:02,741 DEBUG TRAIN Batch 1/14000 loss 65.561211 loss_att 78.837341 loss_ctc 93.331749 loss_rnnt 59.135895 hw_loss 0.126285 lr 0.00068604 rank 1
2023-02-18 04:35:02,743 DEBUG TRAIN Batch 1/14000 loss 65.855934 loss_att 101.594528 loss_ctc 90.102882 loss_rnnt 55.437283 hw_loss 0.071257 lr 0.00068400 rank 0
2023-02-18 04:35:02,743 DEBUG TRAIN Batch 1/14000 loss 60.411465 loss_att 99.348549 loss_ctc 80.386513 loss_rnnt 49.960686 hw_loss 0.000048 lr 0.00068308 rank 3
2023-02-18 04:35:02,746 DEBUG TRAIN Batch 1/14000 loss 27.645857 loss_att 42.101109 loss_ctc 39.266964 loss_rnnt 23.075123 hw_loss 0.244128 lr 0.00068452 rank 7
2023-02-18 04:35:02,748 DEBUG TRAIN Batch 1/14000 loss 40.268253 loss_att 75.257355 loss_ctc 64.975677 loss_rnnt 29.976084 hw_loss 0.000044 lr 0.00068448 rank 2
2023-02-18 04:36:00,263 DEBUG TRAIN Batch 1/14100 loss 34.380779 loss_att 43.879440 loss_ctc 42.158997 loss_rnnt 31.443821 hw_loss 0.000238 lr 0.00068632 rank 5
2023-02-18 04:36:00,267 DEBUG TRAIN Batch 1/14100 loss 40.637383 loss_att 79.484627 loss_ctc 48.890377 loss_rnnt 31.734535 hw_loss 0.061877 lr 0.00068640 rank 6
2023-02-18 04:36:00,268 DEBUG TRAIN Batch 1/14100 loss 21.920219 loss_att 37.293839 loss_ctc 32.953121 loss_rnnt 17.330936 hw_loss 0.081570 lr 0.00068536 rank 4
2023-02-18 04:36:00,269 DEBUG TRAIN Batch 1/14100 loss 27.458828 loss_att 41.213985 loss_ctc 41.619080 loss_rnnt 22.770477 hw_loss 0.092412 lr 0.00068600 rank 0
2023-02-18 04:36:00,270 DEBUG TRAIN Batch 1/14100 loss 41.708500 loss_att 67.155853 loss_ctc 54.852718 loss_rnnt 34.810143 hw_loss 0.105610 lr 0.00068804 rank 1
2023-02-18 04:36:00,276 DEBUG TRAIN Batch 1/14100 loss 48.766075 loss_att 64.110123 loss_ctc 58.684738 loss_rnnt 44.374619 hw_loss 0.000294 lr 0.00068652 rank 7
2023-02-18 04:36:00,281 DEBUG TRAIN Batch 1/14100 loss 31.179359 loss_att 48.179333 loss_ctc 44.881683 loss_rnnt 25.952314 hw_loss 0.000138 lr 0.00068508 rank 3
2023-02-18 04:36:00,334 DEBUG TRAIN Batch 1/14100 loss 26.797392 loss_att 32.563850 loss_ctc 38.015034 loss_rnnt 24.132124 hw_loss 0.030540 lr 0.00068648 rank 2
2023-02-18 04:36:59,557 DEBUG TRAIN Batch 1/14200 loss 25.015932 loss_att 45.510551 loss_ctc 35.126499 loss_rnnt 19.521151 hw_loss 0.089591 lr 0.00069004 rank 1
2023-02-18 04:36:59,558 DEBUG TRAIN Batch 1/14200 loss 65.330261 loss_att 86.323425 loss_ctc 93.241875 loss_rnnt 57.351822 hw_loss 0.109233 lr 0.00068832 rank 5
2023-02-18 04:36:59,559 DEBUG TRAIN Batch 1/14200 loss 43.595673 loss_att 62.041660 loss_ctc 52.145233 loss_rnnt 38.766159 hw_loss 0.000705 lr 0.00068848 rank 2
2023-02-18 04:36:59,559 DEBUG TRAIN Batch 1/14200 loss 35.692318 loss_att 53.816978 loss_ctc 41.916245 loss_rnnt 31.151455 hw_loss 0.161390 lr 0.00068736 rank 4
2023-02-18 04:36:59,561 DEBUG TRAIN Batch 1/14200 loss 77.544975 loss_att 116.464630 loss_ctc 114.641663 loss_rnnt 64.774780 hw_loss 0.075078 lr 0.00068852 rank 7
2023-02-18 04:36:59,562 DEBUG TRAIN Batch 1/14200 loss 25.409618 loss_att 41.686840 loss_ctc 41.479111 loss_rnnt 19.897675 hw_loss 0.213560 lr 0.00068840 rank 6
2023-02-18 04:36:59,565 DEBUG TRAIN Batch 1/14200 loss 23.184542 loss_att 38.538750 loss_ctc 40.789646 loss_rnnt 17.690847 hw_loss 0.141575 lr 0.00068708 rank 3
2023-02-18 04:36:59,566 DEBUG TRAIN Batch 1/14200 loss 48.527664 loss_att 69.026543 loss_ctc 68.655960 loss_rnnt 41.713478 hw_loss 0.057445 lr 0.00068800 rank 0
2023-02-18 04:37:59,086 DEBUG TRAIN Batch 1/14300 loss 82.697548 loss_att 104.384583 loss_ctc 101.571548 loss_rnnt 75.785248 hw_loss 0.109422 lr 0.00068936 rank 4
2023-02-18 04:37:59,091 DEBUG TRAIN Batch 1/14300 loss 35.941525 loss_att 59.125057 loss_ctc 64.667435 loss_rnnt 27.427361 hw_loss 0.088760 lr 0.00069040 rank 6
2023-02-18 04:37:59,092 DEBUG TRAIN Batch 1/14300 loss 15.833890 loss_att 27.429977 loss_ctc 21.315119 loss_rnnt 12.783810 hw_loss 0.000059 lr 0.00069204 rank 1
2023-02-18 04:37:59,093 DEBUG TRAIN Batch 1/14300 loss 37.970207 loss_att 71.220444 loss_ctc 57.504356 loss_rnnt 28.715582 hw_loss 0.000049 lr 0.00069032 rank 5
2023-02-18 04:37:59,097 DEBUG TRAIN Batch 1/14300 loss 16.413651 loss_att 19.899235 loss_ctc 17.597893 loss_rnnt 15.437879 hw_loss 0.226418 lr 0.00069052 rank 7
2023-02-18 04:37:59,100 DEBUG TRAIN Batch 1/14300 loss 63.995159 loss_att 97.078423 loss_ctc 60.451679 loss_rnnt 57.850945 hw_loss 0.000059 lr 0.00068908 rank 3
2023-02-18 04:37:59,105 DEBUG TRAIN Batch 1/14300 loss 44.664120 loss_att 62.440102 loss_ctc 56.519619 loss_rnnt 39.528160 hw_loss 0.000056 lr 0.00069048 rank 2
2023-02-18 04:37:59,152 DEBUG TRAIN Batch 1/14300 loss 75.502487 loss_att 93.438400 loss_ctc 99.350792 loss_rnnt 68.735489 hw_loss 0.000072 lr 0.00069000 rank 0
2023-02-18 04:38:56,544 DEBUG TRAIN Batch 1/14400 loss 49.094532 loss_att 68.837860 loss_ctc 70.594658 loss_rnnt 42.192814 hw_loss 0.161936 lr 0.00069404 rank 1
2023-02-18 04:38:56,547 DEBUG TRAIN Batch 1/14400 loss 43.909122 loss_att 58.519279 loss_ctc 69.971375 loss_rnnt 37.425877 hw_loss 0.161708 lr 0.00069232 rank 5
2023-02-18 04:38:56,550 DEBUG TRAIN Batch 1/14400 loss 41.144413 loss_att 64.064270 loss_ctc 65.097771 loss_rnnt 33.348427 hw_loss 0.034192 lr 0.00069252 rank 7
2023-02-18 04:38:56,550 DEBUG TRAIN Batch 1/14400 loss 26.369528 loss_att 64.197029 loss_ctc 45.988003 loss_rnnt 16.083185 hw_loss 0.196960 lr 0.00069200 rank 0
2023-02-18 04:38:56,553 DEBUG TRAIN Batch 1/14400 loss 31.223497 loss_att 60.809544 loss_ctc 60.828682 loss_rnnt 21.358900 hw_loss 0.000054 lr 0.00069240 rank 6
2023-02-18 04:38:56,554 DEBUG TRAIN Batch 1/14400 loss 22.494852 loss_att 30.252937 loss_ctc 30.194519 loss_rnnt 19.804111 hw_loss 0.210937 lr 0.00069248 rank 2
2023-02-18 04:38:56,555 DEBUG TRAIN Batch 1/14400 loss 30.724987 loss_att 48.456829 loss_ctc 52.318798 loss_rnnt 24.242815 hw_loss 0.106176 lr 0.00069108 rank 3
2023-02-18 04:38:56,607 DEBUG TRAIN Batch 1/14400 loss 36.377323 loss_att 56.204597 loss_ctc 56.174847 loss_rnnt 29.772118 hw_loss 0.000150 lr 0.00069136 rank 4
2023-02-18 04:39:56,533 DEBUG TRAIN Batch 1/14500 loss 38.990604 loss_att 48.732269 loss_ctc 49.241840 loss_rnnt 35.650681 hw_loss 0.046421 lr 0.00069448 rank 2
2023-02-18 04:39:56,534 DEBUG TRAIN Batch 1/14500 loss 34.388802 loss_att 49.725983 loss_ctc 51.850491 loss_rnnt 28.993027 hw_loss 0.000207 lr 0.00069452 rank 7
2023-02-18 04:39:56,535 DEBUG TRAIN Batch 1/14500 loss 32.913761 loss_att 55.991425 loss_ctc 58.440834 loss_rnnt 24.894468 hw_loss 0.000280 lr 0.00069604 rank 1
2023-02-18 04:39:56,535 DEBUG TRAIN Batch 1/14500 loss 33.387718 loss_att 63.039474 loss_ctc 46.173042 loss_rnnt 25.729448 hw_loss 0.043516 lr 0.00069400 rank 0
2023-02-18 04:39:56,537 DEBUG TRAIN Batch 1/14500 loss 93.874863 loss_att 115.860809 loss_ctc 124.745438 loss_rnnt 85.361511 hw_loss 0.000171 lr 0.00069432 rank 5
2023-02-18 04:39:56,539 DEBUG TRAIN Batch 1/14500 loss 11.962710 loss_att 28.914188 loss_ctc 16.541748 loss_rnnt 7.961725 hw_loss 0.000285 lr 0.00069308 rank 3
2023-02-18 04:39:56,546 DEBUG TRAIN Batch 1/14500 loss 50.824745 loss_att 79.180275 loss_ctc 68.633049 loss_rnnt 42.779129 hw_loss 0.000137 lr 0.00069336 rank 4
2023-02-18 04:39:56,563 DEBUG TRAIN Batch 1/14500 loss 44.343636 loss_att 71.013718 loss_ctc 64.580147 loss_rnnt 36.311306 hw_loss 0.000208 lr 0.00069440 rank 6
2023-02-18 04:41:16,200 DEBUG TRAIN Batch 1/14600 loss 40.709114 loss_att 47.823822 loss_ctc 63.864731 loss_rnnt 36.198715 hw_loss 0.000072 lr 0.00069804 rank 1
2023-02-18 04:41:16,201 DEBUG TRAIN Batch 1/14600 loss 32.411217 loss_att 54.299507 loss_ctc 50.720413 loss_rnnt 25.525467 hw_loss 0.125372 lr 0.00069536 rank 4
2023-02-18 04:41:16,204 DEBUG TRAIN Batch 1/14600 loss 63.267284 loss_att 77.517090 loss_ctc 82.699532 loss_rnnt 57.826332 hw_loss 0.000041 lr 0.00069632 rank 5
2023-02-18 04:41:16,205 DEBUG TRAIN Batch 1/14600 loss 33.748661 loss_att 47.350811 loss_ctc 52.364815 loss_rnnt 28.505404 hw_loss 0.076252 lr 0.00069508 rank 3
2023-02-18 04:41:16,208 DEBUG TRAIN Batch 1/14600 loss 27.071388 loss_att 31.085907 loss_ctc 36.783939 loss_rnnt 24.973452 hw_loss 0.000049 lr 0.00069652 rank 7
2023-02-18 04:41:16,208 DEBUG TRAIN Batch 1/14600 loss 120.957672 loss_att 160.930344 loss_ctc 168.461441 loss_rnnt 106.583542 hw_loss 0.085799 lr 0.00069640 rank 6
2023-02-18 04:41:16,209 DEBUG TRAIN Batch 1/14600 loss 46.909725 loss_att 61.911858 loss_ctc 51.060692 loss_rnnt 43.243050 hw_loss 0.211481 lr 0.00069600 rank 0
2023-02-18 04:41:16,243 DEBUG TRAIN Batch 1/14600 loss 84.656479 loss_att 108.943939 loss_ctc 100.825592 loss_rnnt 77.578903 hw_loss 0.120375 lr 0.00069648 rank 2
2023-02-18 04:42:33,454 DEBUG TRAIN Batch 1/14700 loss 115.636017 loss_att 135.555298 loss_ctc 150.081589 loss_rnnt 107.042419 hw_loss 0.031878 lr 0.00069736 rank 4
2023-02-18 04:42:33,454 DEBUG TRAIN Batch 1/14700 loss 59.408546 loss_att 92.316833 loss_ctc 100.352844 loss_rnnt 47.328766 hw_loss 0.072901 lr 0.00069832 rank 5
2023-02-18 04:42:33,454 DEBUG TRAIN Batch 1/14700 loss 87.031479 loss_att 101.051376 loss_ctc 120.046623 loss_rnnt 79.759872 hw_loss 0.122997 lr 0.00070004 rank 1
2023-02-18 04:42:33,460 DEBUG TRAIN Batch 1/14700 loss 23.163664 loss_att 36.573456 loss_ctc 32.119129 loss_rnnt 19.287560 hw_loss 0.000155 lr 0.00069708 rank 3
2023-02-18 04:42:33,469 DEBUG TRAIN Batch 1/14700 loss 56.447536 loss_att 62.054863 loss_ctc 74.704597 loss_rnnt 52.845589 hw_loss 0.086644 lr 0.00069848 rank 2
2023-02-18 04:42:33,490 DEBUG TRAIN Batch 1/14700 loss 90.501335 loss_att 114.620216 loss_ctc 101.856087 loss_rnnt 84.135818 hw_loss 0.052064 lr 0.00069852 rank 7
2023-02-18 04:42:33,505 DEBUG TRAIN Batch 1/14700 loss 114.310066 loss_att 150.686218 loss_ctc 159.394302 loss_rnnt 101.023460 hw_loss 0.000276 lr 0.00069840 rank 6
2023-02-18 04:42:33,549 DEBUG TRAIN Batch 1/14700 loss 57.627735 loss_att 90.330933 loss_ctc 90.291481 loss_rnnt 46.731735 hw_loss 0.000362 lr 0.00069800 rank 0
2023-02-18 04:43:34,747 DEBUG TRAIN Batch 1/14800 loss 65.005775 loss_att 87.759415 loss_ctc 84.571640 loss_rnnt 57.815765 hw_loss 0.057189 lr 0.00070000 rank 0
2023-02-18 04:43:34,750 DEBUG TRAIN Batch 1/14800 loss 60.021011 loss_att 77.874992 loss_ctc 80.482597 loss_rnnt 53.721985 hw_loss 0.000041 lr 0.00069936 rank 4
2023-02-18 04:43:34,751 DEBUG TRAIN Batch 1/14800 loss 86.604149 loss_att 124.064682 loss_ctc 125.195198 loss_rnnt 73.966537 hw_loss 0.000064 lr 0.00070204 rank 1
2023-02-18 04:43:34,753 DEBUG TRAIN Batch 1/14800 loss 98.975662 loss_att 121.408440 loss_ctc 125.096558 loss_rnnt 90.971977 hw_loss 0.064400 lr 0.00070052 rank 7
2023-02-18 04:43:34,753 DEBUG TRAIN Batch 1/14800 loss 69.404793 loss_att 91.471489 loss_ctc 85.758545 loss_rnnt 62.810898 hw_loss 0.000087 lr 0.00070040 rank 6
2023-02-18 04:43:34,754 DEBUG TRAIN Batch 1/14800 loss 40.158581 loss_att 49.850666 loss_ctc 53.220592 loss_rnnt 36.324684 hw_loss 0.288522 lr 0.00070032 rank 5
2023-02-18 04:43:34,755 DEBUG TRAIN Batch 1/14800 loss 44.099575 loss_att 62.467953 loss_ctc 80.015091 loss_rnnt 35.636703 hw_loss 0.000863 lr 0.00069908 rank 3
2023-02-18 04:43:34,810 DEBUG TRAIN Batch 1/14800 loss 65.188385 loss_att 85.213013 loss_ctc 86.475777 loss_rnnt 58.286922 hw_loss 0.109152 lr 0.00070048 rank 2
2023-02-18 04:44:34,437 DEBUG TRAIN Batch 1/14900 loss 48.033154 loss_att 94.145836 loss_ctc 73.248886 loss_rnnt 35.376518 hw_loss 0.134997 lr 0.00070200 rank 0
2023-02-18 04:44:34,438 DEBUG TRAIN Batch 1/14900 loss 21.645174 loss_att 39.837002 loss_ctc 34.834988 loss_rnnt 16.186657 hw_loss 0.115333 lr 0.00070232 rank 5
2023-02-18 04:44:34,439 DEBUG TRAIN Batch 1/14900 loss 19.627790 loss_att 42.150955 loss_ctc 37.197857 loss_rnnt 12.710265 hw_loss 0.131655 lr 0.00070248 rank 2
2023-02-18 04:44:34,439 DEBUG TRAIN Batch 1/14900 loss 74.081291 loss_att 87.253052 loss_ctc 88.541542 loss_rnnt 69.441833 hw_loss 0.144503 lr 0.00070404 rank 1
2023-02-18 04:44:34,441 DEBUG TRAIN Batch 1/14900 loss 27.830185 loss_att 59.917397 loss_ctc 39.972111 loss_rnnt 19.793781 hw_loss 0.000073 lr 0.00070240 rank 6
2023-02-18 04:44:34,443 DEBUG TRAIN Batch 1/14900 loss 22.603195 loss_att 37.771118 loss_ctc 29.329521 loss_rnnt 18.617859 hw_loss 0.102950 lr 0.00070252 rank 7
2023-02-18 04:44:34,451 DEBUG TRAIN Batch 1/14900 loss 41.452641 loss_att 65.917328 loss_ctc 52.952198 loss_rnnt 35.022041 hw_loss 0.008230 lr 0.00070108 rank 3
2023-02-18 04:44:34,501 DEBUG TRAIN Batch 1/14900 loss 60.007214 loss_att 102.368805 loss_ctc 78.685860 loss_rnnt 48.983131 hw_loss 0.114893 lr 0.00070136 rank 4
2023-02-18 04:45:32,259 DEBUG TRAIN Batch 1/15000 loss 52.938313 loss_att 79.019768 loss_ctc 83.120285 loss_rnnt 43.697712 hw_loss 0.000089 lr 0.00070432 rank 5
2023-02-18 04:45:32,263 DEBUG TRAIN Batch 1/15000 loss 72.837540 loss_att 93.997185 loss_ctc 106.746002 loss_rnnt 64.050903 hw_loss 0.062965 lr 0.00070604 rank 1
2023-02-18 04:45:32,265 DEBUG TRAIN Batch 1/15000 loss 36.686672 loss_att 50.979622 loss_ctc 50.273659 loss_rnnt 32.016296 hw_loss 0.000357 lr 0.00070308 rank 3
2023-02-18 04:45:32,265 DEBUG TRAIN Batch 1/15000 loss 68.955109 loss_att 90.124969 loss_ctc 95.929176 loss_rnnt 61.067604 hw_loss 0.106857 lr 0.00070452 rank 7
2023-02-18 04:45:32,267 DEBUG TRAIN Batch 1/15000 loss 29.972620 loss_att 35.127167 loss_ctc 44.262623 loss_rnnt 27.036316 hw_loss 0.000114 lr 0.00070440 rank 6
2023-02-18 04:45:32,269 DEBUG TRAIN Batch 1/15000 loss 38.556950 loss_att 47.530655 loss_ctc 51.753899 loss_rnnt 34.984802 hw_loss 0.033394 lr 0.00070448 rank 2
2023-02-18 04:45:32,309 DEBUG TRAIN Batch 1/15000 loss 57.396381 loss_att 67.905151 loss_ctc 84.841011 loss_rnnt 51.608402 hw_loss 0.050511 lr 0.00070336 rank 4
2023-02-18 04:45:32,314 DEBUG TRAIN Batch 1/15000 loss 45.920475 loss_att 59.420570 loss_ctc 61.710918 loss_rnnt 41.092110 hw_loss 0.043042 lr 0.00070400 rank 0
2023-02-18 04:46:31,603 DEBUG TRAIN Batch 1/15100 loss 73.035400 loss_att 113.194344 loss_ctc 93.733910 loss_rnnt 62.223633 hw_loss 0.037850 lr 0.00070632 rank 5
2023-02-18 04:46:31,604 DEBUG TRAIN Batch 1/15100 loss 54.284470 loss_att 86.343292 loss_ctc 83.147934 loss_rnnt 44.024216 hw_loss 0.000055 lr 0.00070508 rank 3
2023-02-18 04:46:31,604 DEBUG TRAIN Batch 1/15100 loss 46.526299 loss_att 61.764866 loss_ctc 59.907555 loss_rnnt 41.694359 hw_loss 0.000116 lr 0.00070600 rank 0
2023-02-18 04:46:31,605 DEBUG TRAIN Batch 1/15100 loss 31.610306 loss_att 46.914803 loss_ctc 46.188160 loss_rnnt 26.605642 hw_loss 0.000096 lr 0.00070640 rank 6
2023-02-18 04:46:31,607 DEBUG TRAIN Batch 1/15100 loss 63.733315 loss_att 79.853737 loss_ctc 96.819778 loss_rnnt 56.097656 hw_loss 0.000084 lr 0.00070804 rank 1
2023-02-18 04:46:31,607 DEBUG TRAIN Batch 1/15100 loss 35.725765 loss_att 53.095726 loss_ctc 55.036884 loss_rnnt 29.631002 hw_loss 0.086167 lr 0.00070648 rank 2
2023-02-18 04:46:31,610 DEBUG TRAIN Batch 1/15100 loss 46.563774 loss_att 86.289719 loss_ctc 88.692604 loss_rnnt 32.843437 hw_loss 0.296188 lr 0.00070652 rank 7
2023-02-18 04:46:31,666 DEBUG TRAIN Batch 1/15100 loss 32.878525 loss_att 58.416794 loss_ctc 46.288368 loss_rnnt 25.982847 hw_loss 0.000076 lr 0.00070536 rank 4
2023-02-18 04:47:30,220 DEBUG TRAIN Batch 1/15200 loss 25.842226 loss_att 27.725599 loss_ctc 34.282257 loss_rnnt 24.274673 hw_loss 0.122886 lr 0.00070832 rank 5
2023-02-18 04:47:30,223 DEBUG TRAIN Batch 1/15200 loss 19.623299 loss_att 24.141521 loss_ctc 25.939121 loss_rnnt 17.751400 hw_loss 0.236520 lr 0.00070736 rank 4
2023-02-18 04:47:30,223 DEBUG TRAIN Batch 1/15200 loss 24.572477 loss_att 37.099052 loss_ctc 40.526806 loss_rnnt 19.840240 hw_loss 0.186895 lr 0.00070852 rank 7
2023-02-18 04:47:30,226 DEBUG TRAIN Batch 1/15200 loss 97.624504 loss_att 137.942093 loss_ctc 138.713745 loss_rnnt 84.082397 hw_loss 0.000045 lr 0.00070848 rank 2
2023-02-18 04:47:30,228 DEBUG TRAIN Batch 1/15200 loss 69.572113 loss_att 115.402634 loss_ctc 90.554634 loss_rnnt 57.483994 hw_loss 0.233157 lr 0.00070708 rank 3
2023-02-18 04:47:30,231 DEBUG TRAIN Batch 1/15200 loss 84.247192 loss_att 107.343277 loss_ctc 104.262970 loss_rnnt 76.959167 hw_loss 0.000056 lr 0.00070800 rank 0
2023-02-18 04:47:30,232 DEBUG TRAIN Batch 1/15200 loss 68.385605 loss_att 80.990318 loss_ctc 88.103386 loss_rnnt 63.235596 hw_loss 0.000044 lr 0.00070840 rank 6
2023-02-18 04:47:30,285 DEBUG TRAIN Batch 1/15200 loss 67.808296 loss_att 95.918121 loss_ctc 95.337814 loss_rnnt 58.426666 hw_loss 0.166988 lr 0.00071004 rank 1
2023-02-18 04:48:28,054 DEBUG TRAIN Batch 1/15300 loss 52.383457 loss_att 71.870125 loss_ctc 64.357193 loss_rnnt 46.840057 hw_loss 0.092940 lr 0.00071000 rank 0
2023-02-18 04:48:28,054 DEBUG TRAIN Batch 1/15300 loss 23.448318 loss_att 38.418163 loss_ctc 30.323818 loss_rnnt 19.476704 hw_loss 0.114214 lr 0.00070936 rank 4
2023-02-18 04:48:28,056 DEBUG TRAIN Batch 1/15300 loss 54.053261 loss_att 67.127930 loss_ctc 67.830597 loss_rnnt 49.552937 hw_loss 0.090768 lr 0.00071052 rank 7
2023-02-18 04:48:28,062 DEBUG TRAIN Batch 1/15300 loss 42.613400 loss_att 57.087765 loss_ctc 55.668541 loss_rnnt 37.868088 hw_loss 0.205790 lr 0.00071048 rank 2
2023-02-18 04:48:28,063 DEBUG TRAIN Batch 1/15300 loss 52.516159 loss_att 68.948143 loss_ctc 70.804993 loss_rnnt 46.791237 hw_loss 0.000022 lr 0.00071032 rank 5
2023-02-18 04:48:28,064 DEBUG TRAIN Batch 1/15300 loss 45.971626 loss_att 61.039223 loss_ctc 65.803528 loss_rnnt 40.313828 hw_loss 0.000044 lr 0.00070908 rank 3
2023-02-18 04:48:28,065 DEBUG TRAIN Batch 1/15300 loss 47.720184 loss_att 70.206505 loss_ctc 73.359657 loss_rnnt 39.804310 hw_loss 0.000033 lr 0.00071204 rank 1
2023-02-18 04:48:28,119 DEBUG TRAIN Batch 1/15300 loss 32.190460 loss_att 38.367706 loss_ctc 46.798660 loss_rnnt 28.945669 hw_loss 0.115460 lr 0.00071040 rank 6
2023-02-18 04:49:28,714 DEBUG TRAIN Batch 1/15400 loss 22.691002 loss_att 45.583302 loss_ctc 33.983444 loss_rnnt 16.606821 hw_loss 0.000114 lr 0.00071232 rank 5
2023-02-18 04:49:28,717 DEBUG TRAIN Batch 1/15400 loss 21.540844 loss_att 42.350143 loss_ctc 37.649063 loss_rnnt 15.231161 hw_loss 0.000114 lr 0.00071136 rank 4
2023-02-18 04:49:28,718 DEBUG TRAIN Batch 1/15400 loss 34.856789 loss_att 61.290718 loss_ctc 54.373787 loss_rnnt 26.891562 hw_loss 0.142834 lr 0.00071252 rank 7
2023-02-18 04:49:28,721 DEBUG TRAIN Batch 1/15400 loss 31.607616 loss_att 52.700245 loss_ctc 54.306686 loss_rnnt 24.362434 hw_loss 0.000209 lr 0.00071200 rank 0
2023-02-18 04:49:28,722 DEBUG TRAIN Batch 1/15400 loss 19.341158 loss_att 29.173363 loss_ctc 25.818995 loss_rnnt 16.510912 hw_loss 0.000174 lr 0.00071248 rank 2
2023-02-18 04:49:28,726 DEBUG TRAIN Batch 1/15400 loss 18.948360 loss_att 36.859848 loss_ctc 25.656300 loss_rnnt 14.442364 hw_loss 0.054951 lr 0.00071108 rank 3
2023-02-18 04:49:28,735 DEBUG TRAIN Batch 1/15400 loss 33.438869 loss_att 60.052750 loss_ctc 54.577843 loss_rnnt 25.297466 hw_loss 0.000183 lr 0.00071404 rank 1
2023-02-18 04:49:28,750 DEBUG TRAIN Batch 1/15400 loss 57.730324 loss_att 72.213303 loss_ctc 75.750031 loss_rnnt 52.399967 hw_loss 0.058362 lr 0.00071240 rank 6
2023-02-18 04:50:49,474 DEBUG TRAIN Batch 1/15500 loss 26.276821 loss_att 28.031422 loss_ctc 34.706688 loss_rnnt 24.715864 hw_loss 0.161351 lr 0.00071604 rank 1
2023-02-18 04:50:49,477 DEBUG TRAIN Batch 1/15500 loss 59.120739 loss_att 81.452118 loss_ctc 72.165543 loss_rnnt 52.764267 hw_loss 0.282921 lr 0.00071432 rank 5
2023-02-18 04:50:49,482 DEBUG TRAIN Batch 1/15500 loss 47.838772 loss_att 67.865242 loss_ctc 62.723190 loss_rnnt 41.794750 hw_loss 0.101508 lr 0.00071452 rank 7
2023-02-18 04:50:49,484 DEBUG TRAIN Batch 1/15500 loss 57.280079 loss_att 85.490524 loss_ctc 76.404846 loss_rnnt 49.042442 hw_loss 0.085466 lr 0.00071336 rank 4
2023-02-18 04:50:49,483 DEBUG TRAIN Batch 1/15500 loss 50.808208 loss_att 80.800789 loss_ctc 86.435394 loss_rnnt 40.032219 hw_loss 0.050970 lr 0.00071440 rank 6
2023-02-18 04:50:49,484 DEBUG TRAIN Batch 1/15500 loss 52.692883 loss_att 91.120590 loss_ctc 77.875610 loss_rnnt 41.564407 hw_loss 0.159823 lr 0.00071448 rank 2
2023-02-18 04:50:49,484 DEBUG TRAIN Batch 1/15500 loss 61.495327 loss_att 96.318977 loss_ctc 82.710655 loss_rnnt 51.628143 hw_loss 0.138270 lr 0.00071308 rank 3
2023-02-18 04:50:49,488 DEBUG TRAIN Batch 1/15500 loss 95.729050 loss_att 121.028519 loss_ctc 115.448509 loss_rnnt 88.039871 hw_loss 0.000035 lr 0.00071400 rank 0
2023-02-18 04:51:50,342 DEBUG TRAIN Batch 1/15600 loss 49.773586 loss_att 68.532410 loss_ctc 70.119659 loss_rnnt 43.308964 hw_loss 0.000083 lr 0.00071632 rank 5
2023-02-18 04:51:50,342 DEBUG TRAIN Batch 1/15600 loss 77.050629 loss_att 103.818352 loss_ctc 96.706123 loss_rnnt 68.982040 hw_loss 0.176831 lr 0.00071804 rank 1
2023-02-18 04:51:50,348 DEBUG TRAIN Batch 1/15600 loss 31.388340 loss_att 43.174446 loss_ctc 46.055138 loss_rnnt 26.938354 hw_loss 0.257232 lr 0.00071508 rank 3
2023-02-18 04:51:50,348 DEBUG TRAIN Batch 1/15600 loss 30.849501 loss_att 42.566216 loss_ctc 43.192406 loss_rnnt 26.860420 hw_loss 0.000033 lr 0.00071640 rank 6
2023-02-18 04:51:50,353 DEBUG TRAIN Batch 1/15600 loss 43.500210 loss_att 63.929314 loss_ctc 59.229576 loss_rnnt 37.317123 hw_loss 0.000027 lr 0.00071536 rank 4
2023-02-18 04:51:50,355 DEBUG TRAIN Batch 1/15600 loss 73.466347 loss_att 95.172623 loss_ctc 107.589325 loss_rnnt 64.575340 hw_loss 0.000034 lr 0.00071652 rank 7
2023-02-18 04:51:50,357 DEBUG TRAIN Batch 1/15600 loss 30.539560 loss_att 60.799835 loss_ctc 44.743126 loss_rnnt 22.573593 hw_loss 0.037695 lr 0.00071648 rank 2
2023-02-18 04:51:50,407 DEBUG TRAIN Batch 1/15600 loss 40.959732 loss_att 53.847176 loss_ctc 55.902229 loss_rnnt 36.389893 hw_loss 0.000039 lr 0.00071600 rank 0
2023-02-18 04:52:50,927 DEBUG TRAIN Batch 1/15700 loss 26.900677 loss_att 43.524139 loss_ctc 34.268890 loss_rnnt 22.593521 hw_loss 0.000061 lr 0.00071832 rank 5
2023-02-18 04:52:50,929 DEBUG TRAIN Batch 1/15700 loss 31.034519 loss_att 46.213833 loss_ctc 40.657784 loss_rnnt 26.666826 hw_loss 0.091363 lr 0.00071800 rank 0
2023-02-18 04:52:50,932 DEBUG TRAIN Batch 1/15700 loss 36.804398 loss_att 54.656708 loss_ctc 38.560577 loss_rnnt 32.999653 hw_loss 0.000228 lr 0.00071852 rank 7
2023-02-18 04:52:50,932 DEBUG TRAIN Batch 1/15700 loss 60.247807 loss_att 71.293396 loss_ctc 82.045189 loss_rnnt 55.132320 hw_loss 0.000101 lr 0.00072004 rank 1
2023-02-18 04:52:50,937 DEBUG TRAIN Batch 1/15700 loss 55.239719 loss_att 92.731720 loss_ctc 82.476013 loss_rnnt 44.033894 hw_loss 0.142346 lr 0.00071708 rank 3
2023-02-18 04:52:50,938 DEBUG TRAIN Batch 1/15700 loss 46.996502 loss_att 64.008499 loss_ctc 59.036942 loss_rnnt 41.932842 hw_loss 0.104749 lr 0.00071840 rank 6
2023-02-18 04:52:50,941 DEBUG TRAIN Batch 1/15700 loss 37.975880 loss_att 65.726906 loss_ctc 59.846107 loss_rnnt 29.470411 hw_loss 0.073554 lr 0.00071736 rank 4
2023-02-18 04:52:50,943 DEBUG TRAIN Batch 1/15700 loss 79.280060 loss_att 117.181160 loss_ctc 132.634735 loss_rnnt 64.553787 hw_loss 0.060171 lr 0.00071848 rank 2
2023-02-18 04:53:47,738 DEBUG TRAIN Batch 1/15800 loss 34.197838 loss_att 45.300266 loss_ctc 45.797928 loss_rnnt 30.395880 hw_loss 0.065239 lr 0.00072204 rank 1
2023-02-18 04:53:47,740 DEBUG TRAIN Batch 1/15800 loss 45.940022 loss_att 63.493530 loss_ctc 64.098068 loss_rnnt 40.008163 hw_loss 0.000161 lr 0.00072048 rank 2
2023-02-18 04:53:47,742 DEBUG TRAIN Batch 1/15800 loss 40.814857 loss_att 57.764263 loss_ctc 63.356594 loss_rnnt 34.407539 hw_loss 0.022257 lr 0.00072032 rank 5
2023-02-18 04:53:47,743 DEBUG TRAIN Batch 1/15800 loss 56.833225 loss_att 73.602364 loss_ctc 72.759758 loss_rnnt 51.325127 hw_loss 0.057631 lr 0.00072052 rank 7
2023-02-18 04:53:47,744 DEBUG TRAIN Batch 1/15800 loss 32.800045 loss_att 40.980782 loss_ctc 44.357498 loss_rnnt 29.592285 hw_loss 0.057411 lr 0.00071936 rank 4
2023-02-18 04:53:47,746 DEBUG TRAIN Batch 1/15800 loss 26.244513 loss_att 44.572311 loss_ctc 42.814323 loss_rnnt 20.369537 hw_loss 0.000200 lr 0.00072000 rank 0
2023-02-18 04:53:47,748 DEBUG TRAIN Batch 1/15800 loss 32.294521 loss_att 55.633343 loss_ctc 45.373417 loss_rnnt 25.755108 hw_loss 0.239618 lr 0.00071908 rank 3
2023-02-18 04:53:47,751 DEBUG TRAIN Batch 1/15800 loss 58.727062 loss_att 65.875198 loss_ctc 77.665283 loss_rnnt 54.757423 hw_loss 0.027965 lr 0.00072040 rank 6
2023-02-18 04:54:46,979 DEBUG TRAIN Batch 1/15900 loss 50.178627 loss_att 75.968460 loss_ctc 79.174904 loss_rnnt 41.069595 hw_loss 0.159170 lr 0.00072232 rank 5
2023-02-18 04:54:46,979 DEBUG TRAIN Batch 1/15900 loss 35.261826 loss_att 57.235676 loss_ctc 58.912323 loss_rnnt 27.676815 hw_loss 0.069081 lr 0.00072200 rank 0
2023-02-18 04:54:46,979 DEBUG TRAIN Batch 1/15900 loss 30.013075 loss_att 38.967812 loss_ctc 40.539604 loss_rnnt 26.807257 hw_loss 0.021252 lr 0.00072240 rank 6
2023-02-18 04:54:46,979 DEBUG TRAIN Batch 1/15900 loss 22.249216 loss_att 36.360363 loss_ctc 34.804382 loss_rnnt 17.752884 hw_loss 0.000146 lr 0.00072248 rank 2
2023-02-18 04:54:46,980 DEBUG TRAIN Batch 1/15900 loss 55.907532 loss_att 84.374542 loss_ctc 67.051659 loss_rnnt 48.728172 hw_loss 0.000146 lr 0.00072252 rank 7
2023-02-18 04:54:46,982 DEBUG TRAIN Batch 1/15900 loss 61.864727 loss_att 71.900681 loss_ctc 76.737442 loss_rnnt 57.869911 hw_loss 0.008609 lr 0.00072404 rank 1
2023-02-18 04:54:46,984 DEBUG TRAIN Batch 1/15900 loss 51.711121 loss_att 74.549103 loss_ctc 84.290939 loss_rnnt 42.737682 hw_loss 0.116002 lr 0.00072108 rank 3
2023-02-18 04:54:47,040 DEBUG TRAIN Batch 1/15900 loss 58.660027 loss_att 78.671478 loss_ctc 82.795410 loss_rnnt 51.439613 hw_loss 0.000130 lr 0.00072136 rank 4
2023-02-18 04:55:47,319 DEBUG TRAIN Batch 1/16000 loss 38.996620 loss_att 52.447418 loss_ctc 58.050274 loss_rnnt 33.765839 hw_loss 0.000250 lr 0.00072432 rank 5
2023-02-18 04:55:47,319 DEBUG TRAIN Batch 1/16000 loss 80.244110 loss_att 92.825798 loss_ctc 105.091957 loss_rnnt 74.355545 hw_loss 0.110978 lr 0.00072336 rank 4
2023-02-18 04:55:47,320 DEBUG TRAIN Batch 1/16000 loss 43.680798 loss_att 64.500397 loss_ctc 58.362602 loss_rnnt 37.559093 hw_loss 0.000386 lr 0.00072452 rank 7
2023-02-18 04:55:47,323 DEBUG TRAIN Batch 1/16000 loss 30.131836 loss_att 51.684834 loss_ctc 40.364990 loss_rnnt 24.456623 hw_loss 0.000356 lr 0.00072440 rank 6
2023-02-18 04:55:47,326 DEBUG TRAIN Batch 1/16000 loss 69.889412 loss_att 98.113647 loss_ctc 103.041351 loss_rnnt 59.824146 hw_loss 0.000297 lr 0.00072604 rank 1
2023-02-18 04:55:47,326 DEBUG TRAIN Batch 1/16000 loss 53.152725 loss_att 94.856384 loss_ctc 72.930199 loss_rnnt 42.174919 hw_loss 0.000146 lr 0.00072308 rank 3
2023-02-18 04:55:47,329 DEBUG TRAIN Batch 1/16000 loss 46.840923 loss_att 63.722267 loss_ctc 60.422394 loss_rnnt 41.628822 hw_loss 0.046815 lr 0.00072400 rank 0
2023-02-18 04:55:47,381 DEBUG TRAIN Batch 1/16000 loss 39.864845 loss_att 53.328590 loss_ctc 52.647503 loss_rnnt 35.318676 hw_loss 0.279493 lr 0.00072448 rank 2
2023-02-18 04:56:44,112 DEBUG TRAIN Batch 1/16100 loss 32.639565 loss_att 40.687759 loss_ctc 45.380272 loss_rnnt 29.216747 hw_loss 0.214542 lr 0.00072804 rank 1
2023-02-18 04:56:44,117 DEBUG TRAIN Batch 1/16100 loss 38.489521 loss_att 57.005554 loss_ctc 48.774963 loss_rnnt 33.342651 hw_loss 0.135507 lr 0.00072632 rank 5
2023-02-18 04:56:44,120 DEBUG TRAIN Batch 1/16100 loss 98.799652 loss_att 109.984581 loss_ctc 130.915878 loss_rnnt 92.280373 hw_loss 0.000234 lr 0.00072652 rank 7
2023-02-18 04:56:44,121 DEBUG TRAIN Batch 1/16100 loss 23.101616 loss_att 26.567379 loss_ctc 32.152077 loss_rnnt 21.169538 hw_loss 0.060370 lr 0.00072508 rank 3
2023-02-18 04:56:44,122 DEBUG TRAIN Batch 1/16100 loss 49.059978 loss_att 62.986755 loss_ctc 71.850754 loss_rnnt 43.235191 hw_loss 0.001250 lr 0.00072536 rank 4
2023-02-18 04:56:44,131 DEBUG TRAIN Batch 1/16100 loss 50.919075 loss_att 71.190102 loss_ctc 74.000458 loss_rnnt 43.714912 hw_loss 0.135816 lr 0.00072640 rank 6
2023-02-18 04:56:44,133 DEBUG TRAIN Batch 1/16100 loss 35.678356 loss_att 53.027653 loss_ctc 56.492332 loss_rnnt 29.433277 hw_loss 0.000036 lr 0.00072600 rank 0
2023-02-18 04:56:44,134 DEBUG TRAIN Batch 1/16100 loss 35.682480 loss_att 46.595810 loss_ctc 48.692364 loss_rnnt 31.697119 hw_loss 0.127583 lr 0.00072648 rank 2
2023-02-18 04:57:43,048 DEBUG TRAIN Batch 1/16200 loss 35.738926 loss_att 49.921730 loss_ctc 51.916241 loss_rnnt 30.745346 hw_loss 0.000082 lr 0.00072736 rank 4
2023-02-18 04:57:43,053 DEBUG TRAIN Batch 1/16200 loss 35.379055 loss_att 48.619713 loss_ctc 50.387516 loss_rnnt 30.604599 hw_loss 0.234744 lr 0.00072832 rank 5
2023-02-18 04:57:43,055 DEBUG TRAIN Batch 1/16200 loss 55.788006 loss_att 64.991776 loss_ctc 73.370689 loss_rnnt 51.546799 hw_loss 0.105174 lr 0.00072708 rank 3
2023-02-18 04:57:43,058 DEBUG TRAIN Batch 1/16200 loss 49.584679 loss_att 66.040092 loss_ctc 62.689117 loss_rnnt 44.491970 hw_loss 0.101940 lr 0.00072852 rank 7
2023-02-18 04:57:43,059 DEBUG TRAIN Batch 1/16200 loss 56.027885 loss_att 78.637054 loss_ctc 80.256058 loss_rnnt 48.212769 hw_loss 0.117861 lr 0.00072800 rank 0
2023-02-18 04:57:43,060 DEBUG TRAIN Batch 1/16200 loss 22.434439 loss_att 30.561138 loss_ctc 36.409550 loss_rnnt 18.897598 hw_loss 0.090283 lr 0.00072840 rank 6
2023-02-18 04:57:43,062 DEBUG TRAIN Batch 1/16200 loss 45.498562 loss_att 56.633266 loss_ctc 60.546120 loss_rnnt 41.212910 hw_loss 0.098194 lr 0.00073004 rank 1
2023-02-18 04:57:43,127 DEBUG TRAIN Batch 1/16200 loss 38.788334 loss_att 57.783058 loss_ctc 59.216866 loss_rnnt 32.218517 hw_loss 0.088254 lr 0.00072848 rank 2
2023-02-18 04:58:44,313 DEBUG TRAIN Batch 1/16300 loss 102.246994 loss_att 133.198608 loss_ctc 133.108658 loss_rnnt 91.941757 hw_loss 0.000050 lr 0.00073040 rank 6
2023-02-18 04:58:44,318 DEBUG TRAIN Batch 1/16300 loss 15.957218 loss_att 30.011337 loss_ctc 21.300863 loss_rnnt 12.346164 hw_loss 0.164519 lr 0.00073204 rank 1
2023-02-18 04:58:44,318 DEBUG TRAIN Batch 1/16300 loss 36.172115 loss_att 73.749527 loss_ctc 41.947281 loss_rnnt 27.884689 hw_loss 0.003595 lr 0.00073032 rank 5
2023-02-18 04:58:44,319 DEBUG TRAIN Batch 1/16300 loss 73.651825 loss_att 72.647865 loss_ctc 79.398872 loss_rnnt 72.945419 hw_loss 0.264232 lr 0.00073052 rank 7
2023-02-18 04:58:44,319 DEBUG TRAIN Batch 1/16300 loss 63.392345 loss_att 76.646317 loss_ctc 84.938316 loss_rnnt 57.868729 hw_loss 0.000044 lr 0.00072936 rank 4
2023-02-18 04:58:44,322 DEBUG TRAIN Batch 1/16300 loss 22.237097 loss_att 38.967869 loss_ctc 27.625839 loss_rnnt 18.172415 hw_loss 0.000052 lr 0.00073048 rank 2
2023-02-18 04:58:44,323 DEBUG TRAIN Batch 1/16300 loss 25.481037 loss_att 45.739410 loss_ctc 38.801022 loss_rnnt 19.652624 hw_loss 0.001387 lr 0.00073000 rank 0
2023-02-18 04:58:44,326 DEBUG TRAIN Batch 1/16300 loss 72.496979 loss_att 98.171814 loss_ctc 116.837143 loss_rnnt 61.449947 hw_loss 0.000067 lr 0.00072908 rank 3
2023-02-18 05:00:02,442 DEBUG TRAIN Batch 1/16400 loss 43.310276 loss_att 62.417267 loss_ctc 61.823090 loss_rnnt 36.987625 hw_loss 0.061647 lr 0.00073232 rank 5
2023-02-18 05:00:02,450 DEBUG TRAIN Batch 1/16400 loss 40.925182 loss_att 56.778118 loss_ctc 50.331741 loss_rnnt 36.500359 hw_loss 0.000048 lr 0.00073200 rank 0
2023-02-18 05:00:02,452 DEBUG TRAIN Batch 1/16400 loss 21.858253 loss_att 45.939354 loss_ctc 31.826199 loss_rnnt 15.669266 hw_loss 0.081950 lr 0.00073252 rank 7
2023-02-18 05:00:02,452 DEBUG TRAIN Batch 1/16400 loss 54.512794 loss_att 63.035271 loss_ctc 76.224327 loss_rnnt 49.865669 hw_loss 0.089545 lr 0.00073404 rank 1
2023-02-18 05:00:02,453 DEBUG TRAIN Batch 1/16400 loss 38.704113 loss_att 49.782097 loss_ctc 57.391323 loss_rnnt 33.954941 hw_loss 0.078647 lr 0.00073248 rank 2
2023-02-18 05:00:02,456 DEBUG TRAIN Batch 1/16400 loss 31.712166 loss_att 46.785233 loss_ctc 47.979774 loss_rnnt 26.462479 hw_loss 0.123864 lr 0.00073136 rank 4
2023-02-18 05:00:02,457 DEBUG TRAIN Batch 1/16400 loss 65.914932 loss_att 89.037537 loss_ctc 78.896973 loss_rnnt 59.486904 hw_loss 0.136065 lr 0.00073240 rank 6
2023-02-18 05:00:02,460 DEBUG TRAIN Batch 1/16400 loss 47.756901 loss_att 63.835163 loss_ctc 64.384445 loss_rnnt 42.276592 hw_loss 0.089337 lr 0.00073108 rank 3
2023-02-18 05:01:02,832 DEBUG TRAIN Batch 1/16500 loss 56.131680 loss_att 81.324379 loss_ctc 74.761078 loss_rnnt 48.560207 hw_loss 0.091898 lr 0.00073432 rank 5
2023-02-18 05:01:02,839 DEBUG TRAIN Batch 1/16500 loss 19.030680 loss_att 29.402607 loss_ctc 31.532780 loss_rnnt 15.289322 hw_loss 0.000046 lr 0.00073604 rank 1
2023-02-18 05:01:02,840 DEBUG TRAIN Batch 1/16500 loss 29.371515 loss_att 41.706635 loss_ctc 49.926983 loss_rnnt 24.153059 hw_loss 0.020069 lr 0.00073400 rank 0
2023-02-18 05:01:02,845 DEBUG TRAIN Batch 1/16500 loss 41.985542 loss_att 52.384708 loss_ctc 65.137398 loss_rnnt 36.745361 hw_loss 0.137684 lr 0.00073440 rank 6
2023-02-18 05:01:02,845 DEBUG TRAIN Batch 1/16500 loss 37.478527 loss_att 59.010368 loss_ctc 55.568695 loss_rnnt 30.760113 hw_loss 0.000046 lr 0.00073452 rank 7
2023-02-18 05:01:02,846 DEBUG TRAIN Batch 1/16500 loss 34.777111 loss_att 48.854652 loss_ctc 53.232910 loss_rnnt 29.488438 hw_loss 0.023235 lr 0.00073308 rank 3
2023-02-18 05:01:02,846 DEBUG TRAIN Batch 1/16500 loss 45.203568 loss_att 55.475845 loss_ctc 66.390457 loss_rnnt 40.303452 hw_loss 0.038892 lr 0.00073336 rank 4
2023-02-18 05:01:02,903 DEBUG TRAIN Batch 1/16500 loss 77.634659 loss_att 100.337975 loss_ctc 98.639282 loss_rnnt 70.265572 hw_loss 0.052128 lr 0.00073448 rank 2
2023-02-18 05:02:02,791 DEBUG TRAIN Batch 1/16600 loss 75.981155 loss_att 126.355331 loss_ctc 125.730469 loss_rnnt 59.272987 hw_loss 0.000162 lr 0.00073536 rank 4
2023-02-18 05:02:02,793 DEBUG TRAIN Batch 1/16600 loss 63.005424 loss_att 81.045593 loss_ctc 116.047676 loss_rnnt 52.325031 hw_loss 0.000117 lr 0.00073632 rank 5
2023-02-18 05:02:02,795 DEBUG TRAIN Batch 1/16600 loss 75.183098 loss_att 95.244110 loss_ctc 105.562683 loss_rnnt 67.094818 hw_loss 0.047733 lr 0.00073648 rank 2
2023-02-18 05:02:02,796 DEBUG TRAIN Batch 1/16600 loss 34.608681 loss_att 46.934704 loss_ctc 36.495102 loss_rnnt 31.891916 hw_loss 0.000070 lr 0.00073508 rank 3
2023-02-18 05:02:02,797 DEBUG TRAIN Batch 1/16600 loss 37.762627 loss_att 51.365265 loss_ctc 49.398689 loss_rnnt 33.437027 hw_loss 0.100496 lr 0.00073804 rank 1
2023-02-18 05:02:02,807 DEBUG TRAIN Batch 1/16600 loss 74.450966 loss_att 88.852356 loss_ctc 92.491966 loss_rnnt 69.083252 hw_loss 0.153672 lr 0.00073652 rank 7
2023-02-18 05:02:02,819 DEBUG TRAIN Batch 1/16600 loss 21.376303 loss_att 31.917599 loss_ctc 33.049591 loss_rnnt 17.711498 hw_loss 0.000201 lr 0.00073600 rank 0
2023-02-18 05:02:02,831 DEBUG TRAIN Batch 1/16600 loss 53.645653 loss_att 67.723366 loss_ctc 72.456955 loss_rnnt 48.321850 hw_loss 0.000161 lr 0.00073640 rank 6
2023-02-18 05:03:01,346 DEBUG TRAIN Batch 1/16700 loss 25.575214 loss_att 40.955280 loss_ctc 39.054153 loss_rnnt 20.653658 hw_loss 0.090655 lr 0.00074004 rank 1
2023-02-18 05:03:01,346 DEBUG TRAIN Batch 1/16700 loss 36.256218 loss_att 57.476276 loss_ctc 60.336639 loss_rnnt 28.774281 hw_loss 0.050998 lr 0.00073832 rank 5
2023-02-18 05:03:01,346 DEBUG TRAIN Batch 1/16700 loss 38.858543 loss_att 53.187771 loss_ctc 58.097469 loss_rnnt 33.346306 hw_loss 0.152250 lr 0.00073708 rank 3
2023-02-18 05:03:01,348 DEBUG TRAIN Batch 1/16700 loss 21.713341 loss_att 39.191063 loss_ctc 32.683239 loss_rnnt 16.755104 hw_loss 0.000070 lr 0.00073800 rank 0
2023-02-18 05:03:01,348 DEBUG TRAIN Batch 1/16700 loss 19.491648 loss_att 33.196777 loss_ctc 24.584301 loss_rnnt 16.027666 hw_loss 0.082372 lr 0.00073852 rank 7
2023-02-18 05:03:01,353 DEBUG TRAIN Batch 1/16700 loss 50.212673 loss_att 56.273483 loss_ctc 66.194885 loss_rnnt 46.869511 hw_loss 0.000063 lr 0.00073848 rank 2
2023-02-18 05:03:01,360 DEBUG TRAIN Batch 1/16700 loss 23.482319 loss_att 26.571552 loss_ctc 33.360764 loss_rnnt 21.451681 hw_loss 0.179372 lr 0.00073840 rank 6
2023-02-18 05:03:01,370 DEBUG TRAIN Batch 1/16700 loss 18.419443 loss_att 33.551315 loss_ctc 26.814327 loss_rnnt 14.273724 hw_loss 0.000052 lr 0.00073736 rank 4
2023-02-18 05:04:01,653 DEBUG TRAIN Batch 1/16800 loss 56.457016 loss_att 73.017479 loss_ctc 79.402130 loss_rnnt 50.085484 hw_loss 0.000170 lr 0.00074204 rank 1
2023-02-18 05:04:01,656 DEBUG TRAIN Batch 1/16800 loss 65.870323 loss_att 89.075623 loss_ctc 84.474968 loss_rnnt 58.694370 hw_loss 0.101761 lr 0.00074052 rank 7
2023-02-18 05:04:01,657 DEBUG TRAIN Batch 1/16800 loss 65.728584 loss_att 90.048080 loss_ctc 96.514328 loss_rnnt 56.759850 hw_loss 0.000130 lr 0.00074032 rank 5
2023-02-18 05:04:01,659 DEBUG TRAIN Batch 1/16800 loss 31.678064 loss_att 49.473705 loss_ctc 58.831009 loss_rnnt 24.472980 hw_loss 0.047925 lr 0.00074040 rank 6
2023-02-18 05:04:01,663 DEBUG TRAIN Batch 1/16800 loss 33.834507 loss_att 48.775452 loss_ctc 44.611088 loss_rnnt 29.372728 hw_loss 0.068834 lr 0.00074048 rank 2
2023-02-18 05:04:01,669 DEBUG TRAIN Batch 1/16800 loss 33.543274 loss_att 50.570133 loss_ctc 46.205505 loss_rnnt 28.366411 hw_loss 0.155985 lr 0.00073908 rank 3
2023-02-18 05:04:01,688 DEBUG TRAIN Batch 1/16800 loss 54.332905 loss_att 66.872017 loss_ctc 75.042465 loss_rnnt 48.963593 hw_loss 0.187907 lr 0.00073936 rank 4
2023-02-18 05:04:01,688 DEBUG TRAIN Batch 1/16800 loss 56.766750 loss_att 68.780518 loss_ctc 67.973114 loss_rnnt 52.833374 hw_loss 0.068328 lr 0.00074000 rank 0
2023-02-18 05:05:00,561 DEBUG TRAIN Batch 1/16900 loss 22.737869 loss_att 40.507011 loss_ctc 35.159546 loss_rnnt 17.399481 hw_loss 0.240630 lr 0.00074200 rank 0
2023-02-18 05:05:00,562 DEBUG TRAIN Batch 1/16900 loss 40.061451 loss_att 51.178577 loss_ctc 57.557961 loss_rnnt 35.473648 hw_loss 0.059083 lr 0.00074136 rank 4
2023-02-18 05:05:00,563 DEBUG TRAIN Batch 1/16900 loss 91.577560 loss_att 118.425880 loss_ctc 111.694229 loss_rnnt 83.473877 hw_loss 0.097098 lr 0.00074232 rank 5
2023-02-18 05:05:00,563 DEBUG TRAIN Batch 1/16900 loss 38.903973 loss_att 46.071205 loss_ctc 52.752148 loss_rnnt 35.623962 hw_loss 0.000253 lr 0.00074404 rank 1
2023-02-18 05:05:00,565 DEBUG TRAIN Batch 1/16900 loss 25.923445 loss_att 36.157597 loss_ctc 35.623802 loss_rnnt 22.506170 hw_loss 0.144497 lr 0.00074252 rank 7
2023-02-18 05:05:00,565 DEBUG TRAIN Batch 1/16900 loss 73.522659 loss_att 99.688591 loss_ctc 103.869873 loss_rnnt 64.210617 hw_loss 0.061040 lr 0.00074248 rank 2
2023-02-18 05:05:00,570 DEBUG TRAIN Batch 1/16900 loss 33.502213 loss_att 54.995888 loss_ctc 43.201969 loss_rnnt 27.908777 hw_loss 0.002610 lr 0.00074108 rank 3
2023-02-18 05:05:00,626 DEBUG TRAIN Batch 1/16900 loss 32.172138 loss_att 42.817184 loss_ctc 44.856289 loss_rnnt 28.319935 hw_loss 0.059950 lr 0.00074240 rank 6
2023-02-18 05:05:59,432 DEBUG TRAIN Batch 1/17000 loss 30.289444 loss_att 51.807796 loss_ctc 49.737389 loss_rnnt 23.316082 hw_loss 0.143683 lr 0.00074336 rank 4
2023-02-18 05:05:59,433 DEBUG TRAIN Batch 1/17000 loss 41.616642 loss_att 55.667435 loss_ctc 54.862007 loss_rnnt 36.951519 hw_loss 0.166712 lr 0.00074432 rank 5
2023-02-18 05:05:59,433 DEBUG TRAIN Batch 1/17000 loss 87.197159 loss_att 98.918800 loss_ctc 111.849411 loss_rnnt 81.564621 hw_loss 0.002319 lr 0.00074604 rank 1
2023-02-18 05:05:59,435 DEBUG TRAIN Batch 1/17000 loss 36.733006 loss_att 40.857872 loss_ctc 51.117233 loss_rnnt 33.889595 hw_loss 0.188514 lr 0.00074440 rank 6
2023-02-18 05:05:59,436 DEBUG TRAIN Batch 1/17000 loss 32.052311 loss_att 48.498314 loss_ctc 40.310181 loss_rnnt 27.605076 hw_loss 0.106849 lr 0.00074448 rank 2
2023-02-18 05:05:59,439 DEBUG TRAIN Batch 1/17000 loss 155.957870 loss_att 183.605148 loss_ctc 177.880478 loss_rnnt 147.505371 hw_loss 0.000051 lr 0.00074400 rank 0
2023-02-18 05:05:59,440 DEBUG TRAIN Batch 1/17000 loss 72.361687 loss_att 89.278030 loss_ctc 81.786156 loss_rnnt 67.721802 hw_loss 0.000046 lr 0.00074452 rank 7
2023-02-18 05:05:59,442 DEBUG TRAIN Batch 1/17000 loss 29.950190 loss_att 41.135704 loss_ctc 52.367611 loss_rnnt 24.648260 hw_loss 0.142199 lr 0.00074308 rank 3
2023-02-18 05:07:00,319 DEBUG TRAIN Batch 1/17100 loss 27.472759 loss_att 36.590923 loss_ctc 35.637634 loss_rnnt 24.525242 hw_loss 0.066059 lr 0.00074640 rank 6
2023-02-18 05:07:00,319 DEBUG TRAIN Batch 1/17100 loss 26.757795 loss_att 32.584106 loss_ctc 42.765125 loss_rnnt 23.457832 hw_loss 0.000731 lr 0.00074536 rank 4
2023-02-18 05:07:00,320 DEBUG TRAIN Batch 1/17100 loss 64.038025 loss_att 70.845032 loss_ctc 89.086464 loss_rnnt 59.336483 hw_loss 0.000644 lr 0.00074804 rank 1
2023-02-18 05:07:00,321 DEBUG TRAIN Batch 1/17100 loss 37.523014 loss_att 50.584724 loss_ctc 51.519787 loss_rnnt 33.013065 hw_loss 0.058816 lr 0.00074600 rank 0
2023-02-18 05:07:00,323 DEBUG TRAIN Batch 1/17100 loss 59.139790 loss_att 73.291229 loss_ctc 89.383797 loss_rnnt 52.215710 hw_loss 0.114852 lr 0.00074652 rank 7
2023-02-18 05:07:00,324 DEBUG TRAIN Batch 1/17100 loss 26.409918 loss_att 44.350151 loss_ctc 38.594051 loss_rnnt 21.197126 hw_loss 0.000361 lr 0.00074508 rank 3
2023-02-18 05:07:00,325 DEBUG TRAIN Batch 1/17100 loss 25.882263 loss_att 54.934601 loss_ctc 50.579651 loss_rnnt 16.778519 hw_loss 0.000549 lr 0.00074632 rank 5
2023-02-18 05:07:00,384 DEBUG TRAIN Batch 1/17100 loss 28.042973 loss_att 42.066856 loss_ctc 43.343887 loss_rnnt 23.154562 hw_loss 0.081585 lr 0.00074648 rank 2
2023-02-18 05:08:18,975 DEBUG TRAIN Batch 1/17200 loss 40.861565 loss_att 54.532021 loss_ctc 62.052792 loss_rnnt 35.301941 hw_loss 0.000055 lr 0.00074832 rank 5
2023-02-18 05:08:18,979 DEBUG TRAIN Batch 1/17200 loss 21.844738 loss_att 33.621819 loss_ctc 34.361717 loss_rnnt 17.708490 hw_loss 0.209807 lr 0.00074852 rank 7
2023-02-18 05:08:18,983 DEBUG TRAIN Batch 1/17200 loss 84.562408 loss_att 89.093781 loss_ctc 90.052635 loss_rnnt 82.819901 hw_loss 0.195388 lr 0.00075004 rank 1
2023-02-18 05:08:18,984 DEBUG TRAIN Batch 1/17200 loss 129.131241 loss_att 138.068848 loss_ctc 167.179932 loss_rnnt 122.147903 hw_loss 0.229973 lr 0.00074800 rank 0
2023-02-18 05:08:18,988 DEBUG TRAIN Batch 1/17200 loss 45.497726 loss_att 74.162422 loss_ctc 55.690659 loss_rnnt 38.405701 hw_loss 0.000057 lr 0.00074840 rank 6
2023-02-18 05:08:18,987 DEBUG TRAIN Batch 1/17200 loss 37.178307 loss_att 67.182022 loss_ctc 46.818222 loss_rnnt 29.860640 hw_loss 0.059255 lr 0.00074708 rank 3
2023-02-18 05:08:18,990 DEBUG TRAIN Batch 1/17200 loss 67.724564 loss_att 82.810455 loss_ctc 81.496635 loss_rnnt 62.871071 hw_loss 0.000055 lr 0.00074848 rank 2
2023-02-18 05:08:19,046 DEBUG TRAIN Batch 1/17200 loss 33.501328 loss_att 46.023396 loss_ctc 50.767239 loss_rnnt 28.694769 hw_loss 0.000040 lr 0.00074736 rank 4
2023-02-18 05:09:37,682 DEBUG TRAIN Batch 1/17300 loss 34.668476 loss_att 50.819363 loss_ctc 45.479057 loss_rnnt 29.922544 hw_loss 0.139387 lr 0.00074908 rank 3
2023-02-18 05:09:37,685 DEBUG TRAIN Batch 1/17300 loss 30.648497 loss_att 34.393585 loss_ctc 38.577633 loss_rnnt 28.727856 hw_loss 0.214510 lr 0.00075000 rank 0
2023-02-18 05:09:37,686 DEBUG TRAIN Batch 1/17300 loss 26.545877 loss_att 39.925411 loss_ctc 50.485687 loss_rnnt 20.624418 hw_loss 0.100459 lr 0.00075032 rank 5
2023-02-18 05:09:37,688 DEBUG TRAIN Batch 1/17300 loss 22.787167 loss_att 37.843239 loss_ctc 32.269978 loss_rnnt 18.506729 hw_loss 0.009091 lr 0.00075204 rank 1
2023-02-18 05:09:37,688 DEBUG TRAIN Batch 1/17300 loss 42.101765 loss_att 58.305954 loss_ctc 54.288719 loss_rnnt 37.191906 hw_loss 0.082676 lr 0.00075052 rank 7
2023-02-18 05:09:37,694 DEBUG TRAIN Batch 1/17300 loss 33.425591 loss_att 53.014709 loss_ctc 44.490154 loss_rnnt 27.982109 hw_loss 0.094469 lr 0.00075040 rank 6
2023-02-18 05:09:37,701 DEBUG TRAIN Batch 1/17300 loss 40.206520 loss_att 51.356579 loss_ctc 50.046906 loss_rnnt 36.634094 hw_loss 0.056925 lr 0.00074936 rank 4
2023-02-18 05:09:37,748 DEBUG TRAIN Batch 1/17300 loss 40.536751 loss_att 57.256615 loss_ctc 50.162979 loss_rnnt 35.849846 hw_loss 0.111437 lr 0.00075048 rank 2
2023-02-18 05:10:38,411 DEBUG TRAIN Batch 1/17400 loss 68.467697 loss_att 88.782677 loss_ctc 106.405907 loss_rnnt 59.257271 hw_loss 0.166872 lr 0.00075232 rank 5
2023-02-18 05:10:38,412 DEBUG TRAIN Batch 1/17400 loss 26.668413 loss_att 37.937645 loss_ctc 40.488632 loss_rnnt 22.571857 hw_loss 0.000023 lr 0.00075136 rank 4
2023-02-18 05:10:38,412 DEBUG TRAIN Batch 1/17400 loss 46.817616 loss_att 59.323418 loss_ctc 63.109001 loss_rnnt 42.082825 hw_loss 0.115207 lr 0.00075252 rank 7
2023-02-18 05:10:38,415 DEBUG TRAIN Batch 1/17400 loss 39.470966 loss_att 67.764343 loss_ctc 58.675785 loss_rnnt 31.190750 hw_loss 0.114187 lr 0.00075108 rank 3
2023-02-18 05:10:38,415 DEBUG TRAIN Batch 1/17400 loss 33.949051 loss_att 57.230370 loss_ctc 50.968376 loss_rnnt 27.023529 hw_loss 0.000026 lr 0.00075240 rank 6
2023-02-18 05:10:38,416 DEBUG TRAIN Batch 1/17400 loss 64.491570 loss_att 92.193329 loss_ctc 85.847878 loss_rnnt 56.066372 hw_loss 0.069997 lr 0.00075200 rank 0
2023-02-18 05:10:38,417 DEBUG TRAIN Batch 1/17400 loss 42.395947 loss_att 69.665550 loss_ctc 72.617081 loss_rnnt 32.864174 hw_loss 0.090688 lr 0.00075248 rank 2
2023-02-18 05:10:38,419 DEBUG TRAIN Batch 1/17400 loss 56.302811 loss_att 75.314819 loss_ctc 82.387383 loss_rnnt 49.022446 hw_loss 0.000038 lr 0.00075404 rank 1
2023-02-18 05:11:36,774 DEBUG TRAIN Batch 1/17500 loss 27.253662 loss_att 31.016691 loss_ctc 42.072575 loss_rnnt 24.437010 hw_loss 0.165360 lr 0.00075432 rank 5
2023-02-18 05:11:36,775 DEBUG TRAIN Batch 1/17500 loss 66.285851 loss_att 83.401222 loss_ctc 88.636467 loss_rnnt 59.750778 hw_loss 0.247339 lr 0.00075448 rank 2
2023-02-18 05:11:36,779 DEBUG TRAIN Batch 1/17500 loss 98.597153 loss_att 104.201767 loss_ctc 120.311630 loss_rnnt 94.519165 hw_loss 0.115881 lr 0.00075336 rank 4
2023-02-18 05:11:36,781 DEBUG TRAIN Batch 1/17500 loss 71.727478 loss_att 87.220108 loss_ctc 103.891899 loss_rnnt 64.340286 hw_loss 0.000142 lr 0.00075308 rank 3
2023-02-18 05:11:36,783 DEBUG TRAIN Batch 1/17500 loss 16.727940 loss_att 38.166458 loss_ctc 32.338676 loss_rnnt 10.358761 hw_loss 0.000080 lr 0.00075440 rank 6
2023-02-18 05:11:36,783 DEBUG TRAIN Batch 1/17500 loss 65.110596 loss_att 76.739929 loss_ctc 88.963165 loss_rnnt 59.604347 hw_loss 0.000081 lr 0.00075452 rank 7
2023-02-18 05:11:36,783 DEBUG TRAIN Batch 1/17500 loss 62.450924 loss_att 91.986717 loss_ctc 106.434959 loss_rnnt 50.679161 hw_loss 0.000126 lr 0.00075604 rank 1
2023-02-18 05:11:36,837 DEBUG TRAIN Batch 1/17500 loss 65.616859 loss_att 85.273933 loss_ctc 69.391678 loss_rnnt 61.182083 hw_loss 0.000088 lr 0.00075400 rank 0
2023-02-18 05:12:35,651 DEBUG TRAIN Batch 1/17600 loss 35.566635 loss_att 60.114567 loss_ctc 45.933243 loss_rnnt 29.245480 hw_loss 0.055044 lr 0.00075632 rank 5
2023-02-18 05:12:35,656 DEBUG TRAIN Batch 1/17600 loss 18.088539 loss_att 31.692701 loss_ctc 28.825367 loss_rnnt 13.936102 hw_loss 0.000052 lr 0.00075804 rank 1
2023-02-18 05:12:35,656 DEBUG TRAIN Batch 1/17600 loss 33.520607 loss_att 42.143208 loss_ctc 48.696449 loss_rnnt 29.754990 hw_loss 0.033099 lr 0.00075640 rank 6
2023-02-18 05:12:35,657 DEBUG TRAIN Batch 1/17600 loss 27.653635 loss_att 42.418404 loss_ctc 38.558372 loss_rnnt 23.226177 hw_loss 0.038512 lr 0.00075600 rank 0
2023-02-18 05:12:35,661 DEBUG TRAIN Batch 1/17600 loss 30.729345 loss_att 61.242767 loss_ctc 54.653461 loss_rnnt 21.382990 hw_loss 0.100847 lr 0.00075536 rank 4
2023-02-18 05:12:35,663 DEBUG TRAIN Batch 1/17600 loss 47.833084 loss_att 58.766830 loss_ctc 64.948425 loss_rnnt 43.364269 hw_loss 0.000035 lr 0.00075648 rank 2
2023-02-18 05:12:35,665 DEBUG TRAIN Batch 1/17600 loss 40.707035 loss_att 55.268471 loss_ctc 60.928619 loss_rnnt 35.098503 hw_loss 0.000059 lr 0.00075508 rank 3
2023-02-18 05:12:35,666 DEBUG TRAIN Batch 1/17600 loss 47.682560 loss_att 60.491676 loss_ctc 77.085175 loss_rnnt 41.141529 hw_loss 0.110348 lr 0.00075652 rank 7
2023-02-18 05:13:35,615 DEBUG TRAIN Batch 1/17700 loss 65.789772 loss_att 93.003601 loss_ctc 89.750961 loss_rnnt 57.101089 hw_loss 0.095788 lr 0.00075736 rank 4
2023-02-18 05:13:35,615 DEBUG TRAIN Batch 1/17700 loss 45.458542 loss_att 59.253349 loss_ctc 57.641880 loss_rnnt 41.018658 hw_loss 0.105895 lr 0.00075832 rank 5
2023-02-18 05:13:35,620 DEBUG TRAIN Batch 1/17700 loss 79.992012 loss_att 93.181641 loss_ctc 118.500923 loss_rnnt 72.219505 hw_loss 0.000111 lr 0.00076004 rank 1
2023-02-18 05:13:35,620 DEBUG TRAIN Batch 1/17700 loss 58.935539 loss_att 100.705620 loss_ctc 90.140106 loss_rnnt 46.229214 hw_loss 0.359438 lr 0.00075852 rank 7
2023-02-18 05:13:35,626 DEBUG TRAIN Batch 1/17700 loss 27.588501 loss_att 41.719620 loss_ctc 37.978848 loss_rnnt 23.319550 hw_loss 0.107526 lr 0.00075800 rank 0
2023-02-18 05:13:35,625 DEBUG TRAIN Batch 1/17700 loss 20.208332 loss_att 39.632645 loss_ctc 31.833513 loss_rnnt 14.699152 hw_loss 0.139300 lr 0.00075848 rank 2
2023-02-18 05:13:35,629 DEBUG TRAIN Batch 1/17700 loss 45.471218 loss_att 73.514885 loss_ctc 74.716217 loss_rnnt 35.934006 hw_loss 0.054642 lr 0.00075708 rank 3
2023-02-18 05:13:35,682 DEBUG TRAIN Batch 1/17700 loss 35.079903 loss_att 42.168053 loss_ctc 45.622032 loss_rnnt 32.201675 hw_loss 0.103090 lr 0.00075840 rank 6
2023-02-18 05:14:33,758 DEBUG TRAIN Batch 1/17800 loss 33.062763 loss_att 59.225304 loss_ctc 53.422256 loss_rnnt 25.085646 hw_loss 0.056268 lr 0.00076032 rank 5
2023-02-18 05:14:33,759 DEBUG TRAIN Batch 1/17800 loss 36.361294 loss_att 56.300495 loss_ctc 52.267982 loss_rnnt 30.191639 hw_loss 0.114219 lr 0.00076040 rank 6
2023-02-18 05:14:33,760 DEBUG TRAIN Batch 1/17800 loss 57.128284 loss_att 93.169136 loss_ctc 72.300156 loss_rnnt 47.845421 hw_loss 0.097081 lr 0.00076000 rank 0
2023-02-18 05:14:33,762 DEBUG TRAIN Batch 1/17800 loss 28.460077 loss_att 39.048256 loss_ctc 35.437798 loss_rnnt 25.412033 hw_loss 0.000084 lr 0.00075936 rank 4
2023-02-18 05:14:33,762 DEBUG TRAIN Batch 1/17800 loss 25.581974 loss_att 46.845200 loss_ctc 37.541447 loss_rnnt 19.663235 hw_loss 0.134057 lr 0.00076048 rank 2
2023-02-18 05:14:33,764 DEBUG TRAIN Batch 1/17800 loss 7.997652 loss_att 22.927870 loss_ctc 18.406054 loss_rnnt 3.623731 hw_loss 0.000168 lr 0.00075908 rank 3
2023-02-18 05:14:33,770 DEBUG TRAIN Batch 1/17800 loss 14.586823 loss_att 17.029507 loss_ctc 19.554821 loss_rnnt 13.323687 hw_loss 0.210374 lr 0.00076204 rank 1
2023-02-18 05:14:33,774 DEBUG TRAIN Batch 1/17800 loss 19.283863 loss_att 30.661047 loss_ctc 24.413261 loss_rnnt 16.285540 hw_loss 0.073061 lr 0.00076052 rank 7
2023-02-18 05:15:33,136 DEBUG TRAIN Batch 1/17900 loss 45.919262 loss_att 79.135857 loss_ctc 77.635971 loss_rnnt 35.046970 hw_loss 0.000141 lr 0.00076252 rank 7
2023-02-18 05:15:33,139 DEBUG TRAIN Batch 1/17900 loss 55.997047 loss_att 70.018326 loss_ctc 64.417427 loss_rnnt 52.013393 hw_loss 0.106281 lr 0.00076404 rank 1
2023-02-18 05:15:33,139 DEBUG TRAIN Batch 1/17900 loss 49.120834 loss_att 70.546814 loss_ctc 70.325760 loss_rnnt 42.008247 hw_loss 0.000122 lr 0.00076232 rank 5
2023-02-18 05:15:33,139 DEBUG TRAIN Batch 1/17900 loss 60.529369 loss_att 73.946564 loss_ctc 81.435936 loss_rnnt 55.004929 hw_loss 0.100243 lr 0.00076136 rank 4
2023-02-18 05:15:33,140 DEBUG TRAIN Batch 1/17900 loss 44.514381 loss_att 59.525406 loss_ctc 62.418152 loss_rnnt 39.082310 hw_loss 0.080059 lr 0.00076248 rank 2
2023-02-18 05:15:33,142 DEBUG TRAIN Batch 1/17900 loss 26.298145 loss_att 38.610096 loss_ctc 33.934795 loss_rnnt 22.817448 hw_loss 0.000161 lr 0.00076200 rank 0
2023-02-18 05:15:33,146 DEBUG TRAIN Batch 1/17900 loss 26.394697 loss_att 42.062584 loss_ctc 34.707710 loss_rnnt 22.152603 hw_loss 0.000214 lr 0.00076108 rank 3
2023-02-18 05:15:33,197 DEBUG TRAIN Batch 1/17900 loss 33.632309 loss_att 45.216431 loss_ctc 54.541107 loss_rnnt 28.432804 hw_loss 0.177818 lr 0.00076240 rank 6
2023-02-18 05:16:34,175 DEBUG TRAIN Batch 1/18000 loss 30.367136 loss_att 40.637039 loss_ctc 49.471691 loss_rnnt 25.765831 hw_loss 0.000094 lr 0.00076452 rank 7
2023-02-18 05:16:34,178 DEBUG TRAIN Batch 1/18000 loss 32.453224 loss_att 52.660843 loss_ctc 53.862598 loss_rnnt 25.557072 hw_loss 0.000072 lr 0.00076432 rank 5
2023-02-18 05:16:34,179 DEBUG TRAIN Batch 1/18000 loss 24.343737 loss_att 34.330196 loss_ctc 45.405197 loss_rnnt 19.425627 hw_loss 0.211168 lr 0.00076440 rank 6
2023-02-18 05:16:34,182 DEBUG TRAIN Batch 1/18000 loss 24.561628 loss_att 38.040668 loss_ctc 38.823135 loss_rnnt 19.964254 hw_loss 0.000063 lr 0.00076336 rank 4
2023-02-18 05:16:34,188 DEBUG TRAIN Batch 1/18000 loss 50.430496 loss_att 65.335373 loss_ctc 67.434837 loss_rnnt 45.182220 hw_loss 0.000091 lr 0.00076448 rank 2
2023-02-18 05:16:34,190 DEBUG TRAIN Batch 1/18000 loss 56.600063 loss_att 74.640083 loss_ctc 79.180244 loss_rnnt 49.981300 hw_loss 0.000128 lr 0.00076604 rank 1
2023-02-18 05:16:34,192 DEBUG TRAIN Batch 1/18000 loss 25.637423 loss_att 36.683636 loss_ctc 34.823757 loss_rnnt 22.203278 hw_loss 0.000109 lr 0.00076308 rank 3
2023-02-18 05:16:34,237 DEBUG TRAIN Batch 1/18000 loss 97.137146 loss_att 109.402863 loss_ctc 116.822029 loss_rnnt 92.031631 hw_loss 0.051961 lr 0.00076400 rank 0
2023-02-18 05:17:53,807 DEBUG TRAIN Batch 1/18100 loss 23.776295 loss_att 30.959211 loss_ctc 30.627590 loss_rnnt 21.426109 hw_loss 0.000182 lr 0.00076632 rank 5
2023-02-18 05:17:53,808 DEBUG TRAIN Batch 1/18100 loss 41.082870 loss_att 48.257862 loss_ctc 62.065350 loss_rnnt 36.850090 hw_loss 0.000227 lr 0.00076508 rank 3
2023-02-18 05:17:53,812 DEBUG TRAIN Batch 1/18100 loss 48.208740 loss_att 66.031227 loss_ctc 63.419083 loss_rnnt 42.616104 hw_loss 0.000169 lr 0.00076536 rank 4
2023-02-18 05:17:53,813 DEBUG TRAIN Batch 1/18100 loss 45.559677 loss_att 67.160454 loss_ctc 65.301132 loss_rnnt 38.607174 hw_loss 0.000283 lr 0.00076600 rank 0
2023-02-18 05:17:53,813 DEBUG TRAIN Batch 1/18100 loss 62.422771 loss_att 70.687759 loss_ctc 74.854897 loss_rnnt 59.070793 hw_loss 0.077563 lr 0.00076640 rank 6
2023-02-18 05:17:53,819 DEBUG TRAIN Batch 1/18100 loss 46.674164 loss_att 57.153069 loss_ctc 64.856873 loss_rnnt 42.023712 hw_loss 0.244335 lr 0.00076652 rank 7
2023-02-18 05:17:53,844 DEBUG TRAIN Batch 1/18100 loss 33.692001 loss_att 58.911686 loss_ctc 54.349949 loss_rnnt 25.782143 hw_loss 0.209117 lr 0.00076648 rank 2
2023-02-18 05:17:53,872 DEBUG TRAIN Batch 1/18100 loss 19.307098 loss_att 29.790106 loss_ctc 27.499523 loss_rnnt 16.118019 hw_loss 0.000292 lr 0.00076804 rank 1
2023-02-18 05:19:08,584 DEBUG TRAIN Batch 1/18200 loss 47.208218 loss_att 80.541031 loss_ctc 67.260223 loss_rnnt 37.867996 hw_loss 0.000113 lr 0.00076736 rank 4
2023-02-18 05:19:08,584 DEBUG TRAIN Batch 1/18200 loss 36.484795 loss_att 49.044548 loss_ctc 63.488403 loss_rnnt 30.220566 hw_loss 0.284615 lr 0.00077004 rank 1
2023-02-18 05:19:08,586 DEBUG TRAIN Batch 1/18200 loss 43.181885 loss_att 48.297615 loss_ctc 60.181782 loss_rnnt 39.891994 hw_loss 0.000173 lr 0.00076800 rank 0
2023-02-18 05:19:08,587 DEBUG TRAIN Batch 1/18200 loss 20.976709 loss_att 35.830212 loss_ctc 30.728508 loss_rnnt 16.665981 hw_loss 0.074603 lr 0.00076852 rank 7
2023-02-18 05:19:08,587 DEBUG TRAIN Batch 1/18200 loss 42.317680 loss_att 65.020782 loss_ctc 55.276157 loss_rnnt 36.049210 hw_loss 0.000108 lr 0.00076708 rank 3
2023-02-18 05:19:08,587 DEBUG TRAIN Batch 1/18200 loss 24.810253 loss_att 37.436680 loss_ctc 34.466091 loss_rnnt 20.949114 hw_loss 0.090763 lr 0.00076832 rank 5
2023-02-18 05:19:08,590 DEBUG TRAIN Batch 1/18200 loss 26.688112 loss_att 44.247746 loss_ctc 36.827869 loss_rnnt 21.744034 hw_loss 0.150343 lr 0.00076840 rank 6
2023-02-18 05:19:08,652 DEBUG TRAIN Batch 1/18200 loss 39.488094 loss_att 52.641411 loss_ctc 51.433258 loss_rnnt 35.247395 hw_loss 0.032521 lr 0.00076848 rank 2
2023-02-18 05:20:07,933 DEBUG TRAIN Batch 1/18300 loss 40.720650 loss_att 59.822075 loss_ctc 67.079742 loss_rnnt 33.338417 hw_loss 0.088877 lr 0.00077000 rank 0
2023-02-18 05:20:07,933 DEBUG TRAIN Batch 1/18300 loss 13.676028 loss_att 28.329203 loss_ctc 24.431261 loss_rnnt 9.311342 hw_loss 0.000037 lr 0.00077040 rank 6
2023-02-18 05:20:07,935 DEBUG TRAIN Batch 1/18300 loss 29.328985 loss_att 50.486027 loss_ctc 45.382442 loss_rnnt 22.846434 hw_loss 0.207530 lr 0.00076936 rank 4
2023-02-18 05:20:07,935 DEBUG TRAIN Batch 1/18300 loss 32.218025 loss_att 59.066578 loss_ctc 46.187195 loss_rnnt 24.985735 hw_loss 0.000040 lr 0.00077052 rank 7
2023-02-18 05:20:07,936 DEBUG TRAIN Batch 1/18300 loss 39.713463 loss_att 54.649185 loss_ctc 45.265606 loss_rnnt 35.831501 hw_loss 0.289741 lr 0.00077032 rank 5
2023-02-18 05:20:07,945 DEBUG TRAIN Batch 1/18300 loss 47.987354 loss_att 81.094421 loss_ctc 65.315201 loss_rnnt 39.055550 hw_loss 0.000026 lr 0.00076908 rank 3
2023-02-18 05:20:07,984 DEBUG TRAIN Batch 1/18300 loss 39.077602 loss_att 57.060905 loss_ctc 63.309784 loss_rnnt 32.249962 hw_loss 0.000040 lr 0.00077048 rank 2
2023-02-18 05:20:08,022 DEBUG TRAIN Batch 1/18300 loss 17.003717 loss_att 44.520535 loss_ctc 30.927113 loss_rnnt 9.566541 hw_loss 0.145049 lr 0.00077204 rank 1
2023-02-18 05:21:05,555 DEBUG TRAIN Batch 1/18400 loss 65.633636 loss_att 86.453903 loss_ctc 106.142944 loss_rnnt 56.002762 hw_loss 0.122954 lr 0.00077232 rank 5
2023-02-18 05:21:05,556 DEBUG TRAIN Batch 1/18400 loss 44.291439 loss_att 63.320499 loss_ctc 58.810673 loss_rnnt 38.467819 hw_loss 0.153572 lr 0.00077252 rank 7
2023-02-18 05:21:05,556 DEBUG TRAIN Batch 1/18400 loss 28.344435 loss_att 31.426003 loss_ctc 40.422775 loss_rnnt 26.039722 hw_loss 0.146157 lr 0.00077240 rank 6
2023-02-18 05:21:05,557 DEBUG TRAIN Batch 1/18400 loss 29.302698 loss_att 37.942959 loss_ctc 45.874100 loss_rnnt 25.282143 hw_loss 0.155592 lr 0.00077404 rank 1
2023-02-18 05:21:05,560 DEBUG TRAIN Batch 1/18400 loss 43.583675 loss_att 58.257553 loss_ctc 56.516247 loss_rnnt 38.914722 hw_loss 0.018439 lr 0.00077136 rank 4
2023-02-18 05:21:05,565 DEBUG TRAIN Batch 1/18400 loss 75.286736 loss_att 89.604332 loss_ctc 110.704170 loss_rnnt 67.655586 hw_loss 0.084936 lr 0.00077248 rank 2
2023-02-18 05:21:05,566 DEBUG TRAIN Batch 1/18400 loss 43.705711 loss_att 71.315880 loss_ctc 66.616005 loss_rnnt 35.127705 hw_loss 0.002366 lr 0.00077108 rank 3
2023-02-18 05:21:05,624 DEBUG TRAIN Batch 1/18400 loss 53.932945 loss_att 65.211090 loss_ctc 76.844803 loss_rnnt 48.622360 hw_loss 0.000072 lr 0.00077200 rank 0
2023-02-18 05:22:05,191 DEBUG TRAIN Batch 1/18500 loss 30.396566 loss_att 41.741791 loss_ctc 39.545753 loss_rnnt 26.822786 hw_loss 0.159081 lr 0.00077604 rank 1
2023-02-18 05:22:05,191 DEBUG TRAIN Batch 1/18500 loss 62.376724 loss_att 80.638535 loss_ctc 89.458138 loss_rnnt 55.072510 hw_loss 0.076865 lr 0.00077452 rank 7
2023-02-18 05:22:05,192 DEBUG TRAIN Batch 1/18500 loss 55.267597 loss_att 66.940315 loss_ctc 79.744888 loss_rnnt 49.581364 hw_loss 0.165088 lr 0.00077448 rank 2
2023-02-18 05:22:05,193 DEBUG TRAIN Batch 1/18500 loss 57.351032 loss_att 62.930664 loss_ctc 73.811508 loss_rnnt 53.966522 hw_loss 0.138487 lr 0.00077400 rank 0
2023-02-18 05:22:05,194 DEBUG TRAIN Batch 1/18500 loss 22.101704 loss_att 37.817829 loss_ctc 39.854389 loss_rnnt 16.591434 hw_loss 0.000040 lr 0.00077432 rank 5
2023-02-18 05:22:05,195 DEBUG TRAIN Batch 1/18500 loss 49.447876 loss_att 64.649582 loss_ctc 71.083359 loss_rnnt 43.499783 hw_loss 0.043160 lr 0.00077440 rank 6
2023-02-18 05:22:05,197 DEBUG TRAIN Batch 1/18500 loss 110.823433 loss_att 152.760010 loss_ctc 156.690796 loss_rnnt 96.213760 hw_loss 0.200065 lr 0.00077336 rank 4
2023-02-18 05:22:05,199 DEBUG TRAIN Batch 1/18500 loss 27.853125 loss_att 43.062019 loss_ctc 41.818390 loss_rnnt 22.898933 hw_loss 0.094458 lr 0.00077308 rank 3
2023-02-18 05:23:05,112 DEBUG TRAIN Batch 1/18600 loss 53.944221 loss_att 80.441254 loss_ctc 71.679970 loss_rnnt 46.165512 hw_loss 0.214761 lr 0.00077632 rank 5
2023-02-18 05:23:05,113 DEBUG TRAIN Batch 1/18600 loss 32.175594 loss_att 44.494717 loss_ctc 44.905140 loss_rnnt 28.014471 hw_loss 0.000044 lr 0.00077600 rank 0
2023-02-18 05:23:05,117 DEBUG TRAIN Batch 1/18600 loss 61.305637 loss_att 88.747963 loss_ctc 80.995750 loss_rnnt 53.191803 hw_loss 0.000034 lr 0.00077536 rank 4
2023-02-18 05:23:05,116 DEBUG TRAIN Batch 1/18600 loss 58.553993 loss_att 78.532280 loss_ctc 86.997253 loss_rnnt 50.765877 hw_loss 0.000038 lr 0.00077804 rank 1
2023-02-18 05:23:05,118 DEBUG TRAIN Batch 1/18600 loss 25.485125 loss_att 41.313560 loss_ctc 34.422058 loss_rnnt 21.127829 hw_loss 0.000035 lr 0.00077640 rank 6
2023-02-18 05:23:05,119 DEBUG TRAIN Batch 1/18600 loss 70.562569 loss_att 101.462883 loss_ctc 90.250778 loss_rnnt 61.709999 hw_loss 0.088894 lr 0.00077648 rank 2
2023-02-18 05:23:05,124 DEBUG TRAIN Batch 1/18600 loss 75.544029 loss_att 83.770126 loss_ctc 100.429810 loss_rnnt 70.580688 hw_loss 0.000029 lr 0.00077508 rank 3
2023-02-18 05:23:05,175 DEBUG TRAIN Batch 1/18600 loss 64.175179 loss_att 105.888878 loss_ctc 82.576874 loss_rnnt 53.378860 hw_loss 0.000038 lr 0.00077652 rank 7
2023-02-18 05:24:03,366 DEBUG TRAIN Batch 1/18700 loss 37.104202 loss_att 53.014694 loss_ctc 47.942818 loss_rnnt 32.422516 hw_loss 0.102075 lr 0.00077832 rank 5
2023-02-18 05:24:03,367 DEBUG TRAIN Batch 1/18700 loss 30.455116 loss_att 44.409599 loss_ctc 50.651649 loss_rnnt 24.906322 hw_loss 0.121928 lr 0.00078004 rank 1
2023-02-18 05:24:03,368 DEBUG TRAIN Batch 1/18700 loss 17.294397 loss_att 20.016863 loss_ctc 21.191322 loss_rnnt 16.181850 hw_loss 0.090869 lr 0.00077708 rank 3
2023-02-18 05:24:03,369 DEBUG TRAIN Batch 1/18700 loss 48.553944 loss_att 72.976410 loss_ctc 61.051468 loss_rnnt 41.967129 hw_loss 0.067473 lr 0.00077736 rank 4
2023-02-18 05:24:03,371 DEBUG TRAIN Batch 1/18700 loss 38.368633 loss_att 52.728088 loss_ctc 57.245838 loss_rnnt 32.904499 hw_loss 0.141150 lr 0.00077852 rank 7
2023-02-18 05:24:03,373 DEBUG TRAIN Batch 1/18700 loss 50.832237 loss_att 76.494003 loss_ctc 72.857658 loss_rnnt 42.763138 hw_loss 0.000041 lr 0.00077800 rank 0
2023-02-18 05:24:03,379 DEBUG TRAIN Batch 1/18700 loss 24.790556 loss_att 33.321003 loss_ctc 31.093369 loss_rnnt 22.207306 hw_loss 0.068973 lr 0.00077840 rank 6
2023-02-18 05:24:03,385 DEBUG TRAIN Batch 1/18700 loss 23.570721 loss_att 30.606842 loss_ctc 34.562939 loss_rnnt 20.695822 hw_loss 0.003838 lr 0.00077848 rank 2
2023-02-18 05:25:03,290 DEBUG TRAIN Batch 1/18800 loss 25.537401 loss_att 39.392494 loss_ctc 38.114819 loss_rnnt 21.046314 hw_loss 0.080772 lr 0.00078052 rank 7
2023-02-18 05:25:03,291 DEBUG TRAIN Batch 1/18800 loss 45.596943 loss_att 67.886955 loss_ctc 61.181507 loss_rnnt 39.038780 hw_loss 0.041659 lr 0.00078048 rank 2
2023-02-18 05:25:03,292 DEBUG TRAIN Batch 1/18800 loss 66.442406 loss_att 78.061707 loss_ctc 76.529045 loss_rnnt 62.773582 hw_loss 0.000147 lr 0.00078032 rank 5
2023-02-18 05:25:03,293 DEBUG TRAIN Batch 1/18800 loss 73.864136 loss_att 96.380890 loss_ctc 91.473892 loss_rnnt 66.909302 hw_loss 0.194086 lr 0.00078204 rank 1
2023-02-18 05:25:03,295 DEBUG TRAIN Batch 1/18800 loss 18.616585 loss_att 33.313049 loss_ctc 22.266277 loss_rnnt 15.153117 hw_loss 0.070403 lr 0.00078040 rank 6
2023-02-18 05:25:03,298 DEBUG TRAIN Batch 1/18800 loss 33.893787 loss_att 48.488998 loss_ctc 56.266079 loss_rnnt 27.991674 hw_loss 0.000178 lr 0.00077936 rank 4
2023-02-18 05:25:03,300 DEBUG TRAIN Batch 1/18800 loss 66.998184 loss_att 84.849731 loss_ctc 98.896408 loss_rnnt 59.046066 hw_loss 0.241340 lr 0.00077908 rank 3
2023-02-18 05:25:03,350 DEBUG TRAIN Batch 1/18800 loss 31.760218 loss_att 53.306358 loss_ctc 40.016785 loss_rnnt 26.302219 hw_loss 0.089797 lr 0.00078000 rank 0
2023-02-18 05:26:22,929 DEBUG TRAIN Batch 1/18900 loss 16.702545 loss_att 38.948471 loss_ctc 26.806873 loss_rnnt 10.906081 hw_loss 0.000069 lr 0.00078200 rank 0
2023-02-18 05:26:22,929 DEBUG TRAIN Batch 1/18900 loss 83.173752 loss_att 103.776566 loss_ctc 98.930740 loss_rnnt 76.918159 hw_loss 0.063933 lr 0.00078232 rank 5
2023-02-18 05:26:22,933 DEBUG TRAIN Batch 1/18900 loss 10.764890 loss_att 19.454208 loss_ctc 23.110472 loss_rnnt 7.271150 hw_loss 0.205872 lr 0.00078248 rank 2
2023-02-18 05:26:22,933 DEBUG TRAIN Batch 1/18900 loss 67.185638 loss_att 95.707047 loss_ctc 84.932335 loss_rnnt 59.115101 hw_loss 0.000062 lr 0.00078252 rank 7
2023-02-18 05:26:22,934 DEBUG TRAIN Batch 1/18900 loss 19.163729 loss_att 44.901440 loss_ctc 34.669010 loss_rnnt 11.899836 hw_loss 0.091836 lr 0.00078404 rank 1
2023-02-18 05:26:22,938 DEBUG TRAIN Batch 1/18900 loss 32.226795 loss_att 49.705620 loss_ctc 57.458813 loss_rnnt 25.366714 hw_loss 0.000089 lr 0.00078108 rank 3
2023-02-18 05:26:22,938 DEBUG TRAIN Batch 1/18900 loss 46.990517 loss_att 85.971512 loss_ctc 85.586716 loss_rnnt 34.048126 hw_loss 0.000060 lr 0.00078240 rank 6
2023-02-18 05:26:22,996 DEBUG TRAIN Batch 1/18900 loss 21.045948 loss_att 27.875340 loss_ctc 27.019455 loss_rnnt 18.883579 hw_loss 0.000041 lr 0.00078136 rank 4
2023-02-18 05:27:40,280 DEBUG TRAIN Batch 1/19000 loss 44.209324 loss_att 71.754303 loss_ctc 71.263222 loss_rnnt 35.093102 hw_loss 0.000078 lr 0.00078400 rank 0
2023-02-18 05:27:40,281 DEBUG TRAIN Batch 1/19000 loss 16.476757 loss_att 27.740232 loss_ctc 26.123371 loss_rnnt 12.911740 hw_loss 0.048948 lr 0.00078440 rank 6
2023-02-18 05:27:40,281 DEBUG TRAIN Batch 1/19000 loss 24.019215 loss_att 31.801556 loss_ctc 36.474068 loss_rnnt 20.749172 hw_loss 0.099237 lr 0.00078432 rank 5
2023-02-18 05:27:40,286 DEBUG TRAIN Batch 1/19000 loss 30.044605 loss_att 45.456032 loss_ctc 46.344872 loss_rnnt 24.788906 hw_loss 0.000087 lr 0.00078604 rank 1
2023-02-18 05:27:40,287 DEBUG TRAIN Batch 1/19000 loss 25.341112 loss_att 30.663815 loss_ctc 38.092548 loss_rnnt 22.522694 hw_loss 0.100654 lr 0.00078308 rank 3
2023-02-18 05:27:40,288 DEBUG TRAIN Batch 1/19000 loss 38.123917 loss_att 52.140198 loss_ctc 63.036636 loss_rnnt 31.845558 hw_loss 0.287642 lr 0.00078452 rank 7
2023-02-18 05:27:40,291 DEBUG TRAIN Batch 1/19000 loss 21.542492 loss_att 25.358475 loss_ctc 30.482519 loss_rnnt 19.469479 hw_loss 0.220902 lr 0.00078448 rank 2
2023-02-18 05:27:40,342 DEBUG TRAIN Batch 1/19000 loss 65.364182 loss_att 80.723007 loss_ctc 90.447731 loss_rnnt 58.918941 hw_loss 0.054381 lr 0.00078336 rank 4
2023-02-18 05:28:41,341 DEBUG TRAIN Batch 1/19100 loss 49.669220 loss_att 64.725876 loss_ctc 68.163612 loss_rnnt 44.147308 hw_loss 0.083742 lr 0.00078804 rank 1
2023-02-18 05:28:41,343 DEBUG TRAIN Batch 1/19100 loss 29.513874 loss_att 47.368423 loss_ctc 38.465546 loss_rnnt 24.749395 hw_loss 0.000029 lr 0.00078632 rank 5
2023-02-18 05:28:41,343 DEBUG TRAIN Batch 1/19100 loss 26.991022 loss_att 40.781033 loss_ctc 43.391720 loss_rnnt 22.002663 hw_loss 0.081743 lr 0.00078648 rank 2
2023-02-18 05:28:41,345 DEBUG TRAIN Batch 1/19100 loss 32.525303 loss_att 44.017574 loss_ctc 41.945995 loss_rnnt 28.970736 hw_loss 0.000034 lr 0.00078640 rank 6
2023-02-18 05:28:41,346 DEBUG TRAIN Batch 1/19100 loss 27.718838 loss_att 41.061245 loss_ctc 49.955421 loss_rnnt 22.085455 hw_loss 0.000041 lr 0.00078600 rank 0
2023-02-18 05:28:41,346 DEBUG TRAIN Batch 1/19100 loss 52.481895 loss_att 70.997849 loss_ctc 74.738586 loss_rnnt 45.716026 hw_loss 0.178359 lr 0.00078508 rank 3
2023-02-18 05:28:41,350 DEBUG TRAIN Batch 1/19100 loss 28.408693 loss_att 43.389530 loss_ctc 25.278250 loss_rnnt 25.829899 hw_loss 0.000036 lr 0.00078652 rank 7
2023-02-18 05:28:41,415 DEBUG TRAIN Batch 1/19100 loss 84.797295 loss_att 103.116608 loss_ctc 110.624901 loss_rnnt 77.582443 hw_loss 0.201210 lr 0.00078536 rank 4
2023-02-18 05:29:43,486 DEBUG TRAIN Batch 1/19200 loss 49.799309 loss_att 69.907349 loss_ctc 63.310848 loss_rnnt 43.976135 hw_loss 0.000045 lr 0.00078852 rank 7
2023-02-18 05:29:43,490 DEBUG TRAIN Batch 1/19200 loss 38.981598 loss_att 72.438690 loss_ctc 58.630211 loss_rnnt 29.622383 hw_loss 0.089963 lr 0.00078832 rank 5
2023-02-18 05:29:43,492 DEBUG TRAIN Batch 1/19200 loss 59.469128 loss_att 89.083603 loss_ctc 73.351509 loss_rnnt 51.695217 hw_loss 0.000053 lr 0.00079004 rank 1
2023-02-18 05:29:43,493 DEBUG TRAIN Batch 1/19200 loss 21.363379 loss_att 35.714062 loss_ctc 40.260479 loss_rnnt 15.973606 hw_loss 0.000044 lr 0.00078848 rank 2
2023-02-18 05:29:43,493 DEBUG TRAIN Batch 1/19200 loss 25.459427 loss_att 39.354614 loss_ctc 32.624367 loss_rnnt 21.725042 hw_loss 0.000045 lr 0.00078708 rank 3
2023-02-18 05:29:43,494 DEBUG TRAIN Batch 1/19200 loss 54.372734 loss_att 59.334221 loss_ctc 72.740128 loss_rnnt 50.894863 hw_loss 0.068612 lr 0.00078736 rank 4
2023-02-18 05:29:43,494 DEBUG TRAIN Batch 1/19200 loss 53.186089 loss_att 62.421890 loss_ctc 76.908440 loss_rnnt 48.118137 hw_loss 0.108388 lr 0.00078800 rank 0
2023-02-18 05:29:43,499 DEBUG TRAIN Batch 1/19200 loss 59.398415 loss_att 70.174271 loss_ctc 74.088104 loss_rnnt 55.284599 hw_loss 0.000043 lr 0.00078840 rank 6
2023-02-18 05:30:42,613 DEBUG TRAIN Batch 1/19300 loss 48.915993 loss_att 59.665245 loss_ctc 66.382401 loss_rnnt 44.347664 hw_loss 0.168041 lr 0.00079048 rank 2
2023-02-18 05:30:42,616 DEBUG TRAIN Batch 1/19300 loss 46.631420 loss_att 66.595634 loss_ctc 78.333138 loss_rnnt 38.369083 hw_loss 0.079866 lr 0.00079000 rank 0
2023-02-18 05:30:42,618 DEBUG TRAIN Batch 1/19300 loss 59.565678 loss_att 81.431946 loss_ctc 94.738930 loss_rnnt 50.502640 hw_loss 0.000032 lr 0.00079052 rank 7
2023-02-18 05:30:42,621 DEBUG TRAIN Batch 1/19300 loss 35.084019 loss_att 48.340424 loss_ctc 46.782860 loss_rnnt 30.836037 hw_loss 0.069103 lr 0.00078908 rank 3
2023-02-18 05:30:42,622 DEBUG TRAIN Batch 1/19300 loss 26.559231 loss_att 44.628571 loss_ctc 36.941643 loss_rnnt 21.497414 hw_loss 0.119300 lr 0.00079204 rank 1
2023-02-18 05:30:42,624 DEBUG TRAIN Batch 1/19300 loss 33.370419 loss_att 45.263741 loss_ctc 51.205715 loss_rnnt 28.514933 hw_loss 0.185212 lr 0.00079040 rank 6
2023-02-18 05:30:42,628 DEBUG TRAIN Batch 1/19300 loss 91.740341 loss_att 111.446632 loss_ctc 114.228729 loss_rnnt 84.800613 hw_loss 0.000031 lr 0.00079032 rank 5
2023-02-18 05:30:42,682 DEBUG TRAIN Batch 1/19300 loss 41.232574 loss_att 50.328995 loss_ctc 59.536400 loss_rnnt 36.972763 hw_loss 0.000027 lr 0.00078936 rank 4
2023-02-18 05:31:43,649 DEBUG TRAIN Batch 1/19400 loss 57.855503 loss_att 77.500870 loss_ctc 80.780022 loss_rnnt 50.835392 hw_loss 0.064560 lr 0.00079248 rank 2
2023-02-18 05:31:43,650 DEBUG TRAIN Batch 1/19400 loss 20.581886 loss_att 36.201569 loss_ctc 35.695290 loss_rnnt 15.412552 hw_loss 0.056773 lr 0.00079404 rank 1
2023-02-18 05:31:43,651 DEBUG TRAIN Batch 1/19400 loss 26.861961 loss_att 41.310333 loss_ctc 35.847916 loss_rnnt 22.774128 hw_loss 0.000059 lr 0.00079232 rank 5
2023-02-18 05:31:43,654 DEBUG TRAIN Batch 1/19400 loss 48.604244 loss_att 67.417854 loss_ctc 65.768372 loss_rnnt 42.486755 hw_loss 0.124158 lr 0.00079200 rank 0
2023-02-18 05:31:43,656 DEBUG TRAIN Batch 1/19400 loss 64.731064 loss_att 102.219391 loss_ctc 122.901115 loss_rnnt 49.477345 hw_loss 0.000075 lr 0.00079252 rank 7
2023-02-18 05:31:43,656 DEBUG TRAIN Batch 1/19400 loss 40.087349 loss_att 52.453690 loss_ctc 59.566887 loss_rnnt 34.957176 hw_loss 0.111811 lr 0.00079108 rank 3
2023-02-18 05:31:43,658 DEBUG TRAIN Batch 1/19400 loss 40.323853 loss_att 51.249535 loss_ctc 56.751198 loss_rnnt 35.869179 hw_loss 0.148547 lr 0.00079136 rank 4
2023-02-18 05:31:43,658 DEBUG TRAIN Batch 1/19400 loss 36.571091 loss_att 50.460739 loss_ctc 51.492294 loss_rnnt 31.776842 hw_loss 0.050297 lr 0.00079240 rank 6
2023-02-18 05:32:41,424 DEBUG TRAIN Batch 1/19500 loss 13.094667 loss_att 17.050179 loss_ctc 18.981812 loss_rnnt 11.425586 hw_loss 0.174426 lr 0.00079432 rank 5
2023-02-18 05:32:41,425 DEBUG TRAIN Batch 1/19500 loss 109.976761 loss_att 144.459167 loss_ctc 157.577087 loss_rnnt 96.731903 hw_loss 0.003123 lr 0.00079604 rank 1
2023-02-18 05:32:41,427 DEBUG TRAIN Batch 1/19500 loss 26.172710 loss_att 48.386673 loss_ctc 43.074356 loss_rnnt 19.469120 hw_loss 0.013585 lr 0.00079336 rank 4
2023-02-18 05:32:41,428 DEBUG TRAIN Batch 1/19500 loss 14.296703 loss_att 26.193680 loss_ctc 21.259537 loss_rnnt 10.988844 hw_loss 0.000162 lr 0.00079400 rank 0
2023-02-18 05:32:41,432 DEBUG TRAIN Batch 1/19500 loss 34.852642 loss_att 37.675793 loss_ctc 46.815239 loss_rnnt 32.540382 hw_loss 0.286152 lr 0.00079308 rank 3
2023-02-18 05:32:41,434 DEBUG TRAIN Batch 1/19500 loss 81.857536 loss_att 69.908989 loss_ctc 101.712288 loss_rnnt 81.545456 hw_loss 0.102164 lr 0.00079448 rank 2
2023-02-18 05:32:41,436 DEBUG TRAIN Batch 1/19500 loss 34.047123 loss_att 36.343670 loss_ctc 45.640141 loss_rnnt 31.926418 hw_loss 0.216868 lr 0.00079452 rank 7
2023-02-18 05:32:41,441 DEBUG TRAIN Batch 1/19500 loss 29.974005 loss_att 43.995003 loss_ctc 43.863937 loss_rnnt 25.317739 hw_loss 0.000140 lr 0.00079440 rank 6
2023-02-18 05:33:40,741 DEBUG TRAIN Batch 1/19600 loss 38.848362 loss_att 50.257668 loss_ctc 53.285992 loss_rnnt 34.641426 hw_loss 0.000106 lr 0.00079632 rank 5
2023-02-18 05:33:40,745 DEBUG TRAIN Batch 1/19600 loss 23.236374 loss_att 35.737717 loss_ctc 36.217941 loss_rnnt 19.005173 hw_loss 0.000107 lr 0.00079640 rank 6
2023-02-18 05:33:40,750 DEBUG TRAIN Batch 1/19600 loss 37.611156 loss_att 71.360458 loss_ctc 47.755390 loss_rnnt 29.457640 hw_loss 0.095796 lr 0.00079600 rank 0
2023-02-18 05:33:40,751 DEBUG TRAIN Batch 1/19600 loss 37.012394 loss_att 54.474449 loss_ctc 51.697937 loss_rnnt 31.496105 hw_loss 0.123384 lr 0.00079648 rank 2
2023-02-18 05:33:40,753 DEBUG TRAIN Batch 1/19600 loss 44.080540 loss_att 52.891510 loss_ctc 68.020668 loss_rnnt 39.126209 hw_loss 0.000220 lr 0.00079508 rank 3
2023-02-18 05:33:40,756 DEBUG TRAIN Batch 1/19600 loss 61.735130 loss_att 93.421082 loss_ctc 90.027267 loss_rnnt 51.599457 hw_loss 0.049117 lr 0.00079652 rank 7
2023-02-18 05:33:40,757 DEBUG TRAIN Batch 1/19600 loss 63.985996 loss_att 78.940620 loss_ctc 85.824989 loss_rnnt 58.029930 hw_loss 0.099895 lr 0.00079804 rank 1
2023-02-18 05:33:40,809 DEBUG TRAIN Batch 1/19600 loss 33.972389 loss_att 44.278969 loss_ctc 47.291252 loss_rnnt 30.135180 hw_loss 0.000073 lr 0.00079536 rank 4
2023-02-18 05:34:41,442 DEBUG TRAIN Batch 1/19700 loss 34.387348 loss_att 43.201527 loss_ctc 54.613499 loss_rnnt 29.920181 hw_loss 0.014087 lr 0.00079848 rank 2
2023-02-18 05:34:41,443 DEBUG TRAIN Batch 1/19700 loss 32.504974 loss_att 47.567417 loss_ctc 41.148518 loss_rnnt 28.259298 hw_loss 0.151340 lr 0.00079840 rank 6
2023-02-18 05:34:41,443 DEBUG TRAIN Batch 1/19700 loss 63.434372 loss_att 80.740433 loss_ctc 88.734947 loss_rnnt 56.538460 hw_loss 0.114909 lr 0.00079708 rank 3
2023-02-18 05:34:41,447 DEBUG TRAIN Batch 1/19700 loss 49.123798 loss_att 56.689568 loss_ctc 78.236427 loss_rnnt 43.728931 hw_loss 0.000047 lr 0.00080004 rank 1
2023-02-18 05:34:41,449 DEBUG TRAIN Batch 1/19700 loss 92.482224 loss_att 92.711792 loss_ctc 139.937927 loss_rnnt 86.035294 hw_loss 0.137970 lr 0.00079736 rank 4
2023-02-18 05:34:41,462 DEBUG TRAIN Batch 1/19700 loss 52.661068 loss_att 66.517639 loss_ctc 78.797134 loss_rnnt 46.404926 hw_loss 0.000031 lr 0.00079852 rank 7
2023-02-18 05:34:41,463 DEBUG TRAIN Batch 1/19700 loss 54.627243 loss_att 65.967331 loss_ctc 83.233925 loss_rnnt 48.544987 hw_loss 0.000027 lr 0.00079832 rank 5
2023-02-18 05:34:41,522 DEBUG TRAIN Batch 1/19700 loss 29.656260 loss_att 48.709766 loss_ctc 51.466114 loss_rnnt 22.885494 hw_loss 0.097654 lr 0.00079800 rank 0
2023-02-18 05:35:40,706 DEBUG TRAIN Batch 1/19800 loss 70.673874 loss_att 89.349640 loss_ctc 103.794083 loss_rnnt 62.490349 hw_loss 0.060648 lr 0.00079936 rank 4
2023-02-18 05:35:40,705 DEBUG TRAIN Batch 1/19800 loss 20.982574 loss_att 27.603363 loss_ctc 23.814178 loss_rnnt 19.237301 hw_loss 0.081692 lr 0.00080032 rank 5
2023-02-18 05:35:40,708 DEBUG TRAIN Batch 1/19800 loss 70.951019 loss_att 91.593513 loss_ctc 98.794296 loss_rnnt 63.110039 hw_loss 0.000088 lr 0.00080000 rank 0
2023-02-18 05:35:40,709 DEBUG TRAIN Batch 1/19800 loss 43.887905 loss_att 60.453308 loss_ctc 55.183899 loss_rnnt 39.028282 hw_loss 0.075768 lr 0.00079908 rank 3
2023-02-18 05:35:40,711 DEBUG TRAIN Batch 1/19800 loss 79.716911 loss_att 87.881271 loss_ctc 116.307396 loss_rnnt 73.084564 hw_loss 0.226393 lr 0.00080204 rank 1
2023-02-18 05:35:40,712 DEBUG TRAIN Batch 1/19800 loss 14.215962 loss_att 29.792236 loss_ctc 18.275932 loss_rnnt 10.559336 hw_loss 0.000078 lr 0.00080052 rank 7
2023-02-18 05:35:40,715 DEBUG TRAIN Batch 1/19800 loss 49.661453 loss_att 70.445038 loss_ctc 71.552292 loss_rnnt 42.508209 hw_loss 0.145779 lr 0.00080040 rank 6
2023-02-18 05:35:40,716 DEBUG TRAIN Batch 1/19800 loss 72.568825 loss_att 109.881767 loss_ctc 87.613197 loss_rnnt 63.066788 hw_loss 0.062894 lr 0.00080048 rank 2
2023-02-18 05:36:40,423 DEBUG TRAIN Batch 1/19900 loss 43.431595 loss_att 63.627686 loss_ctc 64.304085 loss_rnnt 36.609352 hw_loss 0.000061 lr 0.00080404 rank 1
2023-02-18 05:36:40,423 DEBUG TRAIN Batch 1/19900 loss 40.062855 loss_att 62.317638 loss_ctc 63.698540 loss_rnnt 32.376705 hw_loss 0.157067 lr 0.00080108 rank 3
2023-02-18 05:36:40,425 DEBUG TRAIN Batch 1/19900 loss 44.832687 loss_att 65.567574 loss_ctc 64.408112 loss_rnnt 38.075623 hw_loss 0.000053 lr 0.00080240 rank 6
2023-02-18 05:36:40,429 DEBUG TRAIN Batch 1/19900 loss 41.185619 loss_att 58.769844 loss_ctc 66.918068 loss_rnnt 34.206429 hw_loss 0.058798 lr 0.00080136 rank 4
2023-02-18 05:36:40,428 DEBUG TRAIN Batch 1/19900 loss 34.806889 loss_att 56.284977 loss_ctc 47.311909 loss_rnnt 28.812855 hw_loss 0.058268 lr 0.00080200 rank 0
2023-02-18 05:36:40,430 DEBUG TRAIN Batch 1/19900 loss 33.203781 loss_att 36.707390 loss_ctc 43.411480 loss_rnnt 31.065411 hw_loss 0.143672 lr 0.00080232 rank 5
2023-02-18 05:36:40,430 DEBUG TRAIN Batch 1/19900 loss 56.526321 loss_att 65.855606 loss_ctc 70.112297 loss_rnnt 52.838215 hw_loss 0.020218 lr 0.00080252 rank 7
2023-02-18 05:36:40,484 DEBUG TRAIN Batch 1/19900 loss 16.808289 loss_att 27.402012 loss_ctc 26.674911 loss_rnnt 13.295826 hw_loss 0.146564 lr 0.00080248 rank 2
2023-02-18 05:37:40,643 DEBUG TRAIN Batch 1/20000 loss 24.853630 loss_att 47.774483 loss_ctc 36.087914 loss_rnnt 18.678091 hw_loss 0.175249 lr 0.00080432 rank 5
2023-02-18 05:37:40,646 DEBUG TRAIN Batch 1/20000 loss 35.305443 loss_att 54.094666 loss_ctc 46.728889 loss_rnnt 29.943085 hw_loss 0.152599 lr 0.00080400 rank 0
2023-02-18 05:37:40,648 DEBUG TRAIN Batch 1/20000 loss 52.729088 loss_att 72.313889 loss_ctc 71.924683 loss_rnnt 46.252674 hw_loss 0.000075 lr 0.00080452 rank 7
2023-02-18 05:37:40,650 DEBUG TRAIN Batch 1/20000 loss 52.632851 loss_att 71.485909 loss_ctc 82.989388 loss_rnnt 44.731667 hw_loss 0.155680 lr 0.00080440 rank 6
2023-02-18 05:37:40,650 DEBUG TRAIN Batch 1/20000 loss 34.869453 loss_att 49.100891 loss_ctc 56.078949 loss_rnnt 29.194937 hw_loss 0.000551 lr 0.00080336 rank 4
2023-02-18 05:37:40,653 DEBUG TRAIN Batch 1/20000 loss 71.843498 loss_att 91.527679 loss_ctc 103.403435 loss_rnnt 63.598564 hw_loss 0.187702 lr 0.00080308 rank 3
2023-02-18 05:37:40,654 DEBUG TRAIN Batch 1/20000 loss 64.309555 loss_att 80.043587 loss_ctc 93.098717 loss_rnnt 57.323944 hw_loss 0.000449 lr 0.00080448 rank 2
2023-02-18 05:37:40,711 DEBUG TRAIN Batch 1/20000 loss 32.868092 loss_att 59.887527 loss_ctc 47.586300 loss_rnnt 25.394529 hw_loss 0.201082 lr 0.00080604 rank 1
2023-02-18 05:38:39,197 DEBUG TRAIN Batch 1/20100 loss 41.588299 loss_att 56.677284 loss_ctc 52.582424 loss_rnnt 37.086426 hw_loss 0.034115 lr 0.00080640 rank 6
2023-02-18 05:38:39,199 DEBUG TRAIN Batch 1/20100 loss 55.100796 loss_att 78.255135 loss_ctc 87.119568 loss_rnnt 46.200638 hw_loss 0.000229 lr 0.00080632 rank 5
2023-02-18 05:38:39,200 DEBUG TRAIN Batch 1/20100 loss 37.442822 loss_att 62.895248 loss_ctc 58.495518 loss_rnnt 29.496599 hw_loss 0.091329 lr 0.00080536 rank 4
2023-02-18 05:38:39,201 DEBUG TRAIN Batch 1/20100 loss 14.070914 loss_att 21.258030 loss_ctc 21.836147 loss_rnnt 11.597996 hw_loss 0.000246 lr 0.00080600 rank 0
2023-02-18 05:38:39,203 DEBUG TRAIN Batch 1/20100 loss 57.380005 loss_att 63.653076 loss_ctc 87.576210 loss_rnnt 52.037346 hw_loss 0.116033 lr 0.00080652 rank 7
2023-02-18 05:38:39,204 DEBUG TRAIN Batch 1/20100 loss 41.651173 loss_att 59.079292 loss_ctc 67.187225 loss_rnnt 34.760544 hw_loss 0.000376 lr 0.00080804 rank 1
2023-02-18 05:38:39,211 DEBUG TRAIN Batch 1/20100 loss 52.332916 loss_att 59.505363 loss_ctc 69.641396 loss_rnnt 48.508575 hw_loss 0.153854 lr 0.00080508 rank 3
2023-02-18 05:38:39,266 DEBUG TRAIN Batch 1/20100 loss 43.735100 loss_att 53.901577 loss_ctc 74.526192 loss_rnnt 37.549011 hw_loss 0.088724 lr 0.00080648 rank 2
2023-02-18 05:39:36,814 DEBUG TRAIN Batch 1/20200 loss 37.397038 loss_att 50.330475 loss_ctc 52.319717 loss_rnnt 32.720673 hw_loss 0.187477 lr 0.00080800 rank 0
2023-02-18 05:39:36,819 DEBUG TRAIN Batch 1/20200 loss 36.467453 loss_att 61.309818 loss_ctc 51.018833 loss_rnnt 29.518023 hw_loss 0.076451 lr 0.00080848 rank 2
2023-02-18 05:39:36,821 DEBUG TRAIN Batch 1/20200 loss 57.161797 loss_att 61.707214 loss_ctc 86.455124 loss_rnnt 52.196712 hw_loss 0.281675 lr 0.00080708 rank 3
2023-02-18 05:39:36,822 DEBUG TRAIN Batch 1/20200 loss 43.578892 loss_att 66.767944 loss_ctc 62.412788 loss_rnnt 36.392860 hw_loss 0.069441 lr 0.00081004 rank 1
2023-02-18 05:40:22,263 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 7
2023-02-18 05:40:22,263 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 3
2023-02-18 05:40:22,270 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 2
2023-02-18 05:40:22,271 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 5
2023-02-18 05:40:22,280 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 1
2023-02-18 05:40:22,285 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 0
2023-02-18 05:40:22,307 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 6
2023-02-18 05:40:22,327 DEBUG CV Batch 1/0 loss 7.474274 loss_att 7.342793 loss_ctc 11.877461 loss_rnnt 6.733466 hw_loss 0.337524 history loss 6.975989 rank 4
2023-02-18 05:40:32,708 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 3
2023-02-18 05:40:32,716 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 5
2023-02-18 05:40:32,901 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 0
2023-02-18 05:40:32,905 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 2
2023-02-18 05:40:32,938 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 6
2023-02-18 05:40:32,939 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 1
2023-02-18 05:40:32,977 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 7
2023-02-18 05:40:33,003 DEBUG CV Batch 1/100 loss 56.380272 loss_att 86.812164 loss_ctc 91.999435 loss_rnnt 45.489929 hw_loss 0.102643 history loss 17.536641 rank 4
2023-02-18 05:40:45,251 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 5
2023-02-18 05:40:45,372 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 3
2023-02-18 05:40:45,504 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 1
2023-02-18 05:40:45,597 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 6
2023-02-18 05:40:45,614 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 2
2023-02-18 05:40:45,668 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 0
2023-02-18 05:40:45,808 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 7
2023-02-18 05:40:45,928 DEBUG CV Batch 1/200 loss 18.565691 loss_att 22.425615 loss_ctc 26.934322 loss_rnnt 16.568441 hw_loss 0.205215 history loss 17.025584 rank 4
2023-02-18 05:40:58,740 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 5
2023-02-18 05:40:58,746 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 3
2023-02-18 05:40:59,097 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 1
2023-02-18 05:40:59,188 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 6
2023-02-18 05:40:59,337 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 2
2023-02-18 05:40:59,620 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 4
2023-02-18 05:40:59,715 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 7
2023-02-18 05:40:59,991 DEBUG CV Batch 1/300 loss 11.534956 loss_att 12.285069 loss_ctc 16.271076 loss_rnnt 10.601364 hw_loss 0.285163 history loss 18.666733 rank 0
2023-02-18 05:41:09,117 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 5
2023-02-18 05:41:09,135 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 3
2023-02-18 05:41:09,694 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 1
2023-02-18 05:41:09,785 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 2
2023-02-18 05:41:10,309 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 7
2023-02-18 05:41:10,394 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 6
2023-02-18 05:41:10,430 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 4
2023-02-18 05:41:10,960 DEBUG CV Batch 1/400 loss 47.307888 loss_att 54.448647 loss_ctc 59.116428 loss_rnnt 44.263546 hw_loss 0.078220 history loss 18.652103 rank 0
2023-02-18 05:41:24,499 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 5
2023-02-18 05:41:24,529 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 3
2023-02-18 05:41:24,975 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 1
2023-02-18 05:41:25,347 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 2
2023-02-18 05:41:25,973 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 7
2023-02-18 05:41:26,011 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 6
2023-02-18 05:41:26,104 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 4
2023-02-18 05:41:26,596 DEBUG CV Batch 1/500 loss 16.756630 loss_att 21.025549 loss_ctc 22.592731 loss_rnnt 15.035551 hw_loss 0.167152 history loss 18.688113 rank 0
2023-02-18 05:41:37,402 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 5
2023-02-18 05:41:37,691 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 3
2023-02-18 05:41:37,991 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 1
2023-02-18 05:41:38,279 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 2
2023-02-18 05:41:39,022 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 6
2023-02-18 05:41:39,186 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 7
2023-02-18 05:41:39,559 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 4
2023-02-18 05:41:39,655 DEBUG CV Batch 1/600 loss 50.378227 loss_att 124.066116 loss_ctc 63.951599 loss_rnnt 33.773552 hw_loss 0.107458 history loss 19.851553 rank 0
2023-02-18 05:41:48,383 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 5
2023-02-18 05:41:49,049 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 1
2023-02-18 05:41:49,171 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 3
2023-02-18 05:41:49,445 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 2
2023-02-18 05:41:50,941 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 7
2023-02-18 05:41:50,945 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 0
2023-02-18 05:41:51,250 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 4
2023-02-18 05:41:51,569 DEBUG CV Batch 1/700 loss 54.523636 loss_att 62.044121 loss_ctc 73.466316 loss_rnnt 50.381020 hw_loss 0.211555 history loss 19.722227 rank 6
2023-02-18 05:41:59,810 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 5
2023-02-18 05:42:00,443 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 3
2023-02-18 05:42:00,519 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 1
2023-02-18 05:42:00,925 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 2
2023-02-18 05:42:02,352 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 0
2023-02-18 05:42:02,553 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 7
2023-02-18 05:42:03,085 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 6
2023-02-18 05:42:03,531 DEBUG CV Batch 1/800 loss 32.936871 loss_att 35.988491 loss_ctc 59.097321 loss_rnnt 28.688343 hw_loss 0.281513 history loss 20.006187 rank 4
2023-02-18 05:42:12,324 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 5
2023-02-18 05:42:13,005 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 3
2023-02-18 05:42:13,149 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 1
2023-02-18 05:42:13,386 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 2
2023-02-18 05:42:14,722 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 0
2023-02-18 05:42:15,336 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 7
2023-02-18 05:42:15,822 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 6
2023-02-18 05:42:16,243 DEBUG CV Batch 1/900 loss 19.995779 loss_att 24.039154 loss_ctc 27.509680 loss_rnnt 18.024162 hw_loss 0.302041 history loss 20.925758 rank 4
2023-02-18 05:42:23,250 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 5
2023-02-18 05:42:24,041 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 3
2023-02-18 05:42:24,144 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 1
2023-02-18 05:42:24,204 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 2
2023-02-18 05:42:25,758 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 0
2023-02-18 05:42:26,481 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 7
2023-02-18 05:42:26,805 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 6
2023-02-18 05:42:27,424 DEBUG CV Batch 1/1000 loss 102.148773 loss_att 117.878929 loss_ctc 121.091362 loss_rnnt 96.416321 hw_loss 0.113885 history loss 22.467675 rank 4
2023-02-18 05:42:34,258 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 5
2023-02-18 05:42:35,060 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 3
2023-02-18 05:42:35,087 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 1
2023-02-18 05:42:35,360 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 2
2023-02-18 05:42:36,888 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 0
2023-02-18 05:42:37,953 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 6
2023-02-18 05:42:38,116 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 7
2023-02-18 05:42:38,991 DEBUG CV Batch 1/1100 loss 16.746595 loss_att 31.906216 loss_ctc 28.207161 loss_rnnt 12.114496 hw_loss 0.135188 history loss 22.539892 rank 4
2023-02-18 05:42:46,405 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 5
2023-02-18 05:42:47,172 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 3
2023-02-18 05:42:47,275 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 1
2023-02-18 05:42:47,751 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 2
2023-02-18 05:42:49,261 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 0
2023-02-18 05:42:50,032 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 6
2023-02-18 05:42:50,450 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 7
2023-02-18 05:42:51,446 DEBUG CV Batch 1/1200 loss 25.351742 loss_att 27.318752 loss_ctc 33.734047 loss_rnnt 23.784134 hw_loss 0.106057 history loss 23.085251 rank 4
2023-02-18 05:42:59,324 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 5
2023-02-18 05:43:00,461 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 3
2023-02-18 05:43:00,600 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 1
2023-02-18 05:43:01,020 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 2
2023-02-18 05:43:02,327 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 0
2023-02-18 05:43:03,044 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 6
2023-02-18 05:43:03,507 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 7
2023-02-18 05:43:05,164 DEBUG CV Batch 1/1300 loss 10.673928 loss_att 11.583464 loss_ctc 16.512266 loss_rnnt 9.542712 hw_loss 0.320370 history loss 24.205466 rank 4
2023-02-18 05:43:09,867 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 5
2023-02-18 05:43:11,250 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 3
2023-02-18 05:43:11,427 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 1
2023-02-18 05:43:11,725 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 2
2023-02-18 05:43:13,173 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 0
2023-02-18 05:43:13,727 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 6
2023-02-18 05:43:14,853 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 7
2023-02-18 05:43:15,862 DEBUG CV Batch 1/1400 loss 249.838730 loss_att 358.237549 loss_ctc 294.140381 loss_rnnt 222.240982 hw_loss 0.020780 history loss 25.059292 rank 4
2023-02-18 05:43:19,570 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 5
2023-02-18 05:43:21,297 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 3
2023-02-18 05:43:21,812 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 1
2023-02-18 05:43:22,002 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 2
2023-02-18 05:43:23,564 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 0
2023-02-18 05:43:23,942 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 6
2023-02-18 05:43:25,260 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 7
2023-02-18 05:43:26,254 DEBUG CV Batch 1/1500 loss 31.407116 loss_att 42.969490 loss_ctc 46.594532 loss_rnnt 27.050383 hw_loss 0.036136 history loss 24.959079 rank 4
2023-02-18 05:43:31,438 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 5
2023-02-18 05:43:33,495 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 3
2023-02-18 05:43:34,216 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 2
2023-02-18 05:43:34,595 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 1
2023-02-18 05:43:35,884 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 0
2023-02-18 05:43:36,304 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 6
2023-02-18 05:43:38,024 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 7
2023-02-18 05:43:39,002 DEBUG CV Batch 1/1600 loss 34.695320 loss_att 44.560860 loss_ctc 49.211906 loss_rnnt 30.738255 hw_loss 0.090767 history loss 24.514015 rank 4
2023-02-18 05:43:44,220 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 5
2023-02-18 05:43:46,584 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 3
2023-02-18 05:43:47,608 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 1
2023-02-18 05:43:47,609 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 2
2023-02-18 05:43:48,787 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 0
2023-02-18 05:43:49,496 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 6
2023-02-18 05:43:51,574 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 7
2023-02-18 05:43:52,352 DEBUG CV Batch 1/1700 loss 15.582284 loss_att 18.099337 loss_ctc 23.345774 loss_rnnt 13.959830 hw_loss 0.157335 history loss 24.215873 rank 4
2023-02-18 05:43:55,598 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 5
2023-02-18 05:43:58,332 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 3
2023-02-18 05:43:59,986 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 1
2023-02-18 05:44:00,197 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 2
2023-02-18 05:44:00,625 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 0
2023-02-18 05:44:01,386 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 6
2023-02-18 05:44:03,874 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 7
2023-02-18 05:44:04,363 DEBUG CV Batch 1/1800 loss 63.426662 loss_att 130.925491 loss_ctc 104.093628 loss_rnnt 44.364235 hw_loss 0.263244 history loss 24.282270 rank 4
2023-02-18 05:44:06,701 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 5
2023-02-18 05:44:09,778 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 3
2023-02-18 05:44:11,355 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 2
2023-02-18 05:44:11,864 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 1
2023-02-18 05:44:11,961 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 0
2023-02-18 05:44:12,871 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 6
2023-02-18 05:44:15,803 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 7
2023-02-18 05:44:16,084 DEBUG CV Batch 1/1900 loss 17.870609 loss_att 19.300703 loss_ctc 27.833214 loss_rnnt 16.181286 hw_loss 0.140541 history loss 23.697558 rank 4
2023-02-18 05:44:22,639 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 5
2023-02-18 05:44:25,890 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 3
2023-02-18 05:44:27,436 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 2
2023-02-18 05:44:27,957 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 1
2023-02-18 05:44:28,191 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 0
2023-02-18 05:44:28,546 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 6
2023-02-18 05:44:31,938 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 7
2023-02-18 05:44:32,107 DEBUG CV Batch 1/2000 loss 20.657734 loss_att 22.125841 loss_ctc 28.314703 loss_rnnt 19.255705 hw_loss 0.164021 history loss 23.611805 rank 4
2023-02-18 05:44:33,456 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 5
2023-02-18 05:44:36,713 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 3
2023-02-18 05:44:38,396 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 2
2023-02-18 05:44:38,966 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 0
2023-02-18 05:44:39,417 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 6
2023-02-18 05:44:39,555 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 1
2023-02-18 05:44:43,162 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 4
2023-02-18 05:44:43,854 DEBUG CV Batch 1/2100 loss 40.342789 loss_att 70.236298 loss_ctc 64.749275 loss_rnnt 31.109838 hw_loss 0.000092 history loss 23.670836 rank 7
2023-02-18 05:44:46,659 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 5
2023-02-18 05:44:49,757 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 3
2023-02-18 05:44:51,403 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 2
2023-02-18 05:44:52,368 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 0
2023-02-18 05:44:52,561 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 6
2023-02-18 05:44:52,678 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 1
2023-02-18 05:44:56,522 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 4
2023-02-18 05:44:57,776 DEBUG CV Batch 1/2200 loss 21.252897 loss_att 33.519165 loss_ctc 36.660751 loss_rnnt 16.669422 hw_loss 0.142195 history loss 23.446686 rank 7
2023-02-18 05:44:58,439 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 5
2023-02-18 05:45:01,471 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 3
2023-02-18 05:45:03,050 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 2
2023-02-18 05:45:04,047 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 0
2023-02-18 05:45:04,338 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 6
2023-02-18 05:45:04,400 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 1
2023-02-18 05:45:08,485 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 4
2023-02-18 05:45:09,968 DEBUG CV Batch 1/2300 loss 13.404497 loss_att 14.839513 loss_ctc 20.532137 loss_rnnt 12.046035 hw_loss 0.227076 history loss 23.296491 rank 7
2023-02-18 05:45:10,695 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 5
2023-02-18 05:45:13,609 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 3
2023-02-18 05:45:15,413 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 2
2023-02-18 05:45:16,347 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 0
2023-02-18 05:45:16,730 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 1
2023-02-18 05:45:17,166 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 6
2023-02-18 05:45:20,812 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 5
2023-02-18 05:45:20,954 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 4
2023-02-18 05:45:22,236 DEBUG CV Batch 1/2400 loss 11.527525 loss_att 12.821583 loss_ctc 15.166046 loss_rnnt 10.693554 hw_loss 0.168793 history loss 23.444967 rank 7
2023-02-18 05:45:23,575 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 3
2023-02-18 05:45:26,037 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 2
2023-02-18 05:45:26,935 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 0
2023-02-18 05:45:27,070 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 1
2023-02-18 05:45:27,368 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 6
2023-02-18 05:45:31,241 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 4
2023-02-18 05:45:32,335 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 5
2023-02-18 05:45:32,823 DEBUG CV Batch 1/2500 loss 69.507118 loss_att 77.406746 loss_ctc 98.093811 loss_rnnt 64.115578 hw_loss 0.000092 history loss 23.864124 rank 7
2023-02-18 05:45:35,137 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 3
2023-02-18 05:45:37,331 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 2
2023-02-18 05:45:38,380 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 1
2023-02-18 05:45:38,409 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 0
2023-02-18 05:45:38,682 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 6
2023-02-18 05:45:43,003 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 4
2023-02-18 05:45:44,606 DEBUG CV Batch 1/2600 loss 27.127823 loss_att 32.909004 loss_ctc 49.287903 loss_rnnt 22.946745 hw_loss 0.131561 history loss 23.953295 rank 7
2023-02-18 05:45:45,107 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 5
2023-02-18 05:45:47,987 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 3
2023-02-18 05:45:50,169 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 2
2023-02-18 05:45:50,928 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 0
2023-02-18 05:45:51,055 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 1
2023-02-18 05:45:51,414 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 6
2023-02-18 05:45:55,571 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 5
2023-02-18 05:45:55,822 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 4
2023-02-18 05:45:57,750 DEBUG CV Batch 1/2700 loss 32.168243 loss_att 34.972797 loss_ctc 42.196552 loss_rnnt 30.129683 hw_loss 0.263520 history loss 24.094853 rank 7
2023-02-18 05:45:58,578 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 3
2023-02-18 05:46:00,762 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 2
2023-02-18 05:46:01,636 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 1
2023-02-18 05:46:01,857 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 0
2023-02-18 05:46:02,367 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 6
2023-02-18 05:46:06,238 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 4
2023-02-18 05:46:06,735 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 5
2023-02-18 05:46:08,226 DEBUG CV Batch 1/2800 loss 47.250042 loss_att 62.674828 loss_ctc 70.217957 loss_rnnt 41.018795 hw_loss 0.157304 history loss 24.545459 rank 7
2023-02-18 05:46:09,725 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 3
2023-02-18 05:46:11,893 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 2
2023-02-18 05:46:12,823 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 1
2023-02-18 05:46:13,821 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 0
2023-02-18 05:46:13,933 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 6
2023-02-18 05:46:17,269 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 4
2023-02-18 05:46:17,962 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 5
2023-02-18 05:46:19,482 DEBUG CV Batch 1/2900 loss 64.335464 loss_att 75.812286 loss_ctc 88.217232 loss_rnnt 58.796387 hw_loss 0.111520 history loss 24.821504 rank 7
2023-02-18 05:46:21,075 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 3
2023-02-18 05:46:23,188 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 2
2023-02-18 05:46:24,104 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 1
2023-02-18 05:46:25,263 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 0
2023-02-18 05:46:26,975 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 6
2023-02-18 05:46:28,709 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 4
2023-02-18 05:46:30,265 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 5
2023-02-18 05:46:30,921 DEBUG CV Batch 1/3000 loss 25.870502 loss_att 31.099909 loss_ctc 41.311462 loss_rnnt 22.765579 hw_loss 0.000467 history loss 24.907957 rank 7
2023-02-18 05:46:33,573 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 3
2023-02-18 05:46:35,460 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 2
2023-02-18 05:46:36,420 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 1
2023-02-18 05:46:37,577 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 0
2023-02-18 05:46:39,853 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 6
2023-02-18 05:46:41,225 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 4
2023-02-18 05:46:42,981 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 5
2023-02-18 05:46:43,771 DEBUG CV Batch 1/3100 loss 23.094326 loss_att 26.980080 loss_ctc 37.997139 loss_rnnt 20.252876 hw_loss 0.144861 history loss 24.788239 rank 7
2023-02-18 05:46:46,398 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 3
2023-02-18 05:46:48,211 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 2
2023-02-18 05:46:49,418 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 1
2023-02-18 05:46:50,340 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 0
2023-02-18 05:46:52,398 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 6
2023-02-18 05:46:53,280 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 5
2023-02-18 05:46:53,986 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 4
2023-02-18 05:46:56,619 DEBUG CV Batch 1/3200 loss 6.341297 loss_att 8.914927 loss_ctc 8.884532 loss_rnnt 5.361101 hw_loss 0.236946 history loss 24.768813 rank 7
2023-02-18 05:46:57,056 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 3
2023-02-18 05:46:58,717 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 2
2023-02-18 05:47:01,006 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 0
2023-02-18 05:47:01,445 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 1
2023-02-18 05:47:02,978 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 6
2023-02-18 05:47:04,749 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 4
2023-02-18 05:47:06,190 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 5
2023-02-18 05:47:07,142 DEBUG CV Batch 1/3300 loss 35.285316 loss_att 46.972389 loss_ctc 54.875916 loss_rnnt 30.335550 hw_loss 0.000510 history loss 24.708248 rank 7
2023-02-18 05:47:09,824 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 3
2023-02-18 05:47:11,474 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 2
2023-02-18 05:47:13,661 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 0
2023-02-18 05:47:14,016 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 1
2023-02-18 05:47:15,605 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 6
2023-02-18 05:47:17,537 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 4
2023-02-18 05:47:20,396 DEBUG CV Batch 1/3400 loss 26.080116 loss_att 25.614761 loss_ctc 27.702679 loss_rnnt 25.859116 hw_loss 0.183246 history loss 24.469321 rank 7
2023-02-18 05:47:21,896 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 5
2023-02-18 05:47:25,727 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 3
2023-02-18 05:47:27,262 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 2
2023-02-18 05:47:29,574 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 0
2023-02-18 05:47:29,747 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 1
2023-02-18 05:47:31,303 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 6
2023-02-18 05:47:32,357 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 5
2023-02-18 05:47:33,253 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 4
2023-02-18 05:47:36,136 DEBUG CV Batch 1/3500 loss 130.306030 loss_att 289.399139 loss_ctc 182.874252 loss_rnnt 91.478249 hw_loss 0.000092 history loss 24.626075 rank 7
2023-02-18 05:47:36,405 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 3
2023-02-18 05:47:37,817 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 2
2023-02-18 05:47:40,136 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 0
2023-02-18 05:47:40,422 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 1
2023-02-18 05:47:42,462 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 6
2023-02-18 05:47:44,051 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 4
2023-02-18 05:47:46,264 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 5
2023-02-18 05:47:46,864 DEBUG CV Batch 1/3600 loss 34.407013 loss_att 56.180855 loss_ctc 45.570213 loss_rnnt 28.513229 hw_loss 0.094855 history loss 24.405115 rank 7
2023-02-18 05:47:50,325 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 3
2023-02-18 05:47:51,690 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 2
2023-02-18 05:47:53,752 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 0
2023-02-18 05:47:54,366 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 1
2023-02-18 05:47:56,373 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 6
2023-02-18 05:47:57,631 DEBUG CV Batch 1/3800 loss 13.809672 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 5
2023-02-18 05:47:57,961 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 4
2023-02-18 05:48:00,828 DEBUG CV Batch 1/3700 loss 27.693781 loss_att 28.584751 loss_ctc 39.446758 loss_rnnt 25.889214 hw_loss 0.111201 history loss 24.323058 rank 7
2023-02-18 05:48:01,750 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 3
2023-02-18 05:48:03,223 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 2
2023-02-18 05:48:04,902 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 0
2023-02-18 05:48:05,688 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 1
2023-02-18 05:48:06,460 INFO Epoch 1 CV info cv_loss 24.323925550488106
2023-02-18 05:48:06,461 INFO Epoch 2 TRAIN info lr 0.0008072
2023-02-18 05:48:06,465 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:07,603 DEBUG CV Batch 1/3800 loss 13.809672 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 6
2023-02-18 05:48:09,644 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 4
2023-02-18 05:48:10,563 INFO Epoch 1 CV info cv_loss 24.32392555064317
2023-02-18 05:48:10,563 INFO Epoch 2 TRAIN info lr 0.0008072800000000001
2023-02-18 05:48:10,566 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:11,977 INFO Epoch 1 CV info cv_loss 24.32392555274514
2023-02-18 05:48:11,978 INFO Epoch 2 TRAIN info lr 0.00080872
2023-02-18 05:48:11,983 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:12,215 DEBUG CV Batch 1/3800 loss 13.809671 loss_att 17.953697 loss_ctc 22.309364 loss_rnnt 11.783409 hw_loss 0.120312 history loss 24.301775 rank 7
2023-02-18 05:48:13,436 INFO Epoch 1 CV info cv_loss 24.32392555157355
2023-02-18 05:48:13,436 INFO Checkpoint: save to checkpoint exp/2_17_rnnt_bias_loss_2_class_1word_22/1.pt
2023-02-18 05:48:14,258 INFO Epoch 1 CV info cv_loss 24.32392555227995
2023-02-18 05:48:14,259 INFO Epoch 2 TRAIN info lr 0.00081012
2023-02-18 05:48:14,262 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:21,489 INFO Epoch 1 CV info cv_loss 24.323925553365395
2023-02-18 05:48:21,981 INFO Epoch 2 TRAIN info lr 0.00080824
2023-02-18 05:48:22,012 INFO Epoch 1 CV info cv_loss 24.323925551452945
2023-02-18 05:48:22,013 INFO Epoch 2 TRAIN info lr 0.0008072
2023-02-18 05:48:22,015 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:22,016 INFO Epoch 1 CV info cv_loss 24.323925551918137
2023-02-18 05:48:22,016 INFO Epoch 2 TRAIN info lr 0.0008060400000000001
2023-02-18 05:48:22,018 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:22,022 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:48:22,102 INFO Epoch 2 TRAIN info lr 0.00080932
2023-02-18 05:48:22,107 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-18 05:50:13,486 DEBUG TRAIN Batch 2/0 loss 14.578753 loss_att 17.060949 loss_ctc 21.290457 loss_rnnt 13.121402 hw_loss 0.123781 lr 0.00080828 rank 6
2023-02-18 05:50:13,489 DEBUG TRAIN Batch 2/0 loss 17.707148 loss_att 18.071793 loss_ctc 22.264778 loss_rnnt 16.935061 hw_loss 0.171512 lr 0.00080608 rank 4
2023-02-18 05:50:13,494 DEBUG TRAIN Batch 2/0 loss 27.768063 loss_att 27.853882 loss_ctc 34.735703 loss_rnnt 26.734665 hw_loss 0.163526 lr 0.00080732 rank 3
2023-02-18 05:50:13,503 DEBUG TRAIN Batch 2/0 loss 27.368992 loss_att 27.594208 loss_ctc 36.612610 loss_rnnt 25.995075 hw_loss 0.180733 lr 0.00080724 rank 7
2023-02-18 05:50:13,512 DEBUG TRAIN Batch 2/0 loss 18.559507 loss_att 17.734888 loss_ctc 22.229404 loss_rnnt 18.060034 hw_loss 0.328274 lr 0.00080724 rank 5
2023-02-18 05:50:13,533 DEBUG TRAIN Batch 2/0 loss 20.568037 loss_att 19.193487 loss_ctc 24.403803 loss_rnnt 20.211847 hw_loss 0.224370 lr 0.00081016 rank 1
2023-02-18 05:50:13,555 DEBUG TRAIN Batch 2/0 loss 25.630297 loss_att 26.818075 loss_ctc 31.289465 loss_rnnt 24.615690 hw_loss 0.042180 lr 0.00080876 rank 2
2023-02-18 05:50:13,655 DEBUG TRAIN Batch 2/0 loss 17.258125 loss_att 17.993288 loss_ctc 22.858109 loss_rnnt 16.263691 hw_loss 0.188881 lr 0.00080936 rank 0
2023-02-18 05:51:10,378 DEBUG TRAIN Batch 2/100 loss 35.336239 loss_att 63.519638 loss_ctc 40.965752 loss_rnnt 28.917347 hw_loss 0.059269 lr 0.00081136 rank 0
2023-02-18 05:51:10,379 DEBUG TRAIN Batch 2/100 loss 74.373520 loss_att 110.849030 loss_ctc 106.778000 loss_rnnt 62.707451 hw_loss 0.094444 lr 0.00080924 rank 5
2023-02-18 05:51:10,380 DEBUG TRAIN Batch 2/100 loss 35.222691 loss_att 48.135727 loss_ctc 54.969299 loss_rnnt 29.987061 hw_loss 0.037766 lr 0.00081076 rank 2
2023-02-18 05:51:10,381 DEBUG TRAIN Batch 2/100 loss 23.583387 loss_att 32.565506 loss_ctc 37.354179 loss_rnnt 19.895027 hw_loss 0.104680 lr 0.00080808 rank 4
2023-02-18 05:51:10,382 DEBUG TRAIN Batch 2/100 loss 48.346771 loss_att 56.367355 loss_ctc 65.475983 loss_rnnt 44.453594 hw_loss 0.009683 lr 0.00080932 rank 3
2023-02-18 05:51:10,383 DEBUG TRAIN Batch 2/100 loss 38.620159 loss_att 52.880669 loss_ctc 46.713036 loss_rnnt 34.563095 hw_loss 0.236085 lr 0.00080924 rank 7
2023-02-18 05:51:10,385 DEBUG TRAIN Batch 2/100 loss 52.244240 loss_att 70.721413 loss_ctc 70.928230 loss_rnnt 46.057365 hw_loss 0.000462 lr 0.00081028 rank 6
2023-02-18 05:51:10,387 DEBUG TRAIN Batch 2/100 loss 77.554375 loss_att 99.063797 loss_ctc 105.963104 loss_rnnt 69.464546 hw_loss 0.000213 lr 0.00081216 rank 1
2023-02-18 05:52:08,935 DEBUG TRAIN Batch 2/200 loss 102.278137 loss_att 129.592880 loss_ctc 143.475906 loss_rnnt 91.322128 hw_loss 0.000049 lr 0.00081228 rank 6
2023-02-18 05:52:08,937 DEBUG TRAIN Batch 2/200 loss 73.428131 loss_att 91.785591 loss_ctc 92.513634 loss_rnnt 67.090897 hw_loss 0.226897 lr 0.00081336 rank 0
2023-02-18 05:52:08,939 DEBUG TRAIN Batch 2/200 loss 63.975288 loss_att 85.925362 loss_ctc 81.168411 loss_rnnt 57.292843 hw_loss 0.000027 lr 0.00081124 rank 5
2023-02-18 05:52:08,941 DEBUG TRAIN Batch 2/200 loss 80.115402 loss_att 93.859695 loss_ctc 96.308067 loss_rnnt 75.153938 hw_loss 0.100469 lr 0.00081008 rank 4
2023-02-18 05:52:08,940 DEBUG TRAIN Batch 2/200 loss 80.884628 loss_att 102.965614 loss_ctc 112.813820 loss_rnnt 72.161064 hw_loss 0.094036 lr 0.00081416 rank 1
2023-02-18 05:52:08,942 DEBUG TRAIN Batch 2/200 loss 54.889431 loss_att 61.905373 loss_ctc 82.513367 loss_rnnt 49.803001 hw_loss 0.000106 lr 0.00081132 rank 3
2023-02-18 05:52:08,942 DEBUG TRAIN Batch 2/200 loss 42.073212 loss_att 80.649834 loss_ctc 62.978092 loss_rnnt 31.529623 hw_loss 0.076774 lr 0.00081124 rank 7
2023-02-18 05:52:08,949 DEBUG TRAIN Batch 2/200 loss 51.828087 loss_att 59.900093 loss_ctc 91.001282 loss_rnnt 44.990555 hw_loss 0.000074 lr 0.00081276 rank 2
2023-02-18 05:53:06,219 DEBUG TRAIN Batch 2/300 loss 62.409950 loss_att 84.697571 loss_ctc 77.942345 loss_rnnt 55.842987 hw_loss 0.072099 lr 0.00081616 rank 1
2023-02-18 05:53:06,220 DEBUG TRAIN Batch 2/300 loss 40.434654 loss_att 45.903454 loss_ctc 50.376068 loss_rnnt 37.933731 hw_loss 0.153081 lr 0.00081476 rank 2
2023-02-18 05:53:06,220 DEBUG TRAIN Batch 2/300 loss 22.593067 loss_att 26.768250 loss_ctc 30.165012 loss_rnnt 20.693258 hw_loss 0.103465 lr 0.00081324 rank 5
2023-02-18 05:53:06,221 DEBUG TRAIN Batch 2/300 loss 31.125813 loss_att 47.732742 loss_ctc 40.284019 loss_rnnt 26.490395 hw_loss 0.174255 lr 0.00081332 rank 3
2023-02-18 05:53:06,222 DEBUG TRAIN Batch 2/300 loss 27.931602 loss_att 33.727329 loss_ctc 33.376537 loss_rnnt 25.971441 hw_loss 0.140672 lr 0.00081536 rank 0
2023-02-18 05:53:06,224 DEBUG TRAIN Batch 2/300 loss 19.091751 loss_att 25.390980 loss_ctc 27.106998 loss_rnnt 16.664598 hw_loss 0.184890 lr 0.00081324 rank 7
2023-02-18 05:53:06,227 DEBUG TRAIN Batch 2/300 loss 28.532906 loss_att 32.236694 loss_ctc 40.425278 loss_rnnt 26.137833 hw_loss 0.128745 lr 0.00081428 rank 6
2023-02-18 05:53:06,229 DEBUG TRAIN Batch 2/300 loss 33.639915 loss_att 40.170532 loss_ctc 45.838448 loss_rnnt 30.694096 hw_loss 0.024797 lr 0.00081208 rank 4
2023-02-18 05:54:04,867 DEBUG TRAIN Batch 2/400 loss 20.187296 loss_att 36.102417 loss_ctc 37.305790 loss_rnnt 14.721766 hw_loss 0.000070 lr 0.00081408 rank 4
2023-02-18 05:54:04,869 DEBUG TRAIN Batch 2/400 loss 32.984810 loss_att 50.786156 loss_ctc 43.858475 loss_rnnt 27.950077 hw_loss 0.046200 lr 0.00081816 rank 1
2023-02-18 05:54:04,871 DEBUG TRAIN Batch 2/400 loss 33.127892 loss_att 40.556305 loss_ctc 53.805092 loss_rnnt 28.866365 hw_loss 0.035400 lr 0.00081532 rank 3
2023-02-18 05:54:04,872 DEBUG TRAIN Batch 2/400 loss 56.047195 loss_att 67.299103 loss_ctc 74.057907 loss_rnnt 51.393684 hw_loss 0.003189 lr 0.00081736 rank 0
2023-02-18 05:54:04,874 DEBUG TRAIN Batch 2/400 loss 44.784462 loss_att 68.338638 loss_ctc 74.871758 loss_rnnt 36.061951 hw_loss 0.000070 lr 0.00081524 rank 7
2023-02-18 05:54:04,875 DEBUG TRAIN Batch 2/400 loss 70.393135 loss_att 108.025734 loss_ctc 77.199059 loss_rnnt 61.921314 hw_loss 0.070963 lr 0.00081524 rank 5
2023-02-18 05:54:04,877 DEBUG TRAIN Batch 2/400 loss 36.320175 loss_att 44.719036 loss_ctc 47.723381 loss_rnnt 33.073288 hw_loss 0.087533 lr 0.00081676 rank 2
2023-02-18 05:54:04,933 DEBUG TRAIN Batch 2/400 loss 29.543961 loss_att 47.090012 loss_ctc 49.372692 loss_rnnt 23.352612 hw_loss 0.071829 lr 0.00081628 rank 6
2023-02-18 05:55:03,264 DEBUG TRAIN Batch 2/500 loss 31.058187 loss_att 46.407829 loss_ctc 46.495251 loss_rnnt 25.929937 hw_loss 0.000091 lr 0.00081724 rank 5
2023-02-18 05:55:03,267 DEBUG TRAIN Batch 2/500 loss 32.714848 loss_att 37.608013 loss_ctc 43.969830 loss_rnnt 30.235514 hw_loss 0.000066 lr 0.00081936 rank 0
2023-02-18 05:55:03,267 DEBUG TRAIN Batch 2/500 loss 36.749435 loss_att 39.765949 loss_ctc 45.900417 loss_rnnt 34.909515 hw_loss 0.030907 lr 0.00081724 rank 7
2023-02-18 05:55:03,267 DEBUG TRAIN Batch 2/500 loss 61.009304 loss_att 82.086807 loss_ctc 84.865097 loss_rnnt 53.557411 hw_loss 0.104270 lr 0.00081876 rank 2
2023-02-18 05:55:03,269 DEBUG TRAIN Batch 2/500 loss 42.250969 loss_att 71.513062 loss_ctc 74.092026 loss_rnnt 32.153023 hw_loss 0.000096 lr 0.00081732 rank 3
2023-02-18 05:55:03,301 DEBUG TRAIN Batch 2/500 loss 33.117691 loss_att 55.569096 loss_ctc 48.513859 loss_rnnt 26.512127 hw_loss 0.117110 lr 0.00081828 rank 6
2023-02-18 05:55:03,306 DEBUG TRAIN Batch 2/500 loss 79.825996 loss_att 106.490402 loss_ctc 134.430115 loss_rnnt 67.209702 hw_loss 0.005354 lr 0.00081608 rank 4
2023-02-18 05:55:03,325 DEBUG TRAIN Batch 2/500 loss 91.351540 loss_att 114.388847 loss_ctc 106.413475 loss_rnnt 84.735794 hw_loss 0.000056 lr 0.00082016 rank 1
2023-02-18 05:56:02,266 DEBUG TRAIN Batch 2/600 loss 37.349552 loss_att 47.983704 loss_ctc 46.947304 loss_rnnt 33.920052 hw_loss 0.043071 lr 0.00081924 rank 5
2023-02-18 05:56:02,268 DEBUG TRAIN Batch 2/600 loss 69.850418 loss_att 77.087738 loss_ctc 89.535645 loss_rnnt 65.714111 hw_loss 0.120273 lr 0.00082136 rank 0
2023-02-18 05:56:02,272 DEBUG TRAIN Batch 2/600 loss 48.516018 loss_att 80.014709 loss_ctc 72.569855 loss_rnnt 38.910378 hw_loss 0.185097 lr 0.00081932 rank 3
2023-02-18 05:56:02,275 DEBUG TRAIN Batch 2/600 loss 32.871540 loss_att 42.462288 loss_ctc 43.423737 loss_rnnt 29.459555 hw_loss 0.162899 lr 0.00082028 rank 6
2023-02-18 05:56:02,284 DEBUG TRAIN Batch 2/600 loss 36.325939 loss_att 44.585209 loss_ctc 49.790611 loss_rnnt 32.798065 hw_loss 0.151370 lr 0.00081924 rank 7
2023-02-18 05:56:02,312 DEBUG TRAIN Batch 2/600 loss 19.342684 loss_att 27.984264 loss_ctc 33.901073 loss_rnnt 15.628718 hw_loss 0.083493 lr 0.00082216 rank 1
2023-02-18 05:56:02,313 DEBUG TRAIN Batch 2/600 loss 38.723194 loss_att 43.199196 loss_ctc 50.905632 loss_rnnt 36.090988 hw_loss 0.211269 lr 0.00082076 rank 2
2023-02-18 05:56:02,340 DEBUG TRAIN Batch 2/600 loss 45.754726 loss_att 66.412552 loss_ctc 64.783218 loss_rnnt 39.086002 hw_loss 0.000048 lr 0.00081808 rank 4
2023-02-18 05:57:03,091 DEBUG TRAIN Batch 2/700 loss 31.214394 loss_att 42.199028 loss_ctc 47.597404 loss_rnnt 26.832912 hw_loss 0.000287 lr 0.00082336 rank 0
2023-02-18 05:57:03,092 DEBUG TRAIN Batch 2/700 loss 54.754845 loss_att 67.325455 loss_ctc 69.799698 loss_rnnt 50.181400 hw_loss 0.100011 lr 0.00082008 rank 4
2023-02-18 05:57:03,093 DEBUG TRAIN Batch 2/700 loss 34.686043 loss_att 46.489914 loss_ctc 52.122856 loss_rnnt 30.000278 hw_loss 0.000156 lr 0.00082416 rank 1
2023-02-18 05:57:03,097 DEBUG TRAIN Batch 2/700 loss 105.774124 loss_att 130.452148 loss_ctc 127.843704 loss_rnnt 97.878540 hw_loss 0.032562 lr 0.00082124 rank 5
2023-02-18 05:57:03,099 DEBUG TRAIN Batch 2/700 loss 25.822664 loss_att 35.744991 loss_ctc 35.906666 loss_rnnt 22.454046 hw_loss 0.074285 lr 0.00082276 rank 2
2023-02-18 05:57:03,099 DEBUG TRAIN Batch 2/700 loss 57.279690 loss_att 76.429855 loss_ctc 73.195618 loss_rnnt 51.178226 hw_loss 0.279950 lr 0.00082228 rank 6
2023-02-18 05:57:03,101 DEBUG TRAIN Batch 2/700 loss 14.843440 loss_att 27.465115 loss_ctc 20.472708 loss_rnnt 11.568272 hw_loss 0.000495 lr 0.00082132 rank 3
2023-02-18 05:57:03,109 DEBUG TRAIN Batch 2/700 loss 49.336044 loss_att 64.546989 loss_ctc 72.719910 loss_rnnt 43.138393 hw_loss 0.070525 lr 0.00082124 rank 7
2023-02-18 05:58:01,051 DEBUG TRAIN Batch 2/800 loss 62.784073 loss_att 107.668480 loss_ctc 81.754265 loss_rnnt 51.277748 hw_loss 0.000151 lr 0.00082324 rank 7
2023-02-18 05:58:01,051 DEBUG TRAIN Batch 2/800 loss 48.795277 loss_att 60.390221 loss_ctc 78.738228 loss_rnnt 42.390381 hw_loss 0.175339 lr 0.00082324 rank 5
2023-02-18 05:58:01,054 DEBUG TRAIN Batch 2/800 loss 36.447861 loss_att 50.650421 loss_ctc 55.427944 loss_rnnt 31.007269 hw_loss 0.130124 lr 0.00082616 rank 1
2023-02-18 05:58:01,055 DEBUG TRAIN Batch 2/800 loss 47.507336 loss_att 65.031418 loss_ctc 70.566658 loss_rnnt 40.927822 hw_loss 0.000218 lr 0.00082476 rank 2
2023-02-18 05:58:01,056 DEBUG TRAIN Batch 2/800 loss 43.620960 loss_att 65.621033 loss_ctc 58.286362 loss_rnnt 37.265411 hw_loss 0.000278 lr 0.00082332 rank 3
2023-02-18 05:58:01,090 DEBUG TRAIN Batch 2/800 loss 87.395172 loss_att 112.638718 loss_ctc 128.561111 loss_rnnt 76.857582 hw_loss 0.000150 lr 0.00082208 rank 4
2023-02-18 05:58:01,092 DEBUG TRAIN Batch 2/800 loss 77.370125 loss_att 95.011566 loss_ctc 92.389793 loss_rnnt 71.839104 hw_loss 0.000208 lr 0.00082428 rank 6
2023-02-18 05:58:01,094 DEBUG TRAIN Batch 2/800 loss 32.246601 loss_att 39.633686 loss_ctc 45.608341 loss_rnnt 28.987530 hw_loss 0.000166 lr 0.00082536 rank 0
2023-02-18 05:59:54,306 DEBUG TRAIN Batch 2/900 loss 43.751106 loss_att 59.309093 loss_ctc 64.214706 loss_rnnt 37.819218 hw_loss 0.172136 lr 0.00082736 rank 0
2023-02-18 05:59:54,307 DEBUG TRAIN Batch 2/900 loss 20.155039 loss_att 30.791599 loss_ctc 35.622383 loss_rnnt 15.891775 hw_loss 0.138073 lr 0.00082524 rank 5
2023-02-18 05:59:54,309 DEBUG TRAIN Batch 2/900 loss 53.567001 loss_att 65.576317 loss_ctc 69.981857 loss_rnnt 48.906342 hw_loss 0.131532 lr 0.00082676 rank 2
2023-02-18 05:59:54,309 DEBUG TRAIN Batch 2/900 loss 41.403145 loss_att 46.046364 loss_ctc 56.022633 loss_rnnt 38.426086 hw_loss 0.185899 lr 0.00082628 rank 6
2023-02-18 05:59:54,310 DEBUG TRAIN Batch 2/900 loss 42.552773 loss_att 52.411697 loss_ctc 65.437576 loss_rnnt 37.481453 hw_loss 0.090424 lr 0.00082816 rank 1
2023-02-18 05:59:54,311 DEBUG TRAIN Batch 2/900 loss 40.740986 loss_att 57.615685 loss_ctc 57.255928 loss_rnnt 35.093857 hw_loss 0.131619 lr 0.00082532 rank 3
2023-02-18 05:59:54,311 DEBUG TRAIN Batch 2/900 loss 45.303890 loss_att 58.357552 loss_ctc 54.172462 loss_rnnt 41.433334 hw_loss 0.145024 lr 0.00082524 rank 7
2023-02-18 05:59:54,312 DEBUG TRAIN Batch 2/900 loss 33.343327 loss_att 47.290741 loss_ctc 49.279293 loss_rnnt 28.424438 hw_loss 0.008639 lr 0.00082408 rank 4
2023-02-18 06:00:53,889 DEBUG TRAIN Batch 2/1000 loss 35.956783 loss_att 53.938591 loss_ctc 52.296509 loss_rnnt 30.161278 hw_loss 0.038462 lr 0.00082608 rank 4
2023-02-18 06:00:53,890 DEBUG TRAIN Batch 2/1000 loss 49.338982 loss_att 75.364548 loss_ctc 75.918335 loss_rnnt 40.553577 hw_loss 0.068199 lr 0.00082876 rank 2
2023-02-18 06:00:53,892 DEBUG TRAIN Batch 2/1000 loss 44.117535 loss_att 65.696220 loss_ctc 78.980133 loss_rnnt 35.088524 hw_loss 0.121739 lr 0.00082724 rank 5
2023-02-18 06:00:53,892 DEBUG TRAIN Batch 2/1000 loss 42.737041 loss_att 64.393402 loss_ctc 54.224876 loss_rnnt 36.837376 hw_loss 0.068770 lr 0.00082936 rank 0
2023-02-18 06:00:53,893 DEBUG TRAIN Batch 2/1000 loss 42.667038 loss_att 60.309784 loss_ctc 65.967651 loss_rnnt 35.991653 hw_loss 0.075164 lr 0.00082828 rank 6
2023-02-18 06:00:53,897 DEBUG TRAIN Batch 2/1000 loss 18.506683 loss_att 34.298595 loss_ctc 33.716869 loss_rnnt 13.219506 hw_loss 0.188943 lr 0.00082732 rank 3
2023-02-18 06:00:53,903 DEBUG TRAIN Batch 2/1000 loss 24.619322 loss_att 33.542336 loss_ctc 31.226738 loss_rnnt 21.926746 hw_loss 0.050592 lr 0.00082724 rank 7
2023-02-18 06:00:53,952 DEBUG TRAIN Batch 2/1000 loss 13.171935 loss_att 17.508091 loss_ctc 20.078556 loss_rnnt 11.347884 hw_loss 0.067381 lr 0.00083016 rank 1
2023-02-18 06:01:51,606 DEBUG TRAIN Batch 2/1100 loss 28.176733 loss_att 44.522324 loss_ctc 31.370783 loss_rnnt 24.480942 hw_loss 0.001503 lr 0.00083136 rank 0
2023-02-18 06:01:51,611 DEBUG TRAIN Batch 2/1100 loss 50.096428 loss_att 49.445992 loss_ctc 55.047955 loss_rnnt 49.475052 hw_loss 0.171108 lr 0.00082924 rank 5
2023-02-18 06:01:51,616 DEBUG TRAIN Batch 2/1100 loss 21.934456 loss_att 42.962296 loss_ctc 54.003311 loss_rnnt 13.452066 hw_loss 0.001821 lr 0.00082808 rank 4
2023-02-18 06:01:51,616 DEBUG TRAIN Batch 2/1100 loss 84.847649 loss_att 97.849640 loss_ctc 112.393875 loss_rnnt 78.574364 hw_loss 0.000111 lr 0.00083028 rank 6
2023-02-18 06:01:51,619 DEBUG TRAIN Batch 2/1100 loss 55.917835 loss_att 71.881935 loss_ctc 79.938599 loss_rnnt 49.518150 hw_loss 0.007687 lr 0.00082932 rank 3
2023-02-18 06:01:51,620 DEBUG TRAIN Batch 2/1100 loss 33.405861 loss_att 53.743279 loss_ctc 47.609177 loss_rnnt 27.444553 hw_loss 0.000090 lr 0.00082924 rank 7
2023-02-18 06:01:51,624 DEBUG TRAIN Batch 2/1100 loss 22.573629 loss_att 34.017067 loss_ctc 28.574141 loss_rnnt 19.403460 hw_loss 0.152648 lr 0.00083216 rank 1
2023-02-18 06:01:51,673 DEBUG TRAIN Batch 2/1100 loss 16.250664 loss_att 26.954437 loss_ctc 30.895090 loss_rnnt 12.157181 hw_loss 0.000258 lr 0.00083076 rank 2
2023-02-18 06:02:49,352 DEBUG TRAIN Batch 2/1200 loss 37.476730 loss_att 55.035774 loss_ctc 58.611496 loss_rnnt 31.143541 hw_loss 0.006392 lr 0.00083132 rank 3
2023-02-18 06:02:49,357 DEBUG TRAIN Batch 2/1200 loss 47.878548 loss_att 66.901543 loss_ctc 71.504242 loss_rnnt 40.923721 hw_loss 0.000249 lr 0.00083124 rank 5
2023-02-18 06:02:49,360 DEBUG TRAIN Batch 2/1200 loss 45.010307 loss_att 65.817696 loss_ctc 65.440445 loss_rnnt 38.048317 hw_loss 0.143423 lr 0.00083416 rank 1
2023-02-18 06:02:49,362 DEBUG TRAIN Batch 2/1200 loss 44.156345 loss_att 60.042145 loss_ctc 55.468086 loss_rnnt 39.397675 hw_loss 0.137398 lr 0.00083228 rank 6
2023-02-18 06:02:49,364 DEBUG TRAIN Batch 2/1200 loss 14.760820 loss_att 25.593592 loss_ctc 19.980087 loss_rnnt 11.896353 hw_loss 0.003771 lr 0.00083276 rank 2
2023-02-18 06:02:49,366 DEBUG TRAIN Batch 2/1200 loss 19.413754 loss_att 30.231487 loss_ctc 24.722687 loss_rnnt 16.542253 hw_loss 0.000175 lr 0.00083124 rank 7
2023-02-18 06:02:49,368 DEBUG TRAIN Batch 2/1200 loss 47.459782 loss_att 62.165249 loss_ctc 59.771141 loss_rnnt 42.876179 hw_loss 0.001860 lr 0.00083336 rank 0
2023-02-18 06:02:49,422 DEBUG TRAIN Batch 2/1200 loss 53.391598 loss_att 56.928070 loss_ctc 78.380882 loss_rnnt 49.299255 hw_loss 0.099637 lr 0.00083008 rank 4
2023-02-18 06:03:50,200 DEBUG TRAIN Batch 2/1300 loss 62.584946 loss_att 77.912926 loss_ctc 94.486244 loss_rnnt 55.136288 hw_loss 0.242922 lr 0.00083536 rank 0
2023-02-18 06:03:50,203 DEBUG TRAIN Batch 2/1300 loss 33.512592 loss_att 49.553062 loss_ctc 54.731010 loss_rnnt 27.412893 hw_loss 0.117159 lr 0.00083324 rank 5
2023-02-18 06:03:50,205 DEBUG TRAIN Batch 2/1300 loss 44.963158 loss_att 68.185402 loss_ctc 61.989113 loss_rnnt 38.026981 hw_loss 0.040501 lr 0.00083208 rank 4
2023-02-18 06:03:50,208 DEBUG TRAIN Batch 2/1300 loss 39.220505 loss_att 50.308121 loss_ctc 53.261002 loss_rnnt 35.073795 hw_loss 0.107095 lr 0.00083476 rank 2
2023-02-18 06:03:50,211 DEBUG TRAIN Batch 2/1300 loss 27.695486 loss_att 48.096561 loss_ctc 42.982441 loss_rnnt 21.532505 hw_loss 0.083441 lr 0.00083324 rank 7
2023-02-18 06:03:50,213 DEBUG TRAIN Batch 2/1300 loss 32.311066 loss_att 69.499344 loss_ctc 49.230537 loss_rnnt 22.570602 hw_loss 0.087891 lr 0.00083332 rank 3
2023-02-18 06:03:50,230 DEBUG TRAIN Batch 2/1300 loss 27.474928 loss_att 47.052940 loss_ctc 39.684288 loss_rnnt 21.878187 hw_loss 0.099791 lr 0.00083428 rank 6
2023-02-18 06:03:50,283 DEBUG TRAIN Batch 2/1300 loss 42.191395 loss_att 52.351604 loss_ctc 50.483345 loss_rnnt 38.952370 hw_loss 0.190110 lr 0.00083616 rank 1
2023-02-18 06:04:46,501 DEBUG TRAIN Batch 2/1400 loss 33.210785 loss_att 56.319595 loss_ctc 42.211037 loss_rnnt 27.349566 hw_loss 0.073922 lr 0.00083524 rank 5
2023-02-18 06:04:46,503 DEBUG TRAIN Batch 2/1400 loss 26.596197 loss_att 56.436829 loss_ctc 38.980186 loss_rnnt 18.976854 hw_loss 0.000033 lr 0.00083816 rank 1
2023-02-18 06:04:46,509 DEBUG TRAIN Batch 2/1400 loss 33.979378 loss_att 84.910690 loss_ctc 45.637600 loss_rnnt 22.165188 hw_loss 0.137805 lr 0.00083736 rank 0
2023-02-18 06:04:46,509 DEBUG TRAIN Batch 2/1400 loss 40.480892 loss_att 66.101608 loss_ctc 46.735378 loss_rnnt 34.522789 hw_loss 0.000046 lr 0.00083524 rank 7
2023-02-18 06:04:46,510 DEBUG TRAIN Batch 2/1400 loss 45.454266 loss_att 90.826965 loss_ctc 81.006187 loss_rnnt 31.535030 hw_loss 0.195817 lr 0.00083532 rank 3
2023-02-18 06:04:46,513 DEBUG TRAIN Batch 2/1400 loss 29.366983 loss_att 37.908752 loss_ctc 39.613846 loss_rnnt 26.244171 hw_loss 0.090393 lr 0.00083676 rank 2
2023-02-18 06:04:46,514 DEBUG TRAIN Batch 2/1400 loss 57.516979 loss_att 83.837509 loss_ctc 97.974510 loss_rnnt 46.858509 hw_loss 0.000046 lr 0.00083408 rank 4
2023-02-18 06:04:46,565 DEBUG TRAIN Batch 2/1400 loss 59.055672 loss_att 71.110359 loss_ctc 70.295189 loss_rnnt 54.962372 hw_loss 0.344553 lr 0.00083628 rank 6
2023-02-18 06:05:46,189 DEBUG TRAIN Batch 2/1500 loss 46.267136 loss_att 65.572571 loss_ctc 74.461487 loss_rnnt 38.612267 hw_loss 0.064753 lr 0.00083876 rank 2
2023-02-18 06:05:46,189 DEBUG TRAIN Batch 2/1500 loss 37.671185 loss_att 49.857460 loss_ctc 54.904076 loss_rnnt 32.905659 hw_loss 0.057282 lr 0.00083936 rank 0
2023-02-18 06:05:46,192 DEBUG TRAIN Batch 2/1500 loss 22.325317 loss_att 26.704304 loss_ctc 29.969410 loss_rnnt 20.296759 hw_loss 0.250406 lr 0.00084016 rank 1
2023-02-18 06:05:46,193 DEBUG TRAIN Batch 2/1500 loss 40.398945 loss_att 54.030243 loss_ctc 61.544647 loss_rnnt 34.782707 hw_loss 0.132281 lr 0.00083608 rank 4
2023-02-18 06:05:46,194 DEBUG TRAIN Batch 2/1500 loss 33.799858 loss_att 47.308189 loss_ctc 44.479992 loss_rnnt 29.674109 hw_loss 0.000122 lr 0.00083724 rank 5
2023-02-18 06:05:46,195 DEBUG TRAIN Batch 2/1500 loss 38.822041 loss_att 60.954147 loss_ctc 52.588058 loss_rnnt 32.551804 hw_loss 0.015650 lr 0.00083828 rank 6
2023-02-18 06:05:46,196 DEBUG TRAIN Batch 2/1500 loss 37.346630 loss_att 56.471176 loss_ctc 59.192574 loss_rnnt 30.596392 hw_loss 0.023511 lr 0.00083732 rank 3
2023-02-18 06:05:46,197 DEBUG TRAIN Batch 2/1500 loss 30.896235 loss_att 40.558434 loss_ctc 50.236443 loss_rnnt 26.351278 hw_loss 0.063417 lr 0.00083724 rank 7
2023-02-18 06:06:47,944 DEBUG TRAIN Batch 2/1600 loss 37.536816 loss_att 45.254414 loss_ctc 54.690178 loss_rnnt 33.705933 hw_loss 0.000466 lr 0.00084076 rank 2
2023-02-18 06:06:47,944 DEBUG TRAIN Batch 2/1600 loss 62.342495 loss_att 81.048370 loss_ctc 92.927551 loss_rnnt 54.487637 hw_loss 0.066886 lr 0.00083924 rank 5
2023-02-18 06:06:47,946 DEBUG TRAIN Batch 2/1600 loss 24.415457 loss_att 43.410339 loss_ctc 28.824181 loss_rnnt 20.028486 hw_loss 0.000310 lr 0.00084028 rank 6
2023-02-18 06:06:47,947 DEBUG TRAIN Batch 2/1600 loss 41.920952 loss_att 61.326225 loss_ctc 62.820797 loss_rnnt 35.253151 hw_loss 0.000192 lr 0.00084216 rank 1
2023-02-18 06:06:47,947 DEBUG TRAIN Batch 2/1600 loss 38.750427 loss_att 66.842712 loss_ctc 46.893085 loss_rnnt 31.990362 hw_loss 0.104851 lr 0.00083808 rank 4
2023-02-18 06:06:47,948 DEBUG TRAIN Batch 2/1600 loss 46.757545 loss_att 65.842987 loss_ctc 63.647995 loss_rnnt 40.688278 hw_loss 0.000217 lr 0.00084136 rank 0
2023-02-18 06:06:47,952 DEBUG TRAIN Batch 2/1600 loss 15.268867 loss_att 23.850986 loss_ctc 22.349295 loss_rnnt 12.568226 hw_loss 0.075299 lr 0.00083932 rank 3
2023-02-18 06:06:48,008 DEBUG TRAIN Batch 2/1600 loss 49.983749 loss_att 72.436325 loss_ctc 73.518478 loss_rnnt 42.319118 hw_loss 0.067784 lr 0.00083924 rank 7
2023-02-18 06:07:47,276 DEBUG TRAIN Batch 2/1700 loss 29.082823 loss_att 41.945549 loss_ctc 46.863163 loss_rnnt 24.139130 hw_loss 0.000817 lr 0.00084008 rank 4
2023-02-18 06:07:47,280 DEBUG TRAIN Batch 2/1700 loss 60.388538 loss_att 73.985016 loss_ctc 79.981117 loss_rnnt 55.017548 hw_loss 0.073786 lr 0.00084276 rank 2
2023-02-18 06:07:47,281 DEBUG TRAIN Batch 2/1700 loss 39.937569 loss_att 44.109188 loss_ctc 53.735916 loss_rnnt 37.211533 hw_loss 0.097366 lr 0.00084124 rank 5
2023-02-18 06:07:47,283 DEBUG TRAIN Batch 2/1700 loss 39.410511 loss_att 52.449471 loss_ctc 46.919971 loss_rnnt 35.801182 hw_loss 0.000517 lr 0.00084132 rank 3
2023-02-18 06:07:47,288 DEBUG TRAIN Batch 2/1700 loss 112.323723 loss_att 141.415573 loss_ctc 180.449280 loss_rnnt 97.421623 hw_loss 0.000587 lr 0.00084336 rank 0
2023-02-18 06:07:47,293 DEBUG TRAIN Batch 2/1700 loss 10.623830 loss_att 24.341009 loss_ctc 14.440586 loss_rnnt 7.371164 hw_loss 0.000616 lr 0.00084228 rank 6
2023-02-18 06:07:47,324 DEBUG TRAIN Batch 2/1700 loss 35.223022 loss_att 57.442448 loss_ctc 53.138172 loss_rnnt 28.390291 hw_loss 0.000292 lr 0.00084416 rank 1
2023-02-18 06:07:47,371 DEBUG TRAIN Batch 2/1700 loss 89.101570 loss_att 100.247643 loss_ctc 122.094315 loss_rnnt 82.472885 hw_loss 0.000822 lr 0.00084124 rank 7
2023-02-18 06:09:39,071 DEBUG TRAIN Batch 2/1800 loss 26.740711 loss_att 43.439400 loss_ctc 30.496130 loss_rnnt 22.892899 hw_loss 0.013782 lr 0.00084208 rank 4
2023-02-18 06:09:39,071 DEBUG TRAIN Batch 2/1800 loss 39.531784 loss_att 51.842354 loss_ctc 52.376205 loss_rnnt 35.267715 hw_loss 0.167564 lr 0.00084536 rank 0
2023-02-18 06:09:39,071 DEBUG TRAIN Batch 2/1800 loss 34.182888 loss_att 47.920010 loss_ctc 51.471863 loss_rnnt 29.034451 hw_loss 0.179662 lr 0.00084324 rank 5
2023-02-18 06:09:39,076 DEBUG TRAIN Batch 2/1800 loss 24.495668 loss_att 37.491215 loss_ctc 34.740746 loss_rnnt 20.508297 hw_loss 0.041724 lr 0.00084476 rank 2
2023-02-18 06:09:39,077 DEBUG TRAIN Batch 2/1800 loss 24.053169 loss_att 40.901482 loss_ctc 35.783295 loss_rnnt 19.054131 hw_loss 0.122545 lr 0.00084428 rank 6
2023-02-18 06:09:39,078 DEBUG TRAIN Batch 2/1800 loss 25.183632 loss_att 47.342468 loss_ctc 51.577984 loss_rnnt 17.232597 hw_loss 0.000039 lr 0.00084324 rank 7
2023-02-18 06:09:39,079 DEBUG TRAIN Batch 2/1800 loss 31.262722 loss_att 49.079544 loss_ctc 50.207939 loss_rnnt 25.173315 hw_loss 0.000028 lr 0.00084616 rank 1
2023-02-18 06:09:39,082 DEBUG TRAIN Batch 2/1800 loss 29.278698 loss_att 49.474014 loss_ctc 52.201454 loss_rnnt 22.183239 hw_loss 0.000053 lr 0.00084332 rank 3
2023-02-18 06:10:39,785 DEBUG TRAIN Batch 2/1900 loss 54.228771 loss_att 66.614197 loss_ctc 92.002708 loss_rnnt 46.715111 hw_loss 0.000093 lr 0.00084628 rank 6
2023-02-18 06:10:39,786 DEBUG TRAIN Batch 2/1900 loss 29.561216 loss_att 34.347656 loss_ctc 45.533875 loss_rnnt 26.474186 hw_loss 0.000099 lr 0.00084524 rank 5
2023-02-18 06:10:39,786 DEBUG TRAIN Batch 2/1900 loss 21.518517 loss_att 40.180389 loss_ctc 42.901535 loss_rnnt 14.873033 hw_loss 0.116326 lr 0.00084676 rank 2
2023-02-18 06:10:39,788 DEBUG TRAIN Batch 2/1900 loss 103.339569 loss_att 99.508934 loss_ctc 119.279556 loss_rnnt 101.980339 hw_loss 0.000062 lr 0.00084524 rank 7
2023-02-18 06:10:39,792 DEBUG TRAIN Batch 2/1900 loss 12.303963 loss_att 21.665188 loss_ctc 10.850698 loss_rnnt 10.625394 hw_loss 0.000172 lr 0.00084532 rank 3
2023-02-18 06:10:39,796 DEBUG TRAIN Batch 2/1900 loss 12.740712 loss_att 26.506004 loss_ctc 24.855869 loss_rnnt 8.372266 hw_loss 0.000062 lr 0.00084408 rank 4
2023-02-18 06:10:39,797 DEBUG TRAIN Batch 2/1900 loss 57.222157 loss_att 66.850067 loss_ctc 79.215240 loss_rnnt 52.321930 hw_loss 0.079186 lr 0.00084736 rank 0
2023-02-18 06:10:39,847 DEBUG TRAIN Batch 2/1900 loss 29.563255 loss_att 41.995018 loss_ctc 47.193058 loss_rnnt 24.726219 hw_loss 0.000083 lr 0.00084816 rank 1
2023-02-18 06:11:37,133 DEBUG TRAIN Batch 2/2000 loss 60.311623 loss_att 84.795738 loss_ctc 85.425583 loss_rnnt 51.998199 hw_loss 0.127638 lr 0.00085016 rank 1
2023-02-18 06:11:37,132 DEBUG TRAIN Batch 2/2000 loss 81.026154 loss_att 115.966682 loss_ctc 122.139473 loss_rnnt 68.556198 hw_loss 0.000130 lr 0.00084608 rank 4
2023-02-18 06:11:37,133 DEBUG TRAIN Batch 2/2000 loss 70.095352 loss_att 110.709259 loss_ctc 114.375839 loss_rnnt 56.068413 hw_loss 0.000168 lr 0.00084724 rank 5
2023-02-18 06:11:37,138 DEBUG TRAIN Batch 2/2000 loss 20.811615 loss_att 34.617382 loss_ctc 30.617516 loss_rnnt 16.742926 hw_loss 0.000157 lr 0.00084876 rank 2
2023-02-18 06:11:37,138 DEBUG TRAIN Batch 2/2000 loss 18.244310 loss_att 28.269981 loss_ctc 33.613525 loss_rnnt 14.189879 hw_loss 0.000131 lr 0.00084724 rank 7
2023-02-18 06:11:37,138 DEBUG TRAIN Batch 2/2000 loss 36.808216 loss_att 65.993248 loss_ctc 45.267799 loss_rnnt 29.720093 hw_loss 0.230947 lr 0.00084936 rank 0
2023-02-18 06:11:37,150 DEBUG TRAIN Batch 2/2000 loss 25.680553 loss_att 29.125784 loss_ctc 31.514507 loss_rnnt 24.123316 hw_loss 0.169367 lr 0.00084732 rank 3
2023-02-18 06:11:37,204 DEBUG TRAIN Batch 2/2000 loss 30.114010 loss_att 34.077988 loss_ctc 39.559338 loss_rnnt 27.970860 hw_loss 0.170580 lr 0.00084828 rank 6
2023-02-18 06:12:35,454 DEBUG TRAIN Batch 2/2100 loss 28.409033 loss_att 41.078609 loss_ctc 42.094383 loss_rnnt 23.998272 hw_loss 0.097745 lr 0.00084924 rank 5
2023-02-18 06:12:35,454 DEBUG TRAIN Batch 2/2100 loss 32.827766 loss_att 58.421783 loss_ctc 55.435135 loss_rnnt 24.670538 hw_loss 0.045207 lr 0.00085076 rank 2
2023-02-18 06:12:35,457 DEBUG TRAIN Batch 2/2100 loss 49.851479 loss_att 57.839798 loss_ctc 68.490433 loss_rnnt 45.722534 hw_loss 0.086399 lr 0.00085216 rank 1
2023-02-18 06:12:35,460 DEBUG TRAIN Batch 2/2100 loss 30.618670 loss_att 41.123238 loss_ctc 43.053249 loss_rnnt 26.713533 hw_loss 0.274273 lr 0.00084924 rank 7
2023-02-18 06:12:35,461 DEBUG TRAIN Batch 2/2100 loss 28.274021 loss_att 42.483986 loss_ctc 39.373779 loss_rnnt 23.897301 hw_loss 0.102673 lr 0.00084808 rank 4
2023-02-18 06:12:35,463 DEBUG TRAIN Batch 2/2100 loss 36.316223 loss_att 52.338932 loss_ctc 55.544067 loss_rnnt 30.506849 hw_loss 0.077096 lr 0.00085136 rank 0
2023-02-18 06:12:35,465 DEBUG TRAIN Batch 2/2100 loss 60.939919 loss_att 80.252174 loss_ctc 83.653778 loss_rnnt 53.996536 hw_loss 0.098276 lr 0.00084932 rank 3
2023-02-18 06:12:35,468 DEBUG TRAIN Batch 2/2100 loss 23.862619 loss_att 40.056618 loss_ctc 33.957649 loss_rnnt 19.277580 hw_loss 0.000440 lr 0.00085028 rank 6
2023-02-18 06:13:35,628 DEBUG TRAIN Batch 2/2200 loss 44.735577 loss_att 68.605522 loss_ctc 66.976624 loss_rnnt 36.995956 hw_loss 0.000298 lr 0.00085416 rank 1
2023-02-18 06:13:35,629 DEBUG TRAIN Batch 2/2200 loss 33.611153 loss_att 43.704578 loss_ctc 56.820511 loss_rnnt 28.432859 hw_loss 0.121929 lr 0.00085228 rank 6
2023-02-18 06:13:35,629 DEBUG TRAIN Batch 2/2200 loss 13.794327 loss_att 24.409496 loss_ctc 24.178085 loss_rnnt 10.241737 hw_loss 0.084477 lr 0.00085124 rank 5
2023-02-18 06:13:35,632 DEBUG TRAIN Batch 2/2200 loss 39.685627 loss_att 59.943596 loss_ctc 58.482677 loss_rnnt 33.075386 hw_loss 0.098204 lr 0.00085008 rank 4
2023-02-18 06:13:35,633 DEBUG TRAIN Batch 2/2200 loss 61.673107 loss_att 72.639267 loss_ctc 99.140953 loss_rnnt 54.483829 hw_loss 0.000623 lr 0.00085132 rank 3
2023-02-18 06:13:35,635 DEBUG TRAIN Batch 2/2200 loss 34.512733 loss_att 51.524796 loss_ctc 57.827747 loss_rnnt 28.001499 hw_loss 0.000290 lr 0.00085124 rank 7
2023-02-18 06:13:35,636 DEBUG TRAIN Batch 2/2200 loss 41.757038 loss_att 58.792866 loss_ctc 59.415051 loss_rnnt 35.943943 hw_loss 0.096608 lr 0.00085276 rank 2
2023-02-18 06:13:35,637 DEBUG TRAIN Batch 2/2200 loss 21.522324 loss_att 31.552773 loss_ctc 28.514578 loss_rnnt 18.583761 hw_loss 0.000324 lr 0.00085336 rank 0
2023-02-18 06:14:32,956 DEBUG TRAIN Batch 2/2300 loss 27.376261 loss_att 36.906448 loss_ctc 39.985973 loss_rnnt 23.686035 hw_loss 0.192921 lr 0.00085324 rank 5
2023-02-18 06:14:32,960 DEBUG TRAIN Batch 2/2300 loss 35.408592 loss_att 44.402912 loss_ctc 45.778313 loss_rnnt 32.057861 hw_loss 0.317314 lr 0.00085616 rank 1
2023-02-18 06:14:32,965 DEBUG TRAIN Batch 2/2300 loss 51.966198 loss_att 82.167084 loss_ctc 72.506477 loss_rnnt 43.187271 hw_loss 0.000084 lr 0.00085476 rank 2
2023-02-18 06:14:32,965 DEBUG TRAIN Batch 2/2300 loss 40.497929 loss_att 68.219147 loss_ctc 49.878773 loss_rnnt 33.697033 hw_loss 0.011013 lr 0.00085324 rank 7
2023-02-18 06:14:32,965 DEBUG TRAIN Batch 2/2300 loss 14.992891 loss_att 25.546816 loss_ctc 26.128372 loss_rnnt 11.397342 hw_loss 0.000064 lr 0.00085536 rank 0
2023-02-18 06:14:32,966 DEBUG TRAIN Batch 2/2300 loss 37.939331 loss_att 44.909145 loss_ctc 54.423584 loss_rnnt 34.281925 hw_loss 0.122893 lr 0.00085428 rank 6
2023-02-18 06:14:32,972 DEBUG TRAIN Batch 2/2300 loss 39.332676 loss_att 52.763256 loss_ctc 56.383282 loss_rnnt 34.287056 hw_loss 0.161416 lr 0.00085332 rank 3
2023-02-18 06:14:32,974 DEBUG TRAIN Batch 2/2300 loss 50.183140 loss_att 64.580154 loss_ctc 61.715210 loss_rnnt 45.766087 hw_loss 0.000074 lr 0.00085208 rank 4
2023-02-18 06:15:32,254 DEBUG TRAIN Batch 2/2400 loss 27.532164 loss_att 33.079765 loss_ctc 39.650589 loss_rnnt 24.727165 hw_loss 0.149412 lr 0.00085736 rank 0
2023-02-18 06:15:32,256 DEBUG TRAIN Batch 2/2400 loss 23.897589 loss_att 37.844986 loss_ctc 44.339275 loss_rnnt 18.382488 hw_loss 0.000113 lr 0.00085524 rank 5
2023-02-18 06:15:32,257 DEBUG TRAIN Batch 2/2400 loss 40.504028 loss_att 51.353088 loss_ctc 55.315742 loss_rnnt 36.220692 hw_loss 0.259928 lr 0.00085628 rank 6
2023-02-18 06:15:32,262 DEBUG TRAIN Batch 2/2400 loss 26.524195 loss_att 40.969887 loss_ctc 43.776962 loss_rnnt 21.334612 hw_loss 0.000140 lr 0.00085816 rank 1
2023-02-18 06:15:32,263 DEBUG TRAIN Batch 2/2400 loss 42.772526 loss_att 62.232193 loss_ctc 67.023170 loss_rnnt 35.567089 hw_loss 0.150155 lr 0.00085676 rank 2
2023-02-18 06:15:32,264 DEBUG TRAIN Batch 2/2400 loss 37.685230 loss_att 50.667984 loss_ctc 50.445614 loss_rnnt 33.347645 hw_loss 0.074342 lr 0.00085532 rank 3
2023-02-18 06:15:32,265 DEBUG TRAIN Batch 2/2400 loss 43.453171 loss_att 54.333138 loss_ctc 69.522850 loss_rnnt 37.801170 hw_loss 0.000093 lr 0.00085524 rank 7
2023-02-18 06:15:32,266 DEBUG TRAIN Batch 2/2400 loss 52.506592 loss_att 58.985741 loss_ctc 71.811607 loss_rnnt 48.577091 hw_loss 0.111871 lr 0.00085408 rank 4
2023-02-18 06:16:32,381 DEBUG TRAIN Batch 2/2500 loss 33.190697 loss_att 44.481804 loss_ctc 54.013000 loss_rnnt 28.156124 hw_loss 0.000086 lr 0.00086016 rank 1
2023-02-18 06:16:32,382 DEBUG TRAIN Batch 2/2500 loss 37.879822 loss_att 50.809570 loss_ctc 46.442524 loss_rnnt 34.100517 hw_loss 0.096869 lr 0.00085828 rank 6
2023-02-18 06:16:32,387 DEBUG TRAIN Batch 2/2500 loss 25.099670 loss_att 34.549965 loss_ctc 29.193645 loss_rnnt 22.663652 hw_loss 0.000177 lr 0.00085732 rank 3
2023-02-18 06:16:32,388 DEBUG TRAIN Batch 2/2500 loss 48.597290 loss_att 91.304955 loss_ctc 69.179298 loss_rnnt 37.311424 hw_loss 0.000123 lr 0.00085876 rank 2
2023-02-18 06:16:32,395 DEBUG TRAIN Batch 2/2500 loss 33.446892 loss_att 47.016563 loss_ctc 55.610043 loss_rnnt 27.749275 hw_loss 0.053611 lr 0.00085724 rank 5
2023-02-18 06:16:32,425 DEBUG TRAIN Batch 2/2500 loss 32.822987 loss_att 61.475418 loss_ctc 38.211830 loss_rnnt 26.159695 hw_loss 0.401798 lr 0.00085608 rank 4
2023-02-18 06:16:32,427 DEBUG TRAIN Batch 2/2500 loss 17.731079 loss_att 29.839680 loss_ctc 28.667740 loss_rnnt 13.776795 hw_loss 0.139390 lr 0.00085724 rank 7
2023-02-18 06:16:32,449 DEBUG TRAIN Batch 2/2500 loss 59.928635 loss_att 71.729599 loss_ctc 96.615074 loss_rnnt 52.676872 hw_loss 0.000090 lr 0.00085936 rank 0
2023-02-18 06:18:28,771 DEBUG TRAIN Batch 2/2600 loss 24.846682 loss_att 32.625488 loss_ctc 40.850082 loss_rnnt 21.154406 hw_loss 0.005112 lr 0.00086028 rank 6
2023-02-18 06:18:28,771 DEBUG TRAIN Batch 2/2600 loss 23.558819 loss_att 32.308247 loss_ctc 29.579826 loss_rnnt 20.900217 hw_loss 0.198589 lr 0.00085924 rank 5
2023-02-18 06:18:28,775 DEBUG TRAIN Batch 2/2600 loss 63.206364 loss_att 72.310471 loss_ctc 96.949890 loss_rnnt 56.886379 hw_loss 0.000046 lr 0.00085932 rank 3
2023-02-18 06:18:28,774 DEBUG TRAIN Batch 2/2600 loss 57.076145 loss_att 69.191109 loss_ctc 79.145607 loss_rnnt 51.632507 hw_loss 0.146342 lr 0.00085808 rank 4
2023-02-18 06:18:28,779 DEBUG TRAIN Batch 2/2600 loss 52.181026 loss_att 64.650421 loss_ctc 59.448456 loss_rnnt 48.641502 hw_loss 0.143732 lr 0.00085924 rank 7
2023-02-18 06:18:28,782 DEBUG TRAIN Batch 2/2600 loss 33.037865 loss_att 42.531883 loss_ctc 43.394554 loss_rnnt 29.758114 hw_loss 0.000102 lr 0.00086076 rank 2
2023-02-18 06:18:28,806 DEBUG TRAIN Batch 2/2600 loss 33.443676 loss_att 45.887627 loss_ctc 44.639721 loss_rnnt 29.380926 hw_loss 0.152161 lr 0.00086216 rank 1
2023-02-18 06:18:28,835 DEBUG TRAIN Batch 2/2600 loss 34.417618 loss_att 57.091335 loss_ctc 47.215069 loss_rnnt 28.165766 hw_loss 0.020205 lr 0.00086136 rank 0
2023-02-18 06:19:29,892 DEBUG TRAIN Batch 2/2700 loss 28.395702 loss_att 48.933979 loss_ctc 45.658356 loss_rnnt 21.961746 hw_loss 0.046148 lr 0.00086416 rank 1
2023-02-18 06:19:29,892 DEBUG TRAIN Batch 2/2700 loss 46.302425 loss_att 60.562782 loss_ctc 64.339485 loss_rnnt 40.978752 hw_loss 0.124975 lr 0.00086336 rank 0
2023-02-18 06:19:29,893 DEBUG TRAIN Batch 2/2700 loss 43.384689 loss_att 60.547890 loss_ctc 64.928047 loss_rnnt 37.079525 hw_loss 0.000141 lr 0.00086276 rank 2
2023-02-18 06:19:29,893 DEBUG TRAIN Batch 2/2700 loss 63.351212 loss_att 70.770859 loss_ctc 87.935600 loss_rnnt 58.581627 hw_loss 0.014510 lr 0.00086124 rank 7
2023-02-18 06:19:29,894 DEBUG TRAIN Batch 2/2700 loss 23.679876 loss_att 37.493870 loss_ctc 41.550732 loss_rnnt 18.454334 hw_loss 0.149931 lr 0.00086124 rank 5
2023-02-18 06:19:29,894 DEBUG TRAIN Batch 2/2700 loss 50.058174 loss_att 68.644707 loss_ctc 69.055420 loss_rnnt 43.807808 hw_loss 0.000173 lr 0.00086228 rank 6
2023-02-18 06:19:29,895 DEBUG TRAIN Batch 2/2700 loss 22.830761 loss_att 36.803375 loss_ctc 40.768234 loss_rnnt 17.644463 hw_loss 0.000211 lr 0.00086132 rank 3
2023-02-18 06:19:29,897 DEBUG TRAIN Batch 2/2700 loss 35.416908 loss_att 40.770077 loss_ctc 49.335457 loss_rnnt 32.391617 hw_loss 0.185350 lr 0.00086008 rank 4
2023-02-18 06:20:29,714 DEBUG TRAIN Batch 2/2800 loss 61.326424 loss_att 97.173424 loss_ctc 83.619720 loss_rnnt 51.184547 hw_loss 0.000060 lr 0.00086616 rank 1
2023-02-18 06:20:29,715 DEBUG TRAIN Batch 2/2800 loss 34.663765 loss_att 45.679352 loss_ctc 47.480782 loss_rnnt 30.665583 hw_loss 0.161499 lr 0.00086428 rank 6
2023-02-18 06:20:29,718 DEBUG TRAIN Batch 2/2800 loss 58.371136 loss_att 82.950577 loss_ctc 82.350487 loss_rnnt 50.245205 hw_loss 0.023988 lr 0.00086208 rank 4
2023-02-18 06:20:29,719 DEBUG TRAIN Batch 2/2800 loss 56.022549 loss_att 60.066513 loss_ctc 74.757347 loss_rnnt 52.715725 hw_loss 0.000102 lr 0.00086476 rank 2
2023-02-18 06:20:29,719 DEBUG TRAIN Batch 2/2800 loss 60.238987 loss_att 79.419243 loss_ctc 90.700798 loss_rnnt 52.221100 hw_loss 0.225489 lr 0.00086324 rank 7
2023-02-18 06:20:29,721 DEBUG TRAIN Batch 2/2800 loss 39.914860 loss_att 54.830551 loss_ctc 65.474327 loss_rnnt 33.523727 hw_loss 0.000125 lr 0.00086332 rank 3
2023-02-18 06:20:29,724 DEBUG TRAIN Batch 2/2800 loss 23.362085 loss_att 30.058714 loss_ctc 31.571011 loss_rnnt 20.857616 hw_loss 0.132413 lr 0.00086536 rank 0
2023-02-18 06:20:29,728 DEBUG TRAIN Batch 2/2800 loss 94.753723 loss_att 112.434464 loss_ctc 139.025269 loss_rnnt 85.212677 hw_loss 0.191305 lr 0.00086324 rank 5
2023-02-18 06:21:27,698 DEBUG TRAIN Batch 2/2900 loss 44.408943 loss_att 49.697132 loss_ctc 65.716888 loss_rnnt 40.509136 hw_loss 0.002078 lr 0.00086524 rank 5
2023-02-18 06:21:27,706 DEBUG TRAIN Batch 2/2900 loss 23.969389 loss_att 27.594135 loss_ctc 36.819607 loss_rnnt 21.522461 hw_loss 0.016156 lr 0.00086736 rank 0
2023-02-18 06:21:27,707 DEBUG TRAIN Batch 2/2900 loss 23.626581 loss_att 31.873940 loss_ctc 37.108120 loss_rnnt 20.125603 hw_loss 0.101191 lr 0.00086524 rank 7
2023-02-18 06:21:27,710 DEBUG TRAIN Batch 2/2900 loss 54.446793 loss_att 66.768448 loss_ctc 70.480331 loss_rnnt 49.824062 hw_loss 0.038613 lr 0.00086628 rank 6
2023-02-18 06:21:27,713 DEBUG TRAIN Batch 2/2900 loss 39.145519 loss_att 55.692917 loss_ctc 66.776764 loss_rnnt 32.151779 hw_loss 0.000174 lr 0.00086532 rank 3
2023-02-18 06:21:27,713 DEBUG TRAIN Batch 2/2900 loss 53.005772 loss_att 59.751381 loss_ctc 74.449821 loss_rnnt 48.726940 hw_loss 0.132201 lr 0.00086816 rank 1
2023-02-18 06:21:27,720 DEBUG TRAIN Batch 2/2900 loss 75.037209 loss_att 85.413780 loss_ctc 94.358971 loss_rnnt 70.289833 hw_loss 0.179671 lr 0.00086676 rank 2
2023-02-18 06:21:27,769 DEBUG TRAIN Batch 2/2900 loss 47.672211 loss_att 74.347397 loss_ctc 67.911034 loss_rnnt 39.629211 hw_loss 0.017718 lr 0.00086408 rank 4
2023-02-18 06:22:26,888 DEBUG TRAIN Batch 2/3000 loss 54.700626 loss_att 67.108032 loss_ctc 67.242065 loss_rnnt 50.545742 hw_loss 0.002279 lr 0.00086724 rank 5
2023-02-18 06:22:26,889 DEBUG TRAIN Batch 2/3000 loss 30.485661 loss_att 51.690174 loss_ctc 43.201424 loss_rnnt 24.549261 hw_loss 0.000112 lr 0.00086876 rank 2
2023-02-18 06:22:26,889 DEBUG TRAIN Batch 2/3000 loss 51.503624 loss_att 66.860123 loss_ctc 66.586266 loss_rnnt 46.420792 hw_loss 0.000965 lr 0.00086828 rank 6
2023-02-18 06:22:26,890 DEBUG TRAIN Batch 2/3000 loss 35.134605 loss_att 49.230747 loss_ctc 56.347363 loss_rnnt 29.443985 hw_loss 0.080675 lr 0.00087016 rank 1
2023-02-18 06:22:26,890 DEBUG TRAIN Batch 2/3000 loss 43.162807 loss_att 58.739662 loss_ctc 65.920761 loss_rnnt 37.012352 hw_loss 0.001289 lr 0.00086936 rank 0
2023-02-18 06:22:26,895 DEBUG TRAIN Batch 2/3000 loss 33.077793 loss_att 50.827362 loss_ctc 62.176537 loss_rnnt 25.579967 hw_loss 0.127649 lr 0.00086608 rank 4
2023-02-18 06:22:26,900 DEBUG TRAIN Batch 2/3000 loss 26.278814 loss_att 43.620903 loss_ctc 42.512924 loss_rnnt 20.589359 hw_loss 0.105919 lr 0.00086732 rank 3
2023-02-18 06:22:26,900 DEBUG TRAIN Batch 2/3000 loss 48.797966 loss_att 58.233845 loss_ctc 68.748161 loss_rnnt 44.166302 hw_loss 0.158364 lr 0.00086724 rank 7
2023-02-18 06:23:26,237 DEBUG TRAIN Batch 2/3100 loss 37.775620 loss_att 52.552940 loss_ctc 62.496277 loss_rnnt 31.524025 hw_loss 0.000081 lr 0.00087216 rank 1
2023-02-18 06:23:26,246 DEBUG TRAIN Batch 2/3100 loss 67.529839 loss_att 89.768158 loss_ctc 97.482979 loss_rnnt 59.015427 hw_loss 0.136871 lr 0.00087076 rank 2
2023-02-18 06:23:26,245 DEBUG TRAIN Batch 2/3100 loss 35.959698 loss_att 58.841835 loss_ctc 59.599319 loss_rnnt 28.194027 hw_loss 0.069920 lr 0.00086924 rank 5
2023-02-18 06:23:26,249 DEBUG TRAIN Batch 2/3100 loss 44.004833 loss_att 85.334930 loss_ctc 65.029625 loss_rnnt 32.921257 hw_loss 0.026722 lr 0.00087028 rank 6
2023-02-18 06:23:26,252 DEBUG TRAIN Batch 2/3100 loss 37.353828 loss_att 63.412735 loss_ctc 41.434788 loss_rnnt 31.597807 hw_loss 0.000211 lr 0.00086932 rank 3
2023-02-18 06:23:26,255 DEBUG TRAIN Batch 2/3100 loss 21.467522 loss_att 39.095196 loss_ctc 28.887114 loss_rnnt 16.874287 hw_loss 0.147039 lr 0.00086808 rank 4
2023-02-18 06:23:26,260 DEBUG TRAIN Batch 2/3100 loss 81.569130 loss_att 98.357964 loss_ctc 110.892601 loss_rnnt 74.301544 hw_loss 0.000062 lr 0.00086924 rank 7
2023-02-18 06:23:26,305 DEBUG TRAIN Batch 2/3100 loss 26.246895 loss_att 38.814095 loss_ctc 40.561695 loss_rnnt 21.824770 hw_loss 0.000083 lr 0.00087136 rank 0
2023-02-18 06:24:24,766 DEBUG TRAIN Batch 2/3200 loss 32.298561 loss_att 46.030327 loss_ctc 50.881477 loss_rnnt 27.074459 hw_loss 0.000048 lr 0.00087416 rank 1
2023-02-18 06:24:24,767 DEBUG TRAIN Batch 2/3200 loss 60.088711 loss_att 69.281189 loss_ctc 86.089951 loss_rnnt 54.783348 hw_loss 0.000073 lr 0.00087124 rank 5
2023-02-18 06:24:24,767 DEBUG TRAIN Batch 2/3200 loss 47.055389 loss_att 54.674484 loss_ctc 61.559517 loss_rnnt 43.580589 hw_loss 0.032058 lr 0.00087228 rank 6
2023-02-18 06:24:24,770 DEBUG TRAIN Batch 2/3200 loss 37.579166 loss_att 40.268963 loss_ctc 47.955841 loss_rnnt 35.553158 hw_loss 0.195925 lr 0.00087124 rank 7
2023-02-18 06:24:24,772 DEBUG TRAIN Batch 2/3200 loss 15.234440 loss_att 18.689211 loss_ctc 23.613583 loss_rnnt 13.330311 hw_loss 0.179915 lr 0.00087008 rank 4
2023-02-18 06:24:24,781 DEBUG TRAIN Batch 2/3200 loss 22.425787 loss_att 27.949602 loss_ctc 31.463867 loss_rnnt 20.115559 hw_loss 0.000730 lr 0.00087132 rank 3
2023-02-18 06:24:24,800 DEBUG TRAIN Batch 2/3200 loss 33.881657 loss_att 44.007339 loss_ctc 52.501389 loss_rnnt 29.359722 hw_loss 0.026563 lr 0.00087276 rank 2
2023-02-18 06:24:24,831 DEBUG TRAIN Batch 2/3200 loss 34.194218 loss_att 36.191658 loss_ctc 49.591942 loss_rnnt 31.729290 hw_loss 0.023272 lr 0.00087336 rank 0
2023-02-18 06:25:25,234 DEBUG TRAIN Batch 2/3300 loss 42.575397 loss_att 63.620983 loss_ctc 71.763412 loss_rnnt 34.474472 hw_loss 0.000136 lr 0.00087324 rank 5
2023-02-18 06:25:25,235 DEBUG TRAIN Batch 2/3300 loss 33.802242 loss_att 42.655602 loss_ctc 49.149925 loss_rnnt 29.931736 hw_loss 0.100258 lr 0.00087428 rank 6
2023-02-18 06:25:25,235 DEBUG TRAIN Batch 2/3300 loss 33.747288 loss_att 46.601978 loss_ctc 55.969948 loss_rnnt 28.148136 hw_loss 0.122229 lr 0.00087208 rank 4
2023-02-18 06:25:25,238 DEBUG TRAIN Batch 2/3300 loss 15.128317 loss_att 24.009838 loss_ctc 23.293032 loss_rnnt 12.263236 hw_loss 0.000280 lr 0.00087476 rank 2
2023-02-18 06:25:25,240 DEBUG TRAIN Batch 2/3300 loss 36.743240 loss_att 52.583954 loss_ctc 49.775364 loss_rnnt 31.799896 hw_loss 0.070467 lr 0.00087536 rank 0
2023-02-18 06:25:25,242 DEBUG TRAIN Batch 2/3300 loss 28.215128 loss_att 42.993790 loss_ctc 58.394806 loss_rnnt 21.235340 hw_loss 0.000188 lr 0.00087332 rank 3
2023-02-18 06:25:25,242 DEBUG TRAIN Batch 2/3300 loss 57.434525 loss_att 65.630173 loss_ctc 78.121933 loss_rnnt 53.006927 hw_loss 0.056519 lr 0.00087616 rank 1
2023-02-18 06:25:25,245 DEBUG TRAIN Batch 2/3300 loss 25.978636 loss_att 45.252254 loss_ctc 43.289856 loss_rnnt 19.810976 hw_loss 0.008948 lr 0.00087324 rank 7
2023-02-18 06:26:24,680 DEBUG TRAIN Batch 2/3400 loss 72.416054 loss_att 77.830147 loss_ctc 103.976189 loss_rnnt 67.057846 hw_loss 0.126316 lr 0.00087816 rank 1
2023-02-18 06:26:24,682 DEBUG TRAIN Batch 2/3400 loss 19.768362 loss_att 41.428410 loss_ctc 23.675056 loss_rnnt 14.862379 hw_loss 0.099527 lr 0.00087736 rank 0
2023-02-18 06:26:24,684 DEBUG TRAIN Batch 2/3400 loss 22.124910 loss_att 39.909454 loss_ctc 29.530680 loss_rnnt 17.580484 hw_loss 0.000155 lr 0.00087676 rank 2
2023-02-18 06:26:24,685 DEBUG TRAIN Batch 2/3400 loss 86.190437 loss_att 90.788132 loss_ctc 113.230637 loss_rnnt 81.665512 hw_loss 0.000051 lr 0.00087524 rank 7
2023-02-18 06:26:24,690 DEBUG TRAIN Batch 2/3400 loss 86.455971 loss_att 121.592087 loss_ctc 103.357155 loss_rnnt 77.175201 hw_loss 0.000089 lr 0.00087524 rank 5
2023-02-18 06:26:24,706 DEBUG TRAIN Batch 2/3400 loss 32.625389 loss_att 50.457329 loss_ctc 51.038681 loss_rnnt 26.497862 hw_loss 0.198814 lr 0.00087532 rank 3
2023-02-18 06:26:24,717 DEBUG TRAIN Batch 2/3400 loss 32.941189 loss_att 46.964821 loss_ctc 54.741112 loss_rnnt 27.204306 hw_loss 0.047804 lr 0.00087628 rank 6
2023-02-18 06:26:24,724 DEBUG TRAIN Batch 2/3400 loss 41.289501 loss_att 49.884209 loss_ctc 58.365700 loss_rnnt 37.220524 hw_loss 0.137262 lr 0.00087408 rank 4
2023-02-18 06:28:15,843 DEBUG TRAIN Batch 2/3500 loss 36.695648 loss_att 46.054447 loss_ctc 46.507908 loss_rnnt 33.515564 hw_loss 0.000037 lr 0.00087608 rank 4
2023-02-18 06:28:15,845 DEBUG TRAIN Batch 2/3500 loss 90.801643 loss_att 99.727402 loss_ctc 111.559486 loss_rnnt 86.175140 hw_loss 0.138052 lr 0.00087724 rank 5
2023-02-18 06:28:15,851 DEBUG TRAIN Batch 2/3500 loss 63.203663 loss_att 77.410973 loss_ctc 72.920952 loss_rnnt 59.066540 hw_loss 0.000051 lr 0.00087936 rank 0
2023-02-18 06:28:15,852 DEBUG TRAIN Batch 2/3500 loss 21.377800 loss_att 32.573509 loss_ctc 35.211060 loss_rnnt 17.223392 hw_loss 0.132808 lr 0.00087876 rank 2
2023-02-18 06:28:15,853 DEBUG TRAIN Batch 2/3500 loss 36.483398 loss_att 54.753128 loss_ctc 61.911217 loss_rnnt 29.419670 hw_loss 0.036386 lr 0.00087732 rank 3
2023-02-18 06:28:15,853 DEBUG TRAIN Batch 2/3500 loss 21.462608 loss_att 31.359293 loss_ctc 27.489609 loss_rnnt 18.664181 hw_loss 0.029045 lr 0.00087724 rank 7
2023-02-18 06:28:15,856 DEBUG TRAIN Batch 2/3500 loss 76.948929 loss_att 84.646797 loss_ctc 98.995033 loss_rnnt 72.455605 hw_loss 0.026746 lr 0.00087828 rank 6
2023-02-18 06:28:15,855 DEBUG TRAIN Batch 2/3500 loss 38.307209 loss_att 42.687538 loss_ctc 69.319122 loss_rnnt 33.255379 hw_loss 0.076583 lr 0.00088016 rank 1
2023-02-18 06:29:15,811 DEBUG TRAIN Batch 2/3600 loss 47.519405 loss_att 59.613457 loss_ctc 55.659378 loss_rnnt 43.987724 hw_loss 0.051638 lr 0.00088136 rank 0
2023-02-18 06:29:15,814 DEBUG TRAIN Batch 2/3600 loss 53.378582 loss_att 71.736313 loss_ctc 92.564758 loss_rnnt 44.481964 hw_loss 0.000460 lr 0.00087924 rank 5
2023-02-18 06:29:15,818 DEBUG TRAIN Batch 2/3600 loss 34.671673 loss_att 51.880455 loss_ctc 58.245441 loss_rnnt 28.069332 hw_loss 0.032654 lr 0.00087924 rank 7
2023-02-18 06:29:15,823 DEBUG TRAIN Batch 2/3600 loss 31.963171 loss_att 48.576580 loss_ctc 46.725338 loss_rnnt 26.532850 hw_loss 0.261285 lr 0.00088076 rank 2
2023-02-18 06:29:15,838 DEBUG TRAIN Batch 2/3600 loss 26.641298 loss_att 41.303158 loss_ctc 48.993519 loss_rnnt 20.655937 hw_loss 0.136299 lr 0.00087932 rank 3
2023-02-18 06:29:15,873 DEBUG TRAIN Batch 2/3600 loss 33.426483 loss_att 51.353218 loss_ctc 44.045734 loss_rnnt 28.424994 hw_loss 0.000455 lr 0.00088028 rank 6
2023-02-18 06:29:15,874 DEBUG TRAIN Batch 2/3600 loss 27.747519 loss_att 39.220428 loss_ctc 35.862823 loss_rnnt 24.328026 hw_loss 0.080380 lr 0.00088216 rank 1
2023-02-18 06:29:15,878 DEBUG TRAIN Batch 2/3600 loss 34.735886 loss_att 46.755669 loss_ctc 56.552841 loss_rnnt 29.422842 hw_loss 0.000295 lr 0.00087808 rank 4
2023-02-18 06:30:14,365 DEBUG TRAIN Batch 2/3700 loss 25.251841 loss_att 38.462845 loss_ctc 47.790844 loss_rnnt 19.505106 hw_loss 0.186247 lr 0.00088336 rank 0
2023-02-18 06:30:14,366 DEBUG TRAIN Batch 2/3700 loss 39.060814 loss_att 73.173553 loss_ctc 57.728558 loss_rnnt 29.633755 hw_loss 0.216526 lr 0.00088132 rank 3
2023-02-18 06:30:14,367 DEBUG TRAIN Batch 2/3700 loss 21.287148 loss_att 30.132259 loss_ctc 26.597317 loss_rnnt 18.739332 hw_loss 0.132692 lr 0.00088416 rank 1
2023-02-18 06:30:14,368 DEBUG TRAIN Batch 2/3700 loss 46.462517 loss_att 59.643021 loss_ctc 63.791103 loss_rnnt 41.515900 hw_loss 0.000069 lr 0.00088124 rank 7
2023-02-18 06:30:14,378 DEBUG TRAIN Batch 2/3700 loss 33.546722 loss_att 39.983196 loss_ctc 43.551933 loss_rnnt 30.925364 hw_loss 0.000069 lr 0.00088008 rank 4
2023-02-18 06:30:14,380 DEBUG TRAIN Batch 2/3700 loss 22.122101 loss_att 28.354942 loss_ctc 22.960754 loss_rnnt 20.763666 hw_loss 0.000088 lr 0.00088228 rank 6
2023-02-18 06:30:14,381 DEBUG TRAIN Batch 2/3700 loss 18.141806 loss_att 31.486301 loss_ctc 36.496048 loss_rnnt 13.025614 hw_loss 0.000109 lr 0.00088124 rank 5
2023-02-18 06:30:14,440 DEBUG TRAIN Batch 2/3700 loss 15.970985 loss_att 44.418510 loss_ctc 31.568590 loss_rnnt 8.201730 hw_loss 0.000129 lr 0.00088276 rank 2
2023-02-18 06:31:12,801 DEBUG TRAIN Batch 2/3800 loss 42.955486 loss_att 58.284657 loss_ctc 69.402992 loss_rnnt 36.363121 hw_loss 0.000369 lr 0.00088208 rank 4
2023-02-18 06:31:12,803 DEBUG TRAIN Batch 2/3800 loss 23.648333 loss_att 27.221039 loss_ctc 33.030563 loss_rnnt 21.658009 hw_loss 0.046531 lr 0.00088536 rank 0
2023-02-18 06:31:12,803 DEBUG TRAIN Batch 2/3800 loss 38.065006 loss_att 54.733047 loss_ctc 50.603733 loss_rnnt 33.010761 hw_loss 0.091503 lr 0.00088616 rank 1
2023-02-18 06:31:12,803 DEBUG TRAIN Batch 2/3800 loss 49.024246 loss_att 66.001030 loss_ctc 78.692696 loss_rnnt 41.614296 hw_loss 0.110250 lr 0.00088428 rank 6
2023-02-18 06:31:12,807 DEBUG TRAIN Batch 2/3800 loss 32.554695 loss_att 42.678719 loss_ctc 47.400295 loss_rnnt 28.550188 hw_loss 0.000532 lr 0.00088476 rank 2
2023-02-18 06:31:12,809 DEBUG TRAIN Batch 2/3800 loss 45.572765 loss_att 60.556107 loss_ctc 70.288033 loss_rnnt 39.280453 hw_loss 0.000513 lr 0.00088324 rank 5
2023-02-18 06:31:12,810 DEBUG TRAIN Batch 2/3800 loss 19.142464 loss_att 30.372356 loss_ctc 38.780632 loss_rnnt 14.277865 hw_loss 0.000371 lr 0.00088324 rank 7
2023-02-18 06:31:12,813 DEBUG TRAIN Batch 2/3800 loss 32.442070 loss_att 48.429031 loss_ctc 51.540333 loss_rnnt 26.685341 hw_loss 0.024183 lr 0.00088332 rank 3
2023-02-18 06:32:13,839 DEBUG TRAIN Batch 2/3900 loss 29.850899 loss_att 38.472717 loss_ctc 43.555691 loss_rnnt 26.228273 hw_loss 0.133038 lr 0.00088524 rank 7
2023-02-18 06:32:13,841 DEBUG TRAIN Batch 2/3900 loss 71.757721 loss_att 96.478722 loss_ctc 106.355141 loss_rnnt 62.176971 hw_loss 0.044175 lr 0.00088736 rank 0
2023-02-18 06:32:13,841 DEBUG TRAIN Batch 2/3900 loss 46.847260 loss_att 61.359615 loss_ctc 61.030640 loss_rnnt 41.977280 hw_loss 0.143233 lr 0.00088816 rank 1
2023-02-18 06:32:13,841 DEBUG TRAIN Batch 2/3900 loss 30.824018 loss_att 43.458817 loss_ctc 64.722969 loss_rnnt 23.717613 hw_loss 0.111716 lr 0.00088676 rank 2
2023-02-18 06:32:13,842 DEBUG TRAIN Batch 2/3900 loss 47.833000 loss_att 71.033264 loss_ctc 74.206398 loss_rnnt 39.545975 hw_loss 0.244723 lr 0.00088524 rank 5
2023-02-18 06:32:13,844 DEBUG TRAIN Batch 2/3900 loss 56.168152 loss_att 78.913116 loss_ctc 93.137596 loss_rnnt 46.550041 hw_loss 0.262244 lr 0.00088532 rank 3
2023-02-18 06:32:13,845 DEBUG TRAIN Batch 2/3900 loss 53.392689 loss_att 71.545029 loss_ctc 62.765053 loss_rnnt 48.512527 hw_loss 0.000086 lr 0.00088628 rank 6
2023-02-18 06:32:13,847 DEBUG TRAIN Batch 2/3900 loss 48.687725 loss_att 58.904911 loss_ctc 71.410744 loss_rnnt 43.550842 hw_loss 0.119446 lr 0.00088408 rank 4
2023-02-18 06:33:11,381 DEBUG TRAIN Batch 2/4000 loss 97.968872 loss_att 112.801620 loss_ctc 156.208023 loss_rnnt 87.207069 hw_loss 0.056312 lr 0.00088724 rank 5
2023-02-18 06:33:11,385 DEBUG TRAIN Batch 2/4000 loss 29.021307 loss_att 41.785530 loss_ctc 44.151047 loss_rnnt 24.357103 hw_loss 0.176357 lr 0.00088876 rank 2
2023-02-18 06:33:11,386 DEBUG TRAIN Batch 2/4000 loss 49.081463 loss_att 69.797653 loss_ctc 60.647408 loss_rnnt 43.332397 hw_loss 0.119443 lr 0.00088732 rank 3
2023-02-18 06:33:11,387 DEBUG TRAIN Batch 2/4000 loss 31.438753 loss_att 39.834450 loss_ctc 42.251629 loss_rnnt 28.317833 hw_loss 0.000119 lr 0.00088936 rank 0
2023-02-18 06:33:11,387 DEBUG TRAIN Batch 2/4000 loss 37.584408 loss_att 66.805771 loss_ctc 46.289814 loss_rnnt 30.531590 hw_loss 0.089666 lr 0.00089016 rank 1
2023-02-18 06:33:11,388 DEBUG TRAIN Batch 2/4000 loss 61.178558 loss_att 71.811264 loss_ctc 89.887444 loss_rnnt 55.099487 hw_loss 0.233764 lr 0.00088828 rank 6
2023-02-18 06:33:11,388 DEBUG TRAIN Batch 2/4000 loss 39.127911 loss_att 67.531128 loss_ctc 76.577568 loss_rnnt 28.407915 hw_loss 0.086377 lr 0.00088608 rank 4
2023-02-18 06:33:11,395 DEBUG TRAIN Batch 2/4000 loss 51.564232 loss_att 63.297775 loss_ctc 55.923908 loss_rnnt 48.636169 hw_loss 0.000111 lr 0.00088724 rank 7
2023-02-18 06:34:09,153 DEBUG TRAIN Batch 2/4100 loss 33.261101 loss_att 51.202656 loss_ctc 49.066628 loss_rnnt 27.466583 hw_loss 0.185265 lr 0.00089028 rank 6
2023-02-18 06:34:09,153 DEBUG TRAIN Batch 2/4100 loss 73.298431 loss_att 80.343430 loss_ctc 88.827354 loss_rnnt 69.818771 hw_loss 0.000248 lr 0.00089216 rank 1
2023-02-18 06:34:09,155 DEBUG TRAIN Batch 2/4100 loss 24.515987 loss_att 39.210320 loss_ctc 45.556084 loss_rnnt 18.768375 hw_loss 0.006376 lr 0.00088924 rank 5
2023-02-18 06:34:09,157 DEBUG TRAIN Batch 2/4100 loss 18.646805 loss_att 28.530937 loss_ctc 25.408474 loss_rnnt 15.768240 hw_loss 0.000342 lr 0.00089136 rank 0
2023-02-18 06:34:09,158 DEBUG TRAIN Batch 2/4100 loss 29.097889 loss_att 42.891861 loss_ctc 47.047527 loss_rnnt 23.917255 hw_loss 0.053536 lr 0.00089076 rank 2
2023-02-18 06:34:09,158 DEBUG TRAIN Batch 2/4100 loss 45.709858 loss_att 68.356110 loss_ctc 76.459526 loss_rnnt 37.040260 hw_loss 0.075733 lr 0.00088924 rank 7
2023-02-18 06:34:09,159 DEBUG TRAIN Batch 2/4100 loss 44.403210 loss_att 50.526482 loss_ctc 64.594147 loss_rnnt 40.480301 hw_loss 0.011494 lr 0.00088808 rank 4
2023-02-18 06:34:09,163 DEBUG TRAIN Batch 2/4100 loss 42.844982 loss_att 60.652641 loss_ctc 73.411758 loss_rnnt 35.182755 hw_loss 0.047109 lr 0.00088932 rank 3
2023-02-18 06:35:10,420 DEBUG TRAIN Batch 2/4200 loss 18.741089 loss_att 32.560406 loss_ctc 28.262260 loss_rnnt 14.707660 hw_loss 0.000145 lr 0.00089228 rank 6
2023-02-18 06:35:10,420 DEBUG TRAIN Batch 2/4200 loss 83.940849 loss_att 120.562241 loss_ctc 134.813080 loss_rnnt 69.833557 hw_loss 0.000087 lr 0.00089008 rank 4
2023-02-18 06:35:10,423 DEBUG TRAIN Batch 2/4200 loss 22.443777 loss_att 34.057693 loss_ctc 26.435732 loss_rnnt 19.527924 hw_loss 0.114014 lr 0.00089124 rank 7
2023-02-18 06:35:10,429 DEBUG TRAIN Batch 2/4200 loss 63.550316 loss_att 74.839325 loss_ctc 101.172501 loss_rnnt 56.177376 hw_loss 0.185337 lr 0.00089124 rank 5
2023-02-18 06:35:10,434 DEBUG TRAIN Batch 2/4200 loss 20.185083 loss_att 28.576569 loss_ctc 25.298075 loss_rnnt 17.824907 hw_loss 0.000271 lr 0.00089132 rank 3
2023-02-18 06:35:10,468 DEBUG TRAIN Batch 2/4200 loss 43.928654 loss_att 65.701981 loss_ctc 64.290230 loss_rnnt 36.859047 hw_loss 0.000124 lr 0.00089336 rank 0
2023-02-18 06:35:10,468 DEBUG TRAIN Batch 2/4200 loss 21.444693 loss_att 26.680601 loss_ctc 37.130173 loss_rnnt 18.306021 hw_loss 0.000177 lr 0.00089276 rank 2
2023-02-18 06:35:10,481 DEBUG TRAIN Batch 2/4200 loss 23.589510 loss_att 32.957249 loss_ctc 45.935352 loss_rnnt 18.736443 hw_loss 0.000140 lr 0.00089416 rank 1
2023-02-18 06:37:01,121 DEBUG TRAIN Batch 2/4300 loss 32.663929 loss_att 38.885723 loss_ctc 56.750393 loss_rnnt 28.207907 hw_loss 0.000257 lr 0.00089476 rank 2
2023-02-18 06:37:01,121 DEBUG TRAIN Batch 2/4300 loss 48.777328 loss_att 73.630920 loss_ctc 57.435459 loss_rnnt 42.652096 hw_loss 0.000177 lr 0.00089616 rank 1
2023-02-18 06:37:01,126 DEBUG TRAIN Batch 2/4300 loss 39.157124 loss_att 48.661575 loss_ctc 67.901222 loss_rnnt 33.373482 hw_loss 0.094126 lr 0.00089208 rank 4
2023-02-18 06:37:01,127 DEBUG TRAIN Batch 2/4300 loss 23.439203 loss_att 35.229355 loss_ctc 42.362877 loss_rnnt 18.557869 hw_loss 0.000279 lr 0.00089324 rank 5
2023-02-18 06:37:01,129 DEBUG TRAIN Batch 2/4300 loss 38.725121 loss_att 48.107857 loss_ctc 67.309822 loss_rnnt 32.964142 hw_loss 0.137134 lr 0.00089536 rank 0
2023-02-18 06:37:01,132 DEBUG TRAIN Batch 2/4300 loss 68.820389 loss_att 104.987228 loss_ctc 90.118416 loss_rnnt 58.689926 hw_loss 0.107534 lr 0.00089324 rank 7
2023-02-18 06:37:01,133 DEBUG TRAIN Batch 2/4300 loss 30.741690 loss_att 34.115200 loss_ctc 39.854164 loss_rnnt 28.851795 hw_loss 0.000365 lr 0.00089332 rank 3
2023-02-18 06:37:01,183 DEBUG TRAIN Batch 2/4300 loss 34.685295 loss_att 72.753181 loss_ctc 37.232788 loss_rnnt 26.640902 hw_loss 0.170906 lr 0.00089428 rank 6
2023-02-18 06:38:03,913 DEBUG TRAIN Batch 2/4400 loss 34.353405 loss_att 39.731663 loss_ctc 47.318935 loss_rnnt 31.548943 hw_loss 0.000136 lr 0.00089524 rank 5
2023-02-18 06:38:03,917 DEBUG TRAIN Batch 2/4400 loss 55.477921 loss_att 69.057312 loss_ctc 73.781418 loss_rnnt 50.271736 hw_loss 0.093440 lr 0.00089524 rank 7
2023-02-18 06:38:03,918 DEBUG TRAIN Batch 2/4400 loss 47.345390 loss_att 60.181480 loss_ctc 68.666611 loss_rnnt 41.846859 hw_loss 0.165911 lr 0.00089676 rank 2
2023-02-18 06:38:03,919 DEBUG TRAIN Batch 2/4400 loss 69.408096 loss_att 89.686668 loss_ctc 93.314240 loss_rnnt 62.120525 hw_loss 0.083200 lr 0.00089628 rank 6
2023-02-18 06:38:03,923 DEBUG TRAIN Batch 2/4400 loss 45.914330 loss_att 59.115334 loss_ctc 61.726028 loss_rnnt 41.105358 hw_loss 0.113527 lr 0.00089532 rank 3
2023-02-18 06:38:03,953 DEBUG TRAIN Batch 2/4400 loss 40.062824 loss_att 53.779953 loss_ctc 79.451004 loss_rnnt 32.023895 hw_loss 0.082021 lr 0.00089816 rank 1
2023-02-18 06:38:03,968 DEBUG TRAIN Batch 2/4400 loss 26.926620 loss_att 50.883583 loss_ctc 37.704700 loss_rnnt 20.647926 hw_loss 0.094173 lr 0.00089736 rank 0
2023-02-18 06:38:03,978 DEBUG TRAIN Batch 2/4400 loss 40.064316 loss_att 62.439316 loss_ctc 56.462276 loss_rnnt 33.378372 hw_loss 0.046020 lr 0.00089408 rank 4
2023-02-18 06:39:04,944 DEBUG TRAIN Batch 2/4500 loss 23.263269 loss_att 38.377563 loss_ctc 47.146885 loss_rnnt 17.055889 hw_loss 0.000070 lr 0.00089724 rank 5
2023-02-18 06:39:04,948 DEBUG TRAIN Batch 2/4500 loss 41.184460 loss_att 61.757790 loss_ctc 67.400665 loss_rnnt 33.574276 hw_loss 0.000048 lr 0.00089724 rank 7
2023-02-18 06:39:04,950 DEBUG TRAIN Batch 2/4500 loss 32.682625 loss_att 44.355484 loss_ctc 45.841640 loss_rnnt 28.593479 hw_loss 0.000071 lr 0.00089876 rank 2
2023-02-18 06:39:04,952 DEBUG TRAIN Batch 2/4500 loss 123.120087 loss_att 118.717682 loss_ctc 123.586472 loss_rnnt 123.904373 hw_loss 0.063740 lr 0.00089828 rank 6
2023-02-18 06:39:04,955 DEBUG TRAIN Batch 2/4500 loss 49.223293 loss_att 67.246918 loss_ctc 60.882420 loss_rnnt 44.063950 hw_loss 0.000122 lr 0.00089732 rank 3
2023-02-18 06:39:04,987 DEBUG TRAIN Batch 2/4500 loss 24.244654 loss_att 41.594555 loss_ctc 43.901089 loss_rnnt 18.081770 hw_loss 0.135089 lr 0.00089608 rank 4
2023-02-18 06:39:04,987 DEBUG TRAIN Batch 2/4500 loss 51.321167 loss_att 60.580238 loss_ctc 63.734104 loss_rnnt 47.814259 hw_loss 0.000070 lr 0.00090016 rank 1
2023-02-18 06:39:05,009 DEBUG TRAIN Batch 2/4500 loss 40.975590 loss_att 61.248146 loss_ctc 55.697556 loss_rnnt 34.958122 hw_loss 0.000050 lr 0.00089936 rank 0
2023-02-18 06:40:02,027 DEBUG TRAIN Batch 2/4600 loss 34.852364 loss_att 53.208382 loss_ctc 53.409599 loss_rnnt 28.706825 hw_loss 0.000068 lr 0.00089924 rank 5
2023-02-18 06:40:02,029 DEBUG TRAIN Batch 2/4600 loss 28.606716 loss_att 42.939968 loss_ctc 47.963303 loss_rnnt 23.103813 hw_loss 0.103823 lr 0.00090076 rank 2
2023-02-18 06:40:02,030 DEBUG TRAIN Batch 2/4600 loss 44.928810 loss_att 54.689835 loss_ctc 57.642471 loss_rnnt 41.281429 hw_loss 0.000041 lr 0.00090216 rank 1
2023-02-18 06:40:02,033 DEBUG TRAIN Batch 2/4600 loss 40.962345 loss_att 61.534721 loss_ctc 51.415134 loss_rnnt 35.454140 hw_loss 0.000044 lr 0.00089808 rank 4
2023-02-18 06:40:02,035 DEBUG TRAIN Batch 2/4600 loss 38.840187 loss_att 62.025410 loss_ctc 58.927235 loss_rnnt 31.524839 hw_loss 0.000054 lr 0.00090028 rank 6
2023-02-18 06:40:02,038 DEBUG TRAIN Batch 2/4600 loss 34.178547 loss_att 51.971870 loss_ctc 53.325264 loss_rnnt 28.066961 hw_loss 0.000044 lr 0.00089924 rank 7
2023-02-18 06:40:02,041 DEBUG TRAIN Batch 2/4600 loss 33.290794 loss_att 42.243122 loss_ctc 55.038067 loss_rnnt 28.563087 hw_loss 0.070508 lr 0.00089932 rank 3
2023-02-18 06:40:02,098 DEBUG TRAIN Batch 2/4600 loss 14.700886 loss_att 25.132145 loss_ctc 27.629957 loss_rnnt 10.890734 hw_loss 0.000043 lr 0.00090136 rank 0
2023-02-18 06:40:59,879 DEBUG TRAIN Batch 2/4700 loss 63.721680 loss_att 85.603081 loss_ctc 94.692688 loss_rnnt 55.186073 hw_loss 0.055984 lr 0.00090416 rank 1
2023-02-18 06:40:59,882 DEBUG TRAIN Batch 2/4700 loss 30.543318 loss_att 37.134827 loss_ctc 42.494614 loss_rnnt 27.631447 hw_loss 0.000113 lr 0.00090132 rank 3
2023-02-18 06:40:59,884 DEBUG TRAIN Batch 2/4700 loss 42.777096 loss_att 61.030750 loss_ctc 53.109875 loss_rnnt 37.683475 hw_loss 0.122223 lr 0.00090124 rank 5
2023-02-18 06:40:59,885 DEBUG TRAIN Batch 2/4700 loss 13.883331 loss_att 31.975891 loss_ctc 25.633717 loss_rnnt 8.681391 hw_loss 0.031332 lr 0.00090336 rank 0
2023-02-18 06:40:59,885 DEBUG TRAIN Batch 2/4700 loss 37.723877 loss_att 59.239704 loss_ctc 56.988369 loss_rnnt 30.816097 hw_loss 0.067533 lr 0.00090276 rank 2
2023-02-18 06:40:59,886 DEBUG TRAIN Batch 2/4700 loss 46.137535 loss_att 50.702618 loss_ctc 65.226707 loss_rnnt 42.606316 hw_loss 0.136836 lr 0.00090228 rank 6
2023-02-18 06:40:59,894 DEBUG TRAIN Batch 2/4700 loss 21.792883 loss_att 36.116093 loss_ctc 36.494804 loss_rnnt 16.963409 hw_loss 0.008579 lr 0.00090124 rank 7
2023-02-18 06:40:59,939 DEBUG TRAIN Batch 2/4700 loss 27.477303 loss_att 37.893158 loss_ctc 35.541389 loss_rnnt 24.318863 hw_loss 0.000110 lr 0.00090008 rank 4
2023-02-18 06:41:59,549 DEBUG TRAIN Batch 2/4800 loss 13.336364 loss_att 23.685673 loss_ctc 25.227316 loss_rnnt 9.643765 hw_loss 0.069893 lr 0.00090616 rank 1
2023-02-18 06:41:59,551 DEBUG TRAIN Batch 2/4800 loss 33.809917 loss_att 49.940079 loss_ctc 54.668747 loss_rnnt 27.735462 hw_loss 0.126090 lr 0.00090428 rank 6
2023-02-18 06:41:59,553 DEBUG TRAIN Batch 2/4800 loss 73.119720 loss_att 102.483093 loss_ctc 128.114792 loss_rnnt 59.914337 hw_loss 0.000049 lr 0.00090324 rank 5
2023-02-18 06:41:59,554 DEBUG TRAIN Batch 2/4800 loss 27.701893 loss_att 43.592358 loss_ctc 50.303032 loss_rnnt 21.487028 hw_loss 0.043661 lr 0.00090476 rank 2
2023-02-18 06:41:59,554 DEBUG TRAIN Batch 2/4800 loss 17.769480 loss_att 33.299763 loss_ctc 32.853130 loss_rnnt 12.652250 hw_loss 0.000038 lr 0.00090536 rank 0
2023-02-18 06:41:59,557 DEBUG TRAIN Batch 2/4800 loss 17.840725 loss_att 36.275333 loss_ctc 18.498260 loss_rnnt 14.066067 hw_loss 0.000124 lr 0.00090332 rank 3
2023-02-18 06:41:59,557 DEBUG TRAIN Batch 2/4800 loss 22.690781 loss_att 36.086349 loss_ctc 33.395432 loss_rnnt 18.526630 hw_loss 0.108281 lr 0.00090208 rank 4
2023-02-18 06:41:59,574 DEBUG TRAIN Batch 2/4800 loss 76.035545 loss_att 87.781891 loss_ctc 104.336411 loss_rnnt 69.854370 hw_loss 0.109620 lr 0.00090324 rank 7
2023-02-18 06:42:56,610 DEBUG TRAIN Batch 2/4900 loss 30.651171 loss_att 54.484196 loss_ctc 42.951138 loss_rnnt 24.205360 hw_loss 0.073516 lr 0.00090736 rank 0
2023-02-18 06:42:56,610 DEBUG TRAIN Batch 2/4900 loss 26.691385 loss_att 46.283470 loss_ctc 37.879200 loss_rnnt 21.201460 hw_loss 0.149625 lr 0.00090816 rank 1
2023-02-18 06:42:56,611 DEBUG TRAIN Batch 2/4900 loss 17.652824 loss_att 28.694639 loss_ctc 22.669167 loss_rnnt 14.775558 hw_loss 0.000112 lr 0.00090676 rank 2
2023-02-18 06:42:56,613 DEBUG TRAIN Batch 2/4900 loss 24.641634 loss_att 35.412941 loss_ctc 39.643707 loss_rnnt 20.487066 hw_loss 0.000057 lr 0.00090628 rank 6
2023-02-18 06:42:56,614 DEBUG TRAIN Batch 2/4900 loss 32.484837 loss_att 42.912163 loss_ctc 35.339970 loss_rnnt 29.951902 hw_loss 0.125227 lr 0.00090524 rank 5
2023-02-18 06:42:56,616 DEBUG TRAIN Batch 2/4900 loss 44.452175 loss_att 56.571293 loss_ctc 67.461319 loss_rnnt 38.936878 hw_loss 0.044220 lr 0.00090532 rank 3
2023-02-18 06:42:56,618 DEBUG TRAIN Batch 2/4900 loss 18.630114 loss_att 24.871912 loss_ctc 30.716215 loss_rnnt 15.689161 hw_loss 0.152089 lr 0.00090408 rank 4
2023-02-18 06:42:56,678 DEBUG TRAIN Batch 2/4900 loss 22.348850 loss_att 34.069855 loss_ctc 36.990726 loss_rnnt 18.052382 hw_loss 0.000040 lr 0.00090524 rank 7
2023-02-18 06:43:55,074 DEBUG TRAIN Batch 2/5000 loss 34.478321 loss_att 54.130901 loss_ctc 46.331139 loss_rnnt 28.886292 hw_loss 0.152127 lr 0.00090936 rank 0
2023-02-18 06:43:55,075 DEBUG TRAIN Batch 2/5000 loss 23.140726 loss_att 31.615650 loss_ctc 35.721428 loss_rnnt 19.677502 hw_loss 0.170274 lr 0.00090724 rank 7
2023-02-18 06:43:55,076 DEBUG TRAIN Batch 2/5000 loss 34.283878 loss_att 41.634926 loss_ctc 45.446182 loss_rnnt 31.294111 hw_loss 0.058593 lr 0.00090608 rank 4
2023-02-18 06:43:55,076 DEBUG TRAIN Batch 2/5000 loss 23.438345 loss_att 37.167625 loss_ctc 39.340080 loss_rnnt 18.515463 hw_loss 0.106490 lr 0.00090724 rank 5
2023-02-18 06:43:55,077 DEBUG TRAIN Batch 2/5000 loss 31.188852 loss_att 44.722046 loss_ctc 44.385769 loss_rnnt 26.682800 hw_loss 0.074668 lr 0.00091016 rank 1
2023-02-18 06:43:55,077 DEBUG TRAIN Batch 2/5000 loss 52.127968 loss_att 70.034958 loss_ctc 76.930634 loss_rnnt 45.177338 hw_loss 0.116648 lr 0.00090828 rank 6
2023-02-18 06:43:55,081 DEBUG TRAIN Batch 2/5000 loss 55.701424 loss_att 71.708633 loss_ctc 74.132217 loss_rnnt 49.950863 hw_loss 0.171902 lr 0.00090732 rank 3
2023-02-18 06:43:55,082 DEBUG TRAIN Batch 2/5000 loss 58.901787 loss_att 71.384750 loss_ctc 73.410080 loss_rnnt 54.432976 hw_loss 0.070835 lr 0.00090876 rank 2
2023-02-18 06:44:56,328 DEBUG TRAIN Batch 2/5100 loss 26.359104 loss_att 43.688972 loss_ctc 46.152805 loss_rnnt 20.088654 hw_loss 0.309967 lr 0.00090924 rank 5
2023-02-18 06:44:56,331 DEBUG TRAIN Batch 2/5100 loss 45.348789 loss_att 67.832047 loss_ctc 59.990429 loss_rnnt 38.899788 hw_loss 0.000239 lr 0.00091136 rank 0
2023-02-18 06:44:56,332 DEBUG TRAIN Batch 2/5100 loss 33.811275 loss_att 58.945660 loss_ctc 50.647995 loss_rnnt 26.491190 hw_loss 0.090582 lr 0.00090808 rank 4
2023-02-18 06:44:56,336 DEBUG TRAIN Batch 2/5100 loss 38.712582 loss_att 61.587921 loss_ctc 64.055649 loss_rnnt 30.685732 hw_loss 0.136314 lr 0.00091076 rank 2
2023-02-18 06:44:56,338 DEBUG TRAIN Batch 2/5100 loss 56.758179 loss_att 65.356247 loss_ctc 62.428101 loss_rnnt 54.194443 hw_loss 0.165257 lr 0.00090932 rank 3
2023-02-18 06:44:56,339 DEBUG TRAIN Batch 2/5100 loss 17.108114 loss_att 24.968063 loss_ctc 27.934330 loss_rnnt 14.056005 hw_loss 0.068669 lr 0.00090924 rank 7
2023-02-18 06:44:56,344 DEBUG TRAIN Batch 2/5100 loss 8.051860 loss_att 21.473434 loss_ctc 15.182572 loss_rnnt 4.347195 hw_loss 0.130479 lr 0.00091216 rank 1
2023-02-18 06:44:56,351 DEBUG TRAIN Batch 2/5100 loss 32.381413 loss_att 41.918198 loss_ctc 58.909237 loss_rnnt 26.878300 hw_loss 0.110079 lr 0.00091028 rank 6
2023-02-18 06:46:49,310 DEBUG TRAIN Batch 2/5200 loss 28.102951 loss_att 33.111904 loss_ctc 38.663887 loss_rnnt 25.692999 hw_loss 0.000071 lr 0.00091124 rank 5
2023-02-18 06:46:49,311 DEBUG TRAIN Batch 2/5200 loss 35.369434 loss_att 40.001244 loss_ctc 46.951088 loss_rnnt 32.858620 hw_loss 0.075437 lr 0.00091416 rank 1
2023-02-18 06:46:49,318 DEBUG TRAIN Batch 2/5200 loss 23.803432 loss_att 28.212545 loss_ctc 36.517838 loss_rnnt 21.114000 hw_loss 0.210665 lr 0.00091008 rank 4
2023-02-18 06:46:49,320 DEBUG TRAIN Batch 2/5200 loss 21.965775 loss_att 21.055666 loss_ctc 28.992306 loss_rnnt 21.100670 hw_loss 0.206729 lr 0.00091276 rank 2
2023-02-18 06:46:49,321 DEBUG TRAIN Batch 2/5200 loss 23.600824 loss_att 36.242229 loss_ctc 35.749962 loss_rnnt 19.386944 hw_loss 0.123214 lr 0.00091228 rank 6
2023-02-18 06:46:49,321 DEBUG TRAIN Batch 2/5200 loss 40.245457 loss_att 42.947975 loss_ctc 48.030041 loss_rnnt 38.580925 hw_loss 0.161405 lr 0.00091336 rank 0
2023-02-18 06:46:49,328 DEBUG TRAIN Batch 2/5200 loss 37.577240 loss_att 50.203388 loss_ctc 51.812084 loss_rnnt 33.118378 hw_loss 0.066843 lr 0.00091132 rank 3
2023-02-18 06:46:49,447 DEBUG TRAIN Batch 2/5200 loss 67.874588 loss_att 94.993958 loss_ctc 99.477814 loss_rnnt 58.236919 hw_loss 0.000055 lr 0.00091124 rank 7
2023-02-18 06:47:47,942 DEBUG TRAIN Batch 2/5300 loss 46.454922 loss_att 62.786388 loss_ctc 71.439262 loss_rnnt 39.813248 hw_loss 0.082762 lr 0.00091332 rank 3
2023-02-18 06:47:47,942 DEBUG TRAIN Batch 2/5300 loss 81.268753 loss_att 95.596527 loss_ctc 104.810249 loss_rnnt 75.235077 hw_loss 0.054852 lr 0.00091536 rank 0
2023-02-18 06:47:47,943 DEBUG TRAIN Batch 2/5300 loss 46.784489 loss_att 56.689091 loss_ctc 62.611134 loss_rnnt 42.679058 hw_loss 0.026801 lr 0.00091324 rank 7
2023-02-18 06:47:47,945 DEBUG TRAIN Batch 2/5300 loss 31.161451 loss_att 48.063980 loss_ctc 56.098335 loss_rnnt 24.444971 hw_loss 0.020732 lr 0.00091324 rank 5
2023-02-18 06:47:47,948 DEBUG TRAIN Batch 2/5300 loss 36.806366 loss_att 49.034149 loss_ctc 61.099403 loss_rnnt 31.074734 hw_loss 0.088136 lr 0.00091208 rank 4
2023-02-18 06:47:47,949 DEBUG TRAIN Batch 2/5300 loss 109.127197 loss_att 121.798828 loss_ctc 125.581940 loss_rnnt 104.366600 hw_loss 0.060580 lr 0.00091476 rank 2
2023-02-18 06:47:47,950 DEBUG TRAIN Batch 2/5300 loss 20.491096 loss_att 33.816013 loss_ctc 30.986713 loss_rnnt 16.381922 hw_loss 0.083952 lr 0.00091616 rank 1
2023-02-18 06:47:47,951 DEBUG TRAIN Batch 2/5300 loss 50.133430 loss_att 62.199364 loss_ctc 62.644463 loss_rnnt 46.051044 hw_loss 0.001987 lr 0.00091428 rank 6
2023-02-18 06:48:46,955 DEBUG TRAIN Batch 2/5400 loss 50.063950 loss_att 55.710892 loss_ctc 74.638641 loss_rnnt 45.587551 hw_loss 0.131971 lr 0.00091816 rank 1
2023-02-18 06:48:46,955 DEBUG TRAIN Batch 2/5400 loss 38.820637 loss_att 52.156353 loss_ctc 53.114113 loss_rnnt 34.196304 hw_loss 0.096360 lr 0.00091524 rank 5
2023-02-18 06:48:46,959 DEBUG TRAIN Batch 2/5400 loss 62.742512 loss_att 88.929977 loss_ctc 96.135849 loss_rnnt 53.052544 hw_loss 0.000054 lr 0.00091408 rank 4
2023-02-18 06:48:46,960 DEBUG TRAIN Batch 2/5400 loss 25.586662 loss_att 43.155964 loss_ctc 36.437809 loss_rnnt 20.625929 hw_loss 0.000103 lr 0.00091676 rank 2
2023-02-18 06:48:46,961 DEBUG TRAIN Batch 2/5400 loss 109.742325 loss_att 148.672150 loss_ctc 136.343185 loss_rnnt 98.379257 hw_loss 0.056872 lr 0.00091628 rank 6
2023-02-18 06:48:46,963 DEBUG TRAIN Batch 2/5400 loss 86.640884 loss_att 113.228851 loss_ctc 103.545959 loss_rnnt 79.063560 hw_loss 0.010712 lr 0.00091532 rank 3
2023-02-18 06:48:46,969 DEBUG TRAIN Batch 2/5400 loss 25.203136 loss_att 41.062286 loss_ctc 41.728420 loss_rnnt 19.827906 hw_loss 0.000054 lr 0.00091524 rank 7
2023-02-18 06:48:47,023 DEBUG TRAIN Batch 2/5400 loss 47.009277 loss_att 57.054962 loss_ctc 77.393585 loss_rnnt 40.948860 hw_loss 0.000076 lr 0.00091736 rank 0
2023-02-18 06:49:46,621 DEBUG TRAIN Batch 2/5500 loss 17.113131 loss_att 23.080099 loss_ctc 20.758152 loss_rnnt 15.367609 hw_loss 0.123982 lr 0.00091724 rank 5
2023-02-18 06:49:46,625 DEBUG TRAIN Batch 2/5500 loss 27.842381 loss_att 37.522305 loss_ctc 40.036453 loss_rnnt 24.212122 hw_loss 0.128243 lr 0.00091828 rank 6
2023-02-18 06:49:46,625 DEBUG TRAIN Batch 2/5500 loss 36.755360 loss_att 45.652241 loss_ctc 46.614761 loss_rnnt 33.661358 hw_loss 0.000075 lr 0.00091724 rank 7
2023-02-18 06:49:46,625 DEBUG TRAIN Batch 2/5500 loss 44.734676 loss_att 50.200691 loss_ctc 64.826279 loss_rnnt 40.962551 hw_loss 0.000075 lr 0.00091608 rank 4
2023-02-18 06:49:46,630 DEBUG TRAIN Batch 2/5500 loss 45.253754 loss_att 58.183834 loss_ctc 62.362701 loss_rnnt 40.369869 hw_loss 0.031264 lr 0.00091732 rank 3
2023-02-18 06:49:46,631 DEBUG TRAIN Batch 2/5500 loss 43.590061 loss_att 47.570156 loss_ctc 54.371716 loss_rnnt 41.332611 hw_loss 0.044769 lr 0.00092016 rank 1
2023-02-18 06:49:46,639 DEBUG TRAIN Batch 2/5500 loss 32.459194 loss_att 40.112125 loss_ctc 42.378181 loss_rnnt 29.534307 hw_loss 0.134573 lr 0.00091876 rank 2
2023-02-18 06:49:46,688 DEBUG TRAIN Batch 2/5500 loss 41.909443 loss_att 50.077103 loss_ctc 56.472179 loss_rnnt 38.333656 hw_loss 0.001044 lr 0.00091936 rank 0
2023-02-18 06:50:47,740 DEBUG TRAIN Batch 2/5600 loss 56.392563 loss_att 62.049110 loss_ctc 81.606880 loss_rnnt 51.899300 hw_loss 0.000075 lr 0.00091924 rank 5
2023-02-18 06:50:47,748 DEBUG TRAIN Batch 2/5600 loss 48.186195 loss_att 64.576408 loss_ctc 67.779922 loss_rnnt 42.221020 hw_loss 0.139939 lr 0.00091808 rank 4
2023-02-18 06:50:47,749 DEBUG TRAIN Batch 2/5600 loss 22.041996 loss_att 33.634651 loss_ctc 31.506969 loss_rnnt 18.443857 hw_loss 0.033016 lr 0.00092028 rank 6
2023-02-18 06:50:47,751 DEBUG TRAIN Batch 2/5600 loss 24.117689 loss_att 36.678917 loss_ctc 42.046246 loss_rnnt 19.193445 hw_loss 0.040352 lr 0.00092076 rank 2
2023-02-18 06:50:47,751 DEBUG TRAIN Batch 2/5600 loss 23.203945 loss_att 35.593872 loss_ctc 32.351273 loss_rnnt 19.506294 hw_loss 0.000045 lr 0.00092216 rank 1
2023-02-18 06:50:47,755 DEBUG TRAIN Batch 2/5600 loss 45.733543 loss_att 60.429604 loss_ctc 68.284683 loss_rnnt 39.736557 hw_loss 0.095542 lr 0.00091924 rank 7
2023-02-18 06:50:47,757 DEBUG TRAIN Batch 2/5600 loss 32.617401 loss_att 39.772079 loss_ctc 45.517841 loss_rnnt 29.393284 hw_loss 0.137098 lr 0.00092136 rank 0
2023-02-18 06:50:47,757 DEBUG TRAIN Batch 2/5600 loss 20.526340 loss_att 30.330219 loss_ctc 33.830582 loss_rnnt 16.759975 hw_loss 0.059419 lr 0.00091932 rank 3
2023-02-18 06:51:46,228 DEBUG TRAIN Batch 2/5700 loss 28.408239 loss_att 43.968945 loss_ctc 41.714336 loss_rnnt 23.391455 hw_loss 0.244678 lr 0.00092124 rank 7
2023-02-18 06:51:46,232 DEBUG TRAIN Batch 2/5700 loss 20.519392 loss_att 28.529301 loss_ctc 30.035086 loss_rnnt 17.584639 hw_loss 0.120023 lr 0.00092124 rank 5
2023-02-18 06:51:46,232 DEBUG TRAIN Batch 2/5700 loss 37.903702 loss_att 43.248672 loss_ctc 47.621910 loss_rnnt 35.395641 hw_loss 0.268697 lr 0.00092008 rank 4
2023-02-18 06:51:46,234 DEBUG TRAIN Batch 2/5700 loss 51.366371 loss_att 67.692261 loss_ctc 74.616409 loss_rnnt 44.921082 hw_loss 0.150203 lr 0.00092132 rank 3
2023-02-18 06:51:46,237 DEBUG TRAIN Batch 2/5700 loss 25.507193 loss_att 40.649002 loss_ctc 36.688782 loss_rnnt 20.956541 hw_loss 0.058895 lr 0.00092276 rank 2
2023-02-18 06:51:46,239 DEBUG TRAIN Batch 2/5700 loss 25.183630 loss_att 38.665409 loss_ctc 54.052197 loss_rnnt 18.638100 hw_loss 0.000064 lr 0.00092228 rank 6
2023-02-18 06:51:46,240 DEBUG TRAIN Batch 2/5700 loss 52.907532 loss_att 70.339935 loss_ctc 72.412544 loss_rnnt 46.820366 hw_loss 0.000034 lr 0.00092336 rank 0
2023-02-18 06:51:46,302 DEBUG TRAIN Batch 2/5700 loss 21.761129 loss_att 30.316587 loss_ctc 33.526180 loss_rnnt 18.481329 hw_loss 0.000070 lr 0.00092416 rank 1
2023-02-18 06:52:44,622 DEBUG TRAIN Batch 2/5800 loss 36.481117 loss_att 43.747856 loss_ctc 58.688671 loss_rnnt 32.066711 hw_loss 0.000098 lr 0.00092536 rank 0
2023-02-18 06:52:44,622 DEBUG TRAIN Batch 2/5800 loss 31.631641 loss_att 40.903725 loss_ctc 40.019703 loss_rnnt 28.633026 hw_loss 0.048357 lr 0.00092208 rank 4
2023-02-18 06:52:44,623 DEBUG TRAIN Batch 2/5800 loss 27.952614 loss_att 36.328747 loss_ctc 39.431393 loss_rnnt 24.692581 hw_loss 0.101816 lr 0.00092324 rank 5
2023-02-18 06:52:44,624 DEBUG TRAIN Batch 2/5800 loss 46.723129 loss_att 50.748096 loss_ctc 62.185219 loss_rnnt 43.855484 hw_loss 0.001954 lr 0.00092616 rank 1
2023-02-18 06:52:44,627 DEBUG TRAIN Batch 2/5800 loss 42.709114 loss_att 68.462151 loss_ctc 55.399292 loss_rnnt 35.865856 hw_loss 0.001179 lr 0.00092332 rank 3
2023-02-18 06:52:44,631 DEBUG TRAIN Batch 2/5800 loss 25.838236 loss_att 28.707912 loss_ctc 42.259499 loss_rnnt 23.074734 hw_loss 0.000123 lr 0.00092428 rank 6
2023-02-18 06:52:44,631 DEBUG TRAIN Batch 2/5800 loss 22.155336 loss_att 24.158693 loss_ctc 28.823668 loss_rnnt 20.750031 hw_loss 0.216606 lr 0.00092324 rank 7
2023-02-18 06:52:44,636 DEBUG TRAIN Batch 2/5800 loss 24.500937 loss_att 32.408867 loss_ctc 35.536957 loss_rnnt 21.447760 hw_loss 0.000231 lr 0.00092476 rank 2
2023-02-18 06:53:45,665 DEBUG TRAIN Batch 2/5900 loss 40.346996 loss_att 51.629089 loss_ctc 63.053452 loss_rnnt 35.035957 hw_loss 0.050803 lr 0.00092736 rank 0
2023-02-18 06:53:45,665 DEBUG TRAIN Batch 2/5900 loss 48.584053 loss_att 62.893318 loss_ctc 72.328217 loss_rnnt 42.542618 hw_loss 0.025670 lr 0.00092524 rank 5
2023-02-18 06:53:45,667 DEBUG TRAIN Batch 2/5900 loss 47.770294 loss_att 67.447800 loss_ctc 69.804016 loss_rnnt 40.843750 hw_loss 0.099773 lr 0.00092408 rank 4
2023-02-18 06:53:45,668 DEBUG TRAIN Batch 2/5900 loss 34.842236 loss_att 47.185246 loss_ctc 50.055176 loss_rnnt 30.339424 hw_loss 0.010914 lr 0.00092628 rank 6
2023-02-18 06:53:45,670 DEBUG TRAIN Batch 2/5900 loss 28.734039 loss_att 45.841644 loss_ctc 39.831711 loss_rnnt 23.801935 hw_loss 0.057921 lr 0.00092676 rank 2
2023-02-18 06:53:45,675 DEBUG TRAIN Batch 2/5900 loss 40.965286 loss_att 52.166656 loss_ctc 60.390694 loss_rnnt 36.114246 hw_loss 0.038834 lr 0.00092524 rank 7
2023-02-18 06:53:45,676 DEBUG TRAIN Batch 2/5900 loss 13.277642 loss_att 19.672171 loss_ctc 23.740622 loss_rnnt 10.603620 hw_loss 0.000098 lr 0.00092532 rank 3
2023-02-18 06:53:45,726 DEBUG TRAIN Batch 2/5900 loss 29.604441 loss_att 40.656555 loss_ctc 44.285057 loss_rnnt 25.411116 hw_loss 0.047783 lr 0.00092816 rank 1
2023-02-18 06:54:45,205 DEBUG TRAIN Batch 2/6000 loss 33.555504 loss_att 57.843704 loss_ctc 66.459488 loss_rnnt 24.232653 hw_loss 0.146270 lr 0.00092724 rank 5
2023-02-18 06:54:45,207 DEBUG TRAIN Batch 2/6000 loss 30.494205 loss_att 27.070755 loss_ctc 31.476526 loss_rnnt 31.047907 hw_loss 0.000026 lr 0.00093016 rank 1
2023-02-18 06:54:45,208 DEBUG TRAIN Batch 2/6000 loss 31.202938 loss_att 47.605183 loss_ctc 42.835545 loss_rnnt 26.306042 hw_loss 0.122692 lr 0.00092876 rank 2
2023-02-18 06:54:45,209 DEBUG TRAIN Batch 2/6000 loss 38.537926 loss_att 57.185608 loss_ctc 53.187054 loss_rnnt 32.855160 hw_loss 0.000024 lr 0.00092936 rank 0
2023-02-18 06:54:45,210 DEBUG TRAIN Batch 2/6000 loss 63.081726 loss_att 91.696991 loss_ctc 76.417442 loss_rnnt 55.536690 hw_loss 0.082285 lr 0.00092828 rank 6
2023-02-18 06:54:45,213 DEBUG TRAIN Batch 2/6000 loss 62.873432 loss_att 87.019493 loss_ctc 113.102905 loss_rnnt 51.346947 hw_loss 0.000023 lr 0.00092724 rank 7
2023-02-18 06:54:45,225 DEBUG TRAIN Batch 2/6000 loss 50.033756 loss_att 57.571877 loss_ctc 57.571064 loss_rnnt 47.521126 hw_loss 0.000045 lr 0.00092732 rank 3
2023-02-18 06:54:45,271 DEBUG TRAIN Batch 2/6000 loss 25.065250 loss_att 33.123283 loss_ctc 33.255352 loss_rnnt 22.181732 hw_loss 0.337310 lr 0.00092608 rank 4
2023-02-18 06:56:38,146 DEBUG TRAIN Batch 2/6100 loss 71.099174 loss_att 85.870712 loss_ctc 113.789062 loss_rnnt 62.433128 hw_loss 0.037039 lr 0.00092924 rank 5
2023-02-18 06:56:38,149 DEBUG TRAIN Batch 2/6100 loss 61.674934 loss_att 64.487221 loss_ctc 79.975021 loss_rnnt 58.603230 hw_loss 0.129811 lr 0.00092808 rank 4
2023-02-18 06:56:38,151 DEBUG TRAIN Batch 2/6100 loss 75.079872 loss_att 97.665268 loss_ctc 101.893372 loss_rnnt 66.926773 hw_loss 0.114147 lr 0.00093076 rank 2
2023-02-18 06:56:38,152 DEBUG TRAIN Batch 2/6100 loss 40.030331 loss_att 45.362633 loss_ctc 53.225883 loss_rnnt 37.178242 hw_loss 0.049159 lr 0.00092932 rank 3
2023-02-18 06:56:38,154 DEBUG TRAIN Batch 2/6100 loss 52.997936 loss_att 64.108582 loss_ctc 71.693527 loss_rnnt 48.240765 hw_loss 0.079303 lr 0.00093136 rank 0
2023-02-18 06:56:38,154 DEBUG TRAIN Batch 2/6100 loss 27.283245 loss_att 38.767651 loss_ctc 48.749741 loss_rnnt 22.039015 hw_loss 0.159654 lr 0.00093216 rank 1
2023-02-18 06:56:38,158 DEBUG TRAIN Batch 2/6100 loss 30.119610 loss_att 32.830376 loss_ctc 40.136543 loss_rnnt 28.149338 hw_loss 0.173489 lr 0.00092924 rank 7
2023-02-18 06:56:38,161 DEBUG TRAIN Batch 2/6100 loss 26.717434 loss_att 35.693787 loss_ctc 44.985611 loss_rnnt 22.443863 hw_loss 0.079766 lr 0.00093028 rank 6
2023-02-18 06:57:39,347 DEBUG TRAIN Batch 2/6200 loss 25.420988 loss_att 34.190948 loss_ctc 40.141121 loss_rnnt 21.699287 hw_loss 0.009417 lr 0.00093124 rank 5
2023-02-18 06:57:39,351 DEBUG TRAIN Batch 2/6200 loss 60.018238 loss_att 65.234390 loss_ctc 78.975136 loss_rnnt 56.369080 hw_loss 0.146885 lr 0.00093336 rank 0
2023-02-18 06:57:39,351 DEBUG TRAIN Batch 2/6200 loss 31.378849 loss_att 51.050407 loss_ctc 52.024521 loss_rnnt 24.691723 hw_loss 0.000116 lr 0.00093132 rank 3
2023-02-18 06:57:39,351 DEBUG TRAIN Batch 2/6200 loss 20.283203 loss_att 32.763004 loss_ctc 31.405813 loss_rnnt 16.282413 hw_loss 0.040905 lr 0.00093124 rank 7
2023-02-18 06:57:39,351 DEBUG TRAIN Batch 2/6200 loss 38.644260 loss_att 60.680756 loss_ctc 54.582054 loss_rnnt 32.111877 hw_loss 0.000084 lr 0.00093228 rank 6
2023-02-18 06:57:39,352 DEBUG TRAIN Batch 2/6200 loss 19.885017 loss_att 30.579563 loss_ctc 25.647400 loss_rnnt 16.977757 hw_loss 0.000061 lr 0.00093416 rank 1
2023-02-18 06:57:39,354 DEBUG TRAIN Batch 2/6200 loss 35.076420 loss_att 50.501228 loss_ctc 42.935116 loss_rnnt 30.911209 hw_loss 0.060784 lr 0.00093008 rank 4
2023-02-18 06:57:39,358 DEBUG TRAIN Batch 2/6200 loss 21.552694 loss_att 28.021011 loss_ctc 23.321825 loss_rnnt 19.964012 hw_loss 0.110880 lr 0.00093276 rank 2
2023-02-18 06:58:38,151 DEBUG TRAIN Batch 2/6300 loss 64.410233 loss_att 85.883064 loss_ctc 88.622314 loss_rnnt 56.768349 hw_loss 0.223205 lr 0.00093324 rank 5
2023-02-18 06:58:38,151 DEBUG TRAIN Batch 2/6300 loss 64.117577 loss_att 88.962784 loss_ctc 103.691292 loss_rnnt 53.826599 hw_loss 0.085200 lr 0.00093428 rank 6
2023-02-18 06:58:38,153 DEBUG TRAIN Batch 2/6300 loss 28.028130 loss_att 36.685097 loss_ctc 34.951000 loss_rnnt 25.334230 hw_loss 0.073975 lr 0.00093208 rank 4
2023-02-18 06:58:38,154 DEBUG TRAIN Batch 2/6300 loss 15.449515 loss_att 18.038206 loss_ctc 21.676233 loss_rnnt 14.004603 hw_loss 0.181773 lr 0.00093332 rank 3
2023-02-18 06:58:38,154 DEBUG TRAIN Batch 2/6300 loss 39.404041 loss_att 46.363693 loss_ctc 61.774357 loss_rnnt 35.029297 hw_loss 0.000192 lr 0.00093536 rank 0
2023-02-18 06:58:38,157 DEBUG TRAIN Batch 2/6300 loss 35.115746 loss_att 46.816399 loss_ctc 49.358639 loss_rnnt 30.876453 hw_loss 0.000202 lr 0.00093616 rank 1
2023-02-18 06:58:38,164 DEBUG TRAIN Batch 2/6300 loss 69.069283 loss_att 87.573326 loss_ctc 87.486458 loss_rnnt 62.877453 hw_loss 0.066380 lr 0.00093476 rank 2
2023-02-18 06:58:38,215 DEBUG TRAIN Batch 2/6300 loss 35.762474 loss_att 46.341385 loss_ctc 49.346359 loss_rnnt 31.835423 hw_loss 0.000152 lr 0.00093324 rank 7
2023-02-18 06:59:36,530 DEBUG TRAIN Batch 2/6400 loss 42.959476 loss_att 53.049278 loss_ctc 63.015911 loss_rnnt 38.267262 hw_loss 0.000112 lr 0.00093524 rank 5
2023-02-18 06:59:36,542 DEBUG TRAIN Batch 2/6400 loss 57.917057 loss_att 68.553696 loss_ctc 85.508362 loss_rnnt 52.110844 hw_loss 0.000077 lr 0.00093524 rank 7
2023-02-18 06:59:36,543 DEBUG TRAIN Batch 2/6400 loss 60.091564 loss_att 77.715363 loss_ctc 86.839661 loss_rnnt 53.000343 hw_loss 0.000084 lr 0.00093736 rank 0
2023-02-18 06:59:36,547 DEBUG TRAIN Batch 2/6400 loss 63.705971 loss_att 72.192856 loss_ctc 78.579117 loss_rnnt 59.939495 hw_loss 0.161277 lr 0.00093676 rank 2
2023-02-18 06:59:36,546 DEBUG TRAIN Batch 2/6400 loss 23.519730 loss_att 32.060371 loss_ctc 33.305786 loss_rnnt 20.462809 hw_loss 0.082474 lr 0.00093408 rank 4
2023-02-18 06:59:36,546 DEBUG TRAIN Batch 2/6400 loss 13.120264 loss_att 22.573517 loss_ctc 27.413307 loss_rnnt 9.292803 hw_loss 0.058259 lr 0.00093816 rank 1
2023-02-18 06:59:36,546 DEBUG TRAIN Batch 2/6400 loss 38.131737 loss_att 50.730648 loss_ctc 62.899899 loss_rnnt 32.272392 hw_loss 0.069633 lr 0.00093532 rank 3
2023-02-18 06:59:36,548 DEBUG TRAIN Batch 2/6400 loss 31.005651 loss_att 41.965530 loss_ctc 52.520893 loss_rnnt 25.883739 hw_loss 0.114818 lr 0.00093628 rank 6
2023-02-18 07:00:37,431 DEBUG TRAIN Batch 2/6500 loss 19.100035 loss_att 35.403252 loss_ctc 37.799580 loss_rnnt 13.334334 hw_loss 0.022098 lr 0.00094016 rank 1
2023-02-18 07:00:37,432 DEBUG TRAIN Batch 2/6500 loss 5.995440 loss_att 17.010479 loss_ctc 12.993791 loss_rnnt 2.771924 hw_loss 0.163867 lr 0.00093608 rank 4
2023-02-18 07:00:37,436 DEBUG TRAIN Batch 2/6500 loss 122.723495 loss_att 139.277084 loss_ctc 166.277252 loss_rnnt 113.605583 hw_loss 0.000060 lr 0.00093936 rank 0
2023-02-18 07:00:37,437 DEBUG TRAIN Batch 2/6500 loss 53.338665 loss_att 68.487518 loss_ctc 79.123985 loss_rnnt 46.780025 hw_loss 0.170298 lr 0.00093876 rank 2
2023-02-18 07:00:37,437 DEBUG TRAIN Batch 2/6500 loss 44.001415 loss_att 72.574638 loss_ctc 65.099663 loss_rnnt 35.411667 hw_loss 0.116254 lr 0.00093732 rank 3
2023-02-18 07:00:37,439 DEBUG TRAIN Batch 2/6500 loss 33.098972 loss_att 45.647614 loss_ctc 49.844391 loss_rnnt 28.356480 hw_loss 0.000081 lr 0.00093724 rank 5
2023-02-18 07:00:37,438 DEBUG TRAIN Batch 2/6500 loss 27.222946 loss_att 37.278416 loss_ctc 39.434822 loss_rnnt 23.472343 hw_loss 0.208613 lr 0.00093724 rank 7
2023-02-18 07:00:37,448 DEBUG TRAIN Batch 2/6500 loss 40.254066 loss_att 47.693306 loss_ctc 57.750702 loss_rnnt 36.433285 hw_loss 0.000099 lr 0.00093828 rank 6
2023-02-18 07:01:35,305 DEBUG TRAIN Batch 2/6600 loss 28.576864 loss_att 41.478081 loss_ctc 49.112030 loss_rnnt 23.227594 hw_loss 0.058130 lr 0.00093924 rank 5
2023-02-18 07:01:35,309 DEBUG TRAIN Batch 2/6600 loss 63.416378 loss_att 72.344994 loss_ctc 84.791779 loss_rnnt 58.739906 hw_loss 0.076296 lr 0.00094028 rank 6
2023-02-18 07:01:35,311 DEBUG TRAIN Batch 2/6600 loss 23.731138 loss_att 30.892193 loss_ctc 31.246519 loss_rnnt 21.295315 hw_loss 0.002926 lr 0.00093932 rank 3
2023-02-18 07:01:35,312 DEBUG TRAIN Batch 2/6600 loss 56.598064 loss_att 72.041222 loss_ctc 79.553421 loss_rnnt 50.448692 hw_loss 0.000060 lr 0.00094216 rank 1
2023-02-18 07:01:35,315 DEBUG TRAIN Batch 2/6600 loss 94.171631 loss_att 112.667503 loss_ctc 135.503006 loss_rnnt 84.848358 hw_loss 0.212349 lr 0.00094136 rank 0
2023-02-18 07:01:35,317 DEBUG TRAIN Batch 2/6600 loss 18.658903 loss_att 31.767067 loss_ctc 32.074547 loss_rnnt 14.248486 hw_loss 0.000062 lr 0.00093924 rank 7
2023-02-18 07:01:35,318 DEBUG TRAIN Batch 2/6600 loss 48.002342 loss_att 71.140488 loss_ctc 62.771584 loss_rnnt 41.405445 hw_loss 0.000062 lr 0.00093808 rank 4
2023-02-18 07:01:35,373 DEBUG TRAIN Batch 2/6600 loss 71.109932 loss_att 88.744751 loss_ctc 96.414909 loss_rnnt 64.084015 hw_loss 0.234290 lr 0.00094076 rank 2
2023-02-18 07:02:32,885 DEBUG TRAIN Batch 2/6700 loss 72.596428 loss_att 88.743858 loss_ctc 95.730507 loss_rnnt 66.193024 hw_loss 0.167585 lr 0.00094416 rank 1
2023-02-18 07:02:32,887 DEBUG TRAIN Batch 2/6700 loss 53.758682 loss_att 59.390316 loss_ctc 76.198303 loss_rnnt 49.640301 hw_loss 0.000198 lr 0.00094336 rank 0
2023-02-18 07:02:32,889 DEBUG TRAIN Batch 2/6700 loss 44.428242 loss_att 55.392334 loss_ctc 58.352299 loss_rnnt 40.378746 hw_loss 0.000255 lr 0.00094124 rank 5
2023-02-18 07:02:32,889 DEBUG TRAIN Batch 2/6700 loss 39.092663 loss_att 55.961277 loss_ctc 59.523094 loss_rnnt 32.974850 hw_loss 0.037568 lr 0.00094228 rank 6
2023-02-18 07:02:32,889 DEBUG TRAIN Batch 2/6700 loss 39.062916 loss_att 44.386784 loss_ctc 52.026451 loss_rnnt 36.269588 hw_loss 0.000160 lr 0.00094124 rank 7
2023-02-18 07:02:32,891 DEBUG TRAIN Batch 2/6700 loss 34.328445 loss_att 40.228413 loss_ctc 48.802925 loss_rnnt 31.187147 hw_loss 0.058822 lr 0.00094132 rank 3
2023-02-18 07:02:32,905 DEBUG TRAIN Batch 2/6700 loss 42.242794 loss_att 50.931282 loss_ctc 51.209053 loss_rnnt 39.279701 hw_loss 0.056052 lr 0.00094276 rank 2
2023-02-18 07:02:32,956 DEBUG TRAIN Batch 2/6700 loss 25.142410 loss_att 36.099396 loss_ctc 36.348831 loss_rnnt 21.448410 hw_loss 0.015775 lr 0.00094008 rank 4
2023-02-18 07:03:33,752 DEBUG TRAIN Batch 2/6800 loss 45.672546 loss_att 57.514549 loss_ctc 63.011951 loss_rnnt 40.992172 hw_loss 0.000098 lr 0.00094476 rank 2
2023-02-18 07:03:33,754 DEBUG TRAIN Batch 2/6800 loss 36.734715 loss_att 51.692669 loss_ctc 56.747189 loss_rnnt 31.068323 hw_loss 0.012127 lr 0.00094324 rank 5
2023-02-18 07:03:33,755 DEBUG TRAIN Batch 2/6800 loss 61.273952 loss_att 81.612122 loss_ctc 100.382629 loss_rnnt 51.947792 hw_loss 0.082569 lr 0.00094332 rank 3
2023-02-18 07:03:33,758 DEBUG TRAIN Batch 2/6800 loss 168.858109 loss_att 187.243713 loss_ctc 228.975159 loss_rnnt 157.165344 hw_loss 0.000029 lr 0.00094208 rank 4
2023-02-18 07:03:33,761 DEBUG TRAIN Batch 2/6800 loss 49.525776 loss_att 66.821594 loss_ctc 72.459534 loss_rnnt 42.984020 hw_loss 0.046425 lr 0.00094324 rank 7
2023-02-18 07:03:33,772 DEBUG TRAIN Batch 2/6800 loss 34.869499 loss_att 34.918758 loss_ctc 47.453915 loss_rnnt 33.181694 hw_loss 0.000057 lr 0.00094428 rank 6
2023-02-18 07:03:33,798 DEBUG TRAIN Batch 2/6800 loss 11.366529 loss_att 19.485310 loss_ctc 17.350065 loss_rnnt 8.786928 hw_loss 0.296323 lr 0.00094616 rank 1
2023-02-18 07:03:33,839 DEBUG TRAIN Batch 2/6800 loss 49.499229 loss_att 69.386833 loss_ctc 75.930481 loss_rnnt 41.972282 hw_loss 0.047356 lr 0.00094536 rank 0
2023-02-18 07:05:24,945 DEBUG TRAIN Batch 2/6900 loss 19.801950 loss_att 28.518600 loss_ctc 30.717686 loss_rnnt 16.557068 hw_loss 0.086474 lr 0.00094532 rank 3
2023-02-18 07:05:24,945 DEBUG TRAIN Batch 2/6900 loss 45.578793 loss_att 63.312973 loss_ctc 63.386635 loss_rnnt 39.600761 hw_loss 0.106533 lr 0.00094524 rank 5
2023-02-18 07:05:24,951 DEBUG TRAIN Batch 2/6900 loss 72.939445 loss_att 93.234352 loss_ctc 93.948853 loss_rnnt 66.055634 hw_loss 0.044201 lr 0.00094408 rank 4
2023-02-18 07:05:24,952 DEBUG TRAIN Batch 2/6900 loss 36.958149 loss_att 60.947144 loss_ctc 58.299942 loss_rnnt 29.302689 hw_loss 0.022665 lr 0.00094524 rank 7
2023-02-18 07:05:24,976 DEBUG TRAIN Batch 2/6900 loss 34.937141 loss_att 44.415234 loss_ctc 62.992779 loss_rnnt 29.300709 hw_loss 0.000110 lr 0.00094816 rank 1
2023-02-18 07:05:24,980 DEBUG TRAIN Batch 2/6900 loss 18.132320 loss_att 17.989363 loss_ctc 23.540661 loss_rnnt 17.366955 hw_loss 0.136586 lr 0.00094676 rank 2
2023-02-18 07:05:25,024 DEBUG TRAIN Batch 2/6900 loss 19.627001 loss_att 25.035934 loss_ctc 23.658236 loss_rnnt 18.007677 hw_loss 0.000074 lr 0.00094736 rank 0
2023-02-18 07:05:25,053 DEBUG TRAIN Batch 2/6900 loss 45.732368 loss_att 76.610367 loss_ctc 73.372597 loss_rnnt 35.871346 hw_loss 0.000114 lr 0.00094628 rank 6
2023-02-18 07:06:25,077 DEBUG TRAIN Batch 2/7000 loss 19.930504 loss_att 36.396416 loss_ctc 37.774616 loss_rnnt 14.256145 hw_loss 0.003674 lr 0.00094608 rank 4
2023-02-18 07:06:25,080 DEBUG TRAIN Batch 2/7000 loss 23.558777 loss_att 28.999126 loss_ctc 38.193153 loss_rnnt 20.488325 hw_loss 0.058376 lr 0.00094724 rank 5
2023-02-18 07:06:25,082 DEBUG TRAIN Batch 2/7000 loss 22.254229 loss_att 35.496651 loss_ctc 32.004345 loss_rnnt 18.305696 hw_loss 0.000064 lr 0.00094936 rank 0
2023-02-18 07:06:25,084 DEBUG TRAIN Batch 2/7000 loss 31.733786 loss_att 35.519077 loss_ctc 47.077614 loss_rnnt 28.930481 hw_loss 0.000755 lr 0.00094876 rank 2
2023-02-18 07:06:25,086 DEBUG TRAIN Batch 2/7000 loss 39.128014 loss_att 60.766006 loss_ctc 58.679153 loss_rnnt 32.193546 hw_loss 0.000089 lr 0.00094828 rank 6
2023-02-18 07:06:25,092 DEBUG TRAIN Batch 2/7000 loss 19.545673 loss_att 32.367569 loss_ctc 39.385155 loss_rnnt 14.335940 hw_loss 0.000167 lr 0.00094732 rank 3
2023-02-18 07:06:25,093 DEBUG TRAIN Batch 2/7000 loss 67.334297 loss_att 78.303604 loss_ctc 107.314499 loss_rnnt 59.765938 hw_loss 0.082130 lr 0.00094724 rank 7
2023-02-18 07:06:25,145 DEBUG TRAIN Batch 2/7000 loss 57.421299 loss_att 67.808624 loss_ctc 75.477921 loss_rnnt 52.926064 hw_loss 0.019155 lr 0.00095016 rank 1
2023-02-18 07:07:26,818 DEBUG TRAIN Batch 2/7100 loss 49.717461 loss_att 58.270653 loss_ctc 55.886333 loss_rnnt 47.184269 hw_loss 0.000059 lr 0.00094924 rank 5
2023-02-18 07:07:26,825 DEBUG TRAIN Batch 2/7100 loss 38.512878 loss_att 50.618172 loss_ctc 50.223442 loss_rnnt 34.530384 hw_loss 0.000051 lr 0.00095028 rank 6
2023-02-18 07:07:26,827 DEBUG TRAIN Batch 2/7100 loss 115.504372 loss_att 130.038956 loss_ctc 131.724625 loss_rnnt 110.434731 hw_loss 0.000035 lr 0.00095216 rank 1
2023-02-18 07:07:26,828 DEBUG TRAIN Batch 2/7100 loss 30.631287 loss_att 43.036087 loss_ctc 37.393593 loss_rnnt 27.248661 hw_loss 0.000042 lr 0.00094924 rank 7
2023-02-18 07:07:26,828 DEBUG TRAIN Batch 2/7100 loss 28.031267 loss_att 36.196262 loss_ctc 37.311455 loss_rnnt 25.158941 hw_loss 0.003692 lr 0.00094808 rank 4
2023-02-18 07:07:26,829 DEBUG TRAIN Batch 2/7100 loss 51.537228 loss_att 64.457077 loss_ctc 86.329514 loss_rnnt 44.314247 hw_loss 0.000068 lr 0.00095076 rank 2
2023-02-18 07:07:26,831 DEBUG TRAIN Batch 2/7100 loss 53.315174 loss_att 69.576141 loss_ctc 75.548340 loss_rnnt 47.098526 hw_loss 0.000071 lr 0.00094932 rank 3
2023-02-18 07:07:26,883 DEBUG TRAIN Batch 2/7100 loss 18.292448 loss_att 23.049416 loss_ctc 19.238728 loss_rnnt 17.213367 hw_loss 0.002841 lr 0.00095136 rank 0
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 10.76 GiB total capacity; 5.96 GiB already allocated; 1.61 GiB free; 7.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:48561
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:26523
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:39859
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:1440
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:52140
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:29854
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:39148
