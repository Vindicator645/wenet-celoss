/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_27_rnnt_bias_both_finetune.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_27_rnnt_bias_loss_2_class_both_finetune/ddp_init
2023-02-27 12:54:02,404 INFO training on multiple gpus, this gpu 5
2023-02-27 12:54:02,404 INFO training on multiple gpus, this gpu 3
2023-02-27 12:54:02,405 INFO training on multiple gpus, this gpu 6
2023-02-27 12:54:02,406 INFO training on multiple gpus, this gpu 7
2023-02-27 12:54:02,406 INFO training on multiple gpus, this gpu 0
2023-02-27 12:54:02,407 INFO training on multiple gpus, this gpu 2
2023-02-27 12:54:02,408 INFO training on multiple gpus, this gpu 1
2023-02-27 12:54:02,425 INFO training on multiple gpus, this gpu 4
2023-02-27 12:55:37,210 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-27 12:55:37,254 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-27 12:55:38,165 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-27 12:55:38,175 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-27 12:55:38,249 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-27 12:55:38,281 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-27 12:55:39,507 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-27 12:55:47,377 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-27 12:55:48,258 INFO Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-27 12:55:49,930 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-27 12:55:51,183 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-27 12:55:53,944 INFO Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-27 12:55:54,820 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-27 12:55:54,918 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:55:54,967 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:55:55,162 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:55:56,128 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:55:57,485 INFO Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-27 12:55:57,485 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:56:01,118 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-27 12:56:01,118 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:56:07,454 INFO Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-27 12:56:07,454 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:56:11,670 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:11,706 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:12,506 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:12,530 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:12,557 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:12,629 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:16,924 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:21,449 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:21,451 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:21,504 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:21,505 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 12:56:21,507 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:21,508 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:21,706 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:21,708 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:21,735 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:21,737 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:27,602 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-27 12:56:27,602 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-27 12:56:30,445 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/43.pt for GPU
2023-02-27 12:56:35,085 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:35,087 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:35,156 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:35,158 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:56:35,180 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:56:35,182 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-27 12:57:37,442 DEBUG TRAIN Batch 44/0 loss 8.020283 loss_att 7.941354 loss_ctc 12.017678 loss_rnnt 7.170470 hw_loss 0.623647 lr 0.00029907 rank 3
2023-02-27 12:57:37,453 DEBUG TRAIN Batch 44/0 loss 6.894646 loss_att 7.067374 loss_ctc 9.805549 loss_rnnt 6.141195 hw_loss 0.620222 lr 0.00029907 rank 5
2023-02-27 12:57:37,464 DEBUG TRAIN Batch 44/0 loss 6.231012 loss_att 6.179694 loss_ctc 8.587855 loss_rnnt 5.593529 hw_loss 0.625314 lr 0.00029907 rank 4
2023-02-27 12:57:37,465 DEBUG TRAIN Batch 44/0 loss 7.259022 loss_att 6.863069 loss_ctc 11.173658 loss_rnnt 6.487159 hw_loss 0.617067 lr 0.00029907 rank 6
2023-02-27 12:57:37,471 DEBUG TRAIN Batch 44/0 loss 6.952394 loss_att 7.056477 loss_ctc 9.596700 loss_rnnt 6.255308 hw_loss 0.606928 lr 0.00029907 rank 2
2023-02-27 12:57:37,474 DEBUG TRAIN Batch 44/0 loss 5.573687 loss_att 5.641149 loss_ctc 8.337551 loss_rnnt 4.852991 hw_loss 0.635041 lr 0.00029907 rank 0
2023-02-27 12:57:37,513 DEBUG TRAIN Batch 44/0 loss 10.996017 loss_att 10.810761 loss_ctc 14.950950 loss_rnnt 10.176217 hw_loss 0.617862 lr 0.00029907 rank 7
2023-02-27 12:57:37,565 DEBUG TRAIN Batch 44/0 loss 6.612639 loss_att 6.929337 loss_ctc 8.938934 loss_rnnt 5.895249 hw_loss 0.644771 lr 0.00029907 rank 1
2023-02-27 12:58:17,398 DEBUG TRAIN Batch 44/100 loss 5.910429 loss_att 7.597379 loss_ctc 11.113258 loss_rnnt 4.733114 hw_loss 0.274153 lr 0.00029906 rank 3
2023-02-27 12:58:17,399 DEBUG TRAIN Batch 44/100 loss 4.790646 loss_att 5.812158 loss_ctc 8.101168 loss_rnnt 3.951214 hw_loss 0.363236 lr 0.00029906 rank 6
2023-02-27 12:58:17,400 DEBUG TRAIN Batch 44/100 loss 8.293501 loss_att 9.174831 loss_ctc 12.577277 loss_rnnt 7.445285 hw_loss 0.188962 lr 0.00029906 rank 0
2023-02-27 12:58:17,401 DEBUG TRAIN Batch 44/100 loss 1.912573 loss_att 4.391903 loss_ctc 2.253941 loss_rnnt 1.255442 hw_loss 0.217030 lr 0.00029906 rank 7
2023-02-27 12:58:17,402 DEBUG TRAIN Batch 44/100 loss 6.720549 loss_att 10.517159 loss_ctc 13.757625 loss_rnnt 4.937312 hw_loss 0.160571 lr 0.00029906 rank 5
2023-02-27 12:58:17,403 DEBUG TRAIN Batch 44/100 loss 8.266365 loss_att 9.865254 loss_ctc 13.534395 loss_rnnt 7.173915 hw_loss 0.131754 lr 0.00029906 rank 2
2023-02-27 12:58:17,432 DEBUG TRAIN Batch 44/100 loss 6.384408 loss_att 8.689616 loss_ctc 10.724760 loss_rnnt 5.231168 hw_loss 0.212783 lr 0.00029906 rank 4
2023-02-27 12:58:17,444 DEBUG TRAIN Batch 44/100 loss 3.762306 loss_att 7.502895 loss_ctc 8.274998 loss_rnnt 2.287861 hw_loss 0.233689 lr 0.00029906 rank 1
2023-02-27 12:58:56,287 DEBUG TRAIN Batch 44/200 loss 7.852578 loss_att 8.714736 loss_ctc 10.539070 loss_rnnt 7.091636 hw_loss 0.431834 lr 0.00029905 rank 2
2023-02-27 12:58:56,295 DEBUG TRAIN Batch 44/200 loss 6.581843 loss_att 8.771252 loss_ctc 12.760745 loss_rnnt 5.229362 hw_loss 0.170149 lr 0.00029905 rank 6
2023-02-27 12:58:56,297 DEBUG TRAIN Batch 44/200 loss 6.616939 loss_att 9.053842 loss_ctc 12.100027 loss_rnnt 5.349174 hw_loss 0.092450 lr 0.00029905 rank 0
2023-02-27 12:58:56,298 DEBUG TRAIN Batch 44/200 loss 3.911757 loss_att 5.745008 loss_ctc 7.562033 loss_rnnt 2.967087 hw_loss 0.171216 lr 0.00029905 rank 3
2023-02-27 12:58:56,301 DEBUG TRAIN Batch 44/200 loss 4.231135 loss_att 8.047327 loss_ctc 8.405073 loss_rnnt 2.832039 hw_loss 0.148749 lr 0.00029905 rank 5
2023-02-27 12:58:56,303 DEBUG TRAIN Batch 44/200 loss 7.313120 loss_att 11.602879 loss_ctc 11.791021 loss_rnnt 5.742157 hw_loss 0.217421 lr 0.00029905 rank 4
2023-02-27 12:58:56,306 DEBUG TRAIN Batch 44/200 loss 5.517772 loss_att 7.721630 loss_ctc 7.183467 loss_rnnt 4.724265 hw_loss 0.244955 lr 0.00029905 rank 7
2023-02-27 12:58:56,330 DEBUG TRAIN Batch 44/200 loss 6.593620 loss_att 9.782887 loss_ctc 10.115040 loss_rnnt 5.357265 hw_loss 0.241835 lr 0.00029905 rank 1
2023-02-27 12:59:36,053 DEBUG TRAIN Batch 44/300 loss 7.893293 loss_att 10.648361 loss_ctc 13.810258 loss_rnnt 6.479487 hw_loss 0.138495 lr 0.00029903 rank 4
2023-02-27 12:59:36,053 DEBUG TRAIN Batch 44/300 loss 6.130134 loss_att 9.129959 loss_ctc 8.107258 loss_rnnt 5.079298 hw_loss 0.351102 lr 0.00029903 rank 7
2023-02-27 12:59:36,063 DEBUG TRAIN Batch 44/300 loss 3.330801 loss_att 7.020074 loss_ctc 6.979494 loss_rnnt 2.013309 hw_loss 0.174645 lr 0.00029903 rank 0
2023-02-27 12:59:36,066 DEBUG TRAIN Batch 44/300 loss 10.212408 loss_att 12.819463 loss_ctc 17.917290 loss_rnnt 8.582222 hw_loss 0.152733 lr 0.00029903 rank 3
2023-02-27 12:59:36,069 DEBUG TRAIN Batch 44/300 loss 2.662911 loss_att 6.679829 loss_ctc 6.559108 loss_rnnt 1.181044 hw_loss 0.298107 lr 0.00029903 rank 6
2023-02-27 12:59:36,071 DEBUG TRAIN Batch 44/300 loss 5.404057 loss_att 8.263373 loss_ctc 10.534301 loss_rnnt 4.049663 hw_loss 0.184685 lr 0.00029903 rank 5
2023-02-27 12:59:36,071 DEBUG TRAIN Batch 44/300 loss 11.271976 loss_att 15.242495 loss_ctc 23.549770 loss_rnnt 8.746599 hw_loss 0.176689 lr 0.00029903 rank 1
2023-02-27 12:59:36,075 DEBUG TRAIN Batch 44/300 loss 15.784549 loss_att 18.573444 loss_ctc 27.658045 loss_rnnt 13.514671 hw_loss 0.241812 lr 0.00029903 rank 2
2023-02-27 13:00:40,670 DEBUG TRAIN Batch 44/400 loss 6.439420 loss_att 8.064107 loss_ctc 9.956673 loss_rnnt 5.517250 hw_loss 0.240496 lr 0.00029902 rank 4
2023-02-27 13:00:40,680 DEBUG TRAIN Batch 44/400 loss 5.573061 loss_att 7.560305 loss_ctc 8.996280 loss_rnnt 4.590073 hw_loss 0.242084 lr 0.00029902 rank 7
2023-02-27 13:00:40,681 DEBUG TRAIN Batch 44/400 loss 5.698725 loss_att 7.462312 loss_ctc 7.591993 loss_rnnt 4.927701 hw_loss 0.311008 lr 0.00029902 rank 0
2023-02-27 13:00:40,681 DEBUG TRAIN Batch 44/400 loss 7.787694 loss_att 10.452425 loss_ctc 13.836987 loss_rnnt 6.247256 hw_loss 0.376725 lr 0.00029902 rank 5
2023-02-27 13:00:40,682 DEBUG TRAIN Batch 44/400 loss 7.865116 loss_att 10.921588 loss_ctc 16.558460 loss_rnnt 5.891242 hw_loss 0.381500 lr 0.00029902 rank 6
2023-02-27 13:00:40,685 DEBUG TRAIN Batch 44/400 loss 6.210091 loss_att 9.182767 loss_ctc 9.808870 loss_rnnt 4.960709 hw_loss 0.328144 lr 0.00029902 rank 3
2023-02-27 13:00:40,688 DEBUG TRAIN Batch 44/400 loss 3.064850 loss_att 3.675656 loss_ctc 4.803868 loss_rnnt 2.555340 hw_loss 0.291524 lr 0.00029902 rank 1
2023-02-27 13:00:40,709 DEBUG TRAIN Batch 44/400 loss 11.432921 loss_att 14.651435 loss_ctc 19.391661 loss_rnnt 9.563924 hw_loss 0.307744 lr 0.00029902 rank 2
2023-02-27 13:01:19,704 DEBUG TRAIN Batch 44/500 loss 3.333407 loss_att 6.115390 loss_ctc 4.898278 loss_rnnt 2.395005 hw_loss 0.325041 lr 0.00029901 rank 3
2023-02-27 13:01:19,706 DEBUG TRAIN Batch 44/500 loss 5.290742 loss_att 8.667321 loss_ctc 10.175718 loss_rnnt 3.801574 hw_loss 0.304731 lr 0.00029901 rank 6
2023-02-27 13:01:19,706 DEBUG TRAIN Batch 44/500 loss 4.347187 loss_att 7.895174 loss_ctc 7.296636 loss_rnnt 3.090158 hw_loss 0.289071 lr 0.00029901 rank 5
2023-02-27 13:01:19,709 DEBUG TRAIN Batch 44/500 loss 9.281424 loss_att 11.668764 loss_ctc 14.489207 loss_rnnt 7.940034 hw_loss 0.317908 lr 0.00029901 rank 1
2023-02-27 13:01:19,710 DEBUG TRAIN Batch 44/500 loss 3.243869 loss_att 5.149614 loss_ctc 4.792489 loss_rnnt 2.461383 hw_loss 0.365350 lr 0.00029901 rank 7
2023-02-27 13:01:19,710 DEBUG TRAIN Batch 44/500 loss 5.067959 loss_att 8.450847 loss_ctc 9.690462 loss_rnnt 3.607418 hw_loss 0.314308 lr 0.00029901 rank 0
2023-02-27 13:01:19,714 DEBUG TRAIN Batch 44/500 loss 3.507349 loss_att 5.991128 loss_ctc 7.754430 loss_rnnt 2.225287 hw_loss 0.410678 lr 0.00029901 rank 2
2023-02-27 13:01:19,731 DEBUG TRAIN Batch 44/500 loss 6.291501 loss_att 7.704945 loss_ctc 9.539411 loss_rnnt 5.381176 hw_loss 0.364842 lr 0.00029901 rank 4
2023-02-27 13:01:58,184 DEBUG TRAIN Batch 44/600 loss 7.021881 loss_att 7.549955 loss_ctc 9.881229 loss_rnnt 6.299024 hw_loss 0.442492 lr 0.00029899 rank 0
2023-02-27 13:01:58,188 DEBUG TRAIN Batch 44/600 loss 5.426506 loss_att 6.843697 loss_ctc 8.850452 loss_rnnt 4.435737 hw_loss 0.470259 lr 0.00029899 rank 7
2023-02-27 13:01:58,191 DEBUG TRAIN Batch 44/600 loss 4.881931 loss_att 5.685836 loss_ctc 7.783278 loss_rnnt 3.966373 hw_loss 0.689869 lr 0.00029899 rank 3
2023-02-27 13:01:58,193 DEBUG TRAIN Batch 44/600 loss 7.184827 loss_att 8.685201 loss_ctc 13.076982 loss_rnnt 5.897995 hw_loss 0.377131 lr 0.00029899 rank 2
2023-02-27 13:01:58,193 DEBUG TRAIN Batch 44/600 loss 10.073068 loss_att 10.209504 loss_ctc 13.700919 loss_rnnt 9.275703 hw_loss 0.536931 lr 0.00029899 rank 6
2023-02-27 13:01:58,193 DEBUG TRAIN Batch 44/600 loss 7.574187 loss_att 9.363256 loss_ctc 13.654291 loss_rnnt 6.239425 hw_loss 0.311752 lr 0.00029899 rank 5
2023-02-27 13:01:58,194 DEBUG TRAIN Batch 44/600 loss 6.713327 loss_att 7.612944 loss_ctc 10.554248 loss_rnnt 5.773107 hw_loss 0.465328 lr 0.00029899 rank 4
2023-02-27 13:01:58,198 DEBUG TRAIN Batch 44/600 loss 6.924390 loss_att 8.686409 loss_ctc 9.187552 loss_rnnt 5.997903 hw_loss 0.510616 lr 0.00029899 rank 1
2023-02-27 13:02:37,396 DEBUG TRAIN Batch 44/700 loss 5.214526 loss_att 8.585545 loss_ctc 10.766457 loss_rnnt 3.700524 hw_loss 0.186639 lr 0.00029898 rank 7
2023-02-27 13:02:37,396 DEBUG TRAIN Batch 44/700 loss 3.917681 loss_att 5.932354 loss_ctc 6.879110 loss_rnnt 3.012028 hw_loss 0.202241 lr 0.00029898 rank 3
2023-02-27 13:02:37,399 DEBUG TRAIN Batch 44/700 loss 4.814624 loss_att 7.269958 loss_ctc 6.685608 loss_rnnt 3.885924 hw_loss 0.352818 lr 0.00029898 rank 1
2023-02-27 13:02:37,400 DEBUG TRAIN Batch 44/700 loss 11.211734 loss_att 18.684378 loss_ctc 19.869576 loss_rnnt 8.458973 hw_loss 0.194724 lr 0.00029898 rank 5
2023-02-27 13:02:37,401 DEBUG TRAIN Batch 44/700 loss 4.731845 loss_att 8.766062 loss_ctc 7.805997 loss_rnnt 3.385444 hw_loss 0.243133 lr 0.00029898 rank 2
2023-02-27 13:02:37,402 DEBUG TRAIN Batch 44/700 loss 2.231490 loss_att 4.349196 loss_ctc 2.808816 loss_rnnt 1.536668 hw_loss 0.364321 lr 0.00029898 rank 6
2023-02-27 13:02:37,406 DEBUG TRAIN Batch 44/700 loss 4.145340 loss_att 7.015388 loss_ctc 8.268499 loss_rnnt 2.944532 hw_loss 0.144459 lr 0.00029898 rank 0
2023-02-27 13:02:37,420 DEBUG TRAIN Batch 44/700 loss 8.442305 loss_att 13.193110 loss_ctc 19.706545 loss_rnnt 5.871479 hw_loss 0.222688 lr 0.00029898 rank 4
2023-02-27 13:03:41,386 DEBUG TRAIN Batch 44/800 loss 5.113943 loss_att 8.310165 loss_ctc 10.876262 loss_rnnt 3.564704 hw_loss 0.265660 lr 0.00029897 rank 4
2023-02-27 13:03:41,398 DEBUG TRAIN Batch 44/800 loss 5.830584 loss_att 10.050361 loss_ctc 9.345244 loss_rnnt 4.358124 hw_loss 0.299781 lr 0.00029897 rank 6
2023-02-27 13:03:41,400 DEBUG TRAIN Batch 44/800 loss 8.037106 loss_att 12.410650 loss_ctc 12.794589 loss_rnnt 6.472896 hw_loss 0.103444 lr 0.00029897 rank 0
2023-02-27 13:03:41,402 DEBUG TRAIN Batch 44/800 loss 2.229050 loss_att 5.040602 loss_ctc 4.251716 loss_rnnt 1.269182 hw_loss 0.239752 lr 0.00029897 rank 2
2023-02-27 13:03:41,404 DEBUG TRAIN Batch 44/800 loss 7.235126 loss_att 10.354956 loss_ctc 7.928736 loss_rnnt 6.434301 hw_loss 0.158210 lr 0.00029897 rank 3
2023-02-27 13:03:41,408 DEBUG TRAIN Batch 44/800 loss 8.487079 loss_att 12.878788 loss_ctc 17.062202 loss_rnnt 6.329106 hw_loss 0.255526 lr 0.00029897 rank 7
2023-02-27 13:03:41,410 DEBUG TRAIN Batch 44/800 loss 4.653411 loss_att 9.492710 loss_ctc 7.246537 loss_rnnt 3.200442 hw_loss 0.261298 lr 0.00029897 rank 1
2023-02-27 13:03:41,409 DEBUG TRAIN Batch 44/800 loss 3.943550 loss_att 7.928339 loss_ctc 5.278463 loss_rnnt 2.811276 hw_loss 0.294988 lr 0.00029897 rank 5
2023-02-27 13:04:20,034 DEBUG TRAIN Batch 44/900 loss 1.498149 loss_att 4.448990 loss_ctc 3.463758 loss_rnnt 0.528261 hw_loss 0.220572 lr 0.00029895 rank 7
2023-02-27 13:04:20,046 DEBUG TRAIN Batch 44/900 loss 10.913157 loss_att 15.772255 loss_ctc 20.735054 loss_rnnt 8.571154 hw_loss 0.113619 lr 0.00029895 rank 3
2023-02-27 13:04:20,046 DEBUG TRAIN Batch 44/900 loss 4.127393 loss_att 7.562057 loss_ctc 9.924286 loss_rnnt 2.596245 hw_loss 0.133679 lr 0.00029895 rank 0
2023-02-27 13:04:20,049 DEBUG TRAIN Batch 44/900 loss 7.635464 loss_att 8.447335 loss_ctc 9.073978 loss_rnnt 7.129112 hw_loss 0.285329 lr 0.00029895 rank 4
2023-02-27 13:04:20,053 DEBUG TRAIN Batch 44/900 loss 5.593575 loss_att 8.255530 loss_ctc 9.620670 loss_rnnt 4.424356 hw_loss 0.187275 lr 0.00029895 rank 6
2023-02-27 13:04:20,054 DEBUG TRAIN Batch 44/900 loss 6.197984 loss_att 10.088468 loss_ctc 11.999119 loss_rnnt 4.584086 hw_loss 0.116843 lr 0.00029895 rank 1
2023-02-27 13:04:20,055 DEBUG TRAIN Batch 44/900 loss 7.125548 loss_att 10.263664 loss_ctc 13.371738 loss_rnnt 5.490769 hw_loss 0.326867 lr 0.00029895 rank 5
2023-02-27 13:04:20,057 DEBUG TRAIN Batch 44/900 loss 5.319253 loss_att 9.046627 loss_ctc 11.914398 loss_rnnt 3.530078 hw_loss 0.308152 lr 0.00029895 rank 2
2023-02-27 13:04:58,824 DEBUG TRAIN Batch 44/1000 loss 6.890894 loss_att 7.029579 loss_ctc 8.639391 loss_rnnt 6.497526 hw_loss 0.248435 lr 0.00029894 rank 3
2023-02-27 13:04:58,826 DEBUG TRAIN Batch 44/1000 loss 3.603047 loss_att 5.694478 loss_ctc 5.616563 loss_rnnt 2.813816 hw_loss 0.192142 lr 0.00029894 rank 5
2023-02-27 13:04:58,831 DEBUG TRAIN Batch 44/1000 loss 8.998215 loss_att 12.007784 loss_ctc 13.355936 loss_rnnt 7.739377 hw_loss 0.142302 lr 0.00029894 rank 7
2023-02-27 13:04:58,834 DEBUG TRAIN Batch 44/1000 loss 8.938251 loss_att 11.536478 loss_ctc 18.043240 loss_rnnt 7.076392 hw_loss 0.240401 lr 0.00029894 rank 6
2023-02-27 13:04:58,835 DEBUG TRAIN Batch 44/1000 loss 5.314643 loss_att 8.747561 loss_ctc 9.132337 loss_rnnt 3.987442 hw_loss 0.246734 lr 0.00029894 rank 0
2023-02-27 13:04:58,838 DEBUG TRAIN Batch 44/1000 loss 7.895981 loss_att 9.811333 loss_ctc 12.283846 loss_rnnt 6.842461 hw_loss 0.160128 lr 0.00029894 rank 2
2023-02-27 13:04:58,839 DEBUG TRAIN Batch 44/1000 loss 1.693614 loss_att 4.537563 loss_ctc 2.515971 loss_rnnt 0.870140 hw_loss 0.271943 lr 0.00029894 rank 1
2023-02-27 13:04:58,840 DEBUG TRAIN Batch 44/1000 loss 10.234705 loss_att 14.369879 loss_ctc 19.392763 loss_rnnt 8.082397 hw_loss 0.195371 lr 0.00029894 rank 4
2023-02-27 13:06:03,696 DEBUG TRAIN Batch 44/1100 loss 4.899580 loss_att 7.960358 loss_ctc 7.146261 loss_rnnt 3.811357 hw_loss 0.330957 lr 0.00029893 rank 6
2023-02-27 13:06:03,714 DEBUG TRAIN Batch 44/1100 loss 6.159688 loss_att 9.219095 loss_ctc 8.159858 loss_rnnt 5.124557 hw_loss 0.293552 lr 0.00029893 rank 1
2023-02-27 13:06:03,714 DEBUG TRAIN Batch 44/1100 loss 2.680319 loss_att 5.227997 loss_ctc 5.005714 loss_rnnt 1.679374 hw_loss 0.340043 lr 0.00029893 rank 3
2023-02-27 13:06:03,716 DEBUG TRAIN Batch 44/1100 loss 9.689956 loss_att 11.690650 loss_ctc 15.651535 loss_rnnt 8.375085 hw_loss 0.224727 lr 0.00029893 rank 4
2023-02-27 13:06:03,718 DEBUG TRAIN Batch 44/1100 loss 3.965683 loss_att 6.095423 loss_ctc 9.244594 loss_rnnt 2.653723 hw_loss 0.341543 lr 0.00029893 rank 5
2023-02-27 13:06:03,720 DEBUG TRAIN Batch 44/1100 loss 9.913991 loss_att 11.573226 loss_ctc 20.956335 loss_rnnt 7.931127 hw_loss 0.335071 lr 0.00029893 rank 7
2023-02-27 13:06:03,719 DEBUG TRAIN Batch 44/1100 loss 9.193046 loss_att 13.757940 loss_ctc 15.560270 loss_rnnt 7.317112 hw_loss 0.213736 lr 0.00029893 rank 0
2023-02-27 13:06:03,720 DEBUG TRAIN Batch 44/1100 loss 2.966554 loss_att 4.311251 loss_ctc 5.006635 loss_rnnt 2.315326 hw_loss 0.206771 lr 0.00029893 rank 2
2023-02-27 13:06:42,578 DEBUG TRAIN Batch 44/1200 loss 9.113840 loss_att 11.872523 loss_ctc 14.599726 loss_rnnt 7.768102 hw_loss 0.117282 lr 0.00029891 rank 5
2023-02-27 13:06:42,586 DEBUG TRAIN Batch 44/1200 loss 6.978490 loss_att 11.021438 loss_ctc 13.968964 loss_rnnt 5.157766 hw_loss 0.150135 lr 0.00029891 rank 0
2023-02-27 13:06:42,587 DEBUG TRAIN Batch 44/1200 loss 7.252032 loss_att 10.612404 loss_ctc 14.419147 loss_rnnt 5.525581 hw_loss 0.185177 lr 0.00029891 rank 4
2023-02-27 13:06:42,591 DEBUG TRAIN Batch 44/1200 loss 7.925318 loss_att 8.990994 loss_ctc 12.793731 loss_rnnt 6.882099 hw_loss 0.339304 lr 0.00029891 rank 1
2023-02-27 13:06:42,593 DEBUG TRAIN Batch 44/1200 loss 6.035898 loss_att 9.127264 loss_ctc 13.987379 loss_rnnt 4.185135 hw_loss 0.323049 lr 0.00029891 rank 3
2023-02-27 13:06:42,593 DEBUG TRAIN Batch 44/1200 loss 6.809002 loss_att 9.508109 loss_ctc 11.885684 loss_rnnt 5.481705 hw_loss 0.207345 lr 0.00029891 rank 2
2023-02-27 13:06:42,595 DEBUG TRAIN Batch 44/1200 loss 8.668151 loss_att 10.194746 loss_ctc 12.153471 loss_rnnt 7.772357 hw_loss 0.235808 lr 0.00029891 rank 6
2023-02-27 13:06:42,597 DEBUG TRAIN Batch 44/1200 loss 5.906617 loss_att 7.494858 loss_ctc 10.060160 loss_rnnt 4.888196 hw_loss 0.275561 lr 0.00029891 rank 7
2023-02-27 13:07:20,759 DEBUG TRAIN Batch 44/1300 loss 5.835448 loss_att 9.266666 loss_ctc 10.794289 loss_rnnt 4.340599 hw_loss 0.276426 lr 0.00029890 rank 2
2023-02-27 13:07:20,763 DEBUG TRAIN Batch 44/1300 loss 3.569881 loss_att 6.133528 loss_ctc 6.560287 loss_rnnt 2.605045 hw_loss 0.100099 lr 0.00029890 rank 3
2023-02-27 13:07:20,763 DEBUG TRAIN Batch 44/1300 loss 9.769651 loss_att 11.324341 loss_ctc 15.196590 loss_rnnt 8.555313 hw_loss 0.337141 lr 0.00029890 rank 0
2023-02-27 13:07:20,763 DEBUG TRAIN Batch 44/1300 loss 5.577347 loss_att 8.251450 loss_ctc 10.521940 loss_rnnt 4.332484 hw_loss 0.095181 lr 0.00029890 rank 6
2023-02-27 13:07:20,764 DEBUG TRAIN Batch 44/1300 loss 3.511121 loss_att 7.117911 loss_ctc 6.398641 loss_rnnt 2.280569 hw_loss 0.232857 lr 0.00029890 rank 7
2023-02-27 13:07:20,766 DEBUG TRAIN Batch 44/1300 loss 7.978344 loss_att 10.480121 loss_ctc 12.347373 loss_rnnt 6.800043 hw_loss 0.178891 lr 0.00029890 rank 4
2023-02-27 13:07:20,768 DEBUG TRAIN Batch 44/1300 loss 3.337689 loss_att 6.552730 loss_ctc 5.182871 loss_rnnt 2.283652 hw_loss 0.309385 lr 0.00029890 rank 5
2023-02-27 13:07:20,821 DEBUG TRAIN Batch 44/1300 loss 7.847190 loss_att 9.941597 loss_ctc 11.787738 loss_rnnt 6.749750 hw_loss 0.287161 lr 0.00029890 rank 1
2023-02-27 13:08:00,325 DEBUG TRAIN Batch 44/1400 loss 3.522749 loss_att 6.588430 loss_ctc 8.322994 loss_rnnt 2.143235 hw_loss 0.236898 lr 0.00029889 rank 4
2023-02-27 13:08:00,333 DEBUG TRAIN Batch 44/1400 loss 12.782231 loss_att 13.841160 loss_ctc 21.310595 loss_rnnt 11.405307 hw_loss 0.052545 lr 0.00029889 rank 1
2023-02-27 13:08:00,335 DEBUG TRAIN Batch 44/1400 loss 3.833906 loss_att 5.614242 loss_ctc 6.361137 loss_rnnt 2.961812 hw_loss 0.335744 lr 0.00029889 rank 6
2023-02-27 13:08:00,337 DEBUG TRAIN Batch 44/1400 loss 6.289982 loss_att 13.262062 loss_ctc 20.813385 loss_rnnt 2.899839 hw_loss 0.111137 lr 0.00029889 rank 0
2023-02-27 13:08:00,341 DEBUG TRAIN Batch 44/1400 loss 4.562854 loss_att 8.461979 loss_ctc 8.695711 loss_rnnt 3.204566 hw_loss 0.051403 lr 0.00029889 rank 5
2023-02-27 13:08:00,342 DEBUG TRAIN Batch 44/1400 loss 4.441199 loss_att 8.350707 loss_ctc 5.386796 loss_rnnt 3.405401 hw_loss 0.239658 lr 0.00029889 rank 3
2023-02-27 13:08:00,344 DEBUG TRAIN Batch 44/1400 loss 3.328367 loss_att 6.428308 loss_ctc 5.345647 loss_rnnt 2.274156 hw_loss 0.309848 lr 0.00029889 rank 7
2023-02-27 13:08:00,349 DEBUG TRAIN Batch 44/1400 loss 14.271038 loss_att 16.576885 loss_ctc 19.000370 loss_rnnt 13.046958 hw_loss 0.248123 lr 0.00029889 rank 2
2023-02-27 13:09:03,886 DEBUG TRAIN Batch 44/1500 loss 3.859610 loss_att 7.024151 loss_ctc 9.869102 loss_rnnt 2.279523 hw_loss 0.273587 lr 0.00029887 rank 0
2023-02-27 13:09:03,886 DEBUG TRAIN Batch 44/1500 loss 1.612066 loss_att 4.402661 loss_ctc 3.891671 loss_rnnt 0.571591 hw_loss 0.334517 lr 0.00029887 rank 3
2023-02-27 13:09:03,887 DEBUG TRAIN Batch 44/1500 loss 2.945176 loss_att 6.784657 loss_ctc 3.897279 loss_rnnt 1.949736 hw_loss 0.188618 lr 0.00029887 rank 4
2023-02-27 13:09:03,890 DEBUG TRAIN Batch 44/1500 loss 6.898148 loss_att 8.407701 loss_ctc 9.187159 loss_rnnt 6.135557 hw_loss 0.291523 lr 0.00029887 rank 6
2023-02-27 13:09:03,890 DEBUG TRAIN Batch 44/1500 loss 8.635659 loss_att 11.642529 loss_ctc 15.791438 loss_rnnt 7.060602 hw_loss 0.036708 lr 0.00029887 rank 5
2023-02-27 13:09:03,896 DEBUG TRAIN Batch 44/1500 loss 3.372452 loss_att 7.425205 loss_ctc 7.011662 loss_rnnt 2.057091 hw_loss 0.036716 lr 0.00029887 rank 2
2023-02-27 13:09:03,901 DEBUG TRAIN Batch 44/1500 loss 12.675087 loss_att 16.317183 loss_ctc 26.359322 loss_rnnt 9.953979 hw_loss 0.315232 lr 0.00029887 rank 7
2023-02-27 13:09:03,905 DEBUG TRAIN Batch 44/1500 loss 4.499874 loss_att 6.091365 loss_ctc 6.870691 loss_rnnt 3.706666 hw_loss 0.297753 lr 0.00029887 rank 1
2023-02-27 13:09:42,234 DEBUG TRAIN Batch 44/1600 loss 11.400160 loss_att 14.713119 loss_ctc 25.557341 loss_rnnt 8.737060 hw_loss 0.211659 lr 0.00029886 rank 1
2023-02-27 13:09:42,235 DEBUG TRAIN Batch 44/1600 loss 7.570687 loss_att 10.951815 loss_ctc 16.125340 loss_rnnt 5.628819 hw_loss 0.234415 lr 0.00029886 rank 4
2023-02-27 13:09:42,243 DEBUG TRAIN Batch 44/1600 loss 6.724720 loss_att 9.457823 loss_ctc 9.151837 loss_rnnt 5.679991 hw_loss 0.327172 lr 0.00029886 rank 3
2023-02-27 13:09:42,249 DEBUG TRAIN Batch 44/1600 loss 6.398075 loss_att 9.035484 loss_ctc 13.635831 loss_rnnt 4.809650 hw_loss 0.179829 lr 0.00029886 rank 6
2023-02-27 13:09:42,252 DEBUG TRAIN Batch 44/1600 loss 9.597230 loss_att 11.170847 loss_ctc 14.659370 loss_rnnt 8.428131 hw_loss 0.336419 lr 0.00029886 rank 2
2023-02-27 13:09:42,251 DEBUG TRAIN Batch 44/1600 loss 5.766624 loss_att 8.502195 loss_ctc 11.311944 loss_rnnt 4.384940 hw_loss 0.178490 lr 0.00029886 rank 5
2023-02-27 13:09:42,260 DEBUG TRAIN Batch 44/1600 loss 6.198578 loss_att 9.866260 loss_ctc 7.486852 loss_rnnt 5.199011 hw_loss 0.176740 lr 0.00029886 rank 0
2023-02-27 13:09:42,273 DEBUG TRAIN Batch 44/1600 loss 5.374173 loss_att 7.479249 loss_ctc 8.482049 loss_rnnt 4.411803 hw_loss 0.238070 lr 0.00029886 rank 7
2023-02-27 13:10:21,024 DEBUG TRAIN Batch 44/1700 loss 4.058963 loss_att 6.395938 loss_ctc 6.592358 loss_rnnt 3.094486 hw_loss 0.298681 lr 0.00029885 rank 6
2023-02-27 13:10:21,027 DEBUG TRAIN Batch 44/1700 loss 3.787413 loss_att 5.426213 loss_ctc 5.656236 loss_rnnt 3.083498 hw_loss 0.238085 lr 0.00029885 rank 2
2023-02-27 13:10:21,037 DEBUG TRAIN Batch 44/1700 loss 6.319535 loss_att 10.958548 loss_ctc 20.829769 loss_rnnt 3.386624 hw_loss 0.132020 lr 0.00029885 rank 3
2023-02-27 13:10:21,041 DEBUG TRAIN Batch 44/1700 loss 7.517496 loss_att 9.492706 loss_ctc 10.535770 loss_rnnt 6.538609 hw_loss 0.340140 lr 0.00029885 rank 1
2023-02-27 13:10:21,048 DEBUG TRAIN Batch 44/1700 loss 3.653305 loss_att 5.804614 loss_ctc 6.361532 loss_rnnt 2.733030 hw_loss 0.241718 lr 0.00029885 rank 0
2023-02-27 13:10:21,058 DEBUG TRAIN Batch 44/1700 loss 13.259570 loss_att 17.079969 loss_ctc 18.747780 loss_rnnt 11.633987 hw_loss 0.243265 lr 0.00029885 rank 7
2023-02-27 13:10:21,076 DEBUG TRAIN Batch 44/1700 loss 8.102850 loss_att 12.508880 loss_ctc 13.699177 loss_rnnt 6.312390 hw_loss 0.305766 lr 0.00029885 rank 5
2023-02-27 13:10:21,084 DEBUG TRAIN Batch 44/1700 loss 5.530062 loss_att 8.982080 loss_ctc 7.664764 loss_rnnt 4.449728 hw_loss 0.197446 lr 0.00029885 rank 4
2023-02-27 13:11:26,229 DEBUG TRAIN Batch 44/1800 loss 6.199140 loss_att 8.770922 loss_ctc 12.707582 loss_rnnt 4.658393 hw_loss 0.297371 lr 0.00029883 rank 4
2023-02-27 13:11:26,243 DEBUG TRAIN Batch 44/1800 loss 5.758402 loss_att 8.457830 loss_ctc 12.255793 loss_rnnt 4.225317 hw_loss 0.237900 lr 0.00029883 rank 0
2023-02-27 13:11:26,249 DEBUG TRAIN Batch 44/1800 loss 9.263649 loss_att 12.084086 loss_ctc 11.660583 loss_rnnt 8.233736 hw_loss 0.274191 lr 0.00029883 rank 7
2023-02-27 13:11:26,252 DEBUG TRAIN Batch 44/1800 loss 9.826099 loss_att 12.312595 loss_ctc 14.957727 loss_rnnt 8.547698 hw_loss 0.181662 lr 0.00029883 rank 3
2023-02-27 13:11:26,253 DEBUG TRAIN Batch 44/1800 loss 9.580328 loss_att 12.147003 loss_ctc 14.918833 loss_rnnt 8.206021 hw_loss 0.279693 lr 0.00029883 rank 5
2023-02-27 13:11:26,253 DEBUG TRAIN Batch 44/1800 loss 7.367679 loss_att 9.433207 loss_ctc 8.957546 loss_rnnt 6.569538 hw_loss 0.324473 lr 0.00029883 rank 6
2023-02-27 13:11:26,254 DEBUG TRAIN Batch 44/1800 loss 3.568319 loss_att 8.784915 loss_ctc 5.449745 loss_rnnt 2.128731 hw_loss 0.272647 lr 0.00029883 rank 2
2023-02-27 13:11:26,255 DEBUG TRAIN Batch 44/1800 loss 9.809530 loss_att 13.527481 loss_ctc 15.739065 loss_rnnt 8.110999 hw_loss 0.308133 lr 0.00029883 rank 1
2023-02-27 13:12:05,052 DEBUG TRAIN Batch 44/1900 loss 2.405799 loss_att 5.544335 loss_ctc 2.717525 loss_rnnt 1.540295 hw_loss 0.367939 lr 0.00029882 rank 6
2023-02-27 13:12:05,052 DEBUG TRAIN Batch 44/1900 loss 9.355210 loss_att 11.399234 loss_ctc 12.067924 loss_rnnt 8.422529 hw_loss 0.304090 lr 0.00029882 rank 0
2023-02-27 13:12:05,060 DEBUG TRAIN Batch 44/1900 loss 9.253368 loss_att 12.075688 loss_ctc 11.124129 loss_rnnt 8.323891 hw_loss 0.216710 lr 0.00029882 rank 1
2023-02-27 13:12:05,060 DEBUG TRAIN Batch 44/1900 loss 12.735194 loss_att 12.072947 loss_ctc 18.494394 loss_rnnt 11.790562 hw_loss 0.579729 lr 0.00029882 rank 3
2023-02-27 13:12:05,060 DEBUG TRAIN Batch 44/1900 loss 6.004991 loss_att 6.915040 loss_ctc 8.745131 loss_rnnt 5.262863 hw_loss 0.365185 lr 0.00029882 rank 5
2023-02-27 13:12:05,063 DEBUG TRAIN Batch 44/1900 loss 6.177485 loss_att 7.496520 loss_ctc 10.789599 loss_rnnt 5.081527 hw_loss 0.407255 lr 0.00029882 rank 2
2023-02-27 13:12:05,065 DEBUG TRAIN Batch 44/1900 loss 4.611118 loss_att 4.985475 loss_ctc 5.557098 loss_rnnt 4.216393 hw_loss 0.363231 lr 0.00029882 rank 7
2023-02-27 13:12:05,110 DEBUG TRAIN Batch 44/1900 loss 7.975675 loss_att 7.982475 loss_ctc 11.761632 loss_rnnt 7.232539 hw_loss 0.444340 lr 0.00029882 rank 4
2023-02-27 13:12:43,711 DEBUG TRAIN Batch 44/2000 loss 7.592687 loss_att 11.724480 loss_ctc 15.594496 loss_rnnt 5.464125 hw_loss 0.441179 lr 0.00029881 rank 7
2023-02-27 13:12:43,724 DEBUG TRAIN Batch 44/2000 loss 5.085515 loss_att 6.990153 loss_ctc 6.915569 loss_rnnt 4.290044 hw_loss 0.319753 lr 0.00029881 rank 1
2023-02-27 13:12:43,726 DEBUG TRAIN Batch 44/2000 loss 5.697428 loss_att 9.256111 loss_ctc 11.945529 loss_rnnt 4.131275 hw_loss 0.040006 lr 0.00029881 rank 3
2023-02-27 13:12:43,729 DEBUG TRAIN Batch 44/2000 loss 5.981768 loss_att 9.188120 loss_ctc 10.109921 loss_rnnt 4.530560 hw_loss 0.486596 lr 0.00029881 rank 6
2023-02-27 13:12:43,731 DEBUG TRAIN Batch 44/2000 loss 6.451910 loss_att 9.759988 loss_ctc 10.606240 loss_rnnt 5.145986 hw_loss 0.169495 lr 0.00029881 rank 0
2023-02-27 13:12:43,732 DEBUG TRAIN Batch 44/2000 loss 9.952774 loss_att 11.096824 loss_ctc 11.770540 loss_rnnt 9.297219 hw_loss 0.345701 lr 0.00029881 rank 2
2023-02-27 13:12:43,734 DEBUG TRAIN Batch 44/2000 loss 4.449998 loss_att 6.368620 loss_ctc 4.843804 loss_rnnt 3.844482 hw_loss 0.317408 lr 0.00029881 rank 5
2023-02-27 13:12:43,738 DEBUG TRAIN Batch 44/2000 loss 7.538144 loss_att 14.791333 loss_ctc 21.852972 loss_rnnt 4.043229 hw_loss 0.254310 lr 0.00029881 rank 4
2023-02-27 13:13:23,606 DEBUG TRAIN Batch 44/2100 loss 8.432841 loss_att 12.208973 loss_ctc 14.105949 loss_rnnt 6.840232 hw_loss 0.151818 lr 0.00029879 rank 4
2023-02-27 13:13:23,607 DEBUG TRAIN Batch 44/2100 loss 6.868011 loss_att 9.728155 loss_ctc 10.832470 loss_rnnt 5.750574 hw_loss 0.031526 lr 0.00029879 rank 0
2023-02-27 13:13:23,608 DEBUG TRAIN Batch 44/2100 loss 5.442080 loss_att 7.840480 loss_ctc 9.635103 loss_rnnt 4.246861 hw_loss 0.293381 lr 0.00029879 rank 7
2023-02-27 13:13:23,619 DEBUG TRAIN Batch 44/2100 loss 3.207878 loss_att 5.073681 loss_ctc 4.791284 loss_rnnt 2.511538 hw_loss 0.210109 lr 0.00029879 rank 2
2023-02-27 13:13:23,621 DEBUG TRAIN Batch 44/2100 loss 4.500616 loss_att 8.779262 loss_ctc 6.578867 loss_rnnt 3.287210 hw_loss 0.151081 lr 0.00029879 rank 1
2023-02-27 13:13:23,622 DEBUG TRAIN Batch 44/2100 loss 2.918725 loss_att 5.477154 loss_ctc 7.572946 loss_rnnt 1.709049 hw_loss 0.145175 lr 0.00029879 rank 3
2023-02-27 13:13:23,623 DEBUG TRAIN Batch 44/2100 loss 7.310238 loss_att 12.867236 loss_ctc 14.585872 loss_rnnt 5.085587 hw_loss 0.268439 lr 0.00029879 rank 6
2023-02-27 13:13:23,647 DEBUG TRAIN Batch 44/2100 loss 3.594267 loss_att 5.944683 loss_ctc 9.054329 loss_rnnt 2.325996 hw_loss 0.131585 lr 0.00029879 rank 5
2023-02-27 13:14:27,975 DEBUG TRAIN Batch 44/2200 loss 8.541699 loss_att 12.355284 loss_ctc 16.200024 loss_rnnt 6.692324 hw_loss 0.122905 lr 0.00029878 rank 6
2023-02-27 13:14:27,975 DEBUG TRAIN Batch 44/2200 loss 4.267068 loss_att 8.326558 loss_ctc 7.050371 loss_rnnt 2.912046 hw_loss 0.322533 lr 0.00029878 rank 7
2023-02-27 13:14:27,977 DEBUG TRAIN Batch 44/2200 loss 5.630080 loss_att 9.773814 loss_ctc 8.871449 loss_rnnt 4.283337 hw_loss 0.160901 lr 0.00029878 rank 5
2023-02-27 13:14:27,978 DEBUG TRAIN Batch 44/2200 loss 8.370378 loss_att 10.851511 loss_ctc 13.519498 loss_rnnt 7.095325 hw_loss 0.173024 lr 0.00029878 rank 3
2023-02-27 13:14:27,980 DEBUG TRAIN Batch 44/2200 loss 5.027738 loss_att 9.655935 loss_ctc 17.237761 loss_rnnt 2.355686 hw_loss 0.222018 lr 0.00029878 rank 0
2023-02-27 13:14:27,981 DEBUG TRAIN Batch 44/2200 loss 7.268997 loss_att 10.268846 loss_ctc 12.190250 loss_rnnt 5.856884 hw_loss 0.292456 lr 0.00029878 rank 1
2023-02-27 13:14:27,983 DEBUG TRAIN Batch 44/2200 loss 8.572197 loss_att 12.958047 loss_ctc 17.674824 loss_rnnt 6.355112 hw_loss 0.236683 lr 0.00029878 rank 2
2023-02-27 13:14:27,984 DEBUG TRAIN Batch 44/2200 loss 6.142231 loss_att 9.564103 loss_ctc 9.467949 loss_rnnt 4.933860 hw_loss 0.151066 lr 0.00029878 rank 4
2023-02-27 13:15:06,308 DEBUG TRAIN Batch 44/2300 loss 12.673921 loss_att 17.140800 loss_ctc 25.021049 loss_rnnt 10.046386 hw_loss 0.164766 lr 0.00029877 rank 6
2023-02-27 13:15:06,318 DEBUG TRAIN Batch 44/2300 loss 13.481159 loss_att 19.230072 loss_ctc 27.418381 loss_rnnt 10.397964 hw_loss 0.140844 lr 0.00029877 rank 3
2023-02-27 13:15:06,320 DEBUG TRAIN Batch 44/2300 loss 3.472896 loss_att 6.350895 loss_ctc 6.812188 loss_rnnt 2.344826 hw_loss 0.201060 lr 0.00029877 rank 7
2023-02-27 13:15:06,323 DEBUG TRAIN Batch 44/2300 loss 8.216961 loss_att 11.161026 loss_ctc 15.099097 loss_rnnt 6.623760 hw_loss 0.162693 lr 0.00029877 rank 4
2023-02-27 13:15:06,324 DEBUG TRAIN Batch 44/2300 loss 8.059828 loss_att 14.157094 loss_ctc 19.115036 loss_rnnt 5.199887 hw_loss 0.312111 lr 0.00029877 rank 5
2023-02-27 13:15:06,327 DEBUG TRAIN Batch 44/2300 loss 4.473744 loss_att 7.524868 loss_ctc 5.066870 loss_rnnt 3.617731 hw_loss 0.312571 lr 0.00029877 rank 0
2023-02-27 13:15:06,328 DEBUG TRAIN Batch 44/2300 loss 9.403281 loss_att 12.940661 loss_ctc 15.532950 loss_rnnt 7.766493 hw_loss 0.210042 lr 0.00029877 rank 2
2023-02-27 13:15:06,332 DEBUG TRAIN Batch 44/2300 loss 5.953279 loss_att 8.857272 loss_ctc 11.080975 loss_rnnt 4.508832 hw_loss 0.337418 lr 0.00029877 rank 1
2023-02-27 13:15:45,335 DEBUG TRAIN Batch 44/2400 loss 5.131822 loss_att 8.847412 loss_ctc 9.816350 loss_rnnt 3.622232 hw_loss 0.266003 lr 0.00029875 rank 4
2023-02-27 13:15:45,337 DEBUG TRAIN Batch 44/2400 loss 6.577682 loss_att 9.528154 loss_ctc 13.239886 loss_rnnt 4.890056 hw_loss 0.392321 lr 0.00029875 rank 6
2023-02-27 13:15:45,341 DEBUG TRAIN Batch 44/2400 loss 2.071591 loss_att 4.170379 loss_ctc 4.057834 loss_rnnt 1.221345 hw_loss 0.310603 lr 0.00029875 rank 2
2023-02-27 13:15:45,343 DEBUG TRAIN Batch 44/2400 loss 1.892861 loss_att 4.200271 loss_ctc 4.656069 loss_rnnt 0.893785 hw_loss 0.317188 lr 0.00029875 rank 0
2023-02-27 13:15:45,343 DEBUG TRAIN Batch 44/2400 loss 10.409330 loss_att 12.970953 loss_ctc 15.225979 loss_rnnt 9.199457 hw_loss 0.103741 lr 0.00029875 rank 3
2023-02-27 13:15:45,345 DEBUG TRAIN Batch 44/2400 loss 8.723762 loss_att 10.071182 loss_ctc 13.892792 loss_rnnt 7.573494 hw_loss 0.359208 lr 0.00029875 rank 5
2023-02-27 13:15:45,348 DEBUG TRAIN Batch 44/2400 loss 9.796943 loss_att 14.090633 loss_ctc 16.443768 loss_rnnt 7.871697 hw_loss 0.337994 lr 0.00029875 rank 1
2023-02-27 13:15:45,349 DEBUG TRAIN Batch 44/2400 loss 6.317836 loss_att 8.599648 loss_ctc 12.359694 loss_rnnt 4.904066 hw_loss 0.284674 lr 0.00029875 rank 7
2023-02-27 13:16:52,029 DEBUG TRAIN Batch 44/2500 loss 8.411126 loss_att 10.990868 loss_ctc 14.629059 loss_rnnt 6.910257 hw_loss 0.292244 lr 0.00029874 rank 7
2023-02-27 13:16:52,034 DEBUG TRAIN Batch 44/2500 loss 11.473862 loss_att 18.021997 loss_ctc 22.550180 loss_rnnt 8.484851 hw_loss 0.379764 lr 0.00029874 rank 0
2023-02-27 13:16:52,035 DEBUG TRAIN Batch 44/2500 loss 10.648268 loss_att 11.502958 loss_ctc 17.411562 loss_rnnt 9.333634 hw_loss 0.453604 lr 0.00029874 rank 3
2023-02-27 13:16:52,035 DEBUG TRAIN Batch 44/2500 loss 5.191950 loss_att 7.306775 loss_ctc 9.427455 loss_rnnt 4.064010 hw_loss 0.262952 lr 0.00029874 rank 5
2023-02-27 13:16:52,036 DEBUG TRAIN Batch 44/2500 loss 6.752761 loss_att 10.637948 loss_ctc 16.169180 loss_rnnt 4.540571 hw_loss 0.336805 lr 0.00029874 rank 6
2023-02-27 13:16:52,037 DEBUG TRAIN Batch 44/2500 loss 3.834686 loss_att 6.071776 loss_ctc 7.519052 loss_rnnt 2.723505 hw_loss 0.323466 lr 0.00029874 rank 4
2023-02-27 13:16:52,036 DEBUG TRAIN Batch 44/2500 loss 5.216629 loss_att 7.579464 loss_ctc 9.364464 loss_rnnt 4.054492 hw_loss 0.255984 lr 0.00029874 rank 2
2023-02-27 13:16:52,038 DEBUG TRAIN Batch 44/2500 loss 9.987927 loss_att 11.828810 loss_ctc 17.271576 loss_rnnt 8.503654 hw_loss 0.271768 lr 0.00029874 rank 1
2023-02-27 13:17:30,373 DEBUG TRAIN Batch 44/2600 loss 8.147128 loss_att 11.095547 loss_ctc 14.334190 loss_rnnt 6.627258 hw_loss 0.197333 lr 0.00029873 rank 4
2023-02-27 13:17:30,375 DEBUG TRAIN Batch 44/2600 loss 2.471621 loss_att 5.918518 loss_ctc 6.743280 loss_rnnt 0.981937 hw_loss 0.432656 lr 0.00029873 rank 6
2023-02-27 13:17:30,376 DEBUG TRAIN Batch 44/2600 loss 10.331337 loss_att 12.311795 loss_ctc 33.101341 loss_rnnt 6.708450 hw_loss 0.357739 lr 0.00029873 rank 3
2023-02-27 13:17:30,377 DEBUG TRAIN Batch 44/2600 loss 4.389084 loss_att 9.723715 loss_ctc 6.979593 loss_rnnt 2.880435 hw_loss 0.180603 lr 0.00029873 rank 5
2023-02-27 13:17:30,378 DEBUG TRAIN Batch 44/2600 loss 12.525392 loss_att 15.693203 loss_ctc 22.903263 loss_rnnt 10.325656 hw_loss 0.342109 lr 0.00029873 rank 7
2023-02-27 13:17:30,378 DEBUG TRAIN Batch 44/2600 loss 7.758075 loss_att 7.968159 loss_ctc 11.501443 loss_rnnt 6.899748 hw_loss 0.594740 lr 0.00029873 rank 0
2023-02-27 13:17:30,379 DEBUG TRAIN Batch 44/2600 loss 7.898460 loss_att 10.705893 loss_ctc 12.126349 loss_rnnt 6.590951 hw_loss 0.341818 lr 0.00029873 rank 1
2023-02-27 13:17:30,383 DEBUG TRAIN Batch 44/2600 loss 3.939815 loss_att 8.405765 loss_ctc 7.560157 loss_rnnt 2.465537 hw_loss 0.184455 lr 0.00029873 rank 2
2023-02-27 13:18:08,770 DEBUG TRAIN Batch 44/2700 loss 5.540036 loss_att 8.220850 loss_ctc 11.644264 loss_rnnt 4.064463 hw_loss 0.235339 lr 0.00029871 rank 6
2023-02-27 13:18:08,784 DEBUG TRAIN Batch 44/2700 loss 11.105347 loss_att 13.528753 loss_ctc 18.265373 loss_rnnt 9.512112 hw_loss 0.288533 lr 0.00029871 rank 3
2023-02-27 13:18:08,790 DEBUG TRAIN Batch 44/2700 loss 8.904702 loss_att 12.857023 loss_ctc 18.132637 loss_rnnt 6.748800 hw_loss 0.253212 lr 0.00029871 rank 1
2023-02-27 13:18:08,792 DEBUG TRAIN Batch 44/2700 loss 8.276169 loss_att 9.917448 loss_ctc 10.630287 loss_rnnt 7.377922 hw_loss 0.480203 lr 0.00029871 rank 0
2023-02-27 13:18:08,792 DEBUG TRAIN Batch 44/2700 loss 3.090443 loss_att 6.408617 loss_ctc 8.832345 loss_rnnt 1.577267 hw_loss 0.157415 lr 0.00029871 rank 7
2023-02-27 13:18:08,796 DEBUG TRAIN Batch 44/2700 loss 1.934872 loss_att 5.970911 loss_ctc 3.617359 loss_rnnt 0.736539 hw_loss 0.312740 lr 0.00029871 rank 2
2023-02-27 13:18:08,802 DEBUG TRAIN Batch 44/2700 loss 7.582216 loss_att 11.023947 loss_ctc 10.439474 loss_rnnt 6.423197 hw_loss 0.168197 lr 0.00029871 rank 5
2023-02-27 13:18:08,829 DEBUG TRAIN Batch 44/2700 loss 5.481762 loss_att 9.608444 loss_ctc 13.748804 loss_rnnt 3.421061 hw_loss 0.249548 lr 0.00029871 rank 4
2023-02-27 13:18:48,053 DEBUG TRAIN Batch 44/2800 loss 5.961000 loss_att 9.203770 loss_ctc 11.508567 loss_rnnt 4.446401 hw_loss 0.236945 lr 0.00029870 rank 4
2023-02-27 13:18:48,059 DEBUG TRAIN Batch 44/2800 loss 5.737090 loss_att 9.177217 loss_ctc 11.658310 loss_rnnt 4.191982 hw_loss 0.126724 lr 0.00029870 rank 7
2023-02-27 13:18:48,059 DEBUG TRAIN Batch 44/2800 loss 6.882652 loss_att 10.626554 loss_ctc 11.724923 loss_rnnt 5.362844 hw_loss 0.235110 lr 0.00029870 rank 3
2023-02-27 13:18:48,064 DEBUG TRAIN Batch 44/2800 loss 3.464739 loss_att 6.118217 loss_ctc 4.793854 loss_rnnt 2.553018 hw_loss 0.382143 lr 0.00029870 rank 0
2023-02-27 13:18:48,064 DEBUG TRAIN Batch 44/2800 loss 2.444810 loss_att 5.312836 loss_ctc 3.510431 loss_rnnt 1.568789 hw_loss 0.300626 lr 0.00029870 rank 6
2023-02-27 13:18:48,067 DEBUG TRAIN Batch 44/2800 loss 3.788023 loss_att 6.998527 loss_ctc 6.812802 loss_rnnt 2.702554 hw_loss 0.075122 lr 0.00029870 rank 5
2023-02-27 13:18:48,071 DEBUG TRAIN Batch 44/2800 loss 4.358668 loss_att 9.016069 loss_ctc 13.066193 loss_rnnt 2.095593 hw_loss 0.319860 lr 0.00029870 rank 1
2023-02-27 13:18:48,085 DEBUG TRAIN Batch 44/2800 loss 15.223427 loss_att 18.151258 loss_ctc 27.485296 loss_rnnt 12.861590 hw_loss 0.265040 lr 0.00029870 rank 2
2023-02-27 13:19:53,585 DEBUG TRAIN Batch 44/2900 loss 13.655752 loss_att 16.725468 loss_ctc 24.079142 loss_rnnt 11.490574 hw_loss 0.302717 lr 0.00029869 rank 2
2023-02-27 13:19:53,585 DEBUG TRAIN Batch 44/2900 loss 5.397331 loss_att 11.123033 loss_ctc 11.545277 loss_rnnt 3.368597 hw_loss 0.119751 lr 0.00029869 rank 3
2023-02-27 13:19:53,587 DEBUG TRAIN Batch 44/2900 loss 3.246840 loss_att 5.992711 loss_ctc 5.849815 loss_rnnt 2.207711 hw_loss 0.267922 lr 0.00029869 rank 7
2023-02-27 13:19:53,588 DEBUG TRAIN Batch 44/2900 loss 7.960622 loss_att 11.652083 loss_ctc 9.005125 loss_rnnt 6.980689 hw_loss 0.191949 lr 0.00029869 rank 4
2023-02-27 13:19:53,588 DEBUG TRAIN Batch 44/2900 loss 7.181942 loss_att 8.565928 loss_ctc 11.568762 loss_rnnt 6.167099 hw_loss 0.287131 lr 0.00029869 rank 6
2023-02-27 13:19:53,590 DEBUG TRAIN Batch 44/2900 loss 6.685974 loss_att 8.686287 loss_ctc 11.169355 loss_rnnt 5.523282 hw_loss 0.309084 lr 0.00029869 rank 1
2023-02-27 13:19:53,590 DEBUG TRAIN Batch 44/2900 loss 2.056823 loss_att 6.019444 loss_ctc 3.020043 loss_rnnt 0.953915 hw_loss 0.341165 lr 0.00029869 rank 0
2023-02-27 13:19:53,629 DEBUG TRAIN Batch 44/2900 loss 4.680671 loss_att 9.305546 loss_ctc 13.068670 loss_rnnt 2.449891 hw_loss 0.351384 lr 0.00029869 rank 5
2023-02-27 13:20:32,059 DEBUG TRAIN Batch 44/3000 loss 12.390553 loss_att 15.521450 loss_ctc 18.933214 loss_rnnt 10.807877 hw_loss 0.157766 lr 0.00029867 rank 1
2023-02-27 13:20:32,070 DEBUG TRAIN Batch 44/3000 loss 6.051234 loss_att 7.365257 loss_ctc 6.813721 loss_rnnt 5.543811 hw_loss 0.268037 lr 0.00029867 rank 7
2023-02-27 13:20:32,070 DEBUG TRAIN Batch 44/3000 loss 11.199948 loss_att 14.950745 loss_ctc 18.724110 loss_rnnt 9.319988 hw_loss 0.237336 lr 0.00029867 rank 3
2023-02-27 13:20:32,072 DEBUG TRAIN Batch 44/3000 loss 7.857072 loss_att 9.745810 loss_ctc 10.104645 loss_rnnt 7.083340 hw_loss 0.180578 lr 0.00029867 rank 6
2023-02-27 13:20:32,074 DEBUG TRAIN Batch 44/3000 loss 5.983758 loss_att 8.591740 loss_ctc 10.120707 loss_rnnt 4.800621 hw_loss 0.206151 lr 0.00029867 rank 5
2023-02-27 13:20:32,075 DEBUG TRAIN Batch 44/3000 loss 7.603968 loss_att 9.197990 loss_ctc 12.370886 loss_rnnt 6.508545 hw_loss 0.264429 lr 0.00029867 rank 0
2023-02-27 13:20:32,078 DEBUG TRAIN Batch 44/3000 loss 7.836767 loss_att 9.611169 loss_ctc 12.111835 loss_rnnt 6.762946 hw_loss 0.279247 lr 0.00029867 rank 4
2023-02-27 13:20:32,083 DEBUG TRAIN Batch 44/3000 loss 7.724692 loss_att 10.191648 loss_ctc 13.749187 loss_rnnt 6.248246 hw_loss 0.337104 lr 0.00029867 rank 2
2023-02-27 13:21:11,010 DEBUG TRAIN Batch 44/3100 loss 6.222032 loss_att 7.253708 loss_ctc 10.396992 loss_rnnt 5.431633 hw_loss 0.051381 lr 0.00029866 rank 6
2023-02-27 13:21:11,016 DEBUG TRAIN Batch 44/3100 loss 6.783929 loss_att 9.369410 loss_ctc 14.002518 loss_rnnt 5.173992 hw_loss 0.244429 lr 0.00029866 rank 5
2023-02-27 13:21:11,017 DEBUG TRAIN Batch 44/3100 loss 14.889862 loss_att 16.206385 loss_ctc 23.526947 loss_rnnt 13.310124 hw_loss 0.309041 lr 0.00029866 rank 2
2023-02-27 13:21:11,019 DEBUG TRAIN Batch 44/3100 loss 3.668817 loss_att 7.490507 loss_ctc 5.928718 loss_rnnt 2.449894 hw_loss 0.287372 lr 0.00029866 rank 0
2023-02-27 13:21:11,022 DEBUG TRAIN Batch 44/3100 loss 8.238341 loss_att 13.336207 loss_ctc 13.692178 loss_rnnt 6.287719 hw_loss 0.382259 lr 0.00029866 rank 7
2023-02-27 13:21:11,031 DEBUG TRAIN Batch 44/3100 loss 3.679013 loss_att 5.642770 loss_ctc 5.040970 loss_rnnt 2.887878 hw_loss 0.406479 lr 0.00029866 rank 3
2023-02-27 13:21:11,036 DEBUG TRAIN Batch 44/3100 loss 5.646748 loss_att 7.469554 loss_ctc 10.694807 loss_rnnt 4.423955 hw_loss 0.347168 lr 0.00029866 rank 4
2023-02-27 13:21:11,051 DEBUG TRAIN Batch 44/3100 loss 2.587404 loss_att 6.509201 loss_ctc 5.425718 loss_rnnt 1.273482 hw_loss 0.283351 lr 0.00029866 rank 1
2023-02-27 13:22:16,079 DEBUG TRAIN Batch 44/3200 loss 3.162450 loss_att 5.383245 loss_ctc 7.540594 loss_rnnt 2.018762 hw_loss 0.217080 lr 0.00029865 rank 4
2023-02-27 13:22:16,081 DEBUG TRAIN Batch 44/3200 loss 9.691714 loss_att 11.279043 loss_ctc 15.792516 loss_rnnt 8.440200 hw_loss 0.226140 lr 0.00029865 rank 1
2023-02-27 13:22:16,088 DEBUG TRAIN Batch 44/3200 loss 4.855718 loss_att 7.629707 loss_ctc 11.380478 loss_rnnt 3.330923 hw_loss 0.187555 lr 0.00029865 rank 6
2023-02-27 13:22:16,093 DEBUG TRAIN Batch 44/3200 loss 1.555658 loss_att 5.369748 loss_ctc 3.007472 loss_rnnt 0.406319 hw_loss 0.361773 lr 0.00029865 rank 3
2023-02-27 13:22:16,093 DEBUG TRAIN Batch 44/3200 loss 7.370963 loss_att 9.758884 loss_ctc 11.913917 loss_rnnt 6.180473 hw_loss 0.200961 lr 0.00029865 rank 0
2023-02-27 13:22:16,096 DEBUG TRAIN Batch 44/3200 loss 4.731160 loss_att 5.579486 loss_ctc 6.304268 loss_rnnt 4.139419 hw_loss 0.398117 lr 0.00029865 rank 5
2023-02-27 13:22:16,114 DEBUG TRAIN Batch 44/3200 loss 4.193413 loss_att 5.969834 loss_ctc 5.751602 loss_rnnt 3.386112 hw_loss 0.457986 lr 0.00029865 rank 7
2023-02-27 13:22:16,143 DEBUG TRAIN Batch 44/3200 loss 4.501713 loss_att 6.613218 loss_ctc 6.836063 loss_rnnt 3.663396 hw_loss 0.196442 lr 0.00029865 rank 2
2023-02-27 13:22:54,958 DEBUG TRAIN Batch 44/3300 loss 6.795979 loss_att 10.227094 loss_ctc 16.648384 loss_rnnt 4.652369 hw_loss 0.269499 lr 0.00029863 rank 4
2023-02-27 13:22:54,959 DEBUG TRAIN Batch 44/3300 loss 3.752419 loss_att 7.465292 loss_ctc 6.694229 loss_rnnt 2.565170 hw_loss 0.098311 lr 0.00029863 rank 0
2023-02-27 13:22:54,963 DEBUG TRAIN Batch 44/3300 loss 7.010206 loss_att 9.118779 loss_ctc 12.831965 loss_rnnt 5.692962 hw_loss 0.223678 lr 0.00029863 rank 5
2023-02-27 13:22:54,963 DEBUG TRAIN Batch 44/3300 loss 6.960652 loss_att 9.003742 loss_ctc 10.604803 loss_rnnt 5.941741 hw_loss 0.233262 lr 0.00029863 rank 3
2023-02-27 13:22:54,964 DEBUG TRAIN Batch 44/3300 loss 6.417371 loss_att 8.130238 loss_ctc 8.442039 loss_rnnt 5.583929 hw_loss 0.414212 lr 0.00029863 rank 6
2023-02-27 13:22:54,965 DEBUG TRAIN Batch 44/3300 loss 5.755948 loss_att 11.154652 loss_ctc 12.907992 loss_rnnt 3.542545 hw_loss 0.337606 lr 0.00029863 rank 1
2023-02-27 13:22:54,965 DEBUG TRAIN Batch 44/3300 loss 5.958607 loss_att 7.882032 loss_ctc 6.820682 loss_rnnt 5.351057 hw_loss 0.202353 lr 0.00029863 rank 2
2023-02-27 13:22:54,984 DEBUG TRAIN Batch 44/3300 loss 5.367121 loss_att 8.656500 loss_ctc 9.140715 loss_rnnt 4.108323 hw_loss 0.183330 lr 0.00029863 rank 7
2023-02-27 13:23:33,772 DEBUG TRAIN Batch 44/3400 loss 3.135992 loss_att 7.457562 loss_ctc 5.272635 loss_rnnt 1.924354 hw_loss 0.117071 lr 0.00029862 rank 5
2023-02-27 13:23:33,790 DEBUG TRAIN Batch 44/3400 loss 1.639683 loss_att 5.365250 loss_ctc 2.747519 loss_rnnt 0.599425 hw_loss 0.276435 lr 0.00029862 rank 3
2023-02-27 13:23:33,791 DEBUG TRAIN Batch 44/3400 loss 6.324483 loss_att 9.564517 loss_ctc 12.240097 loss_rnnt 4.778734 hw_loss 0.204365 lr 0.00029862 rank 7
2023-02-27 13:23:33,791 DEBUG TRAIN Batch 44/3400 loss 9.364982 loss_att 12.593187 loss_ctc 17.150642 loss_rnnt 7.579160 hw_loss 0.191424 lr 0.00029862 rank 6
2023-02-27 13:23:33,791 DEBUG TRAIN Batch 44/3400 loss 5.558415 loss_att 7.546911 loss_ctc 5.519344 loss_rnnt 5.067872 hw_loss 0.183850 lr 0.00029862 rank 4
2023-02-27 13:23:33,792 DEBUG TRAIN Batch 44/3400 loss 14.300878 loss_att 19.244930 loss_ctc 25.035961 loss_rnnt 11.698469 hw_loss 0.341725 lr 0.00029862 rank 0
2023-02-27 13:23:33,792 DEBUG TRAIN Batch 44/3400 loss 2.753895 loss_att 6.866122 loss_ctc 7.164662 loss_rnnt 1.260553 hw_loss 0.155239 lr 0.00029862 rank 1
2023-02-27 13:23:33,837 DEBUG TRAIN Batch 44/3400 loss 9.140954 loss_att 13.043320 loss_ctc 19.700775 loss_rnnt 6.791664 hw_loss 0.301578 lr 0.00029862 rank 2
2023-02-27 13:24:13,128 DEBUG TRAIN Batch 44/3500 loss 14.381814 loss_att 18.525541 loss_ctc 24.779413 loss_rnnt 12.066541 hw_loss 0.187839 lr 0.00029861 rank 3
2023-02-27 13:24:13,134 DEBUG TRAIN Batch 44/3500 loss 4.609942 loss_att 7.349602 loss_ctc 7.584752 loss_rnnt 3.520059 hw_loss 0.272457 lr 0.00029861 rank 2
2023-02-27 13:24:13,146 DEBUG TRAIN Batch 44/3500 loss 0.676273 loss_att 2.402430 loss_ctc 0.583523 loss_rnnt 0.214266 hw_loss 0.242143 lr 0.00029861 rank 7
2023-02-27 13:24:13,146 DEBUG TRAIN Batch 44/3500 loss 4.017736 loss_att 7.097943 loss_ctc 6.881392 loss_rnnt 2.874127 hw_loss 0.273275 lr 0.00029861 rank 5
2023-02-27 13:24:13,148 DEBUG TRAIN Batch 44/3500 loss 5.190342 loss_att 8.734171 loss_ctc 12.357738 loss_rnnt 3.415024 hw_loss 0.207937 lr 0.00029861 rank 0
2023-02-27 13:24:13,148 DEBUG TRAIN Batch 44/3500 loss 7.201531 loss_att 10.530846 loss_ctc 12.542276 loss_rnnt 5.656772 hw_loss 0.312744 lr 0.00029861 rank 4
2023-02-27 13:24:13,148 DEBUG TRAIN Batch 44/3500 loss 12.358346 loss_att 17.487877 loss_ctc 22.257423 loss_rnnt 9.859062 hw_loss 0.287814 lr 0.00029861 rank 6
2023-02-27 13:24:13,150 DEBUG TRAIN Batch 44/3500 loss 3.400281 loss_att 6.127912 loss_ctc 5.692419 loss_rnnt 2.421413 hw_loss 0.239481 lr 0.00029861 rank 1
2023-02-27 13:25:18,339 DEBUG TRAIN Batch 44/3600 loss 3.628537 loss_att 5.721162 loss_ctc 8.183476 loss_rnnt 2.574774 hw_loss 0.052336 lr 0.00029859 rank 6
2023-02-27 13:25:18,339 DEBUG TRAIN Batch 44/3600 loss 11.181499 loss_att 13.237355 loss_ctc 16.221119 loss_rnnt 9.918358 hw_loss 0.337536 lr 0.00029859 rank 3
2023-02-27 13:25:18,340 DEBUG TRAIN Batch 44/3600 loss 4.966456 loss_att 7.443643 loss_ctc 7.148132 loss_rnnt 4.117435 hw_loss 0.117549 lr 0.00029859 rank 0
2023-02-27 13:25:18,341 DEBUG TRAIN Batch 44/3600 loss 6.617630 loss_att 9.447564 loss_ctc 8.112661 loss_rnnt 5.632444 hw_loss 0.412242 lr 0.00029859 rank 1
2023-02-27 13:25:18,342 DEBUG TRAIN Batch 44/3600 loss 4.712974 loss_att 7.637308 loss_ctc 8.803657 loss_rnnt 3.443469 hw_loss 0.261026 lr 0.00029859 rank 5
2023-02-27 13:25:18,343 DEBUG TRAIN Batch 44/3600 loss 5.425984 loss_att 11.868231 loss_ctc 11.639821 loss_rnnt 3.194137 hw_loss 0.215412 lr 0.00029859 rank 7
2023-02-27 13:25:18,343 DEBUG TRAIN Batch 44/3600 loss 7.322119 loss_att 12.681620 loss_ctc 15.926289 loss_rnnt 4.911441 hw_loss 0.359167 lr 0.00029859 rank 2
2023-02-27 13:25:18,389 DEBUG TRAIN Batch 44/3600 loss 7.687380 loss_att 10.888924 loss_ctc 9.888142 loss_rnnt 6.586920 hw_loss 0.312594 lr 0.00029859 rank 4
2023-02-27 13:25:57,050 DEBUG TRAIN Batch 44/3700 loss 5.024991 loss_att 8.957064 loss_ctc 8.161269 loss_rnnt 3.738408 hw_loss 0.153744 lr 0.00029858 rank 6
2023-02-27 13:25:57,065 DEBUG TRAIN Batch 44/3700 loss 7.880492 loss_att 9.531655 loss_ctc 13.805710 loss_rnnt 6.619068 hw_loss 0.264680 lr 0.00029858 rank 3
2023-02-27 13:25:57,068 DEBUG TRAIN Batch 44/3700 loss 4.538517 loss_att 7.467230 loss_ctc 8.318787 loss_rnnt 3.377512 hw_loss 0.133549 lr 0.00029858 rank 5
2023-02-27 13:25:57,069 DEBUG TRAIN Batch 44/3700 loss 4.115099 loss_att 4.631644 loss_ctc 5.964864 loss_rnnt 3.637755 hw_loss 0.238875 lr 0.00029858 rank 2
2023-02-27 13:25:57,070 DEBUG TRAIN Batch 44/3700 loss 4.396991 loss_att 6.016552 loss_ctc 7.840604 loss_rnnt 3.534389 hw_loss 0.149138 lr 0.00029858 rank 0
2023-02-27 13:25:57,072 DEBUG TRAIN Batch 44/3700 loss 6.790695 loss_att 9.157297 loss_ctc 10.785080 loss_rnnt 5.693258 hw_loss 0.171622 lr 0.00029858 rank 7
2023-02-27 13:25:57,075 DEBUG TRAIN Batch 44/3700 loss 11.041686 loss_att 14.516599 loss_ctc 16.258532 loss_rnnt 9.520629 hw_loss 0.244678 lr 0.00029858 rank 1
2023-02-27 13:25:57,119 DEBUG TRAIN Batch 44/3700 loss 4.840930 loss_att 6.958066 loss_ctc 7.311288 loss_rnnt 4.081129 hw_loss 0.013114 lr 0.00029858 rank 4
2023-02-27 13:26:36,171 DEBUG TRAIN Batch 44/3800 loss 3.544220 loss_att 6.865605 loss_ctc 8.680523 loss_rnnt 2.071664 hw_loss 0.231450 lr 0.00029857 rank 6
2023-02-27 13:26:36,181 DEBUG TRAIN Batch 44/3800 loss 9.662807 loss_att 11.033231 loss_ctc 14.074227 loss_rnnt 8.542677 hw_loss 0.483478 lr 0.00029857 rank 2
2023-02-27 13:26:36,194 DEBUG TRAIN Batch 44/3800 loss 9.920872 loss_att 13.564744 loss_ctc 19.497526 loss_rnnt 7.702831 hw_loss 0.398211 lr 0.00029857 rank 7
2023-02-27 13:26:36,199 DEBUG TRAIN Batch 44/3800 loss 8.965265 loss_att 10.499146 loss_ctc 12.848110 loss_rnnt 7.839063 hw_loss 0.565711 lr 0.00029857 rank 5
2023-02-27 13:26:36,200 DEBUG TRAIN Batch 44/3800 loss 5.674150 loss_att 6.708219 loss_ctc 10.110501 loss_rnnt 4.707068 hw_loss 0.316414 lr 0.00029857 rank 1
2023-02-27 13:26:36,203 DEBUG TRAIN Batch 44/3800 loss 9.039003 loss_att 13.451002 loss_ctc 18.125059 loss_rnnt 6.805081 hw_loss 0.262590 lr 0.00029857 rank 0
2023-02-27 13:26:36,203 DEBUG TRAIN Batch 44/3800 loss 5.279780 loss_att 8.962755 loss_ctc 12.650829 loss_rnnt 3.469820 hw_loss 0.169798 lr 0.00029857 rank 3
2023-02-27 13:26:36,248 DEBUG TRAIN Batch 44/3800 loss 5.236445 loss_att 6.323410 loss_ctc 7.905448 loss_rnnt 4.467595 hw_loss 0.366732 lr 0.00029857 rank 4
2023-02-27 13:27:37,205 DEBUG TRAIN Batch 44/3900 loss 2.385555 loss_att 4.516353 loss_ctc 4.066696 loss_rnnt 1.564373 hw_loss 0.320381 lr 0.00029855 rank 4
2023-02-27 13:27:37,206 DEBUG TRAIN Batch 44/3900 loss 7.533246 loss_att 11.277415 loss_ctc 11.508220 loss_rnnt 6.188528 hw_loss 0.123539 lr 0.00029855 rank 2
2023-02-27 13:27:37,211 DEBUG TRAIN Batch 44/3900 loss 1.523703 loss_att 4.676546 loss_ctc 5.087825 loss_rnnt 0.270903 hw_loss 0.275655 lr 0.00029855 rank 6
2023-02-27 13:27:37,217 DEBUG TRAIN Batch 44/3900 loss 6.879924 loss_att 12.015585 loss_ctc 11.504749 loss_rnnt 5.087455 hw_loss 0.278802 lr 0.00029855 rank 3
2023-02-27 13:27:37,221 DEBUG TRAIN Batch 44/3900 loss 13.289213 loss_att 16.791992 loss_ctc 20.042068 loss_rnnt 11.629812 hw_loss 0.109622 lr 0.00029855 rank 5
2023-02-27 13:27:37,262 DEBUG TRAIN Batch 44/3900 loss 6.237849 loss_att 8.861702 loss_ctc 10.539748 loss_rnnt 5.039861 hw_loss 0.186809 lr 0.00029855 rank 0
2023-02-27 13:27:37,272 DEBUG TRAIN Batch 44/3900 loss 6.902579 loss_att 8.926356 loss_ctc 7.597759 loss_rnnt 6.225538 hw_loss 0.336739 lr 0.00029855 rank 7
2023-02-27 13:27:37,293 DEBUG TRAIN Batch 44/3900 loss 5.273924 loss_att 10.409409 loss_ctc 7.666512 loss_rnnt 3.689954 hw_loss 0.445989 lr 0.00029855 rank 1
2023-02-27 13:28:21,992 DEBUG TRAIN Batch 44/4000 loss 4.887965 loss_att 7.783816 loss_ctc 8.291461 loss_rnnt 3.721224 hw_loss 0.250821 lr 0.00029854 rank 7
2023-02-27 13:28:21,993 DEBUG TRAIN Batch 44/4000 loss 3.217077 loss_att 6.102946 loss_ctc 6.491146 loss_rnnt 2.085857 hw_loss 0.220318 lr 0.00029854 rank 3
2023-02-27 13:28:21,993 DEBUG TRAIN Batch 44/4000 loss 8.154080 loss_att 11.625881 loss_ctc 12.182430 loss_rnnt 6.838603 hw_loss 0.157506 lr 0.00029854 rank 6
2023-02-27 13:28:21,993 DEBUG TRAIN Batch 44/4000 loss 10.969555 loss_att 15.023745 loss_ctc 15.037169 loss_rnnt 9.511273 hw_loss 0.197054 lr 0.00029854 rank 0
2023-02-27 13:28:21,998 DEBUG TRAIN Batch 44/4000 loss 17.349102 loss_att 19.831320 loss_ctc 27.686750 loss_rnnt 15.343009 hw_loss 0.246183 lr 0.00029854 rank 5
2023-02-27 13:28:22,002 DEBUG TRAIN Batch 44/4000 loss 2.769563 loss_att 5.362929 loss_ctc 8.428815 loss_rnnt 1.397144 hw_loss 0.185961 lr 0.00029854 rank 4
2023-02-27 13:28:22,002 DEBUG TRAIN Batch 44/4000 loss 6.705572 loss_att 9.550991 loss_ctc 10.564093 loss_rnnt 5.502769 hw_loss 0.223595 lr 0.00029854 rank 1
2023-02-27 13:28:22,005 DEBUG TRAIN Batch 44/4000 loss 5.881341 loss_att 9.167054 loss_ctc 13.490981 loss_rnnt 4.039463 hw_loss 0.318967 lr 0.00029854 rank 2
2023-02-27 13:29:00,938 DEBUG TRAIN Batch 44/4100 loss 8.948103 loss_att 11.629143 loss_ctc 15.763236 loss_rnnt 7.375820 hw_loss 0.238857 lr 0.00029853 rank 3
2023-02-27 13:29:00,940 DEBUG TRAIN Batch 44/4100 loss 8.565204 loss_att 12.441929 loss_ctc 14.918789 loss_rnnt 6.898274 hw_loss 0.083323 lr 0.00029853 rank 4
2023-02-27 13:29:00,954 DEBUG TRAIN Batch 44/4100 loss 5.823720 loss_att 10.011848 loss_ctc 9.847742 loss_rnnt 4.300061 hw_loss 0.280305 lr 0.00029853 rank 0
2023-02-27 13:29:00,958 DEBUG TRAIN Batch 44/4100 loss 6.208006 loss_att 10.064319 loss_ctc 8.891417 loss_rnnt 4.924426 hw_loss 0.289742 lr 0.00029853 rank 7
2023-02-27 13:29:00,958 DEBUG TRAIN Batch 44/4100 loss 8.535929 loss_att 12.752295 loss_ctc 14.645517 loss_rnnt 6.750373 hw_loss 0.239382 lr 0.00029853 rank 6
2023-02-27 13:29:00,960 DEBUG TRAIN Batch 44/4100 loss 3.776475 loss_att 6.892078 loss_ctc 5.786435 loss_rnnt 2.755010 hw_loss 0.244406 lr 0.00029853 rank 5
2023-02-27 13:29:00,961 DEBUG TRAIN Batch 44/4100 loss 4.709264 loss_att 8.378669 loss_ctc 6.224565 loss_rnnt 3.708653 hw_loss 0.121293 lr 0.00029853 rank 1
2023-02-27 13:29:00,965 DEBUG TRAIN Batch 44/4100 loss 5.270917 loss_att 8.239608 loss_ctc 8.685136 loss_rnnt 4.121114 hw_loss 0.189067 lr 0.00029853 rank 2
2023-02-27 13:29:40,263 DEBUG TRAIN Batch 44/4200 loss 5.998232 loss_att 8.348865 loss_ctc 10.058683 loss_rnnt 4.840308 hw_loss 0.274507 lr 0.00029851 rank 4
2023-02-27 13:29:40,271 DEBUG TRAIN Batch 44/4200 loss 4.122224 loss_att 7.195257 loss_ctc 4.469498 loss_rnnt 3.382754 hw_loss 0.147301 lr 0.00029851 rank 3
2023-02-27 13:29:40,271 DEBUG TRAIN Batch 44/4200 loss 4.694151 loss_att 8.386781 loss_ctc 7.624423 loss_rnnt 3.344639 hw_loss 0.413030 lr 0.00029851 rank 6
2023-02-27 13:29:40,273 DEBUG TRAIN Batch 44/4200 loss 6.528399 loss_att 8.630817 loss_ctc 7.773651 loss_rnnt 5.842128 hw_loss 0.187039 lr 0.00029851 rank 5
2023-02-27 13:29:40,273 DEBUG TRAIN Batch 44/4200 loss 6.461196 loss_att 10.815741 loss_ctc 12.560963 loss_rnnt 4.675239 hw_loss 0.190774 lr 0.00029851 rank 1
2023-02-27 13:29:40,275 DEBUG TRAIN Batch 44/4200 loss 7.449749 loss_att 9.669772 loss_ctc 15.119720 loss_rnnt 5.837399 hw_loss 0.273155 lr 0.00029851 rank 0
2023-02-27 13:29:40,308 DEBUG TRAIN Batch 44/4200 loss 5.147191 loss_att 8.166067 loss_ctc 5.549124 loss_rnnt 4.394062 hw_loss 0.179555 lr 0.00029851 rank 2
2023-02-27 13:29:40,313 DEBUG TRAIN Batch 44/4200 loss 1.795533 loss_att 3.929402 loss_ctc 5.275561 loss_rnnt 0.801106 hw_loss 0.194344 lr 0.00029851 rank 7
2023-02-27 13:30:45,951 DEBUG TRAIN Batch 44/4300 loss 7.733417 loss_att 9.106686 loss_ctc 11.347570 loss_rnnt 6.846828 hw_loss 0.243840 lr 0.00029850 rank 3
2023-02-27 13:30:45,952 DEBUG TRAIN Batch 44/4300 loss 18.938627 loss_att 19.302441 loss_ctc 21.485706 loss_rnnt 18.387056 hw_loss 0.260994 lr 0.00029850 rank 6
2023-02-27 13:30:45,955 DEBUG TRAIN Batch 44/4300 loss 7.353141 loss_att 9.872437 loss_ctc 10.516726 loss_rnnt 6.241857 hw_loss 0.348025 lr 0.00029850 rank 5
2023-02-27 13:30:45,955 DEBUG TRAIN Batch 44/4300 loss 6.633094 loss_att 10.334997 loss_ctc 10.463912 loss_rnnt 5.316781 hw_loss 0.122171 lr 0.00029850 rank 0
2023-02-27 13:30:45,957 DEBUG TRAIN Batch 44/4300 loss 9.862856 loss_att 9.518044 loss_ctc 17.895046 loss_rnnt 8.630560 hw_loss 0.431813 lr 0.00029850 rank 1
2023-02-27 13:30:45,958 DEBUG TRAIN Batch 44/4300 loss 6.250689 loss_att 9.641984 loss_ctc 10.706499 loss_rnnt 4.828783 hw_loss 0.280384 lr 0.00029850 rank 4
2023-02-27 13:30:45,959 DEBUG TRAIN Batch 44/4300 loss 6.809263 loss_att 9.625969 loss_ctc 13.738302 loss_rnnt 5.198647 hw_loss 0.231381 lr 0.00029850 rank 2
2023-02-27 13:30:45,961 DEBUG TRAIN Batch 44/4300 loss 5.923748 loss_att 8.544274 loss_ctc 9.483915 loss_rnnt 4.799040 hw_loss 0.236086 lr 0.00029850 rank 7
2023-02-27 13:31:25,164 DEBUG TRAIN Batch 44/4400 loss 9.083385 loss_att 14.253283 loss_ctc 23.252815 loss_rnnt 6.029464 hw_loss 0.245036 lr 0.00029849 rank 6
2023-02-27 13:31:25,170 DEBUG TRAIN Batch 44/4400 loss 8.996079 loss_att 15.434397 loss_ctc 24.752480 loss_rnnt 5.396230 hw_loss 0.396247 lr 0.00029849 rank 3
2023-02-27 13:31:25,182 DEBUG TRAIN Batch 44/4400 loss 4.399961 loss_att 5.994840 loss_ctc 4.954011 loss_rnnt 3.926208 hw_loss 0.151695 lr 0.00029849 rank 7
2023-02-27 13:31:25,182 DEBUG TRAIN Batch 44/4400 loss 3.222930 loss_att 4.871830 loss_ctc 7.154099 loss_rnnt 2.183632 hw_loss 0.347556 lr 0.00029849 rank 2
2023-02-27 13:31:25,185 DEBUG TRAIN Batch 44/4400 loss 7.658472 loss_att 9.580704 loss_ctc 11.700950 loss_rnnt 6.582997 hw_loss 0.285057 lr 0.00029849 rank 4
2023-02-27 13:31:25,187 DEBUG TRAIN Batch 44/4400 loss 17.900589 loss_att 20.442482 loss_ctc 23.734818 loss_rnnt 16.524954 hw_loss 0.167552 lr 0.00029849 rank 1
2023-02-27 13:31:25,189 DEBUG TRAIN Batch 44/4400 loss 8.710874 loss_att 12.027988 loss_ctc 15.666277 loss_rnnt 7.060223 hw_loss 0.112201 lr 0.00029849 rank 0
2023-02-27 13:31:25,234 DEBUG TRAIN Batch 44/4400 loss 8.806457 loss_att 11.207774 loss_ctc 12.418511 loss_rnnt 7.701180 hw_loss 0.268886 lr 0.00029849 rank 5
2023-02-27 13:32:04,009 DEBUG TRAIN Batch 44/4500 loss 3.207575 loss_att 4.649195 loss_ctc 3.350775 loss_rnnt 2.837719 hw_loss 0.117071 lr 0.00029847 rank 6
2023-02-27 13:32:04,015 DEBUG TRAIN Batch 44/4500 loss 13.300751 loss_att 15.016040 loss_ctc 24.347778 loss_rnnt 11.438591 hw_loss 0.086559 lr 0.00029847 rank 1
2023-02-27 13:32:04,022 DEBUG TRAIN Batch 44/4500 loss 16.551378 loss_att 19.162941 loss_ctc 31.535599 loss_rnnt 13.842181 hw_loss 0.354353 lr 0.00029847 rank 3
2023-02-27 13:32:04,027 DEBUG TRAIN Batch 44/4500 loss 7.792932 loss_att 7.194464 loss_ctc 10.941923 loss_rnnt 7.234837 hw_loss 0.483604 lr 0.00029847 rank 0
2023-02-27 13:32:04,029 DEBUG TRAIN Batch 44/4500 loss 6.868027 loss_att 9.222220 loss_ctc 10.874881 loss_rnnt 5.771144 hw_loss 0.172119 lr 0.00029847 rank 4
2023-02-27 13:32:04,029 DEBUG TRAIN Batch 44/4500 loss 7.370123 loss_att 12.624554 loss_ctc 13.251826 loss_rnnt 5.457654 hw_loss 0.145042 lr 0.00029847 rank 2
2023-02-27 13:32:04,032 DEBUG TRAIN Batch 44/4500 loss 7.367021 loss_att 7.979324 loss_ctc 12.221933 loss_rnnt 6.445303 hw_loss 0.284879 lr 0.00029847 rank 7
2023-02-27 13:32:04,075 DEBUG TRAIN Batch 44/4500 loss 6.301806 loss_att 9.240379 loss_ctc 14.817623 loss_rnnt 4.414452 hw_loss 0.307870 lr 0.00029847 rank 5
2023-02-27 13:32:43,881 DEBUG TRAIN Batch 44/4600 loss 11.601408 loss_att 13.856153 loss_ctc 15.945666 loss_rnnt 10.443215 hw_loss 0.240016 lr 0.00029846 rank 5
2023-02-27 13:32:43,883 DEBUG TRAIN Batch 44/4600 loss 5.197110 loss_att 7.534350 loss_ctc 8.052579 loss_rnnt 4.242422 hw_loss 0.199709 lr 0.00029846 rank 1
2023-02-27 13:32:43,885 DEBUG TRAIN Batch 44/4600 loss 5.873957 loss_att 7.941009 loss_ctc 8.943729 loss_rnnt 4.961936 hw_loss 0.167452 lr 0.00029846 rank 7
2023-02-27 13:32:43,888 DEBUG TRAIN Batch 44/4600 loss 3.735444 loss_att 9.116993 loss_ctc 6.819115 loss_rnnt 2.080911 hw_loss 0.313250 lr 0.00029846 rank 4
2023-02-27 13:32:43,891 DEBUG TRAIN Batch 44/4600 loss 10.254159 loss_att 14.290607 loss_ctc 12.405177 loss_rnnt 9.085960 hw_loss 0.138950 lr 0.00029846 rank 6
2023-02-27 13:32:43,897 DEBUG TRAIN Batch 44/4600 loss 5.177938 loss_att 9.712594 loss_ctc 6.832284 loss_rnnt 3.855592 hw_loss 0.365316 lr 0.00029846 rank 0
2023-02-27 13:32:43,899 DEBUG TRAIN Batch 44/4600 loss 6.072508 loss_att 7.921157 loss_ctc 12.669456 loss_rnnt 4.674172 hw_loss 0.279399 lr 0.00029846 rank 3
2023-02-27 13:32:43,945 DEBUG TRAIN Batch 44/4600 loss 7.031941 loss_att 10.695770 loss_ctc 11.879032 loss_rnnt 5.454732 hw_loss 0.371558 lr 0.00029846 rank 2
2023-02-27 13:33:50,303 DEBUG TRAIN Batch 44/4700 loss 15.080357 loss_att 18.705715 loss_ctc 23.079285 loss_rnnt 13.192729 hw_loss 0.180061 lr 0.00029845 rank 2
2023-02-27 13:33:50,303 DEBUG TRAIN Batch 44/4700 loss 10.627629 loss_att 12.864228 loss_ctc 22.081091 loss_rnnt 8.532707 hw_loss 0.225889 lr 0.00029845 rank 7
2023-02-27 13:33:50,303 DEBUG TRAIN Batch 44/4700 loss 1.418490 loss_att 4.345799 loss_ctc 3.073464 loss_rnnt 0.555910 hw_loss 0.105854 lr 0.00029845 rank 4
2023-02-27 13:33:50,305 DEBUG TRAIN Batch 44/4700 loss 7.092495 loss_att 8.991817 loss_ctc 11.507218 loss_rnnt 5.943805 hw_loss 0.337867 lr 0.00029845 rank 6
2023-02-27 13:33:50,306 DEBUG TRAIN Batch 44/4700 loss 5.950166 loss_att 8.364655 loss_ctc 10.425352 loss_rnnt 4.695408 hw_loss 0.328441 lr 0.00029845 rank 1
2023-02-27 13:33:50,307 DEBUG TRAIN Batch 44/4700 loss 3.579864 loss_att 5.430606 loss_ctc 3.567104 loss_rnnt 3.083378 hw_loss 0.240072 lr 0.00029845 rank 3
2023-02-27 13:33:50,307 DEBUG TRAIN Batch 44/4700 loss 4.211289 loss_att 6.782879 loss_ctc 6.814233 loss_rnnt 3.131848 hw_loss 0.408871 lr 0.00029845 rank 0
2023-02-27 13:33:50,310 DEBUG TRAIN Batch 44/4700 loss 7.141264 loss_att 8.851438 loss_ctc 13.994220 loss_rnnt 5.819149 hw_loss 0.124412 lr 0.00029845 rank 5
2023-02-27 13:34:29,065 DEBUG TRAIN Batch 44/4800 loss 11.929631 loss_att 16.994184 loss_ctc 18.626152 loss_rnnt 9.952497 hw_loss 0.133789 lr 0.00029843 rank 6
2023-02-27 13:34:29,076 DEBUG TRAIN Batch 44/4800 loss 7.171597 loss_att 8.428547 loss_ctc 8.671507 loss_rnnt 6.548130 hw_loss 0.322667 lr 0.00029843 rank 2
2023-02-27 13:34:29,076 DEBUG TRAIN Batch 44/4800 loss 3.695159 loss_att 6.332039 loss_ctc 8.101772 loss_rnnt 2.480692 hw_loss 0.186642 lr 0.00029843 rank 7
2023-02-27 13:34:29,077 DEBUG TRAIN Batch 44/4800 loss 3.519962 loss_att 6.774451 loss_ctc 6.259418 loss_rnnt 2.385931 hw_loss 0.221010 lr 0.00029843 rank 0
2023-02-27 13:34:29,078 DEBUG TRAIN Batch 44/4800 loss 9.082195 loss_att 11.389862 loss_ctc 16.375071 loss_rnnt 7.483004 hw_loss 0.309891 lr 0.00029843 rank 3
2023-02-27 13:34:29,078 DEBUG TRAIN Batch 44/4800 loss 9.848133 loss_att 12.418280 loss_ctc 16.980040 loss_rnnt 8.298162 hw_loss 0.159414 lr 0.00029843 rank 5
2023-02-27 13:34:29,079 DEBUG TRAIN Batch 44/4800 loss 8.225693 loss_att 11.550931 loss_ctc 17.264023 loss_rnnt 6.242106 hw_loss 0.212678 lr 0.00029843 rank 4
2023-02-27 13:34:29,086 DEBUG TRAIN Batch 44/4800 loss 5.843580 loss_att 7.721084 loss_ctc 13.633768 loss_rnnt 4.295360 hw_loss 0.251302 lr 0.00029843 rank 1
2023-02-27 13:35:08,116 DEBUG TRAIN Batch 44/4900 loss 16.458231 loss_att 21.609882 loss_ctc 26.442612 loss_rnnt 14.011586 hw_loss 0.159493 lr 0.00029842 rank 7
2023-02-27 13:35:08,132 DEBUG TRAIN Batch 44/4900 loss 8.830874 loss_att 11.046179 loss_ctc 13.990707 loss_rnnt 7.547691 hw_loss 0.285271 lr 0.00029842 rank 0
2023-02-27 13:35:08,132 DEBUG TRAIN Batch 44/4900 loss 7.052715 loss_att 11.595930 loss_ctc 8.707364 loss_rnnt 5.825003 hw_loss 0.184591 lr 0.00029842 rank 6
2023-02-27 13:35:08,132 DEBUG TRAIN Batch 44/4900 loss 10.791434 loss_att 11.509482 loss_ctc 15.892269 loss_rnnt 9.885151 hw_loss 0.154806 lr 0.00029842 rank 5
2023-02-27 13:35:08,135 DEBUG TRAIN Batch 44/4900 loss 7.883613 loss_att 11.134550 loss_ctc 11.848778 loss_rnnt 6.462304 hw_loss 0.454560 lr 0.00029842 rank 1
2023-02-27 13:35:08,136 DEBUG TRAIN Batch 44/4900 loss 3.639938 loss_att 7.355151 loss_ctc 7.200073 loss_rnnt 2.271979 hw_loss 0.281684 lr 0.00029842 rank 4
2023-02-27 13:35:08,137 DEBUG TRAIN Batch 44/4900 loss 5.669898 loss_att 7.097386 loss_ctc 9.191818 loss_rnnt 4.756288 hw_loss 0.297230 lr 0.00029842 rank 3
2023-02-27 13:35:08,148 DEBUG TRAIN Batch 44/4900 loss 5.580572 loss_att 8.956263 loss_ctc 8.129782 loss_rnnt 4.452848 hw_loss 0.211294 lr 0.00029842 rank 2
2023-02-27 13:36:13,748 DEBUG TRAIN Batch 44/5000 loss 4.040758 loss_att 5.304242 loss_ctc 7.176356 loss_rnnt 3.142096 hw_loss 0.427285 lr 0.00029841 rank 3
2023-02-27 13:36:13,751 DEBUG TRAIN Batch 44/5000 loss 3.685455 loss_att 7.437641 loss_ctc 7.118801 loss_rnnt 2.303503 hw_loss 0.325754 lr 0.00029841 rank 6
2023-02-27 13:36:13,752 DEBUG TRAIN Batch 44/5000 loss 4.401631 loss_att 6.206933 loss_ctc 6.534363 loss_rnnt 3.560297 hw_loss 0.367330 lr 0.00029841 rank 0
2023-02-27 13:36:13,756 DEBUG TRAIN Batch 44/5000 loss 3.797315 loss_att 5.741317 loss_ctc 7.701944 loss_rnnt 2.668089 hw_loss 0.412140 lr 0.00029841 rank 4
2023-02-27 13:36:13,758 DEBUG TRAIN Batch 44/5000 loss 7.277411 loss_att 9.161190 loss_ctc 9.547427 loss_rnnt 6.473359 hw_loss 0.233676 lr 0.00029841 rank 5
2023-02-27 13:36:13,758 DEBUG TRAIN Batch 44/5000 loss 10.260770 loss_att 12.319633 loss_ctc 14.984293 loss_rnnt 9.058321 hw_loss 0.301639 lr 0.00029841 rank 1
2023-02-27 13:36:13,762 DEBUG TRAIN Batch 44/5000 loss 6.007876 loss_att 8.421881 loss_ctc 10.171063 loss_rnnt 4.779949 hw_loss 0.356313 lr 0.00029841 rank 7
2023-02-27 13:36:13,765 DEBUG TRAIN Batch 44/5000 loss 8.947694 loss_att 11.611425 loss_ctc 15.307962 loss_rnnt 7.379345 hw_loss 0.351690 lr 0.00029841 rank 2
2023-02-27 13:36:52,885 DEBUG TRAIN Batch 44/5100 loss 5.741955 loss_att 8.431171 loss_ctc 9.299583 loss_rnnt 4.645106 hw_loss 0.158728 lr 0.00029840 rank 3
2023-02-27 13:36:52,898 DEBUG TRAIN Batch 44/5100 loss 4.460252 loss_att 8.680924 loss_ctc 7.435426 loss_rnnt 3.024296 hw_loss 0.365871 lr 0.00029840 rank 6
2023-02-27 13:36:52,904 DEBUG TRAIN Batch 44/5100 loss 2.587435 loss_att 6.276922 loss_ctc 5.206517 loss_rnnt 1.432270 hw_loss 0.127607 lr 0.00029840 rank 4
2023-02-27 13:36:52,905 DEBUG TRAIN Batch 44/5100 loss 5.643850 loss_att 7.326448 loss_ctc 9.040193 loss_rnnt 4.657234 hw_loss 0.369843 lr 0.00029840 rank 0
2023-02-27 13:36:52,905 DEBUG TRAIN Batch 44/5100 loss 3.805449 loss_att 4.628577 loss_ctc 6.456412 loss_rnnt 3.162213 hw_loss 0.234654 lr 0.00029840 rank 1
2023-02-27 13:36:52,908 DEBUG TRAIN Batch 44/5100 loss 9.730132 loss_att 16.319084 loss_ctc 12.068230 loss_rnnt 8.010509 hw_loss 0.168913 lr 0.00029840 rank 2
2023-02-27 13:36:52,909 DEBUG TRAIN Batch 44/5100 loss 7.183177 loss_att 8.406611 loss_ctc 12.307809 loss_rnnt 6.079761 hw_loss 0.328959 lr 0.00029840 rank 5
2023-02-27 13:36:52,910 DEBUG TRAIN Batch 44/5100 loss 8.316070 loss_att 10.306122 loss_ctc 11.489742 loss_rnnt 7.349549 hw_loss 0.272538 lr 0.00029840 rank 7
2023-02-27 13:37:31,517 DEBUG TRAIN Batch 44/5200 loss 3.842799 loss_att 6.325905 loss_ctc 8.184049 loss_rnnt 2.601973 hw_loss 0.310071 lr 0.00029838 rank 1
2023-02-27 13:37:31,518 DEBUG TRAIN Batch 44/5200 loss 6.800270 loss_att 9.477335 loss_ctc 12.254405 loss_rnnt 5.435509 hw_loss 0.191494 lr 0.00029838 rank 5
2023-02-27 13:37:31,521 DEBUG TRAIN Batch 44/5200 loss 3.005802 loss_att 7.075721 loss_ctc 5.503911 loss_rnnt 1.741148 hw_loss 0.220478 lr 0.00029838 rank 6
2023-02-27 13:37:31,523 DEBUG TRAIN Batch 44/5200 loss 3.563153 loss_att 7.366772 loss_ctc 6.609651 loss_rnnt 2.290132 hw_loss 0.198932 lr 0.00029838 rank 0
2023-02-27 13:37:31,523 DEBUG TRAIN Batch 44/5200 loss 17.618269 loss_att 19.800045 loss_ctc 28.594467 loss_rnnt 15.619884 hw_loss 0.184758 lr 0.00029838 rank 3
2023-02-27 13:37:31,523 DEBUG TRAIN Batch 44/5200 loss 6.063459 loss_att 10.251806 loss_ctc 10.619739 loss_rnnt 4.582574 hw_loss 0.066959 lr 0.00029838 rank 2
2023-02-27 13:37:31,528 DEBUG TRAIN Batch 44/5200 loss 5.359784 loss_att 6.111384 loss_ctc 6.754400 loss_rnnt 4.871648 hw_loss 0.284751 lr 0.00029838 rank 7
2023-02-27 13:37:31,531 DEBUG TRAIN Batch 44/5200 loss 8.646158 loss_att 13.567334 loss_ctc 15.240845 loss_rnnt 6.537231 hw_loss 0.460124 lr 0.00029838 rank 4
2023-02-27 13:38:11,822 DEBUG TRAIN Batch 44/5300 loss 7.332551 loss_att 9.466093 loss_ctc 11.963961 loss_rnnt 6.157320 hw_loss 0.245625 lr 0.00029837 rank 6
2023-02-27 13:38:11,823 DEBUG TRAIN Batch 44/5300 loss 6.482872 loss_att 9.306775 loss_ctc 13.380831 loss_rnnt 4.911826 hw_loss 0.162260 lr 0.00029837 rank 0
2023-02-27 13:38:11,840 DEBUG TRAIN Batch 44/5300 loss 3.651824 loss_att 6.292448 loss_ctc 8.831985 loss_rnnt 2.225103 hw_loss 0.389828 lr 0.00029837 rank 3
2023-02-27 13:38:11,840 DEBUG TRAIN Batch 44/5300 loss 3.854404 loss_att 5.504390 loss_ctc 5.859058 loss_rnnt 3.063488 hw_loss 0.363060 lr 0.00029837 rank 5
2023-02-27 13:38:11,842 DEBUG TRAIN Batch 44/5300 loss 7.388374 loss_att 10.548335 loss_ctc 16.381521 loss_rnnt 5.363071 hw_loss 0.364173 lr 0.00029837 rank 1
2023-02-27 13:38:11,842 DEBUG TRAIN Batch 44/5300 loss 13.854714 loss_att 17.993622 loss_ctc 23.181438 loss_rnnt 11.722046 hw_loss 0.114983 lr 0.00029837 rank 2
2023-02-27 13:38:11,844 DEBUG TRAIN Batch 44/5300 loss 4.229918 loss_att 7.099269 loss_ctc 7.547011 loss_rnnt 3.105756 hw_loss 0.202526 lr 0.00029837 rank 4
2023-02-27 13:38:11,856 DEBUG TRAIN Batch 44/5300 loss 1.991943 loss_att 4.678777 loss_ctc 2.278488 loss_rnnt 1.221627 hw_loss 0.365143 lr 0.00029837 rank 7
2023-02-27 13:39:17,104 DEBUG TRAIN Batch 44/5400 loss 13.330585 loss_att 17.417475 loss_ctc 22.835285 loss_rnnt 11.151853 hw_loss 0.176364 lr 0.00029836 rank 6
2023-02-27 13:39:17,105 DEBUG TRAIN Batch 44/5400 loss 7.702515 loss_att 9.838601 loss_ctc 12.916289 loss_rnnt 6.443656 hw_loss 0.255885 lr 0.00029836 rank 3
2023-02-27 13:39:17,105 DEBUG TRAIN Batch 44/5400 loss 3.299461 loss_att 5.913749 loss_ctc 6.421213 loss_rnnt 2.222917 hw_loss 0.257726 lr 0.00029836 rank 0
2023-02-27 13:39:17,104 DEBUG TRAIN Batch 44/5400 loss 1.560330 loss_att 4.035662 loss_ctc 2.238408 loss_rnnt 0.818585 hw_loss 0.293003 lr 0.00029836 rank 2
2023-02-27 13:39:17,111 DEBUG TRAIN Batch 44/5400 loss 9.698923 loss_att 13.142913 loss_ctc 17.338282 loss_rnnt 7.856163 hw_loss 0.253839 lr 0.00029836 rank 4
2023-02-27 13:39:17,112 DEBUG TRAIN Batch 44/5400 loss 7.046272 loss_att 8.464614 loss_ctc 13.373189 loss_rnnt 5.770151 hw_loss 0.279119 lr 0.00029836 rank 7
2023-02-27 13:39:17,111 DEBUG TRAIN Batch 44/5400 loss 9.437840 loss_att 11.644136 loss_ctc 14.427695 loss_rnnt 8.102201 hw_loss 0.429498 lr 0.00029836 rank 5
2023-02-27 13:39:17,114 DEBUG TRAIN Batch 44/5400 loss 4.066484 loss_att 6.472739 loss_ctc 12.086071 loss_rnnt 2.396284 hw_loss 0.224384 lr 0.00029836 rank 1
2023-02-27 13:39:56,099 DEBUG TRAIN Batch 44/5500 loss 9.045589 loss_att 9.916057 loss_ctc 12.462576 loss_rnnt 8.309093 hw_loss 0.200256 lr 0.00029834 rank 1
2023-02-27 13:39:56,110 DEBUG TRAIN Batch 44/5500 loss 4.823670 loss_att 7.570154 loss_ctc 5.477664 loss_rnnt 3.920605 hw_loss 0.499816 lr 0.00029834 rank 3
2023-02-27 13:39:56,112 DEBUG TRAIN Batch 44/5500 loss 5.261343 loss_att 9.227455 loss_ctc 9.300362 loss_rnnt 3.821781 hw_loss 0.202131 lr 0.00029834 rank 2
2023-02-27 13:39:56,113 DEBUG TRAIN Batch 44/5500 loss 8.466408 loss_att 9.889259 loss_ctc 12.875899 loss_rnnt 7.491576 hw_loss 0.191866 lr 0.00029834 rank 6
2023-02-27 13:39:56,113 DEBUG TRAIN Batch 44/5500 loss 5.700372 loss_att 6.233132 loss_ctc 8.229708 loss_rnnt 5.150546 hw_loss 0.198805 lr 0.00029834 rank 5
2023-02-27 13:39:56,116 DEBUG TRAIN Batch 44/5500 loss 5.943388 loss_att 8.278161 loss_ctc 9.317316 loss_rnnt 4.910985 hw_loss 0.216733 lr 0.00029834 rank 0
2023-02-27 13:39:56,116 DEBUG TRAIN Batch 44/5500 loss 5.318433 loss_att 8.573812 loss_ctc 7.550107 loss_rnnt 4.215609 hw_loss 0.289109 lr 0.00029834 rank 4
2023-02-27 13:39:56,161 DEBUG TRAIN Batch 44/5500 loss 4.891922 loss_att 7.551749 loss_ctc 7.795556 loss_rnnt 3.829897 hw_loss 0.267955 lr 0.00029834 rank 7
2023-02-27 13:40:35,577 DEBUG TRAIN Batch 44/5600 loss 2.713677 loss_att 5.475429 loss_ctc 4.424478 loss_rnnt 1.779600 hw_loss 0.288038 lr 0.00029833 rank 3
2023-02-27 13:40:35,578 DEBUG TRAIN Batch 44/5600 loss 4.066667 loss_att 6.368231 loss_ctc 8.125900 loss_rnnt 2.882093 hw_loss 0.343181 lr 0.00029833 rank 0
2023-02-27 13:40:35,579 DEBUG TRAIN Batch 44/5600 loss 7.951462 loss_att 9.087975 loss_ctc 13.716888 loss_rnnt 6.837317 hw_loss 0.221475 lr 0.00029833 rank 6
2023-02-27 13:40:35,601 DEBUG TRAIN Batch 44/5600 loss 3.801118 loss_att 5.450906 loss_ctc 6.213988 loss_rnnt 3.107364 hw_loss 0.078900 lr 0.00029833 rank 4
2023-02-27 13:40:35,603 DEBUG TRAIN Batch 44/5600 loss 8.067470 loss_att 10.719667 loss_ctc 14.728370 loss_rnnt 6.452348 hw_loss 0.368553 lr 0.00029833 rank 7
2023-02-27 13:40:35,625 DEBUG TRAIN Batch 44/5600 loss 10.587767 loss_att 17.347782 loss_ctc 15.505137 loss_rnnt 8.460158 hw_loss 0.224917 lr 0.00029833 rank 2
2023-02-27 13:40:35,632 DEBUG TRAIN Batch 44/5600 loss 2.930389 loss_att 6.031775 loss_ctc 4.257916 loss_rnnt 1.998652 hw_loss 0.252106 lr 0.00029833 rank 5
2023-02-27 13:40:35,634 DEBUG TRAIN Batch 44/5600 loss 7.398917 loss_att 8.558443 loss_ctc 10.841475 loss_rnnt 6.588713 hw_loss 0.223673 lr 0.00029833 rank 1
2023-02-27 13:41:42,482 DEBUG TRAIN Batch 44/5700 loss 7.918683 loss_att 9.273722 loss_ctc 13.805179 loss_rnnt 6.754128 hw_loss 0.203778 lr 0.00029832 rank 0
2023-02-27 13:41:42,495 DEBUG TRAIN Batch 44/5700 loss 2.978474 loss_att 5.582733 loss_ctc 7.277954 loss_rnnt 1.828753 hw_loss 0.104260 lr 0.00029832 rank 6
2023-02-27 13:41:42,505 DEBUG TRAIN Batch 44/5700 loss 5.306951 loss_att 5.508462 loss_ctc 7.755566 loss_rnnt 4.693015 hw_loss 0.463409 lr 0.00029832 rank 4
2023-02-27 13:41:42,505 DEBUG TRAIN Batch 44/5700 loss 5.360946 loss_att 5.719266 loss_ctc 6.074225 loss_rnnt 5.051402 hw_loss 0.267705 lr 0.00029832 rank 3
2023-02-27 13:41:42,509 DEBUG TRAIN Batch 44/5700 loss 7.217659 loss_att 8.591128 loss_ctc 10.293653 loss_rnnt 6.359077 hw_loss 0.325789 lr 0.00029832 rank 7
2023-02-27 13:41:42,509 DEBUG TRAIN Batch 44/5700 loss 3.042243 loss_att 4.557859 loss_ctc 4.771998 loss_rnnt 2.332213 hw_loss 0.330511 lr 0.00029832 rank 2
2023-02-27 13:41:42,513 DEBUG TRAIN Batch 44/5700 loss 8.191614 loss_att 8.239844 loss_ctc 11.301790 loss_rnnt 7.584091 hw_loss 0.343475 lr 0.00029832 rank 5
2023-02-27 13:41:42,514 DEBUG TRAIN Batch 44/5700 loss 14.029006 loss_att 15.334910 loss_ctc 19.932371 loss_rnnt 12.811955 hw_loss 0.316413 lr 0.00029832 rank 1
2023-02-27 13:42:21,249 DEBUG TRAIN Batch 44/5800 loss 7.232038 loss_att 10.176140 loss_ctc 9.736708 loss_rnnt 6.189760 hw_loss 0.224066 lr 0.00029830 rank 0
2023-02-27 13:42:21,260 DEBUG TRAIN Batch 44/5800 loss 2.411398 loss_att 4.287177 loss_ctc 3.853961 loss_rnnt 1.770724 hw_loss 0.137205 lr 0.00029830 rank 5
2023-02-27 13:42:21,271 DEBUG TRAIN Batch 44/5800 loss 6.818287 loss_att 11.479949 loss_ctc 11.343337 loss_rnnt 5.123859 hw_loss 0.297667 lr 0.00029830 rank 6
2023-02-27 13:42:21,271 DEBUG TRAIN Batch 44/5800 loss 6.113349 loss_att 8.938600 loss_ctc 7.620273 loss_rnnt 5.238944 hw_loss 0.203310 lr 0.00029830 rank 1
2023-02-27 13:42:21,274 DEBUG TRAIN Batch 44/5800 loss 3.641878 loss_att 8.598607 loss_ctc 7.360868 loss_rnnt 2.125016 hw_loss 0.055596 lr 0.00029830 rank 7
2023-02-27 13:42:21,274 DEBUG TRAIN Batch 44/5800 loss 8.912328 loss_att 9.644629 loss_ctc 13.100220 loss_rnnt 8.202904 hw_loss 0.008585 lr 0.00029830 rank 3
2023-02-27 13:42:21,275 DEBUG TRAIN Batch 44/5800 loss 6.533876 loss_att 9.471893 loss_ctc 10.960606 loss_rnnt 5.263393 hw_loss 0.173718 lr 0.00029830 rank 4
2023-02-27 13:42:21,279 DEBUG TRAIN Batch 44/5800 loss 7.933301 loss_att 13.128433 loss_ctc 14.905151 loss_rnnt 5.891471 hw_loss 0.137295 lr 0.00029830 rank 2
2023-02-27 13:43:00,181 DEBUG TRAIN Batch 44/5900 loss 5.939460 loss_att 11.545115 loss_ctc 10.106823 loss_rnnt 4.087417 hw_loss 0.328620 lr 0.00029829 rank 0
2023-02-27 13:43:00,182 DEBUG TRAIN Batch 44/5900 loss 3.908457 loss_att 8.851883 loss_ctc 8.740107 loss_rnnt 2.073974 hw_loss 0.377959 lr 0.00029829 rank 6
2023-02-27 13:43:00,184 DEBUG TRAIN Batch 44/5900 loss 6.861396 loss_att 7.386836 loss_ctc 7.494786 loss_rnnt 6.615892 hw_loss 0.104933 lr 0.00029829 rank 5
2023-02-27 13:43:00,185 DEBUG TRAIN Batch 44/5900 loss 8.231701 loss_att 12.417230 loss_ctc 12.068106 loss_rnnt 6.741182 hw_loss 0.266049 lr 0.00029829 rank 3
2023-02-27 13:43:00,186 DEBUG TRAIN Batch 44/5900 loss 5.624875 loss_att 9.353816 loss_ctc 9.700190 loss_rnnt 4.198174 hw_loss 0.257884 lr 0.00029829 rank 1
2023-02-27 13:43:00,187 DEBUG TRAIN Batch 44/5900 loss 3.768544 loss_att 7.191812 loss_ctc 5.176572 loss_rnnt 2.689204 hw_loss 0.388030 lr 0.00029829 rank 7
2023-02-27 13:43:00,190 DEBUG TRAIN Batch 44/5900 loss 17.603088 loss_att 19.285198 loss_ctc 27.358044 loss_rnnt 15.864271 hw_loss 0.190755 lr 0.00029829 rank 2
2023-02-27 13:43:00,232 DEBUG TRAIN Batch 44/5900 loss 7.416295 loss_att 11.317101 loss_ctc 15.567513 loss_rnnt 5.454945 hw_loss 0.176924 lr 0.00029829 rank 4
2023-02-27 13:43:39,863 DEBUG TRAIN Batch 44/6000 loss 2.730880 loss_att 6.435647 loss_ctc 7.722212 loss_rnnt 1.165942 hw_loss 0.297139 lr 0.00029828 rank 4
2023-02-27 13:43:39,864 DEBUG TRAIN Batch 44/6000 loss 6.022676 loss_att 8.524654 loss_ctc 9.247271 loss_rnnt 4.858384 hw_loss 0.438657 lr 0.00029828 rank 2
2023-02-27 13:43:39,876 DEBUG TRAIN Batch 44/6000 loss 3.713363 loss_att 7.562076 loss_ctc 7.145723 loss_rnnt 2.310309 hw_loss 0.329369 lr 0.00029828 rank 6
2023-02-27 13:43:39,876 DEBUG TRAIN Batch 44/6000 loss 7.828608 loss_att 10.257658 loss_ctc 13.323983 loss_rnnt 6.559222 hw_loss 0.095362 lr 0.00029828 rank 3
2023-02-27 13:43:39,877 DEBUG TRAIN Batch 44/6000 loss 2.766169 loss_att 6.759856 loss_ctc 8.554567 loss_rnnt 1.099604 hw_loss 0.180078 lr 0.00029828 rank 5
2023-02-27 13:43:39,879 DEBUG TRAIN Batch 44/6000 loss 5.625308 loss_att 8.334463 loss_ctc 8.494476 loss_rnnt 4.563299 hw_loss 0.258042 lr 0.00029828 rank 0
2023-02-27 13:43:39,882 DEBUG TRAIN Batch 44/6000 loss 6.904965 loss_att 8.222045 loss_ctc 10.467924 loss_rnnt 6.038885 hw_loss 0.239254 lr 0.00029828 rank 1
2023-02-27 13:43:39,927 DEBUG TRAIN Batch 44/6000 loss 2.692359 loss_att 5.574733 loss_ctc 9.299551 loss_rnnt 1.163549 hw_loss 0.133830 lr 0.00029828 rank 7
2023-02-27 13:44:46,203 DEBUG TRAIN Batch 44/6100 loss 6.570163 loss_att 10.176346 loss_ctc 11.700238 loss_rnnt 5.047432 hw_loss 0.220281 lr 0.00029826 rank 5
2023-02-27 13:44:46,205 DEBUG TRAIN Batch 44/6100 loss 7.616849 loss_att 9.911220 loss_ctc 13.608120 loss_rnnt 6.172054 hw_loss 0.350784 lr 0.00029826 rank 4
2023-02-27 13:44:46,208 DEBUG TRAIN Batch 44/6100 loss 6.024566 loss_att 9.366813 loss_ctc 9.789970 loss_rnnt 4.708565 hw_loss 0.272805 lr 0.00029826 rank 2
2023-02-27 13:44:46,210 DEBUG TRAIN Batch 44/6100 loss 8.605567 loss_att 13.878749 loss_ctc 13.268746 loss_rnnt 6.771194 hw_loss 0.296214 lr 0.00029826 rank 1
2023-02-27 13:44:46,219 DEBUG TRAIN Batch 44/6100 loss 6.930555 loss_att 9.852795 loss_ctc 13.426137 loss_rnnt 5.413840 hw_loss 0.124106 lr 0.00029826 rank 6
2023-02-27 13:44:46,223 DEBUG TRAIN Batch 44/6100 loss 6.408786 loss_att 10.734358 loss_ctc 12.436132 loss_rnnt 4.577004 hw_loss 0.305664 lr 0.00029826 rank 0
2023-02-27 13:44:46,225 DEBUG TRAIN Batch 44/6100 loss 9.880630 loss_att 12.221298 loss_ctc 18.028582 loss_rnnt 8.229001 hw_loss 0.182065 lr 0.00029826 rank 7
2023-02-27 13:44:46,227 DEBUG TRAIN Batch 44/6100 loss 8.839034 loss_att 10.577261 loss_ctc 13.356143 loss_rnnt 7.782760 hw_loss 0.199403 lr 0.00029826 rank 3
2023-02-27 13:45:25,162 DEBUG TRAIN Batch 44/6200 loss 8.965319 loss_att 12.937527 loss_ctc 15.311439 loss_rnnt 7.221447 hw_loss 0.193650 lr 0.00029825 rank 3
2023-02-27 13:45:25,175 DEBUG TRAIN Batch 44/6200 loss 2.685271 loss_att 4.392020 loss_ctc 3.982020 loss_rnnt 2.070401 hw_loss 0.188663 lr 0.00029825 rank 1
2023-02-27 13:45:25,176 DEBUG TRAIN Batch 44/6200 loss 3.960500 loss_att 5.210427 loss_ctc 6.205680 loss_rnnt 3.303340 hw_loss 0.202156 lr 0.00029825 rank 2
2023-02-27 13:45:25,178 DEBUG TRAIN Batch 44/6200 loss 5.608365 loss_att 10.022012 loss_ctc 9.906172 loss_rnnt 4.007635 hw_loss 0.271799 lr 0.00029825 rank 0
2023-02-27 13:45:25,178 DEBUG TRAIN Batch 44/6200 loss 5.691914 loss_att 7.723960 loss_ctc 13.240303 loss_rnnt 4.138637 hw_loss 0.263279 lr 0.00029825 rank 7
2023-02-27 13:45:25,178 DEBUG TRAIN Batch 44/6200 loss 9.829044 loss_att 13.307378 loss_ctc 16.359259 loss_rnnt 8.145472 hw_loss 0.219769 lr 0.00029825 rank 6
2023-02-27 13:45:25,179 DEBUG TRAIN Batch 44/6200 loss 15.425255 loss_att 17.157433 loss_ctc 17.806442 loss_rnnt 14.674845 hw_loss 0.162152 lr 0.00029825 rank 4
2023-02-27 13:45:25,179 DEBUG TRAIN Batch 44/6200 loss 5.046801 loss_att 7.054496 loss_ctc 8.791877 loss_rnnt 3.944372 hw_loss 0.377900 lr 0.00029825 rank 5
2023-02-27 13:46:04,010 DEBUG TRAIN Batch 44/6300 loss 7.978601 loss_att 11.409004 loss_ctc 11.563715 loss_rnnt 6.598572 hw_loss 0.404873 lr 0.00029824 rank 2
2023-02-27 13:46:04,018 DEBUG TRAIN Batch 44/6300 loss 12.705986 loss_att 14.390608 loss_ctc 24.111130 loss_rnnt 10.735916 hw_loss 0.210861 lr 0.00029824 rank 1
2023-02-27 13:46:04,020 DEBUG TRAIN Batch 44/6300 loss 5.349443 loss_att 9.044679 loss_ctc 11.478394 loss_rnnt 3.605525 hw_loss 0.351895 lr 0.00029824 rank 6
2023-02-27 13:46:04,021 DEBUG TRAIN Batch 44/6300 loss 1.505120 loss_att 4.856586 loss_ctc 3.331716 loss_rnnt 0.467282 hw_loss 0.232497 lr 0.00029824 rank 3
2023-02-27 13:46:04,026 DEBUG TRAIN Batch 44/6300 loss 9.925714 loss_att 12.270637 loss_ctc 17.908503 loss_rnnt 8.263653 hw_loss 0.241321 lr 0.00029824 rank 7
2023-02-27 13:46:04,030 DEBUG TRAIN Batch 44/6300 loss 12.257238 loss_att 14.638459 loss_ctc 19.863983 loss_rnnt 10.689716 hw_loss 0.144457 lr 0.00029824 rank 0
2023-02-27 13:46:04,049 DEBUG TRAIN Batch 44/6300 loss 5.385549 loss_att 6.019325 loss_ctc 7.066112 loss_rnnt 4.879598 hw_loss 0.290851 lr 0.00029824 rank 5
2023-02-27 13:46:04,059 DEBUG TRAIN Batch 44/6300 loss 11.479732 loss_att 13.982727 loss_ctc 15.066244 loss_rnnt 10.343566 hw_loss 0.295059 lr 0.00029824 rank 4
2023-02-27 13:47:09,913 DEBUG TRAIN Batch 44/6400 loss 4.513988 loss_att 8.610220 loss_ctc 5.172927 loss_rnnt 3.430995 hw_loss 0.329791 lr 0.00029822 rank 6
2023-02-27 13:47:09,915 DEBUG TRAIN Batch 44/6400 loss 8.564988 loss_att 12.840967 loss_ctc 13.566706 loss_rnnt 6.888010 hw_loss 0.290413 lr 0.00029822 rank 3
2023-02-27 13:47:09,917 DEBUG TRAIN Batch 44/6400 loss 6.972587 loss_att 11.036169 loss_ctc 14.399645 loss_rnnt 5.098380 hw_loss 0.133530 lr 0.00029822 rank 4
2023-02-27 13:47:09,919 DEBUG TRAIN Batch 44/6400 loss 6.839611 loss_att 6.793571 loss_ctc 9.340326 loss_rnnt 6.316102 hw_loss 0.373664 lr 0.00029822 rank 0
2023-02-27 13:47:09,920 DEBUG TRAIN Batch 44/6400 loss 11.153050 loss_att 13.629173 loss_ctc 21.870701 loss_rnnt 9.184127 hw_loss 0.083772 lr 0.00029822 rank 5
2023-02-27 13:47:09,932 DEBUG TRAIN Batch 44/6400 loss 6.802834 loss_att 9.030870 loss_ctc 10.480694 loss_rnnt 5.635714 hw_loss 0.433372 lr 0.00029822 rank 7
2023-02-27 13:47:09,937 DEBUG TRAIN Batch 44/6400 loss 7.647664 loss_att 7.846498 loss_ctc 9.531690 loss_rnnt 7.102359 hw_loss 0.476879 lr 0.00029822 rank 1
2023-02-27 13:47:09,976 DEBUG TRAIN Batch 44/6400 loss 5.237116 loss_att 6.812284 loss_ctc 9.035602 loss_rnnt 4.346929 hw_loss 0.128792 lr 0.00029822 rank 2
2023-02-27 13:47:49,072 DEBUG TRAIN Batch 44/6500 loss 5.910928 loss_att 10.383093 loss_ctc 11.169635 loss_rnnt 4.139993 hw_loss 0.328765 lr 0.00029821 rank 7
2023-02-27 13:47:49,087 DEBUG TRAIN Batch 44/6500 loss 5.776444 loss_att 10.587519 loss_ctc 15.244334 loss_rnnt 3.360477 hw_loss 0.358813 lr 0.00029821 rank 3
2023-02-27 13:47:49,087 DEBUG TRAIN Batch 44/6500 loss 5.995774 loss_att 7.695486 loss_ctc 8.219355 loss_rnnt 5.148328 hw_loss 0.395674 lr 0.00029821 rank 6
2023-02-27 13:47:49,089 DEBUG TRAIN Batch 44/6500 loss 4.979772 loss_att 7.674937 loss_ctc 7.917165 loss_rnnt 3.969060 hw_loss 0.150049 lr 0.00029821 rank 4
2023-02-27 13:47:49,090 DEBUG TRAIN Batch 44/6500 loss 5.072701 loss_att 8.361973 loss_ctc 8.846025 loss_rnnt 3.803287 hw_loss 0.203342 lr 0.00029821 rank 0
2023-02-27 13:47:49,091 DEBUG TRAIN Batch 44/6500 loss 9.462003 loss_att 12.766966 loss_ctc 16.687515 loss_rnnt 7.750213 hw_loss 0.163866 lr 0.00029821 rank 2
2023-02-27 13:47:49,095 DEBUG TRAIN Batch 44/6500 loss 5.767912 loss_att 8.163788 loss_ctc 9.052122 loss_rnnt 4.602686 hw_loss 0.465292 lr 0.00029821 rank 5
2023-02-27 13:47:49,095 DEBUG TRAIN Batch 44/6500 loss 2.323013 loss_att 5.754740 loss_ctc 5.269005 loss_rnnt 1.057606 hw_loss 0.349243 lr 0.00029821 rank 1
2023-02-27 13:48:28,099 DEBUG TRAIN Batch 44/6600 loss 6.260596 loss_att 8.283520 loss_ctc 9.359436 loss_rnnt 5.242686 hw_loss 0.375276 lr 0.00029820 rank 4
2023-02-27 13:48:28,107 DEBUG TRAIN Batch 44/6600 loss 10.199372 loss_att 14.520349 loss_ctc 15.509051 loss_rnnt 8.519404 hw_loss 0.202153 lr 0.00029820 rank 7
2023-02-27 13:48:28,108 DEBUG TRAIN Batch 44/6600 loss 3.119565 loss_att 6.203287 loss_ctc 3.545569 loss_rnnt 2.256509 hw_loss 0.355335 lr 0.00029820 rank 3
2023-02-27 13:48:28,107 DEBUG TRAIN Batch 44/6600 loss 18.062279 loss_att 20.451015 loss_ctc 26.052910 loss_rnnt 16.417271 hw_loss 0.190954 lr 0.00029820 rank 0
2023-02-27 13:48:28,111 DEBUG TRAIN Batch 44/6600 loss 2.666552 loss_att 4.943471 loss_ctc 6.132836 loss_rnnt 1.656744 hw_loss 0.172975 lr 0.00029820 rank 6
2023-02-27 13:48:28,113 DEBUG TRAIN Batch 44/6600 loss 15.533415 loss_att 20.269680 loss_ctc 29.241676 loss_rnnt 12.689674 hw_loss 0.128849 lr 0.00029820 rank 1
2023-02-27 13:48:28,115 DEBUG TRAIN Batch 44/6600 loss 7.500164 loss_att 8.661445 loss_ctc 7.266023 loss_rnnt 7.202768 hw_loss 0.180673 lr 0.00029820 rank 5
2023-02-27 13:48:28,173 DEBUG TRAIN Batch 44/6600 loss 6.815521 loss_att 8.851782 loss_ctc 7.160430 loss_rnnt 6.237354 hw_loss 0.234238 lr 0.00029820 rank 2
2023-02-27 13:49:07,816 DEBUG TRAIN Batch 44/6700 loss 13.012164 loss_att 16.168644 loss_ctc 24.012585 loss_rnnt 10.706959 hw_loss 0.388475 lr 0.00029818 rank 4
2023-02-27 13:49:07,818 DEBUG TRAIN Batch 44/6700 loss 4.235659 loss_att 9.018250 loss_ctc 8.445890 loss_rnnt 2.542967 hw_loss 0.327767 lr 0.00029818 rank 1
2023-02-27 13:49:07,821 DEBUG TRAIN Batch 44/6700 loss 5.243911 loss_att 8.652658 loss_ctc 9.730959 loss_rnnt 3.961964 hw_loss 0.003607 lr 0.00029818 rank 7
2023-02-27 13:49:07,822 DEBUG TRAIN Batch 44/6700 loss 3.137862 loss_att 6.694828 loss_ctc 5.976253 loss_rnnt 1.902525 hw_loss 0.272796 lr 0.00029818 rank 6
2023-02-27 13:49:07,822 DEBUG TRAIN Batch 44/6700 loss 6.209994 loss_att 10.495941 loss_ctc 14.022901 loss_rnnt 4.187252 hw_loss 0.232184 lr 0.00029818 rank 3
2023-02-27 13:49:07,823 DEBUG TRAIN Batch 44/6700 loss 4.079849 loss_att 6.597812 loss_ctc 6.465606 loss_rnnt 3.079785 hw_loss 0.334444 lr 0.00029818 rank 2
2023-02-27 13:49:07,825 DEBUG TRAIN Batch 44/6700 loss 3.488075 loss_att 7.870657 loss_ctc 10.788775 loss_rnnt 1.549517 hw_loss 0.166153 lr 0.00029818 rank 0
2023-02-27 13:49:07,847 DEBUG TRAIN Batch 44/6700 loss 5.619901 loss_att 8.497557 loss_ctc 10.897349 loss_rnnt 4.208510 hw_loss 0.247876 lr 0.00029818 rank 5
2023-02-27 13:50:12,883 DEBUG TRAIN Batch 44/6800 loss 5.434322 loss_att 7.814538 loss_ctc 11.632188 loss_rnnt 3.930573 hw_loss 0.377481 lr 0.00029817 rank 4
2023-02-27 13:50:12,885 DEBUG TRAIN Batch 44/6800 loss 11.708866 loss_att 14.339170 loss_ctc 15.354416 loss_rnnt 10.567672 hw_loss 0.241989 lr 0.00029817 rank 3
2023-02-27 13:50:12,885 DEBUG TRAIN Batch 44/6800 loss 4.136423 loss_att 6.343838 loss_ctc 7.180854 loss_rnnt 3.166808 hw_loss 0.229139 lr 0.00029817 rank 0
2023-02-27 13:50:12,890 DEBUG TRAIN Batch 44/6800 loss 3.883681 loss_att 6.640248 loss_ctc 6.449104 loss_rnnt 2.877884 hw_loss 0.210803 lr 0.00029817 rank 7
2023-02-27 13:50:12,891 DEBUG TRAIN Batch 44/6800 loss 3.583669 loss_att 6.874262 loss_ctc 7.501026 loss_rnnt 2.259135 hw_loss 0.270188 lr 0.00029817 rank 1
2023-02-27 13:50:12,892 DEBUG TRAIN Batch 44/6800 loss 6.935437 loss_att 10.669901 loss_ctc 11.846175 loss_rnnt 5.343877 hw_loss 0.356065 lr 0.00029817 rank 6
2023-02-27 13:50:12,896 DEBUG TRAIN Batch 44/6800 loss 4.592658 loss_att 7.262335 loss_ctc 11.540184 loss_rnnt 3.031931 hw_loss 0.188351 lr 0.00029817 rank 5
2023-02-27 13:50:12,939 DEBUG TRAIN Batch 44/6800 loss 8.100975 loss_att 9.055556 loss_ctc 10.280788 loss_rnnt 7.520841 hw_loss 0.184830 lr 0.00029817 rank 2
2023-02-27 13:50:52,172 DEBUG TRAIN Batch 44/6900 loss 5.537040 loss_att 7.168849 loss_ctc 9.277377 loss_rnnt 4.494438 hw_loss 0.407866 lr 0.00029816 rank 5
2023-02-27 13:50:52,179 DEBUG TRAIN Batch 44/6900 loss 7.673361 loss_att 10.314093 loss_ctc 16.311512 loss_rnnt 5.803943 hw_loss 0.355348 lr 0.00029816 rank 0
2023-02-27 13:50:52,190 DEBUG TRAIN Batch 44/6900 loss 8.558084 loss_att 10.226824 loss_ctc 17.247482 loss_rnnt 6.909303 hw_loss 0.293338 lr 0.00029816 rank 2
2023-02-27 13:50:52,192 DEBUG TRAIN Batch 44/6900 loss 6.030912 loss_att 7.600060 loss_ctc 8.605210 loss_rnnt 5.167194 hw_loss 0.387466 lr 0.00029816 rank 4
2023-02-27 13:50:52,194 DEBUG TRAIN Batch 44/6900 loss 3.964781 loss_att 6.917330 loss_ctc 5.232178 loss_rnnt 3.028986 hw_loss 0.330562 lr 0.00029816 rank 1
2023-02-27 13:50:52,195 DEBUG TRAIN Batch 44/6900 loss 14.672222 loss_att 16.028919 loss_ctc 23.135363 loss_rnnt 13.088789 hw_loss 0.344392 lr 0.00029816 rank 6
2023-02-27 13:50:52,197 DEBUG TRAIN Batch 44/6900 loss 8.555317 loss_att 9.986303 loss_ctc 13.520892 loss_rnnt 7.412731 hw_loss 0.364335 lr 0.00029816 rank 3
2023-02-27 13:50:52,200 DEBUG TRAIN Batch 44/6900 loss 4.708951 loss_att 10.098956 loss_ctc 10.282420 loss_rnnt 2.747998 hw_loss 0.262170 lr 0.00029816 rank 7
2023-02-27 13:51:31,505 DEBUG TRAIN Batch 44/7000 loss 10.277687 loss_att 11.261720 loss_ctc 14.206672 loss_rnnt 9.410004 hw_loss 0.275647 lr 0.00029814 rank 0
2023-02-27 13:51:31,507 DEBUG TRAIN Batch 44/7000 loss 5.523890 loss_att 11.562634 loss_ctc 10.369050 loss_rnnt 3.556972 hw_loss 0.212152 lr 0.00029814 rank 6
2023-02-27 13:51:31,524 DEBUG TRAIN Batch 44/7000 loss 3.603132 loss_att 5.993995 loss_ctc 6.410470 loss_rnnt 2.681788 hw_loss 0.129112 lr 0.00029814 rank 3
2023-02-27 13:51:31,525 DEBUG TRAIN Batch 44/7000 loss 4.850988 loss_att 6.312204 loss_ctc 7.746709 loss_rnnt 3.903817 hw_loss 0.504061 lr 0.00029814 rank 2
2023-02-27 13:51:31,526 DEBUG TRAIN Batch 44/7000 loss 4.685870 loss_att 5.993232 loss_ctc 8.655670 loss_rnnt 3.657294 hw_loss 0.445869 lr 0.00029814 rank 7
2023-02-27 13:51:31,528 DEBUG TRAIN Batch 44/7000 loss 10.644956 loss_att 12.318028 loss_ctc 16.190355 loss_rnnt 9.478106 hw_loss 0.174092 lr 0.00029814 rank 1
2023-02-27 13:51:31,528 DEBUG TRAIN Batch 44/7000 loss 13.411296 loss_att 17.333858 loss_ctc 16.883713 loss_rnnt 12.066753 hw_loss 0.181951 lr 0.00029814 rank 5
2023-02-27 13:51:31,530 DEBUG TRAIN Batch 44/7000 loss 7.708035 loss_att 12.971207 loss_ctc 16.929899 loss_rnnt 5.327613 hw_loss 0.184134 lr 0.00029814 rank 4
2023-02-27 13:52:32,884 DEBUG TRAIN Batch 44/7100 loss 5.349234 loss_att 7.135502 loss_ctc 5.486198 loss_rnnt 4.889914 hw_loss 0.157134 lr 0.00029813 rank 1
2023-02-27 13:52:32,897 DEBUG TRAIN Batch 44/7100 loss 6.314252 loss_att 5.727643 loss_ctc 9.418434 loss_rnnt 5.742702 hw_loss 0.515588 lr 0.00029813 rank 0
2023-02-27 13:52:32,897 DEBUG TRAIN Batch 44/7100 loss 4.774759 loss_att 8.367306 loss_ctc 7.902171 loss_rnnt 3.573786 hw_loss 0.122766 lr 0.00029813 rank 3
2023-02-27 13:52:32,898 DEBUG TRAIN Batch 44/7100 loss 8.341638 loss_att 11.326331 loss_ctc 11.556042 loss_rnnt 7.220453 hw_loss 0.179359 lr 0.00029813 rank 6
2023-02-27 13:52:32,899 DEBUG TRAIN Batch 44/7100 loss 4.595279 loss_att 8.178550 loss_ctc 5.406731 loss_rnnt 3.673071 hw_loss 0.182549 lr 0.00029813 rank 2
2023-02-27 13:52:32,899 DEBUG TRAIN Batch 44/7100 loss 3.448870 loss_att 5.675720 loss_ctc 4.728928 loss_rnnt 2.695971 hw_loss 0.256603 lr 0.00029813 rank 7
2023-02-27 13:52:32,902 DEBUG TRAIN Batch 44/7100 loss 6.995605 loss_att 9.844108 loss_ctc 9.830233 loss_rnnt 5.936927 hw_loss 0.208178 lr 0.00029813 rank 5
2023-02-27 13:52:32,944 DEBUG TRAIN Batch 44/7100 loss 2.072268 loss_att 4.965058 loss_ctc 2.714198 loss_rnnt 1.262148 hw_loss 0.273696 lr 0.00029813 rank 4
2023-02-27 13:53:16,211 DEBUG TRAIN Batch 44/7200 loss 13.639551 loss_att 16.160463 loss_ctc 20.167763 loss_rnnt 12.170410 hw_loss 0.177245 lr 0.00029812 rank 3
2023-02-27 13:53:16,215 DEBUG TRAIN Batch 44/7200 loss 2.869289 loss_att 5.012403 loss_ctc 3.429476 loss_rnnt 2.265425 hw_loss 0.188530 lr 0.00029812 rank 1
2023-02-27 13:53:16,222 DEBUG TRAIN Batch 44/7200 loss 7.763187 loss_att 9.947172 loss_ctc 12.101084 loss_rnnt 6.553337 hw_loss 0.365000 lr 0.00029812 rank 0
2023-02-27 13:53:16,226 DEBUG TRAIN Batch 44/7200 loss 7.483525 loss_att 10.339595 loss_ctc 14.060081 loss_rnnt 5.936485 hw_loss 0.185534 lr 0.00029812 rank 5
2023-02-27 13:53:16,226 DEBUG TRAIN Batch 44/7200 loss 8.771493 loss_att 11.875765 loss_ctc 13.422483 loss_rnnt 7.381790 hw_loss 0.278841 lr 0.00029812 rank 6
2023-02-27 13:53:16,228 DEBUG TRAIN Batch 44/7200 loss 3.799702 loss_att 9.093025 loss_ctc 11.904160 loss_rnnt 1.489169 hw_loss 0.321137 lr 0.00029812 rank 4
2023-02-27 13:53:16,230 DEBUG TRAIN Batch 44/7200 loss 3.616904 loss_att 5.947093 loss_ctc 5.663979 loss_rnnt 2.762054 hw_loss 0.217256 lr 0.00029812 rank 7
2023-02-27 13:53:16,230 DEBUG TRAIN Batch 44/7200 loss 5.614758 loss_att 10.237940 loss_ctc 11.274814 loss_rnnt 3.859915 hw_loss 0.141625 lr 0.00029812 rank 2
2023-02-27 13:53:54,955 DEBUG TRAIN Batch 44/7300 loss 2.981577 loss_att 5.932442 loss_ctc 9.042825 loss_rnnt 1.380390 hw_loss 0.380340 lr 0.00029810 rank 6
2023-02-27 13:53:54,967 DEBUG TRAIN Batch 44/7300 loss 12.104038 loss_att 18.713539 loss_ctc 19.944725 loss_rnnt 9.593077 hw_loss 0.269321 lr 0.00029810 rank 4
2023-02-27 13:53:54,968 DEBUG TRAIN Batch 44/7300 loss 12.433084 loss_att 17.625381 loss_ctc 23.205481 loss_rnnt 9.848894 hw_loss 0.205148 lr 0.00029810 rank 3
2023-02-27 13:53:54,971 DEBUG TRAIN Batch 44/7300 loss 2.743618 loss_att 5.813474 loss_ctc 5.815197 loss_rnnt 1.571403 hw_loss 0.278814 lr 0.00029810 rank 0
2023-02-27 13:53:54,971 DEBUG TRAIN Batch 44/7300 loss 3.534933 loss_att 8.399843 loss_ctc 7.680579 loss_rnnt 1.876665 hw_loss 0.248498 lr 0.00029810 rank 1
2023-02-27 13:53:54,971 DEBUG TRAIN Batch 44/7300 loss 7.338684 loss_att 9.025536 loss_ctc 10.523700 loss_rnnt 6.467506 hw_loss 0.204634 lr 0.00029810 rank 2
2023-02-27 13:53:54,972 DEBUG TRAIN Batch 44/7300 loss 1.670743 loss_att 4.244775 loss_ctc 2.847445 loss_rnnt 0.880111 hw_loss 0.222999 lr 0.00029810 rank 7
2023-02-27 13:53:54,991 DEBUG TRAIN Batch 44/7300 loss 10.303821 loss_att 15.006222 loss_ctc 15.988893 loss_rnnt 8.487688 hw_loss 0.220580 lr 0.00029810 rank 5
2023-02-27 13:54:33,916 DEBUG TRAIN Batch 44/7400 loss 7.745363 loss_att 10.051608 loss_ctc 12.822296 loss_rnnt 6.503885 hw_loss 0.193694 lr 0.00029809 rank 5
2023-02-27 13:54:33,917 DEBUG TRAIN Batch 44/7400 loss 5.782955 loss_att 9.156000 loss_ctc 10.060892 loss_rnnt 4.450563 hw_loss 0.163858 lr 0.00029809 rank 2
2023-02-27 13:54:33,926 DEBUG TRAIN Batch 44/7400 loss 2.200024 loss_att 5.264662 loss_ctc 4.052179 loss_rnnt 1.200808 hw_loss 0.261251 lr 0.00029809 rank 3
2023-02-27 13:54:33,928 DEBUG TRAIN Batch 44/7400 loss 7.312108 loss_att 12.679142 loss_ctc 16.231430 loss_rnnt 4.844095 hw_loss 0.385055 lr 0.00029809 rank 0
2023-02-27 13:54:33,929 DEBUG TRAIN Batch 44/7400 loss 6.064217 loss_att 9.115295 loss_ctc 15.370070 loss_rnnt 4.106286 hw_loss 0.200503 lr 0.00029809 rank 7
2023-02-27 13:54:33,930 DEBUG TRAIN Batch 44/7400 loss 7.458880 loss_att 9.129963 loss_ctc 12.521339 loss_rnnt 6.346205 hw_loss 0.193996 lr 0.00029809 rank 1
2023-02-27 13:54:33,931 DEBUG TRAIN Batch 44/7400 loss 6.606781 loss_att 8.942804 loss_ctc 14.861502 loss_rnnt 4.893186 hw_loss 0.273304 lr 0.00029809 rank 6
2023-02-27 13:54:33,932 DEBUG TRAIN Batch 44/7400 loss 5.644057 loss_att 6.587352 loss_ctc 8.744484 loss_rnnt 4.947471 hw_loss 0.177257 lr 0.00029809 rank 4
2023-02-27 13:55:39,984 DEBUG TRAIN Batch 44/7500 loss 6.004843 loss_att 8.491066 loss_ctc 11.335145 loss_rnnt 4.715492 hw_loss 0.152624 lr 0.00029808 rank 5
2023-02-27 13:55:39,993 DEBUG TRAIN Batch 44/7500 loss 9.320079 loss_att 12.693250 loss_ctc 14.705259 loss_rnnt 7.776327 hw_loss 0.283300 lr 0.00029808 rank 1
2023-02-27 13:55:40,001 DEBUG TRAIN Batch 44/7500 loss 8.192816 loss_att 10.012189 loss_ctc 10.628934 loss_rnnt 7.404196 hw_loss 0.187365 lr 0.00029808 rank 0
2023-02-27 13:55:40,003 DEBUG TRAIN Batch 44/7500 loss 3.454292 loss_att 4.613678 loss_ctc 6.447693 loss_rnnt 2.614437 hw_loss 0.391608 lr 0.00029808 rank 3
2023-02-27 13:55:40,008 DEBUG TRAIN Batch 44/7500 loss 8.639880 loss_att 10.743830 loss_ctc 13.984797 loss_rnnt 7.425720 hw_loss 0.151340 lr 0.00029808 rank 4
2023-02-27 13:55:40,011 DEBUG TRAIN Batch 44/7500 loss 6.474178 loss_att 8.050320 loss_ctc 8.591858 loss_rnnt 5.740373 hw_loss 0.255412 lr 0.00029808 rank 6
2023-02-27 13:55:40,020 DEBUG TRAIN Batch 44/7500 loss 6.010297 loss_att 9.016474 loss_ctc 11.160507 loss_rnnt 4.670490 hw_loss 0.097272 lr 0.00029808 rank 2
2023-02-27 13:55:40,053 DEBUG TRAIN Batch 44/7500 loss 5.583220 loss_att 7.400065 loss_ctc 8.390256 loss_rnnt 4.739735 hw_loss 0.198459 lr 0.00029808 rank 7
2023-02-27 13:56:19,307 DEBUG TRAIN Batch 44/7600 loss 5.196912 loss_att 7.340902 loss_ctc 12.860674 loss_rnnt 3.599533 hw_loss 0.275151 lr 0.00029806 rank 6
2023-02-27 13:56:19,312 DEBUG TRAIN Batch 44/7600 loss 3.812231 loss_att 6.436852 loss_ctc 5.706077 loss_rnnt 2.902353 hw_loss 0.248326 lr 0.00029806 rank 3
2023-02-27 13:56:19,321 DEBUG TRAIN Batch 44/7600 loss 7.684166 loss_att 9.741218 loss_ctc 15.846008 loss_rnnt 5.989226 hw_loss 0.366159 lr 0.00029806 rank 7
2023-02-27 13:56:19,322 DEBUG TRAIN Batch 44/7600 loss 3.836434 loss_att 6.871172 loss_ctc 6.856454 loss_rnnt 2.683583 hw_loss 0.268563 lr 0.00029806 rank 2
2023-02-27 13:56:19,324 DEBUG TRAIN Batch 44/7600 loss 12.963729 loss_att 13.464405 loss_ctc 19.359480 loss_rnnt 11.858404 hw_loss 0.285792 lr 0.00029806 rank 4
2023-02-27 13:56:19,324 DEBUG TRAIN Batch 44/7600 loss 7.605413 loss_att 8.617393 loss_ctc 13.114484 loss_rnnt 6.482754 hw_loss 0.348223 lr 0.00029806 rank 5
2023-02-27 13:56:19,328 DEBUG TRAIN Batch 44/7600 loss 7.470160 loss_att 9.629495 loss_ctc 10.019068 loss_rnnt 6.600437 hw_loss 0.183754 lr 0.00029806 rank 0
2023-02-27 13:56:19,332 DEBUG TRAIN Batch 44/7600 loss 9.035667 loss_att 11.701050 loss_ctc 15.547192 loss_rnnt 7.453528 hw_loss 0.339111 lr 0.00029806 rank 1
2023-02-27 13:56:58,647 DEBUG TRAIN Batch 44/7700 loss 9.921516 loss_att 10.874462 loss_ctc 13.238661 loss_rnnt 9.194672 hw_loss 0.176194 lr 0.00029805 rank 6
2023-02-27 13:56:58,652 DEBUG TRAIN Batch 44/7700 loss 6.027030 loss_att 9.826710 loss_ctc 11.089538 loss_rnnt 4.530998 hw_loss 0.114551 lr 0.00029805 rank 7
2023-02-27 13:56:58,653 DEBUG TRAIN Batch 44/7700 loss 7.258193 loss_att 11.025148 loss_ctc 11.404575 loss_rnnt 5.753740 hw_loss 0.371643 lr 0.00029805 rank 3
2023-02-27 13:56:58,658 DEBUG TRAIN Batch 44/7700 loss 7.424696 loss_att 8.780286 loss_ctc 10.377223 loss_rnnt 6.578705 hw_loss 0.339755 lr 0.00029805 rank 0
2023-02-27 13:56:58,658 DEBUG TRAIN Batch 44/7700 loss 8.991028 loss_att 9.683756 loss_ctc 12.075561 loss_rnnt 8.264849 hw_loss 0.330680 lr 0.00029805 rank 1
2023-02-27 13:56:58,661 DEBUG TRAIN Batch 44/7700 loss 11.867452 loss_att 16.136621 loss_ctc 18.912533 loss_rnnt 10.009443 hw_loss 0.121558 lr 0.00029805 rank 5
2023-02-27 13:56:58,664 DEBUG TRAIN Batch 44/7700 loss 6.314656 loss_att 11.138628 loss_ctc 16.619318 loss_rnnt 3.872710 hw_loss 0.193492 lr 0.00029805 rank 4
2023-02-27 13:56:58,705 DEBUG TRAIN Batch 44/7700 loss 6.388875 loss_att 7.125502 loss_ctc 8.149521 loss_rnnt 5.868141 hw_loss 0.259982 lr 0.00029805 rank 2
2023-02-27 13:57:38,804 DEBUG TRAIN Batch 44/7800 loss 10.467760 loss_att 17.807745 loss_ctc 19.325478 loss_rnnt 7.699112 hw_loss 0.224292 lr 0.00029804 rank 7
2023-02-27 13:57:38,807 DEBUG TRAIN Batch 44/7800 loss 11.057234 loss_att 13.083810 loss_ctc 17.797483 loss_rnnt 9.620830 hw_loss 0.248233 lr 0.00029804 rank 2
2023-02-27 13:57:38,808 DEBUG TRAIN Batch 44/7800 loss 4.404606 loss_att 8.146865 loss_ctc 9.143133 loss_rnnt 2.921297 hw_loss 0.193227 lr 0.00029804 rank 1
2023-02-27 13:57:38,815 DEBUG TRAIN Batch 44/7800 loss 4.490921 loss_att 7.624921 loss_ctc 6.333990 loss_rnnt 3.433496 hw_loss 0.346653 lr 0.00029804 rank 0
2023-02-27 13:57:38,817 DEBUG TRAIN Batch 44/7800 loss 2.967636 loss_att 5.994634 loss_ctc 5.341838 loss_rnnt 1.856740 hw_loss 0.354255 lr 0.00029804 rank 6
2023-02-27 13:57:38,818 DEBUG TRAIN Batch 44/7800 loss 5.520834 loss_att 8.405157 loss_ctc 7.918489 loss_rnnt 4.442190 hw_loss 0.341423 lr 0.00029804 rank 4
2023-02-27 13:57:38,827 DEBUG TRAIN Batch 44/7800 loss 7.421907 loss_att 10.424105 loss_ctc 10.888514 loss_rnnt 6.244888 hw_loss 0.214435 lr 0.00029804 rank 3
2023-02-27 13:57:38,854 DEBUG TRAIN Batch 44/7800 loss 6.395399 loss_att 10.524942 loss_ctc 10.220735 loss_rnnt 4.915624 hw_loss 0.269664 lr 0.00029804 rank 5
2023-02-27 13:58:42,763 DEBUG TRAIN Batch 44/7900 loss 8.409059 loss_att 10.563380 loss_ctc 14.531301 loss_rnnt 7.048809 hw_loss 0.212037 lr 0.00029802 rank 7
2023-02-27 13:58:42,767 DEBUG TRAIN Batch 44/7900 loss 6.316858 loss_att 8.197582 loss_ctc 9.626884 loss_rnnt 5.348764 hw_loss 0.282396 lr 0.00029802 rank 4
2023-02-27 13:58:42,774 DEBUG TRAIN Batch 44/7900 loss 4.260106 loss_att 7.037939 loss_ctc 6.638499 loss_rnnt 3.205877 hw_loss 0.340393 lr 0.00029802 rank 5
2023-02-27 13:58:42,776 DEBUG TRAIN Batch 44/7900 loss 1.817621 loss_att 6.051245 loss_ctc 4.278302 loss_rnnt 0.533440 hw_loss 0.205061 lr 0.00029802 rank 0
2023-02-27 13:58:42,780 DEBUG TRAIN Batch 44/7900 loss 8.773904 loss_att 10.254018 loss_ctc 16.605640 loss_rnnt 7.381535 hw_loss 0.097713 lr 0.00029802 rank 3
2023-02-27 13:58:42,780 DEBUG TRAIN Batch 44/7900 loss 10.765223 loss_att 12.667780 loss_ctc 15.206823 loss_rnnt 9.643682 hw_loss 0.279027 lr 0.00029802 rank 6
2023-02-27 13:58:42,783 DEBUG TRAIN Batch 44/7900 loss 4.258972 loss_att 7.419418 loss_ctc 5.788862 loss_rnnt 3.248317 hw_loss 0.327340 lr 0.00029802 rank 1
2023-02-27 13:58:42,789 DEBUG TRAIN Batch 44/7900 loss 4.929671 loss_att 7.636544 loss_ctc 8.054481 loss_rnnt 3.898964 hw_loss 0.136297 lr 0.00029802 rank 2
2023-02-27 13:59:21,764 DEBUG TRAIN Batch 44/8000 loss 2.305426 loss_att 5.095068 loss_ctc 4.442932 loss_rnnt 1.343628 hw_loss 0.222878 lr 0.00029801 rank 3
2023-02-27 13:59:21,766 DEBUG TRAIN Batch 44/8000 loss 10.487209 loss_att 12.858486 loss_ctc 15.991472 loss_rnnt 9.138609 hw_loss 0.263330 lr 0.00029801 rank 6
2023-02-27 13:59:21,768 DEBUG TRAIN Batch 44/8000 loss 2.733352 loss_att 4.980339 loss_ctc 3.893455 loss_rnnt 1.914835 hw_loss 0.402073 lr 0.00029801 rank 1
2023-02-27 13:59:21,769 DEBUG TRAIN Batch 44/8000 loss 3.118293 loss_att 5.250233 loss_ctc 4.253423 loss_rnnt 2.364095 hw_loss 0.330862 lr 0.00029801 rank 7
2023-02-27 13:59:21,770 DEBUG TRAIN Batch 44/8000 loss 12.494122 loss_att 14.167776 loss_ctc 21.619106 loss_rnnt 10.731499 hw_loss 0.396052 lr 0.00029801 rank 2
2023-02-27 13:59:21,770 DEBUG TRAIN Batch 44/8000 loss 4.336899 loss_att 6.225178 loss_ctc 7.067688 loss_rnnt 3.441411 hw_loss 0.288237 lr 0.00029801 rank 4
2023-02-27 13:59:21,771 DEBUG TRAIN Batch 44/8000 loss 4.010237 loss_att 6.338422 loss_ctc 5.916055 loss_rnnt 3.165231 hw_loss 0.234862 lr 0.00029801 rank 5
2023-02-27 13:59:21,780 DEBUG TRAIN Batch 44/8000 loss 6.899841 loss_att 14.788933 loss_ctc 9.865530 loss_rnnt 4.768394 hw_loss 0.296631 lr 0.00029801 rank 0
2023-02-27 14:00:00,943 DEBUG TRAIN Batch 44/8100 loss 5.232217 loss_att 8.522239 loss_ctc 6.435384 loss_rnnt 4.212586 hw_loss 0.377258 lr 0.00029800 rank 6
2023-02-27 14:00:00,944 DEBUG TRAIN Batch 44/8100 loss 8.828687 loss_att 11.044914 loss_ctc 21.276676 loss_rnnt 6.618104 hw_loss 0.201760 lr 0.00029800 rank 7
2023-02-27 14:00:00,945 DEBUG TRAIN Batch 44/8100 loss 8.544016 loss_att 11.770720 loss_ctc 13.182570 loss_rnnt 7.204742 hw_loss 0.141486 lr 0.00029800 rank 0
2023-02-27 14:00:00,947 DEBUG TRAIN Batch 44/8100 loss 9.281844 loss_att 12.824487 loss_ctc 16.237669 loss_rnnt 7.500708 hw_loss 0.272183 lr 0.00029800 rank 2
2023-02-27 14:00:00,951 DEBUG TRAIN Batch 44/8100 loss 2.839399 loss_att 5.463497 loss_ctc 5.491215 loss_rnnt 1.928477 hw_loss 0.060989 lr 0.00029800 rank 5
2023-02-27 14:00:00,971 DEBUG TRAIN Batch 44/8100 loss 6.209782 loss_att 8.477171 loss_ctc 11.554743 loss_rnnt 4.956712 hw_loss 0.162995 lr 0.00029800 rank 3
2023-02-27 14:00:00,982 DEBUG TRAIN Batch 44/8100 loss 10.720123 loss_att 11.238285 loss_ctc 14.185318 loss_rnnt 10.107821 hw_loss 0.087457 lr 0.00029800 rank 4
2023-02-27 14:00:00,989 DEBUG TRAIN Batch 44/8100 loss 4.036100 loss_att 5.912749 loss_ctc 5.887465 loss_rnnt 3.289295 hw_loss 0.233674 lr 0.00029800 rank 1
2023-02-27 14:00:41,073 DEBUG TRAIN Batch 44/8200 loss 1.961976 loss_att 4.126265 loss_ctc 3.782016 loss_rnnt 1.152473 hw_loss 0.251200 lr 0.00029798 rank 0
2023-02-27 14:00:41,074 DEBUG TRAIN Batch 44/8200 loss 4.132423 loss_att 4.407859 loss_ctc 5.631267 loss_rnnt 3.675035 hw_loss 0.379603 lr 0.00029798 rank 6
2023-02-27 14:00:41,091 DEBUG TRAIN Batch 44/8200 loss 5.582114 loss_att 8.445587 loss_ctc 10.420721 loss_rnnt 4.174631 hw_loss 0.355575 lr 0.00029798 rank 7
2023-02-27 14:00:41,095 DEBUG TRAIN Batch 44/8200 loss 9.645582 loss_att 12.270263 loss_ctc 18.083212 loss_rnnt 7.817379 hw_loss 0.334219 lr 0.00029798 rank 5
2023-02-27 14:00:41,095 DEBUG TRAIN Batch 44/8200 loss 2.392531 loss_att 5.074514 loss_ctc 4.264834 loss_rnnt 1.525755 hw_loss 0.151387 lr 0.00029798 rank 3
2023-02-27 14:00:41,096 DEBUG TRAIN Batch 44/8200 loss 7.279461 loss_att 10.668752 loss_ctc 18.777620 loss_rnnt 4.896671 hw_loss 0.322207 lr 0.00029798 rank 4
2023-02-27 14:00:41,096 DEBUG TRAIN Batch 44/8200 loss 7.241949 loss_att 8.690604 loss_ctc 11.072375 loss_rnnt 6.344635 hw_loss 0.181611 lr 0.00029798 rank 2
2023-02-27 14:00:41,101 DEBUG TRAIN Batch 44/8200 loss 6.189339 loss_att 6.453345 loss_ctc 7.467772 loss_rnnt 5.833807 hw_loss 0.248011 lr 0.00029798 rank 1
2023-02-27 14:01:19,467 DEBUG TRAIN Batch 44/8300 loss 5.809003 loss_att 9.117897 loss_ctc 9.696520 loss_rnnt 4.475891 hw_loss 0.286872 lr 0.00029797 rank 3
2023-02-27 14:01:19,473 DEBUG TRAIN Batch 44/8300 loss 3.165082 loss_att 6.694307 loss_ctc 6.711973 loss_rnnt 1.932483 hw_loss 0.100940 lr 0.00029797 rank 2
2023-02-27 14:01:19,474 DEBUG TRAIN Batch 44/8300 loss 4.763293 loss_att 7.509462 loss_ctc 6.473591 loss_rnnt 3.893622 hw_loss 0.173246 lr 0.00029797 rank 1
2023-02-27 14:01:19,475 DEBUG TRAIN Batch 44/8300 loss 7.903166 loss_att 11.320864 loss_ctc 12.020901 loss_rnnt 6.581665 hw_loss 0.166744 lr 0.00029797 rank 7
2023-02-27 14:01:19,486 DEBUG TRAIN Batch 44/8300 loss 1.355679 loss_att 3.867435 loss_ctc 3.147711 loss_rnnt 0.428804 hw_loss 0.347975 lr 0.00029797 rank 5
2023-02-27 14:01:19,488 DEBUG TRAIN Batch 44/8300 loss 5.515081 loss_att 8.373022 loss_ctc 10.288702 loss_rnnt 4.223025 hw_loss 0.157470 lr 0.00029797 rank 0
2023-02-27 14:01:19,488 DEBUG TRAIN Batch 44/8300 loss 4.600159 loss_att 5.560455 loss_ctc 8.607339 loss_rnnt 3.690860 hw_loss 0.343029 lr 0.00029797 rank 4
2023-02-27 14:01:19,492 DEBUG TRAIN Batch 44/8300 loss 2.872786 loss_att 6.082563 loss_ctc 4.616627 loss_rnnt 1.924613 hw_loss 0.138199 lr 0.00029797 rank 6
2023-02-27 14:01:48,875 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 6
2023-02-27 14:01:48,892 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 0
2023-02-27 14:01:48,995 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 1
2023-02-27 14:01:49,020 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 3
2023-02-27 14:01:49,065 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 7
2023-02-27 14:01:49,101 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 2
2023-02-27 14:01:49,157 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 4
2023-02-27 14:01:49,212 DEBUG CV Batch 44/0 loss 1.169143 loss_att 1.043662 loss_ctc 1.858761 loss_rnnt 0.872392 hw_loss 0.431059 history loss 1.125842 rank 5
2023-02-27 14:02:00,140 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 6
2023-02-27 14:02:00,289 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 3
2023-02-27 14:02:00,467 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 0
2023-02-27 14:02:00,846 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 2
2023-02-27 14:02:00,920 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 4
2023-02-27 14:02:01,078 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 7
2023-02-27 14:02:01,184 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 1
2023-02-27 14:02:01,341 DEBUG CV Batch 44/100 loss 3.223056 loss_att 4.349737 loss_ctc 6.898823 loss_rnnt 2.330873 hw_loss 0.331395 history loss 2.975795 rank 5
2023-02-27 14:02:13,466 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 6
2023-02-27 14:02:13,756 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 3
2023-02-27 14:02:13,983 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 0
2023-02-27 14:02:14,724 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 2
2023-02-27 14:02:14,830 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 7
2023-02-27 14:02:15,027 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 4
2023-02-27 14:02:15,096 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 5
2023-02-27 14:02:15,177 DEBUG CV Batch 44/200 loss 6.392751 loss_att 8.974203 loss_ctc 9.287947 loss_rnnt 5.422958 hw_loss 0.126518 history loss 3.527718 rank 1
2023-02-27 14:02:25,789 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 6
2023-02-27 14:02:26,532 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 0
2023-02-27 14:02:26,545 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 3
2023-02-27 14:02:27,600 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 1
2023-02-27 14:02:27,650 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 5
2023-02-27 14:02:27,938 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 2
2023-02-27 14:02:28,154 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 4
2023-02-27 14:02:28,399 DEBUG CV Batch 44/300 loss 2.986458 loss_att 3.715447 loss_ctc 5.059206 loss_rnnt 2.362828 hw_loss 0.377749 history loss 3.655970 rank 7
2023-02-27 14:02:37,860 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 6
2023-02-27 14:02:38,714 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 3
2023-02-27 14:02:38,836 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 0
2023-02-27 14:02:40,224 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 1
2023-02-27 14:02:40,290 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 5
2023-02-27 14:02:40,468 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 2
2023-02-27 14:02:41,023 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 4
2023-02-27 14:02:41,160 DEBUG CV Batch 44/400 loss 14.016179 loss_att 52.059166 loss_ctc 8.954798 loss_rnnt 7.009442 hw_loss 0.136858 history loss 4.452489 rank 7
2023-02-27 14:02:48,582 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 6
2023-02-27 14:02:49,626 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 3
2023-02-27 14:02:49,640 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 0
2023-02-27 14:02:51,422 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 1
2023-02-27 14:02:51,422 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 5
2023-02-27 14:02:51,532 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 2
2023-02-27 14:02:52,099 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 4
2023-02-27 14:02:52,642 DEBUG CV Batch 44/500 loss 4.277206 loss_att 4.261559 loss_ctc 5.430921 loss_rnnt 3.978640 hw_loss 0.277252 history loss 5.066319 rank 7
2023-02-27 14:03:01,078 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 6
2023-02-27 14:03:01,900 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 3
2023-02-27 14:03:01,930 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 0
2023-02-27 14:03:03,954 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 1
2023-02-27 14:03:04,070 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 5
2023-02-27 14:03:04,187 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 2
2023-02-27 14:03:04,622 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 4
2023-02-27 14:03:05,454 DEBUG CV Batch 44/600 loss 7.279763 loss_att 6.238310 loss_ctc 10.180508 loss_rnnt 6.915170 hw_loss 0.348970 history loss 5.892443 rank 7
2023-02-27 14:03:12,387 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 6
2023-02-27 14:03:13,377 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 3
2023-02-27 14:03:13,470 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 0
2023-02-27 14:03:15,819 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 5
2023-02-27 14:03:16,043 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 2
2023-02-27 14:03:16,060 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 1
2023-02-27 14:03:16,593 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 4
2023-02-27 14:03:17,489 DEBUG CV Batch 44/700 loss 5.538647 loss_att 14.287097 loss_ctc 6.877210 loss_rnnt 3.443199 hw_loss 0.313656 history loss 6.416410 rank 7
2023-02-27 14:03:23,769 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 6
2023-02-27 14:03:24,764 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 3
2023-02-27 14:03:24,871 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 0
2023-02-27 14:03:27,623 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 5
2023-02-27 14:03:27,777 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 1
2023-02-27 14:03:27,831 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 2
2023-02-27 14:03:28,535 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 4
2023-02-27 14:03:29,321 DEBUG CV Batch 44/800 loss 6.623975 loss_att 7.377947 loss_ctc 13.986828 loss_rnnt 5.342078 hw_loss 0.280104 history loss 5.961106 rank 7
2023-02-27 14:03:37,049 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 6
2023-02-27 14:03:38,109 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 3
2023-02-27 14:03:38,420 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 0
2023-02-27 14:03:41,198 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 5
2023-02-27 14:03:41,627 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 2
2023-02-27 14:03:41,751 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 1
2023-02-27 14:03:42,178 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 4
2023-02-27 14:03:43,272 DEBUG CV Batch 44/900 loss 6.830317 loss_att 9.219066 loss_ctc 15.833875 loss_rnnt 5.150394 hw_loss 0.003187 history loss 5.793696 rank 7
2023-02-27 14:03:49,233 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613066 rank 6
2023-02-27 14:03:50,361 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 3
2023-02-27 14:03:50,772 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613066 rank 0
2023-02-27 14:03:53,820 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 5
2023-02-27 14:03:54,430 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 2
2023-02-27 14:03:54,621 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 1
2023-02-27 14:03:54,859 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 4
2023-02-27 14:03:56,179 DEBUG CV Batch 44/1000 loss 3.062805 loss_att 3.446493 loss_ctc 3.061157 loss_rnnt 2.764282 hw_loss 0.416260 history loss 5.613067 rank 7
2023-02-27 14:04:01,124 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 6
2023-02-27 14:04:02,446 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 3
2023-02-27 14:04:02,945 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 0
2023-02-27 14:04:06,040 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 5
2023-02-27 14:04:06,993 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 2
2023-02-27 14:04:07,181 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 4
2023-02-27 14:04:07,281 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 1
2023-02-27 14:04:08,571 DEBUG CV Batch 44/1100 loss 4.790978 loss_att 4.662868 loss_ctc 8.220243 loss_rnnt 4.122176 hw_loss 0.444728 history loss 5.583247 rank 7
2023-02-27 14:04:11,888 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 6
2023-02-27 14:04:13,323 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 3
2023-02-27 14:04:13,836 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 0
2023-02-27 14:04:17,099 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 5
2023-02-27 14:04:18,081 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 2
2023-02-27 14:04:18,458 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 4
2023-02-27 14:04:18,560 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 1
2023-02-27 14:04:19,844 DEBUG CV Batch 44/1200 loss 6.435938 loss_att 6.375740 loss_ctc 7.579755 loss_rnnt 6.118386 hw_loss 0.332030 history loss 5.856678 rank 7
2023-02-27 14:04:23,938 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 6
2023-02-27 14:04:25,477 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 3
2023-02-27 14:04:26,121 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 0
2023-02-27 14:04:29,596 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 5
2023-02-27 14:04:30,700 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 2
2023-02-27 14:04:30,855 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 4
2023-02-27 14:04:31,112 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 1
2023-02-27 14:04:32,623 DEBUG CV Batch 44/1300 loss 4.616158 loss_att 4.265176 loss_ctc 6.663519 loss_rnnt 4.226608 hw_loss 0.350184 history loss 6.143733 rank 7
2023-02-27 14:04:35,212 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 6
2023-02-27 14:04:36,872 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 3
2023-02-27 14:04:37,590 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 0
2023-02-27 14:04:41,428 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 5
2023-02-27 14:04:42,477 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 2
2023-02-27 14:04:42,488 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 4
2023-02-27 14:04:43,027 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 1
2023-02-27 14:04:44,841 DEBUG CV Batch 44/1400 loss 3.210648 loss_att 11.491937 loss_ctc 3.639942 loss_rnnt 1.378665 hw_loss 0.222161 history loss 6.398415 rank 7
2023-02-27 14:04:46,802 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 6
2023-02-27 14:04:48,523 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 3
2023-02-27 14:04:49,247 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 0
2023-02-27 14:04:53,450 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 5
2023-02-27 14:04:54,493 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 2
2023-02-27 14:04:54,550 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 4
2023-02-27 14:04:54,942 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 1
2023-02-27 14:04:56,824 DEBUG CV Batch 44/1500 loss 7.410645 loss_att 7.268749 loss_ctc 8.333912 loss_rnnt 7.192607 hw_loss 0.231217 history loss 6.264455 rank 7
2023-02-27 14:04:59,879 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 6
2023-02-27 14:05:01,657 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 3
2023-02-27 14:05:02,547 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 0
2023-02-27 14:05:06,856 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 5
2023-02-27 14:05:08,048 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 2
2023-02-27 14:05:08,080 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 4
2023-02-27 14:05:08,713 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 1
2023-02-27 14:05:10,362 DEBUG CV Batch 44/1600 loss 9.829070 loss_att 15.003634 loss_ctc 12.816307 loss_rnnt 8.259709 hw_loss 0.255280 history loss 6.223587 rank 7
2023-02-27 14:05:12,462 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 6
2023-02-27 14:05:14,176 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 3
2023-02-27 14:05:15,133 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 0
2023-02-27 14:05:19,718 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 5
2023-02-27 14:05:20,766 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 4
2023-02-27 14:05:20,933 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 2
2023-02-27 14:05:21,217 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 1
2023-02-27 14:05:21,620 INFO Epoch 44 CV info cv_loss 6.143328773129415
2023-02-27 14:05:21,621 INFO Epoch 45 TRAIN info lr 0.00029796446601846113
2023-02-27 14:05:21,626 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:23,001 DEBUG CV Batch 44/1700 loss 7.326910 loss_att 6.300337 loss_ctc 13.506277 loss_rnnt 6.579052 hw_loss 0.242356 history loss 6.157660 rank 7
2023-02-27 14:05:23,363 INFO Epoch 44 CV info cv_loss 6.143328772287335
2023-02-27 14:05:23,363 INFO Epoch 45 TRAIN info lr 0.00029796287878355686
2023-02-27 14:05:23,368 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:24,434 INFO Epoch 44 CV info cv_loss 6.14332877276114
2023-02-27 14:05:24,434 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/44.pt
2023-02-27 14:05:25,024 INFO Epoch 45 TRAIN info lr 0.00029796869876888424
2023-02-27 14:05:25,028 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:29,051 INFO Epoch 44 CV info cv_loss 6.143328773038962
2023-02-27 14:05:29,052 INFO Epoch 45 TRAIN info lr 0.0002979649951023993
2023-02-27 14:05:29,057 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:29,779 INFO Epoch 44 CV info cv_loss 6.143328773079881
2023-02-27 14:05:29,779 INFO Epoch 45 TRAIN info lr 0.0002979660532787311
2023-02-27 14:05:29,781 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:30,180 INFO Epoch 44 CV info cv_loss 6.143328772616845
2023-02-27 14:05:30,180 INFO Epoch 45 TRAIN info lr 0.000297965524189156
2023-02-27 14:05:30,185 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:30,560 INFO Epoch 44 CV info cv_loss 6.143328774981561
2023-02-27 14:05:30,560 INFO Epoch 45 TRAIN info lr 0.00029796764056436734
2023-02-27 14:05:30,562 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:05:32,298 INFO Epoch 44 CV info cv_loss 6.143328773010964
2023-02-27 14:05:32,298 INFO Epoch 45 TRAIN info lr 0.000297965524189156
2023-02-27 14:05:32,300 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 14:06:34,616 DEBUG TRAIN Batch 45/0 loss 7.567171 loss_att 8.101376 loss_ctc 12.814543 loss_rnnt 6.540381 hw_loss 0.413063 lr 0.00029796 rank 6
2023-02-27 14:06:34,619 DEBUG TRAIN Batch 45/0 loss 6.546824 loss_att 6.432008 loss_ctc 9.847781 loss_rnnt 5.931258 hw_loss 0.372004 lr 0.00029797 rank 0
2023-02-27 14:06:34,624 DEBUG TRAIN Batch 45/0 loss 5.494084 loss_att 5.804969 loss_ctc 8.937454 loss_rnnt 4.761934 hw_loss 0.395359 lr 0.00029796 rank 3
2023-02-27 14:06:34,645 DEBUG TRAIN Batch 45/0 loss 5.659918 loss_att 6.072111 loss_ctc 7.708784 loss_rnnt 5.057461 hw_loss 0.462818 lr 0.00029797 rank 4
2023-02-27 14:06:34,673 DEBUG TRAIN Batch 45/0 loss 4.890940 loss_att 4.819940 loss_ctc 6.348122 loss_rnnt 4.502601 hw_loss 0.390466 lr 0.00029797 rank 1
2023-02-27 14:06:34,674 DEBUG TRAIN Batch 45/0 loss 8.968917 loss_att 9.204317 loss_ctc 13.415135 loss_rnnt 8.071014 hw_loss 0.483738 lr 0.00029796 rank 5
2023-02-27 14:06:34,684 DEBUG TRAIN Batch 45/0 loss 6.833123 loss_att 7.634956 loss_ctc 10.349216 loss_rnnt 5.952902 hw_loss 0.470704 lr 0.00029796 rank 7
2023-02-27 14:06:34,717 DEBUG TRAIN Batch 45/0 loss 6.877820 loss_att 6.537698 loss_ctc 10.282612 loss_rnnt 6.259115 hw_loss 0.436420 lr 0.00029796 rank 2
2023-02-27 14:07:12,638 DEBUG TRAIN Batch 45/100 loss 1.869433 loss_att 4.414868 loss_ctc 3.401097 loss_rnnt 0.997219 hw_loss 0.297948 lr 0.00029795 rank 7
2023-02-27 14:07:12,639 DEBUG TRAIN Batch 45/100 loss 9.590509 loss_att 13.424515 loss_ctc 16.515015 loss_rnnt 7.799061 hw_loss 0.190087 lr 0.00029795 rank 5
2023-02-27 14:07:12,644 DEBUG TRAIN Batch 45/100 loss 5.987041 loss_att 7.491838 loss_ctc 9.332561 loss_rnnt 5.236660 hw_loss 0.006285 lr 0.00029795 rank 2
2023-02-27 14:07:12,651 DEBUG TRAIN Batch 45/100 loss 1.878191 loss_att 4.164912 loss_ctc 1.903751 loss_rnnt 1.249765 hw_loss 0.314389 lr 0.00029795 rank 3
2023-02-27 14:07:12,657 DEBUG TRAIN Batch 45/100 loss 6.533534 loss_att 10.735847 loss_ctc 12.789776 loss_rnnt 4.853105 hw_loss 0.010877 lr 0.00029795 rank 0
2023-02-27 14:07:12,659 DEBUG TRAIN Batch 45/100 loss 9.201032 loss_att 11.573301 loss_ctc 15.263286 loss_rnnt 7.732797 hw_loss 0.347774 lr 0.00029795 rank 1
2023-02-27 14:07:12,659 DEBUG TRAIN Batch 45/100 loss 8.611631 loss_att 13.310736 loss_ctc 12.242371 loss_rnnt 7.050165 hw_loss 0.257901 lr 0.00029795 rank 6
2023-02-27 14:07:12,676 DEBUG TRAIN Batch 45/100 loss 2.581761 loss_att 4.696587 loss_ctc 3.339239 loss_rnnt 1.948769 hw_loss 0.204431 lr 0.00029795 rank 4
2023-02-27 14:07:50,685 DEBUG TRAIN Batch 45/200 loss 4.488460 loss_att 6.343781 loss_ctc 7.633694 loss_rnnt 3.601078 hw_loss 0.181788 lr 0.00029794 rank 5
2023-02-27 14:07:50,685 DEBUG TRAIN Batch 45/200 loss 1.172830 loss_att 4.301919 loss_ctc 1.658456 loss_rnnt 0.297229 hw_loss 0.346938 lr 0.00029794 rank 3
2023-02-27 14:07:50,689 DEBUG TRAIN Batch 45/200 loss 5.353830 loss_att 7.255023 loss_ctc 7.870567 loss_rnnt 4.566269 hw_loss 0.134545 lr 0.00029794 rank 7
2023-02-27 14:07:50,689 DEBUG TRAIN Batch 45/200 loss 15.399797 loss_att 18.871449 loss_ctc 38.301243 loss_rnnt 11.532606 hw_loss 0.223756 lr 0.00029794 rank 4
2023-02-27 14:07:50,691 DEBUG TRAIN Batch 45/200 loss 9.733235 loss_att 12.385042 loss_ctc 20.149485 loss_rnnt 7.730009 hw_loss 0.157558 lr 0.00029794 rank 0
2023-02-27 14:07:50,691 DEBUG TRAIN Batch 45/200 loss 2.019647 loss_att 4.098645 loss_ctc 2.749861 loss_rnnt 1.369597 hw_loss 0.256666 lr 0.00029794 rank 6
2023-02-27 14:07:50,692 DEBUG TRAIN Batch 45/200 loss 6.605990 loss_att 8.834560 loss_ctc 15.135715 loss_rnnt 4.928086 hw_loss 0.177924 lr 0.00029794 rank 1
2023-02-27 14:07:50,693 DEBUG TRAIN Batch 45/200 loss 4.923883 loss_att 8.537262 loss_ctc 8.307108 loss_rnnt 3.674651 hw_loss 0.141487 lr 0.00029794 rank 2
2023-02-27 14:08:29,501 DEBUG TRAIN Batch 45/300 loss 7.209382 loss_att 9.552517 loss_ctc 9.981928 loss_rnnt 6.277654 hw_loss 0.175177 lr 0.00029793 rank 7
2023-02-27 14:08:29,504 DEBUG TRAIN Batch 45/300 loss 5.946139 loss_att 7.868053 loss_ctc 11.780539 loss_rnnt 4.686880 hw_loss 0.181793 lr 0.00029792 rank 6
2023-02-27 14:08:29,513 DEBUG TRAIN Batch 45/300 loss 6.513767 loss_att 9.771948 loss_ctc 9.984458 loss_rnnt 5.256961 hw_loss 0.267019 lr 0.00029793 rank 0
2023-02-27 14:08:29,513 DEBUG TRAIN Batch 45/300 loss 4.657462 loss_att 6.883359 loss_ctc 7.887262 loss_rnnt 3.624436 hw_loss 0.294762 lr 0.00029792 rank 5
2023-02-27 14:08:29,517 DEBUG TRAIN Batch 45/300 loss 10.209510 loss_att 11.771844 loss_ctc 15.411840 loss_rnnt 9.075897 hw_loss 0.239064 lr 0.00029792 rank 3
2023-02-27 14:08:29,519 DEBUG TRAIN Batch 45/300 loss 4.211553 loss_att 6.624769 loss_ctc 7.049988 loss_rnnt 3.197685 hw_loss 0.286439 lr 0.00029793 rank 4
2023-02-27 14:08:29,519 DEBUG TRAIN Batch 45/300 loss 2.643470 loss_att 5.662096 loss_ctc 5.651390 loss_rnnt 1.553687 hw_loss 0.159379 lr 0.00029793 rank 1
2023-02-27 14:08:29,556 DEBUG TRAIN Batch 45/300 loss 5.013890 loss_att 8.842511 loss_ctc 10.290380 loss_rnnt 3.443820 hw_loss 0.189026 lr 0.00029793 rank 2
2023-02-27 14:09:34,612 DEBUG TRAIN Batch 45/400 loss 2.063149 loss_att 4.698856 loss_ctc 2.576288 loss_rnnt 1.367731 hw_loss 0.187235 lr 0.00029791 rank 7
2023-02-27 14:09:34,621 DEBUG TRAIN Batch 45/400 loss 4.724287 loss_att 7.709778 loss_ctc 7.424283 loss_rnnt 3.665958 hw_loss 0.189809 lr 0.00029791 rank 1
2023-02-27 14:09:34,627 DEBUG TRAIN Batch 45/400 loss 4.254543 loss_att 6.198874 loss_ctc 5.802923 loss_rnnt 3.423210 hw_loss 0.442531 lr 0.00029791 rank 4
2023-02-27 14:09:34,632 DEBUG TRAIN Batch 45/400 loss 13.244192 loss_att 16.375452 loss_ctc 21.356129 loss_rnnt 11.379135 hw_loss 0.294776 lr 0.00029791 rank 3
2023-02-27 14:09:34,633 DEBUG TRAIN Batch 45/400 loss 10.476752 loss_att 13.435071 loss_ctc 15.509035 loss_rnnt 9.062737 hw_loss 0.283840 lr 0.00029791 rank 6
2023-02-27 14:09:34,635 DEBUG TRAIN Batch 45/400 loss 6.745343 loss_att 8.425940 loss_ctc 7.969959 loss_rnnt 6.130124 hw_loss 0.217156 lr 0.00029791 rank 2
2023-02-27 14:09:34,635 DEBUG TRAIN Batch 45/400 loss 10.506188 loss_att 12.785177 loss_ctc 12.734719 loss_rnnt 9.643076 hw_loss 0.206581 lr 0.00029792 rank 0
2023-02-27 14:09:34,636 DEBUG TRAIN Batch 45/400 loss 7.798366 loss_att 11.070225 loss_ctc 9.718710 loss_rnnt 6.798432 hw_loss 0.167842 lr 0.00029791 rank 5
2023-02-27 14:10:12,805 DEBUG TRAIN Batch 45/500 loss 9.078709 loss_att 14.785674 loss_ctc 18.239838 loss_rnnt 6.649940 hw_loss 0.123545 lr 0.00029790 rank 3
2023-02-27 14:10:12,809 DEBUG TRAIN Batch 45/500 loss 7.704350 loss_att 10.207265 loss_ctc 14.874832 loss_rnnt 6.115432 hw_loss 0.248010 lr 0.00029790 rank 6
2023-02-27 14:10:12,819 DEBUG TRAIN Batch 45/500 loss 12.866879 loss_att 14.975372 loss_ctc 19.928413 loss_rnnt 11.453794 hw_loss 0.093467 lr 0.00029790 rank 4
2023-02-27 14:10:12,821 DEBUG TRAIN Batch 45/500 loss 8.004302 loss_att 10.902537 loss_ctc 14.419432 loss_rnnt 6.373166 hw_loss 0.367758 lr 0.00029790 rank 7
2023-02-27 14:10:12,825 DEBUG TRAIN Batch 45/500 loss 8.520949 loss_att 10.565340 loss_ctc 11.970434 loss_rnnt 7.546348 hw_loss 0.198362 lr 0.00029790 rank 5
2023-02-27 14:10:12,827 DEBUG TRAIN Batch 45/500 loss 6.696609 loss_att 9.410419 loss_ctc 13.415601 loss_rnnt 5.165812 hw_loss 0.172818 lr 0.00029790 rank 0
2023-02-27 14:10:12,831 DEBUG TRAIN Batch 45/500 loss 12.163449 loss_att 14.253199 loss_ctc 19.316370 loss_rnnt 10.609871 hw_loss 0.341074 lr 0.00029790 rank 1
2023-02-27 14:10:12,836 DEBUG TRAIN Batch 45/500 loss 8.612020 loss_att 10.492476 loss_ctc 12.518876 loss_rnnt 7.550387 hw_loss 0.308679 lr 0.00029790 rank 2
2023-02-27 14:10:51,818 DEBUG TRAIN Batch 45/600 loss 11.311328 loss_att 14.352400 loss_ctc 17.499283 loss_rnnt 9.703838 hw_loss 0.326650 lr 0.00029788 rank 3
2023-02-27 14:10:51,832 DEBUG TRAIN Batch 45/600 loss 4.380744 loss_att 5.710950 loss_ctc 8.989475 loss_rnnt 3.341237 hw_loss 0.298066 lr 0.00029788 rank 6
2023-02-27 14:10:51,834 DEBUG TRAIN Batch 45/600 loss 12.722671 loss_att 13.865669 loss_ctc 17.690353 loss_rnnt 11.633295 hw_loss 0.372034 lr 0.00029789 rank 5
2023-02-27 14:10:51,835 DEBUG TRAIN Batch 45/600 loss 6.045655 loss_att 6.743162 loss_ctc 8.275238 loss_rnnt 5.444446 hw_loss 0.308305 lr 0.00029789 rank 7
2023-02-27 14:10:51,835 DEBUG TRAIN Batch 45/600 loss 5.790740 loss_att 8.959992 loss_ctc 12.192625 loss_rnnt 4.164186 hw_loss 0.260848 lr 0.00029789 rank 0
2023-02-27 14:10:51,836 DEBUG TRAIN Batch 45/600 loss 10.138323 loss_att 12.978693 loss_ctc 16.367723 loss_rnnt 8.580608 hw_loss 0.298225 lr 0.00029789 rank 1
2023-02-27 14:10:51,841 DEBUG TRAIN Batch 45/600 loss 9.271215 loss_att 9.989899 loss_ctc 13.331942 loss_rnnt 8.403294 hw_loss 0.342664 lr 0.00029789 rank 4
2023-02-27 14:10:51,881 DEBUG TRAIN Batch 45/600 loss 8.660207 loss_att 8.729517 loss_ctc 14.096920 loss_rnnt 7.748518 hw_loss 0.324246 lr 0.00029789 rank 2
2023-02-27 14:11:52,588 DEBUG TRAIN Batch 45/700 loss 8.268130 loss_att 10.596430 loss_ctc 15.956564 loss_rnnt 6.614780 hw_loss 0.304811 lr 0.00029787 rank 3
2023-02-27 14:11:52,591 DEBUG TRAIN Batch 45/700 loss 6.485417 loss_att 12.050526 loss_ctc 15.471405 loss_rnnt 4.108730 hw_loss 0.122873 lr 0.00029787 rank 6
2023-02-27 14:11:52,593 DEBUG TRAIN Batch 45/700 loss 2.220644 loss_att 5.732479 loss_ctc 4.585014 loss_rnnt 1.113021 hw_loss 0.168762 lr 0.00029787 rank 4
2023-02-27 14:11:52,594 DEBUG TRAIN Batch 45/700 loss 7.825022 loss_att 9.427511 loss_ctc 13.160975 loss_rnnt 6.663330 hw_loss 0.243251 lr 0.00029787 rank 7
2023-02-27 14:11:52,598 DEBUG TRAIN Batch 45/700 loss 6.371687 loss_att 11.461114 loss_ctc 10.328619 loss_rnnt 4.702763 hw_loss 0.231465 lr 0.00029787 rank 2
2023-02-27 14:11:52,600 DEBUG TRAIN Batch 45/700 loss 11.539432 loss_att 15.588313 loss_ctc 19.661652 loss_rnnt 9.515568 hw_loss 0.245857 lr 0.00029787 rank 1
2023-02-27 14:11:52,602 DEBUG TRAIN Batch 45/700 loss 3.671159 loss_att 7.238715 loss_ctc 4.465805 loss_rnnt 2.775521 hw_loss 0.142826 lr 0.00029787 rank 5
2023-02-27 14:11:52,633 DEBUG TRAIN Batch 45/700 loss 7.484903 loss_att 11.791819 loss_ctc 12.071032 loss_rnnt 5.821653 hw_loss 0.356969 lr 0.00029788 rank 0
2023-02-27 14:12:37,885 DEBUG TRAIN Batch 45/800 loss 5.712002 loss_att 9.362507 loss_ctc 10.641570 loss_rnnt 4.182332 hw_loss 0.266799 lr 0.00029786 rank 3
2023-02-27 14:12:37,892 DEBUG TRAIN Batch 45/800 loss 10.464700 loss_att 13.469471 loss_ctc 15.021370 loss_rnnt 9.152071 hw_loss 0.195222 lr 0.00029786 rank 7
2023-02-27 14:12:37,902 DEBUG TRAIN Batch 45/800 loss 4.658816 loss_att 7.288818 loss_ctc 7.107908 loss_rnnt 3.680176 hw_loss 0.236425 lr 0.00029786 rank 2
2023-02-27 14:12:37,902 DEBUG TRAIN Batch 45/800 loss 11.705220 loss_att 17.469049 loss_ctc 20.770273 loss_rnnt 9.241093 hw_loss 0.192539 lr 0.00029786 rank 0
2023-02-27 14:12:37,904 DEBUG TRAIN Batch 45/800 loss 8.493950 loss_att 11.992579 loss_ctc 13.177478 loss_rnnt 7.068069 hw_loss 0.190658 lr 0.00029786 rank 4
2023-02-27 14:12:37,908 DEBUG TRAIN Batch 45/800 loss 2.268959 loss_att 4.348852 loss_ctc 4.430954 loss_rnnt 1.403709 hw_loss 0.301886 lr 0.00029786 rank 6
2023-02-27 14:12:37,908 DEBUG TRAIN Batch 45/800 loss 4.010771 loss_att 6.992272 loss_ctc 7.183969 loss_rnnt 2.860948 hw_loss 0.244557 lr 0.00029786 rank 1
2023-02-27 14:12:37,958 DEBUG TRAIN Batch 45/800 loss 2.887284 loss_att 7.170467 loss_ctc 4.305122 loss_rnnt 1.723674 hw_loss 0.221114 lr 0.00029786 rank 5
2023-02-27 14:13:16,094 DEBUG TRAIN Batch 45/900 loss 11.213728 loss_att 13.307674 loss_ctc 13.615450 loss_rnnt 10.313763 hw_loss 0.301775 lr 0.00029785 rank 2
2023-02-27 14:13:16,108 DEBUG TRAIN Batch 45/900 loss 4.705566 loss_att 7.941744 loss_ctc 7.425891 loss_rnnt 3.557679 hw_loss 0.258641 lr 0.00029784 rank 3
2023-02-27 14:13:16,109 DEBUG TRAIN Batch 45/900 loss 10.702497 loss_att 15.193844 loss_ctc 20.854269 loss_rnnt 8.331098 hw_loss 0.224174 lr 0.00029784 rank 6
2023-02-27 14:13:16,109 DEBUG TRAIN Batch 45/900 loss 6.091711 loss_att 10.104965 loss_ctc 8.296892 loss_rnnt 4.835788 hw_loss 0.298589 lr 0.00029785 rank 4
2023-02-27 14:13:16,110 DEBUG TRAIN Batch 45/900 loss 8.398390 loss_att 10.216961 loss_ctc 21.878080 loss_rnnt 6.185302 hw_loss 0.097652 lr 0.00029785 rank 7
2023-02-27 14:13:16,120 DEBUG TRAIN Batch 45/900 loss 2.213476 loss_att 5.530880 loss_ctc 3.361819 loss_rnnt 1.260415 hw_loss 0.255878 lr 0.00029785 rank 5
2023-02-27 14:13:16,121 DEBUG TRAIN Batch 45/900 loss 3.555081 loss_att 7.077836 loss_ctc 7.967844 loss_rnnt 2.098342 hw_loss 0.307161 lr 0.00029785 rank 0
2023-02-27 14:13:16,129 DEBUG TRAIN Batch 45/900 loss 4.383891 loss_att 6.910460 loss_ctc 6.572650 loss_rnnt 3.491441 hw_loss 0.178692 lr 0.00029785 rank 1
2023-02-27 14:13:54,440 DEBUG TRAIN Batch 45/1000 loss 4.156043 loss_att 6.776639 loss_ctc 8.385795 loss_rnnt 3.013944 hw_loss 0.101276 lr 0.00029783 rank 4
2023-02-27 14:13:54,442 DEBUG TRAIN Batch 45/1000 loss 8.187810 loss_att 10.996288 loss_ctc 12.590400 loss_rnnt 6.893435 hw_loss 0.273128 lr 0.00029784 rank 0
2023-02-27 14:13:54,450 DEBUG TRAIN Batch 45/1000 loss 9.067106 loss_att 10.308205 loss_ctc 11.804407 loss_rnnt 8.418196 hw_loss 0.066970 lr 0.00029783 rank 5
2023-02-27 14:13:54,461 DEBUG TRAIN Batch 45/1000 loss 6.720096 loss_att 10.333868 loss_ctc 13.289345 loss_rnnt 5.004410 hw_loss 0.219434 lr 0.00029783 rank 1
2023-02-27 14:13:54,464 DEBUG TRAIN Batch 45/1000 loss 7.731907 loss_att 7.733753 loss_ctc 11.703860 loss_rnnt 6.995598 hw_loss 0.386898 lr 0.00029783 rank 7
2023-02-27 14:13:54,470 DEBUG TRAIN Batch 45/1000 loss 7.628840 loss_att 11.500275 loss_ctc 10.964783 loss_rnnt 6.323950 hw_loss 0.160894 lr 0.00029783 rank 6
2023-02-27 14:13:54,471 DEBUG TRAIN Batch 45/1000 loss 10.324789 loss_att 16.113022 loss_ctc 21.352062 loss_rnnt 7.578881 hw_loss 0.221172 lr 0.00029783 rank 3
2023-02-27 14:13:54,472 DEBUG TRAIN Batch 45/1000 loss 7.013719 loss_att 9.896601 loss_ctc 11.064688 loss_rnnt 5.793229 hw_loss 0.194596 lr 0.00029783 rank 2
2023-02-27 14:14:59,252 DEBUG TRAIN Batch 45/1100 loss 9.663998 loss_att 14.629052 loss_ctc 18.518257 loss_rnnt 7.318190 hw_loss 0.322928 lr 0.00029782 rank 7
2023-02-27 14:14:59,262 DEBUG TRAIN Batch 45/1100 loss 8.401314 loss_att 11.526903 loss_ctc 12.528897 loss_rnnt 7.128988 hw_loss 0.181619 lr 0.00029782 rank 3
2023-02-27 14:14:59,264 DEBUG TRAIN Batch 45/1100 loss 6.332829 loss_att 9.670095 loss_ctc 11.278324 loss_rnnt 4.893297 hw_loss 0.211274 lr 0.00029782 rank 0
2023-02-27 14:14:59,265 DEBUG TRAIN Batch 45/1100 loss 9.659041 loss_att 13.222448 loss_ctc 15.313053 loss_rnnt 8.038900 hw_loss 0.287984 lr 0.00029782 rank 5
2023-02-27 14:14:59,267 DEBUG TRAIN Batch 45/1100 loss 1.665554 loss_att 4.004637 loss_ctc 3.172987 loss_rnnt 0.834192 hw_loss 0.304788 lr 0.00029782 rank 1
2023-02-27 14:14:59,268 DEBUG TRAIN Batch 45/1100 loss 4.828146 loss_att 6.289923 loss_ctc 7.767430 loss_rnnt 4.038516 hw_loss 0.197568 lr 0.00029782 rank 4
2023-02-27 14:14:59,271 DEBUG TRAIN Batch 45/1100 loss 12.366105 loss_att 13.447317 loss_ctc 16.825203 loss_rnnt 11.413714 hw_loss 0.265504 lr 0.00029782 rank 6
2023-02-27 14:14:59,318 DEBUG TRAIN Batch 45/1100 loss 7.553015 loss_att 12.901331 loss_ctc 17.429531 loss_rnnt 5.060651 hw_loss 0.198433 lr 0.00029782 rank 2
2023-02-27 14:15:38,106 DEBUG TRAIN Batch 45/1200 loss 10.128545 loss_att 14.364788 loss_ctc 26.205648 loss_rnnt 7.022586 hw_loss 0.215806 lr 0.00029781 rank 7
2023-02-27 14:15:38,107 DEBUG TRAIN Batch 45/1200 loss 2.034235 loss_att 4.345251 loss_ctc 3.985931 loss_rnnt 1.210127 hw_loss 0.190648 lr 0.00029780 rank 3
2023-02-27 14:15:38,109 DEBUG TRAIN Batch 45/1200 loss 10.424541 loss_att 10.561891 loss_ctc 17.839405 loss_rnnt 9.297159 hw_loss 0.208619 lr 0.00029781 rank 2
2023-02-27 14:15:38,109 DEBUG TRAIN Batch 45/1200 loss 4.938299 loss_att 5.868917 loss_ctc 8.098319 loss_rnnt 4.186820 hw_loss 0.270035 lr 0.00029781 rank 6
2023-02-27 14:15:38,110 DEBUG TRAIN Batch 45/1200 loss 7.335954 loss_att 9.843472 loss_ctc 12.841511 loss_rnnt 5.986690 hw_loss 0.213161 lr 0.00029781 rank 0
2023-02-27 14:15:38,115 DEBUG TRAIN Batch 45/1200 loss 13.907221 loss_att 15.901327 loss_ctc 22.800011 loss_rnnt 12.223127 hw_loss 0.186691 lr 0.00029781 rank 4
2023-02-27 14:15:38,118 DEBUG TRAIN Batch 45/1200 loss 6.621937 loss_att 8.351365 loss_ctc 8.928271 loss_rnnt 5.876778 hw_loss 0.172053 lr 0.00029781 rank 1
2023-02-27 14:15:38,161 DEBUG TRAIN Batch 45/1200 loss 12.761537 loss_att 15.600541 loss_ctc 17.418833 loss_rnnt 11.425371 hw_loss 0.276359 lr 0.00029781 rank 5
2023-02-27 14:16:16,848 DEBUG TRAIN Batch 45/1300 loss 2.281242 loss_att 5.615953 loss_ctc 4.047927 loss_rnnt 1.259925 hw_loss 0.222781 lr 0.00029779 rank 7
2023-02-27 14:16:16,851 DEBUG TRAIN Batch 45/1300 loss 9.945613 loss_att 14.809570 loss_ctc 13.161339 loss_rnnt 8.481142 hw_loss 0.117968 lr 0.00029779 rank 4
2023-02-27 14:16:16,851 DEBUG TRAIN Batch 45/1300 loss 4.562474 loss_att 8.704968 loss_ctc 5.124025 loss_rnnt 3.581679 hw_loss 0.145168 lr 0.00029779 rank 6
2023-02-27 14:16:16,853 DEBUG TRAIN Batch 45/1300 loss 3.896013 loss_att 6.887786 loss_ctc 5.987723 loss_rnnt 2.818792 hw_loss 0.374946 lr 0.00029779 rank 3
2023-02-27 14:16:16,854 DEBUG TRAIN Batch 45/1300 loss 9.975008 loss_att 13.038944 loss_ctc 19.773947 loss_rnnt 7.948689 hw_loss 0.200638 lr 0.00029780 rank 1
2023-02-27 14:16:16,857 DEBUG TRAIN Batch 45/1300 loss 9.473832 loss_att 11.255128 loss_ctc 15.654891 loss_rnnt 8.070493 hw_loss 0.418010 lr 0.00029780 rank 0
2023-02-27 14:16:16,858 DEBUG TRAIN Batch 45/1300 loss 5.858980 loss_att 6.624952 loss_ctc 11.888416 loss_rnnt 4.626865 hw_loss 0.515615 lr 0.00029779 rank 5
2023-02-27 14:16:16,858 DEBUG TRAIN Batch 45/1300 loss 6.074623 loss_att 10.600023 loss_ctc 13.390189 loss_rnnt 4.023185 hw_loss 0.320530 lr 0.00029779 rank 2
2023-02-27 14:16:56,461 DEBUG TRAIN Batch 45/1400 loss 5.597513 loss_att 8.678675 loss_ctc 10.003565 loss_rnnt 4.295533 hw_loss 0.184265 lr 0.00029778 rank 3
2023-02-27 14:16:56,468 DEBUG TRAIN Batch 45/1400 loss 4.947528 loss_att 7.561031 loss_ctc 5.812631 loss_rnnt 4.111795 hw_loss 0.370660 lr 0.00029778 rank 0
2023-02-27 14:16:56,469 DEBUG TRAIN Batch 45/1400 loss 2.629914 loss_att 4.352046 loss_ctc 4.127236 loss_rnnt 1.961923 hw_loss 0.232351 lr 0.00029778 rank 6
2023-02-27 14:16:56,472 DEBUG TRAIN Batch 45/1400 loss 10.242088 loss_att 16.487148 loss_ctc 25.914433 loss_rnnt 6.786796 hw_loss 0.218691 lr 0.00029778 rank 1
2023-02-27 14:16:56,474 DEBUG TRAIN Batch 45/1400 loss 10.049518 loss_att 11.890559 loss_ctc 19.046459 loss_rnnt 8.411423 hw_loss 0.131804 lr 0.00029778 rank 2
2023-02-27 14:16:56,474 DEBUG TRAIN Batch 45/1400 loss 3.258028 loss_att 6.712588 loss_ctc 6.734144 loss_rnnt 2.005446 hw_loss 0.184102 lr 0.00029778 rank 5
2023-02-27 14:16:56,486 DEBUG TRAIN Batch 45/1400 loss 7.082247 loss_att 10.078238 loss_ctc 12.778692 loss_rnnt 5.526323 hw_loss 0.369748 lr 0.00029778 rank 4
2023-02-27 14:16:56,491 DEBUG TRAIN Batch 45/1400 loss 5.506998 loss_att 8.633523 loss_ctc 7.859081 loss_rnnt 4.424814 hw_loss 0.268628 lr 0.00029778 rank 7
2023-02-27 14:18:02,100 DEBUG TRAIN Batch 45/1500 loss 11.761451 loss_att 15.237217 loss_ctc 18.364044 loss_rnnt 9.958732 hw_loss 0.426038 lr 0.00029777 rank 7
2023-02-27 14:18:02,110 DEBUG TRAIN Batch 45/1500 loss 7.275955 loss_att 8.637343 loss_ctc 11.344172 loss_rnnt 6.286444 hw_loss 0.327758 lr 0.00029777 rank 5
2023-02-27 14:18:02,112 DEBUG TRAIN Batch 45/1500 loss 3.411084 loss_att 6.106911 loss_ctc 5.426895 loss_rnnt 2.487686 hw_loss 0.216484 lr 0.00029777 rank 1
2023-02-27 14:18:02,112 DEBUG TRAIN Batch 45/1500 loss 13.629761 loss_att 15.570889 loss_ctc 23.883566 loss_rnnt 11.628155 hw_loss 0.461637 lr 0.00029776 rank 3
2023-02-27 14:18:02,113 DEBUG TRAIN Batch 45/1500 loss 1.648320 loss_att 4.511764 loss_ctc 5.119206 loss_rnnt 0.474637 hw_loss 0.259143 lr 0.00029777 rank 0
2023-02-27 14:18:02,117 DEBUG TRAIN Batch 45/1500 loss 3.377629 loss_att 8.102076 loss_ctc 8.762567 loss_rnnt 1.574421 hw_loss 0.263114 lr 0.00029777 rank 6
2023-02-27 14:18:02,121 DEBUG TRAIN Batch 45/1500 loss 6.147923 loss_att 9.034751 loss_ctc 10.977945 loss_rnnt 4.800647 hw_loss 0.236076 lr 0.00029777 rank 4
2023-02-27 14:18:02,124 DEBUG TRAIN Batch 45/1500 loss 9.111616 loss_att 12.711935 loss_ctc 16.418854 loss_rnnt 7.258122 hw_loss 0.298371 lr 0.00029777 rank 2
2023-02-27 14:18:41,376 DEBUG TRAIN Batch 45/1600 loss 3.733356 loss_att 5.855439 loss_ctc 5.234413 loss_rnnt 2.965234 hw_loss 0.269183 lr 0.00029776 rank 0
2023-02-27 14:18:41,387 DEBUG TRAIN Batch 45/1600 loss 9.940475 loss_att 13.970786 loss_ctc 20.607552 loss_rnnt 7.646953 hw_loss 0.122218 lr 0.00029775 rank 3
2023-02-27 14:18:41,393 DEBUG TRAIN Batch 45/1600 loss 8.149987 loss_att 11.491741 loss_ctc 14.338770 loss_rnnt 6.483975 hw_loss 0.323418 lr 0.00029775 rank 2
2023-02-27 14:18:41,393 DEBUG TRAIN Batch 45/1600 loss 14.337217 loss_att 17.627064 loss_ctc 22.703876 loss_rnnt 12.455374 hw_loss 0.203099 lr 0.00029775 rank 6
2023-02-27 14:18:41,393 DEBUG TRAIN Batch 45/1600 loss 5.271162 loss_att 7.258718 loss_ctc 7.649440 loss_rnnt 4.414364 hw_loss 0.266592 lr 0.00029775 rank 7
2023-02-27 14:18:41,394 DEBUG TRAIN Batch 45/1600 loss 13.917488 loss_att 16.089249 loss_ctc 20.024998 loss_rnnt 12.596474 hw_loss 0.135614 lr 0.00029775 rank 4
2023-02-27 14:18:41,396 DEBUG TRAIN Batch 45/1600 loss 4.876731 loss_att 7.171952 loss_ctc 9.384777 loss_rnnt 3.636691 hw_loss 0.337356 lr 0.00029776 rank 1
2023-02-27 14:18:41,442 DEBUG TRAIN Batch 45/1600 loss 3.911579 loss_att 5.854276 loss_ctc 9.573415 loss_rnnt 2.630523 hw_loss 0.258009 lr 0.00029775 rank 5
2023-02-27 14:19:21,139 DEBUG TRAIN Batch 45/1700 loss 3.671696 loss_att 7.429539 loss_ctc 7.089486 loss_rnnt 2.370862 hw_loss 0.175423 lr 0.00029774 rank 7
2023-02-27 14:19:21,141 DEBUG TRAIN Batch 45/1700 loss 12.364398 loss_att 13.742522 loss_ctc 17.713242 loss_rnnt 11.281510 hw_loss 0.176405 lr 0.00029774 rank 4
2023-02-27 14:19:21,144 DEBUG TRAIN Batch 45/1700 loss 4.560518 loss_att 8.161039 loss_ctc 9.695118 loss_rnnt 2.978110 hw_loss 0.333171 lr 0.00029774 rank 1
2023-02-27 14:19:21,148 DEBUG TRAIN Batch 45/1700 loss 8.223310 loss_att 12.923268 loss_ctc 14.243047 loss_rnnt 6.361805 hw_loss 0.222905 lr 0.00029774 rank 3
2023-02-27 14:19:21,149 DEBUG TRAIN Batch 45/1700 loss 14.494222 loss_att 14.339468 loss_ctc 17.514469 loss_rnnt 14.042032 hw_loss 0.150826 lr 0.00029774 rank 2
2023-02-27 14:19:21,152 DEBUG TRAIN Batch 45/1700 loss 3.260411 loss_att 6.260337 loss_ctc 4.945499 loss_rnnt 2.343731 hw_loss 0.172530 lr 0.00029774 rank 6
2023-02-27 14:19:21,155 DEBUG TRAIN Batch 45/1700 loss 9.698737 loss_att 11.447108 loss_ctc 12.764777 loss_rnnt 8.855545 hw_loss 0.158836 lr 0.00029774 rank 0
2023-02-27 14:19:21,198 DEBUG TRAIN Batch 45/1700 loss 5.665061 loss_att 8.127527 loss_ctc 11.071718 loss_rnnt 4.257392 hw_loss 0.364289 lr 0.00029774 rank 5
2023-02-27 14:20:27,552 DEBUG TRAIN Batch 45/1800 loss 3.718229 loss_att 5.960437 loss_ctc 6.364700 loss_rnnt 2.750284 hw_loss 0.312451 lr 0.00029772 rank 3
2023-02-27 14:20:27,556 DEBUG TRAIN Batch 45/1800 loss 8.440758 loss_att 12.706817 loss_ctc 14.386290 loss_rnnt 6.728369 hw_loss 0.124574 lr 0.00029773 rank 5
2023-02-27 14:20:27,556 DEBUG TRAIN Batch 45/1800 loss 5.190669 loss_att 7.221888 loss_ctc 12.330518 loss_rnnt 3.710288 hw_loss 0.229043 lr 0.00029773 rank 4
2023-02-27 14:20:27,559 DEBUG TRAIN Batch 45/1800 loss 5.371189 loss_att 6.882803 loss_ctc 8.522249 loss_rnnt 4.481657 hw_loss 0.313251 lr 0.00029773 rank 6
2023-02-27 14:20:27,559 DEBUG TRAIN Batch 45/1800 loss 10.422535 loss_att 14.347751 loss_ctc 15.480353 loss_rnnt 8.898918 hw_loss 0.120370 lr 0.00029773 rank 0
2023-02-27 14:20:27,560 DEBUG TRAIN Batch 45/1800 loss 9.547159 loss_att 12.011475 loss_ctc 12.351164 loss_rnnt 8.562952 hw_loss 0.220267 lr 0.00029773 rank 7
2023-02-27 14:20:27,560 DEBUG TRAIN Batch 45/1800 loss 9.673017 loss_att 13.506399 loss_ctc 15.091982 loss_rnnt 7.986821 hw_loss 0.369356 lr 0.00029773 rank 2
2023-02-27 14:20:27,566 DEBUG TRAIN Batch 45/1800 loss 6.645028 loss_att 9.132555 loss_ctc 12.015142 loss_rnnt 5.306280 hw_loss 0.234802 lr 0.00029773 rank 1
2023-02-27 14:21:06,351 DEBUG TRAIN Batch 45/1900 loss 13.245481 loss_att 16.457657 loss_ctc 26.339811 loss_rnnt 10.649814 hw_loss 0.388725 lr 0.00029771 rank 2
2023-02-27 14:21:06,354 DEBUG TRAIN Batch 45/1900 loss 3.118934 loss_att 6.612098 loss_ctc 7.390208 loss_rnnt 1.734963 hw_loss 0.217190 lr 0.00029772 rank 1
2023-02-27 14:21:06,359 DEBUG TRAIN Batch 45/1900 loss 3.534418 loss_att 5.928764 loss_ctc 7.455066 loss_rnnt 2.447514 hw_loss 0.159903 lr 0.00029772 rank 0
2023-02-27 14:21:06,361 DEBUG TRAIN Batch 45/1900 loss 9.770737 loss_att 12.520067 loss_ctc 18.390556 loss_rnnt 7.927414 hw_loss 0.270277 lr 0.00029771 rank 5
2023-02-27 14:21:06,364 DEBUG TRAIN Batch 45/1900 loss 7.644057 loss_att 8.334878 loss_ctc 11.564507 loss_rnnt 6.805673 hw_loss 0.332799 lr 0.00029771 rank 4
2023-02-27 14:21:06,376 DEBUG TRAIN Batch 45/1900 loss 8.733874 loss_att 9.544569 loss_ctc 13.484874 loss_rnnt 7.773217 hw_loss 0.309470 lr 0.00029771 rank 7
2023-02-27 14:21:06,380 DEBUG TRAIN Batch 45/1900 loss 7.229609 loss_att 8.364412 loss_ctc 11.516870 loss_rnnt 6.228570 hw_loss 0.379579 lr 0.00029771 rank 6
2023-02-27 14:21:06,383 DEBUG TRAIN Batch 45/1900 loss 7.519978 loss_att 7.627864 loss_ctc 12.184558 loss_rnnt 6.714589 hw_loss 0.303501 lr 0.00029771 rank 3
2023-02-27 14:21:44,244 DEBUG TRAIN Batch 45/2000 loss 10.145269 loss_att 12.214937 loss_ctc 12.570671 loss_rnnt 9.300056 hw_loss 0.202297 lr 0.00029770 rank 7
2023-02-27 14:21:44,256 DEBUG TRAIN Batch 45/2000 loss 6.975971 loss_att 9.389069 loss_ctc 13.283335 loss_rnnt 5.485212 hw_loss 0.313421 lr 0.00029770 rank 0
2023-02-27 14:21:44,256 DEBUG TRAIN Batch 45/2000 loss 1.971968 loss_att 4.788353 loss_ctc 3.584453 loss_rnnt 1.026964 hw_loss 0.312616 lr 0.00029770 rank 3
2023-02-27 14:21:44,258 DEBUG TRAIN Batch 45/2000 loss 2.292766 loss_att 5.086277 loss_ctc 6.126199 loss_rnnt 1.140210 hw_loss 0.155118 lr 0.00029770 rank 6
2023-02-27 14:21:44,258 DEBUG TRAIN Batch 45/2000 loss 8.631132 loss_att 11.502686 loss_ctc 14.884207 loss_rnnt 7.159940 hw_loss 0.118384 lr 0.00029770 rank 2
2023-02-27 14:21:44,259 DEBUG TRAIN Batch 45/2000 loss 8.152542 loss_att 9.751964 loss_ctc 15.532718 loss_rnnt 6.790689 hw_loss 0.108646 lr 0.00029770 rank 4
2023-02-27 14:21:44,264 DEBUG TRAIN Batch 45/2000 loss 1.799924 loss_att 3.912520 loss_ctc 4.361732 loss_rnnt 0.912441 hw_loss 0.231355 lr 0.00029770 rank 1
2023-02-27 14:21:44,278 DEBUG TRAIN Batch 45/2000 loss 6.335684 loss_att 11.629814 loss_ctc 13.399093 loss_rnnt 4.231052 hw_loss 0.195034 lr 0.00029770 rank 5
2023-02-27 14:22:23,884 DEBUG TRAIN Batch 45/2100 loss 3.429214 loss_att 8.223166 loss_ctc 5.589972 loss_rnnt 2.068246 hw_loss 0.213895 lr 0.00029769 rank 4
2023-02-27 14:22:23,889 DEBUG TRAIN Batch 45/2100 loss 11.788419 loss_att 12.291996 loss_ctc 19.512465 loss_rnnt 10.442755 hw_loss 0.403265 lr 0.00029769 rank 0
2023-02-27 14:22:23,892 DEBUG TRAIN Batch 45/2100 loss 2.085067 loss_att 5.757480 loss_ctc 5.919524 loss_rnnt 0.718679 hw_loss 0.226208 lr 0.00029768 rank 3
2023-02-27 14:22:23,893 DEBUG TRAIN Batch 45/2100 loss 4.539409 loss_att 7.656212 loss_ctc 8.638207 loss_rnnt 3.242757 hw_loss 0.237721 lr 0.00029769 rank 6
2023-02-27 14:22:23,894 DEBUG TRAIN Batch 45/2100 loss 7.076328 loss_att 11.072561 loss_ctc 11.771605 loss_rnnt 5.499040 hw_loss 0.285008 lr 0.00029769 rank 2
2023-02-27 14:22:23,907 DEBUG TRAIN Batch 45/2100 loss 3.451357 loss_att 7.047667 loss_ctc 8.633024 loss_rnnt 1.937072 hw_loss 0.195252 lr 0.00029769 rank 7
2023-02-27 14:22:23,924 DEBUG TRAIN Batch 45/2100 loss 3.694867 loss_att 7.335414 loss_ctc 7.980033 loss_rnnt 2.245122 hw_loss 0.281777 lr 0.00029769 rank 5
2023-02-27 14:22:23,925 DEBUG TRAIN Batch 45/2100 loss 9.480977 loss_att 12.285149 loss_ctc 16.851908 loss_rnnt 7.816168 hw_loss 0.227220 lr 0.00029769 rank 1
2023-02-27 14:23:28,888 DEBUG TRAIN Batch 45/2200 loss 5.150919 loss_att 9.214961 loss_ctc 10.284285 loss_rnnt 3.538934 hw_loss 0.215115 lr 0.00029767 rank 4
2023-02-27 14:23:28,894 DEBUG TRAIN Batch 45/2200 loss 8.378577 loss_att 12.448937 loss_ctc 14.793638 loss_rnnt 6.551481 hw_loss 0.295654 lr 0.00029768 rank 1
2023-02-27 14:23:28,896 DEBUG TRAIN Batch 45/2200 loss 5.954427 loss_att 7.677601 loss_ctc 7.835205 loss_rnnt 5.197201 hw_loss 0.303413 lr 0.00029768 rank 0
2023-02-27 14:23:28,904 DEBUG TRAIN Batch 45/2200 loss 9.404064 loss_att 10.901048 loss_ctc 15.278399 loss_rnnt 8.153143 hw_loss 0.315523 lr 0.00029767 rank 6
2023-02-27 14:23:28,906 DEBUG TRAIN Batch 45/2200 loss 5.703984 loss_att 7.481503 loss_ctc 9.743464 loss_rnnt 4.668437 hw_loss 0.265211 lr 0.00029767 rank 3
2023-02-27 14:23:28,935 DEBUG TRAIN Batch 45/2200 loss 15.132385 loss_att 18.677170 loss_ctc 28.817791 loss_rnnt 12.444759 hw_loss 0.288650 lr 0.00029767 rank 7
2023-02-27 14:23:28,944 DEBUG TRAIN Batch 45/2200 loss 5.635599 loss_att 7.834634 loss_ctc 11.032076 loss_rnnt 4.336953 hw_loss 0.261203 lr 0.00029767 rank 2
2023-02-27 14:23:28,950 DEBUG TRAIN Batch 45/2200 loss 6.550128 loss_att 8.817211 loss_ctc 9.461772 loss_rnnt 5.592011 hw_loss 0.218400 lr 0.00029767 rank 5
2023-02-27 14:24:07,572 DEBUG TRAIN Batch 45/2300 loss 10.415475 loss_att 12.232355 loss_ctc 15.218170 loss_rnnt 9.257090 hw_loss 0.289970 lr 0.00029766 rank 3
2023-02-27 14:24:07,572 DEBUG TRAIN Batch 45/2300 loss 9.002587 loss_att 11.066747 loss_ctc 11.877855 loss_rnnt 8.053668 hw_loss 0.286349 lr 0.00029766 rank 5
2023-02-27 14:24:07,573 DEBUG TRAIN Batch 45/2300 loss 7.043213 loss_att 10.655251 loss_ctc 13.984447 loss_rnnt 5.285225 hw_loss 0.206406 lr 0.00029766 rank 0
2023-02-27 14:24:07,574 DEBUG TRAIN Batch 45/2300 loss 6.845054 loss_att 9.665121 loss_ctc 13.453553 loss_rnnt 5.225394 hw_loss 0.327211 lr 0.00029766 rank 7
2023-02-27 14:24:07,578 DEBUG TRAIN Batch 45/2300 loss 7.308854 loss_att 12.796968 loss_ctc 15.533587 loss_rnnt 4.945211 hw_loss 0.317603 lr 0.00029766 rank 2
2023-02-27 14:24:07,577 DEBUG TRAIN Batch 45/2300 loss 5.341801 loss_att 9.993845 loss_ctc 9.611837 loss_rnnt 3.754117 hw_loss 0.164882 lr 0.00029766 rank 6
2023-02-27 14:24:07,579 DEBUG TRAIN Batch 45/2300 loss 4.382829 loss_att 6.468697 loss_ctc 8.000065 loss_rnnt 3.435130 hw_loss 0.090426 lr 0.00029766 rank 4
2023-02-27 14:24:07,628 DEBUG TRAIN Batch 45/2300 loss 8.401705 loss_att 11.592197 loss_ctc 13.286816 loss_rnnt 6.988269 hw_loss 0.232478 lr 0.00029766 rank 1
2023-02-27 14:24:46,140 DEBUG TRAIN Batch 45/2400 loss 5.119543 loss_att 8.052224 loss_ctc 13.509403 loss_rnnt 3.307365 hw_loss 0.200614 lr 0.00029765 rank 7
2023-02-27 14:24:46,145 DEBUG TRAIN Batch 45/2400 loss 8.165332 loss_att 10.839744 loss_ctc 12.670674 loss_rnnt 6.872971 hw_loss 0.293936 lr 0.00029765 rank 1
2023-02-27 14:24:46,148 DEBUG TRAIN Batch 45/2400 loss 2.457282 loss_att 4.322596 loss_ctc 4.624329 loss_rnnt 1.543038 hw_loss 0.472953 lr 0.00029765 rank 2
2023-02-27 14:24:46,151 DEBUG TRAIN Batch 45/2400 loss 11.936954 loss_att 16.947567 loss_ctc 20.422129 loss_rnnt 9.702586 hw_loss 0.189166 lr 0.00029765 rank 0
2023-02-27 14:24:46,152 DEBUG TRAIN Batch 45/2400 loss 7.973683 loss_att 10.891963 loss_ctc 17.012802 loss_rnnt 6.052380 hw_loss 0.248309 lr 0.00029765 rank 6
2023-02-27 14:24:46,152 DEBUG TRAIN Batch 45/2400 loss 5.705070 loss_att 6.760823 loss_ctc 6.064823 loss_rnnt 5.254025 hw_loss 0.359862 lr 0.00029765 rank 3
2023-02-27 14:24:46,187 DEBUG TRAIN Batch 45/2400 loss 9.439253 loss_att 11.378428 loss_ctc 14.546936 loss_rnnt 8.211185 hw_loss 0.298513 lr 0.00029765 rank 5
2023-02-27 14:24:46,195 DEBUG TRAIN Batch 45/2400 loss 1.946751 loss_att 4.205090 loss_ctc 3.434607 loss_rnnt 1.294255 hw_loss 0.004589 lr 0.00029765 rank 4
2023-02-27 14:25:52,728 DEBUG TRAIN Batch 45/2500 loss 8.398523 loss_att 8.614937 loss_ctc 10.698848 loss_rnnt 7.962914 hw_loss 0.160532 lr 0.00029763 rank 3
2023-02-27 14:25:52,728 DEBUG TRAIN Batch 45/2500 loss 3.533678 loss_att 6.131530 loss_ctc 5.986587 loss_rnnt 2.598099 hw_loss 0.166789 lr 0.00029763 rank 6
2023-02-27 14:25:52,729 DEBUG TRAIN Batch 45/2500 loss 8.880635 loss_att 11.988278 loss_ctc 12.688410 loss_rnnt 7.607182 hw_loss 0.270414 lr 0.00029763 rank 5
2023-02-27 14:25:52,731 DEBUG TRAIN Batch 45/2500 loss 6.735805 loss_att 7.180656 loss_ctc 10.167940 loss_rnnt 5.985503 hw_loss 0.381961 lr 0.00029763 rank 2
2023-02-27 14:25:52,733 DEBUG TRAIN Batch 45/2500 loss 8.718196 loss_att 11.734548 loss_ctc 13.964946 loss_rnnt 7.332877 hw_loss 0.154652 lr 0.00029764 rank 0
2023-02-27 14:25:52,734 DEBUG TRAIN Batch 45/2500 loss 11.200001 loss_att 13.343567 loss_ctc 15.390292 loss_rnnt 10.020267 hw_loss 0.360591 lr 0.00029763 rank 7
2023-02-27 14:25:52,736 DEBUG TRAIN Batch 45/2500 loss 3.407667 loss_att 5.601768 loss_ctc 6.743742 loss_rnnt 2.363458 hw_loss 0.301084 lr 0.00029764 rank 1
2023-02-27 14:25:52,773 DEBUG TRAIN Batch 45/2500 loss 2.814201 loss_att 4.554462 loss_ctc 3.983901 loss_rnnt 2.172877 hw_loss 0.257459 lr 0.00029764 rank 4
2023-02-27 14:26:31,081 DEBUG TRAIN Batch 45/2600 loss 12.889969 loss_att 19.657591 loss_ctc 20.614521 loss_rnnt 10.392134 hw_loss 0.214442 lr 0.00029762 rank 2
2023-02-27 14:26:31,086 DEBUG TRAIN Batch 45/2600 loss 1.395057 loss_att 3.364747 loss_ctc 2.059587 loss_rnnt 0.832228 hw_loss 0.150539 lr 0.00029762 rank 4
2023-02-27 14:26:31,100 DEBUG TRAIN Batch 45/2600 loss 3.524490 loss_att 10.756115 loss_ctc 7.376076 loss_rnnt 1.400595 hw_loss 0.307548 lr 0.00029762 rank 0
2023-02-27 14:26:31,102 DEBUG TRAIN Batch 45/2600 loss 2.139499 loss_att 4.823173 loss_ctc 3.382972 loss_rnnt 1.362057 hw_loss 0.140459 lr 0.00029762 rank 3
2023-02-27 14:26:31,102 DEBUG TRAIN Batch 45/2600 loss 4.666728 loss_att 6.611343 loss_ctc 7.823167 loss_rnnt 3.723930 hw_loss 0.249404 lr 0.00029762 rank 6
2023-02-27 14:26:31,109 DEBUG TRAIN Batch 45/2600 loss 8.906666 loss_att 9.800709 loss_ctc 16.654205 loss_rnnt 7.508701 hw_loss 0.349034 lr 0.00029762 rank 5
2023-02-27 14:26:31,113 DEBUG TRAIN Batch 45/2600 loss 2.436366 loss_att 6.117634 loss_ctc 5.096513 loss_rnnt 1.252529 hw_loss 0.174183 lr 0.00029762 rank 1
2023-02-27 14:26:31,128 DEBUG TRAIN Batch 45/2600 loss 5.506844 loss_att 9.861870 loss_ctc 13.682034 loss_rnnt 3.362554 hw_loss 0.343611 lr 0.00029762 rank 7
2023-02-27 14:27:09,248 DEBUG TRAIN Batch 45/2700 loss 8.539194 loss_att 10.920809 loss_ctc 11.148132 loss_rnnt 7.585031 hw_loss 0.243716 lr 0.00029761 rank 3
2023-02-27 14:27:09,249 DEBUG TRAIN Batch 45/2700 loss 12.143061 loss_att 17.079754 loss_ctc 18.335463 loss_rnnt 10.269931 hw_loss 0.112757 lr 0.00029761 rank 0
2023-02-27 14:27:09,252 DEBUG TRAIN Batch 45/2700 loss 7.090950 loss_att 9.698723 loss_ctc 8.233824 loss_rnnt 6.320127 hw_loss 0.181658 lr 0.00029761 rank 4
2023-02-27 14:27:09,265 DEBUG TRAIN Batch 45/2700 loss 5.272286 loss_att 11.080641 loss_ctc 14.943930 loss_rnnt 2.652828 hw_loss 0.315440 lr 0.00029761 rank 6
2023-02-27 14:27:09,265 DEBUG TRAIN Batch 45/2700 loss 5.906331 loss_att 9.843538 loss_ctc 11.342489 loss_rnnt 4.283809 hw_loss 0.206737 lr 0.00029761 rank 1
2023-02-27 14:27:09,270 DEBUG TRAIN Batch 45/2700 loss 8.755102 loss_att 8.700300 loss_ctc 7.340745 loss_rnnt 8.847904 hw_loss 0.200136 lr 0.00029761 rank 2
2023-02-27 14:27:09,272 DEBUG TRAIN Batch 45/2700 loss 5.862352 loss_att 9.844719 loss_ctc 12.873816 loss_rnnt 3.968226 hw_loss 0.305233 lr 0.00029761 rank 7
2023-02-27 14:27:09,330 DEBUG TRAIN Batch 45/2700 loss 4.093549 loss_att 6.444839 loss_ctc 10.521294 loss_rnnt 2.694525 hw_loss 0.134501 lr 0.00029761 rank 5
2023-02-27 14:27:48,926 DEBUG TRAIN Batch 45/2800 loss 15.507378 loss_att 19.680067 loss_ctc 22.702602 loss_rnnt 13.616453 hw_loss 0.181920 lr 0.00029759 rank 3
2023-02-27 14:27:48,940 DEBUG TRAIN Batch 45/2800 loss 8.194602 loss_att 9.708429 loss_ctc 14.144003 loss_rnnt 6.975880 hw_loss 0.230068 lr 0.00029759 rank 6
2023-02-27 14:27:48,941 DEBUG TRAIN Batch 45/2800 loss 4.253163 loss_att 7.005628 loss_ctc 6.185907 loss_rnnt 3.367211 hw_loss 0.145800 lr 0.00029760 rank 4
2023-02-27 14:27:48,944 DEBUG TRAIN Batch 45/2800 loss 5.692877 loss_att 8.827539 loss_ctc 9.551306 loss_rnnt 4.470918 hw_loss 0.151067 lr 0.00029760 rank 7
2023-02-27 14:27:48,944 DEBUG TRAIN Batch 45/2800 loss 16.497322 loss_att 19.308338 loss_ctc 24.380795 loss_rnnt 14.806740 hw_loss 0.144846 lr 0.00029759 rank 5
2023-02-27 14:27:48,946 DEBUG TRAIN Batch 45/2800 loss 5.552647 loss_att 9.684972 loss_ctc 9.033242 loss_rnnt 4.034491 hw_loss 0.426770 lr 0.00029760 rank 2
2023-02-27 14:27:48,949 DEBUG TRAIN Batch 45/2800 loss 10.031932 loss_att 13.946781 loss_ctc 17.074131 loss_rnnt 8.153667 hw_loss 0.293129 lr 0.00029760 rank 0
2023-02-27 14:27:48,949 DEBUG TRAIN Batch 45/2800 loss 5.706235 loss_att 9.388308 loss_ctc 12.844213 loss_rnnt 3.881406 hw_loss 0.256282 lr 0.00029760 rank 1
2023-02-27 14:28:53,646 DEBUG TRAIN Batch 45/2900 loss 5.052979 loss_att 8.471630 loss_ctc 8.833343 loss_rnnt 3.718273 hw_loss 0.275488 lr 0.00029758 rank 1
2023-02-27 14:28:53,649 DEBUG TRAIN Batch 45/2900 loss 1.331857 loss_att 4.327260 loss_ctc 3.032068 loss_rnnt 0.322659 hw_loss 0.343918 lr 0.00029759 rank 0
2023-02-27 14:28:53,652 DEBUG TRAIN Batch 45/2900 loss 10.413434 loss_att 11.908741 loss_ctc 15.075023 loss_rnnt 9.397280 hw_loss 0.179153 lr 0.00029758 rank 6
2023-02-27 14:28:53,652 DEBUG TRAIN Batch 45/2900 loss 4.958364 loss_att 7.498149 loss_ctc 7.556442 loss_rnnt 3.915114 hw_loss 0.354153 lr 0.00029758 rank 3
2023-02-27 14:28:53,658 DEBUG TRAIN Batch 45/2900 loss 6.393111 loss_att 9.984072 loss_ctc 9.140974 loss_rnnt 5.207668 hw_loss 0.189129 lr 0.00029758 rank 2
2023-02-27 14:28:53,657 DEBUG TRAIN Batch 45/2900 loss 4.816586 loss_att 8.237349 loss_ctc 7.182718 loss_rnnt 3.722798 hw_loss 0.176534 lr 0.00029758 rank 4
2023-02-27 14:28:53,658 DEBUG TRAIN Batch 45/2900 loss 12.823541 loss_att 18.078003 loss_ctc 22.848768 loss_rnnt 10.361153 hw_loss 0.140248 lr 0.00029758 rank 5
2023-02-27 14:28:53,658 DEBUG TRAIN Batch 45/2900 loss 4.901763 loss_att 8.449697 loss_ctc 10.216280 loss_rnnt 3.377158 hw_loss 0.199532 lr 0.00029758 rank 7
2023-02-27 14:29:32,367 DEBUG TRAIN Batch 45/3000 loss 7.966732 loss_att 11.568639 loss_ctc 13.514503 loss_rnnt 6.393929 hw_loss 0.211346 lr 0.00029757 rank 3
2023-02-27 14:29:32,385 DEBUG TRAIN Batch 45/3000 loss 7.616595 loss_att 11.131457 loss_ctc 10.438778 loss_rnnt 6.448231 hw_loss 0.167065 lr 0.00029757 rank 7
2023-02-27 14:29:32,389 DEBUG TRAIN Batch 45/3000 loss 8.124891 loss_att 9.416544 loss_ctc 17.536417 loss_rnnt 6.543637 hw_loss 0.127598 lr 0.00029757 rank 0
2023-02-27 14:29:32,389 DEBUG TRAIN Batch 45/3000 loss 4.625360 loss_att 7.468106 loss_ctc 7.434483 loss_rnnt 3.582653 hw_loss 0.186764 lr 0.00029757 rank 5
2023-02-27 14:29:32,391 DEBUG TRAIN Batch 45/3000 loss 3.542524 loss_att 5.647838 loss_ctc 6.609261 loss_rnnt 2.614006 hw_loss 0.184794 lr 0.00029757 rank 6
2023-02-27 14:29:32,392 DEBUG TRAIN Batch 45/3000 loss 4.833186 loss_att 6.266290 loss_ctc 7.660895 loss_rnnt 3.946666 hw_loss 0.417883 lr 0.00029757 rank 1
2023-02-27 14:29:32,393 DEBUG TRAIN Batch 45/3000 loss 5.441656 loss_att 8.023342 loss_ctc 8.866711 loss_rnnt 4.285828 hw_loss 0.342782 lr 0.00029757 rank 4
2023-02-27 14:29:32,439 DEBUG TRAIN Batch 45/3000 loss 4.987985 loss_att 7.986503 loss_ctc 11.297392 loss_rnnt 3.420178 hw_loss 0.237842 lr 0.00029757 rank 2
2023-02-27 14:30:11,919 DEBUG TRAIN Batch 45/3100 loss 5.130392 loss_att 7.378932 loss_ctc 11.793827 loss_rnnt 3.624258 hw_loss 0.314941 lr 0.00029756 rank 1
2023-02-27 14:30:11,919 DEBUG TRAIN Batch 45/3100 loss 5.835868 loss_att 8.278801 loss_ctc 9.363182 loss_rnnt 4.747417 hw_loss 0.242917 lr 0.00029756 rank 7
2023-02-27 14:30:11,919 DEBUG TRAIN Batch 45/3100 loss 9.034192 loss_att 10.003417 loss_ctc 15.913546 loss_rnnt 7.849222 hw_loss 0.138524 lr 0.00029756 rank 4
2023-02-27 14:30:11,922 DEBUG TRAIN Batch 45/3100 loss 8.496227 loss_att 10.659165 loss_ctc 13.362090 loss_rnnt 7.305546 hw_loss 0.204960 lr 0.00029755 rank 6
2023-02-27 14:30:11,922 DEBUG TRAIN Batch 45/3100 loss 2.307052 loss_att 4.856931 loss_ctc 5.233491 loss_rnnt 1.266134 hw_loss 0.263906 lr 0.00029755 rank 3
2023-02-27 14:30:11,924 DEBUG TRAIN Batch 45/3100 loss 15.301010 loss_att 15.665188 loss_ctc 23.593525 loss_rnnt 13.967836 hw_loss 0.290004 lr 0.00029756 rank 5
2023-02-27 14:30:11,925 DEBUG TRAIN Batch 45/3100 loss 7.790843 loss_att 9.408787 loss_ctc 17.610260 loss_rnnt 6.010559 hw_loss 0.276452 lr 0.00029756 rank 0
2023-02-27 14:30:11,926 DEBUG TRAIN Batch 45/3100 loss 5.792863 loss_att 6.736059 loss_ctc 8.461070 loss_rnnt 5.058860 hw_loss 0.355505 lr 0.00029756 rank 2
2023-02-27 14:31:13,893 DEBUG TRAIN Batch 45/3200 loss 3.830085 loss_att 9.004138 loss_ctc 7.776044 loss_rnnt 2.161483 hw_loss 0.201869 lr 0.00029754 rank 1
2023-02-27 14:31:13,896 DEBUG TRAIN Batch 45/3200 loss 3.926306 loss_att 6.648501 loss_ctc 6.149188 loss_rnnt 2.940926 hw_loss 0.271046 lr 0.00029754 rank 7
2023-02-27 14:31:13,900 DEBUG TRAIN Batch 45/3200 loss 4.952649 loss_att 5.282209 loss_ctc 7.416740 loss_rnnt 4.325304 hw_loss 0.436666 lr 0.00029755 rank 0
2023-02-27 14:31:13,900 DEBUG TRAIN Batch 45/3200 loss 8.082629 loss_att 13.107933 loss_ctc 17.737190 loss_rnnt 5.748658 hw_loss 0.078066 lr 0.00029754 rank 4
2023-02-27 14:31:13,906 DEBUG TRAIN Batch 45/3200 loss 4.890623 loss_att 6.660975 loss_ctc 10.520959 loss_rnnt 3.583472 hw_loss 0.379441 lr 0.00029754 rank 5
2023-02-27 14:31:13,906 DEBUG TRAIN Batch 45/3200 loss 10.710534 loss_att 13.290196 loss_ctc 15.166999 loss_rnnt 9.458874 hw_loss 0.265375 lr 0.00029754 rank 3
2023-02-27 14:31:13,908 DEBUG TRAIN Batch 45/3200 loss 3.734320 loss_att 5.587898 loss_ctc 4.422999 loss_rnnt 3.097888 hw_loss 0.326049 lr 0.00029754 rank 2
2023-02-27 14:31:13,940 DEBUG TRAIN Batch 45/3200 loss 3.985045 loss_att 5.628106 loss_ctc 4.357102 loss_rnnt 3.488758 hw_loss 0.221377 lr 0.00029754 rank 6
2023-02-27 14:31:55,923 DEBUG TRAIN Batch 45/3300 loss 3.908302 loss_att 7.321551 loss_ctc 5.548492 loss_rnnt 2.849007 hw_loss 0.296162 lr 0.00029753 rank 3
2023-02-27 14:31:55,925 DEBUG TRAIN Batch 45/3300 loss 3.870795 loss_att 6.188394 loss_ctc 3.523123 loss_rnnt 3.277784 hw_loss 0.329714 lr 0.00029753 rank 5
2023-02-27 14:31:55,932 DEBUG TRAIN Batch 45/3300 loss 3.859168 loss_att 8.245618 loss_ctc 4.353098 loss_rnnt 2.841490 hw_loss 0.139745 lr 0.00029753 rank 7
2023-02-27 14:31:55,938 DEBUG TRAIN Batch 45/3300 loss 5.616620 loss_att 7.845589 loss_ctc 13.525939 loss_rnnt 4.039810 hw_loss 0.143325 lr 0.00029753 rank 6
2023-02-27 14:31:55,938 DEBUG TRAIN Batch 45/3300 loss 5.949853 loss_att 8.213346 loss_ctc 10.508260 loss_rnnt 4.809749 hw_loss 0.149283 lr 0.00029753 rank 4
2023-02-27 14:31:55,939 DEBUG TRAIN Batch 45/3300 loss 6.544750 loss_att 9.950303 loss_ctc 11.825836 loss_rnnt 5.026076 hw_loss 0.250160 lr 0.00029753 rank 0
2023-02-27 14:31:55,942 DEBUG TRAIN Batch 45/3300 loss 5.554500 loss_att 8.748582 loss_ctc 7.093861 loss_rnnt 4.624881 hw_loss 0.160415 lr 0.00029753 rank 2
2023-02-27 14:31:55,946 DEBUG TRAIN Batch 45/3300 loss 7.895751 loss_att 13.629386 loss_ctc 11.642039 loss_rnnt 6.105598 hw_loss 0.269850 lr 0.00029753 rank 1
2023-02-27 14:32:34,392 DEBUG TRAIN Batch 45/3400 loss 5.734144 loss_att 8.790281 loss_ctc 14.785006 loss_rnnt 3.814583 hw_loss 0.190409 lr 0.00029752 rank 0
2023-02-27 14:32:34,394 DEBUG TRAIN Batch 45/3400 loss 8.194202 loss_att 9.235664 loss_ctc 9.291462 loss_rnnt 7.767807 hw_loss 0.134626 lr 0.00029752 rank 2
2023-02-27 14:32:34,404 DEBUG TRAIN Batch 45/3400 loss 4.220166 loss_att 8.167650 loss_ctc 8.230328 loss_rnnt 2.722504 hw_loss 0.325269 lr 0.00029752 rank 4
2023-02-27 14:32:34,409 DEBUG TRAIN Batch 45/3400 loss 3.754118 loss_att 7.711441 loss_ctc 7.740288 loss_rnnt 2.295872 hw_loss 0.253673 lr 0.00029752 rank 6
2023-02-27 14:32:34,409 DEBUG TRAIN Batch 45/3400 loss 2.205787 loss_att 5.062014 loss_ctc 5.648432 loss_rnnt 1.065741 hw_loss 0.205839 lr 0.00029752 rank 7
2023-02-27 14:32:34,411 DEBUG TRAIN Batch 45/3400 loss 4.687622 loss_att 6.614294 loss_ctc 7.284259 loss_rnnt 3.855597 hw_loss 0.188384 lr 0.00029751 rank 3
2023-02-27 14:32:34,413 DEBUG TRAIN Batch 45/3400 loss 4.200699 loss_att 5.816108 loss_ctc 5.416468 loss_rnnt 3.582224 hw_loss 0.249921 lr 0.00029752 rank 1
2023-02-27 14:32:34,456 DEBUG TRAIN Batch 45/3400 loss 1.597064 loss_att 4.705877 loss_ctc 2.189441 loss_rnnt 0.787665 hw_loss 0.203723 lr 0.00029752 rank 5
2023-02-27 14:33:13,515 DEBUG TRAIN Batch 45/3500 loss 5.698140 loss_att 8.247454 loss_ctc 10.418985 loss_rnnt 4.448717 hw_loss 0.206463 lr 0.00029750 rank 4
2023-02-27 14:33:13,516 DEBUG TRAIN Batch 45/3500 loss 9.434508 loss_att 12.666002 loss_ctc 15.546429 loss_rnnt 7.861913 hw_loss 0.208827 lr 0.00029750 rank 7
2023-02-27 14:33:13,517 DEBUG TRAIN Batch 45/3500 loss 6.908455 loss_att 9.122968 loss_ctc 8.921728 loss_rnnt 6.111197 hw_loss 0.161098 lr 0.00029751 rank 1
2023-02-27 14:33:13,529 DEBUG TRAIN Batch 45/3500 loss 6.727769 loss_att 11.855219 loss_ctc 10.721382 loss_rnnt 4.959616 hw_loss 0.394090 lr 0.00029750 rank 2
2023-02-27 14:33:13,529 DEBUG TRAIN Batch 45/3500 loss 2.526034 loss_att 4.728920 loss_ctc 5.241495 loss_rnnt 1.640538 hw_loss 0.155358 lr 0.00029750 rank 3
2023-02-27 14:33:13,530 DEBUG TRAIN Batch 45/3500 loss 6.738738 loss_att 10.909415 loss_ctc 13.781478 loss_rnnt 4.864463 hw_loss 0.189575 lr 0.00029750 rank 6
2023-02-27 14:33:13,533 DEBUG TRAIN Batch 45/3500 loss 9.129270 loss_att 11.010859 loss_ctc 14.921308 loss_rnnt 7.880919 hw_loss 0.187051 lr 0.00029750 rank 5
2023-02-27 14:33:13,535 DEBUG TRAIN Batch 45/3500 loss 9.516348 loss_att 12.135460 loss_ctc 14.158966 loss_rnnt 8.268311 hw_loss 0.197249 lr 0.00029751 rank 0
2023-02-27 14:34:19,222 DEBUG TRAIN Batch 45/3600 loss 11.290237 loss_att 14.304773 loss_ctc 23.056568 loss_rnnt 9.040223 hw_loss 0.146742 lr 0.00029749 rank 1
2023-02-27 14:34:19,239 DEBUG TRAIN Batch 45/3600 loss 4.858842 loss_att 7.133488 loss_ctc 8.636225 loss_rnnt 3.750849 hw_loss 0.280150 lr 0.00029749 rank 3
2023-02-27 14:34:19,239 DEBUG TRAIN Batch 45/3600 loss 6.694006 loss_att 11.055532 loss_ctc 12.341200 loss_rnnt 4.953880 hw_loss 0.215367 lr 0.00029749 rank 0
2023-02-27 14:34:19,241 DEBUG TRAIN Batch 45/3600 loss 9.748894 loss_att 11.627434 loss_ctc 19.389759 loss_rnnt 8.029453 hw_loss 0.109281 lr 0.00029749 rank 6
2023-02-27 14:34:19,242 DEBUG TRAIN Batch 45/3600 loss 12.245074 loss_att 14.425846 loss_ctc 16.082899 loss_rnnt 11.177393 hw_loss 0.224655 lr 0.00029749 rank 4
2023-02-27 14:34:19,242 DEBUG TRAIN Batch 45/3600 loss 1.383383 loss_att 3.004345 loss_ctc 3.446652 loss_rnnt 0.733768 hw_loss 0.094349 lr 0.00029749 rank 5
2023-02-27 14:34:19,247 DEBUG TRAIN Batch 45/3600 loss 6.503108 loss_att 9.150057 loss_ctc 10.735072 loss_rnnt 5.363104 hw_loss 0.086910 lr 0.00029749 rank 2
2023-02-27 14:34:19,250 DEBUG TRAIN Batch 45/3600 loss 3.773705 loss_att 6.898312 loss_ctc 4.900224 loss_rnnt 2.840635 hw_loss 0.296147 lr 0.00029749 rank 7
2023-02-27 14:34:58,369 DEBUG TRAIN Batch 45/3700 loss 5.122071 loss_att 8.496525 loss_ctc 9.906552 loss_rnnt 3.659161 hw_loss 0.281416 lr 0.00029748 rank 7
2023-02-27 14:34:58,374 DEBUG TRAIN Batch 45/3700 loss 4.885704 loss_att 7.193281 loss_ctc 10.090925 loss_rnnt 3.569324 hw_loss 0.301566 lr 0.00029748 rank 5
2023-02-27 14:34:58,382 DEBUG TRAIN Batch 45/3700 loss 4.638491 loss_att 6.804574 loss_ctc 6.689500 loss_rnnt 3.854779 hw_loss 0.144426 lr 0.00029748 rank 4
2023-02-27 14:34:58,384 DEBUG TRAIN Batch 45/3700 loss 9.541294 loss_att 10.305676 loss_ctc 13.307004 loss_rnnt 8.770877 hw_loss 0.216463 lr 0.00029748 rank 6
2023-02-27 14:34:58,384 DEBUG TRAIN Batch 45/3700 loss 4.597215 loss_att 7.452473 loss_ctc 7.870178 loss_rnnt 3.486767 hw_loss 0.193129 lr 0.00029747 rank 3
2023-02-27 14:34:58,385 DEBUG TRAIN Batch 45/3700 loss 8.445240 loss_att 9.301879 loss_ctc 13.747292 loss_rnnt 7.419263 hw_loss 0.276955 lr 0.00029748 rank 0
2023-02-27 14:34:58,390 DEBUG TRAIN Batch 45/3700 loss 9.877881 loss_att 11.371428 loss_ctc 15.097641 loss_rnnt 8.691567 hw_loss 0.359319 lr 0.00029748 rank 1
2023-02-27 14:34:58,440 DEBUG TRAIN Batch 45/3700 loss 4.225857 loss_att 6.859053 loss_ctc 5.677594 loss_rnnt 3.329473 hw_loss 0.330337 lr 0.00029748 rank 2
2023-02-27 14:35:37,259 DEBUG TRAIN Batch 45/3800 loss 11.195963 loss_att 12.667218 loss_ctc 15.278276 loss_rnnt 10.241574 hw_loss 0.217180 lr 0.00029746 rank 5
2023-02-27 14:35:37,270 DEBUG TRAIN Batch 45/3800 loss 5.367473 loss_att 12.019047 loss_ctc 14.091404 loss_rnnt 2.761258 hw_loss 0.211331 lr 0.00029746 rank 7
2023-02-27 14:35:37,273 DEBUG TRAIN Batch 45/3800 loss 7.004443 loss_att 7.575481 loss_ctc 10.832057 loss_rnnt 6.190361 hw_loss 0.355363 lr 0.00029747 rank 1
2023-02-27 14:35:37,275 DEBUG TRAIN Batch 45/3800 loss 8.949988 loss_att 12.002797 loss_ctc 13.407808 loss_rnnt 7.718118 hw_loss 0.050500 lr 0.00029747 rank 0
2023-02-27 14:35:37,278 DEBUG TRAIN Batch 45/3800 loss 4.125058 loss_att 8.095125 loss_ctc 8.167799 loss_rnnt 2.745515 hw_loss 0.087184 lr 0.00029746 rank 2
2023-02-27 14:35:37,279 DEBUG TRAIN Batch 45/3800 loss 6.758612 loss_att 6.695895 loss_ctc 9.641632 loss_rnnt 6.118850 hw_loss 0.502317 lr 0.00029746 rank 6
2023-02-27 14:35:37,279 DEBUG TRAIN Batch 45/3800 loss 6.117766 loss_att 8.828772 loss_ctc 12.086489 loss_rnnt 4.679591 hw_loss 0.187770 lr 0.00029746 rank 3
2023-02-27 14:35:37,328 DEBUG TRAIN Batch 45/3800 loss 7.618437 loss_att 8.410338 loss_ctc 10.521366 loss_rnnt 6.899568 hw_loss 0.325186 lr 0.00029746 rank 4
2023-02-27 14:36:40,195 DEBUG TRAIN Batch 45/3900 loss 2.222074 loss_att 4.858177 loss_ctc 3.891245 loss_rnnt 1.374830 hw_loss 0.182752 lr 0.00029745 rank 3
2023-02-27 14:36:40,205 DEBUG TRAIN Batch 45/3900 loss 4.715716 loss_att 8.052685 loss_ctc 6.579865 loss_rnnt 3.615032 hw_loss 0.346381 lr 0.00029745 rank 2
2023-02-27 14:36:40,207 DEBUG TRAIN Batch 45/3900 loss 1.859932 loss_att 4.602273 loss_ctc 3.356409 loss_rnnt 0.895074 hw_loss 0.406612 lr 0.00029745 rank 6
2023-02-27 14:36:40,229 DEBUG TRAIN Batch 45/3900 loss 4.114774 loss_att 6.027765 loss_ctc 6.124707 loss_rnnt 3.386391 hw_loss 0.145863 lr 0.00029745 rank 7
2023-02-27 14:36:40,230 DEBUG TRAIN Batch 45/3900 loss 2.932062 loss_att 4.345250 loss_ctc 5.640404 loss_rnnt 2.185819 hw_loss 0.192175 lr 0.00029745 rank 4
2023-02-27 14:36:40,244 DEBUG TRAIN Batch 45/3900 loss 5.608562 loss_att 9.127439 loss_ctc 9.386397 loss_rnnt 4.251976 hw_loss 0.279562 lr 0.00029745 rank 1
2023-02-27 14:36:40,261 DEBUG TRAIN Batch 45/3900 loss 2.828031 loss_att 5.352688 loss_ctc 5.047075 loss_rnnt 1.938729 hw_loss 0.165933 lr 0.00029745 rank 0
2023-02-27 14:36:40,271 DEBUG TRAIN Batch 45/3900 loss 4.881450 loss_att 5.518931 loss_ctc 6.587233 loss_rnnt 4.306235 hw_loss 0.413027 lr 0.00029745 rank 5
2023-02-27 14:37:22,702 DEBUG TRAIN Batch 45/4000 loss 5.008756 loss_att 6.157525 loss_ctc 6.608999 loss_rnnt 4.395346 hw_loss 0.319297 lr 0.00029743 rank 3
2023-02-27 14:37:22,705 DEBUG TRAIN Batch 45/4000 loss 6.803451 loss_att 10.573236 loss_ctc 9.026865 loss_rnnt 5.609422 hw_loss 0.269281 lr 0.00029744 rank 0
2023-02-27 14:37:22,706 DEBUG TRAIN Batch 45/4000 loss 5.872460 loss_att 8.755199 loss_ctc 13.468207 loss_rnnt 4.242091 hw_loss 0.076979 lr 0.00029744 rank 4
2023-02-27 14:37:22,709 DEBUG TRAIN Batch 45/4000 loss 10.930140 loss_att 20.134590 loss_ctc 23.868299 loss_rnnt 7.314897 hw_loss 0.092370 lr 0.00029744 rank 6
2023-02-27 14:37:22,712 DEBUG TRAIN Batch 45/4000 loss 5.184283 loss_att 8.385005 loss_ctc 6.255384 loss_rnnt 4.351757 hw_loss 0.092940 lr 0.00029744 rank 5
2023-02-27 14:37:22,715 DEBUG TRAIN Batch 45/4000 loss 3.093984 loss_att 4.273596 loss_ctc 3.075017 loss_rnnt 2.858705 hw_loss 0.003537 lr 0.00029744 rank 1
2023-02-27 14:37:22,730 DEBUG TRAIN Batch 45/4000 loss 1.707114 loss_att 5.632874 loss_ctc 2.414637 loss_rnnt 0.720489 hw_loss 0.200881 lr 0.00029744 rank 7
2023-02-27 14:37:22,773 DEBUG TRAIN Batch 45/4000 loss 6.930279 loss_att 8.913247 loss_ctc 9.163203 loss_rnnt 6.110853 hw_loss 0.234580 lr 0.00029744 rank 2
2023-02-27 14:38:01,630 DEBUG TRAIN Batch 45/4100 loss 7.992496 loss_att 10.616126 loss_ctc 13.405753 loss_rnnt 6.564985 hw_loss 0.339408 lr 0.00029742 rank 4
2023-02-27 14:38:01,631 DEBUG TRAIN Batch 45/4100 loss 7.583244 loss_att 11.452677 loss_ctc 13.342104 loss_rnnt 5.865584 hw_loss 0.329861 lr 0.00029742 rank 7
2023-02-27 14:38:01,638 DEBUG TRAIN Batch 45/4100 loss 6.343965 loss_att 11.651407 loss_ctc 9.696525 loss_rnnt 4.687390 hw_loss 0.277645 lr 0.00029742 rank 6
2023-02-27 14:38:01,637 DEBUG TRAIN Batch 45/4100 loss 2.573553 loss_att 7.994898 loss_ctc 3.053205 loss_rnnt 1.283467 hw_loss 0.265992 lr 0.00029742 rank 3
2023-02-27 14:38:01,639 DEBUG TRAIN Batch 45/4100 loss 4.315092 loss_att 6.788945 loss_ctc 7.149747 loss_rnnt 3.278954 hw_loss 0.306400 lr 0.00029743 rank 0
2023-02-27 14:38:01,640 DEBUG TRAIN Batch 45/4100 loss 4.860083 loss_att 8.918545 loss_ctc 6.847922 loss_rnnt 3.622723 hw_loss 0.301166 lr 0.00029742 rank 2
2023-02-27 14:38:01,641 DEBUG TRAIN Batch 45/4100 loss 10.203136 loss_att 12.826372 loss_ctc 15.119876 loss_rnnt 8.897103 hw_loss 0.235912 lr 0.00029743 rank 1
2023-02-27 14:38:01,643 DEBUG TRAIN Batch 45/4100 loss 4.383811 loss_att 8.104437 loss_ctc 11.335108 loss_rnnt 2.571465 hw_loss 0.265091 lr 0.00029742 rank 5
2023-02-27 14:38:40,878 DEBUG TRAIN Batch 45/4200 loss 3.482128 loss_att 5.550991 loss_ctc 6.739658 loss_rnnt 2.479306 hw_loss 0.290086 lr 0.00029741 rank 4
2023-02-27 14:38:40,883 DEBUG TRAIN Batch 45/4200 loss 7.008869 loss_att 12.709356 loss_ctc 15.873137 loss_rnnt 4.627471 hw_loss 0.111371 lr 0.00029741 rank 7
2023-02-27 14:38:40,885 DEBUG TRAIN Batch 45/4200 loss 7.759382 loss_att 9.295643 loss_ctc 11.649803 loss_rnnt 6.853590 hw_loss 0.149658 lr 0.00029741 rank 6
2023-02-27 14:38:40,888 DEBUG TRAIN Batch 45/4200 loss 6.302593 loss_att 7.936316 loss_ctc 8.809448 loss_rnnt 5.508886 hw_loss 0.248840 lr 0.00029741 rank 2
2023-02-27 14:38:40,889 DEBUG TRAIN Batch 45/4200 loss 4.010802 loss_att 5.798193 loss_ctc 5.564578 loss_rnnt 3.285528 hw_loss 0.301174 lr 0.00029741 rank 0
2023-02-27 14:38:40,893 DEBUG TRAIN Batch 45/4200 loss 10.371867 loss_att 11.915858 loss_ctc 17.391916 loss_rnnt 9.011442 hw_loss 0.216787 lr 0.00029741 rank 5
2023-02-27 14:38:40,893 DEBUG TRAIN Batch 45/4200 loss 9.757146 loss_att 13.312157 loss_ctc 15.099462 loss_rnnt 8.216396 hw_loss 0.220197 lr 0.00029741 rank 1
2023-02-27 14:38:40,895 DEBUG TRAIN Batch 45/4200 loss 2.895540 loss_att 4.902169 loss_ctc 6.419881 loss_rnnt 1.887141 hw_loss 0.257177 lr 0.00029741 rank 3
2023-02-27 14:39:46,221 DEBUG TRAIN Batch 45/4300 loss 5.832229 loss_att 9.273811 loss_ctc 13.065920 loss_rnnt 4.079995 hw_loss 0.186421 lr 0.00029740 rank 6
2023-02-27 14:39:46,221 DEBUG TRAIN Batch 45/4300 loss 6.663068 loss_att 9.766723 loss_ctc 11.567450 loss_rnnt 5.205984 hw_loss 0.342067 lr 0.00029740 rank 3
2023-02-27 14:39:46,222 DEBUG TRAIN Batch 45/4300 loss 5.953392 loss_att 8.632000 loss_ctc 12.167637 loss_rnnt 4.478363 hw_loss 0.207640 lr 0.00029740 rank 0
2023-02-27 14:39:46,223 DEBUG TRAIN Batch 45/4300 loss 6.360954 loss_att 8.864048 loss_ctc 9.893280 loss_rnnt 5.254560 hw_loss 0.252748 lr 0.00029740 rank 1
2023-02-27 14:39:46,224 DEBUG TRAIN Batch 45/4300 loss 3.575759 loss_att 6.542565 loss_ctc 7.597510 loss_rnnt 2.348055 hw_loss 0.183956 lr 0.00029740 rank 7
2023-02-27 14:39:46,226 DEBUG TRAIN Batch 45/4300 loss 7.368639 loss_att 11.269444 loss_ctc 10.254542 loss_rnnt 6.109981 hw_loss 0.175706 lr 0.00029740 rank 4
2023-02-27 14:39:46,227 DEBUG TRAIN Batch 45/4300 loss 5.168065 loss_att 9.829975 loss_ctc 8.848307 loss_rnnt 3.665776 hw_loss 0.148515 lr 0.00029740 rank 2
2023-02-27 14:39:46,227 DEBUG TRAIN Batch 45/4300 loss 8.605247 loss_att 10.997223 loss_ctc 13.923031 loss_rnnt 7.272446 hw_loss 0.272568 lr 0.00029740 rank 5
2023-02-27 14:40:25,094 DEBUG TRAIN Batch 45/4400 loss 6.800169 loss_att 10.349734 loss_ctc 10.702472 loss_rnnt 5.435823 hw_loss 0.251488 lr 0.00029738 rank 6
2023-02-27 14:40:25,110 DEBUG TRAIN Batch 45/4400 loss 6.601119 loss_att 7.187139 loss_ctc 12.020812 loss_rnnt 5.648068 hw_loss 0.212287 lr 0.00029739 rank 4
2023-02-27 14:40:25,112 DEBUG TRAIN Batch 45/4400 loss 5.447358 loss_att 7.164227 loss_ctc 8.837259 loss_rnnt 4.465892 hw_loss 0.348948 lr 0.00029739 rank 0
2023-02-27 14:40:25,114 DEBUG TRAIN Batch 45/4400 loss 6.012619 loss_att 9.071606 loss_ctc 16.707296 loss_rnnt 3.882061 hw_loss 0.174004 lr 0.00029738 rank 3
2023-02-27 14:40:25,115 DEBUG TRAIN Batch 45/4400 loss 8.860670 loss_att 13.427346 loss_ctc 15.795277 loss_rnnt 6.897254 hw_loss 0.235250 lr 0.00029738 rank 7
2023-02-27 14:40:25,117 DEBUG TRAIN Batch 45/4400 loss 9.029860 loss_att 11.893418 loss_ctc 12.606035 loss_rnnt 7.896240 hw_loss 0.157659 lr 0.00029738 rank 5
2023-02-27 14:40:25,119 DEBUG TRAIN Batch 45/4400 loss 5.243379 loss_att 5.971933 loss_ctc 8.293535 loss_rnnt 4.480373 hw_loss 0.394890 lr 0.00029739 rank 1
2023-02-27 14:40:25,159 DEBUG TRAIN Batch 45/4400 loss 8.523717 loss_att 9.279609 loss_ctc 13.171839 loss_rnnt 7.514796 hw_loss 0.446237 lr 0.00029738 rank 2
2023-02-27 14:41:04,253 DEBUG TRAIN Batch 45/4500 loss 8.671208 loss_att 10.392387 loss_ctc 12.831114 loss_rnnt 7.680114 hw_loss 0.172884 lr 0.00029737 rank 7
2023-02-27 14:41:04,264 DEBUG TRAIN Batch 45/4500 loss 5.584545 loss_att 8.907620 loss_ctc 13.937007 loss_rnnt 3.766620 hw_loss 0.074341 lr 0.00029737 rank 1
2023-02-27 14:41:04,275 DEBUG TRAIN Batch 45/4500 loss 3.569363 loss_att 3.991252 loss_ctc 5.999660 loss_rnnt 2.958312 hw_loss 0.379937 lr 0.00029737 rank 0
2023-02-27 14:41:04,275 DEBUG TRAIN Batch 45/4500 loss 9.059809 loss_att 10.847313 loss_ctc 14.289828 loss_rnnt 7.897667 hw_loss 0.201198 lr 0.00029737 rank 3
2023-02-27 14:41:04,275 DEBUG TRAIN Batch 45/4500 loss 5.074834 loss_att 8.797499 loss_ctc 12.670740 loss_rnnt 3.101659 hw_loss 0.404729 lr 0.00029737 rank 6
2023-02-27 14:41:04,298 DEBUG TRAIN Batch 45/4500 loss 3.914687 loss_att 7.171753 loss_ctc 7.428946 loss_rnnt 2.700953 hw_loss 0.175786 lr 0.00029737 rank 4
2023-02-27 14:41:04,308 DEBUG TRAIN Batch 45/4500 loss 5.155478 loss_att 6.722950 loss_ctc 9.230779 loss_rnnt 4.141975 hw_loss 0.293691 lr 0.00029737 rank 5
2023-02-27 14:41:04,311 DEBUG TRAIN Batch 45/4500 loss 5.712259 loss_att 10.689553 loss_ctc 11.900417 loss_rnnt 3.846160 hw_loss 0.085409 lr 0.00029737 rank 2
2023-02-27 14:41:44,143 DEBUG TRAIN Batch 45/4600 loss 6.052734 loss_att 7.062736 loss_ctc 6.715899 loss_rnnt 5.579052 hw_loss 0.343613 lr 0.00029736 rank 2
2023-02-27 14:41:44,147 DEBUG TRAIN Batch 45/4600 loss 6.491090 loss_att 8.871677 loss_ctc 12.350750 loss_rnnt 5.049093 hw_loss 0.346108 lr 0.00029736 rank 6
2023-02-27 14:41:44,147 DEBUG TRAIN Batch 45/4600 loss 1.711034 loss_att 4.546843 loss_ctc 2.396802 loss_rnnt 0.854294 hw_loss 0.371516 lr 0.00029736 rank 1
2023-02-27 14:41:44,149 DEBUG TRAIN Batch 45/4600 loss 3.822574 loss_att 7.014986 loss_ctc 7.889793 loss_rnnt 2.598095 hw_loss 0.081939 lr 0.00029736 rank 4
2023-02-27 14:41:44,159 DEBUG TRAIN Batch 45/4600 loss 4.582629 loss_att 8.243591 loss_ctc 9.174883 loss_rnnt 3.045973 hw_loss 0.360305 lr 0.00029736 rank 3
2023-02-27 14:41:44,163 DEBUG TRAIN Batch 45/4600 loss 7.759036 loss_att 11.177407 loss_ctc 10.676322 loss_rnnt 6.584467 hw_loss 0.191106 lr 0.00029736 rank 0
2023-02-27 14:41:44,185 DEBUG TRAIN Batch 45/4600 loss 15.503500 loss_att 15.928033 loss_ctc 26.066198 loss_rnnt 13.939116 hw_loss 0.133346 lr 0.00029736 rank 5
2023-02-27 14:41:44,187 DEBUG TRAIN Batch 45/4600 loss 3.718220 loss_att 8.473125 loss_ctc 6.906689 loss_rnnt 2.282008 hw_loss 0.112690 lr 0.00029736 rank 7
2023-02-27 14:42:49,965 DEBUG TRAIN Batch 45/4700 loss 5.441458 loss_att 8.300277 loss_ctc 9.150119 loss_rnnt 4.308619 hw_loss 0.124851 lr 0.00029735 rank 7
2023-02-27 14:42:49,973 DEBUG TRAIN Batch 45/4700 loss 9.678577 loss_att 14.715156 loss_ctc 14.684746 loss_rnnt 7.851584 hw_loss 0.285354 lr 0.00029734 rank 3
2023-02-27 14:42:49,981 DEBUG TRAIN Batch 45/4700 loss 8.693396 loss_att 12.713195 loss_ctc 10.753090 loss_rnnt 7.485580 hw_loss 0.242305 lr 0.00029734 rank 6
2023-02-27 14:42:49,982 DEBUG TRAIN Batch 45/4700 loss 9.786371 loss_att 14.797062 loss_ctc 16.741249 loss_rnnt 7.715596 hw_loss 0.264973 lr 0.00029735 rank 0
2023-02-27 14:42:49,985 DEBUG TRAIN Batch 45/4700 loss 2.481301 loss_att 5.572552 loss_ctc 5.490826 loss_rnnt 1.459065 hw_loss 0.005091 lr 0.00029735 rank 4
2023-02-27 14:42:49,985 DEBUG TRAIN Batch 45/4700 loss 6.245368 loss_att 7.969041 loss_ctc 8.029358 loss_rnnt 5.471139 hw_loss 0.359304 lr 0.00029734 rank 5
2023-02-27 14:42:49,985 DEBUG TRAIN Batch 45/4700 loss 8.567946 loss_att 11.627399 loss_ctc 16.685539 loss_rnnt 6.833211 hw_loss 0.075937 lr 0.00029735 rank 2
2023-02-27 14:42:49,986 DEBUG TRAIN Batch 45/4700 loss 8.185722 loss_att 12.076075 loss_ctc 19.489983 loss_rnnt 5.824029 hw_loss 0.143228 lr 0.00029735 rank 1
2023-02-27 14:43:28,621 DEBUG TRAIN Batch 45/4800 loss 5.597073 loss_att 7.316706 loss_ctc 9.851328 loss_rnnt 4.441634 hw_loss 0.458022 lr 0.00029733 rank 6
2023-02-27 14:43:28,624 DEBUG TRAIN Batch 45/4800 loss 4.729138 loss_att 8.607061 loss_ctc 10.742081 loss_rnnt 3.124920 hw_loss 0.050452 lr 0.00029734 rank 0
2023-02-27 14:43:28,639 DEBUG TRAIN Batch 45/4800 loss 13.059031 loss_att 17.394670 loss_ctc 20.222836 loss_rnnt 11.053469 hw_loss 0.343611 lr 0.00029733 rank 3
2023-02-27 14:43:28,642 DEBUG TRAIN Batch 45/4800 loss 5.972499 loss_att 9.222648 loss_ctc 14.249626 loss_rnnt 4.144189 hw_loss 0.139994 lr 0.00029733 rank 2
2023-02-27 14:43:28,642 DEBUG TRAIN Batch 45/4800 loss 4.710869 loss_att 7.663789 loss_ctc 7.594394 loss_rnnt 3.590436 hw_loss 0.272585 lr 0.00029733 rank 7
2023-02-27 14:43:28,647 DEBUG TRAIN Batch 45/4800 loss 7.215921 loss_att 9.697928 loss_ctc 13.178949 loss_rnnt 5.881573 hw_loss 0.080394 lr 0.00029733 rank 5
2023-02-27 14:43:28,647 DEBUG TRAIN Batch 45/4800 loss 4.814588 loss_att 8.279502 loss_ctc 9.887571 loss_rnnt 3.257241 hw_loss 0.352436 lr 0.00029733 rank 1
2023-02-27 14:43:28,693 DEBUG TRAIN Batch 45/4800 loss 6.798226 loss_att 11.520099 loss_ctc 8.894125 loss_rnnt 5.502731 hw_loss 0.134375 lr 0.00029733 rank 4
2023-02-27 14:44:07,369 DEBUG TRAIN Batch 45/4900 loss 5.684390 loss_att 8.113548 loss_ctc 9.819447 loss_rnnt 4.442337 hw_loss 0.384149 lr 0.00029732 rank 6
2023-02-27 14:44:07,375 DEBUG TRAIN Batch 45/4900 loss 10.800191 loss_att 14.308195 loss_ctc 21.821575 loss_rnnt 8.536145 hw_loss 0.174238 lr 0.00029732 rank 2
2023-02-27 14:44:07,378 DEBUG TRAIN Batch 45/4900 loss 16.501497 loss_att 20.413563 loss_ctc 26.150059 loss_rnnt 14.263855 hw_loss 0.316414 lr 0.00029732 rank 1
2023-02-27 14:44:07,386 DEBUG TRAIN Batch 45/4900 loss 5.313189 loss_att 8.973940 loss_ctc 8.892122 loss_rnnt 3.982418 hw_loss 0.227681 lr 0.00029732 rank 5
2023-02-27 14:44:07,389 DEBUG TRAIN Batch 45/4900 loss 7.326575 loss_att 10.345207 loss_ctc 15.487045 loss_rnnt 5.499826 hw_loss 0.253048 lr 0.00029732 rank 4
2023-02-27 14:44:07,389 DEBUG TRAIN Batch 45/4900 loss 9.029317 loss_att 9.432619 loss_ctc 12.267403 loss_rnnt 8.360518 hw_loss 0.293239 lr 0.00029732 rank 0
2023-02-27 14:44:07,390 DEBUG TRAIN Batch 45/4900 loss 4.697321 loss_att 6.672297 loss_ctc 7.220934 loss_rnnt 3.855985 hw_loss 0.205987 lr 0.00029732 rank 7
2023-02-27 14:44:07,394 DEBUG TRAIN Batch 45/4900 loss 3.787514 loss_att 6.445905 loss_ctc 5.863159 loss_rnnt 2.767734 hw_loss 0.396281 lr 0.00029732 rank 3
2023-02-27 14:45:15,370 DEBUG TRAIN Batch 45/5000 loss 7.340053 loss_att 9.986356 loss_ctc 13.637177 loss_rnnt 5.774954 hw_loss 0.367917 lr 0.00029730 rank 6
2023-02-27 14:45:15,373 DEBUG TRAIN Batch 45/5000 loss 2.496480 loss_att 4.162717 loss_ctc 4.891807 loss_rnnt 1.748280 hw_loss 0.179205 lr 0.00029731 rank 0
2023-02-27 14:45:15,379 DEBUG TRAIN Batch 45/5000 loss 4.301472 loss_att 7.723447 loss_ctc 6.329902 loss_rnnt 3.192642 hw_loss 0.288707 lr 0.00029730 rank 3
2023-02-27 14:45:15,382 DEBUG TRAIN Batch 45/5000 loss 6.681039 loss_att 8.844598 loss_ctc 10.952009 loss_rnnt 5.558732 hw_loss 0.225250 lr 0.00029731 rank 5
2023-02-27 14:45:15,383 DEBUG TRAIN Batch 45/5000 loss 9.748913 loss_att 11.801087 loss_ctc 15.640810 loss_rnnt 8.436427 hw_loss 0.218369 lr 0.00029731 rank 2
2023-02-27 14:45:15,385 DEBUG TRAIN Batch 45/5000 loss 10.475132 loss_att 12.523235 loss_ctc 18.356564 loss_rnnt 8.903534 hw_loss 0.208348 lr 0.00029731 rank 1
2023-02-27 14:45:15,385 DEBUG TRAIN Batch 45/5000 loss 6.612720 loss_att 8.746425 loss_ctc 10.537517 loss_rnnt 5.559853 hw_loss 0.192788 lr 0.00029731 rank 7
2023-02-27 14:45:15,431 DEBUG TRAIN Batch 45/5000 loss 9.655182 loss_att 12.306581 loss_ctc 16.062101 loss_rnnt 8.142728 hw_loss 0.239845 lr 0.00029731 rank 4
2023-02-27 14:45:54,657 DEBUG TRAIN Batch 45/5100 loss 4.319583 loss_att 9.260345 loss_ctc 7.236574 loss_rnnt 2.907987 hw_loss 0.064709 lr 0.00029729 rank 4
2023-02-27 14:45:54,662 DEBUG TRAIN Batch 45/5100 loss 4.500801 loss_att 9.606439 loss_ctc 9.092387 loss_rnnt 2.603365 hw_loss 0.495180 lr 0.00029729 rank 2
2023-02-27 14:45:54,666 DEBUG TRAIN Batch 45/5100 loss 6.980783 loss_att 7.237625 loss_ctc 7.991983 loss_rnnt 6.629172 hw_loss 0.310156 lr 0.00029730 rank 0
2023-02-27 14:45:54,667 DEBUG TRAIN Batch 45/5100 loss 8.960292 loss_att 12.201896 loss_ctc 14.012149 loss_rnnt 7.482700 hw_loss 0.291919 lr 0.00029729 rank 3
2023-02-27 14:45:54,670 DEBUG TRAIN Batch 45/5100 loss 4.164514 loss_att 7.474282 loss_ctc 12.038722 loss_rnnt 2.293961 hw_loss 0.297570 lr 0.00029729 rank 7
2023-02-27 14:45:54,671 DEBUG TRAIN Batch 45/5100 loss 9.530951 loss_att 14.103302 loss_ctc 22.447319 loss_rnnt 6.687515 hw_loss 0.387716 lr 0.00029729 rank 6
2023-02-27 14:45:54,671 DEBUG TRAIN Batch 45/5100 loss 3.188895 loss_att 10.644520 loss_ctc 7.428647 loss_rnnt 0.978933 hw_loss 0.287883 lr 0.00029729 rank 1
2023-02-27 14:45:54,675 DEBUG TRAIN Batch 45/5100 loss 8.082794 loss_att 9.739462 loss_ctc 12.847426 loss_rnnt 7.004472 hw_loss 0.209446 lr 0.00029729 rank 5
2023-02-27 14:46:33,038 DEBUG TRAIN Batch 45/5200 loss 1.260931 loss_att 4.068009 loss_ctc 1.933322 loss_rnnt 0.533420 hw_loss 0.143331 lr 0.00029728 rank 3
2023-02-27 14:46:33,041 DEBUG TRAIN Batch 45/5200 loss 1.395605 loss_att 4.504965 loss_ctc 2.187757 loss_rnnt 0.517837 hw_loss 0.281767 lr 0.00029728 rank 0
2023-02-27 14:46:33,053 DEBUG TRAIN Batch 45/5200 loss 9.341621 loss_att 15.265667 loss_ctc 18.163723 loss_rnnt 6.860670 hw_loss 0.224744 lr 0.00029728 rank 4
2023-02-27 14:46:33,054 DEBUG TRAIN Batch 45/5200 loss 2.045527 loss_att 5.415782 loss_ctc 4.539951 loss_rnnt 0.900814 hw_loss 0.258885 lr 0.00029728 rank 5
2023-02-27 14:46:33,058 DEBUG TRAIN Batch 45/5200 loss 4.940130 loss_att 9.138025 loss_ctc 9.613222 loss_rnnt 3.351718 hw_loss 0.235789 lr 0.00029728 rank 1
2023-02-27 14:46:33,061 DEBUG TRAIN Batch 45/5200 loss 5.295771 loss_att 6.993203 loss_ctc 7.762058 loss_rnnt 4.533510 hw_loss 0.176130 lr 0.00029728 rank 2
2023-02-27 14:46:33,061 DEBUG TRAIN Batch 45/5200 loss 1.886738 loss_att 6.238099 loss_ctc 4.160676 loss_rnnt 0.516658 hw_loss 0.368656 lr 0.00029728 rank 7
2023-02-27 14:46:33,062 DEBUG TRAIN Batch 45/5200 loss 3.267539 loss_att 5.594854 loss_ctc 9.186302 loss_rnnt 1.879030 hw_loss 0.251019 lr 0.00029728 rank 6
2023-02-27 14:47:12,807 DEBUG TRAIN Batch 45/5300 loss 2.832173 loss_att 5.854751 loss_ctc 5.417121 loss_rnnt 1.789366 hw_loss 0.175559 lr 0.00029727 rank 0
2023-02-27 14:47:12,815 DEBUG TRAIN Batch 45/5300 loss 11.626120 loss_att 15.504629 loss_ctc 21.734600 loss_rnnt 9.297579 hw_loss 0.384450 lr 0.00029726 rank 3
2023-02-27 14:47:12,821 DEBUG TRAIN Batch 45/5300 loss 7.390502 loss_att 10.752057 loss_ctc 11.158383 loss_rnnt 6.011779 hw_loss 0.382550 lr 0.00029727 rank 6
2023-02-27 14:47:12,821 DEBUG TRAIN Batch 45/5300 loss 7.209038 loss_att 9.966654 loss_ctc 11.282577 loss_rnnt 6.039602 hw_loss 0.140203 lr 0.00029727 rank 5
2023-02-27 14:47:12,821 DEBUG TRAIN Batch 45/5300 loss 6.066608 loss_att 7.475952 loss_ctc 6.475333 loss_rnnt 5.555855 hw_loss 0.326978 lr 0.00029727 rank 2
2023-02-27 14:47:12,823 DEBUG TRAIN Batch 45/5300 loss 17.999170 loss_att 23.778301 loss_ctc 31.179699 loss_rnnt 14.925217 hw_loss 0.301354 lr 0.00029727 rank 1
2023-02-27 14:47:12,825 DEBUG TRAIN Batch 45/5300 loss 7.251090 loss_att 8.875093 loss_ctc 8.954803 loss_rnnt 6.505703 hw_loss 0.362670 lr 0.00029727 rank 4
2023-02-27 14:47:12,841 DEBUG TRAIN Batch 45/5300 loss 5.668376 loss_att 6.449829 loss_ctc 6.340374 loss_rnnt 5.277084 hw_loss 0.272628 lr 0.00029727 rank 7
2023-02-27 14:48:17,348 DEBUG TRAIN Batch 45/5400 loss 5.814187 loss_att 10.889220 loss_ctc 17.548618 loss_rnnt 3.137934 hw_loss 0.181230 lr 0.00029725 rank 2
2023-02-27 14:48:17,350 DEBUG TRAIN Batch 45/5400 loss 8.287691 loss_att 11.030123 loss_ctc 13.938086 loss_rnnt 6.841984 hw_loss 0.269692 lr 0.00029725 rank 6
2023-02-27 14:48:17,349 DEBUG TRAIN Batch 45/5400 loss 9.152054 loss_att 11.146078 loss_ctc 18.039948 loss_rnnt 7.465329 hw_loss 0.192875 lr 0.00029725 rank 4
2023-02-27 14:48:17,354 DEBUG TRAIN Batch 45/5400 loss 13.994171 loss_att 17.604992 loss_ctc 20.515816 loss_rnnt 12.236888 hw_loss 0.310437 lr 0.00029726 rank 0
2023-02-27 14:48:17,355 DEBUG TRAIN Batch 45/5400 loss 2.578750 loss_att 6.484083 loss_ctc 5.101763 loss_rnnt 1.320508 hw_loss 0.263950 lr 0.00029726 rank 1
2023-02-27 14:48:17,354 DEBUG TRAIN Batch 45/5400 loss 6.427369 loss_att 8.389173 loss_ctc 12.443347 loss_rnnt 5.153578 hw_loss 0.148688 lr 0.00029725 rank 3
2023-02-27 14:48:17,355 DEBUG TRAIN Batch 45/5400 loss 4.187269 loss_att 10.106998 loss_ctc 15.446997 loss_rnnt 1.364637 hw_loss 0.257606 lr 0.00029725 rank 7
2023-02-27 14:48:17,374 DEBUG TRAIN Batch 45/5400 loss 6.810982 loss_att 9.696285 loss_ctc 9.281143 loss_rnnt 5.702627 hw_loss 0.378637 lr 0.00029725 rank 5
2023-02-27 14:48:55,898 DEBUG TRAIN Batch 45/5500 loss 8.155896 loss_att 12.579752 loss_ctc 14.090441 loss_rnnt 6.349779 hw_loss 0.243885 lr 0.00029724 rank 6
2023-02-27 14:48:55,898 DEBUG TRAIN Batch 45/5500 loss 6.660627 loss_att 9.606975 loss_ctc 12.312462 loss_rnnt 5.102764 hw_loss 0.403154 lr 0.00029724 rank 0
2023-02-27 14:48:55,898 DEBUG TRAIN Batch 45/5500 loss 5.414859 loss_att 7.056669 loss_ctc 8.527139 loss_rnnt 4.530130 hw_loss 0.265117 lr 0.00029724 rank 4
2023-02-27 14:48:55,899 DEBUG TRAIN Batch 45/5500 loss 3.388899 loss_att 6.129123 loss_ctc 5.802366 loss_rnnt 2.419634 hw_loss 0.186421 lr 0.00029724 rank 7
2023-02-27 14:48:55,899 DEBUG TRAIN Batch 45/5500 loss 3.056643 loss_att 6.867640 loss_ctc 8.900090 loss_rnnt 1.359083 hw_loss 0.292939 lr 0.00029724 rank 1
2023-02-27 14:48:55,901 DEBUG TRAIN Batch 45/5500 loss 9.352886 loss_att 13.922193 loss_ctc 16.540594 loss_rnnt 7.394125 hw_loss 0.162257 lr 0.00029724 rank 3
2023-02-27 14:48:55,902 DEBUG TRAIN Batch 45/5500 loss 5.925941 loss_att 10.037051 loss_ctc 10.461285 loss_rnnt 4.392706 hw_loss 0.199312 lr 0.00029724 rank 2
2023-02-27 14:48:55,903 DEBUG TRAIN Batch 45/5500 loss 10.609176 loss_att 14.390417 loss_ctc 16.762825 loss_rnnt 8.989820 hw_loss 0.079913 lr 0.00029724 rank 5
2023-02-27 14:49:35,105 DEBUG TRAIN Batch 45/5600 loss 9.859347 loss_att 12.609394 loss_ctc 18.869251 loss_rnnt 8.010217 hw_loss 0.183376 lr 0.00029723 rank 0
2023-02-27 14:49:35,110 DEBUG TRAIN Batch 45/5600 loss 8.618994 loss_att 13.364902 loss_ctc 14.693894 loss_rnnt 6.721272 hw_loss 0.259788 lr 0.00029722 rank 3
2023-02-27 14:49:35,117 DEBUG TRAIN Batch 45/5600 loss 1.950621 loss_att 4.927172 loss_ctc 3.832253 loss_rnnt 0.949561 hw_loss 0.290371 lr 0.00029723 rank 7
2023-02-27 14:49:35,120 DEBUG TRAIN Batch 45/5600 loss 6.805427 loss_att 8.861741 loss_ctc 8.011993 loss_rnnt 6.065663 hw_loss 0.314297 lr 0.00029723 rank 2
2023-02-27 14:49:35,121 DEBUG TRAIN Batch 45/5600 loss 4.720091 loss_att 6.894001 loss_ctc 8.441138 loss_rnnt 3.618904 hw_loss 0.319247 lr 0.00029723 rank 6
2023-02-27 14:49:35,121 DEBUG TRAIN Batch 45/5600 loss 7.227372 loss_att 9.485543 loss_ctc 12.278473 loss_rnnt 5.975034 hw_loss 0.238544 lr 0.00029723 rank 4
2023-02-27 14:49:35,144 DEBUG TRAIN Batch 45/5600 loss 11.749922 loss_att 14.886169 loss_ctc 17.975332 loss_rnnt 10.181355 hw_loss 0.208615 lr 0.00029723 rank 5
2023-02-27 14:49:35,156 DEBUG TRAIN Batch 45/5600 loss 12.312096 loss_att 14.873932 loss_ctc 20.425627 loss_rnnt 10.650227 hw_loss 0.126935 lr 0.00029723 rank 1
2023-02-27 14:50:41,117 DEBUG TRAIN Batch 45/5700 loss 3.056941 loss_att 4.751948 loss_ctc 4.192406 loss_rnnt 2.426386 hw_loss 0.262798 lr 0.00029721 rank 6
2023-02-27 14:50:41,118 DEBUG TRAIN Batch 45/5700 loss 6.357745 loss_att 9.215901 loss_ctc 10.190837 loss_rnnt 5.138732 hw_loss 0.255567 lr 0.00029721 rank 3
2023-02-27 14:50:41,124 DEBUG TRAIN Batch 45/5700 loss 7.838118 loss_att 11.605821 loss_ctc 15.431404 loss_rnnt 5.974528 hw_loss 0.183021 lr 0.00029722 rank 0
2023-02-27 14:50:41,127 DEBUG TRAIN Batch 45/5700 loss 6.962901 loss_att 8.502868 loss_ctc 12.029991 loss_rnnt 5.818984 hw_loss 0.300585 lr 0.00029721 rank 7
2023-02-27 14:50:41,127 DEBUG TRAIN Batch 45/5700 loss 6.710013 loss_att 9.234783 loss_ctc 10.366715 loss_rnnt 5.580070 hw_loss 0.257680 lr 0.00029721 rank 2
2023-02-27 14:50:41,128 DEBUG TRAIN Batch 45/5700 loss 7.576035 loss_att 8.672093 loss_ctc 11.910069 loss_rnnt 6.627443 hw_loss 0.284079 lr 0.00029721 rank 4
2023-02-27 14:50:41,129 DEBUG TRAIN Batch 45/5700 loss 1.134753 loss_att 4.674688 loss_ctc 2.124369 loss_rnnt 0.185117 hw_loss 0.205688 lr 0.00029722 rank 1
2023-02-27 14:50:41,133 DEBUG TRAIN Batch 45/5700 loss 7.563154 loss_att 10.141676 loss_ctc 14.069534 loss_rnnt 6.095915 hw_loss 0.157533 lr 0.00029721 rank 5
2023-02-27 14:51:20,540 DEBUG TRAIN Batch 45/5800 loss 3.425955 loss_att 6.588403 loss_ctc 6.497457 loss_rnnt 2.283175 hw_loss 0.188920 lr 0.00029720 rank 7
2023-02-27 14:51:20,544 DEBUG TRAIN Batch 45/5800 loss 5.366439 loss_att 5.865935 loss_ctc 8.491786 loss_rnnt 4.671930 hw_loss 0.333557 lr 0.00029720 rank 3
2023-02-27 14:51:20,545 DEBUG TRAIN Batch 45/5800 loss 8.650041 loss_att 14.571827 loss_ctc 19.007145 loss_rnnt 5.901080 hw_loss 0.344355 lr 0.00029720 rank 6
2023-02-27 14:51:20,546 DEBUG TRAIN Batch 45/5800 loss 9.448667 loss_att 12.627900 loss_ctc 25.252728 loss_rnnt 6.669609 hw_loss 0.067506 lr 0.00029720 rank 0
2023-02-27 14:51:20,547 DEBUG TRAIN Batch 45/5800 loss 6.265319 loss_att 8.720391 loss_ctc 11.051931 loss_rnnt 5.040110 hw_loss 0.179963 lr 0.00029720 rank 4
2023-02-27 14:51:20,548 DEBUG TRAIN Batch 45/5800 loss 1.362265 loss_att 3.840251 loss_ctc 4.103248 loss_rnnt 0.348598 hw_loss 0.286134 lr 0.00029720 rank 5
2023-02-27 14:51:20,553 DEBUG TRAIN Batch 45/5800 loss 9.935163 loss_att 13.083477 loss_ctc 13.958021 loss_rnnt 8.570206 hw_loss 0.372961 lr 0.00029720 rank 1
2023-02-27 14:51:20,565 DEBUG TRAIN Batch 45/5800 loss 9.499388 loss_att 15.577840 loss_ctc 18.398310 loss_rnnt 6.993055 hw_loss 0.195223 lr 0.00029720 rank 2
2023-02-27 14:51:59,166 DEBUG TRAIN Batch 45/5900 loss 5.198051 loss_att 7.307811 loss_ctc 10.354964 loss_rnnt 3.841718 hw_loss 0.462736 lr 0.00029719 rank 5
2023-02-27 14:51:59,165 DEBUG TRAIN Batch 45/5900 loss 8.449379 loss_att 11.368071 loss_ctc 10.446621 loss_rnnt 7.396275 hw_loss 0.380750 lr 0.00029719 rank 4
2023-02-27 14:51:59,165 DEBUG TRAIN Batch 45/5900 loss 4.956714 loss_att 10.113569 loss_ctc 11.726726 loss_rnnt 2.944683 hw_loss 0.146234 lr 0.00029719 rank 3
2023-02-27 14:51:59,169 DEBUG TRAIN Batch 45/5900 loss 5.540831 loss_att 9.095023 loss_ctc 9.053257 loss_rnnt 4.359759 hw_loss 0.003581 lr 0.00029719 rank 0
2023-02-27 14:51:59,169 DEBUG TRAIN Batch 45/5900 loss 2.075062 loss_att 5.421734 loss_ctc 4.217222 loss_rnnt 0.994746 hw_loss 0.235049 lr 0.00029719 rank 6
2023-02-27 14:51:59,170 DEBUG TRAIN Batch 45/5900 loss 14.573039 loss_att 14.556251 loss_ctc 26.664434 loss_rnnt 12.835333 hw_loss 0.241646 lr 0.00029719 rank 7
2023-02-27 14:51:59,174 DEBUG TRAIN Batch 45/5900 loss 11.210981 loss_att 14.388601 loss_ctc 17.595098 loss_rnnt 9.579460 hw_loss 0.271462 lr 0.00029719 rank 2
2023-02-27 14:51:59,176 DEBUG TRAIN Batch 45/5900 loss 2.670913 loss_att 5.911907 loss_ctc 4.236845 loss_rnnt 1.719991 hw_loss 0.176122 lr 0.00029719 rank 1
2023-02-27 14:52:39,468 DEBUG TRAIN Batch 45/6000 loss 7.287282 loss_att 12.007778 loss_ctc 10.655010 loss_rnnt 5.829424 hw_loss 0.121365 lr 0.00029717 rank 5
2023-02-27 14:52:39,468 DEBUG TRAIN Batch 45/6000 loss 9.662001 loss_att 11.913214 loss_ctc 13.269049 loss_rnnt 8.564426 hw_loss 0.311985 lr 0.00029717 rank 3
2023-02-27 14:52:39,470 DEBUG TRAIN Batch 45/6000 loss 4.763481 loss_att 7.922945 loss_ctc 6.230301 loss_rnnt 3.808178 hw_loss 0.239689 lr 0.00029718 rank 1
2023-02-27 14:52:39,471 DEBUG TRAIN Batch 45/6000 loss 6.859282 loss_att 11.157465 loss_ctc 8.904416 loss_rnnt 5.662467 hw_loss 0.120928 lr 0.00029718 rank 4
2023-02-27 14:52:39,478 DEBUG TRAIN Batch 45/6000 loss 3.676832 loss_att 8.004259 loss_ctc 5.707291 loss_rnnt 2.395283 hw_loss 0.272505 lr 0.00029717 rank 2
2023-02-27 14:52:39,479 DEBUG TRAIN Batch 45/6000 loss 4.758922 loss_att 6.957277 loss_ctc 6.683080 loss_rnnt 3.962810 hw_loss 0.187287 lr 0.00029718 rank 0
2023-02-27 14:52:39,479 DEBUG TRAIN Batch 45/6000 loss 3.482464 loss_att 7.917853 loss_ctc 9.012158 loss_rnnt 1.793638 hw_loss 0.120855 lr 0.00029717 rank 6
2023-02-27 14:52:39,518 DEBUG TRAIN Batch 45/6000 loss 3.208696 loss_att 5.888194 loss_ctc 4.896887 loss_rnnt 2.305013 hw_loss 0.267547 lr 0.00029717 rank 7
2023-02-27 14:53:43,862 DEBUG TRAIN Batch 45/6100 loss 6.343040 loss_att 8.196104 loss_ctc 8.494457 loss_rnnt 5.568807 hw_loss 0.218932 lr 0.00029716 rank 6
2023-02-27 14:53:43,874 DEBUG TRAIN Batch 45/6100 loss 3.613659 loss_att 6.572064 loss_ctc 6.355013 loss_rnnt 2.474290 hw_loss 0.341576 lr 0.00029716 rank 0
2023-02-27 14:53:43,877 DEBUG TRAIN Batch 45/6100 loss 10.569977 loss_att 14.659279 loss_ctc 21.889969 loss_rnnt 8.122138 hw_loss 0.226212 lr 0.00029716 rank 3
2023-02-27 14:53:43,879 DEBUG TRAIN Batch 45/6100 loss 14.923913 loss_att 17.942390 loss_ctc 22.048035 loss_rnnt 13.244512 hw_loss 0.235918 lr 0.00029716 rank 5
2023-02-27 14:53:43,880 DEBUG TRAIN Batch 45/6100 loss 7.926427 loss_att 10.485189 loss_ctc 11.217857 loss_rnnt 6.804978 hw_loss 0.320324 lr 0.00029716 rank 7
2023-02-27 14:53:43,880 DEBUG TRAIN Batch 45/6100 loss 5.473404 loss_att 7.626575 loss_ctc 8.454622 loss_rnnt 4.538136 hw_loss 0.200886 lr 0.00029716 rank 1
2023-02-27 14:53:43,889 DEBUG TRAIN Batch 45/6100 loss 7.901668 loss_att 11.196718 loss_ctc 10.723619 loss_rnnt 6.768253 hw_loss 0.184021 lr 0.00029716 rank 4
2023-02-27 14:53:43,933 DEBUG TRAIN Batch 45/6100 loss 4.352885 loss_att 6.658875 loss_ctc 6.881861 loss_rnnt 3.459939 hw_loss 0.177283 lr 0.00029716 rank 2
2023-02-27 14:54:22,653 DEBUG TRAIN Batch 45/6200 loss 6.261579 loss_att 8.519634 loss_ctc 8.267103 loss_rnnt 5.442110 hw_loss 0.188353 lr 0.00029715 rank 3
2023-02-27 14:54:22,656 DEBUG TRAIN Batch 45/6200 loss 1.829394 loss_att 5.097522 loss_ctc 3.264544 loss_rnnt 0.873779 hw_loss 0.207444 lr 0.00029715 rank 6
2023-02-27 14:54:22,658 DEBUG TRAIN Batch 45/6200 loss 10.721928 loss_att 14.292463 loss_ctc 16.459236 loss_rnnt 9.181550 hw_loss 0.114929 lr 0.00029715 rank 5
2023-02-27 14:54:22,658 DEBUG TRAIN Batch 45/6200 loss 4.963284 loss_att 7.324451 loss_ctc 10.208750 loss_rnnt 3.613094 hw_loss 0.334801 lr 0.00029715 rank 0
2023-02-27 14:54:22,666 DEBUG TRAIN Batch 45/6200 loss 7.436136 loss_att 10.749883 loss_ctc 13.165756 loss_rnnt 5.917204 hw_loss 0.172937 lr 0.00029715 rank 4
2023-02-27 14:54:22,667 DEBUG TRAIN Batch 45/6200 loss 4.603947 loss_att 7.653990 loss_ctc 6.414819 loss_rnnt 3.615908 hw_loss 0.256090 lr 0.00029715 rank 7
2023-02-27 14:54:22,671 DEBUG TRAIN Batch 45/6200 loss 5.581659 loss_att 7.758414 loss_ctc 8.973121 loss_rnnt 4.544209 hw_loss 0.281071 lr 0.00029715 rank 1
2023-02-27 14:54:22,714 DEBUG TRAIN Batch 45/6200 loss 6.880299 loss_att 9.483500 loss_ctc 10.009367 loss_rnnt 5.794925 hw_loss 0.276607 lr 0.00029715 rank 2
2023-02-27 14:55:01,877 DEBUG TRAIN Batch 45/6300 loss 7.113634 loss_att 9.585428 loss_ctc 16.529381 loss_rnnt 5.244367 hw_loss 0.224017 lr 0.00029714 rank 0
2023-02-27 14:55:01,880 DEBUG TRAIN Batch 45/6300 loss 3.097585 loss_att 5.443010 loss_ctc 7.455706 loss_rnnt 1.886862 hw_loss 0.301041 lr 0.00029714 rank 4
2023-02-27 14:55:01,880 DEBUG TRAIN Batch 45/6300 loss 9.039161 loss_att 10.324308 loss_ctc 13.340001 loss_rnnt 8.101086 hw_loss 0.201749 lr 0.00029713 rank 5
2023-02-27 14:55:01,880 DEBUG TRAIN Batch 45/6300 loss 8.297505 loss_att 10.037083 loss_ctc 15.029060 loss_rnnt 6.913930 hw_loss 0.258974 lr 0.00029714 rank 7
2023-02-27 14:55:01,881 DEBUG TRAIN Batch 45/6300 loss 7.315933 loss_att 9.521585 loss_ctc 13.555936 loss_rnnt 5.867838 hw_loss 0.328057 lr 0.00029714 rank 2
2023-02-27 14:55:01,882 DEBUG TRAIN Batch 45/6300 loss 5.521270 loss_att 9.308607 loss_ctc 8.901030 loss_rnnt 4.219438 hw_loss 0.175744 lr 0.00029713 rank 6
2023-02-27 14:55:01,883 DEBUG TRAIN Batch 45/6300 loss 10.293591 loss_att 15.111389 loss_ctc 20.815247 loss_rnnt 7.812987 hw_loss 0.214042 lr 0.00029713 rank 3
2023-02-27 14:55:01,930 DEBUG TRAIN Batch 45/6300 loss 7.482165 loss_att 9.748694 loss_ctc 13.056854 loss_rnnt 6.121809 hw_loss 0.307048 lr 0.00029714 rank 1
2023-02-27 14:56:04,139 DEBUG TRAIN Batch 45/6400 loss 2.654937 loss_att 5.519413 loss_ctc 3.896362 loss_rnnt 1.731010 hw_loss 0.347827 lr 0.00029712 rank 7
2023-02-27 14:56:04,140 DEBUG TRAIN Batch 45/6400 loss 6.504008 loss_att 9.724111 loss_ctc 9.775923 loss_rnnt 5.294748 hw_loss 0.241846 lr 0.00029712 rank 2
2023-02-27 14:56:04,145 DEBUG TRAIN Batch 45/6400 loss 8.764963 loss_att 12.583610 loss_ctc 10.817874 loss_rnnt 7.572097 hw_loss 0.291406 lr 0.00029712 rank 6
2023-02-27 14:56:04,147 DEBUG TRAIN Batch 45/6400 loss 6.114197 loss_att 6.058035 loss_ctc 8.903933 loss_rnnt 5.507523 hw_loss 0.461142 lr 0.00029713 rank 0
2023-02-27 14:56:04,148 DEBUG TRAIN Batch 45/6400 loss 7.688381 loss_att 9.514825 loss_ctc 13.053801 loss_rnnt 6.435903 hw_loss 0.322126 lr 0.00029712 rank 3
2023-02-27 14:56:04,154 DEBUG TRAIN Batch 45/6400 loss 8.313319 loss_att 9.435555 loss_ctc 14.159627 loss_rnnt 7.107344 hw_loss 0.378787 lr 0.00029712 rank 5
2023-02-27 14:56:04,155 DEBUG TRAIN Batch 45/6400 loss 10.855543 loss_att 14.584490 loss_ctc 18.571987 loss_rnnt 8.942738 hw_loss 0.259044 lr 0.00029712 rank 4
2023-02-27 14:56:04,167 DEBUG TRAIN Batch 45/6400 loss 2.012562 loss_att 4.222785 loss_ctc 3.837188 loss_rnnt 1.213610 hw_loss 0.213045 lr 0.00029712 rank 1
2023-02-27 14:56:47,737 DEBUG TRAIN Batch 45/6500 loss 8.384473 loss_att 11.572977 loss_ctc 11.164179 loss_rnnt 7.224903 hw_loss 0.283577 lr 0.00029711 rank 2
2023-02-27 14:56:47,742 DEBUG TRAIN Batch 45/6500 loss 3.876909 loss_att 6.041408 loss_ctc 7.447803 loss_rnnt 2.825077 hw_loss 0.267775 lr 0.00029711 rank 3
2023-02-27 14:56:47,743 DEBUG TRAIN Batch 45/6500 loss 3.730395 loss_att 8.629864 loss_ctc 8.571642 loss_rnnt 1.992619 hw_loss 0.210717 lr 0.00029711 rank 5
2023-02-27 14:56:47,746 DEBUG TRAIN Batch 45/6500 loss 7.208754 loss_att 10.905948 loss_ctc 12.252731 loss_rnnt 5.628820 hw_loss 0.314932 lr 0.00029711 rank 6
2023-02-27 14:56:47,752 DEBUG TRAIN Batch 45/6500 loss 2.433952 loss_att 5.754248 loss_ctc 6.167636 loss_rnnt 1.121634 hw_loss 0.282063 lr 0.00029711 rank 7
2023-02-27 14:56:47,752 DEBUG TRAIN Batch 45/6500 loss 2.551877 loss_att 5.320865 loss_ctc 4.625314 loss_rnnt 1.580419 hw_loss 0.264755 lr 0.00029711 rank 1
2023-02-27 14:56:47,752 DEBUG TRAIN Batch 45/6500 loss 3.342173 loss_att 7.404470 loss_ctc 9.706383 loss_rnnt 1.577576 hw_loss 0.194205 lr 0.00029711 rank 0
2023-02-27 14:56:47,799 DEBUG TRAIN Batch 45/6500 loss 17.794268 loss_att 18.176346 loss_ctc 23.654997 loss_rnnt 16.872353 hw_loss 0.120128 lr 0.00029711 rank 4
2023-02-27 14:57:26,493 DEBUG TRAIN Batch 45/6600 loss 5.910081 loss_att 13.885401 loss_ctc 7.324589 loss_rnnt 3.943672 hw_loss 0.342646 lr 0.00029709 rank 3
2023-02-27 14:57:26,496 DEBUG TRAIN Batch 45/6600 loss 4.710483 loss_att 7.143848 loss_ctc 6.312928 loss_rnnt 3.911775 hw_loss 0.184454 lr 0.00029710 rank 4
2023-02-27 14:57:26,496 DEBUG TRAIN Batch 45/6600 loss 5.963055 loss_att 8.285242 loss_ctc 9.848992 loss_rnnt 4.961437 hw_loss 0.035730 lr 0.00029710 rank 0
2023-02-27 14:57:26,497 DEBUG TRAIN Batch 45/6600 loss 4.561110 loss_att 9.114876 loss_ctc 10.586845 loss_rnnt 2.752708 hw_loss 0.176656 lr 0.00029709 rank 6
2023-02-27 14:57:26,501 DEBUG TRAIN Batch 45/6600 loss 3.795297 loss_att 6.092636 loss_ctc 8.236338 loss_rnnt 2.684608 hw_loss 0.110779 lr 0.00029710 rank 2
2023-02-27 14:57:26,503 DEBUG TRAIN Batch 45/6600 loss 5.539486 loss_att 8.173189 loss_ctc 9.451226 loss_rnnt 4.396826 hw_loss 0.176913 lr 0.00029710 rank 7
2023-02-27 14:57:26,509 DEBUG TRAIN Batch 45/6600 loss 8.901338 loss_att 13.294047 loss_ctc 16.487919 loss_rnnt 6.869079 hw_loss 0.266577 lr 0.00029710 rank 5
2023-02-27 14:57:26,520 DEBUG TRAIN Batch 45/6600 loss 9.325625 loss_att 12.910448 loss_ctc 14.911078 loss_rnnt 7.699355 hw_loss 0.308586 lr 0.00029710 rank 1
2023-02-27 14:58:05,551 DEBUG TRAIN Batch 45/6700 loss 5.143268 loss_att 8.449814 loss_ctc 11.724763 loss_rnnt 3.520064 hw_loss 0.158177 lr 0.00029708 rank 4
2023-02-27 14:58:05,554 DEBUG TRAIN Batch 45/6700 loss 4.749191 loss_att 10.452499 loss_ctc 8.687654 loss_rnnt 2.871463 hw_loss 0.397384 lr 0.00029708 rank 7
2023-02-27 14:58:05,562 DEBUG TRAIN Batch 45/6700 loss 3.185734 loss_att 6.227466 loss_ctc 5.175182 loss_rnnt 2.202937 hw_loss 0.204734 lr 0.00029708 rank 5
2023-02-27 14:58:05,567 DEBUG TRAIN Batch 45/6700 loss 6.421666 loss_att 8.607152 loss_ctc 9.771582 loss_rnnt 5.392935 hw_loss 0.271835 lr 0.00029708 rank 6
2023-02-27 14:58:05,568 DEBUG TRAIN Batch 45/6700 loss 2.513175 loss_att 5.713156 loss_ctc 4.903472 loss_rnnt 1.411153 hw_loss 0.268722 lr 0.00029708 rank 2
2023-02-27 14:58:05,583 DEBUG TRAIN Batch 45/6700 loss 8.481774 loss_att 13.016050 loss_ctc 17.690844 loss_rnnt 6.184097 hw_loss 0.305524 lr 0.00029708 rank 3
2023-02-27 14:58:05,591 DEBUG TRAIN Batch 45/6700 loss 6.360907 loss_att 9.070660 loss_ctc 11.338631 loss_rnnt 5.030329 hw_loss 0.234246 lr 0.00029709 rank 0
2023-02-27 14:58:05,600 DEBUG TRAIN Batch 45/6700 loss 7.424588 loss_att 8.846270 loss_ctc 11.898237 loss_rnnt 6.358753 hw_loss 0.346897 lr 0.00029708 rank 1
2023-02-27 14:59:11,479 DEBUG TRAIN Batch 45/6800 loss 3.129362 loss_att 6.280126 loss_ctc 5.632148 loss_rnnt 1.987413 hw_loss 0.333922 lr 0.00029707 rank 6
2023-02-27 14:59:11,479 DEBUG TRAIN Batch 45/6800 loss 5.674189 loss_att 7.783127 loss_ctc 6.182189 loss_rnnt 4.953241 hw_loss 0.433924 lr 0.00029707 rank 3
2023-02-27 14:59:11,483 DEBUG TRAIN Batch 45/6800 loss 6.044871 loss_att 7.745565 loss_ctc 8.121174 loss_rnnt 5.398977 hw_loss 0.054215 lr 0.00029707 rank 1
2023-02-27 14:59:11,486 DEBUG TRAIN Batch 45/6800 loss 7.539590 loss_att 11.333515 loss_ctc 14.796015 loss_rnnt 5.732593 hw_loss 0.151292 lr 0.00029707 rank 0
2023-02-27 14:59:11,487 DEBUG TRAIN Batch 45/6800 loss 9.294966 loss_att 9.356692 loss_ctc 12.215876 loss_rnnt 8.793453 hw_loss 0.186960 lr 0.00029707 rank 2
2023-02-27 14:59:11,487 DEBUG TRAIN Batch 45/6800 loss 2.913931 loss_att 5.293118 loss_ctc 6.060564 loss_rnnt 1.843506 hw_loss 0.328193 lr 0.00029707 rank 7
2023-02-27 14:59:11,488 DEBUG TRAIN Batch 45/6800 loss 8.493307 loss_att 10.548796 loss_ctc 13.017383 loss_rnnt 7.331534 hw_loss 0.276497 lr 0.00029707 rank 4
2023-02-27 14:59:11,490 DEBUG TRAIN Batch 45/6800 loss 1.998776 loss_att 3.532641 loss_ctc 2.595077 loss_rnnt 1.519916 hw_loss 0.173588 lr 0.00029707 rank 5
2023-02-27 14:59:51,064 DEBUG TRAIN Batch 45/6900 loss 12.015343 loss_att 14.145045 loss_ctc 18.451122 loss_rnnt 10.579092 hw_loss 0.285387 lr 0.00029706 rank 6
2023-02-27 14:59:51,066 DEBUG TRAIN Batch 45/6900 loss 2.370669 loss_att 4.017983 loss_ctc 2.517236 loss_rnnt 1.833415 hw_loss 0.352968 lr 0.00029705 rank 3
2023-02-27 14:59:51,070 DEBUG TRAIN Batch 45/6900 loss 11.958455 loss_att 17.673035 loss_ctc 24.856382 loss_rnnt 9.005039 hw_loss 0.170204 lr 0.00029706 rank 0
2023-02-27 14:59:51,073 DEBUG TRAIN Batch 45/6900 loss 7.572790 loss_att 10.037724 loss_ctc 11.357113 loss_rnnt 6.455124 hw_loss 0.225193 lr 0.00029706 rank 2
2023-02-27 14:59:51,074 DEBUG TRAIN Batch 45/6900 loss 4.077160 loss_att 6.283686 loss_ctc 5.131473 loss_rnnt 3.263819 hw_loss 0.433989 lr 0.00029706 rank 5
2023-02-27 14:59:51,078 DEBUG TRAIN Batch 45/6900 loss 8.302475 loss_att 9.990570 loss_ctc 11.637842 loss_rnnt 7.429214 hw_loss 0.170485 lr 0.00029706 rank 7
2023-02-27 14:59:51,090 DEBUG TRAIN Batch 45/6900 loss 8.523381 loss_att 9.586333 loss_ctc 12.562215 loss_rnnt 7.656123 hw_loss 0.217792 lr 0.00029706 rank 1
2023-02-27 14:59:51,118 DEBUG TRAIN Batch 45/6900 loss 6.409475 loss_att 8.494177 loss_ctc 12.328460 loss_rnnt 5.029044 hw_loss 0.326798 lr 0.00029706 rank 4
2023-02-27 15:00:30,473 DEBUG TRAIN Batch 45/7000 loss 5.131276 loss_att 5.389425 loss_ctc 5.682992 loss_rnnt 4.796674 hw_loss 0.392642 lr 0.00029705 rank 0
2023-02-27 15:00:30,486 DEBUG TRAIN Batch 45/7000 loss 2.939521 loss_att 5.715710 loss_ctc 4.991241 loss_rnnt 1.955953 hw_loss 0.290188 lr 0.00029704 rank 5
2023-02-27 15:00:30,488 DEBUG TRAIN Batch 45/7000 loss 8.522680 loss_att 7.794365 loss_ctc 9.552029 loss_rnnt 8.362692 hw_loss 0.315760 lr 0.00029704 rank 2
2023-02-27 15:00:30,488 DEBUG TRAIN Batch 45/7000 loss 4.099578 loss_att 5.352496 loss_ctc 6.651550 loss_rnnt 3.340577 hw_loss 0.315290 lr 0.00029704 rank 6
2023-02-27 15:00:30,490 DEBUG TRAIN Batch 45/7000 loss 6.241351 loss_att 9.538755 loss_ctc 11.824930 loss_rnnt 4.759538 hw_loss 0.145978 lr 0.00029705 rank 1
2023-02-27 15:00:30,497 DEBUG TRAIN Batch 45/7000 loss 5.871095 loss_att 7.641457 loss_ctc 14.081245 loss_rnnt 4.276129 hw_loss 0.274140 lr 0.00029704 rank 3
2023-02-27 15:00:30,519 DEBUG TRAIN Batch 45/7000 loss 7.534021 loss_att 8.188883 loss_ctc 14.444208 loss_rnnt 6.253298 hw_loss 0.428235 lr 0.00029704 rank 4
2023-02-27 15:00:30,557 DEBUG TRAIN Batch 45/7000 loss 6.863043 loss_att 8.657056 loss_ctc 12.234313 loss_rnnt 5.752720 hw_loss 0.066284 lr 0.00029704 rank 7
2023-02-27 15:01:32,920 DEBUG TRAIN Batch 45/7100 loss 10.201534 loss_att 10.731689 loss_ctc 13.707904 loss_rnnt 9.490175 hw_loss 0.258399 lr 0.00029703 rank 5
2023-02-27 15:01:32,928 DEBUG TRAIN Batch 45/7100 loss 3.147090 loss_att 4.682846 loss_ctc 4.737390 loss_rnnt 2.508544 hw_loss 0.223789 lr 0.00029703 rank 7
2023-02-27 15:01:32,934 DEBUG TRAIN Batch 45/7100 loss 4.724594 loss_att 7.724860 loss_ctc 9.682101 loss_rnnt 3.415069 hw_loss 0.090881 lr 0.00029703 rank 6
2023-02-27 15:01:32,935 DEBUG TRAIN Batch 45/7100 loss 4.185375 loss_att 7.450303 loss_ctc 4.655272 loss_rnnt 3.381317 hw_loss 0.165785 lr 0.00029703 rank 4
2023-02-27 15:01:32,938 DEBUG TRAIN Batch 45/7100 loss 3.151394 loss_att 6.910665 loss_ctc 7.754426 loss_rnnt 1.663636 hw_loss 0.229062 lr 0.00029703 rank 1
2023-02-27 15:01:32,956 DEBUG TRAIN Batch 45/7100 loss 3.858246 loss_att 6.885270 loss_ctc 7.920608 loss_rnnt 2.539914 hw_loss 0.321150 lr 0.00029703 rank 0
2023-02-27 15:01:32,959 DEBUG TRAIN Batch 45/7100 loss 2.834676 loss_att 5.427028 loss_ctc 5.778982 loss_rnnt 1.898466 hw_loss 0.047186 lr 0.00029703 rank 2
2023-02-27 15:01:32,964 DEBUG TRAIN Batch 45/7100 loss 9.725531 loss_att 10.720059 loss_ctc 14.975170 loss_rnnt 8.649550 hw_loss 0.332105 lr 0.00029703 rank 3
2023-02-27 15:02:14,819 DEBUG TRAIN Batch 45/7200 loss 3.359247 loss_att 6.259723 loss_ctc 5.406757 loss_rnnt 2.313821 hw_loss 0.360616 lr 0.00029702 rank 0
2023-02-27 15:02:14,838 DEBUG TRAIN Batch 45/7200 loss 6.168653 loss_att 12.698732 loss_ctc 13.795689 loss_rnnt 3.714910 hw_loss 0.245228 lr 0.00029701 rank 3
2023-02-27 15:02:14,841 DEBUG TRAIN Batch 45/7200 loss 6.355860 loss_att 9.425337 loss_ctc 7.681730 loss_rnnt 5.487479 hw_loss 0.145692 lr 0.00029702 rank 7
2023-02-27 15:02:14,847 DEBUG TRAIN Batch 45/7200 loss 5.553046 loss_att 7.337151 loss_ctc 7.832753 loss_rnnt 4.808434 hw_loss 0.157180 lr 0.00029702 rank 2
2023-02-27 15:02:14,847 DEBUG TRAIN Batch 45/7200 loss 5.083969 loss_att 8.194553 loss_ctc 8.621613 loss_rnnt 3.887267 hw_loss 0.192936 lr 0.00029702 rank 6
2023-02-27 15:02:14,848 DEBUG TRAIN Batch 45/7200 loss 5.222194 loss_att 10.515059 loss_ctc 13.474227 loss_rnnt 2.999302 hw_loss 0.120089 lr 0.00029702 rank 5
2023-02-27 15:02:14,852 DEBUG TRAIN Batch 45/7200 loss 5.834677 loss_att 8.150533 loss_ctc 11.339446 loss_rnnt 4.477435 hw_loss 0.300189 lr 0.00029702 rank 4
2023-02-27 15:02:14,891 DEBUG TRAIN Batch 45/7200 loss 4.713953 loss_att 7.259575 loss_ctc 12.583121 loss_rnnt 3.003665 hw_loss 0.284888 lr 0.00029702 rank 1
2023-02-27 15:02:53,889 DEBUG TRAIN Batch 45/7300 loss 4.832843 loss_att 8.854931 loss_ctc 10.132979 loss_rnnt 3.215025 hw_loss 0.200094 lr 0.00029700 rank 6
2023-02-27 15:02:53,889 DEBUG TRAIN Batch 45/7300 loss 5.632294 loss_att 8.757903 loss_ctc 9.152776 loss_rnnt 4.402369 hw_loss 0.253885 lr 0.00029700 rank 2
2023-02-27 15:02:53,892 DEBUG TRAIN Batch 45/7300 loss 3.771189 loss_att 6.325499 loss_ctc 4.425724 loss_rnnt 2.996747 hw_loss 0.330578 lr 0.00029701 rank 0
2023-02-27 15:02:53,892 DEBUG TRAIN Batch 45/7300 loss 7.675383 loss_att 11.184419 loss_ctc 9.912573 loss_rnnt 6.525804 hw_loss 0.280276 lr 0.00029700 rank 4
2023-02-27 15:02:53,892 DEBUG TRAIN Batch 45/7300 loss 5.423607 loss_att 8.920855 loss_ctc 8.379385 loss_rnnt 4.208083 hw_loss 0.228696 lr 0.00029700 rank 3
2023-02-27 15:02:53,894 DEBUG TRAIN Batch 45/7300 loss 9.458220 loss_att 13.542345 loss_ctc 19.009895 loss_rnnt 7.227926 hw_loss 0.262337 lr 0.00029700 rank 7
2023-02-27 15:02:53,897 DEBUG TRAIN Batch 45/7300 loss 8.113206 loss_att 10.382596 loss_ctc 9.463634 loss_rnnt 7.321136 hw_loss 0.296502 lr 0.00029701 rank 1
2023-02-27 15:02:53,901 DEBUG TRAIN Batch 45/7300 loss 8.113743 loss_att 10.939183 loss_ctc 10.728988 loss_rnnt 7.036822 hw_loss 0.305874 lr 0.00029700 rank 5
2023-02-27 15:03:33,569 DEBUG TRAIN Batch 45/7400 loss 1.776927 loss_att 3.515584 loss_ctc 3.441823 loss_rnnt 1.056269 hw_loss 0.283013 lr 0.00029699 rank 7
2023-02-27 15:03:33,571 DEBUG TRAIN Batch 45/7400 loss 7.739553 loss_att 13.846371 loss_ctc 15.236208 loss_rnnt 5.349948 hw_loss 0.316291 lr 0.00029699 rank 2
2023-02-27 15:03:33,572 DEBUG TRAIN Batch 45/7400 loss 3.411434 loss_att 5.597839 loss_ctc 4.574550 loss_rnnt 2.718972 hw_loss 0.187686 lr 0.00029699 rank 5
2023-02-27 15:03:33,573 DEBUG TRAIN Batch 45/7400 loss 6.497158 loss_att 10.341449 loss_ctc 13.327143 loss_rnnt 4.627730 hw_loss 0.356072 lr 0.00029699 rank 4
2023-02-27 15:03:33,577 DEBUG TRAIN Batch 45/7400 loss 4.872559 loss_att 6.801643 loss_ctc 8.636640 loss_rnnt 3.931662 hw_loss 0.099755 lr 0.00029699 rank 1
2023-02-27 15:03:33,583 DEBUG TRAIN Batch 45/7400 loss 4.407195 loss_att 5.976605 loss_ctc 9.007843 loss_rnnt 3.376970 hw_loss 0.192981 lr 0.00029699 rank 3
2023-02-27 15:03:33,584 DEBUG TRAIN Batch 45/7400 loss 2.275456 loss_att 4.516429 loss_ctc 1.980556 loss_rnnt 1.668675 hw_loss 0.371075 lr 0.00029699 rank 6
2023-02-27 15:03:33,586 DEBUG TRAIN Batch 45/7400 loss 4.924912 loss_att 6.684861 loss_ctc 6.746675 loss_rnnt 4.157815 hw_loss 0.322886 lr 0.00029699 rank 0
2023-02-27 15:04:38,125 DEBUG TRAIN Batch 45/7500 loss 4.302418 loss_att 6.480588 loss_ctc 8.553401 loss_rnnt 3.167157 hw_loss 0.249053 lr 0.00029698 rank 6
2023-02-27 15:04:38,126 DEBUG TRAIN Batch 45/7500 loss 4.373368 loss_att 8.451883 loss_ctc 7.019798 loss_rnnt 3.109326 hw_loss 0.179029 lr 0.00029698 rank 0
2023-02-27 15:04:38,128 DEBUG TRAIN Batch 45/7500 loss 5.515582 loss_att 8.450802 loss_ctc 18.331606 loss_rnnt 3.099551 hw_loss 0.225345 lr 0.00029698 rank 3
2023-02-27 15:04:38,135 DEBUG TRAIN Batch 45/7500 loss 4.253919 loss_att 7.749202 loss_ctc 12.121494 loss_rnnt 2.344554 hw_loss 0.302434 lr 0.00029698 rank 2
2023-02-27 15:04:38,135 DEBUG TRAIN Batch 45/7500 loss 3.750677 loss_att 5.976044 loss_ctc 9.202808 loss_rnnt 2.392595 hw_loss 0.348857 lr 0.00029698 rank 1
2023-02-27 15:04:38,166 DEBUG TRAIN Batch 45/7500 loss 6.891254 loss_att 9.536308 loss_ctc 11.678077 loss_rnnt 5.591068 hw_loss 0.249249 lr 0.00029698 rank 4
2023-02-27 15:04:38,178 DEBUG TRAIN Batch 45/7500 loss 7.941706 loss_att 8.799506 loss_ctc 10.309084 loss_rnnt 7.384622 hw_loss 0.131012 lr 0.00029698 rank 5
2023-02-27 15:04:38,178 DEBUG TRAIN Batch 45/7500 loss 12.012096 loss_att 16.594795 loss_ctc 21.623495 loss_rnnt 9.704080 hw_loss 0.206167 lr 0.00029698 rank 7
2023-02-27 15:05:17,618 DEBUG TRAIN Batch 45/7600 loss 6.189678 loss_att 8.409012 loss_ctc 9.052982 loss_rnnt 5.273608 hw_loss 0.169556 lr 0.00029696 rank 3
2023-02-27 15:05:17,622 DEBUG TRAIN Batch 45/7600 loss 4.386612 loss_att 6.761692 loss_ctc 10.418537 loss_rnnt 2.998360 hw_loss 0.204336 lr 0.00029697 rank 0
2023-02-27 15:05:17,633 DEBUG TRAIN Batch 45/7600 loss 6.364026 loss_att 9.582115 loss_ctc 12.284758 loss_rnnt 4.769023 hw_loss 0.303664 lr 0.00029697 rank 4
2023-02-27 15:05:17,637 DEBUG TRAIN Batch 45/7600 loss 6.332607 loss_att 12.575571 loss_ctc 14.217685 loss_rnnt 3.938097 hw_loss 0.177325 lr 0.00029696 rank 6
2023-02-27 15:05:17,638 DEBUG TRAIN Batch 45/7600 loss 8.330637 loss_att 8.975588 loss_ctc 15.322864 loss_rnnt 7.057818 hw_loss 0.396621 lr 0.00029696 rank 7
2023-02-27 15:05:17,641 DEBUG TRAIN Batch 45/7600 loss 2.063076 loss_att 3.903061 loss_ctc 3.833909 loss_rnnt 1.362949 hw_loss 0.180036 lr 0.00029696 rank 5
2023-02-27 15:05:17,642 DEBUG TRAIN Batch 45/7600 loss 4.678293 loss_att 6.662698 loss_ctc 8.169453 loss_rnnt 3.613238 hw_loss 0.380035 lr 0.00029696 rank 2
2023-02-27 15:05:17,644 DEBUG TRAIN Batch 45/7600 loss 3.463857 loss_att 5.632492 loss_ctc 9.622330 loss_rnnt 2.048008 hw_loss 0.301860 lr 0.00029697 rank 1
2023-02-27 15:05:56,362 DEBUG TRAIN Batch 45/7700 loss 4.927550 loss_att 5.786643 loss_ctc 7.823627 loss_rnnt 4.213036 hw_loss 0.293535 lr 0.00029695 rank 0
2023-02-27 15:05:56,378 DEBUG TRAIN Batch 45/7700 loss 4.845831 loss_att 8.852456 loss_ctc 9.052649 loss_rnnt 3.339655 hw_loss 0.269892 lr 0.00029695 rank 6
2023-02-27 15:05:56,380 DEBUG TRAIN Batch 45/7700 loss 3.696770 loss_att 5.242229 loss_ctc 6.176164 loss_rnnt 2.925785 hw_loss 0.246201 lr 0.00029695 rank 3
2023-02-27 15:05:56,382 DEBUG TRAIN Batch 45/7700 loss 4.602668 loss_att 7.665195 loss_ctc 8.608131 loss_rnnt 3.351178 hw_loss 0.196729 lr 0.00029695 rank 4
2023-02-27 15:05:56,386 DEBUG TRAIN Batch 45/7700 loss 6.660822 loss_att 8.627920 loss_ctc 9.344901 loss_rnnt 5.789266 hw_loss 0.225486 lr 0.00029695 rank 5
2023-02-27 15:05:56,387 DEBUG TRAIN Batch 45/7700 loss 4.103774 loss_att 7.315777 loss_ctc 8.143825 loss_rnnt 2.829266 hw_loss 0.175188 lr 0.00029695 rank 7
2023-02-27 15:05:56,393 DEBUG TRAIN Batch 45/7700 loss 3.174463 loss_att 7.431078 loss_ctc 8.305738 loss_rnnt 1.493204 hw_loss 0.273311 lr 0.00029695 rank 1
2023-02-27 15:05:56,439 DEBUG TRAIN Batch 45/7700 loss 4.622083 loss_att 8.457785 loss_ctc 10.541034 loss_rnnt 2.961128 hw_loss 0.196165 lr 0.00029695 rank 2
2023-02-27 15:06:36,953 DEBUG TRAIN Batch 45/7800 loss 5.395418 loss_att 11.915846 loss_ctc 15.212126 loss_rnnt 2.611209 hw_loss 0.321053 lr 0.00029694 rank 5
2023-02-27 15:06:36,955 DEBUG TRAIN Batch 45/7800 loss 6.514960 loss_att 9.815889 loss_ctc 7.413369 loss_rnnt 5.663879 hw_loss 0.133326 lr 0.00029694 rank 4
2023-02-27 15:06:36,955 DEBUG TRAIN Batch 45/7800 loss 7.858860 loss_att 11.517633 loss_ctc 11.738005 loss_rnnt 6.526597 hw_loss 0.156167 lr 0.00029694 rank 6
2023-02-27 15:06:36,955 DEBUG TRAIN Batch 45/7800 loss 4.157084 loss_att 6.135139 loss_ctc 7.880154 loss_rnnt 3.055608 hw_loss 0.392729 lr 0.00029694 rank 2
2023-02-27 15:06:36,958 DEBUG TRAIN Batch 45/7800 loss 4.984260 loss_att 8.835021 loss_ctc 20.839123 loss_rnnt 2.021181 hw_loss 0.148020 lr 0.00029694 rank 7
2023-02-27 15:06:36,959 DEBUG TRAIN Batch 45/7800 loss 2.430284 loss_att 4.765585 loss_ctc 6.031019 loss_rnnt 1.293584 hw_loss 0.355390 lr 0.00029694 rank 0
2023-02-27 15:06:36,960 DEBUG TRAIN Batch 45/7800 loss 2.997905 loss_att 6.342167 loss_ctc 7.880630 loss_rnnt 1.565664 hw_loss 0.210672 lr 0.00029694 rank 3
2023-02-27 15:06:36,974 DEBUG TRAIN Batch 45/7800 loss 7.725586 loss_att 13.080726 loss_ctc 15.317481 loss_rnnt 5.492599 hw_loss 0.280699 lr 0.00029694 rank 1
2023-02-27 15:07:42,615 DEBUG TRAIN Batch 45/7900 loss 5.146049 loss_att 10.119652 loss_ctc 9.461062 loss_rnnt 3.405253 hw_loss 0.320138 lr 0.00029692 rank 5
2023-02-27 15:07:42,621 DEBUG TRAIN Batch 45/7900 loss 4.880458 loss_att 6.049622 loss_ctc 9.235127 loss_rnnt 3.937949 hw_loss 0.240099 lr 0.00029693 rank 7
2023-02-27 15:07:42,627 DEBUG TRAIN Batch 45/7900 loss 7.180532 loss_att 9.793575 loss_ctc 10.066185 loss_rnnt 6.130898 hw_loss 0.266758 lr 0.00029693 rank 4
2023-02-27 15:07:42,627 DEBUG TRAIN Batch 45/7900 loss 9.507328 loss_att 15.020437 loss_ctc 14.275435 loss_rnnt 7.620164 hw_loss 0.278987 lr 0.00029692 rank 3
2023-02-27 15:07:42,631 DEBUG TRAIN Batch 45/7900 loss 4.455055 loss_att 6.479512 loss_ctc 5.881465 loss_rnnt 3.725797 hw_loss 0.251585 lr 0.00029692 rank 6
2023-02-27 15:07:42,633 DEBUG TRAIN Batch 45/7900 loss 2.720258 loss_att 5.074417 loss_ctc 2.818685 loss_rnnt 2.074507 hw_loss 0.303365 lr 0.00029693 rank 1
2023-02-27 15:07:42,634 DEBUG TRAIN Batch 45/7900 loss 5.919782 loss_att 10.545781 loss_ctc 9.227084 loss_rnnt 4.471562 hw_loss 0.153837 lr 0.00029693 rank 0
2023-02-27 15:07:42,633 DEBUG TRAIN Batch 45/7900 loss 7.545813 loss_att 10.633464 loss_ctc 10.166465 loss_rnnt 6.472980 hw_loss 0.198531 lr 0.00029693 rank 2
2023-02-27 15:08:21,582 DEBUG TRAIN Batch 45/8000 loss 6.333048 loss_att 9.210477 loss_ctc 9.588788 loss_rnnt 5.250587 hw_loss 0.136644 lr 0.00029691 rank 4
2023-02-27 15:08:21,584 DEBUG TRAIN Batch 45/8000 loss 7.349643 loss_att 10.246586 loss_ctc 12.837704 loss_rnnt 5.919327 hw_loss 0.223474 lr 0.00029691 rank 2
2023-02-27 15:08:21,584 DEBUG TRAIN Batch 45/8000 loss 9.526659 loss_att 12.706591 loss_ctc 16.428112 loss_rnnt 7.862798 hw_loss 0.201902 lr 0.00029691 rank 6
2023-02-27 15:08:21,586 DEBUG TRAIN Batch 45/8000 loss 3.097143 loss_att 5.812923 loss_ctc 6.159039 loss_rnnt 2.035562 hw_loss 0.206571 lr 0.00029691 rank 7
2023-02-27 15:08:21,585 DEBUG TRAIN Batch 45/8000 loss 1.218491 loss_att 3.000024 loss_ctc 1.208997 loss_rnnt 0.744256 hw_loss 0.223491 lr 0.00029691 rank 3
2023-02-27 15:08:21,588 DEBUG TRAIN Batch 45/8000 loss 5.105333 loss_att 8.557476 loss_ctc 12.443095 loss_rnnt 3.296494 hw_loss 0.262581 lr 0.00029692 rank 0
2023-02-27 15:08:21,588 DEBUG TRAIN Batch 45/8000 loss 3.517248 loss_att 5.245299 loss_ctc 7.944594 loss_rnnt 2.399081 hw_loss 0.341706 lr 0.00029691 rank 1
2023-02-27 15:08:21,590 DEBUG TRAIN Batch 45/8000 loss 2.070549 loss_att 4.627458 loss_ctc 7.596229 loss_rnnt 0.754713 hw_loss 0.126932 lr 0.00029691 rank 5
2023-02-27 15:09:01,111 DEBUG TRAIN Batch 45/8100 loss 5.786712 loss_att 7.102281 loss_ctc 7.967544 loss_rnnt 5.066370 hw_loss 0.312096 lr 0.00029690 rank 5
2023-02-27 15:09:01,114 DEBUG TRAIN Batch 45/8100 loss 3.333570 loss_att 7.119697 loss_ctc 6.083316 loss_rnnt 2.002854 hw_loss 0.387858 lr 0.00029690 rank 2
2023-02-27 15:09:01,129 DEBUG TRAIN Batch 45/8100 loss 15.256900 loss_att 17.082916 loss_ctc 35.162788 loss_rnnt 12.110526 hw_loss 0.238220 lr 0.00029690 rank 6
2023-02-27 15:09:01,141 DEBUG TRAIN Batch 45/8100 loss 9.417051 loss_att 13.060215 loss_ctc 20.646381 loss_rnnt 7.020021 hw_loss 0.320911 lr 0.00029690 rank 1
2023-02-27 15:09:01,151 DEBUG TRAIN Batch 45/8100 loss 6.819380 loss_att 8.372961 loss_ctc 9.210631 loss_rnnt 6.089187 hw_loss 0.188707 lr 0.00029690 rank 4
2023-02-27 15:09:01,152 DEBUG TRAIN Batch 45/8100 loss 3.181477 loss_att 7.047252 loss_ctc 7.462716 loss_rnnt 1.689973 hw_loss 0.276594 lr 0.00029690 rank 7
2023-02-27 15:09:01,160 DEBUG TRAIN Batch 45/8100 loss 6.787410 loss_att 9.016806 loss_ctc 11.530104 loss_rnnt 5.572910 hw_loss 0.255489 lr 0.00029690 rank 0
2023-02-27 15:09:01,168 DEBUG TRAIN Batch 45/8100 loss 1.864297 loss_att 4.875625 loss_ctc 4.175462 loss_rnnt 0.771244 hw_loss 0.342434 lr 0.00029690 rank 3
2023-02-27 15:09:41,238 DEBUG TRAIN Batch 45/8200 loss 8.802816 loss_att 10.393842 loss_ctc 15.664716 loss_rnnt 7.394832 hw_loss 0.327862 lr 0.00029689 rank 0
2023-02-27 15:09:41,239 DEBUG TRAIN Batch 45/8200 loss 13.735478 loss_att 17.348690 loss_ctc 32.862083 loss_rnnt 10.391908 hw_loss 0.132589 lr 0.00029689 rank 6
2023-02-27 15:09:41,240 DEBUG TRAIN Batch 45/8200 loss 2.074196 loss_att 3.397460 loss_ctc 3.743108 loss_rnnt 1.466312 hw_loss 0.226330 lr 0.00029688 rank 3
2023-02-27 15:09:41,242 DEBUG TRAIN Batch 45/8200 loss 7.344371 loss_att 8.619556 loss_ctc 10.493444 loss_rnnt 6.553761 hw_loss 0.216932 lr 0.00029689 rank 5
2023-02-27 15:09:41,242 DEBUG TRAIN Batch 45/8200 loss 6.568644 loss_att 6.447455 loss_ctc 10.574370 loss_rnnt 5.819594 hw_loss 0.448483 lr 0.00029689 rank 7
2023-02-27 15:09:41,244 DEBUG TRAIN Batch 45/8200 loss 2.799575 loss_att 6.154915 loss_ctc 7.338010 loss_rnnt 1.378348 hw_loss 0.271940 lr 0.00029689 rank 2
2023-02-27 15:09:41,253 DEBUG TRAIN Batch 45/8200 loss 2.935614 loss_att 4.678684 loss_ctc 6.520162 loss_rnnt 1.993922 hw_loss 0.215884 lr 0.00029689 rank 4
2023-02-27 15:09:41,257 DEBUG TRAIN Batch 45/8200 loss 4.165045 loss_att 5.393335 loss_ctc 8.739883 loss_rnnt 3.133202 hw_loss 0.330387 lr 0.00029689 rank 1
2023-02-27 15:10:19,987 DEBUG TRAIN Batch 45/8300 loss 10.071855 loss_att 13.938046 loss_ctc 17.317106 loss_rnnt 8.184236 hw_loss 0.278152 lr 0.00029687 rank 3
2023-02-27 15:10:19,988 DEBUG TRAIN Batch 45/8300 loss 1.830432 loss_att 4.976055 loss_ctc 2.403644 loss_rnnt 0.992407 hw_loss 0.248384 lr 0.00029687 rank 6
2023-02-27 15:10:19,993 DEBUG TRAIN Batch 45/8300 loss 4.918482 loss_att 8.127850 loss_ctc 8.378148 loss_rnnt 3.725094 hw_loss 0.169173 lr 0.00029687 rank 5
2023-02-27 15:10:20,003 DEBUG TRAIN Batch 45/8300 loss 4.752708 loss_att 5.225794 loss_ctc 5.699264 loss_rnnt 4.389045 hw_loss 0.267823 lr 0.00029687 rank 4
2023-02-27 15:10:20,006 DEBUG TRAIN Batch 45/8300 loss 2.183216 loss_att 5.765334 loss_ctc 5.023970 loss_rnnt 1.058169 hw_loss 0.055980 lr 0.00029687 rank 7
2023-02-27 15:10:20,007 DEBUG TRAIN Batch 45/8300 loss 6.568856 loss_att 8.099394 loss_ctc 10.451320 loss_rnnt 5.570532 hw_loss 0.327290 lr 0.00029688 rank 0
2023-02-27 15:10:20,009 DEBUG TRAIN Batch 45/8300 loss 7.455873 loss_att 9.263792 loss_ctc 8.510009 loss_rnnt 6.822855 hw_loss 0.245408 lr 0.00029687 rank 2
2023-02-27 15:10:20,009 DEBUG TRAIN Batch 45/8300 loss 7.272725 loss_att 7.961588 loss_ctc 11.208553 loss_rnnt 6.462399 hw_loss 0.277080 lr 0.00029688 rank 1
2023-02-27 15:10:48,126 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 5
2023-02-27 15:10:48,128 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 6
2023-02-27 15:10:48,130 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 1
2023-02-27 15:10:48,134 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 0
2023-02-27 15:10:48,140 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 2
2023-02-27 15:10:48,141 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 3
2023-02-27 15:10:48,142 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 7
2023-02-27 15:10:48,155 DEBUG CV Batch 45/0 loss 0.989078 loss_att 0.883256 loss_ctc 1.277490 loss_rnnt 0.733484 hw_loss 0.446818 history loss 0.952445 rank 4
2023-02-27 15:10:59,517 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 6
2023-02-27 15:10:59,600 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 4
2023-02-27 15:10:59,633 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 7
2023-02-27 15:10:59,699 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 3
2023-02-27 15:10:59,731 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 0
2023-02-27 15:10:59,821 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 1
2023-02-27 15:10:59,850 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 2
2023-02-27 15:10:59,940 DEBUG CV Batch 45/100 loss 4.259327 loss_att 4.911965 loss_ctc 9.247166 loss_rnnt 3.305195 hw_loss 0.297300 history loss 2.974046 rank 5
2023-02-27 15:11:13,173 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 6
2023-02-27 15:11:13,182 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 4
2023-02-27 15:11:13,396 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 2
2023-02-27 15:11:13,413 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 7
2023-02-27 15:11:13,455 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 3
2023-02-27 15:11:13,528 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 1
2023-02-27 15:11:13,555 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 5
2023-02-27 15:11:13,580 DEBUG CV Batch 45/200 loss 8.252069 loss_att 9.997501 loss_ctc 11.231702 loss_rnnt 7.436540 hw_loss 0.129670 history loss 3.551220 rank 0
2023-02-27 15:11:25,517 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 7
2023-02-27 15:11:25,649 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 1
2023-02-27 15:11:25,679 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 2
2023-02-27 15:11:25,681 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 6
2023-02-27 15:11:25,835 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 5
2023-02-27 15:11:26,143 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 3
2023-02-27 15:11:26,353 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 0
2023-02-27 15:11:26,362 DEBUG CV Batch 45/300 loss 2.773188 loss_att 3.393879 loss_ctc 4.427195 loss_rnnt 2.236777 hw_loss 0.359511 history loss 3.697425 rank 4
2023-02-27 15:11:37,836 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 7
2023-02-27 15:11:38,071 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 1
2023-02-27 15:11:38,122 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 6
2023-02-27 15:11:38,313 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 2
2023-02-27 15:11:38,424 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 5
2023-02-27 15:11:38,542 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 4
2023-02-27 15:11:38,673 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 3
2023-02-27 15:11:38,914 DEBUG CV Batch 45/400 loss 17.627455 loss_att 60.309521 loss_ctc 13.052753 loss_rnnt 9.577123 hw_loss 0.232274 history loss 4.519929 rank 0
2023-02-27 15:11:48,545 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 1
2023-02-27 15:11:48,546 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 7
2023-02-27 15:11:49,135 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 6
2023-02-27 15:11:49,185 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 2
2023-02-27 15:11:49,458 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 5
2023-02-27 15:11:49,476 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 4
2023-02-27 15:11:49,739 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 3
2023-02-27 15:11:50,065 DEBUG CV Batch 45/500 loss 5.105146 loss_att 4.768935 loss_ctc 6.689582 loss_rnnt 4.802280 hw_loss 0.297843 history loss 5.162388 rank 0
2023-02-27 15:12:00,787 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 7
2023-02-27 15:12:01,173 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 1
2023-02-27 15:12:01,439 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 2
2023-02-27 15:12:01,565 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 6
2023-02-27 15:12:01,704 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 5
2023-02-27 15:12:01,762 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 4
2023-02-27 15:12:02,331 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 3
2023-02-27 15:12:02,726 DEBUG CV Batch 45/600 loss 7.395590 loss_att 6.362998 loss_ctc 10.363197 loss_rnnt 6.945736 hw_loss 0.488798 history loss 6.033380 rank 0
2023-02-27 15:12:12,434 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 7
2023-02-27 15:12:12,780 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 1
2023-02-27 15:12:13,224 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 4
2023-02-27 15:12:13,234 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 6
2023-02-27 15:12:13,365 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 2
2023-02-27 15:12:13,517 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 5
2023-02-27 15:12:14,248 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 3
2023-02-27 15:12:14,703 DEBUG CV Batch 45/700 loss 10.824030 loss_att 26.154049 loss_ctc 12.025291 loss_rnnt 7.441963 hw_loss 0.292304 history loss 6.537357 rank 0
2023-02-27 15:12:23,744 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 7
2023-02-27 15:12:24,144 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 1
2023-02-27 15:12:24,853 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 4
2023-02-27 15:12:24,909 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 6
2023-02-27 15:12:25,124 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 5
2023-02-27 15:12:25,665 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 2
2023-02-27 15:12:25,930 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 3
2023-02-27 15:12:26,512 DEBUG CV Batch 45/800 loss 6.234691 loss_att 7.239120 loss_ctc 13.312988 loss_rnnt 4.915692 hw_loss 0.326888 history loss 6.057323 rank 0
2023-02-27 15:12:37,341 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 7
2023-02-27 15:12:37,649 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 1
2023-02-27 15:12:38,580 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 6
2023-02-27 15:12:38,725 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 5
2023-02-27 15:12:38,799 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 4
2023-02-27 15:12:39,237 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 2
2023-02-27 15:12:39,649 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 3
2023-02-27 15:12:40,468 DEBUG CV Batch 45/900 loss 8.693020 loss_att 10.398361 loss_ctc 17.722630 loss_rnnt 6.999574 hw_loss 0.278307 history loss 5.890945 rank 0
2023-02-27 15:12:49,912 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 7
2023-02-27 15:12:50,154 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 1
2023-02-27 15:12:51,003 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 6
2023-02-27 15:12:51,323 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 5
2023-02-27 15:12:51,561 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 4
2023-02-27 15:12:51,897 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 2
2023-02-27 15:12:52,274 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 3
2023-02-27 15:12:53,415 DEBUG CV Batch 45/1000 loss 4.014528 loss_att 4.132135 loss_ctc 4.008621 loss_rnnt 3.804088 hw_loss 0.351950 history loss 5.697718 rank 0
2023-02-27 15:13:02,078 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 1
2023-02-27 15:13:02,103 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 7
2023-02-27 15:13:03,261 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 6
2023-02-27 15:13:03,457 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 5
2023-02-27 15:13:03,649 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 4
2023-02-27 15:13:04,078 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 2
2023-02-27 15:13:04,688 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 3
2023-02-27 15:13:06,005 DEBUG CV Batch 45/1100 loss 4.892975 loss_att 4.845909 loss_ctc 8.076917 loss_rnnt 4.237860 hw_loss 0.450005 history loss 5.657299 rank 0
2023-02-27 15:13:12,751 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 1
2023-02-27 15:13:13,561 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 7
2023-02-27 15:13:14,301 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 6
2023-02-27 15:13:14,363 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 5
2023-02-27 15:13:14,724 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 4
2023-02-27 15:13:15,109 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 2
2023-02-27 15:13:15,874 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 3
2023-02-27 15:13:17,352 DEBUG CV Batch 45/1200 loss 6.586023 loss_att 6.676183 loss_ctc 7.566481 loss_rnnt 6.267268 hw_loss 0.318742 history loss 5.956834 rank 0
2023-02-27 15:13:24,963 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 1
2023-02-27 15:13:25,837 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 7
2023-02-27 15:13:26,609 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 5
2023-02-27 15:13:26,665 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 6
2023-02-27 15:13:27,273 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 4
2023-02-27 15:13:27,637 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 2
2023-02-27 15:13:28,404 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 3
2023-02-27 15:13:30,018 DEBUG CV Batch 45/1300 loss 4.739107 loss_att 4.000930 loss_ctc 6.389503 loss_rnnt 4.479618 hw_loss 0.350760 history loss 6.254970 rank 0
2023-02-27 15:13:36,397 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 1
2023-02-27 15:13:37,774 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 7
2023-02-27 15:13:38,150 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 5
2023-02-27 15:13:38,294 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 6
2023-02-27 15:13:38,945 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 4
2023-02-27 15:13:39,219 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 2
2023-02-27 15:13:40,140 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 3
2023-02-27 15:13:41,748 DEBUG CV Batch 45/1400 loss 4.394984 loss_att 11.290756 loss_ctc 5.864432 loss_rnnt 2.713769 hw_loss 0.199001 history loss 6.513581 rank 0
2023-02-27 15:13:48,139 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 1
2023-02-27 15:13:49,605 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 7
2023-02-27 15:13:49,857 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 5
2023-02-27 15:13:50,235 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 6
2023-02-27 15:13:50,724 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 4
2023-02-27 15:13:51,244 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 2
2023-02-27 15:13:52,034 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 3
2023-02-27 15:13:53,899 DEBUG CV Batch 45/1500 loss 6.716734 loss_att 6.851942 loss_ctc 6.488218 loss_rnnt 6.596725 hw_loss 0.231445 history loss 6.373062 rank 0
2023-02-27 15:14:01,351 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 1
2023-02-27 15:14:02,892 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 7
2023-02-27 15:14:03,382 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 5
2023-02-27 15:14:03,531 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 6
2023-02-27 15:14:03,862 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 4
2023-02-27 15:14:04,624 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 2
2023-02-27 15:14:05,566 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 3
2023-02-27 15:14:07,577 DEBUG CV Batch 45/1600 loss 10.010974 loss_att 13.301313 loss_ctc 11.104151 loss_rnnt 9.104738 hw_loss 0.192021 history loss 6.330901 rank 0
2023-02-27 15:14:14,126 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 1
2023-02-27 15:14:15,591 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 7
2023-02-27 15:14:16,009 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 5
2023-02-27 15:14:16,165 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 6
2023-02-27 15:14:16,287 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 4
2023-02-27 15:14:17,212 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 2
2023-02-27 15:14:18,131 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 3
2023-02-27 15:14:20,217 DEBUG CV Batch 45/1700 loss 8.572211 loss_att 7.214172 loss_ctc 14.700487 loss_rnnt 7.889408 hw_loss 0.257453 history loss 6.267047 rank 0
2023-02-27 15:14:23,452 INFO Epoch 45 CV info cv_loss 6.245138076619273
2023-02-27 15:14:23,453 INFO Epoch 46 TRAIN info lr 0.00029687055712183144
2023-02-27 15:14:23,455 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:24,801 INFO Epoch 45 CV info cv_loss 6.245138076791565
2023-02-27 15:14:24,802 INFO Epoch 46 TRAIN info lr 0.0002968663709969595
2023-02-27 15:14:24,807 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:25,236 INFO Epoch 45 CV info cv_loss 6.245138075430453
2023-02-27 15:14:25,236 INFO Epoch 46 TRAIN info lr 0.00029687055712183144
2023-02-27 15:14:25,239 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:25,262 INFO Epoch 45 CV info cv_loss 6.245138077463507
2023-02-27 15:14:25,262 INFO Epoch 46 TRAIN info lr 0.00029686480124578634
2023-02-27 15:14:25,264 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:25,479 INFO Epoch 45 CV info cv_loss 6.245138076298377
2023-02-27 15:14:25,480 INFO Epoch 46 TRAIN info lr 0.00029687003384653806
2023-02-27 15:14:25,487 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:26,522 INFO Epoch 45 CV info cv_loss 6.245138076067936
2023-02-27 15:14:26,523 INFO Epoch 46 TRAIN info lr 0.0002968674175115759
2023-02-27 15:14:26,525 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:27,368 INFO Epoch 45 CV info cv_loss 6.2451380767829505
2023-02-27 15:14:27,368 INFO Epoch 46 TRAIN info lr 0.0002968684640372599
2023-02-27 15:14:27,373 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:14:29,676 INFO Epoch 45 CV info cv_loss 6.245138074922189
2023-02-27 15:14:29,677 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/45.pt
2023-02-27 15:14:30,282 INFO Epoch 46 TRAIN info lr 0.00029687212696431406
2023-02-27 15:14:30,286 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 15:15:32,002 DEBUG TRAIN Batch 46/0 loss 4.339632 loss_att 5.175604 loss_ctc 6.640824 loss_rnnt 3.708468 hw_loss 0.294645 lr 0.00029686 rank 6
2023-02-27 15:15:32,003 DEBUG TRAIN Batch 46/0 loss 7.466753 loss_att 8.094314 loss_ctc 12.416816 loss_rnnt 6.470669 hw_loss 0.394807 lr 0.00029687 rank 4
2023-02-27 15:15:32,013 DEBUG TRAIN Batch 46/0 loss 7.787920 loss_att 8.089853 loss_ctc 11.712029 loss_rnnt 7.071246 hw_loss 0.249511 lr 0.00029687 rank 5
2023-02-27 15:15:32,015 DEBUG TRAIN Batch 46/0 loss 10.573393 loss_att 10.500782 loss_ctc 15.265989 loss_rnnt 9.781408 hw_loss 0.339051 lr 0.00029687 rank 1
2023-02-27 15:15:32,017 DEBUG TRAIN Batch 46/0 loss 9.100393 loss_att 9.600704 loss_ctc 14.349789 loss_rnnt 8.104866 hw_loss 0.366646 lr 0.00029687 rank 7
2023-02-27 15:15:32,058 DEBUG TRAIN Batch 46/0 loss 7.997457 loss_att 7.748694 loss_ctc 12.681441 loss_rnnt 7.210321 hw_loss 0.398168 lr 0.00029687 rank 2
2023-02-27 15:15:32,067 DEBUG TRAIN Batch 46/0 loss 5.281815 loss_att 5.837962 loss_ctc 8.184234 loss_rnnt 4.573087 hw_loss 0.394706 lr 0.00029687 rank 3
2023-02-27 15:15:32,100 DEBUG TRAIN Batch 46/0 loss 5.029833 loss_att 5.662317 loss_ctc 7.808301 loss_rnnt 4.333496 hw_loss 0.373833 lr 0.00029687 rank 0
2023-02-27 15:16:10,407 DEBUG TRAIN Batch 46/100 loss 6.711957 loss_att 11.055206 loss_ctc 13.713065 loss_rnnt 4.844205 hw_loss 0.123039 lr 0.00029685 rank 6
2023-02-27 15:16:10,422 DEBUG TRAIN Batch 46/100 loss 3.303643 loss_att 8.051585 loss_ctc 11.449049 loss_rnnt 1.088450 hw_loss 0.336655 lr 0.00029685 rank 3
2023-02-27 15:16:10,425 DEBUG TRAIN Batch 46/100 loss 10.365658 loss_att 14.808336 loss_ctc 19.939083 loss_rnnt 8.151028 hw_loss 0.093072 lr 0.00029686 rank 5
2023-02-27 15:16:10,425 DEBUG TRAIN Batch 46/100 loss 1.843465 loss_att 3.833852 loss_ctc 2.133057 loss_rnnt 1.223995 hw_loss 0.342714 lr 0.00029686 rank 4
2023-02-27 15:16:10,429 DEBUG TRAIN Batch 46/100 loss 3.181438 loss_att 7.377057 loss_ctc 8.734880 loss_rnnt 1.422484 hw_loss 0.336320 lr 0.00029685 rank 2
2023-02-27 15:16:10,429 DEBUG TRAIN Batch 46/100 loss 13.309203 loss_att 17.009731 loss_ctc 19.231413 loss_rnnt 11.720003 hw_loss 0.111495 lr 0.00029686 rank 0
2023-02-27 15:16:10,433 DEBUG TRAIN Batch 46/100 loss 1.979699 loss_att 5.128118 loss_ctc 3.759039 loss_rnnt 1.009192 hw_loss 0.194209 lr 0.00029686 rank 1
2023-02-27 15:16:10,434 DEBUG TRAIN Batch 46/100 loss 13.688080 loss_att 15.426263 loss_ctc 21.637802 loss_rnnt 12.227321 hw_loss 0.099674 lr 0.00029685 rank 7
2023-02-27 15:16:48,606 DEBUG TRAIN Batch 46/200 loss 11.487099 loss_att 15.508705 loss_ctc 16.992146 loss_rnnt 9.740119 hw_loss 0.391221 lr 0.00029684 rank 6
2023-02-27 15:16:48,609 DEBUG TRAIN Batch 46/200 loss 9.202304 loss_att 12.491661 loss_ctc 14.733816 loss_rnnt 7.684161 hw_loss 0.230130 lr 0.00029685 rank 0
2023-02-27 15:16:48,611 DEBUG TRAIN Batch 46/200 loss 7.605626 loss_att 10.889661 loss_ctc 12.964207 loss_rnnt 6.068602 hw_loss 0.310762 lr 0.00029684 rank 7
2023-02-27 15:16:48,611 DEBUG TRAIN Batch 46/200 loss 5.160642 loss_att 10.115557 loss_ctc 10.314469 loss_rnnt 3.414530 hw_loss 0.127411 lr 0.00029684 rank 2
2023-02-27 15:16:48,611 DEBUG TRAIN Batch 46/200 loss 3.278929 loss_att 6.644854 loss_ctc 8.294442 loss_rnnt 1.916763 hw_loss 0.037962 lr 0.00029684 rank 3
2023-02-27 15:16:48,615 DEBUG TRAIN Batch 46/200 loss 4.307711 loss_att 9.177793 loss_ctc 7.095456 loss_rnnt 2.798166 hw_loss 0.307179 lr 0.00029684 rank 4
2023-02-27 15:16:48,616 DEBUG TRAIN Batch 46/200 loss 4.981850 loss_att 10.515787 loss_ctc 9.181625 loss_rnnt 3.272361 hw_loss 0.080122 lr 0.00029684 rank 1
2023-02-27 15:16:48,660 DEBUG TRAIN Batch 46/200 loss 10.754818 loss_att 16.533514 loss_ctc 24.276373 loss_rnnt 7.697083 hw_loss 0.185853 lr 0.00029684 rank 5
2023-02-27 15:17:27,684 DEBUG TRAIN Batch 46/300 loss 6.053930 loss_att 9.085612 loss_ctc 10.045831 loss_rnnt 4.768613 hw_loss 0.275113 lr 0.00029683 rank 5
2023-02-27 15:17:27,689 DEBUG TRAIN Batch 46/300 loss 2.800518 loss_att 6.432477 loss_ctc 6.887376 loss_rnnt 1.390142 hw_loss 0.260756 lr 0.00029683 rank 2
2023-02-27 15:17:27,692 DEBUG TRAIN Batch 46/300 loss 3.485141 loss_att 5.683963 loss_ctc 6.863564 loss_rnnt 2.434716 hw_loss 0.300384 lr 0.00029683 rank 6
2023-02-27 15:17:27,694 DEBUG TRAIN Batch 46/300 loss 3.409733 loss_att 6.196551 loss_ctc 6.949224 loss_rnnt 2.239692 hw_loss 0.263898 lr 0.00029683 rank 3
2023-02-27 15:17:27,699 DEBUG TRAIN Batch 46/300 loss 4.712687 loss_att 8.127375 loss_ctc 12.145697 loss_rnnt 2.927578 hw_loss 0.208319 lr 0.00029683 rank 0
2023-02-27 15:17:27,699 DEBUG TRAIN Batch 46/300 loss 7.031499 loss_att 11.332257 loss_ctc 17.173615 loss_rnnt 4.780285 hw_loss 0.072713 lr 0.00029683 rank 1
2023-02-27 15:17:27,701 DEBUG TRAIN Batch 46/300 loss 7.940410 loss_att 13.147985 loss_ctc 13.721784 loss_rnnt 5.965610 hw_loss 0.304565 lr 0.00029683 rank 4
2023-02-27 15:17:27,709 DEBUG TRAIN Batch 46/300 loss 10.535321 loss_att 15.069396 loss_ctc 18.955372 loss_rnnt 8.363340 hw_loss 0.267173 lr 0.00029683 rank 7
2023-02-27 15:18:34,279 DEBUG TRAIN Batch 46/400 loss 15.845985 loss_att 20.299883 loss_ctc 30.992966 loss_rnnt 12.850586 hw_loss 0.159416 lr 0.00029682 rank 4
2023-02-27 15:18:34,292 DEBUG TRAIN Batch 46/400 loss 5.064634 loss_att 10.942027 loss_ctc 14.097259 loss_rnnt 2.583791 hw_loss 0.189403 lr 0.00029682 rank 0
2023-02-27 15:18:34,291 DEBUG TRAIN Batch 46/400 loss 5.404107 loss_att 9.534642 loss_ctc 12.745081 loss_rnnt 3.474262 hw_loss 0.234264 lr 0.00029681 rank 6
2023-02-27 15:18:34,293 DEBUG TRAIN Batch 46/400 loss 7.169608 loss_att 12.196077 loss_ctc 12.127716 loss_rnnt 5.403050 hw_loss 0.187842 lr 0.00029682 rank 3
2023-02-27 15:18:34,294 DEBUG TRAIN Batch 46/400 loss 6.760931 loss_att 9.621865 loss_ctc 9.352583 loss_rnnt 5.693636 hw_loss 0.280416 lr 0.00029681 rank 7
2023-02-27 15:18:34,296 DEBUG TRAIN Batch 46/400 loss 4.457456 loss_att 7.488018 loss_ctc 8.414022 loss_rnnt 3.193576 hw_loss 0.244171 lr 0.00029682 rank 1
2023-02-27 15:18:34,298 DEBUG TRAIN Batch 46/400 loss 8.305569 loss_att 10.500572 loss_ctc 14.854552 loss_rnnt 6.893540 hw_loss 0.187181 lr 0.00029681 rank 2
2023-02-27 15:18:34,338 DEBUG TRAIN Batch 46/400 loss 3.594404 loss_att 6.374735 loss_ctc 6.442874 loss_rnnt 2.522313 hw_loss 0.255429 lr 0.00029682 rank 5
2023-02-27 15:19:13,031 DEBUG TRAIN Batch 46/500 loss 4.149426 loss_att 5.471694 loss_ctc 4.977777 loss_rnnt 3.605779 hw_loss 0.316399 lr 0.00029680 rank 7
2023-02-27 15:19:13,034 DEBUG TRAIN Batch 46/500 loss 9.242605 loss_att 12.614433 loss_ctc 14.696177 loss_rnnt 7.694721 hw_loss 0.274456 lr 0.00029680 rank 3
2023-02-27 15:19:13,036 DEBUG TRAIN Batch 46/500 loss 11.853621 loss_att 18.122854 loss_ctc 23.702457 loss_rnnt 8.794562 hw_loss 0.422564 lr 0.00029681 rank 0
2023-02-27 15:19:13,036 DEBUG TRAIN Batch 46/500 loss 9.548357 loss_att 13.173320 loss_ctc 15.782484 loss_rnnt 7.796353 hw_loss 0.367114 lr 0.00029680 rank 6
2023-02-27 15:19:13,036 DEBUG TRAIN Batch 46/500 loss 13.243005 loss_att 17.709150 loss_ctc 21.158133 loss_rnnt 11.204668 hw_loss 0.168292 lr 0.00029680 rank 5
2023-02-27 15:19:13,039 DEBUG TRAIN Batch 46/500 loss 9.950738 loss_att 12.028906 loss_ctc 14.537155 loss_rnnt 8.711959 hw_loss 0.396796 lr 0.00029680 rank 4
2023-02-27 15:19:13,040 DEBUG TRAIN Batch 46/500 loss 8.531492 loss_att 11.762861 loss_ctc 19.882402 loss_rnnt 6.251760 hw_loss 0.225009 lr 0.00029680 rank 2
2023-02-27 15:19:13,042 DEBUG TRAIN Batch 46/500 loss 3.947867 loss_att 8.132292 loss_ctc 7.994635 loss_rnnt 2.515483 hw_loss 0.104867 lr 0.00029680 rank 1
2023-02-27 15:19:52,098 DEBUG TRAIN Batch 46/600 loss 5.169028 loss_att 6.271452 loss_ctc 8.588371 loss_rnnt 4.272288 hw_loss 0.413143 lr 0.00029679 rank 4
2023-02-27 15:19:52,102 DEBUG TRAIN Batch 46/600 loss 10.975959 loss_att 15.989208 loss_ctc 22.384838 loss_rnnt 8.345699 hw_loss 0.199547 lr 0.00029679 rank 7
2023-02-27 15:19:52,120 DEBUG TRAIN Batch 46/600 loss 6.712863 loss_att 7.780819 loss_ctc 8.611071 loss_rnnt 6.017893 hw_loss 0.428035 lr 0.00029679 rank 3
2023-02-27 15:19:52,123 DEBUG TRAIN Batch 46/600 loss 7.096275 loss_att 10.302340 loss_ctc 14.351024 loss_rnnt 5.310847 hw_loss 0.331716 lr 0.00029679 rank 5
2023-02-27 15:19:52,122 DEBUG TRAIN Batch 46/600 loss 5.684388 loss_att 7.443199 loss_ctc 11.252275 loss_rnnt 4.393043 hw_loss 0.369745 lr 0.00029679 rank 0
2023-02-27 15:19:52,125 DEBUG TRAIN Batch 46/600 loss 5.747427 loss_att 8.425660 loss_ctc 9.068558 loss_rnnt 4.677324 hw_loss 0.171823 lr 0.00029679 rank 2
2023-02-27 15:19:52,128 DEBUG TRAIN Batch 46/600 loss 5.340080 loss_att 6.913154 loss_ctc 6.661923 loss_rnnt 4.699919 hw_loss 0.279938 lr 0.00029679 rank 1
2023-02-27 15:19:52,129 DEBUG TRAIN Batch 46/600 loss 6.171125 loss_att 7.403671 loss_ctc 9.084686 loss_rnnt 5.412480 hw_loss 0.231863 lr 0.00029679 rank 6
2023-02-27 15:20:31,664 DEBUG TRAIN Batch 46/700 loss 1.606349 loss_att 4.154593 loss_ctc 3.528701 loss_rnnt 0.652849 hw_loss 0.351634 lr 0.00029678 rank 4
2023-02-27 15:20:31,669 DEBUG TRAIN Batch 46/700 loss 5.314910 loss_att 8.233741 loss_ctc 10.244801 loss_rnnt 3.894318 hw_loss 0.336576 lr 0.00029678 rank 5
2023-02-27 15:20:31,670 DEBUG TRAIN Batch 46/700 loss 9.092183 loss_att 10.238442 loss_ctc 9.577887 loss_rnnt 8.633305 hw_loss 0.309125 lr 0.00029677 rank 6
2023-02-27 15:20:31,675 DEBUG TRAIN Batch 46/700 loss 4.049512 loss_att 6.649255 loss_ctc 7.157222 loss_rnnt 3.014978 hw_loss 0.187920 lr 0.00029678 rank 0
2023-02-27 15:20:31,675 DEBUG TRAIN Batch 46/700 loss 3.554846 loss_att 5.629560 loss_ctc 5.097977 loss_rnnt 2.783350 hw_loss 0.282755 lr 0.00029678 rank 3
2023-02-27 15:20:31,679 DEBUG TRAIN Batch 46/700 loss 2.133127 loss_att 6.291765 loss_ctc 7.347700 loss_rnnt 0.462267 hw_loss 0.269730 lr 0.00029678 rank 1
2023-02-27 15:20:31,680 DEBUG TRAIN Batch 46/700 loss 7.529888 loss_att 10.245690 loss_ctc 13.235739 loss_rnnt 6.124237 hw_loss 0.190705 lr 0.00029677 rank 7
2023-02-27 15:20:31,714 DEBUG TRAIN Batch 46/700 loss 13.373448 loss_att 16.332386 loss_ctc 22.133289 loss_rnnt 11.526206 hw_loss 0.164017 lr 0.00029678 rank 2
2023-02-27 15:21:37,754 DEBUG TRAIN Batch 46/800 loss 2.226712 loss_att 3.703493 loss_ctc 3.962208 loss_rnnt 1.599512 hw_loss 0.188333 lr 0.00029676 rank 4
2023-02-27 15:21:37,754 DEBUG TRAIN Batch 46/800 loss 6.423079 loss_att 7.438527 loss_ctc 9.545511 loss_rnnt 5.656388 hw_loss 0.276143 lr 0.00029676 rank 3
2023-02-27 15:21:37,755 DEBUG TRAIN Batch 46/800 loss 7.256296 loss_att 10.572848 loss_ctc 9.274042 loss_rnnt 6.110482 hw_loss 0.400255 lr 0.00029676 rank 2
2023-02-27 15:21:37,755 DEBUG TRAIN Batch 46/800 loss 4.304822 loss_att 8.174548 loss_ctc 7.267277 loss_rnnt 2.976774 hw_loss 0.298329 lr 0.00029676 rank 6
2023-02-27 15:21:37,758 DEBUG TRAIN Batch 46/800 loss 5.926947 loss_att 8.500186 loss_ctc 8.238468 loss_rnnt 5.004690 hw_loss 0.186388 lr 0.00029677 rank 1
2023-02-27 15:21:37,758 DEBUG TRAIN Batch 46/800 loss 7.046670 loss_att 11.294181 loss_ctc 12.759291 loss_rnnt 5.357486 hw_loss 0.146249 lr 0.00029677 rank 5
2023-02-27 15:21:37,759 DEBUG TRAIN Batch 46/800 loss 6.995980 loss_att 9.495028 loss_ctc 10.376857 loss_rnnt 5.908103 hw_loss 0.257408 lr 0.00029677 rank 0
2023-02-27 15:21:37,782 DEBUG TRAIN Batch 46/800 loss 15.464079 loss_att 15.408287 loss_ctc 27.506634 loss_rnnt 13.817356 hw_loss 0.097889 lr 0.00029676 rank 7
2023-02-27 15:22:16,684 DEBUG TRAIN Batch 46/900 loss 3.769674 loss_att 6.044379 loss_ctc 4.757571 loss_rnnt 3.129349 hw_loss 0.100619 lr 0.00029675 rank 0
2023-02-27 15:22:16,687 DEBUG TRAIN Batch 46/900 loss 9.120870 loss_att 10.563202 loss_ctc 15.654264 loss_rnnt 7.899393 hw_loss 0.116045 lr 0.00029675 rank 3
2023-02-27 15:22:16,688 DEBUG TRAIN Batch 46/900 loss 9.048327 loss_att 14.040964 loss_ctc 19.041079 loss_rnnt 6.608940 hw_loss 0.203424 lr 0.00029675 rank 4
2023-02-27 15:22:16,689 DEBUG TRAIN Batch 46/900 loss 9.015653 loss_att 11.715131 loss_ctc 12.341780 loss_rnnt 7.862612 hw_loss 0.318115 lr 0.00029675 rank 6
2023-02-27 15:22:16,690 DEBUG TRAIN Batch 46/900 loss 2.676988 loss_att 6.342369 loss_ctc 5.544051 loss_rnnt 1.471333 hw_loss 0.169320 lr 0.00029675 rank 1
2023-02-27 15:22:16,693 DEBUG TRAIN Batch 46/900 loss 9.447128 loss_att 13.197262 loss_ctc 18.338341 loss_rnnt 7.368437 hw_loss 0.268444 lr 0.00029675 rank 7
2023-02-27 15:22:16,692 DEBUG TRAIN Batch 46/900 loss 7.488982 loss_att 8.151185 loss_ctc 13.041906 loss_rnnt 6.450990 hw_loss 0.309678 lr 0.00029675 rank 5
2023-02-27 15:22:16,692 DEBUG TRAIN Batch 46/900 loss 1.199285 loss_att 3.036940 loss_ctc 1.529646 loss_rnnt 0.678753 hw_loss 0.204285 lr 0.00029675 rank 2
2023-02-27 15:22:55,772 DEBUG TRAIN Batch 46/1000 loss 12.249301 loss_att 18.022413 loss_ctc 20.148123 loss_rnnt 10.007402 hw_loss 0.063937 lr 0.00029674 rank 7
2023-02-27 15:22:55,781 DEBUG TRAIN Batch 46/1000 loss 5.532347 loss_att 7.939537 loss_ctc 9.079348 loss_rnnt 4.465025 hw_loss 0.211783 lr 0.00029674 rank 4
2023-02-27 15:22:55,782 DEBUG TRAIN Batch 46/1000 loss 5.233903 loss_att 8.454051 loss_ctc 6.017684 loss_rnnt 4.341256 hw_loss 0.270212 lr 0.00029673 rank 6
2023-02-27 15:22:55,784 DEBUG TRAIN Batch 46/1000 loss 10.017802 loss_att 11.259164 loss_ctc 12.422083 loss_rnnt 9.321781 hw_loss 0.238460 lr 0.00029674 rank 3
2023-02-27 15:22:55,786 DEBUG TRAIN Batch 46/1000 loss 5.860357 loss_att 11.019919 loss_ctc 14.133042 loss_rnnt 3.580781 hw_loss 0.271199 lr 0.00029674 rank 0
2023-02-27 15:22:55,788 DEBUG TRAIN Batch 46/1000 loss 7.086124 loss_att 11.029067 loss_ctc 10.646786 loss_rnnt 5.773252 hw_loss 0.092866 lr 0.00029674 rank 5
2023-02-27 15:22:55,788 DEBUG TRAIN Batch 46/1000 loss 7.590588 loss_att 12.614393 loss_ctc 13.589157 loss_rnnt 5.641444 hw_loss 0.271075 lr 0.00029674 rank 2
2023-02-27 15:22:55,795 DEBUG TRAIN Batch 46/1000 loss 5.378164 loss_att 9.479746 loss_ctc 10.545357 loss_rnnt 3.731824 hw_loss 0.256997 lr 0.00029674 rank 1
2023-02-27 15:24:01,383 DEBUG TRAIN Batch 46/1100 loss 11.674548 loss_att 13.766381 loss_ctc 22.985371 loss_rnnt 9.562648 hw_loss 0.347669 lr 0.00029672 rank 3
2023-02-27 15:24:01,385 DEBUG TRAIN Batch 46/1100 loss 4.070984 loss_att 6.584661 loss_ctc 8.313532 loss_rnnt 2.865491 hw_loss 0.257032 lr 0.00029673 rank 0
2023-02-27 15:24:01,385 DEBUG TRAIN Batch 46/1100 loss 6.953735 loss_att 10.223792 loss_ctc 10.072777 loss_rnnt 5.707283 hw_loss 0.331066 lr 0.00029672 rank 6
2023-02-27 15:24:01,386 DEBUG TRAIN Batch 46/1100 loss 5.026470 loss_att 7.764818 loss_ctc 9.471124 loss_rnnt 3.774982 hw_loss 0.208495 lr 0.00029672 rank 2
2023-02-27 15:24:01,386 DEBUG TRAIN Batch 46/1100 loss 6.442186 loss_att 9.091713 loss_ctc 10.747524 loss_rnnt 5.198027 hw_loss 0.262891 lr 0.00029673 rank 4
2023-02-27 15:24:01,389 DEBUG TRAIN Batch 46/1100 loss 7.561076 loss_att 8.158394 loss_ctc 8.894176 loss_rnnt 7.152657 hw_loss 0.208518 lr 0.00029673 rank 1
2023-02-27 15:24:01,392 DEBUG TRAIN Batch 46/1100 loss 5.852763 loss_att 7.073682 loss_ctc 9.349367 loss_rnnt 5.073558 hw_loss 0.129013 lr 0.00029672 rank 7
2023-02-27 15:24:01,394 DEBUG TRAIN Batch 46/1100 loss 8.811520 loss_att 10.522097 loss_ctc 16.709970 loss_rnnt 7.289460 hw_loss 0.237781 lr 0.00029673 rank 5
2023-02-27 15:24:40,017 DEBUG TRAIN Batch 46/1200 loss 5.368960 loss_att 7.194610 loss_ctc 7.316139 loss_rnnt 4.537353 hw_loss 0.387851 lr 0.00029671 rank 3
2023-02-27 15:24:40,019 DEBUG TRAIN Batch 46/1200 loss 3.695208 loss_att 6.966373 loss_ctc 7.720984 loss_rnnt 2.356987 hw_loss 0.276033 lr 0.00029671 rank 2
2023-02-27 15:24:40,020 DEBUG TRAIN Batch 46/1200 loss 6.874017 loss_att 7.244760 loss_ctc 12.759556 loss_rnnt 5.843374 hw_loss 0.322042 lr 0.00029671 rank 4
2023-02-27 15:24:40,023 DEBUG TRAIN Batch 46/1200 loss 4.757163 loss_att 5.560915 loss_ctc 5.448566 loss_rnnt 4.321259 hw_loss 0.343062 lr 0.00029671 rank 0
2023-02-27 15:24:40,024 DEBUG TRAIN Batch 46/1200 loss 3.147657 loss_att 6.862035 loss_ctc 5.628563 loss_rnnt 2.004223 hw_loss 0.130822 lr 0.00029671 rank 6
2023-02-27 15:24:40,024 DEBUG TRAIN Batch 46/1200 loss 9.465875 loss_att 10.800304 loss_ctc 14.012408 loss_rnnt 8.401388 hw_loss 0.358867 lr 0.00029671 rank 7
2023-02-27 15:24:40,025 DEBUG TRAIN Batch 46/1200 loss 4.849373 loss_att 7.726215 loss_ctc 8.255835 loss_rnnt 3.671615 hw_loss 0.277864 lr 0.00029671 rank 1
2023-02-27 15:24:40,032 DEBUG TRAIN Batch 46/1200 loss 6.069103 loss_att 7.474998 loss_ctc 9.928792 loss_rnnt 5.152371 hw_loss 0.226738 lr 0.00029671 rank 5
2023-02-27 15:25:18,169 DEBUG TRAIN Batch 46/1300 loss 5.156118 loss_att 9.962524 loss_ctc 10.097122 loss_rnnt 3.380882 hw_loss 0.290915 lr 0.00029670 rank 0
2023-02-27 15:25:18,171 DEBUG TRAIN Batch 46/1300 loss 2.256702 loss_att 5.739653 loss_ctc 3.950902 loss_rnnt 1.272905 hw_loss 0.114965 lr 0.00029670 rank 5
2023-02-27 15:25:18,181 DEBUG TRAIN Batch 46/1300 loss 7.147622 loss_att 12.214669 loss_ctc 13.709461 loss_rnnt 5.117762 hw_loss 0.265384 lr 0.00029670 rank 4
2023-02-27 15:25:18,182 DEBUG TRAIN Batch 46/1300 loss 4.740160 loss_att 9.479410 loss_ctc 8.595996 loss_rnnt 3.076977 hw_loss 0.377292 lr 0.00029670 rank 7
2023-02-27 15:25:18,182 DEBUG TRAIN Batch 46/1300 loss 8.473462 loss_att 9.613903 loss_ctc 13.589973 loss_rnnt 7.379858 hw_loss 0.343714 lr 0.00029670 rank 3
2023-02-27 15:25:18,184 DEBUG TRAIN Batch 46/1300 loss 7.846881 loss_att 13.660634 loss_ctc 14.955204 loss_rnnt 5.538391 hw_loss 0.371179 lr 0.00029669 rank 6
2023-02-27 15:25:18,187 DEBUG TRAIN Batch 46/1300 loss 7.566116 loss_att 8.455062 loss_ctc 11.980681 loss_rnnt 6.661770 hw_loss 0.258652 lr 0.00029670 rank 2
2023-02-27 15:25:18,189 DEBUG TRAIN Batch 46/1300 loss 2.371975 loss_att 7.965517 loss_ctc 4.123399 loss_rnnt 0.926501 hw_loss 0.174830 lr 0.00029670 rank 1
2023-02-27 15:25:57,688 DEBUG TRAIN Batch 46/1400 loss 4.883520 loss_att 8.032936 loss_ctc 13.259245 loss_rnnt 3.025937 hw_loss 0.208007 lr 0.00029669 rank 1
2023-02-27 15:25:57,696 DEBUG TRAIN Batch 46/1400 loss 5.368317 loss_att 7.686849 loss_ctc 11.384929 loss_rnnt 3.900993 hw_loss 0.377629 lr 0.00029669 rank 4
2023-02-27 15:25:57,697 DEBUG TRAIN Batch 46/1400 loss 6.282681 loss_att 8.740768 loss_ctc 10.504676 loss_rnnt 5.108987 hw_loss 0.223396 lr 0.00029669 rank 0
2023-02-27 15:25:57,697 DEBUG TRAIN Batch 46/1400 loss 6.396735 loss_att 10.226015 loss_ctc 10.964365 loss_rnnt 4.933497 hw_loss 0.165683 lr 0.00029668 rank 6
2023-02-27 15:25:57,698 DEBUG TRAIN Batch 46/1400 loss 2.945254 loss_att 6.554625 loss_ctc 6.370961 loss_rnnt 1.646793 hw_loss 0.224674 lr 0.00029668 rank 3
2023-02-27 15:25:57,706 DEBUG TRAIN Batch 46/1400 loss 9.506657 loss_att 12.812529 loss_ctc 11.904758 loss_rnnt 8.440669 hw_loss 0.159499 lr 0.00029668 rank 7
2023-02-27 15:25:57,761 DEBUG TRAIN Batch 46/1400 loss 4.411282 loss_att 12.057080 loss_ctc 10.865283 loss_rnnt 1.904858 hw_loss 0.218870 lr 0.00029669 rank 5
2023-02-27 15:25:57,769 DEBUG TRAIN Batch 46/1400 loss 8.807427 loss_att 14.066509 loss_ctc 13.722000 loss_rnnt 6.960233 hw_loss 0.262690 lr 0.00029668 rank 2
2023-02-27 15:27:03,397 DEBUG TRAIN Batch 46/1500 loss 9.272704 loss_att 10.804277 loss_ctc 13.278902 loss_rnnt 8.237344 hw_loss 0.365410 lr 0.00029667 rank 5
2023-02-27 15:27:03,416 DEBUG TRAIN Batch 46/1500 loss 5.303825 loss_att 9.600555 loss_ctc 7.105461 loss_rnnt 4.045502 hw_loss 0.297671 lr 0.00029667 rank 6
2023-02-27 15:27:03,418 DEBUG TRAIN Batch 46/1500 loss 9.184666 loss_att 11.748398 loss_ctc 16.189907 loss_rnnt 7.700434 hw_loss 0.070223 lr 0.00029667 rank 3
2023-02-27 15:27:03,419 DEBUG TRAIN Batch 46/1500 loss 8.488832 loss_att 12.906318 loss_ctc 15.170958 loss_rnnt 6.525774 hw_loss 0.353649 lr 0.00029668 rank 0
2023-02-27 15:27:03,422 DEBUG TRAIN Batch 46/1500 loss 8.879415 loss_att 10.214115 loss_ctc 15.326262 loss_rnnt 7.691809 hw_loss 0.114536 lr 0.00029667 rank 4
2023-02-27 15:27:03,449 DEBUG TRAIN Batch 46/1500 loss 4.919495 loss_att 7.199316 loss_ctc 10.903521 loss_rnnt 3.483793 hw_loss 0.341002 lr 0.00029667 rank 7
2023-02-27 15:27:03,456 DEBUG TRAIN Batch 46/1500 loss 4.471754 loss_att 8.133842 loss_ctc 7.712629 loss_rnnt 3.220117 hw_loss 0.163317 lr 0.00029667 rank 2
2023-02-27 15:27:03,462 DEBUG TRAIN Batch 46/1500 loss 4.314244 loss_att 7.708439 loss_ctc 8.511049 loss_rnnt 2.989968 hw_loss 0.160992 lr 0.00029667 rank 1
2023-02-27 15:27:41,694 DEBUG TRAIN Batch 46/1600 loss 8.426459 loss_att 10.622459 loss_ctc 13.434455 loss_rnnt 7.160151 hw_loss 0.298831 lr 0.00029666 rank 3
2023-02-27 15:27:41,708 DEBUG TRAIN Batch 46/1600 loss 7.347041 loss_att 8.872894 loss_ctc 10.595265 loss_rnnt 6.518165 hw_loss 0.169892 lr 0.00029666 rank 6
2023-02-27 15:27:41,712 DEBUG TRAIN Batch 46/1600 loss 7.439179 loss_att 10.612032 loss_ctc 14.053103 loss_rnnt 5.717920 hw_loss 0.384060 lr 0.00029666 rank 5
2023-02-27 15:27:41,712 DEBUG TRAIN Batch 46/1600 loss 5.424170 loss_att 7.393071 loss_ctc 4.964421 loss_rnnt 4.977912 hw_loss 0.213333 lr 0.00029666 rank 2
2023-02-27 15:27:41,715 DEBUG TRAIN Batch 46/1600 loss 5.965876 loss_att 7.922981 loss_ctc 7.316010 loss_rnnt 5.210415 hw_loss 0.345039 lr 0.00029666 rank 0
2023-02-27 15:27:41,715 DEBUG TRAIN Batch 46/1600 loss 3.077297 loss_att 8.036018 loss_ctc 5.668455 loss_rnnt 1.591413 hw_loss 0.278722 lr 0.00029666 rank 4
2023-02-27 15:27:41,715 DEBUG TRAIN Batch 46/1600 loss 9.335723 loss_att 11.636628 loss_ctc 12.085806 loss_rnnt 8.280119 hw_loss 0.428899 lr 0.00029666 rank 7
2023-02-27 15:27:41,768 DEBUG TRAIN Batch 46/1600 loss 9.671784 loss_att 13.738993 loss_ctc 16.511723 loss_rnnt 7.761287 hw_loss 0.346996 lr 0.00029666 rank 1
2023-02-27 15:28:20,799 DEBUG TRAIN Batch 46/1700 loss 8.152037 loss_att 11.607306 loss_ctc 12.868150 loss_rnnt 6.687329 hw_loss 0.271572 lr 0.00029665 rank 4
2023-02-27 15:28:20,804 DEBUG TRAIN Batch 46/1700 loss 5.524463 loss_att 9.494633 loss_ctc 13.635860 loss_rnnt 3.470409 hw_loss 0.334688 lr 0.00029664 rank 7
2023-02-27 15:28:20,811 DEBUG TRAIN Batch 46/1700 loss 3.403730 loss_att 5.755863 loss_ctc 7.232727 loss_rnnt 2.265591 hw_loss 0.294712 lr 0.00029665 rank 3
2023-02-27 15:28:20,816 DEBUG TRAIN Batch 46/1700 loss 2.500247 loss_att 5.407658 loss_ctc 5.743236 loss_rnnt 1.357087 hw_loss 0.242397 lr 0.00029665 rank 0
2023-02-27 15:28:20,817 DEBUG TRAIN Batch 46/1700 loss 6.381827 loss_att 9.836493 loss_ctc 11.210642 loss_rnnt 4.955507 hw_loss 0.171647 lr 0.00029665 rank 1
2023-02-27 15:28:20,822 DEBUG TRAIN Batch 46/1700 loss 2.081216 loss_att 4.311423 loss_ctc 4.835871 loss_rnnt 1.109446 hw_loss 0.297079 lr 0.00029664 rank 2
2023-02-27 15:28:20,832 DEBUG TRAIN Batch 46/1700 loss 2.802777 loss_att 6.318192 loss_ctc 5.682941 loss_rnnt 1.567575 hw_loss 0.277684 lr 0.00029664 rank 6
2023-02-27 15:28:20,837 DEBUG TRAIN Batch 46/1700 loss 1.433021 loss_att 3.259845 loss_ctc 1.129455 loss_rnnt 0.961894 hw_loss 0.274195 lr 0.00029665 rank 5
2023-02-27 15:29:26,074 DEBUG TRAIN Batch 46/1800 loss 11.069971 loss_att 10.518068 loss_ctc 16.700176 loss_rnnt 10.323570 hw_loss 0.198915 lr 0.00029663 rank 5
2023-02-27 15:29:26,074 DEBUG TRAIN Batch 46/1800 loss 7.226588 loss_att 9.922544 loss_ctc 13.826976 loss_rnnt 5.611522 hw_loss 0.367168 lr 0.00029663 rank 6
2023-02-27 15:29:26,075 DEBUG TRAIN Batch 46/1800 loss 3.676658 loss_att 6.029368 loss_ctc 6.013713 loss_rnnt 2.764998 hw_loss 0.242833 lr 0.00029663 rank 3
2023-02-27 15:29:26,077 DEBUG TRAIN Batch 46/1800 loss 8.493683 loss_att 8.938427 loss_ctc 12.180440 loss_rnnt 7.748587 hw_loss 0.308585 lr 0.00029663 rank 7
2023-02-27 15:29:26,078 DEBUG TRAIN Batch 46/1800 loss 8.062281 loss_att 11.051817 loss_ctc 14.841042 loss_rnnt 6.412558 hw_loss 0.277463 lr 0.00029663 rank 4
2023-02-27 15:29:26,084 DEBUG TRAIN Batch 46/1800 loss 3.246069 loss_att 6.155802 loss_ctc 5.672607 loss_rnnt 2.223591 hw_loss 0.219362 lr 0.00029664 rank 0
2023-02-27 15:29:26,086 DEBUG TRAIN Batch 46/1800 loss 7.052660 loss_att 9.956913 loss_ctc 13.223304 loss_rnnt 5.455827 hw_loss 0.362305 lr 0.00029663 rank 1
2023-02-27 15:29:26,129 DEBUG TRAIN Batch 46/1800 loss 11.816536 loss_att 14.567067 loss_ctc 18.744541 loss_rnnt 10.173176 hw_loss 0.317847 lr 0.00029663 rank 2
2023-02-27 15:30:05,151 DEBUG TRAIN Batch 46/1900 loss 3.918506 loss_att 7.359483 loss_ctc 8.494627 loss_rnnt 2.520243 hw_loss 0.187346 lr 0.00029662 rank 6
2023-02-27 15:30:05,151 DEBUG TRAIN Batch 46/1900 loss 8.598326 loss_att 8.458371 loss_ctc 12.548944 loss_rnnt 7.947110 hw_loss 0.285859 lr 0.00029662 rank 3
2023-02-27 15:30:05,152 DEBUG TRAIN Batch 46/1900 loss 6.483460 loss_att 11.193942 loss_ctc 13.604616 loss_rnnt 4.463333 hw_loss 0.241020 lr 0.00029662 rank 0
2023-02-27 15:30:05,152 DEBUG TRAIN Batch 46/1900 loss 8.381271 loss_att 10.205695 loss_ctc 15.662231 loss_rnnt 6.862282 hw_loss 0.343704 lr 0.00029662 rank 2
2023-02-27 15:30:05,153 DEBUG TRAIN Batch 46/1900 loss 8.327456 loss_att 8.399522 loss_ctc 10.827784 loss_rnnt 7.762706 hw_loss 0.406798 lr 0.00029662 rank 1
2023-02-27 15:30:05,153 DEBUG TRAIN Batch 46/1900 loss 4.583130 loss_att 7.354194 loss_ctc 9.405899 loss_rnnt 3.261464 hw_loss 0.233283 lr 0.00029662 rank 5
2023-02-27 15:30:05,153 DEBUG TRAIN Batch 46/1900 loss 7.450918 loss_att 9.331509 loss_ctc 11.175489 loss_rnnt 6.423471 hw_loss 0.290099 lr 0.00029662 rank 4
2023-02-27 15:30:05,155 DEBUG TRAIN Batch 46/1900 loss 10.763409 loss_att 17.609924 loss_ctc 18.777756 loss_rnnt 8.260863 hw_loss 0.121243 lr 0.00029662 rank 7
2023-02-27 15:30:43,200 DEBUG TRAIN Batch 46/2000 loss 7.351202 loss_att 11.841638 loss_ctc 9.706896 loss_rnnt 5.984560 hw_loss 0.289618 lr 0.00029660 rank 6
2023-02-27 15:30:43,212 DEBUG TRAIN Batch 46/2000 loss 6.372865 loss_att 10.608953 loss_ctc 14.148392 loss_rnnt 4.344094 hw_loss 0.271532 lr 0.00029661 rank 3
2023-02-27 15:30:43,214 DEBUG TRAIN Batch 46/2000 loss 5.553268 loss_att 8.749270 loss_ctc 9.812895 loss_rnnt 4.277248 hw_loss 0.129130 lr 0.00029661 rank 0
2023-02-27 15:30:43,214 DEBUG TRAIN Batch 46/2000 loss 10.261992 loss_att 15.106843 loss_ctc 16.809650 loss_rnnt 8.194336 hw_loss 0.423122 lr 0.00029661 rank 5
2023-02-27 15:30:43,215 DEBUG TRAIN Batch 46/2000 loss 5.621239 loss_att 6.424134 loss_ctc 9.909233 loss_rnnt 4.750946 hw_loss 0.258714 lr 0.00029661 rank 4
2023-02-27 15:30:43,219 DEBUG TRAIN Batch 46/2000 loss 3.883602 loss_att 7.841969 loss_ctc 9.693357 loss_rnnt 2.159134 hw_loss 0.296551 lr 0.00029661 rank 2
2023-02-27 15:30:43,220 DEBUG TRAIN Batch 46/2000 loss 10.831927 loss_att 13.568481 loss_ctc 22.380878 loss_rnnt 8.670778 hw_loss 0.138708 lr 0.00029660 rank 7
2023-02-27 15:30:43,224 DEBUG TRAIN Batch 46/2000 loss 2.925476 loss_att 6.494275 loss_ctc 7.517177 loss_rnnt 1.509805 hw_loss 0.168157 lr 0.00029661 rank 1
2023-02-27 15:31:22,842 DEBUG TRAIN Batch 46/2100 loss 2.932720 loss_att 5.725719 loss_ctc 5.127600 loss_rnnt 1.927301 hw_loss 0.289066 lr 0.00029659 rank 7
2023-02-27 15:31:22,843 DEBUG TRAIN Batch 46/2100 loss 2.337540 loss_att 4.933833 loss_ctc 5.318262 loss_rnnt 1.271128 hw_loss 0.280733 lr 0.00029659 rank 3
2023-02-27 15:31:22,846 DEBUG TRAIN Batch 46/2100 loss 8.275610 loss_att 9.870051 loss_ctc 12.941142 loss_rnnt 7.162670 hw_loss 0.322463 lr 0.00029659 rank 6
2023-02-27 15:31:22,851 DEBUG TRAIN Batch 46/2100 loss 3.378615 loss_att 5.016744 loss_ctc 4.746517 loss_rnnt 2.867344 hw_loss 0.002360 lr 0.00029660 rank 1
2023-02-27 15:31:22,852 DEBUG TRAIN Batch 46/2100 loss 3.572304 loss_att 6.170914 loss_ctc 9.016684 loss_rnnt 2.180996 hw_loss 0.273127 lr 0.00029660 rank 0
2023-02-27 15:31:22,852 DEBUG TRAIN Batch 46/2100 loss 14.094287 loss_att 18.722956 loss_ctc 17.280123 loss_rnnt 12.598114 hw_loss 0.273116 lr 0.00029660 rank 4
2023-02-27 15:31:22,852 DEBUG TRAIN Batch 46/2100 loss 2.683419 loss_att 6.541985 loss_ctc 5.591002 loss_rnnt 1.420364 hw_loss 0.194368 lr 0.00029660 rank 5
2023-02-27 15:31:22,862 DEBUG TRAIN Batch 46/2100 loss 2.546287 loss_att 4.166429 loss_ctc 3.733484 loss_rnnt 1.906247 hw_loss 0.295723 lr 0.00029659 rank 2
2023-02-27 15:32:27,827 DEBUG TRAIN Batch 46/2200 loss 7.250796 loss_att 10.561068 loss_ctc 12.116106 loss_rnnt 5.790021 hw_loss 0.281273 lr 0.00029658 rank 6
2023-02-27 15:32:27,828 DEBUG TRAIN Batch 46/2200 loss 5.282619 loss_att 6.763581 loss_ctc 6.218338 loss_rnnt 4.721455 hw_loss 0.262893 lr 0.00029658 rank 0
2023-02-27 15:32:27,832 DEBUG TRAIN Batch 46/2200 loss 6.425180 loss_att 11.445347 loss_ctc 15.703750 loss_rnnt 4.058151 hw_loss 0.235976 lr 0.00029658 rank 1
2023-02-27 15:32:27,834 DEBUG TRAIN Batch 46/2200 loss 9.647605 loss_att 14.089281 loss_ctc 20.937801 loss_rnnt 7.111155 hw_loss 0.267666 lr 0.00029658 rank 5
2023-02-27 15:32:27,834 DEBUG TRAIN Batch 46/2200 loss 6.456082 loss_att 7.375643 loss_ctc 8.312417 loss_rnnt 5.872182 hw_loss 0.285892 lr 0.00029658 rank 7
2023-02-27 15:32:27,839 DEBUG TRAIN Batch 46/2200 loss 8.168382 loss_att 11.391422 loss_ctc 16.527512 loss_rnnt 6.377910 hw_loss 0.058710 lr 0.00029658 rank 4
2023-02-27 15:32:27,838 DEBUG TRAIN Batch 46/2200 loss 6.263066 loss_att 10.788587 loss_ctc 12.480619 loss_rnnt 4.435032 hw_loss 0.176104 lr 0.00029658 rank 3
2023-02-27 15:32:27,883 DEBUG TRAIN Batch 46/2200 loss 4.677406 loss_att 8.326035 loss_ctc 10.176722 loss_rnnt 3.149622 hw_loss 0.121530 lr 0.00029658 rank 2
2023-02-27 15:33:06,198 DEBUG TRAIN Batch 46/2300 loss 7.876110 loss_att 10.645422 loss_ctc 11.911608 loss_rnnt 6.653614 hw_loss 0.244815 lr 0.00029657 rank 2
2023-02-27 15:33:06,203 DEBUG TRAIN Batch 46/2300 loss 9.484531 loss_att 14.140737 loss_ctc 16.362095 loss_rnnt 7.528640 hw_loss 0.201828 lr 0.00029656 rank 6
2023-02-27 15:33:06,208 DEBUG TRAIN Batch 46/2300 loss 10.636646 loss_att 15.512612 loss_ctc 16.042404 loss_rnnt 8.741937 hw_loss 0.372652 lr 0.00029657 rank 4
2023-02-27 15:33:06,210 DEBUG TRAIN Batch 46/2300 loss 6.461275 loss_att 9.715750 loss_ctc 12.103077 loss_rnnt 4.823564 hw_loss 0.439829 lr 0.00029657 rank 0
2023-02-27 15:33:06,210 DEBUG TRAIN Batch 46/2300 loss 8.179477 loss_att 12.384639 loss_ctc 10.380234 loss_rnnt 6.857901 hw_loss 0.350828 lr 0.00029657 rank 3
2023-02-27 15:33:06,210 DEBUG TRAIN Batch 46/2300 loss 10.729358 loss_att 13.215007 loss_ctc 15.196424 loss_rnnt 9.451070 hw_loss 0.347903 lr 0.00029657 rank 5
2023-02-27 15:33:06,213 DEBUG TRAIN Batch 46/2300 loss 6.562018 loss_att 12.293404 loss_ctc 13.083308 loss_rnnt 4.403625 hw_loss 0.267396 lr 0.00029657 rank 1
2023-02-27 15:33:06,214 DEBUG TRAIN Batch 46/2300 loss 4.655266 loss_att 6.529215 loss_ctc 6.856441 loss_rnnt 3.859974 hw_loss 0.238146 lr 0.00029657 rank 7
2023-02-27 15:33:44,929 DEBUG TRAIN Batch 46/2400 loss 4.461017 loss_att 6.373396 loss_ctc 7.484138 loss_rnnt 3.549701 hw_loss 0.235795 lr 0.00029655 rank 2
2023-02-27 15:33:44,936 DEBUG TRAIN Batch 46/2400 loss 5.651751 loss_att 8.204336 loss_ctc 10.580013 loss_rnnt 4.348372 hw_loss 0.254552 lr 0.00029655 rank 3
2023-02-27 15:33:44,937 DEBUG TRAIN Batch 46/2400 loss 6.155491 loss_att 7.962324 loss_ctc 8.794171 loss_rnnt 5.279451 hw_loss 0.305344 lr 0.00029655 rank 7
2023-02-27 15:33:44,941 DEBUG TRAIN Batch 46/2400 loss 6.458366 loss_att 10.143201 loss_ctc 10.865226 loss_rnnt 5.022623 hw_loss 0.208490 lr 0.00029656 rank 0
2023-02-27 15:33:44,942 DEBUG TRAIN Batch 46/2400 loss 4.863324 loss_att 7.078585 loss_ctc 7.299748 loss_rnnt 3.946924 hw_loss 0.278421 lr 0.00029656 rank 1
2023-02-27 15:33:44,943 DEBUG TRAIN Batch 46/2400 loss 6.209060 loss_att 8.360023 loss_ctc 9.404327 loss_rnnt 5.183536 hw_loss 0.317431 lr 0.00029656 rank 5
2023-02-27 15:33:44,946 DEBUG TRAIN Batch 46/2400 loss 5.205130 loss_att 7.801644 loss_ctc 12.600596 loss_rnnt 3.564056 hw_loss 0.254454 lr 0.00029655 rank 6
2023-02-27 15:33:44,948 DEBUG TRAIN Batch 46/2400 loss 3.603011 loss_att 5.450664 loss_ctc 4.773332 loss_rnnt 2.972293 hw_loss 0.197146 lr 0.00029656 rank 4
2023-02-27 15:34:51,795 DEBUG TRAIN Batch 46/2500 loss 7.315392 loss_att 8.157244 loss_ctc 11.610899 loss_rnnt 6.451364 hw_loss 0.230482 lr 0.00029655 rank 0
2023-02-27 15:34:51,798 DEBUG TRAIN Batch 46/2500 loss 6.876953 loss_att 8.411498 loss_ctc 12.990949 loss_rnnt 5.545384 hw_loss 0.392739 lr 0.00029654 rank 6
2023-02-27 15:34:51,801 DEBUG TRAIN Batch 46/2500 loss 4.257201 loss_att 6.976450 loss_ctc 5.874085 loss_rnnt 3.496529 hw_loss 0.002320 lr 0.00029654 rank 4
2023-02-27 15:34:51,801 DEBUG TRAIN Batch 46/2500 loss 7.753593 loss_att 8.605642 loss_ctc 8.804824 loss_rnnt 7.347175 hw_loss 0.179708 lr 0.00029654 rank 2
2023-02-27 15:34:51,803 DEBUG TRAIN Batch 46/2500 loss 8.383724 loss_att 10.106712 loss_ctc 13.088808 loss_rnnt 7.261312 hw_loss 0.282130 lr 0.00029654 rank 7
2023-02-27 15:34:51,803 DEBUG TRAIN Batch 46/2500 loss 6.441378 loss_att 7.343984 loss_ctc 11.392530 loss_rnnt 5.400158 hw_loss 0.376022 lr 0.00029654 rank 3
2023-02-27 15:34:51,805 DEBUG TRAIN Batch 46/2500 loss 4.117318 loss_att 6.744397 loss_ctc 4.582709 loss_rnnt 3.426636 hw_loss 0.193526 lr 0.00029654 rank 5
2023-02-27 15:34:51,806 DEBUG TRAIN Batch 46/2500 loss 6.465762 loss_att 8.212210 loss_ctc 12.855800 loss_rnnt 5.096732 hw_loss 0.314504 lr 0.00029654 rank 1
2023-02-27 15:35:30,488 DEBUG TRAIN Batch 46/2600 loss 5.996839 loss_att 10.721999 loss_ctc 12.859421 loss_rnnt 4.063613 hw_loss 0.137218 lr 0.00029653 rank 3
2023-02-27 15:35:30,500 DEBUG TRAIN Batch 46/2600 loss 3.825641 loss_att 10.417245 loss_ctc 5.838823 loss_rnnt 2.067172 hw_loss 0.321982 lr 0.00029652 rank 6
2023-02-27 15:35:30,500 DEBUG TRAIN Batch 46/2600 loss 4.829920 loss_att 6.662487 loss_ctc 9.804291 loss_rnnt 3.578188 hw_loss 0.416193 lr 0.00029653 rank 5
2023-02-27 15:35:30,507 DEBUG TRAIN Batch 46/2600 loss 8.119526 loss_att 7.918398 loss_ctc 12.667702 loss_rnnt 7.375791 hw_loss 0.332880 lr 0.00029653 rank 2
2023-02-27 15:35:30,508 DEBUG TRAIN Batch 46/2600 loss 4.467359 loss_att 7.921879 loss_ctc 9.356136 loss_rnnt 3.024765 hw_loss 0.187223 lr 0.00029653 rank 0
2023-02-27 15:35:30,509 DEBUG TRAIN Batch 46/2600 loss 6.775744 loss_att 10.885761 loss_ctc 10.187895 loss_rnnt 5.278546 hw_loss 0.412953 lr 0.00029653 rank 4
2023-02-27 15:35:30,537 DEBUG TRAIN Batch 46/2600 loss 4.837797 loss_att 9.202009 loss_ctc 6.687656 loss_rnnt 3.636741 hw_loss 0.152936 lr 0.00029653 rank 7
2023-02-27 15:35:30,548 DEBUG TRAIN Batch 46/2600 loss 3.841547 loss_att 7.181589 loss_ctc 5.681501 loss_rnnt 2.749954 hw_loss 0.334231 lr 0.00029653 rank 1
2023-02-27 15:36:09,107 DEBUG TRAIN Batch 46/2700 loss 2.188208 loss_att 4.360715 loss_ctc 3.855017 loss_rnnt 1.440080 hw_loss 0.171346 lr 0.00029652 rank 0
2023-02-27 15:36:09,128 DEBUG TRAIN Batch 46/2700 loss 3.026169 loss_att 5.750378 loss_ctc 7.297691 loss_rnnt 1.836171 hw_loss 0.141787 lr 0.00029652 rank 4
2023-02-27 15:36:09,127 DEBUG TRAIN Batch 46/2700 loss 1.794209 loss_att 4.159795 loss_ctc 2.857666 loss_rnnt 0.990156 hw_loss 0.354641 lr 0.00029652 rank 5
2023-02-27 15:36:09,129 DEBUG TRAIN Batch 46/2700 loss 2.852676 loss_att 6.174167 loss_ctc 4.920023 loss_rnnt 1.789027 hw_loss 0.231945 lr 0.00029652 rank 3
2023-02-27 15:36:09,130 DEBUG TRAIN Batch 46/2700 loss 13.514523 loss_att 15.040643 loss_ctc 20.056036 loss_rnnt 12.202124 hw_loss 0.253075 lr 0.00029651 rank 2
2023-02-27 15:36:09,130 DEBUG TRAIN Batch 46/2700 loss 10.087645 loss_att 15.686813 loss_ctc 16.699127 loss_rnnt 7.971708 hw_loss 0.214822 lr 0.00029651 rank 7
2023-02-27 15:36:09,133 DEBUG TRAIN Batch 46/2700 loss 2.364151 loss_att 4.927424 loss_ctc 2.111961 loss_rnnt 1.759032 hw_loss 0.236418 lr 0.00029651 rank 6
2023-02-27 15:36:09,134 DEBUG TRAIN Batch 46/2700 loss 2.240162 loss_att 5.195251 loss_ctc 3.600963 loss_rnnt 1.369097 hw_loss 0.184889 lr 0.00029652 rank 1
2023-02-27 15:36:48,877 DEBUG TRAIN Batch 46/2800 loss 13.565253 loss_att 14.335093 loss_ctc 16.139946 loss_rnnt 12.982422 hw_loss 0.160445 lr 0.00029650 rank 7
2023-02-27 15:36:48,878 DEBUG TRAIN Batch 46/2800 loss 7.068172 loss_att 11.203721 loss_ctc 14.406362 loss_rnnt 5.166711 hw_loss 0.179861 lr 0.00029650 rank 3
2023-02-27 15:36:48,885 DEBUG TRAIN Batch 46/2800 loss 5.653398 loss_att 10.611677 loss_ctc 11.696781 loss_rnnt 3.670998 hw_loss 0.346799 lr 0.00029651 rank 0
2023-02-27 15:36:48,888 DEBUG TRAIN Batch 46/2800 loss 7.241790 loss_att 8.369404 loss_ctc 11.610698 loss_rnnt 6.243202 hw_loss 0.357269 lr 0.00029650 rank 6
2023-02-27 15:36:48,891 DEBUG TRAIN Batch 46/2800 loss 5.522831 loss_att 6.949803 loss_ctc 6.949680 loss_rnnt 4.941219 hw_loss 0.198695 lr 0.00029650 rank 5
2023-02-27 15:36:48,894 DEBUG TRAIN Batch 46/2800 loss 13.311931 loss_att 15.823507 loss_ctc 15.563735 loss_rnnt 12.337398 hw_loss 0.322459 lr 0.00029650 rank 1
2023-02-27 15:36:48,896 DEBUG TRAIN Batch 46/2800 loss 3.297993 loss_att 6.690163 loss_ctc 8.213022 loss_rnnt 1.779897 hw_loss 0.345608 lr 0.00029650 rank 4
2023-02-27 15:36:48,935 DEBUG TRAIN Batch 46/2800 loss 4.355165 loss_att 7.781867 loss_ctc 6.443859 loss_rnnt 3.242025 hw_loss 0.279949 lr 0.00029650 rank 2
2023-02-27 15:37:54,082 DEBUG TRAIN Batch 46/2900 loss 2.188274 loss_att 5.532091 loss_ctc 4.288288 loss_rnnt 1.046967 hw_loss 0.361016 lr 0.00029649 rank 3
2023-02-27 15:37:54,093 DEBUG TRAIN Batch 46/2900 loss 8.668902 loss_att 11.427514 loss_ctc 13.421857 loss_rnnt 7.360979 hw_loss 0.229638 lr 0.00029649 rank 6
2023-02-27 15:37:54,094 DEBUG TRAIN Batch 46/2900 loss 6.321454 loss_att 10.170782 loss_ctc 9.778736 loss_rnnt 4.979686 hw_loss 0.207995 lr 0.00029649 rank 5
2023-02-27 15:37:54,100 DEBUG TRAIN Batch 46/2900 loss 4.688532 loss_att 9.226075 loss_ctc 12.499532 loss_rnnt 2.530883 hw_loss 0.391263 lr 0.00029649 rank 7
2023-02-27 15:37:54,101 DEBUG TRAIN Batch 46/2900 loss 6.391667 loss_att 9.150119 loss_ctc 10.517086 loss_rnnt 5.120936 hw_loss 0.316845 lr 0.00029649 rank 4
2023-02-27 15:37:54,103 DEBUG TRAIN Batch 46/2900 loss 5.887934 loss_att 8.302442 loss_ctc 6.962480 loss_rnnt 5.092515 hw_loss 0.317334 lr 0.00029649 rank 1
2023-02-27 15:37:54,107 DEBUG TRAIN Batch 46/2900 loss 8.263795 loss_att 9.493199 loss_ctc 10.573084 loss_rnnt 7.568026 hw_loss 0.266217 lr 0.00029649 rank 0
2023-02-27 15:37:54,152 DEBUG TRAIN Batch 46/2900 loss 6.996820 loss_att 10.605915 loss_ctc 10.836330 loss_rnnt 5.632226 hw_loss 0.245325 lr 0.00029649 rank 2
2023-02-27 15:38:33,157 DEBUG TRAIN Batch 46/3000 loss 5.019485 loss_att 7.066866 loss_ctc 9.195476 loss_rnnt 3.915504 hw_loss 0.258197 lr 0.00029647 rank 7
2023-02-27 15:38:33,163 DEBUG TRAIN Batch 46/3000 loss 6.741921 loss_att 9.995380 loss_ctc 12.648809 loss_rnnt 5.176566 hw_loss 0.238272 lr 0.00029648 rank 3
2023-02-27 15:38:33,166 DEBUG TRAIN Batch 46/3000 loss 5.528433 loss_att 7.933895 loss_ctc 8.341736 loss_rnnt 4.552587 hw_loss 0.224337 lr 0.00029648 rank 4
2023-02-27 15:38:33,166 DEBUG TRAIN Batch 46/3000 loss 2.168310 loss_att 5.513279 loss_ctc 4.252258 loss_rnnt 1.102100 hw_loss 0.223793 lr 0.00029648 rank 0
2023-02-27 15:38:33,167 DEBUG TRAIN Batch 46/3000 loss 5.141805 loss_att 7.893139 loss_ctc 7.635481 loss_rnnt 4.078291 hw_loss 0.338918 lr 0.00029648 rank 2
2023-02-27 15:38:33,168 DEBUG TRAIN Batch 46/3000 loss 6.321659 loss_att 10.669904 loss_ctc 9.349201 loss_rnnt 4.909575 hw_loss 0.260179 lr 0.00029648 rank 5
2023-02-27 15:38:33,171 DEBUG TRAIN Batch 46/3000 loss 4.346710 loss_att 7.510432 loss_ctc 10.027204 loss_rnnt 2.769716 hw_loss 0.350345 lr 0.00029647 rank 6
2023-02-27 15:38:33,217 DEBUG TRAIN Batch 46/3000 loss 5.864249 loss_att 9.057988 loss_ctc 11.416418 loss_rnnt 4.385911 hw_loss 0.186188 lr 0.00029648 rank 1
2023-02-27 15:39:11,801 DEBUG TRAIN Batch 46/3100 loss 8.230481 loss_att 9.814273 loss_ctc 13.443456 loss_rnnt 7.099416 hw_loss 0.223583 lr 0.00029646 rank 4
2023-02-27 15:39:11,815 DEBUG TRAIN Batch 46/3100 loss 6.746721 loss_att 7.273859 loss_ctc 10.473373 loss_rnnt 5.966232 hw_loss 0.334078 lr 0.00029646 rank 6
2023-02-27 15:39:11,817 DEBUG TRAIN Batch 46/3100 loss 6.179832 loss_att 7.886768 loss_ctc 9.880306 loss_rnnt 5.235602 hw_loss 0.205212 lr 0.00029646 rank 3
2023-02-27 15:39:11,819 DEBUG TRAIN Batch 46/3100 loss 8.312310 loss_att 11.637289 loss_ctc 18.043810 loss_rnnt 6.213579 hw_loss 0.255379 lr 0.00029647 rank 5
2023-02-27 15:39:11,820 DEBUG TRAIN Batch 46/3100 loss 4.501591 loss_att 6.426110 loss_ctc 7.696322 loss_rnnt 3.502258 hw_loss 0.353372 lr 0.00029647 rank 0
2023-02-27 15:39:11,822 DEBUG TRAIN Batch 46/3100 loss 5.658181 loss_att 7.666545 loss_ctc 11.589550 loss_rnnt 4.362597 hw_loss 0.193240 lr 0.00029646 rank 7
2023-02-27 15:39:11,823 DEBUG TRAIN Batch 46/3100 loss 4.000065 loss_att 6.523930 loss_ctc 7.155930 loss_rnnt 2.930583 hw_loss 0.269862 lr 0.00029647 rank 1
2023-02-27 15:39:11,864 DEBUG TRAIN Batch 46/3100 loss 5.764011 loss_att 9.101087 loss_ctc 9.833570 loss_rnnt 4.448898 hw_loss 0.197042 lr 0.00029646 rank 2
2023-02-27 15:40:17,790 DEBUG TRAIN Batch 46/3200 loss 7.440536 loss_att 11.210308 loss_ctc 12.045520 loss_rnnt 5.935216 hw_loss 0.257565 lr 0.00029645 rank 4
2023-02-27 15:40:17,797 DEBUG TRAIN Batch 46/3200 loss 6.642016 loss_att 11.657290 loss_ctc 11.449232 loss_rnnt 4.879699 hw_loss 0.221814 lr 0.00029645 rank 6
2023-02-27 15:40:17,803 DEBUG TRAIN Batch 46/3200 loss 5.876151 loss_att 11.427926 loss_ctc 7.648716 loss_rnnt 4.349199 hw_loss 0.337977 lr 0.00029645 rank 0
2023-02-27 15:40:17,804 DEBUG TRAIN Batch 46/3200 loss 1.155011 loss_att 3.819929 loss_ctc 1.921755 loss_rnnt 0.382900 hw_loss 0.256678 lr 0.00029645 rank 7
2023-02-27 15:40:17,804 DEBUG TRAIN Batch 46/3200 loss 10.488532 loss_att 12.116720 loss_ctc 13.504133 loss_rnnt 9.594955 hw_loss 0.310987 lr 0.00029645 rank 2
2023-02-27 15:40:17,806 DEBUG TRAIN Batch 46/3200 loss 5.242392 loss_att 7.545696 loss_ctc 8.295209 loss_rnnt 4.251565 hw_loss 0.230857 lr 0.00029645 rank 5
2023-02-27 15:40:17,814 DEBUG TRAIN Batch 46/3200 loss 6.613898 loss_att 6.661292 loss_ctc 10.594170 loss_rnnt 5.866739 hw_loss 0.388082 lr 0.00029645 rank 1
2023-02-27 15:40:17,845 DEBUG TRAIN Batch 46/3200 loss 5.881239 loss_att 8.902749 loss_ctc 9.190559 loss_rnnt 4.670518 hw_loss 0.309706 lr 0.00029645 rank 3
2023-02-27 15:40:56,768 DEBUG TRAIN Batch 46/3300 loss 7.171104 loss_att 7.699011 loss_ctc 8.307939 loss_rnnt 6.826425 hw_loss 0.164100 lr 0.00029643 rank 6
2023-02-27 15:40:56,769 DEBUG TRAIN Batch 46/3300 loss 5.487760 loss_att 6.897461 loss_ctc 8.753066 loss_rnnt 4.631465 hw_loss 0.260586 lr 0.00029644 rank 0
2023-02-27 15:40:56,769 DEBUG TRAIN Batch 46/3300 loss 5.800601 loss_att 8.753622 loss_ctc 9.563831 loss_rnnt 4.609715 hw_loss 0.184718 lr 0.00029644 rank 3
2023-02-27 15:40:56,776 DEBUG TRAIN Batch 46/3300 loss 1.827445 loss_att 4.418428 loss_ctc 2.523696 loss_rnnt 1.053085 hw_loss 0.306245 lr 0.00029644 rank 7
2023-02-27 15:40:56,777 DEBUG TRAIN Batch 46/3300 loss 9.601781 loss_att 14.674503 loss_ctc 21.910080 loss_rnnt 6.868563 hw_loss 0.145437 lr 0.00029644 rank 4
2023-02-27 15:40:56,776 DEBUG TRAIN Batch 46/3300 loss 4.982534 loss_att 5.840026 loss_ctc 7.454946 loss_rnnt 4.294858 hw_loss 0.349731 lr 0.00029644 rank 5
2023-02-27 15:40:56,791 DEBUG TRAIN Batch 46/3300 loss 10.881497 loss_att 13.921103 loss_ctc 20.443888 loss_rnnt 8.803614 hw_loss 0.365584 lr 0.00029644 rank 2
2023-02-27 15:40:56,820 DEBUG TRAIN Batch 46/3300 loss 5.099889 loss_att 7.529696 loss_ctc 8.176082 loss_rnnt 4.068133 hw_loss 0.254317 lr 0.00029644 rank 1
2023-02-27 15:41:35,581 DEBUG TRAIN Batch 46/3400 loss 3.812088 loss_att 5.678281 loss_ctc 5.410385 loss_rnnt 3.163732 hw_loss 0.116272 lr 0.00029642 rank 7
2023-02-27 15:41:35,582 DEBUG TRAIN Batch 46/3400 loss 5.258859 loss_att 9.089247 loss_ctc 9.099768 loss_rnnt 3.821130 hw_loss 0.299117 lr 0.00029643 rank 0
2023-02-27 15:41:35,584 DEBUG TRAIN Batch 46/3400 loss 2.879135 loss_att 5.251363 loss_ctc 3.751149 loss_rnnt 2.188281 hw_loss 0.187763 lr 0.00029642 rank 3
2023-02-27 15:41:35,589 DEBUG TRAIN Batch 46/3400 loss 7.406397 loss_att 12.420612 loss_ctc 12.144657 loss_rnnt 5.668645 hw_loss 0.193389 lr 0.00029642 rank 6
2023-02-27 15:41:35,594 DEBUG TRAIN Batch 46/3400 loss 8.835658 loss_att 11.339304 loss_ctc 12.686790 loss_rnnt 7.765386 hw_loss 0.105109 lr 0.00029643 rank 5
2023-02-27 15:41:35,597 DEBUG TRAIN Batch 46/3400 loss 3.213188 loss_att 6.700573 loss_ctc 8.821047 loss_rnnt 1.720496 hw_loss 0.089065 lr 0.00029643 rank 1
2023-02-27 15:41:35,607 DEBUG TRAIN Batch 46/3400 loss 3.998853 loss_att 7.190594 loss_ctc 10.486371 loss_rnnt 2.386803 hw_loss 0.203811 lr 0.00029643 rank 4
2023-02-27 15:41:35,641 DEBUG TRAIN Batch 46/3400 loss 4.032367 loss_att 8.587318 loss_ctc 9.533487 loss_rnnt 2.228589 hw_loss 0.298697 lr 0.00029642 rank 2
2023-02-27 15:42:15,856 DEBUG TRAIN Batch 46/3500 loss 9.422369 loss_att 15.188982 loss_ctc 17.808424 loss_rnnt 7.037229 hw_loss 0.213144 lr 0.00029641 rank 5
2023-02-27 15:42:15,861 DEBUG TRAIN Batch 46/3500 loss 9.056273 loss_att 12.250859 loss_ctc 14.122717 loss_rnnt 7.651418 hw_loss 0.169520 lr 0.00029641 rank 2
2023-02-27 15:42:15,862 DEBUG TRAIN Batch 46/3500 loss 3.883349 loss_att 7.155699 loss_ctc 7.813897 loss_rnnt 2.614813 hw_loss 0.168738 lr 0.00029641 rank 4
2023-02-27 15:42:15,866 DEBUG TRAIN Batch 46/3500 loss 5.231040 loss_att 7.770879 loss_ctc 10.790012 loss_rnnt 3.929223 hw_loss 0.098724 lr 0.00029641 rank 3
2023-02-27 15:42:15,867 DEBUG TRAIN Batch 46/3500 loss 8.366199 loss_att 9.537217 loss_ctc 12.701334 loss_rnnt 7.477981 hw_loss 0.142492 lr 0.00029641 rank 7
2023-02-27 15:42:15,868 DEBUG TRAIN Batch 46/3500 loss 11.710040 loss_att 13.981874 loss_ctc 20.219305 loss_rnnt 10.036544 hw_loss 0.158551 lr 0.00029641 rank 6
2023-02-27 15:42:15,871 DEBUG TRAIN Batch 46/3500 loss 3.169014 loss_att 5.337123 loss_ctc 8.362219 loss_rnnt 1.847914 hw_loss 0.365720 lr 0.00029641 rank 1
2023-02-27 15:42:15,897 DEBUG TRAIN Batch 46/3500 loss 2.926414 loss_att 6.038000 loss_ctc 5.639966 loss_rnnt 1.887074 hw_loss 0.103529 lr 0.00029641 rank 0
2023-02-27 15:43:22,032 DEBUG TRAIN Batch 46/3600 loss 4.288326 loss_att 8.141426 loss_ctc 8.792112 loss_rnnt 2.916131 hw_loss 0.002007 lr 0.00029639 rank 6
2023-02-27 15:43:22,033 DEBUG TRAIN Batch 46/3600 loss 7.264364 loss_att 10.362992 loss_ctc 10.193794 loss_rnnt 6.147419 hw_loss 0.199928 lr 0.00029640 rank 7
2023-02-27 15:43:22,034 DEBUG TRAIN Batch 46/3600 loss 7.885845 loss_att 10.225387 loss_ctc 14.092153 loss_rnnt 6.456228 hw_loss 0.251626 lr 0.00029640 rank 3
2023-02-27 15:43:22,036 DEBUG TRAIN Batch 46/3600 loss 5.151779 loss_att 7.367727 loss_ctc 6.967897 loss_rnnt 4.384041 hw_loss 0.154497 lr 0.00029640 rank 0
2023-02-27 15:43:22,036 DEBUG TRAIN Batch 46/3600 loss 7.260775 loss_att 9.802852 loss_ctc 13.365582 loss_rnnt 5.800910 hw_loss 0.257763 lr 0.00029640 rank 1
2023-02-27 15:43:22,043 DEBUG TRAIN Batch 46/3600 loss 4.526198 loss_att 7.044523 loss_ctc 7.895081 loss_rnnt 3.393620 hw_loss 0.336992 lr 0.00029640 rank 2
2023-02-27 15:43:22,068 DEBUG TRAIN Batch 46/3600 loss 7.957366 loss_att 9.423557 loss_ctc 13.772425 loss_rnnt 6.789907 hw_loss 0.185399 lr 0.00029640 rank 4
2023-02-27 15:43:22,109 DEBUG TRAIN Batch 46/3600 loss 7.149346 loss_att 8.579088 loss_ctc 12.133080 loss_rnnt 6.041451 hw_loss 0.295216 lr 0.00029640 rank 5
2023-02-27 15:44:02,028 DEBUG TRAIN Batch 46/3700 loss 5.734897 loss_att 8.097595 loss_ctc 8.735851 loss_rnnt 4.713644 hw_loss 0.278601 lr 0.00029638 rank 6
2023-02-27 15:44:02,028 DEBUG TRAIN Batch 46/3700 loss 5.130330 loss_att 9.395072 loss_ctc 16.943436 loss_rnnt 2.584386 hw_loss 0.221091 lr 0.00029639 rank 1
2023-02-27 15:44:02,028 DEBUG TRAIN Batch 46/3700 loss 2.946235 loss_att 5.219658 loss_ctc 4.272083 loss_rnnt 2.107073 hw_loss 0.389433 lr 0.00029639 rank 4
2023-02-27 15:44:02,030 DEBUG TRAIN Batch 46/3700 loss 10.464090 loss_att 14.153260 loss_ctc 14.289436 loss_rnnt 9.080802 hw_loss 0.253892 lr 0.00029639 rank 0
2023-02-27 15:44:02,031 DEBUG TRAIN Batch 46/3700 loss 3.765893 loss_att 6.814509 loss_ctc 6.533107 loss_rnnt 2.640730 hw_loss 0.274645 lr 0.00029639 rank 5
2023-02-27 15:44:02,032 DEBUG TRAIN Batch 46/3700 loss 9.604239 loss_att 11.499567 loss_ctc 13.517629 loss_rnnt 8.628883 hw_loss 0.139697 lr 0.00029638 rank 7
2023-02-27 15:44:02,035 DEBUG TRAIN Batch 46/3700 loss 5.809523 loss_att 7.346663 loss_ctc 7.778314 loss_rnnt 5.078977 hw_loss 0.301149 lr 0.00029639 rank 3
2023-02-27 15:44:02,057 DEBUG TRAIN Batch 46/3700 loss 5.979851 loss_att 9.035051 loss_ctc 10.186488 loss_rnnt 4.623966 hw_loss 0.344926 lr 0.00029638 rank 2
2023-02-27 15:44:42,545 DEBUG TRAIN Batch 46/3800 loss 7.782707 loss_att 12.301163 loss_ctc 15.387762 loss_rnnt 5.787487 hw_loss 0.145353 lr 0.00029637 rank 3
2023-02-27 15:44:42,548 DEBUG TRAIN Batch 46/3800 loss 3.833025 loss_att 4.464242 loss_ctc 6.092068 loss_rnnt 3.265804 hw_loss 0.262074 lr 0.00029637 rank 6
2023-02-27 15:44:42,548 DEBUG TRAIN Batch 46/3800 loss 3.839923 loss_att 5.774327 loss_ctc 6.808556 loss_rnnt 2.934553 hw_loss 0.230010 lr 0.00029638 rank 0
2023-02-27 15:44:42,552 DEBUG TRAIN Batch 46/3800 loss 5.533114 loss_att 9.930754 loss_ctc 10.373649 loss_rnnt 3.881701 hw_loss 0.237150 lr 0.00029637 rank 7
2023-02-27 15:44:42,553 DEBUG TRAIN Batch 46/3800 loss 8.138396 loss_att 10.294727 loss_ctc 11.846989 loss_rnnt 7.066054 hw_loss 0.274868 lr 0.00029637 rank 5
2023-02-27 15:44:42,556 DEBUG TRAIN Batch 46/3800 loss 11.017578 loss_att 14.566629 loss_ctc 27.352400 loss_rnnt 7.996410 hw_loss 0.250090 lr 0.00029637 rank 2
2023-02-27 15:44:42,588 DEBUG TRAIN Batch 46/3800 loss 8.754515 loss_att 10.619922 loss_ctc 10.989815 loss_rnnt 7.966802 hw_loss 0.218607 lr 0.00029637 rank 4
2023-02-27 15:44:42,964 DEBUG TRAIN Batch 46/3800 loss 10.581634 loss_att 13.250602 loss_ctc 15.070859 loss_rnnt 9.331733 hw_loss 0.220397 lr 0.00029637 rank 1
2023-02-27 15:45:49,419 DEBUG TRAIN Batch 46/3900 loss 8.381375 loss_att 12.102462 loss_ctc 13.789302 loss_rnnt 6.832580 hw_loss 0.156602 lr 0.00029636 rank 4
2023-02-27 15:45:49,424 DEBUG TRAIN Batch 46/3900 loss 2.411973 loss_att 5.237683 loss_ctc 3.597068 loss_rnnt 1.597012 hw_loss 0.172136 lr 0.00029636 rank 6
2023-02-27 15:45:49,430 DEBUG TRAIN Batch 46/3900 loss 3.222886 loss_att 6.511479 loss_ctc 7.659694 loss_rnnt 1.832203 hw_loss 0.265106 lr 0.00029636 rank 3
2023-02-27 15:45:49,438 DEBUG TRAIN Batch 46/3900 loss 5.025871 loss_att 7.441190 loss_ctc 6.779398 loss_rnnt 4.268777 hw_loss 0.075425 lr 0.00029636 rank 7
2023-02-27 15:45:49,450 DEBUG TRAIN Batch 46/3900 loss 7.778380 loss_att 8.197395 loss_ctc 12.465590 loss_rnnt 6.860851 hw_loss 0.391435 lr 0.00029636 rank 2
2023-02-27 15:45:49,455 DEBUG TRAIN Batch 46/3900 loss 4.272514 loss_att 9.669593 loss_ctc 7.505928 loss_rnnt 2.704763 hw_loss 0.107275 lr 0.00029636 rank 1
2023-02-27 15:45:49,462 DEBUG TRAIN Batch 46/3900 loss 3.279938 loss_att 6.263391 loss_ctc 9.827018 loss_rnnt 1.666906 hw_loss 0.268871 lr 0.00029636 rank 0
2023-02-27 15:45:49,478 DEBUG TRAIN Batch 46/3900 loss 8.632439 loss_att 11.765972 loss_ctc 18.454542 loss_rnnt 6.590569 hw_loss 0.197903 lr 0.00029636 rank 5
2023-02-27 15:46:28,939 DEBUG TRAIN Batch 46/4000 loss 4.892359 loss_att 7.192557 loss_ctc 10.223030 loss_rnnt 3.554704 hw_loss 0.312860 lr 0.00029635 rank 2
2023-02-27 15:46:28,952 DEBUG TRAIN Batch 46/4000 loss 2.419416 loss_att 4.901470 loss_ctc 3.095048 loss_rnnt 1.660263 hw_loss 0.323736 lr 0.00029635 rank 1
2023-02-27 15:46:28,953 DEBUG TRAIN Batch 46/4000 loss 1.373052 loss_att 4.142431 loss_ctc 1.904783 loss_rnnt 0.689444 hw_loss 0.110315 lr 0.00029635 rank 3
2023-02-27 15:46:28,953 DEBUG TRAIN Batch 46/4000 loss 7.956073 loss_att 10.155560 loss_ctc 14.072348 loss_rnnt 6.567430 hw_loss 0.249829 lr 0.00029635 rank 0
2023-02-27 15:46:28,954 DEBUG TRAIN Batch 46/4000 loss 1.338016 loss_att 3.853209 loss_ctc 2.545016 loss_rnnt 0.592361 hw_loss 0.153157 lr 0.00029635 rank 4
2023-02-27 15:46:28,954 DEBUG TRAIN Batch 46/4000 loss 4.393177 loss_att 6.827499 loss_ctc 8.147322 loss_rnnt 3.308903 hw_loss 0.181607 lr 0.00029634 rank 6
2023-02-27 15:46:28,955 DEBUG TRAIN Batch 46/4000 loss 3.300179 loss_att 5.766879 loss_ctc 7.624927 loss_rnnt 2.097199 hw_loss 0.249387 lr 0.00029635 rank 5
2023-02-27 15:46:29,005 DEBUG TRAIN Batch 46/4000 loss 8.821251 loss_att 12.662609 loss_ctc 14.717438 loss_rnnt 7.165301 hw_loss 0.190351 lr 0.00029634 rank 7
2023-02-27 15:47:07,918 DEBUG TRAIN Batch 46/4100 loss 8.506272 loss_att 11.612239 loss_ctc 10.730215 loss_rnnt 7.448664 hw_loss 0.262293 lr 0.00029634 rank 0
2023-02-27 15:47:07,934 DEBUG TRAIN Batch 46/4100 loss 3.778277 loss_att 7.323003 loss_ctc 7.490523 loss_rnnt 2.362251 hw_loss 0.397714 lr 0.00029633 rank 3
2023-02-27 15:47:07,934 DEBUG TRAIN Batch 46/4100 loss 13.213166 loss_att 15.677707 loss_ctc 19.324379 loss_rnnt 11.800194 hw_loss 0.197319 lr 0.00029633 rank 6
2023-02-27 15:47:07,936 DEBUG TRAIN Batch 46/4100 loss 4.493913 loss_att 7.480503 loss_ctc 6.047829 loss_rnnt 3.640822 hw_loss 0.091097 lr 0.00029634 rank 5
2023-02-27 15:47:07,943 DEBUG TRAIN Batch 46/4100 loss 8.278824 loss_att 12.444370 loss_ctc 11.658714 loss_rnnt 6.833316 hw_loss 0.303274 lr 0.00029633 rank 4
2023-02-27 15:47:07,944 DEBUG TRAIN Batch 46/4100 loss 7.267119 loss_att 10.971133 loss_ctc 16.817467 loss_rnnt 5.096563 hw_loss 0.293200 lr 0.00029634 rank 1
2023-02-27 15:47:07,944 DEBUG TRAIN Batch 46/4100 loss 8.487603 loss_att 10.366106 loss_ctc 12.604943 loss_rnnt 7.450642 hw_loss 0.210529 lr 0.00029633 rank 7
2023-02-27 15:47:07,945 DEBUG TRAIN Batch 46/4100 loss 7.162327 loss_att 9.614483 loss_ctc 11.044775 loss_rnnt 5.998851 hw_loss 0.291349 lr 0.00029633 rank 2
2023-02-27 15:47:47,800 DEBUG TRAIN Batch 46/4200 loss 11.518068 loss_att 14.142921 loss_ctc 16.366953 loss_rnnt 10.187815 hw_loss 0.297683 lr 0.00029632 rank 0
2023-02-27 15:47:47,804 DEBUG TRAIN Batch 46/4200 loss 6.864156 loss_att 9.481916 loss_ctc 13.525314 loss_rnnt 5.352638 hw_loss 0.187146 lr 0.00029632 rank 1
2023-02-27 15:47:47,805 DEBUG TRAIN Batch 46/4200 loss 13.810918 loss_att 14.778706 loss_ctc 21.175041 loss_rnnt 12.555340 hw_loss 0.150259 lr 0.00029632 rank 4
2023-02-27 15:47:47,808 DEBUG TRAIN Batch 46/4200 loss 6.051478 loss_att 8.886954 loss_ctc 10.209671 loss_rnnt 4.831159 hw_loss 0.185246 lr 0.00029632 rank 7
2023-02-27 15:47:47,809 DEBUG TRAIN Batch 46/4200 loss 8.746974 loss_att 10.981072 loss_ctc 14.486672 loss_rnnt 7.451113 hw_loss 0.157031 lr 0.00029632 rank 3
2023-02-27 15:47:47,810 DEBUG TRAIN Batch 46/4200 loss 6.660291 loss_att 9.694113 loss_ctc 12.048052 loss_rnnt 5.258241 hw_loss 0.144221 lr 0.00029632 rank 5
2023-02-27 15:47:47,813 DEBUG TRAIN Batch 46/4200 loss 6.395501 loss_att 8.826624 loss_ctc 12.164275 loss_rnnt 5.001500 hw_loss 0.259889 lr 0.00029632 rank 6
2023-02-27 15:47:47,816 DEBUG TRAIN Batch 46/4200 loss 6.688568 loss_att 10.237419 loss_ctc 11.600035 loss_rnnt 5.228263 hw_loss 0.179384 lr 0.00029632 rank 2
2023-02-27 15:48:54,836 DEBUG TRAIN Batch 46/4300 loss 5.488866 loss_att 7.570074 loss_ctc 8.246325 loss_rnnt 4.545079 hw_loss 0.299782 lr 0.00029631 rank 3
2023-02-27 15:48:54,835 DEBUG TRAIN Batch 46/4300 loss 5.016850 loss_att 7.110022 loss_ctc 8.798365 loss_rnnt 3.916411 hw_loss 0.333005 lr 0.00029630 rank 6
2023-02-27 15:48:54,836 DEBUG TRAIN Batch 46/4300 loss 7.232948 loss_att 9.492414 loss_ctc 9.681601 loss_rnnt 6.361938 hw_loss 0.173679 lr 0.00029631 rank 0
2023-02-27 15:48:54,838 DEBUG TRAIN Batch 46/4300 loss 7.015428 loss_att 9.340149 loss_ctc 12.490856 loss_rnnt 5.796837 hw_loss 0.044231 lr 0.00029631 rank 4
2023-02-27 15:48:54,840 DEBUG TRAIN Batch 46/4300 loss 7.722379 loss_att 9.548492 loss_ctc 14.631998 loss_rnnt 6.340041 hw_loss 0.179686 lr 0.00029631 rank 2
2023-02-27 15:48:54,839 DEBUG TRAIN Batch 46/4300 loss 6.817004 loss_att 8.092603 loss_ctc 9.126436 loss_rnnt 6.169006 hw_loss 0.159288 lr 0.00029631 rank 5
2023-02-27 15:48:54,853 DEBUG TRAIN Batch 46/4300 loss 2.795366 loss_att 5.041376 loss_ctc 4.418299 loss_rnnt 2.011061 hw_loss 0.222585 lr 0.00029630 rank 7
2023-02-27 15:48:54,860 DEBUG TRAIN Batch 46/4300 loss 12.942076 loss_att 15.489607 loss_ctc 20.147673 loss_rnnt 11.364993 hw_loss 0.200305 lr 0.00029631 rank 1
2023-02-27 15:49:34,539 DEBUG TRAIN Batch 46/4400 loss 9.690070 loss_att 15.454713 loss_ctc 17.823227 loss_rnnt 7.283657 hw_loss 0.316994 lr 0.00029629 rank 7
2023-02-27 15:49:34,552 DEBUG TRAIN Batch 46/4400 loss 9.624173 loss_att 12.000058 loss_ctc 14.930710 loss_rnnt 8.303184 hw_loss 0.259264 lr 0.00029629 rank 6
2023-02-27 15:49:34,553 DEBUG TRAIN Batch 46/4400 loss 5.362741 loss_att 7.141955 loss_ctc 7.046097 loss_rnnt 4.670710 hw_loss 0.209512 lr 0.00029629 rank 3
2023-02-27 15:49:34,554 DEBUG TRAIN Batch 46/4400 loss 6.758351 loss_att 8.416618 loss_ctc 16.735683 loss_rnnt 4.986517 hw_loss 0.206007 lr 0.00029630 rank 4
2023-02-27 15:49:34,557 DEBUG TRAIN Batch 46/4400 loss 5.580098 loss_att 7.500450 loss_ctc 10.368536 loss_rnnt 4.458625 hw_loss 0.185519 lr 0.00029629 rank 2
2023-02-27 15:49:34,558 DEBUG TRAIN Batch 46/4400 loss 6.284345 loss_att 9.028011 loss_ctc 11.893856 loss_rnnt 4.825415 hw_loss 0.304241 lr 0.00029630 rank 5
2023-02-27 15:49:34,560 DEBUG TRAIN Batch 46/4400 loss 11.956696 loss_att 13.123700 loss_ctc 21.139349 loss_rnnt 10.331739 hw_loss 0.313502 lr 0.00029630 rank 0
2023-02-27 15:49:34,563 DEBUG TRAIN Batch 46/4400 loss 8.424195 loss_att 8.661328 loss_ctc 10.616920 loss_rnnt 7.958823 hw_loss 0.235466 lr 0.00029630 rank 1
2023-02-27 15:50:13,958 DEBUG TRAIN Batch 46/4500 loss 10.105881 loss_att 11.972186 loss_ctc 14.826463 loss_rnnt 8.926844 hw_loss 0.330685 lr 0.00029628 rank 5
2023-02-27 15:50:13,971 DEBUG TRAIN Batch 46/4500 loss 9.537482 loss_att 12.617193 loss_ctc 17.830069 loss_rnnt 7.779887 hw_loss 0.067453 lr 0.00029628 rank 3
2023-02-27 15:50:13,970 DEBUG TRAIN Batch 46/4500 loss 2.936192 loss_att 7.775334 loss_ctc 7.499753 loss_rnnt 1.248101 hw_loss 0.209601 lr 0.00029628 rank 0
2023-02-27 15:50:13,973 DEBUG TRAIN Batch 46/4500 loss 5.481428 loss_att 9.662925 loss_ctc 8.133089 loss_rnnt 4.215066 hw_loss 0.143452 lr 0.00029628 rank 6
2023-02-27 15:50:13,973 DEBUG TRAIN Batch 46/4500 loss 5.399959 loss_att 10.049562 loss_ctc 9.783319 loss_rnnt 3.749436 hw_loss 0.255289 lr 0.00029628 rank 7
2023-02-27 15:50:13,976 DEBUG TRAIN Batch 46/4500 loss 7.886302 loss_att 11.460255 loss_ctc 7.957647 loss_rnnt 7.026591 hw_loss 0.253890 lr 0.00029628 rank 4
2023-02-27 15:50:13,979 DEBUG TRAIN Batch 46/4500 loss 8.327307 loss_att 9.770805 loss_ctc 11.577229 loss_rnnt 7.458649 hw_loss 0.274942 lr 0.00029628 rank 1
2023-02-27 15:50:13,987 DEBUG TRAIN Batch 46/4500 loss 5.209697 loss_att 5.841768 loss_ctc 6.614890 loss_rnnt 4.729679 hw_loss 0.311709 lr 0.00029628 rank 2
2023-02-27 15:51:19,948 DEBUG TRAIN Batch 46/4600 loss 10.115685 loss_att 11.437601 loss_ctc 13.802477 loss_rnnt 9.184633 hw_loss 0.328307 lr 0.00029627 rank 5
2023-02-27 15:51:19,954 DEBUG TRAIN Batch 46/4600 loss 7.695242 loss_att 11.092072 loss_ctc 16.460310 loss_rnnt 5.806568 hw_loss 0.076186 lr 0.00029627 rank 0
2023-02-27 15:51:19,957 DEBUG TRAIN Batch 46/4600 loss 1.303544 loss_att 3.538587 loss_ctc 2.655224 loss_rnnt 0.574729 hw_loss 0.190467 lr 0.00029627 rank 7
2023-02-27 15:51:19,969 DEBUG TRAIN Batch 46/4600 loss 0.994289 loss_att 3.446658 loss_ctc 1.419628 loss_rnnt 0.330717 hw_loss 0.218224 lr 0.00029627 rank 3
2023-02-27 15:51:19,975 DEBUG TRAIN Batch 46/4600 loss 2.962849 loss_att 6.648455 loss_ctc 9.929384 loss_rnnt 1.243999 hw_loss 0.099109 lr 0.00029627 rank 4
2023-02-27 15:51:19,977 DEBUG TRAIN Batch 46/4600 loss 4.574048 loss_att 8.218716 loss_ctc 7.510311 loss_rnnt 3.417613 hw_loss 0.067498 lr 0.00029626 rank 6
2023-02-27 15:51:19,978 DEBUG TRAIN Batch 46/4600 loss 4.689881 loss_att 7.602175 loss_ctc 9.619480 loss_rnnt 3.274433 hw_loss 0.329455 lr 0.00029627 rank 2
2023-02-27 15:51:20,009 DEBUG TRAIN Batch 46/4600 loss 5.763258 loss_att 10.367085 loss_ctc 14.677302 loss_rnnt 3.579864 hw_loss 0.138920 lr 0.00029627 rank 1
2023-02-27 15:51:59,707 DEBUG TRAIN Batch 46/4700 loss 5.938087 loss_att 7.466051 loss_ctc 8.982425 loss_rnnt 5.095825 hw_loss 0.245169 lr 0.00029626 rank 3
2023-02-27 15:51:59,724 DEBUG TRAIN Batch 46/4700 loss 6.578797 loss_att 12.486404 loss_ctc 13.164215 loss_rnnt 4.375783 hw_loss 0.268944 lr 0.00029626 rank 1
2023-02-27 15:51:59,725 DEBUG TRAIN Batch 46/4700 loss 4.678002 loss_att 8.166662 loss_ctc 4.940239 loss_rnnt 3.799059 hw_loss 0.274211 lr 0.00029626 rank 5
2023-02-27 15:51:59,726 DEBUG TRAIN Batch 46/4700 loss 5.563565 loss_att 8.562553 loss_ctc 9.275844 loss_rnnt 4.308618 hw_loss 0.300335 lr 0.00029626 rank 0
2023-02-27 15:51:59,727 DEBUG TRAIN Batch 46/4700 loss 6.002171 loss_att 12.974078 loss_ctc 10.109756 loss_rnnt 3.958858 hw_loss 0.189850 lr 0.00029626 rank 4
2023-02-27 15:51:59,727 DEBUG TRAIN Batch 46/4700 loss 6.783115 loss_att 9.727795 loss_ctc 13.334998 loss_rnnt 5.228200 hw_loss 0.173241 lr 0.00029625 rank 2
2023-02-27 15:51:59,727 DEBUG TRAIN Batch 46/4700 loss 6.253066 loss_att 8.676938 loss_ctc 10.902840 loss_rnnt 5.028111 hw_loss 0.225395 lr 0.00029625 rank 6
2023-02-27 15:51:59,775 DEBUG TRAIN Batch 46/4700 loss 5.799036 loss_att 7.973828 loss_ctc 10.267001 loss_rnnt 4.665233 hw_loss 0.193342 lr 0.00029625 rank 7
2023-02-27 15:52:39,049 DEBUG TRAIN Batch 46/4800 loss 4.187485 loss_att 8.521137 loss_ctc 6.987436 loss_rnnt 2.783461 hw_loss 0.307437 lr 0.00029624 rank 4
2023-02-27 15:52:39,067 DEBUG TRAIN Batch 46/4800 loss 7.987987 loss_att 8.870800 loss_ctc 10.379447 loss_rnnt 7.438669 hw_loss 0.101051 lr 0.00029625 rank 0
2023-02-27 15:52:39,068 DEBUG TRAIN Batch 46/4800 loss 7.775183 loss_att 11.786994 loss_ctc 16.053596 loss_rnnt 5.826274 hw_loss 0.080173 lr 0.00029624 rank 3
2023-02-27 15:52:39,068 DEBUG TRAIN Batch 46/4800 loss 7.146765 loss_att 9.198294 loss_ctc 8.384050 loss_rnnt 6.494289 hw_loss 0.144747 lr 0.00029624 rank 1
2023-02-27 15:52:39,070 DEBUG TRAIN Batch 46/4800 loss 10.899039 loss_att 13.834406 loss_ctc 22.060984 loss_rnnt 8.674249 hw_loss 0.280232 lr 0.00029624 rank 6
2023-02-27 15:52:39,075 DEBUG TRAIN Batch 46/4800 loss 1.618395 loss_att 5.525434 loss_ctc 3.003250 loss_rnnt 0.604731 hw_loss 0.089266 lr 0.00029624 rank 2
2023-02-27 15:52:39,082 DEBUG TRAIN Batch 46/4800 loss 3.113393 loss_att 8.963089 loss_ctc 10.503635 loss_rnnt 0.882313 hw_loss 0.142078 lr 0.00029624 rank 5
2023-02-27 15:52:39,116 DEBUG TRAIN Batch 46/4800 loss 6.399168 loss_att 7.348781 loss_ctc 12.885742 loss_rnnt 5.136118 hw_loss 0.390469 lr 0.00029624 rank 7
2023-02-27 15:53:18,647 DEBUG TRAIN Batch 46/4900 loss 5.400118 loss_att 6.995224 loss_ctc 9.348830 loss_rnnt 4.403550 hw_loss 0.283223 lr 0.00029623 rank 7
2023-02-27 15:53:18,650 DEBUG TRAIN Batch 46/4900 loss 6.739098 loss_att 8.389957 loss_ctc 9.033796 loss_rnnt 6.022088 hw_loss 0.151647 lr 0.00029623 rank 0
2023-02-27 15:53:18,654 DEBUG TRAIN Batch 46/4900 loss 5.503767 loss_att 6.719387 loss_ctc 8.956010 loss_rnnt 4.692563 hw_loss 0.202090 lr 0.00029623 rank 6
2023-02-27 15:53:18,655 DEBUG TRAIN Batch 46/4900 loss 7.287867 loss_att 8.480610 loss_ctc 12.359869 loss_rnnt 6.258014 hw_loss 0.215696 lr 0.00029623 rank 1
2023-02-27 15:53:18,656 DEBUG TRAIN Batch 46/4900 loss 10.301805 loss_att 15.048975 loss_ctc 15.792156 loss_rnnt 8.535162 hw_loss 0.159678 lr 0.00029623 rank 3
2023-02-27 15:53:18,656 DEBUG TRAIN Batch 46/4900 loss 6.712176 loss_att 10.427225 loss_ctc 13.620810 loss_rnnt 4.858439 hw_loss 0.355456 lr 0.00029623 rank 5
2023-02-27 15:53:18,660 DEBUG TRAIN Batch 46/4900 loss 2.850685 loss_att 4.742484 loss_ctc 3.941627 loss_rnnt 2.176359 hw_loss 0.282203 lr 0.00029623 rank 2
2023-02-27 15:53:18,660 DEBUG TRAIN Batch 46/4900 loss 3.917732 loss_att 5.141904 loss_ctc 8.406066 loss_rnnt 2.960095 hw_loss 0.214420 lr 0.00029623 rank 4
2023-02-27 15:54:27,584 DEBUG TRAIN Batch 46/5000 loss 3.057295 loss_att 4.813407 loss_ctc 5.676536 loss_rnnt 2.278541 hw_loss 0.146812 lr 0.00029622 rank 3
2023-02-27 15:54:27,587 DEBUG TRAIN Batch 46/5000 loss 9.698118 loss_att 12.464196 loss_ctc 14.386360 loss_rnnt 8.362644 hw_loss 0.294673 lr 0.00029621 rank 6
2023-02-27 15:54:27,588 DEBUG TRAIN Batch 46/5000 loss 5.320425 loss_att 8.640572 loss_ctc 9.719078 loss_rnnt 3.903574 hw_loss 0.311876 lr 0.00029622 rank 5
2023-02-27 15:54:27,589 DEBUG TRAIN Batch 46/5000 loss 5.269565 loss_att 8.742332 loss_ctc 10.873416 loss_rnnt 3.622934 hw_loss 0.384183 lr 0.00029622 rank 0
2023-02-27 15:54:27,590 DEBUG TRAIN Batch 46/5000 loss 13.092964 loss_att 15.603495 loss_ctc 18.521877 loss_rnnt 11.764596 hw_loss 0.192013 lr 0.00029622 rank 1
2023-02-27 15:54:27,594 DEBUG TRAIN Batch 46/5000 loss 9.877307 loss_att 9.325590 loss_ctc 15.764313 loss_rnnt 9.023760 hw_loss 0.335543 lr 0.00029621 rank 7
2023-02-27 15:54:27,597 DEBUG TRAIN Batch 46/5000 loss 16.060368 loss_att 18.892078 loss_ctc 26.735188 loss_rnnt 13.972990 hw_loss 0.183238 lr 0.00029621 rank 2
2023-02-27 15:54:27,600 DEBUG TRAIN Batch 46/5000 loss 6.810762 loss_att 7.673752 loss_ctc 9.095561 loss_rnnt 6.230097 hw_loss 0.193928 lr 0.00029622 rank 4
2023-02-27 15:55:07,341 DEBUG TRAIN Batch 46/5100 loss 5.152153 loss_att 8.038132 loss_ctc 6.503296 loss_rnnt 4.326797 hw_loss 0.127514 lr 0.00029621 rank 0
2023-02-27 15:55:07,353 DEBUG TRAIN Batch 46/5100 loss 7.281307 loss_att 8.985247 loss_ctc 12.765547 loss_rnnt 6.095551 hw_loss 0.213254 lr 0.00029620 rank 6
2023-02-27 15:55:07,353 DEBUG TRAIN Batch 46/5100 loss 3.622829 loss_att 8.189518 loss_ctc 8.498347 loss_rnnt 1.926937 hw_loss 0.248411 lr 0.00029620 rank 3
2023-02-27 15:55:07,355 DEBUG TRAIN Batch 46/5100 loss 3.444967 loss_att 5.012685 loss_ctc 6.519986 loss_rnnt 2.606859 hw_loss 0.214802 lr 0.00029621 rank 5
2023-02-27 15:55:07,356 DEBUG TRAIN Batch 46/5100 loss 4.989384 loss_att 6.731627 loss_ctc 7.683089 loss_rnnt 4.102758 hw_loss 0.335656 lr 0.00029620 rank 4
2023-02-27 15:55:07,359 DEBUG TRAIN Batch 46/5100 loss 6.982556 loss_att 14.414642 loss_ctc 19.376877 loss_rnnt 3.688178 hw_loss 0.291347 lr 0.00029620 rank 7
2023-02-27 15:55:07,362 DEBUG TRAIN Batch 46/5100 loss 6.194133 loss_att 7.908654 loss_ctc 8.446291 loss_rnnt 5.401865 hw_loss 0.279518 lr 0.00029620 rank 2
2023-02-27 15:55:07,408 DEBUG TRAIN Batch 46/5100 loss 5.707439 loss_att 6.472671 loss_ctc 8.312243 loss_rnnt 5.113481 hw_loss 0.175509 lr 0.00029621 rank 1
2023-02-27 15:55:46,589 DEBUG TRAIN Batch 46/5200 loss 3.287675 loss_att 6.941845 loss_ctc 6.960500 loss_rnnt 2.066542 hw_loss 0.001105 lr 0.00029619 rank 7
2023-02-27 15:55:46,602 DEBUG TRAIN Batch 46/5200 loss 4.630782 loss_att 8.291514 loss_ctc 6.552719 loss_rnnt 3.458475 hw_loss 0.344817 lr 0.00029619 rank 6
2023-02-27 15:55:46,604 DEBUG TRAIN Batch 46/5200 loss 7.424937 loss_att 10.253580 loss_ctc 9.374870 loss_rnnt 6.493753 hw_loss 0.197744 lr 0.00029619 rank 3
2023-02-27 15:55:46,606 DEBUG TRAIN Batch 46/5200 loss 4.156886 loss_att 5.901586 loss_ctc 8.488008 loss_rnnt 3.055664 hw_loss 0.327745 lr 0.00029619 rank 5
2023-02-27 15:55:46,610 DEBUG TRAIN Batch 46/5200 loss 2.180437 loss_att 5.776663 loss_ctc 7.364217 loss_rnnt 0.742382 hw_loss 0.051825 lr 0.00029619 rank 2
2023-02-27 15:55:46,618 DEBUG TRAIN Batch 46/5200 loss 8.648160 loss_att 11.292709 loss_ctc 9.338314 loss_rnnt 7.994043 hw_loss 0.062226 lr 0.00029619 rank 0
2023-02-27 15:55:46,623 DEBUG TRAIN Batch 46/5200 loss 5.917743 loss_att 6.199744 loss_ctc 8.794741 loss_rnnt 5.282672 hw_loss 0.365759 lr 0.00029619 rank 1
2023-02-27 15:55:46,657 DEBUG TRAIN Batch 46/5200 loss 1.624904 loss_att 4.143390 loss_ctc 2.298281 loss_rnnt 0.889438 hw_loss 0.266221 lr 0.00029619 rank 4
2023-02-27 15:56:26,460 DEBUG TRAIN Batch 46/5300 loss 10.590051 loss_att 9.914097 loss_ctc 11.802547 loss_rnnt 10.469457 hw_loss 0.176470 lr 0.00029617 rank 7
2023-02-27 15:56:26,469 DEBUG TRAIN Batch 46/5300 loss 14.541717 loss_att 18.007755 loss_ctc 20.570782 loss_rnnt 12.947752 hw_loss 0.181653 lr 0.00029618 rank 0
2023-02-27 15:56:26,470 DEBUG TRAIN Batch 46/5300 loss 8.676829 loss_att 10.792341 loss_ctc 17.256788 loss_rnnt 6.979419 hw_loss 0.244339 lr 0.00029617 rank 6
2023-02-27 15:56:26,472 DEBUG TRAIN Batch 46/5300 loss 4.088582 loss_att 5.357130 loss_ctc 4.912280 loss_rnnt 3.592433 hw_loss 0.248649 lr 0.00029618 rank 3
2023-02-27 15:56:26,476 DEBUG TRAIN Batch 46/5300 loss 2.096071 loss_att 4.831108 loss_ctc 2.911073 loss_rnnt 1.277066 hw_loss 0.306246 lr 0.00029618 rank 1
2023-02-27 15:56:26,476 DEBUG TRAIN Batch 46/5300 loss 10.081541 loss_att 11.934288 loss_ctc 17.476912 loss_rnnt 8.566003 hw_loss 0.298011 lr 0.00029618 rank 4
2023-02-27 15:56:26,492 DEBUG TRAIN Batch 46/5300 loss 1.962073 loss_att 4.277451 loss_ctc 1.304225 loss_rnnt 1.416943 hw_loss 0.318315 lr 0.00029618 rank 2
2023-02-27 15:56:26,498 DEBUG TRAIN Batch 46/5300 loss 4.511118 loss_att 8.516024 loss_ctc 9.634648 loss_rnnt 2.935346 hw_loss 0.171849 lr 0.00029618 rank 5
2023-02-27 15:57:31,586 DEBUG TRAIN Batch 46/5400 loss 7.646004 loss_att 12.238020 loss_ctc 16.866600 loss_rnnt 5.363989 hw_loss 0.251622 lr 0.00029616 rank 3
2023-02-27 15:57:31,589 DEBUG TRAIN Batch 46/5400 loss 8.396141 loss_att 11.402132 loss_ctc 13.271371 loss_rnnt 7.076087 hw_loss 0.129049 lr 0.00029617 rank 0
2023-02-27 15:57:31,594 DEBUG TRAIN Batch 46/5400 loss 8.152027 loss_att 10.976419 loss_ctc 13.330383 loss_rnnt 6.791092 hw_loss 0.198016 lr 0.00029616 rank 2
2023-02-27 15:57:31,594 DEBUG TRAIN Batch 46/5400 loss 4.534586 loss_att 7.190440 loss_ctc 11.630864 loss_rnnt 2.975837 hw_loss 0.152640 lr 0.00029616 rank 7
2023-02-27 15:57:31,594 DEBUG TRAIN Batch 46/5400 loss 17.758802 loss_att 20.108679 loss_ctc 26.342215 loss_rnnt 16.055593 hw_loss 0.166458 lr 0.00029616 rank 6
2023-02-27 15:57:31,596 DEBUG TRAIN Batch 46/5400 loss 3.329726 loss_att 7.868694 loss_ctc 7.149240 loss_rnnt 1.736010 hw_loss 0.331227 lr 0.00029617 rank 5
2023-02-27 15:57:31,598 DEBUG TRAIN Batch 46/5400 loss 5.590075 loss_att 8.133964 loss_ctc 10.039476 loss_rnnt 4.339099 hw_loss 0.279271 lr 0.00029617 rank 1
2023-02-27 15:57:31,599 DEBUG TRAIN Batch 46/5400 loss 4.779707 loss_att 6.377962 loss_ctc 10.105864 loss_rnnt 3.568383 hw_loss 0.340349 lr 0.00029617 rank 4
2023-02-27 15:58:10,561 DEBUG TRAIN Batch 46/5500 loss 4.049515 loss_att 6.525217 loss_ctc 5.596286 loss_rnnt 3.210165 hw_loss 0.258701 lr 0.00029615 rank 2
2023-02-27 15:58:10,575 DEBUG TRAIN Batch 46/5500 loss 7.249267 loss_att 10.249800 loss_ctc 13.438617 loss_rnnt 5.735170 hw_loss 0.166395 lr 0.00029615 rank 6
2023-02-27 15:58:10,576 DEBUG TRAIN Batch 46/5500 loss 9.020894 loss_att 12.255694 loss_ctc 17.637985 loss_rnnt 7.158402 hw_loss 0.124848 lr 0.00029615 rank 3
2023-02-27 15:58:10,576 DEBUG TRAIN Batch 46/5500 loss 4.844673 loss_att 7.991223 loss_ctc 9.469522 loss_rnnt 3.506077 hw_loss 0.173697 lr 0.00029615 rank 5
2023-02-27 15:58:10,577 DEBUG TRAIN Batch 46/5500 loss 6.691929 loss_att 10.597487 loss_ctc 10.972389 loss_rnnt 5.308202 hw_loss 0.059789 lr 0.00029615 rank 1
2023-02-27 15:58:10,580 DEBUG TRAIN Batch 46/5500 loss 4.414618 loss_att 6.820476 loss_ctc 6.165355 loss_rnnt 3.610895 hw_loss 0.167101 lr 0.00029615 rank 4
2023-02-27 15:58:10,582 DEBUG TRAIN Batch 46/5500 loss 15.159817 loss_att 16.217424 loss_ctc 21.585072 loss_rnnt 13.945374 hw_loss 0.274161 lr 0.00029615 rank 0
2023-02-27 15:58:10,625 DEBUG TRAIN Batch 46/5500 loss 4.081625 loss_att 5.742374 loss_ctc 5.831709 loss_rnnt 3.314883 hw_loss 0.377337 lr 0.00029615 rank 7
2023-02-27 15:58:49,837 DEBUG TRAIN Batch 46/5600 loss 2.735031 loss_att 5.176812 loss_ctc 4.697019 loss_rnnt 1.844644 hw_loss 0.263311 lr 0.00029614 rank 0
2023-02-27 15:58:49,841 DEBUG TRAIN Batch 46/5600 loss 7.922280 loss_att 10.333019 loss_ctc 12.907381 loss_rnnt 6.599071 hw_loss 0.330715 lr 0.00029614 rank 3
2023-02-27 15:58:49,842 DEBUG TRAIN Batch 46/5600 loss 6.508668 loss_att 11.399490 loss_ctc 11.388741 loss_rnnt 4.722346 hw_loss 0.295278 lr 0.00029613 rank 6
2023-02-27 15:58:49,843 DEBUG TRAIN Batch 46/5600 loss 4.070347 loss_att 6.778980 loss_ctc 6.141704 loss_rnnt 3.144670 hw_loss 0.202068 lr 0.00029614 rank 2
2023-02-27 15:58:49,845 DEBUG TRAIN Batch 46/5600 loss 7.242018 loss_att 10.163118 loss_ctc 10.706673 loss_rnnt 6.014560 hw_loss 0.339907 lr 0.00029614 rank 1
2023-02-27 15:58:49,846 DEBUG TRAIN Batch 46/5600 loss 3.049300 loss_att 5.317169 loss_ctc 4.959067 loss_rnnt 2.173428 hw_loss 0.314365 lr 0.00029614 rank 4
2023-02-27 15:58:49,851 DEBUG TRAIN Batch 46/5600 loss 10.304380 loss_att 10.717542 loss_ctc 15.521338 loss_rnnt 9.367759 hw_loss 0.296991 lr 0.00029614 rank 7
2023-02-27 15:58:49,889 DEBUG TRAIN Batch 46/5600 loss 4.548906 loss_att 8.264906 loss_ctc 10.243690 loss_rnnt 2.881413 hw_loss 0.309353 lr 0.00029614 rank 5
2023-02-27 15:59:56,809 DEBUG TRAIN Batch 46/5700 loss 5.954775 loss_att 8.215010 loss_ctc 8.928205 loss_rnnt 4.964330 hw_loss 0.266139 lr 0.00029612 rank 6
2023-02-27 15:59:56,814 DEBUG TRAIN Batch 46/5700 loss 3.300647 loss_att 5.860764 loss_ctc 10.880309 loss_rnnt 1.688418 hw_loss 0.167969 lr 0.00029613 rank 5
2023-02-27 15:59:56,816 DEBUG TRAIN Batch 46/5700 loss 6.469073 loss_att 10.363350 loss_ctc 13.731422 loss_rnnt 4.556432 hw_loss 0.310260 lr 0.00029613 rank 0
2023-02-27 15:59:56,817 DEBUG TRAIN Batch 46/5700 loss 4.435514 loss_att 7.137877 loss_ctc 11.477558 loss_rnnt 2.828416 hw_loss 0.239411 lr 0.00029613 rank 1
2023-02-27 15:59:56,821 DEBUG TRAIN Batch 46/5700 loss 6.177658 loss_att 9.606308 loss_ctc 9.786215 loss_rnnt 4.885045 hw_loss 0.235767 lr 0.00029612 rank 2
2023-02-27 15:59:56,825 DEBUG TRAIN Batch 46/5700 loss 3.263111 loss_att 7.866050 loss_ctc 5.939022 loss_rnnt 1.858192 hw_loss 0.239142 lr 0.00029613 rank 3
2023-02-27 15:59:56,824 DEBUG TRAIN Batch 46/5700 loss 3.260093 loss_att 5.773741 loss_ctc 6.088778 loss_rnnt 2.184540 hw_loss 0.366873 lr 0.00029613 rank 4
2023-02-27 15:59:56,876 DEBUG TRAIN Batch 46/5700 loss 3.755072 loss_att 6.901033 loss_ctc 6.477850 loss_rnnt 2.605209 hw_loss 0.295565 lr 0.00029612 rank 7
2023-02-27 16:00:36,068 DEBUG TRAIN Batch 46/5800 loss 3.173965 loss_att 5.269619 loss_ctc 6.774299 loss_rnnt 2.274055 hw_loss 0.001376 lr 0.00029611 rank 4
2023-02-27 16:00:36,068 DEBUG TRAIN Batch 46/5800 loss 3.780039 loss_att 6.932462 loss_ctc 7.439750 loss_rnnt 2.539254 hw_loss 0.229386 lr 0.00029611 rank 3
2023-02-27 16:00:36,068 DEBUG TRAIN Batch 46/5800 loss 4.106936 loss_att 6.132837 loss_ctc 6.849843 loss_rnnt 3.157723 hw_loss 0.334334 lr 0.00029611 rank 5
2023-02-27 16:00:36,078 DEBUG TRAIN Batch 46/5800 loss 2.851685 loss_att 5.264270 loss_ctc 6.013488 loss_rnnt 1.855731 hw_loss 0.172245 lr 0.00029611 rank 2
2023-02-27 16:00:36,078 DEBUG TRAIN Batch 46/5800 loss 15.379562 loss_att 19.326588 loss_ctc 21.312614 loss_rnnt 13.730811 hw_loss 0.128012 lr 0.00029611 rank 6
2023-02-27 16:00:36,083 DEBUG TRAIN Batch 46/5800 loss 7.110663 loss_att 9.743192 loss_ctc 13.741081 loss_rnnt 5.586021 hw_loss 0.213901 lr 0.00029612 rank 0
2023-02-27 16:00:36,092 DEBUG TRAIN Batch 46/5800 loss 6.475008 loss_att 8.745864 loss_ctc 10.416954 loss_rnnt 5.351301 hw_loss 0.269893 lr 0.00029611 rank 7
2023-02-27 16:00:36,129 DEBUG TRAIN Batch 46/5800 loss 7.217430 loss_att 8.381341 loss_ctc 11.715672 loss_rnnt 6.190622 hw_loss 0.364236 lr 0.00029611 rank 1
2023-02-27 16:01:14,892 DEBUG TRAIN Batch 46/5900 loss 5.308549 loss_att 8.854766 loss_ctc 9.917881 loss_rnnt 3.983904 hw_loss 0.001544 lr 0.00029610 rank 4
2023-02-27 16:01:14,907 DEBUG TRAIN Batch 46/5900 loss 6.353572 loss_att 10.143790 loss_ctc 9.115559 loss_rnnt 5.143131 hw_loss 0.157749 lr 0.00029610 rank 3
2023-02-27 16:01:14,907 DEBUG TRAIN Batch 46/5900 loss 4.347824 loss_att 8.360442 loss_ctc 6.492921 loss_rnnt 3.142837 hw_loss 0.218344 lr 0.00029610 rank 6
2023-02-27 16:01:14,909 DEBUG TRAIN Batch 46/5900 loss 4.286313 loss_att 9.728249 loss_ctc 6.924491 loss_rnnt 2.705717 hw_loss 0.263348 lr 0.00029610 rank 2
2023-02-27 16:01:14,909 DEBUG TRAIN Batch 46/5900 loss 10.326328 loss_att 12.219482 loss_ctc 14.812198 loss_rnnt 9.208220 hw_loss 0.265054 lr 0.00029610 rank 1
2023-02-27 16:01:14,910 DEBUG TRAIN Batch 46/5900 loss 5.407565 loss_att 9.911314 loss_ctc 10.358679 loss_rnnt 3.676503 hw_loss 0.319056 lr 0.00029610 rank 0
2023-02-27 16:01:14,913 DEBUG TRAIN Batch 46/5900 loss 5.228503 loss_att 8.885677 loss_ctc 8.173268 loss_rnnt 3.956621 hw_loss 0.277147 lr 0.00029610 rank 7
2023-02-27 16:01:14,960 DEBUG TRAIN Batch 46/5900 loss 1.823712 loss_att 4.994396 loss_ctc 3.462393 loss_rnnt 0.882210 hw_loss 0.166638 lr 0.00029610 rank 5
2023-02-27 16:01:54,515 DEBUG TRAIN Batch 46/6000 loss 8.033078 loss_att 11.001676 loss_ctc 9.232632 loss_rnnt 7.113052 hw_loss 0.311936 lr 0.00029609 rank 3
2023-02-27 16:01:54,525 DEBUG TRAIN Batch 46/6000 loss 11.285796 loss_att 16.067101 loss_ctc 19.910170 loss_rnnt 9.098274 hw_loss 0.152519 lr 0.00029608 rank 7
2023-02-27 16:01:54,527 DEBUG TRAIN Batch 46/6000 loss 0.811725 loss_att 3.038027 loss_ctc 1.404182 loss_rnnt 0.145512 hw_loss 0.266171 lr 0.00029609 rank 2
2023-02-27 16:01:54,527 DEBUG TRAIN Batch 46/6000 loss 7.740391 loss_att 10.929401 loss_ctc 15.269702 loss_rnnt 5.909534 hw_loss 0.354651 lr 0.00029609 rank 5
2023-02-27 16:01:54,530 DEBUG TRAIN Batch 46/6000 loss 3.980319 loss_att 5.670632 loss_ctc 4.362165 loss_rnnt 3.484328 hw_loss 0.200654 lr 0.00029609 rank 1
2023-02-27 16:01:54,531 DEBUG TRAIN Batch 46/6000 loss 5.956793 loss_att 8.996383 loss_ctc 9.252790 loss_rnnt 4.790068 hw_loss 0.223763 lr 0.00029609 rank 0
2023-02-27 16:01:54,535 DEBUG TRAIN Batch 46/6000 loss 7.739225 loss_att 9.582384 loss_ctc 11.543093 loss_rnnt 6.769218 hw_loss 0.176613 lr 0.00029608 rank 6
2023-02-27 16:01:54,546 DEBUG TRAIN Batch 46/6000 loss 5.543634 loss_att 9.986978 loss_ctc 12.799721 loss_rnnt 3.581574 hw_loss 0.198587 lr 0.00029609 rank 4
2023-02-27 16:03:00,321 DEBUG TRAIN Batch 46/6100 loss 3.129960 loss_att 4.653957 loss_ctc 6.344982 loss_rnnt 2.257622 hw_loss 0.260379 lr 0.00029607 rank 6
2023-02-27 16:03:00,322 DEBUG TRAIN Batch 46/6100 loss 6.605108 loss_att 8.513764 loss_ctc 8.901284 loss_rnnt 5.819106 hw_loss 0.183964 lr 0.00029607 rank 3
2023-02-27 16:03:00,329 DEBUG TRAIN Batch 46/6100 loss 11.498960 loss_att 12.590516 loss_ctc 17.515291 loss_rnnt 10.352236 hw_loss 0.236690 lr 0.00029607 rank 7
2023-02-27 16:03:00,331 DEBUG TRAIN Batch 46/6100 loss 4.658854 loss_att 7.336389 loss_ctc 6.047735 loss_rnnt 3.807467 hw_loss 0.245054 lr 0.00029608 rank 5
2023-02-27 16:03:00,332 DEBUG TRAIN Batch 46/6100 loss 5.959606 loss_att 10.410862 loss_ctc 5.515201 loss_rnnt 5.054572 hw_loss 0.138819 lr 0.00029607 rank 4
2023-02-27 16:03:00,333 DEBUG TRAIN Batch 46/6100 loss 8.366885 loss_att 11.965497 loss_ctc 14.152776 loss_rnnt 6.737489 hw_loss 0.259166 lr 0.00029607 rank 2
2023-02-27 16:03:00,338 DEBUG TRAIN Batch 46/6100 loss 7.818792 loss_att 10.953245 loss_ctc 16.034447 loss_rnnt 5.901690 hw_loss 0.365234 lr 0.00029608 rank 0
2023-02-27 16:03:00,381 DEBUG TRAIN Batch 46/6100 loss 5.790177 loss_att 7.977155 loss_ctc 9.254755 loss_rnnt 4.765066 hw_loss 0.235822 lr 0.00029608 rank 1
2023-02-27 16:03:39,534 DEBUG TRAIN Batch 46/6200 loss 10.797384 loss_att 12.428552 loss_ctc 17.504822 loss_rnnt 9.467782 hw_loss 0.204456 lr 0.00029606 rank 7
2023-02-27 16:03:39,543 DEBUG TRAIN Batch 46/6200 loss 1.469750 loss_att 2.947102 loss_ctc 3.234434 loss_rnnt 0.832487 hw_loss 0.199690 lr 0.00029606 rank 0
2023-02-27 16:03:39,545 DEBUG TRAIN Batch 46/6200 loss 5.593114 loss_att 10.500018 loss_ctc 13.140162 loss_rnnt 3.560473 hw_loss 0.084351 lr 0.00029606 rank 6
2023-02-27 16:03:39,548 DEBUG TRAIN Batch 46/6200 loss 3.610543 loss_att 6.319930 loss_ctc 6.423762 loss_rnnt 2.545853 hw_loss 0.276969 lr 0.00029606 rank 3
2023-02-27 16:03:39,550 DEBUG TRAIN Batch 46/6200 loss 10.568000 loss_att 15.679500 loss_ctc 21.236189 loss_rnnt 7.967186 hw_loss 0.292666 lr 0.00029606 rank 4
2023-02-27 16:03:39,551 DEBUG TRAIN Batch 46/6200 loss 4.877882 loss_att 7.969498 loss_ctc 10.355588 loss_rnnt 3.365727 hw_loss 0.306509 lr 0.00029606 rank 5
2023-02-27 16:03:39,560 DEBUG TRAIN Batch 46/6200 loss 2.855357 loss_att 5.023646 loss_ctc 5.146252 loss_rnnt 2.046665 hw_loss 0.130466 lr 0.00029606 rank 2
2023-02-27 16:03:39,599 DEBUG TRAIN Batch 46/6200 loss 10.602365 loss_att 15.238949 loss_ctc 16.556684 loss_rnnt 8.775455 hw_loss 0.198158 lr 0.00029606 rank 1
2023-02-27 16:04:19,035 DEBUG TRAIN Batch 46/6300 loss 1.693413 loss_att 3.799501 loss_ctc 4.008601 loss_rnnt 0.756273 hw_loss 0.388557 lr 0.00029604 rank 6
2023-02-27 16:04:19,038 DEBUG TRAIN Batch 46/6300 loss 2.608227 loss_att 5.291488 loss_ctc 5.785086 loss_rnnt 1.534075 hw_loss 0.213597 lr 0.00029605 rank 4
2023-02-27 16:04:19,042 DEBUG TRAIN Batch 46/6300 loss 4.923572 loss_att 7.728184 loss_ctc 6.771620 loss_rnnt 4.080753 hw_loss 0.066543 lr 0.00029605 rank 2
2023-02-27 16:04:19,043 DEBUG TRAIN Batch 46/6300 loss 6.178891 loss_att 7.945632 loss_ctc 9.532108 loss_rnnt 5.207572 hw_loss 0.320389 lr 0.00029605 rank 3
2023-02-27 16:04:19,047 DEBUG TRAIN Batch 46/6300 loss 6.944952 loss_att 7.409846 loss_ctc 9.381202 loss_rnnt 6.340647 hw_loss 0.349674 lr 0.00029605 rank 0
2023-02-27 16:04:19,050 DEBUG TRAIN Batch 46/6300 loss 8.553828 loss_att 13.332293 loss_ctc 14.698944 loss_rnnt 6.646063 hw_loss 0.248858 lr 0.00029605 rank 5
2023-02-27 16:04:19,052 DEBUG TRAIN Batch 46/6300 loss 5.728343 loss_att 10.587665 loss_ctc 11.847297 loss_rnnt 3.817718 hw_loss 0.230435 lr 0.00029605 rank 7
2023-02-27 16:04:19,052 DEBUG TRAIN Batch 46/6300 loss 5.902049 loss_att 7.544767 loss_ctc 10.756384 loss_rnnt 4.848301 hw_loss 0.146175 lr 0.00029605 rank 1
2023-02-27 16:05:26,166 DEBUG TRAIN Batch 46/6400 loss 3.000522 loss_att 8.135286 loss_ctc 5.481178 loss_rnnt 1.603546 hw_loss 0.073629 lr 0.00029604 rank 0
2023-02-27 16:05:26,181 DEBUG TRAIN Batch 46/6400 loss 3.846926 loss_att 6.946490 loss_ctc 7.947536 loss_rnnt 2.591778 hw_loss 0.165913 lr 0.00029603 rank 2
2023-02-27 16:05:26,182 DEBUG TRAIN Batch 46/6400 loss 4.022151 loss_att 6.393080 loss_ctc 8.172503 loss_rnnt 2.814631 hw_loss 0.337415 lr 0.00029603 rank 3
2023-02-27 16:05:26,185 DEBUG TRAIN Batch 46/6400 loss 3.202017 loss_att 4.259236 loss_ctc 4.826608 loss_rnnt 2.634961 hw_loss 0.260625 lr 0.00029603 rank 7
2023-02-27 16:05:26,188 DEBUG TRAIN Batch 46/6400 loss 6.662747 loss_att 7.969810 loss_ctc 11.013034 loss_rnnt 5.635955 hw_loss 0.347517 lr 0.00029603 rank 6
2023-02-27 16:05:26,189 DEBUG TRAIN Batch 46/6400 loss 5.868938 loss_att 7.695616 loss_ctc 10.701144 loss_rnnt 4.771101 hw_loss 0.165388 lr 0.00029604 rank 1
2023-02-27 16:05:26,191 DEBUG TRAIN Batch 46/6400 loss 8.506962 loss_att 10.572831 loss_ctc 13.458426 loss_rnnt 7.312540 hw_loss 0.226971 lr 0.00029604 rank 5
2023-02-27 16:05:26,194 DEBUG TRAIN Batch 46/6400 loss 8.495163 loss_att 10.639696 loss_ctc 15.933662 loss_rnnt 6.893071 hw_loss 0.340098 lr 0.00029604 rank 4
2023-02-27 16:06:05,456 DEBUG TRAIN Batch 46/6500 loss 5.440485 loss_att 9.732574 loss_ctc 10.968775 loss_rnnt 3.687904 hw_loss 0.294481 lr 0.00029602 rank 2
2023-02-27 16:06:05,464 DEBUG TRAIN Batch 46/6500 loss 2.370221 loss_att 5.824760 loss_ctc 5.503749 loss_rnnt 1.215912 hw_loss 0.085495 lr 0.00029602 rank 0
2023-02-27 16:06:05,469 DEBUG TRAIN Batch 46/6500 loss 3.796849 loss_att 6.209893 loss_ctc 5.606321 loss_rnnt 2.944676 hw_loss 0.240563 lr 0.00029602 rank 3
2023-02-27 16:06:05,470 DEBUG TRAIN Batch 46/6500 loss 6.745899 loss_att 10.162712 loss_ctc 10.528224 loss_rnnt 5.403240 hw_loss 0.290599 lr 0.00029602 rank 6
2023-02-27 16:06:05,471 DEBUG TRAIN Batch 46/6500 loss 5.653487 loss_att 8.603817 loss_ctc 15.337225 loss_rnnt 3.673372 hw_loss 0.185408 lr 0.00029602 rank 4
2023-02-27 16:06:05,474 DEBUG TRAIN Batch 46/6500 loss 6.433898 loss_att 8.119780 loss_ctc 9.657129 loss_rnnt 5.521278 hw_loss 0.273149 lr 0.00029602 rank 5
2023-02-27 16:06:05,475 DEBUG TRAIN Batch 46/6500 loss 1.247952 loss_att 3.034666 loss_ctc 2.447171 loss_rnnt 0.602595 hw_loss 0.240222 lr 0.00029602 rank 7
2023-02-27 16:06:05,481 DEBUG TRAIN Batch 46/6500 loss 2.947633 loss_att 7.435569 loss_ctc 8.842998 loss_rnnt 1.162700 hw_loss 0.189931 lr 0.00029602 rank 1
2023-02-27 16:06:44,727 DEBUG TRAIN Batch 46/6600 loss 7.201572 loss_att 12.412855 loss_ctc 14.439081 loss_rnnt 5.053307 hw_loss 0.264387 lr 0.00029601 rank 0
2023-02-27 16:06:44,744 DEBUG TRAIN Batch 46/6600 loss 7.310271 loss_att 10.764868 loss_ctc 14.781441 loss_rnnt 5.559150 hw_loss 0.120085 lr 0.00029601 rank 3
2023-02-27 16:06:44,745 DEBUG TRAIN Batch 46/6600 loss 5.936297 loss_att 11.377710 loss_ctc 13.779823 loss_rnnt 3.654784 hw_loss 0.276424 lr 0.00029601 rank 5
2023-02-27 16:06:44,745 DEBUG TRAIN Batch 46/6600 loss 6.512740 loss_att 11.324417 loss_ctc 16.633316 loss_rnnt 3.991054 hw_loss 0.393638 lr 0.00029601 rank 4
2023-02-27 16:06:44,746 DEBUG TRAIN Batch 46/6600 loss 4.263455 loss_att 7.693687 loss_ctc 8.971711 loss_rnnt 2.826757 hw_loss 0.230408 lr 0.00029600 rank 6
2023-02-27 16:06:44,747 DEBUG TRAIN Batch 46/6600 loss 6.903406 loss_att 12.376740 loss_ctc 11.681981 loss_rnnt 5.042331 hw_loss 0.242372 lr 0.00029601 rank 7
2023-02-27 16:06:44,748 DEBUG TRAIN Batch 46/6600 loss 7.821397 loss_att 9.783885 loss_ctc 12.039864 loss_rnnt 6.801796 hw_loss 0.121204 lr 0.00029601 rank 2
2023-02-27 16:06:44,753 DEBUG TRAIN Batch 46/6600 loss 3.121463 loss_att 8.051549 loss_ctc 9.366606 loss_rnnt 1.230449 hw_loss 0.135582 lr 0.00029601 rank 1
2023-02-27 16:07:24,716 DEBUG TRAIN Batch 46/6700 loss 6.944834 loss_att 10.888068 loss_ctc 9.645372 loss_rnnt 5.679167 hw_loss 0.219277 lr 0.00029599 rank 6
2023-02-27 16:07:24,728 DEBUG TRAIN Batch 46/6700 loss 10.888639 loss_att 13.109343 loss_ctc 16.492970 loss_rnnt 9.580530 hw_loss 0.218860 lr 0.00029599 rank 7
2023-02-27 16:07:24,735 DEBUG TRAIN Batch 46/6700 loss 4.986568 loss_att 6.983334 loss_ctc 7.254941 loss_rnnt 4.080348 hw_loss 0.383282 lr 0.00029600 rank 3
2023-02-27 16:07:24,739 DEBUG TRAIN Batch 46/6700 loss 6.367951 loss_att 7.938171 loss_ctc 8.549446 loss_rnnt 5.696390 hw_loss 0.124972 lr 0.00029599 rank 2
2023-02-27 16:07:24,742 DEBUG TRAIN Batch 46/6700 loss 3.832957 loss_att 6.168682 loss_ctc 7.462438 loss_rnnt 2.797228 hw_loss 0.158726 lr 0.00029600 rank 0
2023-02-27 16:07:24,741 DEBUG TRAIN Batch 46/6700 loss 9.116690 loss_att 11.991737 loss_ctc 13.259886 loss_rnnt 7.874750 hw_loss 0.214697 lr 0.00029600 rank 5
2023-02-27 16:07:24,752 DEBUG TRAIN Batch 46/6700 loss 6.510805 loss_att 10.494170 loss_ctc 7.597421 loss_rnnt 5.467249 hw_loss 0.191250 lr 0.00029600 rank 4
2023-02-27 16:07:24,752 DEBUG TRAIN Batch 46/6700 loss 5.139771 loss_att 10.278454 loss_ctc 8.250236 loss_rnnt 3.535873 hw_loss 0.302687 lr 0.00029600 rank 1
2023-02-27 16:08:30,392 DEBUG TRAIN Batch 46/6800 loss 6.529344 loss_att 10.522287 loss_ctc 10.116940 loss_rnnt 5.136920 hw_loss 0.216542 lr 0.00029599 rank 0
2023-02-27 16:08:30,393 DEBUG TRAIN Batch 46/6800 loss 9.864762 loss_att 13.361477 loss_ctc 17.191406 loss_rnnt 8.004260 hw_loss 0.345513 lr 0.00029598 rank 3
2023-02-27 16:08:30,397 DEBUG TRAIN Batch 46/6800 loss 8.891671 loss_att 12.613373 loss_ctc 15.236784 loss_rnnt 7.160004 hw_loss 0.264959 lr 0.00029598 rank 7
2023-02-27 16:08:30,397 DEBUG TRAIN Batch 46/6800 loss 6.590585 loss_att 8.856357 loss_ctc 12.018620 loss_rnnt 5.252432 hw_loss 0.302365 lr 0.00029598 rank 6
2023-02-27 16:08:30,399 DEBUG TRAIN Batch 46/6800 loss 7.296965 loss_att 11.289608 loss_ctc 11.882901 loss_rnnt 5.764969 hw_loss 0.228766 lr 0.00029598 rank 2
2023-02-27 16:08:30,399 DEBUG TRAIN Batch 46/6800 loss 9.302984 loss_att 12.813972 loss_ctc 17.177479 loss_rnnt 7.364139 hw_loss 0.350094 lr 0.00029598 rank 4
2023-02-27 16:08:30,403 DEBUG TRAIN Batch 46/6800 loss 10.118076 loss_att 13.328196 loss_ctc 18.262550 loss_rnnt 8.334971 hw_loss 0.103410 lr 0.00029598 rank 1
2023-02-27 16:08:30,406 DEBUG TRAIN Batch 46/6800 loss 15.430832 loss_att 18.026360 loss_ctc 20.370579 loss_rnnt 14.116782 hw_loss 0.255585 lr 0.00029598 rank 5
2023-02-27 16:09:09,890 DEBUG TRAIN Batch 46/6900 loss 5.133196 loss_att 8.372140 loss_ctc 9.241195 loss_rnnt 3.826041 hw_loss 0.209313 lr 0.00029597 rank 4
2023-02-27 16:09:09,893 DEBUG TRAIN Batch 46/6900 loss 11.966395 loss_att 13.292645 loss_ctc 19.117243 loss_rnnt 10.624211 hw_loss 0.231540 lr 0.00029597 rank 3
2023-02-27 16:09:09,895 DEBUG TRAIN Batch 46/6900 loss 3.707743 loss_att 6.531831 loss_ctc 7.065094 loss_rnnt 2.566288 hw_loss 0.241859 lr 0.00029597 rank 6
2023-02-27 16:09:09,897 DEBUG TRAIN Batch 46/6900 loss 12.638056 loss_att 13.866661 loss_ctc 21.985033 loss_rnnt 10.989970 hw_loss 0.292690 lr 0.00029597 rank 0
2023-02-27 16:09:09,902 DEBUG TRAIN Batch 46/6900 loss 5.474276 loss_att 7.528766 loss_ctc 8.163227 loss_rnnt 4.566229 hw_loss 0.259915 lr 0.00029597 rank 2
2023-02-27 16:09:09,904 DEBUG TRAIN Batch 46/6900 loss 6.135456 loss_att 6.808855 loss_ctc 9.166462 loss_rnnt 5.457077 hw_loss 0.261684 lr 0.00029597 rank 7
2023-02-27 16:09:09,906 DEBUG TRAIN Batch 46/6900 loss 7.920449 loss_att 9.154958 loss_ctc 10.014042 loss_rnnt 7.246369 hw_loss 0.277559 lr 0.00029597 rank 1
2023-02-27 16:09:09,943 DEBUG TRAIN Batch 46/6900 loss 10.557707 loss_att 16.790543 loss_ctc 21.022219 loss_rnnt 7.791941 hw_loss 0.232368 lr 0.00029597 rank 5
2023-02-27 16:09:48,990 DEBUG TRAIN Batch 46/7000 loss 4.246548 loss_att 6.851789 loss_ctc 6.327462 loss_rnnt 3.366724 hw_loss 0.152476 lr 0.00029595 rank 7
2023-02-27 16:09:49,011 DEBUG TRAIN Batch 46/7000 loss 5.442594 loss_att 6.618158 loss_ctc 4.978169 loss_rnnt 5.149608 hw_loss 0.224619 lr 0.00029596 rank 0
2023-02-27 16:09:49,013 DEBUG TRAIN Batch 46/7000 loss 6.650762 loss_att 8.021215 loss_ctc 14.095690 loss_rnnt 5.156930 hw_loss 0.425784 lr 0.00029595 rank 6
2023-02-27 16:09:49,015 DEBUG TRAIN Batch 46/7000 loss 6.837553 loss_att 9.908953 loss_ctc 10.039618 loss_rnnt 5.712410 hw_loss 0.157351 lr 0.00029596 rank 5
2023-02-27 16:09:49,016 DEBUG TRAIN Batch 46/7000 loss 5.866753 loss_att 10.946823 loss_ctc 9.640305 loss_rnnt 4.294209 hw_loss 0.100105 lr 0.00029596 rank 3
2023-02-27 16:09:49,016 DEBUG TRAIN Batch 46/7000 loss 6.333448 loss_att 8.864399 loss_ctc 11.547010 loss_rnnt 5.037292 hw_loss 0.177798 lr 0.00029596 rank 1
2023-02-27 16:09:49,018 DEBUG TRAIN Batch 46/7000 loss 6.561694 loss_att 8.999116 loss_ctc 12.908157 loss_rnnt 5.091487 hw_loss 0.255989 lr 0.00029596 rank 2
2023-02-27 16:09:49,020 DEBUG TRAIN Batch 46/7000 loss 8.236571 loss_att 9.256858 loss_ctc 16.445805 loss_rnnt 6.771608 hw_loss 0.311889 lr 0.00029596 rank 4
2023-02-27 16:10:54,194 DEBUG TRAIN Batch 46/7100 loss 11.002559 loss_att 14.933111 loss_ctc 21.250252 loss_rnnt 8.720826 hw_loss 0.242363 lr 0.00029595 rank 0
2023-02-27 16:10:54,198 DEBUG TRAIN Batch 46/7100 loss 8.742631 loss_att 11.663295 loss_ctc 10.948103 loss_rnnt 7.778490 hw_loss 0.161148 lr 0.00029594 rank 3
2023-02-27 16:10:54,198 DEBUG TRAIN Batch 46/7100 loss 5.564630 loss_att 10.841504 loss_ctc 9.715094 loss_rnnt 3.860202 hw_loss 0.179358 lr 0.00029594 rank 7
2023-02-27 16:10:54,200 DEBUG TRAIN Batch 46/7100 loss 1.954636 loss_att 6.122264 loss_ctc 6.010616 loss_rnnt 0.511662 hw_loss 0.128721 lr 0.00029595 rank 1
2023-02-27 16:10:54,204 DEBUG TRAIN Batch 46/7100 loss 8.498678 loss_att 15.697111 loss_ctc 18.496490 loss_rnnt 5.563016 hw_loss 0.305500 lr 0.00029594 rank 6
2023-02-27 16:10:54,205 DEBUG TRAIN Batch 46/7100 loss 4.241147 loss_att 6.480813 loss_ctc 6.954120 loss_rnnt 3.322742 hw_loss 0.203890 lr 0.00029595 rank 4
2023-02-27 16:10:54,208 DEBUG TRAIN Batch 46/7100 loss 11.743534 loss_att 13.570860 loss_ctc 20.229475 loss_rnnt 10.072294 hw_loss 0.326840 lr 0.00029595 rank 5
2023-02-27 16:10:54,209 DEBUG TRAIN Batch 46/7100 loss 10.487116 loss_att 13.440605 loss_ctc 16.056887 loss_rnnt 9.056101 hw_loss 0.183153 lr 0.00029594 rank 2
2023-02-27 16:11:33,931 DEBUG TRAIN Batch 46/7200 loss 4.554901 loss_att 7.648155 loss_ctc 9.793617 loss_rnnt 3.212250 hw_loss 0.047819 lr 0.00029593 rank 5
2023-02-27 16:11:33,939 DEBUG TRAIN Batch 46/7200 loss 7.368436 loss_att 8.627039 loss_ctc 8.409780 loss_rnnt 6.923946 hw_loss 0.101107 lr 0.00029593 rank 4
2023-02-27 16:11:33,941 DEBUG TRAIN Batch 46/7200 loss 7.092575 loss_att 12.031208 loss_ctc 13.276587 loss_rnnt 5.233242 hw_loss 0.088258 lr 0.00029593 rank 2
2023-02-27 16:11:33,943 DEBUG TRAIN Batch 46/7200 loss 3.634550 loss_att 4.635294 loss_ctc 5.323943 loss_rnnt 3.058383 hw_loss 0.282686 lr 0.00029593 rank 7
2023-02-27 16:11:33,949 DEBUG TRAIN Batch 46/7200 loss 10.561106 loss_att 12.145235 loss_ctc 15.108763 loss_rnnt 9.588820 hw_loss 0.092072 lr 0.00029593 rank 6
2023-02-27 16:11:33,949 DEBUG TRAIN Batch 46/7200 loss 10.720567 loss_att 15.537040 loss_ctc 23.606369 loss_rnnt 7.844568 hw_loss 0.364872 lr 0.00029593 rank 3
2023-02-27 16:11:33,953 DEBUG TRAIN Batch 46/7200 loss 5.139744 loss_att 6.538260 loss_ctc 5.094120 loss_rnnt 4.741066 hw_loss 0.234486 lr 0.00029593 rank 1
2023-02-27 16:11:33,953 DEBUG TRAIN Batch 46/7200 loss 8.099048 loss_att 10.266035 loss_ctc 12.290598 loss_rnnt 7.023783 hw_loss 0.155613 lr 0.00029593 rank 0
2023-02-27 16:12:13,375 DEBUG TRAIN Batch 46/7300 loss 12.055646 loss_att 12.796371 loss_ctc 21.764448 loss_rnnt 10.485054 hw_loss 0.239886 lr 0.00029592 rank 7
2023-02-27 16:12:13,381 DEBUG TRAIN Batch 46/7300 loss 8.345928 loss_att 10.709416 loss_ctc 10.692939 loss_rnnt 7.444856 hw_loss 0.216449 lr 0.00029592 rank 1
2023-02-27 16:12:13,388 DEBUG TRAIN Batch 46/7300 loss 3.356758 loss_att 6.123379 loss_ctc 4.644610 loss_rnnt 2.573400 hw_loss 0.109350 lr 0.00029592 rank 3
2023-02-27 16:12:13,388 DEBUG TRAIN Batch 46/7300 loss 2.978147 loss_att 6.548971 loss_ctc 4.815596 loss_rnnt 1.871691 hw_loss 0.276184 lr 0.00029592 rank 5
2023-02-27 16:12:13,389 DEBUG TRAIN Batch 46/7300 loss 11.797879 loss_att 14.138060 loss_ctc 18.585751 loss_rnnt 10.318542 hw_loss 0.199221 lr 0.00029592 rank 0
2023-02-27 16:12:13,389 DEBUG TRAIN Batch 46/7300 loss 3.977797 loss_att 7.463658 loss_ctc 8.653903 loss_rnnt 2.502828 hw_loss 0.289341 lr 0.00029591 rank 6
2023-02-27 16:12:13,400 DEBUG TRAIN Batch 46/7300 loss 7.481145 loss_att 10.175817 loss_ctc 15.603945 loss_rnnt 5.670095 hw_loss 0.354518 lr 0.00029592 rank 2
2023-02-27 16:12:13,451 DEBUG TRAIN Batch 46/7300 loss 4.123596 loss_att 5.660149 loss_ctc 6.632464 loss_rnnt 3.367463 hw_loss 0.214325 lr 0.00029592 rank 4
2023-02-27 16:12:53,263 DEBUG TRAIN Batch 46/7400 loss 3.651524 loss_att 6.499889 loss_ctc 6.412691 loss_rnnt 2.632654 hw_loss 0.151951 lr 0.00029590 rank 2
2023-02-27 16:12:53,273 DEBUG TRAIN Batch 46/7400 loss 9.498182 loss_att 13.039538 loss_ctc 13.007677 loss_rnnt 8.169563 hw_loss 0.285778 lr 0.00029590 rank 7
2023-02-27 16:12:53,274 DEBUG TRAIN Batch 46/7400 loss 12.656468 loss_att 12.697698 loss_ctc 23.953335 loss_rnnt 11.053959 hw_loss 0.165028 lr 0.00029591 rank 0
2023-02-27 16:12:53,274 DEBUG TRAIN Batch 46/7400 loss 11.282207 loss_att 13.843210 loss_ctc 14.155088 loss_rnnt 10.253249 hw_loss 0.250696 lr 0.00029590 rank 6
2023-02-27 16:12:53,276 DEBUG TRAIN Batch 46/7400 loss 3.103865 loss_att 5.912155 loss_ctc 6.451107 loss_rnnt 1.982071 hw_loss 0.213445 lr 0.00029591 rank 4
2023-02-27 16:12:53,279 DEBUG TRAIN Batch 46/7400 loss 8.651217 loss_att 13.785627 loss_ctc 16.493292 loss_rnnt 6.464150 hw_loss 0.214824 lr 0.00029591 rank 5
2023-02-27 16:12:53,280 DEBUG TRAIN Batch 46/7400 loss 8.041359 loss_att 11.007332 loss_ctc 9.986240 loss_rnnt 7.156414 hw_loss 0.060811 lr 0.00029590 rank 3
2023-02-27 16:12:53,283 DEBUG TRAIN Batch 46/7400 loss 6.051086 loss_att 10.197332 loss_ctc 14.038668 loss_rnnt 4.103464 hw_loss 0.100053 lr 0.00029591 rank 1
2023-02-27 16:13:59,445 DEBUG TRAIN Batch 46/7500 loss 5.354248 loss_att 7.515836 loss_ctc 9.298917 loss_rnnt 4.208886 hw_loss 0.350789 lr 0.00029589 rank 2
2023-02-27 16:13:59,445 DEBUG TRAIN Batch 46/7500 loss 8.180268 loss_att 9.196094 loss_ctc 13.334927 loss_rnnt 7.164170 hw_loss 0.235584 lr 0.00029590 rank 0
2023-02-27 16:13:59,446 DEBUG TRAIN Batch 46/7500 loss 5.679766 loss_att 8.871987 loss_ctc 8.489326 loss_rnnt 4.511350 hw_loss 0.291306 lr 0.00029589 rank 6
2023-02-27 16:13:59,447 DEBUG TRAIN Batch 46/7500 loss 2.309274 loss_att 4.411780 loss_ctc 8.424633 loss_rnnt 0.955246 hw_loss 0.221523 lr 0.00029589 rank 3
2023-02-27 16:13:59,447 DEBUG TRAIN Batch 46/7500 loss 8.904112 loss_att 12.061710 loss_ctc 16.487421 loss_rnnt 7.045489 hw_loss 0.404989 lr 0.00029589 rank 7
2023-02-27 16:13:59,452 DEBUG TRAIN Batch 46/7500 loss 6.618527 loss_att 8.697178 loss_ctc 10.512413 loss_rnnt 5.582620 hw_loss 0.189361 lr 0.00029589 rank 1
2023-02-27 16:13:59,453 DEBUG TRAIN Batch 46/7500 loss 8.660435 loss_att 9.693480 loss_ctc 12.275265 loss_rnnt 7.873444 hw_loss 0.184507 lr 0.00029589 rank 5
2023-02-27 16:13:59,462 DEBUG TRAIN Batch 46/7500 loss 7.876952 loss_att 9.936048 loss_ctc 11.080368 loss_rnnt 6.952497 hw_loss 0.160339 lr 0.00029589 rank 4
2023-02-27 16:14:38,492 DEBUG TRAIN Batch 46/7600 loss 4.060321 loss_att 4.990540 loss_ctc 5.801239 loss_rnnt 3.509064 hw_loss 0.249545 lr 0.00029588 rank 6
2023-02-27 16:14:38,493 DEBUG TRAIN Batch 46/7600 loss 5.575999 loss_att 10.764775 loss_ctc 8.730661 loss_rnnt 4.067637 hw_loss 0.093722 lr 0.00029588 rank 3
2023-02-27 16:14:38,497 DEBUG TRAIN Batch 46/7600 loss 1.814285 loss_att 5.303561 loss_ctc 1.650937 loss_rnnt 1.019263 hw_loss 0.223024 lr 0.00029588 rank 0
2023-02-27 16:14:38,508 DEBUG TRAIN Batch 46/7600 loss 2.981610 loss_att 7.389205 loss_ctc 8.511850 loss_rnnt 1.192623 hw_loss 0.318942 lr 0.00029588 rank 7
2023-02-27 16:14:38,509 DEBUG TRAIN Batch 46/7600 loss 6.705267 loss_att 8.772182 loss_ctc 9.700027 loss_rnnt 5.705564 hw_loss 0.350660 lr 0.00029588 rank 4
2023-02-27 16:14:38,510 DEBUG TRAIN Batch 46/7600 loss 6.985110 loss_att 10.963807 loss_ctc 12.413279 loss_rnnt 5.360749 hw_loss 0.196623 lr 0.00029588 rank 5
2023-02-27 16:14:38,511 DEBUG TRAIN Batch 46/7600 loss 8.485993 loss_att 11.143599 loss_ctc 12.486864 loss_rnnt 7.265177 hw_loss 0.292211 lr 0.00029588 rank 2
2023-02-27 16:14:38,513 DEBUG TRAIN Batch 46/7600 loss 1.939795 loss_att 3.857478 loss_ctc 4.551011 loss_rnnt 1.042986 hw_loss 0.309582 lr 0.00029588 rank 1
2023-02-27 16:15:17,591 DEBUG TRAIN Batch 46/7700 loss 8.037684 loss_att 8.678532 loss_ctc 10.356381 loss_rnnt 7.361586 hw_loss 0.447691 lr 0.00029586 rank 2
2023-02-27 16:15:17,597 DEBUG TRAIN Batch 46/7700 loss 1.532426 loss_att 4.900178 loss_ctc 3.639610 loss_rnnt 0.390066 hw_loss 0.352220 lr 0.00029587 rank 0
2023-02-27 16:15:17,603 DEBUG TRAIN Batch 46/7700 loss 5.517205 loss_att 8.562366 loss_ctc 14.045242 loss_rnnt 3.674553 hw_loss 0.181028 lr 0.00029586 rank 7
2023-02-27 16:15:17,606 DEBUG TRAIN Batch 46/7700 loss 7.659684 loss_att 9.980440 loss_ctc 13.988409 loss_rnnt 6.268948 hw_loss 0.155165 lr 0.00029587 rank 4
2023-02-27 16:15:17,610 DEBUG TRAIN Batch 46/7700 loss 1.983953 loss_att 4.068188 loss_ctc 3.925605 loss_rnnt 1.201423 hw_loss 0.200243 lr 0.00029587 rank 5
2023-02-27 16:15:17,623 DEBUG TRAIN Batch 46/7700 loss 9.619549 loss_att 12.238903 loss_ctc 14.461334 loss_rnnt 8.291352 hw_loss 0.297664 lr 0.00029587 rank 3
2023-02-27 16:15:17,627 DEBUG TRAIN Batch 46/7700 loss 6.584505 loss_att 6.934124 loss_ctc 8.722560 loss_rnnt 5.952629 hw_loss 0.519148 lr 0.00029586 rank 6
2023-02-27 16:15:17,651 DEBUG TRAIN Batch 46/7700 loss 9.575463 loss_att 9.082940 loss_ctc 12.772212 loss_rnnt 9.043777 hw_loss 0.382424 lr 0.00029587 rank 1
2023-02-27 16:16:25,156 DEBUG TRAIN Batch 46/7800 loss 7.188493 loss_att 10.906128 loss_ctc 18.206362 loss_rnnt 4.887918 hw_loss 0.164998 lr 0.00029585 rank 6
2023-02-27 16:16:25,157 DEBUG TRAIN Batch 46/7800 loss 2.178178 loss_att 6.109509 loss_ctc 4.017174 loss_rnnt 1.052159 hw_loss 0.177287 lr 0.00029585 rank 3
2023-02-27 16:16:25,158 DEBUG TRAIN Batch 46/7800 loss 7.237653 loss_att 10.490197 loss_ctc 13.358976 loss_rnnt 5.699182 hw_loss 0.134599 lr 0.00029585 rank 4
2023-02-27 16:16:25,161 DEBUG TRAIN Batch 46/7800 loss 7.880018 loss_att 9.764149 loss_ctc 14.489475 loss_rnnt 6.538883 hw_loss 0.155716 lr 0.00029585 rank 7
2023-02-27 16:16:25,161 DEBUG TRAIN Batch 46/7800 loss 5.790538 loss_att 9.178408 loss_ctc 11.041076 loss_rnnt 4.305044 hw_loss 0.202214 lr 0.00029585 rank 2
2023-02-27 16:16:25,167 DEBUG TRAIN Batch 46/7800 loss 4.644920 loss_att 6.709002 loss_ctc 6.116462 loss_rnnt 3.899314 hw_loss 0.256097 lr 0.00029586 rank 0
2023-02-27 16:16:25,174 DEBUG TRAIN Batch 46/7800 loss 3.057539 loss_att 7.305770 loss_ctc 6.919085 loss_rnnt 1.635051 hw_loss 0.108693 lr 0.00029585 rank 1
2023-02-27 16:16:25,186 DEBUG TRAIN Batch 46/7800 loss 7.277154 loss_att 8.578439 loss_ctc 12.333160 loss_rnnt 6.209208 hw_loss 0.250417 lr 0.00029585 rank 5
2023-02-27 16:17:05,254 DEBUG TRAIN Batch 46/7900 loss 11.701491 loss_att 15.671551 loss_ctc 18.447578 loss_rnnt 9.910141 hw_loss 0.183486 lr 0.00029584 rank 5
2023-02-27 16:17:05,260 DEBUG TRAIN Batch 46/7900 loss 4.427983 loss_att 6.681639 loss_ctc 11.468189 loss_rnnt 2.927943 hw_loss 0.207404 lr 0.00029584 rank 7
2023-02-27 16:17:05,270 DEBUG TRAIN Batch 46/7900 loss 5.030498 loss_att 9.423910 loss_ctc 10.726844 loss_rnnt 3.269169 hw_loss 0.230875 lr 0.00029584 rank 0
2023-02-27 16:17:05,272 DEBUG TRAIN Batch 46/7900 loss 4.472085 loss_att 8.044748 loss_ctc 7.304703 loss_rnnt 3.194146 hw_loss 0.348234 lr 0.00029584 rank 6
2023-02-27 16:17:05,276 DEBUG TRAIN Batch 46/7900 loss 2.617422 loss_att 7.742495 loss_ctc 6.748507 loss_rnnt 0.909443 hw_loss 0.247787 lr 0.00029584 rank 3
2023-02-27 16:17:05,277 DEBUG TRAIN Batch 46/7900 loss 3.575621 loss_att 6.688593 loss_ctc 8.394722 loss_rnnt 2.151494 hw_loss 0.298098 lr 0.00029584 rank 2
2023-02-27 16:17:05,281 DEBUG TRAIN Batch 46/7900 loss 7.723915 loss_att 10.249650 loss_ctc 11.623959 loss_rnnt 6.539150 hw_loss 0.299271 lr 0.00029584 rank 1
2023-02-27 16:17:05,281 DEBUG TRAIN Batch 46/7900 loss 7.368328 loss_att 12.954734 loss_ctc 13.608539 loss_rnnt 5.251312 hw_loss 0.314451 lr 0.00029584 rank 4
2023-02-27 16:17:44,585 DEBUG TRAIN Batch 46/8000 loss 6.175539 loss_att 10.705021 loss_ctc 15.064806 loss_rnnt 3.911311 hw_loss 0.324555 lr 0.00029582 rank 6
2023-02-27 16:17:44,598 DEBUG TRAIN Batch 46/8000 loss 10.894160 loss_att 15.336000 loss_ctc 16.511894 loss_rnnt 9.193903 hw_loss 0.117858 lr 0.00029583 rank 5
2023-02-27 16:17:44,599 DEBUG TRAIN Batch 46/8000 loss 4.615390 loss_att 7.901912 loss_ctc 6.439110 loss_rnnt 3.649792 hw_loss 0.122120 lr 0.00029583 rank 0
2023-02-27 16:17:44,602 DEBUG TRAIN Batch 46/8000 loss 3.165161 loss_att 5.523446 loss_ctc 4.318375 loss_rnnt 2.420327 hw_loss 0.223904 lr 0.00029582 rank 7
2023-02-27 16:17:44,602 DEBUG TRAIN Batch 46/8000 loss 6.648826 loss_att 12.347550 loss_ctc 13.248652 loss_rnnt 4.572724 hw_loss 0.105711 lr 0.00029583 rank 3
2023-02-27 16:17:44,604 DEBUG TRAIN Batch 46/8000 loss 6.033985 loss_att 8.616574 loss_ctc 9.166918 loss_rnnt 5.044697 hw_loss 0.103209 lr 0.00029583 rank 4
2023-02-27 16:17:44,608 DEBUG TRAIN Batch 46/8000 loss 8.792249 loss_att 11.576332 loss_ctc 14.285434 loss_rnnt 7.351077 hw_loss 0.284871 lr 0.00029583 rank 1
2023-02-27 16:17:44,649 DEBUG TRAIN Batch 46/8000 loss 5.757690 loss_att 9.159773 loss_ctc 14.511791 loss_rnnt 3.785505 hw_loss 0.233539 lr 0.00029583 rank 2
2023-02-27 16:18:24,911 DEBUG TRAIN Batch 46/8100 loss 2.021310 loss_att 4.187145 loss_ctc 5.216645 loss_rnnt 1.057899 hw_loss 0.195374 lr 0.00029581 rank 6
2023-02-27 16:18:24,913 DEBUG TRAIN Batch 46/8100 loss 4.180928 loss_att 7.819600 loss_ctc 8.312439 loss_rnnt 2.810260 hw_loss 0.172623 lr 0.00029582 rank 5
2023-02-27 16:18:24,916 DEBUG TRAIN Batch 46/8100 loss 4.178054 loss_att 5.275904 loss_ctc 6.530306 loss_rnnt 3.457784 hw_loss 0.350750 lr 0.00029581 rank 7
2023-02-27 16:18:24,925 DEBUG TRAIN Batch 46/8100 loss 10.398254 loss_att 13.253763 loss_ctc 19.247583 loss_rnnt 8.585022 hw_loss 0.116663 lr 0.00029582 rank 0
2023-02-27 16:18:24,927 DEBUG TRAIN Batch 46/8100 loss 16.779135 loss_att 22.456863 loss_ctc 27.732828 loss_rnnt 13.991861 hw_loss 0.358563 lr 0.00029582 rank 4
2023-02-27 16:18:24,929 DEBUG TRAIN Batch 46/8100 loss 7.411284 loss_att 9.895576 loss_ctc 10.096558 loss_rnnt 6.404593 hw_loss 0.284618 lr 0.00029581 rank 3
2023-02-27 16:18:24,933 DEBUG TRAIN Batch 46/8100 loss 8.047899 loss_att 10.976424 loss_ctc 12.542266 loss_rnnt 6.762672 hw_loss 0.188012 lr 0.00029582 rank 1
2023-02-27 16:18:24,934 DEBUG TRAIN Batch 46/8100 loss 6.385757 loss_att 9.923037 loss_ctc 9.399284 loss_rnnt 5.152278 hw_loss 0.232912 lr 0.00029581 rank 2
2023-02-27 16:19:05,209 DEBUG TRAIN Batch 46/8200 loss 6.610235 loss_att 9.490421 loss_ctc 10.443316 loss_rnnt 5.411106 hw_loss 0.210028 lr 0.00029580 rank 6
2023-02-27 16:19:05,210 DEBUG TRAIN Batch 46/8200 loss 6.170169 loss_att 10.390705 loss_ctc 11.197874 loss_rnnt 4.513214 hw_loss 0.267163 lr 0.00029580 rank 0
2023-02-27 16:19:05,210 DEBUG TRAIN Batch 46/8200 loss 10.408518 loss_att 8.928072 loss_ctc 12.975629 loss_rnnt 10.145316 hw_loss 0.406895 lr 0.00029580 rank 3
2023-02-27 16:19:05,211 DEBUG TRAIN Batch 46/8200 loss 2.202421 loss_att 5.201859 loss_ctc 3.224185 loss_rnnt 1.385982 hw_loss 0.150594 lr 0.00029580 rank 7
2023-02-27 16:19:05,212 DEBUG TRAIN Batch 46/8200 loss 4.984285 loss_att 8.027016 loss_ctc 12.131376 loss_rnnt 3.248119 hw_loss 0.327515 lr 0.00029580 rank 1
2023-02-27 16:19:05,214 DEBUG TRAIN Batch 46/8200 loss 5.555025 loss_att 7.799568 loss_ctc 8.684676 loss_rnnt 4.512977 hw_loss 0.329722 lr 0.00029580 rank 4
2023-02-27 16:19:05,217 DEBUG TRAIN Batch 46/8200 loss 3.805224 loss_att 7.494328 loss_ctc 8.594231 loss_rnnt 2.318894 hw_loss 0.206202 lr 0.00029580 rank 5
2023-02-27 16:19:05,219 DEBUG TRAIN Batch 46/8200 loss 9.283970 loss_att 9.520683 loss_ctc 15.077389 loss_rnnt 8.261351 hw_loss 0.380288 lr 0.00029580 rank 2
2023-02-27 16:19:43,851 DEBUG TRAIN Batch 46/8300 loss 3.294436 loss_att 4.981106 loss_ctc 4.915154 loss_rnnt 2.663187 hw_loss 0.145909 lr 0.00029579 rank 2
2023-02-27 16:19:43,856 DEBUG TRAIN Batch 46/8300 loss 5.097637 loss_att 7.732105 loss_ctc 8.571299 loss_rnnt 4.060854 hw_loss 0.087628 lr 0.00029579 rank 1
2023-02-27 16:19:43,865 DEBUG TRAIN Batch 46/8300 loss 4.157643 loss_att 6.902494 loss_ctc 8.607470 loss_rnnt 2.943012 hw_loss 0.135658 lr 0.00029579 rank 5
2023-02-27 16:19:43,867 DEBUG TRAIN Batch 46/8300 loss 9.522103 loss_att 12.747222 loss_ctc 11.565842 loss_rnnt 8.515299 hw_loss 0.167404 lr 0.00029579 rank 3
2023-02-27 16:19:43,870 DEBUG TRAIN Batch 46/8300 loss 9.032101 loss_att 8.836327 loss_ctc 14.210959 loss_rnnt 8.228086 hw_loss 0.286225 lr 0.00029578 rank 6
2023-02-27 16:19:43,873 DEBUG TRAIN Batch 46/8300 loss 6.878119 loss_att 12.423391 loss_ctc 16.214993 loss_rnnt 4.385228 hw_loss 0.260475 lr 0.00029579 rank 4
2023-02-27 16:19:43,875 DEBUG TRAIN Batch 46/8300 loss 3.649490 loss_att 5.929950 loss_ctc 7.774150 loss_rnnt 2.540295 hw_loss 0.193404 lr 0.00029579 rank 0
2023-02-27 16:19:43,926 DEBUG TRAIN Batch 46/8300 loss 4.235232 loss_att 7.590094 loss_ctc 5.625661 loss_rnnt 3.259098 hw_loss 0.224572 lr 0.00029579 rank 7
2023-02-27 16:20:14,474 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505826 history loss 0.873228 rank 1
2023-02-27 16:20:14,502 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505825 history loss 0.873228 rank 3
2023-02-27 16:20:14,636 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505826 history loss 0.873228 rank 0
2023-02-27 16:20:14,672 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505825 history loss 0.873228 rank 5
2023-02-27 16:20:14,854 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505825 history loss 0.873228 rank 4
2023-02-27 16:20:14,900 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505826 history loss 0.873228 rank 7
2023-02-27 16:20:14,934 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505826 history loss 0.873228 rank 2
2023-02-27 16:20:15,139 DEBUG CV Batch 46/0 loss 0.906813 loss_att 0.824191 loss_ctc 1.154669 loss_rnnt 0.620517 hw_loss 0.505826 history loss 0.873228 rank 6
2023-02-27 16:20:25,672 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 1
2023-02-27 16:20:25,976 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 5
2023-02-27 16:20:26,084 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 7
2023-02-27 16:20:26,087 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 2
2023-02-27 16:20:26,135 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 4
2023-02-27 16:20:26,384 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 3
2023-02-27 16:20:26,682 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 0
2023-02-27 16:20:27,025 DEBUG CV Batch 46/100 loss 4.314927 loss_att 4.914100 loss_ctc 9.212011 loss_rnnt 3.414839 hw_loss 0.238703 history loss 2.939759 rank 6
2023-02-27 16:20:39,104 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 1
2023-02-27 16:20:39,448 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 4
2023-02-27 16:20:39,452 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 7
2023-02-27 16:20:39,465 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 5
2023-02-27 16:20:39,785 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 2
2023-02-27 16:20:40,400 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 3
2023-02-27 16:20:40,942 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 0
2023-02-27 16:20:41,009 DEBUG CV Batch 46/200 loss 6.997059 loss_att 8.573677 loss_ctc 7.982878 loss_rnnt 6.416911 hw_loss 0.250091 history loss 3.526838 rank 6
2023-02-27 16:20:51,382 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 4
2023-02-27 16:20:51,427 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 7
2023-02-27 16:20:51,457 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 5
2023-02-27 16:20:51,857 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 1
2023-02-27 16:20:51,916 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 2
2023-02-27 16:20:53,301 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 3
2023-02-27 16:20:53,912 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 6
2023-02-27 16:20:53,995 DEBUG CV Batch 46/300 loss 3.366608 loss_att 4.127060 loss_ctc 5.154428 loss_rnnt 2.827456 hw_loss 0.278785 history loss 3.670418 rank 0
2023-02-27 16:21:03,564 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 4
2023-02-27 16:21:03,564 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 5
2023-02-27 16:21:03,597 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 7
2023-02-27 16:21:04,042 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 1
2023-02-27 16:21:04,103 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 2
2023-02-27 16:21:06,080 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 3
2023-02-27 16:21:06,639 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 6
2023-02-27 16:21:06,856 DEBUG CV Batch 46/400 loss 15.679277 loss_att 74.788223 loss_ctc 8.552538 loss_rnnt 4.730014 hw_loss 0.145698 history loss 4.495771 rank 0
2023-02-27 16:21:14,005 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 5
2023-02-27 16:21:14,071 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 4
2023-02-27 16:21:14,079 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 7
2023-02-27 16:21:14,664 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 2
2023-02-27 16:21:14,831 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 1
2023-02-27 16:21:17,418 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 3
2023-02-27 16:21:18,014 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 6
2023-02-27 16:21:18,477 DEBUG CV Batch 46/500 loss 4.715803 loss_att 4.947341 loss_ctc 6.925054 loss_rnnt 4.273969 hw_loss 0.189300 history loss 5.130818 rank 0
2023-02-27 16:21:26,104 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 5
2023-02-27 16:21:26,136 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 4
2023-02-27 16:21:26,253 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 7
2023-02-27 16:21:26,829 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 2
2023-02-27 16:21:27,033 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 1
2023-02-27 16:21:30,193 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 3
2023-02-27 16:21:30,902 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 6
2023-02-27 16:21:31,559 DEBUG CV Batch 46/600 loss 6.724117 loss_att 6.060751 loss_ctc 9.340253 loss_rnnt 6.282445 hw_loss 0.422861 history loss 5.979926 rank 0
2023-02-27 16:21:37,477 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 4
2023-02-27 16:21:37,599 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 5
2023-02-27 16:21:37,878 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 7
2023-02-27 16:21:38,481 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 2
2023-02-27 16:21:38,591 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 1
2023-02-27 16:21:42,297 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 3
2023-02-27 16:21:43,021 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 6
2023-02-27 16:21:43,947 DEBUG CV Batch 46/700 loss 13.483961 loss_att 38.032017 loss_ctc 11.520522 loss_rnnt 8.763582 hw_loss 0.136049 history loss 6.503479 rank 0
2023-02-27 16:21:48,367 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 4
2023-02-27 16:21:48,588 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 5
2023-02-27 16:21:49,336 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 7
2023-02-27 16:21:49,765 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 2
2023-02-27 16:21:49,797 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 1
2023-02-27 16:21:54,455 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 3
2023-02-27 16:21:55,221 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 6
2023-02-27 16:21:56,245 DEBUG CV Batch 46/800 loss 6.888148 loss_att 7.652186 loss_ctc 14.631361 loss_rnnt 5.582097 hw_loss 0.226526 history loss 6.021239 rank 0
2023-02-27 16:22:01,721 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 4
2023-02-27 16:22:01,908 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 5
2023-02-27 16:22:02,946 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 7
2023-02-27 16:22:03,261 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 2
2023-02-27 16:22:03,291 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 1
2023-02-27 16:22:08,438 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 3
2023-02-27 16:22:09,290 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 6
2023-02-27 16:22:10,307 DEBUG CV Batch 46/900 loss 7.610766 loss_att 11.213456 loss_ctc 16.192244 loss_rnnt 5.646498 hw_loss 0.186624 history loss 5.859310 rank 0
2023-02-27 16:22:14,078 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 4
2023-02-27 16:22:14,352 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 5
2023-02-27 16:22:15,350 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 7
2023-02-27 16:22:15,753 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 2
2023-02-27 16:22:15,754 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 1
2023-02-27 16:22:21,328 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 3
2023-02-27 16:22:22,151 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 6
2023-02-27 16:22:23,175 DEBUG CV Batch 46/1000 loss 3.728720 loss_att 3.916999 loss_ctc 3.376497 loss_rnnt 3.607382 hw_loss 0.244961 history loss 5.670442 rank 0
2023-02-27 16:22:25,905 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 4
2023-02-27 16:22:26,466 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 5
2023-02-27 16:22:27,223 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 7
2023-02-27 16:22:27,885 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 1
2023-02-27 16:22:27,889 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 2
2023-02-27 16:22:33,984 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 3
2023-02-27 16:22:34,808 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 6
2023-02-27 16:22:36,019 DEBUG CV Batch 46/1100 loss 4.491213 loss_att 4.225638 loss_ctc 7.792705 loss_rnnt 3.857022 hw_loss 0.463326 history loss 5.629235 rank 0
2023-02-27 16:22:36,529 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 4
2023-02-27 16:22:37,113 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 5
2023-02-27 16:22:38,019 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 7
2023-02-27 16:22:38,551 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 1
2023-02-27 16:22:38,821 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 2
2023-02-27 16:22:45,340 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 3
2023-02-27 16:22:46,129 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 6
2023-02-27 16:22:47,727 DEBUG CV Batch 46/1200 loss 5.908618 loss_att 6.010986 loss_ctc 7.227824 loss_rnnt 5.584074 hw_loss 0.240330 history loss 5.916707 rank 0
2023-02-27 16:22:48,681 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 4
2023-02-27 16:22:49,147 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 5
2023-02-27 16:22:50,141 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 7
2023-02-27 16:22:50,488 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 1
2023-02-27 16:22:51,109 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 2
2023-02-27 16:22:58,377 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 3
2023-02-27 16:22:58,833 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 6
2023-02-27 16:23:00,122 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 4
2023-02-27 16:23:00,475 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 5
2023-02-27 16:23:00,599 DEBUG CV Batch 46/1300 loss 4.085906 loss_att 3.973217 loss_ctc 6.096542 loss_rnnt 3.671879 hw_loss 0.315900 history loss 6.208502 rank 0
2023-02-27 16:23:01,550 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 7
2023-02-27 16:23:01,704 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 1
2023-02-27 16:23:02,421 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 2
2023-02-27 16:23:10,438 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 3
2023-02-27 16:23:10,929 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 6
2023-02-27 16:23:11,922 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 4
2023-02-27 16:23:12,207 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 5
2023-02-27 16:23:12,687 DEBUG CV Batch 46/1400 loss 4.607464 loss_att 16.807545 loss_ctc 5.267184 loss_rnnt 1.899635 hw_loss 0.337220 history loss 6.479114 rank 0
2023-02-27 16:23:13,022 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 1
2023-02-27 16:23:13,082 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 7
2023-02-27 16:23:14,003 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 2
2023-02-27 16:23:22,685 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 3
2023-02-27 16:23:22,993 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 6
2023-02-27 16:23:24,808 DEBUG CV Batch 46/1500 loss 7.067274 loss_att 7.130079 loss_ctc 7.398223 loss_rnnt 6.896347 hw_loss 0.214199 history loss 6.338669 rank 0
2023-02-27 16:23:24,863 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 4
2023-02-27 16:23:25,374 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 5
2023-02-27 16:23:26,093 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 1
2023-02-27 16:23:26,157 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 7
2023-02-27 16:23:27,119 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 2
2023-02-27 16:23:36,305 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 3
2023-02-27 16:23:36,551 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 6
2023-02-27 16:23:37,402 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 4
2023-02-27 16:23:38,060 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 5
2023-02-27 16:23:38,477 DEBUG CV Batch 46/1600 loss 10.229552 loss_att 13.821279 loss_ctc 13.684218 loss_rnnt 8.869092 hw_loss 0.340299 history loss 6.297966 rank 0
2023-02-27 16:23:38,612 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 1
2023-02-27 16:23:38,729 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 7
2023-02-27 16:23:39,857 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 2
2023-02-27 16:23:46,667 INFO Epoch 46 CV info cv_loss 6.201161517066958
2023-02-27 16:23:46,668 INFO Epoch 47 TRAIN info lr 0.00029578446802426724
2023-02-27 16:23:46,673 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:47,460 INFO Epoch 46 CV info cv_loss 6.201161515994436
2023-02-27 16:23:47,464 INFO Epoch 47 TRAIN info lr 0.0002957880909723432
2023-02-27 16:23:47,468 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:48,204 INFO Epoch 46 CV info cv_loss 6.201161516769753
2023-02-27 16:23:48,204 INFO Epoch 47 TRAIN info lr 0.00029578705583073705
2023-02-27 16:23:48,209 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:48,250 INFO Epoch 46 CV info cv_loss 6.201161516190419
2023-02-27 16:23:48,251 INFO Epoch 47 TRAIN info lr 0.0002957777400453044
2023-02-27 16:23:48,252 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:48,804 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 3
2023-02-27 16:23:48,991 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 6
2023-02-27 16:23:49,298 INFO Epoch 46 CV info cv_loss 6.201161515158817
2023-02-27 16:23:49,298 INFO Epoch 47 TRAIN info lr 0.00029578446802426724
2023-02-27 16:23:49,303 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:51,136 DEBUG CV Batch 46/1700 loss 7.885046 loss_att 6.936338 loss_ctc 14.435726 loss_rnnt 7.084986 hw_loss 0.218206 history loss 6.225568 rank 0
2023-02-27 16:23:58,410 INFO Epoch 46 CV info cv_loss 6.20116151599659
2023-02-27 16:23:58,410 INFO Epoch 47 TRAIN info lr 0.00029578136274615777
2023-02-27 16:23:58,415 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:23:58,582 INFO Epoch 46 CV info cv_loss 6.201161516952815
2023-02-27 16:23:58,582 INFO Epoch 47 TRAIN info lr 0.00029577981014377826
2023-02-27 16:23:58,587 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:24:00,966 INFO Epoch 46 CV info cv_loss 6.201161515456022
2023-02-27 16:24:00,967 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/46.pt
2023-02-27 16:24:01,538 INFO Epoch 47 TRAIN info lr 0.00029578446802426724
2023-02-27 16:24:01,541 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 16:25:01,827 DEBUG TRAIN Batch 47/0 loss 7.226505 loss_att 7.382302 loss_ctc 10.861724 loss_rnnt 6.495671 hw_loss 0.403085 lr 0.00029579 rank 1
2023-02-27 16:25:01,833 DEBUG TRAIN Batch 47/0 loss 6.496396 loss_att 7.243921 loss_ctc 10.030227 loss_rnnt 5.635664 hw_loss 0.450093 lr 0.00029578 rank 2
2023-02-27 16:25:01,835 DEBUG TRAIN Batch 47/0 loss 8.283572 loss_att 8.216320 loss_ctc 12.329309 loss_rnnt 7.503918 hw_loss 0.475637 lr 0.00029578 rank 7
2023-02-27 16:25:01,837 DEBUG TRAIN Batch 47/0 loss 7.083491 loss_att 7.062319 loss_ctc 11.217328 loss_rnnt 6.359290 hw_loss 0.332357 lr 0.00029578 rank 4
2023-02-27 16:25:01,852 DEBUG TRAIN Batch 47/0 loss 4.881166 loss_att 5.062423 loss_ctc 8.102667 loss_rnnt 4.220029 hw_loss 0.366284 lr 0.00029578 rank 3
2023-02-27 16:25:01,857 DEBUG TRAIN Batch 47/0 loss 5.174057 loss_att 5.462436 loss_ctc 8.588027 loss_rnnt 4.422846 hw_loss 0.446887 lr 0.00029578 rank 0
2023-02-27 16:25:01,858 DEBUG TRAIN Batch 47/0 loss 5.605220 loss_att 6.484467 loss_ctc 8.145132 loss_rnnt 4.882582 hw_loss 0.390252 lr 0.00029579 rank 5
2023-02-27 16:25:01,859 DEBUG TRAIN Batch 47/0 loss 5.716407 loss_att 6.307932 loss_ctc 8.628057 loss_rnnt 5.029312 hw_loss 0.338570 lr 0.00029578 rank 6
2023-02-27 16:25:39,293 DEBUG TRAIN Batch 47/100 loss 5.653740 loss_att 11.209829 loss_ctc 11.025722 loss_rnnt 3.660085 hw_loss 0.311575 lr 0.00029577 rank 1
2023-02-27 16:25:39,295 DEBUG TRAIN Batch 47/100 loss 1.731110 loss_att 4.821184 loss_ctc 4.034472 loss_rnnt 0.718488 hw_loss 0.164047 lr 0.00029577 rank 0
2023-02-27 16:25:39,295 DEBUG TRAIN Batch 47/100 loss 9.881520 loss_att 14.686913 loss_ctc 13.229998 loss_rnnt 8.337839 hw_loss 0.255261 lr 0.00029577 rank 3
2023-02-27 16:25:39,295 DEBUG TRAIN Batch 47/100 loss 6.338417 loss_att 7.535677 loss_ctc 8.678003 loss_rnnt 5.660221 hw_loss 0.237748 lr 0.00029577 rank 4
2023-02-27 16:25:39,296 DEBUG TRAIN Batch 47/100 loss 2.757420 loss_att 5.543807 loss_ctc 5.703960 loss_rnnt 1.676711 hw_loss 0.244800 lr 0.00029577 rank 5
2023-02-27 16:25:39,296 DEBUG TRAIN Batch 47/100 loss 2.702856 loss_att 5.924525 loss_ctc 4.532069 loss_rnnt 1.730747 hw_loss 0.157276 lr 0.00029576 rank 7
2023-02-27 16:25:39,297 DEBUG TRAIN Batch 47/100 loss 4.276243 loss_att 8.098930 loss_ctc 9.019932 loss_rnnt 2.718131 hw_loss 0.302030 lr 0.00029577 rank 6
2023-02-27 16:25:39,346 DEBUG TRAIN Batch 47/100 loss 9.470184 loss_att 12.833681 loss_ctc 18.019594 loss_rnnt 7.600141 hw_loss 0.107667 lr 0.00029577 rank 2
2023-02-27 16:26:17,660 DEBUG TRAIN Batch 47/200 loss 3.746484 loss_att 6.747314 loss_ctc 6.398834 loss_rnnt 2.790813 hw_loss 0.003483 lr 0.00029575 rank 3
2023-02-27 16:26:17,666 DEBUG TRAIN Batch 47/200 loss 3.661538 loss_att 8.684391 loss_ctc 8.143629 loss_rnnt 1.840785 hw_loss 0.409818 lr 0.00029576 rank 0
2023-02-27 16:26:17,667 DEBUG TRAIN Batch 47/200 loss 5.556414 loss_att 7.194519 loss_ctc 11.257736 loss_rnnt 4.276730 hw_loss 0.359787 lr 0.00029575 rank 7
2023-02-27 16:26:17,669 DEBUG TRAIN Batch 47/200 loss 4.421361 loss_att 7.315099 loss_ctc 9.311488 loss_rnnt 3.102543 hw_loss 0.165101 lr 0.00029576 rank 2
2023-02-27 16:26:17,669 DEBUG TRAIN Batch 47/200 loss 4.170228 loss_att 6.695776 loss_ctc 9.229681 loss_rnnt 2.788793 hw_loss 0.378245 lr 0.00029575 rank 6
2023-02-27 16:26:17,674 DEBUG TRAIN Batch 47/200 loss 5.138715 loss_att 9.119525 loss_ctc 9.764666 loss_rnnt 3.646846 hw_loss 0.147962 lr 0.00029576 rank 1
2023-02-27 16:26:17,674 DEBUG TRAIN Batch 47/200 loss 0.880575 loss_att 3.510095 loss_ctc 1.131048 loss_rnnt 0.159316 hw_loss 0.303672 lr 0.00029576 rank 5
2023-02-27 16:26:17,716 DEBUG TRAIN Batch 47/200 loss 9.692532 loss_att 13.798327 loss_ctc 15.880019 loss_rnnt 7.794353 hw_loss 0.472542 lr 0.00029576 rank 4
2023-02-27 16:26:56,912 DEBUG TRAIN Batch 47/300 loss 6.794706 loss_att 9.401630 loss_ctc 12.921320 loss_rnnt 5.310593 hw_loss 0.273464 lr 0.00029574 rank 6
2023-02-27 16:26:56,920 DEBUG TRAIN Batch 47/300 loss 5.752740 loss_att 11.408648 loss_ctc 13.934237 loss_rnnt 3.417342 hw_loss 0.212530 lr 0.00029574 rank 3
2023-02-27 16:26:56,924 DEBUG TRAIN Batch 47/300 loss 5.963638 loss_att 10.382790 loss_ctc 11.814107 loss_rnnt 4.203568 hw_loss 0.180331 lr 0.00029575 rank 0
2023-02-27 16:26:56,931 DEBUG TRAIN Batch 47/300 loss 6.111661 loss_att 7.976802 loss_ctc 10.199501 loss_rnnt 5.139521 hw_loss 0.101375 lr 0.00029574 rank 7
2023-02-27 16:26:56,930 DEBUG TRAIN Batch 47/300 loss 3.934687 loss_att 7.597205 loss_ctc 11.242533 loss_rnnt 2.130021 hw_loss 0.183343 lr 0.00029575 rank 2
2023-02-27 16:26:56,932 DEBUG TRAIN Batch 47/300 loss 9.379172 loss_att 11.082952 loss_ctc 17.507195 loss_rnnt 7.859909 hw_loss 0.177695 lr 0.00029575 rank 5
2023-02-27 16:26:56,936 DEBUG TRAIN Batch 47/300 loss 5.892484 loss_att 9.091320 loss_ctc 9.717385 loss_rnnt 4.636117 hw_loss 0.199901 lr 0.00029575 rank 1
2023-02-27 16:26:56,941 DEBUG TRAIN Batch 47/300 loss 1.876662 loss_att 5.019172 loss_ctc 5.888473 loss_rnnt 0.558675 hw_loss 0.289832 lr 0.00029575 rank 4
2023-02-27 16:28:03,232 DEBUG TRAIN Batch 47/400 loss 10.616213 loss_att 17.533173 loss_ctc 17.525940 loss_rnnt 8.181746 hw_loss 0.243334 lr 0.00029573 rank 1
2023-02-27 16:28:03,233 DEBUG TRAIN Batch 47/400 loss 6.326593 loss_att 7.627199 loss_ctc 11.098646 loss_rnnt 5.286544 hw_loss 0.269352 lr 0.00029573 rank 6
2023-02-27 16:28:03,233 DEBUG TRAIN Batch 47/400 loss 7.080569 loss_att 8.462416 loss_ctc 10.225096 loss_rnnt 6.268163 hw_loss 0.218936 lr 0.00029573 rank 3
2023-02-27 16:28:03,233 DEBUG TRAIN Batch 47/400 loss 7.077232 loss_att 7.952755 loss_ctc 10.089405 loss_rnnt 6.366702 hw_loss 0.250880 lr 0.00029573 rank 0
2023-02-27 16:28:03,236 DEBUG TRAIN Batch 47/400 loss 8.453275 loss_att 9.736540 loss_ctc 11.064950 loss_rnnt 7.659480 hw_loss 0.354224 lr 0.00029573 rank 4
2023-02-27 16:28:03,235 DEBUG TRAIN Batch 47/400 loss 5.850725 loss_att 7.809852 loss_ctc 7.338842 loss_rnnt 5.164721 hw_loss 0.179555 lr 0.00029573 rank 2
2023-02-27 16:28:03,239 DEBUG TRAIN Batch 47/400 loss 8.403040 loss_att 9.338880 loss_ctc 10.961991 loss_rnnt 7.727084 hw_loss 0.276739 lr 0.00029574 rank 5
2023-02-27 16:28:03,288 DEBUG TRAIN Batch 47/400 loss 5.625914 loss_att 7.995126 loss_ctc 7.679911 loss_rnnt 4.752589 hw_loss 0.235532 lr 0.00029573 rank 7
2023-02-27 16:28:42,149 DEBUG TRAIN Batch 47/500 loss 6.919400 loss_att 11.124285 loss_ctc 13.301476 loss_rnnt 5.112407 hw_loss 0.215760 lr 0.00029572 rank 0
2023-02-27 16:28:42,154 DEBUG TRAIN Batch 47/500 loss 4.799607 loss_att 6.998071 loss_ctc 9.119400 loss_rnnt 3.641100 hw_loss 0.267827 lr 0.00029572 rank 3
2023-02-27 16:28:42,157 DEBUG TRAIN Batch 47/500 loss 2.531906 loss_att 4.561247 loss_ctc 5.161108 loss_rnnt 1.667578 hw_loss 0.202312 lr 0.00029572 rank 2
2023-02-27 16:28:42,157 DEBUG TRAIN Batch 47/500 loss 2.236731 loss_att 5.208566 loss_ctc 4.584421 loss_rnnt 1.205559 hw_loss 0.232088 lr 0.00029571 rank 6
2023-02-27 16:28:42,162 DEBUG TRAIN Batch 47/500 loss 5.119800 loss_att 8.253132 loss_ctc 10.717621 loss_rnnt 3.656841 hw_loss 0.168592 lr 0.00029572 rank 1
2023-02-27 16:28:42,170 DEBUG TRAIN Batch 47/500 loss 12.134556 loss_att 14.002252 loss_ctc 22.684631 loss_rnnt 10.254849 hw_loss 0.186544 lr 0.00029571 rank 7
2023-02-27 16:28:42,181 DEBUG TRAIN Batch 47/500 loss 4.577186 loss_att 7.860481 loss_ctc 9.277679 loss_rnnt 3.183129 hw_loss 0.207498 lr 0.00029572 rank 4
2023-02-27 16:28:42,215 DEBUG TRAIN Batch 47/500 loss 6.042018 loss_att 8.112074 loss_ctc 9.446495 loss_rnnt 5.039042 hw_loss 0.253188 lr 0.00029572 rank 5
2023-02-27 16:29:21,338 DEBUG TRAIN Batch 47/600 loss 11.332108 loss_att 13.049494 loss_ctc 19.386860 loss_rnnt 9.758243 hw_loss 0.293292 lr 0.00029570 rank 6
2023-02-27 16:29:21,341 DEBUG TRAIN Batch 47/600 loss 10.194152 loss_att 11.576530 loss_ctc 19.556761 loss_rnnt 8.502294 hw_loss 0.313190 lr 0.00029570 rank 3
2023-02-27 16:29:21,342 DEBUG TRAIN Batch 47/600 loss 8.725021 loss_att 9.295902 loss_ctc 12.496840 loss_rnnt 7.941161 hw_loss 0.312704 lr 0.00029571 rank 0
2023-02-27 16:29:21,346 DEBUG TRAIN Batch 47/600 loss 5.385847 loss_att 6.832596 loss_ctc 8.713144 loss_rnnt 4.399184 hw_loss 0.475636 lr 0.00029571 rank 5
2023-02-27 16:29:21,347 DEBUG TRAIN Batch 47/600 loss 4.414255 loss_att 8.077727 loss_ctc 8.161465 loss_rnnt 3.158275 hw_loss 0.044358 lr 0.00029571 rank 2
2023-02-27 16:29:21,347 DEBUG TRAIN Batch 47/600 loss 1.749120 loss_att 3.373540 loss_ctc 2.639478 loss_rnnt 1.146473 hw_loss 0.298216 lr 0.00029571 rank 4
2023-02-27 16:29:21,348 DEBUG TRAIN Batch 47/600 loss 11.950829 loss_att 15.008536 loss_ctc 19.622154 loss_rnnt 10.238947 hw_loss 0.145305 lr 0.00029570 rank 7
2023-02-27 16:29:21,358 DEBUG TRAIN Batch 47/600 loss 6.355117 loss_att 7.696817 loss_ctc 8.810604 loss_rnnt 5.567415 hw_loss 0.359933 lr 0.00029571 rank 1
2023-02-27 16:30:00,659 DEBUG TRAIN Batch 47/700 loss 4.890272 loss_att 11.304924 loss_ctc 10.052813 loss_rnnt 2.803546 hw_loss 0.216482 lr 0.00029569 rank 2
2023-02-27 16:30:00,667 DEBUG TRAIN Batch 47/700 loss 0.955108 loss_att 2.578488 loss_ctc 2.667182 loss_rnnt 0.278050 hw_loss 0.232697 lr 0.00029569 rank 4
2023-02-27 16:30:00,674 DEBUG TRAIN Batch 47/700 loss 4.633224 loss_att 7.039371 loss_ctc 6.511157 loss_rnnt 3.858370 hw_loss 0.081063 lr 0.00029569 rank 6
2023-02-27 16:30:00,678 DEBUG TRAIN Batch 47/700 loss 4.217624 loss_att 8.535868 loss_ctc 7.364358 loss_rnnt 2.889181 hw_loss 0.084805 lr 0.00029569 rank 3
2023-02-27 16:30:00,681 DEBUG TRAIN Batch 47/700 loss 5.044413 loss_att 8.335964 loss_ctc 13.021536 loss_rnnt 3.212741 hw_loss 0.205770 lr 0.00029569 rank 0
2023-02-27 16:30:00,681 DEBUG TRAIN Batch 47/700 loss 10.362306 loss_att 11.753269 loss_ctc 18.198448 loss_rnnt 8.972205 hw_loss 0.125791 lr 0.00029569 rank 7
2023-02-27 16:30:00,682 DEBUG TRAIN Batch 47/700 loss 1.561714 loss_att 4.205993 loss_ctc 2.398801 loss_rnnt 0.820343 hw_loss 0.189194 lr 0.00029570 rank 5
2023-02-27 16:30:00,683 DEBUG TRAIN Batch 47/700 loss 11.211909 loss_att 13.650279 loss_ctc 16.641750 loss_rnnt 9.929012 hw_loss 0.133584 lr 0.00029570 rank 1
2023-02-27 16:31:05,017 DEBUG TRAIN Batch 47/800 loss 5.024030 loss_att 8.660095 loss_ctc 8.382648 loss_rnnt 3.818014 hw_loss 0.058100 lr 0.00029568 rank 5
2023-02-27 16:31:05,020 DEBUG TRAIN Batch 47/800 loss 4.529248 loss_att 5.826243 loss_ctc 7.674004 loss_rnnt 3.693016 hw_loss 0.295373 lr 0.00029568 rank 0
2023-02-27 16:31:05,020 DEBUG TRAIN Batch 47/800 loss 7.136955 loss_att 8.748518 loss_ctc 13.454188 loss_rnnt 5.797556 hw_loss 0.327728 lr 0.00029568 rank 6
2023-02-27 16:31:05,020 DEBUG TRAIN Batch 47/800 loss 4.456340 loss_att 7.123613 loss_ctc 6.329233 loss_rnnt 3.475566 hw_loss 0.370500 lr 0.00029568 rank 3
2023-02-27 16:31:05,022 DEBUG TRAIN Batch 47/800 loss 4.269366 loss_att 8.574156 loss_ctc 6.959263 loss_rnnt 2.860967 hw_loss 0.353977 lr 0.00029568 rank 2
2023-02-27 16:31:05,023 DEBUG TRAIN Batch 47/800 loss 3.292593 loss_att 5.320806 loss_ctc 5.526891 loss_rnnt 2.347698 hw_loss 0.452523 lr 0.00029568 rank 1
2023-02-27 16:31:05,054 DEBUG TRAIN Batch 47/800 loss 4.291524 loss_att 7.155491 loss_ctc 11.341753 loss_rnnt 2.664670 hw_loss 0.213808 lr 0.00029568 rank 4
2023-02-27 16:31:05,056 DEBUG TRAIN Batch 47/800 loss 1.798061 loss_att 3.619325 loss_ctc 1.405429 loss_rnnt 1.393477 hw_loss 0.173780 lr 0.00029567 rank 7
2023-02-27 16:31:43,169 DEBUG TRAIN Batch 47/900 loss 6.004845 loss_att 8.298441 loss_ctc 9.780400 loss_rnnt 4.938167 hw_loss 0.196034 lr 0.00029566 rank 3
2023-02-27 16:31:43,173 DEBUG TRAIN Batch 47/900 loss 5.372880 loss_att 9.981951 loss_ctc 14.773473 loss_rnnt 3.009241 hw_loss 0.353275 lr 0.00029566 rank 6
2023-02-27 16:31:43,182 DEBUG TRAIN Batch 47/900 loss 9.099846 loss_att 13.571618 loss_ctc 13.777567 loss_rnnt 7.454043 hw_loss 0.239535 lr 0.00029567 rank 0
2023-02-27 16:31:43,183 DEBUG TRAIN Batch 47/900 loss 4.885090 loss_att 8.578128 loss_ctc 10.601692 loss_rnnt 3.219458 hw_loss 0.309020 lr 0.00029567 rank 5
2023-02-27 16:31:43,194 DEBUG TRAIN Batch 47/900 loss 7.392941 loss_att 8.310955 loss_ctc 11.599177 loss_rnnt 6.529381 hw_loss 0.223361 lr 0.00029567 rank 4
2023-02-27 16:31:43,200 DEBUG TRAIN Batch 47/900 loss 2.532212 loss_att 6.496161 loss_ctc 6.863643 loss_rnnt 1.004671 hw_loss 0.294800 lr 0.00029567 rank 2
2023-02-27 16:31:43,219 DEBUG TRAIN Batch 47/900 loss 4.235110 loss_att 8.339916 loss_ctc 6.956046 loss_rnnt 2.970698 hw_loss 0.151236 lr 0.00029566 rank 7
2023-02-27 16:31:43,230 DEBUG TRAIN Batch 47/900 loss 6.983430 loss_att 10.822016 loss_ctc 11.488390 loss_rnnt 5.481487 hw_loss 0.250435 lr 0.00029567 rank 1
2023-02-27 16:32:21,314 DEBUG TRAIN Batch 47/1000 loss 11.340501 loss_att 15.602373 loss_ctc 22.666172 loss_rnnt 8.865503 hw_loss 0.211000 lr 0.00029565 rank 7
2023-02-27 16:32:21,315 DEBUG TRAIN Batch 47/1000 loss 5.623746 loss_att 8.474156 loss_ctc 6.791903 loss_rnnt 4.792573 hw_loss 0.197507 lr 0.00029565 rank 3
2023-02-27 16:32:21,321 DEBUG TRAIN Batch 47/1000 loss 3.587545 loss_att 6.564741 loss_ctc 7.323358 loss_rnnt 2.375287 hw_loss 0.222583 lr 0.00029566 rank 1
2023-02-27 16:32:21,322 DEBUG TRAIN Batch 47/1000 loss 3.672244 loss_att 5.831190 loss_ctc 8.162848 loss_rnnt 2.487648 hw_loss 0.288862 lr 0.00029566 rank 5
2023-02-27 16:32:21,331 DEBUG TRAIN Batch 47/1000 loss 7.611844 loss_att 10.572526 loss_ctc 11.341452 loss_rnnt 6.432323 hw_loss 0.168942 lr 0.00029565 rank 0
2023-02-27 16:32:21,331 DEBUG TRAIN Batch 47/1000 loss 10.522497 loss_att 13.159533 loss_ctc 18.311872 loss_rnnt 8.775707 hw_loss 0.338997 lr 0.00029565 rank 6
2023-02-27 16:32:21,334 DEBUG TRAIN Batch 47/1000 loss 5.103549 loss_att 9.149717 loss_ctc 9.673415 loss_rnnt 3.574324 hw_loss 0.207517 lr 0.00029565 rank 2
2023-02-27 16:32:21,334 DEBUG TRAIN Batch 47/1000 loss 4.979898 loss_att 6.690887 loss_ctc 11.858835 loss_rnnt 3.670274 hw_loss 0.094190 lr 0.00029565 rank 4
2023-02-27 16:33:26,506 DEBUG TRAIN Batch 47/1100 loss 10.032575 loss_att 13.626069 loss_ctc 17.174709 loss_rnnt 8.328791 hw_loss 0.061499 lr 0.00029564 rank 2
2023-02-27 16:33:26,512 DEBUG TRAIN Batch 47/1100 loss 4.405696 loss_att 7.822074 loss_ctc 7.844652 loss_rnnt 3.211870 hw_loss 0.097544 lr 0.00029565 rank 5
2023-02-27 16:33:26,521 DEBUG TRAIN Batch 47/1100 loss 6.606086 loss_att 9.735020 loss_ctc 11.720166 loss_rnnt 5.197853 hw_loss 0.188565 lr 0.00029564 rank 4
2023-02-27 16:33:26,522 DEBUG TRAIN Batch 47/1100 loss 2.378871 loss_att 6.232491 loss_ctc 3.682004 loss_rnnt 1.322360 hw_loss 0.210068 lr 0.00029564 rank 6
2023-02-27 16:33:26,524 DEBUG TRAIN Batch 47/1100 loss 5.976690 loss_att 9.335433 loss_ctc 7.665706 loss_rnnt 4.948199 hw_loss 0.246638 lr 0.00029564 rank 0
2023-02-27 16:33:26,524 DEBUG TRAIN Batch 47/1100 loss 3.540357 loss_att 5.423941 loss_ctc 5.243565 loss_rnnt 2.739486 hw_loss 0.369486 lr 0.00029564 rank 3
2023-02-27 16:33:26,527 DEBUG TRAIN Batch 47/1100 loss 5.979879 loss_att 8.124451 loss_ctc 10.603714 loss_rnnt 4.802785 hw_loss 0.246878 lr 0.00029564 rank 7
2023-02-27 16:33:26,538 DEBUG TRAIN Batch 47/1100 loss 6.172661 loss_att 9.678796 loss_ctc 7.943112 loss_rnnt 5.089532 hw_loss 0.273454 lr 0.00029564 rank 1
2023-02-27 16:34:05,134 DEBUG TRAIN Batch 47/1200 loss 6.799716 loss_att 7.612318 loss_ctc 8.483539 loss_rnnt 6.198099 hw_loss 0.402349 lr 0.00029563 rank 0
2023-02-27 16:34:05,141 DEBUG TRAIN Batch 47/1200 loss 4.834017 loss_att 7.075101 loss_ctc 10.645153 loss_rnnt 3.476352 hw_loss 0.252430 lr 0.00029563 rank 3
2023-02-27 16:34:05,141 DEBUG TRAIN Batch 47/1200 loss 6.013095 loss_att 8.470562 loss_ctc 10.243361 loss_rnnt 4.786767 hw_loss 0.320249 lr 0.00029563 rank 5
2023-02-27 16:34:05,141 DEBUG TRAIN Batch 47/1200 loss 8.398765 loss_att 10.905690 loss_ctc 13.618884 loss_rnnt 7.071787 hw_loss 0.242956 lr 0.00029562 rank 7
2023-02-27 16:34:05,141 DEBUG TRAIN Batch 47/1200 loss 3.629825 loss_att 3.978646 loss_ctc 6.532529 loss_rnnt 3.008421 hw_loss 0.308648 lr 0.00029563 rank 1
2023-02-27 16:34:05,142 DEBUG TRAIN Batch 47/1200 loss 8.953166 loss_att 10.037886 loss_ctc 15.528425 loss_rnnt 7.655476 hw_loss 0.382583 lr 0.00029563 rank 2
2023-02-27 16:34:05,146 DEBUG TRAIN Batch 47/1200 loss 5.574436 loss_att 6.976728 loss_ctc 10.977920 loss_rnnt 4.436603 hw_loss 0.256707 lr 0.00029562 rank 6
2023-02-27 16:34:05,146 DEBUG TRAIN Batch 47/1200 loss 4.286215 loss_att 6.523985 loss_ctc 8.001254 loss_rnnt 3.244781 hw_loss 0.184764 lr 0.00029563 rank 4
2023-02-27 16:34:43,212 DEBUG TRAIN Batch 47/1300 loss 5.655030 loss_att 8.826347 loss_ctc 9.654049 loss_rnnt 4.320612 hw_loss 0.313035 lr 0.00029561 rank 3
2023-02-27 16:34:43,214 DEBUG TRAIN Batch 47/1300 loss 3.649357 loss_att 5.376605 loss_ctc 4.974344 loss_rnnt 2.956593 hw_loss 0.319966 lr 0.00029562 rank 5
2023-02-27 16:34:43,216 DEBUG TRAIN Batch 47/1300 loss 5.991008 loss_att 9.902320 loss_ctc 10.657672 loss_rnnt 4.584167 hw_loss 0.004421 lr 0.00029562 rank 0
2023-02-27 16:34:43,217 DEBUG TRAIN Batch 47/1300 loss 7.087944 loss_att 11.683601 loss_ctc 13.313743 loss_rnnt 5.188759 hw_loss 0.281151 lr 0.00029561 rank 6
2023-02-27 16:34:43,218 DEBUG TRAIN Batch 47/1300 loss 3.236303 loss_att 6.528499 loss_ctc 7.102448 loss_rnnt 1.981651 hw_loss 0.151364 lr 0.00029562 rank 2
2023-02-27 16:34:43,221 DEBUG TRAIN Batch 47/1300 loss 6.568671 loss_att 10.535551 loss_ctc 13.481242 loss_rnnt 4.728825 hw_loss 0.233987 lr 0.00029562 rank 1
2023-02-27 16:34:43,258 DEBUG TRAIN Batch 47/1300 loss 4.505051 loss_att 7.226817 loss_ctc 8.004294 loss_rnnt 3.377203 hw_loss 0.219242 lr 0.00029562 rank 4
2023-02-27 16:34:43,290 DEBUG TRAIN Batch 47/1300 loss 5.699288 loss_att 9.853313 loss_ctc 10.576375 loss_rnnt 4.082054 hw_loss 0.255285 lr 0.00029561 rank 7
2023-02-27 16:35:22,742 DEBUG TRAIN Batch 47/1400 loss 7.673271 loss_att 8.845395 loss_ctc 9.360809 loss_rnnt 6.968796 hw_loss 0.459460 lr 0.00029560 rank 4
2023-02-27 16:35:22,750 DEBUG TRAIN Batch 47/1400 loss 5.877070 loss_att 7.608719 loss_ctc 11.427273 loss_rnnt 4.620985 hw_loss 0.318240 lr 0.00029560 rank 6
2023-02-27 16:35:22,755 DEBUG TRAIN Batch 47/1400 loss 7.596686 loss_att 10.685268 loss_ctc 14.178122 loss_rnnt 6.084535 hw_loss 0.031706 lr 0.00029560 rank 3
2023-02-27 16:35:22,755 DEBUG TRAIN Batch 47/1400 loss 3.776566 loss_att 6.791166 loss_ctc 5.356224 loss_rnnt 2.800532 hw_loss 0.304673 lr 0.00029560 rank 0
2023-02-27 16:35:22,757 DEBUG TRAIN Batch 47/1400 loss 0.989120 loss_att 2.785435 loss_ctc 1.204725 loss_rnnt 0.580117 hw_loss 0.039361 lr 0.00029560 rank 2
2023-02-27 16:35:22,757 DEBUG TRAIN Batch 47/1400 loss 4.092424 loss_att 7.096676 loss_ctc 9.030349 loss_rnnt 2.684075 hw_loss 0.279579 lr 0.00029561 rank 5
2023-02-27 16:35:22,807 DEBUG TRAIN Batch 47/1400 loss 5.119072 loss_att 7.570171 loss_ctc 10.811599 loss_rnnt 3.717527 hw_loss 0.285603 lr 0.00029561 rank 1
2023-02-27 16:35:22,828 DEBUG TRAIN Batch 47/1400 loss 7.137664 loss_att 9.452768 loss_ctc 10.313072 loss_rnnt 6.123969 hw_loss 0.238662 lr 0.00029560 rank 7
2023-02-27 16:36:26,827 DEBUG TRAIN Batch 47/1500 loss 9.044710 loss_att 12.216063 loss_ctc 16.030294 loss_rnnt 7.420570 hw_loss 0.109608 lr 0.00029559 rank 3
2023-02-27 16:36:26,841 DEBUG TRAIN Batch 47/1500 loss 5.467681 loss_att 9.572402 loss_ctc 8.552382 loss_rnnt 4.173295 hw_loss 0.116528 lr 0.00029559 rank 5
2023-02-27 16:36:26,841 DEBUG TRAIN Batch 47/1500 loss 5.896988 loss_att 7.435481 loss_ctc 11.664106 loss_rnnt 4.721824 hw_loss 0.184717 lr 0.00029559 rank 0
2023-02-27 16:36:26,844 DEBUG TRAIN Batch 47/1500 loss 3.741416 loss_att 5.409050 loss_ctc 3.857622 loss_rnnt 3.301310 hw_loss 0.170784 lr 0.00029559 rank 4
2023-02-27 16:36:26,844 DEBUG TRAIN Batch 47/1500 loss 2.722771 loss_att 5.262184 loss_ctc 3.183576 loss_rnnt 1.996150 hw_loss 0.294932 lr 0.00029559 rank 6
2023-02-27 16:36:26,848 DEBUG TRAIN Batch 47/1500 loss 1.919141 loss_att 5.034185 loss_ctc 3.814447 loss_rnnt 1.042559 hw_loss 0.001624 lr 0.00029558 rank 7
2023-02-27 16:36:26,850 DEBUG TRAIN Batch 47/1500 loss 3.967129 loss_att 5.659984 loss_ctc 5.272850 loss_rnnt 3.335817 hw_loss 0.222460 lr 0.00029559 rank 1
2023-02-27 16:36:26,893 DEBUG TRAIN Batch 47/1500 loss 5.742717 loss_att 9.215604 loss_ctc 10.605533 loss_rnnt 4.271160 hw_loss 0.241131 lr 0.00029559 rank 2
2023-02-27 16:37:04,914 DEBUG TRAIN Batch 47/1600 loss 6.353819 loss_att 9.178676 loss_ctc 12.144096 loss_rnnt 4.789928 hw_loss 0.425403 lr 0.00029557 rank 3
2023-02-27 16:37:04,930 DEBUG TRAIN Batch 47/1600 loss 5.963964 loss_att 8.388860 loss_ctc 10.729011 loss_rnnt 4.676439 hw_loss 0.313511 lr 0.00029558 rank 0
2023-02-27 16:37:04,934 DEBUG TRAIN Batch 47/1600 loss 5.723199 loss_att 10.049419 loss_ctc 7.916153 loss_rnnt 4.486902 hw_loss 0.147485 lr 0.00029558 rank 2
2023-02-27 16:37:04,938 DEBUG TRAIN Batch 47/1600 loss 3.142635 loss_att 5.140990 loss_ctc 4.213078 loss_rnnt 2.451910 hw_loss 0.278116 lr 0.00029557 rank 6
2023-02-27 16:37:04,939 DEBUG TRAIN Batch 47/1600 loss 4.530852 loss_att 7.859985 loss_ctc 10.775424 loss_rnnt 2.978313 hw_loss 0.101443 lr 0.00029558 rank 4
2023-02-27 16:37:04,939 DEBUG TRAIN Batch 47/1600 loss 9.093053 loss_att 9.969517 loss_ctc 13.424906 loss_rnnt 8.209249 hw_loss 0.245494 lr 0.00029558 rank 5
2023-02-27 16:37:04,941 DEBUG TRAIN Batch 47/1600 loss 4.179301 loss_att 7.610110 loss_ctc 8.494166 loss_rnnt 2.870051 hw_loss 0.089572 lr 0.00029558 rank 1
2023-02-27 16:37:04,983 DEBUG TRAIN Batch 47/1600 loss 4.215115 loss_att 8.070901 loss_ctc 10.562363 loss_rnnt 2.460058 hw_loss 0.257999 lr 0.00029557 rank 7
2023-02-27 16:37:44,414 DEBUG TRAIN Batch 47/1700 loss 3.779102 loss_att 6.446472 loss_ctc 6.341825 loss_rnnt 2.765042 hw_loss 0.260419 lr 0.00029557 rank 5
2023-02-27 16:37:44,421 DEBUG TRAIN Batch 47/1700 loss 6.941897 loss_att 9.450945 loss_ctc 12.523628 loss_rnnt 5.525531 hw_loss 0.319363 lr 0.00029556 rank 3
2023-02-27 16:37:44,425 DEBUG TRAIN Batch 47/1700 loss 7.533433 loss_att 10.394020 loss_ctc 9.738161 loss_rnnt 6.522805 hw_loss 0.271027 lr 0.00029556 rank 0
2023-02-27 16:37:44,427 DEBUG TRAIN Batch 47/1700 loss 15.274698 loss_att 19.454674 loss_ctc 23.610840 loss_rnnt 13.208481 hw_loss 0.222630 lr 0.00029556 rank 6
2023-02-27 16:37:44,429 DEBUG TRAIN Batch 47/1700 loss 4.135774 loss_att 8.176464 loss_ctc 7.338207 loss_rnnt 2.756330 hw_loss 0.270590 lr 0.00029556 rank 4
2023-02-27 16:37:44,439 DEBUG TRAIN Batch 47/1700 loss 9.173306 loss_att 13.338197 loss_ctc 16.464432 loss_rnnt 7.268414 hw_loss 0.187058 lr 0.00029557 rank 1
2023-02-27 16:37:44,447 DEBUG TRAIN Batch 47/1700 loss 6.387934 loss_att 10.381967 loss_ctc 10.194016 loss_rnnt 4.974106 hw_loss 0.201643 lr 0.00029556 rank 7
2023-02-27 16:37:44,455 DEBUG TRAIN Batch 47/1700 loss 3.889077 loss_att 6.646378 loss_ctc 9.222084 loss_rnnt 2.469503 hw_loss 0.294462 lr 0.00029556 rank 2
2023-02-27 16:38:50,359 DEBUG TRAIN Batch 47/1800 loss 7.296729 loss_att 11.499246 loss_ctc 14.211265 loss_rnnt 5.401873 hw_loss 0.248276 lr 0.00029555 rank 6
2023-02-27 16:38:50,361 DEBUG TRAIN Batch 47/1800 loss 7.617977 loss_att 9.757553 loss_ctc 14.768277 loss_rnnt 6.086115 hw_loss 0.282323 lr 0.00029555 rank 5
2023-02-27 16:38:50,363 DEBUG TRAIN Batch 47/1800 loss 8.229916 loss_att 11.352440 loss_ctc 17.123161 loss_rnnt 6.262959 hw_loss 0.293784 lr 0.00029554 rank 7
2023-02-27 16:38:50,364 DEBUG TRAIN Batch 47/1800 loss 5.222538 loss_att 7.560306 loss_ctc 9.974751 loss_rnnt 3.978702 hw_loss 0.267475 lr 0.00029555 rank 2
2023-02-27 16:38:50,373 DEBUG TRAIN Batch 47/1800 loss 6.550367 loss_att 9.110889 loss_ctc 9.361404 loss_rnnt 5.600636 hw_loss 0.117790 lr 0.00029555 rank 1
2023-02-27 16:38:50,378 DEBUG TRAIN Batch 47/1800 loss 4.571698 loss_att 5.897689 loss_ctc 7.787125 loss_rnnt 3.696229 hw_loss 0.340402 lr 0.00029555 rank 0
2023-02-27 16:38:50,385 DEBUG TRAIN Batch 47/1800 loss 6.465189 loss_att 8.883318 loss_ctc 10.108944 loss_rnnt 5.297633 hw_loss 0.371433 lr 0.00029555 rank 3
2023-02-27 16:38:50,406 DEBUG TRAIN Batch 47/1800 loss 5.285441 loss_att 7.539043 loss_ctc 9.411880 loss_rnnt 4.180024 hw_loss 0.195945 lr 0.00029555 rank 4
2023-02-27 16:39:29,320 DEBUG TRAIN Batch 47/1900 loss 3.573336 loss_att 4.237681 loss_ctc 5.308058 loss_rnnt 2.963069 hw_loss 0.461439 lr 0.00029554 rank 3
2023-02-27 16:39:29,320 DEBUG TRAIN Batch 47/1900 loss 2.313579 loss_att 7.610667 loss_ctc 6.082638 loss_rnnt 0.596174 hw_loss 0.291462 lr 0.00029554 rank 0
2023-02-27 16:39:29,322 DEBUG TRAIN Batch 47/1900 loss 2.329583 loss_att 5.645918 loss_ctc 5.486270 loss_rnnt 1.166087 hw_loss 0.148758 lr 0.00029554 rank 5
2023-02-27 16:39:29,324 DEBUG TRAIN Batch 47/1900 loss 11.766529 loss_att 15.319447 loss_ctc 19.791613 loss_rnnt 9.896946 hw_loss 0.166851 lr 0.00029553 rank 6
2023-02-27 16:39:29,326 DEBUG TRAIN Batch 47/1900 loss 7.595099 loss_att 8.661289 loss_ctc 11.355370 loss_rnnt 6.689095 hw_loss 0.358870 lr 0.00029554 rank 4
2023-02-27 16:39:29,328 DEBUG TRAIN Batch 47/1900 loss 9.608595 loss_att 12.509119 loss_ctc 16.059204 loss_rnnt 8.036829 hw_loss 0.246714 lr 0.00029553 rank 7
2023-02-27 16:39:29,329 DEBUG TRAIN Batch 47/1900 loss 5.260064 loss_att 8.854978 loss_ctc 8.383750 loss_rnnt 3.945617 hw_loss 0.335574 lr 0.00029554 rank 2
2023-02-27 16:39:29,329 DEBUG TRAIN Batch 47/1900 loss 3.895976 loss_att 6.782825 loss_ctc 7.181757 loss_rnnt 2.799428 hw_loss 0.152015 lr 0.00029554 rank 1
2023-02-27 16:40:07,740 DEBUG TRAIN Batch 47/2000 loss 2.745603 loss_att 5.176919 loss_ctc 3.231913 loss_rnnt 2.105361 hw_loss 0.167132 lr 0.00029553 rank 0
2023-02-27 16:40:07,741 DEBUG TRAIN Batch 47/2000 loss 8.037993 loss_att 10.158987 loss_ctc 14.843612 loss_rnnt 6.597814 hw_loss 0.203560 lr 0.00029552 rank 7
2023-02-27 16:40:07,741 DEBUG TRAIN Batch 47/2000 loss 9.692348 loss_att 11.962815 loss_ctc 13.889426 loss_rnnt 8.520882 hw_loss 0.295804 lr 0.00029553 rank 1
2023-02-27 16:40:07,743 DEBUG TRAIN Batch 47/2000 loss 4.573387 loss_att 7.886068 loss_ctc 6.606493 loss_rnnt 3.530590 hw_loss 0.204713 lr 0.00029552 rank 3
2023-02-27 16:40:07,744 DEBUG TRAIN Batch 47/2000 loss 5.453318 loss_att 6.590721 loss_ctc 8.014257 loss_rnnt 4.813172 hw_loss 0.133512 lr 0.00029552 rank 6
2023-02-27 16:40:07,744 DEBUG TRAIN Batch 47/2000 loss 2.427818 loss_att 4.833188 loss_ctc 4.522859 loss_rnnt 1.537147 hw_loss 0.244235 lr 0.00029553 rank 2
2023-02-27 16:40:07,750 DEBUG TRAIN Batch 47/2000 loss 3.376357 loss_att 7.117806 loss_ctc 6.526326 loss_rnnt 2.045753 hw_loss 0.304346 lr 0.00029553 rank 5
2023-02-27 16:40:07,762 DEBUG TRAIN Batch 47/2000 loss 4.190261 loss_att 9.213987 loss_ctc 10.121902 loss_rnnt 2.247316 hw_loss 0.276213 lr 0.00029553 rank 4
2023-02-27 16:40:47,356 DEBUG TRAIN Batch 47/2100 loss 4.612059 loss_att 10.974009 loss_ctc 6.864816 loss_rnnt 2.926258 hw_loss 0.211957 lr 0.00029551 rank 4
2023-02-27 16:40:47,357 DEBUG TRAIN Batch 47/2100 loss 11.515688 loss_att 17.729286 loss_ctc 23.142673 loss_rnnt 8.585007 hw_loss 0.258182 lr 0.00029552 rank 5
2023-02-27 16:40:47,365 DEBUG TRAIN Batch 47/2100 loss 11.587180 loss_att 14.125433 loss_ctc 18.162270 loss_rnnt 10.115635 hw_loss 0.163528 lr 0.00029551 rank 0
2023-02-27 16:40:47,369 DEBUG TRAIN Batch 47/2100 loss 4.137813 loss_att 6.850890 loss_ctc 6.984743 loss_rnnt 3.137298 hw_loss 0.146829 lr 0.00029552 rank 1
2023-02-27 16:40:47,371 DEBUG TRAIN Batch 47/2100 loss 9.311771 loss_att 12.508294 loss_ctc 12.639019 loss_rnnt 8.082457 hw_loss 0.274456 lr 0.00029551 rank 3
2023-02-27 16:40:47,372 DEBUG TRAIN Batch 47/2100 loss 6.072211 loss_att 8.453552 loss_ctc 7.760549 loss_rnnt 5.174139 hw_loss 0.368798 lr 0.00029551 rank 2
2023-02-27 16:40:47,375 DEBUG TRAIN Batch 47/2100 loss 13.754977 loss_att 17.902601 loss_ctc 22.574020 loss_rnnt 11.570305 hw_loss 0.336140 lr 0.00029551 rank 6
2023-02-27 16:40:47,409 DEBUG TRAIN Batch 47/2100 loss 6.262436 loss_att 10.524979 loss_ctc 11.555999 loss_rnnt 4.571719 hw_loss 0.248250 lr 0.00029551 rank 7
2023-02-27 16:41:52,004 DEBUG TRAIN Batch 47/2200 loss 0.814881 loss_att 3.189893 loss_ctc 0.542883 loss_rnnt 0.311348 hw_loss 0.121494 lr 0.00029550 rank 0
2023-02-27 16:41:52,005 DEBUG TRAIN Batch 47/2200 loss 9.133717 loss_att 11.816216 loss_ctc 12.822916 loss_rnnt 7.931570 hw_loss 0.325787 lr 0.00029550 rank 3
2023-02-27 16:41:52,007 DEBUG TRAIN Batch 47/2200 loss 8.647978 loss_att 11.483727 loss_ctc 12.565059 loss_rnnt 7.441724 hw_loss 0.219050 lr 0.00029549 rank 7
2023-02-27 16:41:52,008 DEBUG TRAIN Batch 47/2200 loss 3.911407 loss_att 5.983734 loss_ctc 11.019837 loss_rnnt 2.484852 hw_loss 0.120562 lr 0.00029550 rank 6
2023-02-27 16:41:52,008 DEBUG TRAIN Batch 47/2200 loss 7.364184 loss_att 10.638277 loss_ctc 10.207790 loss_rnnt 6.181347 hw_loss 0.279131 lr 0.00029550 rank 1
2023-02-27 16:41:52,008 DEBUG TRAIN Batch 47/2200 loss 9.655608 loss_att 13.548484 loss_ctc 15.367653 loss_rnnt 8.012489 hw_loss 0.193010 lr 0.00029550 rank 4
2023-02-27 16:41:52,009 DEBUG TRAIN Batch 47/2200 loss 7.115754 loss_att 9.953449 loss_ctc 13.105524 loss_rnnt 5.636141 hw_loss 0.212694 lr 0.00029550 rank 2
2023-02-27 16:41:52,013 DEBUG TRAIN Batch 47/2200 loss 6.279781 loss_att 10.082414 loss_ctc 9.196127 loss_rnnt 4.984818 hw_loss 0.272982 lr 0.00029550 rank 5
2023-02-27 16:42:29,951 DEBUG TRAIN Batch 47/2300 loss 11.550810 loss_att 12.427707 loss_ctc 15.413963 loss_rnnt 10.757986 hw_loss 0.191922 lr 0.00029548 rank 6
2023-02-27 16:42:29,953 DEBUG TRAIN Batch 47/2300 loss 6.380124 loss_att 9.663633 loss_ctc 11.584563 loss_rnnt 4.930887 hw_loss 0.184895 lr 0.00029549 rank 5
2023-02-27 16:42:29,953 DEBUG TRAIN Batch 47/2300 loss 6.960357 loss_att 10.626031 loss_ctc 9.380934 loss_rnnt 5.779387 hw_loss 0.234547 lr 0.00029548 rank 3
2023-02-27 16:42:29,954 DEBUG TRAIN Batch 47/2300 loss 6.898886 loss_att 10.093663 loss_ctc 11.161165 loss_rnnt 5.578703 hw_loss 0.211730 lr 0.00029549 rank 0
2023-02-27 16:42:29,956 DEBUG TRAIN Batch 47/2300 loss 4.496483 loss_att 7.162221 loss_ctc 6.483846 loss_rnnt 3.589169 hw_loss 0.204720 lr 0.00029548 rank 7
2023-02-27 16:42:29,960 DEBUG TRAIN Batch 47/2300 loss 7.063485 loss_att 8.616487 loss_ctc 11.228243 loss_rnnt 6.065546 hw_loss 0.247571 lr 0.00029549 rank 4
2023-02-27 16:42:29,961 DEBUG TRAIN Batch 47/2300 loss 1.782490 loss_att 4.376675 loss_ctc 4.279561 loss_rnnt 0.845149 hw_loss 0.160427 lr 0.00029549 rank 1
2023-02-27 16:42:29,962 DEBUG TRAIN Batch 47/2300 loss 6.576138 loss_att 7.621960 loss_ctc 9.259502 loss_rnnt 5.942030 hw_loss 0.125929 lr 0.00029549 rank 2
2023-02-27 16:43:09,266 DEBUG TRAIN Batch 47/2400 loss 2.492557 loss_att 5.698814 loss_ctc 6.002251 loss_rnnt 1.233777 hw_loss 0.280442 lr 0.00029547 rank 6
2023-02-27 16:43:09,268 DEBUG TRAIN Batch 47/2400 loss 5.577620 loss_att 8.102060 loss_ctc 11.797943 loss_rnnt 4.075753 hw_loss 0.314253 lr 0.00029548 rank 5
2023-02-27 16:43:09,268 DEBUG TRAIN Batch 47/2400 loss 11.160891 loss_att 12.875051 loss_ctc 13.748053 loss_rnnt 10.313757 hw_loss 0.298775 lr 0.00029547 rank 4
2023-02-27 16:43:09,269 DEBUG TRAIN Batch 47/2400 loss 7.242487 loss_att 9.826965 loss_ctc 13.397376 loss_rnnt 5.727445 hw_loss 0.332803 lr 0.00029547 rank 3
2023-02-27 16:43:09,271 DEBUG TRAIN Batch 47/2400 loss 6.479359 loss_att 9.833078 loss_ctc 12.112269 loss_rnnt 4.928163 hw_loss 0.242621 lr 0.00029547 rank 7
2023-02-27 16:43:09,273 DEBUG TRAIN Batch 47/2400 loss 3.121945 loss_att 6.312352 loss_ctc 7.461096 loss_rnnt 1.753687 hw_loss 0.284294 lr 0.00029548 rank 1
2023-02-27 16:43:09,278 DEBUG TRAIN Batch 47/2400 loss 5.748580 loss_att 8.744690 loss_ctc 9.654633 loss_rnnt 4.488344 hw_loss 0.262889 lr 0.00029547 rank 0
2023-02-27 16:43:09,282 DEBUG TRAIN Batch 47/2400 loss 4.264530 loss_att 5.908535 loss_ctc 5.796684 loss_rnnt 3.557915 hw_loss 0.325362 lr 0.00029547 rank 2
2023-02-27 16:44:16,644 DEBUG TRAIN Batch 47/2500 loss 5.902732 loss_att 9.821907 loss_ctc 5.648599 loss_rnnt 5.039420 hw_loss 0.212551 lr 0.00029546 rank 5
2023-02-27 16:44:16,649 DEBUG TRAIN Batch 47/2500 loss 6.401252 loss_att 9.217569 loss_ctc 11.514802 loss_rnnt 5.015281 hw_loss 0.264189 lr 0.00029546 rank 3
2023-02-27 16:44:16,653 DEBUG TRAIN Batch 47/2500 loss 5.509247 loss_att 6.765395 loss_ctc 10.309229 loss_rnnt 4.514609 hw_loss 0.193896 lr 0.00029546 rank 4
2023-02-27 16:44:16,658 DEBUG TRAIN Batch 47/2500 loss 5.534749 loss_att 8.292864 loss_ctc 9.186241 loss_rnnt 4.334161 hw_loss 0.303935 lr 0.00029546 rank 0
2023-02-27 16:44:16,660 DEBUG TRAIN Batch 47/2500 loss 4.659459 loss_att 6.971957 loss_ctc 7.125181 loss_rnnt 3.745652 hw_loss 0.229770 lr 0.00029546 rank 6
2023-02-27 16:44:16,663 DEBUG TRAIN Batch 47/2500 loss 6.534236 loss_att 7.623944 loss_ctc 8.901040 loss_rnnt 5.811907 hw_loss 0.354026 lr 0.00029546 rank 2
2023-02-27 16:44:16,681 DEBUG TRAIN Batch 47/2500 loss 4.627964 loss_att 6.957316 loss_ctc 8.878107 loss_rnnt 3.482259 hw_loss 0.212156 lr 0.00029545 rank 7
2023-02-27 16:44:16,682 DEBUG TRAIN Batch 47/2500 loss 3.364278 loss_att 6.903395 loss_ctc 5.491634 loss_rnnt 2.278873 hw_loss 0.176126 lr 0.00029546 rank 1
2023-02-27 16:44:55,112 DEBUG TRAIN Batch 47/2600 loss 3.638227 loss_att 6.901866 loss_ctc 7.733307 loss_rnnt 2.301298 hw_loss 0.259108 lr 0.00029545 rank 4
2023-02-27 16:44:55,122 DEBUG TRAIN Batch 47/2600 loss 5.816946 loss_att 10.116819 loss_ctc 9.726620 loss_rnnt 4.377494 hw_loss 0.109101 lr 0.00029544 rank 6
2023-02-27 16:44:55,124 DEBUG TRAIN Batch 47/2600 loss 3.840751 loss_att 6.860056 loss_ctc 5.365020 loss_rnnt 2.930909 hw_loss 0.192649 lr 0.00029545 rank 5
2023-02-27 16:44:55,127 DEBUG TRAIN Batch 47/2600 loss 5.002948 loss_att 8.287521 loss_ctc 8.409109 loss_rnnt 3.687168 hw_loss 0.383832 lr 0.00029545 rank 0
2023-02-27 16:44:55,128 DEBUG TRAIN Batch 47/2600 loss 3.544557 loss_att 7.096020 loss_ctc 4.471918 loss_rnnt 2.569490 hw_loss 0.264612 lr 0.00029545 rank 3
2023-02-27 16:44:55,132 DEBUG TRAIN Batch 47/2600 loss 5.230559 loss_att 8.558346 loss_ctc 6.573419 loss_rnnt 4.256965 hw_loss 0.241854 lr 0.00029545 rank 1
2023-02-27 16:44:55,134 DEBUG TRAIN Batch 47/2600 loss 2.374398 loss_att 5.599317 loss_ctc 5.785429 loss_rnnt 1.150011 hw_loss 0.233622 lr 0.00029545 rank 2
2023-02-27 16:44:55,135 DEBUG TRAIN Batch 47/2600 loss 3.885744 loss_att 8.768684 loss_ctc 9.693676 loss_rnnt 2.029519 hw_loss 0.197337 lr 0.00029544 rank 7
2023-02-27 16:45:34,154 DEBUG TRAIN Batch 47/2700 loss 5.868927 loss_att 8.740223 loss_ctc 10.060249 loss_rnnt 4.684430 hw_loss 0.096366 lr 0.00029544 rank 0
2023-02-27 16:45:34,156 DEBUG TRAIN Batch 47/2700 loss 6.831274 loss_att 11.066650 loss_ctc 14.447508 loss_rnnt 4.836299 hw_loss 0.248254 lr 0.00029543 rank 3
2023-02-27 16:45:34,157 DEBUG TRAIN Batch 47/2700 loss 2.369675 loss_att 3.762571 loss_ctc 3.589821 loss_rnnt 1.698283 hw_loss 0.431486 lr 0.00029544 rank 2
2023-02-27 16:45:34,158 DEBUG TRAIN Batch 47/2700 loss 5.014415 loss_att 7.287036 loss_ctc 8.170374 loss_rnnt 4.121944 hw_loss 0.032159 lr 0.00029543 rank 6
2023-02-27 16:45:34,160 DEBUG TRAIN Batch 47/2700 loss 1.889520 loss_att 3.476244 loss_ctc 2.105453 loss_rnnt 1.327645 hw_loss 0.404510 lr 0.00029544 rank 4
2023-02-27 16:45:34,163 DEBUG TRAIN Batch 47/2700 loss 3.506719 loss_att 5.300470 loss_ctc 6.158838 loss_rnnt 2.735218 hw_loss 0.110878 lr 0.00029544 rank 5
2023-02-27 16:45:34,164 DEBUG TRAIN Batch 47/2700 loss 10.393851 loss_att 14.412601 loss_ctc 16.113523 loss_rnnt 8.670183 hw_loss 0.294929 lr 0.00029543 rank 7
2023-02-27 16:45:34,211 DEBUG TRAIN Batch 47/2700 loss 1.188654 loss_att 2.599119 loss_ctc 3.826966 loss_rnnt 0.404476 hw_loss 0.281831 lr 0.00029544 rank 1
2023-02-27 16:46:13,326 DEBUG TRAIN Batch 47/2800 loss 1.980998 loss_att 4.309784 loss_ctc 4.056604 loss_rnnt 1.062247 hw_loss 0.330463 lr 0.00029542 rank 2
2023-02-27 16:46:13,330 DEBUG TRAIN Batch 47/2800 loss 8.941410 loss_att 11.673349 loss_ctc 10.436291 loss_rnnt 8.123957 hw_loss 0.134528 lr 0.00029542 rank 1
2023-02-27 16:46:13,331 DEBUG TRAIN Batch 47/2800 loss 3.380733 loss_att 5.946078 loss_ctc 6.625546 loss_rnnt 2.253668 hw_loss 0.340039 lr 0.00029542 rank 3
2023-02-27 16:46:13,333 DEBUG TRAIN Batch 47/2800 loss 5.469138 loss_att 9.534270 loss_ctc 9.912483 loss_rnnt 3.941295 hw_loss 0.229443 lr 0.00029542 rank 0
2023-02-27 16:46:13,333 DEBUG TRAIN Batch 47/2800 loss 8.775946 loss_att 9.469185 loss_ctc 10.329508 loss_rnnt 8.353580 hw_loss 0.143580 lr 0.00029542 rank 6
2023-02-27 16:46:13,334 DEBUG TRAIN Batch 47/2800 loss 5.552245 loss_att 10.093427 loss_ctc 10.202782 loss_rnnt 3.931651 hw_loss 0.173035 lr 0.00029542 rank 4
2023-02-27 16:46:13,360 DEBUG TRAIN Batch 47/2800 loss 4.223579 loss_att 7.011354 loss_ctc 7.901356 loss_rnnt 3.041033 hw_loss 0.252414 lr 0.00029543 rank 5
2023-02-27 16:46:13,362 DEBUG TRAIN Batch 47/2800 loss 8.827303 loss_att 12.918382 loss_ctc 15.109835 loss_rnnt 7.053892 hw_loss 0.220357 lr 0.00029542 rank 7
2023-02-27 16:47:19,787 DEBUG TRAIN Batch 47/2900 loss 6.334392 loss_att 9.223642 loss_ctc 10.947570 loss_rnnt 5.067706 hw_loss 0.138274 lr 0.00029540 rank 6
2023-02-27 16:47:19,789 DEBUG TRAIN Batch 47/2900 loss 7.651062 loss_att 8.608932 loss_ctc 9.877207 loss_rnnt 7.064096 hw_loss 0.184822 lr 0.00029541 rank 2
2023-02-27 16:47:19,791 DEBUG TRAIN Batch 47/2900 loss 4.085504 loss_att 7.649587 loss_ctc 8.635565 loss_rnnt 2.672976 hw_loss 0.174445 lr 0.00029541 rank 0
2023-02-27 16:47:19,791 DEBUG TRAIN Batch 47/2900 loss 3.237275 loss_att 5.218154 loss_ctc 5.052863 loss_rnnt 2.473051 hw_loss 0.236194 lr 0.00029541 rank 3
2023-02-27 16:47:19,795 DEBUG TRAIN Batch 47/2900 loss 7.086994 loss_att 8.956113 loss_ctc 9.911220 loss_rnnt 6.177335 hw_loss 0.298634 lr 0.00029541 rank 1
2023-02-27 16:47:19,802 DEBUG TRAIN Batch 47/2900 loss 3.815993 loss_att 8.020708 loss_ctc 8.898973 loss_rnnt 2.117047 hw_loss 0.338010 lr 0.00029540 rank 7
2023-02-27 16:47:19,811 DEBUG TRAIN Batch 47/2900 loss 2.683319 loss_att 5.746487 loss_ctc 5.315166 loss_rnnt 1.557281 hw_loss 0.304672 lr 0.00029541 rank 5
2023-02-27 16:47:19,820 DEBUG TRAIN Batch 47/2900 loss 6.666547 loss_att 9.272570 loss_ctc 13.348463 loss_rnnt 5.140124 hw_loss 0.214305 lr 0.00029541 rank 4
2023-02-27 16:47:58,489 DEBUG TRAIN Batch 47/3000 loss 4.366934 loss_att 7.311045 loss_ctc 8.805594 loss_rnnt 3.065270 hw_loss 0.226913 lr 0.00029539 rank 3
2023-02-27 16:47:58,506 DEBUG TRAIN Batch 47/3000 loss 3.984053 loss_att 6.388884 loss_ctc 7.462201 loss_rnnt 2.857549 hw_loss 0.340845 lr 0.00029539 rank 6
2023-02-27 16:47:58,509 DEBUG TRAIN Batch 47/3000 loss 2.237897 loss_att 4.935064 loss_ctc 3.796474 loss_rnnt 1.392387 hw_loss 0.184250 lr 0.00029540 rank 4
2023-02-27 16:47:58,513 DEBUG TRAIN Batch 47/3000 loss 8.358611 loss_att 12.739286 loss_ctc 14.306313 loss_rnnt 6.546501 hw_loss 0.268027 lr 0.00029539 rank 7
2023-02-27 16:47:58,516 DEBUG TRAIN Batch 47/3000 loss 3.579285 loss_att 4.699494 loss_ctc 4.028520 loss_rnnt 3.126291 hw_loss 0.316978 lr 0.00029540 rank 5
2023-02-27 16:47:58,516 DEBUG TRAIN Batch 47/3000 loss 5.606191 loss_att 7.573042 loss_ctc 11.202760 loss_rnnt 4.321195 hw_loss 0.272657 lr 0.00029540 rank 2
2023-02-27 16:47:58,517 DEBUG TRAIN Batch 47/3000 loss 10.302050 loss_att 13.450455 loss_ctc 14.640256 loss_rnnt 8.942351 hw_loss 0.284232 lr 0.00029540 rank 0
2023-02-27 16:47:58,565 DEBUG TRAIN Batch 47/3000 loss 3.944883 loss_att 6.864972 loss_ctc 6.798950 loss_rnnt 2.824412 hw_loss 0.292334 lr 0.00029540 rank 1
2023-02-27 16:48:37,667 DEBUG TRAIN Batch 47/3100 loss 5.796144 loss_att 7.350468 loss_ctc 8.983120 loss_rnnt 4.912518 hw_loss 0.277183 lr 0.00029538 rank 3
2023-02-27 16:48:37,678 DEBUG TRAIN Batch 47/3100 loss 8.023404 loss_att 9.205331 loss_ctc 12.483238 loss_rnnt 7.059824 hw_loss 0.248530 lr 0.00029539 rank 5
2023-02-27 16:48:37,680 DEBUG TRAIN Batch 47/3100 loss 5.760633 loss_att 11.772211 loss_ctc 8.835390 loss_rnnt 4.044937 hw_loss 0.193899 lr 0.00029538 rank 0
2023-02-27 16:48:37,680 DEBUG TRAIN Batch 47/3100 loss 6.424032 loss_att 10.387466 loss_ctc 11.404436 loss_rnnt 4.815258 hw_loss 0.285063 lr 0.00029538 rank 7
2023-02-27 16:48:37,681 DEBUG TRAIN Batch 47/3100 loss 8.862961 loss_att 10.377559 loss_ctc 16.898483 loss_rnnt 7.343456 hw_loss 0.272217 lr 0.00029538 rank 6
2023-02-27 16:48:37,683 DEBUG TRAIN Batch 47/3100 loss 5.464619 loss_att 8.692860 loss_ctc 10.594810 loss_rnnt 3.960519 hw_loss 0.327050 lr 0.00029538 rank 4
2023-02-27 16:48:37,689 DEBUG TRAIN Batch 47/3100 loss 7.672068 loss_att 9.571335 loss_ctc 12.505559 loss_rnnt 6.525510 hw_loss 0.229198 lr 0.00029538 rank 2
2023-02-27 16:48:37,691 DEBUG TRAIN Batch 47/3100 loss 9.909829 loss_att 10.613288 loss_ctc 14.915934 loss_rnnt 8.930861 hw_loss 0.320240 lr 0.00029539 rank 1
2023-02-27 16:49:40,211 DEBUG TRAIN Batch 47/3200 loss 7.098677 loss_att 10.344908 loss_ctc 8.575083 loss_rnnt 6.113148 hw_loss 0.261429 lr 0.00029537 rank 0
2023-02-27 16:49:40,221 DEBUG TRAIN Batch 47/3200 loss 3.871156 loss_att 5.099201 loss_ctc 5.370508 loss_rnnt 3.246914 hw_loss 0.335099 lr 0.00029537 rank 6
2023-02-27 16:49:40,223 DEBUG TRAIN Batch 47/3200 loss 7.731915 loss_att 8.837193 loss_ctc 12.196785 loss_rnnt 6.739336 hw_loss 0.330389 lr 0.00029537 rank 3
2023-02-27 16:49:40,223 DEBUG TRAIN Batch 47/3200 loss 3.156072 loss_att 6.254362 loss_ctc 8.960684 loss_rnnt 1.661388 hw_loss 0.189520 lr 0.00029537 rank 5
2023-02-27 16:49:40,228 DEBUG TRAIN Batch 47/3200 loss 5.511181 loss_att 7.928136 loss_ctc 6.299892 loss_rnnt 4.808107 hw_loss 0.214727 lr 0.00029537 rank 4
2023-02-27 16:49:40,234 DEBUG TRAIN Batch 47/3200 loss 6.777622 loss_att 9.927149 loss_ctc 19.235130 loss_rnnt 4.396347 hw_loss 0.169440 lr 0.00029537 rank 2
2023-02-27 16:49:40,235 DEBUG TRAIN Batch 47/3200 loss 5.640064 loss_att 7.167128 loss_ctc 9.544977 loss_rnnt 4.679610 hw_loss 0.251974 lr 0.00029536 rank 7
2023-02-27 16:49:40,236 DEBUG TRAIN Batch 47/3200 loss 4.694067 loss_att 7.971449 loss_ctc 9.564309 loss_rnnt 3.330247 hw_loss 0.110584 lr 0.00029537 rank 1
2023-02-27 16:50:23,129 DEBUG TRAIN Batch 47/3300 loss 13.976220 loss_att 17.303127 loss_ctc 28.736507 loss_rnnt 11.252510 hw_loss 0.169295 lr 0.00029536 rank 4
2023-02-27 16:50:23,140 DEBUG TRAIN Batch 47/3300 loss 2.468064 loss_att 4.968081 loss_ctc 3.786196 loss_rnnt 1.745465 hw_loss 0.087834 lr 0.00029536 rank 0
2023-02-27 16:50:23,139 DEBUG TRAIN Batch 47/3300 loss 6.328399 loss_att 10.516226 loss_ctc 11.134163 loss_rnnt 4.729994 hw_loss 0.225133 lr 0.00029535 rank 3
2023-02-27 16:50:23,141 DEBUG TRAIN Batch 47/3300 loss 4.677360 loss_att 6.767866 loss_ctc 6.244607 loss_rnnt 3.938530 hw_loss 0.209553 lr 0.00029536 rank 2
2023-02-27 16:50:23,141 DEBUG TRAIN Batch 47/3300 loss 11.923011 loss_att 15.458307 loss_ctc 19.050030 loss_rnnt 10.093697 hw_loss 0.322474 lr 0.00029535 rank 6
2023-02-27 16:50:23,142 DEBUG TRAIN Batch 47/3300 loss 6.624637 loss_att 9.947735 loss_ctc 10.083312 loss_rnnt 5.388216 hw_loss 0.207458 lr 0.00029536 rank 1
2023-02-27 16:50:23,154 DEBUG TRAIN Batch 47/3300 loss 6.975897 loss_att 8.740414 loss_ctc 9.031165 loss_rnnt 6.185014 hw_loss 0.307395 lr 0.00029536 rank 5
2023-02-27 16:50:23,186 DEBUG TRAIN Batch 47/3300 loss 8.363149 loss_att 13.090742 loss_ctc 15.266826 loss_rnnt 6.396413 hw_loss 0.188862 lr 0.00029535 rank 7
2023-02-27 16:51:01,884 DEBUG TRAIN Batch 47/3400 loss 8.271943 loss_att 11.522405 loss_ctc 11.574046 loss_rnnt 7.024325 hw_loss 0.294833 lr 0.00029534 rank 3
2023-02-27 16:51:01,885 DEBUG TRAIN Batch 47/3400 loss 2.347469 loss_att 4.333588 loss_ctc 4.826648 loss_rnnt 1.483316 hw_loss 0.255696 lr 0.00029534 rank 6
2023-02-27 16:51:01,886 DEBUG TRAIN Batch 47/3400 loss 4.404284 loss_att 8.558622 loss_ctc 9.274833 loss_rnnt 2.787117 hw_loss 0.256674 lr 0.00029535 rank 0
2023-02-27 16:51:01,886 DEBUG TRAIN Batch 47/3400 loss 3.153906 loss_att 5.956285 loss_ctc 5.466915 loss_rnnt 2.111668 hw_loss 0.325051 lr 0.00029535 rank 2
2023-02-27 16:51:01,888 DEBUG TRAIN Batch 47/3400 loss 7.625171 loss_att 10.791588 loss_ctc 9.936623 loss_rnnt 6.502174 hw_loss 0.340349 lr 0.00029535 rank 5
2023-02-27 16:51:01,889 DEBUG TRAIN Batch 47/3400 loss 4.429044 loss_att 8.062584 loss_ctc 10.113518 loss_rnnt 2.763662 hw_loss 0.338895 lr 0.00029535 rank 1
2023-02-27 16:51:01,892 DEBUG TRAIN Batch 47/3400 loss 6.376205 loss_att 7.984770 loss_ctc 10.631892 loss_rnnt 5.350068 hw_loss 0.256871 lr 0.00029535 rank 4
2023-02-27 16:51:01,899 DEBUG TRAIN Batch 47/3400 loss 0.974168 loss_att 3.065844 loss_ctc 0.796086 loss_rnnt 0.462869 hw_loss 0.218829 lr 0.00029534 rank 7
2023-02-27 16:51:41,290 DEBUG TRAIN Batch 47/3500 loss 2.233658 loss_att 5.759716 loss_ctc 4.640075 loss_rnnt 1.003697 hw_loss 0.382299 lr 0.00029534 rank 5
2023-02-27 16:51:41,291 DEBUG TRAIN Batch 47/3500 loss 11.813175 loss_att 17.897104 loss_ctc 19.782679 loss_rnnt 9.414411 hw_loss 0.223835 lr 0.00029533 rank 2
2023-02-27 16:51:41,305 DEBUG TRAIN Batch 47/3500 loss 7.954801 loss_att 12.154593 loss_ctc 13.122715 loss_rnnt 6.260315 hw_loss 0.310260 lr 0.00029533 rank 4
2023-02-27 16:51:41,306 DEBUG TRAIN Batch 47/3500 loss 11.097589 loss_att 13.770670 loss_ctc 21.713573 loss_rnnt 9.046103 hw_loss 0.190137 lr 0.00029533 rank 6
2023-02-27 16:51:41,310 DEBUG TRAIN Batch 47/3500 loss 5.717753 loss_att 8.368172 loss_ctc 8.444263 loss_rnnt 4.729332 hw_loss 0.177755 lr 0.00029533 rank 0
2023-02-27 16:51:41,310 DEBUG TRAIN Batch 47/3500 loss 3.349055 loss_att 7.838959 loss_ctc 8.544169 loss_rnnt 1.608391 hw_loss 0.281252 lr 0.00029533 rank 1
2023-02-27 16:51:41,321 DEBUG TRAIN Batch 47/3500 loss 8.238869 loss_att 11.125697 loss_ctc 15.212826 loss_rnnt 6.621486 hw_loss 0.206543 lr 0.00029533 rank 3
2023-02-27 16:51:41,325 DEBUG TRAIN Batch 47/3500 loss 3.949913 loss_att 6.100326 loss_ctc 5.559077 loss_rnnt 3.153289 hw_loss 0.284974 lr 0.00029533 rank 7
2023-02-27 16:52:47,365 DEBUG TRAIN Batch 47/3600 loss 1.621153 loss_att 3.380305 loss_ctc 2.893818 loss_rnnt 1.048917 hw_loss 0.095094 lr 0.00029532 rank 1
2023-02-27 16:52:47,376 DEBUG TRAIN Batch 47/3600 loss 4.082134 loss_att 7.017457 loss_ctc 6.319720 loss_rnnt 3.089876 hw_loss 0.200341 lr 0.00029532 rank 4
2023-02-27 16:52:47,377 DEBUG TRAIN Batch 47/3600 loss 6.704123 loss_att 8.094585 loss_ctc 15.223128 loss_rnnt 5.228865 hw_loss 0.114934 lr 0.00029532 rank 0
2023-02-27 16:52:47,377 DEBUG TRAIN Batch 47/3600 loss 11.126974 loss_att 14.069838 loss_ctc 18.308411 loss_rnnt 9.409390 hw_loss 0.321537 lr 0.00029532 rank 2
2023-02-27 16:52:47,377 DEBUG TRAIN Batch 47/3600 loss 8.685308 loss_att 11.912102 loss_ctc 16.626263 loss_rnnt 6.818527 hw_loss 0.304927 lr 0.00029532 rank 3
2023-02-27 16:52:47,377 DEBUG TRAIN Batch 47/3600 loss 6.340917 loss_att 10.188427 loss_ctc 13.089661 loss_rnnt 4.566734 hw_loss 0.196590 lr 0.00029531 rank 6
2023-02-27 16:52:47,377 DEBUG TRAIN Batch 47/3600 loss 5.015994 loss_att 7.047435 loss_ctc 9.636595 loss_rnnt 3.887192 hw_loss 0.199562 lr 0.00029532 rank 5
2023-02-27 16:52:47,380 DEBUG TRAIN Batch 47/3600 loss 2.576829 loss_att 5.986009 loss_ctc 4.131535 loss_rnnt 1.582759 hw_loss 0.196762 lr 0.00029531 rank 7
2023-02-27 16:53:26,164 DEBUG TRAIN Batch 47/3700 loss 7.299301 loss_att 10.547416 loss_ctc 12.241884 loss_rnnt 5.898948 hw_loss 0.171975 lr 0.00029531 rank 0
2023-02-27 16:53:26,167 DEBUG TRAIN Batch 47/3700 loss 4.890517 loss_att 6.079513 loss_ctc 7.936107 loss_rnnt 4.118401 hw_loss 0.240446 lr 0.00029530 rank 6
2023-02-27 16:53:26,171 DEBUG TRAIN Batch 47/3700 loss 7.127490 loss_att 11.763748 loss_ctc 15.039993 loss_rnnt 5.002735 hw_loss 0.267194 lr 0.00029531 rank 5
2023-02-27 16:53:26,173 DEBUG TRAIN Batch 47/3700 loss 4.431592 loss_att 8.758225 loss_ctc 9.773895 loss_rnnt 2.690121 hw_loss 0.307194 lr 0.00029530 rank 3
2023-02-27 16:53:26,174 DEBUG TRAIN Batch 47/3700 loss 8.369760 loss_att 9.553448 loss_ctc 15.207829 loss_rnnt 7.104801 hw_loss 0.218400 lr 0.00029531 rank 2
2023-02-27 16:53:26,175 DEBUG TRAIN Batch 47/3700 loss 8.722012 loss_att 11.500315 loss_ctc 15.486246 loss_rnnt 7.185797 hw_loss 0.147480 lr 0.00029531 rank 1
2023-02-27 16:53:26,177 DEBUG TRAIN Batch 47/3700 loss 4.677257 loss_att 8.031894 loss_ctc 10.234589 loss_rnnt 3.175438 hw_loss 0.168589 lr 0.00029531 rank 4
2023-02-27 16:53:26,231 DEBUG TRAIN Batch 47/3700 loss 7.993852 loss_att 10.528065 loss_ctc 14.648619 loss_rnnt 6.452503 hw_loss 0.276008 lr 0.00029530 rank 7
2023-02-27 16:54:05,089 DEBUG TRAIN Batch 47/3800 loss 8.365470 loss_att 13.297337 loss_ctc 14.052790 loss_rnnt 6.470027 hw_loss 0.282677 lr 0.00029529 rank 0
2023-02-27 16:54:05,101 DEBUG TRAIN Batch 47/3800 loss 4.293203 loss_att 6.118557 loss_ctc 8.007648 loss_rnnt 3.295934 hw_loss 0.256760 lr 0.00029529 rank 6
2023-02-27 16:54:05,101 DEBUG TRAIN Batch 47/3800 loss 5.309386 loss_att 7.381920 loss_ctc 7.169683 loss_rnnt 4.409276 hw_loss 0.445433 lr 0.00029529 rank 3
2023-02-27 16:54:05,103 DEBUG TRAIN Batch 47/3800 loss 3.552814 loss_att 6.950135 loss_ctc 6.641346 loss_rnnt 2.240897 hw_loss 0.413716 lr 0.00029529 rank 7
2023-02-27 16:54:05,106 DEBUG TRAIN Batch 47/3800 loss 5.762793 loss_att 6.324083 loss_ctc 10.392257 loss_rnnt 4.858692 hw_loss 0.327339 lr 0.00029530 rank 5
2023-02-27 16:54:05,107 DEBUG TRAIN Batch 47/3800 loss 4.313093 loss_att 7.206372 loss_ctc 8.284220 loss_rnnt 3.093545 hw_loss 0.208891 lr 0.00029529 rank 4
2023-02-27 16:54:05,110 DEBUG TRAIN Batch 47/3800 loss 3.668350 loss_att 6.601683 loss_ctc 6.765314 loss_rnnt 2.547276 hw_loss 0.227773 lr 0.00029530 rank 1
2023-02-27 16:54:05,110 DEBUG TRAIN Batch 47/3800 loss 4.939775 loss_att 7.458290 loss_ctc 6.690163 loss_rnnt 4.036770 hw_loss 0.311096 lr 0.00029529 rank 2
2023-02-27 16:55:08,383 DEBUG TRAIN Batch 47/3900 loss 7.136412 loss_att 10.470107 loss_ctc 10.656659 loss_rnnt 5.891213 hw_loss 0.204547 lr 0.00029528 rank 1
2023-02-27 16:55:08,393 DEBUG TRAIN Batch 47/3900 loss 4.327421 loss_att 6.498084 loss_ctc 6.288206 loss_rnnt 3.505435 hw_loss 0.237029 lr 0.00029528 rank 4
2023-02-27 16:55:08,396 DEBUG TRAIN Batch 47/3900 loss 5.574782 loss_att 8.010515 loss_ctc 8.614470 loss_rnnt 4.521599 hw_loss 0.301398 lr 0.00029528 rank 0
2023-02-27 16:55:08,404 DEBUG TRAIN Batch 47/3900 loss 5.716844 loss_att 7.202373 loss_ctc 9.036871 loss_rnnt 4.768246 hw_loss 0.391540 lr 0.00029528 rank 6
2023-02-27 16:55:08,419 DEBUG TRAIN Batch 47/3900 loss 10.479323 loss_att 15.115685 loss_ctc 22.394506 loss_rnnt 7.814192 hw_loss 0.279689 lr 0.00029528 rank 3
2023-02-27 16:55:08,420 DEBUG TRAIN Batch 47/3900 loss 9.745050 loss_att 13.487938 loss_ctc 20.300877 loss_rnnt 7.513851 hw_loss 0.140959 lr 0.00029527 rank 7
2023-02-27 16:55:08,424 DEBUG TRAIN Batch 47/3900 loss 3.692929 loss_att 8.247864 loss_ctc 6.101593 loss_rnnt 2.382977 hw_loss 0.145893 lr 0.00029528 rank 2
2023-02-27 16:55:08,432 DEBUG TRAIN Batch 47/3900 loss 4.326913 loss_att 6.955897 loss_ctc 10.091394 loss_rnnt 2.948833 hw_loss 0.156911 lr 0.00029528 rank 5
2023-02-27 16:55:49,677 DEBUG TRAIN Batch 47/4000 loss 3.776177 loss_att 5.284622 loss_ctc 3.890645 loss_rnnt 3.348026 hw_loss 0.208499 lr 0.00029526 rank 6
2023-02-27 16:55:49,681 DEBUG TRAIN Batch 47/4000 loss 4.841174 loss_att 6.870525 loss_ctc 7.783170 loss_rnnt 4.004060 hw_loss 0.073082 lr 0.00029527 rank 0
2023-02-27 16:55:49,681 DEBUG TRAIN Batch 47/4000 loss 3.029243 loss_att 6.786536 loss_ctc 5.605600 loss_rnnt 1.899741 hw_loss 0.064743 lr 0.00029526 rank 3
2023-02-27 16:55:49,684 DEBUG TRAIN Batch 47/4000 loss 5.359512 loss_att 9.073992 loss_ctc 9.982788 loss_rnnt 3.847534 hw_loss 0.286208 lr 0.00029527 rank 2
2023-02-27 16:55:49,687 DEBUG TRAIN Batch 47/4000 loss 0.991039 loss_att 3.330449 loss_ctc 2.338818 loss_rnnt 0.206508 hw_loss 0.256773 lr 0.00029527 rank 1
2023-02-27 16:55:49,689 DEBUG TRAIN Batch 47/4000 loss 9.219031 loss_att 12.815712 loss_ctc 15.311808 loss_rnnt 7.516319 hw_loss 0.320636 lr 0.00029527 rank 4
2023-02-27 16:55:49,697 DEBUG TRAIN Batch 47/4000 loss 2.109957 loss_att 5.042816 loss_ctc 2.578844 loss_rnnt 1.459462 hw_loss 0.002635 lr 0.00029526 rank 7
2023-02-27 16:55:49,713 DEBUG TRAIN Batch 47/4000 loss 4.689456 loss_att 8.197594 loss_ctc 11.016849 loss_rnnt 2.991681 hw_loss 0.285929 lr 0.00029527 rank 5
2023-02-27 16:56:28,287 DEBUG TRAIN Batch 47/4100 loss 6.791942 loss_att 11.293491 loss_ctc 11.546255 loss_rnnt 5.116214 hw_loss 0.265331 lr 0.00029525 rank 3
2023-02-27 16:56:28,298 DEBUG TRAIN Batch 47/4100 loss 14.151015 loss_att 19.282824 loss_ctc 20.978073 loss_rnnt 12.033747 hw_loss 0.338685 lr 0.00029525 rank 4
2023-02-27 16:56:28,301 DEBUG TRAIN Batch 47/4100 loss 3.186916 loss_att 5.866469 loss_ctc 3.549507 loss_rnnt 2.531211 hw_loss 0.133966 lr 0.00029525 rank 6
2023-02-27 16:56:28,303 DEBUG TRAIN Batch 47/4100 loss 7.774150 loss_att 12.834179 loss_ctc 14.315861 loss_rnnt 5.759279 hw_loss 0.244944 lr 0.00029525 rank 0
2023-02-27 16:56:28,304 DEBUG TRAIN Batch 47/4100 loss 6.854926 loss_att 8.064964 loss_ctc 8.429960 loss_rnnt 6.268482 hw_loss 0.252058 lr 0.00029526 rank 5
2023-02-27 16:56:28,306 DEBUG TRAIN Batch 47/4100 loss 6.744275 loss_att 9.732359 loss_ctc 9.806605 loss_rnnt 5.648104 hw_loss 0.169208 lr 0.00029526 rank 1
2023-02-27 16:56:28,334 DEBUG TRAIN Batch 47/4100 loss 10.777716 loss_att 15.710432 loss_ctc 21.323425 loss_rnnt 8.244883 hw_loss 0.262867 lr 0.00029525 rank 2
2023-02-27 16:56:28,341 DEBUG TRAIN Batch 47/4100 loss 5.767182 loss_att 8.756021 loss_ctc 12.134354 loss_rnnt 4.228192 hw_loss 0.172997 lr 0.00029525 rank 7
2023-02-27 16:57:07,479 DEBUG TRAIN Batch 47/4200 loss 7.975255 loss_att 9.178191 loss_ctc 10.209671 loss_rnnt 7.314411 hw_loss 0.229379 lr 0.00029524 rank 4
2023-02-27 16:57:07,485 DEBUG TRAIN Batch 47/4200 loss 6.904588 loss_att 10.163945 loss_ctc 13.364426 loss_rnnt 5.269689 hw_loss 0.228217 lr 0.00029525 rank 5
2023-02-27 16:57:07,488 DEBUG TRAIN Batch 47/4200 loss 4.806238 loss_att 5.720770 loss_ctc 8.388666 loss_rnnt 4.052285 hw_loss 0.175104 lr 0.00029524 rank 3
2023-02-27 16:57:07,492 DEBUG TRAIN Batch 47/4200 loss 9.194376 loss_att 13.074608 loss_ctc 18.474430 loss_rnnt 7.052425 hw_loss 0.241055 lr 0.00029524 rank 0
2023-02-27 16:57:07,494 DEBUG TRAIN Batch 47/4200 loss 4.602842 loss_att 6.612116 loss_ctc 7.574861 loss_rnnt 3.677575 hw_loss 0.238393 lr 0.00029524 rank 2
2023-02-27 16:57:07,494 DEBUG TRAIN Batch 47/4200 loss 4.581532 loss_att 8.763229 loss_ctc 10.100134 loss_rnnt 2.914689 hw_loss 0.177542 lr 0.00029524 rank 6
2023-02-27 16:57:07,498 DEBUG TRAIN Batch 47/4200 loss 10.394158 loss_att 12.910110 loss_ctc 17.871790 loss_rnnt 8.784872 hw_loss 0.204521 lr 0.00029524 rank 7
2023-02-27 16:57:07,498 DEBUG TRAIN Batch 47/4200 loss 7.472834 loss_att 11.003589 loss_ctc 12.246329 loss_rnnt 6.004605 hw_loss 0.235520 lr 0.00029524 rank 1
2023-02-27 16:58:15,085 DEBUG TRAIN Batch 47/4300 loss 12.764181 loss_att 17.456497 loss_ctc 18.081398 loss_rnnt 11.018480 hw_loss 0.184265 lr 0.00029523 rank 5
2023-02-27 16:58:15,088 DEBUG TRAIN Batch 47/4300 loss 7.241190 loss_att 8.796659 loss_ctc 13.416885 loss_rnnt 5.952265 hw_loss 0.289510 lr 0.00029523 rank 4
2023-02-27 16:58:15,098 DEBUG TRAIN Batch 47/4300 loss 6.317889 loss_att 8.292212 loss_ctc 10.613100 loss_rnnt 5.225269 hw_loss 0.234489 lr 0.00029523 rank 0
2023-02-27 16:58:15,102 DEBUG TRAIN Batch 47/4300 loss 7.754742 loss_att 11.993539 loss_ctc 8.184157 loss_rnnt 6.735701 hw_loss 0.213799 lr 0.00029523 rank 3
2023-02-27 16:58:15,102 DEBUG TRAIN Batch 47/4300 loss 5.709288 loss_att 8.562456 loss_ctc 7.657719 loss_rnnt 4.673639 hw_loss 0.384797 lr 0.00029522 rank 6
2023-02-27 16:58:15,105 DEBUG TRAIN Batch 47/4300 loss 4.856878 loss_att 7.455543 loss_ctc 6.244473 loss_rnnt 4.095371 hw_loss 0.106428 lr 0.00029522 rank 7
2023-02-27 16:58:15,107 DEBUG TRAIN Batch 47/4300 loss 3.859583 loss_att 7.774290 loss_ctc 9.600393 loss_rnnt 2.175164 hw_loss 0.255069 lr 0.00029523 rank 1
2023-02-27 16:58:15,152 DEBUG TRAIN Batch 47/4300 loss 7.514529 loss_att 12.209246 loss_ctc 15.324855 loss_rnnt 5.403877 hw_loss 0.244372 lr 0.00029523 rank 2
2023-02-27 16:58:54,436 DEBUG TRAIN Batch 47/4400 loss 7.517256 loss_att 8.135762 loss_ctc 11.383952 loss_rnnt 6.741983 hw_loss 0.255022 lr 0.00029522 rank 0
2023-02-27 16:58:54,436 DEBUG TRAIN Batch 47/4400 loss 7.345976 loss_att 8.925076 loss_ctc 12.289586 loss_rnnt 6.173281 hw_loss 0.370739 lr 0.00029521 rank 3
2023-02-27 16:58:54,436 DEBUG TRAIN Batch 47/4400 loss 10.597685 loss_att 13.289293 loss_ctc 22.164719 loss_rnnt 8.516384 hw_loss 0.001328 lr 0.00029521 rank 6
2023-02-27 16:58:54,439 DEBUG TRAIN Batch 47/4400 loss 7.761425 loss_att 7.862659 loss_ctc 12.488757 loss_rnnt 6.935193 hw_loss 0.329387 lr 0.00029522 rank 4
2023-02-27 16:58:54,441 DEBUG TRAIN Batch 47/4400 loss 6.023052 loss_att 7.003540 loss_ctc 9.046568 loss_rnnt 5.224780 hw_loss 0.373198 lr 0.00029522 rank 1
2023-02-27 16:58:54,441 DEBUG TRAIN Batch 47/4400 loss 6.658826 loss_att 8.141235 loss_ctc 10.789876 loss_rnnt 5.667186 hw_loss 0.270659 lr 0.00029521 rank 7
2023-02-27 16:58:54,442 DEBUG TRAIN Batch 47/4400 loss 11.431200 loss_att 13.202866 loss_ctc 20.242550 loss_rnnt 9.765987 hw_loss 0.255062 lr 0.00029522 rank 2
2023-02-27 16:58:54,487 DEBUG TRAIN Batch 47/4400 loss 5.857564 loss_att 6.960987 loss_ctc 9.721197 loss_rnnt 4.976873 hw_loss 0.271603 lr 0.00029522 rank 5
2023-02-27 16:59:33,563 DEBUG TRAIN Batch 47/4500 loss 4.903019 loss_att 10.220374 loss_ctc 5.930440 loss_rnnt 3.605602 hw_loss 0.181794 lr 0.00029520 rank 4
2023-02-27 16:59:33,574 DEBUG TRAIN Batch 47/4500 loss 4.092258 loss_att 6.296705 loss_ctc 5.264097 loss_rnnt 3.368410 hw_loss 0.237588 lr 0.00029520 rank 2
2023-02-27 16:59:33,582 DEBUG TRAIN Batch 47/4500 loss 10.632222 loss_att 16.278849 loss_ctc 15.242785 loss_rnnt 8.741100 hw_loss 0.275730 lr 0.00029521 rank 1
2023-02-27 16:59:33,583 DEBUG TRAIN Batch 47/4500 loss 6.762800 loss_att 8.955539 loss_ctc 11.727931 loss_rnnt 5.527322 hw_loss 0.252960 lr 0.00029520 rank 3
2023-02-27 16:59:33,584 DEBUG TRAIN Batch 47/4500 loss 5.570549 loss_att 5.917016 loss_ctc 8.399513 loss_rnnt 4.892553 hw_loss 0.434078 lr 0.00029521 rank 5
2023-02-27 16:59:33,584 DEBUG TRAIN Batch 47/4500 loss 3.732643 loss_att 7.770308 loss_ctc 7.564142 loss_rnnt 2.272872 hw_loss 0.265070 lr 0.00029520 rank 7
2023-02-27 16:59:33,585 DEBUG TRAIN Batch 47/4500 loss 5.569211 loss_att 7.540249 loss_ctc 8.789673 loss_rnnt 4.600954 hw_loss 0.271228 lr 0.00029520 rank 6
2023-02-27 16:59:33,588 DEBUG TRAIN Batch 47/4500 loss 8.379467 loss_att 10.634508 loss_ctc 12.868625 loss_rnnt 7.241661 hw_loss 0.165457 lr 0.00029520 rank 0
2023-02-27 17:00:13,803 DEBUG TRAIN Batch 47/4600 loss 2.980557 loss_att 7.283017 loss_ctc 7.202944 loss_rnnt 1.420206 hw_loss 0.256639 lr 0.00029519 rank 3
2023-02-27 17:00:13,808 DEBUG TRAIN Batch 47/4600 loss 3.816535 loss_att 8.029870 loss_ctc 7.983680 loss_rnnt 2.357374 hw_loss 0.114141 lr 0.00029519 rank 1
2023-02-27 17:00:13,812 DEBUG TRAIN Batch 47/4600 loss 3.617181 loss_att 5.971087 loss_ctc 7.370494 loss_rnnt 2.507318 hw_loss 0.259950 lr 0.00029519 rank 5
2023-02-27 17:00:13,814 DEBUG TRAIN Batch 47/4600 loss 5.232438 loss_att 7.635781 loss_ctc 7.028509 loss_rnnt 4.406366 hw_loss 0.198614 lr 0.00029519 rank 6
2023-02-27 17:00:13,817 DEBUG TRAIN Batch 47/4600 loss 3.518072 loss_att 5.446023 loss_ctc 4.967658 loss_rnnt 2.810692 hw_loss 0.240959 lr 0.00029519 rank 2
2023-02-27 17:00:13,821 DEBUG TRAIN Batch 47/4600 loss 5.063812 loss_att 7.069085 loss_ctc 7.653046 loss_rnnt 4.168572 hw_loss 0.279289 lr 0.00029519 rank 4
2023-02-27 17:00:13,822 DEBUG TRAIN Batch 47/4600 loss 3.095661 loss_att 6.396155 loss_ctc 6.160115 loss_rnnt 1.917838 hw_loss 0.204617 lr 0.00029518 rank 7
2023-02-27 17:00:13,826 DEBUG TRAIN Batch 47/4600 loss 3.906336 loss_att 6.337685 loss_ctc 4.539321 loss_rnnt 3.250057 hw_loss 0.160521 lr 0.00029519 rank 0
2023-02-27 17:01:19,540 DEBUG TRAIN Batch 47/4700 loss 3.966993 loss_att 7.700125 loss_ctc 7.245903 loss_rnnt 2.679043 hw_loss 0.195254 lr 0.00029518 rank 4
2023-02-27 17:01:19,550 DEBUG TRAIN Batch 47/4700 loss 6.975830 loss_att 7.158558 loss_ctc 10.130801 loss_rnnt 6.399894 hw_loss 0.222614 lr 0.00029517 rank 3
2023-02-27 17:01:19,555 DEBUG TRAIN Batch 47/4700 loss 4.240888 loss_att 6.663175 loss_ctc 6.017817 loss_rnnt 3.504587 hw_loss 0.027975 lr 0.00029517 rank 6
2023-02-27 17:01:19,555 DEBUG TRAIN Batch 47/4700 loss 3.683454 loss_att 6.394534 loss_ctc 7.792746 loss_rnnt 2.462067 hw_loss 0.246122 lr 0.00029517 rank 7
2023-02-27 17:01:19,557 DEBUG TRAIN Batch 47/4700 loss 3.699838 loss_att 6.692742 loss_ctc 6.107824 loss_rnnt 2.680101 hw_loss 0.187671 lr 0.00029518 rank 0
2023-02-27 17:01:19,558 DEBUG TRAIN Batch 47/4700 loss 5.066339 loss_att 6.797462 loss_ctc 8.991867 loss_rnnt 4.062542 hw_loss 0.251566 lr 0.00029518 rank 5
2023-02-27 17:01:19,561 DEBUG TRAIN Batch 47/4700 loss 2.345388 loss_att 6.395508 loss_ctc 6.364085 loss_rnnt 0.964731 hw_loss 0.065261 lr 0.00029518 rank 2
2023-02-27 17:01:19,616 DEBUG TRAIN Batch 47/4700 loss 10.093473 loss_att 12.988913 loss_ctc 16.787228 loss_rnnt 8.485037 hw_loss 0.256591 lr 0.00029518 rank 1
2023-02-27 17:01:57,993 DEBUG TRAIN Batch 47/4800 loss 8.608383 loss_att 11.511617 loss_ctc 11.685685 loss_rnnt 7.506662 hw_loss 0.207688 lr 0.00029517 rank 5
2023-02-27 17:01:57,997 DEBUG TRAIN Batch 47/4800 loss 8.723157 loss_att 10.301820 loss_ctc 14.769008 loss_rnnt 7.498886 hw_loss 0.192048 lr 0.00029516 rank 2
2023-02-27 17:01:58,005 DEBUG TRAIN Batch 47/4800 loss 3.701972 loss_att 7.608761 loss_ctc 8.671297 loss_rnnt 2.118145 hw_loss 0.262300 lr 0.00029516 rank 6
2023-02-27 17:01:58,008 DEBUG TRAIN Batch 47/4800 loss 11.197067 loss_att 16.501478 loss_ctc 17.780771 loss_rnnt 9.099962 hw_loss 0.296994 lr 0.00029516 rank 0
2023-02-27 17:01:58,008 DEBUG TRAIN Batch 47/4800 loss 10.259151 loss_att 11.980082 loss_ctc 18.614624 loss_rnnt 8.685667 hw_loss 0.216066 lr 0.00029516 rank 3
2023-02-27 17:01:58,012 DEBUG TRAIN Batch 47/4800 loss 4.429132 loss_att 7.179899 loss_ctc 9.605895 loss_rnnt 3.079955 hw_loss 0.203981 lr 0.00029516 rank 7
2023-02-27 17:01:58,013 DEBUG TRAIN Batch 47/4800 loss 6.125618 loss_att 10.526243 loss_ctc 12.172761 loss_rnnt 4.342779 hw_loss 0.180802 lr 0.00029517 rank 1
2023-02-27 17:01:58,019 DEBUG TRAIN Batch 47/4800 loss 2.861146 loss_att 4.087463 loss_ctc 5.355268 loss_rnnt 2.182696 hw_loss 0.188695 lr 0.00029516 rank 4
2023-02-27 17:02:37,187 DEBUG TRAIN Batch 47/4900 loss 9.072386 loss_att 9.850025 loss_ctc 14.069580 loss_rnnt 8.137979 hw_loss 0.211101 lr 0.00029515 rank 2
2023-02-27 17:02:37,188 DEBUG TRAIN Batch 47/4900 loss 2.985830 loss_att 6.691938 loss_ctc 5.132020 loss_rnnt 1.783457 hw_loss 0.328110 lr 0.00029515 rank 6
2023-02-27 17:02:37,198 DEBUG TRAIN Batch 47/4900 loss 7.780501 loss_att 10.694981 loss_ctc 12.214782 loss_rnnt 6.460183 hw_loss 0.274095 lr 0.00029515 rank 3
2023-02-27 17:02:37,201 DEBUG TRAIN Batch 47/4900 loss 9.354699 loss_att 9.986174 loss_ctc 16.722628 loss_rnnt 8.080624 hw_loss 0.310107 lr 0.00029515 rank 0
2023-02-27 17:02:37,203 DEBUG TRAIN Batch 47/4900 loss 5.001220 loss_att 6.945236 loss_ctc 7.489966 loss_rnnt 4.166459 hw_loss 0.213984 lr 0.00029515 rank 4
2023-02-27 17:02:37,206 DEBUG TRAIN Batch 47/4900 loss 9.845780 loss_att 12.082176 loss_ctc 22.803596 loss_rnnt 7.598060 hw_loss 0.136373 lr 0.00029515 rank 7
2023-02-27 17:02:37,206 DEBUG TRAIN Batch 47/4900 loss 5.156106 loss_att 7.236977 loss_ctc 7.312887 loss_rnnt 4.199004 hw_loss 0.475043 lr 0.00029515 rank 1
2023-02-27 17:02:37,247 DEBUG TRAIN Batch 47/4900 loss 7.552932 loss_att 10.228045 loss_ctc 13.743681 loss_rnnt 6.118164 hw_loss 0.139335 lr 0.00029516 rank 5
2023-02-27 17:03:42,029 DEBUG TRAIN Batch 47/5000 loss 5.675517 loss_att 9.438752 loss_ctc 10.438353 loss_rnnt 4.149182 hw_loss 0.259954 lr 0.00029514 rank 3
2023-02-27 17:03:42,029 DEBUG TRAIN Batch 47/5000 loss 10.760234 loss_att 13.785257 loss_ctc 21.030684 loss_rnnt 8.631959 hw_loss 0.288517 lr 0.00029514 rank 0
2023-02-27 17:03:42,030 DEBUG TRAIN Batch 47/5000 loss 6.173259 loss_att 7.383584 loss_ctc 8.713686 loss_rnnt 5.446702 hw_loss 0.273316 lr 0.00029513 rank 6
2023-02-27 17:03:42,032 DEBUG TRAIN Batch 47/5000 loss 6.527985 loss_att 6.095298 loss_ctc 7.329227 loss_rnnt 6.282792 hw_loss 0.421683 lr 0.00029514 rank 4
2023-02-27 17:03:42,034 DEBUG TRAIN Batch 47/5000 loss 7.399598 loss_att 10.570913 loss_ctc 12.443310 loss_rnnt 5.928682 hw_loss 0.307797 lr 0.00029513 rank 7
2023-02-27 17:03:42,035 DEBUG TRAIN Batch 47/5000 loss 6.083386 loss_att 8.101451 loss_ctc 8.760979 loss_rnnt 5.284492 hw_loss 0.071753 lr 0.00029514 rank 5
2023-02-27 17:03:42,035 DEBUG TRAIN Batch 47/5000 loss 5.613340 loss_att 7.757195 loss_ctc 7.103238 loss_rnnt 4.926032 hw_loss 0.112283 lr 0.00029514 rank 1
2023-02-27 17:03:42,082 DEBUG TRAIN Batch 47/5000 loss 9.418234 loss_att 10.348287 loss_ctc 14.896145 loss_rnnt 8.392480 hw_loss 0.205041 lr 0.00029514 rank 2
2023-02-27 17:04:20,608 DEBUG TRAIN Batch 47/5100 loss 6.855900 loss_att 8.228156 loss_ctc 11.302229 loss_rnnt 5.841640 hw_loss 0.275561 lr 0.00029513 rank 4
2023-02-27 17:04:20,618 DEBUG TRAIN Batch 47/5100 loss 4.331401 loss_att 6.645075 loss_ctc 7.256692 loss_rnnt 3.325294 hw_loss 0.287502 lr 0.00029512 rank 6
2023-02-27 17:04:20,621 DEBUG TRAIN Batch 47/5100 loss 6.679585 loss_att 7.130008 loss_ctc 10.916262 loss_rnnt 5.803342 hw_loss 0.414878 lr 0.00029513 rank 5
2023-02-27 17:04:20,622 DEBUG TRAIN Batch 47/5100 loss 3.170814 loss_att 5.370717 loss_ctc 7.006967 loss_rnnt 2.120446 hw_loss 0.185437 lr 0.00029512 rank 3
2023-02-27 17:04:20,624 DEBUG TRAIN Batch 47/5100 loss 6.648124 loss_att 9.369814 loss_ctc 13.357112 loss_rnnt 5.074789 hw_loss 0.252123 lr 0.00029513 rank 2
2023-02-27 17:04:20,625 DEBUG TRAIN Batch 47/5100 loss 8.043831 loss_att 10.145207 loss_ctc 12.912809 loss_rnnt 6.806063 hw_loss 0.315553 lr 0.00029512 rank 7
2023-02-27 17:04:20,628 DEBUG TRAIN Batch 47/5100 loss 5.555916 loss_att 8.447956 loss_ctc 11.077927 loss_rnnt 4.163437 hw_loss 0.145880 lr 0.00029513 rank 0
2023-02-27 17:04:20,629 DEBUG TRAIN Batch 47/5100 loss 9.373733 loss_att 12.799708 loss_ctc 20.125145 loss_rnnt 7.109068 hw_loss 0.273652 lr 0.00029513 rank 1
2023-02-27 17:05:00,146 DEBUG TRAIN Batch 47/5200 loss 7.651906 loss_att 7.533304 loss_ctc 10.308809 loss_rnnt 7.126660 hw_loss 0.365085 lr 0.00029511 rank 3
2023-02-27 17:05:00,147 DEBUG TRAIN Batch 47/5200 loss 5.775432 loss_att 9.529276 loss_ctc 9.132672 loss_rnnt 4.472457 hw_loss 0.196077 lr 0.00029511 rank 4
2023-02-27 17:05:00,157 DEBUG TRAIN Batch 47/5200 loss 4.816466 loss_att 8.780931 loss_ctc 6.858438 loss_rnnt 3.682495 hw_loss 0.129027 lr 0.00029511 rank 0
2023-02-27 17:05:00,160 DEBUG TRAIN Batch 47/5200 loss 6.738846 loss_att 11.160211 loss_ctc 12.900200 loss_rnnt 5.032307 hw_loss 0.001410 lr 0.00029512 rank 1
2023-02-27 17:05:00,160 DEBUG TRAIN Batch 47/5200 loss 9.702624 loss_att 9.050320 loss_ctc 12.741167 loss_rnnt 9.222569 hw_loss 0.385085 lr 0.00029511 rank 6
2023-02-27 17:05:00,162 DEBUG TRAIN Batch 47/5200 loss 6.609998 loss_att 10.424473 loss_ctc 9.529914 loss_rnnt 5.298408 hw_loss 0.298824 lr 0.00029511 rank 7
2023-02-27 17:05:00,161 DEBUG TRAIN Batch 47/5200 loss 8.934948 loss_att 13.203522 loss_ctc 16.919563 loss_rnnt 6.942698 hw_loss 0.138597 lr 0.00029511 rank 2
2023-02-27 17:05:00,170 DEBUG TRAIN Batch 47/5200 loss 5.675016 loss_att 7.733378 loss_ctc 11.203430 loss_rnnt 4.509304 hw_loss 0.031720 lr 0.00029512 rank 5
2023-02-27 17:05:40,254 DEBUG TRAIN Batch 47/5300 loss 3.776751 loss_att 6.500290 loss_ctc 4.634369 loss_rnnt 2.984438 hw_loss 0.249856 lr 0.00029510 rank 5
2023-02-27 17:05:40,262 DEBUG TRAIN Batch 47/5300 loss 5.003829 loss_att 8.125162 loss_ctc 10.913028 loss_rnnt 3.451477 hw_loss 0.262863 lr 0.00029510 rank 6
2023-02-27 17:05:40,267 DEBUG TRAIN Batch 47/5300 loss 10.119508 loss_att 12.604513 loss_ctc 13.246898 loss_rnnt 9.102775 hw_loss 0.192650 lr 0.00029510 rank 0
2023-02-27 17:05:40,267 DEBUG TRAIN Batch 47/5300 loss 4.956185 loss_att 7.549251 loss_ctc 9.105504 loss_rnnt 3.823635 hw_loss 0.113803 lr 0.00029509 rank 7
2023-02-27 17:05:40,266 DEBUG TRAIN Batch 47/5300 loss 5.830745 loss_att 8.102461 loss_ctc 9.614002 loss_rnnt 4.808674 hw_loss 0.118675 lr 0.00029510 rank 3
2023-02-27 17:05:40,267 DEBUG TRAIN Batch 47/5300 loss 7.162796 loss_att 10.750590 loss_ctc 8.631495 loss_rnnt 6.149605 hw_loss 0.187136 lr 0.00029510 rank 4
2023-02-27 17:05:40,268 DEBUG TRAIN Batch 47/5300 loss 8.587883 loss_att 11.230789 loss_ctc 15.126724 loss_rnnt 7.067716 hw_loss 0.224513 lr 0.00029510 rank 2
2023-02-27 17:05:40,269 DEBUG TRAIN Batch 47/5300 loss 5.479402 loss_att 7.316494 loss_ctc 9.797922 loss_rnnt 4.361600 hw_loss 0.327338 lr 0.00029510 rank 1
2023-02-27 17:06:46,054 DEBUG TRAIN Batch 47/5400 loss 4.902782 loss_att 7.672270 loss_ctc 6.470802 loss_rnnt 3.992406 hw_loss 0.276392 lr 0.00029508 rank 6
2023-02-27 17:06:46,055 DEBUG TRAIN Batch 47/5400 loss 3.950679 loss_att 6.427250 loss_ctc 6.449127 loss_rnnt 3.001763 hw_loss 0.225892 lr 0.00029508 rank 3
2023-02-27 17:06:46,057 DEBUG TRAIN Batch 47/5400 loss 5.657780 loss_att 9.892893 loss_ctc 7.825160 loss_rnnt 4.377514 hw_loss 0.270486 lr 0.00029509 rank 0
2023-02-27 17:06:46,058 DEBUG TRAIN Batch 47/5400 loss 2.527091 loss_att 5.616016 loss_ctc 3.222628 loss_rnnt 1.680840 hw_loss 0.254490 lr 0.00029509 rank 5
2023-02-27 17:06:46,060 DEBUG TRAIN Batch 47/5400 loss 3.729819 loss_att 5.254928 loss_ctc 7.754291 loss_rnnt 2.739280 hw_loss 0.279226 lr 0.00029509 rank 4
2023-02-27 17:06:46,063 DEBUG TRAIN Batch 47/5400 loss 7.191838 loss_att 9.189150 loss_ctc 10.078428 loss_rnnt 6.305629 hw_loss 0.191003 lr 0.00029509 rank 1
2023-02-27 17:06:46,068 DEBUG TRAIN Batch 47/5400 loss 6.443605 loss_att 9.279109 loss_ctc 12.511875 loss_rnnt 4.955228 hw_loss 0.210326 lr 0.00029508 rank 7
2023-02-27 17:06:46,106 DEBUG TRAIN Batch 47/5400 loss 6.076849 loss_att 9.363841 loss_ctc 7.593946 loss_rnnt 5.074717 hw_loss 0.267102 lr 0.00029509 rank 2
2023-02-27 17:07:24,593 DEBUG TRAIN Batch 47/5500 loss 3.297928 loss_att 6.387469 loss_ctc 7.102324 loss_rnnt 2.064198 hw_loss 0.203567 lr 0.00029507 rank 6
2023-02-27 17:07:24,595 DEBUG TRAIN Batch 47/5500 loss 8.734494 loss_att 11.000168 loss_ctc 12.615348 loss_rnnt 7.629930 hw_loss 0.251218 lr 0.00029507 rank 0
2023-02-27 17:07:24,595 DEBUG TRAIN Batch 47/5500 loss 4.804098 loss_att 7.536310 loss_ctc 6.510448 loss_rnnt 3.908289 hw_loss 0.228474 lr 0.00029507 rank 2
2023-02-27 17:07:24,595 DEBUG TRAIN Batch 47/5500 loss 4.516368 loss_att 7.999729 loss_ctc 9.524872 loss_rnnt 3.086842 hw_loss 0.121976 lr 0.00029507 rank 3
2023-02-27 17:07:24,601 DEBUG TRAIN Batch 47/5500 loss 3.225623 loss_att 7.544662 loss_ctc 8.477571 loss_rnnt 1.625644 hw_loss 0.067335 lr 0.00029508 rank 5
2023-02-27 17:07:24,602 DEBUG TRAIN Batch 47/5500 loss 9.776908 loss_att 11.506473 loss_ctc 13.550222 loss_rnnt 8.829684 hw_loss 0.184128 lr 0.00029507 rank 7
2023-02-27 17:07:24,602 DEBUG TRAIN Batch 47/5500 loss 7.676322 loss_att 11.634398 loss_ctc 10.489235 loss_rnnt 6.398752 hw_loss 0.207936 lr 0.00029507 rank 4
2023-02-27 17:07:24,605 DEBUG TRAIN Batch 47/5500 loss 10.229322 loss_att 13.453986 loss_ctc 19.296722 loss_rnnt 8.282433 hw_loss 0.174320 lr 0.00029508 rank 1
2023-02-27 17:08:03,843 DEBUG TRAIN Batch 47/5600 loss 12.725041 loss_att 19.232897 loss_ctc 23.628908 loss_rnnt 9.833462 hw_loss 0.255299 lr 0.00029506 rank 7
2023-02-27 17:08:03,858 DEBUG TRAIN Batch 47/5600 loss 4.533417 loss_att 6.680326 loss_ctc 6.248268 loss_rnnt 3.771989 hw_loss 0.193874 lr 0.00029506 rank 3
2023-02-27 17:08:03,862 DEBUG TRAIN Batch 47/5600 loss 11.244774 loss_att 11.846416 loss_ctc 16.938322 loss_rnnt 10.247109 hw_loss 0.221617 lr 0.00029506 rank 0
2023-02-27 17:08:03,863 DEBUG TRAIN Batch 47/5600 loss 5.641861 loss_att 9.276485 loss_ctc 8.522887 loss_rnnt 4.395576 hw_loss 0.253545 lr 0.00029506 rank 6
2023-02-27 17:08:03,865 DEBUG TRAIN Batch 47/5600 loss 6.984324 loss_att 9.082633 loss_ctc 10.040855 loss_rnnt 6.017536 hw_loss 0.261731 lr 0.00029506 rank 1
2023-02-27 17:08:03,866 DEBUG TRAIN Batch 47/5600 loss 2.284386 loss_att 4.418004 loss_ctc 3.384883 loss_rnnt 1.546862 hw_loss 0.307627 lr 0.00029506 rank 4
2023-02-27 17:08:03,870 DEBUG TRAIN Batch 47/5600 loss 5.958210 loss_att 8.402544 loss_ctc 12.592751 loss_rnnt 4.414095 hw_loss 0.319953 lr 0.00029506 rank 2
2023-02-27 17:08:03,910 DEBUG TRAIN Batch 47/5600 loss 4.812691 loss_att 9.693202 loss_ctc 12.608239 loss_rnnt 2.658515 hw_loss 0.260002 lr 0.00029507 rank 5
2023-02-27 17:09:10,638 DEBUG TRAIN Batch 47/5700 loss 8.010363 loss_att 10.204401 loss_ctc 12.101946 loss_rnnt 6.958200 hw_loss 0.127145 lr 0.00029505 rank 2
2023-02-27 17:09:10,642 DEBUG TRAIN Batch 47/5700 loss 5.471992 loss_att 7.400966 loss_ctc 10.672169 loss_rnnt 4.275193 hw_loss 0.220588 lr 0.00029505 rank 3
2023-02-27 17:09:10,643 DEBUG TRAIN Batch 47/5700 loss 11.203065 loss_att 11.320988 loss_ctc 15.939292 loss_rnnt 10.362778 hw_loss 0.347259 lr 0.00029504 rank 6
2023-02-27 17:09:10,648 DEBUG TRAIN Batch 47/5700 loss 4.102478 loss_att 4.783520 loss_ctc 6.549888 loss_rnnt 3.442645 hw_loss 0.369943 lr 0.00029505 rank 0
2023-02-27 17:09:10,648 DEBUG TRAIN Batch 47/5700 loss 7.578353 loss_att 9.839306 loss_ctc 15.011349 loss_rnnt 5.954532 hw_loss 0.338560 lr 0.00029505 rank 5
2023-02-27 17:09:10,656 DEBUG TRAIN Batch 47/5700 loss 8.779606 loss_att 10.326794 loss_ctc 12.935444 loss_rnnt 7.726213 hw_loss 0.355956 lr 0.00029505 rank 1
2023-02-27 17:09:10,663 DEBUG TRAIN Batch 47/5700 loss 6.870519 loss_att 7.569768 loss_ctc 9.644583 loss_rnnt 6.137147 hw_loss 0.419337 lr 0.00029505 rank 4
2023-02-27 17:09:10,702 DEBUG TRAIN Batch 47/5700 loss 3.819412 loss_att 5.445317 loss_ctc 7.392333 loss_rnnt 2.893469 hw_loss 0.233197 lr 0.00029504 rank 7
2023-02-27 17:09:50,003 DEBUG TRAIN Batch 47/5800 loss 14.388512 loss_att 17.711067 loss_ctc 20.940475 loss_rnnt 12.780441 hw_loss 0.131182 lr 0.00029504 rank 4
2023-02-27 17:09:50,018 DEBUG TRAIN Batch 47/5800 loss 7.435639 loss_att 10.216490 loss_ctc 12.672690 loss_rnnt 6.072989 hw_loss 0.202888 lr 0.00029503 rank 6
2023-02-27 17:09:50,018 DEBUG TRAIN Batch 47/5800 loss 5.925524 loss_att 7.400750 loss_ctc 9.495776 loss_rnnt 5.030012 hw_loss 0.233312 lr 0.00029503 rank 3
2023-02-27 17:09:50,022 DEBUG TRAIN Batch 47/5800 loss 4.570498 loss_att 5.551772 loss_ctc 7.467155 loss_rnnt 3.861486 hw_loss 0.237257 lr 0.00029503 rank 7
2023-02-27 17:09:50,023 DEBUG TRAIN Batch 47/5800 loss 3.777391 loss_att 7.089643 loss_ctc 10.848765 loss_rnnt 1.987226 hw_loss 0.346621 lr 0.00029504 rank 5
2023-02-27 17:09:50,023 DEBUG TRAIN Batch 47/5800 loss 8.203201 loss_att 11.877553 loss_ctc 18.590887 loss_rnnt 6.050666 hw_loss 0.061199 lr 0.00029504 rank 2
2023-02-27 17:09:50,026 DEBUG TRAIN Batch 47/5800 loss 4.715177 loss_att 9.678944 loss_ctc 10.093391 loss_rnnt 2.840247 hw_loss 0.309528 lr 0.00029504 rank 0
2023-02-27 17:09:50,026 DEBUG TRAIN Batch 47/5800 loss 5.725949 loss_att 6.999636 loss_ctc 7.563119 loss_rnnt 5.119770 hw_loss 0.199662 lr 0.00029504 rank 1
2023-02-27 17:10:29,732 DEBUG TRAIN Batch 47/5900 loss 5.922579 loss_att 8.925545 loss_ctc 8.321836 loss_rnnt 4.892771 hw_loss 0.204963 lr 0.00029502 rank 4
2023-02-27 17:10:29,735 DEBUG TRAIN Batch 47/5900 loss 10.200230 loss_att 14.264754 loss_ctc 13.780760 loss_rnnt 8.746807 hw_loss 0.305838 lr 0.00029503 rank 5
2023-02-27 17:10:29,737 DEBUG TRAIN Batch 47/5900 loss 6.199492 loss_att 8.648140 loss_ctc 7.154780 loss_rnnt 5.475057 hw_loss 0.201250 lr 0.00029502 rank 7
2023-02-27 17:10:29,742 DEBUG TRAIN Batch 47/5900 loss 7.375894 loss_att 10.655296 loss_ctc 10.790312 loss_rnnt 6.152607 hw_loss 0.210284 lr 0.00029502 rank 3
2023-02-27 17:10:29,744 DEBUG TRAIN Batch 47/5900 loss 4.123932 loss_att 6.328627 loss_ctc 6.477019 loss_rnnt 3.280509 hw_loss 0.166385 lr 0.00029502 rank 6
2023-02-27 17:10:29,748 DEBUG TRAIN Batch 47/5900 loss 5.661793 loss_att 8.690231 loss_ctc 9.288679 loss_rnnt 4.449559 hw_loss 0.230551 lr 0.00029502 rank 0
2023-02-27 17:10:29,749 DEBUG TRAIN Batch 47/5900 loss 14.851980 loss_att 19.131321 loss_ctc 22.756041 loss_rnnt 12.858372 hw_loss 0.157246 lr 0.00029503 rank 1
2023-02-27 17:10:29,757 DEBUG TRAIN Batch 47/5900 loss 8.019255 loss_att 13.029961 loss_ctc 15.700899 loss_rnnt 5.865685 hw_loss 0.238516 lr 0.00029502 rank 2
2023-02-27 17:11:10,617 DEBUG TRAIN Batch 47/6000 loss 3.495522 loss_att 8.041885 loss_ctc 8.647411 loss_rnnt 1.722041 hw_loss 0.332420 lr 0.00029501 rank 5
2023-02-27 17:11:10,621 DEBUG TRAIN Batch 47/6000 loss 2.458757 loss_att 6.034397 loss_ctc 4.890241 loss_rnnt 1.291873 hw_loss 0.239173 lr 0.00029501 rank 1
2023-02-27 17:11:10,623 DEBUG TRAIN Batch 47/6000 loss 8.064333 loss_att 12.334848 loss_ctc 12.457344 loss_rnnt 6.509810 hw_loss 0.215034 lr 0.00029501 rank 2
2023-02-27 17:11:10,625 DEBUG TRAIN Batch 47/6000 loss 6.904764 loss_att 9.426928 loss_ctc 14.109715 loss_rnnt 5.305196 hw_loss 0.252141 lr 0.00029501 rank 4
2023-02-27 17:11:10,628 DEBUG TRAIN Batch 47/6000 loss 3.470290 loss_att 6.497642 loss_ctc 6.356231 loss_rnnt 2.479448 hw_loss 0.001086 lr 0.00029501 rank 6
2023-02-27 17:11:10,632 DEBUG TRAIN Batch 47/6000 loss 2.898335 loss_att 6.950483 loss_ctc 6.475117 loss_rnnt 1.489245 hw_loss 0.228293 lr 0.00029501 rank 0
2023-02-27 17:11:10,635 DEBUG TRAIN Batch 47/6000 loss 2.555599 loss_att 5.049322 loss_ctc 3.142184 loss_rnnt 1.831136 hw_loss 0.276576 lr 0.00029501 rank 3
2023-02-27 17:11:10,641 DEBUG TRAIN Batch 47/6000 loss 6.180689 loss_att 9.090089 loss_ctc 7.135746 loss_rnnt 5.413497 hw_loss 0.108696 lr 0.00029500 rank 7
2023-02-27 17:12:15,112 DEBUG TRAIN Batch 47/6100 loss 9.138122 loss_att 13.115275 loss_ctc 15.843350 loss_rnnt 7.331500 hw_loss 0.219677 lr 0.00029499 rank 3
2023-02-27 17:12:15,122 DEBUG TRAIN Batch 47/6100 loss 8.874617 loss_att 11.218466 loss_ctc 13.390001 loss_rnnt 7.701470 hw_loss 0.191858 lr 0.00029500 rank 4
2023-02-27 17:12:15,125 DEBUG TRAIN Batch 47/6100 loss 4.839165 loss_att 7.932341 loss_ctc 9.491863 loss_rnnt 3.480769 hw_loss 0.223876 lr 0.00029500 rank 1
2023-02-27 17:12:15,125 DEBUG TRAIN Batch 47/6100 loss 9.877720 loss_att 13.014105 loss_ctc 15.027128 loss_rnnt 8.433993 hw_loss 0.243492 lr 0.00029500 rank 2
2023-02-27 17:12:15,128 DEBUG TRAIN Batch 47/6100 loss 5.499295 loss_att 8.498737 loss_ctc 9.394972 loss_rnnt 4.306925 hw_loss 0.136984 lr 0.00029500 rank 5
2023-02-27 17:12:15,129 DEBUG TRAIN Batch 47/6100 loss 8.719491 loss_att 11.955348 loss_ctc 15.658607 loss_rnnt 7.069435 hw_loss 0.145629 lr 0.00029499 rank 6
2023-02-27 17:12:15,130 DEBUG TRAIN Batch 47/6100 loss 5.075154 loss_att 8.410488 loss_ctc 7.801158 loss_rnnt 3.925365 hw_loss 0.223602 lr 0.00029500 rank 0
2023-02-27 17:12:15,130 DEBUG TRAIN Batch 47/6100 loss 10.043699 loss_att 13.799055 loss_ctc 21.177986 loss_rnnt 7.715278 hw_loss 0.173960 lr 0.00029499 rank 7
2023-02-27 17:12:53,783 DEBUG TRAIN Batch 47/6200 loss 7.815776 loss_att 9.110928 loss_ctc 11.694729 loss_rnnt 6.975487 hw_loss 0.120122 lr 0.00029498 rank 3
2023-02-27 17:12:53,788 DEBUG TRAIN Batch 47/6200 loss 3.616927 loss_att 4.868574 loss_ctc 7.732264 loss_rnnt 2.657227 hw_loss 0.301236 lr 0.00029498 rank 6
2023-02-27 17:12:53,791 DEBUG TRAIN Batch 47/6200 loss 4.854154 loss_att 7.616646 loss_ctc 11.794733 loss_rnnt 3.245152 hw_loss 0.245799 lr 0.00029498 rank 0
2023-02-27 17:12:53,790 DEBUG TRAIN Batch 47/6200 loss 5.845333 loss_att 8.451264 loss_ctc 8.679548 loss_rnnt 4.821119 hw_loss 0.234623 lr 0.00029499 rank 1
2023-02-27 17:12:53,791 DEBUG TRAIN Batch 47/6200 loss 9.583939 loss_att 12.734972 loss_ctc 17.086601 loss_rnnt 7.826180 hw_loss 0.238495 lr 0.00029498 rank 4
2023-02-27 17:12:53,795 DEBUG TRAIN Batch 47/6200 loss 1.684905 loss_att 4.102716 loss_ctc 2.985451 loss_rnnt 0.997774 hw_loss 0.056555 lr 0.00029499 rank 5
2023-02-27 17:12:53,795 DEBUG TRAIN Batch 47/6200 loss 12.081494 loss_att 15.073309 loss_ctc 21.843798 loss_rnnt 10.067459 hw_loss 0.213811 lr 0.00029498 rank 7
2023-02-27 17:12:53,795 DEBUG TRAIN Batch 47/6200 loss 3.948050 loss_att 6.041926 loss_ctc 7.802391 loss_rnnt 2.874945 hw_loss 0.263282 lr 0.00029498 rank 2
2023-02-27 17:13:32,992 DEBUG TRAIN Batch 47/6300 loss 7.768323 loss_att 8.671893 loss_ctc 10.695074 loss_rnnt 7.081374 hw_loss 0.217502 lr 0.00029497 rank 4
2023-02-27 17:13:32,993 DEBUG TRAIN Batch 47/6300 loss 12.400948 loss_att 13.969658 loss_ctc 21.671055 loss_rnnt 10.743521 hw_loss 0.201884 lr 0.00029497 rank 1
2023-02-27 17:13:32,993 DEBUG TRAIN Batch 47/6300 loss 7.234664 loss_att 8.773048 loss_ctc 11.393314 loss_rnnt 6.204133 hw_loss 0.315690 lr 0.00029497 rank 0
2023-02-27 17:13:32,994 DEBUG TRAIN Batch 47/6300 loss 5.034962 loss_att 7.083287 loss_ctc 7.061464 loss_rnnt 4.219917 hw_loss 0.253461 lr 0.00029498 rank 5
2023-02-27 17:13:32,995 DEBUG TRAIN Batch 47/6300 loss 11.822598 loss_att 17.047821 loss_ctc 18.876570 loss_rnnt 9.717804 hw_loss 0.223540 lr 0.00029497 rank 6
2023-02-27 17:13:32,996 DEBUG TRAIN Batch 47/6300 loss 4.904542 loss_att 7.962474 loss_ctc 10.324663 loss_rnnt 3.407117 hw_loss 0.305918 lr 0.00029497 rank 2
2023-02-27 17:13:33,005 DEBUG TRAIN Batch 47/6300 loss 10.448077 loss_att 14.819942 loss_ctc 17.560066 loss_rnnt 8.523601 hw_loss 0.190947 lr 0.00029497 rank 3
2023-02-27 17:13:33,021 DEBUG TRAIN Batch 47/6300 loss 6.933399 loss_att 10.521577 loss_ctc 13.734767 loss_rnnt 5.161092 hw_loss 0.277166 lr 0.00029497 rank 7
2023-02-27 17:14:35,002 DEBUG TRAIN Batch 47/6400 loss 4.420794 loss_att 11.915626 loss_ctc 15.114496 loss_rnnt 1.368144 hw_loss 0.239730 lr 0.00029496 rank 4
2023-02-27 17:14:35,010 DEBUG TRAIN Batch 47/6400 loss 1.446067 loss_att 4.506126 loss_ctc 2.378120 loss_rnnt 0.567605 hw_loss 0.266581 lr 0.00029496 rank 5
2023-02-27 17:14:35,013 DEBUG TRAIN Batch 47/6400 loss 5.122421 loss_att 6.503736 loss_ctc 8.047923 loss_rnnt 4.309073 hw_loss 0.275659 lr 0.00029496 rank 3
2023-02-27 17:14:35,015 DEBUG TRAIN Batch 47/6400 loss 2.439023 loss_att 6.591500 loss_ctc 5.876417 loss_rnnt 1.074756 hw_loss 0.141473 lr 0.00029495 rank 7
2023-02-27 17:14:35,016 DEBUG TRAIN Batch 47/6400 loss 4.065447 loss_att 7.089647 loss_ctc 5.160786 loss_rnnt 3.198662 hw_loss 0.217313 lr 0.00029496 rank 2
2023-02-27 17:14:35,018 DEBUG TRAIN Batch 47/6400 loss 8.912422 loss_att 11.472506 loss_ctc 15.094489 loss_rnnt 7.389079 hw_loss 0.350721 lr 0.00029495 rank 6
2023-02-27 17:14:35,018 DEBUG TRAIN Batch 47/6400 loss 8.543947 loss_att 8.211109 loss_ctc 12.821312 loss_rnnt 7.884486 hw_loss 0.291962 lr 0.00029496 rank 0
2023-02-27 17:14:35,042 DEBUG TRAIN Batch 47/6400 loss 6.056714 loss_att 11.444715 loss_ctc 11.940379 loss_rnnt 4.008774 hw_loss 0.348470 lr 0.00029496 rank 1
2023-02-27 17:15:17,756 DEBUG TRAIN Batch 47/6500 loss 5.499391 loss_att 7.767148 loss_ctc 9.480152 loss_rnnt 4.484470 hw_loss 0.057376 lr 0.00029495 rank 2
2023-02-27 17:15:17,757 DEBUG TRAIN Batch 47/6500 loss 6.954203 loss_att 12.682418 loss_ctc 8.933460 loss_rnnt 5.543962 hw_loss 0.001307 lr 0.00029494 rank 3
2023-02-27 17:15:17,758 DEBUG TRAIN Batch 47/6500 loss 11.665070 loss_att 12.212749 loss_ctc 25.369627 loss_rnnt 9.561014 hw_loss 0.313583 lr 0.00029494 rank 6
2023-02-27 17:15:17,761 DEBUG TRAIN Batch 47/6500 loss 6.112813 loss_att 8.995731 loss_ctc 13.302272 loss_rnnt 4.456967 hw_loss 0.226253 lr 0.00029495 rank 1
2023-02-27 17:15:17,760 DEBUG TRAIN Batch 47/6500 loss 10.430331 loss_att 12.764647 loss_ctc 19.966467 loss_rnnt 8.551834 hw_loss 0.262780 lr 0.00029495 rank 4
2023-02-27 17:15:17,760 DEBUG TRAIN Batch 47/6500 loss 2.838041 loss_att 7.047567 loss_ctc 4.166795 loss_rnnt 1.666964 hw_loss 0.285009 lr 0.00029495 rank 0
2023-02-27 17:15:17,764 DEBUG TRAIN Batch 47/6500 loss 3.625248 loss_att 9.273068 loss_ctc 7.232670 loss_rnnt 1.857769 hw_loss 0.294236 lr 0.00029494 rank 7
2023-02-27 17:15:17,765 DEBUG TRAIN Batch 47/6500 loss 8.907140 loss_att 9.841579 loss_ctc 11.369087 loss_rnnt 8.277301 hw_loss 0.215047 lr 0.00029495 rank 5
2023-02-27 17:15:56,185 DEBUG TRAIN Batch 47/6600 loss 2.528971 loss_att 5.113203 loss_ctc 3.813159 loss_rnnt 1.699487 hw_loss 0.265148 lr 0.00029493 rank 6
2023-02-27 17:15:56,210 DEBUG TRAIN Batch 47/6600 loss 4.455338 loss_att 7.321605 loss_ctc 5.551433 loss_rnnt 3.588860 hw_loss 0.275770 lr 0.00029493 rank 4
2023-02-27 17:15:56,212 DEBUG TRAIN Batch 47/6600 loss 2.719267 loss_att 5.940849 loss_ctc 6.699343 loss_rnnt 1.426628 hw_loss 0.220586 lr 0.00029493 rank 0
2023-02-27 17:15:56,212 DEBUG TRAIN Batch 47/6600 loss 4.946355 loss_att 7.895463 loss_ctc 7.215900 loss_rnnt 3.974029 hw_loss 0.149810 lr 0.00029494 rank 5
2023-02-27 17:15:56,213 DEBUG TRAIN Batch 47/6600 loss 3.211835 loss_att 6.236186 loss_ctc 5.061057 loss_rnnt 2.275653 hw_loss 0.158903 lr 0.00029493 rank 3
2023-02-27 17:15:56,214 DEBUG TRAIN Batch 47/6600 loss 1.802299 loss_att 4.110272 loss_ctc 2.500160 loss_rnnt 1.114100 hw_loss 0.250420 lr 0.00029494 rank 1
2023-02-27 17:15:56,214 DEBUG TRAIN Batch 47/6600 loss 8.526905 loss_att 11.804388 loss_ctc 15.320102 loss_rnnt 6.869932 hw_loss 0.179470 lr 0.00029493 rank 7
2023-02-27 17:15:56,217 DEBUG TRAIN Batch 47/6600 loss 7.648886 loss_att 7.757982 loss_ctc 8.847698 loss_rnnt 7.431561 hw_loss 0.066870 lr 0.00029493 rank 2
2023-02-27 17:16:35,620 DEBUG TRAIN Batch 47/6700 loss 1.731826 loss_att 4.442406 loss_ctc 2.444150 loss_rnnt 0.922423 hw_loss 0.323083 lr 0.00029492 rank 3
2023-02-27 17:16:35,634 DEBUG TRAIN Batch 47/6700 loss 3.504894 loss_att 6.928881 loss_ctc 9.317812 loss_rnnt 1.919910 hw_loss 0.234618 lr 0.00029492 rank 5
2023-02-27 17:16:35,634 DEBUG TRAIN Batch 47/6700 loss 3.801914 loss_att 6.182157 loss_ctc 10.258584 loss_rnnt 2.372071 hw_loss 0.174197 lr 0.00029492 rank 4
2023-02-27 17:16:35,641 DEBUG TRAIN Batch 47/6700 loss 3.445674 loss_att 7.247093 loss_ctc 7.628368 loss_rnnt 2.040509 hw_loss 0.163478 lr 0.00029492 rank 0
2023-02-27 17:16:35,642 DEBUG TRAIN Batch 47/6700 loss 3.427830 loss_att 8.271646 loss_ctc 8.356597 loss_rnnt 1.656030 hw_loss 0.273502 lr 0.00029492 rank 2
2023-02-27 17:16:35,643 DEBUG TRAIN Batch 47/6700 loss 3.456845 loss_att 5.427308 loss_ctc 5.502439 loss_rnnt 2.654082 hw_loss 0.254858 lr 0.00029492 rank 6
2023-02-27 17:16:35,644 DEBUG TRAIN Batch 47/6700 loss 4.748531 loss_att 11.609083 loss_ctc 8.654356 loss_rnnt 2.671703 hw_loss 0.344889 lr 0.00029492 rank 1
2023-02-27 17:16:35,663 DEBUG TRAIN Batch 47/6700 loss 3.813628 loss_att 7.260915 loss_ctc 10.685722 loss_rnnt 2.115016 hw_loss 0.174141 lr 0.00029491 rank 7
2023-02-27 17:17:40,421 DEBUG TRAIN Batch 47/6800 loss 14.211030 loss_att 14.933893 loss_ctc 23.666246 loss_rnnt 12.743254 hw_loss 0.117204 lr 0.00029490 rank 6
2023-02-27 17:17:40,433 DEBUG TRAIN Batch 47/6800 loss 6.650841 loss_att 9.059269 loss_ctc 10.979210 loss_rnnt 5.512158 hw_loss 0.149776 lr 0.00029490 rank 3
2023-02-27 17:17:40,435 DEBUG TRAIN Batch 47/6800 loss 5.373348 loss_att 6.855425 loss_ctc 7.009051 loss_rnnt 4.685902 hw_loss 0.324256 lr 0.00029491 rank 4
2023-02-27 17:17:40,440 DEBUG TRAIN Batch 47/6800 loss 8.617219 loss_att 9.993849 loss_ctc 13.834623 loss_rnnt 7.495363 hw_loss 0.282892 lr 0.00029490 rank 7
2023-02-27 17:17:40,441 DEBUG TRAIN Batch 47/6800 loss 5.067062 loss_att 6.804396 loss_ctc 9.429953 loss_rnnt 4.004427 hw_loss 0.250216 lr 0.00029491 rank 5
2023-02-27 17:17:40,442 DEBUG TRAIN Batch 47/6800 loss 8.555988 loss_att 10.525695 loss_ctc 14.040636 loss_rnnt 7.384583 hw_loss 0.086581 lr 0.00029491 rank 0
2023-02-27 17:17:40,444 DEBUG TRAIN Batch 47/6800 loss 3.154479 loss_att 5.613200 loss_ctc 6.708703 loss_rnnt 2.013762 hw_loss 0.328268 lr 0.00029491 rank 1
2023-02-27 17:17:40,494 DEBUG TRAIN Batch 47/6800 loss 4.716520 loss_att 6.813829 loss_ctc 7.947493 loss_rnnt 3.666142 hw_loss 0.375226 lr 0.00029491 rank 2
2023-02-27 17:18:19,852 DEBUG TRAIN Batch 47/6900 loss 4.501485 loss_att 6.387553 loss_ctc 6.601173 loss_rnnt 3.674737 hw_loss 0.317955 lr 0.00029490 rank 4
2023-02-27 17:18:19,859 DEBUG TRAIN Batch 47/6900 loss 3.123681 loss_att 5.118160 loss_ctc 5.195050 loss_rnnt 2.379642 hw_loss 0.129302 lr 0.00029490 rank 0
2023-02-27 17:18:19,861 DEBUG TRAIN Batch 47/6900 loss 5.171114 loss_att 8.384618 loss_ctc 10.272058 loss_rnnt 3.762045 hw_loss 0.161702 lr 0.00029490 rank 5
2023-02-27 17:18:19,862 DEBUG TRAIN Batch 47/6900 loss 3.178528 loss_att 8.138008 loss_ctc 9.026359 loss_rnnt 1.234040 hw_loss 0.324153 lr 0.00029489 rank 3
2023-02-27 17:18:19,868 DEBUG TRAIN Batch 47/6900 loss 6.791387 loss_att 9.106676 loss_ctc 10.858550 loss_rnnt 5.785570 hw_loss 0.000881 lr 0.00029490 rank 1
2023-02-27 17:18:19,869 DEBUG TRAIN Batch 47/6900 loss 6.627239 loss_att 9.863669 loss_ctc 10.932054 loss_rnnt 5.264032 hw_loss 0.266149 lr 0.00029489 rank 6
2023-02-27 17:18:19,871 DEBUG TRAIN Batch 47/6900 loss 8.924838 loss_att 10.957179 loss_ctc 14.821073 loss_rnnt 7.587564 hw_loss 0.271204 lr 0.00029489 rank 7
2023-02-27 17:18:19,912 DEBUG TRAIN Batch 47/6900 loss 4.332868 loss_att 5.965317 loss_ctc 7.854341 loss_rnnt 3.345277 hw_loss 0.359197 lr 0.00029490 rank 2
2023-02-27 17:18:58,812 DEBUG TRAIN Batch 47/7000 loss 10.398067 loss_att 11.256007 loss_ctc 15.922134 loss_rnnt 9.383361 hw_loss 0.199831 lr 0.00029488 rank 3
2023-02-27 17:18:58,815 DEBUG TRAIN Batch 47/7000 loss 2.528188 loss_att 5.230923 loss_ctc 3.058310 loss_rnnt 1.777540 hw_loss 0.261409 lr 0.00029488 rank 4
2023-02-27 17:18:58,816 DEBUG TRAIN Batch 47/7000 loss 5.376033 loss_att 9.240191 loss_ctc 9.600826 loss_rnnt 3.874531 hw_loss 0.310060 lr 0.00029488 rank 0
2023-02-27 17:18:58,817 DEBUG TRAIN Batch 47/7000 loss 6.077256 loss_att 6.397462 loss_ctc 8.846010 loss_rnnt 5.425872 hw_loss 0.409079 lr 0.00029488 rank 1
2023-02-27 17:18:58,816 DEBUG TRAIN Batch 47/7000 loss 14.097539 loss_att 17.267187 loss_ctc 28.866152 loss_rnnt 11.354347 hw_loss 0.262710 lr 0.00029488 rank 6
2023-02-27 17:18:58,819 DEBUG TRAIN Batch 47/7000 loss 3.810261 loss_att 7.787683 loss_ctc 5.140763 loss_rnnt 2.690072 hw_loss 0.276194 lr 0.00029488 rank 2
2023-02-27 17:18:58,821 DEBUG TRAIN Batch 47/7000 loss 4.436047 loss_att 4.764190 loss_ctc 6.172393 loss_rnnt 3.964150 hw_loss 0.327668 lr 0.00029489 rank 5
2023-02-27 17:18:58,821 DEBUG TRAIN Batch 47/7000 loss 10.295806 loss_att 11.066669 loss_ctc 14.875410 loss_rnnt 9.316332 hw_loss 0.402539 lr 0.00029488 rank 7
2023-02-27 17:20:01,238 DEBUG TRAIN Batch 47/7100 loss 3.494617 loss_att 7.359755 loss_ctc 6.173208 loss_rnnt 2.205763 hw_loss 0.297526 lr 0.00029486 rank 7
2023-02-27 17:20:01,243 DEBUG TRAIN Batch 47/7100 loss 2.090648 loss_att 5.794042 loss_ctc 4.303697 loss_rnnt 0.936472 hw_loss 0.222045 lr 0.00029487 rank 4
2023-02-27 17:20:01,244 DEBUG TRAIN Batch 47/7100 loss 6.089025 loss_att 10.107271 loss_ctc 9.172096 loss_rnnt 4.780665 hw_loss 0.175565 lr 0.00029487 rank 5
2023-02-27 17:20:01,246 DEBUG TRAIN Batch 47/7100 loss 11.323834 loss_att 13.709429 loss_ctc 18.048744 loss_rnnt 9.712521 hw_loss 0.445388 lr 0.00029486 rank 6
2023-02-27 17:20:01,252 DEBUG TRAIN Batch 47/7100 loss 4.779178 loss_att 8.156215 loss_ctc 12.464355 loss_rnnt 2.949280 hw_loss 0.243376 lr 0.00029487 rank 1
2023-02-27 17:20:01,270 DEBUG TRAIN Batch 47/7100 loss 3.700659 loss_att 6.673874 loss_ctc 4.971704 loss_rnnt 2.774743 hw_loss 0.303376 lr 0.00029487 rank 2
2023-02-27 17:20:01,271 DEBUG TRAIN Batch 47/7100 loss 6.567472 loss_att 9.635107 loss_ctc 11.361979 loss_rnnt 5.186566 hw_loss 0.240206 lr 0.00029487 rank 3
2023-02-27 17:20:01,275 DEBUG TRAIN Batch 47/7100 loss 3.359152 loss_att 6.474844 loss_ctc 6.684603 loss_rnnt 2.191750 hw_loss 0.189131 lr 0.00029487 rank 0
2023-02-27 17:20:43,626 DEBUG TRAIN Batch 47/7200 loss 6.412760 loss_att 10.832690 loss_ctc 16.004877 loss_rnnt 4.120895 hw_loss 0.241745 lr 0.00029486 rank 0
2023-02-27 17:20:43,629 DEBUG TRAIN Batch 47/7200 loss 8.801484 loss_att 12.545916 loss_ctc 15.408191 loss_rnnt 7.094136 hw_loss 0.145438 lr 0.00029485 rank 7
2023-02-27 17:20:43,629 DEBUG TRAIN Batch 47/7200 loss 2.776788 loss_att 5.286383 loss_ctc 8.225163 loss_rnnt 1.451557 hw_loss 0.181616 lr 0.00029486 rank 4
2023-02-27 17:20:43,631 DEBUG TRAIN Batch 47/7200 loss 5.530541 loss_att 8.311433 loss_ctc 8.162604 loss_rnnt 4.515479 hw_loss 0.202391 lr 0.00029486 rank 5
2023-02-27 17:20:43,630 DEBUG TRAIN Batch 47/7200 loss 5.912594 loss_att 9.656452 loss_ctc 10.425387 loss_rnnt 4.517613 hw_loss 0.083444 lr 0.00029485 rank 3
2023-02-27 17:20:43,636 DEBUG TRAIN Batch 47/7200 loss 3.887792 loss_att 5.769497 loss_ctc 5.196990 loss_rnnt 3.255937 hw_loss 0.151789 lr 0.00029486 rank 2
2023-02-27 17:20:43,637 DEBUG TRAIN Batch 47/7200 loss 10.219490 loss_att 10.793364 loss_ctc 11.961050 loss_rnnt 9.712131 hw_loss 0.300709 lr 0.00029486 rank 1
2023-02-27 17:20:43,654 DEBUG TRAIN Batch 47/7200 loss 2.919714 loss_att 7.238961 loss_ctc 6.312032 loss_rnnt 1.602497 hw_loss 0.001984 lr 0.00029485 rank 6
2023-02-27 17:21:22,228 DEBUG TRAIN Batch 47/7300 loss 5.032050 loss_att 7.787125 loss_ctc 7.449133 loss_rnnt 4.125733 hw_loss 0.061920 lr 0.00029485 rank 1
2023-02-27 17:21:22,228 DEBUG TRAIN Batch 47/7300 loss 11.776026 loss_att 16.124365 loss_ctc 21.883209 loss_rnnt 9.427629 hw_loss 0.245819 lr 0.00029484 rank 3
2023-02-27 17:21:22,229 DEBUG TRAIN Batch 47/7300 loss 11.768215 loss_att 15.063747 loss_ctc 13.831701 loss_rnnt 10.729881 hw_loss 0.195179 lr 0.00029484 rank 6
2023-02-27 17:21:22,229 DEBUG TRAIN Batch 47/7300 loss 8.753279 loss_att 9.871881 loss_ctc 11.244579 loss_rnnt 8.168904 hw_loss 0.053401 lr 0.00029484 rank 0
2023-02-27 17:21:22,229 DEBUG TRAIN Batch 47/7300 loss 8.115290 loss_att 9.815942 loss_ctc 13.409420 loss_rnnt 6.971170 hw_loss 0.183946 lr 0.00029484 rank 4
2023-02-27 17:21:22,235 DEBUG TRAIN Batch 47/7300 loss 2.850029 loss_att 5.501883 loss_ctc 5.138583 loss_rnnt 1.831377 hw_loss 0.343389 lr 0.00029485 rank 5
2023-02-27 17:21:22,234 DEBUG TRAIN Batch 47/7300 loss 6.947575 loss_att 9.386396 loss_ctc 8.262808 loss_rnnt 6.099095 hw_loss 0.347532 lr 0.00029484 rank 2
2023-02-27 17:21:22,236 DEBUG TRAIN Batch 47/7300 loss 11.153117 loss_att 13.212608 loss_ctc 18.227577 loss_rnnt 9.748472 hw_loss 0.092786 lr 0.00029484 rank 7
2023-02-27 17:22:01,190 DEBUG TRAIN Batch 47/7400 loss 7.393842 loss_att 9.030272 loss_ctc 14.564054 loss_rnnt 5.991244 hw_loss 0.223657 lr 0.00029483 rank 4
2023-02-27 17:22:01,191 DEBUG TRAIN Batch 47/7400 loss 4.967568 loss_att 8.631291 loss_ctc 13.362419 loss_rnnt 2.986416 hw_loss 0.242051 lr 0.00029483 rank 5
2023-02-27 17:22:01,192 DEBUG TRAIN Batch 47/7400 loss 2.212143 loss_att 4.050432 loss_ctc 6.068027 loss_rnnt 1.197333 hw_loss 0.249439 lr 0.00029483 rank 6
2023-02-27 17:22:01,200 DEBUG TRAIN Batch 47/7400 loss 12.472147 loss_att 13.801066 loss_ctc 25.169827 loss_rnnt 10.415448 hw_loss 0.183544 lr 0.00029482 rank 7
2023-02-27 17:22:01,203 DEBUG TRAIN Batch 47/7400 loss 6.653731 loss_att 11.070453 loss_ctc 12.284865 loss_rnnt 4.893658 hw_loss 0.236081 lr 0.00029483 rank 3
2023-02-27 17:22:01,205 DEBUG TRAIN Batch 47/7400 loss 6.147284 loss_att 9.871256 loss_ctc 9.762615 loss_rnnt 4.849030 hw_loss 0.133903 lr 0.00029483 rank 1
2023-02-27 17:22:01,208 DEBUG TRAIN Batch 47/7400 loss 4.192133 loss_att 7.415415 loss_ctc 8.584179 loss_rnnt 2.910600 hw_loss 0.096134 lr 0.00029483 rank 0
2023-02-27 17:22:01,216 DEBUG TRAIN Batch 47/7400 loss 5.634710 loss_att 8.798573 loss_ctc 7.076922 loss_rnnt 4.713997 hw_loss 0.179335 lr 0.00029483 rank 2
2023-02-27 17:23:07,284 DEBUG TRAIN Batch 47/7500 loss 5.770586 loss_att 8.677404 loss_ctc 12.025712 loss_rnnt 4.298639 hw_loss 0.106061 lr 0.00029482 rank 3
2023-02-27 17:23:07,284 DEBUG TRAIN Batch 47/7500 loss 11.317544 loss_att 11.783834 loss_ctc 19.159891 loss_rnnt 10.106092 hw_loss 0.136026 lr 0.00029482 rank 0
2023-02-27 17:23:07,301 DEBUG TRAIN Batch 47/7500 loss 6.199039 loss_att 10.400637 loss_ctc 11.156803 loss_rnnt 4.602815 hw_loss 0.177881 lr 0.00029481 rank 6
2023-02-27 17:23:07,304 DEBUG TRAIN Batch 47/7500 loss 6.562225 loss_att 9.341888 loss_ctc 11.466230 loss_rnnt 5.195472 hw_loss 0.294288 lr 0.00029482 rank 5
2023-02-27 17:23:07,304 DEBUG TRAIN Batch 47/7500 loss 7.275805 loss_att 8.979137 loss_ctc 11.339979 loss_rnnt 6.258743 hw_loss 0.252197 lr 0.00029482 rank 4
2023-02-27 17:23:07,305 DEBUG TRAIN Batch 47/7500 loss 3.447397 loss_att 5.013761 loss_ctc 4.933858 loss_rnnt 2.804078 hw_loss 0.247222 lr 0.00029481 rank 7
2023-02-27 17:23:07,311 DEBUG TRAIN Batch 47/7500 loss 6.177234 loss_att 9.049429 loss_ctc 10.342104 loss_rnnt 4.902562 hw_loss 0.271719 lr 0.00029482 rank 2
2023-02-27 17:23:07,357 DEBUG TRAIN Batch 47/7500 loss 8.000895 loss_att 11.075002 loss_ctc 14.460829 loss_rnnt 6.353859 hw_loss 0.320417 lr 0.00029482 rank 1
2023-02-27 17:23:46,064 DEBUG TRAIN Batch 47/7600 loss 11.927469 loss_att 13.695961 loss_ctc 20.759079 loss_rnnt 10.209347 hw_loss 0.350393 lr 0.00029480 rank 6
2023-02-27 17:23:46,065 DEBUG TRAIN Batch 47/7600 loss 7.553649 loss_att 9.144213 loss_ctc 12.303044 loss_rnnt 6.462680 hw_loss 0.261757 lr 0.00029481 rank 5
2023-02-27 17:23:46,071 DEBUG TRAIN Batch 47/7600 loss 8.980026 loss_att 12.821398 loss_ctc 14.383042 loss_rnnt 7.281338 hw_loss 0.393769 lr 0.00029481 rank 0
2023-02-27 17:23:46,071 DEBUG TRAIN Batch 47/7600 loss 9.426094 loss_att 10.431985 loss_ctc 17.806692 loss_rnnt 7.991646 hw_loss 0.217233 lr 0.00029481 rank 1
2023-02-27 17:23:46,072 DEBUG TRAIN Batch 47/7600 loss 1.982205 loss_att 3.795754 loss_ctc 4.913403 loss_rnnt 1.071309 hw_loss 0.295049 lr 0.00029480 rank 3
2023-02-27 17:23:46,075 DEBUG TRAIN Batch 47/7600 loss 4.095419 loss_att 4.986031 loss_ctc 7.916032 loss_rnnt 3.218716 hw_loss 0.354686 lr 0.00029481 rank 4
2023-02-27 17:23:46,077 DEBUG TRAIN Batch 47/7600 loss 10.178486 loss_att 13.213947 loss_ctc 19.371983 loss_rnnt 8.227831 hw_loss 0.220806 lr 0.00029481 rank 2
2023-02-27 17:23:46,119 DEBUG TRAIN Batch 47/7600 loss 7.056019 loss_att 8.019987 loss_ctc 9.335794 loss_rnnt 6.443633 hw_loss 0.216793 lr 0.00029480 rank 7
2023-02-27 17:24:24,514 DEBUG TRAIN Batch 47/7700 loss 4.555684 loss_att 8.076862 loss_ctc 8.696708 loss_rnnt 3.149873 hw_loss 0.280198 lr 0.00029479 rank 0
2023-02-27 17:24:24,517 DEBUG TRAIN Batch 47/7700 loss 5.929729 loss_att 10.056210 loss_ctc 11.875093 loss_rnnt 4.268916 hw_loss 0.080252 lr 0.00029480 rank 5
2023-02-27 17:24:24,517 DEBUG TRAIN Batch 47/7700 loss 7.224588 loss_att 8.079708 loss_ctc 11.968534 loss_rnnt 6.240567 hw_loss 0.338386 lr 0.00029479 rank 3
2023-02-27 17:24:24,518 DEBUG TRAIN Batch 47/7700 loss 2.040556 loss_att 4.187912 loss_ctc 4.292056 loss_rnnt 1.197192 hw_loss 0.213174 lr 0.00029479 rank 6
2023-02-27 17:24:24,518 DEBUG TRAIN Batch 47/7700 loss 5.964445 loss_att 10.489404 loss_ctc 11.277472 loss_rnnt 4.226008 hw_loss 0.234451 lr 0.00029480 rank 1
2023-02-27 17:24:24,518 DEBUG TRAIN Batch 47/7700 loss 3.726575 loss_att 6.825612 loss_ctc 4.592060 loss_rnnt 2.956994 hw_loss 0.064454 lr 0.00029479 rank 7
2023-02-27 17:24:24,523 DEBUG TRAIN Batch 47/7700 loss 1.919196 loss_att 5.115951 loss_ctc 2.496596 loss_rnnt 1.088723 hw_loss 0.214002 lr 0.00029479 rank 4
2023-02-27 17:24:24,523 DEBUG TRAIN Batch 47/7700 loss 5.519376 loss_att 9.418399 loss_ctc 11.916418 loss_rnnt 3.743313 hw_loss 0.268724 lr 0.00029479 rank 2
2023-02-27 17:25:04,723 DEBUG TRAIN Batch 47/7800 loss 4.253519 loss_att 8.065259 loss_ctc 8.414200 loss_rnnt 2.747895 hw_loss 0.353471 lr 0.00029478 rank 5
2023-02-27 17:25:04,725 DEBUG TRAIN Batch 47/7800 loss 6.563016 loss_att 12.726459 loss_ctc 14.179599 loss_rnnt 4.211092 hw_loss 0.194420 lr 0.00029478 rank 4
2023-02-27 17:25:04,728 DEBUG TRAIN Batch 47/7800 loss 6.407598 loss_att 7.259412 loss_ctc 9.890797 loss_rnnt 5.697145 hw_loss 0.141869 lr 0.00029477 rank 7
2023-02-27 17:25:04,739 DEBUG TRAIN Batch 47/7800 loss 7.170205 loss_att 8.186289 loss_ctc 10.297680 loss_rnnt 6.406821 hw_loss 0.268444 lr 0.00029478 rank 2
2023-02-27 17:25:04,739 DEBUG TRAIN Batch 47/7800 loss 5.534371 loss_att 8.195050 loss_ctc 8.650328 loss_rnnt 4.430555 hw_loss 0.292912 lr 0.00029478 rank 3
2023-02-27 17:25:04,740 DEBUG TRAIN Batch 47/7800 loss 1.424497 loss_att 4.743821 loss_ctc 1.556549 loss_rnnt 0.590852 hw_loss 0.285324 lr 0.00029478 rank 6
2023-02-27 17:25:04,743 DEBUG TRAIN Batch 47/7800 loss 6.197416 loss_att 7.357728 loss_ctc 8.656229 loss_rnnt 5.555820 hw_loss 0.153174 lr 0.00029478 rank 1
2023-02-27 17:25:04,743 DEBUG TRAIN Batch 47/7800 loss 5.188783 loss_att 7.203849 loss_ctc 5.213848 loss_rnnt 4.690736 hw_loss 0.171921 lr 0.00029478 rank 0
2023-02-27 17:26:07,787 DEBUG TRAIN Batch 47/7900 loss 5.885324 loss_att 8.973407 loss_ctc 9.701665 loss_rnnt 4.579381 hw_loss 0.336526 lr 0.00029477 rank 2
2023-02-27 17:26:07,790 DEBUG TRAIN Batch 47/7900 loss 3.949598 loss_att 7.639123 loss_ctc 6.721839 loss_rnnt 2.750696 hw_loss 0.171308 lr 0.00029477 rank 5
2023-02-27 17:26:07,800 DEBUG TRAIN Batch 47/7900 loss 3.962408 loss_att 6.259359 loss_ctc 6.127580 loss_rnnt 3.180354 hw_loss 0.063701 lr 0.00029477 rank 0
2023-02-27 17:26:07,807 DEBUG TRAIN Batch 47/7900 loss 7.960286 loss_att 9.580713 loss_ctc 13.347975 loss_rnnt 6.691546 hw_loss 0.424304 lr 0.00029477 rank 4
2023-02-27 17:26:07,808 DEBUG TRAIN Batch 47/7900 loss 2.978673 loss_att 5.139734 loss_ctc 4.578404 loss_rnnt 2.251329 hw_loss 0.153438 lr 0.00029476 rank 6
2023-02-27 17:26:07,808 DEBUG TRAIN Batch 47/7900 loss 2.433166 loss_att 5.836581 loss_ctc 5.023975 loss_rnnt 1.263743 hw_loss 0.268684 lr 0.00029476 rank 7
2023-02-27 17:26:07,808 DEBUG TRAIN Batch 47/7900 loss 6.332179 loss_att 9.472931 loss_ctc 12.933543 loss_rnnt 4.673548 hw_loss 0.281807 lr 0.00029476 rank 3
2023-02-27 17:26:07,810 DEBUG TRAIN Batch 47/7900 loss 8.528935 loss_att 11.804018 loss_ctc 16.408947 loss_rnnt 6.664690 hw_loss 0.297298 lr 0.00029477 rank 1
2023-02-27 17:26:46,628 DEBUG TRAIN Batch 47/8000 loss 6.910621 loss_att 9.289710 loss_ctc 11.697832 loss_rnnt 5.660406 hw_loss 0.255191 lr 0.00029475 rank 0
2023-02-27 17:26:46,632 DEBUG TRAIN Batch 47/8000 loss 5.916381 loss_att 8.743505 loss_ctc 8.603307 loss_rnnt 4.864336 hw_loss 0.240682 lr 0.00029475 rank 3
2023-02-27 17:26:46,651 DEBUG TRAIN Batch 47/8000 loss 6.598159 loss_att 9.023426 loss_ctc 7.149091 loss_rnnt 5.888926 hw_loss 0.282602 lr 0.00029476 rank 5
2023-02-27 17:26:46,652 DEBUG TRAIN Batch 47/8000 loss 4.021325 loss_att 8.235582 loss_ctc 7.265653 loss_rnnt 2.691113 hw_loss 0.102717 lr 0.00029475 rank 7
2023-02-27 17:26:46,651 DEBUG TRAIN Batch 47/8000 loss 6.243429 loss_att 9.061602 loss_ctc 12.992765 loss_rnnt 4.685208 hw_loss 0.177514 lr 0.00029476 rank 1
2023-02-27 17:26:46,653 DEBUG TRAIN Batch 47/8000 loss 11.291900 loss_att 12.151454 loss_ctc 17.665257 loss_rnnt 10.111406 hw_loss 0.297753 lr 0.00029475 rank 2
2023-02-27 17:26:46,656 DEBUG TRAIN Batch 47/8000 loss 4.144773 loss_att 8.053617 loss_ctc 8.349341 loss_rnnt 2.752534 hw_loss 0.093490 lr 0.00029475 rank 6
2023-02-27 17:26:46,699 DEBUG TRAIN Batch 47/8000 loss 13.866179 loss_att 18.543007 loss_ctc 23.788313 loss_rnnt 11.528502 hw_loss 0.148801 lr 0.00029475 rank 4
2023-02-27 17:27:25,573 DEBUG TRAIN Batch 47/8100 loss 4.628195 loss_att 7.370301 loss_ctc 7.516048 loss_rnnt 3.520770 hw_loss 0.326167 lr 0.00029474 rank 2
2023-02-27 17:27:25,576 DEBUG TRAIN Batch 47/8100 loss 11.011223 loss_att 13.285851 loss_ctc 21.112797 loss_rnnt 9.089927 hw_loss 0.224050 lr 0.00029474 rank 4
2023-02-27 17:27:25,578 DEBUG TRAIN Batch 47/8100 loss 3.760314 loss_att 6.808751 loss_ctc 7.610196 loss_rnnt 2.452916 hw_loss 0.345736 lr 0.00029473 rank 7
2023-02-27 17:27:25,583 DEBUG TRAIN Batch 47/8100 loss 10.107411 loss_att 15.089555 loss_ctc 23.265175 loss_rnnt 7.262357 hw_loss 0.176732 lr 0.00029474 rank 3
2023-02-27 17:27:25,585 DEBUG TRAIN Batch 47/8100 loss 10.483895 loss_att 13.434584 loss_ctc 17.699682 loss_rnnt 8.847567 hw_loss 0.157658 lr 0.00029474 rank 6
2023-02-27 17:27:25,585 DEBUG TRAIN Batch 47/8100 loss 5.176700 loss_att 6.950346 loss_ctc 9.223475 loss_rnnt 4.178607 hw_loss 0.194613 lr 0.00029474 rank 0
2023-02-27 17:27:25,598 DEBUG TRAIN Batch 47/8100 loss 7.567520 loss_att 10.113620 loss_ctc 11.845432 loss_rnnt 6.329808 hw_loss 0.296443 lr 0.00029474 rank 1
2023-02-27 17:27:25,604 DEBUG TRAIN Batch 47/8100 loss 3.662932 loss_att 6.079707 loss_ctc 6.939548 loss_rnnt 2.575957 hw_loss 0.312633 lr 0.00029475 rank 5
2023-02-27 17:28:05,843 DEBUG TRAIN Batch 47/8200 loss 3.385223 loss_att 6.118715 loss_ctc 5.900710 loss_rnnt 2.339897 hw_loss 0.306055 lr 0.00029472 rank 7
2023-02-27 17:28:05,845 DEBUG TRAIN Batch 47/8200 loss 5.771251 loss_att 10.045165 loss_ctc 12.695338 loss_rnnt 3.908799 hw_loss 0.158357 lr 0.00029473 rank 2
2023-02-27 17:28:05,849 DEBUG TRAIN Batch 47/8200 loss 10.314604 loss_att 13.065071 loss_ctc 16.233738 loss_rnnt 8.876376 hw_loss 0.185469 lr 0.00029473 rank 0
2023-02-27 17:28:05,851 DEBUG TRAIN Batch 47/8200 loss 3.121851 loss_att 5.437296 loss_ctc 5.900381 loss_rnnt 2.186715 hw_loss 0.190456 lr 0.00029473 rank 3
2023-02-27 17:28:05,853 DEBUG TRAIN Batch 47/8200 loss 5.825180 loss_att 9.576000 loss_ctc 9.063099 loss_rnnt 4.510777 hw_loss 0.248468 lr 0.00029472 rank 6
2023-02-27 17:28:05,854 DEBUG TRAIN Batch 47/8200 loss 6.084499 loss_att 7.150626 loss_ctc 9.510051 loss_rnnt 5.306789 hw_loss 0.202021 lr 0.00029473 rank 5
2023-02-27 17:28:05,859 DEBUG TRAIN Batch 47/8200 loss 3.814214 loss_att 6.281102 loss_ctc 6.055558 loss_rnnt 2.853777 hw_loss 0.315400 lr 0.00029473 rank 4
2023-02-27 17:28:05,862 DEBUG TRAIN Batch 47/8200 loss 15.133368 loss_att 16.891195 loss_ctc 27.053562 loss_rnnt 13.096486 hw_loss 0.179918 lr 0.00029473 rank 1
2023-02-27 17:28:43,903 DEBUG TRAIN Batch 47/8300 loss 9.784814 loss_att 11.496056 loss_ctc 15.188423 loss_rnnt 8.589655 hw_loss 0.248307 lr 0.00029471 rank 3
2023-02-27 17:28:43,912 DEBUG TRAIN Batch 47/8300 loss 2.251940 loss_att 4.120279 loss_ctc 3.554206 loss_rnnt 1.528223 hw_loss 0.330774 lr 0.00029471 rank 6
2023-02-27 17:28:43,912 DEBUG TRAIN Batch 47/8300 loss 5.790094 loss_att 12.805449 loss_ctc 12.679377 loss_rnnt 3.303715 hw_loss 0.308882 lr 0.00029472 rank 0
2023-02-27 17:28:43,916 DEBUG TRAIN Batch 47/8300 loss 5.625092 loss_att 8.627910 loss_ctc 9.266638 loss_rnnt 4.472233 hw_loss 0.125166 lr 0.00029472 rank 1
2023-02-27 17:28:43,921 DEBUG TRAIN Batch 47/8300 loss 7.138078 loss_att 9.182430 loss_ctc 11.057320 loss_rnnt 6.069438 hw_loss 0.257257 lr 0.00029472 rank 2
2023-02-27 17:28:43,924 DEBUG TRAIN Batch 47/8300 loss 7.459605 loss_att 8.520523 loss_ctc 8.761415 loss_rnnt 6.895391 hw_loss 0.334603 lr 0.00029472 rank 4
2023-02-27 17:28:43,929 DEBUG TRAIN Batch 47/8300 loss 9.113573 loss_att 10.442003 loss_ctc 16.818935 loss_rnnt 7.714004 hw_loss 0.199693 lr 0.00029471 rank 7
2023-02-27 17:28:43,931 DEBUG TRAIN Batch 47/8300 loss 9.540428 loss_att 10.350570 loss_ctc 15.901683 loss_rnnt 8.417788 hw_loss 0.210834 lr 0.00029472 rank 5
2023-02-27 17:29:12,077 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 5
2023-02-27 17:29:12,083 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 0
2023-02-27 17:29:12,119 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 3
2023-02-27 17:29:12,158 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 1
2023-02-27 17:29:12,229 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 4
2023-02-27 17:29:12,231 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 2
2023-02-27 17:29:12,239 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 7
2023-02-27 17:29:12,265 DEBUG CV Batch 47/0 loss 0.712581 loss_att 0.780593 loss_ctc 1.058954 loss_rnnt 0.440943 hw_loss 0.397224 history loss 0.686189 rank 6
2023-02-27 17:29:23,331 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 5
2023-02-27 17:29:23,562 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 1
2023-02-27 17:29:23,581 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 2
2023-02-27 17:29:23,601 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 7
2023-02-27 17:29:23,608 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 4
2023-02-27 17:29:23,846 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 0
2023-02-27 17:29:23,864 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 3
2023-02-27 17:29:24,010 DEBUG CV Batch 47/100 loss 3.134336 loss_att 4.429729 loss_ctc 6.620044 loss_rnnt 2.239221 hw_loss 0.321140 history loss 2.939486 rank 6
2023-02-27 17:29:36,743 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 5
2023-02-27 17:29:36,972 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 1
2023-02-27 17:29:36,975 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 7
2023-02-27 17:29:37,018 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 4
2023-02-27 17:29:37,115 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 2
2023-02-27 17:29:37,780 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 3
2023-02-27 17:29:37,863 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 6
2023-02-27 17:29:37,927 DEBUG CV Batch 47/200 loss 5.150263 loss_att 8.131324 loss_ctc 7.963558 loss_rnnt 4.143983 hw_loss 0.065554 history loss 3.496167 rank 0
2023-02-27 17:29:48,862 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 5
2023-02-27 17:29:49,058 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 1
2023-02-27 17:29:49,132 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 4
2023-02-27 17:29:49,173 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 7
2023-02-27 17:29:49,352 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 2
2023-02-27 17:29:50,441 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 6
2023-02-27 17:29:50,653 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 3
2023-02-27 17:29:50,864 DEBUG CV Batch 47/300 loss 3.601785 loss_att 4.038630 loss_ctc 6.215836 loss_rnnt 3.017606 hw_loss 0.278006 history loss 3.646202 rank 0
2023-02-27 17:30:01,104 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 5
2023-02-27 17:30:01,319 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 7
2023-02-27 17:30:01,399 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 1
2023-02-27 17:30:01,701 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 4
2023-02-27 17:30:01,797 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 2
2023-02-27 17:30:02,888 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 6
2023-02-27 17:30:03,396 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 3
2023-02-27 17:30:03,630 DEBUG CV Batch 47/400 loss 16.266808 loss_att 67.077377 loss_ctc 12.048958 loss_rnnt 6.536041 hw_loss 0.245686 history loss 4.435307 rank 0
2023-02-27 17:30:12,059 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 1
2023-02-27 17:30:12,081 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 7
2023-02-27 17:30:12,333 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 5
2023-02-27 17:30:12,459 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 4
2023-02-27 17:30:12,734 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 2
2023-02-27 17:30:14,142 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 6
2023-02-27 17:30:14,653 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 3
2023-02-27 17:30:15,180 DEBUG CV Batch 47/500 loss 4.131605 loss_att 4.099513 loss_ctc 5.463542 loss_rnnt 3.841880 hw_loss 0.222286 history loss 5.106862 rank 0
2023-02-27 17:30:24,286 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 1
2023-02-27 17:30:24,548 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 7
2023-02-27 17:30:24,614 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 5
2023-02-27 17:30:24,754 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 4
2023-02-27 17:30:25,009 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 2
2023-02-27 17:30:26,803 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 6
2023-02-27 17:30:27,383 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 3
2023-02-27 17:30:28,087 DEBUG CV Batch 47/600 loss 6.598307 loss_att 6.058768 loss_ctc 9.421710 loss_rnnt 6.084445 hw_loss 0.459966 history loss 5.961467 rank 0
2023-02-27 17:30:35,807 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 1
2023-02-27 17:30:35,996 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 7
2023-02-27 17:30:36,212 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 5
2023-02-27 17:30:36,301 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 4
2023-02-27 17:30:36,519 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 2
2023-02-27 17:30:38,744 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 6
2023-02-27 17:30:39,442 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 3
2023-02-27 17:30:40,274 DEBUG CV Batch 47/700 loss 11.253832 loss_att 23.537376 loss_ctc 13.734268 loss_rnnt 8.465378 hw_loss 0.001913 history loss 6.496597 rank 0
2023-02-27 17:30:47,242 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 1
2023-02-27 17:30:47,408 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 7
2023-02-27 17:30:47,889 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 4
2023-02-27 17:30:47,905 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 5
2023-02-27 17:30:47,989 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 2
2023-02-27 17:30:50,511 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 6
2023-02-27 17:30:51,505 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 3
2023-02-27 17:30:52,449 DEBUG CV Batch 47/800 loss 5.557914 loss_att 7.097090 loss_ctc 11.870367 loss_rnnt 4.292181 hw_loss 0.217943 history loss 6.016812 rank 0
2023-02-27 17:31:00,821 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 7
2023-02-27 17:31:00,934 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 1
2023-02-27 17:31:01,174 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 4
2023-02-27 17:31:01,314 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 5
2023-02-27 17:31:01,631 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 2
2023-02-27 17:31:04,287 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 6
2023-02-27 17:31:05,366 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 3
2023-02-27 17:31:06,425 DEBUG CV Batch 47/900 loss 9.359003 loss_att 9.755365 loss_ctc 18.079899 loss_rnnt 8.034485 hw_loss 0.154611 history loss 5.853768 rank 0
2023-02-27 17:31:13,336 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 7
2023-02-27 17:31:13,588 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 1
2023-02-27 17:31:13,795 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 4
2023-02-27 17:31:13,886 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 5
2023-02-27 17:31:14,092 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 2
2023-02-27 17:31:17,091 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 6
2023-02-27 17:31:18,280 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 3
2023-02-27 17:31:19,297 DEBUG CV Batch 47/1000 loss 4.141827 loss_att 4.048757 loss_ctc 5.400083 loss_rnnt 3.834817 hw_loss 0.295981 history loss 5.661875 rank 0
2023-02-27 17:31:25,324 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 7
2023-02-27 17:31:25,612 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 1
2023-02-27 17:31:25,620 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 4
2023-02-27 17:31:25,910 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 5
2023-02-27 17:31:26,064 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 2
2023-02-27 17:31:29,591 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 6
2023-02-27 17:31:30,985 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 3
2023-02-27 17:31:32,049 DEBUG CV Batch 47/1100 loss 4.924933 loss_att 4.729453 loss_ctc 8.360659 loss_rnnt 4.328133 hw_loss 0.333375 history loss 5.621859 rank 0
2023-02-27 17:31:36,284 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 7
2023-02-27 17:31:36,373 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 1
2023-02-27 17:31:36,476 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 4
2023-02-27 17:31:36,875 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 5
2023-02-27 17:31:37,043 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 2
2023-02-27 17:31:40,915 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 6
2023-02-27 17:31:42,345 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 3
2023-02-27 17:31:43,411 DEBUG CV Batch 47/1200 loss 7.165441 loss_att 6.668135 loss_ctc 8.315581 loss_rnnt 6.933258 hw_loss 0.334299 history loss 5.914759 rank 0
2023-02-27 17:31:48,450 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 1
2023-02-27 17:31:48,561 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 4
2023-02-27 17:31:48,611 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 7
2023-02-27 17:31:49,153 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 5
2023-02-27 17:31:49,379 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 2
2023-02-27 17:31:53,609 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 6
2023-02-27 17:31:55,178 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 3
2023-02-27 17:31:56,246 DEBUG CV Batch 47/1300 loss 4.051722 loss_att 4.192894 loss_ctc 6.285949 loss_rnnt 3.545039 hw_loss 0.338536 history loss 6.201667 rank 0
2023-02-27 17:31:59,837 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 1
2023-02-27 17:31:59,935 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 7
2023-02-27 17:31:59,965 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 4
2023-02-27 17:32:00,512 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 5
2023-02-27 17:32:01,014 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 2
2023-02-27 17:32:05,293 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 6
2023-02-27 17:32:07,144 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 3
2023-02-27 17:32:08,245 DEBUG CV Batch 47/1400 loss 3.929447 loss_att 15.006420 loss_ctc 3.553664 loss_rnnt 1.724818 hw_loss 0.073758 history loss 6.474696 rank 0
2023-02-27 17:32:11,524 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 4
2023-02-27 17:32:11,544 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 1
2023-02-27 17:32:11,666 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 7
2023-02-27 17:32:12,224 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 5
2023-02-27 17:32:13,187 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 2
2023-02-27 17:32:17,524 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 6
2023-02-27 17:32:19,372 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 3
2023-02-27 17:32:20,364 DEBUG CV Batch 47/1500 loss 8.238779 loss_att 7.996560 loss_ctc 8.935188 loss_rnnt 8.037888 hw_loss 0.293400 history loss 6.339507 rank 0
2023-02-27 17:32:24,592 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 1
2023-02-27 17:32:24,597 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 4
2023-02-27 17:32:24,803 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 7
2023-02-27 17:32:25,323 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 5
2023-02-27 17:32:26,435 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 2
2023-02-27 17:32:31,046 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 6
2023-02-27 17:32:33,002 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 3
2023-02-27 17:32:34,090 DEBUG CV Batch 47/1600 loss 9.805180 loss_att 13.069275 loss_ctc 12.224753 loss_rnnt 8.759295 hw_loss 0.132105 history loss 6.299318 rank 0
2023-02-27 17:32:37,083 DEBUG CV Batch 47/1700 loss 8.325190 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 4
2023-02-27 17:32:37,316 DEBUG CV Batch 47/1700 loss 8.325190 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 7
2023-02-27 17:32:37,406 DEBUG CV Batch 47/1700 loss 8.325191 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 1
2023-02-27 17:32:37,876 DEBUG CV Batch 47/1700 loss 8.325191 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 5
2023-02-27 17:32:39,019 DEBUG CV Batch 47/1700 loss 8.325190 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 2
2023-02-27 17:32:43,615 DEBUG CV Batch 47/1700 loss 8.325190 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 6
2023-02-27 17:32:45,571 DEBUG CV Batch 47/1700 loss 8.325191 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 3
2023-02-27 17:32:46,191 INFO Epoch 47 CV info cv_loss 6.209897036113255
2023-02-27 17:32:46,191 INFO Epoch 48 TRAIN info lr 0.000294710212607797
2023-02-27 17:32:46,193 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:46,531 INFO Epoch 47 CV info cv_loss 6.209897036494453
2023-02-27 17:32:46,532 INFO Epoch 48 TRAIN info lr 0.0002947045814747238
2023-02-27 17:32:46,537 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:46,720 DEBUG CV Batch 47/1700 loss 8.325190 loss_att 7.475621 loss_ctc 15.363056 loss_rnnt 7.389166 hw_loss 0.314165 history loss 6.231677 rank 0
2023-02-27 17:32:46,808 INFO Epoch 47 CV info cv_loss 6.209897036431997
2023-02-27 17:32:46,809 INFO Epoch 48 TRAIN info lr 0.00029471328427098484
2023-02-27 17:32:46,813 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:47,113 INFO Epoch 47 CV info cv_loss 6.209897035755748
2023-02-27 17:32:47,114 INFO Epoch 48 TRAIN info lr 0.0002947137962241872
2023-02-27 17:32:47,119 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:48,343 INFO Epoch 47 CV info cv_loss 6.209897036890726
2023-02-27 17:32:48,343 INFO Epoch 48 TRAIN info lr 0.000294708676812219
2023-02-27 17:32:48,348 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:52,931 INFO Epoch 47 CV info cv_loss 6.209897035363782
2023-02-27 17:32:52,931 INFO Epoch 48 TRAIN info lr 0.0002947091887414105
2023-02-27 17:32:52,935 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:55,007 INFO Epoch 47 CV info cv_loss 6.2098970368799575
2023-02-27 17:32:55,007 INFO Epoch 48 TRAIN info lr 0.0002947097006732698
2023-02-27 17:32:55,009 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:32:56,306 INFO Epoch 47 CV info cv_loss 6.209897035800974
2023-02-27 17:32:56,306 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/47.pt
2023-02-27 17:32:56,863 INFO Epoch 48 TRAIN info lr 0.00029471123648485467
2023-02-27 17:32:56,866 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 17:33:55,997 DEBUG TRAIN Batch 48/0 loss 5.456457 loss_att 5.605555 loss_ctc 8.875354 loss_rnnt 4.742856 hw_loss 0.427365 lr 0.00029470 rank 7
2023-02-27 17:33:55,999 DEBUG TRAIN Batch 48/0 loss 5.424263 loss_att 5.437704 loss_ctc 6.866240 loss_rnnt 5.066416 hw_loss 0.305428 lr 0.00029471 rank 6
2023-02-27 17:33:55,999 DEBUG TRAIN Batch 48/0 loss 6.360644 loss_att 6.702549 loss_ctc 8.611200 loss_rnnt 5.787625 hw_loss 0.383557 lr 0.00029471 rank 5
2023-02-27 17:33:55,999 DEBUG TRAIN Batch 48/0 loss 7.404782 loss_att 7.325936 loss_ctc 10.868308 loss_rnnt 6.744790 hw_loss 0.401171 lr 0.00029471 rank 4
2023-02-27 17:33:56,002 DEBUG TRAIN Batch 48/0 loss 9.596249 loss_att 9.950264 loss_ctc 13.199568 loss_rnnt 8.842820 hw_loss 0.379092 lr 0.00029471 rank 1
2023-02-27 17:33:56,004 DEBUG TRAIN Batch 48/0 loss 6.240797 loss_att 6.294929 loss_ctc 8.554910 loss_rnnt 5.751119 hw_loss 0.319319 lr 0.00029471 rank 2
2023-02-27 17:33:56,006 DEBUG TRAIN Batch 48/0 loss 8.206752 loss_att 7.974771 loss_ctc 11.285330 loss_rnnt 7.615757 hw_loss 0.425462 lr 0.00029471 rank 3
2023-02-27 17:33:56,060 DEBUG TRAIN Batch 48/0 loss 8.075141 loss_att 7.945266 loss_ctc 11.990964 loss_rnnt 7.379099 hw_loss 0.374825 lr 0.00029471 rank 0
2023-02-27 17:34:33,313 DEBUG TRAIN Batch 48/100 loss 4.641239 loss_att 8.732985 loss_ctc 10.439541 loss_rnnt 2.991964 hw_loss 0.108411 lr 0.00029470 rank 0
2023-02-27 17:34:33,314 DEBUG TRAIN Batch 48/100 loss 4.227520 loss_att 7.435883 loss_ctc 8.660631 loss_rnnt 2.925456 hw_loss 0.129955 lr 0.00029470 rank 2
2023-02-27 17:34:33,314 DEBUG TRAIN Batch 48/100 loss 9.057409 loss_att 13.329649 loss_ctc 25.332306 loss_rnnt 5.943565 hw_loss 0.167643 lr 0.00029470 rank 6
2023-02-27 17:34:33,315 DEBUG TRAIN Batch 48/100 loss 3.206789 loss_att 5.713497 loss_ctc 3.850724 loss_rnnt 2.491829 hw_loss 0.239551 lr 0.00029470 rank 3
2023-02-27 17:34:33,317 DEBUG TRAIN Batch 48/100 loss 12.087921 loss_att 13.195112 loss_ctc 18.338818 loss_rnnt 10.976343 hw_loss 0.106286 lr 0.00029469 rank 7
2023-02-27 17:34:33,317 DEBUG TRAIN Batch 48/100 loss 2.972413 loss_att 7.350376 loss_ctc 6.880811 loss_rnnt 1.482236 hw_loss 0.175247 lr 0.00029470 rank 4
2023-02-27 17:34:33,322 DEBUG TRAIN Batch 48/100 loss 6.861414 loss_att 11.450381 loss_ctc 16.595747 loss_rnnt 4.434437 hw_loss 0.396137 lr 0.00029470 rank 1
2023-02-27 17:34:33,364 DEBUG TRAIN Batch 48/100 loss 10.864963 loss_att 15.921989 loss_ctc 12.407862 loss_rnnt 9.507474 hw_loss 0.263178 lr 0.00029470 rank 5
2023-02-27 17:35:11,301 DEBUG TRAIN Batch 48/200 loss 3.844489 loss_att 5.305988 loss_ctc 5.879910 loss_rnnt 3.171091 hw_loss 0.205705 lr 0.00029468 rank 4
2023-02-27 17:35:11,323 DEBUG TRAIN Batch 48/200 loss 5.681797 loss_att 8.598434 loss_ctc 8.406680 loss_rnnt 4.609310 hw_loss 0.235951 lr 0.00029468 rank 3
2023-02-27 17:35:11,324 DEBUG TRAIN Batch 48/200 loss 4.823544 loss_att 7.704300 loss_ctc 6.006287 loss_rnnt 3.977494 hw_loss 0.210373 lr 0.00029468 rank 6
2023-02-27 17:35:11,326 DEBUG TRAIN Batch 48/200 loss 6.004318 loss_att 9.252644 loss_ctc 6.687243 loss_rnnt 5.124805 hw_loss 0.260233 lr 0.00029469 rank 1
2023-02-27 17:35:11,327 DEBUG TRAIN Batch 48/200 loss 7.011296 loss_att 10.394182 loss_ctc 12.674324 loss_rnnt 5.417795 hw_loss 0.303476 lr 0.00029469 rank 0
2023-02-27 17:35:11,327 DEBUG TRAIN Batch 48/200 loss 8.852509 loss_att 12.823439 loss_ctc 14.814545 loss_rnnt 7.179941 hw_loss 0.156458 lr 0.00029468 rank 2
2023-02-27 17:35:11,329 DEBUG TRAIN Batch 48/200 loss 6.233484 loss_att 9.446611 loss_ctc 11.537807 loss_rnnt 4.745408 hw_loss 0.259139 lr 0.00029468 rank 7
2023-02-27 17:35:11,372 DEBUG TRAIN Batch 48/200 loss 3.586480 loss_att 6.852447 loss_ctc 8.244568 loss_rnnt 2.139254 hw_loss 0.324290 lr 0.00029469 rank 5
2023-02-27 17:35:50,175 DEBUG TRAIN Batch 48/300 loss 6.219264 loss_att 8.814323 loss_ctc 11.489258 loss_rnnt 4.953651 hw_loss 0.082377 lr 0.00029467 rank 2
2023-02-27 17:35:50,180 DEBUG TRAIN Batch 48/300 loss 4.390332 loss_att 7.372534 loss_ctc 5.771138 loss_rnnt 3.441886 hw_loss 0.314810 lr 0.00029467 rank 6
2023-02-27 17:35:50,181 DEBUG TRAIN Batch 48/300 loss 2.343888 loss_att 5.493371 loss_ctc 4.174178 loss_rnnt 1.376894 hw_loss 0.174484 lr 0.00029467 rank 3
2023-02-27 17:35:50,183 DEBUG TRAIN Batch 48/300 loss 5.077116 loss_att 8.053248 loss_ctc 7.846342 loss_rnnt 3.994540 hw_loss 0.221474 lr 0.00029467 rank 0
2023-02-27 17:35:50,186 DEBUG TRAIN Batch 48/300 loss 7.825483 loss_att 8.982117 loss_ctc 13.133652 loss_rnnt 6.713900 hw_loss 0.323438 lr 0.00029467 rank 5
2023-02-27 17:35:50,193 DEBUG TRAIN Batch 48/300 loss 2.495705 loss_att 5.805373 loss_ctc 4.164137 loss_rnnt 1.514038 hw_loss 0.182391 lr 0.00029467 rank 4
2023-02-27 17:35:50,194 DEBUG TRAIN Batch 48/300 loss 9.063432 loss_att 10.955660 loss_ctc 14.969362 loss_rnnt 7.858194 hw_loss 0.073752 lr 0.00029467 rank 1
2023-02-27 17:35:50,221 DEBUG TRAIN Batch 48/300 loss 3.284704 loss_att 7.850673 loss_ctc 9.350510 loss_rnnt 1.407297 hw_loss 0.291450 lr 0.00029467 rank 7
2023-02-27 17:36:56,400 DEBUG TRAIN Batch 48/400 loss 2.893946 loss_att 5.336242 loss_ctc 3.999694 loss_rnnt 2.070667 hw_loss 0.351350 lr 0.00029466 rank 3
2023-02-27 17:36:56,413 DEBUG TRAIN Batch 48/400 loss 9.185707 loss_att 13.172407 loss_ctc 14.447615 loss_rnnt 7.528171 hw_loss 0.297392 lr 0.00029466 rank 4
2023-02-27 17:36:56,420 DEBUG TRAIN Batch 48/400 loss 6.602116 loss_att 8.159323 loss_ctc 12.759653 loss_rnnt 5.343096 hw_loss 0.237326 lr 0.00029466 rank 6
2023-02-27 17:36:56,421 DEBUG TRAIN Batch 48/400 loss 6.147043 loss_att 10.099622 loss_ctc 10.838493 loss_rnnt 4.628342 hw_loss 0.192483 lr 0.00029466 rank 5
2023-02-27 17:36:56,421 DEBUG TRAIN Batch 48/400 loss 7.272903 loss_att 10.249361 loss_ctc 13.744377 loss_rnnt 5.664179 hw_loss 0.282318 lr 0.00029466 rank 0
2023-02-27 17:36:56,425 DEBUG TRAIN Batch 48/400 loss 6.757363 loss_att 8.798542 loss_ctc 12.508256 loss_rnnt 5.398181 hw_loss 0.345302 lr 0.00029465 rank 7
2023-02-27 17:36:56,426 DEBUG TRAIN Batch 48/400 loss 8.551804 loss_att 11.140957 loss_ctc 15.412470 loss_rnnt 6.985523 hw_loss 0.250678 lr 0.00029466 rank 2
2023-02-27 17:36:56,470 DEBUG TRAIN Batch 48/400 loss 1.864558 loss_att 4.123156 loss_ctc 3.220012 loss_rnnt 1.105935 hw_loss 0.236582 lr 0.00029466 rank 1
2023-02-27 17:37:34,875 DEBUG TRAIN Batch 48/500 loss 8.554428 loss_att 12.893511 loss_ctc 16.435953 loss_rnnt 6.497380 hw_loss 0.259429 lr 0.00029464 rank 2
2023-02-27 17:37:34,884 DEBUG TRAIN Batch 48/500 loss 7.539020 loss_att 12.097799 loss_ctc 14.304066 loss_rnnt 5.645574 hw_loss 0.149406 lr 0.00029465 rank 3
2023-02-27 17:37:34,885 DEBUG TRAIN Batch 48/500 loss 2.051766 loss_att 6.241086 loss_ctc 5.071073 loss_rnnt 0.636738 hw_loss 0.327355 lr 0.00029464 rank 6
2023-02-27 17:37:34,887 DEBUG TRAIN Batch 48/500 loss 8.272795 loss_att 10.588020 loss_ctc 16.676884 loss_rnnt 6.546182 hw_loss 0.268167 lr 0.00029465 rank 0
2023-02-27 17:37:34,895 DEBUG TRAIN Batch 48/500 loss 5.338778 loss_att 8.134645 loss_ctc 9.025253 loss_rnnt 4.157555 hw_loss 0.244724 lr 0.00029464 rank 7
2023-02-27 17:37:34,896 DEBUG TRAIN Batch 48/500 loss 4.876080 loss_att 8.012163 loss_ctc 8.542099 loss_rnnt 3.611618 hw_loss 0.278330 lr 0.00029465 rank 4
2023-02-27 17:37:34,897 DEBUG TRAIN Batch 48/500 loss 6.400231 loss_att 7.543559 loss_ctc 10.932877 loss_rnnt 5.447083 hw_loss 0.225244 lr 0.00029465 rank 5
2023-02-27 17:37:34,914 DEBUG TRAIN Batch 48/500 loss 9.601565 loss_att 12.697160 loss_ctc 17.453293 loss_rnnt 7.767502 hw_loss 0.315089 lr 0.00029465 rank 1
2023-02-27 17:38:13,509 DEBUG TRAIN Batch 48/600 loss 5.290456 loss_att 6.415510 loss_ctc 9.035240 loss_rnnt 4.410664 hw_loss 0.291517 lr 0.00029463 rank 6
2023-02-27 17:38:13,511 DEBUG TRAIN Batch 48/600 loss 3.524758 loss_att 4.756144 loss_ctc 7.391682 loss_rnnt 2.614321 hw_loss 0.278568 lr 0.00029464 rank 1
2023-02-27 17:38:13,512 DEBUG TRAIN Batch 48/600 loss 6.085299 loss_att 8.432423 loss_ctc 9.135813 loss_rnnt 4.998284 hw_loss 0.395354 lr 0.00029463 rank 2
2023-02-27 17:38:13,519 DEBUG TRAIN Batch 48/600 loss 9.476826 loss_att 11.293859 loss_ctc 16.427963 loss_rnnt 8.086873 hw_loss 0.186986 lr 0.00029463 rank 4
2023-02-27 17:38:13,519 DEBUG TRAIN Batch 48/600 loss 5.605577 loss_att 7.941895 loss_ctc 9.284596 loss_rnnt 4.478616 hw_loss 0.317179 lr 0.00029464 rank 5
2023-02-27 17:38:13,523 DEBUG TRAIN Batch 48/600 loss 4.648021 loss_att 5.511729 loss_ctc 9.242474 loss_rnnt 3.652710 hw_loss 0.393704 lr 0.00029463 rank 0
2023-02-27 17:38:13,523 DEBUG TRAIN Batch 48/600 loss 4.436027 loss_att 4.799108 loss_ctc 5.626537 loss_rnnt 4.017403 hw_loss 0.351138 lr 0.00029463 rank 7
2023-02-27 17:38:13,523 DEBUG TRAIN Batch 48/600 loss 4.801221 loss_att 6.113942 loss_ctc 7.379234 loss_rnnt 4.012117 hw_loss 0.342796 lr 0.00029463 rank 3
2023-02-27 17:38:53,230 DEBUG TRAIN Batch 48/700 loss 4.716698 loss_att 9.174920 loss_ctc 10.293791 loss_rnnt 2.906948 hw_loss 0.327176 lr 0.00029462 rank 4
2023-02-27 17:38:53,238 DEBUG TRAIN Batch 48/700 loss 4.735781 loss_att 11.291915 loss_ctc 8.921513 loss_rnnt 2.684141 hw_loss 0.341840 lr 0.00029462 rank 6
2023-02-27 17:38:53,240 DEBUG TRAIN Batch 48/700 loss 8.510180 loss_att 11.547180 loss_ctc 12.524402 loss_rnnt 7.197635 hw_loss 0.318591 lr 0.00029462 rank 5
2023-02-27 17:38:53,243 DEBUG TRAIN Batch 48/700 loss 1.710759 loss_att 5.648555 loss_ctc 3.495894 loss_rnnt 0.631977 hw_loss 0.099759 lr 0.00029462 rank 0
2023-02-27 17:38:53,244 DEBUG TRAIN Batch 48/700 loss 4.639359 loss_att 8.279537 loss_ctc 7.347373 loss_rnnt 3.472278 hw_loss 0.146208 lr 0.00029461 rank 7
2023-02-27 17:38:53,244 DEBUG TRAIN Batch 48/700 loss 3.600277 loss_att 7.721900 loss_ctc 11.731154 loss_rnnt 1.555127 hw_loss 0.256329 lr 0.00029462 rank 1
2023-02-27 17:38:53,247 DEBUG TRAIN Batch 48/700 loss 6.162701 loss_att 9.249478 loss_ctc 8.207056 loss_rnnt 5.085042 hw_loss 0.351981 lr 0.00029462 rank 3
2023-02-27 17:38:53,247 DEBUG TRAIN Batch 48/700 loss 3.759919 loss_att 7.657177 loss_ctc 6.944825 loss_rnnt 2.425059 hw_loss 0.245164 lr 0.00029462 rank 2
2023-02-27 17:39:56,912 DEBUG TRAIN Batch 48/800 loss 8.642719 loss_att 11.801509 loss_ctc 13.704132 loss_rnnt 7.243866 hw_loss 0.172951 lr 0.00029461 rank 6
2023-02-27 17:39:56,913 DEBUG TRAIN Batch 48/800 loss 4.273450 loss_att 6.916492 loss_ctc 7.439661 loss_rnnt 3.160661 hw_loss 0.303788 lr 0.00029461 rank 0
2023-02-27 17:39:56,932 DEBUG TRAIN Batch 48/800 loss 16.059925 loss_att 18.802881 loss_ctc 20.634628 loss_rnnt 14.758696 hw_loss 0.267522 lr 0.00029461 rank 3
2023-02-27 17:39:56,935 DEBUG TRAIN Batch 48/800 loss 8.273365 loss_att 10.510482 loss_ctc 11.807925 loss_rnnt 7.240552 hw_loss 0.213963 lr 0.00029461 rank 4
2023-02-27 17:39:56,936 DEBUG TRAIN Batch 48/800 loss 4.345300 loss_att 8.431277 loss_ctc 5.937388 loss_rnnt 3.243744 hw_loss 0.135154 lr 0.00029460 rank 7
2023-02-27 17:39:56,938 DEBUG TRAIN Batch 48/800 loss 3.825410 loss_att 7.423067 loss_ctc 9.038425 loss_rnnt 2.348211 hw_loss 0.117373 lr 0.00029461 rank 5
2023-02-27 17:39:56,941 DEBUG TRAIN Batch 48/800 loss 6.729300 loss_att 10.560367 loss_ctc 10.428343 loss_rnnt 5.444139 hw_loss 0.048266 lr 0.00029461 rank 1
2023-02-27 17:39:56,987 DEBUG TRAIN Batch 48/800 loss 2.649524 loss_att 6.036933 loss_ctc 4.903842 loss_rnnt 1.640491 hw_loss 0.058078 lr 0.00029461 rank 2
2023-02-27 17:40:35,005 DEBUG TRAIN Batch 48/900 loss 8.453983 loss_att 11.663930 loss_ctc 14.022840 loss_rnnt 6.996716 hw_loss 0.136431 lr 0.00029460 rank 0
2023-02-27 17:40:35,005 DEBUG TRAIN Batch 48/900 loss 4.919721 loss_att 8.077471 loss_ctc 6.653900 loss_rnnt 3.926084 hw_loss 0.245369 lr 0.00029459 rank 3
2023-02-27 17:40:35,007 DEBUG TRAIN Batch 48/900 loss 6.621202 loss_att 10.340801 loss_ctc 10.575395 loss_rnnt 5.199666 hw_loss 0.281982 lr 0.00029460 rank 1
2023-02-27 17:40:35,008 DEBUG TRAIN Batch 48/900 loss 9.804899 loss_att 12.761980 loss_ctc 16.244823 loss_rnnt 8.187556 hw_loss 0.313633 lr 0.00029460 rank 5
2023-02-27 17:40:35,008 DEBUG TRAIN Batch 48/900 loss 8.254070 loss_att 8.608274 loss_ctc 13.651569 loss_rnnt 7.323911 hw_loss 0.261847 lr 0.00029459 rank 2
2023-02-27 17:40:35,010 DEBUG TRAIN Batch 48/900 loss 5.426082 loss_att 7.841811 loss_ctc 8.535088 loss_rnnt 4.421197 hw_loss 0.201010 lr 0.00029459 rank 7
2023-02-27 17:40:35,010 DEBUG TRAIN Batch 48/900 loss 2.838164 loss_att 5.666608 loss_ctc 7.934033 loss_rnnt 1.535487 hw_loss 0.107886 lr 0.00029459 rank 6
2023-02-27 17:40:35,061 DEBUG TRAIN Batch 48/900 loss 13.729066 loss_att 18.096046 loss_ctc 24.290569 loss_rnnt 11.298857 hw_loss 0.278650 lr 0.00029459 rank 4
2023-02-27 17:41:13,308 DEBUG TRAIN Batch 48/1000 loss 9.788763 loss_att 12.548882 loss_ctc 13.258207 loss_rnnt 8.600801 hw_loss 0.325021 lr 0.00029458 rank 7
2023-02-27 17:41:13,310 DEBUG TRAIN Batch 48/1000 loss 8.182675 loss_att 10.405190 loss_ctc 10.543385 loss_rnnt 7.274228 hw_loss 0.279720 lr 0.00029459 rank 5
2023-02-27 17:41:13,313 DEBUG TRAIN Batch 48/1000 loss 3.177949 loss_att 7.819576 loss_ctc 5.027482 loss_rnnt 1.833928 hw_loss 0.317046 lr 0.00029458 rank 0
2023-02-27 17:41:13,320 DEBUG TRAIN Batch 48/1000 loss 6.842396 loss_att 10.247820 loss_ctc 12.525480 loss_rnnt 5.230479 hw_loss 0.324539 lr 0.00029458 rank 6
2023-02-27 17:41:13,324 DEBUG TRAIN Batch 48/1000 loss 6.765608 loss_att 8.790703 loss_ctc 9.177938 loss_rnnt 5.846777 hw_loss 0.360315 lr 0.00029458 rank 3
2023-02-27 17:41:13,327 DEBUG TRAIN Batch 48/1000 loss 5.046455 loss_att 7.846139 loss_ctc 9.900882 loss_rnnt 3.691825 hw_loss 0.276444 lr 0.00029458 rank 2
2023-02-27 17:41:13,329 DEBUG TRAIN Batch 48/1000 loss 4.634642 loss_att 6.455589 loss_ctc 6.959620 loss_rnnt 3.835772 hw_loss 0.233783 lr 0.00029458 rank 1
2023-02-27 17:41:13,344 DEBUG TRAIN Batch 48/1000 loss 3.716721 loss_att 6.908061 loss_ctc 7.409217 loss_rnnt 2.410960 hw_loss 0.328425 lr 0.00029458 rank 4
2023-02-27 17:42:19,585 DEBUG TRAIN Batch 48/1100 loss 5.937547 loss_att 7.757110 loss_ctc 8.323364 loss_rnnt 5.119194 hw_loss 0.255622 lr 0.00029457 rank 0
2023-02-27 17:42:19,586 DEBUG TRAIN Batch 48/1100 loss 2.665485 loss_att 4.760067 loss_ctc 3.164479 loss_rnnt 1.953408 hw_loss 0.424926 lr 0.00029457 rank 3
2023-02-27 17:42:19,589 DEBUG TRAIN Batch 48/1100 loss 4.596982 loss_att 6.934994 loss_ctc 6.819345 loss_rnnt 3.682088 hw_loss 0.283080 lr 0.00029457 rank 1
2023-02-27 17:42:19,589 DEBUG TRAIN Batch 48/1100 loss 3.453161 loss_att 7.035367 loss_ctc 7.173526 loss_rnnt 2.210975 hw_loss 0.055681 lr 0.00029457 rank 5
2023-02-27 17:42:19,590 DEBUG TRAIN Batch 48/1100 loss 3.442962 loss_att 6.965634 loss_ctc 8.879930 loss_rnnt 1.948038 hw_loss 0.122737 lr 0.00029457 rank 6
2023-02-27 17:42:19,592 DEBUG TRAIN Batch 48/1100 loss 5.195146 loss_att 8.318586 loss_ctc 8.168650 loss_rnnt 4.084038 hw_loss 0.168660 lr 0.00029457 rank 2
2023-02-27 17:42:19,593 DEBUG TRAIN Batch 48/1100 loss 6.134271 loss_att 7.205828 loss_ctc 6.454945 loss_rnnt 5.749839 hw_loss 0.238806 lr 0.00029457 rank 4
2023-02-27 17:42:19,640 DEBUG TRAIN Batch 48/1100 loss 10.438134 loss_att 14.443512 loss_ctc 15.925516 loss_rnnt 8.693691 hw_loss 0.396970 lr 0.00029456 rank 7
2023-02-27 17:42:58,061 DEBUG TRAIN Batch 48/1200 loss 5.452027 loss_att 6.940681 loss_ctc 7.186413 loss_rnnt 4.785669 hw_loss 0.257579 lr 0.00029456 rank 3
2023-02-27 17:42:58,081 DEBUG TRAIN Batch 48/1200 loss 4.980177 loss_att 7.544805 loss_ctc 7.385049 loss_rnnt 4.041322 hw_loss 0.197400 lr 0.00029455 rank 7
2023-02-27 17:42:58,083 DEBUG TRAIN Batch 48/1200 loss 3.429099 loss_att 5.560357 loss_ctc 5.702800 loss_rnnt 2.590224 hw_loss 0.205243 lr 0.00029456 rank 6
2023-02-27 17:42:58,086 DEBUG TRAIN Batch 48/1200 loss 3.460047 loss_att 5.423064 loss_ctc 6.538887 loss_rnnt 2.514522 hw_loss 0.267019 lr 0.00029456 rank 0
2023-02-27 17:42:58,088 DEBUG TRAIN Batch 48/1200 loss 12.255208 loss_att 13.667845 loss_ctc 20.036831 loss_rnnt 10.800183 hw_loss 0.253026 lr 0.00029456 rank 5
2023-02-27 17:42:58,089 DEBUG TRAIN Batch 48/1200 loss 7.408558 loss_att 9.830500 loss_ctc 12.656850 loss_rnnt 6.056668 hw_loss 0.314492 lr 0.00029456 rank 4
2023-02-27 17:42:58,090 DEBUG TRAIN Batch 48/1200 loss 4.775521 loss_att 7.927279 loss_ctc 9.105742 loss_rnnt 3.446099 hw_loss 0.228201 lr 0.00029456 rank 1
2023-02-27 17:42:58,135 DEBUG TRAIN Batch 48/1200 loss 9.225831 loss_att 10.961336 loss_ctc 12.234273 loss_rnnt 8.321170 hw_loss 0.293314 lr 0.00029455 rank 2
2023-02-27 17:43:36,637 DEBUG TRAIN Batch 48/1300 loss 4.035697 loss_att 6.457834 loss_ctc 6.698409 loss_rnnt 3.021138 hw_loss 0.328318 lr 0.00029454 rank 7
2023-02-27 17:43:36,638 DEBUG TRAIN Batch 48/1300 loss 7.153272 loss_att 8.790910 loss_ctc 11.135324 loss_rnnt 6.066509 hw_loss 0.428054 lr 0.00029454 rank 3
2023-02-27 17:43:36,639 DEBUG TRAIN Batch 48/1300 loss 7.136476 loss_att 8.391801 loss_ctc 19.184673 loss_rnnt 5.175591 hw_loss 0.193863 lr 0.00029455 rank 1
2023-02-27 17:43:36,639 DEBUG TRAIN Batch 48/1300 loss 2.582045 loss_att 5.036896 loss_ctc 5.530126 loss_rnnt 1.578485 hw_loss 0.224086 lr 0.00029454 rank 4
2023-02-27 17:43:36,641 DEBUG TRAIN Batch 48/1300 loss 6.530457 loss_att 9.386769 loss_ctc 12.466034 loss_rnnt 5.041192 hw_loss 0.237361 lr 0.00029454 rank 6
2023-02-27 17:43:36,642 DEBUG TRAIN Batch 48/1300 loss 6.499480 loss_att 12.142546 loss_ctc 10.164800 loss_rnnt 4.780460 hw_loss 0.190682 lr 0.00029454 rank 0
2023-02-27 17:43:36,647 DEBUG TRAIN Batch 48/1300 loss 3.357451 loss_att 7.425171 loss_ctc 5.895073 loss_rnnt 2.067409 hw_loss 0.259028 lr 0.00029455 rank 5
2023-02-27 17:43:36,660 DEBUG TRAIN Batch 48/1300 loss 8.087844 loss_att 10.980041 loss_ctc 13.261663 loss_rnnt 6.690219 hw_loss 0.242520 lr 0.00029454 rank 2
2023-02-27 17:44:15,952 DEBUG TRAIN Batch 48/1400 loss 4.997837 loss_att 9.015272 loss_ctc 7.642177 loss_rnnt 3.763580 hw_loss 0.146609 lr 0.00029453 rank 4
2023-02-27 17:44:15,955 DEBUG TRAIN Batch 48/1400 loss 6.168162 loss_att 9.307494 loss_ctc 8.521048 loss_rnnt 5.093088 hw_loss 0.250294 lr 0.00029453 rank 0
2023-02-27 17:44:15,967 DEBUG TRAIN Batch 48/1400 loss 3.628194 loss_att 6.606043 loss_ctc 7.654426 loss_rnnt 2.372922 hw_loss 0.230384 lr 0.00029453 rank 6
2023-02-27 17:44:15,967 DEBUG TRAIN Batch 48/1400 loss 3.458389 loss_att 6.990388 loss_ctc 7.717808 loss_rnnt 2.041540 hw_loss 0.267237 lr 0.00029453 rank 3
2023-02-27 17:44:15,971 DEBUG TRAIN Batch 48/1400 loss 5.088649 loss_att 9.951599 loss_ctc 11.842882 loss_rnnt 3.118658 hw_loss 0.181569 lr 0.00029453 rank 2
2023-02-27 17:44:15,983 DEBUG TRAIN Batch 48/1400 loss 3.307300 loss_att 7.902624 loss_ctc 7.482260 loss_rnnt 1.718470 hw_loss 0.212071 lr 0.00029453 rank 5
2023-02-27 17:44:15,995 DEBUG TRAIN Batch 48/1400 loss 2.754424 loss_att 5.530986 loss_ctc 5.111995 loss_rnnt 1.775894 hw_loss 0.204141 lr 0.00029453 rank 7
2023-02-27 17:44:16,001 DEBUG TRAIN Batch 48/1400 loss 6.195168 loss_att 8.986072 loss_ctc 11.907305 loss_rnnt 4.676110 hw_loss 0.373612 lr 0.00029453 rank 1
2023-02-27 17:45:20,738 DEBUG TRAIN Batch 48/1500 loss 2.033741 loss_att 4.727911 loss_ctc 4.080604 loss_rnnt 1.067309 hw_loss 0.290031 lr 0.00029452 rank 1
2023-02-27 17:45:20,751 DEBUG TRAIN Batch 48/1500 loss 10.363642 loss_att 11.225269 loss_ctc 16.689198 loss_rnnt 9.201116 hw_loss 0.275237 lr 0.00029452 rank 6
2023-02-27 17:45:20,754 DEBUG TRAIN Batch 48/1500 loss 2.757375 loss_att 7.353108 loss_ctc 8.101122 loss_rnnt 1.017395 hw_loss 0.203124 lr 0.00029452 rank 3
2023-02-27 17:45:20,755 DEBUG TRAIN Batch 48/1500 loss 2.067558 loss_att 4.507614 loss_ctc 3.620525 loss_rnnt 1.249478 hw_loss 0.230638 lr 0.00029451 rank 7
2023-02-27 17:45:20,755 DEBUG TRAIN Batch 48/1500 loss 9.164623 loss_att 10.525612 loss_ctc 14.912405 loss_rnnt 8.054384 hw_loss 0.134382 lr 0.00029452 rank 0
2023-02-27 17:45:20,756 DEBUG TRAIN Batch 48/1500 loss 9.749613 loss_att 10.004101 loss_ctc 14.637096 loss_rnnt 8.862011 hw_loss 0.346950 lr 0.00029452 rank 5
2023-02-27 17:45:20,757 DEBUG TRAIN Batch 48/1500 loss 4.387873 loss_att 8.517980 loss_ctc 7.837936 loss_rnnt 2.910648 hw_loss 0.358491 lr 0.00029452 rank 4
2023-02-27 17:45:20,757 DEBUG TRAIN Batch 48/1500 loss 2.498769 loss_att 4.094502 loss_ctc 3.805484 loss_rnnt 1.907929 hw_loss 0.182745 lr 0.00029452 rank 2
2023-02-27 17:45:58,591 DEBUG TRAIN Batch 48/1600 loss 4.302777 loss_att 10.298795 loss_ctc 7.378249 loss_rnnt 2.585248 hw_loss 0.202993 lr 0.00029451 rank 4
2023-02-27 17:45:58,592 DEBUG TRAIN Batch 48/1600 loss 15.487193 loss_att 18.041592 loss_ctc 30.578037 loss_rnnt 12.884602 hw_loss 0.149248 lr 0.00029450 rank 2
2023-02-27 17:45:58,603 DEBUG TRAIN Batch 48/1600 loss 11.272673 loss_att 17.120018 loss_ctc 20.887699 loss_rnnt 8.646753 hw_loss 0.327087 lr 0.00029450 rank 6
2023-02-27 17:45:58,605 DEBUG TRAIN Batch 48/1600 loss 4.019142 loss_att 5.782796 loss_ctc 6.646379 loss_rnnt 3.238362 hw_loss 0.145782 lr 0.00029451 rank 0
2023-02-27 17:45:58,606 DEBUG TRAIN Batch 48/1600 loss 2.931501 loss_att 5.236030 loss_ctc 4.905354 loss_rnnt 2.076284 hw_loss 0.245871 lr 0.00029450 rank 3
2023-02-27 17:45:58,609 DEBUG TRAIN Batch 48/1600 loss 6.583190 loss_att 9.351843 loss_ctc 10.808369 loss_rnnt 5.385332 hw_loss 0.151444 lr 0.00029451 rank 1
2023-02-27 17:45:58,611 DEBUG TRAIN Batch 48/1600 loss 6.911511 loss_att 9.230904 loss_ctc 12.387041 loss_rnnt 5.563232 hw_loss 0.289369 lr 0.00029450 rank 7
2023-02-27 17:45:58,657 DEBUG TRAIN Batch 48/1600 loss 4.890271 loss_att 7.256016 loss_ctc 6.717297 loss_rnnt 4.073546 hw_loss 0.187447 lr 0.00029451 rank 5
2023-02-27 17:46:37,247 DEBUG TRAIN Batch 48/1700 loss 5.630203 loss_att 7.923226 loss_ctc 8.138032 loss_rnnt 4.735620 hw_loss 0.190502 lr 0.00029449 rank 4
2023-02-27 17:46:37,253 DEBUG TRAIN Batch 48/1700 loss 6.134488 loss_att 8.881167 loss_ctc 11.953157 loss_rnnt 4.669131 hw_loss 0.262870 lr 0.00029450 rank 5
2023-02-27 17:46:37,257 DEBUG TRAIN Batch 48/1700 loss 9.890592 loss_att 9.527033 loss_ctc 16.466507 loss_rnnt 8.958357 hw_loss 0.240295 lr 0.00029449 rank 0
2023-02-27 17:46:37,260 DEBUG TRAIN Batch 48/1700 loss 2.821204 loss_att 6.880985 loss_ctc 7.270555 loss_rnnt 1.303006 hw_loss 0.211866 lr 0.00029449 rank 3
2023-02-27 17:46:37,264 DEBUG TRAIN Batch 48/1700 loss 7.635296 loss_att 11.329199 loss_ctc 13.593876 loss_rnnt 5.971763 hw_loss 0.244268 lr 0.00029449 rank 7
2023-02-27 17:46:37,265 DEBUG TRAIN Batch 48/1700 loss 7.191529 loss_att 9.641532 loss_ctc 12.156197 loss_rnnt 5.887286 hw_loss 0.285538 lr 0.00029449 rank 6
2023-02-27 17:46:37,267 DEBUG TRAIN Batch 48/1700 loss 10.459221 loss_att 14.802357 loss_ctc 21.065414 loss_rnnt 8.058759 hw_loss 0.220642 lr 0.00029450 rank 1
2023-02-27 17:46:37,313 DEBUG TRAIN Batch 48/1700 loss 5.437533 loss_att 7.897988 loss_ctc 9.598137 loss_rnnt 4.288866 hw_loss 0.190931 lr 0.00029449 rank 2
2023-02-27 17:47:42,001 DEBUG TRAIN Batch 48/1800 loss 5.039310 loss_att 9.199667 loss_ctc 8.283349 loss_rnnt 3.625625 hw_loss 0.279516 lr 0.00029448 rank 4
2023-02-27 17:47:42,002 DEBUG TRAIN Batch 48/1800 loss 11.722422 loss_att 11.236414 loss_ctc 18.928373 loss_rnnt 10.667644 hw_loss 0.358474 lr 0.00029448 rank 3
2023-02-27 17:47:42,005 DEBUG TRAIN Batch 48/1800 loss 6.304143 loss_att 8.870550 loss_ctc 9.973019 loss_rnnt 5.167803 hw_loss 0.251015 lr 0.00029448 rank 0
2023-02-27 17:47:42,005 DEBUG TRAIN Batch 48/1800 loss 8.751764 loss_att 11.881804 loss_ctc 17.912863 loss_rnnt 6.838199 hw_loss 0.123895 lr 0.00029447 rank 7
2023-02-27 17:47:42,006 DEBUG TRAIN Batch 48/1800 loss 7.192948 loss_att 9.182673 loss_ctc 13.397260 loss_rnnt 5.837451 hw_loss 0.244334 lr 0.00029448 rank 1
2023-02-27 17:47:42,010 DEBUG TRAIN Batch 48/1800 loss 9.100913 loss_att 10.832880 loss_ctc 15.403255 loss_rnnt 7.758404 hw_loss 0.292129 lr 0.00029448 rank 2
2023-02-27 17:47:42,015 DEBUG TRAIN Batch 48/1800 loss 7.273341 loss_att 10.276147 loss_ctc 12.253812 loss_rnnt 5.966199 hw_loss 0.079719 lr 0.00029448 rank 6
2023-02-27 17:47:42,053 DEBUG TRAIN Batch 48/1800 loss 5.441411 loss_att 7.165548 loss_ctc 8.306550 loss_rnnt 4.675020 hw_loss 0.074148 lr 0.00029448 rank 5
2023-02-27 17:48:20,239 DEBUG TRAIN Batch 48/1900 loss 5.184442 loss_att 5.637711 loss_ctc 8.169367 loss_rnnt 4.460784 hw_loss 0.440651 lr 0.00029447 rank 5
2023-02-27 17:48:20,245 DEBUG TRAIN Batch 48/1900 loss 5.346811 loss_att 5.803046 loss_ctc 7.681205 loss_rnnt 4.793955 hw_loss 0.281918 lr 0.00029447 rank 3
2023-02-27 17:48:20,246 DEBUG TRAIN Batch 48/1900 loss 10.897476 loss_att 10.713896 loss_ctc 16.624451 loss_rnnt 10.021279 hw_loss 0.279967 lr 0.00029447 rank 6
2023-02-27 17:48:20,247 DEBUG TRAIN Batch 48/1900 loss 11.048094 loss_att 11.767932 loss_ctc 16.298428 loss_rnnt 10.042053 hw_loss 0.303802 lr 0.00029447 rank 0
2023-02-27 17:48:20,250 DEBUG TRAIN Batch 48/1900 loss 4.826755 loss_att 5.438439 loss_ctc 8.287847 loss_rnnt 4.173920 hw_loss 0.129410 lr 0.00029447 rank 2
2023-02-27 17:48:20,254 DEBUG TRAIN Batch 48/1900 loss 6.676337 loss_att 8.045449 loss_ctc 10.792054 loss_rnnt 5.707329 hw_loss 0.274544 lr 0.00029447 rank 4
2023-02-27 17:48:20,254 DEBUG TRAIN Batch 48/1900 loss 7.867587 loss_att 7.291853 loss_ctc 10.370069 loss_rnnt 7.444384 hw_loss 0.383786 lr 0.00029447 rank 1
2023-02-27 17:48:20,298 DEBUG TRAIN Batch 48/1900 loss 6.596442 loss_att 8.185648 loss_ctc 7.919251 loss_rnnt 5.924226 hw_loss 0.333751 lr 0.00029446 rank 7
2023-02-27 17:48:58,231 DEBUG TRAIN Batch 48/2000 loss 7.113987 loss_att 9.554399 loss_ctc 10.253700 loss_rnnt 6.128509 hw_loss 0.147689 lr 0.00029445 rank 6
2023-02-27 17:48:58,231 DEBUG TRAIN Batch 48/2000 loss 3.149699 loss_att 7.331526 loss_ctc 6.617243 loss_rnnt 1.753898 hw_loss 0.182056 lr 0.00029445 rank 2
2023-02-27 17:48:58,232 DEBUG TRAIN Batch 48/2000 loss 9.148605 loss_att 13.260710 loss_ctc 15.053196 loss_rnnt 7.435167 hw_loss 0.194510 lr 0.00029445 rank 3
2023-02-27 17:48:58,232 DEBUG TRAIN Batch 48/2000 loss 2.281550 loss_att 5.178204 loss_ctc 4.299325 loss_rnnt 1.310545 hw_loss 0.229945 lr 0.00029445 rank 7
2023-02-27 17:48:58,232 DEBUG TRAIN Batch 48/2000 loss 5.486596 loss_att 8.110096 loss_ctc 10.007719 loss_rnnt 4.255457 hw_loss 0.194292 lr 0.00029446 rank 1
2023-02-27 17:48:58,236 DEBUG TRAIN Batch 48/2000 loss 6.542471 loss_att 12.435286 loss_ctc 12.542555 loss_rnnt 4.389963 hw_loss 0.326126 lr 0.00029446 rank 0
2023-02-27 17:48:58,265 DEBUG TRAIN Batch 48/2000 loss 3.306334 loss_att 6.250372 loss_ctc 6.268774 loss_rnnt 2.173178 hw_loss 0.280042 lr 0.00029445 rank 4
2023-02-27 17:48:58,266 DEBUG TRAIN Batch 48/2000 loss 3.179343 loss_att 7.262583 loss_ctc 9.899845 loss_rnnt 1.342119 hw_loss 0.233453 lr 0.00029446 rank 5
2023-02-27 17:49:37,551 DEBUG TRAIN Batch 48/2100 loss 9.232038 loss_att 10.382632 loss_ctc 15.282799 loss_rnnt 8.090731 hw_loss 0.195786 lr 0.00029444 rank 4
2023-02-27 17:49:37,560 DEBUG TRAIN Batch 48/2100 loss 5.555054 loss_att 8.482804 loss_ctc 7.349595 loss_rnnt 4.654203 hw_loss 0.142552 lr 0.00029444 rank 6
2023-02-27 17:49:37,561 DEBUG TRAIN Batch 48/2100 loss 1.943814 loss_att 5.989980 loss_ctc 3.860204 loss_rnnt 0.739105 hw_loss 0.262418 lr 0.00029444 rank 3
2023-02-27 17:49:37,562 DEBUG TRAIN Batch 48/2100 loss 15.211974 loss_att 20.557007 loss_ctc 21.956503 loss_rnnt 13.116558 hw_loss 0.238384 lr 0.00029444 rank 7
2023-02-27 17:49:37,564 DEBUG TRAIN Batch 48/2100 loss 7.042773 loss_att 8.866384 loss_ctc 11.735245 loss_rnnt 5.861787 hw_loss 0.357377 lr 0.00029444 rank 1
2023-02-27 17:49:37,566 DEBUG TRAIN Batch 48/2100 loss 2.959734 loss_att 5.037145 loss_ctc 5.844736 loss_rnnt 2.074210 hw_loss 0.160077 lr 0.00029444 rank 0
2023-02-27 17:49:37,571 DEBUG TRAIN Batch 48/2100 loss 7.070479 loss_att 9.944211 loss_ctc 10.363787 loss_rnnt 5.919704 hw_loss 0.256726 lr 0.00029444 rank 2
2023-02-27 17:49:37,582 DEBUG TRAIN Batch 48/2100 loss 7.744556 loss_att 14.119049 loss_ctc 15.178557 loss_rnnt 5.377039 hw_loss 0.190159 lr 0.00029444 rank 5
2023-02-27 17:50:43,358 DEBUG TRAIN Batch 48/2200 loss 13.065581 loss_att 13.813019 loss_ctc 19.974625 loss_rnnt 11.873429 hw_loss 0.227735 lr 0.00029443 rank 5
2023-02-27 17:50:43,361 DEBUG TRAIN Batch 48/2200 loss 2.503923 loss_att 5.031833 loss_ctc 2.775891 loss_rnnt 1.919830 hw_loss 0.079216 lr 0.00029443 rank 3
2023-02-27 17:50:43,375 DEBUG TRAIN Batch 48/2200 loss 3.994867 loss_att 6.196618 loss_ctc 5.791366 loss_rnnt 3.206094 hw_loss 0.204167 lr 0.00029442 rank 7
2023-02-27 17:50:43,375 DEBUG TRAIN Batch 48/2200 loss 9.984376 loss_att 13.304553 loss_ctc 15.686117 loss_rnnt 8.531855 hw_loss 0.052977 lr 0.00029443 rank 0
2023-02-27 17:50:43,380 DEBUG TRAIN Batch 48/2200 loss 6.192425 loss_att 9.681732 loss_ctc 12.896313 loss_rnnt 4.535297 hw_loss 0.122653 lr 0.00029443 rank 6
2023-02-27 17:50:43,380 DEBUG TRAIN Batch 48/2200 loss 10.659065 loss_att 13.453850 loss_ctc 13.909713 loss_rnnt 9.535320 hw_loss 0.246316 lr 0.00029443 rank 1
2023-02-27 17:50:43,383 DEBUG TRAIN Batch 48/2200 loss 3.908490 loss_att 6.679782 loss_ctc 9.514994 loss_rnnt 2.501094 hw_loss 0.198006 lr 0.00029443 rank 4
2023-02-27 17:50:43,421 DEBUG TRAIN Batch 48/2200 loss 2.980271 loss_att 6.296781 loss_ctc 6.499990 loss_rnnt 1.685014 hw_loss 0.304986 lr 0.00029443 rank 2
2023-02-27 17:51:21,835 DEBUG TRAIN Batch 48/2300 loss 4.667569 loss_att 8.345321 loss_ctc 5.343572 loss_rnnt 3.730066 hw_loss 0.209660 lr 0.00029441 rank 7
2023-02-27 17:51:21,848 DEBUG TRAIN Batch 48/2300 loss 5.396569 loss_att 8.403099 loss_ctc 10.913185 loss_rnnt 3.932257 hw_loss 0.238981 lr 0.00029442 rank 0
2023-02-27 17:51:21,848 DEBUG TRAIN Batch 48/2300 loss 4.556834 loss_att 8.124756 loss_ctc 12.500976 loss_rnnt 2.702777 hw_loss 0.152351 lr 0.00029442 rank 3
2023-02-27 17:51:21,849 DEBUG TRAIN Batch 48/2300 loss 6.339971 loss_att 8.976298 loss_ctc 9.826290 loss_rnnt 5.187105 hw_loss 0.301422 lr 0.00029442 rank 1
2023-02-27 17:51:21,850 DEBUG TRAIN Batch 48/2300 loss 4.408235 loss_att 7.159757 loss_ctc 7.704241 loss_rnnt 3.226915 hw_loss 0.359153 lr 0.00029441 rank 2
2023-02-27 17:51:21,852 DEBUG TRAIN Batch 48/2300 loss 5.607627 loss_att 9.968634 loss_ctc 9.094114 loss_rnnt 4.215911 hw_loss 0.102468 lr 0.00029441 rank 6
2023-02-27 17:51:21,853 DEBUG TRAIN Batch 48/2300 loss 5.551702 loss_att 6.509000 loss_ctc 8.291110 loss_rnnt 4.891341 hw_loss 0.194339 lr 0.00029442 rank 5
2023-02-27 17:51:21,903 DEBUG TRAIN Batch 48/2300 loss 6.790852 loss_att 9.922258 loss_ctc 12.067499 loss_rnnt 5.291965 hw_loss 0.316973 lr 0.00029442 rank 4
2023-02-27 17:52:00,500 DEBUG TRAIN Batch 48/2400 loss 3.330728 loss_att 6.025743 loss_ctc 4.605845 loss_rnnt 2.509994 hw_loss 0.209467 lr 0.00029440 rank 2
2023-02-27 17:52:00,502 DEBUG TRAIN Batch 48/2400 loss 13.447518 loss_att 17.114157 loss_ctc 22.623932 loss_rnnt 11.420013 hw_loss 0.132480 lr 0.00029441 rank 5
2023-02-27 17:52:00,507 DEBUG TRAIN Batch 48/2400 loss 3.024639 loss_att 4.934643 loss_ctc 4.515851 loss_rnnt 2.324013 hw_loss 0.224617 lr 0.00029440 rank 6
2023-02-27 17:52:00,509 DEBUG TRAIN Batch 48/2400 loss 10.435732 loss_att 12.842015 loss_ctc 21.996534 loss_rnnt 8.247671 hw_loss 0.310059 lr 0.00029440 rank 4
2023-02-27 17:52:00,510 DEBUG TRAIN Batch 48/2400 loss 12.430595 loss_att 14.597808 loss_ctc 25.962141 loss_rnnt 10.114803 hw_loss 0.146518 lr 0.00029440 rank 0
2023-02-27 17:52:00,512 DEBUG TRAIN Batch 48/2400 loss 7.400070 loss_att 9.381802 loss_ctc 10.209461 loss_rnnt 6.489778 hw_loss 0.261301 lr 0.00029440 rank 3
2023-02-27 17:52:00,520 DEBUG TRAIN Batch 48/2400 loss 4.738710 loss_att 6.592425 loss_ctc 6.843112 loss_rnnt 4.069780 hw_loss 0.033001 lr 0.00029440 rank 7
2023-02-27 17:52:00,565 DEBUG TRAIN Batch 48/2400 loss 4.851192 loss_att 8.345333 loss_ctc 10.928757 loss_rnnt 3.247962 hw_loss 0.176360 lr 0.00029441 rank 1
2023-02-27 17:53:07,836 DEBUG TRAIN Batch 48/2500 loss 4.794525 loss_att 7.734976 loss_ctc 10.305801 loss_rnnt 3.346631 hw_loss 0.234314 lr 0.00029439 rank 6
2023-02-27 17:53:07,836 DEBUG TRAIN Batch 48/2500 loss 5.293106 loss_att 8.136774 loss_ctc 9.114773 loss_rnnt 4.041043 hw_loss 0.325825 lr 0.00029439 rank 1
2023-02-27 17:53:07,839 DEBUG TRAIN Batch 48/2500 loss 6.523911 loss_att 7.406854 loss_ctc 9.032461 loss_rnnt 5.831267 hw_loss 0.340468 lr 0.00029439 rank 3
2023-02-27 17:53:07,839 DEBUG TRAIN Batch 48/2500 loss 7.720540 loss_att 9.719688 loss_ctc 11.975307 loss_rnnt 6.612438 hw_loss 0.264319 lr 0.00029439 rank 0
2023-02-27 17:53:07,844 DEBUG TRAIN Batch 48/2500 loss 2.626859 loss_att 5.027735 loss_ctc 6.982298 loss_rnnt 1.435905 hw_loss 0.243852 lr 0.00029439 rank 5
2023-02-27 17:53:07,844 DEBUG TRAIN Batch 48/2500 loss 7.111290 loss_att 7.844062 loss_ctc 10.579283 loss_rnnt 6.321678 hw_loss 0.338735 lr 0.00029439 rank 2
2023-02-27 17:53:07,847 DEBUG TRAIN Batch 48/2500 loss 3.764242 loss_att 5.417946 loss_ctc 4.629969 loss_rnnt 3.174592 hw_loss 0.269023 lr 0.00029438 rank 7
2023-02-27 17:53:07,886 DEBUG TRAIN Batch 48/2500 loss 6.770000 loss_att 9.126466 loss_ctc 11.515016 loss_rnnt 5.516707 hw_loss 0.279994 lr 0.00029439 rank 4
2023-02-27 17:53:46,644 DEBUG TRAIN Batch 48/2600 loss 9.240516 loss_att 14.741455 loss_ctc 19.105570 loss_rnnt 6.742803 hw_loss 0.154098 lr 0.00029438 rank 2
2023-02-27 17:53:46,659 DEBUG TRAIN Batch 48/2600 loss 5.364376 loss_att 5.447745 loss_ctc 8.405776 loss_rnnt 4.748220 hw_loss 0.363679 lr 0.00029438 rank 0
2023-02-27 17:53:46,662 DEBUG TRAIN Batch 48/2600 loss 9.102921 loss_att 9.427082 loss_ctc 14.046896 loss_rnnt 8.278525 hw_loss 0.188187 lr 0.00029438 rank 4
2023-02-27 17:53:46,666 DEBUG TRAIN Batch 48/2600 loss 6.276327 loss_att 10.542087 loss_ctc 13.491999 loss_rnnt 4.343616 hw_loss 0.220253 lr 0.00029438 rank 1
2023-02-27 17:53:46,667 DEBUG TRAIN Batch 48/2600 loss 6.702148 loss_att 10.253473 loss_ctc 10.131213 loss_rnnt 5.379887 hw_loss 0.290226 lr 0.00029438 rank 5
2023-02-27 17:53:46,668 DEBUG TRAIN Batch 48/2600 loss 8.629876 loss_att 12.301433 loss_ctc 18.685139 loss_rnnt 6.414236 hw_loss 0.263676 lr 0.00029437 rank 7
2023-02-27 17:53:46,678 DEBUG TRAIN Batch 48/2600 loss 13.589481 loss_att 20.012758 loss_ctc 28.092606 loss_rnnt 10.280642 hw_loss 0.169563 lr 0.00029438 rank 6
2023-02-27 17:53:46,690 DEBUG TRAIN Batch 48/2600 loss 3.867678 loss_att 6.089161 loss_ctc 3.665314 loss_rnnt 3.266655 hw_loss 0.344453 lr 0.00029438 rank 3
2023-02-27 17:54:25,356 DEBUG TRAIN Batch 48/2700 loss 2.224775 loss_att 5.094116 loss_ctc 3.787737 loss_rnnt 1.288529 hw_loss 0.288717 lr 0.00029436 rank 6
2023-02-27 17:54:25,358 DEBUG TRAIN Batch 48/2700 loss 6.442180 loss_att 8.538313 loss_ctc 10.421405 loss_rnnt 5.365865 hw_loss 0.237234 lr 0.00029436 rank 3
2023-02-27 17:54:25,371 DEBUG TRAIN Batch 48/2700 loss 3.715183 loss_att 6.578414 loss_ctc 7.071514 loss_rnnt 2.572325 hw_loss 0.230063 lr 0.00029437 rank 0
2023-02-27 17:54:25,375 DEBUG TRAIN Batch 48/2700 loss 6.051382 loss_att 8.947886 loss_ctc 11.145956 loss_rnnt 4.663765 hw_loss 0.241946 lr 0.00029436 rank 7
2023-02-27 17:54:25,376 DEBUG TRAIN Batch 48/2700 loss 5.983083 loss_att 8.492839 loss_ctc 8.513218 loss_rnnt 5.071386 hw_loss 0.135741 lr 0.00029436 rank 2
2023-02-27 17:54:25,377 DEBUG TRAIN Batch 48/2700 loss 10.808012 loss_att 11.989659 loss_ctc 14.105560 loss_rnnt 10.080313 hw_loss 0.096931 lr 0.00029436 rank 4
2023-02-27 17:54:25,383 DEBUG TRAIN Batch 48/2700 loss 7.677963 loss_att 14.430832 loss_ctc 17.852793 loss_rnnt 4.826161 hw_loss 0.271096 lr 0.00029437 rank 1
2023-02-27 17:54:25,421 DEBUG TRAIN Batch 48/2700 loss 8.823331 loss_att 10.291691 loss_ctc 11.270622 loss_rnnt 8.063944 hw_loss 0.261393 lr 0.00029437 rank 5
2023-02-27 17:55:05,253 DEBUG TRAIN Batch 48/2800 loss 9.431562 loss_att 11.303839 loss_ctc 15.074868 loss_rnnt 8.119911 hw_loss 0.346416 lr 0.00029436 rank 1
2023-02-27 17:55:05,256 DEBUG TRAIN Batch 48/2800 loss 13.959985 loss_att 15.211381 loss_ctc 27.909794 loss_rnnt 11.743476 hw_loss 0.199230 lr 0.00029435 rank 3
2023-02-27 17:55:05,257 DEBUG TRAIN Batch 48/2800 loss 9.867977 loss_att 12.583832 loss_ctc 16.331299 loss_rnnt 8.432446 hw_loss 0.057343 lr 0.00029435 rank 0
2023-02-27 17:55:05,259 DEBUG TRAIN Batch 48/2800 loss 4.925695 loss_att 8.326090 loss_ctc 8.249774 loss_rnnt 3.649852 hw_loss 0.286039 lr 0.00029435 rank 6
2023-02-27 17:55:05,261 DEBUG TRAIN Batch 48/2800 loss 1.435283 loss_att 4.142623 loss_ctc 2.713282 loss_rnnt 0.559976 hw_loss 0.306448 lr 0.00029435 rank 7
2023-02-27 17:55:05,271 DEBUG TRAIN Batch 48/2800 loss 4.306896 loss_att 7.724953 loss_ctc 7.332416 loss_rnnt 3.107632 hw_loss 0.210468 lr 0.00029436 rank 5
2023-02-27 17:55:05,275 DEBUG TRAIN Batch 48/2800 loss 6.850496 loss_att 12.639807 loss_ctc 14.861835 loss_rnnt 4.441699 hw_loss 0.342668 lr 0.00029435 rank 2
2023-02-27 17:55:05,290 DEBUG TRAIN Batch 48/2800 loss 4.926340 loss_att 8.475572 loss_ctc 10.679127 loss_rnnt 3.295374 hw_loss 0.288904 lr 0.00029435 rank 4
2023-02-27 17:56:09,226 DEBUG TRAIN Batch 48/2900 loss 3.990693 loss_att 6.043314 loss_ctc 12.830251 loss_rnnt 2.223232 hw_loss 0.334366 lr 0.00029434 rank 5
2023-02-27 17:56:09,227 DEBUG TRAIN Batch 48/2900 loss 4.051829 loss_att 6.749011 loss_ctc 8.219561 loss_rnnt 2.863160 hw_loss 0.175379 lr 0.00029434 rank 4
2023-02-27 17:56:09,233 DEBUG TRAIN Batch 48/2900 loss 3.382146 loss_att 6.088562 loss_ctc 7.730394 loss_rnnt 2.133281 hw_loss 0.239653 lr 0.00029434 rank 6
2023-02-27 17:56:09,238 DEBUG TRAIN Batch 48/2900 loss 6.320066 loss_att 10.980733 loss_ctc 12.770157 loss_rnnt 4.367097 hw_loss 0.301545 lr 0.00029434 rank 0
2023-02-27 17:56:09,240 DEBUG TRAIN Batch 48/2900 loss 5.883511 loss_att 7.303108 loss_ctc 9.520679 loss_rnnt 4.966432 hw_loss 0.277882 lr 0.00029434 rank 3
2023-02-27 17:56:09,243 DEBUG TRAIN Batch 48/2900 loss 5.134290 loss_att 7.764118 loss_ctc 10.389780 loss_rnnt 3.777827 hw_loss 0.243309 lr 0.00029434 rank 1
2023-02-27 17:56:09,246 DEBUG TRAIN Batch 48/2900 loss 5.528585 loss_att 7.019817 loss_ctc 6.833553 loss_rnnt 4.934320 hw_loss 0.228792 lr 0.00029434 rank 2
2023-02-27 17:56:09,269 DEBUG TRAIN Batch 48/2900 loss 4.609309 loss_att 8.055598 loss_ctc 5.082172 loss_rnnt 3.719467 hw_loss 0.257878 lr 0.00029433 rank 7
2023-02-27 17:56:47,756 DEBUG TRAIN Batch 48/3000 loss 4.710926 loss_att 8.933370 loss_ctc 11.244839 loss_rnnt 2.840506 hw_loss 0.290143 lr 0.00029432 rank 7
2023-02-27 17:56:47,766 DEBUG TRAIN Batch 48/3000 loss 5.022688 loss_att 9.531141 loss_ctc 8.568541 loss_rnnt 3.431878 hw_loss 0.405635 lr 0.00029433 rank 5
2023-02-27 17:56:47,769 DEBUG TRAIN Batch 48/3000 loss 11.718655 loss_att 12.676340 loss_ctc 17.236135 loss_rnnt 10.707940 hw_loss 0.156587 lr 0.00029433 rank 4
2023-02-27 17:56:47,774 DEBUG TRAIN Batch 48/3000 loss 5.471028 loss_att 7.562634 loss_ctc 13.532780 loss_rnnt 3.912304 hw_loss 0.122816 lr 0.00029433 rank 3
2023-02-27 17:56:47,776 DEBUG TRAIN Batch 48/3000 loss 4.161789 loss_att 6.651737 loss_ctc 6.808575 loss_rnnt 3.225731 hw_loss 0.159679 lr 0.00029433 rank 6
2023-02-27 17:56:47,778 DEBUG TRAIN Batch 48/3000 loss 5.068098 loss_att 7.459017 loss_ctc 11.327100 loss_rnnt 3.660577 hw_loss 0.177756 lr 0.00029433 rank 0
2023-02-27 17:56:47,783 DEBUG TRAIN Batch 48/3000 loss 9.559049 loss_att 14.046257 loss_ctc 15.797276 loss_rnnt 7.746018 hw_loss 0.157173 lr 0.00029433 rank 1
2023-02-27 17:56:47,786 DEBUG TRAIN Batch 48/3000 loss 10.749884 loss_att 15.409819 loss_ctc 27.832407 loss_rnnt 7.417048 hw_loss 0.230961 lr 0.00029432 rank 2
2023-02-27 17:57:27,109 DEBUG TRAIN Batch 48/3100 loss 5.704112 loss_att 7.720564 loss_ctc 11.457911 loss_rnnt 4.368010 hw_loss 0.310573 lr 0.00029431 rank 3
2023-02-27 17:57:27,116 DEBUG TRAIN Batch 48/3100 loss 3.730312 loss_att 6.875196 loss_ctc 8.138357 loss_rnnt 2.320477 hw_loss 0.362100 lr 0.00029431 rank 0
2023-02-27 17:57:27,117 DEBUG TRAIN Batch 48/3100 loss 13.275437 loss_att 15.143999 loss_ctc 17.636353 loss_rnnt 12.149530 hw_loss 0.320134 lr 0.00029431 rank 6
2023-02-27 17:57:27,120 DEBUG TRAIN Batch 48/3100 loss 4.150930 loss_att 6.160147 loss_ctc 7.653765 loss_rnnt 3.122872 hw_loss 0.298446 lr 0.00029431 rank 4
2023-02-27 17:57:27,120 DEBUG TRAIN Batch 48/3100 loss 3.762955 loss_att 4.713594 loss_ctc 5.626280 loss_rnnt 3.089649 hw_loss 0.440129 lr 0.00029432 rank 5
2023-02-27 17:57:27,120 DEBUG TRAIN Batch 48/3100 loss 4.321695 loss_att 7.697982 loss_ctc 12.901611 loss_rnnt 2.384858 hw_loss 0.220483 lr 0.00029431 rank 2
2023-02-27 17:57:27,135 DEBUG TRAIN Batch 48/3100 loss 9.823396 loss_att 10.180866 loss_ctc 12.555069 loss_rnnt 9.219714 hw_loss 0.314935 lr 0.00029432 rank 1
2023-02-27 17:57:27,140 DEBUG TRAIN Batch 48/3100 loss 9.731771 loss_att 11.953708 loss_ctc 16.477571 loss_rnnt 8.279227 hw_loss 0.203844 lr 0.00029431 rank 7
2023-02-27 17:58:33,781 DEBUG TRAIN Batch 48/3200 loss 4.101188 loss_att 7.938794 loss_ctc 6.526136 loss_rnnt 2.902412 hw_loss 0.202366 lr 0.00029430 rank 3
2023-02-27 17:58:33,786 DEBUG TRAIN Batch 48/3200 loss 10.008872 loss_att 9.987443 loss_ctc 12.012351 loss_rnnt 9.599650 hw_loss 0.274459 lr 0.00029430 rank 6
2023-02-27 17:58:33,790 DEBUG TRAIN Batch 48/3200 loss 4.045606 loss_att 8.006379 loss_ctc 8.071306 loss_rnnt 2.574366 hw_loss 0.266859 lr 0.00029430 rank 5
2023-02-27 17:58:33,792 DEBUG TRAIN Batch 48/3200 loss 6.216643 loss_att 8.206683 loss_ctc 12.767187 loss_rnnt 4.918337 hw_loss 0.050423 lr 0.00029430 rank 0
2023-02-27 17:58:33,793 DEBUG TRAIN Batch 48/3200 loss 2.251067 loss_att 3.894096 loss_ctc 4.261212 loss_rnnt 1.519831 hw_loss 0.252394 lr 0.00029430 rank 1
2023-02-27 17:58:33,816 DEBUG TRAIN Batch 48/3200 loss 4.755009 loss_att 4.819379 loss_ctc 6.896751 loss_rnnt 4.286398 hw_loss 0.319071 lr 0.00029430 rank 4
2023-02-27 17:58:33,825 DEBUG TRAIN Batch 48/3200 loss 5.960245 loss_att 7.323207 loss_ctc 11.791385 loss_rnnt 4.727319 hw_loss 0.342840 lr 0.00029430 rank 2
2023-02-27 17:58:33,844 DEBUG TRAIN Batch 48/3200 loss 6.374789 loss_att 10.098500 loss_ctc 12.495967 loss_rnnt 4.730004 hw_loss 0.157284 lr 0.00029430 rank 7
2023-02-27 17:59:12,632 DEBUG TRAIN Batch 48/3300 loss 8.930683 loss_att 11.139004 loss_ctc 12.580398 loss_rnnt 7.852903 hw_loss 0.280290 lr 0.00029429 rank 0
2023-02-27 17:59:12,634 DEBUG TRAIN Batch 48/3300 loss 2.117591 loss_att 5.122392 loss_ctc 3.984243 loss_rnnt 1.128177 hw_loss 0.261689 lr 0.00029428 rank 7
2023-02-27 17:59:12,634 DEBUG TRAIN Batch 48/3300 loss 8.437499 loss_att 10.588285 loss_ctc 12.061645 loss_rnnt 7.444458 hw_loss 0.149371 lr 0.00029429 rank 6
2023-02-27 17:59:12,636 DEBUG TRAIN Batch 48/3300 loss 7.048453 loss_att 11.674851 loss_ctc 11.849807 loss_rnnt 5.357225 hw_loss 0.235814 lr 0.00029429 rank 3
2023-02-27 17:59:12,645 DEBUG TRAIN Batch 48/3300 loss 4.308179 loss_att 7.284546 loss_ctc 7.139188 loss_rnnt 3.214436 hw_loss 0.226881 lr 0.00029429 rank 4
2023-02-27 17:59:12,643 DEBUG TRAIN Batch 48/3300 loss 3.733385 loss_att 7.723889 loss_ctc 6.031333 loss_rnnt 2.483798 hw_loss 0.272049 lr 0.00029429 rank 2
2023-02-27 17:59:12,651 DEBUG TRAIN Batch 48/3300 loss 2.309107 loss_att 5.143503 loss_ctc 4.971594 loss_rnnt 1.215752 hw_loss 0.321520 lr 0.00029429 rank 1
2023-02-27 17:59:12,687 DEBUG TRAIN Batch 48/3300 loss 2.440481 loss_att 4.803965 loss_ctc 4.542315 loss_rnnt 1.531485 hw_loss 0.292602 lr 0.00029429 rank 5
2023-02-27 17:59:51,942 DEBUG TRAIN Batch 48/3400 loss 7.020844 loss_att 11.298128 loss_ctc 12.248718 loss_rnnt 5.304627 hw_loss 0.306956 lr 0.00029427 rank 6
2023-02-27 17:59:51,943 DEBUG TRAIN Batch 48/3400 loss 11.280226 loss_att 14.180641 loss_ctc 18.589798 loss_rnnt 9.628510 hw_loss 0.181918 lr 0.00029428 rank 4
2023-02-27 17:59:51,944 DEBUG TRAIN Batch 48/3400 loss 7.778440 loss_att 9.048464 loss_ctc 8.916944 loss_rnnt 7.193676 hw_loss 0.335548 lr 0.00029428 rank 3
2023-02-27 17:59:51,947 DEBUG TRAIN Batch 48/3400 loss 6.940079 loss_att 11.383133 loss_ctc 11.598979 loss_rnnt 5.282939 hw_loss 0.276268 lr 0.00029427 rank 2
2023-02-27 17:59:51,947 DEBUG TRAIN Batch 48/3400 loss 3.377850 loss_att 6.903802 loss_ctc 8.267490 loss_rnnt 1.924625 hw_loss 0.180153 lr 0.00029428 rank 0
2023-02-27 17:59:51,966 DEBUG TRAIN Batch 48/3400 loss 10.662490 loss_att 11.064186 loss_ctc 13.453769 loss_rnnt 10.055132 hw_loss 0.290342 lr 0.00029427 rank 7
2023-02-27 17:59:51,968 DEBUG TRAIN Batch 48/3400 loss 7.528640 loss_att 8.432547 loss_ctc 10.645864 loss_rnnt 6.824151 hw_loss 0.202645 lr 0.00029428 rank 5
2023-02-27 17:59:52,005 DEBUG TRAIN Batch 48/3400 loss 4.826087 loss_att 5.834258 loss_ctc 12.051483 loss_rnnt 3.548578 hw_loss 0.210915 lr 0.00029428 rank 1
2023-02-27 18:00:31,659 DEBUG TRAIN Batch 48/3500 loss 5.882354 loss_att 9.634928 loss_ctc 10.069045 loss_rnnt 4.479359 hw_loss 0.176728 lr 0.00029426 rank 6
2023-02-27 18:00:31,665 DEBUG TRAIN Batch 48/3500 loss 9.017790 loss_att 9.462473 loss_ctc 14.612980 loss_rnnt 8.091440 hw_loss 0.171352 lr 0.00029426 rank 4
2023-02-27 18:00:31,669 DEBUG TRAIN Batch 48/3500 loss 2.444371 loss_att 5.376467 loss_ctc 3.994934 loss_rnnt 1.585410 hw_loss 0.123374 lr 0.00029426 rank 2
2023-02-27 18:00:31,674 DEBUG TRAIN Batch 48/3500 loss 2.794537 loss_att 6.472948 loss_ctc 5.194398 loss_rnnt 1.676422 hw_loss 0.117095 lr 0.00029427 rank 5
2023-02-27 18:00:31,679 DEBUG TRAIN Batch 48/3500 loss 7.005196 loss_att 9.566630 loss_ctc 12.136742 loss_rnnt 5.719931 hw_loss 0.166447 lr 0.00029427 rank 1
2023-02-27 18:00:31,694 DEBUG TRAIN Batch 48/3500 loss 8.323533 loss_att 11.587801 loss_ctc 13.996490 loss_rnnt 6.913924 hw_loss 0.000678 lr 0.00029426 rank 3
2023-02-27 18:00:31,700 DEBUG TRAIN Batch 48/3500 loss 7.963033 loss_att 9.326065 loss_ctc 11.117826 loss_rnnt 7.200129 hw_loss 0.130608 lr 0.00029426 rank 0
2023-02-27 18:00:31,708 DEBUG TRAIN Batch 48/3500 loss 1.832406 loss_att 4.621243 loss_ctc 3.440462 loss_rnnt 0.890702 hw_loss 0.317867 lr 0.00029426 rank 7
2023-02-27 18:01:38,345 DEBUG TRAIN Batch 48/3600 loss 3.190772 loss_att 6.409186 loss_ctc 6.234690 loss_rnnt 2.029089 hw_loss 0.210269 lr 0.00029425 rank 4
2023-02-27 18:01:38,348 DEBUG TRAIN Batch 48/3600 loss 6.284018 loss_att 9.748433 loss_ctc 14.369698 loss_rnnt 4.334754 hw_loss 0.334293 lr 0.00029425 rank 3
2023-02-27 18:01:38,350 DEBUG TRAIN Batch 48/3600 loss 5.281868 loss_att 8.003368 loss_ctc 13.272160 loss_rnnt 3.553554 hw_loss 0.222455 lr 0.00029425 rank 6
2023-02-27 18:01:38,350 DEBUG TRAIN Batch 48/3600 loss 13.206630 loss_att 16.793562 loss_ctc 21.615723 loss_rnnt 11.266788 hw_loss 0.189832 lr 0.00029425 rank 0
2023-02-27 18:01:38,353 DEBUG TRAIN Batch 48/3600 loss 6.617591 loss_att 10.050160 loss_ctc 13.708360 loss_rnnt 4.903943 hw_loss 0.153184 lr 0.00029425 rank 1
2023-02-27 18:01:38,353 DEBUG TRAIN Batch 48/3600 loss 4.831684 loss_att 7.582457 loss_ctc 7.344899 loss_rnnt 3.780273 hw_loss 0.311553 lr 0.00029425 rank 5
2023-02-27 18:01:38,394 DEBUG TRAIN Batch 48/3600 loss 6.439084 loss_att 8.553598 loss_ctc 10.336992 loss_rnnt 5.397911 hw_loss 0.184779 lr 0.00029424 rank 7
2023-02-27 18:01:38,396 DEBUG TRAIN Batch 48/3600 loss 4.236708 loss_att 7.170597 loss_ctc 6.921122 loss_rnnt 3.173841 hw_loss 0.221565 lr 0.00029425 rank 2
2023-02-27 18:02:17,057 DEBUG TRAIN Batch 48/3700 loss 4.230410 loss_att 7.407914 loss_ctc 7.828241 loss_rnnt 2.993679 hw_loss 0.227847 lr 0.00029424 rank 0
2023-02-27 18:02:17,058 DEBUG TRAIN Batch 48/3700 loss 9.010379 loss_att 11.562809 loss_ctc 17.579308 loss_rnnt 7.285545 hw_loss 0.134667 lr 0.00029424 rank 6
2023-02-27 18:02:17,059 DEBUG TRAIN Batch 48/3700 loss 7.643861 loss_att 9.300942 loss_ctc 13.298775 loss_rnnt 6.457376 hw_loss 0.189527 lr 0.00029424 rank 3
2023-02-27 18:02:17,059 DEBUG TRAIN Batch 48/3700 loss 5.693871 loss_att 8.321602 loss_ctc 8.896018 loss_rnnt 4.610338 hw_loss 0.245689 lr 0.00029424 rank 2
2023-02-27 18:02:17,061 DEBUG TRAIN Batch 48/3700 loss 11.429856 loss_att 13.635009 loss_ctc 21.270145 loss_rnnt 9.527564 hw_loss 0.279792 lr 0.00029424 rank 1
2023-02-27 18:02:17,066 DEBUG TRAIN Batch 48/3700 loss 7.784553 loss_att 12.438295 loss_ctc 19.847691 loss_rnnt 5.026399 hw_loss 0.410600 lr 0.00029424 rank 5
2023-02-27 18:02:17,067 DEBUG TRAIN Batch 48/3700 loss 6.579165 loss_att 10.545670 loss_ctc 13.157984 loss_rnnt 4.723814 hw_loss 0.346639 lr 0.00029423 rank 7
2023-02-27 18:02:17,109 DEBUG TRAIN Batch 48/3700 loss 3.659777 loss_att 6.026536 loss_ctc 9.627316 loss_rnnt 2.306071 hw_loss 0.158780 lr 0.00029424 rank 4
2023-02-27 18:02:56,365 DEBUG TRAIN Batch 48/3800 loss 7.539361 loss_att 11.611042 loss_ctc 12.439330 loss_rnnt 5.996952 hw_loss 0.140142 lr 0.00029422 rank 6
2023-02-27 18:02:56,365 DEBUG TRAIN Batch 48/3800 loss 8.097342 loss_att 10.439842 loss_ctc 11.606620 loss_rnnt 7.017274 hw_loss 0.269368 lr 0.00029422 rank 4
2023-02-27 18:02:56,369 DEBUG TRAIN Batch 48/3800 loss 4.504192 loss_att 6.340769 loss_ctc 10.620616 loss_rnnt 3.122834 hw_loss 0.372222 lr 0.00029423 rank 0
2023-02-27 18:02:56,369 DEBUG TRAIN Batch 48/3800 loss 2.039422 loss_att 4.424551 loss_ctc 2.158050 loss_rnnt 1.393034 hw_loss 0.287898 lr 0.00029422 rank 3
2023-02-27 18:02:56,370 DEBUG TRAIN Batch 48/3800 loss 6.406707 loss_att 8.893140 loss_ctc 10.741960 loss_rnnt 5.187057 hw_loss 0.270620 lr 0.00029422 rank 7
2023-02-27 18:02:56,371 DEBUG TRAIN Batch 48/3800 loss 5.399114 loss_att 7.050656 loss_ctc 9.504932 loss_rnnt 4.399692 hw_loss 0.228133 lr 0.00029423 rank 5
2023-02-27 18:02:56,371 DEBUG TRAIN Batch 48/3800 loss 5.165529 loss_att 9.291594 loss_ctc 10.375212 loss_rnnt 3.576970 hw_loss 0.128854 lr 0.00029423 rank 1
2023-02-27 18:02:56,371 DEBUG TRAIN Batch 48/3800 loss 10.435055 loss_att 11.707341 loss_ctc 15.501650 loss_rnnt 9.414684 hw_loss 0.169438 lr 0.00029422 rank 2
2023-02-27 18:04:02,254 DEBUG TRAIN Batch 48/3900 loss 6.866145 loss_att 9.161447 loss_ctc 14.222334 loss_rnnt 5.291920 hw_loss 0.251884 lr 0.00029421 rank 2
2023-02-27 18:04:02,268 DEBUG TRAIN Batch 48/3900 loss 12.801682 loss_att 14.912790 loss_ctc 18.597982 loss_rnnt 11.452542 hw_loss 0.288894 lr 0.00029421 rank 4
2023-02-27 18:04:02,267 DEBUG TRAIN Batch 48/3900 loss 8.983558 loss_att 10.470659 loss_ctc 13.849646 loss_rnnt 7.825075 hw_loss 0.397972 lr 0.00029421 rank 6
2023-02-27 18:04:02,268 DEBUG TRAIN Batch 48/3900 loss 6.808181 loss_att 10.116496 loss_ctc 15.802109 loss_rnnt 4.877532 hw_loss 0.130867 lr 0.00029422 rank 5
2023-02-27 18:04:02,268 DEBUG TRAIN Batch 48/3900 loss 2.762165 loss_att 5.369894 loss_ctc 5.034296 loss_rnnt 1.855670 hw_loss 0.153748 lr 0.00029421 rank 3
2023-02-27 18:04:02,267 DEBUG TRAIN Batch 48/3900 loss 4.122945 loss_att 9.072737 loss_ctc 10.389153 loss_rnnt 2.133341 hw_loss 0.307784 lr 0.00029421 rank 7
2023-02-27 18:04:02,306 DEBUG TRAIN Batch 48/3900 loss 7.317011 loss_att 9.337040 loss_ctc 10.286485 loss_rnnt 6.402992 hw_loss 0.213906 lr 0.00029421 rank 0
2023-02-27 18:04:02,317 DEBUG TRAIN Batch 48/3900 loss 10.116737 loss_att 12.535604 loss_ctc 18.886375 loss_rnnt 8.402455 hw_loss 0.114794 lr 0.00029421 rank 1
2023-02-27 18:04:42,823 DEBUG TRAIN Batch 48/4000 loss 9.685794 loss_att 12.492654 loss_ctc 14.149315 loss_rnnt 8.461542 hw_loss 0.127018 lr 0.00029420 rank 2
2023-02-27 18:04:42,825 DEBUG TRAIN Batch 48/4000 loss 4.872600 loss_att 7.193523 loss_ctc 7.217479 loss_rnnt 4.051805 hw_loss 0.082425 lr 0.00029420 rank 5
2023-02-27 18:04:42,839 DEBUG TRAIN Batch 48/4000 loss 1.241557 loss_att 3.806173 loss_ctc 2.363745 loss_rnnt 0.451180 hw_loss 0.239679 lr 0.00029419 rank 7
2023-02-27 18:04:42,839 DEBUG TRAIN Batch 48/4000 loss 5.083480 loss_att 9.800705 loss_ctc 10.378001 loss_rnnt 3.270050 hw_loss 0.307592 lr 0.00029420 rank 0
2023-02-27 18:04:42,839 DEBUG TRAIN Batch 48/4000 loss 2.258692 loss_att 5.620933 loss_ctc 6.229134 loss_rnnt 0.954707 hw_loss 0.191520 lr 0.00029420 rank 6
2023-02-27 18:04:42,841 DEBUG TRAIN Batch 48/4000 loss 1.342903 loss_att 3.818782 loss_ctc 2.723723 loss_rnnt 0.475668 hw_loss 0.352404 lr 0.00029420 rank 3
2023-02-27 18:04:42,844 DEBUG TRAIN Batch 48/4000 loss 2.335029 loss_att 5.496388 loss_ctc 4.035204 loss_rnnt 1.295685 hw_loss 0.338218 lr 0.00029420 rank 1
2023-02-27 18:04:42,896 DEBUG TRAIN Batch 48/4000 loss 6.623272 loss_att 11.069037 loss_ctc 16.485516 loss_rnnt 4.302455 hw_loss 0.218809 lr 0.00029420 rank 4
2023-02-27 18:05:21,999 DEBUG TRAIN Batch 48/4100 loss 2.272606 loss_att 5.841455 loss_ctc 4.559338 loss_rnnt 1.124012 hw_loss 0.243613 lr 0.00029419 rank 4
2023-02-27 18:05:21,999 DEBUG TRAIN Batch 48/4100 loss 4.775107 loss_att 7.010824 loss_ctc 6.951065 loss_rnnt 3.953859 hw_loss 0.157456 lr 0.00029418 rank 7
2023-02-27 18:05:22,001 DEBUG TRAIN Batch 48/4100 loss 7.464484 loss_att 11.362804 loss_ctc 12.903427 loss_rnnt 5.890627 hw_loss 0.129375 lr 0.00029419 rank 1
2023-02-27 18:05:22,010 DEBUG TRAIN Batch 48/4100 loss 5.053064 loss_att 8.758095 loss_ctc 12.454388 loss_rnnt 3.176462 hw_loss 0.278912 lr 0.00029419 rank 5
2023-02-27 18:05:22,011 DEBUG TRAIN Batch 48/4100 loss 5.545749 loss_att 9.547929 loss_ctc 11.763186 loss_rnnt 3.853959 hw_loss 0.116929 lr 0.00029419 rank 6
2023-02-27 18:05:22,014 DEBUG TRAIN Batch 48/4100 loss 5.981462 loss_att 7.642642 loss_ctc 8.956148 loss_rnnt 5.095084 hw_loss 0.295345 lr 0.00029419 rank 3
2023-02-27 18:05:22,014 DEBUG TRAIN Batch 48/4100 loss 4.598704 loss_att 6.909890 loss_ctc 6.734218 loss_rnnt 3.647883 hw_loss 0.382215 lr 0.00029419 rank 0
2023-02-27 18:05:22,015 DEBUG TRAIN Batch 48/4100 loss 2.531624 loss_att 5.099106 loss_ctc 5.100521 loss_rnnt 1.558914 hw_loss 0.218799 lr 0.00029418 rank 2
2023-02-27 18:06:02,547 DEBUG TRAIN Batch 48/4200 loss 3.841779 loss_att 6.516098 loss_ctc 5.930392 loss_rnnt 2.921005 hw_loss 0.201428 lr 0.00029417 rank 2
2023-02-27 18:06:02,548 DEBUG TRAIN Batch 48/4200 loss 2.151748 loss_att 5.212414 loss_ctc 5.735470 loss_rnnt 0.920946 hw_loss 0.264072 lr 0.00029418 rank 5
2023-02-27 18:06:02,558 DEBUG TRAIN Batch 48/4200 loss 4.818677 loss_att 6.183784 loss_ctc 7.170766 loss_rnnt 4.057726 hw_loss 0.326845 lr 0.00029417 rank 6
2023-02-27 18:06:02,560 DEBUG TRAIN Batch 48/4200 loss 10.461152 loss_att 12.403064 loss_ctc 12.938275 loss_rnnt 9.549027 hw_loss 0.362735 lr 0.00029417 rank 3
2023-02-27 18:06:02,561 DEBUG TRAIN Batch 48/4200 loss 7.298115 loss_att 10.493648 loss_ctc 11.745043 loss_rnnt 5.943492 hw_loss 0.229863 lr 0.00029417 rank 0
2023-02-27 18:06:02,566 DEBUG TRAIN Batch 48/4200 loss 8.099884 loss_att 10.276384 loss_ctc 13.682182 loss_rnnt 6.874140 hw_loss 0.086508 lr 0.00029418 rank 1
2023-02-27 18:06:02,573 DEBUG TRAIN Batch 48/4200 loss 3.623694 loss_att 6.416068 loss_ctc 9.467104 loss_rnnt 2.145432 hw_loss 0.263748 lr 0.00029417 rank 4
2023-02-27 18:06:02,591 DEBUG TRAIN Batch 48/4200 loss 9.155363 loss_att 11.385435 loss_ctc 13.746801 loss_rnnt 7.965551 hw_loss 0.246760 lr 0.00029417 rank 7
2023-02-27 18:07:09,036 DEBUG TRAIN Batch 48/4300 loss 4.846618 loss_att 6.660904 loss_ctc 7.099482 loss_rnnt 4.052441 hw_loss 0.245508 lr 0.00029416 rank 6
2023-02-27 18:07:09,038 DEBUG TRAIN Batch 48/4300 loss 8.655859 loss_att 13.808502 loss_ctc 15.212180 loss_rnnt 6.668710 hw_loss 0.154581 lr 0.00029416 rank 0
2023-02-27 18:07:09,043 DEBUG TRAIN Batch 48/4300 loss 3.359026 loss_att 5.309441 loss_ctc 6.187864 loss_rnnt 2.437553 hw_loss 0.289145 lr 0.00029416 rank 5
2023-02-27 18:07:09,043 DEBUG TRAIN Batch 48/4300 loss 8.095468 loss_att 13.356176 loss_ctc 16.755505 loss_rnnt 5.748350 hw_loss 0.263072 lr 0.00029416 rank 3
2023-02-27 18:07:09,049 DEBUG TRAIN Batch 48/4300 loss 6.614225 loss_att 12.289244 loss_ctc 11.971897 loss_rnnt 4.674373 hw_loss 0.169673 lr 0.00029416 rank 4
2023-02-27 18:07:09,050 DEBUG TRAIN Batch 48/4300 loss 5.522972 loss_att 8.672549 loss_ctc 10.857180 loss_rnnt 4.062323 hw_loss 0.224074 lr 0.00029416 rank 2
2023-02-27 18:07:09,053 DEBUG TRAIN Batch 48/4300 loss 4.717382 loss_att 7.215935 loss_ctc 7.051694 loss_rnnt 3.818153 hw_loss 0.165519 lr 0.00029416 rank 7
2023-02-27 18:07:09,052 DEBUG TRAIN Batch 48/4300 loss 2.640341 loss_att 5.045664 loss_ctc 8.081866 loss_rnnt 1.291217 hw_loss 0.267230 lr 0.00029416 rank 1
2023-02-27 18:07:49,246 DEBUG TRAIN Batch 48/4400 loss 4.737768 loss_att 6.432685 loss_ctc 7.927460 loss_rnnt 3.906560 hw_loss 0.125498 lr 0.00029415 rank 1
2023-02-27 18:07:49,254 DEBUG TRAIN Batch 48/4400 loss 10.416215 loss_att 13.551826 loss_ctc 19.863276 loss_rnnt 8.406404 hw_loss 0.230774 lr 0.00029415 rank 3
2023-02-27 18:07:49,255 DEBUG TRAIN Batch 48/4400 loss 3.978623 loss_att 7.678169 loss_ctc 8.588868 loss_rnnt 2.471446 hw_loss 0.286067 lr 0.00029415 rank 6
2023-02-27 18:07:49,258 DEBUG TRAIN Batch 48/4400 loss 3.358868 loss_att 5.280798 loss_ctc 4.875576 loss_rnnt 2.689434 hw_loss 0.155287 lr 0.00029415 rank 2
2023-02-27 18:07:49,258 DEBUG TRAIN Batch 48/4400 loss 7.373512 loss_att 8.842138 loss_ctc 10.834004 loss_rnnt 6.457015 hw_loss 0.302575 lr 0.00029415 rank 0
2023-02-27 18:07:49,258 DEBUG TRAIN Batch 48/4400 loss 4.096571 loss_att 6.007901 loss_ctc 5.897957 loss_rnnt 3.321737 hw_loss 0.285721 lr 0.00029414 rank 7
2023-02-27 18:07:49,259 DEBUG TRAIN Batch 48/4400 loss 11.738150 loss_att 12.381721 loss_ctc 19.665752 loss_rnnt 10.437577 hw_loss 0.215331 lr 0.00029415 rank 5
2023-02-27 18:07:49,263 DEBUG TRAIN Batch 48/4400 loss 7.054881 loss_att 9.721666 loss_ctc 18.131691 loss_rnnt 4.942501 hw_loss 0.191467 lr 0.00029415 rank 4
2023-02-27 18:08:29,328 DEBUG TRAIN Batch 48/4500 loss 8.680392 loss_att 9.992361 loss_ctc 14.625590 loss_rnnt 7.420177 hw_loss 0.384616 lr 0.00029413 rank 6
2023-02-27 18:08:29,336 DEBUG TRAIN Batch 48/4500 loss 1.781927 loss_att 4.726822 loss_ctc 1.834793 loss_rnnt 1.051008 hw_loss 0.252922 lr 0.00029414 rank 5
2023-02-27 18:08:29,336 DEBUG TRAIN Batch 48/4500 loss 3.884382 loss_att 5.136027 loss_ctc 6.015058 loss_rnnt 3.198239 hw_loss 0.284483 lr 0.00029413 rank 2
2023-02-27 18:08:29,336 DEBUG TRAIN Batch 48/4500 loss 7.121004 loss_att 8.359241 loss_ctc 14.485106 loss_rnnt 5.792222 hw_loss 0.186102 lr 0.00029413 rank 3
2023-02-27 18:08:29,338 DEBUG TRAIN Batch 48/4500 loss 5.676565 loss_att 8.587824 loss_ctc 8.746667 loss_rnnt 4.622040 hw_loss 0.117985 lr 0.00029414 rank 0
2023-02-27 18:08:29,342 DEBUG TRAIN Batch 48/4500 loss 4.482145 loss_att 11.411654 loss_ctc 12.148775 loss_rnnt 1.944504 hw_loss 0.242856 lr 0.00029414 rank 1
2023-02-27 18:08:29,342 DEBUG TRAIN Batch 48/4500 loss 6.053918 loss_att 7.546127 loss_ctc 9.322164 loss_rnnt 5.180812 hw_loss 0.260434 lr 0.00029414 rank 4
2023-02-27 18:08:29,359 DEBUG TRAIN Batch 48/4500 loss 7.626990 loss_att 12.436079 loss_ctc 20.609970 loss_rnnt 4.810728 hw_loss 0.231338 lr 0.00029413 rank 7
2023-02-27 18:09:10,508 DEBUG TRAIN Batch 48/4600 loss 3.415962 loss_att 7.055616 loss_ctc 7.353203 loss_rnnt 1.962827 hw_loss 0.375446 lr 0.00029412 rank 4
2023-02-27 18:09:10,516 DEBUG TRAIN Batch 48/4600 loss 4.647842 loss_att 7.469239 loss_ctc 7.743957 loss_rnnt 3.512322 hw_loss 0.297047 lr 0.00029412 rank 3
2023-02-27 18:09:10,519 DEBUG TRAIN Batch 48/4600 loss 13.330403 loss_att 17.914551 loss_ctc 25.447964 loss_rnnt 10.742205 hw_loss 0.104428 lr 0.00029412 rank 6
2023-02-27 18:09:10,519 DEBUG TRAIN Batch 48/4600 loss 3.325105 loss_att 7.427558 loss_ctc 6.737080 loss_rnnt 1.912363 hw_loss 0.257477 lr 0.00029413 rank 5
2023-02-27 18:09:10,524 DEBUG TRAIN Batch 48/4600 loss 9.783580 loss_att 12.351353 loss_ctc 17.031826 loss_rnnt 8.170639 hw_loss 0.249288 lr 0.00029412 rank 0
2023-02-27 18:09:10,530 DEBUG TRAIN Batch 48/4600 loss 10.317991 loss_att 11.256362 loss_ctc 16.430016 loss_rnnt 9.217285 hw_loss 0.183929 lr 0.00029413 rank 1
2023-02-27 18:09:10,563 DEBUG TRAIN Batch 48/4600 loss 4.958004 loss_att 7.337793 loss_ctc 7.831264 loss_rnnt 3.971397 hw_loss 0.239153 lr 0.00029412 rank 2
2023-02-27 18:09:10,566 DEBUG TRAIN Batch 48/4600 loss 2.971652 loss_att 7.150908 loss_ctc 6.040161 loss_rnnt 1.554995 hw_loss 0.321882 lr 0.00029412 rank 7
2023-02-27 18:10:15,849 DEBUG TRAIN Batch 48/4700 loss 3.599255 loss_att 7.270013 loss_ctc 8.093620 loss_rnnt 2.124440 hw_loss 0.265152 lr 0.00029411 rank 6
2023-02-27 18:10:15,849 DEBUG TRAIN Batch 48/4700 loss 4.816090 loss_att 7.033290 loss_ctc 7.311067 loss_rnnt 3.879909 hw_loss 0.300142 lr 0.00029411 rank 3
2023-02-27 18:10:15,853 DEBUG TRAIN Batch 48/4700 loss 4.754667 loss_att 8.447806 loss_ctc 9.237610 loss_rnnt 3.296248 hw_loss 0.228872 lr 0.00029410 rank 7
2023-02-27 18:10:15,853 DEBUG TRAIN Batch 48/4700 loss 4.594070 loss_att 5.746984 loss_ctc 5.219895 loss_rnnt 4.043925 hw_loss 0.442722 lr 0.00029411 rank 0
2023-02-27 18:10:15,857 DEBUG TRAIN Batch 48/4700 loss 9.465067 loss_att 11.573709 loss_ctc 11.626434 loss_rnnt 8.611982 hw_loss 0.268450 lr 0.00029411 rank 1
2023-02-27 18:10:15,860 DEBUG TRAIN Batch 48/4700 loss 4.867884 loss_att 7.356188 loss_ctc 7.410285 loss_rnnt 3.845097 hw_loss 0.349011 lr 0.00029411 rank 5
2023-02-27 18:10:15,863 DEBUG TRAIN Batch 48/4700 loss 10.291401 loss_att 12.248045 loss_ctc 15.413455 loss_rnnt 9.118736 hw_loss 0.184492 lr 0.00029411 rank 4
2023-02-27 18:10:15,871 DEBUG TRAIN Batch 48/4700 loss 13.796885 loss_att 19.556705 loss_ctc 27.528725 loss_rnnt 10.637809 hw_loss 0.330375 lr 0.00029411 rank 2
2023-02-27 18:10:55,438 DEBUG TRAIN Batch 48/4800 loss 6.780302 loss_att 8.102503 loss_ctc 11.051567 loss_rnnt 5.831641 hw_loss 0.215098 lr 0.00029410 rank 5
2023-02-27 18:10:55,453 DEBUG TRAIN Batch 48/4800 loss 4.374375 loss_att 5.356339 loss_ctc 5.429658 loss_rnnt 3.820888 hw_loss 0.405731 lr 0.00029410 rank 6
2023-02-27 18:10:55,460 DEBUG TRAIN Batch 48/4800 loss 8.061193 loss_att 13.002731 loss_ctc 11.736572 loss_rnnt 6.493609 hw_loss 0.167295 lr 0.00029409 rank 7
2023-02-27 18:10:55,460 DEBUG TRAIN Batch 48/4800 loss 16.173172 loss_att 16.900686 loss_ctc 21.706774 loss_rnnt 15.186185 hw_loss 0.194382 lr 0.00029410 rank 3
2023-02-27 18:10:55,461 DEBUG TRAIN Batch 48/4800 loss 4.580803 loss_att 6.409918 loss_ctc 9.442090 loss_rnnt 3.435340 hw_loss 0.246503 lr 0.00029410 rank 0
2023-02-27 18:10:55,461 DEBUG TRAIN Batch 48/4800 loss 5.152035 loss_att 8.134244 loss_ctc 5.741949 loss_rnnt 4.405101 hw_loss 0.134694 lr 0.00029410 rank 4
2023-02-27 18:10:55,461 DEBUG TRAIN Batch 48/4800 loss 5.334714 loss_att 7.324340 loss_ctc 9.155149 loss_rnnt 4.257373 hw_loss 0.318797 lr 0.00029410 rank 1
2023-02-27 18:10:55,516 DEBUG TRAIN Batch 48/4800 loss 10.961635 loss_att 14.733500 loss_ctc 20.095154 loss_rnnt 8.819359 hw_loss 0.318937 lr 0.00029410 rank 2
2023-02-27 18:11:35,215 DEBUG TRAIN Batch 48/4900 loss 6.626434 loss_att 11.616941 loss_ctc 13.069704 loss_rnnt 4.585686 hw_loss 0.344146 lr 0.00029409 rank 0
2023-02-27 18:11:35,217 DEBUG TRAIN Batch 48/4900 loss 1.209581 loss_att 3.557665 loss_ctc 3.161392 loss_rnnt 0.354479 hw_loss 0.234832 lr 0.00029408 rank 6
2023-02-27 18:11:35,231 DEBUG TRAIN Batch 48/4900 loss 5.159292 loss_att 8.861616 loss_ctc 11.264086 loss_rnnt 3.429036 hw_loss 0.329661 lr 0.00029408 rank 3
2023-02-27 18:11:35,233 DEBUG TRAIN Batch 48/4900 loss 3.479818 loss_att 5.058847 loss_ctc 4.970927 loss_rnnt 2.793981 hw_loss 0.321029 lr 0.00029408 rank 2
2023-02-27 18:11:35,236 DEBUG TRAIN Batch 48/4900 loss 7.731225 loss_att 11.487405 loss_ctc 11.677699 loss_rnnt 6.314260 hw_loss 0.261623 lr 0.00029409 rank 1
2023-02-27 18:11:35,255 DEBUG TRAIN Batch 48/4900 loss 4.747409 loss_att 6.568580 loss_ctc 7.854919 loss_rnnt 3.927678 hw_loss 0.077178 lr 0.00029408 rank 4
2023-02-27 18:11:35,262 DEBUG TRAIN Batch 48/4900 loss 4.278398 loss_att 7.800826 loss_ctc 6.670198 loss_rnnt 3.166013 hw_loss 0.166861 lr 0.00029408 rank 7
2023-02-27 18:11:35,268 DEBUG TRAIN Batch 48/4900 loss 6.664634 loss_att 12.911966 loss_ctc 13.593079 loss_rnnt 4.367308 hw_loss 0.232626 lr 0.00029409 rank 5
2023-02-27 18:12:43,767 DEBUG TRAIN Batch 48/5000 loss 5.817150 loss_att 7.625607 loss_ctc 8.930500 loss_rnnt 4.962675 hw_loss 0.145632 lr 0.00029407 rank 3
2023-02-27 18:12:43,771 DEBUG TRAIN Batch 48/5000 loss 11.328650 loss_att 12.817236 loss_ctc 13.795163 loss_rnnt 10.576541 hw_loss 0.235357 lr 0.00029408 rank 5
2023-02-27 18:12:43,771 DEBUG TRAIN Batch 48/5000 loss 12.081306 loss_att 17.665506 loss_ctc 24.756607 loss_rnnt 9.160339 hw_loss 0.213913 lr 0.00029407 rank 4
2023-02-27 18:12:43,771 DEBUG TRAIN Batch 48/5000 loss 13.570652 loss_att 14.262678 loss_ctc 19.775938 loss_rnnt 12.479002 hw_loss 0.236016 lr 0.00029407 rank 6
2023-02-27 18:12:43,773 DEBUG TRAIN Batch 48/5000 loss 4.990581 loss_att 7.010149 loss_ctc 8.240015 loss_rnnt 4.032777 hw_loss 0.226185 lr 0.00029407 rank 0
2023-02-27 18:12:43,774 DEBUG TRAIN Batch 48/5000 loss 2.837116 loss_att 4.884303 loss_ctc 5.297935 loss_rnnt 1.965007 hw_loss 0.252304 lr 0.00029407 rank 7
2023-02-27 18:12:43,775 DEBUG TRAIN Batch 48/5000 loss 5.475044 loss_att 7.209161 loss_ctc 8.573486 loss_rnnt 4.576215 hw_loss 0.260401 lr 0.00029407 rank 2
2023-02-27 18:12:43,779 DEBUG TRAIN Batch 48/5000 loss 7.174947 loss_att 8.618407 loss_ctc 10.335066 loss_rnnt 6.327186 hw_loss 0.258226 lr 0.00029407 rank 1
2023-02-27 18:13:23,449 DEBUG TRAIN Batch 48/5100 loss 4.261979 loss_att 5.303481 loss_ctc 6.105516 loss_rnnt 3.733643 hw_loss 0.139184 lr 0.00029406 rank 6
2023-02-27 18:13:23,451 DEBUG TRAIN Batch 48/5100 loss 7.398724 loss_att 9.754730 loss_ctc 12.171655 loss_rnnt 6.191719 hw_loss 0.186399 lr 0.00029406 rank 2
2023-02-27 18:13:23,452 DEBUG TRAIN Batch 48/5100 loss 8.414123 loss_att 9.783437 loss_ctc 13.121959 loss_rnnt 7.359309 hw_loss 0.287324 lr 0.00029406 rank 3
2023-02-27 18:13:23,453 DEBUG TRAIN Batch 48/5100 loss 9.468288 loss_att 9.884828 loss_ctc 12.454952 loss_rnnt 8.859271 hw_loss 0.239038 lr 0.00029405 rank 7
2023-02-27 18:13:23,456 DEBUG TRAIN Batch 48/5100 loss 9.366106 loss_att 10.276581 loss_ctc 14.722364 loss_rnnt 8.250952 hw_loss 0.410420 lr 0.00029406 rank 5
2023-02-27 18:13:23,456 DEBUG TRAIN Batch 48/5100 loss 7.781660 loss_att 10.296685 loss_ctc 14.350726 loss_rnnt 6.265889 hw_loss 0.256667 lr 0.00029406 rank 1
2023-02-27 18:13:23,458 DEBUG TRAIN Batch 48/5100 loss 6.393450 loss_att 8.550671 loss_ctc 10.342270 loss_rnnt 5.250211 hw_loss 0.347411 lr 0.00029406 rank 4
2023-02-27 18:13:23,474 DEBUG TRAIN Batch 48/5100 loss 7.188871 loss_att 13.279320 loss_ctc 16.143715 loss_rnnt 4.757790 hw_loss 0.035647 lr 0.00029406 rank 0
2023-02-27 18:14:02,712 DEBUG TRAIN Batch 48/5200 loss 3.389222 loss_att 6.447059 loss_ctc 4.687152 loss_rnnt 2.433633 hw_loss 0.320558 lr 0.00029404 rank 2
2023-02-27 18:14:02,718 DEBUG TRAIN Batch 48/5200 loss 4.021158 loss_att 7.454707 loss_ctc 6.714059 loss_rnnt 2.784910 hw_loss 0.357158 lr 0.00029404 rank 7
2023-02-27 18:14:02,722 DEBUG TRAIN Batch 48/5200 loss 3.857000 loss_att 6.998148 loss_ctc 7.825866 loss_rnnt 2.590637 hw_loss 0.204284 lr 0.00029405 rank 5
2023-02-27 18:14:02,725 DEBUG TRAIN Batch 48/5200 loss 5.686682 loss_att 8.523507 loss_ctc 7.162433 loss_rnnt 4.838586 hw_loss 0.157432 lr 0.00029405 rank 0
2023-02-27 18:14:02,727 DEBUG TRAIN Batch 48/5200 loss 9.121510 loss_att 11.999846 loss_ctc 13.441848 loss_rnnt 7.881782 hw_loss 0.165029 lr 0.00029405 rank 3
2023-02-27 18:14:02,728 DEBUG TRAIN Batch 48/5200 loss 7.518215 loss_att 8.061060 loss_ctc 10.582880 loss_rnnt 6.809993 hw_loss 0.358185 lr 0.00029405 rank 4
2023-02-27 18:14:02,734 DEBUG TRAIN Batch 48/5200 loss 5.600514 loss_att 9.425303 loss_ctc 10.528472 loss_rnnt 4.013085 hw_loss 0.310143 lr 0.00029405 rank 1
2023-02-27 18:14:02,735 DEBUG TRAIN Batch 48/5200 loss 4.937024 loss_att 8.812108 loss_ctc 8.603297 loss_rnnt 3.614749 hw_loss 0.109541 lr 0.00029405 rank 6
2023-02-27 18:14:43,342 DEBUG TRAIN Batch 48/5300 loss 4.648941 loss_att 8.772549 loss_ctc 7.278310 loss_rnnt 3.414441 hw_loss 0.110991 lr 0.00029404 rank 1
2023-02-27 18:14:43,346 DEBUG TRAIN Batch 48/5300 loss 6.932426 loss_att 10.256568 loss_ctc 11.707290 loss_rnnt 5.560524 hw_loss 0.132047 lr 0.00029403 rank 7
2023-02-27 18:14:43,349 DEBUG TRAIN Batch 48/5300 loss 9.145058 loss_att 14.095932 loss_ctc 14.226469 loss_rnnt 7.384626 hw_loss 0.173880 lr 0.00029403 rank 6
2023-02-27 18:14:43,349 DEBUG TRAIN Batch 48/5300 loss 14.499233 loss_att 15.846962 loss_ctc 20.559608 loss_rnnt 13.259562 hw_loss 0.303892 lr 0.00029403 rank 3
2023-02-27 18:14:43,352 DEBUG TRAIN Batch 48/5300 loss 2.555786 loss_att 5.286849 loss_ctc 5.653385 loss_rnnt 1.378360 hw_loss 0.409124 lr 0.00029403 rank 0
2023-02-27 18:14:43,355 DEBUG TRAIN Batch 48/5300 loss 2.476592 loss_att 4.266238 loss_ctc 3.400596 loss_rnnt 1.929410 hw_loss 0.123848 lr 0.00029403 rank 4
2023-02-27 18:14:43,356 DEBUG TRAIN Batch 48/5300 loss 4.378901 loss_att 7.631349 loss_ctc 9.060054 loss_rnnt 2.944269 hw_loss 0.299980 lr 0.00029404 rank 5
2023-02-27 18:14:43,401 DEBUG TRAIN Batch 48/5300 loss 2.922353 loss_att 5.353802 loss_ctc 7.540792 loss_rnnt 1.675382 hw_loss 0.271667 lr 0.00029403 rank 2
2023-02-27 18:15:48,309 DEBUG TRAIN Batch 48/5400 loss 6.109994 loss_att 11.606142 loss_ctc 9.969176 loss_rnnt 4.476625 hw_loss 0.036716 lr 0.00029402 rank 2
2023-02-27 18:15:48,321 DEBUG TRAIN Batch 48/5400 loss 11.501226 loss_att 14.461523 loss_ctc 17.187122 loss_rnnt 10.039680 hw_loss 0.208814 lr 0.00029402 rank 3
2023-02-27 18:15:48,326 DEBUG TRAIN Batch 48/5400 loss 4.032212 loss_att 6.114658 loss_ctc 8.256272 loss_rnnt 2.987579 hw_loss 0.121756 lr 0.00029402 rank 6
2023-02-27 18:15:48,326 DEBUG TRAIN Batch 48/5400 loss 4.245338 loss_att 6.884682 loss_ctc 10.519027 loss_rnnt 2.847956 hw_loss 0.061915 lr 0.00029402 rank 4
2023-02-27 18:15:48,332 DEBUG TRAIN Batch 48/5400 loss 6.089568 loss_att 8.748634 loss_ctc 10.673296 loss_rnnt 4.775273 hw_loss 0.321221 lr 0.00029402 rank 7
2023-02-27 18:15:48,331 DEBUG TRAIN Batch 48/5400 loss 7.571480 loss_att 10.818413 loss_ctc 15.947442 loss_rnnt 5.673021 hw_loss 0.248021 lr 0.00029402 rank 5
2023-02-27 18:15:48,336 DEBUG TRAIN Batch 48/5400 loss 2.914526 loss_att 5.104278 loss_ctc 5.483370 loss_rnnt 2.046547 hw_loss 0.164090 lr 0.00029402 rank 0
2023-02-27 18:15:48,339 DEBUG TRAIN Batch 48/5400 loss 4.790524 loss_att 8.457436 loss_ctc 8.133328 loss_rnnt 3.444458 hw_loss 0.313081 lr 0.00029402 rank 1
2023-02-27 18:16:27,494 DEBUG TRAIN Batch 48/5500 loss 6.238730 loss_att 9.089452 loss_ctc 12.303356 loss_rnnt 4.797984 hw_loss 0.116222 lr 0.00029401 rank 0
2023-02-27 18:16:27,496 DEBUG TRAIN Batch 48/5500 loss 11.066644 loss_att 12.577106 loss_ctc 15.007550 loss_rnnt 10.122109 hw_loss 0.219350 lr 0.00029401 rank 4
2023-02-27 18:16:27,509 DEBUG TRAIN Batch 48/5500 loss 8.604951 loss_att 13.070889 loss_ctc 12.951929 loss_rnnt 7.005015 hw_loss 0.238409 lr 0.00029401 rank 6
2023-02-27 18:16:27,514 DEBUG TRAIN Batch 48/5500 loss 10.404656 loss_att 13.628958 loss_ctc 23.120544 loss_rnnt 7.992673 hw_loss 0.134382 lr 0.00029401 rank 1
2023-02-27 18:16:27,513 DEBUG TRAIN Batch 48/5500 loss 5.185896 loss_att 6.462109 loss_ctc 9.024738 loss_rnnt 4.292549 hw_loss 0.236737 lr 0.00029401 rank 5
2023-02-27 18:16:27,514 DEBUG TRAIN Batch 48/5500 loss 7.284414 loss_att 8.894234 loss_ctc 7.733224 loss_rnnt 6.721430 hw_loss 0.339708 lr 0.00029401 rank 2
2023-02-27 18:16:27,514 DEBUG TRAIN Batch 48/5500 loss 8.572839 loss_att 10.821978 loss_ctc 12.801625 loss_rnnt 7.428589 hw_loss 0.244844 lr 0.00029400 rank 7
2023-02-27 18:16:27,516 DEBUG TRAIN Batch 48/5500 loss 6.921144 loss_att 8.798104 loss_ctc 14.838227 loss_rnnt 5.322918 hw_loss 0.313541 lr 0.00029401 rank 3
2023-02-27 18:17:07,060 DEBUG TRAIN Batch 48/5600 loss 2.506454 loss_att 4.675292 loss_ctc 3.749665 loss_rnnt 1.760932 hw_loss 0.273735 lr 0.00029400 rank 4
2023-02-27 18:17:07,077 DEBUG TRAIN Batch 48/5600 loss 4.244160 loss_att 7.989738 loss_ctc 8.920324 loss_rnnt 2.711886 hw_loss 0.299380 lr 0.00029399 rank 6
2023-02-27 18:17:07,079 DEBUG TRAIN Batch 48/5600 loss 6.383030 loss_att 9.159220 loss_ctc 11.736880 loss_rnnt 4.987254 hw_loss 0.237547 lr 0.00029400 rank 0
2023-02-27 18:17:07,080 DEBUG TRAIN Batch 48/5600 loss 5.221021 loss_att 8.597500 loss_ctc 7.925365 loss_rnnt 4.115516 hw_loss 0.130556 lr 0.00029400 rank 3
2023-02-27 18:17:07,084 DEBUG TRAIN Batch 48/5600 loss 3.802225 loss_att 4.883334 loss_ctc 5.796416 loss_rnnt 3.182394 hw_loss 0.258220 lr 0.00029399 rank 7
2023-02-27 18:17:07,086 DEBUG TRAIN Batch 48/5600 loss 3.122905 loss_att 5.485054 loss_ctc 5.860008 loss_rnnt 2.163064 hw_loss 0.229620 lr 0.00029400 rank 5
2023-02-27 18:17:07,105 DEBUG TRAIN Batch 48/5600 loss 4.687106 loss_att 8.928623 loss_ctc 10.481793 loss_rnnt 2.976863 hw_loss 0.167464 lr 0.00029400 rank 1
2023-02-27 18:17:07,113 DEBUG TRAIN Batch 48/5600 loss 5.821695 loss_att 10.311810 loss_ctc 13.249462 loss_rnnt 3.821365 hw_loss 0.209884 lr 0.00029399 rank 2
2023-02-27 18:18:13,220 DEBUG TRAIN Batch 48/5700 loss 5.272919 loss_att 8.248336 loss_ctc 8.481289 loss_rnnt 4.102431 hw_loss 0.276790 lr 0.00029398 rank 4
2023-02-27 18:18:13,221 DEBUG TRAIN Batch 48/5700 loss 3.627833 loss_att 6.667412 loss_ctc 9.976086 loss_rnnt 2.079226 hw_loss 0.176732 lr 0.00029398 rank 6
2023-02-27 18:18:13,223 DEBUG TRAIN Batch 48/5700 loss 11.190730 loss_att 11.667372 loss_ctc 16.214661 loss_rnnt 10.321002 hw_loss 0.196019 lr 0.00029399 rank 5
2023-02-27 18:18:13,224 DEBUG TRAIN Batch 48/5700 loss 4.055870 loss_att 8.801831 loss_ctc 11.969005 loss_rnnt 2.008384 hw_loss 0.081016 lr 0.00029398 rank 3
2023-02-27 18:18:13,226 DEBUG TRAIN Batch 48/5700 loss 4.473415 loss_att 9.312512 loss_ctc 10.088430 loss_rnnt 2.593026 hw_loss 0.307315 lr 0.00029398 rank 0
2023-02-27 18:18:13,228 DEBUG TRAIN Batch 48/5700 loss 8.127615 loss_att 13.027090 loss_ctc 16.648886 loss_rnnt 5.823895 hw_loss 0.351856 lr 0.00029399 rank 1
2023-02-27 18:18:13,232 DEBUG TRAIN Batch 48/5700 loss 7.661941 loss_att 10.578424 loss_ctc 14.932314 loss_rnnt 6.041317 hw_loss 0.127394 lr 0.00029398 rank 7
2023-02-27 18:18:13,232 DEBUG TRAIN Batch 48/5700 loss 7.446748 loss_att 8.765138 loss_ctc 11.629582 loss_rnnt 6.459152 hw_loss 0.311638 lr 0.00029398 rank 2
2023-02-27 18:18:52,669 DEBUG TRAIN Batch 48/5800 loss 6.704291 loss_att 6.952915 loss_ctc 9.021048 loss_rnnt 6.135077 hw_loss 0.394855 lr 0.00029396 rank 7
2023-02-27 18:18:52,671 DEBUG TRAIN Batch 48/5800 loss 2.131886 loss_att 4.312925 loss_ctc 6.982498 loss_rnnt 1.000909 hw_loss 0.090038 lr 0.00029397 rank 6
2023-02-27 18:18:52,675 DEBUG TRAIN Batch 48/5800 loss 5.251728 loss_att 6.788258 loss_ctc 7.049940 loss_rnnt 4.600268 hw_loss 0.195735 lr 0.00029397 rank 4
2023-02-27 18:18:52,677 DEBUG TRAIN Batch 48/5800 loss 3.763409 loss_att 6.302700 loss_ctc 4.288522 loss_rnnt 3.121961 hw_loss 0.119202 lr 0.00029397 rank 0
2023-02-27 18:18:52,679 DEBUG TRAIN Batch 48/5800 loss 3.878654 loss_att 5.822721 loss_ctc 5.112897 loss_rnnt 3.199143 hw_loss 0.236498 lr 0.00029397 rank 1
2023-02-27 18:18:52,679 DEBUG TRAIN Batch 48/5800 loss 7.846232 loss_att 8.078033 loss_ctc 12.101737 loss_rnnt 7.060802 hw_loss 0.321880 lr 0.00029397 rank 2
2023-02-27 18:18:52,680 DEBUG TRAIN Batch 48/5800 loss 5.891915 loss_att 8.444536 loss_ctc 11.745447 loss_rnnt 4.547099 hw_loss 0.100914 lr 0.00029397 rank 3
2023-02-27 18:18:52,725 DEBUG TRAIN Batch 48/5800 loss 2.013982 loss_att 5.179121 loss_ctc 4.392471 loss_rnnt 0.899827 hw_loss 0.307492 lr 0.00029397 rank 5
2023-02-27 18:19:31,607 DEBUG TRAIN Batch 48/5900 loss 9.723940 loss_att 11.436360 loss_ctc 17.263641 loss_rnnt 8.265525 hw_loss 0.207447 lr 0.00029396 rank 6
2023-02-27 18:19:31,607 DEBUG TRAIN Batch 48/5900 loss 3.031440 loss_att 8.187317 loss_ctc 8.386071 loss_rnnt 1.190181 hw_loss 0.180248 lr 0.00029396 rank 2
2023-02-27 18:19:31,613 DEBUG TRAIN Batch 48/5900 loss 12.671335 loss_att 18.161682 loss_ctc 21.932877 loss_rnnt 10.182930 hw_loss 0.291492 lr 0.00029396 rank 5
2023-02-27 18:19:31,613 DEBUG TRAIN Batch 48/5900 loss 4.110730 loss_att 10.849264 loss_ctc 8.279331 loss_rnnt 2.039494 hw_loss 0.314467 lr 0.00029396 rank 4
2023-02-27 18:19:31,613 DEBUG TRAIN Batch 48/5900 loss 6.619784 loss_att 9.144305 loss_ctc 14.576259 loss_rnnt 4.892894 hw_loss 0.302105 lr 0.00029395 rank 7
2023-02-27 18:19:31,613 DEBUG TRAIN Batch 48/5900 loss 2.299582 loss_att 5.256716 loss_ctc 4.606044 loss_rnnt 1.284451 hw_loss 0.217831 lr 0.00029396 rank 3
2023-02-27 18:19:31,613 DEBUG TRAIN Batch 48/5900 loss 7.042552 loss_att 12.033682 loss_ctc 13.891743 loss_rnnt 4.917452 hw_loss 0.400589 lr 0.00029396 rank 0
2023-02-27 18:19:31,618 DEBUG TRAIN Batch 48/5900 loss 9.750604 loss_att 12.467777 loss_ctc 13.812782 loss_rnnt 8.546471 hw_loss 0.223266 lr 0.00029396 rank 1
2023-02-27 18:20:12,063 DEBUG TRAIN Batch 48/6000 loss 10.465930 loss_att 15.331532 loss_ctc 14.627586 loss_rnnt 8.743361 hw_loss 0.364801 lr 0.00029394 rank 3
2023-02-27 18:20:12,067 DEBUG TRAIN Batch 48/6000 loss 3.634791 loss_att 6.681844 loss_ctc 7.088150 loss_rnnt 2.465392 hw_loss 0.186639 lr 0.00029394 rank 6
2023-02-27 18:20:12,076 DEBUG TRAIN Batch 48/6000 loss 1.639928 loss_att 4.448668 loss_ctc 2.464186 loss_rnnt 0.887446 hw_loss 0.151562 lr 0.00029394 rank 4
2023-02-27 18:20:12,077 DEBUG TRAIN Batch 48/6000 loss 4.273668 loss_att 8.067560 loss_ctc 6.401737 loss_rnnt 3.105482 hw_loss 0.235620 lr 0.00029395 rank 5
2023-02-27 18:20:12,078 DEBUG TRAIN Batch 48/6000 loss 9.360127 loss_att 12.912637 loss_ctc 16.176949 loss_rnnt 7.655340 hw_loss 0.160083 lr 0.00029395 rank 0
2023-02-27 18:20:12,080 DEBUG TRAIN Batch 48/6000 loss 9.725824 loss_att 12.408773 loss_ctc 15.319160 loss_rnnt 8.291010 hw_loss 0.285836 lr 0.00029395 rank 1
2023-02-27 18:20:12,084 DEBUG TRAIN Batch 48/6000 loss 6.964011 loss_att 9.564139 loss_ctc 8.390003 loss_rnnt 6.188713 hw_loss 0.122139 lr 0.00029394 rank 2
2023-02-27 18:20:12,127 DEBUG TRAIN Batch 48/6000 loss 5.417045 loss_att 7.501868 loss_ctc 12.606466 loss_rnnt 3.822307 hw_loss 0.410968 lr 0.00029394 rank 7
2023-02-27 18:21:17,991 DEBUG TRAIN Batch 48/6100 loss 9.127795 loss_att 12.188389 loss_ctc 12.653412 loss_rnnt 7.945263 hw_loss 0.188121 lr 0.00029393 rank 6
2023-02-27 18:21:17,992 DEBUG TRAIN Batch 48/6100 loss 4.918025 loss_att 9.312187 loss_ctc 10.707093 loss_rnnt 3.122292 hw_loss 0.271921 lr 0.00029394 rank 1
2023-02-27 18:21:17,994 DEBUG TRAIN Batch 48/6100 loss 3.405241 loss_att 6.938706 loss_ctc 5.826062 loss_rnnt 2.229193 hw_loss 0.274834 lr 0.00029393 rank 7
2023-02-27 18:21:17,994 DEBUG TRAIN Batch 48/6100 loss 4.625005 loss_att 7.779293 loss_ctc 7.362886 loss_rnnt 3.406381 hw_loss 0.417593 lr 0.00029394 rank 5
2023-02-27 18:21:17,996 DEBUG TRAIN Batch 48/6100 loss 5.466894 loss_att 6.581602 loss_ctc 8.117086 loss_rnnt 4.766912 hw_loss 0.231901 lr 0.00029393 rank 3
2023-02-27 18:21:17,999 DEBUG TRAIN Batch 48/6100 loss 8.396693 loss_att 11.880705 loss_ctc 15.232592 loss_rnnt 6.722795 hw_loss 0.123081 lr 0.00029393 rank 0
2023-02-27 18:21:18,002 DEBUG TRAIN Batch 48/6100 loss 6.298706 loss_att 8.805717 loss_ctc 8.382867 loss_rnnt 5.390499 hw_loss 0.241718 lr 0.00029393 rank 4
2023-02-27 18:21:18,002 DEBUG TRAIN Batch 48/6100 loss 4.407757 loss_att 5.417867 loss_ctc 4.815866 loss_rnnt 3.991696 hw_loss 0.299294 lr 0.00029393 rank 2
2023-02-27 18:21:57,364 DEBUG TRAIN Batch 48/6200 loss 5.389350 loss_att 8.852781 loss_ctc 8.260895 loss_rnnt 4.214383 hw_loss 0.186389 lr 0.00029392 rank 2
2023-02-27 18:21:57,364 DEBUG TRAIN Batch 48/6200 loss 2.813760 loss_att 6.139328 loss_ctc 5.487747 loss_rnnt 1.737289 hw_loss 0.102798 lr 0.00029392 rank 6
2023-02-27 18:21:57,378 DEBUG TRAIN Batch 48/6200 loss 5.601606 loss_att 9.912196 loss_ctc 7.771795 loss_rnnt 4.390730 hw_loss 0.111375 lr 0.00029392 rank 4
2023-02-27 18:21:57,380 DEBUG TRAIN Batch 48/6200 loss 3.626459 loss_att 5.674915 loss_ctc 6.515768 loss_rnnt 2.806014 hw_loss 0.047836 lr 0.00029392 rank 0
2023-02-27 18:21:57,388 DEBUG TRAIN Batch 48/6200 loss 8.090199 loss_att 10.466488 loss_ctc 11.292141 loss_rnnt 7.050386 hw_loss 0.258056 lr 0.00029392 rank 5
2023-02-27 18:21:57,388 DEBUG TRAIN Batch 48/6200 loss 4.701231 loss_att 6.905145 loss_ctc 7.555106 loss_rnnt 3.735796 hw_loss 0.270251 lr 0.00029392 rank 3
2023-02-27 18:21:57,437 DEBUG TRAIN Batch 48/6200 loss 7.249995 loss_att 12.657086 loss_ctc 16.515574 loss_rnnt 4.817165 hw_loss 0.217503 lr 0.00029391 rank 7
2023-02-27 18:21:57,463 DEBUG TRAIN Batch 48/6200 loss 5.852946 loss_att 9.281535 loss_ctc 8.933669 loss_rnnt 4.570276 hw_loss 0.349103 lr 0.00029392 rank 1
2023-02-27 18:22:36,921 DEBUG TRAIN Batch 48/6300 loss 8.714334 loss_att 10.767036 loss_ctc 13.474312 loss_rnnt 7.511383 hw_loss 0.295775 lr 0.00029391 rank 0
2023-02-27 18:22:36,921 DEBUG TRAIN Batch 48/6300 loss 5.065260 loss_att 6.337083 loss_ctc 6.304550 loss_rnnt 4.599896 hw_loss 0.085801 lr 0.00029390 rank 7
2023-02-27 18:22:36,922 DEBUG TRAIN Batch 48/6300 loss 10.431159 loss_att 11.118554 loss_ctc 16.158249 loss_rnnt 9.447289 hw_loss 0.155212 lr 0.00029391 rank 2
2023-02-27 18:22:36,923 DEBUG TRAIN Batch 48/6300 loss 2.342196 loss_att 4.768839 loss_ctc 3.252686 loss_rnnt 1.640207 hw_loss 0.178614 lr 0.00029391 rank 6
2023-02-27 18:22:36,924 DEBUG TRAIN Batch 48/6300 loss 7.327824 loss_att 8.308577 loss_ctc 10.716664 loss_rnnt 6.510988 hw_loss 0.316574 lr 0.00029391 rank 3
2023-02-27 18:22:36,927 DEBUG TRAIN Batch 48/6300 loss 6.705870 loss_att 8.937433 loss_ctc 9.440664 loss_rnnt 5.799829 hw_loss 0.178292 lr 0.00029391 rank 4
2023-02-27 18:22:36,927 DEBUG TRAIN Batch 48/6300 loss 7.574894 loss_att 9.710023 loss_ctc 13.040870 loss_rnnt 6.303552 hw_loss 0.216600 lr 0.00029391 rank 5
2023-02-27 18:22:36,933 DEBUG TRAIN Batch 48/6300 loss 4.142337 loss_att 7.500504 loss_ctc 7.797456 loss_rnnt 2.841135 hw_loss 0.266660 lr 0.00029391 rank 1
2023-02-27 18:23:43,874 DEBUG TRAIN Batch 48/6400 loss 4.841511 loss_att 9.771791 loss_ctc 11.448385 loss_rnnt 2.880460 hw_loss 0.176398 lr 0.00029390 rank 0
2023-02-27 18:23:43,877 DEBUG TRAIN Batch 48/6400 loss 7.131370 loss_att 9.285417 loss_ctc 10.442940 loss_rnnt 6.222291 hw_loss 0.068863 lr 0.00029390 rank 1
2023-02-27 18:23:43,879 DEBUG TRAIN Batch 48/6400 loss 5.385924 loss_att 7.096019 loss_ctc 7.201691 loss_rnnt 4.677327 hw_loss 0.233391 lr 0.00029389 rank 6
2023-02-27 18:23:43,879 DEBUG TRAIN Batch 48/6400 loss 14.525627 loss_att 16.343954 loss_ctc 21.186008 loss_rnnt 13.107954 hw_loss 0.311170 lr 0.00029389 rank 2
2023-02-27 18:23:43,882 DEBUG TRAIN Batch 48/6400 loss 10.658619 loss_att 12.696411 loss_ctc 15.069715 loss_rnnt 9.552726 hw_loss 0.206604 lr 0.00029389 rank 4
2023-02-27 18:23:43,884 DEBUG TRAIN Batch 48/6400 loss 5.920622 loss_att 7.485282 loss_ctc 10.982652 loss_rnnt 4.805744 hw_loss 0.238141 lr 0.00029389 rank 7
2023-02-27 18:23:43,901 DEBUG TRAIN Batch 48/6400 loss 10.039716 loss_att 13.591534 loss_ctc 19.779980 loss_rnnt 7.987796 hw_loss 0.080353 lr 0.00029389 rank 3
2023-02-27 18:23:43,919 DEBUG TRAIN Batch 48/6400 loss 5.476925 loss_att 6.870339 loss_ctc 5.733219 loss_rnnt 5.021256 hw_loss 0.267775 lr 0.00029390 rank 5
2023-02-27 18:24:23,217 DEBUG TRAIN Batch 48/6500 loss 2.855823 loss_att 5.476086 loss_ctc 5.565362 loss_rnnt 1.790804 hw_loss 0.336926 lr 0.00029388 rank 6
2023-02-27 18:24:23,220 DEBUG TRAIN Batch 48/6500 loss 4.414506 loss_att 6.158882 loss_ctc 7.114757 loss_rnnt 3.533425 hw_loss 0.322822 lr 0.00029388 rank 2
2023-02-27 18:24:23,221 DEBUG TRAIN Batch 48/6500 loss 5.503912 loss_att 9.279982 loss_ctc 9.530712 loss_rnnt 4.065561 hw_loss 0.274183 lr 0.00029388 rank 5
2023-02-27 18:24:23,221 DEBUG TRAIN Batch 48/6500 loss 2.887587 loss_att 5.895301 loss_ctc 4.177545 loss_rnnt 2.024361 hw_loss 0.168168 lr 0.00029388 rank 0
2023-02-27 18:24:23,221 DEBUG TRAIN Batch 48/6500 loss 9.016363 loss_att 13.894426 loss_ctc 14.920990 loss_rnnt 7.041275 hw_loss 0.397858 lr 0.00029388 rank 3
2023-02-27 18:24:23,222 DEBUG TRAIN Batch 48/6500 loss 3.019175 loss_att 5.923517 loss_ctc 7.280890 loss_rnnt 1.803086 hw_loss 0.125610 lr 0.00029388 rank 4
2023-02-27 18:24:23,222 DEBUG TRAIN Batch 48/6500 loss 4.327152 loss_att 6.984466 loss_ctc 5.600647 loss_rnnt 3.477739 hw_loss 0.277783 lr 0.00029388 rank 7
2023-02-27 18:24:23,272 DEBUG TRAIN Batch 48/6500 loss 7.004449 loss_att 8.739899 loss_ctc 13.107516 loss_rnnt 5.679162 hw_loss 0.308355 lr 0.00029388 rank 1
2023-02-27 18:25:02,471 DEBUG TRAIN Batch 48/6600 loss 3.621019 loss_att 7.788083 loss_ctc 6.097519 loss_rnnt 2.265155 hw_loss 0.360471 lr 0.00029386 rank 7
2023-02-27 18:25:02,480 DEBUG TRAIN Batch 48/6600 loss 8.106051 loss_att 12.141174 loss_ctc 15.779840 loss_rnnt 6.094241 hw_loss 0.340528 lr 0.00029387 rank 0
2023-02-27 18:25:02,480 DEBUG TRAIN Batch 48/6600 loss 3.390557 loss_att 5.926169 loss_ctc 8.109713 loss_rnnt 2.253740 hw_loss 0.000887 lr 0.00029387 rank 4
2023-02-27 18:25:02,480 DEBUG TRAIN Batch 48/6600 loss 7.399471 loss_att 9.749187 loss_ctc 11.919518 loss_rnnt 6.235346 hw_loss 0.171579 lr 0.00029387 rank 3
2023-02-27 18:25:02,482 DEBUG TRAIN Batch 48/6600 loss 11.542468 loss_att 13.430993 loss_ctc 20.118511 loss_rnnt 9.874886 hw_loss 0.274509 lr 0.00029387 rank 6
2023-02-27 18:25:02,483 DEBUG TRAIN Batch 48/6600 loss 1.744742 loss_att 3.918138 loss_ctc 2.868222 loss_rnnt 1.018926 hw_loss 0.265011 lr 0.00029387 rank 2
2023-02-27 18:25:02,487 DEBUG TRAIN Batch 48/6600 loss 7.241444 loss_att 9.011619 loss_ctc 14.788569 loss_rnnt 5.768996 hw_loss 0.210244 lr 0.00029387 rank 5
2023-02-27 18:25:02,488 DEBUG TRAIN Batch 48/6600 loss 3.873499 loss_att 7.347706 loss_ctc 6.285123 loss_rnnt 2.774622 hw_loss 0.154662 lr 0.00029387 rank 1
2023-02-27 18:25:42,708 DEBUG TRAIN Batch 48/6700 loss 9.343114 loss_att 13.614285 loss_ctc 20.291069 loss_rnnt 7.012894 hw_loss 0.030483 lr 0.00029386 rank 1
2023-02-27 18:25:42,710 DEBUG TRAIN Batch 48/6700 loss 4.138450 loss_att 5.404724 loss_ctc 6.792777 loss_rnnt 3.431052 hw_loss 0.187937 lr 0.00029386 rank 5
2023-02-27 18:25:42,719 DEBUG TRAIN Batch 48/6700 loss 11.144560 loss_att 17.234747 loss_ctc 20.824198 loss_rnnt 8.589364 hw_loss 0.087261 lr 0.00029385 rank 2
2023-02-27 18:25:42,719 DEBUG TRAIN Batch 48/6700 loss 5.556884 loss_att 6.607471 loss_ctc 8.154330 loss_rnnt 4.876776 hw_loss 0.231870 lr 0.00029386 rank 3
2023-02-27 18:25:42,719 DEBUG TRAIN Batch 48/6700 loss 4.605470 loss_att 6.649390 loss_ctc 10.786654 loss_rnnt 3.306706 hw_loss 0.123415 lr 0.00029386 rank 4
2023-02-27 18:25:42,719 DEBUG TRAIN Batch 48/6700 loss 4.821356 loss_att 6.990909 loss_ctc 5.184394 loss_rnnt 4.172740 hw_loss 0.311813 lr 0.00029385 rank 6
2023-02-27 18:25:42,723 DEBUG TRAIN Batch 48/6700 loss 1.812010 loss_att 2.677294 loss_ctc 1.675657 loss_rnnt 1.481668 hw_loss 0.328999 lr 0.00029386 rank 0
2023-02-27 18:25:42,774 DEBUG TRAIN Batch 48/6700 loss 4.563164 loss_att 8.138230 loss_ctc 10.221390 loss_rnnt 3.021272 hw_loss 0.135841 lr 0.00029385 rank 7
2023-02-27 18:26:47,786 DEBUG TRAIN Batch 48/6800 loss 6.554480 loss_att 11.334194 loss_ctc 9.220152 loss_rnnt 5.176248 hw_loss 0.125374 lr 0.00029384 rank 6
2023-02-27 18:26:47,788 DEBUG TRAIN Batch 48/6800 loss 6.480058 loss_att 11.189323 loss_ctc 12.759729 loss_rnnt 4.610876 hw_loss 0.168824 lr 0.00029385 rank 5
2023-02-27 18:26:47,789 DEBUG TRAIN Batch 48/6800 loss 8.641322 loss_att 11.125435 loss_ctc 12.907780 loss_rnnt 7.419297 hw_loss 0.293141 lr 0.00029384 rank 0
2023-02-27 18:26:47,790 DEBUG TRAIN Batch 48/6800 loss 6.171444 loss_att 10.311691 loss_ctc 10.283768 loss_rnnt 4.700509 hw_loss 0.177329 lr 0.00029384 rank 4
2023-02-27 18:26:47,790 DEBUG TRAIN Batch 48/6800 loss 3.971713 loss_att 6.917768 loss_ctc 5.171070 loss_rnnt 3.106426 hw_loss 0.217804 lr 0.00029384 rank 7
2023-02-27 18:26:47,790 DEBUG TRAIN Batch 48/6800 loss 9.259597 loss_att 12.610548 loss_ctc 17.472950 loss_rnnt 7.313537 hw_loss 0.338920 lr 0.00029384 rank 3
2023-02-27 18:26:47,794 DEBUG TRAIN Batch 48/6800 loss 9.104111 loss_att 10.060394 loss_ctc 10.610700 loss_rnnt 8.521282 hw_loss 0.357550 lr 0.00029385 rank 1
2023-02-27 18:26:47,795 DEBUG TRAIN Batch 48/6800 loss 7.377140 loss_att 12.916417 loss_ctc 14.988379 loss_rnnt 5.233026 hw_loss 0.040176 lr 0.00029384 rank 2
2023-02-27 18:27:27,541 DEBUG TRAIN Batch 48/6900 loss 4.966627 loss_att 7.083611 loss_ctc 9.907624 loss_rnnt 3.759577 hw_loss 0.234101 lr 0.00029383 rank 3
2023-02-27 18:27:27,543 DEBUG TRAIN Batch 48/6900 loss 7.470774 loss_att 12.150730 loss_ctc 12.030296 loss_rnnt 5.822423 hw_loss 0.195793 lr 0.00029383 rank 6
2023-02-27 18:27:27,544 DEBUG TRAIN Batch 48/6900 loss 7.354355 loss_att 10.298829 loss_ctc 15.703800 loss_rnnt 5.586218 hw_loss 0.123718 lr 0.00029383 rank 2
2023-02-27 18:27:27,545 DEBUG TRAIN Batch 48/6900 loss 5.318054 loss_att 8.919821 loss_ctc 8.118214 loss_rnnt 4.154777 hw_loss 0.130443 lr 0.00029383 rank 4
2023-02-27 18:27:27,545 DEBUG TRAIN Batch 48/6900 loss 4.727451 loss_att 8.147649 loss_ctc 6.458426 loss_rnnt 3.771407 hw_loss 0.077265 lr 0.00029382 rank 7
2023-02-27 18:27:27,547 DEBUG TRAIN Batch 48/6900 loss 4.172713 loss_att 6.620559 loss_ctc 6.511832 loss_rnnt 3.288708 hw_loss 0.154787 lr 0.00029383 rank 0
2023-02-27 18:27:27,547 DEBUG TRAIN Batch 48/6900 loss 6.165323 loss_att 9.021226 loss_ctc 11.911489 loss_rnnt 4.655406 hw_loss 0.323589 lr 0.00029383 rank 5
2023-02-27 18:27:27,549 DEBUG TRAIN Batch 48/6900 loss 13.590881 loss_att 16.777220 loss_ctc 20.808859 loss_rnnt 11.921558 hw_loss 0.130610 lr 0.00029383 rank 1
2023-02-27 18:28:07,347 DEBUG TRAIN Batch 48/7000 loss 4.407849 loss_att 6.303642 loss_ctc 6.870154 loss_rnnt 3.543187 hw_loss 0.294744 lr 0.00029382 rank 3
2023-02-27 18:28:07,355 DEBUG TRAIN Batch 48/7000 loss 6.281511 loss_att 7.547365 loss_ctc 9.887643 loss_rnnt 5.489594 hw_loss 0.108614 lr 0.00029382 rank 6
2023-02-27 18:28:07,357 DEBUG TRAIN Batch 48/7000 loss 10.492103 loss_att 12.270181 loss_ctc 14.699516 loss_rnnt 9.421534 hw_loss 0.288682 lr 0.00029382 rank 0
2023-02-27 18:28:07,360 DEBUG TRAIN Batch 48/7000 loss 5.392594 loss_att 9.241953 loss_ctc 11.054664 loss_rnnt 3.742877 hw_loss 0.234192 lr 0.00029381 rank 7
2023-02-27 18:28:07,361 DEBUG TRAIN Batch 48/7000 loss 5.187654 loss_att 6.673204 loss_ctc 10.105658 loss_rnnt 4.007210 hw_loss 0.426749 lr 0.00029382 rank 5
2023-02-27 18:28:07,361 DEBUG TRAIN Batch 48/7000 loss 3.218800 loss_att 5.369459 loss_ctc 4.904593 loss_rnnt 2.443501 hw_loss 0.225740 lr 0.00029382 rank 4
2023-02-27 18:28:07,362 DEBUG TRAIN Batch 48/7000 loss 4.423283 loss_att 7.227878 loss_ctc 6.547708 loss_rnnt 3.422200 hw_loss 0.294199 lr 0.00029382 rank 2
2023-02-27 18:28:07,365 DEBUG TRAIN Batch 48/7000 loss 10.253573 loss_att 13.049236 loss_ctc 20.680426 loss_rnnt 8.147913 hw_loss 0.293029 lr 0.00029382 rank 1
2023-02-27 18:29:12,623 DEBUG TRAIN Batch 48/7100 loss 8.740183 loss_att 13.843089 loss_ctc 15.524631 loss_rnnt 6.701393 hw_loss 0.213029 lr 0.00029380 rank 6
2023-02-27 18:29:12,624 DEBUG TRAIN Batch 48/7100 loss 6.406737 loss_att 8.896247 loss_ctc 10.318154 loss_rnnt 5.294846 hw_loss 0.173375 lr 0.00029380 rank 3
2023-02-27 18:29:12,628 DEBUG TRAIN Batch 48/7100 loss 12.656940 loss_att 15.268150 loss_ctc 19.990637 loss_rnnt 11.116313 hw_loss 0.076050 lr 0.00029381 rank 5
2023-02-27 18:29:12,629 DEBUG TRAIN Batch 48/7100 loss 2.359404 loss_att 5.096016 loss_ctc 4.364607 loss_rnnt 1.442692 hw_loss 0.191303 lr 0.00029381 rank 0
2023-02-27 18:29:12,630 DEBUG TRAIN Batch 48/7100 loss 9.523682 loss_att 15.539471 loss_ctc 18.242077 loss_rnnt 6.981931 hw_loss 0.330262 lr 0.00029381 rank 4
2023-02-27 18:29:12,636 DEBUG TRAIN Batch 48/7100 loss 7.461809 loss_att 9.528461 loss_ctc 10.981060 loss_rnnt 6.378869 hw_loss 0.375705 lr 0.00029381 rank 1
2023-02-27 18:29:12,661 DEBUG TRAIN Batch 48/7100 loss 6.961330 loss_att 7.965879 loss_ctc 11.200132 loss_rnnt 5.967637 hw_loss 0.426768 lr 0.00029380 rank 7
2023-02-27 18:29:12,679 DEBUG TRAIN Batch 48/7100 loss 4.423547 loss_att 8.420251 loss_ctc 8.386791 loss_rnnt 3.061619 hw_loss 0.064041 lr 0.00029380 rank 2
2023-02-27 18:29:52,376 DEBUG TRAIN Batch 48/7200 loss 7.116207 loss_att 10.844566 loss_ctc 15.533585 loss_rnnt 5.076526 hw_loss 0.321921 lr 0.00029379 rank 3
2023-02-27 18:29:52,378 DEBUG TRAIN Batch 48/7200 loss 5.341174 loss_att 9.261172 loss_ctc 11.077044 loss_rnnt 3.675664 hw_loss 0.218863 lr 0.00029379 rank 0
2023-02-27 18:29:52,380 DEBUG TRAIN Batch 48/7200 loss 9.357664 loss_att 12.594318 loss_ctc 19.569235 loss_rnnt 7.206621 hw_loss 0.266565 lr 0.00029379 rank 4
2023-02-27 18:29:52,380 DEBUG TRAIN Batch 48/7200 loss 3.241563 loss_att 5.493302 loss_ctc 7.146013 loss_rnnt 2.159020 hw_loss 0.209254 lr 0.00029380 rank 1
2023-02-27 18:29:52,381 DEBUG TRAIN Batch 48/7200 loss 3.368731 loss_att 7.349679 loss_ctc 5.469162 loss_rnnt 2.139545 hw_loss 0.286761 lr 0.00029379 rank 6
2023-02-27 18:29:52,384 DEBUG TRAIN Batch 48/7200 loss 4.033473 loss_att 6.491154 loss_ctc 8.959097 loss_rnnt 2.832512 hw_loss 0.098764 lr 0.00029379 rank 2
2023-02-27 18:29:52,384 DEBUG TRAIN Batch 48/7200 loss 3.966891 loss_att 6.563464 loss_ctc 5.126631 loss_rnnt 3.167508 hw_loss 0.235194 lr 0.00029380 rank 5
2023-02-27 18:29:52,386 DEBUG TRAIN Batch 48/7200 loss 6.061331 loss_att 7.846796 loss_ctc 8.424396 loss_rnnt 5.271996 hw_loss 0.219687 lr 0.00029379 rank 7
2023-02-27 18:30:31,534 DEBUG TRAIN Batch 48/7300 loss 15.261133 loss_att 15.164741 loss_ctc 25.831303 loss_rnnt 13.793987 hw_loss 0.144500 lr 0.00029378 rank 6
2023-02-27 18:30:31,537 DEBUG TRAIN Batch 48/7300 loss 5.838547 loss_att 10.716721 loss_ctc 15.931860 loss_rnnt 3.428151 hw_loss 0.166849 lr 0.00029377 rank 7
2023-02-27 18:30:31,537 DEBUG TRAIN Batch 48/7300 loss 3.656736 loss_att 7.664366 loss_ctc 9.507442 loss_rnnt 1.938322 hw_loss 0.256488 lr 0.00029378 rank 3
2023-02-27 18:30:31,538 DEBUG TRAIN Batch 48/7300 loss 5.712886 loss_att 8.623651 loss_ctc 8.370828 loss_rnnt 4.657645 hw_loss 0.222556 lr 0.00029378 rank 0
2023-02-27 18:30:31,538 DEBUG TRAIN Batch 48/7300 loss 3.200021 loss_att 6.427660 loss_ctc 6.035629 loss_rnnt 2.049712 hw_loss 0.237563 lr 0.00029378 rank 2
2023-02-27 18:30:31,539 DEBUG TRAIN Batch 48/7300 loss 3.904168 loss_att 6.406449 loss_ctc 5.126130 loss_rnnt 3.074091 hw_loss 0.312547 lr 0.00029378 rank 5
2023-02-27 18:30:31,540 DEBUG TRAIN Batch 48/7300 loss 17.413931 loss_att 20.612480 loss_ctc 24.107342 loss_rnnt 15.760496 hw_loss 0.227378 lr 0.00029378 rank 4
2023-02-27 18:30:31,546 DEBUG TRAIN Batch 48/7300 loss 6.121962 loss_att 9.556967 loss_ctc 8.754743 loss_rnnt 4.936179 hw_loss 0.277021 lr 0.00029378 rank 1
2023-02-27 18:31:11,786 DEBUG TRAIN Batch 48/7400 loss 4.653429 loss_att 9.323683 loss_ctc 8.412006 loss_rnnt 3.096797 hw_loss 0.227693 lr 0.00029377 rank 2
2023-02-27 18:31:11,800 DEBUG TRAIN Batch 48/7400 loss 8.421606 loss_att 14.954334 loss_ctc 18.559994 loss_rnnt 5.679276 hw_loss 0.157499 lr 0.00029377 rank 6
2023-02-27 18:31:11,805 DEBUG TRAIN Batch 48/7400 loss 2.575340 loss_att 5.065256 loss_ctc 5.015437 loss_rnnt 1.609740 hw_loss 0.266757 lr 0.00029377 rank 3
2023-02-27 18:31:11,812 DEBUG TRAIN Batch 48/7400 loss 9.012263 loss_att 12.812534 loss_ctc 17.927773 loss_rnnt 6.963975 hw_loss 0.186561 lr 0.00029377 rank 5
2023-02-27 18:31:11,818 DEBUG TRAIN Batch 48/7400 loss 2.183073 loss_att 6.241842 loss_ctc 3.619884 loss_rnnt 1.117616 hw_loss 0.116491 lr 0.00029377 rank 0
2023-02-27 18:31:11,819 DEBUG TRAIN Batch 48/7400 loss 12.296858 loss_att 15.297322 loss_ctc 22.271544 loss_rnnt 10.197301 hw_loss 0.317822 lr 0.00029377 rank 1
2023-02-27 18:31:11,828 DEBUG TRAIN Batch 48/7400 loss 4.516788 loss_att 8.607730 loss_ctc 8.949419 loss_rnnt 3.033564 hw_loss 0.138783 lr 0.00029377 rank 4
2023-02-27 18:31:11,854 DEBUG TRAIN Batch 48/7400 loss 5.794027 loss_att 9.132009 loss_ctc 11.666206 loss_rnnt 4.187106 hw_loss 0.293189 lr 0.00029376 rank 7
2023-02-27 18:32:18,606 DEBUG TRAIN Batch 48/7500 loss 6.922006 loss_att 8.848482 loss_ctc 11.068057 loss_rnnt 5.876390 hw_loss 0.201588 lr 0.00029375 rank 3
2023-02-27 18:32:18,606 DEBUG TRAIN Batch 48/7500 loss 4.192944 loss_att 6.605049 loss_ctc 7.948047 loss_rnnt 3.081093 hw_loss 0.241405 lr 0.00029376 rank 5
2023-02-27 18:32:18,608 DEBUG TRAIN Batch 48/7500 loss 6.747289 loss_att 9.956547 loss_ctc 11.943386 loss_rnnt 5.314267 hw_loss 0.184421 lr 0.00029375 rank 2
2023-02-27 18:32:18,608 DEBUG TRAIN Batch 48/7500 loss 7.138374 loss_att 10.673150 loss_ctc 9.443357 loss_rnnt 5.983149 hw_loss 0.264260 lr 0.00029375 rank 6
2023-02-27 18:32:18,609 DEBUG TRAIN Batch 48/7500 loss 1.868453 loss_att 3.321852 loss_ctc 3.717281 loss_rnnt 1.121597 hw_loss 0.393124 lr 0.00029375 rank 4
2023-02-27 18:32:18,609 DEBUG TRAIN Batch 48/7500 loss 4.912492 loss_att 6.564377 loss_ctc 5.798085 loss_rnnt 4.316146 hw_loss 0.277294 lr 0.00029376 rank 0
2023-02-27 18:32:18,610 DEBUG TRAIN Batch 48/7500 loss 7.339741 loss_att 6.940263 loss_ctc 10.365748 loss_rnnt 6.819191 hw_loss 0.369335 lr 0.00029376 rank 1
2023-02-27 18:32:18,615 DEBUG TRAIN Batch 48/7500 loss 7.860983 loss_att 9.152183 loss_ctc 9.349852 loss_rnnt 7.360168 hw_loss 0.082613 lr 0.00029375 rank 7
2023-02-27 18:32:57,893 DEBUG TRAIN Batch 48/7600 loss 6.818349 loss_att 9.518525 loss_ctc 12.339422 loss_rnnt 5.396087 hw_loss 0.273906 lr 0.00029375 rank 5
2023-02-27 18:32:57,893 DEBUG TRAIN Batch 48/7600 loss 7.028828 loss_att 9.054074 loss_ctc 9.435992 loss_rnnt 6.100938 hw_loss 0.378535 lr 0.00029374 rank 0
2023-02-27 18:32:57,894 DEBUG TRAIN Batch 48/7600 loss 9.195851 loss_att 11.935861 loss_ctc 12.896367 loss_rnnt 8.041051 hw_loss 0.212618 lr 0.00029374 rank 4
2023-02-27 18:32:57,894 DEBUG TRAIN Batch 48/7600 loss 5.048033 loss_att 5.999017 loss_ctc 8.495289 loss_rnnt 4.260229 hw_loss 0.258700 lr 0.00029374 rank 3
2023-02-27 18:32:57,895 DEBUG TRAIN Batch 48/7600 loss 8.667861 loss_att 11.253947 loss_ctc 14.106407 loss_rnnt 7.289137 hw_loss 0.255687 lr 0.00029374 rank 7
2023-02-27 18:32:57,896 DEBUG TRAIN Batch 48/7600 loss 6.274000 loss_att 9.259353 loss_ctc 12.832549 loss_rnnt 4.695473 hw_loss 0.200594 lr 0.00029374 rank 6
2023-02-27 18:32:57,899 DEBUG TRAIN Batch 48/7600 loss 5.573337 loss_att 8.719856 loss_ctc 8.727693 loss_rnnt 4.343418 hw_loss 0.337565 lr 0.00029374 rank 1
2023-02-27 18:32:57,900 DEBUG TRAIN Batch 48/7600 loss 4.687916 loss_att 6.171537 loss_ctc 6.243204 loss_rnnt 4.068071 hw_loss 0.217028 lr 0.00029374 rank 2
2023-02-27 18:33:38,155 DEBUG TRAIN Batch 48/7700 loss 18.482473 loss_att 19.819880 loss_ctc 20.340334 loss_rnnt 17.892269 hw_loss 0.140639 lr 0.00029373 rank 5
2023-02-27 18:33:38,169 DEBUG TRAIN Batch 48/7700 loss 5.848458 loss_att 5.861446 loss_ctc 8.697701 loss_rnnt 5.317894 hw_loss 0.277627 lr 0.00029373 rank 6
2023-02-27 18:33:38,169 DEBUG TRAIN Batch 48/7700 loss 9.101195 loss_att 9.963325 loss_ctc 13.086286 loss_rnnt 8.249803 hw_loss 0.276790 lr 0.00029373 rank 4
2023-02-27 18:33:38,170 DEBUG TRAIN Batch 48/7700 loss 1.252983 loss_att 3.367962 loss_ctc 2.612414 loss_rnnt 0.524096 hw_loss 0.233687 lr 0.00029373 rank 0
2023-02-27 18:33:38,172 DEBUG TRAIN Batch 48/7700 loss 7.192192 loss_att 7.293159 loss_ctc 12.455987 loss_rnnt 6.273114 hw_loss 0.369460 lr 0.00029373 rank 2
2023-02-27 18:33:38,172 DEBUG TRAIN Batch 48/7700 loss 3.169618 loss_att 7.666994 loss_ctc 7.626495 loss_rnnt 1.548425 hw_loss 0.239002 lr 0.00029373 rank 3
2023-02-27 18:33:38,175 DEBUG TRAIN Batch 48/7700 loss 7.133149 loss_att 7.030743 loss_ctc 10.604820 loss_rnnt 6.511647 hw_loss 0.335800 lr 0.00029372 rank 7
2023-02-27 18:33:38,189 DEBUG TRAIN Batch 48/7700 loss 9.572142 loss_att 14.389643 loss_ctc 14.775005 loss_rnnt 7.814472 hw_loss 0.188354 lr 0.00029373 rank 1
2023-02-27 18:34:18,611 DEBUG TRAIN Batch 48/7800 loss 7.137423 loss_att 10.114981 loss_ctc 21.422270 loss_rnnt 4.520863 hw_loss 0.218255 lr 0.00029372 rank 5
2023-02-27 18:34:18,613 DEBUG TRAIN Batch 48/7800 loss 5.628445 loss_att 8.351655 loss_ctc 9.156855 loss_rnnt 4.540308 hw_loss 0.136948 lr 0.00029372 rank 0
2023-02-27 18:34:18,613 DEBUG TRAIN Batch 48/7800 loss 11.918117 loss_att 15.039301 loss_ctc 19.558737 loss_rnnt 10.236633 hw_loss 0.072181 lr 0.00029372 rank 4
2023-02-27 18:34:18,613 DEBUG TRAIN Batch 48/7800 loss 6.469936 loss_att 10.630801 loss_ctc 15.715200 loss_rnnt 4.277521 hw_loss 0.239138 lr 0.00029371 rank 2
2023-02-27 18:34:18,616 DEBUG TRAIN Batch 48/7800 loss 5.923928 loss_att 7.952361 loss_ctc 8.913252 loss_rnnt 4.986491 hw_loss 0.249701 lr 0.00029372 rank 1
2023-02-27 18:34:18,621 DEBUG TRAIN Batch 48/7800 loss 3.937939 loss_att 5.625874 loss_ctc 4.905385 loss_rnnt 3.344979 hw_loss 0.236963 lr 0.00029372 rank 6
2023-02-27 18:34:18,623 DEBUG TRAIN Batch 48/7800 loss 1.589325 loss_att 4.449860 loss_ctc 4.106121 loss_rnnt 0.605634 hw_loss 0.142522 lr 0.00029371 rank 7
2023-02-27 18:34:18,621 DEBUG TRAIN Batch 48/7800 loss 6.159053 loss_att 9.199024 loss_ctc 7.829205 loss_rnnt 5.164733 hw_loss 0.306823 lr 0.00029372 rank 3
2023-02-27 18:35:25,053 DEBUG TRAIN Batch 48/7900 loss 21.872974 loss_att 24.287407 loss_ctc 35.955627 loss_rnnt 19.357616 hw_loss 0.290224 lr 0.00029370 rank 3
2023-02-27 18:35:25,060 DEBUG TRAIN Batch 48/7900 loss 7.774080 loss_att 12.782593 loss_ctc 14.356032 loss_rnnt 5.692291 hw_loss 0.379674 lr 0.00029370 rank 4
2023-02-27 18:35:25,066 DEBUG TRAIN Batch 48/7900 loss 5.782223 loss_att 9.359568 loss_ctc 8.691040 loss_rnnt 4.549318 hw_loss 0.242989 lr 0.00029370 rank 6
2023-02-27 18:35:25,067 DEBUG TRAIN Batch 48/7900 loss 13.971417 loss_att 15.835211 loss_ctc 15.322677 loss_rnnt 13.294371 hw_loss 0.232727 lr 0.00029370 rank 2
2023-02-27 18:35:25,070 DEBUG TRAIN Batch 48/7900 loss 9.354544 loss_att 12.043467 loss_ctc 17.089357 loss_rnnt 7.623744 hw_loss 0.303200 lr 0.00029370 rank 0
2023-02-27 18:35:25,073 DEBUG TRAIN Batch 48/7900 loss 3.491510 loss_att 7.916865 loss_ctc 5.142937 loss_rnnt 2.246556 hw_loss 0.261923 lr 0.00029370 rank 7
2023-02-27 18:35:25,074 DEBUG TRAIN Batch 48/7900 loss 6.253467 loss_att 7.311823 loss_ctc 7.726351 loss_rnnt 5.703779 hw_loss 0.265559 lr 0.00029371 rank 1
2023-02-27 18:35:25,123 DEBUG TRAIN Batch 48/7900 loss 8.101246 loss_att 8.657146 loss_ctc 14.937033 loss_rnnt 7.045854 hw_loss 0.061449 lr 0.00029371 rank 5
2023-02-27 18:36:05,775 DEBUG TRAIN Batch 48/8000 loss 4.371719 loss_att 6.745393 loss_ctc 5.695180 loss_rnnt 3.642116 hw_loss 0.147013 lr 0.00029369 rank 5
2023-02-27 18:36:05,783 DEBUG TRAIN Batch 48/8000 loss 4.026308 loss_att 6.801510 loss_ctc 7.561451 loss_rnnt 2.837240 hw_loss 0.305015 lr 0.00029369 rank 0
2023-02-27 18:36:05,786 DEBUG TRAIN Batch 48/8000 loss 5.282650 loss_att 8.037559 loss_ctc 12.703831 loss_rnnt 3.630491 hw_loss 0.209412 lr 0.00029369 rank 3
2023-02-27 18:36:05,786 DEBUG TRAIN Batch 48/8000 loss 5.971690 loss_att 8.554342 loss_ctc 10.201054 loss_rnnt 4.793296 hw_loss 0.183653 lr 0.00029369 rank 6
2023-02-27 18:36:05,788 DEBUG TRAIN Batch 48/8000 loss 6.705485 loss_att 9.657761 loss_ctc 10.391302 loss_rnnt 5.466205 hw_loss 0.295093 lr 0.00029369 rank 7
2023-02-27 18:36:05,818 DEBUG TRAIN Batch 48/8000 loss 8.005478 loss_att 11.418400 loss_ctc 14.673836 loss_rnnt 6.354680 hw_loss 0.148312 lr 0.00029369 rank 2
2023-02-27 18:36:05,821 DEBUG TRAIN Batch 48/8000 loss 3.122916 loss_att 6.087826 loss_ctc 6.394913 loss_rnnt 1.939886 hw_loss 0.288339 lr 0.00029369 rank 1
2023-02-27 18:36:05,836 DEBUG TRAIN Batch 48/8000 loss 8.948113 loss_att 11.286326 loss_ctc 13.363800 loss_rnnt 7.760729 hw_loss 0.245595 lr 0.00029369 rank 4
2023-02-27 18:36:47,108 DEBUG TRAIN Batch 48/8100 loss 5.950548 loss_att 10.386102 loss_ctc 13.379522 loss_rnnt 3.971692 hw_loss 0.189779 lr 0.00029368 rank 0
2023-02-27 18:36:47,108 DEBUG TRAIN Batch 48/8100 loss 3.436798 loss_att 7.653351 loss_ctc 4.345152 loss_rnnt 2.350370 hw_loss 0.228756 lr 0.00029367 rank 7
2023-02-27 18:36:47,107 DEBUG TRAIN Batch 48/8100 loss 9.079288 loss_att 10.692403 loss_ctc 11.100414 loss_rnnt 8.365149 hw_loss 0.228807 lr 0.00029368 rank 6
2023-02-27 18:36:47,109 DEBUG TRAIN Batch 48/8100 loss 9.996217 loss_att 10.665541 loss_ctc 13.580891 loss_rnnt 9.287465 hw_loss 0.181741 lr 0.00029368 rank 2
2023-02-27 18:36:47,113 DEBUG TRAIN Batch 48/8100 loss 7.798288 loss_att 9.570873 loss_ctc 10.067575 loss_rnnt 7.040674 hw_loss 0.188482 lr 0.00029368 rank 4
2023-02-27 18:36:47,114 DEBUG TRAIN Batch 48/8100 loss 2.355435 loss_att 4.349273 loss_ctc 4.619618 loss_rnnt 1.476612 hw_loss 0.334059 lr 0.00029368 rank 3
2023-02-27 18:36:47,133 DEBUG TRAIN Batch 48/8100 loss 5.754238 loss_att 7.755864 loss_ctc 7.436292 loss_rnnt 5.044387 hw_loss 0.159846 lr 0.00029368 rank 5
2023-02-27 18:36:47,139 DEBUG TRAIN Batch 48/8100 loss 7.697183 loss_att 10.133668 loss_ctc 14.893130 loss_rnnt 6.095155 hw_loss 0.291134 lr 0.00029368 rank 1
2023-02-27 18:37:28,404 DEBUG TRAIN Batch 48/8200 loss 8.435085 loss_att 10.395306 loss_ctc 13.991067 loss_rnnt 7.172282 hw_loss 0.243677 lr 0.00029367 rank 3
2023-02-27 18:37:28,406 DEBUG TRAIN Batch 48/8200 loss 6.557811 loss_att 7.828907 loss_ctc 9.904857 loss_rnnt 5.732399 hw_loss 0.234225 lr 0.00029367 rank 0
2023-02-27 18:37:28,407 DEBUG TRAIN Batch 48/8200 loss 8.271542 loss_att 11.371208 loss_ctc 15.744970 loss_rnnt 6.548504 hw_loss 0.199962 lr 0.00029367 rank 4
2023-02-27 18:37:28,409 DEBUG TRAIN Batch 48/8200 loss 8.511692 loss_att 11.054528 loss_ctc 16.458632 loss_rnnt 6.817268 hw_loss 0.236748 lr 0.00029366 rank 7
2023-02-27 18:37:28,410 DEBUG TRAIN Batch 48/8200 loss 1.801907 loss_att 3.006379 loss_ctc 3.263146 loss_rnnt 1.224739 hw_loss 0.265202 lr 0.00029366 rank 6
2023-02-27 18:37:28,413 DEBUG TRAIN Batch 48/8200 loss 4.087444 loss_att 5.671705 loss_ctc 7.040170 loss_rnnt 3.260042 hw_loss 0.219100 lr 0.00029367 rank 5
2023-02-27 18:37:28,413 DEBUG TRAIN Batch 48/8200 loss 10.432648 loss_att 16.135166 loss_ctc 19.065235 loss_rnnt 8.000587 hw_loss 0.263522 lr 0.00029366 rank 2
2023-02-27 18:37:28,429 DEBUG TRAIN Batch 48/8200 loss 7.704097 loss_att 13.451880 loss_ctc 14.652622 loss_rnnt 5.410933 hw_loss 0.407134 lr 0.00029367 rank 1
2023-02-27 18:38:07,559 DEBUG TRAIN Batch 48/8300 loss 7.110970 loss_att 10.501918 loss_ctc 16.158354 loss_rnnt 5.071504 hw_loss 0.290547 lr 0.00029366 rank 5
2023-02-27 18:38:07,564 DEBUG TRAIN Batch 48/8300 loss 6.891277 loss_att 11.427754 loss_ctc 11.504568 loss_rnnt 5.187809 hw_loss 0.339500 lr 0.00029365 rank 2
2023-02-27 18:38:07,573 DEBUG TRAIN Batch 48/8300 loss 2.065866 loss_att 5.133564 loss_ctc 3.952479 loss_rnnt 1.029420 hw_loss 0.321296 lr 0.00029365 rank 0
2023-02-27 18:38:07,573 DEBUG TRAIN Batch 48/8300 loss 5.764624 loss_att 5.689861 loss_ctc 10.032869 loss_rnnt 5.031115 hw_loss 0.336302 lr 0.00029365 rank 6
2023-02-27 18:38:07,576 DEBUG TRAIN Batch 48/8300 loss 5.316136 loss_att 8.447888 loss_ctc 7.888493 loss_rnnt 4.187459 hw_loss 0.298773 lr 0.00029365 rank 3
2023-02-27 18:38:07,578 DEBUG TRAIN Batch 48/8300 loss 4.730766 loss_att 6.617941 loss_ctc 7.007370 loss_rnnt 3.966138 hw_loss 0.156836 lr 0.00029365 rank 4
2023-02-27 18:38:07,582 DEBUG TRAIN Batch 48/8300 loss 4.050990 loss_att 7.520487 loss_ctc 5.011148 loss_rnnt 3.070145 hw_loss 0.297984 lr 0.00029366 rank 1
2023-02-27 18:38:07,601 DEBUG TRAIN Batch 48/8300 loss 4.855360 loss_att 5.872682 loss_ctc 5.975708 loss_rnnt 4.456741 hw_loss 0.085827 lr 0.00029365 rank 7
2023-02-27 18:38:40,189 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 1
2023-02-27 18:38:40,191 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 6
2023-02-27 18:38:40,198 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 0
2023-02-27 18:38:40,200 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 3
2023-02-27 18:38:40,248 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 2
2023-02-27 18:38:40,341 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 7
2023-02-27 18:38:40,374 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 5
2023-02-27 18:38:40,405 DEBUG CV Batch 48/0 loss 1.179912 loss_att 1.069051 loss_ctc 1.912359 loss_rnnt 0.888196 hw_loss 0.405428 history loss 1.136212 rank 4
2023-02-27 18:38:51,622 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 7
2023-02-27 18:38:51,632 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 5
2023-02-27 18:38:51,674 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 1
2023-02-27 18:38:51,918 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 6
2023-02-27 18:38:51,997 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 0
2023-02-27 18:38:52,018 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 3
2023-02-27 18:38:52,045 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 2
2023-02-27 18:38:52,255 DEBUG CV Batch 48/100 loss 3.610862 loss_att 4.987489 loss_ctc 9.744696 loss_rnnt 2.353878 hw_loss 0.307151 history loss 2.953122 rank 4
2023-02-27 18:39:04,972 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 7
2023-02-27 18:39:05,010 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 5
2023-02-27 18:39:05,254 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 1
2023-02-27 18:39:05,628 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 6
2023-02-27 18:39:05,895 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 3
2023-02-27 18:39:05,992 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 0
2023-02-27 18:39:06,007 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 2
2023-02-27 18:39:06,138 DEBUG CV Batch 48/200 loss 7.143972 loss_att 9.131299 loss_ctc 9.082878 loss_rnnt 6.389386 hw_loss 0.184873 history loss 3.551273 rank 4
2023-02-27 18:39:17,041 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 7
2023-02-27 18:39:17,389 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 5
2023-02-27 18:39:17,555 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 1
2023-02-27 18:39:18,253 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 6
2023-02-27 18:39:18,392 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 2
2023-02-27 18:39:18,751 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 3
2023-02-27 18:39:18,882 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 0
2023-02-27 18:39:18,952 DEBUG CV Batch 48/300 loss 2.384813 loss_att 3.205745 loss_ctc 3.669918 loss_rnnt 1.921962 hw_loss 0.238720 history loss 3.677647 rank 4
2023-02-27 18:39:29,214 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 7
2023-02-27 18:39:29,652 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 1
2023-02-27 18:39:29,695 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 5
2023-02-27 18:39:30,589 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 2
2023-02-27 18:39:30,613 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 6
2023-02-27 18:39:31,323 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 3
2023-02-27 18:39:31,542 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 4
2023-02-27 18:39:31,742 DEBUG CV Batch 48/400 loss 17.141323 loss_att 65.613449 loss_ctc 9.800314 loss_rnnt 8.425379 hw_loss 0.000602 history loss 4.475052 rank 0
2023-02-27 18:39:39,836 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 7
2023-02-27 18:39:40,290 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 1
2023-02-27 18:39:40,643 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 5
2023-02-27 18:39:41,772 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 6
2023-02-27 18:39:41,868 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 2
2023-02-27 18:39:42,601 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 3
2023-02-27 18:39:42,822 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 4
2023-02-27 18:39:43,099 DEBUG CV Batch 48/500 loss 3.917105 loss_att 4.266633 loss_ctc 5.977683 loss_rnnt 3.473380 hw_loss 0.185767 history loss 5.082270 rank 0
2023-02-27 18:39:51,992 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 7
2023-02-27 18:39:52,509 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 1
2023-02-27 18:39:52,719 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 5
2023-02-27 18:39:54,331 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 6
2023-02-27 18:39:54,419 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 2
2023-02-27 18:39:55,378 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 3
2023-02-27 18:39:55,385 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 4
2023-02-27 18:39:55,957 DEBUG CV Batch 48/600 loss 6.297989 loss_att 5.809652 loss_ctc 8.999079 loss_rnnt 5.801478 hw_loss 0.438811 history loss 5.913564 rank 0
2023-02-27 18:40:03,456 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 7
2023-02-27 18:40:04,058 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 1
2023-02-27 18:40:04,342 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 5
2023-02-27 18:40:05,998 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 2
2023-02-27 18:40:06,205 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 6
2023-02-27 18:40:07,388 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 4
2023-02-27 18:40:07,458 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 3
2023-02-27 18:40:08,188 DEBUG CV Batch 48/700 loss 15.095679 loss_att 37.732357 loss_ctc 17.787756 loss_rnnt 10.025130 hw_loss 0.345506 history loss 6.430137 rank 0
2023-02-27 18:40:14,834 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 7
2023-02-27 18:40:15,712 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 1
2023-02-27 18:40:15,806 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 5
2023-02-27 18:40:17,806 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 2
2023-02-27 18:40:17,996 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 6
2023-02-27 18:40:19,206 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 4
2023-02-27 18:40:19,422 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 3
2023-02-27 18:40:20,212 DEBUG CV Batch 48/800 loss 6.305561 loss_att 7.731926 loss_ctc 15.161133 loss_rnnt 4.690548 hw_loss 0.279369 history loss 5.967608 rank 0
2023-02-27 18:40:28,230 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 7
2023-02-27 18:40:29,118 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 1
2023-02-27 18:40:29,180 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 5
2023-02-27 18:40:31,326 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 2
2023-02-27 18:40:31,696 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 6
2023-02-27 18:40:32,974 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 4
2023-02-27 18:40:33,256 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 3
2023-02-27 18:40:34,168 DEBUG CV Batch 48/900 loss 8.508892 loss_att 10.702042 loss_ctc 16.248867 loss_rnnt 6.990965 hw_loss 0.088687 history loss 5.812423 rank 0
2023-02-27 18:40:40,647 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 7
2023-02-27 18:40:41,531 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 5
2023-02-27 18:40:41,723 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 1
2023-02-27 18:40:44,000 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 2
2023-02-27 18:40:44,356 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 6
2023-02-27 18:40:45,602 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 4
2023-02-27 18:40:46,030 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 3
2023-02-27 18:40:47,174 DEBUG CV Batch 48/1000 loss 3.240279 loss_att 3.294984 loss_ctc 3.309634 loss_rnnt 3.014278 hw_loss 0.385899 history loss 5.622644 rank 0
2023-02-27 18:40:52,659 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 7
2023-02-27 18:40:53,719 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 1
2023-02-27 18:40:53,958 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 5
2023-02-27 18:40:56,401 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 2
2023-02-27 18:40:56,742 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 6
2023-02-27 18:40:58,080 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 4
2023-02-27 18:40:58,602 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 3
2023-02-27 18:40:59,840 DEBUG CV Batch 48/1100 loss 4.518878 loss_att 4.464794 loss_ctc 8.061531 loss_rnnt 3.844800 hw_loss 0.398513 history loss 5.581546 rank 0
2023-02-27 18:41:03,479 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 7
2023-02-27 18:41:04,404 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 1
2023-02-27 18:41:04,972 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 5
2023-02-27 18:41:07,451 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 2
2023-02-27 18:41:07,831 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 6
2023-02-27 18:41:09,369 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 4
2023-02-27 18:41:09,953 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 3
2023-02-27 18:41:11,306 DEBUG CV Batch 48/1200 loss 5.961220 loss_att 6.012024 loss_ctc 6.580419 loss_rnnt 5.724324 hw_loss 0.270329 history loss 5.861628 rank 0
2023-02-27 18:41:15,653 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 7
2023-02-27 18:41:16,766 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 1
2023-02-27 18:41:17,047 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 5
2023-02-27 18:41:20,078 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 2
2023-02-27 18:41:20,291 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 6
2023-02-27 18:41:22,006 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 4
2023-02-27 18:41:22,621 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 3
2023-02-27 18:41:24,085 DEBUG CV Batch 48/1300 loss 4.574585 loss_att 4.077878 loss_ctc 6.524379 loss_rnnt 4.229155 hw_loss 0.346498 history loss 6.136002 rank 0
2023-02-27 18:41:27,039 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 7
2023-02-27 18:41:28,454 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 5
2023-02-27 18:41:28,551 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 1
2023-02-27 18:41:31,527 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 2
2023-02-27 18:41:31,947 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 6
2023-02-27 18:41:33,749 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 4
2023-02-27 18:41:34,601 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 3
2023-02-27 18:41:35,982 DEBUG CV Batch 48/1400 loss 3.641289 loss_att 13.291586 loss_ctc 3.435222 loss_rnnt 1.635826 hw_loss 0.192898 history loss 6.399659 rank 0
2023-02-27 18:41:38,705 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 7
2023-02-27 18:41:40,050 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 5
2023-02-27 18:41:40,228 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 1
2023-02-27 18:41:43,295 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 2
2023-02-27 18:41:44,066 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 6
2023-02-27 18:41:45,894 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 4
2023-02-27 18:41:46,878 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 3
2023-02-27 18:41:48,216 DEBUG CV Batch 48/1500 loss 7.816231 loss_att 7.814157 loss_ctc 8.590405 loss_rnnt 7.520183 hw_loss 0.362326 history loss 6.266473 rank 0
2023-02-27 18:41:51,895 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 7
2023-02-27 18:41:53,170 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 5
2023-02-27 18:41:53,578 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 1
2023-02-27 18:41:56,730 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 2
2023-02-27 18:41:57,598 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 6
2023-02-27 18:41:59,360 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 4
2023-02-27 18:42:00,564 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 3
2023-02-27 18:42:02,012 DEBUG CV Batch 48/1600 loss 10.215462 loss_att 12.969654 loss_ctc 11.890193 loss_rnnt 9.302759 hw_loss 0.259811 history loss 6.232158 rank 0
2023-02-27 18:42:04,683 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 7
2023-02-27 18:42:05,621 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 5
2023-02-27 18:42:06,151 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 1
2023-02-27 18:42:09,572 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 2
2023-02-27 18:42:10,379 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 6
2023-02-27 18:42:11,827 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 4
2023-02-27 18:42:13,143 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 3
2023-02-27 18:42:14,033 INFO Epoch 48 CV info cv_loss 6.155720157914638
2023-02-27 18:42:14,034 INFO Epoch 49 TRAIN info lr 0.000293644539066527
2023-02-27 18:42:14,036 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:14,639 DEBUG CV Batch 48/1700 loss 7.794362 loss_att 6.471936 loss_ctc 13.980696 loss_rnnt 7.120324 hw_loss 0.213146 history loss 6.174801 rank 0
2023-02-27 18:42:14,948 INFO Epoch 48 CV info cv_loss 6.155720156260628
2023-02-27 18:42:14,949 INFO Epoch 49 TRAIN info lr 0.0002936521353977429
2023-02-27 18:42:14,954 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:15,346 INFO Epoch 48 CV info cv_loss 6.15572015744514
2023-02-27 18:42:15,347 INFO Epoch 49 TRAIN info lr 0.00029364757752827083
2023-02-27 18:42:15,349 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:19,529 INFO Epoch 48 CV info cv_loss 6.15572015671505
2023-02-27 18:42:19,530 INFO Epoch 49 TRAIN info lr 0.0002936485903698124
2023-02-27 18:42:19,534 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:19,619 INFO Epoch 48 CV info cv_loss 6.155720157658353
2023-02-27 18:42:19,620 INFO Epoch 49 TRAIN info lr 0.0002936485903698124
2023-02-27 18:42:19,622 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:20,979 INFO Epoch 48 CV info cv_loss 6.15572015788018
2023-02-27 18:42:20,980 INFO Epoch 49 TRAIN info lr 0.0002936501096517758
2023-02-27 18:42:20,985 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:22,530 INFO Epoch 48 CV info cv_loss 6.155720157328843
2023-02-27 18:42:22,530 INFO Epoch 49 TRAIN info lr 0.0002936465646972095
2023-02-27 18:42:22,532 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:42:24,071 INFO Epoch 48 CV info cv_loss 6.155720157302999
2023-02-27 18:42:24,071 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune/48.pt
2023-02-27 18:42:24,627 INFO Epoch 49 TRAIN info lr 0.0002936490967945134
2023-02-27 18:42:24,631 INFO using accumulate grad, new batch size is 4 times larger than before
2023-02-27 18:43:25,934 DEBUG TRAIN Batch 49/0 loss 6.557419 loss_att 6.095406 loss_ctc 9.568100 loss_rnnt 6.021168 hw_loss 0.426054 lr 0.00029365 rank 3
2023-02-27 18:43:25,936 DEBUG TRAIN Batch 49/0 loss 9.030635 loss_att 9.552690 loss_ctc 14.455651 loss_rnnt 8.013242 hw_loss 0.355587 lr 0.00029365 rank 5
2023-02-27 18:43:25,939 DEBUG TRAIN Batch 49/0 loss 7.093350 loss_att 7.303607 loss_ctc 10.978905 loss_rnnt 6.311784 hw_loss 0.415202 lr 0.00029365 rank 0
2023-02-27 18:43:25,941 DEBUG TRAIN Batch 49/0 loss 7.484717 loss_att 7.579290 loss_ctc 10.490614 loss_rnnt 6.899031 hw_loss 0.311222 lr 0.00029365 rank 2
2023-02-27 18:43:25,942 DEBUG TRAIN Batch 49/0 loss 6.778964 loss_att 6.410028 loss_ctc 10.123664 loss_rnnt 6.189833 hw_loss 0.406796 lr 0.00029365 rank 6
2023-02-27 18:43:25,944 DEBUG TRAIN Batch 49/0 loss 7.841521 loss_att 7.999207 loss_ctc 11.065011 loss_rnnt 7.160337 hw_loss 0.412214 lr 0.00029364 rank 7
2023-02-27 18:43:25,982 DEBUG TRAIN Batch 49/0 loss 9.397622 loss_att 9.311164 loss_ctc 14.252187 loss_rnnt 8.555703 hw_loss 0.397380 lr 0.00029365 rank 4
2023-02-27 18:43:25,990 DEBUG TRAIN Batch 49/0 loss 7.174712 loss_att 7.585625 loss_ctc 11.029361 loss_rnnt 6.350255 hw_loss 0.428100 lr 0.00029365 rank 1
2023-02-27 18:44:03,068 DEBUG TRAIN Batch 49/100 loss 1.287963 loss_att 4.012189 loss_ctc 2.559096 loss_rnnt 0.520690 hw_loss 0.099268 lr 0.00029363 rank 3
2023-02-27 18:44:03,072 DEBUG TRAIN Batch 49/100 loss 7.500705 loss_att 9.953735 loss_ctc 14.726550 loss_rnnt 5.989135 hw_loss 0.107847 lr 0.00029364 rank 0
2023-02-27 18:44:03,086 DEBUG TRAIN Batch 49/100 loss 4.708607 loss_att 8.895422 loss_ctc 7.364331 loss_rnnt 3.454527 hw_loss 0.117411 lr 0.00029363 rank 1
2023-02-27 18:44:03,089 DEBUG TRAIN Batch 49/100 loss 1.461929 loss_att 3.507041 loss_ctc 1.763727 loss_rnnt 0.794764 hw_loss 0.408568 lr 0.00029363 rank 7
2023-02-27 18:44:03,089 DEBUG TRAIN Batch 49/100 loss 4.845263 loss_att 5.990286 loss_ctc 8.010686 loss_rnnt 3.996582 hw_loss 0.370538 lr 0.00029364 rank 6
2023-02-27 18:44:03,094 DEBUG TRAIN Batch 49/100 loss 8.493991 loss_att 12.056274 loss_ctc 21.710022 loss_rnnt 5.915864 hw_loss 0.194122 lr 0.00029364 rank 2
2023-02-27 18:44:03,099 DEBUG TRAIN Batch 49/100 loss 8.348541 loss_att 8.455079 loss_ctc 10.274986 loss_rnnt 7.879238 hw_loss 0.358382 lr 0.00029364 rank 4
2023-02-27 18:44:03,142 DEBUG TRAIN Batch 49/100 loss 2.920343 loss_att 4.181020 loss_ctc 3.098564 loss_rnnt 2.454766 hw_loss 0.355649 lr 0.00029364 rank 5
2023-02-27 18:44:40,741 DEBUG TRAIN Batch 49/200 loss 4.750633 loss_att 9.468057 loss_ctc 9.321377 loss_rnnt 3.037359 hw_loss 0.300669 lr 0.00029362 rank 0
2023-02-27 18:44:40,749 DEBUG TRAIN Batch 49/200 loss 12.922538 loss_att 16.443047 loss_ctc 17.581532 loss_rnnt 11.507505 hw_loss 0.168246 lr 0.00029362 rank 3
2023-02-27 18:44:40,757 DEBUG TRAIN Batch 49/200 loss 10.312431 loss_att 11.755789 loss_ctc 9.270948 loss_rnnt 10.047382 hw_loss 0.216078 lr 0.00029362 rank 6
2023-02-27 18:44:40,761 DEBUG TRAIN Batch 49/200 loss 7.500723 loss_att 9.303780 loss_ctc 13.287036 loss_rnnt 6.300179 hw_loss 0.128297 lr 0.00029362 rank 7
2023-02-27 18:44:40,765 DEBUG TRAIN Batch 49/200 loss 7.100858 loss_att 8.643294 loss_ctc 9.472338 loss_rnnt 6.374942 hw_loss 0.189807 lr 0.00029363 rank 5
2023-02-27 18:44:40,765 DEBUG TRAIN Batch 49/200 loss 5.792464 loss_att 8.488744 loss_ctc 10.310850 loss_rnnt 4.535162 hw_loss 0.216741 lr 0.00029362 rank 2
2023-02-27 18:44:40,769 DEBUG TRAIN Batch 49/200 loss 4.418298 loss_att 7.875421 loss_ctc 7.342397 loss_rnnt 3.272682 hw_loss 0.120584 lr 0.00029362 rank 1
2023-02-27 18:44:40,771 DEBUG TRAIN Batch 49/200 loss 3.663715 loss_att 5.977076 loss_ctc 5.291664 loss_rnnt 2.907090 hw_loss 0.144174 lr 0.00029362 rank 4
2023-02-27 18:45:18,966 DEBUG TRAIN Batch 49/300 loss 12.990157 loss_att 16.201927 loss_ctc 22.789387 loss_rnnt 11.026535 hw_loss 0.027571 lr 0.00029361 rank 6
2023-02-27 18:45:18,971 DEBUG TRAIN Batch 49/300 loss 4.463711 loss_att 6.162106 loss_ctc 7.590049 loss_rnnt 3.598357 hw_loss 0.204057 lr 0.00029361 rank 0
2023-02-27 18:45:18,971 DEBUG TRAIN Batch 49/300 loss 5.371093 loss_att 8.859829 loss_ctc 9.463768 loss_rnnt 4.022724 hw_loss 0.196747 lr 0.00029361 rank 3
2023-02-27 18:45:18,976 DEBUG TRAIN Batch 49/300 loss 1.863631 loss_att 4.318708 loss_ctc 1.730754 loss_rnnt 1.230434 hw_loss 0.299809 lr 0.00029361 rank 2
2023-02-27 18:45:18,978 DEBUG TRAIN Batch 49/300 loss 7.445690 loss_att 12.146969 loss_ctc 14.155038 loss_rnnt 5.490531 hw_loss 0.225606 lr 0.00029361 rank 7
2023-02-27 18:45:18,978 DEBUG TRAIN Batch 49/300 loss 4.251112 loss_att 7.135835 loss_ctc 8.077129 loss_rnnt 3.027198 hw_loss 0.256564 lr 0.00029361 rank 5
2023-02-27 18:45:18,980 DEBUG TRAIN Batch 49/300 loss 3.793120 loss_att 6.825697 loss_ctc 10.038048 loss_rnnt 2.163537 hw_loss 0.357019 lr 0.00029361 rank 1
2023-02-27 18:45:18,987 DEBUG TRAIN Batch 49/300 loss 3.396422 loss_att 6.243989 loss_ctc 5.860888 loss_rnnt 2.379346 hw_loss 0.223064 lr 0.00029361 rank 4
2023-02-27 18:46:23,707 DEBUG TRAIN Batch 49/400 loss 9.535419 loss_att 14.852587 loss_ctc 21.389706 loss_rnnt 6.816939 hw_loss 0.139642 lr 0.00029360 rank 6
2023-02-27 18:46:23,712 DEBUG TRAIN Batch 49/400 loss 2.086709 loss_att 5.301036 loss_ctc 4.635212 loss_rnnt 1.047449 hw_loss 0.106112 lr 0.00029360 rank 0
2023-02-27 18:46:23,714 DEBUG TRAIN Batch 49/400 loss 6.007319 loss_att 8.330064 loss_ctc 10.591644 loss_rnnt 4.689890 hw_loss 0.453069 lr 0.00029360 rank 4
2023-02-27 18:46:23,725 DEBUG TRAIN Batch 49/400 loss 11.308161 loss_att 14.296269 loss_ctc 21.077858 loss_rnnt 9.227821 hw_loss 0.337669 lr 0.00029360 rank 1
2023-02-27 18:46:23,725 DEBUG TRAIN Batch 49/400 loss 2.978077 loss_att 5.685549 loss_ctc 6.403300 loss_rnnt 1.916204 hw_loss 0.119405 lr 0.00029360 rank 3
2023-02-27 18:46:23,725 DEBUG TRAIN Batch 49/400 loss 3.624619 loss_att 6.550927 loss_ctc 5.921865 loss_rnnt 2.663402 hw_loss 0.130605 lr 0.00029360 rank 5
2023-02-27 18:46:23,728 DEBUG TRAIN Batch 49/400 loss 8.442393 loss_att 11.437055 loss_ctc 12.783372 loss_rnnt 7.163440 hw_loss 0.189797 lr 0.00029359 rank 7
2023-02-27 18:46:23,776 DEBUG TRAIN Batch 49/400 loss 6.271902 loss_att 9.597908 loss_ctc 15.994195 loss_rnnt 4.202623 hw_loss 0.202070 lr 0.00029360 rank 2
2023-02-27 18:47:01,377 DEBUG TRAIN Batch 49/500 loss 7.443215 loss_att 10.179807 loss_ctc 14.519782 loss_rnnt 5.784157 hw_loss 0.315369 lr 0.00029358 rank 3
2023-02-27 18:47:01,378 DEBUG TRAIN Batch 49/500 loss 17.346638 loss_att 18.516754 loss_ctc 30.877274 loss_rnnt 15.156800 hw_loss 0.284489 lr 0.00029359 rank 5
2023-02-27 18:47:01,378 DEBUG TRAIN Batch 49/500 loss 4.693414 loss_att 5.971677 loss_ctc 5.111674 loss_rnnt 4.231532 hw_loss 0.282115 lr 0.00029359 rank 0
2023-02-27 18:47:01,380 DEBUG TRAIN Batch 49/500 loss 4.819425 loss_att 6.524477 loss_ctc 6.599907 loss_rnnt 4.108613 hw_loss 0.248257 lr 0.00029358 rank 2
2023-02-27 18:47:01,380 DEBUG TRAIN Batch 49/500 loss 8.436529 loss_att 10.618063 loss_ctc 15.277636 loss_rnnt 6.902312 hw_loss 0.348305 lr 0.00029358 rank 7
2023-02-27 18:47:01,381 DEBUG TRAIN Batch 49/500 loss 3.094292 loss_att 4.978000 loss_ctc 6.690308 loss_rnnt 2.102309 hw_loss 0.254574 lr 0.00029359 rank 4
2023-02-27 18:47:01,382 DEBUG TRAIN Batch 49/500 loss 12.115682 loss_att 12.342228 loss_ctc 16.443993 loss_rnnt 11.377478 hw_loss 0.217100 lr 0.00029358 rank 6
2023-02-27 18:47:01,385 DEBUG TRAIN Batch 49/500 loss 10.996074 loss_att 13.247107 loss_ctc 17.030983 loss_rnnt 9.568587 hw_loss 0.323672 lr 0.00029358 rank 1
2023-02-27 18:47:39,806 DEBUG TRAIN Batch 49/600 loss 2.263638 loss_att 3.639985 loss_ctc 4.688218 loss_rnnt 1.550343 hw_loss 0.215151 lr 0.00029357 rank 3
2023-02-27 18:47:39,818 DEBUG TRAIN Batch 49/600 loss 4.488732 loss_att 7.474591 loss_ctc 8.143195 loss_rnnt 3.236025 hw_loss 0.315514 lr 0.00029357 rank 4
2023-02-27 18:47:39,818 DEBUG TRAIN Batch 49/600 loss 6.736251 loss_att 7.191105 loss_ctc 11.058375 loss_rnnt 5.941438 hw_loss 0.239173 lr 0.00029358 rank 5
2023-02-27 18:47:39,819 DEBUG TRAIN Batch 49/600 loss 8.478045 loss_att 9.142501 loss_ctc 11.043695 loss_rnnt 7.863854 hw_loss 0.261026 lr 0.00029357 rank 6
2023-02-27 18:47:39,821 DEBUG TRAIN Batch 49/600 loss 12.876526 loss_att 14.012120 loss_ctc 20.435324 loss_rnnt 11.468624 hw_loss 0.324268 lr 0.00029357 rank 2
2023-02-27 18:47:39,823 DEBUG TRAIN Batch 49/600 loss 7.278594 loss_att 8.403921 loss_ctc 10.840821 loss_rnnt 6.408033 hw_loss 0.319748 lr 0.00029357 rank 7
2023-02-27 18:47:39,824 DEBUG TRAIN Batch 49/600 loss 11.410038 loss_att 14.071260 loss_ctc 19.977779 loss_rnnt 9.599339 hw_loss 0.255167 lr 0.00029357 rank 0
2023-02-27 18:47:39,830 DEBUG TRAIN Batch 49/600 loss 5.984564 loss_att 6.173772 loss_ctc 9.135586 loss_rnnt 5.362196 hw_loss 0.308231 lr 0.00029357 rank 1
2023-02-27 18:48:19,104 DEBUG TRAIN Batch 49/700 loss 7.940518 loss_att 10.750967 loss_ctc 13.327907 loss_rnnt 6.588190 hw_loss 0.134850 lr 0.00029356 rank 3
2023-02-27 18:48:19,106 DEBUG TRAIN Batch 49/700 loss 1.022063 loss_att 3.483015 loss_ctc 1.895375 loss_rnnt 0.303180 hw_loss 0.206721 lr 0.00029356 rank 0
2023-02-27 18:48:19,112 DEBUG TRAIN Batch 49/700 loss 7.133649 loss_att 8.567755 loss_ctc 13.000111 loss_rnnt 5.965276 hw_loss 0.186294 lr 0.00029356 rank 1
2023-02-27 18:48:19,114 DEBUG TRAIN Batch 49/700 loss 9.174489 loss_att 10.706578 loss_ctc 14.748553 loss_rnnt 7.951428 hw_loss 0.325189 lr 0.00029356 rank 4
2023-02-27 18:48:19,115 DEBUG TRAIN Batch 49/700 loss 2.908247 loss_att 5.140814 loss_ctc 6.568472 loss_rnnt 1.888233 hw_loss 0.160259 lr 0.00029356 rank 5
2023-02-27 18:48:19,116 DEBUG TRAIN Batch 49/700 loss 4.144645 loss_att 7.461622 loss_ctc 6.856668 loss_rnnt 2.935637 hw_loss 0.345019 lr 0.00029356 rank 2
2023-02-27 18:48:19,116 DEBUG TRAIN Batch 49/700 loss 3.777480 loss_att 8.879646 loss_ctc 7.475081 loss_rnnt 2.176014 hw_loss 0.165037 lr 0.00029356 rank 7
2023-02-27 18:48:19,132 DEBUG TRAIN Batch 49/700 loss 3.881178 loss_att 8.528699 loss_ctc 4.742726 loss_rnnt 2.685771 hw_loss 0.283179 lr 0.00029356 rank 6
2023-02-27 18:49:24,272 DEBUG TRAIN Batch 49/800 loss 6.216752 loss_att 9.895475 loss_ctc 14.790545 loss_rnnt 4.117887 hw_loss 0.412401 lr 0.00029355 rank 6
2023-02-27 18:49:24,282 DEBUG TRAIN Batch 49/800 loss 1.950150 loss_att 5.104540 loss_ctc 5.715392 loss_rnnt 0.684352 hw_loss 0.249165 lr 0.00029355 rank 0
2023-02-27 18:49:24,283 DEBUG TRAIN Batch 49/800 loss 4.915704 loss_att 7.251245 loss_ctc 8.022764 loss_rnnt 3.896106 hw_loss 0.259151 lr 0.00029355 rank 4
2023-02-27 18:49:24,283 DEBUG TRAIN Batch 49/800 loss 7.249059 loss_att 8.742677 loss_ctc 10.369267 loss_rnnt 6.445398 hw_loss 0.166704 lr 0.00029354 rank 3
2023-02-27 18:49:24,285 DEBUG TRAIN Batch 49/800 loss 2.810813 loss_att 6.513658 loss_ctc 4.732223 loss_rnnt 1.747836 hw_loss 0.124164 lr 0.00029354 rank 7
2023-02-27 18:49:24,291 DEBUG TRAIN Batch 49/800 loss 1.748487 loss_att 3.821056 loss_ctc 2.276450 loss_rnnt 1.109499 hw_loss 0.288898 lr 0.00029355 rank 5
2023-02-27 18:49:24,291 DEBUG TRAIN Batch 49/800 loss 6.627907 loss_att 12.110601 loss_ctc 13.421199 loss_rnnt 4.492185 hw_loss 0.250145 lr 0.00029355 rank 2
2023-02-27 18:49:24,339 DEBUG TRAIN Batch 49/800 loss 2.431496 loss_att 4.085906 loss_ctc 1.801041 loss_rnnt 1.974559 hw_loss 0.393965 lr 0.00029355 rank 1
2023-02-27 18:50:02,153 DEBUG TRAIN Batch 49/900 loss 2.322902 loss_att 3.937167 loss_ctc 4.314495 loss_rnnt 1.592248 hw_loss 0.266728 lr 0.00029353 rank 6
2023-02-27 18:50:02,153 DEBUG TRAIN Batch 49/900 loss 3.683729 loss_att 7.376221 loss_ctc 7.203935 loss_rnnt 2.402768 hw_loss 0.137067 lr 0.00029353 rank 0
2023-02-27 18:50:02,164 DEBUG TRAIN Batch 49/900 loss 2.377965 loss_att 5.748359 loss_ctc 2.704558 loss_rnnt 1.548308 hw_loss 0.210061 lr 0.00029353 rank 2
2023-02-27 18:50:02,166 DEBUG TRAIN Batch 49/900 loss 7.920831 loss_att 9.425982 loss_ctc 13.303755 loss_rnnt 6.795283 hw_loss 0.200240 lr 0.00029353 rank 3
2023-02-27 18:50:02,170 DEBUG TRAIN Batch 49/900 loss 7.851301 loss_att 9.815660 loss_ctc 11.954445 loss_rnnt 6.856786 hw_loss 0.102293 lr 0.00029354 rank 4
2023-02-27 18:50:02,170 DEBUG TRAIN Batch 49/900 loss 2.164539 loss_att 5.148790 loss_ctc 6.096988 loss_rnnt 0.926826 hw_loss 0.218503 lr 0.00029353 rank 7
2023-02-27 18:50:02,170 DEBUG TRAIN Batch 49/900 loss 1.089503 loss_att 4.408306 loss_ctc 1.029073 loss_rnnt 0.311730 hw_loss 0.228881 lr 0.00029354 rank 5
2023-02-27 18:50:02,172 DEBUG TRAIN Batch 49/900 loss 3.524301 loss_att 6.670695 loss_ctc 6.944626 loss_rnnt 2.314286 hw_loss 0.233797 lr 0.00029353 rank 1
2023-02-27 18:50:40,080 DEBUG TRAIN Batch 49/1000 loss 6.031876 loss_att 10.106985 loss_ctc 11.469586 loss_rnnt 4.428933 hw_loss 0.117924 lr 0.00029353 rank 5
2023-02-27 18:50:40,082 DEBUG TRAIN Batch 49/1000 loss 5.867217 loss_att 7.434456 loss_ctc 13.196815 loss_rnnt 4.437428 hw_loss 0.260739 lr 0.00029352 rank 2
2023-02-27 18:50:40,082 DEBUG TRAIN Batch 49/1000 loss 10.953712 loss_att 12.547586 loss_ctc 13.932036 loss_rnnt 10.072062 hw_loss 0.310809 lr 0.00029352 rank 3
2023-02-27 18:50:40,097 DEBUG TRAIN Batch 49/1000 loss 4.486336 loss_att 6.890139 loss_ctc 12.239193 loss_rnnt 2.903949 hw_loss 0.127336 lr 0.00029352 rank 1
2023-02-27 18:50:40,100 DEBUG TRAIN Batch 49/1000 loss 6.801884 loss_att 9.667529 loss_ctc 7.858416 loss_rnnt 6.020140 hw_loss 0.127020 lr 0.00029352 rank 4
2023-02-27 18:50:40,100 DEBUG TRAIN Batch 49/1000 loss 4.056507 loss_att 6.225292 loss_ctc 5.894170 loss_rnnt 3.267894 hw_loss 0.205939 lr 0.00029352 rank 0
2023-02-27 18:50:40,114 DEBUG TRAIN Batch 49/1000 loss 4.689779 loss_att 7.045684 loss_ctc 8.772871 loss_rnnt 3.619413 hw_loss 0.102699 lr 0.00029352 rank 6
2023-02-27 18:50:40,141 DEBUG TRAIN Batch 49/1000 loss 4.242975 loss_att 6.120648 loss_ctc 8.472751 loss_rnnt 3.258842 hw_loss 0.083677 lr 0.00029352 rank 7
2023-02-27 18:51:44,956 DEBUG TRAIN Batch 49/1100 loss 11.589206 loss_att 10.829926 loss_ctc 13.812679 loss_rnnt 11.335680 hw_loss 0.204222 lr 0.00029351 rank 6
2023-02-27 18:51:44,959 DEBUG TRAIN Batch 49/1100 loss 12.491676 loss_att 18.723640 loss_ctc 22.114872 loss_rnnt 9.874471 hw_loss 0.164475 lr 0.00029351 rank 5
2023-02-27 18:51:44,960 DEBUG TRAIN Batch 49/1100 loss 5.008882 loss_att 7.186715 loss_ctc 6.513688 loss_rnnt 4.304394 hw_loss 0.128026 lr 0.00029350 rank 7
2023-02-27 18:51:44,963 DEBUG TRAIN Batch 49/1100 loss 6.560656 loss_att 8.794651 loss_ctc 9.846793 loss_rnnt 5.640133 hw_loss 0.066697 lr 0.00029351 rank 2
2023-02-27 18:51:44,965 DEBUG TRAIN Batch 49/1100 loss 9.975500 loss_att 14.864689 loss_ctc 21.323666 loss_rnnt 7.391031 hw_loss 0.175393 lr 0.00029351 rank 4
2023-02-27 18:51:44,965 DEBUG TRAIN Batch 49/1100 loss 8.711964 loss_att 10.614134 loss_ctc 16.899998 loss_rnnt 7.062681 hw_loss 0.332081 lr 0.00029351 rank 3
2023-02-27 18:51:44,964 DEBUG TRAIN Batch 49/1100 loss 9.540087 loss_att 14.027288 loss_ctc 15.888712 loss_rnnt 7.653143 hw_loss 0.268161 lr 0.00029351 rank 0
2023-02-27 18:51:44,966 DEBUG TRAIN Batch 49/1100 loss 3.903726 loss_att 4.949558 loss_ctc 6.028480 loss_rnnt 3.239075 hw_loss 0.322844 lr 0.00029351 rank 1
2023-02-27 18:52:23,394 DEBUG TRAIN Batch 49/1200 loss 3.533501 loss_att 5.494835 loss_ctc 6.423845 loss_rnnt 2.662494 hw_loss 0.175053 lr 0.00029350 rank 5
2023-02-27 18:52:23,399 DEBUG TRAIN Batch 49/1200 loss 4.470445 loss_att 6.653098 loss_ctc 6.703710 loss_rnnt 3.597689 hw_loss 0.259607 lr 0.00029350 rank 6
2023-02-27 18:52:23,399 DEBUG TRAIN Batch 49/1200 loss 7.955992 loss_att 9.648760 loss_ctc 15.826297 loss_rnnt 6.384597 hw_loss 0.344000 lr 0.00029349 rank 3
2023-02-27 18:52:23,401 DEBUG TRAIN Batch 49/1200 loss 5.664010 loss_att 6.103626 loss_ctc 6.329854 loss_rnnt 5.318485 hw_loss 0.316540 lr 0.00029350 rank 0
2023-02-27 18:52:23,402 DEBUG TRAIN Batch 49/1200 loss 10.131666 loss_att 11.402472 loss_ctc 15.562422 loss_rnnt 8.962342 hw_loss 0.358240 lr 0.00029349 rank 7
2023-02-27 18:52:23,402 DEBUG TRAIN Batch 49/1200 loss 5.977171 loss_att 8.197071 loss_ctc 9.055561 loss_rnnt 4.981318 hw_loss 0.265164 lr 0.00029350 rank 1
2023-02-27 18:52:23,404 DEBUG TRAIN Batch 49/1200 loss 2.820093 loss_att 4.691126 loss_ctc 3.793264 loss_rnnt 2.134184 hw_loss 0.341150 lr 0.00029350 rank 4
2023-02-27 18:52:23,451 DEBUG TRAIN Batch 49/1200 loss 4.140334 loss_att 7.072934 loss_ctc 5.673530 loss_rnnt 3.231534 hw_loss 0.220976 lr 0.00029350 rank 2
2023-02-27 18:53:01,776 DEBUG TRAIN Batch 49/1300 loss 4.382973 loss_att 5.969602 loss_ctc 8.894859 loss_rnnt 3.245685 hw_loss 0.409459 lr 0.00029349 rank 4
2023-02-27 18:53:01,776 DEBUG TRAIN Batch 49/1300 loss 5.033634 loss_att 8.000175 loss_ctc 6.290039 loss_rnnt 4.149449 hw_loss 0.231293 lr 0.00029348 rank 3
2023-02-27 18:53:01,778 DEBUG TRAIN Batch 49/1300 loss 6.091802 loss_att 7.284465 loss_ctc 7.550532 loss_rnnt 5.628556 hw_loss 0.056654 lr 0.00029348 rank 0
2023-02-27 18:53:01,793 DEBUG TRAIN Batch 49/1300 loss 5.747854 loss_att 10.790252 loss_ctc 7.302713 loss_rnnt 4.442335 hw_loss 0.168235 lr 0.00029348 rank 1
2023-02-27 18:53:01,795 DEBUG TRAIN Batch 49/1300 loss 3.883708 loss_att 4.982534 loss_ctc 7.526579 loss_rnnt 2.969073 hw_loss 0.392164 lr 0.00029348 rank 6
2023-02-27 18:53:01,797 DEBUG TRAIN Batch 49/1300 loss 8.020791 loss_att 11.429885 loss_ctc 12.868557 loss_rnnt 6.556385 hw_loss 0.255410 lr 0.00029348 rank 7
2023-02-27 18:53:01,798 DEBUG TRAIN Batch 49/1300 loss 5.960226 loss_att 7.831096 loss_ctc 7.684790 loss_rnnt 5.159975 hw_loss 0.367754 lr 0.00029348 rank 2
2023-02-27 18:53:01,842 DEBUG TRAIN Batch 49/1300 loss 6.809850 loss_att 10.205215 loss_ctc 12.413904 loss_rnnt 5.304466 hw_loss 0.148318 lr 0.00029349 rank 5
2023-02-27 18:53:41,473 DEBUG TRAIN Batch 49/1400 loss 9.712749 loss_att 16.774628 loss_ctc 19.653763 loss_rnnt 6.875854 hw_loss 0.185716 lr 0.00029347 rank 3
2023-02-27 18:53:41,475 DEBUG TRAIN Batch 49/1400 loss 7.188301 loss_att 10.480298 loss_ctc 14.009754 loss_rnnt 5.498030 hw_loss 0.229395 lr 0.00029347 rank 1
2023-02-27 18:53:41,488 DEBUG TRAIN Batch 49/1400 loss 1.465451 loss_att 5.012245 loss_ctc 1.833362 loss_rnnt 0.627676 hw_loss 0.148802 lr 0.00029347 rank 7
2023-02-27 18:53:41,491 DEBUG TRAIN Batch 49/1400 loss 4.858999 loss_att 7.282157 loss_ctc 8.799821 loss_rnnt 3.728340 hw_loss 0.226098 lr 0.00029347 rank 4
2023-02-27 18:53:41,494 DEBUG TRAIN Batch 49/1400 loss 5.582622 loss_att 10.228080 loss_ctc 11.642741 loss_rnnt 3.675868 hw_loss 0.318087 lr 0.00029347 rank 2
2023-02-27 18:53:41,495 DEBUG TRAIN Batch 49/1400 loss 5.735925 loss_att 7.653929 loss_ctc 10.195398 loss_rnnt 4.630568 hw_loss 0.238426 lr 0.00029347 rank 0
2023-02-27 18:53:41,496 DEBUG TRAIN Batch 49/1400 loss 2.778972 loss_att 5.975052 loss_ctc 4.125767 loss_rnnt 1.876726 hw_loss 0.156482 lr 0.00029347 rank 6
2023-02-27 18:53:41,508 DEBUG TRAIN Batch 49/1400 loss 7.411121 loss_att 11.093637 loss_ctc 11.595825 loss_rnnt 5.933214 hw_loss 0.343954 lr 0.00029347 rank 5
2023-02-27 18:54:45,799 DEBUG TRAIN Batch 49/1500 loss 4.496618 loss_att 7.402212 loss_ctc 7.213950 loss_rnnt 3.432487 hw_loss 0.226315 lr 0.00029346 rank 0
2023-02-27 18:54:45,824 DEBUG TRAIN Batch 49/1500 loss 4.513308 loss_att 8.187641 loss_ctc 7.402628 loss_rnnt 3.320238 hw_loss 0.136801 lr 0.00029345 rank 7
2023-02-27 18:54:45,825 DEBUG TRAIN Batch 49/1500 loss 3.866657 loss_att 6.555348 loss_ctc 5.585783 loss_rnnt 2.973511 hw_loss 0.236607 lr 0.00029346 rank 6
2023-02-27 18:54:45,827 DEBUG TRAIN Batch 49/1500 loss 4.635106 loss_att 8.779634 loss_ctc 8.187542 loss_rnnt 3.266520 hw_loss 0.123792 lr 0.00029346 rank 5
2023-02-27 18:54:45,830 DEBUG TRAIN Batch 49/1500 loss 2.385067 loss_att 6.144953 loss_ctc 2.626045 loss_rnnt 1.586489 hw_loss 0.027133 lr 0.00029346 rank 3
2023-02-27 18:54:45,831 DEBUG TRAIN Batch 49/1500 loss 3.793268 loss_att 6.026209 loss_ctc 5.463957 loss_rnnt 3.082848 hw_loss 0.077012 lr 0.00029346 rank 1
2023-02-27 18:54:45,831 DEBUG TRAIN Batch 49/1500 loss 5.287664 loss_att 9.166390 loss_ctc 10.218532 loss_rnnt 3.657995 hw_loss 0.368390 lr 0.00029346 rank 2
2023-02-27 18:54:45,840 DEBUG TRAIN Batch 49/1500 loss 7.107382 loss_att 6.427847 loss_ctc 6.354988 loss_rnnt 7.228337 hw_loss 0.216131 lr 0.00029346 rank 4
2023-02-27 18:55:23,723 DEBUG TRAIN Batch 49/1600 loss 8.580412 loss_att 11.292254 loss_ctc 15.129524 loss_rnnt 7.092609 hw_loss 0.135412 lr 0.00029345 rank 0
2023-02-27 18:55:23,725 DEBUG TRAIN Batch 49/1600 loss 2.985753 loss_att 4.626319 loss_ctc 4.613387 loss_rnnt 2.332355 hw_loss 0.203001 lr 0.00029345 rank 6
2023-02-27 18:55:23,750 DEBUG TRAIN Batch 49/1600 loss 4.545162 loss_att 6.440377 loss_ctc 8.738702 loss_rnnt 3.472999 hw_loss 0.251216 lr 0.00029344 rank 7
2023-02-27 18:55:23,751 DEBUG TRAIN Batch 49/1600 loss 6.003173 loss_att 8.258604 loss_ctc 8.370051 loss_rnnt 5.216671 hw_loss 0.037185 lr 0.00029344 rank 1
2023-02-27 18:55:23,752 DEBUG TRAIN Batch 49/1600 loss 12.149240 loss_att 16.764708 loss_ctc 27.291494 loss_rnnt 9.112489 hw_loss 0.177544 lr 0.00029345 rank 4
2023-02-27 18:55:23,754 DEBUG TRAIN Batch 49/1600 loss 10.467422 loss_att 12.152753 loss_ctc 14.255484 loss_rnnt 9.539661 hw_loss 0.160534 lr 0.00029345 rank 2
2023-02-27 18:55:23,759 DEBUG TRAIN Batch 49/1600 loss 6.425879 loss_att 10.547232 loss_ctc 13.841469 loss_rnnt 4.464005 hw_loss 0.279112 lr 0.00029344 rank 3
2023-02-27 18:55:23,802 DEBUG TRAIN Batch 49/1600 loss 5.291106 loss_att 8.525930 loss_ctc 8.977991 loss_rnnt 4.004171 hw_loss 0.278221 lr 0.00029345 rank 5
2023-02-27 18:56:02,308 DEBUG TRAIN Batch 49/1700 loss 8.038152 loss_att 10.882656 loss_ctc 10.367726 loss_rnnt 7.001840 hw_loss 0.294001 lr 0.00029343 rank 3
2023-02-27 18:56:02,316 DEBUG TRAIN Batch 49/1700 loss 7.750053 loss_att 10.184467 loss_ctc 13.366741 loss_rnnt 6.343236 hw_loss 0.320703 lr 0.00029343 rank 4
2023-02-27 18:56:02,318 DEBUG TRAIN Batch 49/1700 loss 7.452586 loss_att 10.344048 loss_ctc 9.349100 loss_rnnt 6.538542 hw_loss 0.155406 lr 0.00029344 rank 5
2023-02-27 18:56:02,320 DEBUG TRAIN Batch 49/1700 loss 8.542145 loss_att 9.773802 loss_ctc 10.995707 loss_rnnt 7.868800 hw_loss 0.187260 lr 0.00029343 rank 7
2023-02-27 18:56:02,322 DEBUG TRAIN Batch 49/1700 loss 5.431838 loss_att 7.585705 loss_ctc 9.202146 loss_rnnt 4.378236 hw_loss 0.225225 lr 0.00029343 rank 0
2023-02-27 18:56:02,326 DEBUG TRAIN Batch 49/1700 loss 6.404245 loss_att 10.530696 loss_ctc 12.117240 loss_rnnt 4.690166 hw_loss 0.238230 lr 0.00029343 rank 1
2023-02-27 18:56:02,333 DEBUG TRAIN Batch 49/1700 loss 7.216906 loss_att 10.831426 loss_ctc 13.026743 loss_rnnt 5.668066 hw_loss 0.096169 lr 0.00029343 rank 2
2023-02-27 18:56:02,338 DEBUG TRAIN Batch 49/1700 loss 3.531268 loss_att 5.584116 loss_ctc 4.591686 loss_rnnt 2.851547 hw_loss 0.239555 lr 0.00029343 rank 6
2023-02-27 18:57:10,085 DEBUG TRAIN Batch 49/1800 loss 3.516169 loss_att 6.390214 loss_ctc 7.083420 loss_rnnt 2.389579 hw_loss 0.142775 lr 0.00029342 rank 5
2023-02-27 18:57:10,085 DEBUG TRAIN Batch 49/1800 loss 5.596105 loss_att 6.888183 loss_ctc 7.527530 loss_rnnt 4.895494 hw_loss 0.346261 lr 0.00029342 rank 1
2023-02-27 18:57:10,086 DEBUG TRAIN Batch 49/1800 loss 6.491157 loss_att 7.617931 loss_ctc 11.195853 loss_rnnt 5.430765 hw_loss 0.389522 lr 0.00029342 rank 7
2023-02-27 18:57:10,087 DEBUG TRAIN Batch 49/1800 loss 7.467096 loss_att 9.279304 loss_ctc 13.068976 loss_rnnt 6.220422 hw_loss 0.257466 lr 0.00029342 rank 0
2023-02-27 18:57:10,087 DEBUG TRAIN Batch 49/1800 loss 3.336720 loss_att 4.512311 loss_ctc 7.357551 loss_rnnt 2.435937 hw_loss 0.242913 lr 0.00029342 rank 6
2023-02-27 18:57:10,087 DEBUG TRAIN Batch 49/1800 loss 6.634665 loss_att 9.670464 loss_ctc 12.120272 loss_rnnt 5.145469 hw_loss 0.282416 lr 0.00029342 rank 3
2023-02-27 18:57:10,088 DEBUG TRAIN Batch 49/1800 loss 9.360155 loss_att 14.375719 loss_ctc 14.620111 loss_rnnt 7.508255 hw_loss 0.276486 lr 0.00029342 rank 2
2023-02-27 18:57:10,091 DEBUG TRAIN Batch 49/1800 loss 9.351703 loss_att 11.366890 loss_ctc 17.891474 loss_rnnt 7.605343 hw_loss 0.383788 lr 0.00029342 rank 4
2023-02-27 18:57:48,090 DEBUG TRAIN Batch 49/1900 loss 3.208601 loss_att 5.997920 loss_ctc 5.556044 loss_rnnt 2.244338 hw_loss 0.175137 lr 0.00029341 rank 3
2023-02-27 18:57:48,095 DEBUG TRAIN Batch 49/1900 loss 4.773268 loss_att 6.728350 loss_ctc 8.606707 loss_rnnt 3.831686 hw_loss 0.073951 lr 0.00029341 rank 4
2023-02-27 18:57:48,116 DEBUG TRAIN Batch 49/1900 loss 7.457068 loss_att 10.784614 loss_ctc 11.127938 loss_rnnt 6.078196 hw_loss 0.419838 lr 0.00029341 rank 6
2023-02-27 18:57:48,117 DEBUG TRAIN Batch 49/1900 loss 8.811607 loss_att 8.639087 loss_ctc 13.246554 loss_rnnt 8.114645 hw_loss 0.262764 lr 0.00029341 rank 0
2023-02-27 18:57:48,118 DEBUG TRAIN Batch 49/1900 loss 9.219695 loss_att 10.167934 loss_ctc 13.052025 loss_rnnt 8.381884 hw_loss 0.257224 lr 0.00029341 rank 2
2023-02-27 18:57:48,119 DEBUG TRAIN Batch 49/1900 loss 4.575158 loss_att 4.306247 loss_ctc 6.238282 loss_rnnt 4.249016 hw_loss 0.296576 lr 0.00029341 rank 5
2023-02-27 18:57:48,134 DEBUG TRAIN Batch 49/1900 loss 3.303995 loss_att 7.382180 loss_ctc 4.957197 loss_rnnt 2.123037 hw_loss 0.271675 lr 0.00029340 rank 7
2023-02-27 18:57:48,137 DEBUG TRAIN Batch 49/1900 loss 3.816776 loss_att 8.065594 loss_ctc 8.766865 loss_rnnt 2.212045 hw_loss 0.178043 lr 0.00029341 rank 1
2023-02-27 18:58:26,074 DEBUG TRAIN Batch 49/2000 loss 10.688030 loss_att 13.534098 loss_ctc 21.930767 loss_rnnt 8.541523 hw_loss 0.146742 lr 0.00029340 rank 6
2023-02-27 18:58:26,079 DEBUG TRAIN Batch 49/2000 loss 5.562380 loss_att 8.341928 loss_ctc 7.868607 loss_rnnt 4.639526 hw_loss 0.111464 lr 0.00029339 rank 3
2023-02-27 18:58:26,079 DEBUG TRAIN Batch 49/2000 loss 6.964021 loss_att 8.668783 loss_ctc 7.117835 loss_rnnt 6.477432 hw_loss 0.234613 lr 0.00029339 rank 7
2023-02-27 18:58:26,079 DEBUG TRAIN Batch 49/2000 loss 3.249840 loss_att 3.754172 loss_ctc 6.652854 loss_rnnt 2.457452 hw_loss 0.445848 lr 0.00029340 rank 2
2023-02-27 18:58:26,082 DEBUG TRAIN Batch 49/2000 loss 5.466057 loss_att 7.466399 loss_ctc 8.783854 loss_rnnt 4.465011 hw_loss 0.297384 lr 0.00029340 rank 5
2023-02-27 18:58:26,084 DEBUG TRAIN Batch 49/2000 loss 5.185740 loss_att 10.024670 loss_ctc 10.446533 loss_rnnt 3.485200 hw_loss 0.058717 lr 0.00029339 rank 1
2023-02-27 18:58:26,103 DEBUG TRAIN Batch 49/2000 loss 2.774846 loss_att 5.669673 loss_ctc 4.723583 loss_rnnt 1.866044 hw_loss 0.131258 lr 0.00029340 rank 4
2023-02-27 18:58:26,112 DEBUG TRAIN Batch 49/2000 loss 8.349395 loss_att 12.735276 loss_ctc 15.491850 loss_rnnt 6.423759 hw_loss 0.180248 lr 0.00029340 rank 0
2023-02-27 18:59:06,026 DEBUG TRAIN Batch 49/2100 loss 7.500773 loss_att 10.677170 loss_ctc 16.401905 loss_rnnt 5.519979 hw_loss 0.297558 lr 0.00029338 rank 1
2023-02-27 18:59:06,038 DEBUG TRAIN Batch 49/2100 loss 6.078831 loss_att 10.888557 loss_ctc 14.800639 loss_rnnt 3.784760 hw_loss 0.317284 lr 0.00029338 rank 6
2023-02-27 18:59:06,038 DEBUG TRAIN Batch 49/2100 loss 10.160824 loss_att 12.186388 loss_ctc 17.198235 loss_rnnt 8.759323 hw_loss 0.108874 lr 0.00029338 rank 3
2023-02-27 18:59:06,041 DEBUG TRAIN Batch 49/2100 loss 6.459381 loss_att 10.411193 loss_ctc 10.335209 loss_rnnt 5.076356 hw_loss 0.142285 lr 0.00029338 rank 4
2023-02-27 18:59:06,042 DEBUG TRAIN Batch 49/2100 loss 1.851220 loss_att 4.711633 loss_ctc 2.222374 loss_rnnt 1.106282 hw_loss 0.231316 lr 0.00029338 rank 7
2023-02-27 18:59:06,044 DEBUG TRAIN Batch 49/2100 loss 3.634365 loss_att 7.039601 loss_ctc 4.498553 loss_rnnt 2.785706 hw_loss 0.098225 lr 0.00029339 rank 5
2023-02-27 18:59:06,045 DEBUG TRAIN Batch 49/2100 loss 5.326848 loss_att 8.462378 loss_ctc 10.086699 loss_rnnt 3.962169 hw_loss 0.192986 lr 0.00029338 rank 2
2023-02-27 18:59:06,049 DEBUG TRAIN Batch 49/2100 loss 4.233005 loss_att 6.310189 loss_ctc 5.975062 loss_rnnt 3.398777 hw_loss 0.349721 lr 0.00029338 rank 0
2023-02-27 19:00:11,559 DEBUG TRAIN Batch 49/2200 loss 2.494310 loss_att 5.947190 loss_ctc 6.043833 loss_rnnt 1.212168 hw_loss 0.221806 lr 0.00029337 rank 3
2023-02-27 19:00:11,559 DEBUG TRAIN Batch 49/2200 loss 13.803236 loss_att 14.448475 loss_ctc 23.317814 loss_rnnt 12.323478 hw_loss 0.153938 lr 0.00029337 rank 0
2023-02-27 19:00:11,563 DEBUG TRAIN Batch 49/2200 loss 2.642374 loss_att 6.514834 loss_ctc 4.171523 loss_rnnt 1.547106 hw_loss 0.219167 lr 0.00029337 rank 6
2023-02-27 19:00:11,580 DEBUG TRAIN Batch 49/2200 loss 5.570122 loss_att 8.512268 loss_ctc 9.700529 loss_rnnt 4.288052 hw_loss 0.267975 lr 0.00029337 rank 4
2023-02-27 19:00:11,581 DEBUG TRAIN Batch 49/2200 loss 3.808167 loss_att 6.985168 loss_ctc 9.325075 loss_rnnt 2.317514 hw_loss 0.224370 lr 0.00029337 rank 5
2023-02-27 19:00:11,585 DEBUG TRAIN Batch 49/2200 loss 11.023912 loss_att 13.950451 loss_ctc 23.659948 loss_rnnt 8.647017 hw_loss 0.200218 lr 0.00029337 rank 2
2023-02-27 19:00:11,586 DEBUG TRAIN Batch 49/2200 loss 7.899572 loss_att 10.537691 loss_ctc 13.467622 loss_rnnt 6.518263 hw_loss 0.208647 lr 0.00029337 rank 1
2023-02-27 19:00:11,587 DEBUG TRAIN Batch 49/2200 loss 6.436893 loss_att 9.695472 loss_ctc 11.274376 loss_rnnt 4.970373 hw_loss 0.318387 lr 0.00029337 rank 7
2023-02-27 19:00:50,044 DEBUG TRAIN Batch 49/2300 loss 5.216739 loss_att 7.488291 loss_ctc 7.180903 loss_rnnt 4.306273 hw_loss 0.364252 lr 0.00029336 rank 0
2023-02-27 19:00:50,046 DEBUG TRAIN Batch 49/2300 loss 5.176474 loss_att 8.092248 loss_ctc 9.046362 loss_rnnt 3.959274 hw_loss 0.221362 lr 0.00029336 rank 5
2023-02-27 19:00:50,055 DEBUG TRAIN Batch 49/2300 loss 4.092787 loss_att 6.803936 loss_ctc 6.232253 loss_rnnt 3.158491 hw_loss 0.200257 lr 0.00029336 rank 3
2023-02-27 19:00:50,060 DEBUG TRAIN Batch 49/2300 loss 4.258241 loss_att 6.587610 loss_ctc 6.181584 loss_rnnt 3.390322 hw_loss 0.272999 lr 0.00029336 rank 2
2023-02-27 19:00:50,058 DEBUG TRAIN Batch 49/2300 loss 2.269920 loss_att 4.651510 loss_ctc 5.725697 loss_rnnt 1.332558 hw_loss 0.000513 lr 0.00029336 rank 6
2023-02-27 19:00:50,063 DEBUG TRAIN Batch 49/2300 loss 7.761907 loss_att 11.119619 loss_ctc 17.211214 loss_rnnt 5.741236 hw_loss 0.167290 lr 0.00029336 rank 1
2023-02-27 19:00:50,062 DEBUG TRAIN Batch 49/2300 loss 5.768687 loss_att 8.003970 loss_ctc 9.607058 loss_rnnt 4.641863 hw_loss 0.314969 lr 0.00029336 rank 4
2023-02-27 19:00:50,103 DEBUG TRAIN Batch 49/2300 loss 3.920626 loss_att 6.178136 loss_ctc 8.487775 loss_rnnt 2.817897 hw_loss 0.079262 lr 0.00029335 rank 7
2023-02-27 19:01:28,414 DEBUG TRAIN Batch 49/2400 loss 4.035869 loss_att 6.476476 loss_ctc 8.247881 loss_rnnt 2.855419 hw_loss 0.245115 lr 0.00029334 rank 7
2023-02-27 19:01:28,418 DEBUG TRAIN Batch 49/2400 loss 13.953559 loss_att 15.445337 loss_ctc 17.531570 loss_rnnt 13.147788 hw_loss 0.056904 lr 0.00029334 rank 2
2023-02-27 19:01:28,432 DEBUG TRAIN Batch 49/2400 loss 3.430016 loss_att 6.670724 loss_ctc 5.094990 loss_rnnt 2.383285 hw_loss 0.331112 lr 0.00029334 rank 6
2023-02-27 19:01:28,434 DEBUG TRAIN Batch 49/2400 loss 3.240381 loss_att 4.419510 loss_ctc 6.540479 loss_rnnt 2.442477 hw_loss 0.228873 lr 0.00029335 rank 0
2023-02-27 19:01:28,435 DEBUG TRAIN Batch 49/2400 loss 7.936308 loss_att 9.799245 loss_ctc 11.641677 loss_rnnt 6.966882 hw_loss 0.192730 lr 0.00029334 rank 3
2023-02-27 19:01:28,435 DEBUG TRAIN Batch 49/2400 loss 4.416388 loss_att 5.288098 loss_ctc 6.684812 loss_rnnt 3.747889 hw_loss 0.359436 lr 0.00029335 rank 5
2023-02-27 19:01:28,438 DEBUG TRAIN Batch 49/2400 loss 7.387194 loss_att 10.833456 loss_ctc 12.295424 loss_rnnt 5.937866 hw_loss 0.198086 lr 0.00029334 rank 1
2023-02-27 19:01:28,445 DEBUG TRAIN Batch 49/2400 loss 6.442804 loss_att 9.545321 loss_ctc 13.886915 loss_rnnt 4.708439 hw_loss 0.227463 lr 0.00029335 rank 4
2023-02-27 19:02:37,656 DEBUG TRAIN Batch 49/2500 loss 8.425233 loss_att 9.492794 loss_ctc 15.504503 loss_rnnt 7.131473 hw_loss 0.255647 lr 0.00029333 rank 4
2023-02-27 19:02:37,657 DEBUG TRAIN Batch 49/2500 loss 3.431432 loss_att 6.105354 loss_ctc 6.888805 loss_rnnt 2.303897 hw_loss 0.247063 lr 0.00029333 rank 7
2023-02-27 19:02:37,658 DEBUG TRAIN Batch 49/2500 loss 7.667114 loss_att 8.432646 loss_ctc 14.094836 loss_rnnt 6.506255 hw_loss 0.282605 lr 0.00029334 rank 5
2023-02-27 19:02:37,658 DEBUG TRAIN Batch 49/2500 loss 7.102407 loss_att 9.659879 loss_ctc 12.248055 loss_rnnt 5.778768 hw_loss 0.236359 lr 0.00029333 rank 2
2023-02-27 19:02:37,661 DEBUG TRAIN Batch 49/2500 loss 4.410906 loss_att 4.637355 loss_ctc 7.452448 loss_rnnt 3.750058 hw_loss 0.393784 lr 0.00029333 rank 3
2023-02-27 19:02:37,662 DEBUG TRAIN Batch 49/2500 loss 5.386430 loss_att 6.627973 loss_ctc 8.241104 loss_rnnt 4.583265 hw_loss 0.326686 lr 0.00029333 rank 6
2023-02-27 19:02:37,664 DEBUG TRAIN Batch 49/2500 loss 9.499503 loss_att 9.863083 loss_ctc 13.346400 loss_rnnt 8.720271 hw_loss 0.362996 lr 0.00029333 rank 1
2023-02-27 19:02:37,665 DEBUG TRAIN Batch 49/2500 loss 5.101132 loss_att 6.917576 loss_ctc 8.385005 loss_rnnt 4.164027 hw_loss 0.254938 lr 0.00029333 rank 0
2023-02-27 19:03:15,544 DEBUG TRAIN Batch 49/2600 loss 5.717619 loss_att 7.634546 loss_ctc 9.557249 loss_rnnt 4.631577 hw_loss 0.357577 lr 0.00029332 rank 6
2023-02-27 19:03:15,545 DEBUG TRAIN Batch 49/2600 loss 3.199704 loss_att 6.216945 loss_ctc 6.225539 loss_rnnt 2.064391 hw_loss 0.240788 lr 0.00029332 rank 7
2023-02-27 19:03:15,545 DEBUG TRAIN Batch 49/2600 loss 7.369831 loss_att 11.830521 loss_ctc 19.934984 loss_rnnt 4.712731 hw_loss 0.168014 lr 0.00029332 rank 0
2023-02-27 19:03:15,547 DEBUG TRAIN Batch 49/2600 loss 2.999779 loss_att 4.618915 loss_ctc 3.223307 loss_rnnt 2.503595 hw_loss 0.267286 lr 0.00029332 rank 3
2023-02-27 19:03:15,548 DEBUG TRAIN Batch 49/2600 loss 6.606171 loss_att 10.087364 loss_ctc 8.890277 loss_rnnt 5.524377 hw_loss 0.151891 lr 0.00029332 rank 5
2023-02-27 19:03:15,548 DEBUG TRAIN Batch 49/2600 loss 4.908719 loss_att 6.895698 loss_ctc 6.953384 loss_rnnt 4.065917 hw_loss 0.323970 lr 0.00029332 rank 2
2023-02-27 19:03:15,552 DEBUG TRAIN Batch 49/2600 loss 4.789453 loss_att 6.903656 loss_ctc 8.074980 loss_rnnt 3.796200 hw_loss 0.248140 lr 0.00029332 rank 1
2023-02-27 19:03:15,566 DEBUG TRAIN Batch 49/2600 loss 8.143920 loss_att 9.041817 loss_ctc 12.097690 loss_rnnt 7.287846 hw_loss 0.279983 lr 0.00029332 rank 4
2023-02-27 19:03:53,451 DEBUG TRAIN Batch 49/2700 loss 3.642713 loss_att 7.127497 loss_ctc 7.580020 loss_rnnt 2.336858 hw_loss 0.157358 lr 0.00029331 rank 4
2023-02-27 19:03:53,453 DEBUG TRAIN Batch 49/2700 loss 5.786262 loss_att 7.954329 loss_ctc 7.955518 loss_rnnt 4.959448 hw_loss 0.194936 lr 0.00029331 rank 6
2023-02-27 19:03:53,453 DEBUG TRAIN Batch 49/2700 loss 6.545431 loss_att 8.255774 loss_ctc 9.660667 loss_rnnt 5.609533 hw_loss 0.334620 lr 0.00029330 rank 3
2023-02-27 19:03:53,469 DEBUG TRAIN Batch 49/2700 loss 7.502843 loss_att 9.741996 loss_ctc 11.667735 loss_rnnt 6.345678 hw_loss 0.288779 lr 0.00029331 rank 0
2023-02-27 19:03:53,472 DEBUG TRAIN Batch 49/2700 loss 3.748156 loss_att 6.181890 loss_ctc 4.906930 loss_rnnt 2.977595 hw_loss 0.242458 lr 0.00029331 rank 2
2023-02-27 19:03:53,476 DEBUG TRAIN Batch 49/2700 loss 1.188955 loss_att 4.442181 loss_ctc 2.463204 loss_rnnt 0.292651 hw_loss 0.142050 lr 0.00029331 rank 1
2023-02-27 19:03:53,476 DEBUG TRAIN Batch 49/2700 loss 6.563837 loss_att 12.871143 loss_ctc 11.953156 loss_rnnt 4.536304 hw_loss 0.089054 lr 0.00029330 rank 7
2023-02-27 19:03:53,476 DEBUG TRAIN Batch 49/2700 loss 3.302482 loss_att 6.024147 loss_ctc 5.592210 loss_rnnt 2.332980 hw_loss 0.224761 lr 0.00029331 rank 5
2023-02-27 19:04:32,933 DEBUG TRAIN Batch 49/2800 loss 7.876483 loss_att 11.021618 loss_ctc 12.152239 loss_rnnt 6.546352 hw_loss 0.245631 lr 0.00029329 rank 3
2023-02-27 19:04:32,934 DEBUG TRAIN Batch 49/2800 loss 5.294005 loss_att 7.243765 loss_ctc 8.827462 loss_rnnt 4.345196 hw_loss 0.164493 lr 0.00029330 rank 5
2023-02-27 19:04:32,938 DEBUG TRAIN Batch 49/2800 loss 2.901585 loss_att 6.217592 loss_ctc 3.554685 loss_rnnt 2.050399 hw_loss 0.189198 lr 0.00029329 rank 2
2023-02-27 19:04:32,939 DEBUG TRAIN Batch 49/2800 loss 2.645349 loss_att 4.259037 loss_ctc 3.905178 loss_rnnt 1.999169 hw_loss 0.291496 lr 0.00029329 rank 7
2023-02-27 19:04:32,942 DEBUG TRAIN Batch 49/2800 loss 3.946631 loss_att 5.604965 loss_ctc 3.802515 loss_rnnt 3.559305 hw_loss 0.140390 lr 0.00029330 rank 4
2023-02-27 19:04:32,942 DEBUG TRAIN Batch 49/2800 loss 5.464655 loss_att 9.807238 loss_ctc 6.300144 loss_rnnt 4.354402 hw_loss 0.244385 lr 0.00029329 rank 1
2023-02-27 19:04:32,943 DEBUG TRAIN Batch 49/2800 loss 5.388658 loss_att 8.185791 loss_ctc 8.722980 loss_rnnt 4.282503 hw_loss 0.191534 lr 0.00029329 rank 6
2023-02-27 19:04:32,947 DEBUG TRAIN Batch 49/2800 loss 2.885917 loss_att 8.419780 loss_ctc 4.376386 loss_rnnt 1.445196 hw_loss 0.253536 lr 0.00029329 rank 0
2023-02-27 19:05:38,558 DEBUG TRAIN Batch 49/2900 loss 2.801642 loss_att 5.543812 loss_ctc 2.886339 loss_rnnt 2.085468 hw_loss 0.293338 lr 0.00029328 rank 4
2023-02-27 19:05:38,572 DEBUG TRAIN Batch 49/2900 loss 7.206378 loss_att 9.382832 loss_ctc 9.774155 loss_rnnt 6.281547 hw_loss 0.275944 lr 0.00029328 rank 7
2023-02-27 19:05:38,572 DEBUG TRAIN Batch 49/2900 loss 7.233878 loss_att 13.671442 loss_ctc 15.109987 loss_rnnt 4.770887 hw_loss 0.234994 lr 0.00029329 rank 5
2023-02-27 19:05:38,574 DEBUG TRAIN Batch 49/2900 loss 7.280009 loss_att 10.199488 loss_ctc 10.689950 loss_rnnt 6.133353 hw_loss 0.202692 lr 0.00029328 rank 0
2023-02-27 19:05:38,574 DEBUG TRAIN Batch 49/2900 loss 4.378985 loss_att 5.437772 loss_ctc 6.030949 loss_rnnt 3.846140 hw_loss 0.189049 lr 0.00029328 rank 3
2023-02-27 19:05:38,574 DEBUG TRAIN Batch 49/2900 loss 5.889213 loss_att 9.463620 loss_ctc 9.134974 loss_rnnt 4.603881 hw_loss 0.258155 lr 0.00029328 rank 6
2023-02-27 19:05:38,599 DEBUG TRAIN Batch 49/2900 loss 4.857537 loss_att 7.034446 loss_ctc 7.815946 loss_rnnt 3.916834 hw_loss 0.207876 lr 0.00029328 rank 2
2023-02-27 19:05:38,603 DEBUG TRAIN Batch 49/2900 loss 4.340246 loss_att 6.881631 loss_ctc 5.463655 loss_rnnt 3.576127 hw_loss 0.198853 lr 0.00029328 rank 1
2023-02-27 19:06:16,462 DEBUG TRAIN Batch 49/3000 loss 5.392531 loss_att 8.609271 loss_ctc 10.264189 loss_rnnt 4.025308 hw_loss 0.139352 lr 0.00029327 rank 0
2023-02-27 19:06:16,481 DEBUG TRAIN Batch 49/3000 loss 7.696249 loss_att 10.623681 loss_ctc 10.401609 loss_rnnt 6.725688 hw_loss 0.045674 lr 0.00029327 rank 3
2023-02-27 19:06:16,485 DEBUG TRAIN Batch 49/3000 loss 5.736333 loss_att 7.517911 loss_ctc 8.598338 loss_rnnt 4.867773 hw_loss 0.244960 lr 0.00029327 rank 6
2023-02-27 19:06:16,486 DEBUG TRAIN Batch 49/3000 loss 4.861409 loss_att 7.625383 loss_ctc 9.186150 loss_rnnt 3.577110 hw_loss 0.290384 lr 0.00029327 rank 5
2023-02-27 19:06:16,487 DEBUG TRAIN Batch 49/3000 loss 6.875471 loss_att 9.132683 loss_ctc 13.593472 loss_rnnt 5.473116 hw_loss 0.103460 lr 0.00029326 rank 7
2023-02-27 19:06:16,489 DEBUG TRAIN Batch 49/3000 loss 7.146881 loss_att 13.208738 loss_ctc 15.159764 loss_rnnt 4.777223 hw_loss 0.166691 lr 0.00029327 rank 4
2023-02-27 19:06:16,489 DEBUG TRAIN Batch 49/3000 loss 6.452641 loss_att 11.906380 loss_ctc 17.682188 loss_rnnt 3.812158 hw_loss 0.098367 lr 0.00029327 rank 1
2023-02-27 19:06:16,509 DEBUG TRAIN Batch 49/3000 loss 4.736991 loss_att 7.373385 loss_ctc 7.455325 loss_rnnt 3.820729 hw_loss 0.049762 lr 0.00029327 rank 2
2023-02-27 19:06:54,998 DEBUG TRAIN Batch 49/3100 loss 10.230874 loss_att 11.678082 loss_ctc 14.783585 loss_rnnt 9.305732 hw_loss 0.053762 lr 0.00029326 rank 5
2023-02-27 19:06:55,000 DEBUG TRAIN Batch 49/3100 loss 6.924755 loss_att 8.446858 loss_ctc 10.551448 loss_rnnt 5.983202 hw_loss 0.287949 lr 0.00029326 rank 6
2023-02-27 19:06:55,001 DEBUG TRAIN Batch 49/3100 loss 6.508708 loss_att 7.790117 loss_ctc 14.051429 loss_rnnt 5.125561 hw_loss 0.227192 lr 0.00029325 rank 7
2023-02-27 19:06:55,001 DEBUG TRAIN Batch 49/3100 loss 6.129888 loss_att 7.771645 loss_ctc 11.747244 loss_rnnt 4.966985 hw_loss 0.160446 lr 0.00029325 rank 3
2023-02-27 19:06:55,001 DEBUG TRAIN Batch 49/3100 loss 9.621520 loss_att 12.583238 loss_ctc 10.170795 loss_rnnt 8.843120 hw_loss 0.211539 lr 0.00029326 rank 4
2023-02-27 19:06:55,002 DEBUG TRAIN Batch 49/3100 loss 4.096032 loss_att 8.508779 loss_ctc 7.773032 loss_rnnt 2.622494 hw_loss 0.188855 lr 0.00029326 rank 2
2023-02-27 19:06:55,003 DEBUG TRAIN Batch 49/3100 loss 9.508296 loss_att 10.348047 loss_ctc 16.294662 loss_rnnt 8.259724 hw_loss 0.329576 lr 0.00029326 rank 1
2023-02-27 19:06:55,005 DEBUG TRAIN Batch 49/3100 loss 2.382532 loss_att 3.877938 loss_ctc 3.068926 loss_rnnt 1.804248 hw_loss 0.351908 lr 0.00029326 rank 0
2023-02-27 19:08:01,291 DEBUG TRAIN Batch 49/3200 loss 3.311676 loss_att 5.369406 loss_ctc 6.992346 loss_rnnt 2.200745 hw_loss 0.391177 lr 0.00029324 rank 2
2023-02-27 19:08:01,292 DEBUG TRAIN Batch 49/3200 loss 9.164418 loss_att 10.304726 loss_ctc 14.094679 loss_rnnt 8.167984 hw_loss 0.208134 lr 0.00029325 rank 4
2023-02-27 19:08:01,319 DEBUG TRAIN Batch 49/3200 loss 5.267587 loss_att 7.389256 loss_ctc 9.517986 loss_rnnt 4.108317 hw_loss 0.315404 lr 0.00029324 rank 0
2023-02-27 19:08:01,326 DEBUG TRAIN Batch 49/3200 loss 12.099092 loss_att 14.386519 loss_ctc 20.137665 loss_rnnt 10.401766 hw_loss 0.315056 lr 0.00029325 rank 5
2023-02-27 19:08:01,330 DEBUG TRAIN Batch 49/3200 loss 4.051156 loss_att 6.876984 loss_ctc 6.933826 loss_rnnt 3.003943 hw_loss 0.183171 lr 0.00029324 rank 3
2023-02-27 19:08:01,335 DEBUG TRAIN Batch 49/3200 loss 4.255717 loss_att 8.166021 loss_ctc 6.624517 loss_rnnt 3.038857 hw_loss 0.223048 lr 0.00029324 rank 7
2023-02-27 19:08:01,335 DEBUG TRAIN Batch 49/3200 loss 6.201110 loss_att 10.754585 loss_ctc 15.774122 loss_rnnt 3.922012 hw_loss 0.172502 lr 0.00029324 rank 1
2023-02-27 19:08:01,375 DEBUG TRAIN Batch 49/3200 loss 4.781487 loss_att 7.490764 loss_ctc 6.575620 loss_rnnt 3.879578 hw_loss 0.226568 lr 0.00029324 rank 6
2023-02-27 19:08:39,363 DEBUG TRAIN Batch 49/3300 loss 11.298040 loss_att 13.431900 loss_ctc 14.879759 loss_rnnt 10.246627 hw_loss 0.275775 lr 0.00029323 rank 4
2023-02-27 19:08:39,378 DEBUG TRAIN Batch 49/3300 loss 7.630779 loss_att 9.385035 loss_ctc 10.904114 loss_rnnt 6.805721 hw_loss 0.070803 lr 0.00029323 rank 3
2023-02-27 19:08:39,379 DEBUG TRAIN Batch 49/3300 loss 2.931326 loss_att 6.203738 loss_ctc 5.692093 loss_rnnt 1.682912 hw_loss 0.423431 lr 0.00029323 rank 0
2023-02-27 19:08:39,383 DEBUG TRAIN Batch 49/3300 loss 3.980584 loss_att 9.836466 loss_ctc 7.655597 loss_rnnt 2.167402 hw_loss 0.285008 lr 0.00029323 rank 1
2023-02-27 19:08:39,384 DEBUG TRAIN Batch 49/3300 loss 5.753467 loss_att 8.426801 loss_ctc 10.500279 loss_rnnt 4.505681 hw_loss 0.150394 lr 0.00029323 rank 6
2023-02-27 19:08:39,388 DEBUG TRAIN Batch 49/3300 loss 10.093301 loss_att 13.282492 loss_ctc 13.390910 loss_rnnt 8.908684 hw_loss 0.200807 lr 0.00029323 rank 5
2023-02-27 19:08:39,388 DEBUG TRAIN Batch 49/3300 loss 6.242288 loss_att 9.401048 loss_ctc 10.106735 loss_rnnt 4.989194 hw_loss 0.198905 lr 0.00029323 rank 2
2023-02-27 19:08:39,390 DEBUG TRAIN Batch 49/3300 loss 2.964132 loss_att 6.084393 loss_ctc 4.198524 loss_rnnt 2.060001 hw_loss 0.216550 lr 0.00029323 rank 7
2023-02-27 19:09:17,547 DEBUG TRAIN Batch 49/3400 loss 4.481688 loss_att 9.243253 loss_ctc 12.327396 loss_rnnt 2.407417 hw_loss 0.142246 lr 0.00029322 rank 3
2023-02-27 19:09:17,548 DEBUG TRAIN Batch 49/3400 loss 4.057578 loss_att 7.265140 loss_ctc 5.579078 loss_rnnt 3.079764 hw_loss 0.250189 lr 0.00029322 rank 5
2023-02-27 19:09:17,550 DEBUG TRAIN Batch 49/3400 loss 14.401099 loss_att 15.704641 loss_ctc 24.455803 loss_rnnt 12.639253 hw_loss 0.300959 lr 0.00029321 rank 7
2023-02-27 19:09:17,553 DEBUG TRAIN Batch 49/3400 loss 7.824300 loss_att 10.055687 loss_ctc 14.710100 loss_rnnt 6.338319 hw_loss 0.227993 lr 0.00029322 rank 6
2023-02-27 19:09:17,552 DEBUG TRAIN Batch 49/3400 loss 6.080918 loss_att 8.322248 loss_ctc 10.150758 loss_rnnt 5.016734 hw_loss 0.137387 lr 0.00029322 rank 4
2023-02-27 19:09:17,553 DEBUG TRAIN Batch 49/3400 loss 5.930157 loss_att 6.543228 loss_ctc 7.421814 loss_rnnt 5.539267 hw_loss 0.130102 lr 0.00029322 rank 0
2023-02-27 19:09:17,558 DEBUG TRAIN Batch 49/3400 loss 5.095824 loss_att 10.279112 loss_ctc 10.050302 loss_rnnt 3.284930 hw_loss 0.213073 lr 0.00029322 rank 2
2023-02-27 19:09:17,604 DEBUG TRAIN Batch 49/3400 loss 7.981766 loss_att 10.788952 loss_ctc 13.088136 loss_rnnt 6.698434 hw_loss 0.076959 lr 0.00029322 rank 1
2023-02-27 19:09:56,484 DEBUG TRAIN Batch 49/3500 loss 2.610820 loss_att 6.469178 loss_ctc 4.350653 loss_rnnt 1.533068 hw_loss 0.138941 lr 0.00029321 rank 0
2023-02-27 19:09:56,492 DEBUG TRAIN Batch 49/3500 loss 7.315462 loss_att 9.877169 loss_ctc 8.868719 loss_rnnt 6.533092 hw_loss 0.117989 lr 0.00029320 rank 7
2023-02-27 19:09:56,492 DEBUG TRAIN Batch 49/3500 loss 8.995074 loss_att 12.629300 loss_ctc 15.009045 loss_rnnt 7.392974 hw_loss 0.137610 lr 0.00029321 rank 2
2023-02-27 19:09:56,496 DEBUG TRAIN Batch 49/3500 loss 5.017520 loss_att 7.848717 loss_ctc 7.802138 loss_rnnt 3.919101 hw_loss 0.301685 lr 0.00029321 rank 5
2023-02-27 19:09:56,497 DEBUG TRAIN Batch 49/3500 loss 3.688496 loss_att 5.940676 loss_ctc 6.162294 loss_rnnt 2.759569 hw_loss 0.278723 lr 0.00029320 rank 1
2023-02-27 19:09:56,499 DEBUG TRAIN Batch 49/3500 loss 7.909802 loss_att 9.968999 loss_ctc 12.698872 loss_rnnt 6.748701 hw_loss 0.207597 lr 0.00029321 rank 4
2023-02-27 19:09:56,504 DEBUG TRAIN Batch 49/3500 loss 6.109650 loss_att 10.307132 loss_ctc 8.980870 loss_rnnt 4.801410 hw_loss 0.161089 lr 0.00029321 rank 6
2023-02-27 19:09:56,506 DEBUG TRAIN Batch 49/3500 loss 5.969735 loss_att 8.891838 loss_ctc 10.706278 loss_rnnt 4.638702 hw_loss 0.215761 lr 0.00029320 rank 3
2023-02-27 19:11:02,655 DEBUG TRAIN Batch 49/3600 loss 5.480850 loss_att 8.008734 loss_ctc 10.127602 loss_rnnt 4.273512 hw_loss 0.154114 lr 0.00029319 rank 0
2023-02-27 19:11:02,665 DEBUG TRAIN Batch 49/3600 loss 8.471441 loss_att 12.450578 loss_ctc 15.261549 loss_rnnt 6.691785 hw_loss 0.147151 lr 0.00029320 rank 5
2023-02-27 19:11:02,667 DEBUG TRAIN Batch 49/3600 loss 5.068002 loss_att 8.111505 loss_ctc 11.006435 loss_rnnt 3.642910 hw_loss 0.046125 lr 0.00029319 rank 6
2023-02-27 19:11:02,670 DEBUG TRAIN Batch 49/3600 loss 2.664825 loss_att 4.453862 loss_ctc 3.701091 loss_rnnt 2.005815 hw_loss 0.305689 lr 0.00029319 rank 7
2023-02-27 19:11:02,671 DEBUG TRAIN Batch 49/3600 loss 3.746219 loss_att 6.473621 loss_ctc 7.726535 loss_rnnt 2.561137 hw_loss 0.204174 lr 0.00029319 rank 2
2023-02-27 19:11:02,672 DEBUG TRAIN Batch 49/3600 loss 8.966303 loss_att 10.163224 loss_ctc 12.148419 loss_rnnt 8.149152 hw_loss 0.287782 lr 0.00029319 rank 3
2023-02-27 19:11:02,675 DEBUG TRAIN Batch 49/3600 loss 5.664558 loss_att 9.782258 loss_ctc 9.119136 loss_rnnt 4.302686 hw_loss 0.145728 lr 0.00029319 rank 4
2023-02-27 19:11:02,677 DEBUG TRAIN Batch 49/3600 loss 3.777066 loss_att 7.415348 loss_ctc 8.179424 loss_rnnt 2.303833 hw_loss 0.297368 lr 0.00029319 rank 1
2023-02-27 19:11:41,099 DEBUG TRAIN Batch 49/3700 loss 7.179976 loss_att 8.628083 loss_ctc 10.103202 loss_rnnt 6.420106 hw_loss 0.150910 lr 0.00029318 rank 6
2023-02-27 19:11:41,110 DEBUG TRAIN Batch 49/3700 loss 8.324398 loss_att 11.346252 loss_ctc 15.271622 loss_rnnt 6.718449 hw_loss 0.141153 lr 0.00029318 rank 0
2023-02-27 19:11:41,111 DEBUG TRAIN Batch 49/3700 loss 4.885077 loss_att 7.904714 loss_ctc 10.124522 loss_rnnt 3.466907 hw_loss 0.216845 lr 0.00029318 rank 7
2023-02-27 19:11:41,111 DEBUG TRAIN Batch 49/3700 loss 1.359604 loss_att 3.673615 loss_ctc 2.852096 loss_rnnt 0.503516 hw_loss 0.364287 lr 0.00029318 rank 5
2023-02-27 19:11:41,112 DEBUG TRAIN Batch 49/3700 loss 5.357056 loss_att 8.402986 loss_ctc 12.633377 loss_rnnt 3.616293 hw_loss 0.302626 lr 0.00029318 rank 4
2023-02-27 19:11:41,115 DEBUG TRAIN Batch 49/3700 loss 3.405670 loss_att 7.121153 loss_ctc 8.496663 loss_rnnt 1.860026 hw_loss 0.232027 lr 0.00029318 rank 1
2023-02-27 19:11:41,119 DEBUG TRAIN Batch 49/3700 loss 4.489906 loss_att 7.364773 loss_ctc 8.104997 loss_rnnt 3.326465 hw_loss 0.199604 lr 0.00029318 rank 3
2023-02-27 19:11:41,163 DEBUG TRAIN Batch 49/3700 loss 6.450783 loss_att 11.321608 loss_ctc 11.547477 loss_rnnt 4.688273 hw_loss 0.203972 lr 0.00029318 rank 2
2023-02-27 19:12:19,728 DEBUG TRAIN Batch 49/3800 loss 4.488366 loss_att 5.349686 loss_ctc 5.416706 loss_rnnt 4.069631 hw_loss 0.230048 lr 0.00029317 rank 3
2023-02-27 19:12:19,740 DEBUG TRAIN Batch 49/3800 loss 3.747549 loss_att 7.046287 loss_ctc 2.725837 loss_rnnt 3.181010 hw_loss 0.080661 lr 0.00029317 rank 6
2023-02-27 19:12:19,742 DEBUG TRAIN Batch 49/3800 loss 6.629550 loss_att 9.129759 loss_ctc 12.655143 loss_rnnt 5.169241 hw_loss 0.294102 lr 0.00029317 rank 0
2023-02-27 19:12:19,744 DEBUG TRAIN Batch 49/3800 loss 2.828398 loss_att 6.149232 loss_ctc 7.747949 loss_rnnt 1.440701 hw_loss 0.126733 lr 0.00029317 rank 4
2023-02-27 19:12:19,747 DEBUG TRAIN Batch 49/3800 loss 5.218248 loss_att 7.030739 loss_ctc 9.593613 loss_rnnt 4.115663 hw_loss 0.293823 lr 0.00029317 rank 5
2023-02-27 19:12:19,749 DEBUG TRAIN Batch 49/3800 loss 8.082411 loss_att 11.260534 loss_ctc 11.148395 loss_rnnt 6.915868 hw_loss 0.228975 lr 0.00029316 rank 7
2023-02-27 19:12:19,751 DEBUG TRAIN Batch 49/3800 loss 5.042487 loss_att 8.155819 loss_ctc 10.463550 loss_rnnt 3.661669 hw_loss 0.066267 lr 0.00029317 rank 1
2023-02-27 19:12:19,750 DEBUG TRAIN Batch 49/3800 loss 4.838187 loss_att 7.075586 loss_ctc 12.596172 loss_rnnt 3.244361 hw_loss 0.209903 lr 0.00029317 rank 2
2023-02-27 19:13:26,501 DEBUG TRAIN Batch 49/3900 loss 4.449100 loss_att 8.758957 loss_ctc 10.076607 loss_rnnt 2.753597 hw_loss 0.155996 lr 0.00029315 rank 1
2023-02-27 19:13:26,509 DEBUG TRAIN Batch 49/3900 loss 1.986005 loss_att 5.374636 loss_ctc 3.804555 loss_rnnt 0.997069 hw_loss 0.128882 lr 0.00029316 rank 4
2023-02-27 19:13:26,509 DEBUG TRAIN Batch 49/3900 loss 3.321835 loss_att 6.339605 loss_ctc 6.153350 loss_rnnt 2.212892 hw_loss 0.239726 lr 0.00029316 rank 6
2023-02-27 19:13:26,512 DEBUG TRAIN Batch 49/3900 loss 6.052726 loss_att 8.855627 loss_ctc 13.008888 loss_rnnt 4.454966 hw_loss 0.205673 lr 0.00029316 rank 0
2023-02-27 19:13:26,515 DEBUG TRAIN Batch 49/3900 loss 11.515756 loss_att 12.489376 loss_ctc 22.322838 loss_rnnt 9.746813 hw_loss 0.249890 lr 0.00029316 rank 2
2023-02-27 19:13:26,517 DEBUG TRAIN Batch 49/3900 loss 3.572995 loss_att 6.751667 loss_ctc 8.136803 loss_rnnt 2.207774 hw_loss 0.226835 lr 0.00029315 rank 3
2023-02-27 19:13:26,518 DEBUG TRAIN Batch 49/3900 loss 1.298069 loss_att 3.528587 loss_ctc 2.104979 loss_rnnt 0.681007 hw_loss 0.118820 lr 0.00029315 rank 7
2023-02-27 19:13:26,521 DEBUG TRAIN Batch 49/3900 loss 3.550250 loss_att 6.554618 loss_ctc 4.934729 loss_rnnt 2.646837 hw_loss 0.221142 lr 0.00029316 rank 5
2023-02-27 19:14:04,802 DEBUG TRAIN Batch 49/4000 loss 3.454689 loss_att 6.155370 loss_ctc 4.835651 loss_rnnt 2.713235 hw_loss 0.032231 lr 0.00029314 rank 0
2023-02-27 19:14:04,807 DEBUG TRAIN Batch 49/4000 loss 5.505561 loss_att 7.288090 loss_ctc 7.995090 loss_rnnt 4.713896 hw_loss 0.193542 lr 0.00029314 rank 3
2023-02-27 19:14:04,811 DEBUG TRAIN Batch 49/4000 loss 1.977919 loss_att 4.518213 loss_ctc 3.091081 loss_rnnt 1.214359 hw_loss 0.200773 lr 0.00029314 rank 7
2023-02-27 19:14:04,814 DEBUG TRAIN Batch 49/4000 loss 3.058169 loss_att 4.938430 loss_ctc 4.093468 loss_rnnt 2.357614 hw_loss 0.349617 lr 0.00029314 rank 4
2023-02-27 19:14:04,823 DEBUG TRAIN Batch 49/4000 loss 6.033182 loss_att 12.562819 loss_ctc 6.906698 loss_rnnt 4.446012 hw_loss 0.308950 lr 0.00029314 rank 2
2023-02-27 19:14:04,823 DEBUG TRAIN Batch 49/4000 loss 3.034364 loss_att 4.855261 loss_ctc 6.793062 loss_rnnt 1.985557 hw_loss 0.344003 lr 0.00029314 rank 6
2023-02-27 19:14:04,826 DEBUG TRAIN Batch 49/4000 loss 4.312984 loss_att 6.816610 loss_ctc 5.178505 loss_rnnt 3.579189 hw_loss 0.220625 lr 0.00029314 rank 1
2023-02-27 19:14:04,826 DEBUG TRAIN Batch 49/4000 loss 4.316558 loss_att 6.218919 loss_ctc 9.006641 loss_rnnt 3.210981 hw_loss 0.187052 lr 0.00029315 rank 5
2023-02-27 19:14:42,967 DEBUG TRAIN Batch 49/4100 loss 8.822938 loss_att 11.711683 loss_ctc 17.386866 loss_rnnt 7.039709 hw_loss 0.119292 lr 0.00029313 rank 3
2023-02-27 19:14:42,969 DEBUG TRAIN Batch 49/4100 loss 6.563241 loss_att 8.709843 loss_ctc 10.662334 loss_rnnt 5.427382 hw_loss 0.299986 lr 0.00029313 rank 4
2023-02-27 19:14:42,970 DEBUG TRAIN Batch 49/4100 loss 4.625816 loss_att 8.256627 loss_ctc 9.009552 loss_rnnt 3.202785 hw_loss 0.210694 lr 0.00029313 rank 6
2023-02-27 19:14:42,986 DEBUG TRAIN Batch 49/4100 loss 1.741935 loss_att 5.214767 loss_ctc 2.936030 loss_rnnt 0.718958 hw_loss 0.317244 lr 0.00029313 rank 2
2023-02-27 19:14:42,987 DEBUG TRAIN Batch 49/4100 loss 6.854702 loss_att 10.350634 loss_ctc 9.244967 loss_rnnt 5.720876 hw_loss 0.217384 lr 0.00029313 rank 0
2023-02-27 19:14:42,988 DEBUG TRAIN Batch 49/4100 loss 4.328183 loss_att 8.128750 loss_ctc 10.462868 loss_rnnt 2.685555 hw_loss 0.121045 lr 0.00029313 rank 5
2023-02-27 19:14:42,990 DEBUG TRAIN Batch 49/4100 loss 9.967699 loss_att 12.750893 loss_ctc 16.415226 loss_rnnt 8.401355 hw_loss 0.281316 lr 0.00029313 rank 7
2023-02-27 19:14:42,992 DEBUG TRAIN Batch 49/4100 loss 7.552731 loss_att 10.900316 loss_ctc 12.296667 loss_rnnt 6.147190 hw_loss 0.194059 lr 0.00029313 rank 1
2023-02-27 19:15:21,881 DEBUG TRAIN Batch 49/4200 loss 12.363753 loss_att 16.083786 loss_ctc 24.779528 loss_rnnt 9.902719 hw_loss 0.115484 lr 0.00029312 rank 6
2023-02-27 19:15:21,882 DEBUG TRAIN Batch 49/4200 loss 1.609176 loss_att 3.063902 loss_ctc 2.346182 loss_rnnt 1.086163 hw_loss 0.250876 lr 0.00029312 rank 1
2023-02-27 19:15:21,897 DEBUG TRAIN Batch 49/4200 loss 3.774763 loss_att 5.194503 loss_ctc 5.357498 loss_rnnt 3.082265 hw_loss 0.370348 lr 0.00029311 rank 7
2023-02-27 19:15:21,897 DEBUG TRAIN Batch 49/4200 loss 10.571805 loss_att 13.004341 loss_ctc 25.274982 loss_rnnt 8.019732 hw_loss 0.197142 lr 0.00029312 rank 3
2023-02-27 19:15:21,898 DEBUG TRAIN Batch 49/4200 loss 7.805707 loss_att 11.699655 loss_ctc 15.623829 loss_rnnt 5.889163 hw_loss 0.178760 lr 0.00029312 rank 0
2023-02-27 19:15:21,900 DEBUG TRAIN Batch 49/4200 loss 4.387170 loss_att 6.252192 loss_ctc 5.422351 loss_rnnt 3.811296 hw_loss 0.121585 lr 0.00029312 rank 4
2023-02-27 19:15:21,932 DEBUG TRAIN Batch 49/4200 loss 9.549090 loss_att 15.257634 loss_ctc 14.020534 loss_rnnt 7.649419 hw_loss 0.303318 lr 0.00029312 rank 2
2023-02-27 19:15:21,940 DEBUG TRAIN Batch 49/4200 loss 5.340552 loss_att 8.328937 loss_ctc 12.259140 loss_rnnt 3.695587 hw_loss 0.234019 lr 0.00029312 rank 5
2023-02-27 19:16:27,857 DEBUG TRAIN Batch 49/4300 loss 4.322949 loss_att 6.721992 loss_ctc 8.518339 loss_rnnt 3.186945 hw_loss 0.181519 lr 0.00029310 rank 1
2023-02-27 19:16:27,858 DEBUG TRAIN Batch 49/4300 loss 4.624352 loss_att 7.091246 loss_ctc 8.508922 loss_rnnt 3.510872 hw_loss 0.191547 lr 0.00029310 rank 3
2023-02-27 19:16:27,861 DEBUG TRAIN Batch 49/4300 loss 7.084698 loss_att 11.728865 loss_ctc 15.389427 loss_rnnt 4.925321 hw_loss 0.231088 lr 0.00029311 rank 6
2023-02-27 19:16:27,862 DEBUG TRAIN Batch 49/4300 loss 6.721815 loss_att 12.708441 loss_ctc 14.693930 loss_rnnt 4.354916 hw_loss 0.199921 lr 0.00029311 rank 0
2023-02-27 19:16:27,863 DEBUG TRAIN Batch 49/4300 loss 6.197547 loss_att 9.128632 loss_ctc 13.733914 loss_rnnt 4.564659 hw_loss 0.078416 lr 0.00029311 rank 4
2023-02-27 19:16:27,863 DEBUG TRAIN Batch 49/4300 loss 7.625362 loss_att 10.409912 loss_ctc 13.252380 loss_rnnt 6.255391 hw_loss 0.117735 lr 0.00029310 rank 7
2023-02-27 19:16:27,865 DEBUG TRAIN Batch 49/4300 loss 9.350684 loss_att 10.068517 loss_ctc 14.506029 loss_rnnt 8.386694 hw_loss 0.249458 lr 0.00029311 rank 2
2023-02-27 19:16:27,880 DEBUG TRAIN Batch 49/4300 loss 4.468945 loss_att 5.659720 loss_ctc 6.526731 loss_rnnt 3.844801 hw_loss 0.209283 lr 0.00029311 rank 5
2023-02-27 19:17:06,889 DEBUG TRAIN Batch 49/4400 loss 8.053329 loss_att 13.157968 loss_ctc 12.753636 loss_rnnt 6.281137 hw_loss 0.233541 lr 0.00029309 rank 2
2023-02-27 19:17:06,892 DEBUG TRAIN Batch 49/4400 loss 7.842987 loss_att 10.795424 loss_ctc 12.414034 loss_rnnt 6.608035 hw_loss 0.065609 lr 0.00029309 rank 3
2023-02-27 19:17:06,893 DEBUG TRAIN Batch 49/4400 loss 7.286561 loss_att 10.435212 loss_ctc 11.981511 loss_rnnt 5.910225 hw_loss 0.226147 lr 0.00029309 rank 4
2023-02-27 19:17:06,895 DEBUG TRAIN Batch 49/4400 loss 6.737891 loss_att 7.240347 loss_ctc 10.155367 loss_rnnt 6.081974 hw_loss 0.187054 lr 0.00029310 rank 5
2023-02-27 19:17:06,895 DEBUG TRAIN Batch 49/4400 loss 11.666652 loss_att 14.654844 loss_ctc 18.708912 loss_rnnt 9.976994 hw_loss 0.286970 lr 0.00029309 rank 0
2023-02-27 19:17:06,896 DEBUG TRAIN Batch 49/4400 loss 7.617822 loss_att 10.203938 loss_ctc 12.005124 loss_rnnt 6.340682 hw_loss 0.328019 lr 0.00029309 rank 6
2023-02-27 19:17:06,898 DEBUG TRAIN Batch 49/4400 loss 9.215747 loss_att 11.024855 loss_ctc 14.056648 loss_rnnt 8.079825 hw_loss 0.241212 lr 0.00029309 rank 1
2023-02-27 19:17:06,898 DEBUG TRAIN Batch 49/4400 loss 11.077415 loss_att 12.071212 loss_ctc 17.509075 loss_rnnt 9.890839 hw_loss 0.244242 lr 0.00029309 rank 7
2023-02-27 19:17:45,160 DEBUG TRAIN Batch 49/4500 loss 15.253796 loss_att 16.335262 loss_ctc 26.706470 loss_rnnt 13.406296 hw_loss 0.195344 lr 0.00029308 rank 0
2023-02-27 19:17:45,168 DEBUG TRAIN Batch 49/4500 loss 5.564012 loss_att 7.318102 loss_ctc 9.696182 loss_rnnt 4.532854 hw_loss 0.242593 lr 0.00029308 rank 6
2023-02-27 19:17:45,168 DEBUG TRAIN Batch 49/4500 loss 5.501899 loss_att 7.627705 loss_ctc 9.149195 loss_rnnt 4.531622 hw_loss 0.110268 lr 0.00029308 rank 3
2023-02-27 19:17:45,172 DEBUG TRAIN Batch 49/4500 loss 2.288124 loss_att 5.245443 loss_ctc 6.490608 loss_rnnt 1.066596 hw_loss 0.130750 lr 0.00029308 rank 7
2023-02-27 19:17:45,171 DEBUG TRAIN Batch 49/4500 loss 7.343038 loss_att 9.115992 loss_ctc 13.546305 loss_rnnt 6.018945 hw_loss 0.267000 lr 0.00029308 rank 2
2023-02-27 19:17:45,175 DEBUG TRAIN Batch 49/4500 loss 6.912412 loss_att 12.437042 loss_ctc 21.261864 loss_rnnt 3.832649 hw_loss 0.115457 lr 0.00029308 rank 5
2023-02-27 19:17:45,177 DEBUG TRAIN Batch 49/4500 loss 1.902152 loss_att 4.804906 loss_ctc 3.165229 loss_rnnt 1.031418 hw_loss 0.228324 lr 0.00029308 rank 1
2023-02-27 19:17:45,182 DEBUG TRAIN Batch 49/4500 loss 8.393530 loss_att 9.063841 loss_ctc 12.113836 loss_rnnt 7.674095 hw_loss 0.167499 lr 0.00029308 rank 4
2023-02-27 19:18:52,933 DEBUG TRAIN Batch 49/4600 loss 5.238626 loss_att 7.791482 loss_ctc 7.280396 loss_rnnt 4.354919 hw_loss 0.189186 lr 0.00029307 rank 5
2023-02-27 19:18:52,944 DEBUG TRAIN Batch 49/4600 loss 7.587183 loss_att 11.717756 loss_ctc 12.479480 loss_rnnt 6.015665 hw_loss 0.174557 lr 0.00029307 rank 6
2023-02-27 19:18:52,945 DEBUG TRAIN Batch 49/4600 loss 6.137006 loss_att 9.340164 loss_ctc 10.157475 loss_rnnt 4.837778 hw_loss 0.229751 lr 0.00029307 rank 3
2023-02-27 19:18:52,949 DEBUG TRAIN Batch 49/4600 loss 7.384690 loss_att 9.219835 loss_ctc 11.874930 loss_rnnt 6.222765 hw_loss 0.367868 lr 0.00029307 rank 2
2023-02-27 19:18:52,952 DEBUG TRAIN Batch 49/4600 loss 5.002977 loss_att 6.348089 loss_ctc 9.955849 loss_rnnt 3.931205 hw_loss 0.266938 lr 0.00029306 rank 7
2023-02-27 19:18:52,959 DEBUG TRAIN Batch 49/4600 loss 3.865572 loss_att 5.383477 loss_ctc 7.006604 loss_rnnt 3.084094 hw_loss 0.110799 lr 0.00029307 rank 4
2023-02-27 19:18:53,000 DEBUG TRAIN Batch 49/4600 loss 3.089631 loss_att 4.850186 loss_ctc 7.108528 loss_rnnt 2.045788 hw_loss 0.292272 lr 0.00029307 rank 1
2023-02-27 19:18:53,022 DEBUG TRAIN Batch 49/4600 loss 2.351297 loss_att 5.641283 loss_ctc 5.414310 loss_rnnt 1.130430 hw_loss 0.289628 lr 0.00029307 rank 0
2023-02-27 19:19:31,917 DEBUG TRAIN Batch 49/4700 loss 11.170088 loss_att 14.861130 loss_ctc 17.660210 loss_rnnt 9.499331 hw_loss 0.125998 lr 0.00029306 rank 0
2023-02-27 19:19:31,918 DEBUG TRAIN Batch 49/4700 loss 5.707705 loss_att 6.819251 loss_ctc 7.872069 loss_rnnt 5.044611 hw_loss 0.285381 lr 0.00029305 rank 7
2023-02-27 19:19:31,921 DEBUG TRAIN Batch 49/4700 loss 3.301484 loss_att 5.357531 loss_ctc 4.800937 loss_rnnt 2.463325 hw_loss 0.425667 lr 0.00029305 rank 3
2023-02-27 19:19:31,923 DEBUG TRAIN Batch 49/4700 loss 7.870730 loss_att 12.396063 loss_ctc 12.802001 loss_rnnt 6.174965 hw_loss 0.249742 lr 0.00029306 rank 5
2023-02-27 19:19:31,923 DEBUG TRAIN Batch 49/4700 loss 4.966090 loss_att 7.651769 loss_ctc 9.496414 loss_rnnt 3.753864 hw_loss 0.133212 lr 0.00029305 rank 6
2023-02-27 19:19:31,924 DEBUG TRAIN Batch 49/4700 loss 1.549233 loss_att 4.450771 loss_ctc 2.503300 loss_rnnt 0.766513 hw_loss 0.141007 lr 0.00029305 rank 2
2023-02-27 19:19:31,924 DEBUG TRAIN Batch 49/4700 loss 4.427175 loss_att 7.255240 loss_ctc 8.977039 loss_rnnt 3.117060 hw_loss 0.258474 lr 0.00029306 rank 4
2023-02-27 19:19:31,927 DEBUG TRAIN Batch 49/4700 loss 6.751031 loss_att 11.446388 loss_ctc 16.141228 loss_rnnt 4.408977 hw_loss 0.283043 lr 0.00029305 rank 1
2023-02-27 19:20:09,870 DEBUG TRAIN Batch 49/4800 loss 5.678545 loss_att 9.432984 loss_ctc 10.153553 loss_rnnt 4.253801 hw_loss 0.144726 lr 0.00029304 rank 3
2023-02-27 19:20:09,880 DEBUG TRAIN Batch 49/4800 loss 4.376772 loss_att 5.925757 loss_ctc 7.349716 loss_rnnt 3.509499 hw_loss 0.302033 lr 0.00029304 rank 4
2023-02-27 19:20:09,887 DEBUG TRAIN Batch 49/4800 loss 3.888032 loss_att 7.716113 loss_ctc 6.688926 loss_rnnt 2.748657 hw_loss 0.000573 lr 0.00029304 rank 0
2023-02-27 19:20:09,888 DEBUG TRAIN Batch 49/4800 loss 2.361979 loss_att 4.304916 loss_ctc 3.377924 loss_rnnt 1.735236 hw_loss 0.192554 lr 0.00029304 rank 6
2023-02-27 19:20:09,904 DEBUG TRAIN Batch 49/4800 loss 9.024533 loss_att 14.732792 loss_ctc 16.284348 loss_rnnt 6.856931 hw_loss 0.108703 lr 0.00029304 rank 1
2023-02-27 19:20:09,905 DEBUG TRAIN Batch 49/4800 loss 4.335547 loss_att 9.168153 loss_ctc 8.802490 loss_rnnt 2.643641 hw_loss 0.243360 lr 0.00029305 rank 5
2023-02-27 19:20:09,906 DEBUG TRAIN Batch 49/4800 loss 5.615662 loss_att 7.275000 loss_ctc 10.884281 loss_rnnt 4.392160 hw_loss 0.354659 lr 0.00029304 rank 2
2023-02-27 19:20:09,909 DEBUG TRAIN Batch 49/4800 loss 2.717810 loss_att 6.136092 loss_ctc 7.688241 loss_rnnt 1.288244 hw_loss 0.155972 lr 0.00029304 rank 7
2023-02-27 19:20:48,473 DEBUG TRAIN Batch 49/4900 loss 5.794919 loss_att 11.352293 loss_ctc 10.685220 loss_rnnt 3.876464 hw_loss 0.290512 lr 0.00029303 rank 7
2023-02-27 19:20:48,477 DEBUG TRAIN Batch 49/4900 loss 7.747767 loss_att 8.674204 loss_ctc 10.318993 loss_rnnt 7.140857 hw_loss 0.147737 lr 0.00029303 rank 5
2023-02-27 19:20:48,481 DEBUG TRAIN Batch 49/4900 loss 11.150998 loss_att 13.253338 loss_ctc 18.260973 loss_rnnt 9.674548 hw_loss 0.202471 lr 0.00029303 rank 2
2023-02-27 19:20:48,481 DEBUG TRAIN Batch 49/4900 loss 4.951554 loss_att 7.818457 loss_ctc 8.332142 loss_rnnt 3.834357 hw_loss 0.174510 lr 0.00029303 rank 0
2023-02-27 19:20:48,483 DEBUG TRAIN Batch 49/4900 loss 8.103241 loss_att 9.511077 loss_ctc 13.338266 loss_rnnt 6.997221 hw_loss 0.237093 lr 0.00029303 rank 3
2023-02-27 19:20:48,485 DEBUG TRAIN Batch 49/4900 loss 2.817118 loss_att 5.361208 loss_ctc 5.398750 loss_rnnt 1.828941 hw_loss 0.253391 lr 0.00029303 rank 1
2023-02-27 19:20:48,488 DEBUG TRAIN Batch 49/4900 loss 3.469621 loss_att 7.944523 loss_ctc 4.896027 loss_rnnt 2.267955 hw_loss 0.218433 lr 0.00029303 rank 4
2023-02-27 19:20:48,498 DEBUG TRAIN Batch 49/4900 loss 6.569908 loss_att 9.110758 loss_ctc 9.844091 loss_rnnt 5.547261 hw_loss 0.146099 lr 0.00029303 rank 6
2023-02-27 19:21:56,635 DEBUG TRAIN Batch 49/5000 loss 7.509164 loss_att 9.721891 loss_ctc 9.284399 loss_rnnt 6.684039 hw_loss 0.273527 lr 0.00029302 rank 0
2023-02-27 19:21:56,640 DEBUG TRAIN Batch 49/5000 loss 6.783169 loss_att 7.641335 loss_ctc 12.420455 loss_rnnt 5.801098 hw_loss 0.110250 lr 0.00029302 rank 5
2023-02-27 19:21:56,641 DEBUG TRAIN Batch 49/5000 loss 3.382682 loss_att 8.028952 loss_ctc 6.976351 loss_rnnt 1.895256 hw_loss 0.148156 lr 0.00029302 rank 1
2023-02-27 19:21:56,642 DEBUG TRAIN Batch 49/5000 loss 8.199341 loss_att 10.478171 loss_ctc 11.195383 loss_rnnt 7.223590 hw_loss 0.225961 lr 0.00029302 rank 3
2023-02-27 19:21:56,642 DEBUG TRAIN Batch 49/5000 loss 3.787683 loss_att 6.124752 loss_ctc 7.200684 loss_rnnt 2.700051 hw_loss 0.309659 lr 0.00029301 rank 7
2023-02-27 19:21:56,644 DEBUG TRAIN Batch 49/5000 loss 2.704346 loss_att 4.343966 loss_ctc 5.409652 loss_rnnt 1.877739 hw_loss 0.258704 lr 0.00029302 rank 4
2023-02-27 19:21:56,645 DEBUG TRAIN Batch 49/5000 loss 6.027919 loss_att 8.270098 loss_ctc 9.200291 loss_rnnt 5.100120 hw_loss 0.105712 lr 0.00029302 rank 6
2023-02-27 19:21:56,692 DEBUG TRAIN Batch 49/5000 loss 5.248382 loss_att 7.315349 loss_ctc 8.179320 loss_rnnt 4.337008 hw_loss 0.200979 lr 0.00029302 rank 2
2023-02-27 19:22:36,350 DEBUG TRAIN Batch 49/5100 loss 3.729667 loss_att 5.974353 loss_ctc 6.059317 loss_rnnt 2.886951 hw_loss 0.155922 lr 0.00029300 rank 7
2023-02-27 19:22:36,354 DEBUG TRAIN Batch 49/5100 loss 1.771014 loss_att 5.735310 loss_ctc 3.471124 loss_rnnt 0.718775 hw_loss 0.061308 lr 0.00029300 rank 6
2023-02-27 19:22:36,361 DEBUG TRAIN Batch 49/5100 loss 8.732802 loss_att 11.276755 loss_ctc 11.699329 loss_rnnt 7.719772 hw_loss 0.203817 lr 0.00029300 rank 3
2023-02-27 19:22:36,361 DEBUG TRAIN Batch 49/5100 loss 6.192348 loss_att 7.058887 loss_ctc 8.029880 loss_rnnt 5.684149 hw_loss 0.168537 lr 0.00029301 rank 0
2023-02-27 19:22:36,363 DEBUG TRAIN Batch 49/5100 loss 6.856650 loss_att 13.113549 loss_ctc 10.761274 loss_rnnt 4.962293 hw_loss 0.229426 lr 0.00029301 rank 5
2023-02-27 19:22:36,368 DEBUG TRAIN Batch 49/5100 loss 5.063443 loss_att 7.427526 loss_ctc 10.818550 loss_rnnt 3.701634 hw_loss 0.228082 lr 0.00029301 rank 4
2023-02-27 19:22:36,372 DEBUG TRAIN Batch 49/5100 loss 11.826377 loss_att 13.892559 loss_ctc 16.448795 loss_rnnt 10.719503 hw_loss 0.144966 lr 0.00029300 rank 2
2023-02-27 19:22:36,381 DEBUG TRAIN Batch 49/5100 loss 8.186534 loss_att 8.754904 loss_ctc 12.585058 loss_rnnt 7.329902 hw_loss 0.293416 lr 0.00029300 rank 1
2023-02-27 19:23:15,854 DEBUG TRAIN Batch 49/5200 loss 2.892150 loss_att 6.227664 loss_ctc 3.914036 loss_rnnt 2.002420 hw_loss 0.161955 lr 0.00029299 rank 6
2023-02-27 19:23:15,858 DEBUG TRAIN Batch 49/5200 loss 6.584897 loss_att 9.084780 loss_ctc 8.354634 loss_rnnt 5.758736 hw_loss 0.169161 lr 0.00029299 rank 3
2023-02-27 19:23:15,859 DEBUG TRAIN Batch 49/5200 loss 10.878574 loss_att 16.475811 loss_ctc 15.216780 loss_rnnt 9.111166 hw_loss 0.130375 lr 0.00029299 rank 4
2023-02-27 19:23:15,861 DEBUG TRAIN Batch 49/5200 loss 5.138409 loss_att 9.386689 loss_ctc 9.972720 loss_rnnt 3.589577 hw_loss 0.102377 lr 0.00029299 rank 0
2023-02-27 19:23:15,878 DEBUG TRAIN Batch 49/5200 loss 2.760709 loss_att 4.666569 loss_ctc 4.755376 loss_rnnt 2.009884 hw_loss 0.194433 lr 0.00029300 rank 5
2023-02-27 19:23:15,880 DEBUG TRAIN Batch 49/5200 loss 2.269810 loss_att 5.376951 loss_ctc 4.199333 loss_rnnt 1.283704 hw_loss 0.201391 lr 0.00029299 rank 1
2023-02-27 19:23:15,882 DEBUG TRAIN Batch 49/5200 loss 8.518237 loss_att 9.720149 loss_ctc 15.074053 loss_rnnt 7.320261 hw_loss 0.156535 lr 0.00029299 rank 2
2023-02-27 19:23:15,885 DEBUG TRAIN Batch 49/5200 loss 10.590537 loss_att 12.662272 loss_ctc 15.737411 loss_rnnt 9.368557 hw_loss 0.227595 lr 0.00029299 rank 7
2023-02-27 19:23:56,359 DEBUG TRAIN Batch 49/5300 loss 3.925211 loss_att 6.512008 loss_ctc 8.379503 loss_rnnt 2.660097 hw_loss 0.288467 lr 0.00029298 rank 0
2023-02-27 19:23:56,361 DEBUG TRAIN Batch 49/5300 loss 8.433854 loss_att 10.661830 loss_ctc 15.914148 loss_rnnt 6.950224 hw_loss 0.076242 lr 0.00029298 rank 6
2023-02-27 19:23:56,364 DEBUG TRAIN Batch 49/5300 loss 5.088377 loss_att 7.897362 loss_ctc 8.886045 loss_rnnt 3.871851 hw_loss 0.278198 lr 0.00029298 rank 2
2023-02-27 19:23:56,364 DEBUG TRAIN Batch 49/5300 loss 5.327667 loss_att 10.960526 loss_ctc 12.030201 loss_rnnt 3.095361 hw_loss 0.397619 lr 0.00029298 rank 5
2023-02-27 19:23:56,369 DEBUG TRAIN Batch 49/5300 loss 6.333242 loss_att 11.198708 loss_ctc 11.599991 loss_rnnt 4.566807 hw_loss 0.170829 lr 0.00029298 rank 7
2023-02-27 19:23:56,368 DEBUG TRAIN Batch 49/5300 loss 3.657237 loss_att 6.897051 loss_ctc 4.837424 loss_rnnt 2.746187 hw_loss 0.198243 lr 0.00029298 rank 3
2023-02-27 19:23:56,369 DEBUG TRAIN Batch 49/5300 loss 8.584457 loss_att 11.368818 loss_ctc 14.934242 loss_rnnt 7.180674 hw_loss 0.000513 lr 0.00029298 rank 1
2023-02-27 19:23:56,381 DEBUG TRAIN Batch 49/5300 loss 1.867063 loss_att 5.340151 loss_ctc 4.613275 loss_rnnt 0.698213 hw_loss 0.202631 lr 0.00029298 rank 4
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 97, in train
    loss_dict = model(feats, feats_lengths, target,
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 888, in forward
    output = self.module(*inputs, **kwargs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/transducer/transducer.py", line 151, in forward
    loss_att, _ = self._calc_att_loss(encoder_out, encoder_mask, text,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/transformer/asr_model.py", line 123, in _calc_att_loss
    ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/common.py", line 122, in add_sos_eos
    _sos = torch.tensor([sos],
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 7014) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:59116
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:13719
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:34989
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:20649
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Application timeout caused pair closure
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:5029
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:11663
