/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_14_rnnt_bias.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_16_rnnt_bias_loss_2_class/ddp_init
2023-02-17 00:17:33,257 INFO training on multiple gpus, this gpu 6
2023-02-17 00:17:33,261 INFO training on multiple gpus, this gpu 4
2023-02-17 00:17:33,261 INFO training on multiple gpus, this gpu 5
2023-02-17 00:17:33,261 INFO training on multiple gpus, this gpu 7
2023-02-17 00:17:33,263 INFO training on multiple gpus, this gpu 1
2023-02-17 00:17:33,263 INFO training on multiple gpus, this gpu 0
2023-02-17 00:17:33,272 INFO training on multiple gpus, this gpu 3
2023-02-17 00:17:33,276 INFO training on multiple gpus, this gpu 2
2023-02-17 00:18:04,696 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-17 00:18:06,061 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-17 00:18:06,115 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-17 00:18:06,127 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-17 00:18:06,128 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-17 00:18:06,961 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-17 00:18:07,097 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-17 00:18:09,076 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-17 00:18:09,077 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:09,082 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:09,224 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:09,533 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:09,968 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:10,065 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:10,071 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:10,478 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 00:18:26,837 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,839 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 00:18:26,845 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,847 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 00:18:26,865 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,866 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 00:18:26,868 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,870 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 00:18:26,887 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,887 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/init.pt
2023-02-17 00:18:26,888 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,889 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 00:18:26,890 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 00:18:26,891 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:26,893 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 00:18:28,211 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 00:18:28,214 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 00:19:35,400 DEBUG TRAIN Batch 0/0 loss 552.910950 loss_att 75.620827 loss_ctc 530.062988 loss_rnnt 650.979126 hw_loss 0.817898 lr 0.00000004 rank 6
2023-02-17 00:19:35,401 DEBUG TRAIN Batch 0/0 loss 499.952881 loss_att 74.218575 loss_ctc 478.883026 loss_rnnt 587.462769 hw_loss 0.836805 lr 0.00000004 rank 7
2023-02-17 00:19:35,403 DEBUG TRAIN Batch 0/0 loss 591.049377 loss_att 77.090462 loss_ctc 564.144531 loss_rnnt 696.953125 hw_loss 0.891237 lr 0.00000004 rank 1
2023-02-17 00:19:35,411 DEBUG TRAIN Batch 0/0 loss 559.971863 loss_att 77.507202 loss_ctc 525.405396 loss_rnnt 660.674500 hw_loss 0.748516 lr 0.00000004 rank 2
2023-02-17 00:19:35,434 DEBUG TRAIN Batch 0/0 loss 529.872131 loss_att 74.563431 loss_ctc 500.771606 loss_rnnt 624.374207 hw_loss 0.824621 lr 0.00000004 rank 3
2023-02-17 00:19:35,444 DEBUG TRAIN Batch 0/0 loss 543.157654 loss_att 68.154587 loss_ctc 512.310425 loss_rnnt 641.827271 hw_loss 0.832435 lr 0.00000004 rank 4
2023-02-17 00:19:35,446 DEBUG TRAIN Batch 0/0 loss 553.447144 loss_att 83.791847 loss_ctc 506.145538 loss_rnnt 653.105774 hw_loss 1.086124 lr 0.00000004 rank 0
2023-02-17 00:19:35,471 DEBUG TRAIN Batch 0/0 loss 553.307312 loss_att 74.437080 loss_ctc 529.166443 loss_rnnt 651.741943 hw_loss 1.046547 lr 0.00000004 rank 5
2023-02-17 00:20:53,214 DEBUG TRAIN Batch 0/100 loss 2145.112305 loss_att 384.976501 loss_ctc 3084.955566 loss_rnnt 2371.394043 hw_loss 0.811803 lr 0.00000404 rank 6
2023-02-17 00:20:53,214 DEBUG TRAIN Batch 0/100 loss 2260.335693 loss_att 384.859497 loss_ctc 3144.366699 loss_rnnt 2517.260498 hw_loss 0.561386 lr 0.00000404 rank 3
2023-02-17 00:20:53,215 DEBUG TRAIN Batch 0/100 loss 2214.695068 loss_att 459.136810 loss_ctc 3096.078857 loss_rnnt 2447.710693 hw_loss 1.083719 lr 0.00000404 rank 7
2023-02-17 00:20:53,217 DEBUG TRAIN Batch 0/100 loss 2072.251221 loss_att 393.459412 loss_ctc 2874.103516 loss_rnnt 2300.792725 hw_loss 0.568949 lr 0.00000404 rank 5
2023-02-17 00:20:53,219 DEBUG TRAIN Batch 0/100 loss 2153.577393 loss_att 363.059448 loss_ctc 3093.222168 loss_rnnt 2386.083984 hw_loss 0.582892 lr 0.00000404 rank 1
2023-02-17 00:20:53,222 DEBUG TRAIN Batch 0/100 loss 2236.139648 loss_att 432.121277 loss_ctc 3025.522217 loss_rnnt 2491.202393 hw_loss 0.918500 lr 0.00000404 rank 0
2023-02-17 00:20:53,222 DEBUG TRAIN Batch 0/100 loss 2058.894043 loss_att 349.150024 loss_ctc 3003.427246 loss_rnnt 2274.655029 hw_loss 0.468972 lr 0.00000404 rank 4
2023-02-17 00:20:53,233 DEBUG TRAIN Batch 0/100 loss 2091.526611 loss_att 357.895447 loss_ctc 3016.387695 loss_rnnt 2314.658936 hw_loss 0.523737 lr 0.00000404 rank 2
2023-02-17 00:22:08,919 DEBUG TRAIN Batch 0/200 loss 871.782166 loss_att 349.466278 loss_ctc 2641.956055 loss_rnnt 739.987427 hw_loss 0.440028 lr 0.00000804 rank 0
2023-02-17 00:22:08,922 DEBUG TRAIN Batch 0/200 loss 954.282288 loss_att 421.408081 loss_ctc 2595.031982 loss_rnnt 841.856140 hw_loss 0.439498 lr 0.00000804 rank 2
2023-02-17 00:22:08,921 DEBUG TRAIN Batch 0/200 loss 887.344238 loss_att 357.444061 loss_ctc 2730.832764 loss_rnnt 747.314697 hw_loss 0.395730 lr 0.00000804 rank 4
2023-02-17 00:22:08,925 DEBUG TRAIN Batch 0/200 loss 1022.201050 loss_att 456.143738 loss_ctc 2756.425293 loss_rnnt 904.015991 hw_loss 0.312276 lr 0.00000804 rank 7
2023-02-17 00:22:08,926 DEBUG TRAIN Batch 0/200 loss 877.484985 loss_att 389.684814 loss_ctc 2496.175293 loss_rnnt 758.980652 hw_loss 0.448221 lr 0.00000804 rank 1
2023-02-17 00:22:08,926 DEBUG TRAIN Batch 0/200 loss 860.200012 loss_att 348.247864 loss_ctc 2507.338379 loss_rnnt 742.737000 hw_loss 0.440621 lr 0.00000804 rank 5
2023-02-17 00:22:08,927 DEBUG TRAIN Batch 0/200 loss 955.916260 loss_att 439.099915 loss_ctc 2754.915039 loss_rnnt 819.167603 hw_loss 0.460064 lr 0.00000804 rank 3
2023-02-17 00:22:08,969 DEBUG TRAIN Batch 0/200 loss 913.738892 loss_att 380.013855 loss_ctc 2623.215332 loss_rnnt 792.305420 hw_loss 0.465487 lr 0.00000804 rank 6
2023-02-17 00:23:29,180 DEBUG TRAIN Batch 0/300 loss 430.270233 loss_att 332.300690 loss_ctc 1137.884033 loss_rnnt 355.299438 hw_loss 0.405373 lr 0.00001204 rank 1
2023-02-17 00:23:29,186 DEBUG TRAIN Batch 0/300 loss 478.188293 loss_att 351.849640 loss_ctc 1419.099243 loss_rnnt 377.766235 hw_loss 0.440633 lr 0.00001204 rank 3
2023-02-17 00:23:29,197 DEBUG TRAIN Batch 0/300 loss 516.569763 loss_att 391.704590 loss_ctc 1408.822754 loss_rnnt 422.380493 hw_loss 0.365992 lr 0.00001204 rank 7
2023-02-17 00:23:29,222 DEBUG TRAIN Batch 0/300 loss 433.194153 loss_att 307.799072 loss_ctc 1350.399902 loss_rnnt 335.736877 hw_loss 0.454090 lr 0.00001204 rank 0
2023-02-17 00:23:29,225 DEBUG TRAIN Batch 0/300 loss 474.599701 loss_att 372.579254 loss_ctc 1196.862061 loss_rnnt 398.513123 hw_loss 0.354382 lr 0.00001204 rank 2
2023-02-17 00:23:29,242 DEBUG TRAIN Batch 0/300 loss 545.328674 loss_att 420.301056 loss_ctc 1428.747070 loss_rnnt 452.376160 hw_loss 0.316702 lr 0.00001204 rank 4
2023-02-17 00:23:29,248 DEBUG TRAIN Batch 0/300 loss 510.773285 loss_att 387.569946 loss_ctc 1419.987061 loss_rnnt 413.977905 hw_loss 0.389153 lr 0.00001204 rank 6
2023-02-17 00:23:29,249 DEBUG TRAIN Batch 0/300 loss 551.147766 loss_att 384.788239 loss_ctc 1842.074585 loss_rnnt 412.041351 hw_loss 0.477594 lr 0.00001204 rank 5
2023-02-17 00:25:00,612 DEBUG TRAIN Batch 0/400 loss 356.399170 loss_att 320.944397 loss_ctc 526.422058 loss_rnnt 340.650787 hw_loss 0.318013 lr 0.00001604 rank 6
2023-02-17 00:25:00,614 DEBUG TRAIN Batch 0/400 loss 327.670929 loss_att 269.079346 loss_ctc 679.638611 loss_rnnt 292.244873 hw_loss 0.403728 lr 0.00001604 rank 3
2023-02-17 00:25:00,615 DEBUG TRAIN Batch 0/400 loss 342.868073 loss_att 304.780518 loss_ctc 510.174988 loss_rnnt 327.963287 hw_loss 0.402627 lr 0.00001604 rank 7
2023-02-17 00:25:00,620 DEBUG TRAIN Batch 0/400 loss 398.140289 loss_att 351.411316 loss_ctc 572.468384 loss_rnnt 384.050568 hw_loss 0.359586 lr 0.00001604 rank 4
2023-02-17 00:25:00,622 DEBUG TRAIN Batch 0/400 loss 335.555115 loss_att 298.158569 loss_ctc 498.970764 loss_rnnt 321.031372 hw_loss 0.401870 lr 0.00001604 rank 5
2023-02-17 00:25:00,622 DEBUG TRAIN Batch 0/400 loss 310.176727 loss_att 274.192139 loss_ctc 463.812378 loss_rnnt 296.668945 hw_loss 0.412475 lr 0.00001604 rank 0
2023-02-17 00:25:00,662 DEBUG TRAIN Batch 0/400 loss 333.052948 loss_att 290.352753 loss_ctc 484.577850 loss_rnnt 321.153076 hw_loss 0.443540 lr 0.00001604 rank 1
2023-02-17 00:25:00,669 DEBUG TRAIN Batch 0/400 loss 311.440033 loss_att 276.581787 loss_ctc 464.877502 loss_rnnt 297.716125 hw_loss 0.444800 lr 0.00001604 rank 2
2023-02-17 00:26:16,865 DEBUG TRAIN Batch 0/500 loss 341.098541 loss_att 280.736633 loss_ctc 611.230896 loss_rnnt 316.939514 hw_loss 0.400800 lr 0.00002004 rank 6
2023-02-17 00:26:16,866 DEBUG TRAIN Batch 0/500 loss 274.585693 loss_att 217.338211 loss_ctc 572.796082 loss_rnnt 246.038727 hw_loss 0.440757 lr 0.00002004 rank 2
2023-02-17 00:26:16,867 DEBUG TRAIN Batch 0/500 loss 338.868835 loss_att 269.003510 loss_ctc 691.879456 loss_rnnt 305.540710 hw_loss 0.437118 lr 0.00002004 rank 7
2023-02-17 00:26:16,867 DEBUG TRAIN Batch 0/500 loss 324.720276 loss_att 283.079224 loss_ctc 455.247864 loss_rnnt 315.411957 hw_loss 0.436653 lr 0.00002004 rank 0
2023-02-17 00:26:16,871 DEBUG TRAIN Batch 0/500 loss 270.819519 loss_att 234.071777 loss_ctc 371.823059 loss_rnnt 264.444763 hw_loss 0.482145 lr 0.00002004 rank 1
2023-02-17 00:26:16,875 DEBUG TRAIN Batch 0/500 loss 316.084534 loss_att 256.246887 loss_ctc 572.326294 loss_rnnt 293.682739 hw_loss 0.382012 lr 0.00002004 rank 3
2023-02-17 00:26:16,892 DEBUG TRAIN Batch 0/500 loss 248.655609 loss_att 199.195541 loss_ctc 516.201843 loss_rnnt 222.624985 hw_loss 0.468357 lr 0.00002004 rank 4
2023-02-17 00:26:16,908 DEBUG TRAIN Batch 0/500 loss 344.248993 loss_att 297.803894 loss_ctc 465.349365 loss_rnnt 337.194794 hw_loss 0.368439 lr 0.00002004 rank 5
2023-02-17 00:27:32,773 DEBUG TRAIN Batch 0/600 loss 257.112885 loss_att 196.491379 loss_ctc 584.330505 loss_rnnt 225.363190 hw_loss 0.459352 lr 0.00002404 rank 3
2023-02-17 00:27:32,775 DEBUG TRAIN Batch 0/600 loss 295.765381 loss_att 207.786758 loss_ctc 880.652588 loss_rnnt 235.209030 hw_loss 0.313227 lr 0.00002404 rank 4
2023-02-17 00:27:32,775 DEBUG TRAIN Batch 0/600 loss 143.081543 loss_att 121.805710 loss_ctc 194.103149 loss_rnnt 140.212311 hw_loss 0.602843 lr 0.00002404 rank 2
2023-02-17 00:27:32,778 DEBUG TRAIN Batch 0/600 loss 196.295258 loss_att 141.208496 loss_ctc 541.665527 loss_rnnt 160.989410 hw_loss 0.513423 lr 0.00002404 rank 5
2023-02-17 00:27:32,793 DEBUG TRAIN Batch 0/600 loss 180.746109 loss_att 143.122589 loss_ctc 369.167023 loss_rnnt 162.873672 hw_loss 0.514428 lr 0.00002404 rank 7
2023-02-17 00:27:32,809 DEBUG TRAIN Batch 0/600 loss 229.215408 loss_att 180.017395 loss_ctc 482.141815 loss_rnnt 205.075226 hw_loss 0.480473 lr 0.00002404 rank 6
2023-02-17 00:27:32,810 DEBUG TRAIN Batch 0/600 loss 124.251274 loss_att 77.640350 loss_ctc 466.178589 loss_rnnt 87.631599 hw_loss 0.659160 lr 0.00002404 rank 1
2023-02-17 00:27:32,845 DEBUG TRAIN Batch 0/600 loss 195.333664 loss_att 158.440552 loss_ctc 366.018646 loss_rnnt 179.657730 hw_loss 0.556041 lr 0.00002404 rank 0
2023-02-17 00:28:52,994 DEBUG TRAIN Batch 0/700 loss 365.948364 loss_att 317.008850 loss_ctc 424.809265 loss_rnnt 367.638489 hw_loss 0.468038 lr 0.00002804 rank 6
2023-02-17 00:28:52,998 DEBUG TRAIN Batch 0/700 loss 363.858643 loss_att 284.716400 loss_ctc 761.239441 loss_rnnt 326.483398 hw_loss 0.411709 lr 0.00002804 rank 1
2023-02-17 00:28:53,000 DEBUG TRAIN Batch 0/700 loss 431.479248 loss_att 321.128937 loss_ctc 1051.961670 loss_rnnt 370.630585 hw_loss 0.352026 lr 0.00002804 rank 0
2023-02-17 00:28:53,002 DEBUG TRAIN Batch 0/700 loss 363.480042 loss_att 315.868896 loss_ctc 425.251923 loss_rnnt 364.572754 hw_loss 0.362342 lr 0.00002804 rank 7
2023-02-17 00:28:53,017 DEBUG TRAIN Batch 0/700 loss 399.032806 loss_att 343.000946 loss_ctc 510.402527 loss_rnnt 395.188416 hw_loss 0.377792 lr 0.00002804 rank 5
2023-02-17 00:28:53,021 DEBUG TRAIN Batch 0/700 loss 406.712616 loss_att 349.982941 loss_ctc 498.438965 loss_rnnt 405.655518 hw_loss 0.324089 lr 0.00002804 rank 3
2023-02-17 00:28:53,028 DEBUG TRAIN Batch 0/700 loss 345.476074 loss_att 298.840210 loss_ctc 432.149780 loss_rnnt 343.053802 hw_loss 0.361821 lr 0.00002804 rank 4
2023-02-17 00:28:53,060 DEBUG TRAIN Batch 0/700 loss 386.057739 loss_att 334.158051 loss_ctc 459.967712 loss_rnnt 386.403290 hw_loss 0.336982 lr 0.00002804 rank 2
2023-02-17 00:31:47,652 DEBUG TRAIN Batch 0/800 loss 392.805359 loss_att 337.067169 loss_ctc 477.412689 loss_rnnt 392.524109 hw_loss 0.277340 lr 0.00003204 rank 7
2023-02-17 00:31:47,655 DEBUG TRAIN Batch 0/800 loss 385.979065 loss_att 308.895782 loss_ctc 727.170288 loss_rnnt 355.699432 hw_loss 0.382719 lr 0.00003204 rank 5
2023-02-17 00:31:47,659 DEBUG TRAIN Batch 0/800 loss 351.653168 loss_att 304.089966 loss_ctc 406.332367 loss_rnnt 353.658905 hw_loss 0.405659 lr 0.00003204 rank 1
2023-02-17 00:31:47,705 DEBUG TRAIN Batch 0/800 loss 297.213776 loss_att 255.649582 loss_ctc 359.300751 loss_rnnt 297.020844 hw_loss 0.426576 lr 0.00003204 rank 4
2023-02-17 00:31:47,709 DEBUG TRAIN Batch 0/800 loss 296.952911 loss_att 225.800949 loss_ctc 674.969421 loss_rnnt 260.532898 hw_loss 0.465364 lr 0.00003204 rank 3
2023-02-17 00:31:47,716 DEBUG TRAIN Batch 0/800 loss 364.133087 loss_att 316.076233 loss_ctc 428.872131 loss_rnnt 364.930298 hw_loss 0.341779 lr 0.00003204 rank 6
2023-02-17 00:31:47,717 DEBUG TRAIN Batch 0/800 loss 330.863678 loss_att 286.189362 loss_ctc 413.494110 loss_rnnt 328.605316 hw_loss 0.329701 lr 0.00003204 rank 0
2023-02-17 00:31:47,722 DEBUG TRAIN Batch 0/800 loss 373.995911 loss_att 259.788208 loss_ctc 1106.162964 loss_rnnt 298.959442 hw_loss 0.479473 lr 0.00003204 rank 2
2023-02-17 00:33:03,291 DEBUG TRAIN Batch 0/900 loss 319.648804 loss_att 283.141541 loss_ctc 328.875305 loss_rnnt 325.493469 hw_loss 0.424911 lr 0.00003604 rank 6
2023-02-17 00:33:03,294 DEBUG TRAIN Batch 0/900 loss 327.787323 loss_att 289.866119 loss_ctc 338.564270 loss_rnnt 333.737183 hw_loss 0.370252 lr 0.00003604 rank 2
2023-02-17 00:33:03,295 DEBUG TRAIN Batch 0/900 loss 307.626038 loss_att 269.424866 loss_ctc 318.346863 loss_rnnt 313.600800 hw_loss 0.442604 lr 0.00003604 rank 3
2023-02-17 00:33:03,296 DEBUG TRAIN Batch 0/900 loss 327.996460 loss_att 289.751312 loss_ctc 338.404510 loss_rnnt 334.080566 hw_loss 0.332218 lr 0.00003604 rank 0
2023-02-17 00:33:03,299 DEBUG TRAIN Batch 0/900 loss 312.890411 loss_att 275.582855 loss_ctc 322.084106 loss_rnnt 318.893402 hw_loss 0.436323 lr 0.00003604 rank 4
2023-02-17 00:33:03,315 DEBUG TRAIN Batch 0/900 loss 340.190826 loss_att 297.956055 loss_ctc 350.880005 loss_rnnt 346.999298 hw_loss 0.399776 lr 0.00003604 rank 1
2023-02-17 00:33:03,317 DEBUG TRAIN Batch 0/900 loss 319.047394 loss_att 281.330200 loss_ctc 329.670227 loss_rnnt 324.949890 hw_loss 0.421082 lr 0.00003604 rank 5
2023-02-17 00:33:03,362 DEBUG TRAIN Batch 0/900 loss 341.247406 loss_att 300.618439 loss_ctc 353.549164 loss_rnnt 347.557312 hw_loss 0.329331 lr 0.00003604 rank 7
2023-02-17 00:34:20,833 DEBUG TRAIN Batch 0/1000 loss 291.815338 loss_att 259.056274 loss_ctc 302.273621 loss_rnnt 296.783508 hw_loss 0.354771 lr 0.00004004 rank 5
2023-02-17 00:34:20,838 DEBUG TRAIN Batch 0/1000 loss 302.906158 loss_att 269.131134 loss_ctc 312.378845 loss_rnnt 308.218384 hw_loss 0.337046 lr 0.00004004 rank 1
2023-02-17 00:34:20,842 DEBUG TRAIN Batch 0/1000 loss 324.424469 loss_att 287.575592 loss_ctc 335.309601 loss_rnnt 330.107422 hw_loss 0.441517 lr 0.00004004 rank 3
2023-02-17 00:34:20,860 DEBUG TRAIN Batch 0/1000 loss 268.184448 loss_att 235.952362 loss_ctc 277.589569 loss_rnnt 273.166718 hw_loss 0.393967 lr 0.00004004 rank 2
2023-02-17 00:34:20,907 DEBUG TRAIN Batch 0/1000 loss 321.641937 loss_att 283.834747 loss_ctc 331.129944 loss_rnnt 327.731049 hw_loss 0.388553 lr 0.00004004 rank 7
2023-02-17 00:34:20,912 DEBUG TRAIN Batch 0/1000 loss 265.797546 loss_att 235.717926 loss_ctc 274.934570 loss_rnnt 270.399292 hw_loss 0.367363 lr 0.00004004 rank 6
2023-02-17 00:34:20,915 DEBUG TRAIN Batch 0/1000 loss 319.984650 loss_att 281.513672 loss_ctc 331.978821 loss_rnnt 325.925903 hw_loss 0.288259 lr 0.00004004 rank 4
2023-02-17 00:34:20,915 DEBUG TRAIN Batch 0/1000 loss 364.389893 loss_att 321.135925 loss_ctc 375.392822 loss_rnnt 371.396210 hw_loss 0.332694 lr 0.00004004 rank 0
2023-02-17 00:37:00,043 DEBUG TRAIN Batch 0/1100 loss 278.587494 loss_att 246.676605 loss_ctc 290.508942 loss_rnnt 283.127960 hw_loss 0.472881 lr 0.00004404 rank 6
2023-02-17 00:37:00,044 DEBUG TRAIN Batch 0/1100 loss 304.647705 loss_att 269.884216 loss_ctc 314.267944 loss_rnnt 310.099304 hw_loss 0.409436 lr 0.00004404 rank 7
2023-02-17 00:37:00,044 DEBUG TRAIN Batch 0/1100 loss 335.141571 loss_att 296.205963 loss_ctc 349.685577 loss_rnnt 340.839722 hw_loss 0.280767 lr 0.00004404 rank 3
2023-02-17 00:37:00,046 DEBUG TRAIN Batch 0/1100 loss 298.267303 loss_att 265.052002 loss_ctc 310.612122 loss_rnnt 303.068787 hw_loss 0.366727 lr 0.00004404 rank 4
2023-02-17 00:37:00,048 DEBUG TRAIN Batch 0/1100 loss 277.453674 loss_att 245.791718 loss_ctc 292.111725 loss_rnnt 281.620911 hw_loss 0.395174 lr 0.00004404 rank 5
2023-02-17 00:37:00,048 DEBUG TRAIN Batch 0/1100 loss 288.694305 loss_att 256.563660 loss_ctc 300.783783 loss_rnnt 293.312897 hw_loss 0.366757 lr 0.00004404 rank 0
2023-02-17 00:37:00,048 DEBUG TRAIN Batch 0/1100 loss 277.023834 loss_att 246.096924 loss_ctc 286.295807 loss_rnnt 281.782227 hw_loss 0.357594 lr 0.00004404 rank 1
2023-02-17 00:37:00,052 DEBUG TRAIN Batch 0/1100 loss 264.947754 loss_att 234.000092 loss_ctc 274.112152 loss_rnnt 269.723938 hw_loss 0.358970 lr 0.00004404 rank 2
2023-02-17 00:38:16,422 DEBUG TRAIN Batch 0/1200 loss 271.840179 loss_att 242.354736 loss_ctc 283.498993 loss_rnnt 276.017029 hw_loss 0.310777 lr 0.00004804 rank 5
2023-02-17 00:38:16,422 DEBUG TRAIN Batch 0/1200 loss 251.172531 loss_att 222.567261 loss_ctc 268.524719 loss_rnnt 254.377197 hw_loss 0.380174 lr 0.00004804 rank 3
2023-02-17 00:38:16,422 DEBUG TRAIN Batch 0/1200 loss 111.074211 loss_att 98.394958 loss_ctc 117.522034 loss_rnnt 112.546043 hw_loss 0.383072 lr 0.00004804 rank 1
2023-02-17 00:38:16,432 DEBUG TRAIN Batch 0/1200 loss 223.909897 loss_att 198.038544 loss_ctc 232.690842 loss_rnnt 227.679199 hw_loss 0.439072 lr 0.00004804 rank 0
2023-02-17 00:38:16,442 DEBUG TRAIN Batch 0/1200 loss 153.949463 loss_att 137.549835 loss_ctc 160.532028 loss_rnnt 156.055496 hw_loss 0.555390 lr 0.00004804 rank 2
2023-02-17 00:38:16,454 DEBUG TRAIN Batch 0/1200 loss 185.858887 loss_att 164.888901 loss_ctc 194.849609 loss_rnnt 188.646942 hw_loss 0.388496 lr 0.00004804 rank 4
2023-02-17 00:38:16,461 DEBUG TRAIN Batch 0/1200 loss 268.403931 loss_att 238.639191 loss_ctc 277.451752 loss_rnnt 272.910370 hw_loss 0.450284 lr 0.00004804 rank 6
2023-02-17 00:38:16,472 DEBUG TRAIN Batch 0/1200 loss 224.485580 loss_att 197.846680 loss_ctc 235.651901 loss_rnnt 228.136032 hw_loss 0.353436 lr 0.00004804 rank 7
2023-02-17 00:39:31,933 DEBUG TRAIN Batch 0/1300 loss 376.940369 loss_att 333.640778 loss_ctc 389.526001 loss_rnnt 383.697266 hw_loss 0.421782 lr 0.00005204 rank 4
2023-02-17 00:39:31,936 DEBUG TRAIN Batch 0/1300 loss 347.343933 loss_att 310.817108 loss_ctc 370.315643 loss_rnnt 351.436340 hw_loss 0.281407 lr 0.00005204 rank 0
2023-02-17 00:39:31,937 DEBUG TRAIN Batch 0/1300 loss 210.121490 loss_att 186.599243 loss_ctc 222.041428 loss_rnnt 213.095184 hw_loss 0.265181 lr 0.00005204 rank 3
2023-02-17 00:39:31,937 DEBUG TRAIN Batch 0/1300 loss 361.310181 loss_att 322.500092 loss_ctc 382.906433 loss_rnnt 366.027893 hw_loss 0.309017 lr 0.00005204 rank 2
2023-02-17 00:39:31,939 DEBUG TRAIN Batch 0/1300 loss 359.950745 loss_att 319.767548 loss_ctc 381.031494 loss_rnnt 364.981750 hw_loss 0.365305 lr 0.00005204 rank 1
2023-02-17 00:39:31,939 DEBUG TRAIN Batch 0/1300 loss 309.540405 loss_att 277.565369 loss_ctc 326.994995 loss_rnnt 313.440704 hw_loss 0.313913 lr 0.00005204 rank 7
2023-02-17 00:39:31,949 DEBUG TRAIN Batch 0/1300 loss 434.488678 loss_att 382.878479 loss_ctc 455.558777 loss_rnnt 441.804474 hw_loss 0.369173 lr 0.00005204 rank 6
2023-02-17 00:39:31,956 DEBUG TRAIN Batch 0/1300 loss 150.313080 loss_att 133.586365 loss_ctc 158.422409 loss_rnnt 152.313721 hw_loss 0.494010 lr 0.00005204 rank 5
2023-02-17 00:40:51,894 DEBUG TRAIN Batch 0/1400 loss 358.044861 loss_att 321.575348 loss_ctc 377.082611 loss_rnnt 362.565308 hw_loss 0.440821 lr 0.00005604 rank 0
2023-02-17 00:40:51,899 DEBUG TRAIN Batch 0/1400 loss 345.788818 loss_att 304.661560 loss_ctc 368.649902 loss_rnnt 350.759979 hw_loss 0.386525 lr 0.00005604 rank 6
2023-02-17 00:40:51,902 DEBUG TRAIN Batch 0/1400 loss 302.706207 loss_att 269.756439 loss_ctc 317.501892 loss_rnnt 307.145081 hw_loss 0.334335 lr 0.00005604 rank 3
2023-02-17 00:40:51,923 DEBUG TRAIN Batch 0/1400 loss 294.265167 loss_att 262.385925 loss_ctc 308.881378 loss_rnnt 298.481812 hw_loss 0.394426 lr 0.00005604 rank 4
2023-02-17 00:40:51,928 DEBUG TRAIN Batch 0/1400 loss 282.988434 loss_att 252.133270 loss_ctc 299.650757 loss_rnnt 286.733063 hw_loss 0.383892 lr 0.00005604 rank 7
2023-02-17 00:40:51,930 DEBUG TRAIN Batch 0/1400 loss 266.573669 loss_att 236.093994 loss_ctc 282.536835 loss_rnnt 270.324829 hw_loss 0.405651 lr 0.00005604 rank 2
2023-02-17 00:40:51,947 DEBUG TRAIN Batch 0/1400 loss 290.376251 loss_att 256.813904 loss_ctc 304.207947 loss_rnnt 295.033936 hw_loss 0.394790 lr 0.00005604 rank 1
2023-02-17 00:40:51,953 DEBUG TRAIN Batch 0/1400 loss 311.676605 loss_att 279.272736 loss_ctc 330.783173 loss_rnnt 315.399292 hw_loss 0.394848 lr 0.00005604 rank 5
2023-02-17 00:43:25,790 DEBUG TRAIN Batch 0/1500 loss 325.939819 loss_att 292.484192 loss_ctc 349.683777 loss_rnnt 329.273376 hw_loss 0.359463 lr 0.00006004 rank 3
2023-02-17 00:43:25,791 DEBUG TRAIN Batch 0/1500 loss 263.341156 loss_att 233.793839 loss_ctc 283.421204 loss_rnnt 266.407166 hw_loss 0.311520 lr 0.00006004 rank 1
2023-02-17 00:43:25,791 DEBUG TRAIN Batch 0/1500 loss 314.296265 loss_att 280.086548 loss_ctc 334.076111 loss_rnnt 318.266022 hw_loss 0.440397 lr 0.00006004 rank 2
2023-02-17 00:43:25,793 DEBUG TRAIN Batch 0/1500 loss 272.465637 loss_att 243.262451 loss_ctc 295.953827 loss_rnnt 274.981689 hw_loss 0.361588 lr 0.00006004 rank 5
2023-02-17 00:43:25,793 DEBUG TRAIN Batch 0/1500 loss 304.262909 loss_att 271.748108 loss_ctc 323.007050 loss_rnnt 308.075165 hw_loss 0.359103 lr 0.00006004 rank 0
2023-02-17 00:43:25,794 DEBUG TRAIN Batch 0/1500 loss 345.806366 loss_att 308.988037 loss_ctc 365.815552 loss_rnnt 350.270508 hw_loss 0.434325 lr 0.00006004 rank 7
2023-02-17 00:43:25,794 DEBUG TRAIN Batch 0/1500 loss 291.513214 loss_att 259.212463 loss_ctc 307.517212 loss_rnnt 295.656342 hw_loss 0.343364 lr 0.00006004 rank 6
2023-02-17 00:43:25,796 DEBUG TRAIN Batch 0/1500 loss 331.575562 loss_att 297.345123 loss_ctc 359.178772 loss_rnnt 334.534271 hw_loss 0.388002 lr 0.00006004 rank 4
2023-02-17 00:44:42,026 DEBUG TRAIN Batch 0/1600 loss 273.210602 loss_att 244.326813 loss_ctc 301.861572 loss_rnnt 274.927673 hw_loss 0.449122 lr 0.00006404 rank 4
2023-02-17 00:44:42,033 DEBUG TRAIN Batch 0/1600 loss 314.889862 loss_att 282.455170 loss_ctc 341.150635 loss_rnnt 317.745422 hw_loss 0.243630 lr 0.00006404 rank 1
2023-02-17 00:44:42,035 DEBUG TRAIN Batch 0/1600 loss 315.415009 loss_att 279.225983 loss_ctc 339.973267 loss_rnnt 319.177795 hw_loss 0.376040 lr 0.00006404 rank 2
2023-02-17 00:44:42,037 DEBUG TRAIN Batch 0/1600 loss 281.792084 loss_att 251.694489 loss_ctc 301.664886 loss_rnnt 284.967255 hw_loss 0.365016 lr 0.00006404 rank 6
2023-02-17 00:44:42,037 DEBUG TRAIN Batch 0/1600 loss 326.619812 loss_att 291.584229 loss_ctc 355.359528 loss_rnnt 329.577637 hw_loss 0.407487 lr 0.00006404 rank 5
2023-02-17 00:44:42,039 DEBUG TRAIN Batch 0/1600 loss 330.661530 loss_att 295.994202 loss_ctc 350.614075 loss_rnnt 334.740448 hw_loss 0.364167 lr 0.00006404 rank 3
2023-02-17 00:44:42,044 DEBUG TRAIN Batch 0/1600 loss 241.620560 loss_att 216.420059 loss_ctc 261.512573 loss_rnnt 243.734283 hw_loss 0.513924 lr 0.00006404 rank 7
2023-02-17 00:44:42,082 DEBUG TRAIN Batch 0/1600 loss 282.523346 loss_att 252.853271 loss_ctc 303.970001 loss_rnnt 285.413513 hw_loss 0.345512 lr 0.00006404 rank 0
2023-02-17 00:45:59,087 DEBUG TRAIN Batch 0/1700 loss 288.964813 loss_att 257.266449 loss_ctc 314.724854 loss_rnnt 291.689636 hw_loss 0.337821 lr 0.00006804 rank 6
2023-02-17 00:45:59,089 DEBUG TRAIN Batch 0/1700 loss 262.008850 loss_att 233.833771 loss_ctc 280.710205 loss_rnnt 264.933716 hw_loss 0.406197 lr 0.00006804 rank 1
2023-02-17 00:45:59,089 DEBUG TRAIN Batch 0/1700 loss 263.927307 loss_att 235.198273 loss_ctc 286.259674 loss_rnnt 266.497772 hw_loss 0.370703 lr 0.00006804 rank 7
2023-02-17 00:45:59,090 DEBUG TRAIN Batch 0/1700 loss 288.116394 loss_att 256.245697 loss_ctc 311.923889 loss_rnnt 291.174408 hw_loss 0.265902 lr 0.00006804 rank 2
2023-02-17 00:45:59,094 DEBUG TRAIN Batch 0/1700 loss 310.358368 loss_att 278.172974 loss_ctc 338.778229 loss_rnnt 312.808716 hw_loss 0.370076 lr 0.00006804 rank 5
2023-02-17 00:45:59,094 DEBUG TRAIN Batch 0/1700 loss 318.604431 loss_att 285.508087 loss_ctc 349.789673 loss_rnnt 320.892517 hw_loss 0.324710 lr 0.00006804 rank 0
2023-02-17 00:45:59,098 DEBUG TRAIN Batch 0/1700 loss 275.518524 loss_att 245.142090 loss_ctc 296.799774 loss_rnnt 278.560486 hw_loss 0.367240 lr 0.00006804 rank 4
2023-02-17 00:45:59,099 DEBUG TRAIN Batch 0/1700 loss 324.838684 loss_att 286.831421 loss_ctc 348.003357 loss_rnnt 329.184204 hw_loss 0.313719 lr 0.00006804 rank 3
2023-02-17 00:48:26,117 DEBUG TRAIN Batch 0/1800 loss 276.030426 loss_att 244.035477 loss_ctc 301.683197 loss_rnnt 278.836578 hw_loss 0.323353 lr 0.00007204 rank 7
2023-02-17 00:48:26,117 DEBUG TRAIN Batch 0/1800 loss 220.606400 loss_att 195.642838 loss_ctc 240.954269 loss_rnnt 222.678146 hw_loss 0.389844 lr 0.00007204 rank 4
2023-02-17 00:48:26,117 DEBUG TRAIN Batch 0/1800 loss 264.377350 loss_att 234.890106 loss_ctc 288.701141 loss_rnnt 266.818756 hw_loss 0.399067 lr 0.00007204 rank 2
2023-02-17 00:48:26,117 DEBUG TRAIN Batch 0/1800 loss 219.826080 loss_att 197.547653 loss_ctc 245.122513 loss_rnnt 220.709381 hw_loss 0.374069 lr 0.00007204 rank 0
2023-02-17 00:48:26,120 DEBUG TRAIN Batch 0/1800 loss 231.966827 loss_att 206.860931 loss_ctc 255.905762 loss_rnnt 233.568665 hw_loss 0.426519 lr 0.00007204 rank 3
2023-02-17 00:48:26,124 DEBUG TRAIN Batch 0/1800 loss 87.115105 loss_att 77.126755 loss_ctc 94.726463 loss_rnnt 87.856323 hw_loss 0.452989 lr 0.00007204 rank 1
2023-02-17 00:48:26,147 DEBUG TRAIN Batch 0/1800 loss 293.016724 loss_att 262.318787 loss_ctc 312.782043 loss_rnnt 296.332123 hw_loss 0.354043 lr 0.00007204 rank 5
2023-02-17 00:48:26,161 DEBUG TRAIN Batch 0/1800 loss 271.099854 loss_att 242.888916 loss_ctc 300.197998 loss_rnnt 272.652710 hw_loss 0.392940 lr 0.00007204 rank 6
2023-02-17 00:49:43,062 DEBUG TRAIN Batch 0/1900 loss 207.561996 loss_att 185.371323 loss_ctc 226.135590 loss_rnnt 209.316452 hw_loss 0.388542 lr 0.00007604 rank 3
2023-02-17 00:49:43,062 DEBUG TRAIN Batch 0/1900 loss 172.010727 loss_att 152.774689 loss_ctc 189.139969 loss_rnnt 173.376282 hw_loss 0.370804 lr 0.00007604 rank 0
2023-02-17 00:49:43,064 DEBUG TRAIN Batch 0/1900 loss 221.466537 loss_att 197.273056 loss_ctc 247.122528 loss_rnnt 222.651703 hw_loss 0.436393 lr 0.00007604 rank 7
2023-02-17 00:49:43,065 DEBUG TRAIN Batch 0/1900 loss 225.155121 loss_att 204.252243 loss_ctc 251.469452 loss_rnnt 225.648102 hw_loss 0.335664 lr 0.00007604 rank 5
2023-02-17 00:49:43,066 DEBUG TRAIN Batch 0/1900 loss 336.753082 loss_att 301.134521 loss_ctc 369.239349 loss_rnnt 339.371216 hw_loss 0.326446 lr 0.00007604 rank 4
2023-02-17 00:49:43,073 DEBUG TRAIN Batch 0/1900 loss 222.674332 loss_att 200.399857 loss_ctc 246.145599 loss_rnnt 223.787521 hw_loss 0.397881 lr 0.00007604 rank 1
2023-02-17 00:49:43,077 DEBUG TRAIN Batch 0/1900 loss 184.827011 loss_att 163.761887 loss_ctc 202.079803 loss_rnnt 186.521729 hw_loss 0.408621 lr 0.00007604 rank 2
2023-02-17 00:49:43,121 DEBUG TRAIN Batch 0/1900 loss 174.356003 loss_att 157.335342 loss_ctc 188.221619 loss_rnnt 175.666122 hw_loss 0.459894 lr 0.00007604 rank 6
2023-02-17 00:50:59,448 DEBUG TRAIN Batch 0/2000 loss 302.928314 loss_att 270.418701 loss_ctc 329.846222 loss_rnnt 305.596985 hw_loss 0.457875 lr 0.00008004 rank 6
2023-02-17 00:50:59,460 DEBUG TRAIN Batch 0/2000 loss 306.221344 loss_att 274.780121 loss_ctc 342.981537 loss_rnnt 307.404877 hw_loss 0.381322 lr 0.00008004 rank 7
2023-02-17 00:50:59,463 DEBUG TRAIN Batch 0/2000 loss 317.670319 loss_att 285.280273 loss_ctc 350.364929 loss_rnnt 319.616943 hw_loss 0.322609 lr 0.00008004 rank 3
2023-02-17 00:50:59,463 DEBUG TRAIN Batch 0/2000 loss 270.384613 loss_att 243.135880 loss_ctc 299.042358 loss_rnnt 271.810028 hw_loss 0.381239 lr 0.00008004 rank 2
2023-02-17 00:50:59,463 DEBUG TRAIN Batch 0/2000 loss 320.066254 loss_att 284.554749 loss_ctc 352.437744 loss_rnnt 322.664551 hw_loss 0.352130 lr 0.00008004 rank 5
2023-02-17 00:50:59,482 DEBUG TRAIN Batch 0/2000 loss 343.232452 loss_att 306.638031 loss_ctc 376.207367 loss_rnnt 346.057068 hw_loss 0.183103 lr 0.00008004 rank 1
2023-02-17 00:50:59,484 DEBUG TRAIN Batch 0/2000 loss 329.260284 loss_att 294.119415 loss_ctc 359.363739 loss_rnnt 332.096436 hw_loss 0.334137 lr 0.00008004 rank 0
2023-02-17 00:50:59,487 DEBUG TRAIN Batch 0/2000 loss 295.054871 loss_att 265.756775 loss_ctc 326.803894 loss_rnnt 296.505188 hw_loss 0.330235 lr 0.00008004 rank 4
2023-02-17 00:52:19,773 DEBUG TRAIN Batch 0/2100 loss 352.816223 loss_att 314.717926 loss_ctc 377.330505 loss_rnnt 356.989136 hw_loss 0.334049 lr 0.00008404 rank 3
2023-02-17 00:52:19,780 DEBUG TRAIN Batch 0/2100 loss 292.926392 loss_att 261.711700 loss_ctc 321.129211 loss_rnnt 295.195038 hw_loss 0.401190 lr 0.00008404 rank 5
2023-02-17 00:52:19,795 DEBUG TRAIN Batch 0/2100 loss 311.803741 loss_att 279.289368 loss_ctc 348.801941 loss_rnnt 313.189331 hw_loss 0.345398 lr 0.00008404 rank 7
2023-02-17 00:52:19,808 DEBUG TRAIN Batch 0/2100 loss 285.969208 loss_att 254.186478 loss_ctc 312.868896 loss_rnnt 288.554626 hw_loss 0.345920 lr 0.00008404 rank 1
2023-02-17 00:52:19,821 DEBUG TRAIN Batch 0/2100 loss 306.892120 loss_att 273.336426 loss_ctc 334.050049 loss_rnnt 309.791718 hw_loss 0.357199 lr 0.00008404 rank 4
2023-02-17 00:52:19,829 DEBUG TRAIN Batch 0/2100 loss 279.755615 loss_att 250.658768 loss_ctc 311.979584 loss_rnnt 281.156738 hw_loss 0.228239 lr 0.00008404 rank 0
2023-02-17 00:52:19,844 DEBUG TRAIN Batch 0/2100 loss 353.945801 loss_att 313.941925 loss_ctc 385.486176 loss_rnnt 357.539429 hw_loss 0.378364 lr 0.00008404 rank 2
2023-02-17 00:52:19,844 DEBUG TRAIN Batch 0/2100 loss 259.076233 loss_att 230.837158 loss_ctc 297.553375 loss_rnnt 259.423920 hw_loss 0.318460 lr 0.00008404 rank 6
2023-02-17 00:54:31,410 DEBUG TRAIN Batch 0/2200 loss 267.155090 loss_att 243.476044 loss_ctc 297.118408 loss_rnnt 267.651611 hw_loss 0.457876 lr 0.00008804 rank 6
2023-02-17 00:54:31,411 DEBUG TRAIN Batch 0/2200 loss 311.929596 loss_att 278.386841 loss_ctc 337.758362 loss_rnnt 315.014465 hw_loss 0.337185 lr 0.00008804 rank 0
2023-02-17 00:54:31,411 DEBUG TRAIN Batch 0/2200 loss 344.147217 loss_att 307.063416 loss_ctc 380.349976 loss_rnnt 346.508026 hw_loss 0.429236 lr 0.00008804 rank 5
2023-02-17 00:54:31,411 DEBUG TRAIN Batch 0/2200 loss 289.026917 loss_att 257.424683 loss_ctc 322.024567 loss_rnnt 290.716766 hw_loss 0.432952 lr 0.00008804 rank 2
2023-02-17 00:54:31,413 DEBUG TRAIN Batch 0/2200 loss 321.157440 loss_att 285.143341 loss_ctc 353.498596 loss_rnnt 323.880432 hw_loss 0.314306 lr 0.00008804 rank 4
2023-02-17 00:54:31,413 DEBUG TRAIN Batch 0/2200 loss 235.563553 loss_att 209.633255 loss_ctc 264.442139 loss_rnnt 236.710983 hw_loss 0.352773 lr 0.00008804 rank 3
2023-02-17 00:54:31,417 DEBUG TRAIN Batch 0/2200 loss 258.116852 loss_att 230.320160 loss_ctc 287.239258 loss_rnnt 259.645813 hw_loss 0.276342 lr 0.00008804 rank 1
2023-02-17 00:54:31,418 DEBUG TRAIN Batch 0/2200 loss 271.614929 loss_att 243.578751 loss_ctc 298.046997 loss_rnnt 273.509735 hw_loss 0.352821 lr 0.00008804 rank 7
2023-02-17 00:55:46,678 DEBUG TRAIN Batch 0/2300 loss 272.105377 loss_att 241.177750 loss_ctc 298.622437 loss_rnnt 274.539551 hw_loss 0.404481 lr 0.00009204 rank 6
2023-02-17 00:55:46,684 DEBUG TRAIN Batch 0/2300 loss 266.188110 loss_att 238.629684 loss_ctc 300.537964 loss_rnnt 266.912354 hw_loss 0.388919 lr 0.00009204 rank 4
2023-02-17 00:55:46,684 DEBUG TRAIN Batch 0/2300 loss 314.196350 loss_att 281.758911 loss_ctc 341.754700 loss_rnnt 316.831177 hw_loss 0.334141 lr 0.00009204 rank 7
2023-02-17 00:55:46,684 DEBUG TRAIN Batch 0/2300 loss 310.945892 loss_att 278.632904 loss_ctc 349.181641 loss_rnnt 312.149750 hw_loss 0.301291 lr 0.00009204 rank 3
2023-02-17 00:55:46,688 DEBUG TRAIN Batch 0/2300 loss 297.129791 loss_att 267.355408 loss_ctc 326.954590 loss_rnnt 298.942200 hw_loss 0.311005 lr 0.00009204 rank 5
2023-02-17 00:55:46,694 DEBUG TRAIN Batch 0/2300 loss 292.831604 loss_att 263.389618 loss_ctc 323.097748 loss_rnnt 294.479370 hw_loss 0.384560 lr 0.00009204 rank 1
2023-02-17 00:55:46,716 DEBUG TRAIN Batch 0/2300 loss 301.190399 loss_att 269.611389 loss_ctc 337.407532 loss_rnnt 302.484741 hw_loss 0.360985 lr 0.00009204 rank 0
2023-02-17 00:55:46,725 DEBUG TRAIN Batch 0/2300 loss 290.980621 loss_att 258.701233 loss_ctc 320.651489 loss_rnnt 293.266327 hw_loss 0.401375 lr 0.00009204 rank 2
2023-02-17 00:57:03,653 DEBUG TRAIN Batch 0/2400 loss 282.962769 loss_att 255.934494 loss_ctc 311.072937 loss_rnnt 284.419464 hw_loss 0.376741 lr 0.00009604 rank 6
2023-02-17 00:57:03,653 DEBUG TRAIN Batch 0/2400 loss 312.844604 loss_att 278.244141 loss_ctc 342.902985 loss_rnnt 315.561310 hw_loss 0.366684 lr 0.00009604 rank 7
2023-02-17 00:57:03,654 DEBUG TRAIN Batch 0/2400 loss 232.324554 loss_att 205.380798 loss_ctc 253.425491 loss_rnnt 234.711884 hw_loss 0.352477 lr 0.00009604 rank 4
2023-02-17 00:57:03,655 DEBUG TRAIN Batch 0/2400 loss 284.756073 loss_att 254.497635 loss_ctc 319.410126 loss_rnnt 285.993866 hw_loss 0.362549 lr 0.00009604 rank 2
2023-02-17 00:57:03,673 DEBUG TRAIN Batch 0/2400 loss 253.724991 loss_att 227.560715 loss_ctc 282.553284 loss_rnnt 254.940231 hw_loss 0.325981 lr 0.00009604 rank 3
2023-02-17 00:57:03,698 DEBUG TRAIN Batch 0/2400 loss 116.572800 loss_att 105.587830 loss_ctc 127.228409 loss_rnnt 117.118683 hw_loss 0.431937 lr 0.00009604 rank 1
2023-02-17 00:57:03,700 DEBUG TRAIN Batch 0/2400 loss 254.070175 loss_att 228.290680 loss_ctc 278.820862 loss_rnnt 255.748062 hw_loss 0.333596 lr 0.00009604 rank 5
2023-02-17 00:57:03,705 DEBUG TRAIN Batch 0/2400 loss 214.365555 loss_att 193.403427 loss_ctc 240.490387 loss_rnnt 214.883926 hw_loss 0.357643 lr 0.00009604 rank 0
2023-02-17 00:59:22,498 DEBUG TRAIN Batch 0/2500 loss 188.109467 loss_att 167.137512 loss_ctc 206.666321 loss_rnnt 189.586243 hw_loss 0.456327 lr 0.00010004 rank 6
2023-02-17 00:59:22,501 DEBUG TRAIN Batch 0/2500 loss 122.142723 loss_att 109.727798 loss_ctc 136.882156 loss_rnnt 122.367653 hw_loss 0.548983 lr 0.00010004 rank 0
2023-02-17 00:59:22,501 DEBUG TRAIN Batch 0/2500 loss 197.740982 loss_att 175.648163 loss_ctc 220.157516 loss_rnnt 198.913239 hw_loss 0.482659 lr 0.00010004 rank 2
2023-02-17 00:59:22,503 DEBUG TRAIN Batch 0/2500 loss 194.531540 loss_att 177.873520 loss_ctc 216.810120 loss_rnnt 194.709290 hw_loss 0.343835 lr 0.00010004 rank 4
2023-02-17 00:59:22,505 DEBUG TRAIN Batch 0/2500 loss 230.384262 loss_att 206.204529 loss_ctc 251.261093 loss_rnnt 232.244583 hw_loss 0.360065 lr 0.00010004 rank 7
2023-02-17 00:59:22,517 DEBUG TRAIN Batch 0/2500 loss 255.431335 loss_att 228.764557 loss_ctc 281.291199 loss_rnnt 257.105164 hw_loss 0.396670 lr 0.00010004 rank 5
2023-02-17 00:59:22,571 DEBUG TRAIN Batch 0/2500 loss 196.147369 loss_att 177.524994 loss_ctc 218.516113 loss_rnnt 196.638214 hw_loss 0.470859 lr 0.00010004 rank 3
2023-02-17 00:59:22,573 DEBUG TRAIN Batch 0/2500 loss 352.228394 loss_att 315.344635 loss_ctc 386.226562 loss_rnnt 354.935638 hw_loss 0.255800 lr 0.00010004 rank 1
2023-02-17 01:00:38,370 DEBUG TRAIN Batch 0/2600 loss 149.541534 loss_att 135.172012 loss_ctc 168.375351 loss_rnnt 149.653641 hw_loss 0.469888 lr 0.00010404 rank 7
2023-02-17 01:00:38,371 DEBUG TRAIN Batch 0/2600 loss 121.245415 loss_att 110.014328 loss_ctc 132.644150 loss_rnnt 121.735954 hw_loss 0.442210 lr 0.00010404 rank 5
2023-02-17 01:00:38,374 DEBUG TRAIN Batch 0/2600 loss 275.819794 loss_att 249.933594 loss_ctc 303.072266 loss_rnnt 277.157654 hw_loss 0.385765 lr 0.00010404 rank 4
2023-02-17 01:00:38,379 DEBUG TRAIN Batch 0/2600 loss 315.791992 loss_att 281.736298 loss_ctc 346.248474 loss_rnnt 318.323334 hw_loss 0.410529 lr 0.00010404 rank 1
2023-02-17 01:00:38,378 DEBUG TRAIN Batch 0/2600 loss 301.680084 loss_att 273.689392 loss_ctc 337.408264 loss_rnnt 302.299744 hw_loss 0.402618 lr 0.00010404 rank 0
2023-02-17 01:00:38,378 DEBUG TRAIN Batch 0/2600 loss 300.232147 loss_att 268.690552 loss_ctc 330.337952 loss_rnnt 302.351868 hw_loss 0.327176 lr 0.00010404 rank 2
2023-02-17 01:00:38,380 DEBUG TRAIN Batch 0/2600 loss 241.515930 loss_att 221.009918 loss_ctc 260.732697 loss_rnnt 242.843109 hw_loss 0.397127 lr 0.00010404 rank 6
2023-02-17 01:00:38,422 DEBUG TRAIN Batch 0/2600 loss 326.194916 loss_att 288.651733 loss_ctc 360.500916 loss_rnnt 328.996674 hw_loss 0.248958 lr 0.00010404 rank 3
2023-02-17 01:01:53,571 DEBUG TRAIN Batch 0/2700 loss 293.082062 loss_att 264.225037 loss_ctc 328.588013 loss_rnnt 293.916473 hw_loss 0.380367 lr 0.00010804 rank 1
2023-02-17 01:01:53,574 DEBUG TRAIN Batch 0/2700 loss 242.383087 loss_att 215.493134 loss_ctc 274.196747 loss_rnnt 243.358688 hw_loss 0.301074 lr 0.00010804 rank 5
2023-02-17 01:01:53,574 DEBUG TRAIN Batch 0/2700 loss 296.650238 loss_att 269.229553 loss_ctc 326.447845 loss_rnnt 297.969482 hw_loss 0.359808 lr 0.00010804 rank 2
2023-02-17 01:01:53,576 DEBUG TRAIN Batch 0/2700 loss 303.901520 loss_att 275.046814 loss_ctc 338.469513 loss_rnnt 304.905579 hw_loss 0.295932 lr 0.00010804 rank 7
2023-02-17 01:01:53,601 DEBUG TRAIN Batch 0/2700 loss 320.321564 loss_att 287.906403 loss_ctc 353.828125 loss_rnnt 322.182159 hw_loss 0.290341 lr 0.00010804 rank 4
2023-02-17 01:01:53,614 DEBUG TRAIN Batch 0/2700 loss 315.935608 loss_att 282.989502 loss_ctc 349.984253 loss_rnnt 317.805786 hw_loss 0.336106 lr 0.00010804 rank 3
2023-02-17 01:01:53,615 DEBUG TRAIN Batch 0/2700 loss 306.270355 loss_att 275.798767 loss_ctc 341.210632 loss_rnnt 307.587097 hw_loss 0.222856 lr 0.00010804 rank 6
2023-02-17 01:01:53,630 DEBUG TRAIN Batch 0/2700 loss 312.642487 loss_att 279.524475 loss_ctc 347.461273 loss_rnnt 314.463135 hw_loss 0.300858 lr 0.00010804 rank 0
2023-02-17 01:03:13,110 DEBUG TRAIN Batch 0/2800 loss 263.939362 loss_att 236.610947 loss_ctc 294.073456 loss_rnnt 265.188599 hw_loss 0.372319 lr 0.00011204 rank 6
2023-02-17 01:03:13,126 DEBUG TRAIN Batch 0/2800 loss 274.954956 loss_att 248.512939 loss_ctc 309.681580 loss_rnnt 275.429291 hw_loss 0.344786 lr 0.00011204 rank 7
2023-02-17 01:03:13,128 DEBUG TRAIN Batch 0/2800 loss 333.608124 loss_att 302.439056 loss_ctc 367.404785 loss_rnnt 335.190674 hw_loss 0.272021 lr 0.00011204 rank 2
2023-02-17 01:03:13,130 DEBUG TRAIN Batch 0/2800 loss 285.835938 loss_att 256.523621 loss_ctc 322.315125 loss_rnnt 286.626312 hw_loss 0.390385 lr 0.00011204 rank 1
2023-02-17 01:03:13,151 DEBUG TRAIN Batch 0/2800 loss 275.463531 loss_att 248.479187 loss_ctc 308.507874 loss_rnnt 276.276459 hw_loss 0.333756 lr 0.00011204 rank 3
2023-02-17 01:03:13,157 DEBUG TRAIN Batch 0/2800 loss 296.959290 loss_att 271.468994 loss_ctc 329.765961 loss_rnnt 297.499847 hw_loss 0.343646 lr 0.00011204 rank 5
2023-02-17 01:03:13,157 DEBUG TRAIN Batch 0/2800 loss 348.675964 loss_att 312.587860 loss_ctc 381.187134 loss_rnnt 351.447205 hw_loss 0.209223 lr 0.00011204 rank 4
2023-02-17 01:03:13,196 DEBUG TRAIN Batch 0/2800 loss 276.384796 loss_att 250.247040 loss_ctc 308.637268 loss_rnnt 277.203552 hw_loss 0.203373 lr 0.00011204 rank 0
2023-02-17 01:05:34,220 DEBUG TRAIN Batch 0/2900 loss 295.184601 loss_att 264.999298 loss_ctc 328.182922 loss_rnnt 296.651062 hw_loss 0.320288 lr 0.00011604 rank 7
2023-02-17 01:05:34,236 DEBUG TRAIN Batch 0/2900 loss 325.324371 loss_att 296.671143 loss_ctc 353.016602 loss_rnnt 327.169891 hw_loss 0.361563 lr 0.00011604 rank 4
2023-02-17 01:05:34,237 DEBUG TRAIN Batch 0/2900 loss 321.007019 loss_att 292.369690 loss_ctc 347.221222 loss_rnnt 323.107178 hw_loss 0.247688 lr 0.00011604 rank 0
2023-02-17 01:05:34,238 DEBUG TRAIN Batch 0/2900 loss 222.253113 loss_att 201.619797 loss_ctc 244.792648 loss_rnnt 223.162292 hw_loss 0.397915 lr 0.00011604 rank 1
2023-02-17 01:05:34,239 DEBUG TRAIN Batch 0/2900 loss 280.962708 loss_att 255.841248 loss_ctc 307.982117 loss_rnnt 282.222443 hw_loss 0.303649 lr 0.00011604 rank 2
2023-02-17 01:05:34,245 DEBUG TRAIN Batch 0/2900 loss 261.506287 loss_att 237.404358 loss_ctc 284.998657 loss_rnnt 262.992615 hw_loss 0.378240 lr 0.00011604 rank 6
2023-02-17 01:05:34,257 DEBUG TRAIN Batch 0/2900 loss 270.455627 loss_att 244.679504 loss_ctc 295.803040 loss_rnnt 272.065125 hw_loss 0.311327 lr 0.00011604 rank 3
2023-02-17 01:05:34,256 DEBUG TRAIN Batch 0/2900 loss 320.144409 loss_att 290.574951 loss_ctc 347.850952 loss_rnnt 322.183167 hw_loss 0.339174 lr 0.00011604 rank 5
2023-02-17 01:06:50,167 DEBUG TRAIN Batch 0/3000 loss 258.820404 loss_att 233.861450 loss_ctc 287.949860 loss_rnnt 259.768280 hw_loss 0.299977 lr 0.00012004 rank 6
2023-02-17 01:06:50,169 DEBUG TRAIN Batch 0/3000 loss 275.982941 loss_att 247.105606 loss_ctc 302.138672 loss_rnnt 278.098572 hw_loss 0.323275 lr 0.00012004 rank 0
2023-02-17 01:06:50,172 DEBUG TRAIN Batch 0/3000 loss 259.248871 loss_att 235.590210 loss_ctc 282.744415 loss_rnnt 260.631256 hw_loss 0.406086 lr 0.00012004 rank 3
2023-02-17 01:06:50,172 DEBUG TRAIN Batch 0/3000 loss 170.611633 loss_att 158.295044 loss_ctc 190.416992 loss_rnnt 170.190521 hw_loss 0.456959 lr 0.00012004 rank 1
2023-02-17 01:06:50,174 DEBUG TRAIN Batch 0/3000 loss 262.931122 loss_att 242.583405 loss_ctc 289.263702 loss_rnnt 263.294983 hw_loss 0.364988 lr 0.00012004 rank 5
2023-02-17 01:06:50,186 DEBUG TRAIN Batch 0/3000 loss 265.223785 loss_att 246.018417 loss_ctc 288.873932 loss_rnnt 265.714905 hw_loss 0.368627 lr 0.00012004 rank 4
2023-02-17 01:06:50,201 DEBUG TRAIN Batch 0/3000 loss 262.597504 loss_att 242.609482 loss_ctc 289.756470 loss_rnnt 262.796204 hw_loss 0.333199 lr 0.00012004 rank 2
2023-02-17 01:06:50,243 DEBUG TRAIN Batch 0/3000 loss 275.566193 loss_att 251.754608 loss_ctc 299.361572 loss_rnnt 276.950684 hw_loss 0.384597 lr 0.00012004 rank 7
2023-02-17 01:08:06,011 DEBUG TRAIN Batch 0/3100 loss 206.083481 loss_att 193.172485 loss_ctc 228.054855 loss_rnnt 205.563354 hw_loss 0.324031 lr 0.00012404 rank 6
2023-02-17 01:08:06,013 DEBUG TRAIN Batch 0/3100 loss 156.215851 loss_att 144.800018 loss_ctc 175.260864 loss_rnnt 155.701889 hw_loss 0.483362 lr 0.00012404 rank 0
2023-02-17 01:08:06,028 DEBUG TRAIN Batch 0/3100 loss 294.508728 loss_att 270.667297 loss_ctc 310.522095 loss_rnnt 296.942108 hw_loss 0.374611 lr 0.00012404 rank 1
2023-02-17 01:08:06,037 DEBUG TRAIN Batch 0/3100 loss 218.425583 loss_att 203.213684 loss_ctc 239.623352 loss_rnnt 218.456680 hw_loss 0.346694 lr 0.00012404 rank 5
2023-02-17 01:08:06,048 DEBUG TRAIN Batch 0/3100 loss 225.595413 loss_att 208.544983 loss_ctc 247.960602 loss_rnnt 225.809082 hw_loss 0.401955 lr 0.00012404 rank 3
2023-02-17 01:08:06,062 DEBUG TRAIN Batch 0/3100 loss 174.852951 loss_att 162.734116 loss_ctc 185.359070 loss_rnnt 175.665771 hw_loss 0.394034 lr 0.00012404 rank 2
2023-02-17 01:08:06,074 DEBUG TRAIN Batch 0/3100 loss 199.064377 loss_att 187.689575 loss_ctc 220.112915 loss_rnnt 198.400650 hw_loss 0.247867 lr 0.00012404 rank 4
2023-02-17 01:08:06,080 DEBUG TRAIN Batch 0/3100 loss 209.954453 loss_att 198.819977 loss_ctc 231.549042 loss_rnnt 209.125366 hw_loss 0.331309 lr 0.00012404 rank 7
2023-02-17 01:10:31,152 DEBUG TRAIN Batch 0/3200 loss 297.434937 loss_att 280.175232 loss_ctc 316.934662 loss_rnnt 298.108521 hw_loss 0.334464 lr 0.00012804 rank 6
2023-02-17 01:10:31,153 DEBUG TRAIN Batch 0/3200 loss 207.915176 loss_att 201.289108 loss_ctc 237.828491 loss_rnnt 205.021271 hw_loss 0.432485 lr 0.00012804 rank 1
2023-02-17 01:10:31,153 DEBUG TRAIN Batch 0/3200 loss 161.004333 loss_att 152.147827 loss_ctc 173.148163 loss_rnnt 160.943207 hw_loss 0.399851 lr 0.00012804 rank 5
2023-02-17 01:10:31,154 DEBUG TRAIN Batch 0/3200 loss 260.810150 loss_att 239.182831 loss_ctc 284.749420 loss_rnnt 261.738098 hw_loss 0.385520 lr 0.00012804 rank 7
2023-02-17 01:10:31,155 DEBUG TRAIN Batch 0/3200 loss 325.119263 loss_att 297.642334 loss_ctc 355.158997 loss_rnnt 326.437561 hw_loss 0.322133 lr 0.00012804 rank 2
2023-02-17 01:10:31,156 DEBUG TRAIN Batch 0/3200 loss 151.943268 loss_att 143.254135 loss_ctc 168.048813 loss_rnnt 151.315643 hw_loss 0.408865 lr 0.00012804 rank 3
2023-02-17 01:10:31,201 DEBUG TRAIN Batch 0/3200 loss 102.737419 loss_att 96.838913 loss_ctc 114.416382 loss_rnnt 102.178566 hw_loss 0.340040 lr 0.00012804 rank 4
2023-02-17 01:10:31,205 DEBUG TRAIN Batch 0/3200 loss 286.992188 loss_att 269.363129 loss_ctc 311.409302 loss_rnnt 287.067352 hw_loss 0.365675 lr 0.00012804 rank 0
2023-02-17 01:11:46,792 DEBUG TRAIN Batch 0/3300 loss 289.953094 loss_att 276.667358 loss_ctc 310.824280 loss_rnnt 289.621582 hw_loss 0.385952 lr 0.00013204 rank 3
2023-02-17 01:11:46,792 DEBUG TRAIN Batch 0/3300 loss 265.511353 loss_att 257.325409 loss_ctc 295.365631 loss_rnnt 263.007263 hw_loss 0.301348 lr 0.00013204 rank 0
2023-02-17 01:11:46,796 DEBUG TRAIN Batch 0/3300 loss 244.893692 loss_att 239.344147 loss_ctc 263.444794 loss_rnnt 243.352539 hw_loss 0.332944 lr 0.00013204 rank 2
2023-02-17 01:11:46,796 DEBUG TRAIN Batch 0/3300 loss 237.398193 loss_att 234.790192 loss_ctc 254.168610 loss_rnnt 235.457016 hw_loss 0.425105 lr 0.00013204 rank 5
2023-02-17 01:11:46,798 DEBUG TRAIN Batch 0/3300 loss 337.963989 loss_att 323.056946 loss_ctc 358.818726 loss_rnnt 337.978241 hw_loss 0.349736 lr 0.00013204 rank 7
2023-02-17 01:11:46,801 DEBUG TRAIN Batch 0/3300 loss 243.969528 loss_att 232.661011 loss_ctc 263.583405 loss_rnnt 243.434174 hw_loss 0.341026 lr 0.00013204 rank 6
2023-02-17 01:11:46,805 DEBUG TRAIN Batch 0/3300 loss 245.334732 loss_att 232.320999 loss_ctc 267.687866 loss_rnnt 244.761505 hw_loss 0.366634 lr 0.00013204 rank 4
2023-02-17 01:11:46,830 DEBUG TRAIN Batch 0/3300 loss 301.516388 loss_att 283.481659 loss_ctc 328.275696 loss_rnnt 301.325317 hw_loss 0.431440 lr 0.00013204 rank 1
2023-02-17 01:13:03,510 DEBUG TRAIN Batch 0/3400 loss 276.003265 loss_att 275.309479 loss_ctc 297.299133 loss_rnnt 273.108612 hw_loss 0.363671 lr 0.00013604 rank 6
2023-02-17 01:13:03,513 DEBUG TRAIN Batch 0/3400 loss 226.907471 loss_att 223.813370 loss_ctc 233.978607 loss_rnnt 226.329315 hw_loss 0.476574 lr 0.00013604 rank 4
2023-02-17 01:13:03,515 DEBUG TRAIN Batch 0/3400 loss 244.327499 loss_att 248.811493 loss_ctc 252.878281 loss_rnnt 242.130051 hw_loss 0.301033 lr 0.00013604 rank 7
2023-02-17 01:13:03,516 DEBUG TRAIN Batch 0/3400 loss 299.326782 loss_att 292.114868 loss_ctc 325.551178 loss_rnnt 297.120483 hw_loss 0.285198 lr 0.00013604 rank 5
2023-02-17 01:13:03,517 DEBUG TRAIN Batch 0/3400 loss 222.502716 loss_att 221.542603 loss_ctc 230.791321 loss_rnnt 221.427277 hw_loss 0.304319 lr 0.00013604 rank 1
2023-02-17 01:13:03,517 DEBUG TRAIN Batch 0/3400 loss 252.509079 loss_att 245.757721 loss_ctc 267.484406 loss_rnnt 251.664825 hw_loss 0.370855 lr 0.00013604 rank 0
2023-02-17 01:13:03,518 DEBUG TRAIN Batch 0/3400 loss 298.184570 loss_att 283.008911 loss_ctc 313.126953 loss_rnnt 298.959045 hw_loss 0.503066 lr 0.00013604 rank 2
2023-02-17 01:13:03,518 DEBUG TRAIN Batch 0/3400 loss 298.105499 loss_att 293.548462 loss_ctc 316.464081 loss_rnnt 296.377991 hw_loss 0.358389 lr 0.00013604 rank 3
2023-02-17 01:14:21,621 DEBUG TRAIN Batch 0/3500 loss 228.039627 loss_att 233.449524 loss_ctc 233.230652 loss_rnnt 226.105438 hw_loss 0.300181 lr 0.00014004 rank 0
2023-02-17 01:14:21,622 DEBUG TRAIN Batch 0/3500 loss 297.933319 loss_att 300.348938 loss_ctc 305.043549 loss_rnnt 296.333801 hw_loss 0.315681 lr 0.00014004 rank 7
2023-02-17 01:14:21,640 DEBUG TRAIN Batch 0/3500 loss 235.160614 loss_att 226.437210 loss_ctc 252.899277 loss_rnnt 234.339081 hw_loss 0.376961 lr 0.00014004 rank 1
2023-02-17 01:14:21,654 DEBUG TRAIN Batch 0/3500 loss 239.613968 loss_att 241.962769 loss_ctc 253.307556 loss_rnnt 237.081268 hw_loss 0.444653 lr 0.00014004 rank 3
2023-02-17 01:14:21,655 DEBUG TRAIN Batch 0/3500 loss 241.500793 loss_att 245.412430 loss_ctc 258.760620 loss_rnnt 238.229660 hw_loss 0.351580 lr 0.00014004 rank 6
2023-02-17 01:14:21,671 DEBUG TRAIN Batch 0/3500 loss 255.115250 loss_att 253.896347 loss_ctc 273.557983 loss_rnnt 252.724747 hw_loss 0.328552 lr 0.00014004 rank 5
2023-02-17 01:14:21,671 DEBUG TRAIN Batch 0/3500 loss 225.156387 loss_att 228.499771 loss_ctc 235.820679 loss_rnnt 222.875153 hw_loss 0.357461 lr 0.00014004 rank 4
2023-02-17 01:14:21,695 DEBUG TRAIN Batch 0/3500 loss 272.041077 loss_att 266.704468 loss_ctc 285.846405 loss_rnnt 271.102631 hw_loss 0.309534 lr 0.00014004 rank 2
2023-02-17 01:16:45,999 DEBUG TRAIN Batch 0/3600 loss 220.021103 loss_att 222.613281 loss_ctc 226.621078 loss_rnnt 218.428818 hw_loss 0.363447 lr 0.00014404 rank 2
2023-02-17 01:16:46,000 DEBUG TRAIN Batch 0/3600 loss 229.978333 loss_att 233.171356 loss_ctc 252.687012 loss_rnnt 226.097839 hw_loss 0.401326 lr 0.00014404 rank 3
2023-02-17 01:16:46,000 DEBUG TRAIN Batch 0/3600 loss 241.590302 loss_att 244.066971 loss_ctc 256.019653 loss_rnnt 238.967041 hw_loss 0.382535 lr 0.00014404 rank 6
2023-02-17 01:16:46,001 DEBUG TRAIN Batch 0/3600 loss 197.862076 loss_att 209.803314 loss_ctc 203.104248 loss_rnnt 194.574036 hw_loss 0.376533 lr 0.00014404 rank 4
2023-02-17 01:16:46,001 DEBUG TRAIN Batch 0/3600 loss 290.361267 loss_att 292.947876 loss_ctc 295.235992 loss_rnnt 289.029694 hw_loss 0.307982 lr 0.00014404 rank 7
2023-02-17 01:16:46,001 DEBUG TRAIN Batch 0/3600 loss 261.911560 loss_att 264.843262 loss_ctc 272.988983 loss_rnnt 259.652161 hw_loss 0.367604 lr 0.00014404 rank 0
2023-02-17 01:16:46,004 DEBUG TRAIN Batch 0/3600 loss 217.817169 loss_att 224.263000 loss_ctc 233.811111 loss_rnnt 214.182053 hw_loss 0.400208 lr 0.00014404 rank 5
2023-02-17 01:16:46,004 DEBUG TRAIN Batch 0/3600 loss 255.760742 loss_att 250.838440 loss_ctc 274.010101 loss_rnnt 254.141113 hw_loss 0.320331 lr 0.00014404 rank 1
2023-02-17 01:18:02,226 DEBUG TRAIN Batch 0/3700 loss 83.013535 loss_att 85.827415 loss_ctc 85.767410 loss_rnnt 81.843018 hw_loss 0.451031 lr 0.00014804 rank 1
2023-02-17 01:18:02,226 DEBUG TRAIN Batch 0/3700 loss 163.585526 loss_att 175.488434 loss_ctc 165.707733 loss_rnnt 160.731354 hw_loss 0.357381 lr 0.00014804 rank 0
2023-02-17 01:18:02,227 DEBUG TRAIN Batch 0/3700 loss 214.920425 loss_att 222.295456 loss_ctc 223.052002 loss_rnnt 212.203568 hw_loss 0.295555 lr 0.00014804 rank 2
2023-02-17 01:18:02,229 DEBUG TRAIN Batch 0/3700 loss 251.027451 loss_att 260.724426 loss_ctc 267.881958 loss_rnnt 246.671936 hw_loss 0.316574 lr 0.00014804 rank 3
2023-02-17 01:18:02,233 DEBUG TRAIN Batch 0/3700 loss 242.634460 loss_att 260.338165 loss_ctc 246.766815 loss_rnnt 238.346954 hw_loss 0.367060 lr 0.00014804 rank 7
2023-02-17 01:18:02,245 DEBUG TRAIN Batch 0/3700 loss 177.023636 loss_att 193.606979 loss_ctc 179.833359 loss_rnnt 173.186584 hw_loss 0.273299 lr 0.00014804 rank 5
2023-02-17 01:18:02,276 DEBUG TRAIN Batch 0/3700 loss 227.989822 loss_att 230.526794 loss_ctc 236.596191 loss_rnnt 226.118439 hw_loss 0.405841 lr 0.00014804 rank 4
2023-02-17 01:18:02,277 DEBUG TRAIN Batch 0/3700 loss 206.784485 loss_att 210.738129 loss_ctc 207.671585 loss_rnnt 205.647583 hw_loss 0.427331 lr 0.00014804 rank 6
2023-02-17 01:19:20,855 DEBUG TRAIN Batch 0/3800 loss 219.664276 loss_att 234.011322 loss_ctc 231.125519 loss_rnnt 215.068375 hw_loss 0.371851 lr 0.00015204 rank 7
2023-02-17 01:19:20,856 DEBUG TRAIN Batch 0/3800 loss 229.325165 loss_att 254.527527 loss_ctc 228.421875 loss_rnnt 224.234497 hw_loss 0.319884 lr 0.00015204 rank 1
2023-02-17 01:19:20,858 DEBUG TRAIN Batch 0/3800 loss 50.097420 loss_att 53.085045 loss_ctc 53.869186 loss_rnnt 48.781998 hw_loss 0.403117 lr 0.00015204 rank 2
2023-02-17 01:19:20,858 DEBUG TRAIN Batch 0/3800 loss 72.254082 loss_att 76.834534 loss_ctc 74.403809 loss_rnnt 70.767151 hw_loss 0.532889 lr 0.00015204 rank 5
2023-02-17 01:19:20,872 DEBUG TRAIN Batch 0/3800 loss 220.904404 loss_att 231.054535 loss_ctc 236.653931 loss_rnnt 216.687912 hw_loss 0.162235 lr 0.00015204 rank 0
2023-02-17 01:19:20,895 DEBUG TRAIN Batch 0/3800 loss 109.162544 loss_att 117.880920 loss_ctc 112.616440 loss_rnnt 106.742714 hw_loss 0.404311 lr 0.00015204 rank 6
2023-02-17 01:19:20,907 DEBUG TRAIN Batch 0/3800 loss 176.515640 loss_att 186.025162 loss_ctc 180.779922 loss_rnnt 173.860672 hw_loss 0.345925 lr 0.00015204 rank 4
2023-02-17 01:19:20,936 DEBUG TRAIN Batch 0/3800 loss 203.775162 loss_att 211.164734 loss_ctc 210.522614 loss_rnnt 201.187973 hw_loss 0.393045 lr 0.00015204 rank 3
2023-02-17 01:21:45,519 DEBUG TRAIN Batch 0/3900 loss 245.803604 loss_att 277.214386 loss_ctc 241.985519 loss_rnnt 239.881226 hw_loss 0.279968 lr 0.00015604 rank 0
2023-02-17 01:21:45,522 DEBUG TRAIN Batch 0/3900 loss 248.343887 loss_att 268.957825 loss_ctc 255.690170 loss_rnnt 243.066330 hw_loss 0.328627 lr 0.00015604 rank 1
2023-02-17 01:21:45,522 DEBUG TRAIN Batch 0/3900 loss 213.396545 loss_att 239.610458 loss_ctc 208.656067 loss_rnnt 208.602417 hw_loss 0.343879 lr 0.00015604 rank 5
2023-02-17 01:21:45,523 DEBUG TRAIN Batch 0/3900 loss 223.216705 loss_att 254.271393 loss_ctc 226.561951 loss_rnnt 216.417603 hw_loss 0.266513 lr 0.00015604 rank 6
2023-02-17 01:21:45,533 DEBUG TRAIN Batch 0/3900 loss 137.322311 loss_att 146.456696 loss_ctc 139.027328 loss_rnnt 135.078354 hw_loss 0.355781 lr 0.00015604 rank 7
2023-02-17 01:21:45,541 DEBUG TRAIN Batch 0/3900 loss 225.387512 loss_att 253.528015 loss_ctc 226.293213 loss_rnnt 219.434784 hw_loss 0.382269 lr 0.00015604 rank 4
2023-02-17 01:21:45,542 DEBUG TRAIN Batch 0/3900 loss 207.606155 loss_att 229.148087 loss_ctc 208.884491 loss_rnnt 202.980331 hw_loss 0.275629 lr 0.00015604 rank 3
2023-02-17 01:21:45,547 DEBUG TRAIN Batch 0/3900 loss 239.543198 loss_att 270.205139 loss_ctc 247.636444 loss_rnnt 232.114807 hw_loss 0.406683 lr 0.00015604 rank 2
2023-02-17 01:23:07,621 DEBUG TRAIN Batch 0/4000 loss 236.543198 loss_att 271.284363 loss_ctc 233.628662 loss_rnnt 229.804230 hw_loss 0.336235 lr 0.00016004 rank 2
2023-02-17 01:23:07,637 DEBUG TRAIN Batch 0/4000 loss 203.220245 loss_att 241.899628 loss_ctc 200.267914 loss_rnnt 195.713196 hw_loss 0.309020 lr 0.00016004 rank 6
2023-02-17 01:23:07,638 DEBUG TRAIN Batch 0/4000 loss 167.974838 loss_att 208.645737 loss_ctc 171.349213 loss_rnnt 159.259415 hw_loss 0.246224 lr 0.00016004 rank 4
2023-02-17 01:23:07,638 DEBUG TRAIN Batch 0/4000 loss 202.231064 loss_att 230.743805 loss_ctc 207.318893 loss_rnnt 195.672485 hw_loss 0.333111 lr 0.00016004 rank 3
2023-02-17 01:23:07,639 DEBUG TRAIN Batch 0/4000 loss 241.544739 loss_att 283.476440 loss_ctc 250.521515 loss_rnnt 231.820923 hw_loss 0.263576 lr 0.00016004 rank 1
2023-02-17 01:23:07,639 DEBUG TRAIN Batch 0/4000 loss 234.064224 loss_att 263.970734 loss_ctc 244.430145 loss_rnnt 226.514832 hw_loss 0.348705 lr 0.00016004 rank 5
2023-02-17 01:23:07,647 DEBUG TRAIN Batch 0/4000 loss 235.234116 loss_att 265.851562 loss_ctc 236.499115 loss_rnnt 228.761658 hw_loss 0.338063 lr 0.00016004 rank 7
2023-02-17 01:23:07,671 DEBUG TRAIN Batch 0/4000 loss 182.016602 loss_att 221.242691 loss_ctc 171.472946 loss_rnnt 175.428391 hw_loss 0.279035 lr 0.00016004 rank 0
2023-02-17 01:24:22,757 DEBUG TRAIN Batch 0/4100 loss 178.379562 loss_att 220.788193 loss_ctc 178.781219 loss_rnnt 169.647430 hw_loss 0.369091 lr 0.00016404 rank 0
2023-02-17 01:24:22,762 DEBUG TRAIN Batch 0/4100 loss 207.898468 loss_att 253.771362 loss_ctc 207.426575 loss_rnnt 198.654922 hw_loss 0.247266 lr 0.00016404 rank 1
2023-02-17 01:24:22,762 DEBUG TRAIN Batch 0/4100 loss 193.193878 loss_att 239.214447 loss_ctc 186.174835 loss_rnnt 184.725586 hw_loss 0.375093 lr 0.00016404 rank 6
2023-02-17 01:24:22,764 DEBUG TRAIN Batch 0/4100 loss 231.107178 loss_att 267.045471 loss_ctc 233.249786 loss_rnnt 223.460297 hw_loss 0.325381 lr 0.00016404 rank 2
2023-02-17 01:24:22,765 DEBUG TRAIN Batch 0/4100 loss 205.319275 loss_att 234.244080 loss_ctc 218.218811 loss_rnnt 197.652496 hw_loss 0.303491 lr 0.00016404 rank 5
2023-02-17 01:24:22,765 DEBUG TRAIN Batch 0/4100 loss 197.935074 loss_att 227.809433 loss_ctc 196.372971 loss_rnnt 191.956100 hw_loss 0.398202 lr 0.00016404 rank 4
2023-02-17 01:24:22,766 DEBUG TRAIN Batch 0/4100 loss 176.086639 loss_att 215.767029 loss_ctc 183.217392 loss_rnnt 166.995163 hw_loss 0.383700 lr 0.00016404 rank 7
2023-02-17 01:24:22,776 DEBUG TRAIN Batch 0/4100 loss 215.680832 loss_att 250.129379 loss_ctc 218.921600 loss_rnnt 208.205246 hw_loss 0.288301 lr 0.00016404 rank 3
2023-02-17 01:25:39,903 DEBUG TRAIN Batch 0/4200 loss 229.522308 loss_att 282.296173 loss_ctc 225.994644 loss_rnnt 219.275955 hw_loss 0.303647 lr 0.00016804 rank 3
2023-02-17 01:25:39,903 DEBUG TRAIN Batch 0/4200 loss 193.831680 loss_att 240.868225 loss_ctc 193.293396 loss_rnnt 184.352509 hw_loss 0.269305 lr 0.00016804 rank 7
2023-02-17 01:25:39,904 DEBUG TRAIN Batch 0/4200 loss 210.440414 loss_att 243.084137 loss_ctc 212.517761 loss_rnnt 203.473175 hw_loss 0.302862 lr 0.00016804 rank 5
2023-02-17 01:25:39,906 DEBUG TRAIN Batch 0/4200 loss 167.927032 loss_att 223.100037 loss_ctc 165.488708 loss_rnnt 157.044388 hw_loss 0.324639 lr 0.00016804 rank 0
2023-02-17 01:25:39,907 DEBUG TRAIN Batch 0/4200 loss 179.883865 loss_att 218.129578 loss_ctc 185.496094 loss_rnnt 171.293152 hw_loss 0.362391 lr 0.00016804 rank 2
2023-02-17 01:25:39,909 DEBUG TRAIN Batch 0/4200 loss 171.829346 loss_att 217.984436 loss_ctc 168.960052 loss_rnnt 162.815536 hw_loss 0.310056 lr 0.00016804 rank 4
2023-02-17 01:25:39,917 DEBUG TRAIN Batch 0/4200 loss 198.305008 loss_att 249.795227 loss_ctc 205.065262 loss_rnnt 186.932800 hw_loss 0.323969 lr 0.00016804 rank 6
2023-02-17 01:25:39,927 DEBUG TRAIN Batch 0/4200 loss 187.841888 loss_att 227.451111 loss_ctc 184.697037 loss_rnnt 180.168137 hw_loss 0.321051 lr 0.00016804 rank 1
2023-02-17 01:28:10,162 DEBUG TRAIN Batch 0/4300 loss 191.497467 loss_att 243.612152 loss_ctc 186.744232 loss_rnnt 181.438843 hw_loss 0.505226 lr 0.00017204 rank 6
2023-02-17 01:28:10,164 DEBUG TRAIN Batch 0/4300 loss 201.099991 loss_att 244.514328 loss_ctc 199.140640 loss_rnnt 192.536102 hw_loss 0.266778 lr 0.00017204 rank 7
2023-02-17 01:28:10,165 DEBUG TRAIN Batch 0/4300 loss 163.632660 loss_att 190.432800 loss_ctc 168.769928 loss_rnnt 157.437912 hw_loss 0.280759 lr 0.00017204 rank 2
2023-02-17 01:28:10,168 DEBUG TRAIN Batch 0/4300 loss 177.127914 loss_att 220.789383 loss_ctc 181.320435 loss_rnnt 167.621262 hw_loss 0.403781 lr 0.00017204 rank 3
2023-02-17 01:28:10,207 DEBUG TRAIN Batch 0/4300 loss 130.547379 loss_att 146.705231 loss_ctc 133.858734 loss_rnnt 126.647942 hw_loss 0.424412 lr 0.00017204 rank 1
2023-02-17 01:28:10,208 DEBUG TRAIN Batch 0/4300 loss 169.590958 loss_att 218.829376 loss_ctc 164.423096 loss_rnnt 160.223602 hw_loss 0.391367 lr 0.00017204 rank 5
2023-02-17 01:28:10,213 DEBUG TRAIN Batch 0/4300 loss 141.053268 loss_att 207.239014 loss_ctc 136.117920 loss_rnnt 128.313416 hw_loss 0.301397 lr 0.00017204 rank 4
2023-02-17 01:28:10,216 DEBUG TRAIN Batch 0/4300 loss 177.480743 loss_att 218.164368 loss_ctc 180.672882 loss_rnnt 168.745605 hw_loss 0.323986 lr 0.00017204 rank 0
2023-02-17 01:29:26,424 DEBUG TRAIN Batch 0/4400 loss 187.768524 loss_att 255.236664 loss_ctc 183.292175 loss_rnnt 174.648788 hw_loss 0.418002 lr 0.00017604 rank 1
2023-02-17 01:29:26,424 DEBUG TRAIN Batch 0/4400 loss 113.863495 loss_att 136.612457 loss_ctc 110.364357 loss_rnnt 109.598320 hw_loss 0.341121 lr 0.00017604 rank 2
2023-02-17 01:29:26,425 DEBUG TRAIN Batch 0/4400 loss 144.783005 loss_att 184.198593 loss_ctc 143.026428 loss_rnnt 136.922028 hw_loss 0.397639 lr 0.00017604 rank 6
2023-02-17 01:29:26,426 DEBUG TRAIN Batch 0/4400 loss 100.533951 loss_att 126.813385 loss_ctc 103.309006 loss_rnnt 94.679703 hw_loss 0.428140 lr 0.00017604 rank 0
2023-02-17 01:29:26,429 DEBUG TRAIN Batch 0/4400 loss 66.939613 loss_att 82.408569 loss_ctc 68.209915 loss_rnnt 63.431374 hw_loss 0.459502 lr 0.00017604 rank 5
2023-02-17 01:29:26,433 DEBUG TRAIN Batch 0/4400 loss 203.784729 loss_att 236.339172 loss_ctc 202.802780 loss_rnnt 197.194336 hw_loss 0.394563 lr 0.00017604 rank 4
2023-02-17 01:29:26,462 DEBUG TRAIN Batch 0/4400 loss 190.050430 loss_att 232.867111 loss_ctc 187.907104 loss_rnnt 181.553467 hw_loss 0.411373 lr 0.00017604 rank 7
2023-02-17 01:29:26,472 DEBUG TRAIN Batch 0/4400 loss 155.445969 loss_att 201.042847 loss_ctc 153.165619 loss_rnnt 146.475357 hw_loss 0.291134 lr 0.00017604 rank 3
2023-02-17 01:30:42,171 DEBUG TRAIN Batch 0/4500 loss 187.749207 loss_att 235.051834 loss_ctc 189.242249 loss_rnnt 177.822754 hw_loss 0.500326 lr 0.00018004 rank 0
2023-02-17 01:30:42,171 DEBUG TRAIN Batch 0/4500 loss 119.735512 loss_att 157.147705 loss_ctc 116.838676 loss_rnnt 112.462463 hw_loss 0.331590 lr 0.00018004 rank 7
2023-02-17 01:30:42,170 DEBUG TRAIN Batch 0/4500 loss 136.243683 loss_att 173.208771 loss_ctc 132.855560 loss_rnnt 129.071198 hw_loss 0.433542 lr 0.00018004 rank 3
2023-02-17 01:30:42,175 DEBUG TRAIN Batch 0/4500 loss 156.368011 loss_att 210.752274 loss_ctc 153.243576 loss_rnnt 145.756592 hw_loss 0.283416 lr 0.00018004 rank 5
2023-02-17 01:30:42,182 DEBUG TRAIN Batch 0/4500 loss 228.651367 loss_att 290.977783 loss_ctc 226.262619 loss_rnnt 216.352417 hw_loss 0.285298 lr 0.00018004 rank 2
2023-02-17 01:30:42,203 DEBUG TRAIN Batch 0/4500 loss 68.208847 loss_att 85.492668 loss_ctc 69.729713 loss_rnnt 64.320892 hw_loss 0.428255 lr 0.00018004 rank 4
2023-02-17 01:30:42,204 DEBUG TRAIN Batch 0/4500 loss 176.143356 loss_att 222.879761 loss_ctc 177.072601 loss_rnnt 166.470047 hw_loss 0.378993 lr 0.00018004 rank 6
2023-02-17 01:30:42,222 DEBUG TRAIN Batch 0/4500 loss 205.197800 loss_att 275.419464 loss_ctc 207.033981 loss_rnnt 190.736267 hw_loss 0.323227 lr 0.00018004 rank 1
2023-02-17 01:32:01,466 DEBUG TRAIN Batch 0/4600 loss 207.474625 loss_att 268.880615 loss_ctc 203.025909 loss_rnnt 195.622559 hw_loss 0.307545 lr 0.00018404 rank 4
2023-02-17 01:32:01,472 DEBUG TRAIN Batch 0/4600 loss 157.489029 loss_att 220.644852 loss_ctc 152.342224 loss_rnnt 145.358124 hw_loss 0.348707 lr 0.00018404 rank 1
2023-02-17 01:32:01,480 DEBUG TRAIN Batch 0/4600 loss 189.664276 loss_att 248.728683 loss_ctc 194.326172 loss_rnnt 177.057678 hw_loss 0.322741 lr 0.00018404 rank 7
2023-02-17 01:32:01,503 DEBUG TRAIN Batch 0/4600 loss 202.414948 loss_att 265.020813 loss_ctc 190.925171 loss_rnnt 191.219467 hw_loss 0.386766 lr 0.00018404 rank 5
2023-02-17 01:32:01,503 DEBUG TRAIN Batch 0/4600 loss 202.475037 loss_att 255.572876 loss_ctc 199.750366 loss_rnnt 192.015076 hw_loss 0.381891 lr 0.00018404 rank 3
2023-02-17 01:32:01,508 DEBUG TRAIN Batch 0/4600 loss 198.456100 loss_att 250.986298 loss_ctc 191.795868 loss_rnnt 188.634735 hw_loss 0.381335 lr 0.00018404 rank 2
2023-02-17 01:32:01,510 DEBUG TRAIN Batch 0/4600 loss 191.657120 loss_att 231.577362 loss_ctc 183.943085 loss_rnnt 184.512497 hw_loss 0.354606 lr 0.00018404 rank 6
2023-02-17 01:32:01,513 DEBUG TRAIN Batch 0/4600 loss 173.659851 loss_att 247.385132 loss_ctc 170.437469 loss_rnnt 159.198181 hw_loss 0.274256 lr 0.00018404 rank 0
2023-02-17 01:34:36,149 DEBUG TRAIN Batch 0/4700 loss 165.634064 loss_att 221.608093 loss_ctc 162.416061 loss_rnnt 154.681686 hw_loss 0.349943 lr 0.00018804 rank 4
2023-02-17 01:34:36,154 DEBUG TRAIN Batch 0/4700 loss 179.348480 loss_att 263.296661 loss_ctc 178.888550 loss_rnnt 162.490448 hw_loss 0.243208 lr 0.00018804 rank 7
2023-02-17 01:34:36,154 DEBUG TRAIN Batch 0/4700 loss 159.546158 loss_att 244.879120 loss_ctc 155.013992 loss_rnnt 142.898788 hw_loss 0.347014 lr 0.00018804 rank 2
2023-02-17 01:34:36,155 DEBUG TRAIN Batch 0/4700 loss 176.659256 loss_att 230.664612 loss_ctc 172.005829 loss_rnnt 166.272858 hw_loss 0.385875 lr 0.00018804 rank 6
2023-02-17 01:34:36,162 DEBUG TRAIN Batch 0/4700 loss 178.391129 loss_att 232.072220 loss_ctc 176.496536 loss_rnnt 167.761353 hw_loss 0.274055 lr 0.00018804 rank 1
2023-02-17 01:34:36,163 DEBUG TRAIN Batch 0/4700 loss 154.320190 loss_att 226.186584 loss_ctc 158.429520 loss_rnnt 139.221695 hw_loss 0.332450 lr 0.00018804 rank 5
2023-02-17 01:34:36,191 DEBUG TRAIN Batch 0/4700 loss 176.314316 loss_att 243.833252 loss_ctc 174.189941 loss_rnnt 162.907379 hw_loss 0.349461 lr 0.00018804 rank 0
2023-02-17 01:34:36,202 DEBUG TRAIN Batch 0/4700 loss 182.135986 loss_att 246.785461 loss_ctc 191.482513 loss_rnnt 167.777069 hw_loss 0.342795 lr 0.00018804 rank 3
2023-02-17 01:35:52,058 DEBUG TRAIN Batch 0/4800 loss 130.274658 loss_att 200.178589 loss_ctc 121.276611 loss_rnnt 117.298759 hw_loss 0.365331 lr 0.00019204 rank 6
2023-02-17 01:35:52,060 DEBUG TRAIN Batch 0/4800 loss 137.789886 loss_att 215.336212 loss_ctc 129.599579 loss_rnnt 123.157707 hw_loss 0.403027 lr 0.00019204 rank 7
2023-02-17 01:35:52,060 DEBUG TRAIN Batch 0/4800 loss 181.280975 loss_att 248.975647 loss_ctc 180.816406 loss_rnnt 167.677475 hw_loss 0.237251 lr 0.00019204 rank 3
2023-02-17 01:35:52,061 DEBUG TRAIN Batch 0/4800 loss 158.483734 loss_att 238.197449 loss_ctc 157.156723 loss_rnnt 142.505844 hw_loss 0.397662 lr 0.00019204 rank 4
2023-02-17 01:35:52,062 DEBUG TRAIN Batch 0/4800 loss 137.734283 loss_att 186.181946 loss_ctc 141.041367 loss_rnnt 127.397064 hw_loss 0.387663 lr 0.00019204 rank 1
2023-02-17 01:35:52,064 DEBUG TRAIN Batch 0/4800 loss 150.350616 loss_att 238.493698 loss_ctc 145.527771 loss_rnnt 133.204712 hw_loss 0.300612 lr 0.00019204 rank 0
2023-02-17 01:35:52,066 DEBUG TRAIN Batch 0/4800 loss 156.070282 loss_att 242.893845 loss_ctc 147.433533 loss_rnnt 139.697968 hw_loss 0.298438 lr 0.00019204 rank 5
2023-02-17 01:35:52,066 DEBUG TRAIN Batch 0/4800 loss 175.647858 loss_att 231.058121 loss_ctc 174.026611 loss_rnnt 164.581024 hw_loss 0.376760 lr 0.00019204 rank 2
2023-02-17 01:37:08,890 DEBUG TRAIN Batch 0/4900 loss 162.077469 loss_att 225.292908 loss_ctc 154.643509 loss_rnnt 150.294189 hw_loss 0.246306 lr 0.00019604 rank 3
2023-02-17 01:37:08,891 DEBUG TRAIN Batch 0/4900 loss 130.469086 loss_att 197.004456 loss_ctc 128.746613 loss_rnnt 117.153595 hw_loss 0.446383 lr 0.00019604 rank 4
2023-02-17 01:37:08,892 DEBUG TRAIN Batch 0/4900 loss 152.797333 loss_att 213.945663 loss_ctc 153.398544 loss_rnnt 140.335312 hw_loss 0.285362 lr 0.00019604 rank 2
2023-02-17 01:37:08,896 DEBUG TRAIN Batch 0/4900 loss 138.822937 loss_att 216.413513 loss_ctc 133.671936 loss_rnnt 123.798599 hw_loss 0.361910 lr 0.00019604 rank 7
2023-02-17 01:37:08,902 DEBUG TRAIN Batch 0/4900 loss 167.752472 loss_att 217.756104 loss_ctc 172.449417 loss_rnnt 156.917587 hw_loss 0.389777 lr 0.00019604 rank 5
2023-02-17 01:37:08,919 DEBUG TRAIN Batch 0/4900 loss 93.434601 loss_att 135.105713 loss_ctc 94.081215 loss_rnnt 84.773682 hw_loss 0.450906 lr 0.00019604 rank 1
2023-02-17 01:37:08,935 DEBUG TRAIN Batch 0/4900 loss 131.967834 loss_att 205.053650 loss_ctc 128.246582 loss_rnnt 117.632660 hw_loss 0.401575 lr 0.00019604 rank 0
2023-02-17 01:37:08,939 DEBUG TRAIN Batch 0/4900 loss 191.308792 loss_att 257.325073 loss_ctc 191.887421 loss_rnnt 177.866333 hw_loss 0.303844 lr 0.00019604 rank 6
2023-02-17 01:39:40,116 DEBUG TRAIN Batch 0/5000 loss 147.974365 loss_att 210.495605 loss_ctc 144.752060 loss_rnnt 135.711655 hw_loss 0.352703 lr 0.00020004 rank 6
2023-02-17 01:39:40,117 DEBUG TRAIN Batch 0/5000 loss 116.946457 loss_att 197.810944 loss_ctc 106.526779 loss_rnnt 102.037949 hw_loss 0.234184 lr 0.00020004 rank 4
2023-02-17 01:39:40,116 DEBUG TRAIN Batch 0/5000 loss 152.313141 loss_att 238.758636 loss_ctc 151.449982 loss_rnnt 134.976837 hw_loss 0.304295 lr 0.00020004 rank 7
2023-02-17 01:39:40,116 DEBUG TRAIN Batch 0/5000 loss 170.138702 loss_att 237.592468 loss_ctc 177.226364 loss_rnnt 155.545898 hw_loss 0.294403 lr 0.00020004 rank 1
2023-02-17 01:39:40,119 DEBUG TRAIN Batch 0/5000 loss 72.916336 loss_att 101.416931 loss_ctc 72.743134 loss_rnnt 67.020813 hw_loss 0.409673 lr 0.00020004 rank 5
2023-02-17 01:39:40,124 DEBUG TRAIN Batch 0/5000 loss 119.943893 loss_att 185.673264 loss_ctc 111.884888 loss_rnnt 107.715263 hw_loss 0.294908 lr 0.00020004 rank 0
2023-02-17 01:39:40,178 DEBUG TRAIN Batch 0/5000 loss 102.621185 loss_att 149.712616 loss_ctc 100.065941 loss_rnnt 93.349648 hw_loss 0.363651 lr 0.00020004 rank 2
2023-02-17 01:39:40,193 DEBUG TRAIN Batch 0/5000 loss 170.538239 loss_att 237.012390 loss_ctc 163.186188 loss_rnnt 157.980438 hw_loss 0.456030 lr 0.00020004 rank 3
2023-02-17 01:40:56,256 DEBUG TRAIN Batch 0/5100 loss 169.176071 loss_att 274.221466 loss_ctc 165.359482 loss_rnnt 148.543503 hw_loss 0.248196 lr 0.00020404 rank 1
2023-02-17 01:40:56,265 DEBUG TRAIN Batch 0/5100 loss 110.333977 loss_att 160.834747 loss_ctc 112.865822 loss_rnnt 99.740166 hw_loss 0.292643 lr 0.00020404 rank 7
2023-02-17 01:40:56,266 DEBUG TRAIN Batch 0/5100 loss 97.008270 loss_att 133.749008 loss_ctc 95.292572 loss_rnnt 89.687614 hw_loss 0.377388 lr 0.00020404 rank 4
2023-02-17 01:40:56,265 DEBUG TRAIN Batch 0/5100 loss 40.574768 loss_att 47.640430 loss_ctc 40.885525 loss_rnnt 38.928421 hw_loss 0.359592 lr 0.00020404 rank 6
2023-02-17 01:40:56,266 DEBUG TRAIN Batch 0/5100 loss 117.116837 loss_att 173.278320 loss_ctc 116.702141 loss_rnnt 105.735512 hw_loss 0.383109 lr 0.00020404 rank 3
2023-02-17 01:40:56,267 DEBUG TRAIN Batch 0/5100 loss 163.081192 loss_att 245.057755 loss_ctc 151.483704 loss_rnnt 148.032547 hw_loss 0.374375 lr 0.00020404 rank 5
2023-02-17 01:40:56,273 DEBUG TRAIN Batch 0/5100 loss 83.042519 loss_att 127.848244 loss_ctc 79.494545 loss_rnnt 74.361671 hw_loss 0.361429 lr 0.00020404 rank 0
2023-02-17 01:40:56,320 DEBUG TRAIN Batch 0/5100 loss 153.496933 loss_att 243.250610 loss_ctc 150.570068 loss_rnnt 135.774078 hw_loss 0.304458 lr 0.00020404 rank 2
2023-02-17 01:42:12,921 DEBUG TRAIN Batch 0/5200 loss 134.589691 loss_att 218.967697 loss_ctc 139.579956 loss_rnnt 116.860229 hw_loss 0.353424 lr 0.00020804 rank 6
2023-02-17 01:42:12,924 DEBUG TRAIN Batch 0/5200 loss 148.308624 loss_att 258.912659 loss_ctc 149.078888 loss_rnnt 125.906906 hw_loss 0.334175 lr 0.00020804 rank 0
2023-02-17 01:42:12,925 DEBUG TRAIN Batch 0/5200 loss 207.627228 loss_att 300.823059 loss_ctc 213.890503 loss_rnnt 187.985840 hw_loss 0.313336 lr 0.00020804 rank 5
2023-02-17 01:42:12,925 DEBUG TRAIN Batch 0/5200 loss 145.435455 loss_att 231.621887 loss_ctc 139.024063 loss_rnnt 128.858566 hw_loss 0.364600 lr 0.00020804 rank 1
2023-02-17 01:42:12,933 DEBUG TRAIN Batch 0/5200 loss 148.124954 loss_att 251.122864 loss_ctc 144.730209 loss_rnnt 127.823936 hw_loss 0.288902 lr 0.00020804 rank 4
2023-02-17 01:42:12,940 DEBUG TRAIN Batch 0/5200 loss 151.367020 loss_att 243.396362 loss_ctc 150.386566 loss_rnnt 132.922333 hw_loss 0.317892 lr 0.00020804 rank 2
2023-02-17 01:42:12,949 DEBUG TRAIN Batch 0/5200 loss 160.727448 loss_att 250.726440 loss_ctc 158.674683 loss_rnnt 142.809830 hw_loss 0.359068 lr 0.00020804 rank 7
2023-02-17 01:42:12,953 DEBUG TRAIN Batch 0/5200 loss 170.602890 loss_att 279.623749 loss_ctc 163.731598 loss_rnnt 149.557037 hw_loss 0.295981 lr 0.00020804 rank 3
2023-02-17 01:43:32,531 DEBUG TRAIN Batch 0/5300 loss 151.897400 loss_att 237.816345 loss_ctc 148.580521 loss_rnnt 135.011841 hw_loss 0.270014 lr 0.00021204 rank 6
2023-02-17 01:43:32,536 DEBUG TRAIN Batch 0/5300 loss 171.897629 loss_att 277.416046 loss_ctc 163.861771 loss_rnnt 151.694763 hw_loss 0.319891 lr 0.00021204 rank 2
2023-02-17 01:43:32,536 DEBUG TRAIN Batch 0/5300 loss 140.867203 loss_att 232.119919 loss_ctc 133.288513 loss_rnnt 123.438354 hw_loss 0.353990 lr 0.00021204 rank 5
2023-02-17 01:43:32,537 DEBUG TRAIN Batch 0/5300 loss 157.868439 loss_att 243.770599 loss_ctc 149.082397 loss_rnnt 141.612152 hw_loss 0.463728 lr 0.00021204 rank 1
2023-02-17 01:43:32,536 DEBUG TRAIN Batch 0/5300 loss 132.955795 loss_att 232.302460 loss_ctc 124.213821 loss_rnnt 114.109619 hw_loss 0.267070 lr 0.00021204 rank 7
2023-02-17 01:43:32,542 DEBUG TRAIN Batch 0/5300 loss 147.615021 loss_att 241.610886 loss_ctc 145.678040 loss_rnnt 128.973846 hw_loss 0.187980 lr 0.00021204 rank 4
2023-02-17 01:43:32,545 DEBUG TRAIN Batch 0/5300 loss 143.827911 loss_att 234.665375 loss_ctc 141.716278 loss_rnnt 125.822350 hw_loss 0.224294 lr 0.00021204 rank 3
2023-02-17 01:43:32,552 DEBUG TRAIN Batch 0/5300 loss 175.271118 loss_att 277.359619 loss_ctc 162.127792 loss_rnnt 156.477539 hw_loss 0.240582 lr 0.00021204 rank 0
2023-02-17 01:46:09,916 DEBUG TRAIN Batch 0/5400 loss 199.209579 loss_att 283.282562 loss_ctc 209.203415 loss_rnnt 180.850861 hw_loss 0.396713 lr 0.00021604 rank 0
2023-02-17 01:46:09,919 DEBUG TRAIN Batch 0/5400 loss 141.974350 loss_att 242.565521 loss_ctc 137.278778 loss_rnnt 122.315781 hw_loss 0.312034 lr 0.00021604 rank 7
2023-02-17 01:46:09,921 DEBUG TRAIN Batch 0/5400 loss 150.600754 loss_att 244.149551 loss_ctc 148.781387 loss_rnnt 131.976196 hw_loss 0.295105 lr 0.00021604 rank 1
2023-02-17 01:46:09,921 DEBUG TRAIN Batch 0/5400 loss 154.168274 loss_att 243.186050 loss_ctc 155.486053 loss_rnnt 136.013275 hw_loss 0.329519 lr 0.00021604 rank 3
2023-02-17 01:46:09,922 DEBUG TRAIN Batch 0/5400 loss 152.424957 loss_att 254.583588 loss_ctc 148.914093 loss_rnnt 132.272003 hw_loss 0.355012 lr 0.00021604 rank 6
2023-02-17 01:46:09,926 DEBUG TRAIN Batch 0/5400 loss 157.870773 loss_att 246.365845 loss_ctc 155.004318 loss_rnnt 140.377472 hw_loss 0.330894 lr 0.00021604 rank 2
2023-02-17 01:46:09,927 DEBUG TRAIN Batch 0/5400 loss 130.262634 loss_att 219.792297 loss_ctc 121.897079 loss_rnnt 113.296661 hw_loss 0.328962 lr 0.00021604 rank 4
2023-02-17 01:46:09,927 DEBUG TRAIN Batch 0/5400 loss 148.712738 loss_att 240.004333 loss_ctc 152.170654 loss_rnnt 129.879883 hw_loss 0.212769 lr 0.00021604 rank 5
2023-02-17 01:47:24,920 DEBUG TRAIN Batch 0/5500 loss 139.529373 loss_att 231.649094 loss_ctc 138.847687 loss_rnnt 120.974701 hw_loss 0.415518 lr 0.00022004 rank 2
2023-02-17 01:47:24,935 DEBUG TRAIN Batch 0/5500 loss 167.682648 loss_att 263.796234 loss_ctc 172.089905 loss_rnnt 147.654968 hw_loss 0.407498 lr 0.00022004 rank 6
2023-02-17 01:47:24,938 DEBUG TRAIN Batch 0/5500 loss 111.223953 loss_att 177.856903 loss_ctc 102.603287 loss_rnnt 98.894531 hw_loss 0.285476 lr 0.00022004 rank 1
2023-02-17 01:47:24,941 DEBUG TRAIN Batch 0/5500 loss 125.721390 loss_att 221.773895 loss_ctc 121.158768 loss_rnnt 106.958237 hw_loss 0.301900 lr 0.00022004 rank 5
2023-02-17 01:47:24,942 DEBUG TRAIN Batch 0/5500 loss 160.931656 loss_att 249.821838 loss_ctc 158.787354 loss_rnnt 143.222321 hw_loss 0.407250 lr 0.00022004 rank 0
2023-02-17 01:47:24,942 DEBUG TRAIN Batch 0/5500 loss 101.311615 loss_att 207.083893 loss_ctc 97.606613 loss_rnnt 80.438217 hw_loss 0.399260 lr 0.00022004 rank 3
2023-02-17 01:47:24,945 DEBUG TRAIN Batch 0/5500 loss 156.343170 loss_att 270.884216 loss_ctc 153.360718 loss_rnnt 133.687256 hw_loss 0.272573 lr 0.00022004 rank 4
2023-02-17 01:47:24,945 DEBUG TRAIN Batch 0/5500 loss 133.848755 loss_att 223.757263 loss_ctc 138.381012 loss_rnnt 115.089569 hw_loss 0.324691 lr 0.00022004 rank 7
2023-02-17 01:48:41,889 DEBUG TRAIN Batch 0/5600 loss 115.596771 loss_att 186.204865 loss_ctc 116.822205 loss_rnnt 101.076576 hw_loss 0.440962 lr 0.00022404 rank 5
2023-02-17 01:48:41,889 DEBUG TRAIN Batch 0/5600 loss 139.704422 loss_att 209.313873 loss_ctc 134.649780 loss_rnnt 126.308617 hw_loss 0.277255 lr 0.00022404 rank 2
2023-02-17 01:48:41,898 DEBUG TRAIN Batch 0/5600 loss 153.967224 loss_att 236.817841 loss_ctc 155.215179 loss_rnnt 137.075439 hw_loss 0.291145 lr 0.00022404 rank 0
2023-02-17 01:48:41,925 DEBUG TRAIN Batch 0/5600 loss 190.259720 loss_att 286.854309 loss_ctc 196.447144 loss_rnnt 169.941147 hw_loss 0.327474 lr 0.00022404 rank 1
2023-02-17 01:48:41,924 DEBUG TRAIN Batch 0/5600 loss 116.457954 loss_att 209.466263 loss_ctc 110.135666 loss_rnnt 98.554298 hw_loss 0.271823 lr 0.00022404 rank 7
2023-02-17 01:48:41,927 DEBUG TRAIN Batch 0/5600 loss 112.024399 loss_att 203.633881 loss_ctc 113.551224 loss_rnnt 93.302475 hw_loss 0.368347 lr 0.00022404 rank 3
2023-02-17 01:48:41,928 DEBUG TRAIN Batch 0/5600 loss 108.951469 loss_att 208.202209 loss_ctc 105.217316 loss_rnnt 89.452026 hw_loss 0.275975 lr 0.00022404 rank 4
2023-02-17 01:48:41,976 DEBUG TRAIN Batch 0/5600 loss 130.184311 loss_att 204.534622 loss_ctc 126.328262 loss_rnnt 115.637962 hw_loss 0.357065 lr 0.00022404 rank 6
2023-02-17 01:51:18,840 DEBUG TRAIN Batch 0/5700 loss 108.251541 loss_att 181.787384 loss_ctc 109.697609 loss_rnnt 93.163330 hw_loss 0.352934 lr 0.00022804 rank 3
2023-02-17 01:51:18,841 DEBUG TRAIN Batch 0/5700 loss 83.924965 loss_att 144.060120 loss_ctc 81.727036 loss_rnnt 72.023041 hw_loss 0.314911 lr 0.00022804 rank 0
2023-02-17 01:51:18,842 DEBUG TRAIN Batch 0/5700 loss 70.614120 loss_att 105.984863 loss_ctc 76.905067 loss_rnnt 62.452847 hw_loss 0.465620 lr 0.00022804 rank 6
2023-02-17 01:51:18,842 DEBUG TRAIN Batch 0/5700 loss 123.209190 loss_att 200.569656 loss_ctc 123.843262 loss_rnnt 107.422264 hw_loss 0.431801 lr 0.00022804 rank 4
2023-02-17 01:51:18,846 DEBUG TRAIN Batch 0/5700 loss 133.353073 loss_att 223.142334 loss_ctc 132.960327 loss_rnnt 115.239548 hw_loss 0.390064 lr 0.00022804 rank 1
2023-02-17 01:51:18,847 DEBUG TRAIN Batch 0/5700 loss 176.517883 loss_att 294.065399 loss_ctc 177.582138 loss_rnnt 152.697678 hw_loss 0.316557 lr 0.00022804 rank 5
2023-02-17 01:51:18,849 DEBUG TRAIN Batch 0/5700 loss 115.237114 loss_att 188.426605 loss_ctc 116.491898 loss_rnnt 100.209793 hw_loss 0.416442 lr 0.00022804 rank 7
2023-02-17 01:51:18,888 DEBUG TRAIN Batch 0/5700 loss 83.101158 loss_att 118.425278 loss_ctc 84.573601 loss_rnnt 75.664474 hw_loss 0.329118 lr 0.00022804 rank 2
2023-02-17 01:52:35,248 DEBUG TRAIN Batch 0/5800 loss 135.186996 loss_att 230.022736 loss_ctc 135.969788 loss_rnnt 115.957726 hw_loss 0.295799 lr 0.00023204 rank 4
2023-02-17 01:52:35,248 DEBUG TRAIN Batch 0/5800 loss 133.928864 loss_att 249.956390 loss_ctc 121.763718 loss_rnnt 112.174973 hw_loss 0.319505 lr 0.00023204 rank 6
2023-02-17 01:52:35,250 DEBUG TRAIN Batch 0/5800 loss 37.633644 loss_att 58.520100 loss_ctc 37.098476 loss_rnnt 33.286819 hw_loss 0.451657 lr 0.00023204 rank 3
2023-02-17 01:52:35,252 DEBUG TRAIN Batch 0/5800 loss 153.826462 loss_att 260.236420 loss_ctc 156.580750 loss_rnnt 131.998016 hw_loss 0.336025 lr 0.00023204 rank 0
2023-02-17 01:52:35,254 DEBUG TRAIN Batch 0/5800 loss 136.731995 loss_att 252.052094 loss_ctc 127.645615 loss_rnnt 114.710190 hw_loss 0.317452 lr 0.00023204 rank 7
2023-02-17 01:52:35,256 DEBUG TRAIN Batch 0/5800 loss 132.624802 loss_att 245.746246 loss_ctc 126.259018 loss_rnnt 110.687668 hw_loss 0.303046 lr 0.00023204 rank 5
2023-02-17 01:52:35,256 DEBUG TRAIN Batch 0/5800 loss 123.859528 loss_att 213.818695 loss_ctc 116.519699 loss_rnnt 106.664116 hw_loss 0.341661 lr 0.00023204 rank 1
2023-02-17 01:52:35,257 DEBUG TRAIN Batch 0/5800 loss 158.816391 loss_att 249.677246 loss_ctc 164.082397 loss_rnnt 139.748688 hw_loss 0.362617 lr 0.00023204 rank 2
2023-02-17 01:53:50,527 DEBUG TRAIN Batch 0/5900 loss 155.382462 loss_att 258.014587 loss_ctc 148.688583 loss_rnnt 135.623627 hw_loss 0.234210 lr 0.00023604 rank 5
2023-02-17 01:53:50,527 DEBUG TRAIN Batch 0/5900 loss 138.890610 loss_att 251.511841 loss_ctc 133.370270 loss_rnnt 116.961510 hw_loss 0.264165 lr 0.00023604 rank 7
2023-02-17 01:53:50,527 DEBUG TRAIN Batch 0/5900 loss 162.555069 loss_att 234.647598 loss_ctc 164.546722 loss_rnnt 147.633606 hw_loss 0.445147 lr 0.00023604 rank 2
2023-02-17 01:53:50,530 DEBUG TRAIN Batch 0/5900 loss 118.016708 loss_att 248.348480 loss_ctc 110.763718 loss_rnnt 92.796898 hw_loss 0.225983 lr 0.00023604 rank 6
2023-02-17 01:53:50,536 DEBUG TRAIN Batch 0/5900 loss 113.190994 loss_att 225.386963 loss_ctc 107.377060 loss_rnnt 91.357803 hw_loss 0.317205 lr 0.00023604 rank 1
2023-02-17 01:53:50,540 DEBUG TRAIN Batch 0/5900 loss 154.154236 loss_att 257.392578 loss_ctc 151.165588 loss_rnnt 133.752472 hw_loss 0.286087 lr 0.00023604 rank 4
2023-02-17 01:53:50,548 DEBUG TRAIN Batch 0/5900 loss 151.196579 loss_att 271.508850 loss_ctc 146.107361 loss_rnnt 127.680954 hw_loss 0.246993 lr 0.00023604 rank 3
2023-02-17 01:53:50,551 DEBUG TRAIN Batch 0/5900 loss 156.549988 loss_att 268.752045 loss_ctc 152.243027 loss_rnnt 134.496765 hw_loss 0.350761 lr 0.00023604 rank 0
2023-02-17 01:55:08,947 DEBUG TRAIN Batch 0/6000 loss 114.424469 loss_att 226.204086 loss_ctc 109.507668 loss_rnnt 92.523697 hw_loss 0.375791 lr 0.00024004 rank 2
2023-02-17 01:55:08,993 DEBUG TRAIN Batch 0/6000 loss 139.918213 loss_att 236.858200 loss_ctc 142.427139 loss_rnnt 120.031403 hw_loss 0.308030 lr 0.00024004 rank 5
2023-02-17 01:55:09,001 DEBUG TRAIN Batch 0/6000 loss 125.409477 loss_att 235.623657 loss_ctc 120.177757 loss_rnnt 103.870468 hw_loss 0.363263 lr 0.00024004 rank 4
2023-02-17 01:55:09,005 DEBUG TRAIN Batch 0/6000 loss 137.009537 loss_att 237.827698 loss_ctc 136.179382 loss_rnnt 116.775505 hw_loss 0.339552 lr 0.00024004 rank 0
2023-02-17 01:55:09,006 DEBUG TRAIN Batch 0/6000 loss 132.680710 loss_att 230.765381 loss_ctc 128.553162 loss_rnnt 113.441689 hw_loss 0.323277 lr 0.00024004 rank 1
2023-02-17 01:55:09,006 DEBUG TRAIN Batch 0/6000 loss 118.120026 loss_att 232.023560 loss_ctc 116.473900 loss_rnnt 95.359680 hw_loss 0.373342 lr 0.00024004 rank 3
2023-02-17 01:55:09,009 DEBUG TRAIN Batch 0/6000 loss 125.797699 loss_att 227.499542 loss_ctc 124.558670 loss_rnnt 105.444122 hw_loss 0.334538 lr 0.00024004 rank 6
2023-02-17 01:55:09,012 DEBUG TRAIN Batch 0/6000 loss 108.785019 loss_att 198.294891 loss_ctc 103.123077 loss_rnnt 91.449036 hw_loss 0.354243 lr 0.00024004 rank 7
2023-02-17 01:57:53,336 DEBUG TRAIN Batch 0/6100 loss 125.848091 loss_att 238.171112 loss_ctc 119.566833 loss_rnnt 104.098228 hw_loss 0.230183 lr 0.00024404 rank 0
2023-02-17 01:57:53,338 DEBUG TRAIN Batch 0/6100 loss 125.953529 loss_att 225.808624 loss_ctc 121.593796 loss_rnnt 106.399948 hw_loss 0.307255 lr 0.00024404 rank 1
2023-02-17 01:57:53,339 DEBUG TRAIN Batch 0/6100 loss 118.739983 loss_att 208.230865 loss_ctc 122.726372 loss_rnnt 100.127380 hw_loss 0.342951 lr 0.00024404 rank 5
2023-02-17 01:57:53,338 DEBUG TRAIN Batch 0/6100 loss 119.461884 loss_att 228.254639 loss_ctc 115.338974 loss_rnnt 98.091721 hw_loss 0.302508 lr 0.00024404 rank 4
2023-02-17 01:57:53,340 DEBUG TRAIN Batch 0/6100 loss 106.948875 loss_att 213.000061 loss_ctc 104.864693 loss_rnnt 85.877098 hw_loss 0.261445 lr 0.00024404 rank 6
2023-02-17 01:57:53,342 DEBUG TRAIN Batch 0/6100 loss 133.583054 loss_att 261.272217 loss_ctc 126.684280 loss_rnnt 108.831390 hw_loss 0.250643 lr 0.00024404 rank 7
2023-02-17 01:57:53,345 DEBUG TRAIN Batch 0/6100 loss 102.447083 loss_att 220.512878 loss_ctc 94.974365 loss_rnnt 79.675049 hw_loss 0.291065 lr 0.00024404 rank 3
2023-02-17 01:57:53,353 DEBUG TRAIN Batch 0/6100 loss 112.821465 loss_att 220.747818 loss_ctc 102.289871 loss_rnnt 92.445190 hw_loss 0.366013 lr 0.00024404 rank 2
2023-02-17 01:59:09,904 DEBUG TRAIN Batch 0/6200 loss 113.189629 loss_att 216.124969 loss_ctc 109.328415 loss_rnnt 92.914703 hw_loss 0.380047 lr 0.00024804 rank 7
2023-02-17 01:59:09,906 DEBUG TRAIN Batch 0/6200 loss 142.901337 loss_att 231.765106 loss_ctc 144.327942 loss_rnnt 124.744926 hw_loss 0.362702 lr 0.00024804 rank 6
2023-02-17 01:59:09,906 DEBUG TRAIN Batch 0/6200 loss 136.225891 loss_att 234.800018 loss_ctc 140.792480 loss_rnnt 115.768555 hw_loss 0.250536 lr 0.00024804 rank 4
2023-02-17 01:59:09,907 DEBUG TRAIN Batch 0/6200 loss 128.745346 loss_att 226.250870 loss_ctc 135.244415 loss_rnnt 108.217896 hw_loss 0.299588 lr 0.00024804 rank 3
2023-02-17 01:59:09,907 DEBUG TRAIN Batch 0/6200 loss 131.536575 loss_att 225.497177 loss_ctc 137.064850 loss_rnnt 111.808929 hw_loss 0.372044 lr 0.00024804 rank 0
2023-02-17 01:59:09,909 DEBUG TRAIN Batch 0/6200 loss 112.707573 loss_att 200.205688 loss_ctc 104.585930 loss_rnnt 96.117302 hw_loss 0.325383 lr 0.00024804 rank 2
2023-02-17 01:59:09,918 DEBUG TRAIN Batch 0/6200 loss 126.661201 loss_att 209.762054 loss_ctc 129.978607 loss_rnnt 109.416801 hw_loss 0.341076 lr 0.00024804 rank 5
2023-02-17 01:59:09,920 DEBUG TRAIN Batch 0/6200 loss 87.355881 loss_att 142.831070 loss_ctc 90.478874 loss_rnnt 75.641190 hw_loss 0.381100 lr 0.00024804 rank 1
2023-02-17 02:00:26,021 DEBUG TRAIN Batch 0/6300 loss 113.941887 loss_att 203.925903 loss_ctc 117.533607 loss_rnnt 95.285133 hw_loss 0.339496 lr 0.00025204 rank 7
2023-02-17 02:00:26,022 DEBUG TRAIN Batch 0/6300 loss 113.062103 loss_att 202.329285 loss_ctc 114.763916 loss_rnnt 94.796471 hw_loss 0.347403 lr 0.00025204 rank 4
2023-02-17 02:00:26,024 DEBUG TRAIN Batch 0/6300 loss 84.648094 loss_att 164.773041 loss_ctc 86.158615 loss_rnnt 68.228973 hw_loss 0.361368 lr 0.00025204 rank 3
2023-02-17 02:00:26,024 DEBUG TRAIN Batch 0/6300 loss 103.396637 loss_att 178.917358 loss_ctc 103.705360 loss_rnnt 88.095016 hw_loss 0.293081 lr 0.00025204 rank 2
2023-02-17 02:00:26,025 DEBUG TRAIN Batch 0/6300 loss 100.763756 loss_att 227.954010 loss_ctc 97.806976 loss_rnnt 75.517334 hw_loss 0.379895 lr 0.00025204 rank 1
2023-02-17 02:00:26,027 DEBUG TRAIN Batch 0/6300 loss 113.069794 loss_att 188.227173 loss_ctc 120.364555 loss_rnnt 96.895012 hw_loss 0.320024 lr 0.00025204 rank 0
2023-02-17 02:00:26,028 DEBUG TRAIN Batch 0/6300 loss 72.064156 loss_att 122.095398 loss_ctc 71.892982 loss_rnnt 61.854424 hw_loss 0.424322 lr 0.00025204 rank 5
2023-02-17 02:00:26,061 DEBUG TRAIN Batch 0/6300 loss 73.527351 loss_att 151.046524 loss_ctc 65.872223 loss_rnnt 58.855217 hw_loss 0.354345 lr 0.00025204 rank 6
2023-02-17 02:02:56,777 DEBUG TRAIN Batch 0/6400 loss 147.435135 loss_att 254.785583 loss_ctc 160.495102 loss_rnnt 124.005417 hw_loss 0.409326 lr 0.00025604 rank 6
2023-02-17 02:02:56,778 DEBUG TRAIN Batch 0/6400 loss 40.737476 loss_att 65.882973 loss_ctc 42.064816 loss_rnnt 35.359585 hw_loss 0.322153 lr 0.00025604 rank 0
2023-02-17 02:02:56,779 DEBUG TRAIN Batch 0/6400 loss 59.032879 loss_att 111.051666 loss_ctc 57.554329 loss_rnnt 48.636150 hw_loss 0.356456 lr 0.00025604 rank 4
2023-02-17 02:02:56,779 DEBUG TRAIN Batch 0/6400 loss 61.680412 loss_att 138.242096 loss_ctc 55.879852 loss_rnnt 46.969383 hw_loss 0.322685 lr 0.00025604 rank 7
2023-02-17 02:02:56,781 DEBUG TRAIN Batch 0/6400 loss 124.100128 loss_att 246.104797 loss_ctc 116.019005 loss_rnnt 100.635460 hw_loss 0.264784 lr 0.00025604 rank 1
2023-02-17 02:02:56,791 DEBUG TRAIN Batch 0/6400 loss 156.914505 loss_att 284.173431 loss_ctc 167.156952 loss_rnnt 129.978394 hw_loss 0.222514 lr 0.00025604 rank 2
2023-02-17 02:02:56,819 DEBUG TRAIN Batch 0/6400 loss 87.310455 loss_att 145.262466 loss_ctc 86.290543 loss_rnnt 75.653152 hw_loss 0.380399 lr 0.00025604 rank 3
2023-02-17 02:02:56,827 DEBUG TRAIN Batch 0/6400 loss 91.981499 loss_att 171.104523 loss_ctc 99.910767 loss_rnnt 74.951187 hw_loss 0.278377 lr 0.00025604 rank 5
2023-02-17 02:04:13,202 DEBUG TRAIN Batch 0/6500 loss 111.383385 loss_att 235.959137 loss_ctc 112.482750 loss_rnnt 86.164696 hw_loss 0.294284 lr 0.00026004 rank 5
2023-02-17 02:04:13,203 DEBUG TRAIN Batch 0/6500 loss 88.854172 loss_att 186.419678 loss_ctc 80.881546 loss_rnnt 70.208725 hw_loss 0.366299 lr 0.00026004 rank 2
2023-02-17 02:04:13,205 DEBUG TRAIN Batch 0/6500 loss 114.776131 loss_att 209.101974 loss_ctc 114.856659 loss_rnnt 95.737999 hw_loss 0.304180 lr 0.00026004 rank 3
2023-02-17 02:04:13,211 DEBUG TRAIN Batch 0/6500 loss 156.084900 loss_att 251.097015 loss_ctc 158.174286 loss_rnnt 136.540741 hw_loss 0.493404 lr 0.00026004 rank 6
2023-02-17 02:04:13,215 DEBUG TRAIN Batch 0/6500 loss 120.598007 loss_att 250.528046 loss_ctc 120.850487 loss_rnnt 94.357864 hw_loss 0.413368 lr 0.00026004 rank 7
2023-02-17 02:04:13,249 DEBUG TRAIN Batch 0/6500 loss 131.910675 loss_att 236.777649 loss_ctc 134.482880 loss_rnnt 110.452103 hw_loss 0.266653 lr 0.00026004 rank 4
2023-02-17 02:04:13,256 DEBUG TRAIN Batch 0/6500 loss 131.444794 loss_att 248.494644 loss_ctc 135.778915 loss_rnnt 107.327896 hw_loss 0.241983 lr 0.00026004 rank 0
2023-02-17 02:04:13,264 DEBUG TRAIN Batch 0/6500 loss 125.346687 loss_att 225.805573 loss_ctc 131.108704 loss_rnnt 104.335335 hw_loss 0.283673 lr 0.00026004 rank 1
2023-02-17 02:05:28,381 DEBUG TRAIN Batch 0/6600 loss 116.381172 loss_att 240.796616 loss_ctc 113.610352 loss_rnnt 91.720963 hw_loss 0.274789 lr 0.00026404 rank 4
2023-02-17 02:05:28,382 DEBUG TRAIN Batch 0/6600 loss 88.319725 loss_att 220.847015 loss_ctc 78.306747 loss_rnnt 63.022213 hw_loss 0.238356 lr 0.00026404 rank 7
2023-02-17 02:05:28,383 DEBUG TRAIN Batch 0/6600 loss 110.650368 loss_att 209.129929 loss_ctc 114.910912 loss_rnnt 90.272888 hw_loss 0.212809 lr 0.00026404 rank 6
2023-02-17 02:05:28,383 DEBUG TRAIN Batch 0/6600 loss 89.212593 loss_att 214.814926 loss_ctc 81.919922 loss_rnnt 64.849854 hw_loss 0.402426 lr 0.00026404 rank 5
2023-02-17 02:05:28,384 DEBUG TRAIN Batch 0/6600 loss 94.987671 loss_att 218.406570 loss_ctc 84.616989 loss_rnnt 71.527100 hw_loss 0.299147 lr 0.00026404 rank 0
2023-02-17 02:05:28,387 DEBUG TRAIN Batch 0/6600 loss 131.954300 loss_att 242.591034 loss_ctc 130.113770 loss_rnnt 109.960686 hw_loss 0.209402 lr 0.00026404 rank 2
2023-02-17 02:05:28,443 DEBUG TRAIN Batch 0/6600 loss 96.298752 loss_att 212.646606 loss_ctc 92.028000 loss_rnnt 73.416786 hw_loss 0.340929 lr 0.00026404 rank 3
2023-02-17 02:05:28,454 DEBUG TRAIN Batch 0/6600 loss 102.971291 loss_att 211.851730 loss_ctc 104.315559 loss_rnnt 80.828011 hw_loss 0.352411 lr 0.00026404 rank 1
2023-02-17 02:06:46,466 DEBUG TRAIN Batch 0/6700 loss 113.547821 loss_att 205.320511 loss_ctc 116.123901 loss_rnnt 94.700409 hw_loss 0.280112 lr 0.00026804 rank 2
2023-02-17 02:06:46,472 DEBUG TRAIN Batch 0/6700 loss 124.373497 loss_att 231.501480 loss_ctc 123.533035 loss_rnnt 102.891228 hw_loss 0.316370 lr 0.00026804 rank 6
2023-02-17 02:06:46,480 DEBUG TRAIN Batch 0/6700 loss 101.971863 loss_att 203.548904 loss_ctc 96.944939 loss_rnnt 82.186264 hw_loss 0.263334 lr 0.00026804 rank 4
2023-02-17 02:06:46,480 DEBUG TRAIN Batch 0/6700 loss 136.778030 loss_att 235.967621 loss_ctc 129.293167 loss_rnnt 117.781311 hw_loss 0.293951 lr 0.00026804 rank 3
2023-02-17 02:06:46,483 DEBUG TRAIN Batch 0/6700 loss 108.351006 loss_att 200.143921 loss_ctc 110.133904 loss_rnnt 89.564011 hw_loss 0.357552 lr 0.00026804 rank 1
2023-02-17 02:06:46,484 DEBUG TRAIN Batch 0/6700 loss 107.230225 loss_att 197.397812 loss_ctc 107.881348 loss_rnnt 88.948036 hw_loss 0.303478 lr 0.00026804 rank 5
2023-02-17 02:06:46,484 DEBUG TRAIN Batch 0/6700 loss 97.076675 loss_att 219.850616 loss_ctc 86.653603 loss_rnnt 73.772369 hw_loss 0.261120 lr 0.00026804 rank 0
2023-02-17 02:06:46,487 DEBUG TRAIN Batch 0/6700 loss 122.676041 loss_att 239.127563 loss_ctc 121.294167 loss_rnnt 99.414871 hw_loss 0.290823 lr 0.00026804 rank 7
2023-02-17 02:09:08,104 DEBUG TRAIN Batch 0/6800 loss 78.146843 loss_att 190.913406 loss_ctc 67.261963 loss_rnnt 56.852966 hw_loss 0.359789 lr 0.00027204 rank 3
2023-02-17 02:09:08,108 DEBUG TRAIN Batch 0/6800 loss 104.879166 loss_att 210.283142 loss_ctc 105.891068 loss_rnnt 83.506821 hw_loss 0.293682 lr 0.00027204 rank 4
2023-02-17 02:09:08,109 DEBUG TRAIN Batch 0/6800 loss 96.391449 loss_att 201.443756 loss_ctc 94.675110 loss_rnnt 75.472656 hw_loss 0.257190 lr 0.00027204 rank 6
2023-02-17 02:09:08,111 DEBUG TRAIN Batch 0/6800 loss 77.597740 loss_att 185.568115 loss_ctc 71.605530 loss_rnnt 56.634789 hw_loss 0.314690 lr 0.00027204 rank 2
2023-02-17 02:09:08,113 DEBUG TRAIN Batch 0/6800 loss 109.897995 loss_att 220.862030 loss_ctc 103.771721 loss_rnnt 88.434189 hw_loss 0.164680 lr 0.00027204 rank 5
2023-02-17 02:09:08,119 DEBUG TRAIN Batch 0/6800 loss 107.854042 loss_att 233.109344 loss_ctc 94.893929 loss_rnnt 84.386452 hw_loss 0.271021 lr 0.00027204 rank 7
2023-02-17 02:09:08,120 DEBUG TRAIN Batch 0/6800 loss 78.924782 loss_att 142.808258 loss_ctc 82.268326 loss_rnnt 65.520370 hw_loss 0.341071 lr 0.00027204 rank 1
2023-02-17 02:09:08,132 DEBUG TRAIN Batch 0/6800 loss 89.086334 loss_att 198.214996 loss_ctc 84.506714 loss_rnnt 67.704193 hw_loss 0.313176 lr 0.00027204 rank 0
2023-02-17 02:10:23,377 DEBUG TRAIN Batch 0/6900 loss 130.709015 loss_att 254.436249 loss_ctc 128.505737 loss_rnnt 106.085297 hw_loss 0.322561 lr 0.00027604 rank 3
2023-02-17 02:10:23,383 DEBUG TRAIN Batch 0/6900 loss 103.339867 loss_att 188.258270 loss_ctc 102.628914 loss_rnnt 86.204865 hw_loss 0.461470 lr 0.00027604 rank 4
2023-02-17 02:10:23,385 DEBUG TRAIN Batch 0/6900 loss 92.560791 loss_att 176.873901 loss_ctc 94.610794 loss_rnnt 75.248161 hw_loss 0.331260 lr 0.00027604 rank 5
2023-02-17 02:10:23,389 DEBUG TRAIN Batch 0/6900 loss 131.290482 loss_att 226.873856 loss_ctc 140.722443 loss_rnnt 110.758667 hw_loss 0.295414 lr 0.00027604 rank 6
2023-02-17 02:10:23,390 DEBUG TRAIN Batch 0/6900 loss 75.128769 loss_att 164.968658 loss_ctc 75.330444 loss_rnnt 56.948406 hw_loss 0.347799 lr 0.00027604 rank 2
2023-02-17 02:10:23,420 DEBUG TRAIN Batch 0/6900 loss 101.495888 loss_att 191.655472 loss_ctc 93.048172 loss_rnnt 84.367271 hw_loss 0.418251 lr 0.00027604 rank 0
2023-02-17 02:10:23,424 DEBUG TRAIN Batch 0/6900 loss 112.369324 loss_att 230.525421 loss_ctc 104.038879 loss_rnnt 89.690811 hw_loss 0.296283 lr 0.00027604 rank 1
2023-02-17 02:10:23,427 DEBUG TRAIN Batch 0/6900 loss 102.329330 loss_att 192.502899 loss_ctc 94.829376 loss_rnnt 85.144829 hw_loss 0.280829 lr 0.00027604 rank 7
2023-02-17 02:11:39,695 DEBUG TRAIN Batch 0/7000 loss 122.697388 loss_att 196.635681 loss_ctc 136.034912 loss_rnnt 105.933548 hw_loss 0.370940 lr 0.00028004 rank 1
2023-02-17 02:11:39,707 DEBUG TRAIN Batch 0/7000 loss 112.025978 loss_att 196.613388 loss_ctc 115.162285 loss_rnnt 94.551353 hw_loss 0.260570 lr 0.00028004 rank 7
2023-02-17 02:11:39,712 DEBUG TRAIN Batch 0/7000 loss 66.405312 loss_att 111.213654 loss_ctc 70.464737 loss_rnnt 56.692604 hw_loss 0.393337 lr 0.00028004 rank 5
2023-02-17 02:11:39,713 DEBUG TRAIN Batch 0/7000 loss 68.276924 loss_att 150.426697 loss_ctc 71.015282 loss_rnnt 51.325649 hw_loss 0.292891 lr 0.00028004 rank 4
2023-02-17 02:11:39,716 DEBUG TRAIN Batch 0/7000 loss 76.239204 loss_att 132.019943 loss_ctc 80.086731 loss_rnnt 64.330452 hw_loss 0.449231 lr 0.00028004 rank 6
2023-02-17 02:11:39,716 DEBUG TRAIN Batch 0/7000 loss 70.117683 loss_att 159.882446 loss_ctc 71.757202 loss_rnnt 51.742886 hw_loss 0.381060 lr 0.00028004 rank 3
2023-02-17 02:11:39,717 DEBUG TRAIN Batch 0/7000 loss 123.513779 loss_att 247.826874 loss_ctc 127.529465 loss_rnnt 98.022797 hw_loss 0.174251 lr 0.00028004 rank 2
2023-02-17 02:11:39,739 DEBUG TRAIN Batch 0/7000 loss 81.828430 loss_att 147.679398 loss_ctc 82.306465 loss_rnnt 68.405357 hw_loss 0.354632 lr 0.00028004 rank 0
2023-02-17 02:14:11,887 DEBUG TRAIN Batch 0/7100 loss 91.643471 loss_att 205.360733 loss_ctc 92.521240 loss_rnnt 68.637680 hw_loss 0.272425 lr 0.00028404 rank 5
2023-02-17 02:14:11,896 DEBUG TRAIN Batch 0/7100 loss 124.847900 loss_att 252.016785 loss_ctc 109.835983 loss_rnnt 101.242538 hw_loss 0.324707 lr 0.00028404 rank 0
2023-02-17 02:14:11,901 DEBUG TRAIN Batch 0/7100 loss 121.473755 loss_att 227.601074 loss_ctc 126.635765 loss_rnnt 99.400681 hw_loss 0.298776 lr 0.00028404 rank 3
2023-02-17 02:14:11,918 DEBUG TRAIN Batch 0/7100 loss 134.766022 loss_att 258.217957 loss_ctc 133.622665 loss_rnnt 110.086586 hw_loss 0.265277 lr 0.00028404 rank 1
2023-02-17 02:14:11,930 DEBUG TRAIN Batch 0/7100 loss 96.269493 loss_att 223.268463 loss_ctc 88.129517 loss_rnnt 71.837364 hw_loss 0.220625 lr 0.00028404 rank 6
2023-02-17 02:14:11,943 DEBUG TRAIN Batch 0/7100 loss 125.831940 loss_att 239.097900 loss_ctc 126.013580 loss_rnnt 103.021538 hw_loss 0.249374 lr 0.00028404 rank 7
2023-02-17 02:14:11,955 DEBUG TRAIN Batch 0/7100 loss 97.057938 loss_att 202.052917 loss_ctc 101.892746 loss_rnnt 75.257835 hw_loss 0.293366 lr 0.00028404 rank 2
2023-02-17 02:14:11,968 DEBUG TRAIN Batch 0/7100 loss 47.866753 loss_att 78.149353 loss_ctc 50.800671 loss_rnnt 41.304604 hw_loss 0.214570 lr 0.00028404 rank 4
2023-02-17 02:15:30,563 DEBUG TRAIN Batch 0/7200 loss 79.644684 loss_att 198.911774 loss_ctc 77.874146 loss_rnnt 55.889812 hw_loss 0.257852 lr 0.00028804 rank 3
2023-02-17 02:15:30,566 DEBUG TRAIN Batch 0/7200 loss 133.163589 loss_att 251.929535 loss_ctc 133.128754 loss_rnnt 109.271866 hw_loss 0.268440 lr 0.00028804 rank 7
2023-02-17 02:15:30,567 DEBUG TRAIN Batch 0/7200 loss 74.768364 loss_att 191.153992 loss_ctc 69.569946 loss_rnnt 51.998901 hw_loss 0.347736 lr 0.00028804 rank 2
2023-02-17 02:15:30,568 DEBUG TRAIN Batch 0/7200 loss 138.052460 loss_att 239.539520 loss_ctc 146.352844 loss_rnnt 116.495239 hw_loss 0.287001 lr 0.00028804 rank 5
2023-02-17 02:15:30,568 DEBUG TRAIN Batch 0/7200 loss 74.468307 loss_att 175.026840 loss_ctc 69.583305 loss_rnnt 54.826302 hw_loss 0.340555 lr 0.00028804 rank 6
2023-02-17 02:15:30,568 DEBUG TRAIN Batch 0/7200 loss 95.740448 loss_att 195.962372 loss_ctc 91.063782 loss_rnnt 76.180389 hw_loss 0.261060 lr 0.00028804 rank 4
2023-02-17 02:15:30,573 DEBUG TRAIN Batch 0/7200 loss 145.318542 loss_att 261.887695 loss_ctc 147.634583 loss_rnnt 121.488052 hw_loss 0.389711 lr 0.00028804 rank 1
2023-02-17 02:15:30,613 DEBUG TRAIN Batch 0/7200 loss 94.392448 loss_att 197.400589 loss_ctc 90.223267 loss_rnnt 74.189056 hw_loss 0.295609 lr 0.00028804 rank 0
2023-02-17 02:16:46,339 DEBUG TRAIN Batch 0/7300 loss 104.391823 loss_att 228.830566 loss_ctc 105.126465 loss_rnnt 79.223038 hw_loss 0.343293 lr 0.00029204 rank 6
2023-02-17 02:16:46,341 DEBUG TRAIN Batch 0/7300 loss 91.915817 loss_att 203.183914 loss_ctc 85.747284 loss_rnnt 70.331924 hw_loss 0.286387 lr 0.00029204 rank 1
2023-02-17 02:16:46,341 DEBUG TRAIN Batch 0/7300 loss 86.810661 loss_att 194.554840 loss_ctc 88.216339 loss_rnnt 64.899994 hw_loss 0.327023 lr 0.00029204 rank 0
2023-02-17 02:16:46,344 DEBUG TRAIN Batch 0/7300 loss 120.278915 loss_att 237.722565 loss_ctc 125.601181 loss_rnnt 95.906357 hw_loss 0.326607 lr 0.00029204 rank 4
2023-02-17 02:16:46,346 DEBUG TRAIN Batch 0/7300 loss 79.680458 loss_att 204.049805 loss_ctc 72.663589 loss_rnnt 55.595409 hw_loss 0.275172 lr 0.00029204 rank 7
2023-02-17 02:16:46,388 DEBUG TRAIN Batch 0/7300 loss 93.152718 loss_att 199.868576 loss_ctc 91.189178 loss_rnnt 71.915573 hw_loss 0.292079 lr 0.00029204 rank 2
2023-02-17 02:16:46,393 DEBUG TRAIN Batch 0/7300 loss 129.851059 loss_att 249.834396 loss_ctc 140.843506 loss_rnnt 104.211784 hw_loss 0.331747 lr 0.00029204 rank 3
2023-02-17 02:16:46,404 DEBUG TRAIN Batch 0/7300 loss 80.684799 loss_att 191.590851 loss_ctc 75.855392 loss_rnnt 58.985779 hw_loss 0.303228 lr 0.00029204 rank 5
2023-02-17 02:18:03,861 DEBUG TRAIN Batch 0/7400 loss 90.472908 loss_att 197.895798 loss_ctc 87.661621 loss_rnnt 69.199501 hw_loss 0.306873 lr 0.00029604 rank 3
2023-02-17 02:18:03,862 DEBUG TRAIN Batch 0/7400 loss 113.325188 loss_att 241.794952 loss_ctc 109.448135 loss_rnnt 87.967117 hw_loss 0.339445 lr 0.00029604 rank 0
2023-02-17 02:18:03,865 DEBUG TRAIN Batch 0/7400 loss 89.399452 loss_att 184.246490 loss_ctc 91.129425 loss_rnnt 70.027527 hw_loss 0.322231 lr 0.00029604 rank 2
2023-02-17 02:18:03,867 DEBUG TRAIN Batch 0/7400 loss 134.453827 loss_att 244.477173 loss_ctc 138.065125 loss_rnnt 111.858627 hw_loss 0.204412 lr 0.00029604 rank 5
2023-02-17 02:18:03,878 DEBUG TRAIN Batch 0/7400 loss 83.909790 loss_att 184.272568 loss_ctc 83.421577 loss_rnnt 63.740337 hw_loss 0.303729 lr 0.00029604 rank 6
2023-02-17 02:18:03,882 DEBUG TRAIN Batch 0/7400 loss 93.540985 loss_att 177.274567 loss_ctc 96.996468 loss_rnnt 76.168518 hw_loss 0.309407 lr 0.00029604 rank 1
2023-02-17 02:18:03,890 DEBUG TRAIN Batch 0/7400 loss 129.305084 loss_att 251.758652 loss_ctc 133.124634 loss_rnnt 104.106644 hw_loss 0.372098 lr 0.00029604 rank 4
2023-02-17 02:18:03,904 DEBUG TRAIN Batch 0/7400 loss 94.550583 loss_att 196.081284 loss_ctc 94.583054 loss_rnnt 74.011375 hw_loss 0.428886 lr 0.00029604 rank 7
2023-02-17 02:20:41,842 DEBUG TRAIN Batch 0/7500 loss 79.592743 loss_att 173.613251 loss_ctc 77.906593 loss_rnnt 60.854069 hw_loss 0.298844 lr 0.00030004 rank 0
2023-02-17 02:20:41,843 DEBUG TRAIN Batch 0/7500 loss 97.881119 loss_att 204.726379 loss_ctc 96.804108 loss_rnnt 76.504410 hw_loss 0.283631 lr 0.00030004 rank 6
2023-02-17 02:20:41,844 DEBUG TRAIN Batch 0/7500 loss 101.541016 loss_att 199.705170 loss_ctc 102.177261 loss_rnnt 81.663956 hw_loss 0.298890 lr 0.00030004 rank 7
2023-02-17 02:20:41,848 DEBUG TRAIN Batch 0/7500 loss 100.029945 loss_att 212.003326 loss_ctc 97.843727 loss_rnnt 77.758270 hw_loss 0.315927 lr 0.00030004 rank 3
2023-02-17 02:20:41,849 DEBUG TRAIN Batch 0/7500 loss 95.858925 loss_att 188.100342 loss_ctc 100.777557 loss_rnnt 76.557419 hw_loss 0.370141 lr 0.00030004 rank 2
2023-02-17 02:20:41,857 DEBUG TRAIN Batch 0/7500 loss 87.901878 loss_att 185.028168 loss_ctc 87.481636 loss_rnnt 68.377121 hw_loss 0.291610 lr 0.00030004 rank 5
2023-02-17 02:20:41,867 DEBUG TRAIN Batch 0/7500 loss 39.324001 loss_att 59.223091 loss_ctc 44.355621 loss_rnnt 34.414062 hw_loss 0.486064 lr 0.00030004 rank 1
2023-02-17 02:20:41,876 DEBUG TRAIN Batch 0/7500 loss 86.524666 loss_att 182.802963 loss_ctc 85.395775 loss_rnnt 67.217392 hw_loss 0.378995 lr 0.00030004 rank 4
2023-02-17 02:21:58,193 DEBUG TRAIN Batch 0/7600 loss 89.619484 loss_att 169.433273 loss_ctc 90.101799 loss_rnnt 73.412758 hw_loss 0.336848 lr 0.00030404 rank 0
2023-02-17 02:21:58,196 DEBUG TRAIN Batch 0/7600 loss 84.265442 loss_att 162.897568 loss_ctc 82.138771 loss_rnnt 68.656235 hw_loss 0.311873 lr 0.00030404 rank 1
2023-02-17 02:21:58,197 DEBUG TRAIN Batch 0/7600 loss 75.017769 loss_att 155.827835 loss_ctc 75.834808 loss_rnnt 58.531750 hw_loss 0.403251 lr 0.00030404 rank 4
2023-02-17 02:21:58,197 DEBUG TRAIN Batch 0/7600 loss 53.941265 loss_att 88.887619 loss_ctc 57.921021 loss_rnnt 46.220707 hw_loss 0.376214 lr 0.00030404 rank 2
2023-02-17 02:21:58,198 DEBUG TRAIN Batch 0/7600 loss 54.396118 loss_att 122.504608 loss_ctc 51.391766 loss_rnnt 40.984081 hw_loss 0.357969 lr 0.00030404 rank 5
2023-02-17 02:21:58,197 DEBUG TRAIN Batch 0/7600 loss 94.403961 loss_att 182.992645 loss_ctc 91.105927 loss_rnnt 76.924904 hw_loss 0.376977 lr 0.00030404 rank 7
2023-02-17 02:21:58,198 DEBUG TRAIN Batch 0/7600 loss 88.944672 loss_att 181.466156 loss_ctc 90.680603 loss_rnnt 70.013954 hw_loss 0.365543 lr 0.00030404 rank 3
2023-02-17 02:21:58,198 DEBUG TRAIN Batch 0/7600 loss 72.207947 loss_att 149.947266 loss_ctc 72.948143 loss_rnnt 56.368862 hw_loss 0.360990 lr 0.00030404 rank 6
2023-02-17 02:23:15,825 DEBUG TRAIN Batch 0/7700 loss 57.537415 loss_att 107.362030 loss_ctc 59.328743 loss_rnnt 47.121349 hw_loss 0.398068 lr 0.00030804 rank 4
2023-02-17 02:23:15,826 DEBUG TRAIN Batch 0/7700 loss 49.384106 loss_att 79.379097 loss_ctc 52.636250 loss_rnnt 42.692097 hw_loss 0.486354 lr 0.00030804 rank 0
2023-02-17 02:23:15,826 DEBUG TRAIN Batch 0/7700 loss 89.203568 loss_att 215.559097 loss_ctc 86.831535 loss_rnnt 64.000931 hw_loss 0.464626 lr 0.00030804 rank 2
2023-02-17 02:23:15,829 DEBUG TRAIN Batch 0/7700 loss 132.809357 loss_att 257.961212 loss_ctc 139.247284 loss_rnnt 106.787262 hw_loss 0.250032 lr 0.00030804 rank 5
2023-02-17 02:23:15,832 DEBUG TRAIN Batch 0/7700 loss 56.278057 loss_att 100.447685 loss_ctc 59.678223 loss_rnnt 46.757378 hw_loss 0.437624 lr 0.00030804 rank 7
2023-02-17 02:23:15,841 DEBUG TRAIN Batch 0/7700 loss 34.572556 loss_att 49.701202 loss_ctc 35.552185 loss_rnnt 31.238962 hw_loss 0.332333 lr 0.00030804 rank 6
2023-02-17 02:23:15,873 DEBUG TRAIN Batch 0/7700 loss 120.128365 loss_att 246.189575 loss_ctc 112.278069 loss_rnnt 95.794861 hw_loss 0.314949 lr 0.00030804 rank 1
2023-02-17 02:23:15,875 DEBUG TRAIN Batch 0/7700 loss 50.604168 loss_att 80.595200 loss_ctc 53.747566 loss_rnnt 43.932205 hw_loss 0.477447 lr 0.00030804 rank 3
2023-02-17 02:24:35,791 DEBUG TRAIN Batch 0/7800 loss 87.344086 loss_att 186.736923 loss_ctc 78.763992 loss_rnnt 68.486588 hw_loss 0.230526 lr 0.00031204 rank 3
2023-02-17 02:24:35,792 DEBUG TRAIN Batch 0/7800 loss 109.419189 loss_att 213.407288 loss_ctc 114.383057 loss_rnnt 87.775574 hw_loss 0.345261 lr 0.00031204 rank 6
2023-02-17 02:24:35,795 DEBUG TRAIN Batch 0/7800 loss 70.402344 loss_att 179.191986 loss_ctc 70.533806 loss_rnnt 48.514801 hw_loss 0.210146 lr 0.00031204 rank 4
2023-02-17 02:24:35,798 DEBUG TRAIN Batch 0/7800 loss 100.295456 loss_att 229.675995 loss_ctc 100.662964 loss_rnnt 74.232483 hw_loss 0.258487 lr 0.00031204 rank 5
2023-02-17 02:24:35,810 DEBUG TRAIN Batch 0/7800 loss 93.802902 loss_att 209.528870 loss_ctc 83.975922 loss_rnnt 71.850891 hw_loss 0.219536 lr 0.00031204 rank 7
2023-02-17 02:24:35,836 DEBUG TRAIN Batch 0/7800 loss 85.539001 loss_att 207.694000 loss_ctc 90.659653 loss_rnnt 60.244591 hw_loss 0.338737 lr 0.00031204 rank 2
2023-02-17 02:24:35,844 DEBUG TRAIN Batch 0/7800 loss 86.776611 loss_att 198.534134 loss_ctc 84.015259 loss_rnnt 64.643311 hw_loss 0.281205 lr 0.00031204 rank 1
2023-02-17 02:24:35,846 DEBUG TRAIN Batch 0/7800 loss 114.113411 loss_att 242.969086 loss_ctc 117.389977 loss_rnnt 87.806946 hw_loss 0.184601 lr 0.00031204 rank 0
2023-02-17 02:27:02,949 DEBUG TRAIN Batch 0/7900 loss 92.491074 loss_att 193.427460 loss_ctc 92.697189 loss_rnnt 72.134254 hw_loss 0.266356 lr 0.00031604 rank 1
2023-02-17 02:27:02,962 DEBUG TRAIN Batch 0/7900 loss 89.762634 loss_att 190.248383 loss_ctc 85.977859 loss_rnnt 69.990128 hw_loss 0.337484 lr 0.00031604 rank 0
2023-02-17 02:27:02,962 DEBUG TRAIN Batch 0/7900 loss 135.970032 loss_att 266.907990 loss_ctc 139.556229 loss_rnnt 109.126221 hw_loss 0.333876 lr 0.00031604 rank 5
2023-02-17 02:27:02,965 DEBUG TRAIN Batch 0/7900 loss 87.641670 loss_att 196.822006 loss_ctc 94.712402 loss_rnnt 64.729080 hw_loss 0.250803 lr 0.00031604 rank 7
2023-02-17 02:27:02,969 DEBUG TRAIN Batch 0/7900 loss 103.885620 loss_att 202.609375 loss_ctc 115.369797 loss_rnnt 82.450584 hw_loss 0.298228 lr 0.00031604 rank 4
2023-02-17 02:27:02,987 DEBUG TRAIN Batch 0/7900 loss 74.841995 loss_att 194.640167 loss_ctc 65.133583 loss_rnnt 51.992905 hw_loss 0.344830 lr 0.00031604 rank 6
2023-02-17 02:27:02,998 DEBUG TRAIN Batch 0/7900 loss 102.736267 loss_att 218.152679 loss_ctc 103.813484 loss_rnnt 79.359192 hw_loss 0.281558 lr 0.00031604 rank 2
2023-02-17 02:27:03,022 DEBUG TRAIN Batch 0/7900 loss 78.809845 loss_att 156.890762 loss_ctc 78.606491 loss_rnnt 63.068401 hw_loss 0.285707 lr 0.00031604 rank 3
2023-02-17 02:28:17,998 DEBUG TRAIN Batch 0/8000 loss 74.883904 loss_att 173.383698 loss_ctc 78.103394 loss_rnnt 54.543182 hw_loss 0.396550 lr 0.00032004 rank 5
2023-02-17 02:28:18,014 DEBUG TRAIN Batch 0/8000 loss 105.535446 loss_att 213.266998 loss_ctc 107.711571 loss_rnnt 83.544067 hw_loss 0.290462 lr 0.00032004 rank 7
2023-02-17 02:28:18,016 DEBUG TRAIN Batch 0/8000 loss 68.744514 loss_att 160.876480 loss_ctc 66.348488 loss_rnnt 50.505299 hw_loss 0.248049 lr 0.00032004 rank 1
2023-02-17 02:28:18,021 DEBUG TRAIN Batch 0/8000 loss 81.444672 loss_att 186.111938 loss_ctc 85.500404 loss_rnnt 59.790962 hw_loss 0.336524 lr 0.00032004 rank 2
2023-02-17 02:28:18,051 DEBUG TRAIN Batch 0/8000 loss 103.848022 loss_att 207.334747 loss_ctc 107.949234 loss_rnnt 82.409447 hw_loss 0.364522 lr 0.00032004 rank 3
2023-02-17 02:28:18,053 DEBUG TRAIN Batch 0/8000 loss 75.645500 loss_att 174.425995 loss_ctc 75.385513 loss_rnnt 55.755089 hw_loss 0.316821 lr 0.00032004 rank 4
2023-02-17 02:28:18,053 DEBUG TRAIN Batch 0/8000 loss 99.689392 loss_att 211.840271 loss_ctc 105.272400 loss_rnnt 76.347244 hw_loss 0.314189 lr 0.00032004 rank 0
2023-02-17 02:28:18,059 DEBUG TRAIN Batch 0/8000 loss 122.500420 loss_att 230.750290 loss_ctc 127.113220 loss_rnnt 100.041641 hw_loss 0.363292 lr 0.00032004 rank 6
2023-02-17 02:29:35,389 DEBUG TRAIN Batch 0/8100 loss 78.969727 loss_att 145.507904 loss_ctc 79.852722 loss_rnnt 65.372345 hw_loss 0.322527 lr 0.00032404 rank 6
2023-02-17 02:29:35,390 DEBUG TRAIN Batch 0/8100 loss 85.035637 loss_att 180.645996 loss_ctc 96.332970 loss_rnnt 64.247345 hw_loss 0.299814 lr 0.00032404 rank 7
2023-02-17 02:29:35,396 DEBUG TRAIN Batch 0/8100 loss 94.326546 loss_att 181.811386 loss_ctc 100.154083 loss_rnnt 75.903061 hw_loss 0.280315 lr 0.00032404 rank 3
2023-02-17 02:29:35,419 DEBUG TRAIN Batch 0/8100 loss 95.384926 loss_att 211.958740 loss_ctc 93.209923 loss_rnnt 72.214775 hw_loss 0.272623 lr 0.00032404 rank 1
2023-02-17 02:29:35,447 DEBUG TRAIN Batch 0/8100 loss 104.347847 loss_att 210.362808 loss_ctc 110.638199 loss_rnnt 82.176483 hw_loss 0.243118 lr 0.00032404 rank 4
2023-02-17 02:29:35,447 DEBUG TRAIN Batch 0/8100 loss 67.390129 loss_att 166.654877 loss_ctc 68.442093 loss_rnnt 47.252995 hw_loss 0.269842 lr 0.00032404 rank 5
2023-02-17 02:29:35,450 DEBUG TRAIN Batch 0/8100 loss 70.101524 loss_att 174.560303 loss_ctc 65.079041 loss_rnnt 49.644943 hw_loss 0.439660 lr 0.00032404 rank 0
2023-02-17 02:29:35,461 DEBUG TRAIN Batch 0/8100 loss 68.197899 loss_att 154.427002 loss_ctc 64.209534 loss_rnnt 51.335499 hw_loss 0.278188 lr 0.00032404 rank 2
2023-02-17 02:30:54,415 DEBUG TRAIN Batch 0/8200 loss 89.676178 loss_att 179.958267 loss_ctc 89.853188 loss_rnnt 71.466949 hw_loss 0.242243 lr 0.00032804 rank 0
2023-02-17 02:30:54,420 DEBUG TRAIN Batch 0/8200 loss 131.737030 loss_att 230.827087 loss_ctc 141.391312 loss_rnnt 110.485947 hw_loss 0.273441 lr 0.00032804 rank 7
2023-02-17 02:30:54,425 DEBUG TRAIN Batch 0/8200 loss 78.633163 loss_att 167.830658 loss_ctc 80.919182 loss_rnnt 60.327824 hw_loss 0.301936 lr 0.00032804 rank 3
2023-02-17 02:30:54,425 DEBUG TRAIN Batch 0/8200 loss 48.462944 loss_att 102.887260 loss_ctc 50.268162 loss_rnnt 37.109669 hw_loss 0.426958 lr 0.00032804 rank 2
2023-02-17 02:30:54,436 DEBUG TRAIN Batch 0/8200 loss 94.759026 loss_att 173.901215 loss_ctc 101.589783 loss_rnnt 77.862930 hw_loss 0.294167 lr 0.00032804 rank 4
2023-02-17 02:30:54,450 DEBUG TRAIN Batch 0/8200 loss 79.249138 loss_att 169.474091 loss_ctc 86.323273 loss_rnnt 60.133129 hw_loss 0.239619 lr 0.00032804 rank 6
2023-02-17 02:30:54,467 DEBUG TRAIN Batch 0/8200 loss 61.009949 loss_att 144.700912 loss_ctc 63.088039 loss_rnnt 43.829483 hw_loss 0.309736 lr 0.00032804 rank 5
2023-02-17 02:30:54,485 DEBUG TRAIN Batch 0/8200 loss 109.918945 loss_att 203.670242 loss_ctc 112.300072 loss_rnnt 90.705750 hw_loss 0.272718 lr 0.00032804 rank 1
2023-02-17 02:32:09,992 DEBUG TRAIN Batch 0/8300 loss 86.839676 loss_att 168.794830 loss_ctc 90.153618 loss_rnnt 69.856552 hw_loss 0.281672 lr 0.00033204 rank 3
2023-02-17 02:32:10,006 DEBUG TRAIN Batch 0/8300 loss 79.374840 loss_att 165.471634 loss_ctc 78.616142 loss_rnnt 62.067604 hw_loss 0.354436 lr 0.00033204 rank 4
2023-02-17 02:32:10,006 DEBUG TRAIN Batch 0/8300 loss 85.651474 loss_att 164.876648 loss_ctc 89.424866 loss_rnnt 69.093269 hw_loss 0.393838 lr 0.00033204 rank 6
2023-02-17 02:32:10,007 DEBUG TRAIN Batch 0/8300 loss 74.435669 loss_att 185.362732 loss_ctc 71.758392 loss_rnnt 52.426643 hw_loss 0.338602 lr 0.00033204 rank 2
2023-02-17 02:32:10,009 DEBUG TRAIN Batch 0/8300 loss 93.388046 loss_att 178.896393 loss_ctc 100.091301 loss_rnnt 75.269302 hw_loss 0.231217 lr 0.00033204 rank 0
2023-02-17 02:32:10,011 DEBUG TRAIN Batch 0/8300 loss 80.054054 loss_att 147.136932 loss_ctc 87.646896 loss_rnnt 65.422668 hw_loss 0.379545 lr 0.00033204 rank 5
2023-02-17 02:32:10,013 DEBUG TRAIN Batch 0/8300 loss 69.500832 loss_att 162.939850 loss_ctc 69.022209 loss_rnnt 50.721230 hw_loss 0.291764 lr 0.00033204 rank 7
2023-02-17 02:32:10,034 DEBUG TRAIN Batch 0/8300 loss 109.278679 loss_att 202.605789 loss_ctc 116.386070 loss_rnnt 89.462303 hw_loss 0.381182 lr 0.00033204 rank 1
2023-02-17 02:33:12,392 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 7
2023-02-17 02:33:12,392 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 1
2023-02-17 02:33:12,398 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 3
2023-02-17 02:33:12,401 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 5
2023-02-17 02:33:12,403 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 6
2023-02-17 02:33:12,404 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 0
2023-02-17 02:33:12,405 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 4
2023-02-17 02:33:12,408 DEBUG CV Batch 0/0 loss 15.191278 loss_att 21.496233 loss_ctc 16.382015 loss_rnnt 13.513027 hw_loss 0.484678 history loss 14.628639 rank 2
2023-02-17 02:33:29,242 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 7
2023-02-17 02:33:29,284 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 4
2023-02-17 02:33:29,290 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 5
2023-02-17 02:33:29,291 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 1
2023-02-17 02:33:29,351 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 0
2023-02-17 02:33:29,354 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 6
2023-02-17 02:33:29,407 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 2
2023-02-17 02:33:29,528 DEBUG CV Batch 0/100 loss 64.119576 loss_att 118.898216 loss_ctc 72.773506 loss_rnnt 51.838299 hw_loss 0.321918 history loss 37.166767 rank 3
2023-02-17 02:33:50,331 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 7
2023-02-17 02:33:50,350 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 6
2023-02-17 02:33:50,359 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 1
2023-02-17 02:33:50,381 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 0
2023-02-17 02:33:50,389 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 4
2023-02-17 02:33:50,400 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 3
2023-02-17 02:33:50,521 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 2
2023-02-17 02:33:50,893 DEBUG CV Batch 0/200 loss 146.877884 loss_att 326.168365 loss_ctc 141.662415 loss_rnnt 111.633400 hw_loss 0.153319 history loss 42.087573 rank 5
2023-02-17 02:34:08,375 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 0
2023-02-17 02:34:08,382 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 5
2023-02-17 02:34:08,387 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 7
2023-02-17 02:34:08,403 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 6
2023-02-17 02:34:08,466 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 2
2023-02-17 02:34:08,470 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 4
2023-02-17 02:34:08,492 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 1
2023-02-17 02:34:08,497 DEBUG CV Batch 0/300 loss 43.753700 loss_att 83.022270 loss_ctc 48.414680 loss_rnnt 35.057625 hw_loss 0.414181 history loss 41.370449 rank 3
2023-02-17 02:34:32,216 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 3
2023-02-17 02:34:32,221 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 2
2023-02-17 02:34:32,228 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 7
2023-02-17 02:34:32,231 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 1
2023-02-17 02:34:32,236 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 0
2023-02-17 02:34:32,247 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 5
2023-02-17 02:34:32,253 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 4
2023-02-17 02:34:32,254 DEBUG CV Batch 0/400 loss 201.718872 loss_att 466.907776 loss_ctc 177.731644 loss_rnnt 151.776886 hw_loss 0.192182 history loss 44.180772 rank 6
2023-02-17 02:34:47,584 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 2
2023-02-17 02:34:47,601 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 6
2023-02-17 02:34:47,634 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 3
2023-02-17 02:34:47,666 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 7
2023-02-17 02:34:47,692 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 5
2023-02-17 02:34:47,698 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 4
2023-02-17 02:34:47,745 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 0
2023-02-17 02:34:47,937 DEBUG CV Batch 0/500 loss 65.190765 loss_att 121.355560 loss_ctc 70.872208 loss_rnnt 53.000496 hw_loss 0.374597 history loss 44.355133 rank 1
2023-02-17 02:35:07,478 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 3
2023-02-17 02:35:07,527 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 1
2023-02-17 02:35:07,535 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 6
2023-02-17 02:35:07,546 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 7
2023-02-17 02:35:07,556 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 2
2023-02-17 02:35:07,560 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 0
2023-02-17 02:35:07,565 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 5
2023-02-17 02:35:07,576 DEBUG CV Batch 0/600 loss 32.245579 loss_att 48.007591 loss_ctc 35.850883 loss_rnnt 28.306879 hw_loss 0.572980 history loss 46.010498 rank 4
2023-02-17 02:35:27,516 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 6
2023-02-17 02:35:27,612 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 3
2023-02-17 02:35:27,675 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 1
2023-02-17 02:35:27,720 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 0
2023-02-17 02:35:27,775 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 2
2023-02-17 02:35:27,814 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 7
2023-02-17 02:35:27,968 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 4
2023-02-17 02:35:28,034 DEBUG CV Batch 0/700 loss 169.821472 loss_att 398.970184 loss_ctc 163.291122 loss_rnnt 124.740646 hw_loss 0.228361 history loss 47.147926 rank 5
2023-02-17 02:35:43,560 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 3
2023-02-17 02:35:43,673 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 6
2023-02-17 02:35:43,821 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 1
2023-02-17 02:35:43,861 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 5
2023-02-17 02:35:43,915 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 2
2023-02-17 02:35:44,029 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 4
2023-02-17 02:35:44,125 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 7
2023-02-17 02:35:44,137 DEBUG CV Batch 0/800 loss 61.421627 loss_att 118.227325 loss_ctc 69.587250 loss_rnnt 48.795120 hw_loss 0.331150 history loss 45.693917 rank 0
2023-02-17 02:35:59,903 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 3
2023-02-17 02:36:00,315 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 5
2023-02-17 02:36:00,357 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 6
2023-02-17 02:36:00,432 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 1
2023-02-17 02:36:00,514 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 4
2023-02-17 02:36:00,584 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 7
2023-02-17 02:36:00,748 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 0
2023-02-17 02:36:01,173 DEBUG CV Batch 0/900 loss 117.759300 loss_att 288.881897 loss_ctc 110.251083 loss_rnnt 84.381027 hw_loss 0.290316 history loss 45.930723 rank 2
2023-02-17 02:36:14,285 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 5
2023-02-17 02:36:14,329 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 7
2023-02-17 02:36:14,426 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 1
2023-02-17 02:36:14,487 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 6
2023-02-17 02:36:14,642 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 3
2023-02-17 02:36:14,884 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 4
2023-02-17 02:36:15,333 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 0
2023-02-17 02:36:15,750 DEBUG CV Batch 0/1000 loss 38.609108 loss_att 73.136124 loss_ctc 39.331825 loss_rnnt 31.398750 hw_loss 0.391108 history loss 45.427600 rank 2
2023-02-17 02:36:29,451 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 1
2023-02-17 02:36:29,562 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 6
2023-02-17 02:36:29,577 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 5
2023-02-17 02:36:29,669 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 3
2023-02-17 02:36:29,765 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 7
2023-02-17 02:36:29,918 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 4
2023-02-17 02:36:29,951 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 2
2023-02-17 02:36:29,968 DEBUG CV Batch 0/1100 loss 18.638805 loss_att 24.904678 loss_ctc 20.685072 loss_rnnt 16.833387 hw_loss 0.523892 history loss 45.612130 rank 0
2023-02-17 02:36:43,270 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 6
2023-02-17 02:36:43,336 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 7
2023-02-17 02:36:43,518 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 1
2023-02-17 02:36:43,530 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 3
2023-02-17 02:36:43,747 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 5
2023-02-17 02:36:43,825 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 2
2023-02-17 02:36:43,829 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 4
2023-02-17 02:36:43,883 DEBUG CV Batch 0/1200 loss 78.361427 loss_att 133.640289 loss_ctc 84.506721 loss_rnnt 66.327118 hw_loss 0.298447 history loss 45.949599 rank 0
2023-02-17 02:36:58,104 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 4
2023-02-17 02:36:58,134 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 7
2023-02-17 02:36:58,301 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 3
2023-02-17 02:36:58,427 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 1
2023-02-17 02:36:58,495 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 6
2023-02-17 02:36:58,698 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 0
2023-02-17 02:36:58,699 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 2
2023-02-17 02:36:58,706 DEBUG CV Batch 0/1300 loss 31.805029 loss_att 46.531006 loss_ctc 37.783783 loss_rnnt 27.836664 hw_loss 0.423754 history loss 46.416286 rank 5
2023-02-17 02:37:13,836 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 4
2023-02-17 02:37:13,838 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 7
2023-02-17 02:37:13,879 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 3
2023-02-17 02:37:14,036 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 1
2023-02-17 02:37:14,469 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 6
2023-02-17 02:37:14,778 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 5
2023-02-17 02:37:14,958 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 2
2023-02-17 02:37:15,034 DEBUG CV Batch 0/1400 loss 136.926056 loss_att 315.547607 loss_ctc 125.697830 loss_rnnt 102.548752 hw_loss 0.281414 history loss 47.033307 rank 0
2023-02-17 02:37:29,144 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 4
2023-02-17 02:37:29,658 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 7
2023-02-17 02:37:29,695 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 1
2023-02-17 02:37:30,010 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 3
2023-02-17 02:37:30,118 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 6
2023-02-17 02:37:30,332 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 0
2023-02-17 02:37:30,951 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 5
2023-02-17 02:37:31,222 DEBUG CV Batch 0/1500 loss 58.949707 loss_att 124.229904 loss_ctc 59.449215 loss_rnnt 45.642075 hw_loss 0.346864 history loss 46.343468 rank 2
2023-02-17 02:37:45,762 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 1
2023-02-17 02:37:45,844 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 4
2023-02-17 02:37:45,855 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 7
2023-02-17 02:37:46,717 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 6
2023-02-17 02:37:46,811 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 3
2023-02-17 02:37:46,815 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 0
2023-02-17 02:37:47,300 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 2
2023-02-17 02:37:47,760 DEBUG CV Batch 0/1600 loss 109.554504 loss_att 303.501343 loss_ctc 103.170547 loss_rnnt 71.463623 hw_loss 0.286323 history loss 46.486398 rank 5
2023-02-17 02:37:59,970 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 7
2023-02-17 02:38:00,316 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 4
2023-02-17 02:38:00,410 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 1
2023-02-17 02:38:01,174 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 6
2023-02-17 02:38:01,205 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 3
2023-02-17 02:38:01,874 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 2
2023-02-17 02:38:02,189 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 0
2023-02-17 02:38:02,605 DEBUG CV Batch 0/1700 loss 56.494118 loss_att 94.922714 loss_ctc 61.933796 loss_rnnt 47.842098 hw_loss 0.451899 history loss 46.166997 rank 5
2023-02-17 02:38:10,186 INFO Epoch 0 CV info cv_loss 46.328124623506085
2023-02-17 02:38:10,186 INFO Epoch 1 TRAIN info lr 0.000333
2023-02-17 02:38:10,191 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:10,550 INFO Epoch 0 CV info cv_loss 46.3281246259871
2023-02-17 02:38:10,550 INFO Epoch 1 TRAIN info lr 0.00033288
2023-02-17 02:38:10,554 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:10,864 INFO Epoch 0 CV info cv_loss 46.328124623506085
2023-02-17 02:38:10,865 INFO Epoch 1 TRAIN info lr 0.00033487999999999997
2023-02-17 02:38:10,866 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:11,138 INFO Epoch 0 CV info cv_loss 46.3281246259871
2023-02-17 02:38:11,139 INFO Epoch 1 TRAIN info lr 0.0003332
2023-02-17 02:38:11,142 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:11,226 INFO Epoch 0 CV info cv_loss 46.3281246259871
2023-02-17 02:38:11,227 INFO Epoch 1 TRAIN info lr 0.00033312
2023-02-17 02:38:11,231 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:11,991 INFO Epoch 0 CV info cv_loss 46.3281246255736
2023-02-17 02:38:11,991 INFO Epoch 1 TRAIN info lr 0.00033392000000000003
2023-02-17 02:38:11,994 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:12,434 INFO Epoch 0 CV info cv_loss 46.32812462860595
2023-02-17 02:38:12,435 INFO Epoch 1 TRAIN info lr 0.00033336
2023-02-17 02:38:12,438 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:38:12,469 INFO Epoch 0 CV info cv_loss 46.3281246259871
2023-02-17 02:38:12,470 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/0.pt
2023-02-17 02:38:13,364 INFO Epoch 1 TRAIN info lr 0.00033316
2023-02-17 02:38:13,370 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 02:39:21,260 DEBUG TRAIN Batch 1/0 loss 25.782894 loss_att 32.574230 loss_ctc 28.850281 loss_rnnt 23.798340 hw_loss 0.407440 lr 0.00033492 rank 1
2023-02-17 02:39:21,267 DEBUG TRAIN Batch 1/0 loss 30.845669 loss_att 33.939285 loss_ctc 35.951397 loss_rnnt 29.261431 hw_loss 0.533904 lr 0.00033304 rank 7
2023-02-17 02:39:21,267 DEBUG TRAIN Batch 1/0 loss 28.250153 loss_att 36.100121 loss_ctc 31.021376 loss_rnnt 26.010122 hw_loss 0.563512 lr 0.00033316 rank 3
2023-02-17 02:39:21,269 DEBUG TRAIN Batch 1/0 loss 30.106476 loss_att 34.910950 loss_ctc 32.506264 loss_rnnt 28.616938 hw_loss 0.391259 lr 0.00033292 rank 4
2023-02-17 02:39:21,270 DEBUG TRAIN Batch 1/0 loss 30.128267 loss_att 36.720081 loss_ctc 33.231262 loss_rnnt 28.161264 hw_loss 0.440451 lr 0.00033324 rank 6
2023-02-17 02:39:21,298 DEBUG TRAIN Batch 1/0 loss 33.867825 loss_att 40.478531 loss_ctc 41.416897 loss_rnnt 31.346693 hw_loss 0.360843 lr 0.00033340 rank 5
2023-02-17 02:39:21,330 DEBUG TRAIN Batch 1/0 loss 27.096012 loss_att 37.184612 loss_ctc 28.430729 loss_rnnt 24.653965 hw_loss 0.461931 lr 0.00033320 rank 0
2023-02-17 02:39:21,333 DEBUG TRAIN Batch 1/0 loss 27.376759 loss_att 35.237751 loss_ctc 28.793020 loss_rnnt 25.369009 hw_loss 0.462591 lr 0.00033396 rank 2
2023-02-17 02:40:36,518 DEBUG TRAIN Batch 1/100 loss 91.036728 loss_att 189.053146 loss_ctc 90.497208 loss_rnnt 71.335442 hw_loss 0.318641 lr 0.00033692 rank 4
2023-02-17 02:40:36,519 DEBUG TRAIN Batch 1/100 loss 76.807693 loss_att 159.587158 loss_ctc 81.029755 loss_rnnt 59.540100 hw_loss 0.278938 lr 0.00033892 rank 1
2023-02-17 02:40:36,522 DEBUG TRAIN Batch 1/100 loss 87.956146 loss_att 185.058945 loss_ctc 99.164413 loss_rnnt 66.899254 hw_loss 0.266053 lr 0.00033724 rank 6
2023-02-17 02:40:36,559 DEBUG TRAIN Batch 1/100 loss 80.562233 loss_att 199.254547 loss_ctc 78.061356 loss_rnnt 56.937958 hw_loss 0.411114 lr 0.00033720 rank 0
2023-02-17 02:40:36,568 DEBUG TRAIN Batch 1/100 loss 111.202682 loss_att 220.258362 loss_ctc 110.951172 loss_rnnt 89.229004 hw_loss 0.367634 lr 0.00033740 rank 5
2023-02-17 02:40:36,569 DEBUG TRAIN Batch 1/100 loss 66.635666 loss_att 161.664978 loss_ctc 56.115276 loss_rnnt 48.855804 hw_loss 0.331345 lr 0.00033716 rank 3
2023-02-17 02:40:36,590 DEBUG TRAIN Batch 1/100 loss 99.176453 loss_att 183.514526 loss_ctc 91.408447 loss_rnnt 83.123512 hw_loss 0.414470 lr 0.00033704 rank 7
2023-02-17 02:40:36,592 DEBUG TRAIN Batch 1/100 loss 111.181961 loss_att 220.190979 loss_ctc 123.441589 loss_rnnt 87.598328 hw_loss 0.276007 lr 0.00033796 rank 2
2023-02-17 02:41:51,943 DEBUG TRAIN Batch 1/200 loss 84.775032 loss_att 194.340591 loss_ctc 101.138611 loss_rnnt 60.516525 hw_loss 0.306717 lr 0.00034120 rank 0
2023-02-17 02:41:51,947 DEBUG TRAIN Batch 1/200 loss 107.156967 loss_att 197.327820 loss_ctc 113.705795 loss_rnnt 88.078720 hw_loss 0.320443 lr 0.00034104 rank 7
2023-02-17 02:41:51,958 DEBUG TRAIN Batch 1/200 loss 61.471092 loss_att 160.241257 loss_ctc 59.622295 loss_rnnt 41.794365 hw_loss 0.317251 lr 0.00034196 rank 2
2023-02-17 02:41:51,967 DEBUG TRAIN Batch 1/200 loss 73.624054 loss_att 149.041901 loss_ctc 74.081345 loss_rnnt 58.339428 hw_loss 0.262660 lr 0.00034092 rank 4
2023-02-17 02:41:51,971 DEBUG TRAIN Batch 1/200 loss 94.501740 loss_att 188.802734 loss_ctc 106.849174 loss_rnnt 73.840370 hw_loss 0.290315 lr 0.00034124 rank 6
2023-02-17 02:41:51,994 DEBUG TRAIN Batch 1/200 loss 84.948921 loss_att 193.732574 loss_ctc 92.348495 loss_rnnt 62.029827 hw_loss 0.329535 lr 0.00034116 rank 3
2023-02-17 02:41:51,995 DEBUG TRAIN Batch 1/200 loss 98.069862 loss_att 190.312378 loss_ctc 109.495262 loss_rnnt 78.018196 hw_loss 0.149569 lr 0.00034292 rank 1
2023-02-17 02:41:52,018 DEBUG TRAIN Batch 1/200 loss 110.108101 loss_att 218.120819 loss_ctc 119.012878 loss_rnnt 87.129715 hw_loss 0.353489 lr 0.00034140 rank 5
2023-02-17 02:43:12,364 DEBUG TRAIN Batch 1/300 loss 91.827110 loss_att 186.399017 loss_ctc 93.223198 loss_rnnt 72.597076 hw_loss 0.242828 lr 0.00034492 rank 4
2023-02-17 02:43:12,375 DEBUG TRAIN Batch 1/300 loss 69.374527 loss_att 156.768036 loss_ctc 66.370636 loss_rnnt 52.168053 hw_loss 0.240561 lr 0.00034520 rank 0
2023-02-17 02:43:12,381 DEBUG TRAIN Batch 1/300 loss 74.439606 loss_att 183.643188 loss_ctc 75.602448 loss_rnnt 52.280678 hw_loss 0.305917 lr 0.00034504 rank 7
2023-02-17 02:43:12,411 DEBUG TRAIN Batch 1/300 loss 82.967415 loss_att 166.268799 loss_ctc 86.326309 loss_rnnt 65.687721 hw_loss 0.321669 lr 0.00034524 rank 6
2023-02-17 02:43:12,418 DEBUG TRAIN Batch 1/300 loss 86.415627 loss_att 170.080688 loss_ctc 97.200424 loss_rnnt 68.078720 hw_loss 0.311112 lr 0.00034516 rank 3
2023-02-17 02:43:12,445 DEBUG TRAIN Batch 1/300 loss 75.558189 loss_att 147.733215 loss_ctc 75.067352 loss_rnnt 61.020088 hw_loss 0.316001 lr 0.00034540 rank 5
2023-02-17 02:43:12,448 DEBUG TRAIN Batch 1/300 loss 106.661880 loss_att 221.255646 loss_ctc 102.937119 loss_rnnt 84.068115 hw_loss 0.321853 lr 0.00034692 rank 1
2023-02-17 02:43:12,451 DEBUG TRAIN Batch 1/300 loss 86.511772 loss_att 196.492462 loss_ctc 85.701271 loss_rnnt 64.460274 hw_loss 0.306416 lr 0.00034596 rank 2
2023-02-17 02:44:43,312 DEBUG TRAIN Batch 1/400 loss 76.673088 loss_att 155.623459 loss_ctc 82.520584 loss_rnnt 59.978184 hw_loss 0.234685 lr 0.00034940 rank 5
2023-02-17 02:44:43,313 DEBUG TRAIN Batch 1/400 loss 77.986908 loss_att 165.319336 loss_ctc 81.942108 loss_rnnt 59.780449 hw_loss 0.398652 lr 0.00034924 rank 6
2023-02-17 02:44:43,314 DEBUG TRAIN Batch 1/400 loss 70.384445 loss_att 153.643341 loss_ctc 73.621796 loss_rnnt 53.160355 hw_loss 0.263755 lr 0.00034920 rank 0
2023-02-17 02:44:43,315 DEBUG TRAIN Batch 1/400 loss 86.423225 loss_att 171.573120 loss_ctc 94.870316 loss_rnnt 68.142708 hw_loss 0.232983 lr 0.00035092 rank 1
2023-02-17 02:44:43,319 DEBUG TRAIN Batch 1/400 loss 86.509514 loss_att 180.942932 loss_ctc 93.016647 loss_rnnt 66.599716 hw_loss 0.291543 lr 0.00034892 rank 4
2023-02-17 02:44:43,336 DEBUG TRAIN Batch 1/400 loss 84.665771 loss_att 169.987152 loss_ctc 93.131012 loss_rnnt 66.295319 hw_loss 0.332756 lr 0.00034996 rank 2
2023-02-17 02:44:43,343 DEBUG TRAIN Batch 1/400 loss 60.947235 loss_att 143.381729 loss_ctc 63.378494 loss_rnnt 43.985718 hw_loss 0.282094 lr 0.00034904 rank 7
2023-02-17 02:44:43,355 DEBUG TRAIN Batch 1/400 loss 72.063660 loss_att 158.969513 loss_ctc 71.845154 loss_rnnt 54.549210 hw_loss 0.304523 lr 0.00034916 rank 3
2023-02-17 02:45:59,331 DEBUG TRAIN Batch 1/500 loss 76.872681 loss_att 145.013840 loss_ctc 77.879402 loss_rnnt 62.961765 hw_loss 0.278342 lr 0.00035292 rank 4
2023-02-17 02:45:59,342 DEBUG TRAIN Batch 1/500 loss 58.549812 loss_att 138.528183 loss_ctc 56.743378 loss_rnnt 42.635529 hw_loss 0.298997 lr 0.00035304 rank 7
2023-02-17 02:45:59,343 DEBUG TRAIN Batch 1/500 loss 58.781895 loss_att 139.363358 loss_ctc 62.291168 loss_rnnt 42.072929 hw_loss 0.233950 lr 0.00035320 rank 0
2023-02-17 02:45:59,343 DEBUG TRAIN Batch 1/500 loss 79.989212 loss_att 169.551987 loss_ctc 86.682358 loss_rnnt 60.980476 hw_loss 0.382043 lr 0.00035340 rank 5
2023-02-17 02:45:59,343 DEBUG TRAIN Batch 1/500 loss 69.861717 loss_att 150.801224 loss_ctc 67.074356 loss_rnnt 53.924767 hw_loss 0.226302 lr 0.00035324 rank 6
2023-02-17 02:45:59,344 DEBUG TRAIN Batch 1/500 loss 97.756447 loss_att 176.390121 loss_ctc 103.045509 loss_rnnt 81.188171 hw_loss 0.255626 lr 0.00035396 rank 2
2023-02-17 02:45:59,345 DEBUG TRAIN Batch 1/500 loss 60.317280 loss_att 128.091309 loss_ctc 60.869247 loss_rnnt 46.516357 hw_loss 0.323480 lr 0.00035316 rank 3
2023-02-17 02:45:59,349 DEBUG TRAIN Batch 1/500 loss 82.436287 loss_att 175.348373 loss_ctc 84.948166 loss_rnnt 63.403732 hw_loss 0.216040 lr 0.00035492 rank 1
2023-02-17 02:47:15,003 DEBUG TRAIN Batch 1/600 loss 61.689384 loss_att 116.926865 loss_ctc 68.239273 loss_rnnt 49.574669 hw_loss 0.363564 lr 0.00035720 rank 0
2023-02-17 02:47:15,004 DEBUG TRAIN Batch 1/600 loss 82.064293 loss_att 129.449478 loss_ctc 90.656982 loss_rnnt 71.265228 hw_loss 0.330644 lr 0.00035740 rank 5
2023-02-17 02:47:15,004 DEBUG TRAIN Batch 1/600 loss 37.885891 loss_att 58.460289 loss_ctc 43.204365 loss_rnnt 32.845528 hw_loss 0.405669 lr 0.00035724 rank 6
2023-02-17 02:47:15,007 DEBUG TRAIN Batch 1/600 loss 75.007683 loss_att 110.855972 loss_ctc 83.030144 loss_rnnt 66.585846 hw_loss 0.342225 lr 0.00035704 rank 7
2023-02-17 02:47:15,016 DEBUG TRAIN Batch 1/600 loss 48.643242 loss_att 99.644424 loss_ctc 54.847473 loss_rnnt 37.399052 hw_loss 0.406356 lr 0.00035796 rank 2
2023-02-17 02:47:15,049 DEBUG TRAIN Batch 1/600 loss 48.838379 loss_att 76.577293 loss_ctc 55.088509 loss_rnnt 42.282825 hw_loss 0.327037 lr 0.00035692 rank 4
2023-02-17 02:47:15,052 DEBUG TRAIN Batch 1/600 loss 42.346165 loss_att 63.931236 loss_ctc 48.394711 loss_rnnt 37.041199 hw_loss 0.340285 lr 0.00035716 rank 3
2023-02-17 02:47:15,055 DEBUG TRAIN Batch 1/600 loss 54.271629 loss_att 104.686752 loss_ctc 58.181389 loss_rnnt 43.474129 hw_loss 0.362197 lr 0.00035892 rank 1
2023-02-17 02:48:41,124 DEBUG TRAIN Batch 1/700 loss 71.694717 loss_att 169.016388 loss_ctc 79.235649 loss_rnnt 51.103085 hw_loss 0.228454 lr 0.00036092 rank 4
2023-02-17 02:48:41,127 DEBUG TRAIN Batch 1/700 loss 91.384605 loss_att 212.857910 loss_ctc 90.438461 loss_rnnt 67.094475 hw_loss 0.228047 lr 0.00036116 rank 3
2023-02-17 02:48:41,143 DEBUG TRAIN Batch 1/700 loss 91.425598 loss_att 179.794540 loss_ctc 95.422874 loss_rnnt 73.050613 hw_loss 0.315410 lr 0.00036196 rank 2
2023-02-17 02:48:41,145 DEBUG TRAIN Batch 1/700 loss 69.358376 loss_att 143.000610 loss_ctc 75.410080 loss_rnnt 53.712749 hw_loss 0.206781 lr 0.00036104 rank 7
2023-02-17 02:48:41,149 DEBUG TRAIN Batch 1/700 loss 75.030037 loss_att 174.971100 loss_ctc 82.386520 loss_rnnt 53.897675 hw_loss 0.306155 lr 0.00036120 rank 0
2023-02-17 02:48:41,157 DEBUG TRAIN Batch 1/700 loss 76.844078 loss_att 163.224640 loss_ctc 80.314636 loss_rnnt 59.002178 hw_loss 0.193207 lr 0.00036292 rank 1
2023-02-17 02:48:41,167 DEBUG TRAIN Batch 1/700 loss 85.112938 loss_att 176.232849 loss_ctc 91.989944 loss_rnnt 65.806396 hw_loss 0.310532 lr 0.00036124 rank 6
2023-02-17 02:48:41,183 DEBUG TRAIN Batch 1/700 loss 56.091133 loss_att 163.246368 loss_ctc 55.118500 loss_rnnt 34.652245 hw_loss 0.257858 lr 0.00036140 rank 5
2023-02-17 02:50:07,573 DEBUG TRAIN Batch 1/800 loss 94.289589 loss_att 190.630447 loss_ctc 92.541199 loss_rnnt 75.118309 hw_loss 0.255431 lr 0.00036504 rank 7
2023-02-17 02:50:07,574 DEBUG TRAIN Batch 1/800 loss 96.224243 loss_att 183.906479 loss_ctc 110.449265 loss_rnnt 76.677719 hw_loss 0.212628 lr 0.00036516 rank 3
2023-02-17 02:50:07,578 DEBUG TRAIN Batch 1/800 loss 101.038811 loss_att 208.690216 loss_ctc 112.946259 loss_rnnt 77.796989 hw_loss 0.232283 lr 0.00036692 rank 1
2023-02-17 02:50:07,579 DEBUG TRAIN Batch 1/800 loss 88.028008 loss_att 196.490555 loss_ctc 101.405449 loss_rnnt 64.452652 hw_loss 0.185973 lr 0.00036520 rank 0
2023-02-17 02:50:07,579 DEBUG TRAIN Batch 1/800 loss 91.549263 loss_att 212.091400 loss_ctc 94.225403 loss_rnnt 66.998184 hw_loss 0.160923 lr 0.00036524 rank 6
2023-02-17 02:50:07,579 DEBUG TRAIN Batch 1/800 loss 67.720558 loss_att 154.908112 loss_ctc 72.001648 loss_rnnt 49.592270 hw_loss 0.224928 lr 0.00036596 rank 2
2023-02-17 02:50:07,581 DEBUG TRAIN Batch 1/800 loss 73.169411 loss_att 175.599396 loss_ctc 78.563881 loss_rnnt 51.872009 hw_loss 0.172777 lr 0.00036492 rank 4
2023-02-17 02:50:07,642 DEBUG TRAIN Batch 1/800 loss 70.652840 loss_att 174.313339 loss_ctc 72.836075 loss_rnnt 49.470284 hw_loss 0.298787 lr 0.00036540 rank 5
2023-02-17 02:51:23,632 DEBUG TRAIN Batch 1/900 loss 91.058479 loss_att 156.128433 loss_ctc 90.941467 loss_rnnt 77.872360 hw_loss 0.352001 lr 0.00036916 rank 3
2023-02-17 02:51:23,632 DEBUG TRAIN Batch 1/900 loss 95.115021 loss_att 201.589966 loss_ctc 106.567871 loss_rnnt 72.120842 hw_loss 0.322771 lr 0.00036920 rank 0
2023-02-17 02:51:23,634 DEBUG TRAIN Batch 1/900 loss 130.279053 loss_att 237.132111 loss_ctc 144.215759 loss_rnnt 106.907112 hw_loss 0.268329 lr 0.00036904 rank 7
2023-02-17 02:51:23,636 DEBUG TRAIN Batch 1/900 loss 72.319359 loss_att 164.617661 loss_ctc 83.990456 loss_rnnt 52.192589 hw_loss 0.208048 lr 0.00037092 rank 1
2023-02-17 02:51:23,637 DEBUG TRAIN Batch 1/900 loss 68.280342 loss_att 154.105743 loss_ctc 75.735214 loss_rnnt 49.969688 hw_loss 0.284225 lr 0.00036996 rank 2
2023-02-17 02:51:23,639 DEBUG TRAIN Batch 1/900 loss 74.236679 loss_att 183.696777 loss_ctc 77.828110 loss_rnnt 51.704155 hw_loss 0.303102 lr 0.00036892 rank 4
2023-02-17 02:51:23,652 DEBUG TRAIN Batch 1/900 loss 96.951546 loss_att 180.314209 loss_ctc 109.482841 loss_rnnt 78.473938 hw_loss 0.251682 lr 0.00036924 rank 6
2023-02-17 02:51:23,670 DEBUG TRAIN Batch 1/900 loss 74.060318 loss_att 162.698532 loss_ctc 82.675781 loss_rnnt 54.943001 hw_loss 0.451778 lr 0.00036940 rank 5
2023-02-17 02:52:42,665 DEBUG TRAIN Batch 1/1000 loss 77.514671 loss_att 176.426468 loss_ctc 78.599152 loss_rnnt 57.393879 hw_loss 0.363431 lr 0.00037492 rank 1
2023-02-17 02:52:42,687 DEBUG TRAIN Batch 1/1000 loss 92.925812 loss_att 173.712006 loss_ctc 104.805786 loss_rnnt 75.025978 hw_loss 0.297381 lr 0.00037396 rank 2
2023-02-17 02:52:42,713 DEBUG TRAIN Batch 1/1000 loss 88.986153 loss_att 176.505264 loss_ctc 95.059471 loss_rnnt 70.529556 hw_loss 0.268119 lr 0.00037304 rank 7
2023-02-17 02:52:42,716 DEBUG TRAIN Batch 1/1000 loss 89.624397 loss_att 183.863602 loss_ctc 100.500137 loss_rnnt 69.164719 hw_loss 0.303255 lr 0.00037324 rank 6
2023-02-17 02:52:42,720 DEBUG TRAIN Batch 1/1000 loss 59.953724 loss_att 125.869179 loss_ctc 64.153275 loss_rnnt 46.072895 hw_loss 0.258360 lr 0.00037316 rank 3
2023-02-17 02:52:42,727 DEBUG TRAIN Batch 1/1000 loss 110.066002 loss_att 194.406860 loss_ctc 117.177940 loss_rnnt 92.054482 hw_loss 0.365771 lr 0.00037340 rank 5
2023-02-17 02:52:42,758 DEBUG TRAIN Batch 1/1000 loss 82.383263 loss_att 154.174622 loss_ctc 92.736351 loss_rnnt 66.432762 hw_loss 0.397176 lr 0.00037292 rank 4
2023-02-17 02:52:42,760 DEBUG TRAIN Batch 1/1000 loss 69.492401 loss_att 164.488556 loss_ctc 73.920326 loss_rnnt 49.765503 hw_loss 0.257395 lr 0.00037320 rank 0
2023-02-17 02:54:15,212 DEBUG TRAIN Batch 1/1100 loss 70.411385 loss_att 170.481613 loss_ctc 79.978653 loss_rnnt 48.951561 hw_loss 0.319019 lr 0.00037716 rank 3
2023-02-17 02:54:15,244 DEBUG TRAIN Batch 1/1100 loss 64.730919 loss_att 137.664734 loss_ctc 72.845673 loss_rnnt 48.899059 hw_loss 0.305862 lr 0.00037704 rank 7
2023-02-17 02:54:15,257 DEBUG TRAIN Batch 1/1100 loss 89.482483 loss_att 161.623291 loss_ctc 97.268188 loss_rnnt 73.895294 hw_loss 0.226739 lr 0.00037720 rank 0
2023-02-17 02:54:15,261 DEBUG TRAIN Batch 1/1100 loss 100.851974 loss_att 184.487152 loss_ctc 117.369049 loss_rnnt 81.778526 hw_loss 0.270255 lr 0.00037692 rank 4
2023-02-17 02:54:15,264 DEBUG TRAIN Batch 1/1100 loss 56.210369 loss_att 132.063782 loss_ctc 54.951622 loss_rnnt 41.021843 hw_loss 0.348135 lr 0.00037740 rank 5
2023-02-17 02:54:15,278 DEBUG TRAIN Batch 1/1100 loss 90.277611 loss_att 181.302475 loss_ctc 98.560295 loss_rnnt 70.794754 hw_loss 0.325357 lr 0.00037892 rank 1
2023-02-17 02:54:15,279 DEBUG TRAIN Batch 1/1100 loss 83.247681 loss_att 155.949203 loss_ctc 86.699181 loss_rnnt 68.104309 hw_loss 0.267875 lr 0.00037724 rank 6
2023-02-17 02:54:15,284 DEBUG TRAIN Batch 1/1100 loss 74.503487 loss_att 134.668839 loss_ctc 89.390739 loss_rnnt 60.260723 hw_loss 0.421350 lr 0.00037796 rank 2
2023-02-17 02:55:32,207 DEBUG TRAIN Batch 1/1200 loss 51.386356 loss_att 85.513779 loss_ctc 58.856628 loss_rnnt 43.337299 hw_loss 0.426623 lr 0.00038116 rank 3
2023-02-17 02:55:32,208 DEBUG TRAIN Batch 1/1200 loss 66.543098 loss_att 136.152054 loss_ctc 81.073006 loss_rnnt 50.474545 hw_loss 0.392698 lr 0.00038140 rank 5
2023-02-17 02:55:32,209 DEBUG TRAIN Batch 1/1200 loss 47.455486 loss_att 70.839828 loss_ctc 52.631218 loss_rnnt 41.884914 hw_loss 0.381765 lr 0.00038124 rank 6
2023-02-17 02:55:32,211 DEBUG TRAIN Batch 1/1200 loss 71.840927 loss_att 135.102737 loss_ctc 79.563499 loss_rnnt 57.997284 hw_loss 0.303003 lr 0.00038104 rank 7
2023-02-17 02:55:32,212 DEBUG TRAIN Batch 1/1200 loss 85.299225 loss_att 173.089203 loss_ctc 95.373535 loss_rnnt 66.176712 hw_loss 0.414893 lr 0.00038292 rank 1
2023-02-17 02:55:32,212 DEBUG TRAIN Batch 1/1200 loss 46.048218 loss_att 82.040436 loss_ctc 48.298977 loss_rnnt 38.389023 hw_loss 0.301215 lr 0.00038196 rank 2
2023-02-17 02:55:32,230 DEBUG TRAIN Batch 1/1200 loss 47.400604 loss_att 94.551559 loss_ctc 51.887928 loss_rnnt 37.209297 hw_loss 0.305263 lr 0.00038092 rank 4
2023-02-17 02:55:32,235 DEBUG TRAIN Batch 1/1200 loss 66.002480 loss_att 125.473297 loss_ctc 74.249321 loss_rnnt 52.848598 hw_loss 0.300268 lr 0.00038120 rank 0
2023-02-17 02:56:47,862 DEBUG TRAIN Batch 1/1300 loss 83.355736 loss_att 181.895142 loss_ctc 90.373375 loss_rnnt 62.530880 hw_loss 0.339913 lr 0.00038524 rank 6
2023-02-17 02:56:47,863 DEBUG TRAIN Batch 1/1300 loss 107.502007 loss_att 184.905426 loss_ctc 122.497314 loss_rnnt 89.889160 hw_loss 0.248959 lr 0.00038520 rank 0
2023-02-17 02:56:47,864 DEBUG TRAIN Batch 1/1300 loss 98.696976 loss_att 172.310059 loss_ctc 112.334801 loss_rnnt 82.001640 hw_loss 0.289390 lr 0.00038596 rank 2
2023-02-17 02:56:47,867 DEBUG TRAIN Batch 1/1300 loss 81.034462 loss_att 178.522568 loss_ctc 83.592041 loss_rnnt 61.019863 hw_loss 0.329940 lr 0.00038504 rank 7
2023-02-17 02:56:47,868 DEBUG TRAIN Batch 1/1300 loss 40.503212 loss_att 63.036346 loss_ctc 48.549248 loss_rnnt 34.712349 hw_loss 0.396437 lr 0.00038692 rank 1
2023-02-17 02:56:47,892 DEBUG TRAIN Batch 1/1300 loss 70.472603 loss_att 166.295685 loss_ctc 67.420334 loss_rnnt 51.564240 hw_loss 0.282601 lr 0.00038492 rank 4
2023-02-17 02:56:47,895 DEBUG TRAIN Batch 1/1300 loss 32.426937 loss_att 45.129940 loss_ctc 35.081627 loss_rnnt 29.333136 hw_loss 0.373579 lr 0.00038540 rank 5
2023-02-17 02:56:47,969 DEBUG TRAIN Batch 1/1300 loss 66.430077 loss_att 146.090210 loss_ctc 80.112602 loss_rnnt 48.540283 hw_loss 0.250182 lr 0.00038516 rank 3
2023-02-17 02:58:11,823 DEBUG TRAIN Batch 1/1400 loss 86.142128 loss_att 185.916672 loss_ctc 91.407799 loss_rnnt 65.336349 hw_loss 0.278948 lr 0.00038940 rank 5
2023-02-17 02:58:11,823 DEBUG TRAIN Batch 1/1400 loss 76.191025 loss_att 162.299896 loss_ctc 79.061424 loss_rnnt 58.392284 hw_loss 0.364216 lr 0.00039092 rank 1
2023-02-17 02:58:11,831 DEBUG TRAIN Batch 1/1400 loss 67.165138 loss_att 155.397812 loss_ctc 83.856277 loss_rnnt 47.178139 hw_loss 0.215580 lr 0.00038916 rank 3
2023-02-17 02:58:11,833 DEBUG TRAIN Batch 1/1400 loss 90.807030 loss_att 183.400284 loss_ctc 89.078064 loss_rnnt 72.408333 hw_loss 0.207307 lr 0.00038924 rank 6
2023-02-17 02:58:11,858 DEBUG TRAIN Batch 1/1400 loss 82.394981 loss_att 163.693634 loss_ctc 87.773125 loss_rnnt 65.273056 hw_loss 0.272090 lr 0.00038996 rank 2
2023-02-17 02:58:11,864 DEBUG TRAIN Batch 1/1400 loss 92.666534 loss_att 189.861816 loss_ctc 105.940208 loss_rnnt 71.322662 hw_loss 0.253122 lr 0.00038904 rank 7
2023-02-17 02:58:11,876 DEBUG TRAIN Batch 1/1400 loss 90.349892 loss_att 174.947083 loss_ctc 106.449844 loss_rnnt 71.118378 hw_loss 0.310147 lr 0.00038892 rank 4
2023-02-17 02:58:11,933 DEBUG TRAIN Batch 1/1400 loss 73.131973 loss_att 158.126129 loss_ctc 73.642975 loss_rnnt 55.903450 hw_loss 0.302931 lr 0.00038920 rank 0
2023-02-17 02:59:38,466 DEBUG TRAIN Batch 1/1500 loss 62.147797 loss_att 158.320465 loss_ctc 67.680206 loss_rnnt 42.000095 hw_loss 0.329083 lr 0.00039304 rank 7
2023-02-17 02:59:38,467 DEBUG TRAIN Batch 1/1500 loss 79.534821 loss_att 175.942444 loss_ctc 85.758011 loss_rnnt 59.304008 hw_loss 0.224113 lr 0.00039396 rank 2
2023-02-17 02:59:38,477 DEBUG TRAIN Batch 1/1500 loss 53.951820 loss_att 147.536560 loss_ctc 52.283195 loss_rnnt 35.295349 hw_loss 0.303765 lr 0.00039340 rank 5
2023-02-17 02:59:38,477 DEBUG TRAIN Batch 1/1500 loss 84.252754 loss_att 167.736069 loss_ctc 91.706688 loss_rnnt 66.377617 hw_loss 0.346167 lr 0.00039292 rank 4
2023-02-17 02:59:38,483 DEBUG TRAIN Batch 1/1500 loss 80.087082 loss_att 160.899445 loss_ctc 90.782974 loss_rnnt 62.342804 hw_loss 0.291903 lr 0.00039320 rank 0
2023-02-17 02:59:38,487 DEBUG TRAIN Batch 1/1500 loss 65.400452 loss_att 150.006042 loss_ctc 73.670563 loss_rnnt 47.201309 hw_loss 0.328754 lr 0.00039316 rank 3
2023-02-17 02:59:38,506 DEBUG TRAIN Batch 1/1500 loss 70.894638 loss_att 163.153610 loss_ctc 84.838684 loss_rnnt 50.423889 hw_loss 0.299525 lr 0.00039492 rank 1
2023-02-17 02:59:38,537 DEBUG TRAIN Batch 1/1500 loss 66.433128 loss_att 130.571320 loss_ctc 75.504890 loss_rnnt 52.201744 hw_loss 0.364103 lr 0.00039324 rank 6
2023-02-17 03:00:53,819 DEBUG TRAIN Batch 1/1600 loss 46.616608 loss_att 117.885727 loss_ctc 46.256042 loss_rnnt 32.273216 hw_loss 0.258083 lr 0.00039720 rank 0
2023-02-17 03:00:53,822 DEBUG TRAIN Batch 1/1600 loss 61.020191 loss_att 136.718979 loss_ctc 69.706001 loss_rnnt 44.612442 hw_loss 0.206030 lr 0.00039892 rank 1
2023-02-17 03:00:53,822 DEBUG TRAIN Batch 1/1600 loss 41.922260 loss_att 101.900406 loss_ctc 44.360302 loss_rnnt 29.401049 hw_loss 0.375953 lr 0.00039692 rank 4
2023-02-17 03:00:53,824 DEBUG TRAIN Batch 1/1600 loss 57.721321 loss_att 132.754791 loss_ctc 65.360703 loss_rnnt 41.533585 hw_loss 0.304600 lr 0.00039704 rank 7
2023-02-17 03:00:53,824 DEBUG TRAIN Batch 1/1600 loss 78.976768 loss_att 152.329895 loss_ctc 87.789772 loss_rnnt 62.883430 hw_loss 0.464321 lr 0.00039796 rank 2
2023-02-17 03:00:53,825 DEBUG TRAIN Batch 1/1600 loss 60.643005 loss_att 119.303658 loss_ctc 65.857567 loss_rnnt 48.031509 hw_loss 0.345166 lr 0.00039724 rank 6
2023-02-17 03:00:53,893 DEBUG TRAIN Batch 1/1600 loss 102.670990 loss_att 198.076599 loss_ctc 113.226440 loss_rnnt 82.016907 hw_loss 0.310449 lr 0.00039716 rank 3
2023-02-17 03:00:53,898 DEBUG TRAIN Batch 1/1600 loss 67.338821 loss_att 140.738968 loss_ctc 80.090073 loss_rnnt 50.815281 hw_loss 0.268769 lr 0.00039740 rank 5
2023-02-17 03:02:12,024 DEBUG TRAIN Batch 1/1700 loss 44.601398 loss_att 105.421761 loss_ctc 49.063980 loss_rnnt 31.696581 hw_loss 0.273246 lr 0.00040116 rank 3
2023-02-17 03:02:12,025 DEBUG TRAIN Batch 1/1700 loss 75.873146 loss_att 125.788818 loss_ctc 85.112419 loss_rnnt 64.516403 hw_loss 0.265696 lr 0.00040104 rank 7
2023-02-17 03:02:12,038 DEBUG TRAIN Batch 1/1700 loss 44.470257 loss_att 94.206436 loss_ctc 48.420837 loss_rnnt 33.840752 hw_loss 0.291604 lr 0.00040092 rank 4
2023-02-17 03:02:12,043 DEBUG TRAIN Batch 1/1700 loss 49.715321 loss_att 96.521683 loss_ctc 50.288006 loss_rnnt 40.069019 hw_loss 0.391251 lr 0.00040196 rank 2
2023-02-17 03:02:12,058 DEBUG TRAIN Batch 1/1700 loss 90.082237 loss_att 158.367172 loss_ctc 97.195976 loss_rnnt 75.286537 hw_loss 0.356652 lr 0.00040292 rank 1
2023-02-17 03:02:12,073 DEBUG TRAIN Batch 1/1700 loss 75.185020 loss_att 146.003311 loss_ctc 87.045700 loss_rnnt 59.288574 hw_loss 0.283790 lr 0.00040124 rank 6
2023-02-17 03:02:12,082 DEBUG TRAIN Batch 1/1700 loss 50.063599 loss_att 111.166870 loss_ctc 57.870060 loss_rnnt 36.676414 hw_loss 0.235638 lr 0.00040140 rank 5
2023-02-17 03:02:12,089 DEBUG TRAIN Batch 1/1700 loss 56.416222 loss_att 113.767471 loss_ctc 64.677063 loss_rnnt 43.688820 hw_loss 0.291944 lr 0.00040120 rank 0
2023-02-17 03:03:46,081 DEBUG TRAIN Batch 1/1800 loss 55.386166 loss_att 112.509804 loss_ctc 64.124390 loss_rnnt 42.620937 hw_loss 0.328893 lr 0.00040524 rank 6
2023-02-17 03:03:46,086 DEBUG TRAIN Batch 1/1800 loss 63.241360 loss_att 126.244095 loss_ctc 70.893105 loss_rnnt 49.496758 hw_loss 0.232174 lr 0.00040504 rank 7
2023-02-17 03:03:46,085 DEBUG TRAIN Batch 1/1800 loss 83.297920 loss_att 143.316467 loss_ctc 92.365692 loss_rnnt 69.982323 hw_loss 0.192856 lr 0.00040540 rank 5
2023-02-17 03:03:46,085 DEBUG TRAIN Batch 1/1800 loss 74.779114 loss_att 138.131348 loss_ctc 84.082512 loss_rnnt 60.718472 hw_loss 0.280768 lr 0.00040492 rank 4
2023-02-17 03:03:46,090 DEBUG TRAIN Batch 1/1800 loss 93.614464 loss_att 153.133102 loss_ctc 106.252831 loss_rnnt 79.827003 hw_loss 0.372416 lr 0.00040516 rank 3
2023-02-17 03:03:46,109 DEBUG TRAIN Batch 1/1800 loss 64.389481 loss_att 112.189972 loss_ctc 73.002457 loss_rnnt 53.511616 hw_loss 0.317554 lr 0.00040692 rank 1
2023-02-17 03:03:46,108 DEBUG TRAIN Batch 1/1800 loss 52.549904 loss_att 77.342674 loss_ctc 55.574001 loss_rnnt 46.957104 hw_loss 0.433189 lr 0.00040596 rank 2
2023-02-17 03:03:46,122 DEBUG TRAIN Batch 1/1800 loss 56.112900 loss_att 108.970757 loss_ctc 63.991177 loss_rnnt 44.324436 hw_loss 0.312097 lr 0.00040520 rank 0
2023-02-17 03:05:01,673 DEBUG TRAIN Batch 1/1900 loss 92.308449 loss_att 175.789536 loss_ctc 116.362717 loss_rnnt 72.271637 hw_loss 0.250046 lr 0.00040924 rank 6
2023-02-17 03:05:01,674 DEBUG TRAIN Batch 1/1900 loss 40.499199 loss_att 76.221970 loss_ctc 43.313068 loss_rnnt 32.767864 hw_loss 0.396752 lr 0.00040892 rank 4
2023-02-17 03:05:01,674 DEBUG TRAIN Batch 1/1900 loss 81.027763 loss_att 173.511948 loss_ctc 91.376602 loss_rnnt 60.989697 hw_loss 0.302594 lr 0.00040996 rank 2
2023-02-17 03:05:01,675 DEBUG TRAIN Batch 1/1900 loss 80.837418 loss_att 164.322144 loss_ctc 94.204193 loss_rnnt 62.164032 hw_loss 0.364124 lr 0.00040916 rank 3
2023-02-17 03:05:01,677 DEBUG TRAIN Batch 1/1900 loss 68.144737 loss_att 145.082840 loss_ctc 76.868027 loss_rnnt 51.477245 hw_loss 0.218940 lr 0.00040920 rank 0
2023-02-17 03:05:01,679 DEBUG TRAIN Batch 1/1900 loss 42.246281 loss_att 60.102623 loss_ctc 50.609673 loss_rnnt 37.413502 hw_loss 0.274478 lr 0.00040940 rank 5
2023-02-17 03:05:01,708 DEBUG TRAIN Batch 1/1900 loss 39.942459 loss_att 89.879990 loss_ctc 45.819729 loss_rnnt 28.968100 hw_loss 0.381032 lr 0.00040904 rank 7
2023-02-17 03:05:01,711 DEBUG TRAIN Batch 1/1900 loss 41.511734 loss_att 78.083008 loss_ctc 43.312450 loss_rnnt 33.792274 hw_loss 0.309584 lr 0.00041092 rank 1
2023-02-17 03:06:17,916 DEBUG TRAIN Batch 1/2000 loss 79.342781 loss_att 144.518372 loss_ctc 85.735870 loss_rnnt 65.254745 hw_loss 0.375944 lr 0.00041316 rank 3
2023-02-17 03:06:17,916 DEBUG TRAIN Batch 1/2000 loss 77.493073 loss_att 158.580139 loss_ctc 87.765411 loss_rnnt 59.779121 hw_loss 0.237925 lr 0.00041292 rank 4
2023-02-17 03:06:17,917 DEBUG TRAIN Batch 1/2000 loss 74.516289 loss_att 151.781921 loss_ctc 81.217560 loss_rnnt 58.015617 hw_loss 0.288824 lr 0.00041304 rank 7
2023-02-17 03:06:17,920 DEBUG TRAIN Batch 1/2000 loss 73.464928 loss_att 159.510010 loss_ctc 75.619415 loss_rnnt 55.821167 hw_loss 0.276512 lr 0.00041492 rank 1
2023-02-17 03:06:17,918 DEBUG TRAIN Batch 1/2000 loss 76.216637 loss_att 156.025696 loss_ctc 84.164955 loss_rnnt 59.044991 hw_loss 0.281345 lr 0.00041396 rank 2
2023-02-17 03:06:17,949 DEBUG TRAIN Batch 1/2000 loss 104.189026 loss_att 196.228455 loss_ctc 120.637756 loss_rnnt 83.483749 hw_loss 0.195432 lr 0.00041320 rank 0
2023-02-17 03:06:17,957 DEBUG TRAIN Batch 1/2000 loss 60.053314 loss_att 120.457352 loss_ctc 58.271347 loss_rnnt 48.075600 hw_loss 0.252192 lr 0.00041324 rank 6
2023-02-17 03:06:17,959 DEBUG TRAIN Batch 1/2000 loss 74.510101 loss_att 148.495163 loss_ctc 87.284698 loss_rnnt 57.859997 hw_loss 0.280902 lr 0.00041340 rank 5
2023-02-17 03:07:42,183 DEBUG TRAIN Batch 1/2100 loss 61.680618 loss_att 133.751450 loss_ctc 69.371948 loss_rnnt 46.095184 hw_loss 0.273297 lr 0.00041704 rank 7
2023-02-17 03:07:42,193 DEBUG TRAIN Batch 1/2100 loss 79.752548 loss_att 156.198624 loss_ctc 87.686996 loss_rnnt 63.215210 hw_loss 0.356617 lr 0.00041892 rank 1
2023-02-17 03:07:42,207 DEBUG TRAIN Batch 1/2100 loss 75.054985 loss_att 138.157928 loss_ctc 87.974350 loss_rnnt 60.497742 hw_loss 0.401380 lr 0.00041724 rank 6
2023-02-17 03:07:42,208 DEBUG TRAIN Batch 1/2100 loss 46.904194 loss_att 104.628540 loss_ctc 50.327709 loss_rnnt 34.739552 hw_loss 0.306187 lr 0.00041796 rank 2
2023-02-17 03:07:42,229 DEBUG TRAIN Batch 1/2100 loss 81.686134 loss_att 161.000122 loss_ctc 99.516426 loss_rnnt 63.241379 hw_loss 0.383615 lr 0.00041716 rank 3
2023-02-17 03:07:42,239 DEBUG TRAIN Batch 1/2100 loss 66.399521 loss_att 157.306671 loss_ctc 72.515854 loss_rnnt 47.280968 hw_loss 0.228023 lr 0.00041692 rank 4
2023-02-17 03:07:42,238 DEBUG TRAIN Batch 1/2100 loss 90.388123 loss_att 159.070068 loss_ctc 94.390892 loss_rnnt 75.983055 hw_loss 0.253091 lr 0.00041740 rank 5
2023-02-17 03:07:42,245 DEBUG TRAIN Batch 1/2100 loss 59.429523 loss_att 127.808517 loss_ctc 67.950645 loss_rnnt 44.468548 hw_loss 0.279433 lr 0.00041720 rank 0
2023-02-17 03:09:12,119 DEBUG TRAIN Batch 1/2200 loss 59.075958 loss_att 117.413788 loss_ctc 64.287277 loss_rnnt 46.539566 hw_loss 0.326219 lr 0.00042092 rank 4
2023-02-17 03:09:12,128 DEBUG TRAIN Batch 1/2200 loss 69.308334 loss_att 130.351898 loss_ctc 79.375351 loss_rnnt 55.568733 hw_loss 0.353668 lr 0.00042124 rank 6
2023-02-17 03:09:12,128 DEBUG TRAIN Batch 1/2200 loss 74.640198 loss_att 142.839783 loss_ctc 87.402924 loss_rnnt 59.100655 hw_loss 0.371115 lr 0.00042116 rank 3
2023-02-17 03:09:12,129 DEBUG TRAIN Batch 1/2200 loss 43.855247 loss_att 117.660614 loss_ctc 44.365395 loss_rnnt 28.868410 hw_loss 0.295761 lr 0.00042196 rank 2
2023-02-17 03:09:12,131 DEBUG TRAIN Batch 1/2200 loss 75.813202 loss_att 162.930862 loss_ctc 81.886429 loss_rnnt 57.465042 hw_loss 0.215367 lr 0.00042120 rank 0
2023-02-17 03:09:12,133 DEBUG TRAIN Batch 1/2200 loss 75.428986 loss_att 140.641342 loss_ctc 85.351944 loss_rnnt 60.921173 hw_loss 0.266770 lr 0.00042104 rank 7
2023-02-17 03:09:12,138 DEBUG TRAIN Batch 1/2200 loss 57.355873 loss_att 122.658096 loss_ctc 68.808311 loss_rnnt 42.613274 hw_loss 0.290931 lr 0.00042292 rank 1
2023-02-17 03:09:12,157 DEBUG TRAIN Batch 1/2200 loss 67.445007 loss_att 137.913727 loss_ctc 77.898460 loss_rnnt 51.837536 hw_loss 0.224872 lr 0.00042140 rank 5
2023-02-17 03:10:27,669 DEBUG TRAIN Batch 1/2300 loss 58.590832 loss_att 113.090958 loss_ctc 65.180122 loss_rnnt 46.649002 hw_loss 0.306066 lr 0.00042596 rank 2
2023-02-17 03:10:27,683 DEBUG TRAIN Batch 1/2300 loss 62.041759 loss_att 128.853027 loss_ctc 63.427849 loss_rnnt 48.344193 hw_loss 0.282183 lr 0.00042540 rank 5
2023-02-17 03:10:27,687 DEBUG TRAIN Batch 1/2300 loss 71.849632 loss_att 142.085938 loss_ctc 81.079117 loss_rnnt 56.462250 hw_loss 0.205356 lr 0.00042692 rank 1
2023-02-17 03:10:27,686 DEBUG TRAIN Batch 1/2300 loss 79.901543 loss_att 138.911621 loss_ctc 96.055664 loss_rnnt 65.792145 hw_loss 0.287807 lr 0.00042492 rank 4
2023-02-17 03:10:27,691 DEBUG TRAIN Batch 1/2300 loss 47.810417 loss_att 98.245819 loss_ctc 45.820549 loss_rnnt 37.831757 hw_loss 0.294183 lr 0.00042516 rank 3
2023-02-17 03:10:27,694 DEBUG TRAIN Batch 1/2300 loss 63.080235 loss_att 138.212830 loss_ctc 72.957947 loss_rnnt 46.566368 hw_loss 0.319348 lr 0.00042520 rank 0
2023-02-17 03:10:27,736 DEBUG TRAIN Batch 1/2300 loss 61.624828 loss_att 138.290497 loss_ctc 63.620632 loss_rnnt 45.885395 hw_loss 0.262855 lr 0.00042524 rank 6
2023-02-17 03:10:27,739 DEBUG TRAIN Batch 1/2300 loss 69.250221 loss_att 130.290924 loss_ctc 79.818550 loss_rnnt 55.495651 hw_loss 0.257478 lr 0.00042504 rank 7
2023-02-17 03:11:45,553 DEBUG TRAIN Batch 1/2400 loss 51.198475 loss_att 94.424820 loss_ctc 58.162270 loss_rnnt 41.426777 hw_loss 0.371107 lr 0.00042924 rank 6
2023-02-17 03:11:45,555 DEBUG TRAIN Batch 1/2400 loss 46.359936 loss_att 76.953819 loss_ctc 52.579266 loss_rnnt 39.207123 hw_loss 0.383980 lr 0.00042916 rank 3
2023-02-17 03:11:45,560 DEBUG TRAIN Batch 1/2400 loss 54.446499 loss_att 108.777016 loss_ctc 59.804981 loss_rnnt 42.691158 hw_loss 0.327694 lr 0.00042920 rank 0
2023-02-17 03:11:45,574 DEBUG TRAIN Batch 1/2400 loss 50.185307 loss_att 110.953796 loss_ctc 54.600018 loss_rnnt 37.254196 hw_loss 0.353969 lr 0.00043092 rank 1
2023-02-17 03:11:45,577 DEBUG TRAIN Batch 1/2400 loss 52.230518 loss_att 93.010971 loss_ctc 59.026676 loss_rnnt 42.992920 hw_loss 0.328787 lr 0.00042892 rank 4
2023-02-17 03:11:45,577 DEBUG TRAIN Batch 1/2400 loss 47.070999 loss_att 99.533150 loss_ctc 55.228558 loss_rnnt 35.376232 hw_loss 0.214989 lr 0.00042996 rank 2
2023-02-17 03:11:45,591 DEBUG TRAIN Batch 1/2400 loss 66.807228 loss_att 125.754959 loss_ctc 81.438141 loss_rnnt 52.875359 hw_loss 0.359133 lr 0.00042904 rank 7
2023-02-17 03:11:45,608 DEBUG TRAIN Batch 1/2400 loss 61.663651 loss_att 116.313766 loss_ctc 70.294861 loss_rnnt 49.431904 hw_loss 0.282915 lr 0.00042940 rank 5
2023-02-17 03:13:20,439 DEBUG TRAIN Batch 1/2500 loss 45.700714 loss_att 87.466049 loss_ctc 48.954159 loss_rnnt 36.727516 hw_loss 0.349390 lr 0.00043340 rank 5
2023-02-17 03:13:20,441 DEBUG TRAIN Batch 1/2500 loss 54.288754 loss_att 110.837219 loss_ctc 62.177536 loss_rnnt 41.770943 hw_loss 0.293022 lr 0.00043304 rank 7
2023-02-17 03:13:20,458 DEBUG TRAIN Batch 1/2500 loss 33.713158 loss_att 44.815464 loss_ctc 40.497070 loss_rnnt 30.353045 hw_loss 0.440866 lr 0.00043324 rank 6
2023-02-17 03:13:20,469 DEBUG TRAIN Batch 1/2500 loss 45.721584 loss_att 84.273392 loss_ctc 51.350201 loss_rnnt 37.095901 hw_loss 0.309066 lr 0.00043492 rank 1
2023-02-17 03:13:20,487 DEBUG TRAIN Batch 1/2500 loss 27.304043 loss_att 34.175575 loss_ctc 30.355986 loss_rnnt 25.324194 hw_loss 0.372406 lr 0.00043396 rank 2
2023-02-17 03:13:20,492 DEBUG TRAIN Batch 1/2500 loss 53.373333 loss_att 123.677803 loss_ctc 58.209404 loss_rnnt 38.545845 hw_loss 0.228341 lr 0.00043316 rank 3
2023-02-17 03:13:20,514 DEBUG TRAIN Batch 1/2500 loss 40.015484 loss_att 57.315331 loss_ctc 49.193378 loss_rnnt 35.126526 hw_loss 0.384875 lr 0.00043320 rank 0
2023-02-17 03:13:20,515 DEBUG TRAIN Batch 1/2500 loss 60.919724 loss_att 108.614227 loss_ctc 70.258621 loss_rnnt 50.000706 hw_loss 0.252993 lr 0.00043292 rank 4
2023-02-17 03:14:36,703 DEBUG TRAIN Batch 1/2600 loss 81.721191 loss_att 137.244598 loss_ctc 85.370667 loss_rnnt 70.032433 hw_loss 0.182779 lr 0.00043740 rank 5
2023-02-17 03:14:36,704 DEBUG TRAIN Batch 1/2600 loss 64.383850 loss_att 127.835434 loss_ctc 65.618629 loss_rnnt 51.328850 hw_loss 0.375104 lr 0.00043720 rank 0
2023-02-17 03:14:36,705 DEBUG TRAIN Batch 1/2600 loss 63.583549 loss_att 144.868759 loss_ctc 74.720421 loss_rnnt 45.709179 hw_loss 0.248278 lr 0.00043724 rank 6
2023-02-17 03:14:36,709 DEBUG TRAIN Batch 1/2600 loss 69.203377 loss_att 135.309204 loss_ctc 78.129074 loss_rnnt 54.627239 hw_loss 0.309134 lr 0.00043716 rank 3
2023-02-17 03:14:36,745 DEBUG TRAIN Batch 1/2600 loss 69.408562 loss_att 160.613602 loss_ctc 79.863937 loss_rnnt 49.585152 hw_loss 0.353142 lr 0.00043892 rank 1
2023-02-17 03:14:36,747 DEBUG TRAIN Batch 1/2600 loss 34.276566 loss_att 42.273010 loss_ctc 38.885693 loss_rnnt 31.851427 hw_loss 0.396181 lr 0.00043692 rank 4
2023-02-17 03:14:36,748 DEBUG TRAIN Batch 1/2600 loss 55.238754 loss_att 121.645309 loss_ctc 63.913555 loss_rnnt 40.676907 hw_loss 0.232316 lr 0.00043796 rank 2
2023-02-17 03:14:36,753 DEBUG TRAIN Batch 1/2600 loss 46.125481 loss_att 67.729446 loss_ctc 53.595718 loss_rnnt 40.666039 hw_loss 0.267414 lr 0.00043704 rank 7
2023-02-17 03:15:53,105 DEBUG TRAIN Batch 1/2700 loss 49.460136 loss_att 124.838348 loss_ctc 51.897423 loss_rnnt 33.915707 hw_loss 0.269654 lr 0.00044092 rank 4
2023-02-17 03:15:53,111 DEBUG TRAIN Batch 1/2700 loss 78.723991 loss_att 142.175766 loss_ctc 84.044708 loss_rnnt 65.141541 hw_loss 0.342508 lr 0.00044124 rank 6
2023-02-17 03:15:53,113 DEBUG TRAIN Batch 1/2700 loss 48.894588 loss_att 103.062363 loss_ctc 50.649254 loss_rnnt 37.665916 hw_loss 0.302178 lr 0.00044104 rank 7
2023-02-17 03:15:53,114 DEBUG TRAIN Batch 1/2700 loss 87.434402 loss_att 170.964493 loss_ctc 97.732330 loss_rnnt 69.217789 hw_loss 0.257869 lr 0.00044196 rank 2
2023-02-17 03:15:53,114 DEBUG TRAIN Batch 1/2700 loss 65.393837 loss_att 136.668167 loss_ctc 72.604141 loss_rnnt 50.065590 hw_loss 0.210020 lr 0.00044140 rank 5
2023-02-17 03:15:53,115 DEBUG TRAIN Batch 1/2700 loss 48.152035 loss_att 108.638107 loss_ctc 51.668346 loss_rnnt 35.482460 hw_loss 0.194105 lr 0.00044116 rank 3
2023-02-17 03:15:53,118 DEBUG TRAIN Batch 1/2700 loss 106.386589 loss_att 162.972672 loss_ctc 122.100113 loss_rnnt 92.834900 hw_loss 0.261256 lr 0.00044120 rank 0
2023-02-17 03:15:53,120 DEBUG TRAIN Batch 1/2700 loss 50.729671 loss_att 119.882187 loss_ctc 53.803570 loss_rnnt 36.358253 hw_loss 0.245736 lr 0.00044292 rank 1
2023-02-17 03:17:15,767 DEBUG TRAIN Batch 1/2800 loss 74.334106 loss_att 134.273438 loss_ctc 91.367790 loss_rnnt 59.867001 hw_loss 0.390142 lr 0.00044504 rank 7
2023-02-17 03:17:15,781 DEBUG TRAIN Batch 1/2800 loss 59.643173 loss_att 124.988525 loss_ctc 68.600662 loss_rnnt 45.194416 hw_loss 0.347539 lr 0.00044524 rank 6
2023-02-17 03:17:15,788 DEBUG TRAIN Batch 1/2800 loss 53.363079 loss_att 108.971176 loss_ctc 63.435616 loss_rnnt 40.744808 hw_loss 0.288087 lr 0.00044596 rank 2
2023-02-17 03:17:15,792 DEBUG TRAIN Batch 1/2800 loss 64.838585 loss_att 150.302246 loss_ctc 71.956192 loss_rnnt 46.653790 hw_loss 0.268228 lr 0.00044492 rank 4
2023-02-17 03:17:15,836 DEBUG TRAIN Batch 1/2800 loss 55.658684 loss_att 117.812164 loss_ctc 63.100719 loss_rnnt 42.044422 hw_loss 0.358663 lr 0.00044692 rank 1
2023-02-17 03:17:15,838 DEBUG TRAIN Batch 1/2800 loss 51.105152 loss_att 102.022827 loss_ctc 56.830143 loss_rnnt 40.019188 hw_loss 0.260804 lr 0.00044520 rank 0
2023-02-17 03:17:15,841 DEBUG TRAIN Batch 1/2800 loss 69.556908 loss_att 131.510544 loss_ctc 82.694046 loss_rnnt 55.274326 hw_loss 0.262927 lr 0.00044516 rank 3
2023-02-17 03:17:15,845 DEBUG TRAIN Batch 1/2800 loss 53.811577 loss_att 111.366943 loss_ctc 58.549034 loss_rnnt 41.580620 hw_loss 0.165429 lr 0.00044540 rank 5
2023-02-17 03:18:46,088 DEBUG TRAIN Batch 1/2900 loss 59.578968 loss_att 104.510147 loss_ctc 68.525536 loss_rnnt 49.232307 hw_loss 0.314153 lr 0.00044916 rank 3
2023-02-17 03:18:46,088 DEBUG TRAIN Batch 1/2900 loss 60.965477 loss_att 131.612976 loss_ctc 74.119476 loss_rnnt 44.927177 hw_loss 0.290499 lr 0.00044996 rank 2
2023-02-17 03:18:46,091 DEBUG TRAIN Batch 1/2900 loss 40.082039 loss_att 93.660019 loss_ctc 45.567764 loss_rnnt 28.495029 hw_loss 0.262473 lr 0.00044892 rank 4
2023-02-17 03:18:46,092 DEBUG TRAIN Batch 1/2900 loss 94.062584 loss_att 156.728958 loss_ctc 110.178909 loss_rnnt 79.193100 hw_loss 0.351303 lr 0.00044920 rank 0
2023-02-17 03:18:46,094 DEBUG TRAIN Batch 1/2900 loss 70.203827 loss_att 131.858627 loss_ctc 79.457474 loss_rnnt 56.513397 hw_loss 0.235601 lr 0.00044940 rank 5
2023-02-17 03:18:46,095 DEBUG TRAIN Batch 1/2900 loss 66.629944 loss_att 129.894531 loss_ctc 80.919403 loss_rnnt 51.949879 hw_loss 0.228532 lr 0.00044904 rank 7
2023-02-17 03:18:46,123 DEBUG TRAIN Batch 1/2900 loss 78.111717 loss_att 151.473297 loss_ctc 94.162888 loss_rnnt 61.193451 hw_loss 0.198368 lr 0.00044924 rank 6
2023-02-17 03:18:46,139 DEBUG TRAIN Batch 1/2900 loss 77.203339 loss_att 138.638428 loss_ctc 88.997734 loss_rnnt 63.186775 hw_loss 0.294299 lr 0.00045092 rank 1
2023-02-17 03:20:02,447 DEBUG TRAIN Batch 1/3000 loss 59.118462 loss_att 120.688522 loss_ctc 66.756020 loss_rnnt 45.594246 hw_loss 0.359732 lr 0.00045492 rank 1
2023-02-17 03:20:02,452 DEBUG TRAIN Batch 1/3000 loss 53.975433 loss_att 105.310150 loss_ctc 64.881241 loss_rnnt 42.156433 hw_loss 0.183654 lr 0.00045320 rank 0
2023-02-17 03:20:02,454 DEBUG TRAIN Batch 1/3000 loss 86.718857 loss_att 154.593018 loss_ctc 98.298065 loss_rnnt 71.460335 hw_loss 0.262114 lr 0.00045340 rank 5
2023-02-17 03:20:02,454 DEBUG TRAIN Batch 1/3000 loss 70.890862 loss_att 134.481155 loss_ctc 92.296432 loss_rnnt 55.218258 hw_loss 0.188366 lr 0.00045292 rank 4
2023-02-17 03:20:02,455 DEBUG TRAIN Batch 1/3000 loss 70.516266 loss_att 130.118530 loss_ctc 90.415375 loss_rnnt 55.756645 hw_loss 0.348661 lr 0.00045324 rank 6
2023-02-17 03:20:02,459 DEBUG TRAIN Batch 1/3000 loss 54.358788 loss_att 109.984978 loss_ctc 62.991844 loss_rnnt 41.926132 hw_loss 0.293142 lr 0.00045304 rank 7
2023-02-17 03:20:02,460 DEBUG TRAIN Batch 1/3000 loss 51.123356 loss_att 96.170158 loss_ctc 68.744598 loss_rnnt 39.523254 hw_loss 0.452328 lr 0.00045396 rank 2
2023-02-17 03:20:02,461 DEBUG TRAIN Batch 1/3000 loss 61.269032 loss_att 109.555893 loss_ctc 70.144608 loss_rnnt 50.250141 hw_loss 0.333951 lr 0.00045316 rank 3
2023-02-17 03:21:19,155 DEBUG TRAIN Batch 1/3100 loss 55.113728 loss_att 78.853569 loss_ctc 65.837570 loss_rnnt 48.761108 hw_loss 0.327770 lr 0.00045720 rank 0
2023-02-17 03:21:19,156 DEBUG TRAIN Batch 1/3100 loss 25.558733 loss_att 45.427013 loss_ctc 30.009706 loss_rnnt 20.827332 hw_loss 0.308026 lr 0.00045796 rank 2
2023-02-17 03:21:19,197 DEBUG TRAIN Batch 1/3100 loss 36.971985 loss_att 55.413322 loss_ctc 43.752205 loss_rnnt 32.171013 hw_loss 0.391260 lr 0.00045724 rank 6
2023-02-17 03:21:19,218 DEBUG TRAIN Batch 1/3100 loss 23.233696 loss_att 35.641373 loss_ctc 30.956394 loss_rnnt 19.558313 hw_loss 0.307782 lr 0.00045716 rank 3
2023-02-17 03:21:19,218 DEBUG TRAIN Batch 1/3100 loss 60.917034 loss_att 124.910950 loss_ctc 67.289497 loss_rnnt 47.075668 hw_loss 0.361724 lr 0.00045704 rank 7
2023-02-17 03:21:19,224 DEBUG TRAIN Batch 1/3100 loss 36.928082 loss_att 83.972328 loss_ctc 44.335007 loss_rnnt 26.351105 hw_loss 0.338506 lr 0.00045892 rank 1
2023-02-17 03:21:19,231 DEBUG TRAIN Batch 1/3100 loss 40.077179 loss_att 94.596367 loss_ctc 45.370113 loss_rnnt 28.314928 hw_loss 0.286288 lr 0.00045740 rank 5
2023-02-17 03:21:19,232 DEBUG TRAIN Batch 1/3100 loss 46.032394 loss_att 95.649010 loss_ctc 55.648872 loss_rnnt 34.651505 hw_loss 0.328814 lr 0.00045692 rank 4
2023-02-17 03:22:53,587 DEBUG TRAIN Batch 1/3200 loss 74.017036 loss_att 124.035583 loss_ctc 80.445282 loss_rnnt 63.068829 hw_loss 0.163856 lr 0.00046116 rank 3
2023-02-17 03:22:53,591 DEBUG TRAIN Batch 1/3200 loss 41.570793 loss_att 72.180969 loss_ctc 48.550087 loss_rnnt 34.347748 hw_loss 0.319570 lr 0.00046104 rank 7
2023-02-17 03:22:53,597 DEBUG TRAIN Batch 1/3200 loss 47.285854 loss_att 70.334099 loss_ctc 54.730129 loss_rnnt 41.513775 hw_loss 0.318479 lr 0.00046092 rank 4
2023-02-17 03:22:53,608 DEBUG TRAIN Batch 1/3200 loss 60.588371 loss_att 112.579544 loss_ctc 68.433891 loss_rnnt 48.959053 hw_loss 0.346897 lr 0.00046120 rank 0
2023-02-17 03:22:53,623 DEBUG TRAIN Batch 1/3200 loss 64.261391 loss_att 121.293877 loss_ctc 81.573143 loss_rnnt 50.398510 hw_loss 0.277777 lr 0.00046124 rank 6
2023-02-17 03:22:53,642 DEBUG TRAIN Batch 1/3200 loss 52.925037 loss_att 108.057999 loss_ctc 64.829300 loss_rnnt 40.129730 hw_loss 0.340277 lr 0.00046196 rank 2
2023-02-17 03:22:53,653 DEBUG TRAIN Batch 1/3200 loss 39.005482 loss_att 72.301239 loss_ctc 47.513321 loss_rnnt 31.069422 hw_loss 0.267246 lr 0.00046140 rank 5
2023-02-17 03:22:54,025 DEBUG TRAIN Batch 1/3200 loss 34.731308 loss_att 55.438728 loss_ctc 41.556351 loss_rnnt 29.517731 hw_loss 0.303914 lr 0.00046292 rank 1
2023-02-17 03:24:11,432 DEBUG TRAIN Batch 1/3300 loss 83.671402 loss_att 162.286774 loss_ctc 89.850632 loss_rnnt 67.000519 hw_loss 0.232329 lr 0.00046504 rank 7
2023-02-17 03:24:11,436 DEBUG TRAIN Batch 1/3300 loss 81.509064 loss_att 155.703842 loss_ctc 92.520950 loss_rnnt 65.021072 hw_loss 0.338966 lr 0.00046492 rank 4
2023-02-17 03:24:11,439 DEBUG TRAIN Batch 1/3300 loss 64.734688 loss_att 120.917450 loss_ctc 79.921432 loss_rnnt 51.332672 hw_loss 0.263562 lr 0.00046516 rank 3
2023-02-17 03:24:11,455 DEBUG TRAIN Batch 1/3300 loss 84.726997 loss_att 153.362137 loss_ctc 103.733116 loss_rnnt 68.349579 hw_loss 0.217958 lr 0.00046520 rank 0
2023-02-17 03:24:11,467 DEBUG TRAIN Batch 1/3300 loss 51.977234 loss_att 109.757515 loss_ctc 64.061943 loss_rnnt 38.684166 hw_loss 0.235709 lr 0.00046596 rank 2
2023-02-17 03:24:11,468 DEBUG TRAIN Batch 1/3300 loss 70.146584 loss_att 131.901489 loss_ctc 80.330032 loss_rnnt 56.282219 hw_loss 0.291724 lr 0.00046692 rank 1
2023-02-17 03:24:11,475 DEBUG TRAIN Batch 1/3300 loss 61.383293 loss_att 120.719879 loss_ctc 68.455620 loss_rnnt 48.409111 hw_loss 0.307297 lr 0.00046524 rank 6
2023-02-17 03:24:11,480 DEBUG TRAIN Batch 1/3300 loss 69.663223 loss_att 150.449219 loss_ctc 79.175034 loss_rnnt 52.072182 hw_loss 0.310491 lr 0.00046540 rank 5
2023-02-17 03:25:26,831 DEBUG TRAIN Batch 1/3400 loss 48.762184 loss_att 106.006569 loss_ctc 64.141602 loss_rnnt 35.113708 hw_loss 0.279385 lr 0.00046924 rank 6
2023-02-17 03:25:26,836 DEBUG TRAIN Batch 1/3400 loss 88.084839 loss_att 146.365189 loss_ctc 101.826805 loss_rnnt 74.383629 hw_loss 0.399143 lr 0.00047092 rank 1
2023-02-17 03:25:26,839 DEBUG TRAIN Batch 1/3400 loss 57.741806 loss_att 113.958862 loss_ctc 63.825195 loss_rnnt 45.572964 hw_loss 0.214336 lr 0.00046940 rank 5
2023-02-17 03:25:26,845 DEBUG TRAIN Batch 1/3400 loss 75.445763 loss_att 132.135300 loss_ctc 89.258804 loss_rnnt 62.109535 hw_loss 0.293581 lr 0.00046996 rank 2
2023-02-17 03:25:26,861 DEBUG TRAIN Batch 1/3400 loss 49.579391 loss_att 105.780548 loss_ctc 56.437088 loss_rnnt 37.267639 hw_loss 0.294684 lr 0.00046920 rank 0
2023-02-17 03:25:26,867 DEBUG TRAIN Batch 1/3400 loss 44.521946 loss_att 95.286087 loss_ctc 53.534363 loss_rnnt 33.015537 hw_loss 0.284855 lr 0.00046892 rank 4
2023-02-17 03:25:26,874 DEBUG TRAIN Batch 1/3400 loss 51.502422 loss_att 108.325790 loss_ctc 64.847076 loss_rnnt 38.193054 hw_loss 0.310148 lr 0.00046916 rank 3
2023-02-17 03:25:26,873 DEBUG TRAIN Batch 1/3400 loss 39.796959 loss_att 97.598450 loss_ctc 40.710354 loss_rnnt 27.994034 hw_loss 0.226570 lr 0.00046904 rank 7
2023-02-17 03:26:46,755 DEBUG TRAIN Batch 1/3500 loss 61.097637 loss_att 110.538170 loss_ctc 67.715683 loss_rnnt 50.170593 hw_loss 0.293495 lr 0.00047320 rank 0
2023-02-17 03:26:46,773 DEBUG TRAIN Batch 1/3500 loss 67.335701 loss_att 121.442062 loss_ctc 75.873062 loss_rnnt 55.238392 hw_loss 0.258214 lr 0.00047316 rank 3
2023-02-17 03:26:46,799 DEBUG TRAIN Batch 1/3500 loss 47.591339 loss_att 95.900284 loss_ctc 57.862183 loss_rnnt 36.355003 hw_loss 0.384562 lr 0.00047340 rank 5
2023-02-17 03:26:46,803 DEBUG TRAIN Batch 1/3500 loss 55.750603 loss_att 113.616394 loss_ctc 69.209839 loss_rnnt 42.210892 hw_loss 0.322478 lr 0.00047324 rank 6
2023-02-17 03:26:46,804 DEBUG TRAIN Batch 1/3500 loss 60.944496 loss_att 123.418365 loss_ctc 69.794312 loss_rnnt 47.103729 hw_loss 0.311279 lr 0.00047292 rank 4
2023-02-17 03:26:46,807 DEBUG TRAIN Batch 1/3500 loss 63.445503 loss_att 111.864716 loss_ctc 73.121964 loss_rnnt 52.280468 hw_loss 0.358121 lr 0.00047492 rank 1
2023-02-17 03:26:46,843 DEBUG TRAIN Batch 1/3500 loss 54.564152 loss_att 128.512360 loss_ctc 63.220062 loss_rnnt 38.472542 hw_loss 0.277205 lr 0.00047304 rank 7
2023-02-17 03:26:46,846 DEBUG TRAIN Batch 1/3500 loss 64.182373 loss_att 118.145142 loss_ctc 73.346497 loss_rnnt 51.996758 hw_loss 0.320970 lr 0.00047396 rank 2
2023-02-17 03:28:18,410 DEBUG TRAIN Batch 1/3600 loss 39.757851 loss_att 80.329987 loss_ctc 48.276413 loss_rnnt 30.339268 hw_loss 0.315643 lr 0.00047724 rank 6
2023-02-17 03:28:18,411 DEBUG TRAIN Batch 1/3600 loss 64.075348 loss_att 113.541809 loss_ctc 76.390656 loss_rnnt 52.370140 hw_loss 0.318512 lr 0.00047892 rank 1
2023-02-17 03:28:18,413 DEBUG TRAIN Batch 1/3600 loss 76.650536 loss_att 124.747414 loss_ctc 84.284653 loss_rnnt 65.795944 hw_loss 0.407514 lr 0.00047796 rank 2
2023-02-17 03:28:18,414 DEBUG TRAIN Batch 1/3600 loss 49.986145 loss_att 97.623741 loss_ctc 58.017799 loss_rnnt 39.237164 hw_loss 0.282326 lr 0.00047740 rank 5
2023-02-17 03:28:18,414 DEBUG TRAIN Batch 1/3600 loss 88.043182 loss_att 142.274368 loss_ctc 100.605347 loss_rnnt 75.385063 hw_loss 0.256745 lr 0.00047692 rank 4
2023-02-17 03:28:18,417 DEBUG TRAIN Batch 1/3600 loss 61.612637 loss_att 110.080994 loss_ctc 69.945572 loss_rnnt 50.621429 hw_loss 0.349648 lr 0.00047716 rank 3
2023-02-17 03:28:18,453 DEBUG TRAIN Batch 1/3600 loss 71.073814 loss_att 131.972031 loss_ctc 84.973618 loss_rnnt 56.936607 hw_loss 0.195477 lr 0.00047704 rank 7
2023-02-17 03:28:18,467 DEBUG TRAIN Batch 1/3600 loss 41.877750 loss_att 78.809448 loss_ctc 51.835648 loss_rnnt 33.020180 hw_loss 0.269081 lr 0.00047720 rank 0
2023-02-17 03:29:33,696 DEBUG TRAIN Batch 1/3700 loss 56.261082 loss_att 78.265190 loss_ctc 69.248734 loss_rnnt 49.919842 hw_loss 0.391377 lr 0.00048120 rank 0
2023-02-17 03:29:33,707 DEBUG TRAIN Batch 1/3700 loss 47.831795 loss_att 72.852905 loss_ctc 57.546719 loss_rnnt 41.317528 hw_loss 0.402600 lr 0.00048124 rank 6
2023-02-17 03:29:33,710 DEBUG TRAIN Batch 1/3700 loss 44.772617 loss_att 92.639778 loss_ctc 56.630138 loss_rnnt 33.484169 hw_loss 0.251271 lr 0.00048104 rank 7
2023-02-17 03:29:33,712 DEBUG TRAIN Batch 1/3700 loss 49.261650 loss_att 85.467453 loss_ctc 61.343601 loss_rnnt 40.255669 hw_loss 0.288553 lr 0.00048292 rank 1
2023-02-17 03:29:33,712 DEBUG TRAIN Batch 1/3700 loss 49.289085 loss_att 86.982216 loss_ctc 57.646996 loss_rnnt 40.461849 hw_loss 0.326669 lr 0.00048196 rank 2
2023-02-17 03:29:33,713 DEBUG TRAIN Batch 1/3700 loss 53.155739 loss_att 88.932732 loss_ctc 66.118080 loss_rnnt 44.127514 hw_loss 0.270956 lr 0.00048116 rank 3
2023-02-17 03:29:33,715 DEBUG TRAIN Batch 1/3700 loss 42.321106 loss_att 87.739349 loss_ctc 50.081818 loss_rnnt 31.999134 hw_loss 0.381665 lr 0.00048092 rank 4
2023-02-17 03:29:33,735 DEBUG TRAIN Batch 1/3700 loss 54.460678 loss_att 97.624054 loss_ctc 61.415207 loss_rnnt 44.695557 hw_loss 0.384701 lr 0.00048140 rank 5
2023-02-17 03:30:51,049 DEBUG TRAIN Batch 1/3800 loss 67.063446 loss_att 123.442886 loss_ctc 79.315109 loss_rnnt 54.041504 hw_loss 0.210928 lr 0.00048524 rank 6
2023-02-17 03:30:51,052 DEBUG TRAIN Batch 1/3800 loss 40.602356 loss_att 75.894798 loss_ctc 52.176197 loss_rnnt 31.845703 hw_loss 0.290592 lr 0.00048492 rank 4
2023-02-17 03:30:51,054 DEBUG TRAIN Batch 1/3800 loss 82.665176 loss_att 149.065033 loss_ctc 99.889915 loss_rnnt 66.938698 hw_loss 0.281007 lr 0.00048520 rank 0
2023-02-17 03:30:51,059 DEBUG TRAIN Batch 1/3800 loss 77.778969 loss_att 126.380463 loss_ctc 96.430870 loss_rnnt 65.459366 hw_loss 0.210725 lr 0.00048504 rank 7
2023-02-17 03:30:51,077 DEBUG TRAIN Batch 1/3800 loss 47.110058 loss_att 79.078247 loss_ctc 59.299206 loss_rnnt 38.907650 hw_loss 0.344165 lr 0.00048692 rank 1
2023-02-17 03:30:51,083 DEBUG TRAIN Batch 1/3800 loss 56.753094 loss_att 121.178307 loss_ctc 66.414856 loss_rnnt 42.434467 hw_loss 0.272524 lr 0.00048516 rank 3
2023-02-17 03:30:51,083 DEBUG TRAIN Batch 1/3800 loss 64.807274 loss_att 96.387245 loss_ctc 78.767349 loss_rnnt 56.465645 hw_loss 0.308032 lr 0.00048540 rank 5
2023-02-17 03:30:51,092 DEBUG TRAIN Batch 1/3800 loss 47.057056 loss_att 117.191338 loss_ctc 55.859108 loss_rnnt 31.725201 hw_loss 0.246360 lr 0.00048596 rank 2
2023-02-17 03:32:25,583 DEBUG TRAIN Batch 1/3900 loss 52.367256 loss_att 95.311935 loss_ctc 67.540207 loss_rnnt 41.653885 hw_loss 0.190075 lr 0.00048920 rank 0
2023-02-17 03:32:25,607 DEBUG TRAIN Batch 1/3900 loss 46.360085 loss_att 102.236259 loss_ctc 52.738792 loss_rnnt 34.146893 hw_loss 0.351483 lr 0.00048924 rank 6
2023-02-17 03:32:25,610 DEBUG TRAIN Batch 1/3900 loss 51.666111 loss_att 107.947716 loss_ctc 66.475616 loss_rnnt 38.262306 hw_loss 0.324155 lr 0.00048996 rank 2
2023-02-17 03:32:25,620 DEBUG TRAIN Batch 1/3900 loss 42.060818 loss_att 63.008991 loss_ctc 53.504509 loss_rnnt 36.159500 hw_loss 0.348484 lr 0.00048904 rank 7
2023-02-17 03:32:25,635 DEBUG TRAIN Batch 1/3900 loss 49.193466 loss_att 66.289215 loss_ctc 55.680267 loss_rnnt 44.783821 hw_loss 0.235472 lr 0.00048940 rank 5
2023-02-17 03:32:25,636 DEBUG TRAIN Batch 1/3900 loss 41.005558 loss_att 90.248734 loss_ctc 50.178566 loss_rnnt 29.832027 hw_loss 0.190929 lr 0.00048916 rank 3
2023-02-17 03:32:25,647 DEBUG TRAIN Batch 1/3900 loss 61.950630 loss_att 124.494911 loss_ctc 70.079399 loss_rnnt 48.218803 hw_loss 0.260882 lr 0.00048892 rank 4
2023-02-17 03:32:25,667 DEBUG TRAIN Batch 1/3900 loss 73.878609 loss_att 165.197540 loss_ctc 90.434235 loss_rnnt 53.274197 hw_loss 0.249759 lr 0.00049092 rank 1
2023-02-17 03:33:43,912 DEBUG TRAIN Batch 1/4000 loss 53.854061 loss_att 100.209816 loss_ctc 62.438183 loss_rnnt 43.286949 hw_loss 0.283895 lr 0.00049396 rank 2
2023-02-17 03:33:43,912 DEBUG TRAIN Batch 1/4000 loss 64.503113 loss_att 134.491821 loss_ctc 88.546921 loss_rnnt 47.205101 hw_loss 0.177067 lr 0.00049320 rank 0
2023-02-17 03:33:43,912 DEBUG TRAIN Batch 1/4000 loss 63.746346 loss_att 124.148949 loss_ctc 77.196243 loss_rnnt 49.743988 hw_loss 0.240975 lr 0.00049292 rank 4
2023-02-17 03:33:43,914 DEBUG TRAIN Batch 1/4000 loss 51.141548 loss_att 116.034485 loss_ctc 61.476288 loss_rnnt 36.649502 hw_loss 0.254050 lr 0.00049492 rank 1
2023-02-17 03:33:43,914 DEBUG TRAIN Batch 1/4000 loss 84.868607 loss_att 158.258118 loss_ctc 90.251938 loss_rnnt 69.332031 hw_loss 0.264196 lr 0.00049340 rank 5
2023-02-17 03:33:43,915 DEBUG TRAIN Batch 1/4000 loss 50.428043 loss_att 99.243164 loss_ctc 60.070740 loss_rnnt 39.227562 hw_loss 0.284555 lr 0.00049316 rank 3
2023-02-17 03:33:43,916 DEBUG TRAIN Batch 1/4000 loss 77.872704 loss_att 125.750748 loss_ctc 94.949684 loss_rnnt 65.852409 hw_loss 0.314547 lr 0.00049324 rank 6
2023-02-17 03:33:43,964 DEBUG TRAIN Batch 1/4000 loss 45.968204 loss_att 108.801880 loss_ctc 55.696220 loss_rnnt 31.986454 hw_loss 0.221145 lr 0.00049304 rank 7
2023-02-17 03:35:00,408 DEBUG TRAIN Batch 1/4100 loss 62.036694 loss_att 111.061348 loss_ctc 78.147026 loss_rnnt 49.960442 hw_loss 0.231152 lr 0.00049720 rank 0
2023-02-17 03:35:00,409 DEBUG TRAIN Batch 1/4100 loss 60.777748 loss_att 108.998161 loss_ctc 77.465805 loss_rnnt 48.812080 hw_loss 0.180953 lr 0.00049740 rank 5
2023-02-17 03:35:00,410 DEBUG TRAIN Batch 1/4100 loss 64.611588 loss_att 118.640152 loss_ctc 73.669853 loss_rnnt 52.503765 hw_loss 0.176880 lr 0.00049704 rank 7
2023-02-17 03:35:00,416 DEBUG TRAIN Batch 1/4100 loss 37.355652 loss_att 75.809944 loss_ctc 45.537048 loss_rnnt 28.440571 hw_loss 0.250060 lr 0.00049692 rank 4
2023-02-17 03:35:00,435 DEBUG TRAIN Batch 1/4100 loss 51.578884 loss_att 101.081451 loss_ctc 68.758484 loss_rnnt 39.326138 hw_loss 0.115539 lr 0.00049892 rank 1
2023-02-17 03:35:00,438 DEBUG TRAIN Batch 1/4100 loss 56.672859 loss_att 111.400566 loss_ctc 70.960762 loss_rnnt 43.670689 hw_loss 0.284197 lr 0.00049796 rank 2
2023-02-17 03:35:00,445 DEBUG TRAIN Batch 1/4100 loss 54.807896 loss_att 101.124603 loss_ctc 67.592049 loss_rnnt 43.642021 hw_loss 0.371218 lr 0.00049716 rank 3
2023-02-17 03:35:00,462 DEBUG TRAIN Batch 1/4100 loss 70.794334 loss_att 122.169739 loss_ctc 82.209244 loss_rnnt 58.829430 hw_loss 0.314702 lr 0.00049724 rank 6
2023-02-17 03:36:20,194 DEBUG TRAIN Batch 1/4200 loss 56.593639 loss_att 107.127197 loss_ctc 65.916466 loss_rnnt 45.141663 hw_loss 0.191663 lr 0.00050104 rank 7
2023-02-17 03:36:20,226 DEBUG TRAIN Batch 1/4200 loss 44.928669 loss_att 83.256760 loss_ctc 58.522846 loss_rnnt 35.280262 hw_loss 0.319186 lr 0.00050116 rank 3
2023-02-17 03:36:20,229 DEBUG TRAIN Batch 1/4200 loss 50.368595 loss_att 88.341492 loss_ctc 56.482887 loss_rnnt 41.782047 hw_loss 0.331366 lr 0.00050196 rank 2
2023-02-17 03:36:20,233 DEBUG TRAIN Batch 1/4200 loss 44.446468 loss_att 83.520889 loss_ctc 58.277359 loss_rnnt 34.582169 hw_loss 0.384936 lr 0.00050120 rank 0
2023-02-17 03:36:20,234 DEBUG TRAIN Batch 1/4200 loss 50.569569 loss_att 90.181313 loss_ctc 62.050186 loss_rnnt 40.913658 hw_loss 0.380265 lr 0.00050124 rank 6
2023-02-17 03:36:20,236 DEBUG TRAIN Batch 1/4200 loss 46.325275 loss_att 86.525803 loss_ctc 52.471561 loss_rnnt 37.346802 hw_loss 0.222876 lr 0.00050292 rank 1
2023-02-17 03:36:20,236 DEBUG TRAIN Batch 1/4200 loss 49.764675 loss_att 96.843536 loss_ctc 58.752232 loss_rnnt 39.003181 hw_loss 0.276332 lr 0.00050092 rank 4
2023-02-17 03:36:20,238 DEBUG TRAIN Batch 1/4200 loss 53.587242 loss_att 97.735107 loss_ctc 65.066711 loss_rnnt 43.058769 hw_loss 0.315566 lr 0.00050140 rank 5
2023-02-17 03:37:52,186 DEBUG TRAIN Batch 1/4300 loss 52.714798 loss_att 81.449959 loss_ctc 62.595001 loss_rnnt 45.460102 hw_loss 0.356816 lr 0.00050524 rank 6
2023-02-17 03:37:52,186 DEBUG TRAIN Batch 1/4300 loss 56.983273 loss_att 96.688461 loss_ctc 71.395493 loss_rnnt 46.971485 hw_loss 0.279602 lr 0.00050520 rank 0
2023-02-17 03:37:52,187 DEBUG TRAIN Batch 1/4300 loss 59.920372 loss_att 111.666771 loss_ctc 84.727432 loss_rnnt 46.126183 hw_loss 0.257435 lr 0.00050504 rank 7
2023-02-17 03:37:52,187 DEBUG TRAIN Batch 1/4300 loss 42.468174 loss_att 75.060112 loss_ctc 50.704292 loss_rnnt 34.701378 hw_loss 0.281738 lr 0.00050516 rank 3
2023-02-17 03:37:52,188 DEBUG TRAIN Batch 1/4300 loss 41.710915 loss_att 72.945419 loss_ctc 56.881695 loss_rnnt 33.296181 hw_loss 0.271983 lr 0.00050492 rank 4
2023-02-17 03:37:52,191 DEBUG TRAIN Batch 1/4300 loss 65.847023 loss_att 124.638435 loss_ctc 79.132797 loss_rnnt 52.149387 hw_loss 0.314840 lr 0.00050692 rank 1
2023-02-17 03:37:52,192 DEBUG TRAIN Batch 1/4300 loss 59.721157 loss_att 111.599380 loss_ctc 75.901733 loss_rnnt 47.009048 hw_loss 0.335725 lr 0.00050540 rank 5
2023-02-17 03:37:52,248 DEBUG TRAIN Batch 1/4300 loss 39.620613 loss_att 75.884506 loss_ctc 44.520874 loss_rnnt 31.584955 hw_loss 0.242837 lr 0.00050596 rank 2
2023-02-17 03:39:08,586 DEBUG TRAIN Batch 1/4400 loss 37.527584 loss_att 50.266655 loss_ctc 48.666492 loss_rnnt 33.306351 hw_loss 0.352931 lr 0.00050916 rank 3
2023-02-17 03:39:08,587 DEBUG TRAIN Batch 1/4400 loss 19.227171 loss_att 23.567196 loss_ctc 23.622465 loss_rnnt 17.545584 hw_loss 0.426645 lr 0.00050920 rank 0
2023-02-17 03:39:08,587 DEBUG TRAIN Batch 1/4400 loss 79.510513 loss_att 121.177505 loss_ctc 102.676025 loss_rnnt 67.929832 hw_loss 0.297273 lr 0.00051092 rank 1
2023-02-17 03:39:08,589 DEBUG TRAIN Batch 1/4400 loss 34.455223 loss_att 67.868423 loss_ctc 45.586548 loss_rnnt 26.117302 hw_loss 0.320819 lr 0.00050904 rank 7
2023-02-17 03:39:08,593 DEBUG TRAIN Batch 1/4400 loss 21.262218 loss_att 28.101608 loss_ctc 27.185352 loss_rnnt 18.845911 hw_loss 0.485020 lr 0.00050996 rank 2
2023-02-17 03:39:08,602 DEBUG TRAIN Batch 1/4400 loss 60.957302 loss_att 93.933228 loss_ctc 76.119621 loss_rnnt 52.154655 hw_loss 0.348419 lr 0.00050940 rank 5
2023-02-17 03:39:08,610 DEBUG TRAIN Batch 1/4400 loss 61.854897 loss_att 110.506020 loss_ctc 66.960320 loss_rnnt 51.330536 hw_loss 0.212647 lr 0.00050924 rank 6
2023-02-17 03:39:08,664 DEBUG TRAIN Batch 1/4400 loss 46.539532 loss_att 77.132156 loss_ctc 62.411964 loss_rnnt 38.132439 hw_loss 0.322948 lr 0.00050892 rank 4
2023-02-17 03:40:24,307 DEBUG TRAIN Batch 1/4500 loss 37.526245 loss_att 81.765793 loss_ctc 51.055191 loss_rnnt 26.737186 hw_loss 0.257411 lr 0.00051324 rank 6
2023-02-17 03:40:24,312 DEBUG TRAIN Batch 1/4500 loss 28.591930 loss_att 36.009457 loss_ctc 35.667507 loss_rnnt 25.927937 hw_loss 0.444517 lr 0.00051340 rank 5
2023-02-17 03:40:24,313 DEBUG TRAIN Batch 1/4500 loss 45.821705 loss_att 68.427612 loss_ctc 56.468178 loss_rnnt 39.704987 hw_loss 0.330017 lr 0.00051492 rank 1
2023-02-17 03:40:24,317 DEBUG TRAIN Batch 1/4500 loss 52.206062 loss_att 98.629112 loss_ctc 60.934975 loss_rnnt 41.583656 hw_loss 0.326135 lr 0.00051396 rank 2
2023-02-17 03:40:24,320 DEBUG TRAIN Batch 1/4500 loss 57.566120 loss_att 116.123108 loss_ctc 74.737297 loss_rnnt 43.432487 hw_loss 0.248902 lr 0.00051316 rank 3
2023-02-17 03:40:24,354 DEBUG TRAIN Batch 1/4500 loss 53.968357 loss_att 87.209351 loss_ctc 71.121819 loss_rnnt 44.856136 hw_loss 0.331681 lr 0.00051304 rank 7
2023-02-17 03:40:24,356 DEBUG TRAIN Batch 1/4500 loss 17.832085 loss_att 22.347172 loss_ctc 22.271217 loss_rnnt 16.112251 hw_loss 0.421749 lr 0.00051292 rank 4
2023-02-17 03:40:24,370 DEBUG TRAIN Batch 1/4500 loss 74.042946 loss_att 121.781013 loss_ctc 99.475632 loss_rnnt 60.889801 hw_loss 0.402209 lr 0.00051320 rank 0
2023-02-17 03:41:49,014 DEBUG TRAIN Batch 1/4600 loss 45.067963 loss_att 74.097435 loss_ctc 49.008339 loss_rnnt 38.647106 hw_loss 0.167963 lr 0.00051796 rank 2
2023-02-17 03:41:49,034 DEBUG TRAIN Batch 1/4600 loss 41.929543 loss_att 76.745430 loss_ctc 46.053326 loss_rnnt 34.270424 hw_loss 0.273946 lr 0.00051720 rank 0
2023-02-17 03:41:49,038 DEBUG TRAIN Batch 1/4600 loss 49.506104 loss_att 77.380516 loss_ctc 56.814079 loss_rnnt 42.762920 hw_loss 0.363568 lr 0.00051716 rank 3
2023-02-17 03:41:49,056 DEBUG TRAIN Batch 1/4600 loss 48.088470 loss_att 101.751816 loss_ctc 57.753918 loss_rnnt 35.966213 hw_loss 0.189124 lr 0.00051704 rank 7
2023-02-17 03:41:49,067 DEBUG TRAIN Batch 1/4600 loss 74.619362 loss_att 130.398819 loss_ctc 97.818687 loss_rnnt 60.237579 hw_loss 0.248712 lr 0.00051724 rank 6
2023-02-17 03:41:49,069 DEBUG TRAIN Batch 1/4600 loss 46.453114 loss_att 92.143982 loss_ctc 63.570625 loss_rnnt 34.903912 hw_loss 0.241295 lr 0.00051692 rank 4
2023-02-17 03:41:49,096 DEBUG TRAIN Batch 1/4600 loss 47.587311 loss_att 93.192398 loss_ctc 59.466408 loss_rnnt 36.666752 hw_loss 0.404366 lr 0.00051892 rank 1
2023-02-17 03:41:49,097 DEBUG TRAIN Batch 1/4600 loss 53.615505 loss_att 101.236275 loss_ctc 75.238228 loss_rnnt 41.077881 hw_loss 0.244571 lr 0.00051740 rank 5
2023-02-17 03:43:15,896 DEBUG TRAIN Batch 1/4700 loss 51.641064 loss_att 92.170212 loss_ctc 64.340111 loss_rnnt 41.728729 hw_loss 0.212442 lr 0.00052116 rank 3
2023-02-17 03:43:15,898 DEBUG TRAIN Batch 1/4700 loss 58.061543 loss_att 106.099579 loss_ctc 81.162476 loss_rnnt 45.227905 hw_loss 0.273575 lr 0.00052120 rank 0
2023-02-17 03:43:15,899 DEBUG TRAIN Batch 1/4700 loss 59.322338 loss_att 112.043098 loss_ctc 70.094757 loss_rnnt 47.271202 hw_loss 0.132497 lr 0.00052292 rank 1
2023-02-17 03:43:15,901 DEBUG TRAIN Batch 1/4700 loss 48.053471 loss_att 91.273727 loss_ctc 64.038589 loss_rnnt 37.101677 hw_loss 0.330736 lr 0.00052104 rank 7
2023-02-17 03:43:15,902 DEBUG TRAIN Batch 1/4700 loss 49.011387 loss_att 97.835648 loss_ctc 53.755352 loss_rnnt 38.478043 hw_loss 0.254934 lr 0.00052092 rank 4
2023-02-17 03:43:15,903 DEBUG TRAIN Batch 1/4700 loss 42.843204 loss_att 76.655701 loss_ctc 56.599159 loss_rnnt 34.039547 hw_loss 0.388181 lr 0.00052196 rank 2
2023-02-17 03:43:15,902 DEBUG TRAIN Batch 1/4700 loss 79.307037 loss_att 131.871780 loss_ctc 90.012344 loss_rnnt 67.224289 hw_loss 0.267057 lr 0.00052124 rank 6
2023-02-17 03:43:15,908 DEBUG TRAIN Batch 1/4700 loss 44.958313 loss_att 92.891296 loss_ctc 62.696457 loss_rnnt 32.863464 hw_loss 0.268437 lr 0.00052140 rank 5
2023-02-17 03:44:31,167 DEBUG TRAIN Batch 1/4800 loss 53.413326 loss_att 86.261734 loss_ctc 75.425514 loss_rnnt 43.741364 hw_loss 0.313732 lr 0.00052516 rank 3
2023-02-17 03:44:31,167 DEBUG TRAIN Batch 1/4800 loss 57.984722 loss_att 95.493912 loss_ctc 77.592278 loss_rnnt 47.627773 hw_loss 0.451444 lr 0.00052596 rank 2
2023-02-17 03:44:31,168 DEBUG TRAIN Batch 1/4800 loss 44.276661 loss_att 85.942886 loss_ctc 58.109146 loss_rnnt 33.950130 hw_loss 0.279281 lr 0.00052504 rank 7
2023-02-17 03:44:31,168 DEBUG TRAIN Batch 1/4800 loss 53.357464 loss_att 90.430008 loss_ctc 71.831108 loss_rnnt 43.331009 hw_loss 0.278991 lr 0.00052520 rank 0
2023-02-17 03:44:31,169 DEBUG TRAIN Batch 1/4800 loss 28.786184 loss_att 56.332077 loss_ctc 41.463840 loss_rnnt 21.376236 hw_loss 0.394523 lr 0.00052492 rank 4
2023-02-17 03:44:31,169 DEBUG TRAIN Batch 1/4800 loss 63.374092 loss_att 117.037598 loss_ctc 82.246284 loss_rnnt 50.000866 hw_loss 0.232936 lr 0.00052524 rank 6
2023-02-17 03:44:31,171 DEBUG TRAIN Batch 1/4800 loss 58.766388 loss_att 93.719986 loss_ctc 71.428772 loss_rnnt 49.939308 hw_loss 0.277575 lr 0.00052540 rank 5
2023-02-17 03:44:31,192 DEBUG TRAIN Batch 1/4800 loss 83.721878 loss_att 125.249481 loss_ctc 100.328110 loss_rnnt 73.060898 hw_loss 0.264925 lr 0.00052692 rank 1
2023-02-17 03:45:49,474 DEBUG TRAIN Batch 1/4900 loss 34.481140 loss_att 64.158287 loss_ctc 49.703415 loss_rnnt 26.324625 hw_loss 0.358958 lr 0.00052920 rank 0
2023-02-17 03:45:49,480 DEBUG TRAIN Batch 1/4900 loss 50.445766 loss_att 83.246002 loss_ctc 66.642395 loss_rnnt 41.628029 hw_loss 0.184010 lr 0.00052904 rank 7
2023-02-17 03:45:49,480 DEBUG TRAIN Batch 1/4900 loss 70.281746 loss_att 107.370735 loss_ctc 97.129158 loss_rnnt 59.161324 hw_loss 0.230556 lr 0.00052892 rank 4
2023-02-17 03:45:49,480 DEBUG TRAIN Batch 1/4900 loss 72.087181 loss_att 101.321106 loss_ctc 89.720535 loss_rnnt 63.722893 hw_loss 0.311993 lr 0.00053092 rank 1
2023-02-17 03:45:49,503 DEBUG TRAIN Batch 1/4900 loss 46.990871 loss_att 74.970779 loss_ctc 59.404320 loss_rnnt 39.584988 hw_loss 0.290206 lr 0.00052924 rank 6
2023-02-17 03:45:49,518 DEBUG TRAIN Batch 1/4900 loss 40.679531 loss_att 66.301178 loss_ctc 51.406151 loss_rnnt 34.037895 hw_loss 0.163295 lr 0.00052996 rank 2
2023-02-17 03:45:49,519 DEBUG TRAIN Batch 1/4900 loss 52.583168 loss_att 85.212753 loss_ctc 60.811943 loss_rnnt 44.789986 hw_loss 0.318917 lr 0.00052940 rank 5
2023-02-17 03:45:49,521 DEBUG TRAIN Batch 1/4900 loss 42.405033 loss_att 71.622513 loss_ctc 58.475502 loss_rnnt 34.295933 hw_loss 0.230386 lr 0.00052916 rank 3
2023-02-17 03:47:22,428 DEBUG TRAIN Batch 1/5000 loss 50.837749 loss_att 66.173370 loss_ctc 59.812893 loss_rnnt 46.391376 hw_loss 0.342300 lr 0.00053316 rank 3
2023-02-17 03:47:22,428 DEBUG TRAIN Batch 1/5000 loss 59.645180 loss_att 90.639229 loss_ctc 75.113129 loss_rnnt 51.219444 hw_loss 0.308493 lr 0.00053292 rank 4
2023-02-17 03:47:22,429 DEBUG TRAIN Batch 1/5000 loss 67.947151 loss_att 103.432411 loss_ctc 86.265434 loss_rnnt 58.230122 hw_loss 0.332883 lr 0.00053492 rank 1
2023-02-17 03:47:22,431 DEBUG TRAIN Batch 1/5000 loss 41.683765 loss_att 68.208519 loss_ctc 55.514683 loss_rnnt 34.339706 hw_loss 0.365603 lr 0.00053340 rank 5
2023-02-17 03:47:22,438 DEBUG TRAIN Batch 1/5000 loss 41.103867 loss_att 62.867111 loss_ctc 52.828796 loss_rnnt 35.021027 hw_loss 0.312884 lr 0.00053396 rank 2
2023-02-17 03:47:22,481 DEBUG TRAIN Batch 1/5000 loss 40.447201 loss_att 56.310417 loss_ctc 53.714066 loss_rnnt 35.320820 hw_loss 0.346539 lr 0.00053320 rank 0
2023-02-17 03:47:22,498 DEBUG TRAIN Batch 1/5000 loss 63.042801 loss_att 83.490616 loss_ctc 76.490616 loss_rnnt 56.997417 hw_loss 0.305200 lr 0.00053304 rank 7
2023-02-17 03:47:22,497 DEBUG TRAIN Batch 1/5000 loss 34.644192 loss_att 44.144169 loss_ctc 44.141418 loss_rnnt 31.218927 hw_loss 0.485568 lr 0.00053324 rank 6
2023-02-17 03:48:38,252 DEBUG TRAIN Batch 1/5100 loss 44.395069 loss_att 78.067017 loss_ctc 60.518730 loss_rnnt 35.344688 hw_loss 0.311566 lr 0.00053716 rank 3
2023-02-17 03:48:38,268 DEBUG TRAIN Batch 1/5100 loss 58.528709 loss_att 100.007614 loss_ctc 80.180199 loss_rnnt 47.193890 hw_loss 0.285322 lr 0.00053796 rank 2
2023-02-17 03:48:38,267 DEBUG TRAIN Batch 1/5100 loss 43.649345 loss_att 71.165016 loss_ctc 61.084621 loss_rnnt 35.607174 hw_loss 0.401881 lr 0.00053892 rank 1
2023-02-17 03:48:38,268 DEBUG TRAIN Batch 1/5100 loss 34.907372 loss_att 57.307484 loss_ctc 46.459187 loss_rnnt 28.755610 hw_loss 0.246556 lr 0.00053704 rank 7
2023-02-17 03:48:38,269 DEBUG TRAIN Batch 1/5100 loss 40.150051 loss_att 62.039200 loss_ctc 49.618889 loss_rnnt 34.350723 hw_loss 0.298095 lr 0.00053740 rank 5
2023-02-17 03:48:38,281 DEBUG TRAIN Batch 1/5100 loss 45.723770 loss_att 78.270737 loss_ctc 63.723572 loss_rnnt 36.698257 hw_loss 0.217769 lr 0.00053724 rank 6
2023-02-17 03:48:38,286 DEBUG TRAIN Batch 1/5100 loss 38.076923 loss_att 65.097801 loss_ctc 50.552879 loss_rnnt 30.835836 hw_loss 0.325222 lr 0.00053720 rank 0
2023-02-17 03:48:38,290 DEBUG TRAIN Batch 1/5100 loss 51.248314 loss_att 65.934479 loss_ctc 63.592278 loss_rnnt 46.548714 hw_loss 0.218450 lr 0.00053692 rank 4
2023-02-17 03:49:53,609 DEBUG TRAIN Batch 1/5200 loss 18.521292 loss_att 28.462635 loss_ctc 21.658005 loss_rnnt 15.898707 hw_loss 0.405164 lr 0.00054292 rank 1
2023-02-17 03:49:53,614 DEBUG TRAIN Batch 1/5200 loss 38.439770 loss_att 67.795769 loss_ctc 46.933754 loss_rnnt 31.253456 hw_loss 0.342339 lr 0.00054092 rank 4
2023-02-17 03:49:53,616 DEBUG TRAIN Batch 1/5200 loss 71.142044 loss_att 102.914093 loss_ctc 91.116028 loss_rnnt 62.000530 hw_loss 0.232315 lr 0.00054116 rank 3
2023-02-17 03:49:53,616 DEBUG TRAIN Batch 1/5200 loss 55.569305 loss_att 91.825470 loss_ctc 65.515778 loss_rnnt 46.819805 hw_loss 0.322631 lr 0.00054140 rank 5
2023-02-17 03:49:53,619 DEBUG TRAIN Batch 1/5200 loss 31.122639 loss_att 53.580753 loss_ctc 37.799320 loss_rnnt 25.703054 hw_loss 0.070760 lr 0.00054104 rank 7
2023-02-17 03:49:53,620 DEBUG TRAIN Batch 1/5200 loss 51.285847 loss_att 83.965363 loss_ctc 68.451508 loss_rnnt 42.295517 hw_loss 0.310643 lr 0.00054120 rank 0
2023-02-17 03:49:53,622 DEBUG TRAIN Batch 1/5200 loss 45.591690 loss_att 81.507858 loss_ctc 62.281410 loss_rnnt 36.068737 hw_loss 0.214540 lr 0.00054196 rank 2
2023-02-17 03:49:53,622 DEBUG TRAIN Batch 1/5200 loss 39.673897 loss_att 75.098129 loss_ctc 51.309135 loss_rnnt 30.864645 hw_loss 0.324446 lr 0.00054124 rank 6
2023-02-17 03:51:16,534 DEBUG TRAIN Batch 1/5300 loss 47.885929 loss_att 80.381210 loss_ctc 60.234844 loss_rnnt 39.549568 hw_loss 0.357716 lr 0.00054540 rank 5
2023-02-17 03:51:16,539 DEBUG TRAIN Batch 1/5300 loss 28.401320 loss_att 50.488632 loss_ctc 39.426086 loss_rnnt 22.268463 hw_loss 0.460172 lr 0.00054692 rank 1
2023-02-17 03:51:16,539 DEBUG TRAIN Batch 1/5300 loss 64.888702 loss_att 99.554077 loss_ctc 85.851913 loss_rnnt 54.999260 hw_loss 0.302382 lr 0.00054492 rank 4
2023-02-17 03:51:16,543 DEBUG TRAIN Batch 1/5300 loss 92.454422 loss_att 129.392975 loss_ctc 110.690170 loss_rnnt 82.516121 hw_loss 0.223429 lr 0.00054504 rank 7
2023-02-17 03:51:16,553 DEBUG TRAIN Batch 1/5300 loss 56.506447 loss_att 88.884872 loss_ctc 65.246521 loss_rnnt 48.726593 hw_loss 0.260297 lr 0.00054596 rank 2
2023-02-17 03:51:16,576 DEBUG TRAIN Batch 1/5300 loss 58.735176 loss_att 94.241592 loss_ctc 68.086044 loss_rnnt 50.257805 hw_loss 0.242455 lr 0.00054524 rank 6
2023-02-17 03:51:16,576 DEBUG TRAIN Batch 1/5300 loss 45.346439 loss_att 73.503494 loss_ctc 64.929466 loss_rnnt 36.945023 hw_loss 0.298006 lr 0.00054516 rank 3
2023-02-17 03:51:16,617 DEBUG TRAIN Batch 1/5300 loss 47.404564 loss_att 76.665680 loss_ctc 64.121193 loss_rnnt 39.202316 hw_loss 0.227139 lr 0.00054520 rank 0
2023-02-17 03:52:43,723 DEBUG TRAIN Batch 1/5400 loss 90.939392 loss_att 123.161797 loss_ctc 110.011154 loss_rnnt 81.802170 hw_loss 0.280951 lr 0.00054924 rank 6
2023-02-17 03:52:43,725 DEBUG TRAIN Batch 1/5400 loss 56.656742 loss_att 82.513710 loss_ctc 71.612175 loss_rnnt 49.307777 hw_loss 0.344087 lr 0.00055092 rank 1
2023-02-17 03:52:43,725 DEBUG TRAIN Batch 1/5400 loss 62.273972 loss_att 92.441673 loss_ctc 72.802780 loss_rnnt 54.715897 hw_loss 0.226302 lr 0.00054940 rank 5
2023-02-17 03:52:43,731 DEBUG TRAIN Batch 1/5400 loss 35.584759 loss_att 65.964409 loss_ctc 52.921761 loss_rnnt 27.077370 hw_loss 0.224735 lr 0.00054996 rank 2
2023-02-17 03:52:43,732 DEBUG TRAIN Batch 1/5400 loss 46.132931 loss_att 63.968311 loss_ctc 55.606884 loss_rnnt 41.168999 hw_loss 0.250612 lr 0.00054904 rank 7
2023-02-17 03:52:43,733 DEBUG TRAIN Batch 1/5400 loss 61.092041 loss_att 89.842987 loss_ctc 77.046097 loss_rnnt 53.098888 hw_loss 0.217048 lr 0.00054916 rank 3
2023-02-17 03:52:43,755 DEBUG TRAIN Batch 1/5400 loss 49.661770 loss_att 81.128204 loss_ctc 61.576218 loss_rnnt 41.582527 hw_loss 0.370056 lr 0.00054892 rank 4
2023-02-17 03:52:43,766 DEBUG TRAIN Batch 1/5400 loss 33.993896 loss_att 60.461285 loss_ctc 42.886848 loss_rnnt 27.353729 hw_loss 0.301804 lr 0.00054920 rank 0
2023-02-17 03:53:59,218 DEBUG TRAIN Batch 1/5500 loss 48.406345 loss_att 70.063690 loss_ctc 61.581146 loss_rnnt 42.225082 hw_loss 0.174666 lr 0.00055324 rank 6
2023-02-17 03:53:59,221 DEBUG TRAIN Batch 1/5500 loss 73.522339 loss_att 104.770149 loss_ctc 87.851059 loss_rnnt 65.188492 hw_loss 0.325851 lr 0.00055340 rank 5
2023-02-17 03:53:59,225 DEBUG TRAIN Batch 1/5500 loss 50.798637 loss_att 81.205887 loss_ctc 66.310120 loss_rnnt 42.477493 hw_loss 0.321562 lr 0.00055492 rank 1
2023-02-17 03:53:59,249 DEBUG TRAIN Batch 1/5500 loss 61.608009 loss_att 79.617668 loss_ctc 76.037811 loss_rnnt 55.935265 hw_loss 0.275330 lr 0.00055304 rank 7
2023-02-17 03:53:59,260 DEBUG TRAIN Batch 1/5500 loss 28.773745 loss_att 50.452049 loss_ctc 39.916489 loss_rnnt 22.768234 hw_loss 0.345277 lr 0.00055396 rank 2
2023-02-17 03:53:59,264 DEBUG TRAIN Batch 1/5500 loss 62.423939 loss_att 98.951607 loss_ctc 84.160271 loss_rnnt 52.055763 hw_loss 0.308357 lr 0.00055320 rank 0
2023-02-17 03:53:59,269 DEBUG TRAIN Batch 1/5500 loss 34.065063 loss_att 61.121605 loss_ctc 51.193905 loss_rnnt 26.216070 hw_loss 0.288445 lr 0.00055316 rank 3
2023-02-17 03:53:59,271 DEBUG TRAIN Batch 1/5500 loss 37.719990 loss_att 70.262596 loss_ctc 48.019604 loss_rnnt 29.683477 hw_loss 0.290078 lr 0.00055292 rank 4
2023-02-17 03:55:16,431 DEBUG TRAIN Batch 1/5600 loss 37.394562 loss_att 59.699028 loss_ctc 58.456329 loss_rnnt 29.939550 hw_loss 0.348534 lr 0.00055796 rank 2
2023-02-17 03:55:16,444 DEBUG TRAIN Batch 1/5600 loss 36.984726 loss_att 50.912212 loss_ctc 50.287334 loss_rnnt 32.295986 hw_loss 0.242932 lr 0.00055724 rank 6
2023-02-17 03:55:16,444 DEBUG TRAIN Batch 1/5600 loss 51.915081 loss_att 73.541641 loss_ctc 65.598022 loss_rnnt 45.605385 hw_loss 0.299982 lr 0.00055704 rank 7
2023-02-17 03:55:16,445 DEBUG TRAIN Batch 1/5600 loss 35.214729 loss_att 55.196693 loss_ctc 49.943089 loss_rnnt 29.094288 hw_loss 0.300500 lr 0.00055720 rank 0
2023-02-17 03:55:16,508 DEBUG TRAIN Batch 1/5600 loss 51.136570 loss_att 72.753197 loss_ctc 64.687347 loss_rnnt 44.846790 hw_loss 0.299406 lr 0.00055716 rank 3
2023-02-17 03:55:16,517 DEBUG TRAIN Batch 1/5600 loss 72.765045 loss_att 98.749451 loss_ctc 94.019310 loss_rnnt 64.608856 hw_loss 0.235138 lr 0.00055692 rank 4
2023-02-17 03:55:16,518 DEBUG TRAIN Batch 1/5600 loss 46.002132 loss_att 68.458946 loss_ctc 57.000153 loss_rnnt 39.902596 hw_loss 0.265817 lr 0.00055892 rank 1
2023-02-17 03:55:16,524 DEBUG TRAIN Batch 1/5600 loss 40.964352 loss_att 57.218330 loss_ctc 51.228069 loss_rnnt 36.165344 hw_loss 0.336961 lr 0.00055740 rank 5
2023-02-17 03:56:51,641 DEBUG TRAIN Batch 1/5700 loss 56.011929 loss_att 78.161575 loss_ctc 75.405670 loss_rnnt 48.807533 hw_loss 0.353690 lr 0.00056124 rank 6
2023-02-17 03:56:51,649 DEBUG TRAIN Batch 1/5700 loss 75.398224 loss_att 105.136673 loss_ctc 99.386955 loss_rnnt 66.092224 hw_loss 0.299637 lr 0.00056140 rank 5
2023-02-17 03:56:51,650 DEBUG TRAIN Batch 1/5700 loss 47.239925 loss_att 68.759796 loss_ctc 56.717072 loss_rnnt 41.509422 hw_loss 0.305460 lr 0.00056292 rank 1
2023-02-17 03:56:51,651 DEBUG TRAIN Batch 1/5700 loss 26.643927 loss_att 37.968414 loss_ctc 34.925606 loss_rnnt 23.045469 hw_loss 0.430003 lr 0.00056116 rank 3
2023-02-17 03:56:51,653 DEBUG TRAIN Batch 1/5700 loss 55.602875 loss_att 69.489235 loss_ctc 63.064465 loss_rnnt 51.672741 hw_loss 0.296220 lr 0.00056092 rank 4
2023-02-17 03:56:51,678 DEBUG TRAIN Batch 1/5700 loss 71.711952 loss_att 116.566666 loss_ctc 95.017761 loss_rnnt 59.505127 hw_loss 0.240818 lr 0.00056196 rank 2
2023-02-17 03:56:51,679 DEBUG TRAIN Batch 1/5700 loss 32.010517 loss_att 44.983101 loss_ctc 41.503036 loss_rnnt 28.057770 hw_loss 0.173559 lr 0.00056120 rank 0
2023-02-17 03:56:51,738 DEBUG TRAIN Batch 1/5700 loss 51.456940 loss_att 68.326088 loss_ctc 64.768250 loss_rnnt 46.119896 hw_loss 0.353199 lr 0.00056104 rank 7
2023-02-17 03:58:08,774 DEBUG TRAIN Batch 1/5800 loss 26.891575 loss_att 35.946587 loss_ctc 37.140026 loss_rnnt 23.583302 hw_loss 0.245267 lr 0.00056504 rank 7
2023-02-17 03:58:08,775 DEBUG TRAIN Batch 1/5800 loss 41.696297 loss_att 66.193512 loss_ctc 55.690548 loss_rnnt 34.792152 hw_loss 0.260247 lr 0.00056520 rank 0
2023-02-17 03:58:08,776 DEBUG TRAIN Batch 1/5800 loss 61.395763 loss_att 92.132027 loss_ctc 73.044266 loss_rnnt 53.558167 hw_loss 0.257266 lr 0.00056492 rank 4
2023-02-17 03:58:08,776 DEBUG TRAIN Batch 1/5800 loss 30.304771 loss_att 33.896782 loss_ctc 32.425800 loss_rnnt 29.094450 hw_loss 0.392094 lr 0.00056540 rank 5
2023-02-17 03:58:08,777 DEBUG TRAIN Batch 1/5800 loss 48.902855 loss_att 81.856720 loss_ctc 64.351952 loss_rnnt 40.061790 hw_loss 0.357030 lr 0.00056524 rank 6
2023-02-17 03:58:08,805 DEBUG TRAIN Batch 1/5800 loss 23.049118 loss_att 29.984632 loss_ctc 29.441772 loss_rnnt 20.623470 hw_loss 0.349110 lr 0.00056692 rank 1
2023-02-17 03:58:08,811 DEBUG TRAIN Batch 1/5800 loss 97.513573 loss_att 134.464676 loss_ctc 116.076012 loss_rnnt 87.504013 hw_loss 0.270654 lr 0.00056516 rank 3
2023-02-17 03:58:08,831 DEBUG TRAIN Batch 1/5800 loss 45.065754 loss_att 65.275444 loss_ctc 64.054619 loss_rnnt 38.357979 hw_loss 0.251230 lr 0.00056596 rank 2
2023-02-17 03:59:25,008 DEBUG TRAIN Batch 1/5900 loss 69.427330 loss_att 103.755569 loss_ctc 92.117188 loss_rnnt 59.410095 hw_loss 0.236768 lr 0.00056924 rank 6
2023-02-17 03:59:25,010 DEBUG TRAIN Batch 1/5900 loss 43.988041 loss_att 70.648834 loss_ctc 63.677551 loss_rnnt 35.881016 hw_loss 0.280498 lr 0.00056920 rank 0
2023-02-17 03:59:25,012 DEBUG TRAIN Batch 1/5900 loss 54.301735 loss_att 80.981964 loss_ctc 64.682755 loss_rnnt 47.419975 hw_loss 0.302966 lr 0.00056916 rank 3
2023-02-17 03:59:25,013 DEBUG TRAIN Batch 1/5900 loss 59.887936 loss_att 84.571602 loss_ctc 77.041443 loss_rnnt 52.457664 hw_loss 0.387006 lr 0.00056892 rank 4
2023-02-17 03:59:25,013 DEBUG TRAIN Batch 1/5900 loss 57.281555 loss_att 92.925156 loss_ctc 76.631454 loss_rnnt 47.484261 hw_loss 0.166091 lr 0.00056904 rank 7
2023-02-17 03:59:25,014 DEBUG TRAIN Batch 1/5900 loss 47.145618 loss_att 74.533356 loss_ctc 63.247429 loss_rnnt 39.383301 hw_loss 0.258490 lr 0.00056940 rank 5
2023-02-17 03:59:25,020 DEBUG TRAIN Batch 1/5900 loss 80.889771 loss_att 113.637985 loss_ctc 107.407669 loss_rnnt 70.717392 hw_loss 0.163150 lr 0.00057092 rank 1
2023-02-17 03:59:25,021 DEBUG TRAIN Batch 1/5900 loss 43.832844 loss_att 71.639130 loss_ctc 62.359039 loss_rnnt 35.679684 hw_loss 0.228269 lr 0.00056996 rank 2
2023-02-17 04:00:46,818 DEBUG TRAIN Batch 1/6000 loss 42.350460 loss_att 69.810577 loss_ctc 57.316917 loss_rnnt 34.727066 hw_loss 0.254710 lr 0.00057320 rank 0
2023-02-17 04:00:46,831 DEBUG TRAIN Batch 1/6000 loss 46.206730 loss_att 69.852600 loss_ctc 59.134308 loss_rnnt 39.573666 hw_loss 0.337908 lr 0.00057292 rank 4
2023-02-17 04:00:46,835 DEBUG TRAIN Batch 1/6000 loss 29.974915 loss_att 52.651772 loss_ctc 40.182503 loss_rnnt 23.903912 hw_loss 0.327417 lr 0.00057304 rank 7
2023-02-17 04:00:46,839 DEBUG TRAIN Batch 1/6000 loss 32.453796 loss_att 55.088295 loss_ctc 43.652584 loss_rnnt 26.266144 hw_loss 0.314219 lr 0.00057492 rank 1
2023-02-17 04:00:46,841 DEBUG TRAIN Batch 1/6000 loss 50.212780 loss_att 83.733521 loss_ctc 67.665100 loss_rnnt 41.053600 hw_loss 0.240103 lr 0.00057316 rank 3
2023-02-17 04:00:46,850 DEBUG TRAIN Batch 1/6000 loss 41.711891 loss_att 66.198853 loss_ctc 50.402992 loss_rnnt 35.525925 hw_loss 0.243297 lr 0.00057324 rank 6
2023-02-17 04:00:46,872 DEBUG TRAIN Batch 1/6000 loss 43.809418 loss_att 77.745941 loss_ctc 64.244003 loss_rnnt 34.145683 hw_loss 0.284660 lr 0.00057340 rank 5
2023-02-17 04:00:46,875 DEBUG TRAIN Batch 1/6000 loss 82.820030 loss_att 102.034286 loss_ctc 110.518066 loss_rnnt 75.112762 hw_loss 0.321272 lr 0.00057396 rank 2
2023-02-17 04:02:17,792 DEBUG TRAIN Batch 1/6100 loss 37.710678 loss_att 54.270157 loss_ctc 45.540840 loss_rnnt 33.241692 hw_loss 0.212007 lr 0.00057716 rank 3
2023-02-17 04:02:17,793 DEBUG TRAIN Batch 1/6100 loss 57.443703 loss_att 81.500404 loss_ctc 78.012283 loss_rnnt 49.750664 hw_loss 0.261038 lr 0.00057692 rank 4
2023-02-17 04:02:17,816 DEBUG TRAIN Batch 1/6100 loss 52.975468 loss_att 74.854965 loss_ctc 70.219879 loss_rnnt 46.153576 hw_loss 0.275134 lr 0.00057740 rank 5
2023-02-17 04:02:17,816 DEBUG TRAIN Batch 1/6100 loss 45.709507 loss_att 65.081535 loss_ctc 63.026722 loss_rnnt 39.384598 hw_loss 0.265386 lr 0.00057796 rank 2
2023-02-17 04:02:17,822 DEBUG TRAIN Batch 1/6100 loss 38.351124 loss_att 63.717880 loss_ctc 49.959816 loss_rnnt 31.565405 hw_loss 0.308522 lr 0.00057704 rank 7
2023-02-17 04:02:17,826 DEBUG TRAIN Batch 1/6100 loss 51.241302 loss_att 83.641190 loss_ctc 70.529617 loss_rnnt 42.065468 hw_loss 0.232659 lr 0.00057720 rank 0
2023-02-17 04:02:17,830 DEBUG TRAIN Batch 1/6100 loss 24.576376 loss_att 39.701576 loss_ctc 38.368637 loss_rnnt 19.592556 hw_loss 0.224647 lr 0.00057724 rank 6
2023-02-17 04:02:17,831 DEBUG TRAIN Batch 1/6100 loss 48.600277 loss_att 70.958313 loss_ctc 64.126564 loss_rnnt 41.893272 hw_loss 0.309800 lr 0.00057892 rank 1
2023-02-17 04:03:33,171 DEBUG TRAIN Batch 1/6200 loss 64.584320 loss_att 80.072029 loss_ctc 78.378212 loss_rnnt 59.531288 hw_loss 0.218066 lr 0.00058124 rank 6
2023-02-17 04:03:33,175 DEBUG TRAIN Batch 1/6200 loss 50.295956 loss_att 70.798866 loss_ctc 66.426773 loss_rnnt 43.852364 hw_loss 0.360434 lr 0.00058196 rank 2
2023-02-17 04:03:33,176 DEBUG TRAIN Batch 1/6200 loss 67.691521 loss_att 87.791039 loss_ctc 78.803543 loss_rnnt 62.055725 hw_loss 0.251793 lr 0.00058092 rank 4
2023-02-17 04:03:33,186 DEBUG TRAIN Batch 1/6200 loss 57.476025 loss_att 80.784622 loss_ctc 78.913086 loss_rnnt 49.805389 hw_loss 0.282453 lr 0.00058120 rank 0
2023-02-17 04:03:33,202 DEBUG TRAIN Batch 1/6200 loss 37.860271 loss_att 54.415283 loss_ctc 49.883240 loss_rnnt 32.801846 hw_loss 0.270683 lr 0.00058104 rank 7
2023-02-17 04:03:33,207 DEBUG TRAIN Batch 1/6200 loss 52.132801 loss_att 90.541611 loss_ctc 80.385353 loss_rnnt 40.527855 hw_loss 0.292834 lr 0.00058292 rank 1
2023-02-17 04:03:33,206 DEBUG TRAIN Batch 1/6200 loss 70.423805 loss_att 92.974579 loss_ctc 91.993225 loss_rnnt 62.889893 hw_loss 0.277189 lr 0.00058140 rank 5
2023-02-17 04:03:33,221 DEBUG TRAIN Batch 1/6200 loss 62.467297 loss_att 85.472572 loss_ctc 82.772705 loss_rnnt 54.991547 hw_loss 0.313689 lr 0.00058116 rank 3
2023-02-17 04:04:50,026 DEBUG TRAIN Batch 1/6300 loss 28.247427 loss_att 38.493637 loss_ctc 38.614937 loss_rnnt 24.594837 hw_loss 0.414396 lr 0.00058516 rank 3
2023-02-17 04:04:50,027 DEBUG TRAIN Batch 1/6300 loss 25.304777 loss_att 33.890629 loss_ctc 28.111135 loss_rnnt 23.038223 hw_loss 0.328509 lr 0.00058492 rank 4
2023-02-17 04:04:50,028 DEBUG TRAIN Batch 1/6300 loss 41.635426 loss_att 62.695034 loss_ctc 54.751713 loss_rnnt 35.476795 hw_loss 0.371006 lr 0.00058692 rank 1
2023-02-17 04:04:50,028 DEBUG TRAIN Batch 1/6300 loss 29.644026 loss_att 45.432171 loss_ctc 40.891121 loss_rnnt 24.805624 hw_loss 0.339675 lr 0.00058520 rank 0
2023-02-17 04:04:50,032 DEBUG TRAIN Batch 1/6300 loss 24.491383 loss_att 34.367416 loss_ctc 33.984207 loss_rnnt 21.074661 hw_loss 0.329635 lr 0.00058596 rank 2
2023-02-17 04:04:50,050 DEBUG TRAIN Batch 1/6300 loss 50.898266 loss_att 87.666733 loss_ctc 66.795258 loss_rnnt 41.276424 hw_loss 0.278529 lr 0.00058524 rank 6
2023-02-17 04:04:50,092 DEBUG TRAIN Batch 1/6300 loss 40.625481 loss_att 54.893127 loss_ctc 54.130840 loss_rnnt 35.806301 hw_loss 0.309257 lr 0.00058504 rank 7
2023-02-17 04:04:50,101 DEBUG TRAIN Batch 1/6300 loss 64.604637 loss_att 84.885628 loss_ctc 76.942261 loss_rnnt 58.726753 hw_loss 0.331242 lr 0.00058540 rank 5
2023-02-17 04:06:26,331 DEBUG TRAIN Batch 1/6400 loss 75.964317 loss_att 106.123444 loss_ctc 108.545929 loss_rnnt 65.424973 hw_loss 0.306180 lr 0.00058924 rank 6
2023-02-17 04:06:26,333 DEBUG TRAIN Batch 1/6400 loss 47.309444 loss_att 68.285248 loss_ctc 70.647369 loss_rnnt 39.881184 hw_loss 0.227587 lr 0.00058916 rank 3
2023-02-17 04:06:26,335 DEBUG TRAIN Batch 1/6400 loss 23.697426 loss_att 35.831181 loss_ctc 35.731079 loss_rnnt 19.514879 hw_loss 0.283701 lr 0.00059092 rank 1
2023-02-17 04:06:26,338 DEBUG TRAIN Batch 1/6400 loss 51.589527 loss_att 77.041992 loss_ctc 71.108002 loss_rnnt 43.733261 hw_loss 0.306200 lr 0.00058920 rank 0
2023-02-17 04:06:26,348 DEBUG TRAIN Batch 1/6400 loss 59.312080 loss_att 93.794296 loss_ctc 75.941521 loss_rnnt 50.109726 hw_loss 0.166225 lr 0.00058892 rank 4
2023-02-17 04:06:26,352 DEBUG TRAIN Batch 1/6400 loss 42.396328 loss_att 56.835781 loss_ctc 57.164455 loss_rnnt 37.357361 hw_loss 0.341232 lr 0.00058940 rank 5
2023-02-17 04:06:26,384 DEBUG TRAIN Batch 1/6400 loss 50.269421 loss_att 71.807686 loss_ctc 71.823814 loss_rnnt 42.889553 hw_loss 0.371806 lr 0.00058904 rank 7
2023-02-17 04:06:26,397 DEBUG TRAIN Batch 1/6400 loss 54.107964 loss_att 85.269096 loss_ctc 71.785660 loss_rnnt 45.379955 hw_loss 0.260165 lr 0.00058996 rank 2
2023-02-17 04:07:43,054 DEBUG TRAIN Batch 1/6500 loss 36.833328 loss_att 55.294430 loss_ctc 53.703209 loss_rnnt 30.702141 hw_loss 0.355589 lr 0.00059316 rank 3
2023-02-17 04:07:43,055 DEBUG TRAIN Batch 1/6500 loss 53.965313 loss_att 81.674622 loss_ctc 74.096985 loss_rnnt 45.579735 hw_loss 0.299049 lr 0.00059340 rank 5
2023-02-17 04:07:43,056 DEBUG TRAIN Batch 1/6500 loss 37.308075 loss_att 58.227619 loss_ctc 58.405453 loss_rnnt 30.123260 hw_loss 0.352358 lr 0.00059396 rank 2
2023-02-17 04:07:43,058 DEBUG TRAIN Batch 1/6500 loss 25.884745 loss_att 41.663990 loss_ctc 41.008202 loss_rnnt 20.622721 hw_loss 0.168211 lr 0.00059492 rank 1
2023-02-17 04:07:43,097 DEBUG TRAIN Batch 1/6500 loss 34.400864 loss_att 55.769611 loss_ctc 46.461723 loss_rnnt 28.348083 hw_loss 0.320477 lr 0.00059292 rank 4
2023-02-17 04:07:43,098 DEBUG TRAIN Batch 1/6500 loss 64.958916 loss_att 100.761879 loss_ctc 91.733574 loss_rnnt 54.075466 hw_loss 0.286690 lr 0.00059320 rank 0
2023-02-17 04:07:43,099 DEBUG TRAIN Batch 1/6500 loss 95.550240 loss_att 131.274139 loss_ctc 136.300476 loss_rnnt 82.789093 hw_loss 0.343113 lr 0.00059304 rank 7
2023-02-17 04:07:43,103 DEBUG TRAIN Batch 1/6500 loss 59.278454 loss_att 84.346001 loss_ctc 80.539635 loss_rnnt 51.261772 hw_loss 0.315659 lr 0.00059324 rank 6
2023-02-17 04:08:58,892 DEBUG TRAIN Batch 1/6600 loss 36.596191 loss_att 52.775429 loss_ctc 52.840912 loss_rnnt 30.980520 hw_loss 0.400995 lr 0.00059720 rank 0
2023-02-17 04:08:58,893 DEBUG TRAIN Batch 1/6600 loss 44.729481 loss_att 70.969429 loss_ctc 60.322197 loss_rnnt 37.254341 hw_loss 0.277721 lr 0.00059724 rank 6
2023-02-17 04:08:58,898 DEBUG TRAIN Batch 1/6600 loss 38.298546 loss_att 63.383881 loss_ctc 56.485344 loss_rnnt 30.693420 hw_loss 0.305907 lr 0.00059892 rank 1
2023-02-17 04:08:58,897 DEBUG TRAIN Batch 1/6600 loss 33.539062 loss_att 55.922112 loss_ctc 49.105972 loss_rnnt 26.851284 hw_loss 0.254210 lr 0.00059704 rank 7
2023-02-17 04:08:58,898 DEBUG TRAIN Batch 1/6600 loss 30.544462 loss_att 46.023991 loss_ctc 34.468086 loss_rnnt 26.802017 hw_loss 0.231359 lr 0.00059796 rank 2
2023-02-17 04:08:58,901 DEBUG TRAIN Batch 1/6600 loss 52.185329 loss_att 71.429443 loss_ctc 69.203049 loss_rnnt 45.915264 hw_loss 0.285402 lr 0.00059740 rank 5
2023-02-17 04:08:58,909 DEBUG TRAIN Batch 1/6600 loss 34.955212 loss_att 55.015968 loss_ctc 50.376408 loss_rnnt 28.762001 hw_loss 0.234187 lr 0.00059716 rank 3
2023-02-17 04:08:58,947 DEBUG TRAIN Batch 1/6600 loss 45.383755 loss_att 62.331947 loss_ctc 61.616730 loss_rnnt 39.665051 hw_loss 0.308751 lr 0.00059692 rank 4
2023-02-17 04:10:19,793 DEBUG TRAIN Batch 1/6700 loss 37.078159 loss_att 55.409035 loss_ctc 47.291920 loss_rnnt 31.913065 hw_loss 0.257028 lr 0.00060196 rank 2
2023-02-17 04:10:19,794 DEBUG TRAIN Batch 1/6700 loss 36.035145 loss_att 54.950779 loss_ctc 53.754051 loss_rnnt 29.654728 hw_loss 0.440191 lr 0.00060092 rank 4
2023-02-17 04:10:19,808 DEBUG TRAIN Batch 1/6700 loss 51.232525 loss_att 73.534515 loss_ctc 76.163223 loss_rnnt 43.273239 hw_loss 0.327739 lr 0.00060104 rank 7
2023-02-17 04:10:19,810 DEBUG TRAIN Batch 1/6700 loss 46.181538 loss_att 68.992775 loss_ctc 65.863274 loss_rnnt 38.862103 hw_loss 0.249303 lr 0.00060120 rank 0
2023-02-17 04:10:19,812 DEBUG TRAIN Batch 1/6700 loss 38.166538 loss_att 59.250847 loss_ctc 52.926537 loss_rnnt 31.858681 hw_loss 0.230622 lr 0.00060140 rank 5
2023-02-17 04:10:19,813 DEBUG TRAIN Batch 1/6700 loss 37.032024 loss_att 55.913841 loss_ctc 54.438965 loss_rnnt 30.787449 hw_loss 0.276157 lr 0.00060124 rank 6
2023-02-17 04:10:19,836 DEBUG TRAIN Batch 1/6700 loss 49.877819 loss_att 66.168861 loss_ctc 62.450592 loss_rnnt 44.745895 hw_loss 0.370015 lr 0.00060292 rank 1
2023-02-17 04:10:19,837 DEBUG TRAIN Batch 1/6700 loss 43.021523 loss_att 70.657974 loss_ctc 62.247444 loss_rnnt 34.746418 hw_loss 0.345671 lr 0.00060116 rank 3
2023-02-17 04:11:52,177 DEBUG TRAIN Batch 1/6800 loss 29.275721 loss_att 47.279724 loss_ctc 41.181805 loss_rnnt 23.940588 hw_loss 0.275349 lr 0.00060524 rank 6
2023-02-17 04:11:52,177 DEBUG TRAIN Batch 1/6800 loss 41.061859 loss_att 62.427917 loss_ctc 56.698425 loss_rnnt 34.571026 hw_loss 0.248898 lr 0.00060692 rank 1
2023-02-17 04:11:52,177 DEBUG TRAIN Batch 1/6800 loss 51.740807 loss_att 67.229446 loss_ctc 64.716225 loss_rnnt 46.798775 hw_loss 0.214217 lr 0.00060520 rank 0
2023-02-17 04:11:52,206 DEBUG TRAIN Batch 1/6800 loss 71.752022 loss_att 82.770500 loss_ctc 86.114891 loss_rnnt 67.498802 hw_loss 0.252160 lr 0.00060596 rank 2
2023-02-17 04:11:52,208 DEBUG TRAIN Batch 1/6800 loss 51.781998 loss_att 75.054626 loss_ctc 64.087082 loss_rnnt 45.363548 hw_loss 0.231093 lr 0.00060540 rank 5
2023-02-17 04:11:52,218 DEBUG TRAIN Batch 1/6800 loss 32.282150 loss_att 51.466858 loss_ctc 42.782097 loss_rnnt 26.874374 hw_loss 0.320332 lr 0.00060492 rank 4
2023-02-17 04:11:52,219 DEBUG TRAIN Batch 1/6800 loss 36.553722 loss_att 54.586086 loss_ctc 46.897995 loss_rnnt 31.394178 hw_loss 0.325941 lr 0.00060516 rank 3
2023-02-17 04:11:52,225 DEBUG TRAIN Batch 1/6800 loss 53.473064 loss_att 77.641907 loss_ctc 64.713715 loss_rnnt 46.962852 hw_loss 0.333180 lr 0.00060504 rank 7
2023-02-17 04:13:07,660 DEBUG TRAIN Batch 1/6900 loss 39.402763 loss_att 55.935501 loss_ctc 56.005993 loss_rnnt 33.643391 hw_loss 0.448241 lr 0.00060920 rank 0
2023-02-17 04:13:07,660 DEBUG TRAIN Batch 1/6900 loss 51.743622 loss_att 80.058220 loss_ctc 70.841461 loss_rnnt 43.385635 hw_loss 0.278794 lr 0.00060904 rank 7
2023-02-17 04:13:07,661 DEBUG TRAIN Batch 1/6900 loss 42.319851 loss_att 63.187431 loss_ctc 51.581345 loss_rnnt 36.778309 hw_loss 0.249671 lr 0.00061092 rank 1
2023-02-17 04:13:07,660 DEBUG TRAIN Batch 1/6900 loss 40.253777 loss_att 53.897758 loss_ctc 56.277401 loss_rnnt 35.193722 hw_loss 0.365211 lr 0.00060996 rank 2
2023-02-17 04:13:07,663 DEBUG TRAIN Batch 1/6900 loss 35.282856 loss_att 40.687714 loss_ctc 44.370594 loss_rnnt 32.795673 hw_loss 0.364703 lr 0.00060924 rank 6
2023-02-17 04:13:07,665 DEBUG TRAIN Batch 1/6900 loss 26.264687 loss_att 39.005711 loss_ctc 37.986752 loss_rnnt 22.038050 hw_loss 0.216540 lr 0.00060916 rank 3
2023-02-17 04:13:07,669 DEBUG TRAIN Batch 1/6900 loss 61.869286 loss_att 87.337227 loss_ctc 78.396851 loss_rnnt 54.421844 hw_loss 0.281585 lr 0.00060940 rank 5
2023-02-17 04:13:07,688 DEBUG TRAIN Batch 1/6900 loss 43.071239 loss_att 62.649086 loss_ctc 53.692665 loss_rnnt 37.601509 hw_loss 0.258700 lr 0.00060892 rank 4
2023-02-17 04:14:24,966 DEBUG TRAIN Batch 1/7000 loss 67.218147 loss_att 97.721519 loss_ctc 83.585281 loss_rnnt 58.800564 hw_loss 0.252414 lr 0.00061324 rank 6
2023-02-17 04:14:24,967 DEBUG TRAIN Batch 1/7000 loss 68.552490 loss_att 95.574142 loss_ctc 80.697510 loss_rnnt 61.392529 hw_loss 0.255546 lr 0.00061316 rank 3
2023-02-17 04:14:24,975 DEBUG TRAIN Batch 1/7000 loss 48.254841 loss_att 82.027351 loss_ctc 68.236893 loss_rnnt 38.736588 hw_loss 0.186517 lr 0.00061396 rank 2
2023-02-17 04:14:24,979 DEBUG TRAIN Batch 1/7000 loss 49.383781 loss_att 78.168358 loss_ctc 63.757153 loss_rnnt 41.576080 hw_loss 0.251877 lr 0.00061320 rank 0
2023-02-17 04:14:24,979 DEBUG TRAIN Batch 1/7000 loss 31.788662 loss_att 48.932884 loss_ctc 46.418316 loss_rnnt 26.238440 hw_loss 0.320170 lr 0.00061492 rank 1
2023-02-17 04:14:24,979 DEBUG TRAIN Batch 1/7000 loss 36.664993 loss_att 52.722126 loss_ctc 49.964558 loss_rnnt 31.511768 hw_loss 0.315980 lr 0.00061304 rank 7
2023-02-17 04:14:24,983 DEBUG TRAIN Batch 1/7000 loss 38.779030 loss_att 53.488838 loss_ctc 54.368690 loss_rnnt 33.594971 hw_loss 0.306520 lr 0.00061340 rank 5
2023-02-17 04:14:24,988 DEBUG TRAIN Batch 1/7000 loss 36.718193 loss_att 48.508900 loss_ctc 50.282726 loss_rnnt 32.389023 hw_loss 0.304544 lr 0.00061292 rank 4
2023-02-17 04:15:59,108 DEBUG TRAIN Batch 1/7100 loss 64.037971 loss_att 92.257866 loss_ctc 88.801872 loss_rnnt 54.970299 hw_loss 0.228460 lr 0.00061716 rank 3
2023-02-17 04:15:59,122 DEBUG TRAIN Batch 1/7100 loss 43.300426 loss_att 73.250542 loss_ctc 51.224541 loss_rnnt 36.109814 hw_loss 0.270072 lr 0.00061724 rank 6
2023-02-17 04:15:59,125 DEBUG TRAIN Batch 1/7100 loss 51.754314 loss_att 79.390854 loss_ctc 72.301941 loss_rnnt 43.359413 hw_loss 0.239816 lr 0.00061692 rank 4
2023-02-17 04:15:59,130 DEBUG TRAIN Batch 1/7100 loss 45.474880 loss_att 74.035217 loss_ctc 55.007156 loss_rnnt 38.341953 hw_loss 0.281040 lr 0.00061892 rank 1
2023-02-17 04:15:59,143 DEBUG TRAIN Batch 1/7100 loss 33.164398 loss_att 46.523205 loss_ctc 44.955620 loss_rnnt 28.787415 hw_loss 0.249494 lr 0.00061704 rank 7
2023-02-17 04:15:59,147 DEBUG TRAIN Batch 1/7100 loss 53.546143 loss_att 78.857063 loss_ctc 69.459671 loss_rnnt 46.260422 hw_loss 0.190741 lr 0.00061796 rank 2
2023-02-17 04:15:59,161 DEBUG TRAIN Batch 1/7100 loss 59.496792 loss_att 86.084824 loss_ctc 90.344650 loss_rnnt 49.950745 hw_loss 0.216362 lr 0.00061720 rank 0
2023-02-17 04:15:59,178 DEBUG TRAIN Batch 1/7100 loss 29.993269 loss_att 57.255302 loss_ctc 46.127350 loss_rnnt 22.233513 hw_loss 0.292754 lr 0.00061740 rank 5
2023-02-17 04:17:18,049 DEBUG TRAIN Batch 1/7200 loss 33.271904 loss_att 57.330437 loss_ctc 46.557243 loss_rnnt 26.560020 hw_loss 0.241492 lr 0.00062124 rank 6
2023-02-17 04:17:18,053 DEBUG TRAIN Batch 1/7200 loss 37.284943 loss_att 59.273098 loss_ctc 46.809826 loss_rnnt 31.528023 hw_loss 0.167445 lr 0.00062092 rank 4
2023-02-17 04:17:18,054 DEBUG TRAIN Batch 1/7200 loss 46.552135 loss_att 67.003441 loss_ctc 64.864120 loss_rnnt 39.870163 hw_loss 0.281460 lr 0.00062116 rank 3
2023-02-17 04:17:18,054 DEBUG TRAIN Batch 1/7200 loss 39.340931 loss_att 66.780922 loss_ctc 59.420921 loss_rnnt 31.003574 hw_loss 0.322540 lr 0.00062292 rank 1
2023-02-17 04:17:18,056 DEBUG TRAIN Batch 1/7200 loss 58.190407 loss_att 83.697464 loss_ctc 76.551292 loss_rnnt 50.483669 hw_loss 0.294775 lr 0.00062104 rank 7
2023-02-17 04:17:18,058 DEBUG TRAIN Batch 1/7200 loss 32.825573 loss_att 48.446800 loss_ctc 43.700207 loss_rnnt 28.082212 hw_loss 0.317178 lr 0.00062140 rank 5
2023-02-17 04:17:18,064 DEBUG TRAIN Batch 1/7200 loss 45.161404 loss_att 71.048355 loss_ctc 68.293884 loss_rnnt 36.790398 hw_loss 0.204911 lr 0.00062120 rank 0
2023-02-17 04:17:18,065 DEBUG TRAIN Batch 1/7200 loss 41.518646 loss_att 61.556313 loss_ctc 59.449707 loss_rnnt 34.977463 hw_loss 0.267829 lr 0.00062196 rank 2
2023-02-17 04:18:33,564 DEBUG TRAIN Batch 1/7300 loss 40.408916 loss_att 57.655022 loss_ctc 53.098488 loss_rnnt 35.134075 hw_loss 0.250645 lr 0.00062516 rank 3
2023-02-17 04:18:33,567 DEBUG TRAIN Batch 1/7300 loss 36.003452 loss_att 51.874172 loss_ctc 51.832485 loss_rnnt 30.568119 hw_loss 0.282464 lr 0.00062520 rank 0
2023-02-17 04:18:33,589 DEBUG TRAIN Batch 1/7300 loss 41.277996 loss_att 61.867569 loss_ctc 58.887600 loss_rnnt 34.623428 hw_loss 0.353818 lr 0.00062492 rank 4
2023-02-17 04:18:33,589 DEBUG TRAIN Batch 1/7300 loss 60.456535 loss_att 84.875893 loss_ctc 78.091492 loss_rnnt 53.087456 hw_loss 0.251023 lr 0.00062504 rank 7
2023-02-17 04:18:33,593 DEBUG TRAIN Batch 1/7300 loss 47.423038 loss_att 75.751160 loss_ctc 69.463562 loss_rnnt 38.697613 hw_loss 0.226998 lr 0.00062692 rank 1
2023-02-17 04:18:33,599 DEBUG TRAIN Batch 1/7300 loss 46.559574 loss_att 70.649704 loss_ctc 59.446194 loss_rnnt 39.864960 hw_loss 0.296944 lr 0.00062524 rank 6
2023-02-17 04:18:33,619 DEBUG TRAIN Batch 1/7300 loss 72.595215 loss_att 97.482292 loss_ctc 95.603233 loss_rnnt 64.387337 hw_loss 0.305109 lr 0.00062596 rank 2
2023-02-17 04:18:33,619 DEBUG TRAIN Batch 1/7300 loss 35.774372 loss_att 61.342300 loss_ctc 50.448620 loss_rnnt 28.582685 hw_loss 0.227877 lr 0.00062540 rank 5
2023-02-17 04:19:52,560 DEBUG TRAIN Batch 1/7400 loss 41.819515 loss_att 57.418228 loss_ctc 58.799660 loss_rnnt 36.322861 hw_loss 0.211671 lr 0.00062920 rank 0
2023-02-17 04:19:52,576 DEBUG TRAIN Batch 1/7400 loss 45.580147 loss_att 62.024818 loss_ctc 62.788357 loss_rnnt 39.860752 hw_loss 0.255064 lr 0.00062904 rank 7
2023-02-17 04:19:52,577 DEBUG TRAIN Batch 1/7400 loss 46.723877 loss_att 66.581917 loss_ctc 61.276306 loss_rnnt 40.660667 hw_loss 0.283640 lr 0.00062924 rank 6
2023-02-17 04:19:52,578 DEBUG TRAIN Batch 1/7400 loss 51.388058 loss_att 65.394058 loss_ctc 67.122276 loss_rnnt 46.322071 hw_loss 0.312924 lr 0.00063092 rank 1
2023-02-17 04:19:52,579 DEBUG TRAIN Batch 1/7400 loss 31.127777 loss_att 50.278175 loss_ctc 43.015862 loss_rnnt 25.573973 hw_loss 0.259960 lr 0.00062940 rank 5
2023-02-17 04:19:52,610 DEBUG TRAIN Batch 1/7400 loss 36.514549 loss_att 54.413780 loss_ctc 47.475243 loss_rnnt 31.322739 hw_loss 0.282257 lr 0.00062916 rank 3
2023-02-17 04:19:52,624 DEBUG TRAIN Batch 1/7400 loss 60.848919 loss_att 92.082222 loss_ctc 89.259094 loss_rnnt 50.646599 hw_loss 0.314324 lr 0.00062892 rank 4
2023-02-17 04:19:52,625 DEBUG TRAIN Batch 1/7400 loss 41.276211 loss_att 64.750809 loss_ctc 55.438587 loss_rnnt 34.565762 hw_loss 0.238528 lr 0.00062996 rank 2
2023-02-17 04:21:25,314 DEBUG TRAIN Batch 1/7500 loss 38.156448 loss_att 54.594738 loss_ctc 55.826599 loss_rnnt 32.345837 hw_loss 0.312998 lr 0.00063324 rank 6
2023-02-17 04:21:25,315 DEBUG TRAIN Batch 1/7500 loss 34.724926 loss_att 43.253273 loss_ctc 50.209877 loss_rnnt 30.746708 hw_loss 0.389790 lr 0.00063316 rank 3
2023-02-17 04:21:25,319 DEBUG TRAIN Batch 1/7500 loss 39.354996 loss_att 61.342579 loss_ctc 57.867989 loss_rnnt 32.354988 hw_loss 0.251426 lr 0.00063304 rank 7
2023-02-17 04:21:25,324 DEBUG TRAIN Batch 1/7500 loss 40.356647 loss_att 54.691986 loss_ctc 51.976208 loss_rnnt 35.768002 hw_loss 0.323076 lr 0.00063320 rank 0
2023-02-17 04:21:25,336 DEBUG TRAIN Batch 1/7500 loss 37.084415 loss_att 52.736679 loss_ctc 42.960888 loss_rnnt 33.001923 hw_loss 0.315954 lr 0.00063340 rank 5
2023-02-17 04:21:25,348 DEBUG TRAIN Batch 1/7500 loss 39.780792 loss_att 53.443336 loss_ctc 54.334042 loss_rnnt 35.005463 hw_loss 0.191974 lr 0.00063292 rank 4
2023-02-17 04:21:25,355 DEBUG TRAIN Batch 1/7500 loss 32.762512 loss_att 44.180984 loss_ctc 44.964569 loss_rnnt 28.681187 hw_loss 0.320047 lr 0.00063396 rank 2
2023-02-17 04:21:25,362 DEBUG TRAIN Batch 1/7500 loss 79.985222 loss_att 110.454956 loss_ctc 107.905273 loss_rnnt 70.030708 hw_loss 0.258533 lr 0.00063492 rank 1
2023-02-17 04:22:41,442 DEBUG TRAIN Batch 1/7600 loss 46.218624 loss_att 54.230556 loss_ctc 58.397964 loss_rnnt 42.831551 hw_loss 0.301444 lr 0.00063740 rank 5
2023-02-17 04:22:41,447 DEBUG TRAIN Batch 1/7600 loss 50.057804 loss_att 77.069664 loss_ctc 68.342361 loss_rnnt 42.065186 hw_loss 0.285574 lr 0.00063704 rank 7
2023-02-17 04:22:41,457 DEBUG TRAIN Batch 1/7600 loss 29.325275 loss_att 42.618397 loss_ctc 48.820885 loss_rnnt 23.995710 hw_loss 0.134112 lr 0.00063716 rank 3
2023-02-17 04:22:41,458 DEBUG TRAIN Batch 1/7600 loss 18.258642 loss_att 22.429895 loss_ctc 24.941467 loss_rnnt 16.333008 hw_loss 0.375638 lr 0.00063720 rank 0
2023-02-17 04:22:41,461 DEBUG TRAIN Batch 1/7600 loss 43.813030 loss_att 64.519951 loss_ctc 63.611237 loss_rnnt 36.882843 hw_loss 0.279458 lr 0.00063892 rank 1
2023-02-17 04:22:41,473 DEBUG TRAIN Batch 1/7600 loss 47.669785 loss_att 63.409798 loss_ctc 66.205742 loss_rnnt 41.895203 hw_loss 0.290844 lr 0.00063692 rank 4
2023-02-17 04:22:41,494 DEBUG TRAIN Batch 1/7600 loss 34.117714 loss_att 54.229977 loss_ctc 50.286724 loss_rnnt 27.804863 hw_loss 0.252250 lr 0.00063724 rank 6
2023-02-17 04:22:41,504 DEBUG TRAIN Batch 1/7600 loss 39.156635 loss_att 55.489864 loss_ctc 55.920593 loss_rnnt 33.462307 hw_loss 0.360914 lr 0.00063796 rank 2
2023-02-17 04:23:59,178 DEBUG TRAIN Batch 1/7700 loss 33.885548 loss_att 56.368732 loss_ctc 48.906898 loss_rnnt 27.258884 hw_loss 0.238454 lr 0.00064292 rank 1
2023-02-17 04:23:59,178 DEBUG TRAIN Batch 1/7700 loss 33.991543 loss_att 57.450294 loss_ctc 45.667839 loss_rnnt 27.594315 hw_loss 0.278695 lr 0.00064124 rank 6
2023-02-17 04:23:59,180 DEBUG TRAIN Batch 1/7700 loss 39.233982 loss_att 45.957756 loss_ctc 54.488403 loss_rnnt 35.645973 hw_loss 0.392499 lr 0.00064104 rank 7
2023-02-17 04:23:59,180 DEBUG TRAIN Batch 1/7700 loss 43.229195 loss_att 65.015625 loss_ctc 54.770195 loss_rnnt 37.152199 hw_loss 0.339204 lr 0.00064120 rank 0
2023-02-17 04:23:59,182 DEBUG TRAIN Batch 1/7700 loss 16.057726 loss_att 19.508593 loss_ctc 21.299507 loss_rnnt 14.442173 hw_loss 0.424644 lr 0.00064092 rank 4
2023-02-17 04:23:59,188 DEBUG TRAIN Batch 1/7700 loss 51.340397 loss_att 77.931931 loss_ctc 79.920258 loss_rnnt 42.054176 hw_loss 0.294876 lr 0.00064196 rank 2
2023-02-17 04:23:59,194 DEBUG TRAIN Batch 1/7700 loss 36.082272 loss_att 45.370934 loss_ctc 52.444572 loss_rnnt 31.882833 hw_loss 0.300121 lr 0.00064140 rank 5
2023-02-17 04:23:59,200 DEBUG TRAIN Batch 1/7700 loss 33.770477 loss_att 56.476265 loss_ctc 41.065460 loss_rnnt 28.104071 hw_loss 0.286088 lr 0.00064116 rank 3
2023-02-17 04:25:22,963 DEBUG TRAIN Batch 1/7800 loss 43.155224 loss_att 61.666916 loss_ctc 52.994904 loss_rnnt 37.960442 hw_loss 0.338409 lr 0.00064692 rank 1
2023-02-17 04:25:22,975 DEBUG TRAIN Batch 1/7800 loss 46.909451 loss_att 63.860146 loss_ctc 59.519459 loss_rnnt 41.714287 hw_loss 0.231920 lr 0.00064596 rank 2
2023-02-17 04:25:22,978 DEBUG TRAIN Batch 1/7800 loss 38.412621 loss_att 57.889065 loss_ctc 54.663910 loss_rnnt 32.177071 hw_loss 0.325166 lr 0.00064504 rank 7
2023-02-17 04:25:22,979 DEBUG TRAIN Batch 1/7800 loss 47.010399 loss_att 62.572639 loss_ctc 65.706810 loss_rnnt 41.258495 hw_loss 0.274866 lr 0.00064520 rank 0
2023-02-17 04:25:22,983 DEBUG TRAIN Batch 1/7800 loss 59.806496 loss_att 82.440399 loss_ctc 85.445427 loss_rnnt 51.704063 hw_loss 0.294613 lr 0.00064516 rank 3
2023-02-17 04:25:22,984 DEBUG TRAIN Batch 1/7800 loss 49.367290 loss_att 67.828926 loss_ctc 64.230621 loss_rnnt 43.589050 hw_loss 0.195257 lr 0.00064540 rank 5
2023-02-17 04:25:23,004 DEBUG TRAIN Batch 1/7800 loss 40.236904 loss_att 53.410149 loss_ctc 48.620052 loss_rnnt 36.338905 hw_loss 0.272993 lr 0.00064524 rank 6
2023-02-17 04:25:23,008 DEBUG TRAIN Batch 1/7800 loss 53.951088 loss_att 84.368347 loss_ctc 74.466156 loss_rnnt 44.989990 hw_loss 0.266814 lr 0.00064492 rank 4
2023-02-17 04:26:51,594 DEBUG TRAIN Batch 1/7900 loss 44.513836 loss_att 64.079285 loss_ctc 61.632973 loss_rnnt 38.161438 hw_loss 0.293918 lr 0.00064924 rank 6
2023-02-17 04:26:51,597 DEBUG TRAIN Batch 1/7900 loss 39.226128 loss_att 64.803093 loss_ctc 57.324394 loss_rnnt 31.578873 hw_loss 0.222671 lr 0.00064904 rank 7
2023-02-17 04:26:51,597 DEBUG TRAIN Batch 1/7900 loss 39.147388 loss_att 61.672192 loss_ctc 55.865593 loss_rnnt 32.267544 hw_loss 0.273366 lr 0.00064916 rank 3
2023-02-17 04:26:51,599 DEBUG TRAIN Batch 1/7900 loss 48.143429 loss_att 68.409042 loss_ctc 69.093773 loss_rnnt 41.130436 hw_loss 0.312162 lr 0.00064892 rank 4
2023-02-17 04:26:51,600 DEBUG TRAIN Batch 1/7900 loss 79.817039 loss_att 93.988441 loss_ctc 106.149216 loss_rnnt 73.321548 hw_loss 0.281731 lr 0.00065092 rank 1
2023-02-17 04:26:51,602 DEBUG TRAIN Batch 1/7900 loss 46.992783 loss_att 59.097523 loss_ctc 69.027748 loss_rnnt 41.445614 hw_loss 0.352929 lr 0.00064920 rank 0
2023-02-17 04:26:51,603 DEBUG TRAIN Batch 1/7900 loss 43.234009 loss_att 61.349281 loss_ctc 63.615643 loss_rnnt 36.771179 hw_loss 0.229171 lr 0.00064940 rank 5
2023-02-17 04:26:51,613 DEBUG TRAIN Batch 1/7900 loss 73.776237 loss_att 93.346558 loss_ctc 96.627373 loss_rnnt 66.685341 hw_loss 0.243766 lr 0.00064996 rank 2
2023-02-17 04:28:06,868 DEBUG TRAIN Batch 1/8000 loss 42.301907 loss_att 50.957748 loss_ctc 53.762722 loss_rnnt 38.893074 hw_loss 0.280414 lr 0.00065324 rank 6
2023-02-17 04:28:06,871 DEBUG TRAIN Batch 1/8000 loss 47.311371 loss_att 65.798119 loss_ctc 62.078369 loss_rnnt 41.490379 hw_loss 0.290071 lr 0.00065492 rank 1
2023-02-17 04:28:06,872 DEBUG TRAIN Batch 1/8000 loss 23.519859 loss_att 38.540653 loss_ctc 39.234482 loss_rnnt 18.275984 hw_loss 0.270816 lr 0.00065320 rank 0
2023-02-17 04:28:06,872 DEBUG TRAIN Batch 1/8000 loss 33.167217 loss_att 52.035213 loss_ctc 43.950363 loss_rnnt 27.798531 hw_loss 0.295001 lr 0.00065304 rank 7
2023-02-17 04:28:06,872 DEBUG TRAIN Batch 1/8000 loss 49.734135 loss_att 65.973450 loss_ctc 61.837254 loss_rnnt 44.678268 hw_loss 0.364220 lr 0.00065316 rank 3
2023-02-17 04:28:06,874 DEBUG TRAIN Batch 1/8000 loss 44.423031 loss_att 63.628979 loss_ctc 64.615746 loss_rnnt 37.744408 hw_loss 0.272002 lr 0.00065396 rank 2
2023-02-17 04:28:06,877 DEBUG TRAIN Batch 1/8000 loss 31.880331 loss_att 54.086643 loss_ctc 39.796616 loss_rnnt 26.242529 hw_loss 0.264442 lr 0.00065292 rank 4
2023-02-17 04:28:06,878 DEBUG TRAIN Batch 1/8000 loss 47.116997 loss_att 72.774994 loss_ctc 67.646774 loss_rnnt 39.107128 hw_loss 0.264308 lr 0.00065340 rank 5
2023-02-17 04:29:25,853 DEBUG TRAIN Batch 1/8100 loss 35.492867 loss_att 53.985336 loss_ctc 50.625103 loss_rnnt 29.660994 hw_loss 0.217031 lr 0.00065740 rank 5
2023-02-17 04:29:25,861 DEBUG TRAIN Batch 1/8100 loss 63.006374 loss_att 78.865097 loss_ctc 78.890549 loss_rnnt 57.598621 hw_loss 0.221471 lr 0.00065724 rank 6
2023-02-17 04:29:25,903 DEBUG TRAIN Batch 1/8100 loss 53.495861 loss_att 74.468475 loss_ctc 59.882858 loss_rnnt 48.287704 hw_loss 0.303812 lr 0.00065704 rank 7
2023-02-17 04:29:25,906 DEBUG TRAIN Batch 1/8100 loss 35.022034 loss_att 49.636658 loss_ctc 54.775864 loss_rnnt 29.314169 hw_loss 0.283312 lr 0.00065892 rank 1
2023-02-17 04:29:25,912 DEBUG TRAIN Batch 1/8100 loss 39.337116 loss_att 54.974346 loss_ctc 53.745499 loss_rnnt 34.186356 hw_loss 0.191625 lr 0.00065720 rank 0
2023-02-17 04:29:25,941 DEBUG TRAIN Batch 1/8100 loss 39.055466 loss_att 53.661400 loss_ctc 54.789043 loss_rnnt 33.888592 hw_loss 0.277275 lr 0.00065796 rank 2
2023-02-17 04:29:25,940 DEBUG TRAIN Batch 1/8100 loss 53.074524 loss_att 68.279160 loss_ctc 65.647720 loss_rnnt 48.218422 hw_loss 0.260144 lr 0.00065692 rank 4
2023-02-17 04:29:25,974 DEBUG TRAIN Batch 1/8100 loss 38.514431 loss_att 52.913910 loss_ctc 49.391571 loss_rnnt 34.041916 hw_loss 0.266876 lr 0.00065716 rank 3
2023-02-17 04:30:46,071 DEBUG TRAIN Batch 1/8200 loss 31.004869 loss_att 46.851631 loss_ctc 35.911552 loss_rnnt 27.055870 hw_loss 0.235165 lr 0.00066104 rank 7
2023-02-17 04:30:46,078 DEBUG TRAIN Batch 1/8200 loss 29.908110 loss_att 35.504116 loss_ctc 39.425869 loss_rnnt 27.319721 hw_loss 0.375283 lr 0.00066120 rank 0
2023-02-17 04:30:46,081 DEBUG TRAIN Batch 1/8200 loss 39.525120 loss_att 53.110031 loss_ctc 50.707417 loss_rnnt 35.108047 hw_loss 0.392092 lr 0.00066116 rank 3
2023-02-17 04:30:46,083 DEBUG TRAIN Batch 1/8200 loss 39.025913 loss_att 61.425598 loss_ctc 60.503990 loss_rnnt 31.551199 hw_loss 0.245689 lr 0.00066140 rank 5
2023-02-17 04:30:46,085 DEBUG TRAIN Batch 1/8200 loss 46.629032 loss_att 60.649597 loss_ctc 67.978905 loss_rnnt 40.763885 hw_loss 0.401973 lr 0.00066292 rank 1
2023-02-17 04:30:46,107 DEBUG TRAIN Batch 1/8200 loss 47.814758 loss_att 65.328049 loss_ctc 70.770828 loss_rnnt 41.072113 hw_loss 0.335956 lr 0.00066196 rank 2
2023-02-17 04:30:46,119 DEBUG TRAIN Batch 1/8200 loss 35.441666 loss_att 46.037773 loss_ctc 49.871288 loss_rnnt 31.208904 hw_loss 0.355482 lr 0.00066124 rank 6
2023-02-17 04:30:46,128 DEBUG TRAIN Batch 1/8200 loss 46.129372 loss_att 61.840759 loss_ctc 61.767303 loss_rnnt 40.719852 hw_loss 0.341585 lr 0.00066092 rank 4
2023-02-17 04:32:01,446 DEBUG TRAIN Batch 1/8300 loss 52.472633 loss_att 78.376839 loss_ctc 70.082008 loss_rnnt 44.844067 hw_loss 0.187137 lr 0.00066540 rank 5
2023-02-17 04:32:01,453 DEBUG TRAIN Batch 1/8300 loss 52.702938 loss_att 73.174744 loss_ctc 74.366730 loss_rnnt 45.562538 hw_loss 0.295376 lr 0.00066516 rank 3
2023-02-17 04:32:01,455 DEBUG TRAIN Batch 1/8300 loss 67.343765 loss_att 83.452034 loss_ctc 84.501884 loss_rnnt 61.666130 hw_loss 0.315446 lr 0.00066524 rank 6
2023-02-17 04:32:01,458 DEBUG TRAIN Batch 1/8300 loss 43.359367 loss_att 59.921841 loss_ctc 66.832733 loss_rnnt 36.806908 hw_loss 0.206595 lr 0.00066504 rank 7
2023-02-17 04:32:01,460 DEBUG TRAIN Batch 1/8300 loss 46.135281 loss_att 68.512863 loss_ctc 59.864433 loss_rnnt 39.707386 hw_loss 0.228411 lr 0.00066596 rank 2
2023-02-17 04:32:01,460 DEBUG TRAIN Batch 1/8300 loss 40.275639 loss_att 49.573021 loss_ctc 54.545238 loss_rnnt 36.307472 hw_loss 0.386390 lr 0.00066692 rank 1
2023-02-17 04:32:01,475 DEBUG TRAIN Batch 1/8300 loss 33.963165 loss_att 49.062897 loss_ctc 44.381168 loss_rnnt 29.396194 hw_loss 0.296168 lr 0.00066492 rank 4
2023-02-17 04:32:01,505 DEBUG TRAIN Batch 1/8300 loss 26.583885 loss_att 47.573456 loss_ctc 43.382118 loss_rnnt 19.959181 hw_loss 0.350670 lr 0.00066520 rank 0
2023-02-17 04:32:50,501 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 3
2023-02-17 04:32:50,505 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 6
2023-02-17 04:32:50,512 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 4
2023-02-17 04:32:50,514 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 7
2023-02-17 04:32:50,518 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 1
2023-02-17 04:32:50,525 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 2
2023-02-17 04:32:50,537 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 5
2023-02-17 04:32:50,538 DEBUG CV Batch 1/0 loss 7.688412 loss_att 8.288656 loss_ctc 10.950280 loss_rnnt 6.891436 hw_loss 0.453771 history loss 7.403656 rank 0
2023-02-17 04:33:03,710 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 6
2023-02-17 04:33:03,765 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 2
2023-02-17 04:33:03,829 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 5
2023-02-17 04:33:04,048 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 4
2023-02-17 04:33:04,086 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 3
2023-02-17 04:33:04,170 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 0
2023-02-17 04:33:04,309 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 7
2023-02-17 04:33:04,644 DEBUG CV Batch 1/100 loss 30.000193 loss_att 41.530167 loss_ctc 44.100994 loss_rnnt 25.655380 hw_loss 0.297583 history loss 15.584471 rank 1
2023-02-17 04:33:20,060 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 6
2023-02-17 04:33:20,092 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 0
2023-02-17 04:33:20,206 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 4
2023-02-17 04:33:20,403 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 7
2023-02-17 04:33:20,454 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 3
2023-02-17 04:33:20,877 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 2
2023-02-17 04:33:21,006 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 5
2023-02-17 04:33:22,838 DEBUG CV Batch 1/200 loss 47.996773 loss_att 91.216080 loss_ctc 78.633774 loss_rnnt 35.141827 hw_loss 0.236537 history loss 17.321687 rank 1
2023-02-17 04:33:33,857 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 6
2023-02-17 04:33:34,346 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 7
2023-02-17 04:33:34,380 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 3
2023-02-17 04:33:34,506 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 0
2023-02-17 04:33:34,557 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 4
2023-02-17 04:33:35,183 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 2
2023-02-17 04:33:35,247 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 5
2023-02-17 04:33:37,544 DEBUG CV Batch 1/300 loss 21.440512 loss_att 25.010204 loss_ctc 32.463760 loss_rnnt 19.063200 hw_loss 0.363011 history loss 17.456466 rank 1
2023-02-17 04:33:48,741 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 6
2023-02-17 04:33:49,095 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 0
2023-02-17 04:33:49,243 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 7
2023-02-17 04:33:49,853 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 2
2023-02-17 04:33:50,009 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 3
2023-02-17 04:33:50,429 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 5
2023-02-17 04:33:50,470 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 4
2023-02-17 04:33:51,898 DEBUG CV Batch 1/400 loss 83.271088 loss_att 224.391678 loss_ctc 92.095985 loss_rnnt 53.786674 hw_loss 0.156819 history loss 19.237047 rank 1
2023-02-17 04:34:02,465 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 7
2023-02-17 04:34:02,467 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 0
2023-02-17 04:34:02,474 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 6
2023-02-17 04:34:03,646 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 2
2023-02-17 04:34:03,752 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 3
2023-02-17 04:34:04,171 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 5
2023-02-17 04:34:04,394 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 4
2023-02-17 04:34:05,440 DEBUG CV Batch 1/500 loss 23.484083 loss_att 28.653982 loss_ctc 33.309418 loss_rnnt 20.978275 hw_loss 0.303345 history loss 20.494584 rank 1
2023-02-17 04:34:17,639 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 6
2023-02-17 04:34:17,839 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 0
2023-02-17 04:34:18,230 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 7
2023-02-17 04:34:19,274 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 3
2023-02-17 04:34:19,284 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 5
2023-02-17 04:34:19,388 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 2
2023-02-17 04:34:19,944 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 4
2023-02-17 04:34:20,765 DEBUG CV Batch 1/600 loss 19.589283 loss_att 22.389429 loss_ctc 25.227737 loss_rnnt 18.045797 hw_loss 0.434368 history loss 21.893460 rank 1
2023-02-17 04:34:32,953 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 6
2023-02-17 04:34:33,503 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 0
2023-02-17 04:34:33,561 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 7
2023-02-17 04:34:33,822 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 2
2023-02-17 04:34:34,482 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 5
2023-02-17 04:34:34,859 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 3
2023-02-17 04:34:35,790 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 4
2023-02-17 04:34:36,523 DEBUG CV Batch 1/700 loss 75.254738 loss_att 204.915192 loss_ctc 88.616432 loss_rnnt 47.476143 hw_loss 0.121756 history loss 23.112396 rank 1
2023-02-17 04:34:48,079 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 6
2023-02-17 04:34:48,235 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 7
2023-02-17 04:34:48,376 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 0
2023-02-17 04:34:48,554 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 2
2023-02-17 04:34:49,355 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 5
2023-02-17 04:34:50,273 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 3
2023-02-17 04:34:51,976 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 4
2023-02-17 04:34:52,420 DEBUG CV Batch 1/800 loss 28.658365 loss_att 37.969376 loss_ctc 43.318504 loss_rnnt 24.656799 hw_loss 0.346273 history loss 22.065059 rank 1
2023-02-17 04:35:04,820 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 7
2023-02-17 04:35:04,903 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 6
2023-02-17 04:35:05,130 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 0
2023-02-17 04:35:05,355 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 2
2023-02-17 04:35:05,989 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 5
2023-02-17 04:35:06,971 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 3
2023-02-17 04:35:08,549 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 4
2023-02-17 04:35:09,277 DEBUG CV Batch 1/900 loss 57.026249 loss_att 102.460388 loss_ctc 72.151321 loss_rnnt 45.804783 hw_loss 0.221164 history loss 21.856392 rank 1
2023-02-17 04:35:18,947 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 6
2023-02-17 04:35:19,244 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 7
2023-02-17 04:35:19,546 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 0
2023-02-17 04:35:19,825 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 2
2023-02-17 04:35:20,130 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 5
2023-02-17 04:35:21,429 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 3
2023-02-17 04:35:23,019 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 4
2023-02-17 04:35:23,321 DEBUG CV Batch 1/1000 loss 15.835776 loss_att 23.526434 loss_ctc 21.279991 loss_rnnt 13.400640 hw_loss 0.320831 history loss 21.450076 rank 1
2023-02-17 04:35:33,405 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 7
2023-02-17 04:35:33,725 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 6
2023-02-17 04:35:33,814 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 0
2023-02-17 04:35:33,862 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 2
2023-02-17 04:35:33,988 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 5
2023-02-17 04:35:36,847 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 3
2023-02-17 04:35:38,008 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 4
2023-02-17 04:35:38,792 DEBUG CV Batch 1/1100 loss 12.442122 loss_att 12.051827 loss_ctc 18.127525 loss_rnnt 11.536572 hw_loss 0.422915 history loss 21.442496 rank 1
2023-02-17 04:35:46,331 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 7
2023-02-17 04:35:46,576 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 0
2023-02-17 04:35:47,267 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 5
2023-02-17 04:35:47,350 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 2
2023-02-17 04:35:47,767 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 6
2023-02-17 04:35:50,949 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 3
2023-02-17 04:35:51,836 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 4
2023-02-17 04:35:52,670 DEBUG CV Batch 1/1200 loss 26.575554 loss_att 34.770973 loss_ctc 39.359245 loss_rnnt 23.052490 hw_loss 0.336539 history loss 22.000733 rank 1
2023-02-17 04:36:01,085 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 0
2023-02-17 04:36:01,542 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 7
2023-02-17 04:36:02,196 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 6
2023-02-17 04:36:02,336 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 2
2023-02-17 04:36:02,419 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 5
2023-02-17 04:36:06,482 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 3
2023-02-17 04:36:07,093 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 4
2023-02-17 04:36:07,587 DEBUG CV Batch 1/1300 loss 18.667698 loss_att 20.355452 loss_ctc 24.823977 loss_rnnt 17.279768 hw_loss 0.430391 history loss 22.411440 rank 1
2023-02-17 04:36:16,071 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 0
2023-02-17 04:36:16,572 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 7
2023-02-17 04:36:16,972 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 2
2023-02-17 04:36:17,443 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 5
2023-02-17 04:36:17,585 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 6
2023-02-17 04:36:21,774 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 4
2023-02-17 04:36:21,873 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 3
2023-02-17 04:36:22,926 DEBUG CV Batch 1/1400 loss 61.671753 loss_att 151.058502 loss_ctc 66.976555 loss_rnnt 42.987789 hw_loss 0.186207 history loss 23.037978 rank 1
2023-02-17 04:36:31,531 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 0
2023-02-17 04:36:32,147 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 2
2023-02-17 04:36:32,291 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 5
2023-02-17 04:36:32,332 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 7
2023-02-17 04:36:33,656 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 6
2023-02-17 04:36:37,143 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 3
2023-02-17 04:36:37,642 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 4
2023-02-17 04:36:38,982 DEBUG CV Batch 1/1500 loss 29.576195 loss_att 38.830647 loss_ctc 39.399864 loss_rnnt 26.206137 hw_loss 0.392523 history loss 22.595613 rank 1
2023-02-17 04:36:47,894 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 0
2023-02-17 04:36:48,362 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 5
2023-02-17 04:36:48,794 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 2
2023-02-17 04:36:48,850 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 7
2023-02-17 04:36:49,879 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 6
2023-02-17 04:36:53,560 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 3
2023-02-17 04:36:53,700 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 4
2023-02-17 04:36:55,439 DEBUG CV Batch 1/1600 loss 39.720524 loss_att 82.244896 loss_ctc 57.518951 loss_rnnt 28.713829 hw_loss 0.241306 history loss 22.498918 rank 1
2023-02-17 04:37:02,681 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 0
2023-02-17 04:37:02,702 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 5
2023-02-17 04:37:03,418 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 7
2023-02-17 04:37:03,628 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 2
2023-02-17 04:37:04,354 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 6
2023-02-17 04:37:08,163 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 3
2023-02-17 04:37:08,214 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 4
2023-02-17 04:37:09,745 DEBUG CV Batch 1/1700 loss 27.634892 loss_att 35.064800 loss_ctc 38.126675 loss_rnnt 24.569458 hw_loss 0.338524 history loss 22.277702 rank 1
2023-02-17 04:37:13,002 INFO Epoch 1 CV info cv_loss 22.238552455665026
2023-02-17 04:37:13,002 INFO Epoch 2 TRAIN info lr 0.00066636
2023-02-17 04:37:13,007 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:13,150 INFO Epoch 1 CV info cv_loss 22.238552458214958
2023-02-17 04:37:13,150 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/1.pt
2023-02-17 04:37:13,747 INFO Epoch 1 CV info cv_loss 22.238552456319738
2023-02-17 04:37:13,748 INFO Epoch 2 TRAIN info lr 0.00066572
2023-02-17 04:37:13,752 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:13,991 INFO Epoch 2 TRAIN info lr 0.00066708
2023-02-17 04:37:13,995 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:14,290 INFO Epoch 1 CV info cv_loss 22.2385524539421
2023-02-17 04:37:14,290 INFO Epoch 2 TRAIN info lr 0.0006682400000000001
2023-02-17 04:37:14,295 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:14,688 INFO Epoch 1 CV info cv_loss 22.23855245270159
2023-02-17 04:37:14,688 INFO Epoch 2 TRAIN info lr 0.00066712
2023-02-17 04:37:14,690 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:17,895 INFO Epoch 1 CV info cv_loss 22.23855245697445
2023-02-17 04:37:17,895 INFO Epoch 2 TRAIN info lr 0.0006662799999999999
2023-02-17 04:37:17,900 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:17,938 INFO Epoch 1 CV info cv_loss 22.238552454355602
2023-02-17 04:37:17,939 INFO Epoch 2 TRAIN info lr 0.00066692
2023-02-17 04:37:17,943 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:37:19,926 INFO Epoch 1 CV info cv_loss 22.238552455113688
2023-02-17 04:37:19,926 INFO Epoch 2 TRAIN info lr 0.0006682799999999999
2023-02-17 04:37:19,929 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 04:38:28,399 DEBUG TRAIN Batch 2/0 loss 22.802702 loss_att 24.458561 loss_ctc 26.727896 loss_rnnt 21.694597 hw_loss 0.475451 lr 0.00066640 rank 5
2023-02-17 04:38:28,399 DEBUG TRAIN Batch 2/0 loss 23.419930 loss_att 25.758738 loss_ctc 29.558407 loss_rnnt 21.914913 hw_loss 0.410234 lr 0.00066696 rank 3
2023-02-17 04:38:28,403 DEBUG TRAIN Batch 2/0 loss 20.614454 loss_att 21.693787 loss_ctc 25.046085 loss_rnnt 19.565975 hw_loss 0.453240 lr 0.00066712 rank 0
2023-02-17 04:38:28,412 DEBUG TRAIN Batch 2/0 loss 23.836628 loss_att 24.171076 loss_ctc 28.353045 loss_rnnt 22.962643 hw_loss 0.384202 lr 0.00066828 rank 2
2023-02-17 04:38:28,429 DEBUG TRAIN Batch 2/0 loss 20.958944 loss_att 21.629396 loss_ctc 23.550774 loss_rnnt 20.197571 hw_loss 0.528198 lr 0.00066832 rank 1
2023-02-17 04:38:28,433 DEBUG TRAIN Batch 2/0 loss 21.279739 loss_att 21.659994 loss_ctc 25.560339 loss_rnnt 20.406864 hw_loss 0.423897 lr 0.00066632 rank 4
2023-02-17 04:38:28,473 DEBUG TRAIN Batch 2/0 loss 21.760513 loss_att 25.054523 loss_ctc 28.258627 loss_rnnt 20.036152 hw_loss 0.373395 lr 0.00066716 rank 6
2023-02-17 04:38:28,476 DEBUG TRAIN Batch 2/0 loss 25.675341 loss_att 29.619953 loss_ctc 31.479256 loss_rnnt 23.868021 hw_loss 0.458513 lr 0.00066576 rank 7
2023-02-17 04:39:42,698 DEBUG TRAIN Batch 2/100 loss 77.949333 loss_att 105.441841 loss_ctc 109.594238 loss_rnnt 68.038391 hw_loss 0.362106 lr 0.00067040 rank 5
2023-02-17 04:39:42,698 DEBUG TRAIN Batch 2/100 loss 25.811150 loss_att 43.266731 loss_ctc 36.895996 loss_rnnt 20.696098 hw_loss 0.273664 lr 0.00067096 rank 3
2023-02-17 04:39:42,698 DEBUG TRAIN Batch 2/100 loss 36.831875 loss_att 63.895790 loss_ctc 46.924080 loss_rnnt 29.927185 hw_loss 0.274276 lr 0.00067112 rank 0
2023-02-17 04:39:42,699 DEBUG TRAIN Batch 2/100 loss 36.045162 loss_att 66.005798 loss_ctc 52.680614 loss_rnnt 27.720837 hw_loss 0.214004 lr 0.00067116 rank 6
2023-02-17 04:39:42,700 DEBUG TRAIN Batch 2/100 loss 31.929905 loss_att 49.410538 loss_ctc 46.161388 loss_rnnt 26.359386 hw_loss 0.331616 lr 0.00066976 rank 7
2023-02-17 04:39:42,702 DEBUG TRAIN Batch 2/100 loss 56.251747 loss_att 79.448189 loss_ctc 69.384720 loss_rnnt 49.698101 hw_loss 0.306179 lr 0.00067228 rank 2
2023-02-17 04:39:42,703 DEBUG TRAIN Batch 2/100 loss 32.915527 loss_att 47.579510 loss_ctc 52.314999 loss_rnnt 27.232597 hw_loss 0.306623 lr 0.00067232 rank 1
2023-02-17 04:39:42,709 DEBUG TRAIN Batch 2/100 loss 33.211277 loss_att 58.074722 loss_ctc 52.198296 loss_rnnt 25.548660 hw_loss 0.296864 lr 0.00067032 rank 4
2023-02-17 04:40:57,454 DEBUG TRAIN Batch 2/200 loss 42.805878 loss_att 63.004349 loss_ctc 61.722317 loss_rnnt 36.149448 hw_loss 0.177269 lr 0.00067496 rank 3
2023-02-17 04:40:57,456 DEBUG TRAIN Batch 2/200 loss 51.937180 loss_att 74.138145 loss_ctc 78.811478 loss_rnnt 43.770489 hw_loss 0.268616 lr 0.00067516 rank 6
2023-02-17 04:40:57,458 DEBUG TRAIN Batch 2/200 loss 47.201542 loss_att 65.810043 loss_ctc 68.802216 loss_rnnt 40.470249 hw_loss 0.242819 lr 0.00067632 rank 1
2023-02-17 04:40:57,458 DEBUG TRAIN Batch 2/200 loss 52.915512 loss_att 67.085228 loss_ctc 71.981194 loss_rnnt 47.359264 hw_loss 0.337908 lr 0.00067440 rank 5
2023-02-17 04:40:57,458 DEBUG TRAIN Batch 2/200 loss 61.521564 loss_att 84.281509 loss_ctc 78.434349 loss_rnnt 54.597305 hw_loss 0.219814 lr 0.00067512 rank 0
2023-02-17 04:40:57,461 DEBUG TRAIN Batch 2/200 loss 46.457031 loss_att 67.673508 loss_ctc 65.346420 loss_rnnt 39.573112 hw_loss 0.228819 lr 0.00067376 rank 7
2023-02-17 04:40:57,499 DEBUG TRAIN Batch 2/200 loss 34.535320 loss_att 48.357231 loss_ctc 44.038067 loss_rnnt 30.416950 hw_loss 0.163036 lr 0.00067432 rank 4
2023-02-17 04:40:57,510 DEBUG TRAIN Batch 2/200 loss 55.770313 loss_att 71.166084 loss_ctc 61.743847 loss_rnnt 51.802773 hw_loss 0.172342 lr 0.00067628 rank 2
2023-02-17 04:42:18,397 DEBUG TRAIN Batch 2/300 loss 42.444599 loss_att 64.303696 loss_ctc 58.764587 loss_rnnt 35.759903 hw_loss 0.256651 lr 0.00067840 rank 5
2023-02-17 04:42:18,413 DEBUG TRAIN Batch 2/300 loss 50.942486 loss_att 68.722824 loss_ctc 70.147217 loss_rnnt 44.736137 hw_loss 0.168107 lr 0.00067916 rank 6
2023-02-17 04:42:18,417 DEBUG TRAIN Batch 2/300 loss 30.873653 loss_att 48.295010 loss_ctc 47.675606 loss_rnnt 25.025883 hw_loss 0.231073 lr 0.00068028 rank 2
2023-02-17 04:42:18,439 DEBUG TRAIN Batch 2/300 loss 35.803375 loss_att 55.606419 loss_ctc 47.420563 loss_rnnt 30.105452 hw_loss 0.353161 lr 0.00068032 rank 1
2023-02-17 04:42:18,456 DEBUG TRAIN Batch 2/300 loss 37.582462 loss_att 52.358452 loss_ctc 52.065552 loss_rnnt 32.594086 hw_loss 0.191439 lr 0.00067912 rank 0
2023-02-17 04:42:18,478 DEBUG TRAIN Batch 2/300 loss 29.045567 loss_att 46.564056 loss_ctc 42.427162 loss_rnnt 23.665415 hw_loss 0.172951 lr 0.00067832 rank 4
2023-02-17 04:42:18,478 DEBUG TRAIN Batch 2/300 loss 42.070343 loss_att 67.788185 loss_ctc 60.617310 loss_rnnt 34.273064 hw_loss 0.338965 lr 0.00067776 rank 7
2023-02-17 04:42:18,484 DEBUG TRAIN Batch 2/300 loss 50.418034 loss_att 70.043709 loss_ctc 72.221039 loss_rnnt 43.436672 hw_loss 0.279673 lr 0.00067896 rank 3
2023-02-17 04:43:48,086 DEBUG TRAIN Batch 2/400 loss 30.330454 loss_att 48.331310 loss_ctc 44.191643 loss_rnnt 24.737968 hw_loss 0.270290 lr 0.00068296 rank 3
2023-02-17 04:43:48,086 DEBUG TRAIN Batch 2/400 loss 26.211948 loss_att 42.232582 loss_ctc 41.083652 loss_rnnt 20.876713 hw_loss 0.277902 lr 0.00068316 rank 6
2023-02-17 04:43:48,087 DEBUG TRAIN Batch 2/400 loss 56.679554 loss_att 80.524429 loss_ctc 89.029602 loss_rnnt 47.379200 hw_loss 0.408810 lr 0.00068240 rank 5
2023-02-17 04:43:48,089 DEBUG TRAIN Batch 2/400 loss 49.693386 loss_att 70.091095 loss_ctc 63.335899 loss_rnnt 43.666786 hw_loss 0.240094 lr 0.00068312 rank 0
2023-02-17 04:43:48,119 DEBUG TRAIN Batch 2/400 loss 49.578094 loss_att 77.569725 loss_ctc 74.341293 loss_rnnt 40.514229 hw_loss 0.307084 lr 0.00068176 rank 7
2023-02-17 04:43:48,121 DEBUG TRAIN Batch 2/400 loss 46.775658 loss_att 73.677689 loss_ctc 63.934471 loss_rnnt 39.006760 hw_loss 0.188716 lr 0.00068428 rank 2
2023-02-17 04:43:48,126 DEBUG TRAIN Batch 2/400 loss 41.153851 loss_att 54.974525 loss_ctc 56.358585 loss_rnnt 36.217484 hw_loss 0.271753 lr 0.00068432 rank 1
2023-02-17 04:43:48,165 DEBUG TRAIN Batch 2/400 loss 47.466938 loss_att 64.508286 loss_ctc 59.075283 loss_rnnt 42.307888 hw_loss 0.380631 lr 0.00068232 rank 4
2023-02-17 04:45:04,491 DEBUG TRAIN Batch 2/500 loss 35.089413 loss_att 48.054741 loss_ctc 45.146759 loss_rnnt 31.009180 hw_loss 0.274107 lr 0.00068632 rank 4
2023-02-17 04:45:04,493 DEBUG TRAIN Batch 2/500 loss 62.034737 loss_att 82.936462 loss_ctc 82.961021 loss_rnnt 54.892151 hw_loss 0.322629 lr 0.00068696 rank 3
2023-02-17 04:45:04,495 DEBUG TRAIN Batch 2/500 loss 42.108253 loss_att 57.290558 loss_ctc 57.651928 loss_rnnt 36.831654 hw_loss 0.314331 lr 0.00068640 rank 5
2023-02-17 04:45:04,497 DEBUG TRAIN Batch 2/500 loss 51.361961 loss_att 68.181343 loss_ctc 73.978111 loss_rnnt 44.813065 hw_loss 0.317880 lr 0.00068832 rank 1
2023-02-17 04:45:04,502 DEBUG TRAIN Batch 2/500 loss 31.664667 loss_att 48.802269 loss_ctc 47.503464 loss_rnnt 25.973492 hw_loss 0.284652 lr 0.00068716 rank 6
2023-02-17 04:45:04,520 DEBUG TRAIN Batch 2/500 loss 43.423691 loss_att 58.119835 loss_ctc 59.670208 loss_rnnt 38.159973 hw_loss 0.296783 lr 0.00068576 rank 7
2023-02-17 04:45:04,531 DEBUG TRAIN Batch 2/500 loss 51.427860 loss_att 65.842224 loss_ctc 72.165810 loss_rnnt 45.614876 hw_loss 0.309472 lr 0.00068712 rank 0
2023-02-17 04:45:04,560 DEBUG TRAIN Batch 2/500 loss 42.980034 loss_att 65.193939 loss_ctc 58.826515 loss_rnnt 36.276634 hw_loss 0.277038 lr 0.00068828 rank 2
2023-02-17 04:46:20,827 DEBUG TRAIN Batch 2/600 loss 46.394901 loss_att 51.361649 loss_ctc 62.107914 loss_rnnt 43.105362 hw_loss 0.377102 lr 0.00069116 rank 6
2023-02-17 04:46:20,829 DEBUG TRAIN Batch 2/600 loss 28.917696 loss_att 34.984310 loss_ctc 37.246872 loss_rnnt 26.405909 hw_loss 0.352326 lr 0.00069040 rank 5
2023-02-17 04:46:20,829 DEBUG TRAIN Batch 2/600 loss 23.868233 loss_att 27.498562 loss_ctc 28.815832 loss_rnnt 22.280170 hw_loss 0.379340 lr 0.00069112 rank 0
2023-02-17 04:46:20,830 DEBUG TRAIN Batch 2/600 loss 42.547859 loss_att 52.724739 loss_ctc 54.431297 loss_rnnt 38.790295 hw_loss 0.258248 lr 0.00069096 rank 3
2023-02-17 04:46:20,834 DEBUG TRAIN Batch 2/600 loss 28.703613 loss_att 34.213917 loss_ctc 35.352043 loss_rnnt 26.537397 hw_loss 0.333185 lr 0.00069032 rank 4
2023-02-17 04:46:20,853 DEBUG TRAIN Batch 2/600 loss 37.535160 loss_att 41.048416 loss_ctc 47.194206 loss_rnnt 35.362991 hw_loss 0.340590 lr 0.00069232 rank 1
2023-02-17 04:46:20,860 DEBUG TRAIN Batch 2/600 loss 26.166939 loss_att 34.439926 loss_ctc 33.666031 loss_rnnt 23.341499 hw_loss 0.320555 lr 0.00068976 rank 7
2023-02-17 04:46:20,897 DEBUG TRAIN Batch 2/600 loss 18.964960 loss_att 24.146601 loss_ctc 27.165169 loss_rnnt 16.676006 hw_loss 0.298621 lr 0.00069228 rank 2
2023-02-17 04:47:46,498 DEBUG TRAIN Batch 2/700 loss 25.085522 loss_att 42.864075 loss_ctc 39.502762 loss_rnnt 19.442904 hw_loss 0.308642 lr 0.00069440 rank 5
2023-02-17 04:47:46,512 DEBUG TRAIN Batch 2/700 loss 34.708530 loss_att 58.701378 loss_ctc 42.395958 loss_rnnt 28.706890 hw_loss 0.333901 lr 0.00069376 rank 7
2023-02-17 04:47:46,524 DEBUG TRAIN Batch 2/700 loss 63.448631 loss_att 84.605103 loss_ctc 85.202927 loss_rnnt 56.184227 hw_loss 0.248509 lr 0.00069628 rank 2
2023-02-17 04:47:46,526 DEBUG TRAIN Batch 2/700 loss 36.000015 loss_att 59.032715 loss_ctc 60.851044 loss_rnnt 27.937338 hw_loss 0.267508 lr 0.00069432 rank 4
2023-02-17 04:47:46,543 DEBUG TRAIN Batch 2/700 loss 51.682106 loss_att 79.709198 loss_ctc 83.260361 loss_rnnt 41.788425 hw_loss 0.145920 lr 0.00069632 rank 1
2023-02-17 04:47:46,552 DEBUG TRAIN Batch 2/700 loss 29.146179 loss_att 52.802612 loss_ctc 38.244698 loss_rnnt 23.045200 hw_loss 0.293540 lr 0.00069512 rank 0
2023-02-17 04:47:46,568 DEBUG TRAIN Batch 2/700 loss 21.960510 loss_att 32.674278 loss_ctc 32.645378 loss_rnnt 18.235989 hw_loss 0.294601 lr 0.00069496 rank 3
2023-02-17 04:47:46,573 DEBUG TRAIN Batch 2/700 loss 27.693987 loss_att 45.127365 loss_ctc 44.693703 loss_rnnt 21.831825 hw_loss 0.204109 lr 0.00069516 rank 6
2023-02-17 04:49:12,280 DEBUG TRAIN Batch 2/800 loss 34.397598 loss_att 50.643661 loss_ctc 52.536461 loss_rnnt 28.594555 hw_loss 0.253712 lr 0.00069916 rank 6
2023-02-17 04:49:12,282 DEBUG TRAIN Batch 2/800 loss 38.164688 loss_att 56.138138 loss_ctc 49.566986 loss_rnnt 32.903542 hw_loss 0.274027 lr 0.00069896 rank 3
2023-02-17 04:49:12,283 DEBUG TRAIN Batch 2/800 loss 33.821106 loss_att 47.397240 loss_ctc 51.478180 loss_rnnt 28.563896 hw_loss 0.351950 lr 0.00069912 rank 0
2023-02-17 04:49:12,285 DEBUG TRAIN Batch 2/800 loss 45.718250 loss_att 57.282791 loss_ctc 60.030037 loss_rnnt 41.353184 hw_loss 0.269847 lr 0.00069776 rank 7
2023-02-17 04:49:12,290 DEBUG TRAIN Batch 2/800 loss 53.849972 loss_att 68.765259 loss_ctc 74.950974 loss_rnnt 47.883614 hw_loss 0.318428 lr 0.00069832 rank 4
2023-02-17 04:49:12,314 DEBUG TRAIN Batch 2/800 loss 56.895462 loss_att 70.755493 loss_ctc 77.825874 loss_rnnt 51.179680 hw_loss 0.286981 lr 0.00069840 rank 5
2023-02-17 04:49:12,322 DEBUG TRAIN Batch 2/800 loss 47.628021 loss_att 71.454239 loss_ctc 67.166809 loss_rnnt 40.086975 hw_loss 0.319929 lr 0.00070028 rank 2
2023-02-17 04:49:12,361 DEBUG TRAIN Batch 2/800 loss 37.856686 loss_att 60.197525 loss_ctc 53.814178 loss_rnnt 31.085850 hw_loss 0.328134 lr 0.00070032 rank 1
2023-02-17 04:50:27,926 DEBUG TRAIN Batch 2/900 loss 44.345886 loss_att 53.887630 loss_ctc 59.686443 loss_rnnt 40.205296 hw_loss 0.350317 lr 0.00070240 rank 5
2023-02-17 04:50:27,927 DEBUG TRAIN Batch 2/900 loss 28.050844 loss_att 56.011299 loss_ctc 44.169991 loss_rnnt 20.115173 hw_loss 0.364425 lr 0.00070428 rank 2
2023-02-17 04:50:27,928 DEBUG TRAIN Batch 2/900 loss 47.217548 loss_att 65.187340 loss_ctc 64.811073 loss_rnnt 41.098080 hw_loss 0.336961 lr 0.00070316 rank 6
2023-02-17 04:50:27,933 DEBUG TRAIN Batch 2/900 loss 44.830803 loss_att 69.620598 loss_ctc 72.429314 loss_rnnt 35.986862 hw_loss 0.386578 lr 0.00070176 rank 7
2023-02-17 04:50:27,966 DEBUG TRAIN Batch 2/900 loss 55.015358 loss_att 79.225693 loss_ctc 72.043701 loss_rnnt 47.735867 hw_loss 0.313090 lr 0.00070432 rank 1
2023-02-17 04:50:27,976 DEBUG TRAIN Batch 2/900 loss 52.483505 loss_att 67.529808 loss_ctc 75.562561 loss_rnnt 46.235016 hw_loss 0.303790 lr 0.00070232 rank 4
2023-02-17 04:50:27,976 DEBUG TRAIN Batch 2/900 loss 34.203041 loss_att 45.342060 loss_ctc 46.125282 loss_rnnt 30.243469 hw_loss 0.266507 lr 0.00070312 rank 0
2023-02-17 04:50:27,986 DEBUG TRAIN Batch 2/900 loss 32.394470 loss_att 52.606102 loss_ctc 46.063278 loss_rnnt 26.414814 hw_loss 0.215288 lr 0.00070296 rank 3
2023-02-17 04:51:47,271 DEBUG TRAIN Batch 2/1000 loss 27.981960 loss_att 43.325085 loss_ctc 49.672825 loss_rnnt 21.864355 hw_loss 0.294125 lr 0.00070832 rank 1
2023-02-17 04:51:47,285 DEBUG TRAIN Batch 2/1000 loss 51.600159 loss_att 70.000092 loss_ctc 79.102249 loss_rnnt 44.138039 hw_loss 0.215967 lr 0.00070632 rank 4
2023-02-17 04:51:47,315 DEBUG TRAIN Batch 2/1000 loss 53.869350 loss_att 66.268745 loss_ctc 65.750610 loss_rnnt 49.688171 hw_loss 0.219625 lr 0.00070576 rank 7
2023-02-17 04:51:47,318 DEBUG TRAIN Batch 2/1000 loss 46.899929 loss_att 65.285522 loss_ctc 57.417587 loss_rnnt 41.686295 hw_loss 0.251547 lr 0.00070640 rank 5
2023-02-17 04:51:47,321 DEBUG TRAIN Batch 2/1000 loss 65.289261 loss_att 86.568359 loss_ctc 94.977753 loss_rnnt 56.934761 hw_loss 0.262891 lr 0.00070696 rank 3
2023-02-17 04:51:47,336 DEBUG TRAIN Batch 2/1000 loss 40.657898 loss_att 57.937832 loss_ctc 59.376411 loss_rnnt 34.546120 hw_loss 0.299975 lr 0.00070712 rank 0
2023-02-17 04:51:47,350 DEBUG TRAIN Batch 2/1000 loss 70.056885 loss_att 94.953224 loss_ctc 98.947121 loss_rnnt 61.085655 hw_loss 0.262382 lr 0.00070828 rank 2
2023-02-17 04:51:47,355 DEBUG TRAIN Batch 2/1000 loss 42.233376 loss_att 57.263420 loss_ctc 61.627605 loss_rnnt 36.521286 hw_loss 0.225333 lr 0.00070716 rank 6
2023-02-17 04:53:19,406 DEBUG TRAIN Batch 2/1100 loss 31.620657 loss_att 42.249481 loss_ctc 37.876892 loss_rnnt 28.495106 hw_loss 0.310541 lr 0.00071232 rank 1
2023-02-17 04:53:19,412 DEBUG TRAIN Batch 2/1100 loss 29.678762 loss_att 38.888306 loss_ctc 40.765587 loss_rnnt 26.220688 hw_loss 0.258603 lr 0.00071116 rank 6
2023-02-17 04:53:19,414 DEBUG TRAIN Batch 2/1100 loss 33.551804 loss_att 47.359310 loss_ctc 45.723381 loss_rnnt 29.041883 hw_loss 0.235391 lr 0.00071096 rank 3
2023-02-17 04:53:19,415 DEBUG TRAIN Batch 2/1100 loss 39.355728 loss_att 51.060795 loss_ctc 57.465485 loss_rnnt 34.428879 hw_loss 0.321011 lr 0.00071032 rank 4
2023-02-17 04:53:19,416 DEBUG TRAIN Batch 2/1100 loss 32.878941 loss_att 55.060741 loss_ctc 52.585548 loss_rnnt 25.656294 hw_loss 0.297637 lr 0.00071228 rank 2
2023-02-17 04:53:19,418 DEBUG TRAIN Batch 2/1100 loss 45.406216 loss_att 51.252762 loss_ctc 59.232414 loss_rnnt 42.228607 hw_loss 0.309006 lr 0.00071112 rank 0
2023-02-17 04:53:19,421 DEBUG TRAIN Batch 2/1100 loss 31.580093 loss_att 43.424297 loss_ctc 43.977505 loss_rnnt 27.411510 hw_loss 0.275162 lr 0.00071040 rank 5
2023-02-17 04:53:19,420 DEBUG TRAIN Batch 2/1100 loss 64.806679 loss_att 81.103493 loss_ctc 81.015991 loss_rnnt 59.214008 hw_loss 0.322618 lr 0.00070976 rank 7
2023-02-17 04:54:34,846 DEBUG TRAIN Batch 2/1200 loss 33.150974 loss_att 49.036045 loss_ctc 48.021267 loss_rnnt 27.816319 hw_loss 0.327995 lr 0.00071516 rank 6
2023-02-17 04:54:34,863 DEBUG TRAIN Batch 2/1200 loss 29.737473 loss_att 41.465820 loss_ctc 40.763275 loss_rnnt 25.793570 hw_loss 0.240234 lr 0.00071632 rank 1
2023-02-17 04:54:34,864 DEBUG TRAIN Batch 2/1200 loss 33.946999 loss_att 46.005516 loss_ctc 50.401691 loss_rnnt 29.178638 hw_loss 0.305049 lr 0.00071496 rank 3
2023-02-17 04:54:34,864 DEBUG TRAIN Batch 2/1200 loss 39.403557 loss_att 46.308769 loss_ctc 55.451260 loss_rnnt 35.713188 hw_loss 0.318057 lr 0.00071440 rank 5
2023-02-17 04:54:34,867 DEBUG TRAIN Batch 2/1200 loss 34.839325 loss_att 46.271111 loss_ctc 42.901623 loss_rnnt 31.325367 hw_loss 0.286167 lr 0.00071628 rank 2
2023-02-17 04:54:34,867 DEBUG TRAIN Batch 2/1200 loss 33.557953 loss_att 43.215263 loss_ctc 43.023521 loss_rnnt 30.207655 hw_loss 0.293925 lr 0.00071376 rank 7
2023-02-17 04:54:34,872 DEBUG TRAIN Batch 2/1200 loss 31.663040 loss_att 42.975830 loss_ctc 49.582382 loss_rnnt 26.824995 hw_loss 0.349201 lr 0.00071432 rank 4
2023-02-17 04:54:34,911 DEBUG TRAIN Batch 2/1200 loss 25.043930 loss_att 30.693808 loss_ctc 35.041805 loss_rnnt 22.379185 hw_loss 0.378224 lr 0.00071512 rank 0
2023-02-17 04:55:50,215 DEBUG TRAIN Batch 2/1300 loss 44.560936 loss_att 67.384399 loss_ctc 60.118195 loss_rnnt 37.824120 hw_loss 0.183409 lr 0.00072028 rank 2
2023-02-17 04:55:50,227 DEBUG TRAIN Batch 2/1300 loss 21.355246 loss_att 29.569611 loss_ctc 29.774292 loss_rnnt 18.473972 hw_loss 0.217238 lr 0.00071916 rank 6
2023-02-17 04:55:50,230 DEBUG TRAIN Batch 2/1300 loss 42.454735 loss_att 60.792385 loss_ctc 60.330257 loss_rnnt 36.251701 hw_loss 0.285188 lr 0.00071776 rank 7
2023-02-17 04:55:50,233 DEBUG TRAIN Batch 2/1300 loss 29.117718 loss_att 50.673000 loss_ctc 49.987026 loss_rnnt 21.900152 hw_loss 0.232377 lr 0.00071896 rank 3
2023-02-17 04:55:50,233 DEBUG TRAIN Batch 2/1300 loss 29.344475 loss_att 49.245468 loss_ctc 41.275795 loss_rnnt 23.684813 hw_loss 0.166162 lr 0.00071840 rank 5
2023-02-17 04:55:50,292 DEBUG TRAIN Batch 2/1300 loss 46.285400 loss_att 67.641335 loss_ctc 65.460060 loss_rnnt 39.328377 hw_loss 0.242269 lr 0.00071912 rank 0
2023-02-17 04:55:50,298 DEBUG TRAIN Batch 2/1300 loss 40.408798 loss_att 71.864258 loss_ctc 57.262009 loss_rnnt 31.725273 hw_loss 0.272500 lr 0.00071832 rank 4
2023-02-17 04:55:50,334 DEBUG TRAIN Batch 2/1300 loss 30.215151 loss_att 49.513466 loss_ctc 46.256420 loss_rnnt 24.066145 hw_loss 0.282195 lr 0.00072032 rank 1
2023-02-17 04:57:14,183 DEBUG TRAIN Batch 2/1400 loss 47.361061 loss_att 73.982925 loss_ctc 64.801834 loss_rnnt 39.492828 hw_loss 0.409548 lr 0.00072232 rank 4
2023-02-17 04:57:14,184 DEBUG TRAIN Batch 2/1400 loss 25.628693 loss_att 41.875744 loss_ctc 37.156509 loss_rnnt 20.721516 hw_loss 0.226356 lr 0.00072432 rank 1
2023-02-17 04:57:14,186 DEBUG TRAIN Batch 2/1400 loss 57.229652 loss_att 81.066330 loss_ctc 81.169273 loss_rnnt 49.103806 hw_loss 0.312295 lr 0.00072316 rank 6
2023-02-17 04:57:14,194 DEBUG TRAIN Batch 2/1400 loss 17.588022 loss_att 25.746756 loss_ctc 26.706440 loss_rnnt 14.571760 hw_loss 0.316361 lr 0.00072176 rank 7
2023-02-17 04:57:14,198 DEBUG TRAIN Batch 2/1400 loss 54.202652 loss_att 74.991798 loss_ctc 88.843201 loss_rnnt 45.269073 hw_loss 0.294400 lr 0.00072428 rank 2
2023-02-17 04:57:14,219 DEBUG TRAIN Batch 2/1400 loss 60.575432 loss_att 95.039810 loss_ctc 83.777802 loss_rnnt 50.437126 hw_loss 0.284597 lr 0.00072296 rank 3
2023-02-17 04:57:14,219 DEBUG TRAIN Batch 2/1400 loss 40.484207 loss_att 56.175011 loss_ctc 55.754196 loss_rnnt 35.180305 hw_loss 0.243265 lr 0.00072240 rank 5
2023-02-17 04:57:14,228 DEBUG TRAIN Batch 2/1400 loss 57.803673 loss_att 66.141159 loss_ctc 74.068466 loss_rnnt 53.809715 hw_loss 0.295929 lr 0.00072312 rank 0
2023-02-17 04:58:40,345 DEBUG TRAIN Batch 2/1500 loss 42.996071 loss_att 62.355934 loss_ctc 56.283566 loss_rnnt 37.217823 hw_loss 0.252386 lr 0.00072828 rank 2
2023-02-17 04:58:40,354 DEBUG TRAIN Batch 2/1500 loss 84.377388 loss_att 110.408173 loss_ctc 112.069305 loss_rnnt 75.328644 hw_loss 0.281865 lr 0.00072576 rank 7
2023-02-17 04:58:40,355 DEBUG TRAIN Batch 2/1500 loss 41.638344 loss_att 62.569397 loss_ctc 58.059891 loss_rnnt 35.138653 hw_loss 0.232393 lr 0.00072696 rank 3
2023-02-17 04:58:40,354 DEBUG TRAIN Batch 2/1500 loss 49.123524 loss_att 73.046097 loss_ctc 68.509422 loss_rnnt 41.613678 hw_loss 0.263527 lr 0.00072716 rank 6
2023-02-17 04:58:40,357 DEBUG TRAIN Batch 2/1500 loss 33.826859 loss_att 52.007732 loss_ctc 48.242321 loss_rnnt 28.113693 hw_loss 0.290489 lr 0.00072640 rank 5
2023-02-17 04:58:40,359 DEBUG TRAIN Batch 2/1500 loss 53.444714 loss_att 67.023903 loss_ctc 62.892956 loss_rnnt 49.302544 hw_loss 0.312316 lr 0.00072832 rank 1
2023-02-17 04:58:40,358 DEBUG TRAIN Batch 2/1500 loss 54.246044 loss_att 75.738281 loss_ctc 70.488274 loss_rnnt 47.696026 hw_loss 0.161145 lr 0.00072632 rank 4
2023-02-17 04:58:40,361 DEBUG TRAIN Batch 2/1500 loss 47.393482 loss_att 55.740673 loss_ctc 68.227852 loss_rnnt 42.776909 hw_loss 0.317296 lr 0.00072712 rank 0
2023-02-17 04:59:55,407 DEBUG TRAIN Batch 2/1600 loss 53.323421 loss_att 77.714630 loss_ctc 85.433762 loss_rnnt 43.992645 hw_loss 0.320911 lr 0.00073116 rank 6
2023-02-17 04:59:55,410 DEBUG TRAIN Batch 2/1600 loss 33.176506 loss_att 50.283302 loss_ctc 48.375526 loss_rnnt 27.603062 hw_loss 0.235407 lr 0.00073112 rank 0
2023-02-17 04:59:55,414 DEBUG TRAIN Batch 2/1600 loss 36.481457 loss_att 51.902519 loss_ctc 51.357231 loss_rnnt 31.278137 hw_loss 0.254379 lr 0.00073040 rank 5
2023-02-17 04:59:55,415 DEBUG TRAIN Batch 2/1600 loss 44.286686 loss_att 65.445892 loss_ctc 62.918602 loss_rnnt 37.478619 hw_loss 0.172450 lr 0.00073096 rank 3
2023-02-17 04:59:55,418 DEBUG TRAIN Batch 2/1600 loss 69.683876 loss_att 92.048111 loss_ctc 96.536179 loss_rnnt 61.508686 hw_loss 0.228806 lr 0.00073228 rank 2
2023-02-17 04:59:55,421 DEBUG TRAIN Batch 2/1600 loss 59.988190 loss_att 69.966187 loss_ctc 73.615448 loss_rnnt 55.998108 hw_loss 0.332830 lr 0.00072976 rank 7
2023-02-17 04:59:55,439 DEBUG TRAIN Batch 2/1600 loss 40.061951 loss_att 58.713951 loss_ctc 63.550522 loss_rnnt 33.029339 hw_loss 0.319496 lr 0.00073032 rank 4
2023-02-17 04:59:55,449 DEBUG TRAIN Batch 2/1600 loss 37.761108 loss_att 52.732304 loss_ctc 52.871979 loss_rnnt 32.593498 hw_loss 0.297349 lr 0.00073232 rank 1
2023-02-17 05:01:12,881 DEBUG TRAIN Batch 2/1700 loss 30.695469 loss_att 41.374794 loss_ctc 41.638168 loss_rnnt 26.969843 hw_loss 0.245127 lr 0.00073628 rank 2
2023-02-17 05:01:12,885 DEBUG TRAIN Batch 2/1700 loss 43.841095 loss_att 56.384510 loss_ctc 63.240112 loss_rnnt 38.562923 hw_loss 0.343038 lr 0.00073512 rank 0
2023-02-17 05:01:12,904 DEBUG TRAIN Batch 2/1700 loss 43.149662 loss_att 64.217148 loss_ctc 58.691265 loss_rnnt 36.758804 hw_loss 0.197149 lr 0.00073432 rank 4
2023-02-17 05:01:12,914 DEBUG TRAIN Batch 2/1700 loss 30.890066 loss_att 43.311253 loss_ctc 45.997093 loss_rnnt 26.239151 hw_loss 0.285764 lr 0.00073516 rank 6
2023-02-17 05:01:12,917 DEBUG TRAIN Batch 2/1700 loss 44.695442 loss_att 54.892498 loss_ctc 60.701656 loss_rnnt 40.399807 hw_loss 0.228868 lr 0.00073376 rank 7
2023-02-17 05:01:12,923 DEBUG TRAIN Batch 2/1700 loss 51.417847 loss_att 66.332687 loss_ctc 75.183022 loss_rnnt 45.164925 hw_loss 0.189860 lr 0.00073632 rank 1
2023-02-17 05:01:12,953 DEBUG TRAIN Batch 2/1700 loss 37.685940 loss_att 53.301437 loss_ctc 47.730053 loss_rnnt 33.044884 hw_loss 0.335145 lr 0.00073440 rank 5
2023-02-17 05:01:12,959 DEBUG TRAIN Batch 2/1700 loss 57.124081 loss_att 68.347458 loss_ctc 78.182892 loss_rnnt 51.867630 hw_loss 0.382375 lr 0.00073496 rank 3
2023-02-17 05:02:47,110 DEBUG TRAIN Batch 2/1800 loss 35.101826 loss_att 44.080364 loss_ctc 47.687317 loss_rnnt 31.470732 hw_loss 0.294982 lr 0.00073896 rank 3
2023-02-17 05:02:47,111 DEBUG TRAIN Batch 2/1800 loss 41.245659 loss_att 46.742722 loss_ctc 61.944580 loss_rnnt 37.248928 hw_loss 0.257736 lr 0.00073912 rank 0
2023-02-17 05:02:47,113 DEBUG TRAIN Batch 2/1800 loss 24.834614 loss_att 35.055561 loss_ctc 36.739185 loss_rnnt 21.089064 hw_loss 0.213910 lr 0.00074032 rank 1
2023-02-17 05:02:47,115 DEBUG TRAIN Batch 2/1800 loss 37.998482 loss_att 53.188908 loss_ctc 51.369629 loss_rnnt 33.007290 hw_loss 0.319292 lr 0.00073916 rank 6
2023-02-17 05:02:47,116 DEBUG TRAIN Batch 2/1800 loss 34.589699 loss_att 40.928371 loss_ctc 46.436848 loss_rnnt 31.624537 hw_loss 0.220888 lr 0.00073840 rank 5
2023-02-17 05:02:47,119 DEBUG TRAIN Batch 2/1800 loss 52.236340 loss_att 71.536865 loss_ctc 76.696938 loss_rnnt 44.994591 hw_loss 0.225443 lr 0.00073776 rank 7
2023-02-17 05:02:47,131 DEBUG TRAIN Batch 2/1800 loss 39.096027 loss_att 52.254871 loss_ctc 54.573330 loss_rnnt 34.248226 hw_loss 0.285739 lr 0.00074028 rank 2
2023-02-17 05:02:47,139 DEBUG TRAIN Batch 2/1800 loss 39.943195 loss_att 53.428787 loss_ctc 54.043129 loss_rnnt 35.207729 hw_loss 0.296926 lr 0.00073832 rank 4
2023-02-17 05:04:03,037 DEBUG TRAIN Batch 2/1900 loss 26.530050 loss_att 41.530426 loss_ctc 37.196682 loss_rnnt 22.060347 hw_loss 0.088897 lr 0.00074312 rank 0
2023-02-17 05:04:03,037 DEBUG TRAIN Batch 2/1900 loss 43.102463 loss_att 50.009655 loss_ctc 58.097862 loss_rnnt 39.611687 hw_loss 0.206157 lr 0.00074316 rank 6
2023-02-17 05:04:03,037 DEBUG TRAIN Batch 2/1900 loss 33.797951 loss_att 37.250969 loss_ctc 42.539967 loss_rnnt 31.709915 hw_loss 0.434679 lr 0.00074296 rank 3
2023-02-17 05:04:03,038 DEBUG TRAIN Batch 2/1900 loss 31.265337 loss_att 46.014851 loss_ctc 43.314362 loss_rnnt 26.659056 hw_loss 0.093453 lr 0.00074232 rank 4
2023-02-17 05:04:03,039 DEBUG TRAIN Batch 2/1900 loss 60.573353 loss_att 89.017349 loss_ctc 93.124527 loss_rnnt 50.386398 hw_loss 0.296243 lr 0.00074428 rank 2
2023-02-17 05:04:03,039 DEBUG TRAIN Batch 2/1900 loss 33.727585 loss_att 57.640778 loss_ctc 53.771526 loss_rnnt 26.111403 hw_loss 0.301907 lr 0.00074240 rank 5
2023-02-17 05:04:03,040 DEBUG TRAIN Batch 2/1900 loss 23.446568 loss_att 26.126795 loss_ctc 33.025208 loss_rnnt 21.420473 hw_loss 0.399177 lr 0.00074176 rank 7
2023-02-17 05:04:03,071 DEBUG TRAIN Batch 2/1900 loss 42.149441 loss_att 61.402653 loss_ctc 57.222652 loss_rnnt 36.163361 hw_loss 0.235638 lr 0.00074432 rank 1
2023-02-17 05:05:17,867 DEBUG TRAIN Batch 2/2000 loss 57.834354 loss_att 80.594070 loss_ctc 89.045349 loss_rnnt 48.972088 hw_loss 0.279115 lr 0.00074716 rank 6
2023-02-17 05:05:17,868 DEBUG TRAIN Batch 2/2000 loss 36.117706 loss_att 57.984421 loss_ctc 45.174286 loss_rnnt 30.410049 hw_loss 0.237697 lr 0.00074712 rank 0
2023-02-17 05:05:17,870 DEBUG TRAIN Batch 2/2000 loss 24.974773 loss_att 36.634098 loss_ctc 33.589706 loss_rnnt 21.358908 hw_loss 0.253769 lr 0.00074640 rank 5
2023-02-17 05:05:17,873 DEBUG TRAIN Batch 2/2000 loss 36.554611 loss_att 45.140388 loss_ctc 51.987514 loss_rnnt 32.610378 hw_loss 0.317543 lr 0.00074696 rank 3
2023-02-17 05:05:17,875 DEBUG TRAIN Batch 2/2000 loss 33.668961 loss_att 47.114452 loss_ctc 47.667114 loss_rnnt 28.953041 hw_loss 0.300748 lr 0.00074576 rank 7
2023-02-17 05:05:17,874 DEBUG TRAIN Batch 2/2000 loss 27.887411 loss_att 47.478947 loss_ctc 47.524101 loss_rnnt 21.201399 hw_loss 0.280270 lr 0.00074632 rank 4
2023-02-17 05:05:17,905 DEBUG TRAIN Batch 2/2000 loss 31.697039 loss_att 39.406544 loss_ctc 56.632030 loss_rnnt 26.700918 hw_loss 0.242909 lr 0.00074832 rank 1
2023-02-17 05:05:17,915 DEBUG TRAIN Batch 2/2000 loss 43.614025 loss_att 67.978981 loss_ctc 61.366879 loss_rnnt 36.229675 hw_loss 0.270585 lr 0.00074828 rank 2
2023-02-17 05:06:40,640 DEBUG TRAIN Batch 2/2100 loss 46.931297 loss_att 60.210026 loss_ctc 66.261353 loss_rnnt 41.543709 hw_loss 0.289689 lr 0.00075112 rank 0
2023-02-17 05:06:40,653 DEBUG TRAIN Batch 2/2100 loss 40.788891 loss_att 55.403679 loss_ctc 64.799789 loss_rnnt 34.525932 hw_loss 0.259775 lr 0.00075116 rank 6
2023-02-17 05:06:40,669 DEBUG TRAIN Batch 2/2100 loss 33.610580 loss_att 46.880253 loss_ctc 44.252861 loss_rnnt 29.381542 hw_loss 0.292751 lr 0.00075232 rank 1
2023-02-17 05:06:40,672 DEBUG TRAIN Batch 2/2100 loss 28.917553 loss_att 40.619606 loss_ctc 39.971542 loss_rnnt 24.957390 hw_loss 0.273536 lr 0.00075096 rank 3
2023-02-17 05:06:40,724 DEBUG TRAIN Batch 2/2100 loss 40.731701 loss_att 59.969841 loss_ctc 68.308434 loss_rnnt 33.027756 hw_loss 0.336413 lr 0.00075040 rank 5
2023-02-17 05:06:40,725 DEBUG TRAIN Batch 2/2100 loss 32.130951 loss_att 47.587734 loss_ctc 48.442207 loss_rnnt 26.731863 hw_loss 0.249186 lr 0.00074976 rank 7
2023-02-17 05:06:40,725 DEBUG TRAIN Batch 2/2100 loss 24.529591 loss_att 34.762871 loss_ctc 38.083141 loss_rnnt 20.520687 hw_loss 0.290823 lr 0.00075228 rank 2
2023-02-17 05:06:40,733 DEBUG TRAIN Batch 2/2100 loss 29.408680 loss_att 41.441147 loss_ctc 37.085686 loss_rnnt 25.789898 hw_loss 0.353793 lr 0.00075032 rank 4
2023-02-17 05:08:10,531 DEBUG TRAIN Batch 2/2200 loss 59.596962 loss_att 72.951233 loss_ctc 78.436401 loss_rnnt 54.258751 hw_loss 0.291444 lr 0.00075632 rank 1
2023-02-17 05:08:10,539 DEBUG TRAIN Batch 2/2200 loss 59.404686 loss_att 68.973740 loss_ctc 81.956459 loss_rnnt 54.365974 hw_loss 0.221236 lr 0.00075440 rank 5
2023-02-17 05:08:10,540 DEBUG TRAIN Batch 2/2200 loss 35.204319 loss_att 50.404369 loss_ctc 49.182022 loss_rnnt 30.191191 hw_loss 0.205162 lr 0.00075432 rank 4
2023-02-17 05:08:10,544 DEBUG TRAIN Batch 2/2200 loss 41.906307 loss_att 64.067024 loss_ctc 59.670670 loss_rnnt 34.960297 hw_loss 0.272404 lr 0.00075628 rank 2
2023-02-17 05:08:10,545 DEBUG TRAIN Batch 2/2200 loss 60.014641 loss_att 77.787949 loss_ctc 78.241074 loss_rnnt 53.869942 hw_loss 0.299699 lr 0.00075376 rank 7
2023-02-17 05:08:10,547 DEBUG TRAIN Batch 2/2200 loss 31.092709 loss_att 47.022583 loss_ctc 41.103577 loss_rnnt 26.412212 hw_loss 0.299509 lr 0.00075496 rank 3
2023-02-17 05:08:10,552 DEBUG TRAIN Batch 2/2200 loss 31.689917 loss_att 43.870434 loss_ctc 48.393272 loss_rnnt 26.932236 hw_loss 0.177121 lr 0.00075516 rank 6
2023-02-17 05:08:10,554 DEBUG TRAIN Batch 2/2200 loss 59.929825 loss_att 75.030685 loss_ctc 80.510330 loss_rnnt 54.006325 hw_loss 0.298601 lr 0.00075512 rank 0
2023-02-17 05:09:25,915 DEBUG TRAIN Batch 2/2300 loss 26.572899 loss_att 38.283714 loss_ctc 32.607422 loss_rnnt 23.298313 hw_loss 0.239660 lr 0.00075896 rank 3
2023-02-17 05:09:25,915 DEBUG TRAIN Batch 2/2300 loss 23.521994 loss_att 34.332039 loss_ctc 35.024658 loss_rnnt 19.721422 hw_loss 0.196632 lr 0.00076032 rank 1
2023-02-17 05:09:25,916 DEBUG TRAIN Batch 2/2300 loss 36.516186 loss_att 52.648403 loss_ctc 51.655968 loss_rnnt 31.110107 hw_loss 0.301870 lr 0.00076028 rank 2
2023-02-17 05:09:25,919 DEBUG TRAIN Batch 2/2300 loss 47.552017 loss_att 67.130272 loss_ctc 72.253929 loss_rnnt 40.197227 hw_loss 0.272899 lr 0.00075832 rank 4
2023-02-17 05:09:25,921 DEBUG TRAIN Batch 2/2300 loss 41.445541 loss_att 57.134384 loss_ctc 62.501526 loss_rnnt 35.373127 hw_loss 0.238456 lr 0.00075776 rank 7
2023-02-17 05:09:25,922 DEBUG TRAIN Batch 2/2300 loss 42.190819 loss_att 57.830284 loss_ctc 60.154457 loss_rnnt 36.530045 hw_loss 0.258233 lr 0.00075840 rank 5
2023-02-17 05:09:25,952 DEBUG TRAIN Batch 2/2300 loss 29.113024 loss_att 45.351410 loss_ctc 42.269127 loss_rnnt 23.971939 hw_loss 0.261112 lr 0.00075912 rank 0
2023-02-17 05:09:25,955 DEBUG TRAIN Batch 2/2300 loss 51.314457 loss_att 76.214478 loss_ctc 73.377335 loss_rnnt 43.240253 hw_loss 0.285903 lr 0.00075916 rank 6
2023-02-17 05:10:43,354 DEBUG TRAIN Batch 2/2400 loss 53.950226 loss_att 64.898392 loss_ctc 66.157028 loss_rnnt 49.994972 hw_loss 0.258827 lr 0.00076232 rank 4
2023-02-17 05:10:43,361 DEBUG TRAIN Batch 2/2400 loss 40.471546 loss_att 56.490604 loss_ctc 59.978867 loss_rnnt 34.488930 hw_loss 0.333431 lr 0.00076316 rank 6
2023-02-17 05:10:43,366 DEBUG TRAIN Batch 2/2400 loss 36.604179 loss_att 48.340378 loss_ctc 51.823353 loss_rnnt 32.117432 hw_loss 0.206786 lr 0.00076176 rank 7
2023-02-17 05:10:43,399 DEBUG TRAIN Batch 2/2400 loss 31.197927 loss_att 43.813248 loss_ctc 42.757946 loss_rnnt 27.005909 hw_loss 0.239283 lr 0.00076312 rank 0
2023-02-17 05:10:43,403 DEBUG TRAIN Batch 2/2400 loss 39.179459 loss_att 52.969505 loss_ctc 52.175529 loss_rnnt 34.585098 hw_loss 0.194141 lr 0.00076296 rank 3
2023-02-17 05:10:43,404 DEBUG TRAIN Batch 2/2400 loss 31.838146 loss_att 45.549671 loss_ctc 46.181740 loss_rnnt 27.019217 hw_loss 0.307777 lr 0.00076432 rank 1
2023-02-17 05:10:43,406 DEBUG TRAIN Batch 2/2400 loss 25.267841 loss_att 38.884850 loss_ctc 33.854599 loss_rnnt 21.239338 hw_loss 0.300374 lr 0.00076428 rank 2
2023-02-17 05:10:43,407 DEBUG TRAIN Batch 2/2400 loss 40.354061 loss_att 52.820457 loss_ctc 56.532799 loss_rnnt 35.581795 hw_loss 0.228418 lr 0.00076240 rank 5
2023-02-17 05:12:19,767 DEBUG TRAIN Batch 2/2500 loss 22.207371 loss_att 28.583492 loss_ctc 36.463490 loss_rnnt 18.912640 hw_loss 0.222540 lr 0.00076696 rank 3
2023-02-17 05:12:19,770 DEBUG TRAIN Batch 2/2500 loss 28.911465 loss_att 34.785778 loss_ctc 39.171108 loss_rnnt 26.161581 hw_loss 0.388257 lr 0.00076640 rank 5
2023-02-17 05:12:19,772 DEBUG TRAIN Batch 2/2500 loss 42.850365 loss_att 48.060184 loss_ctc 50.463715 loss_rnnt 40.618385 hw_loss 0.327938 lr 0.00076576 rank 7
2023-02-17 05:12:19,773 DEBUG TRAIN Batch 2/2500 loss 29.773466 loss_att 35.524220 loss_ctc 34.787712 loss_rnnt 27.802151 hw_loss 0.286126 lr 0.00076712 rank 0
2023-02-17 05:12:19,773 DEBUG TRAIN Batch 2/2500 loss 24.808619 loss_att 28.484756 loss_ctc 34.614986 loss_rnnt 22.602087 hw_loss 0.307109 lr 0.00076832 rank 1
2023-02-17 05:12:19,774 DEBUG TRAIN Batch 2/2500 loss 31.408379 loss_att 39.833351 loss_ctc 47.126617 loss_rnnt 27.468275 hw_loss 0.298770 lr 0.00076716 rank 6
2023-02-17 05:12:19,786 DEBUG TRAIN Batch 2/2500 loss 18.043161 loss_att 27.148869 loss_ctc 22.910431 loss_rnnt 15.496531 hw_loss 0.143474 lr 0.00076632 rank 4
2023-02-17 05:12:19,802 DEBUG TRAIN Batch 2/2500 loss 24.855923 loss_att 30.714678 loss_ctc 35.374458 loss_rnnt 22.054731 hw_loss 0.425570 lr 0.00076828 rank 2
2023-02-17 05:13:36,790 DEBUG TRAIN Batch 2/2600 loss 30.675186 loss_att 47.516682 loss_ctc 43.719635 loss_rnnt 25.435417 hw_loss 0.247894 lr 0.00077116 rank 6
2023-02-17 05:13:36,791 DEBUG TRAIN Batch 2/2600 loss 32.450672 loss_att 47.331066 loss_ctc 51.038784 loss_rnnt 26.803474 hw_loss 0.361323 lr 0.00077228 rank 2
2023-02-17 05:13:36,791 DEBUG TRAIN Batch 2/2600 loss 36.428181 loss_att 60.434158 loss_ctc 48.547802 loss_rnnt 29.848303 hw_loss 0.305129 lr 0.00077032 rank 4
2023-02-17 05:13:36,792 DEBUG TRAIN Batch 2/2600 loss 27.975700 loss_att 37.453846 loss_ctc 40.897442 loss_rnnt 24.186478 hw_loss 0.320051 lr 0.00076976 rank 7
2023-02-17 05:13:36,833 DEBUG TRAIN Batch 2/2600 loss 45.338566 loss_att 58.327671 loss_ctc 65.916374 loss_rnnt 39.781857 hw_loss 0.403470 lr 0.00077232 rank 1
2023-02-17 05:13:36,838 DEBUG TRAIN Batch 2/2600 loss 40.262428 loss_att 56.565269 loss_ctc 61.844566 loss_rnnt 33.962452 hw_loss 0.303356 lr 0.00077096 rank 3
2023-02-17 05:13:36,858 DEBUG TRAIN Batch 2/2600 loss 40.580566 loss_att 58.478462 loss_ctc 66.072441 loss_rnnt 33.413349 hw_loss 0.353851 lr 0.00077040 rank 5
2023-02-17 05:13:36,867 DEBUG TRAIN Batch 2/2600 loss 56.610386 loss_att 68.014854 loss_ctc 77.598587 loss_rnnt 51.365536 hw_loss 0.310374 lr 0.00077112 rank 0
2023-02-17 05:14:54,321 DEBUG TRAIN Batch 2/2700 loss 42.187374 loss_att 60.644714 loss_ctc 58.823303 loss_rnnt 36.187305 hw_loss 0.169644 lr 0.00077516 rank 6
2023-02-17 05:14:54,337 DEBUG TRAIN Batch 2/2700 loss 63.252201 loss_att 96.617668 loss_ctc 89.478905 loss_rnnt 52.987423 hw_loss 0.177740 lr 0.00077496 rank 3
2023-02-17 05:14:54,337 DEBUG TRAIN Batch 2/2700 loss 45.687237 loss_att 54.031723 loss_ctc 61.367367 loss_rnnt 41.824375 hw_loss 0.193642 lr 0.00077512 rank 0
2023-02-17 05:14:54,336 DEBUG TRAIN Batch 2/2700 loss 18.721750 loss_att 32.042976 loss_ctc 28.508961 loss_rnnt 14.591589 hw_loss 0.301790 lr 0.00077376 rank 7
2023-02-17 05:14:54,342 DEBUG TRAIN Batch 2/2700 loss 34.381859 loss_att 50.914196 loss_ctc 55.835079 loss_rnnt 28.038092 hw_loss 0.331632 lr 0.00077432 rank 4
2023-02-17 05:14:54,372 DEBUG TRAIN Batch 2/2700 loss 40.451431 loss_att 52.811230 loss_ctc 54.090759 loss_rnnt 36.061096 hw_loss 0.187116 lr 0.00077632 rank 1
2023-02-17 05:14:54,377 DEBUG TRAIN Batch 2/2700 loss 35.189014 loss_att 41.885513 loss_ctc 43.428940 loss_rnnt 32.634613 hw_loss 0.218337 lr 0.00077628 rank 2
2023-02-17 05:14:54,381 DEBUG TRAIN Batch 2/2700 loss 41.630047 loss_att 54.732246 loss_ctc 63.354855 loss_rnnt 36.037037 hw_loss 0.142357 lr 0.00077440 rank 5
2023-02-17 05:16:17,044 DEBUG TRAIN Batch 2/2800 loss 36.949894 loss_att 51.458618 loss_ctc 59.241375 loss_rnnt 30.957739 hw_loss 0.221657 lr 0.00077916 rank 6
2023-02-17 05:16:17,049 DEBUG TRAIN Batch 2/2800 loss 42.931541 loss_att 55.221870 loss_ctc 59.581402 loss_rnnt 38.076050 hw_loss 0.332697 lr 0.00077832 rank 4
2023-02-17 05:16:17,050 DEBUG TRAIN Batch 2/2800 loss 44.212433 loss_att 55.447865 loss_ctc 60.779942 loss_rnnt 39.575287 hw_loss 0.339480 lr 0.00077840 rank 5
2023-02-17 05:16:17,057 DEBUG TRAIN Batch 2/2800 loss 56.302162 loss_att 67.446083 loss_ctc 78.169365 loss_rnnt 51.015862 hw_loss 0.266045 lr 0.00078032 rank 1
2023-02-17 05:16:17,085 DEBUG TRAIN Batch 2/2800 loss 22.873167 loss_att 33.736683 loss_ctc 39.011139 loss_rnnt 18.355003 hw_loss 0.363247 lr 0.00077776 rank 7
2023-02-17 05:16:17,107 DEBUG TRAIN Batch 2/2800 loss 37.282627 loss_att 54.399208 loss_ctc 56.163872 loss_rnnt 31.244778 hw_loss 0.181946 lr 0.00077912 rank 0
2023-02-17 05:16:17,131 DEBUG TRAIN Batch 2/2800 loss 18.921423 loss_att 29.110470 loss_ctc 23.061798 loss_rnnt 16.178083 hw_loss 0.287774 lr 0.00078028 rank 2
2023-02-17 05:16:17,133 DEBUG TRAIN Batch 2/2800 loss 58.844646 loss_att 70.935806 loss_ctc 85.452255 loss_rnnt 52.686905 hw_loss 0.359681 lr 0.00077896 rank 3
2023-02-17 05:17:45,595 DEBUG TRAIN Batch 2/2900 loss 55.632107 loss_att 74.144684 loss_ctc 73.054123 loss_rnnt 49.481857 hw_loss 0.233996 lr 0.00078316 rank 6
2023-02-17 05:17:45,597 DEBUG TRAIN Batch 2/2900 loss 67.714302 loss_att 89.180344 loss_ctc 91.426132 loss_rnnt 60.109303 hw_loss 0.281664 lr 0.00078296 rank 3
2023-02-17 05:17:45,597 DEBUG TRAIN Batch 2/2900 loss 31.475460 loss_att 38.928337 loss_ctc 37.112114 loss_rnnt 29.062401 hw_loss 0.320493 lr 0.00078428 rank 2
2023-02-17 05:17:45,598 DEBUG TRAIN Batch 2/2900 loss 31.375223 loss_att 51.363876 loss_ctc 47.819214 loss_rnnt 25.029251 hw_loss 0.291954 lr 0.00078432 rank 1
2023-02-17 05:17:45,599 DEBUG TRAIN Batch 2/2900 loss 46.075500 loss_att 53.408592 loss_ctc 71.285362 loss_rnnt 41.064518 hw_loss 0.343212 lr 0.00078312 rank 0
2023-02-17 05:17:45,601 DEBUG TRAIN Batch 2/2900 loss 45.910458 loss_att 56.213936 loss_ctc 55.064560 loss_rnnt 42.487560 hw_loss 0.265610 lr 0.00078240 rank 5
2023-02-17 05:17:45,627 DEBUG TRAIN Batch 2/2900 loss 34.558022 loss_att 49.259949 loss_ctc 47.523239 loss_rnnt 29.697451 hw_loss 0.359037 lr 0.00078176 rank 7
2023-02-17 05:17:45,639 DEBUG TRAIN Batch 2/2900 loss 40.066578 loss_att 59.108215 loss_ctc 58.741364 loss_rnnt 33.599827 hw_loss 0.315844 lr 0.00078232 rank 4
2023-02-17 05:19:00,651 DEBUG TRAIN Batch 2/3000 loss 54.068035 loss_att 68.955185 loss_ctc 78.315948 loss_rnnt 47.699780 hw_loss 0.295824 lr 0.00078712 rank 0
2023-02-17 05:19:00,651 DEBUG TRAIN Batch 2/3000 loss 49.569546 loss_att 59.325737 loss_ctc 74.473236 loss_rnnt 44.176186 hw_loss 0.228049 lr 0.00078828 rank 2
2023-02-17 05:19:00,651 DEBUG TRAIN Batch 2/3000 loss 39.557758 loss_att 50.245426 loss_ctc 58.572838 loss_rnnt 34.748711 hw_loss 0.255318 lr 0.00078576 rank 7
2023-02-17 05:19:00,652 DEBUG TRAIN Batch 2/3000 loss 22.580666 loss_att 32.907433 loss_ctc 33.990940 loss_rnnt 18.856783 hw_loss 0.257172 lr 0.00078716 rank 6
2023-02-17 05:19:00,653 DEBUG TRAIN Batch 2/3000 loss 42.988899 loss_att 49.741566 loss_ctc 52.108925 loss_rnnt 40.310108 hw_loss 0.210477 lr 0.00078632 rank 4
2023-02-17 05:19:00,655 DEBUG TRAIN Batch 2/3000 loss 35.612106 loss_att 49.223164 loss_ctc 48.516525 loss_rnnt 30.954144 hw_loss 0.403430 lr 0.00078696 rank 3
2023-02-17 05:19:00,658 DEBUG TRAIN Batch 2/3000 loss 31.903191 loss_att 38.356228 loss_ctc 41.586201 loss_rnnt 29.155979 hw_loss 0.310372 lr 0.00078640 rank 5
2023-02-17 05:19:00,658 DEBUG TRAIN Batch 2/3000 loss 57.937153 loss_att 70.726944 loss_ctc 87.256790 loss_rnnt 51.323784 hw_loss 0.273994 lr 0.00078832 rank 1
2023-02-17 05:20:16,888 DEBUG TRAIN Batch 2/3100 loss 40.561520 loss_att 53.003971 loss_ctc 51.590237 loss_rnnt 36.423946 hw_loss 0.334853 lr 0.00079228 rank 2
2023-02-17 05:20:16,890 DEBUG TRAIN Batch 2/3100 loss 28.682457 loss_att 35.675457 loss_ctc 42.422268 loss_rnnt 25.258171 hw_loss 0.363206 lr 0.00079112 rank 0
2023-02-17 05:20:16,924 DEBUG TRAIN Batch 2/3100 loss 35.595943 loss_att 48.285042 loss_ctc 51.747314 loss_rnnt 30.786695 hw_loss 0.221078 lr 0.00079032 rank 4
2023-02-17 05:20:16,934 DEBUG TRAIN Batch 2/3100 loss 31.122736 loss_att 38.318279 loss_ctc 42.757038 loss_rnnt 27.940094 hw_loss 0.360552 lr 0.00079096 rank 3
2023-02-17 05:20:16,936 DEBUG TRAIN Batch 2/3100 loss 41.422016 loss_att 52.777176 loss_ctc 62.078190 loss_rnnt 36.281315 hw_loss 0.216580 lr 0.00078976 rank 7
2023-02-17 05:20:16,938 DEBUG TRAIN Batch 2/3100 loss 39.521931 loss_att 54.942528 loss_ctc 59.519985 loss_rnnt 33.648903 hw_loss 0.229692 lr 0.00079116 rank 6
2023-02-17 05:20:16,956 DEBUG TRAIN Batch 2/3100 loss 29.058651 loss_att 37.404854 loss_ctc 41.102165 loss_rnnt 25.575689 hw_loss 0.389853 lr 0.00079040 rank 5
2023-02-17 05:20:16,961 DEBUG TRAIN Batch 2/3100 loss 53.878517 loss_att 65.688126 loss_ctc 69.123802 loss_rnnt 49.357914 hw_loss 0.236202 lr 0.00079232 rank 1
2023-02-17 05:21:51,698 DEBUG TRAIN Batch 2/3200 loss 24.918674 loss_att 43.243767 loss_ctc 37.835632 loss_rnnt 19.365013 hw_loss 0.311965 lr 0.00079440 rank 5
2023-02-17 05:21:51,706 DEBUG TRAIN Batch 2/3200 loss 23.519480 loss_att 24.000284 loss_ctc 32.298508 loss_rnnt 22.057575 hw_loss 0.366009 lr 0.00079376 rank 7
2023-02-17 05:21:51,710 DEBUG TRAIN Batch 2/3200 loss 57.336407 loss_att 88.145309 loss_ctc 83.375687 loss_rnnt 47.591454 hw_loss 0.208627 lr 0.00079432 rank 4
2023-02-17 05:21:51,714 DEBUG TRAIN Batch 2/3200 loss 25.822895 loss_att 37.298084 loss_ctc 33.488556 loss_rnnt 22.362106 hw_loss 0.269366 lr 0.00079512 rank 0
2023-02-17 05:21:51,732 DEBUG TRAIN Batch 2/3200 loss 35.828659 loss_att 51.567001 loss_ctc 44.525520 loss_rnnt 31.403021 hw_loss 0.221979 lr 0.00079496 rank 3
2023-02-17 05:21:51,740 DEBUG TRAIN Batch 2/3200 loss 20.057205 loss_att 20.672796 loss_ctc 25.012772 loss_rnnt 19.095839 hw_loss 0.332823 lr 0.00079628 rank 2
2023-02-17 05:21:51,755 DEBUG TRAIN Batch 2/3200 loss 15.650969 loss_att 22.115694 loss_ctc 27.402670 loss_rnnt 12.699900 hw_loss 0.171057 lr 0.00079632 rank 1
2023-02-17 05:21:51,758 DEBUG TRAIN Batch 2/3200 loss 34.737801 loss_att 39.582378 loss_ctc 45.036804 loss_rnnt 32.171875 hw_loss 0.419637 lr 0.00079516 rank 6
2023-02-17 05:23:07,295 DEBUG TRAIN Batch 2/3300 loss 41.017872 loss_att 54.849689 loss_ctc 58.842194 loss_rnnt 35.758041 hw_loss 0.219168 lr 0.00079832 rank 4
2023-02-17 05:23:07,296 DEBUG TRAIN Batch 2/3300 loss 42.295120 loss_att 48.425564 loss_ctc 61.600109 loss_rnnt 38.291504 hw_loss 0.381611 lr 0.00079896 rank 3
2023-02-17 05:23:07,301 DEBUG TRAIN Batch 2/3300 loss 35.798222 loss_att 47.232510 loss_ctc 45.889107 loss_rnnt 31.992895 hw_loss 0.324411 lr 0.00079776 rank 7
2023-02-17 05:23:07,300 DEBUG TRAIN Batch 2/3300 loss 37.749420 loss_att 48.488808 loss_ctc 51.213928 loss_rnnt 33.672588 hw_loss 0.250666 lr 0.00080032 rank 1
2023-02-17 05:23:07,304 DEBUG TRAIN Batch 2/3300 loss 53.392628 loss_att 68.701637 loss_ctc 67.905769 loss_rnnt 48.288940 hw_loss 0.200247 lr 0.00079840 rank 5
2023-02-17 05:23:07,305 DEBUG TRAIN Batch 2/3300 loss 19.870775 loss_att 36.485233 loss_ctc 34.078293 loss_rnnt 14.441157 hw_loss 0.398231 lr 0.00079912 rank 0
2023-02-17 05:23:07,308 DEBUG TRAIN Batch 2/3300 loss 32.188412 loss_att 47.409477 loss_ctc 45.196365 loss_rnnt 27.275112 hw_loss 0.252555 lr 0.00079916 rank 6
2023-02-17 05:23:07,350 DEBUG TRAIN Batch 2/3300 loss 44.208721 loss_att 54.783974 loss_ctc 62.377884 loss_rnnt 39.482170 hw_loss 0.354276 lr 0.00080028 rank 2
2023-02-17 05:24:22,825 DEBUG TRAIN Batch 2/3400 loss 34.829872 loss_att 44.390762 loss_ctc 48.002373 loss_rnnt 31.041857 hw_loss 0.224067 lr 0.00080312 rank 0
2023-02-17 05:24:22,828 DEBUG TRAIN Batch 2/3400 loss 52.799900 loss_att 57.240505 loss_ctc 71.798950 loss_rnnt 49.195610 hw_loss 0.343068 lr 0.00080176 rank 7
2023-02-17 05:24:22,829 DEBUG TRAIN Batch 2/3400 loss 36.978977 loss_att 52.449051 loss_ctc 55.081066 loss_rnnt 31.359442 hw_loss 0.209825 lr 0.00080296 rank 3
2023-02-17 05:24:22,844 DEBUG TRAIN Batch 2/3400 loss 38.502846 loss_att 58.481506 loss_ctc 62.763458 loss_rnnt 31.089455 hw_loss 0.342964 lr 0.00080432 rank 1
2023-02-17 05:24:22,845 DEBUG TRAIN Batch 2/3400 loss 35.886589 loss_att 51.763138 loss_ctc 52.333542 loss_rnnt 30.409721 hw_loss 0.203690 lr 0.00080316 rank 6
2023-02-17 05:24:22,848 DEBUG TRAIN Batch 2/3400 loss 26.960056 loss_att 40.864994 loss_ctc 44.134674 loss_rnnt 21.727451 hw_loss 0.303127 lr 0.00080428 rank 2
2023-02-17 05:24:22,850 DEBUG TRAIN Batch 2/3400 loss 47.572628 loss_att 61.911148 loss_ctc 67.919861 loss_rnnt 41.832302 hw_loss 0.299373 lr 0.00080240 rank 5
2023-02-17 05:24:22,891 DEBUG TRAIN Batch 2/3400 loss 25.566708 loss_att 35.194904 loss_ctc 36.748489 loss_rnnt 22.040213 hw_loss 0.206159 lr 0.00080232 rank 4
2023-02-17 05:25:43,291 DEBUG TRAIN Batch 2/3500 loss 34.040565 loss_att 42.546478 loss_ctc 54.056454 loss_rnnt 29.475218 hw_loss 0.366339 lr 0.00080696 rank 3
2023-02-17 05:25:43,294 DEBUG TRAIN Batch 2/3500 loss 34.609486 loss_att 47.321739 loss_ctc 49.636658 loss_rnnt 29.877064 hw_loss 0.349395 lr 0.00080632 rank 4
2023-02-17 05:25:43,331 DEBUG TRAIN Batch 2/3500 loss 43.764660 loss_att 59.401108 loss_ctc 65.353798 loss_rnnt 37.645802 hw_loss 0.211896 lr 0.00080640 rank 5
2023-02-17 05:25:43,335 DEBUG TRAIN Batch 2/3500 loss 27.602633 loss_att 43.875946 loss_ctc 42.187325 loss_rnnt 22.295414 hw_loss 0.202365 lr 0.00080716 rank 6
2023-02-17 05:25:43,336 DEBUG TRAIN Batch 2/3500 loss 45.669144 loss_att 60.120369 loss_ctc 62.952785 loss_rnnt 40.352032 hw_loss 0.229466 lr 0.00080832 rank 1
2023-02-17 05:25:43,336 DEBUG TRAIN Batch 2/3500 loss 31.265186 loss_att 48.292179 loss_ctc 50.114532 loss_rnnt 25.192587 hw_loss 0.288663 lr 0.00080576 rank 7
2023-02-17 05:25:43,364 DEBUG TRAIN Batch 2/3500 loss 47.176308 loss_att 61.822025 loss_ctc 66.316269 loss_rnnt 41.588531 hw_loss 0.199943 lr 0.00080828 rank 2
2023-02-17 05:25:43,367 DEBUG TRAIN Batch 2/3500 loss 18.863878 loss_att 25.640879 loss_ctc 26.088583 loss_rnnt 16.405239 hw_loss 0.262398 lr 0.00080712 rank 0
2023-02-17 05:27:14,836 DEBUG TRAIN Batch 2/3600 loss 55.097900 loss_att 65.409233 loss_ctc 76.211014 loss_rnnt 50.088657 hw_loss 0.247293 lr 0.00081032 rank 4
2023-02-17 05:27:14,837 DEBUG TRAIN Batch 2/3600 loss 30.862816 loss_att 44.960850 loss_ctc 44.439354 loss_rnnt 26.094187 hw_loss 0.260282 lr 0.00081232 rank 1
2023-02-17 05:27:14,837 DEBUG TRAIN Batch 2/3600 loss 33.077885 loss_att 46.413914 loss_ctc 46.715103 loss_rnnt 28.434713 hw_loss 0.295628 lr 0.00080976 rank 7
2023-02-17 05:27:14,839 DEBUG TRAIN Batch 2/3600 loss 34.094269 loss_att 52.862511 loss_ctc 57.216797 loss_rnnt 27.074986 hw_loss 0.342427 lr 0.00081228 rank 2
2023-02-17 05:27:14,841 DEBUG TRAIN Batch 2/3600 loss 45.834187 loss_att 51.360405 loss_ctc 59.817745 loss_rnnt 42.691326 hw_loss 0.324646 lr 0.00081096 rank 3
2023-02-17 05:27:14,845 DEBUG TRAIN Batch 2/3600 loss 36.631989 loss_att 47.828823 loss_ctc 54.986732 loss_rnnt 31.854786 hw_loss 0.169754 lr 0.00081040 rank 5
2023-02-17 05:27:14,945 DEBUG TRAIN Batch 2/3600 loss 28.866041 loss_att 45.434258 loss_ctc 34.395126 loss_rnnt 24.640352 hw_loss 0.327812 lr 0.00081112 rank 0
2023-02-17 05:27:14,950 DEBUG TRAIN Batch 2/3600 loss 57.143715 loss_att 73.502655 loss_ctc 88.325272 loss_rnnt 49.574722 hw_loss 0.261869 lr 0.00081116 rank 6
2023-02-17 05:28:30,051 DEBUG TRAIN Batch 2/3700 loss 20.701506 loss_att 28.815893 loss_ctc 27.908127 loss_rnnt 18.006920 hw_loss 0.207798 lr 0.00081516 rank 6
2023-02-17 05:28:30,052 DEBUG TRAIN Batch 2/3700 loss 25.988134 loss_att 37.635963 loss_ctc 38.392574 loss_rnnt 21.874294 hw_loss 0.244404 lr 0.00081628 rank 2
2023-02-17 05:28:30,052 DEBUG TRAIN Batch 2/3700 loss 34.517513 loss_att 42.142776 loss_ctc 48.426712 loss_rnnt 30.961294 hw_loss 0.331145 lr 0.00081432 rank 4
2023-02-17 05:28:30,054 DEBUG TRAIN Batch 2/3700 loss 33.161449 loss_att 43.193920 loss_ctc 52.601284 loss_rnnt 28.432947 hw_loss 0.243802 lr 0.00081512 rank 0
2023-02-17 05:28:30,059 DEBUG TRAIN Batch 2/3700 loss 33.875935 loss_att 46.475540 loss_ctc 46.729717 loss_rnnt 29.455969 hw_loss 0.349139 lr 0.00081496 rank 3
2023-02-17 05:28:30,060 DEBUG TRAIN Batch 2/3700 loss 31.065786 loss_att 40.625656 loss_ctc 48.490765 loss_rnnt 26.677713 hw_loss 0.286435 lr 0.00081376 rank 7
2023-02-17 05:28:30,111 DEBUG TRAIN Batch 2/3700 loss 45.202652 loss_att 52.704132 loss_ctc 60.045162 loss_rnnt 41.608093 hw_loss 0.216110 lr 0.00081632 rank 1
2023-02-17 05:28:30,128 DEBUG TRAIN Batch 2/3700 loss 43.226452 loss_att 48.257057 loss_ctc 56.319683 loss_rnnt 40.344837 hw_loss 0.243237 lr 0.00081440 rank 5
2023-02-17 05:29:45,914 DEBUG TRAIN Batch 2/3800 loss 32.274952 loss_att 36.027550 loss_ctc 50.103317 loss_rnnt 28.947647 hw_loss 0.374384 lr 0.00081776 rank 7
2023-02-17 05:29:45,916 DEBUG TRAIN Batch 2/3800 loss 99.688248 loss_att 124.002747 loss_ctc 152.000839 loss_rnnt 87.666016 hw_loss 0.345615 lr 0.00081832 rank 4
2023-02-17 05:29:45,917 DEBUG TRAIN Batch 2/3800 loss 31.196110 loss_att 50.656277 loss_ctc 50.004356 loss_rnnt 24.701637 hw_loss 0.177507 lr 0.00081912 rank 0
2023-02-17 05:29:45,919 DEBUG TRAIN Batch 2/3800 loss 46.968246 loss_att 52.245083 loss_ctc 66.698280 loss_rnnt 43.051647 hw_loss 0.432309 lr 0.00082032 rank 1
2023-02-17 05:29:45,924 DEBUG TRAIN Batch 2/3800 loss 18.537062 loss_att 21.817247 loss_ctc 24.430115 loss_rnnt 16.896738 hw_loss 0.372271 lr 0.00082028 rank 2
2023-02-17 05:29:45,946 DEBUG TRAIN Batch 2/3800 loss 29.154392 loss_att 49.853012 loss_ctc 44.107834 loss_rnnt 22.931976 hw_loss 0.166682 lr 0.00081840 rank 5
2023-02-17 05:29:45,951 DEBUG TRAIN Batch 2/3800 loss 16.044174 loss_att 21.903828 loss_ctc 23.778875 loss_rnnt 13.665601 hw_loss 0.328780 lr 0.00081916 rank 6
2023-02-17 05:29:45,965 DEBUG TRAIN Batch 2/3800 loss 32.058918 loss_att 40.552437 loss_ctc 44.022778 loss_rnnt 28.627319 hw_loss 0.258206 lr 0.00081896 rank 3
2023-02-17 05:31:10,954 DEBUG TRAIN Batch 2/3900 loss 32.470695 loss_att 42.001564 loss_ctc 49.262360 loss_rnnt 28.181616 hw_loss 0.270033 lr 0.00082296 rank 3
2023-02-17 05:31:10,968 DEBUG TRAIN Batch 2/3900 loss 40.482227 loss_att 59.279411 loss_ctc 52.227570 loss_rnnt 35.027443 hw_loss 0.242431 lr 0.00082176 rank 7
2023-02-17 05:31:10,985 DEBUG TRAIN Batch 2/3900 loss 37.365696 loss_att 52.021172 loss_ctc 56.715065 loss_rnnt 31.747036 hw_loss 0.201847 lr 0.00082428 rank 2
2023-02-17 05:31:10,998 DEBUG TRAIN Batch 2/3900 loss 53.522652 loss_att 67.372696 loss_ctc 68.752838 loss_rnnt 48.610256 hw_loss 0.209437 lr 0.00082240 rank 5
2023-02-17 05:31:11,006 DEBUG TRAIN Batch 2/3900 loss 24.064339 loss_att 45.436855 loss_ctc 40.606468 loss_rnnt 17.424145 hw_loss 0.300135 lr 0.00082232 rank 4
2023-02-17 05:31:11,010 DEBUG TRAIN Batch 2/3900 loss 55.437397 loss_att 70.019928 loss_ctc 78.552353 loss_rnnt 49.308220 hw_loss 0.245015 lr 0.00082312 rank 0
2023-02-17 05:31:11,035 DEBUG TRAIN Batch 2/3900 loss 34.964104 loss_att 46.241562 loss_ctc 54.477280 loss_rnnt 29.950928 hw_loss 0.292369 lr 0.00082432 rank 1
2023-02-17 05:31:11,040 DEBUG TRAIN Batch 2/3900 loss 39.770206 loss_att 62.183826 loss_ctc 57.631435 loss_rnnt 32.753197 hw_loss 0.286477 lr 0.00082316 rank 6
2023-02-17 05:32:37,779 DEBUG TRAIN Batch 2/4000 loss 36.713242 loss_att 58.620407 loss_ctc 48.963409 loss_rnnt 30.620993 hw_loss 0.145234 lr 0.00082696 rank 3
2023-02-17 05:32:37,793 DEBUG TRAIN Batch 2/4000 loss 58.621048 loss_att 75.922318 loss_ctc 79.786095 loss_rnnt 52.238400 hw_loss 0.188227 lr 0.00082640 rank 5
2023-02-17 05:32:37,796 DEBUG TRAIN Batch 2/4000 loss 39.739792 loss_att 51.093540 loss_ctc 53.516125 loss_rnnt 35.478920 hw_loss 0.287403 lr 0.00082632 rank 4
2023-02-17 05:32:37,799 DEBUG TRAIN Batch 2/4000 loss 24.572206 loss_att 35.475201 loss_ctc 40.885674 loss_rnnt 20.137114 hw_loss 0.148806 lr 0.00082576 rank 7
2023-02-17 05:32:37,807 DEBUG TRAIN Batch 2/4000 loss 36.579266 loss_att 49.761124 loss_ctc 55.674740 loss_rnnt 31.216175 hw_loss 0.338728 lr 0.00082832 rank 1
2023-02-17 05:32:37,808 DEBUG TRAIN Batch 2/4000 loss 34.799587 loss_att 46.229710 loss_ctc 44.620968 loss_rnnt 31.102921 hw_loss 0.189609 lr 0.00082716 rank 6
2023-02-17 05:32:37,817 DEBUG TRAIN Batch 2/4000 loss 42.844585 loss_att 57.273491 loss_ctc 60.073849 loss_rnnt 37.514481 hw_loss 0.275789 lr 0.00082712 rank 0
2023-02-17 05:32:37,852 DEBUG TRAIN Batch 2/4000 loss 27.685751 loss_att 37.111229 loss_ctc 40.591930 loss_rnnt 23.867247 hw_loss 0.398597 lr 0.00082828 rank 2
2023-02-17 05:33:53,882 DEBUG TRAIN Batch 2/4100 loss 45.411106 loss_att 53.995724 loss_ctc 64.872162 loss_rnnt 40.929787 hw_loss 0.317984 lr 0.00083112 rank 0
2023-02-17 05:33:53,883 DEBUG TRAIN Batch 2/4100 loss 31.999508 loss_att 50.308350 loss_ctc 50.995514 loss_rnnt 25.650829 hw_loss 0.288953 lr 0.00083032 rank 4
2023-02-17 05:33:53,884 DEBUG TRAIN Batch 2/4100 loss 34.527653 loss_att 48.088455 loss_ctc 59.119736 loss_rnnt 28.356255 hw_loss 0.338050 lr 0.00083096 rank 3
2023-02-17 05:33:53,886 DEBUG TRAIN Batch 2/4100 loss 37.062344 loss_att 50.304474 loss_ctc 49.813301 loss_rnnt 32.584042 hw_loss 0.243276 lr 0.00083232 rank 1
2023-02-17 05:33:53,887 DEBUG TRAIN Batch 2/4100 loss 24.185637 loss_att 38.205692 loss_ctc 40.313751 loss_rnnt 19.079044 hw_loss 0.285312 lr 0.00082976 rank 7
2023-02-17 05:33:53,887 DEBUG TRAIN Batch 2/4100 loss 32.239887 loss_att 43.995964 loss_ctc 50.436119 loss_rnnt 27.306425 hw_loss 0.292654 lr 0.00083040 rank 5
2023-02-17 05:33:53,890 DEBUG TRAIN Batch 2/4100 loss 43.038563 loss_att 50.115173 loss_ctc 58.592651 loss_rnnt 39.381325 hw_loss 0.315063 lr 0.00083228 rank 2
2023-02-17 05:33:53,929 DEBUG TRAIN Batch 2/4100 loss 49.957657 loss_att 64.649399 loss_ctc 77.720627 loss_rnnt 43.142761 hw_loss 0.327781 lr 0.00083116 rank 6
2023-02-17 05:35:11,515 DEBUG TRAIN Batch 2/4200 loss 41.098022 loss_att 47.154816 loss_ctc 65.167374 loss_rnnt 36.565620 hw_loss 0.209617 lr 0.00083432 rank 4
2023-02-17 05:35:11,536 DEBUG TRAIN Batch 2/4200 loss 53.234135 loss_att 74.752075 loss_ctc 78.839622 loss_rnnt 45.397427 hw_loss 0.223234 lr 0.00083632 rank 1
2023-02-17 05:35:11,538 DEBUG TRAIN Batch 2/4200 loss 57.490517 loss_att 68.602997 loss_ctc 76.457031 loss_rnnt 52.592972 hw_loss 0.274083 lr 0.00083512 rank 0
2023-02-17 05:35:11,578 DEBUG TRAIN Batch 2/4200 loss 38.054359 loss_att 47.410503 loss_ctc 55.125877 loss_rnnt 33.799736 hw_loss 0.200990 lr 0.00083496 rank 3
2023-02-17 05:35:11,593 DEBUG TRAIN Batch 2/4200 loss 41.500603 loss_att 54.258579 loss_ctc 58.292168 loss_rnnt 36.582314 hw_loss 0.239661 lr 0.00083516 rank 6
2023-02-17 05:35:11,606 DEBUG TRAIN Batch 2/4200 loss 34.900517 loss_att 42.446480 loss_ctc 56.068253 loss_rnnt 30.401329 hw_loss 0.314302 lr 0.00083440 rank 5
2023-02-17 05:35:11,606 DEBUG TRAIN Batch 2/4200 loss 51.689842 loss_att 62.486607 loss_ctc 71.520615 loss_rnnt 46.692181 hw_loss 0.364128 lr 0.00083376 rank 7
2023-02-17 05:35:11,608 DEBUG TRAIN Batch 2/4200 loss 35.195965 loss_att 47.693920 loss_ctc 50.655937 loss_rnnt 30.472889 hw_loss 0.304045 lr 0.00083628 rank 2
2023-02-17 05:36:45,004 DEBUG TRAIN Batch 2/4300 loss 35.818123 loss_att 51.021088 loss_ctc 51.384438 loss_rnnt 30.535894 hw_loss 0.311493 lr 0.00083916 rank 6
2023-02-17 05:36:45,007 DEBUG TRAIN Batch 2/4300 loss 40.615276 loss_att 48.629807 loss_ctc 59.834797 loss_rnnt 36.326649 hw_loss 0.230850 lr 0.00083912 rank 0
2023-02-17 05:36:45,008 DEBUG TRAIN Batch 2/4300 loss 31.452049 loss_att 44.416134 loss_ctc 40.471237 loss_rnnt 27.538645 hw_loss 0.221305 lr 0.00084028 rank 2
2023-02-17 05:36:45,009 DEBUG TRAIN Batch 2/4300 loss 56.876747 loss_att 72.586273 loss_ctc 74.501961 loss_rnnt 51.253761 hw_loss 0.245728 lr 0.00083832 rank 4
2023-02-17 05:36:45,009 DEBUG TRAIN Batch 2/4300 loss 37.095451 loss_att 49.393635 loss_ctc 54.869205 loss_rnnt 32.095024 hw_loss 0.320536 lr 0.00083776 rank 7
2023-02-17 05:36:45,011 DEBUG TRAIN Batch 2/4300 loss 27.438250 loss_att 35.379559 loss_ctc 36.435280 loss_rnnt 24.526382 hw_loss 0.232503 lr 0.00083896 rank 3
2023-02-17 05:36:45,047 DEBUG TRAIN Batch 2/4300 loss 41.325737 loss_att 52.853107 loss_ctc 55.704895 loss_rnnt 36.959751 hw_loss 0.268670 lr 0.00083840 rank 5
2023-02-17 05:36:45,052 DEBUG TRAIN Batch 2/4300 loss 39.084122 loss_att 52.805267 loss_ctc 54.654095 loss_rnnt 34.123093 hw_loss 0.264003 lr 0.00084032 rank 1
2023-02-17 05:38:00,821 DEBUG TRAIN Batch 2/4400 loss 27.013376 loss_att 33.488445 loss_ctc 36.202747 loss_rnnt 24.295456 hw_loss 0.370603 lr 0.00084296 rank 3
2023-02-17 05:38:00,822 DEBUG TRAIN Batch 2/4400 loss 26.933441 loss_att 29.562412 loss_ctc 36.412842 loss_rnnt 24.939867 hw_loss 0.382230 lr 0.00084428 rank 2
2023-02-17 05:38:00,823 DEBUG TRAIN Batch 2/4400 loss 23.479980 loss_att 37.358063 loss_ctc 35.455769 loss_rnnt 18.964146 hw_loss 0.268961 lr 0.00084312 rank 0
2023-02-17 05:38:00,824 DEBUG TRAIN Batch 2/4400 loss 18.306646 loss_att 18.591236 loss_ctc 22.540995 loss_rnnt 17.451406 hw_loss 0.438265 lr 0.00084240 rank 5
2023-02-17 05:38:00,824 DEBUG TRAIN Batch 2/4400 loss 25.621202 loss_att 34.160637 loss_ctc 36.361275 loss_rnnt 22.330671 hw_loss 0.282440 lr 0.00084232 rank 4
2023-02-17 05:38:00,826 DEBUG TRAIN Batch 2/4400 loss 37.422447 loss_att 41.859154 loss_ctc 55.894924 loss_rnnt 33.909542 hw_loss 0.304813 lr 0.00084316 rank 6
2023-02-17 05:38:00,829 DEBUG TRAIN Batch 2/4400 loss 64.854050 loss_att 79.187073 loss_ctc 89.142654 loss_rnnt 58.571571 hw_loss 0.332594 lr 0.00084432 rank 1
2023-02-17 05:38:00,833 DEBUG TRAIN Batch 2/4400 loss 37.223007 loss_att 45.004189 loss_ctc 55.340088 loss_rnnt 33.058441 hw_loss 0.361348 lr 0.00084176 rank 7
2023-02-17 05:39:16,994 DEBUG TRAIN Batch 2/4500 loss 54.585167 loss_att 70.745018 loss_ctc 83.039780 loss_rnnt 47.386635 hw_loss 0.323639 lr 0.00084716 rank 6
2023-02-17 05:39:17,000 DEBUG TRAIN Batch 2/4500 loss 50.509727 loss_att 59.574451 loss_ctc 71.286697 loss_rnnt 45.796295 hw_loss 0.244170 lr 0.00084640 rank 5
2023-02-17 05:39:17,000 DEBUG TRAIN Batch 2/4500 loss 65.020073 loss_att 82.221886 loss_ctc 87.279968 loss_rnnt 58.528442 hw_loss 0.156149 lr 0.00084832 rank 1
2023-02-17 05:39:17,000 DEBUG TRAIN Batch 2/4500 loss 52.166550 loss_att 65.177986 loss_ctc 65.866791 loss_rnnt 47.613335 hw_loss 0.232932 lr 0.00084576 rank 7
2023-02-17 05:39:17,001 DEBUG TRAIN Batch 2/4500 loss 47.492722 loss_att 63.366295 loss_ctc 66.928970 loss_rnnt 41.528210 hw_loss 0.371809 lr 0.00084712 rank 0
2023-02-17 05:39:17,000 DEBUG TRAIN Batch 2/4500 loss 34.134777 loss_att 39.718529 loss_ctc 43.705654 loss_rnnt 31.638851 hw_loss 0.193234 lr 0.00084632 rank 4
2023-02-17 05:39:17,003 DEBUG TRAIN Batch 2/4500 loss 21.896624 loss_att 32.932510 loss_ctc 30.570066 loss_rnnt 18.396610 hw_loss 0.255709 lr 0.00084828 rank 2
2023-02-17 05:39:17,003 DEBUG TRAIN Batch 2/4500 loss 49.146152 loss_att 57.524323 loss_ctc 59.745403 loss_rnnt 45.854580 hw_loss 0.380070 lr 0.00084696 rank 3
2023-02-17 05:40:41,056 DEBUG TRAIN Batch 2/4600 loss 26.066818 loss_att 36.003273 loss_ctc 39.401772 loss_rnnt 22.182018 hw_loss 0.224089 lr 0.00085116 rank 6
2023-02-17 05:40:41,077 DEBUG TRAIN Batch 2/4600 loss 31.383469 loss_att 48.998463 loss_ctc 40.018059 loss_rnnt 26.556692 hw_loss 0.285937 lr 0.00084976 rank 7
2023-02-17 05:40:41,097 DEBUG TRAIN Batch 2/4600 loss 29.389519 loss_att 41.040096 loss_ctc 40.757771 loss_rnnt 25.352764 hw_loss 0.357886 lr 0.00085228 rank 2
2023-02-17 05:40:41,101 DEBUG TRAIN Batch 2/4600 loss 41.397518 loss_att 57.146942 loss_ctc 61.764511 loss_rnnt 35.387394 hw_loss 0.271194 lr 0.00085032 rank 4
2023-02-17 05:40:41,104 DEBUG TRAIN Batch 2/4600 loss 16.659359 loss_att 29.057854 loss_ctc 26.084366 loss_rnnt 12.795424 hw_loss 0.239190 lr 0.00085112 rank 0
2023-02-17 05:40:41,106 DEBUG TRAIN Batch 2/4600 loss 35.927250 loss_att 49.500282 loss_ctc 52.847656 loss_rnnt 30.788692 hw_loss 0.314805 lr 0.00085040 rank 5
2023-02-17 05:40:41,123 DEBUG TRAIN Batch 2/4600 loss 29.775328 loss_att 42.457733 loss_ctc 44.329910 loss_rnnt 25.109938 hw_loss 0.353061 lr 0.00085096 rank 3
2023-02-17 05:40:41,127 DEBUG TRAIN Batch 2/4600 loss 26.765162 loss_att 39.188545 loss_ctc 41.007530 loss_rnnt 22.222641 hw_loss 0.297863 lr 0.00085232 rank 1
2023-02-17 05:42:09,250 DEBUG TRAIN Batch 2/4700 loss 24.584259 loss_att 35.110214 loss_ctc 35.547691 loss_rnnt 20.915916 hw_loss 0.190051 lr 0.00085496 rank 3
2023-02-17 05:42:09,250 DEBUG TRAIN Batch 2/4700 loss 26.824471 loss_att 32.195450 loss_ctc 39.849319 loss_rnnt 23.882896 hw_loss 0.245124 lr 0.00085632 rank 1
2023-02-17 05:42:09,251 DEBUG TRAIN Batch 2/4700 loss 40.125504 loss_att 52.014614 loss_ctc 57.321148 loss_rnnt 35.286186 hw_loss 0.316389 lr 0.00085376 rank 7
2023-02-17 05:42:09,251 DEBUG TRAIN Batch 2/4700 loss 37.592266 loss_att 42.415237 loss_ctc 54.036957 loss_rnnt 34.295074 hw_loss 0.262438 lr 0.00085628 rank 2
2023-02-17 05:42:09,254 DEBUG TRAIN Batch 2/4700 loss 24.497036 loss_att 33.733315 loss_ctc 43.863567 loss_rnnt 19.918375 hw_loss 0.279752 lr 0.00085512 rank 0
2023-02-17 05:42:09,255 DEBUG TRAIN Batch 2/4700 loss 35.390533 loss_att 51.359810 loss_ctc 61.308132 loss_rnnt 28.633411 hw_loss 0.201720 lr 0.00085432 rank 4
2023-02-17 05:42:09,263 DEBUG TRAIN Batch 2/4700 loss 36.901428 loss_att 44.724003 loss_ctc 53.483715 loss_rnnt 33.025043 hw_loss 0.189185 lr 0.00085440 rank 5
2023-02-17 05:42:09,265 DEBUG TRAIN Batch 2/4700 loss 21.158855 loss_att 34.018631 loss_ctc 34.509598 loss_rnnt 16.699886 hw_loss 0.200461 lr 0.00085516 rank 6
2023-02-17 05:43:25,077 DEBUG TRAIN Batch 2/4800 loss 26.607866 loss_att 36.599136 loss_ctc 43.486984 loss_rnnt 22.213835 hw_loss 0.272304 lr 0.00085916 rank 6
2023-02-17 05:43:25,079 DEBUG TRAIN Batch 2/4800 loss 37.931213 loss_att 52.014683 loss_ctc 50.386608 loss_rnnt 33.288132 hw_loss 0.310624 lr 0.00085776 rank 7
2023-02-17 05:43:25,080 DEBUG TRAIN Batch 2/4800 loss 40.487461 loss_att 57.980976 loss_ctc 57.880989 loss_rnnt 34.504410 hw_loss 0.309772 lr 0.00086032 rank 1
2023-02-17 05:43:25,080 DEBUG TRAIN Batch 2/4800 loss 40.139977 loss_att 50.114304 loss_ctc 58.843193 loss_rnnt 35.509918 hw_loss 0.265176 lr 0.00085896 rank 3
2023-02-17 05:43:25,084 DEBUG TRAIN Batch 2/4800 loss 33.839016 loss_att 55.921078 loss_ctc 53.726444 loss_rnnt 26.682009 hw_loss 0.166756 lr 0.00086028 rank 2
2023-02-17 05:43:25,084 DEBUG TRAIN Batch 2/4800 loss 38.736774 loss_att 49.514542 loss_ctc 53.738754 loss_rnnt 34.438641 hw_loss 0.266848 lr 0.00085912 rank 0
2023-02-17 05:43:25,086 DEBUG TRAIN Batch 2/4800 loss 32.180607 loss_att 48.704754 loss_ctc 47.979755 loss_rnnt 26.678963 hw_loss 0.169239 lr 0.00085832 rank 4
2023-02-17 05:43:25,091 DEBUG TRAIN Batch 2/4800 loss 34.259003 loss_att 48.035168 loss_ctc 49.206421 loss_rnnt 29.395098 hw_loss 0.216902 lr 0.00085840 rank 5
2023-02-17 05:44:43,380 DEBUG TRAIN Batch 2/4900 loss 36.440861 loss_att 50.106972 loss_ctc 62.767639 loss_rnnt 30.061005 hw_loss 0.255744 lr 0.00086428 rank 2
2023-02-17 05:44:43,391 DEBUG TRAIN Batch 2/4900 loss 44.178833 loss_att 59.763931 loss_ctc 66.577255 loss_rnnt 37.929523 hw_loss 0.273436 lr 0.00086432 rank 1
2023-02-17 05:44:43,395 DEBUG TRAIN Batch 2/4900 loss 20.512066 loss_att 33.670513 loss_ctc 37.343597 loss_rnnt 15.474755 hw_loss 0.302659 lr 0.00086232 rank 4
2023-02-17 05:44:43,417 DEBUG TRAIN Batch 2/4900 loss 17.922668 loss_att 31.618158 loss_ctc 28.438873 loss_rnnt 13.572695 hw_loss 0.391341 lr 0.00086312 rank 0
2023-02-17 05:44:43,418 DEBUG TRAIN Batch 2/4900 loss 33.979027 loss_att 40.421314 loss_ctc 46.908260 loss_rnnt 30.720137 hw_loss 0.462256 lr 0.00086240 rank 5
2023-02-17 05:44:43,419 DEBUG TRAIN Batch 2/4900 loss 29.965698 loss_att 41.733829 loss_ctc 47.479950 loss_rnnt 25.118734 hw_loss 0.296443 lr 0.00086316 rank 6
2023-02-17 05:44:43,439 DEBUG TRAIN Batch 2/4900 loss 42.515308 loss_att 50.091125 loss_ctc 57.466232 loss_rnnt 38.867680 hw_loss 0.260637 lr 0.00086296 rank 3
2023-02-17 05:44:43,445 DEBUG TRAIN Batch 2/4900 loss 35.480618 loss_att 47.111782 loss_ctc 52.372478 loss_rnnt 30.755550 hw_loss 0.274851 lr 0.00086176 rank 7
2023-02-17 05:46:16,264 DEBUG TRAIN Batch 2/5000 loss 31.271749 loss_att 39.937485 loss_ctc 43.157162 loss_rnnt 27.813612 hw_loss 0.263002 lr 0.00086716 rank 6
2023-02-17 05:46:16,278 DEBUG TRAIN Batch 2/5000 loss 26.501011 loss_att 30.310555 loss_ctc 40.599174 loss_rnnt 23.646894 hw_loss 0.398351 lr 0.00086696 rank 3
2023-02-17 05:46:16,283 DEBUG TRAIN Batch 2/5000 loss 28.350679 loss_att 37.900925 loss_ctc 48.774906 loss_rnnt 23.520403 hw_loss 0.369370 lr 0.00086828 rank 2
2023-02-17 05:46:16,284 DEBUG TRAIN Batch 2/5000 loss 22.525763 loss_att 23.648823 loss_ctc 30.483568 loss_rnnt 21.073534 hw_loss 0.312326 lr 0.00086640 rank 5
2023-02-17 05:46:16,285 DEBUG TRAIN Batch 2/5000 loss 31.701597 loss_att 40.850243 loss_ctc 50.936245 loss_rnnt 27.170404 hw_loss 0.256581 lr 0.00086832 rank 1
2023-02-17 05:46:16,325 DEBUG TRAIN Batch 2/5000 loss 43.603477 loss_att 53.888344 loss_ctc 56.137810 loss_rnnt 39.749527 hw_loss 0.235745 lr 0.00086632 rank 4
2023-02-17 05:46:16,333 DEBUG TRAIN Batch 2/5000 loss 29.752474 loss_att 34.178986 loss_ctc 44.583427 loss_rnnt 26.693018 hw_loss 0.368795 lr 0.00086576 rank 7
2023-02-17 05:46:16,338 DEBUG TRAIN Batch 2/5000 loss 21.458824 loss_att 24.982635 loss_ctc 33.792713 loss_rnnt 18.958687 hw_loss 0.282858 lr 0.00086712 rank 0
2023-02-17 05:47:31,719 DEBUG TRAIN Batch 2/5100 loss 34.253429 loss_att 46.937969 loss_ctc 61.344276 loss_rnnt 27.997990 hw_loss 0.199531 lr 0.00087116 rank 6
2023-02-17 05:47:31,720 DEBUG TRAIN Batch 2/5100 loss 27.830599 loss_att 42.967480 loss_ctc 41.107403 loss_rnnt 22.949667 hw_loss 0.156219 lr 0.00087232 rank 1
2023-02-17 05:47:31,722 DEBUG TRAIN Batch 2/5100 loss 14.945330 loss_att 14.995916 loss_ctc 18.724981 loss_rnnt 14.257141 hw_loss 0.326468 lr 0.00087228 rank 2
2023-02-17 05:47:31,725 DEBUG TRAIN Batch 2/5100 loss 44.703205 loss_att 52.035351 loss_ctc 63.115982 loss_rnnt 40.660477 hw_loss 0.227370 lr 0.00087096 rank 3
2023-02-17 05:47:31,727 DEBUG TRAIN Batch 2/5100 loss 25.827049 loss_att 31.658001 loss_ctc 34.211243 loss_rnnt 23.344181 hw_loss 0.372726 lr 0.00087032 rank 4
2023-02-17 05:47:31,728 DEBUG TRAIN Batch 2/5100 loss 21.389299 loss_att 34.697445 loss_ctc 37.148464 loss_rnnt 16.529428 hw_loss 0.181915 lr 0.00087112 rank 0
2023-02-17 05:47:31,735 DEBUG TRAIN Batch 2/5100 loss 39.660778 loss_att 58.181728 loss_ctc 56.580391 loss_rnnt 33.591057 hw_loss 0.205470 lr 0.00087040 rank 5
2023-02-17 05:47:31,778 DEBUG TRAIN Batch 2/5100 loss 42.103935 loss_att 53.581188 loss_ctc 54.755825 loss_rnnt 37.988400 hw_loss 0.249691 lr 0.00086976 rank 7
2023-02-17 05:48:47,959 DEBUG TRAIN Batch 2/5200 loss 52.307507 loss_att 60.913437 loss_ctc 68.537598 loss_rnnt 48.287823 hw_loss 0.252161 lr 0.00087432 rank 4
2023-02-17 05:48:47,968 DEBUG TRAIN Batch 2/5200 loss 23.025404 loss_att 38.167885 loss_ctc 35.660103 loss_rnnt 18.146315 hw_loss 0.311191 lr 0.00087376 rank 7
2023-02-17 05:48:47,970 DEBUG TRAIN Batch 2/5200 loss 25.553652 loss_att 43.170696 loss_ctc 45.222603 loss_rnnt 19.294968 hw_loss 0.211402 lr 0.00087516 rank 6
2023-02-17 05:48:47,971 DEBUG TRAIN Batch 2/5200 loss 38.713058 loss_att 58.452255 loss_ctc 57.626095 loss_rnnt 32.097118 hw_loss 0.274428 lr 0.00087628 rank 2
2023-02-17 05:48:47,973 DEBUG TRAIN Batch 2/5200 loss 17.397413 loss_att 28.505039 loss_ctc 25.699749 loss_rnnt 13.938698 hw_loss 0.244147 lr 0.00087440 rank 5
2023-02-17 05:48:47,976 DEBUG TRAIN Batch 2/5200 loss 62.604733 loss_att 78.313545 loss_ctc 94.022812 loss_rnnt 55.092949 hw_loss 0.339271 lr 0.00087632 rank 1
2023-02-17 05:48:47,978 DEBUG TRAIN Batch 2/5200 loss 49.178478 loss_att 66.309692 loss_ctc 63.491974 loss_rnnt 43.719032 hw_loss 0.233876 lr 0.00087496 rank 3
2023-02-17 05:48:48,025 DEBUG TRAIN Batch 2/5200 loss 18.323906 loss_att 29.323673 loss_ctc 30.738613 loss_rnnt 14.322661 hw_loss 0.273745 lr 0.00087512 rank 0
2023-02-17 05:50:10,516 DEBUG TRAIN Batch 2/5300 loss 41.112263 loss_att 51.773315 loss_ctc 54.399807 loss_rnnt 37.072502 hw_loss 0.254767 lr 0.00087832 rank 4
2023-02-17 05:50:10,528 DEBUG TRAIN Batch 2/5300 loss 19.331881 loss_att 34.018734 loss_ctc 33.039856 loss_rnnt 14.410098 hw_loss 0.293780 lr 0.00087916 rank 6
2023-02-17 05:50:10,544 DEBUG TRAIN Batch 2/5300 loss 55.694073 loss_att 67.270912 loss_ctc 73.056137 loss_rnnt 50.921581 hw_loss 0.266589 lr 0.00088028 rank 2
2023-02-17 05:50:10,545 DEBUG TRAIN Batch 2/5300 loss 21.970493 loss_att 34.652863 loss_ctc 47.307739 loss_rnnt 15.895930 hw_loss 0.299606 lr 0.00087776 rank 7
2023-02-17 05:50:10,551 DEBUG TRAIN Batch 2/5300 loss 21.641432 loss_att 33.401329 loss_ctc 35.227242 loss_rnnt 17.351206 hw_loss 0.237757 lr 0.00087840 rank 5
2023-02-17 05:50:10,558 DEBUG TRAIN Batch 2/5300 loss 31.996479 loss_att 48.696953 loss_ctc 46.434479 loss_rnnt 26.560743 hw_loss 0.319829 lr 0.00087896 rank 3
2023-02-17 05:50:10,561 DEBUG TRAIN Batch 2/5300 loss 57.083366 loss_att 81.211220 loss_ctc 87.421921 loss_rnnt 48.044674 hw_loss 0.314973 lr 0.00088032 rank 1
2023-02-17 05:50:10,566 DEBUG TRAIN Batch 2/5300 loss 39.151642 loss_att 50.584053 loss_ctc 54.849255 loss_rnnt 34.652649 hw_loss 0.224060 lr 0.00087912 rank 0
2023-02-17 05:51:38,603 DEBUG TRAIN Batch 2/5400 loss 40.108124 loss_att 46.276669 loss_ctc 58.835014 loss_rnnt 36.222672 hw_loss 0.290286 lr 0.00088176 rank 7
2023-02-17 05:51:38,604 DEBUG TRAIN Batch 2/5400 loss 27.501806 loss_att 37.916157 loss_ctc 40.059761 loss_rnnt 23.618294 hw_loss 0.236717 lr 0.00088312 rank 0
2023-02-17 05:51:38,606 DEBUG TRAIN Batch 2/5400 loss 36.640411 loss_att 49.588371 loss_ctc 55.793335 loss_rnnt 31.333036 hw_loss 0.307605 lr 0.00088240 rank 5
2023-02-17 05:51:38,608 DEBUG TRAIN Batch 2/5400 loss 26.605892 loss_att 35.898102 loss_ctc 39.859219 loss_rnnt 22.828690 hw_loss 0.284341 lr 0.00088316 rank 6
2023-02-17 05:51:38,616 DEBUG TRAIN Batch 2/5400 loss 33.274063 loss_att 44.654434 loss_ctc 49.235794 loss_rnnt 28.704334 hw_loss 0.310164 lr 0.00088232 rank 4
2023-02-17 05:51:38,651 DEBUG TRAIN Batch 2/5400 loss 38.365170 loss_att 51.074718 loss_ctc 61.819908 loss_rnnt 32.569973 hw_loss 0.236222 lr 0.00088428 rank 2
2023-02-17 05:51:38,652 DEBUG TRAIN Batch 2/5400 loss 26.111450 loss_att 37.823303 loss_ctc 34.968460 loss_rnnt 22.437628 hw_loss 0.282223 lr 0.00088296 rank 3
2023-02-17 05:51:38,660 DEBUG TRAIN Batch 2/5400 loss 37.694073 loss_att 49.509163 loss_ctc 56.567242 loss_rnnt 32.667427 hw_loss 0.276004 lr 0.00088432 rank 1
2023-02-17 05:52:54,433 DEBUG TRAIN Batch 2/5500 loss 39.840153 loss_att 53.314857 loss_ctc 60.230618 loss_rnnt 34.264297 hw_loss 0.304095 lr 0.00088832 rank 1
2023-02-17 05:52:54,433 DEBUG TRAIN Batch 2/5500 loss 35.337521 loss_att 49.941006 loss_ctc 52.685944 loss_rnnt 29.965803 hw_loss 0.258556 lr 0.00088696 rank 3
2023-02-17 05:52:54,434 DEBUG TRAIN Batch 2/5500 loss 72.239838 loss_att 85.274307 loss_ctc 97.057144 loss_rnnt 66.185051 hw_loss 0.260473 lr 0.00088828 rank 2
2023-02-17 05:52:54,440 DEBUG TRAIN Batch 2/5500 loss 27.526335 loss_att 39.789253 loss_ctc 41.854710 loss_rnnt 23.040041 hw_loss 0.231115 lr 0.00088640 rank 5
2023-02-17 05:52:54,440 DEBUG TRAIN Batch 2/5500 loss 37.921638 loss_att 47.282173 loss_ctc 54.022560 loss_rnnt 33.725014 hw_loss 0.333238 lr 0.00088716 rank 6
2023-02-17 05:52:54,447 DEBUG TRAIN Batch 2/5500 loss 27.001268 loss_att 39.320892 loss_ctc 39.030579 loss_rnnt 22.761333 hw_loss 0.322696 lr 0.00088712 rank 0
2023-02-17 05:52:54,447 DEBUG TRAIN Batch 2/5500 loss 41.522434 loss_att 53.906864 loss_ctc 64.402504 loss_rnnt 35.820518 hw_loss 0.326908 lr 0.00088632 rank 4
2023-02-17 05:52:54,448 DEBUG TRAIN Batch 2/5500 loss 34.948231 loss_att 44.591309 loss_ctc 62.686901 loss_rnnt 29.158745 hw_loss 0.304465 lr 0.00088576 rank 7
2023-02-17 05:54:11,166 DEBUG TRAIN Batch 2/5600 loss 46.984097 loss_att 51.062855 loss_ctc 64.102356 loss_rnnt 43.747078 hw_loss 0.260302 lr 0.00088976 rank 7
2023-02-17 05:54:11,166 DEBUG TRAIN Batch 2/5600 loss 37.465073 loss_att 43.945618 loss_ctc 52.399548 loss_rnnt 34.015957 hw_loss 0.303270 lr 0.00089112 rank 0
2023-02-17 05:54:11,167 DEBUG TRAIN Batch 2/5600 loss 63.648346 loss_att 73.120010 loss_ctc 81.654831 loss_rnnt 59.178474 hw_loss 0.327509 lr 0.00089228 rank 2
2023-02-17 05:54:11,171 DEBUG TRAIN Batch 2/5600 loss 25.123386 loss_att 33.461674 loss_ctc 41.627605 loss_rnnt 21.097033 hw_loss 0.296502 lr 0.00089040 rank 5
2023-02-17 05:54:11,175 DEBUG TRAIN Batch 2/5600 loss 38.963715 loss_att 47.205444 loss_ctc 60.459175 loss_rnnt 34.301304 hw_loss 0.277498 lr 0.00089232 rank 1
2023-02-17 05:54:11,183 DEBUG TRAIN Batch 2/5600 loss 37.553211 loss_att 47.993332 loss_ctc 56.571140 loss_rnnt 32.797283 hw_loss 0.247840 lr 0.00089032 rank 4
2023-02-17 05:54:11,219 DEBUG TRAIN Batch 2/5600 loss 44.653606 loss_att 57.359901 loss_ctc 63.182045 loss_rnnt 39.458138 hw_loss 0.344537 lr 0.00089116 rank 6
2023-02-17 05:54:11,221 DEBUG TRAIN Batch 2/5600 loss 46.056187 loss_att 55.432285 loss_ctc 81.391479 loss_rnnt 39.369759 hw_loss 0.187196 lr 0.00089096 rank 3
2023-02-17 05:55:45,257 DEBUG TRAIN Batch 2/5700 loss 33.364597 loss_att 37.404541 loss_ctc 49.138454 loss_rnnt 30.319067 hw_loss 0.251928 lr 0.00089628 rank 2
2023-02-17 05:55:45,257 DEBUG TRAIN Batch 2/5700 loss 24.730423 loss_att 37.704147 loss_ctc 42.538799 loss_rnnt 19.643654 hw_loss 0.220450 lr 0.00089440 rank 5
2023-02-17 05:55:45,258 DEBUG TRAIN Batch 2/5700 loss 20.694204 loss_att 27.384741 loss_ctc 36.114712 loss_rnnt 17.196791 hw_loss 0.193573 lr 0.00089432 rank 4
2023-02-17 05:55:45,259 DEBUG TRAIN Batch 2/5700 loss 18.344025 loss_att 18.871265 loss_ctc 23.728359 loss_rnnt 17.339067 hw_loss 0.340495 lr 0.00089376 rank 7
2023-02-17 05:55:45,259 DEBUG TRAIN Batch 2/5700 loss 25.924023 loss_att 37.253208 loss_ctc 40.286137 loss_rnnt 21.635448 hw_loss 0.202105 lr 0.00089512 rank 0
2023-02-17 05:55:45,260 DEBUG TRAIN Batch 2/5700 loss 28.517321 loss_att 28.374706 loss_ctc 41.934021 loss_rnnt 26.570124 hw_loss 0.350301 lr 0.00089496 rank 3
2023-02-17 05:55:45,301 DEBUG TRAIN Batch 2/5700 loss 27.469143 loss_att 33.321053 loss_ctc 40.836128 loss_rnnt 24.365520 hw_loss 0.283083 lr 0.00089632 rank 1
2023-02-17 05:55:45,304 DEBUG TRAIN Batch 2/5700 loss 37.223843 loss_att 44.604992 loss_ctc 53.459808 loss_rnnt 33.433716 hw_loss 0.279563 lr 0.00089516 rank 6
2023-02-17 05:57:01,368 DEBUG TRAIN Batch 2/5800 loss 27.171011 loss_att 34.757984 loss_ctc 40.221661 loss_rnnt 23.771893 hw_loss 0.265568 lr 0.00089896 rank 3
2023-02-17 05:57:01,369 DEBUG TRAIN Batch 2/5800 loss 27.717541 loss_att 37.697784 loss_ctc 46.249420 loss_rnnt 23.125462 hw_loss 0.234587 lr 0.00089912 rank 0
2023-02-17 05:57:01,369 DEBUG TRAIN Batch 2/5800 loss 27.861847 loss_att 41.820290 loss_ctc 37.535278 loss_rnnt 23.625738 hw_loss 0.289927 lr 0.00090028 rank 2
2023-02-17 05:57:01,370 DEBUG TRAIN Batch 2/5800 loss 25.087261 loss_att 38.142532 loss_ctc 39.943016 loss_rnnt 20.365112 hw_loss 0.244365 lr 0.00089916 rank 6
2023-02-17 05:57:01,370 DEBUG TRAIN Batch 2/5800 loss 52.713047 loss_att 70.665665 loss_ctc 73.098999 loss_rnnt 46.286541 hw_loss 0.220988 lr 0.00089840 rank 5
2023-02-17 05:57:01,372 DEBUG TRAIN Batch 2/5800 loss 51.377766 loss_att 57.206863 loss_ctc 80.852753 loss_rnnt 46.122833 hw_loss 0.298341 lr 0.00089776 rank 7
2023-02-17 05:57:01,376 DEBUG TRAIN Batch 2/5800 loss 46.608536 loss_att 59.452232 loss_ctc 68.548599 loss_rnnt 40.989098 hw_loss 0.235045 lr 0.00090032 rank 1
2023-02-17 05:57:01,429 DEBUG TRAIN Batch 2/5800 loss 33.196255 loss_att 44.870064 loss_ctc 50.693924 loss_rnnt 28.407364 hw_loss 0.227067 lr 0.00089832 rank 4
2023-02-17 05:58:17,331 DEBUG TRAIN Batch 2/5900 loss 48.353546 loss_att 58.455570 loss_ctc 68.332672 loss_rnnt 43.528801 hw_loss 0.263358 lr 0.00090296 rank 3
2023-02-17 05:58:17,333 DEBUG TRAIN Batch 2/5900 loss 74.574150 loss_att 86.419945 loss_ctc 84.964508 loss_rnnt 70.714531 hw_loss 0.197022 lr 0.00090232 rank 4
2023-02-17 05:58:17,335 DEBUG TRAIN Batch 2/5900 loss 40.706268 loss_att 47.579880 loss_ctc 66.177567 loss_rnnt 35.817268 hw_loss 0.221448 lr 0.00090316 rank 6
2023-02-17 05:58:17,337 DEBUG TRAIN Batch 2/5900 loss 25.872206 loss_att 36.760368 loss_ctc 48.375881 loss_rnnt 20.581631 hw_loss 0.210848 lr 0.00090432 rank 1
2023-02-17 05:58:17,338 DEBUG TRAIN Batch 2/5900 loss 31.022400 loss_att 42.636654 loss_ctc 47.500839 loss_rnnt 26.389256 hw_loss 0.212186 lr 0.00090176 rank 7
2023-02-17 05:58:17,365 DEBUG TRAIN Batch 2/5900 loss 25.590042 loss_att 38.814037 loss_ctc 32.620865 loss_rnnt 21.894054 hw_loss 0.213274 lr 0.00090312 rank 0
2023-02-17 05:58:17,380 DEBUG TRAIN Batch 2/5900 loss 47.237881 loss_att 66.016724 loss_ctc 71.460907 loss_rnnt 40.144341 hw_loss 0.202566 lr 0.00090240 rank 5
2023-02-17 05:58:17,380 DEBUG TRAIN Batch 2/5900 loss 45.045963 loss_att 53.803566 loss_ctc 66.621872 loss_rnnt 40.233635 hw_loss 0.345029 lr 0.00090428 rank 2
2023-02-17 05:59:38,617 DEBUG TRAIN Batch 2/6000 loss 40.834450 loss_att 51.108517 loss_ctc 61.285748 loss_rnnt 35.929024 hw_loss 0.232068 lr 0.00090696 rank 3
2023-02-17 05:59:38,622 DEBUG TRAIN Batch 2/6000 loss 26.870258 loss_att 37.523392 loss_ctc 39.664028 loss_rnnt 22.861116 hw_loss 0.323774 lr 0.00090640 rank 5
2023-02-17 05:59:38,641 DEBUG TRAIN Batch 2/6000 loss 60.458626 loss_att 68.674911 loss_ctc 81.037247 loss_rnnt 55.930885 hw_loss 0.263751 lr 0.00090576 rank 7
2023-02-17 05:59:38,653 DEBUG TRAIN Batch 2/6000 loss 32.278427 loss_att 39.344395 loss_ctc 52.680603 loss_rnnt 28.013762 hw_loss 0.245961 lr 0.00090712 rank 0
2023-02-17 05:59:38,685 DEBUG TRAIN Batch 2/6000 loss 31.910063 loss_att 45.457680 loss_ctc 48.564842 loss_rnnt 26.840940 hw_loss 0.260551 lr 0.00090632 rank 4
2023-02-17 05:59:38,697 DEBUG TRAIN Batch 2/6000 loss 38.834255 loss_att 46.518280 loss_ctc 52.167618 loss_rnnt 35.359661 hw_loss 0.300021 lr 0.00090828 rank 2
2023-02-17 05:59:38,698 DEBUG TRAIN Batch 2/6000 loss 33.752594 loss_att 53.655212 loss_ctc 57.634396 loss_rnnt 26.421528 hw_loss 0.311812 lr 0.00090716 rank 6
2023-02-17 05:59:38,699 DEBUG TRAIN Batch 2/6000 loss 37.939835 loss_att 54.281487 loss_ctc 55.655701 loss_rnnt 32.182583 hw_loss 0.237763 lr 0.00090832 rank 1
2023-02-17 06:01:07,630 DEBUG TRAIN Batch 2/6100 loss 26.632883 loss_att 35.589905 loss_ctc 40.513073 loss_rnnt 22.861153 hw_loss 0.243062 lr 0.00091096 rank 3
2023-02-17 06:01:07,632 DEBUG TRAIN Batch 2/6100 loss 26.226631 loss_att 42.880352 loss_ctc 36.713322 loss_rnnt 21.339924 hw_loss 0.295760 lr 0.00091040 rank 5
2023-02-17 06:01:07,634 DEBUG TRAIN Batch 2/6100 loss 47.389153 loss_att 58.571926 loss_ctc 71.872833 loss_rnnt 41.700764 hw_loss 0.351271 lr 0.00090976 rank 7
2023-02-17 06:01:07,634 DEBUG TRAIN Batch 2/6100 loss 36.597469 loss_att 47.621704 loss_ctc 49.050850 loss_rnnt 32.597404 hw_loss 0.252681 lr 0.00091032 rank 4
2023-02-17 06:01:07,639 DEBUG TRAIN Batch 2/6100 loss 51.811871 loss_att 66.019363 loss_ctc 69.878792 loss_rnnt 46.468773 hw_loss 0.173768 lr 0.00091228 rank 2
2023-02-17 06:01:07,642 DEBUG TRAIN Batch 2/6100 loss 31.997639 loss_att 38.273788 loss_ctc 42.277382 loss_rnnt 29.222260 hw_loss 0.280348 lr 0.00091112 rank 0
2023-02-17 06:01:07,644 DEBUG TRAIN Batch 2/6100 loss 32.502110 loss_att 44.268158 loss_ctc 46.909035 loss_rnnt 28.075512 hw_loss 0.285876 lr 0.00091232 rank 1
2023-02-17 06:01:07,680 DEBUG TRAIN Batch 2/6100 loss 36.235332 loss_att 50.898773 loss_ctc 50.626431 loss_rnnt 31.252369 hw_loss 0.246489 lr 0.00091116 rank 6
2023-02-17 06:02:23,931 DEBUG TRAIN Batch 2/6200 loss 35.772392 loss_att 51.055878 loss_ctc 46.150887 loss_rnnt 31.167248 hw_loss 0.308714 lr 0.00091516 rank 6
2023-02-17 06:02:23,931 DEBUG TRAIN Batch 2/6200 loss 35.229298 loss_att 48.293800 loss_ctc 60.752895 loss_rnnt 29.074030 hw_loss 0.261028 lr 0.00091496 rank 3
2023-02-17 06:02:23,936 DEBUG TRAIN Batch 2/6200 loss 23.005249 loss_att 35.925709 loss_ctc 33.936375 loss_rnnt 18.828468 hw_loss 0.253508 lr 0.00091632 rank 1
2023-02-17 06:02:23,937 DEBUG TRAIN Batch 2/6200 loss 20.841490 loss_att 25.854446 loss_ctc 27.277748 loss_rnnt 18.811682 hw_loss 0.316967 lr 0.00091376 rank 7
2023-02-17 06:02:23,938 DEBUG TRAIN Batch 2/6200 loss 44.590004 loss_att 51.886700 loss_ctc 57.117126 loss_rnnt 41.251778 hw_loss 0.391131 lr 0.00091512 rank 0
2023-02-17 06:02:23,939 DEBUG TRAIN Batch 2/6200 loss 34.047653 loss_att 40.725273 loss_ctc 50.204906 loss_rnnt 30.351974 hw_loss 0.385973 lr 0.00091440 rank 5
2023-02-17 06:02:23,940 DEBUG TRAIN Batch 2/6200 loss 19.020901 loss_att 27.523613 loss_ctc 32.020660 loss_rnnt 15.430599 hw_loss 0.293358 lr 0.00091628 rank 2
2023-02-17 06:02:23,980 DEBUG TRAIN Batch 2/6200 loss 42.788216 loss_att 53.282501 loss_ctc 63.267441 loss_rnnt 37.839996 hw_loss 0.222746 lr 0.00091432 rank 4
2023-02-17 06:03:39,177 DEBUG TRAIN Batch 2/6300 loss 29.122280 loss_att 34.002884 loss_ctc 42.895927 loss_rnnt 26.185093 hw_loss 0.233583 lr 0.00091916 rank 6
2023-02-17 06:03:39,179 DEBUG TRAIN Batch 2/6300 loss 26.955315 loss_att 32.588181 loss_ctc 33.745827 loss_rnnt 24.785931 hw_loss 0.257647 lr 0.00091832 rank 4
2023-02-17 06:03:39,180 DEBUG TRAIN Batch 2/6300 loss 46.994511 loss_att 61.895969 loss_ctc 81.416405 loss_rnnt 39.309738 hw_loss 0.215426 lr 0.00091840 rank 5
2023-02-17 06:03:39,181 DEBUG TRAIN Batch 2/6300 loss 23.516153 loss_att 27.527008 loss_ctc 39.260529 loss_rnnt 20.408348 hw_loss 0.386970 lr 0.00091912 rank 0
2023-02-17 06:03:39,236 DEBUG TRAIN Batch 2/6300 loss 36.592606 loss_att 40.529564 loss_ctc 51.527493 loss_rnnt 33.628471 hw_loss 0.347670 lr 0.00092028 rank 2
2023-02-17 06:03:39,237 DEBUG TRAIN Batch 2/6300 loss 31.147821 loss_att 37.012062 loss_ctc 46.210327 loss_rnnt 27.849463 hw_loss 0.219709 lr 0.00092032 rank 1
2023-02-17 06:03:39,252 DEBUG TRAIN Batch 2/6300 loss 24.310114 loss_att 30.103512 loss_ctc 34.279110 loss_rnnt 21.675257 hw_loss 0.275579 lr 0.00091776 rank 7
2023-02-17 06:03:39,253 DEBUG TRAIN Batch 2/6300 loss 47.130333 loss_att 47.114170 loss_ctc 56.078003 loss_rnnt 45.731972 hw_loss 0.391063 lr 0.00091896 rank 3
2023-02-17 06:05:13,147 DEBUG TRAIN Batch 2/6400 loss 23.795115 loss_att 28.220629 loss_ctc 36.668758 loss_rnnt 21.029560 hw_loss 0.307434 lr 0.00092232 rank 4
2023-02-17 06:05:13,148 DEBUG TRAIN Batch 2/6400 loss 37.569061 loss_att 41.288254 loss_ctc 48.079826 loss_rnnt 35.352871 hw_loss 0.132967 lr 0.00092240 rank 5
2023-02-17 06:05:13,150 DEBUG TRAIN Batch 2/6400 loss 31.076078 loss_att 38.715687 loss_ctc 45.524818 loss_rnnt 27.460648 hw_loss 0.301891 lr 0.00092296 rank 3
2023-02-17 06:05:13,152 DEBUG TRAIN Batch 2/6400 loss 75.754753 loss_att 91.135895 loss_ctc 97.298660 loss_rnnt 69.696579 hw_loss 0.205166 lr 0.00092432 rank 1
2023-02-17 06:05:13,151 DEBUG TRAIN Batch 2/6400 loss 54.367874 loss_att 63.134811 loss_ctc 70.125748 loss_rnnt 50.416748 hw_loss 0.181295 lr 0.00092428 rank 2
2023-02-17 06:05:13,191 DEBUG TRAIN Batch 2/6400 loss 45.365417 loss_att 62.969650 loss_ctc 66.409561 loss_rnnt 38.908283 hw_loss 0.244506 lr 0.00092312 rank 0
2023-02-17 06:05:13,194 DEBUG TRAIN Batch 2/6400 loss 39.708965 loss_att 56.262238 loss_ctc 64.445496 loss_rnnt 32.903526 hw_loss 0.368583 lr 0.00092176 rank 7
2023-02-17 06:05:13,199 DEBUG TRAIN Batch 2/6400 loss 30.934792 loss_att 35.397606 loss_ctc 45.066895 loss_rnnt 27.982014 hw_loss 0.329874 lr 0.00092316 rank 6
2023-02-17 06:06:32,118 DEBUG TRAIN Batch 2/6500 loss 33.261383 loss_att 43.106953 loss_ctc 46.636505 loss_rnnt 29.362768 hw_loss 0.274037 lr 0.00092712 rank 0
2023-02-17 06:06:32,122 DEBUG TRAIN Batch 2/6500 loss 21.988377 loss_att 32.360844 loss_ctc 30.761459 loss_rnnt 18.638144 hw_loss 0.198748 lr 0.00092716 rank 6
2023-02-17 06:06:32,127 DEBUG TRAIN Batch 2/6500 loss 51.769108 loss_att 60.989300 loss_ctc 65.430344 loss_rnnt 47.927582 hw_loss 0.329979 lr 0.00092640 rank 5
2023-02-17 06:06:32,128 DEBUG TRAIN Batch 2/6500 loss 36.404552 loss_att 45.621098 loss_ctc 59.832329 loss_rnnt 31.310734 hw_loss 0.237760 lr 0.00092696 rank 3
2023-02-17 06:06:32,129 DEBUG TRAIN Batch 2/6500 loss 33.850971 loss_att 48.456909 loss_ctc 54.663147 loss_rnnt 28.029106 hw_loss 0.235733 lr 0.00092576 rank 7
2023-02-17 06:06:32,133 DEBUG TRAIN Batch 2/6500 loss 41.910896 loss_att 51.557034 loss_ctc 58.366161 loss_rnnt 37.674603 hw_loss 0.211937 lr 0.00092832 rank 1
2023-02-17 06:06:32,195 DEBUG TRAIN Batch 2/6500 loss 46.044861 loss_att 59.631161 loss_ctc 64.537567 loss_rnnt 40.739517 hw_loss 0.229470 lr 0.00092632 rank 4
2023-02-17 06:06:32,202 DEBUG TRAIN Batch 2/6500 loss 32.590057 loss_att 40.894405 loss_ctc 45.120449 loss_rnnt 29.151190 hw_loss 0.201148 lr 0.00092828 rank 2
2023-02-17 06:07:49,713 DEBUG TRAIN Batch 2/6600 loss 34.078846 loss_att 46.120880 loss_ctc 65.708832 loss_rnnt 27.303448 hw_loss 0.280616 lr 0.00092976 rank 7
2023-02-17 06:07:49,715 DEBUG TRAIN Batch 2/6600 loss 36.265984 loss_att 50.544331 loss_ctc 55.944164 loss_rnnt 30.620199 hw_loss 0.311914 lr 0.00093228 rank 2
2023-02-17 06:07:49,715 DEBUG TRAIN Batch 2/6600 loss 19.273148 loss_att 26.727945 loss_ctc 35.863701 loss_rnnt 15.413889 hw_loss 0.292923 lr 0.00093032 rank 4
2023-02-17 06:07:49,717 DEBUG TRAIN Batch 2/6600 loss 56.053265 loss_att 62.964104 loss_ctc 75.302795 loss_rnnt 51.941971 hw_loss 0.304728 lr 0.00093112 rank 0
2023-02-17 06:07:49,718 DEBUG TRAIN Batch 2/6600 loss 36.527462 loss_att 49.784111 loss_ctc 59.503860 loss_rnnt 30.639511 hw_loss 0.324560 lr 0.00093232 rank 1
2023-02-17 06:07:49,721 DEBUG TRAIN Batch 2/6600 loss 47.972286 loss_att 56.567459 loss_ctc 67.441574 loss_rnnt 43.500053 hw_loss 0.294930 lr 0.00093096 rank 3
2023-02-17 06:07:49,726 DEBUG TRAIN Batch 2/6600 loss 33.262627 loss_att 44.469082 loss_ctc 48.331757 loss_rnnt 28.883888 hw_loss 0.240433 lr 0.00093116 rank 6
2023-02-17 06:07:49,737 DEBUG TRAIN Batch 2/6600 loss 28.640673 loss_att 43.346130 loss_ctc 46.498306 loss_rnnt 23.171318 hw_loss 0.276085 lr 0.00093040 rank 5
2023-02-17 06:09:09,610 DEBUG TRAIN Batch 2/6700 loss 47.552120 loss_att 54.995674 loss_ctc 66.936852 loss_rnnt 43.357109 hw_loss 0.228140 lr 0.00093516 rank 6
2023-02-17 06:09:09,613 DEBUG TRAIN Batch 2/6700 loss 42.535728 loss_att 52.633762 loss_ctc 59.461098 loss_rnnt 38.140045 hw_loss 0.223803 lr 0.00093376 rank 7
2023-02-17 06:09:09,631 DEBUG TRAIN Batch 2/6700 loss 23.971342 loss_att 33.778694 loss_ctc 32.520107 loss_rnnt 20.740252 hw_loss 0.243342 lr 0.00093628 rank 2
2023-02-17 06:09:09,634 DEBUG TRAIN Batch 2/6700 loss 32.655117 loss_att 42.153881 loss_ctc 47.911160 loss_rnnt 28.577852 hw_loss 0.268825 lr 0.00093512 rank 0
2023-02-17 06:09:09,637 DEBUG TRAIN Batch 2/6700 loss 51.471458 loss_att 62.508614 loss_ctc 71.336273 loss_rnnt 46.513100 hw_loss 0.191778 lr 0.00093432 rank 4
2023-02-17 06:09:09,645 DEBUG TRAIN Batch 2/6700 loss 41.968037 loss_att 56.468235 loss_ctc 58.759914 loss_rnnt 36.721771 hw_loss 0.201193 lr 0.00093632 rank 1
2023-02-17 06:09:09,663 DEBUG TRAIN Batch 2/6700 loss 37.616470 loss_att 49.794605 loss_ctc 57.477516 loss_rnnt 32.382359 hw_loss 0.281895 lr 0.00093496 rank 3
2023-02-17 06:09:09,667 DEBUG TRAIN Batch 2/6700 loss 26.308050 loss_att 31.278484 loss_ctc 41.477768 loss_rnnt 23.105778 hw_loss 0.347915 lr 0.00093440 rank 5
2023-02-17 06:10:40,134 DEBUG TRAIN Batch 2/6800 loss 28.377022 loss_att 34.864761 loss_ctc 46.386639 loss_rnnt 24.524773 hw_loss 0.287657 lr 0.00093916 rank 6
2023-02-17 06:10:40,135 DEBUG TRAIN Batch 2/6800 loss 55.947453 loss_att 65.375519 loss_ctc 80.106506 loss_rnnt 50.649147 hw_loss 0.359027 lr 0.00093896 rank 3
2023-02-17 06:10:40,135 DEBUG TRAIN Batch 2/6800 loss 34.864933 loss_att 50.802132 loss_ctc 52.692055 loss_rnnt 29.136141 hw_loss 0.308255 lr 0.00094032 rank 1
2023-02-17 06:10:40,137 DEBUG TRAIN Batch 2/6800 loss 35.479645 loss_att 46.695038 loss_ctc 56.361244 loss_rnnt 30.333090 hw_loss 0.223615 lr 0.00094028 rank 2
2023-02-17 06:10:40,137 DEBUG TRAIN Batch 2/6800 loss 34.830318 loss_att 42.109726 loss_ctc 51.289772 loss_rnnt 31.042088 hw_loss 0.258296 lr 0.00093832 rank 4
2023-02-17 06:10:40,142 DEBUG TRAIN Batch 2/6800 loss 34.044903 loss_att 37.781708 loss_ctc 49.559959 loss_rnnt 31.013741 hw_loss 0.403369 lr 0.00093840 rank 5
2023-02-17 06:10:40,142 DEBUG TRAIN Batch 2/6800 loss 45.668556 loss_att 59.227051 loss_ctc 65.500809 loss_rnnt 40.142708 hw_loss 0.318468 lr 0.00093776 rank 7
2023-02-17 06:10:40,148 DEBUG TRAIN Batch 2/6800 loss 59.388710 loss_att 72.637985 loss_ctc 76.120407 loss_rnnt 54.384850 hw_loss 0.230839 lr 0.00093912 rank 0
2023-02-17 06:11:56,979 DEBUG TRAIN Batch 2/6900 loss 73.317795 loss_att 78.053932 loss_ctc 108.223755 loss_rnnt 67.579010 hw_loss 0.257688 lr 0.00094240 rank 5
2023-02-17 06:11:56,979 DEBUG TRAIN Batch 2/6900 loss 23.094624 loss_att 25.944542 loss_ctc 30.870214 loss_rnnt 21.332800 hw_loss 0.290804 lr 0.00094428 rank 2
2023-02-17 06:11:56,982 DEBUG TRAIN Batch 2/6900 loss 34.778011 loss_att 48.506462 loss_ctc 56.523140 loss_rnnt 29.000589 hw_loss 0.248210 lr 0.00094296 rank 3
2023-02-17 06:11:56,984 DEBUG TRAIN Batch 2/6900 loss 37.829288 loss_att 41.587566 loss_ctc 48.922436 loss_rnnt 35.429440 hw_loss 0.317081 lr 0.00094312 rank 0
2023-02-17 06:11:57,012 DEBUG TRAIN Batch 2/6900 loss 27.457815 loss_att 39.295525 loss_ctc 43.996124 loss_rnnt 22.745182 hw_loss 0.262465 lr 0.00094316 rank 6
2023-02-17 06:11:57,015 DEBUG TRAIN Batch 2/6900 loss 27.141045 loss_att 35.719368 loss_ctc 44.437065 loss_rnnt 22.973564 hw_loss 0.273152 lr 0.00094176 rank 7
2023-02-17 06:11:57,026 DEBUG TRAIN Batch 2/6900 loss 32.265144 loss_att 39.660030 loss_ctc 47.765034 loss_rnnt 28.596668 hw_loss 0.230327 lr 0.00094432 rank 1
2023-02-17 06:11:57,075 DEBUG TRAIN Batch 2/6900 loss 27.895348 loss_att 34.344116 loss_ctc 39.797356 loss_rnnt 24.888872 hw_loss 0.243351 lr 0.00094232 rank 4
2023-02-17 06:13:13,607 DEBUG TRAIN Batch 2/7000 loss 18.651995 loss_att 18.249447 loss_ctc 24.364582 loss_rnnt 17.762991 hw_loss 0.389690 lr 0.00094696 rank 3
2023-02-17 06:13:13,609 DEBUG TRAIN Batch 2/7000 loss 25.179556 loss_att 29.301102 loss_ctc 36.530441 loss_rnnt 22.672039 hw_loss 0.318292 lr 0.00094716 rank 6
2023-02-17 06:13:13,611 DEBUG TRAIN Batch 2/7000 loss 40.677929 loss_att 44.909271 loss_ctc 52.409863 loss_rnnt 38.186611 hw_loss 0.151492 lr 0.00094632 rank 4
2023-02-17 06:13:13,612 DEBUG TRAIN Batch 2/7000 loss 41.482018 loss_att 56.150856 loss_ctc 60.858131 loss_rnnt 35.879173 hw_loss 0.160486 lr 0.00094828 rank 2
2023-02-17 06:13:13,612 DEBUG TRAIN Batch 2/7000 loss 36.144497 loss_att 53.747532 loss_ctc 54.533260 loss_rnnt 30.027203 hw_loss 0.271594 lr 0.00094640 rank 5
2023-02-17 06:13:13,613 DEBUG TRAIN Batch 2/7000 loss 38.312008 loss_att 40.835041 loss_ctc 50.794632 loss_rnnt 35.965923 hw_loss 0.332117 lr 0.00094832 rank 1
2023-02-17 06:13:13,613 DEBUG TRAIN Batch 2/7000 loss 38.893833 loss_att 46.239796 loss_ctc 55.185833 loss_rnnt 35.121075 hw_loss 0.246195 lr 0.00094576 rank 7
2023-02-17 06:13:13,649 DEBUG TRAIN Batch 2/7000 loss 16.456314 loss_att 24.279354 loss_ctc 28.086098 loss_rnnt 13.298930 hw_loss 0.079007 lr 0.00094712 rank 0
2023-02-17 06:14:39,849 DEBUG TRAIN Batch 2/7100 loss 45.530293 loss_att 58.948868 loss_ctc 64.426086 loss_rnnt 40.198437 hw_loss 0.241313 lr 0.00095032 rank 4
2023-02-17 06:14:39,861 DEBUG TRAIN Batch 2/7100 loss 39.700424 loss_att 50.099838 loss_ctc 59.549835 loss_rnnt 34.832336 hw_loss 0.265534 lr 0.00095096 rank 3
2023-02-17 06:14:39,862 DEBUG TRAIN Batch 2/7100 loss 56.026264 loss_att 65.633400 loss_ctc 82.104965 loss_rnnt 50.422638 hw_loss 0.384454 lr 0.00095112 rank 0
2023-02-17 06:14:39,899 DEBUG TRAIN Batch 2/7100 loss 52.991661 loss_att 60.644272 loss_ctc 67.051804 loss_rnnt 49.487709 hw_loss 0.185140 lr 0.00095232 rank 1
2023-02-17 06:14:39,906 DEBUG TRAIN Batch 2/7100 loss 27.116207 loss_att 40.809792 loss_ctc 42.497658 loss_rnnt 22.205219 hw_loss 0.227641 lr 0.00095040 rank 5
2023-02-17 06:14:39,906 DEBUG TRAIN Batch 2/7100 loss 42.967697 loss_att 60.744171 loss_ctc 68.958763 loss_rnnt 35.793621 hw_loss 0.287445 lr 0.00095228 rank 2
2023-02-17 06:14:39,934 DEBUG TRAIN Batch 2/7100 loss 23.813440 loss_att 32.324402 loss_ctc 35.882374 loss_rnnt 20.385460 hw_loss 0.218617 lr 0.00094976 rank 7
2023-02-17 06:14:39,936 DEBUG TRAIN Batch 2/7100 loss 39.181541 loss_att 46.869659 loss_ctc 54.910500 loss_rnnt 35.388359 hw_loss 0.296931 lr 0.00095116 rank 6
2023-02-17 06:16:06,525 DEBUG TRAIN Batch 2/7200 loss 42.197895 loss_att 56.175659 loss_ctc 62.113327 loss_rnnt 36.599216 hw_loss 0.276996 lr 0.00095512 rank 0
2023-02-17 06:16:06,526 DEBUG TRAIN Batch 2/7200 loss 32.000858 loss_att 41.158638 loss_ctc 49.591415 loss_rnnt 27.699291 hw_loss 0.233637 lr 0.00095496 rank 3
2023-02-17 06:16:06,526 DEBUG TRAIN Batch 2/7200 loss 28.022928 loss_att 35.618866 loss_ctc 40.268227 loss_rnnt 24.720451 hw_loss 0.282339 lr 0.00095628 rank 2
2023-02-17 06:16:06,530 DEBUG TRAIN Batch 2/7200 loss 33.908669 loss_att 44.002106 loss_ctc 44.846737 loss_rnnt 30.321600 hw_loss 0.206198 lr 0.00095516 rank 6
2023-02-17 06:16:06,530 DEBUG TRAIN Batch 2/7200 loss 34.772522 loss_att 49.405632 loss_ctc 56.876091 loss_rnnt 28.756493 hw_loss 0.266738 lr 0.00095432 rank 4
2023-02-17 06:16:06,532 DEBUG TRAIN Batch 2/7200 loss 30.462755 loss_att 37.798702 loss_ctc 45.605206 loss_rnnt 26.826588 hw_loss 0.281217 lr 0.00095376 rank 7
2023-02-17 06:16:06,531 DEBUG TRAIN Batch 2/7200 loss 55.564087 loss_att 68.830475 loss_ctc 78.902740 loss_rnnt 49.669052 hw_loss 0.243625 lr 0.00095440 rank 5
2023-02-17 06:16:06,578 DEBUG TRAIN Batch 2/7200 loss 32.369862 loss_att 50.640499 loss_ctc 43.697441 loss_rnnt 27.082014 hw_loss 0.231332 lr 0.00095632 rank 1
2023-02-17 06:17:22,743 DEBUG TRAIN Batch 2/7300 loss 25.598888 loss_att 33.549572 loss_ctc 39.452675 loss_rnnt 22.017708 hw_loss 0.269760 lr 0.00095776 rank 7
2023-02-17 06:17:22,750 DEBUG TRAIN Batch 2/7300 loss 25.395786 loss_att 30.519966 loss_ctc 33.166084 loss_rnnt 23.222713 hw_loss 0.210372 lr 0.00095840 rank 5
2023-02-17 06:17:22,755 DEBUG TRAIN Batch 2/7300 loss 20.767454 loss_att 32.321110 loss_ctc 34.197327 loss_rnnt 16.500494 hw_loss 0.310463 lr 0.00095832 rank 4
2023-02-17 06:17:22,760 DEBUG TRAIN Batch 2/7300 loss 51.805508 loss_att 63.229698 loss_ctc 74.638191 loss_rnnt 46.327404 hw_loss 0.279207 lr 0.00095896 rank 3
2023-02-17 06:17:22,759 DEBUG TRAIN Batch 2/7300 loss 52.596649 loss_att 63.977654 loss_ctc 71.657730 loss_rnnt 47.592926 hw_loss 0.348836 lr 0.00095912 rank 0
2023-02-17 06:17:22,786 DEBUG TRAIN Batch 2/7300 loss 35.142391 loss_att 45.402313 loss_ctc 53.762127 loss_rnnt 30.422104 hw_loss 0.348133 lr 0.00095916 rank 6
2023-02-17 06:17:22,800 DEBUG TRAIN Batch 2/7300 loss 26.965887 loss_att 39.533508 loss_ctc 43.904419 loss_rnnt 22.045250 hw_loss 0.278703 lr 0.00096032 rank 1
2023-02-17 06:17:22,802 DEBUG TRAIN Batch 2/7300 loss 51.430225 loss_att 53.046074 loss_ctc 65.424339 loss_rnnt 49.119595 hw_loss 0.227956 lr 0.00096028 rank 2
2023-02-17 06:18:41,777 DEBUG TRAIN Batch 2/7400 loss 23.805405 loss_att 29.295795 loss_ctc 34.737434 loss_rnnt 21.060255 hw_loss 0.355253 lr 0.00096428 rank 2
2023-02-17 06:18:41,788 DEBUG TRAIN Batch 2/7400 loss 47.158379 loss_att 57.974586 loss_ctc 66.216675 loss_rnnt 42.350094 hw_loss 0.194892 lr 0.00096176 rank 7
2023-02-17 06:18:41,788 DEBUG TRAIN Batch 2/7400 loss 31.122087 loss_att 38.341595 loss_ctc 52.860180 loss_rnnt 26.656635 hw_loss 0.230880 lr 0.00096232 rank 4
2023-02-17 06:18:41,803 DEBUG TRAIN Batch 2/7400 loss 38.665783 loss_att 51.990868 loss_ctc 48.927563 loss_rnnt 34.483505 hw_loss 0.279418 lr 0.00096432 rank 1
2023-02-17 06:18:41,826 DEBUG TRAIN Batch 2/7400 loss 22.758638 loss_att 32.452782 loss_ctc 34.349503 loss_rnnt 19.174831 hw_loss 0.186614 lr 0.00096312 rank 0
2023-02-17 06:18:41,827 DEBUG TRAIN Batch 2/7400 loss 33.727062 loss_att 43.478024 loss_ctc 50.092621 loss_rnnt 29.444412 hw_loss 0.281976 lr 0.00096296 rank 3
2023-02-17 06:18:41,852 DEBUG TRAIN Batch 2/7400 loss 28.226944 loss_att 44.988167 loss_ctc 49.453125 loss_rnnt 21.934319 hw_loss 0.206670 lr 0.00096316 rank 6
2023-02-17 06:18:41,858 DEBUG TRAIN Batch 2/7400 loss 23.954344 loss_att 28.522141 loss_ctc 38.630280 loss_rnnt 20.938072 hw_loss 0.273604 lr 0.00096240 rank 5
2023-02-17 06:20:13,516 DEBUG TRAIN Batch 2/7500 loss 35.835709 loss_att 40.674175 loss_ctc 44.501633 loss_rnnt 33.580673 hw_loss 0.247285 lr 0.00096576 rank 7
2023-02-17 06:20:13,518 DEBUG TRAIN Batch 2/7500 loss 32.770355 loss_att 41.866394 loss_ctc 42.672386 loss_rnnt 29.447512 hw_loss 0.343811 lr 0.00096832 rank 1
2023-02-17 06:20:13,519 DEBUG TRAIN Batch 2/7500 loss 25.275801 loss_att 31.454754 loss_ctc 35.023441 loss_rnnt 22.582190 hw_loss 0.296500 lr 0.00096696 rank 3
2023-02-17 06:20:13,521 DEBUG TRAIN Batch 2/7500 loss 31.300732 loss_att 37.595291 loss_ctc 46.928127 loss_rnnt 27.779144 hw_loss 0.335667 lr 0.00096828 rank 2
2023-02-17 06:20:13,525 DEBUG TRAIN Batch 2/7500 loss 30.061110 loss_att 42.502895 loss_ctc 45.072392 loss_rnnt 25.447678 hw_loss 0.231689 lr 0.00096716 rank 6
2023-02-17 06:20:13,558 DEBUG TRAIN Batch 2/7500 loss 33.374554 loss_att 39.007496 loss_ctc 52.798157 loss_rnnt 29.578176 hw_loss 0.149957 lr 0.00096640 rank 5
2023-02-17 06:20:13,588 DEBUG TRAIN Batch 2/7500 loss 41.938313 loss_att 49.188095 loss_ctc 59.966908 loss_rnnt 37.956749 hw_loss 0.239613 lr 0.00096632 rank 4
2023-02-17 06:20:13,606 DEBUG TRAIN Batch 2/7500 loss 28.537865 loss_att 31.411324 loss_ctc 43.324306 loss_rnnt 25.861526 hw_loss 0.243974 lr 0.00096712 rank 0
2023-02-17 06:21:30,394 DEBUG TRAIN Batch 2/7600 loss 22.089470 loss_att 32.528942 loss_ctc 34.997253 loss_rnnt 18.119516 hw_loss 0.301913 lr 0.00097116 rank 6
2023-02-17 06:21:30,396 DEBUG TRAIN Batch 2/7600 loss 23.708199 loss_att 27.394909 loss_ctc 39.253876 loss_rnnt 20.685778 hw_loss 0.398100 lr 0.00097112 rank 0
2023-02-17 06:21:30,399 DEBUG TRAIN Batch 2/7600 loss 38.462677 loss_att 47.252708 loss_ctc 57.389477 loss_rnnt 34.050106 hw_loss 0.245607 lr 0.00097040 rank 5
2023-02-17 06:21:30,399 DEBUG TRAIN Batch 2/7600 loss 28.299522 loss_att 33.512989 loss_ctc 44.364967 loss_rnnt 24.938681 hw_loss 0.330167 lr 0.00097096 rank 3
2023-02-17 06:21:30,400 DEBUG TRAIN Batch 2/7600 loss 26.795456 loss_att 33.205986 loss_ctc 42.052666 loss_rnnt 23.297720 hw_loss 0.340002 lr 0.00097232 rank 1
2023-02-17 06:21:30,404 DEBUG TRAIN Batch 2/7600 loss 44.894501 loss_att 58.901604 loss_ctc 73.836655 loss_rnnt 38.060570 hw_loss 0.325415 lr 0.00097228 rank 2
2023-02-17 06:21:30,405 DEBUG TRAIN Batch 2/7600 loss 31.432383 loss_att 41.514530 loss_ctc 48.074043 loss_rnnt 27.040943 hw_loss 0.292724 lr 0.00097032 rank 4
2023-02-17 06:21:30,418 DEBUG TRAIN Batch 2/7600 loss 42.270271 loss_att 53.315788 loss_ctc 59.842190 loss_rnnt 37.586605 hw_loss 0.246825 lr 0.00096976 rank 7
2023-02-17 06:22:47,244 DEBUG TRAIN Batch 2/7700 loss 28.254045 loss_att 33.128990 loss_ctc 41.892841 loss_rnnt 25.267456 hw_loss 0.362049 lr 0.00097516 rank 6
2023-02-17 06:22:47,245 DEBUG TRAIN Batch 2/7700 loss 45.111053 loss_att 50.386433 loss_ctc 56.141876 loss_rnnt 42.429668 hw_loss 0.291624 lr 0.00097496 rank 3
2023-02-17 06:22:47,245 DEBUG TRAIN Batch 2/7700 loss 41.251839 loss_att 43.483582 loss_ctc 56.143593 loss_rnnt 38.639954 hw_loss 0.337438 lr 0.00097432 rank 4
2023-02-17 06:22:47,248 DEBUG TRAIN Batch 2/7700 loss 32.983856 loss_att 47.285503 loss_ctc 43.439552 loss_rnnt 28.631573 hw_loss 0.183486 lr 0.00097376 rank 7
2023-02-17 06:22:47,251 DEBUG TRAIN Batch 2/7700 loss 39.931839 loss_att 52.500595 loss_ctc 58.302582 loss_rnnt 34.784866 hw_loss 0.344597 lr 0.00097632 rank 1
2023-02-17 06:22:47,252 DEBUG TRAIN Batch 2/7700 loss 19.029354 loss_att 34.521122 loss_ctc 33.755661 loss_rnnt 13.889332 hw_loss 0.146552 lr 0.00097440 rank 5
2023-02-17 06:22:47,269 DEBUG TRAIN Batch 2/7700 loss 46.802074 loss_att 54.811081 loss_ctc 61.584118 loss_rnnt 43.088455 hw_loss 0.264136 lr 0.00097512 rank 0
2023-02-17 06:22:47,292 DEBUG TRAIN Batch 2/7700 loss 38.887257 loss_att 47.156357 loss_ctc 57.402618 loss_rnnt 34.660244 hw_loss 0.195893 lr 0.00097628 rank 2
2023-02-17 06:24:11,145 DEBUG TRAIN Batch 2/7800 loss 34.084919 loss_att 53.947021 loss_ctc 51.391262 loss_rnnt 27.705242 hw_loss 0.187027 lr 0.00097916 rank 6
2023-02-17 06:24:11,169 DEBUG TRAIN Batch 2/7800 loss 22.797152 loss_att 31.913818 loss_ctc 34.986839 loss_rnnt 19.208330 hw_loss 0.262866 lr 0.00097912 rank 0
2023-02-17 06:24:11,175 DEBUG TRAIN Batch 2/7800 loss 43.898861 loss_att 62.396210 loss_ctc 67.910675 loss_rnnt 36.910423 hw_loss 0.163859 lr 0.00097832 rank 4
2023-02-17 06:24:11,188 DEBUG TRAIN Batch 2/7800 loss 36.715824 loss_att 35.272717 loss_ctc 48.896111 loss_rnnt 35.218410 hw_loss 0.303748 lr 0.00098028 rank 2
2023-02-17 06:24:11,190 DEBUG TRAIN Batch 2/7800 loss 33.131123 loss_att 38.095798 loss_ctc 55.388893 loss_rnnt 29.061071 hw_loss 0.205155 lr 0.00097776 rank 7
2023-02-17 06:24:11,192 DEBUG TRAIN Batch 2/7800 loss 33.741875 loss_att 42.861603 loss_ctc 50.269417 loss_rnnt 29.608013 hw_loss 0.199214 lr 0.00098032 rank 1
2023-02-17 06:24:11,196 DEBUG TRAIN Batch 2/7800 loss 25.764532 loss_att 37.243134 loss_ctc 42.496689 loss_rnnt 21.082127 hw_loss 0.291995 lr 0.00097840 rank 5
2023-02-17 06:24:11,199 DEBUG TRAIN Batch 2/7800 loss 31.576336 loss_att 42.056595 loss_ctc 51.300880 loss_rnnt 26.727295 hw_loss 0.230718 lr 0.00097896 rank 3
2023-02-17 06:25:39,963 DEBUG TRAIN Batch 2/7900 loss 31.175747 loss_att 35.812031 loss_ctc 45.038277 loss_rnnt 28.255301 hw_loss 0.271597 lr 0.00098432 rank 1
2023-02-17 06:25:39,963 DEBUG TRAIN Batch 2/7900 loss 32.880909 loss_att 41.905380 loss_ctc 54.999840 loss_rnnt 27.963665 hw_loss 0.305917 lr 0.00098240 rank 5
2023-02-17 06:25:39,963 DEBUG TRAIN Batch 2/7900 loss 28.105717 loss_att 41.023300 loss_ctc 43.428238 loss_rnnt 23.293587 hw_loss 0.348013 lr 0.00098296 rank 3
2023-02-17 06:25:39,963 DEBUG TRAIN Batch 2/7900 loss 38.618004 loss_att 37.733109 loss_ctc 47.134399 loss_rnnt 37.493839 hw_loss 0.310541 lr 0.00098176 rank 7
2023-02-17 06:25:39,968 DEBUG TRAIN Batch 2/7900 loss 24.007500 loss_att 31.396370 loss_ctc 32.869728 loss_rnnt 21.280823 hw_loss 0.126139 lr 0.00098232 rank 4
2023-02-17 06:25:39,968 DEBUG TRAIN Batch 2/7900 loss 28.413534 loss_att 35.037193 loss_ctc 44.340019 loss_rnnt 24.814795 hw_loss 0.282147 lr 0.00098312 rank 0
2023-02-17 06:25:39,970 DEBUG TRAIN Batch 2/7900 loss 61.121391 loss_att 68.520447 loss_ctc 87.392075 loss_rnnt 56.018387 hw_loss 0.225814 lr 0.00098428 rank 2
2023-02-17 06:25:39,988 DEBUG TRAIN Batch 2/7900 loss 34.065144 loss_att 43.452034 loss_ctc 51.738277 loss_rnnt 29.676840 hw_loss 0.289696 lr 0.00098316 rank 6
2023-02-17 06:26:55,961 DEBUG TRAIN Batch 2/8000 loss 37.490700 loss_att 52.586445 loss_ctc 62.492462 loss_rnnt 31.012342 hw_loss 0.235576 lr 0.00098832 rank 1
2023-02-17 06:26:55,961 DEBUG TRAIN Batch 2/8000 loss 58.111176 loss_att 68.863892 loss_ctc 72.120888 loss_rnnt 53.976280 hw_loss 0.218234 lr 0.00098632 rank 4
2023-02-17 06:26:55,965 DEBUG TRAIN Batch 2/8000 loss 24.589987 loss_att 32.995571 loss_ctc 40.963989 loss_rnnt 20.607477 hw_loss 0.221610 lr 0.00098696 rank 3
2023-02-17 06:26:55,983 DEBUG TRAIN Batch 2/8000 loss 34.451012 loss_att 48.710541 loss_ctc 51.427250 loss_rnnt 29.148270 hw_loss 0.351254 lr 0.00098712 rank 0
2023-02-17 06:26:55,989 DEBUG TRAIN Batch 2/8000 loss 40.933926 loss_att 52.884533 loss_ctc 54.534149 loss_rnnt 36.601242 hw_loss 0.242241 lr 0.00098640 rank 5
2023-02-17 06:26:56,001 DEBUG TRAIN Batch 2/8000 loss 24.679310 loss_att 32.854027 loss_ctc 40.414883 loss_rnnt 20.841856 hw_loss 0.195815 lr 0.00098828 rank 2
2023-02-17 06:26:56,006 DEBUG TRAIN Batch 2/8000 loss 21.018772 loss_att 30.281994 loss_ctc 33.526222 loss_rnnt 17.373756 hw_loss 0.233833 lr 0.00098576 rank 7
2023-02-17 06:26:56,006 DEBUG TRAIN Batch 2/8000 loss 12.822269 loss_att 22.495363 loss_ctc 23.268265 loss_rnnt 9.346085 hw_loss 0.278939 lr 0.00098716 rank 6
2023-02-17 06:28:13,516 DEBUG TRAIN Batch 2/8100 loss 24.326427 loss_att 37.241661 loss_ctc 35.197937 loss_rnnt 20.104410 hw_loss 0.355188 lr 0.00098976 rank 7
2023-02-17 06:28:13,523 DEBUG TRAIN Batch 2/8100 loss 21.532879 loss_att 22.118301 loss_ctc 29.830505 loss_rnnt 20.098789 hw_loss 0.394980 lr 0.00099040 rank 5
2023-02-17 06:28:13,523 DEBUG TRAIN Batch 2/8100 loss 32.694511 loss_att 43.092659 loss_ctc 44.625717 loss_rnnt 28.879616 hw_loss 0.270820 lr 0.00099116 rank 6
2023-02-17 06:28:13,525 DEBUG TRAIN Batch 2/8100 loss 26.353487 loss_att 31.453735 loss_ctc 36.181793 loss_rnnt 23.873707 hw_loss 0.279916 lr 0.00099228 rank 2
2023-02-17 06:28:13,562 DEBUG TRAIN Batch 2/8100 loss 44.857815 loss_att 51.501175 loss_ctc 63.385162 loss_rnnt 40.947201 hw_loss 0.209307 lr 0.00099112 rank 0
2023-02-17 06:28:13,562 DEBUG TRAIN Batch 2/8100 loss 29.958712 loss_att 44.155235 loss_ctc 46.209644 loss_rnnt 24.787453 hw_loss 0.309681 lr 0.00099032 rank 4
2023-02-17 06:28:13,563 DEBUG TRAIN Batch 2/8100 loss 53.276615 loss_att 64.614929 loss_ctc 69.915359 loss_rnnt 48.617516 hw_loss 0.324261 lr 0.00099232 rank 1
2023-02-17 06:28:13,607 DEBUG TRAIN Batch 2/8100 loss 26.120068 loss_att 32.989223 loss_ctc 41.936081 loss_rnnt 22.504732 hw_loss 0.248813 lr 0.00099096 rank 3
2023-02-17 06:29:33,055 DEBUG TRAIN Batch 2/8200 loss 25.446762 loss_att 32.853516 loss_ctc 37.848846 loss_rnnt 22.180725 hw_loss 0.245760 lr 0.00099496 rank 3
2023-02-17 06:29:33,055 DEBUG TRAIN Batch 2/8200 loss 36.877014 loss_att 37.484558 loss_ctc 43.692455 loss_rnnt 35.605324 hw_loss 0.452730 lr 0.00099628 rank 2
2023-02-17 06:29:33,059 DEBUG TRAIN Batch 2/8200 loss 25.192755 loss_att 34.867680 loss_ctc 32.127457 loss_rnnt 22.187765 hw_loss 0.272583 lr 0.00099516 rank 6
2023-02-17 06:29:33,058 DEBUG TRAIN Batch 2/8200 loss 30.373962 loss_att 41.560307 loss_ctc 44.995754 loss_rnnt 26.085850 hw_loss 0.189878 lr 0.00099432 rank 4
2023-02-17 06:29:33,062 DEBUG TRAIN Batch 2/8200 loss 55.699989 loss_att 64.336456 loss_ctc 66.613503 loss_rnnt 52.406879 hw_loss 0.207522 lr 0.00099632 rank 1
2023-02-17 06:29:33,063 DEBUG TRAIN Batch 2/8200 loss 20.835968 loss_att 25.243114 loss_ctc 32.666988 loss_rnnt 18.183332 hw_loss 0.363258 lr 0.00099512 rank 0
2023-02-17 06:29:33,064 DEBUG TRAIN Batch 2/8200 loss 36.262028 loss_att 50.899048 loss_ctc 57.380173 loss_rnnt 30.389334 hw_loss 0.242884 lr 0.00099440 rank 5
2023-02-17 06:29:33,069 DEBUG TRAIN Batch 2/8200 loss 40.889393 loss_att 53.924927 loss_ctc 59.949554 loss_rnnt 35.631798 hw_loss 0.204632 lr 0.00099376 rank 7
2023-02-17 06:30:49,461 DEBUG TRAIN Batch 2/8300 loss 43.788963 loss_att 59.546150 loss_ctc 58.714722 loss_rnnt 38.490040 hw_loss 0.295091 lr 0.00099896 rank 3
2023-02-17 06:30:49,463 DEBUG TRAIN Batch 2/8300 loss 25.947142 loss_att 28.488697 loss_ctc 38.241161 loss_rnnt 23.609171 hw_loss 0.357108 lr 0.00099984 rank 1
2023-02-17 06:30:49,466 DEBUG TRAIN Batch 2/8300 loss 24.942471 loss_att 33.489002 loss_ctc 35.995552 loss_rnnt 21.608078 hw_loss 0.283764 lr 0.00099832 rank 4
2023-02-17 06:30:49,497 DEBUG TRAIN Batch 2/8300 loss 44.305683 loss_att 54.959091 loss_ctc 64.231308 loss_rnnt 39.352509 hw_loss 0.310775 lr 0.00099916 rank 6
2023-02-17 06:30:49,502 DEBUG TRAIN Batch 2/8300 loss 53.518738 loss_att 74.082436 loss_ctc 83.301422 loss_rnnt 45.293648 hw_loss 0.264982 lr 0.00099840 rank 5
2023-02-17 06:30:49,509 DEBUG TRAIN Batch 2/8300 loss 33.684345 loss_att 39.099689 loss_ctc 47.655952 loss_rnnt 30.611259 hw_loss 0.238384 lr 0.00099776 rank 7
2023-02-17 06:30:49,508 DEBUG TRAIN Batch 2/8300 loss 46.697750 loss_att 57.849747 loss_ctc 70.966034 loss_rnnt 41.124489 hw_loss 0.200782 lr 0.00099986 rank 2
2023-02-17 06:30:49,544 DEBUG TRAIN Batch 2/8300 loss 38.467194 loss_att 45.949356 loss_ctc 54.506264 loss_rnnt 34.675064 hw_loss 0.294668 lr 0.00099912 rank 0
2023-02-17 06:31:51,427 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 5
2023-02-17 06:31:51,435 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 1
2023-02-17 06:31:51,442 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 6
2023-02-17 06:31:51,443 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 2
2023-02-17 06:31:51,445 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 0
2023-02-17 06:31:51,451 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 4
2023-02-17 06:31:51,455 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 3
2023-02-17 06:31:51,457 DEBUG CV Batch 2/0 loss 7.110981 loss_att 7.003853 loss_ctc 9.554440 loss_rnnt 6.549516 hw_loss 0.482055 history loss 6.847611 rank 7
2023-02-17 06:32:04,923 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 7
2023-02-17 06:32:05,022 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 6
2023-02-17 06:32:05,226 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 5
2023-02-17 06:32:05,280 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 2
2023-02-17 06:32:05,288 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 3
2023-02-17 06:32:05,339 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 0
2023-02-17 06:32:05,393 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 4
2023-02-17 06:32:05,629 DEBUG CV Batch 2/100 loss 27.833130 loss_att 34.762115 loss_ctc 40.370956 loss_rnnt 24.594646 hw_loss 0.339330 history loss 13.911495 rank 1
2023-02-17 06:32:21,646 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 5
2023-02-17 06:32:21,748 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 7
2023-02-17 06:32:22,055 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 3
2023-02-17 06:32:22,099 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 2
2023-02-17 06:32:22,203 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 0
2023-02-17 06:32:22,344 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 6
2023-02-17 06:32:22,700 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 4
2023-02-17 06:32:22,818 DEBUG CV Batch 2/200 loss 41.388039 loss_att 80.703720 loss_ctc 53.400768 loss_rnnt 31.814785 hw_loss 0.203289 history loss 15.414274 rank 1
2023-02-17 06:32:35,469 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 5
2023-02-17 06:32:36,149 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 7
2023-02-17 06:32:36,295 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 6
2023-02-17 06:32:36,416 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 0
2023-02-17 06:32:36,507 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 3
2023-02-17 06:32:36,509 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 2
2023-02-17 06:32:37,083 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 4
2023-02-17 06:32:37,513 DEBUG CV Batch 2/300 loss 19.528278 loss_att 24.842241 loss_ctc 29.866188 loss_rnnt 16.924982 hw_loss 0.303964 history loss 15.515963 rank 1
2023-02-17 06:32:49,597 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 5
2023-02-17 06:32:50,132 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 6
2023-02-17 06:32:50,583 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 2
2023-02-17 06:32:50,673 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 3
2023-02-17 06:32:50,917 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 7
2023-02-17 06:32:51,048 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 0
2023-02-17 06:32:53,437 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 4
2023-02-17 06:32:53,677 DEBUG CV Batch 2/400 loss 62.001904 loss_att 224.556091 loss_ctc 65.240181 loss_rnnt 28.928959 hw_loss 0.244378 history loss 17.211960 rank 1
2023-02-17 06:33:03,872 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 5
2023-02-17 06:33:04,084 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 6
2023-02-17 06:33:04,202 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 7
2023-02-17 06:33:04,285 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 2
2023-02-17 06:33:04,417 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 0
2023-02-17 06:33:04,518 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 3
2023-02-17 06:33:07,161 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 4
2023-02-17 06:33:07,641 DEBUG CV Batch 2/500 loss 30.632549 loss_att 33.688114 loss_ctc 43.643158 loss_rnnt 28.141193 hw_loss 0.272803 history loss 18.555378 rank 1
2023-02-17 06:33:18,923 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 5
2023-02-17 06:33:19,498 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 6
2023-02-17 06:33:19,572 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 2
2023-02-17 06:33:19,912 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 3
2023-02-17 06:33:20,008 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 0
2023-02-17 06:33:20,134 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 7
2023-02-17 06:33:23,134 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 1
2023-02-17 06:33:23,500 DEBUG CV Batch 2/600 loss 17.760429 loss_att 20.158772 loss_ctc 23.454185 loss_rnnt 16.299141 hw_loss 0.417101 history loss 20.112723 rank 4
2023-02-17 06:33:34,139 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 5
2023-02-17 06:33:34,600 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 3
2023-02-17 06:33:34,646 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 0
2023-02-17 06:33:34,766 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 2
2023-02-17 06:33:34,816 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 6
2023-02-17 06:33:35,385 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 7
2023-02-17 06:33:38,916 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 1
2023-02-17 06:33:39,101 DEBUG CV Batch 2/700 loss 81.664528 loss_att 180.904053 loss_ctc 96.889565 loss_rnnt 59.667198 hw_loss 0.223896 history loss 21.281119 rank 4
2023-02-17 06:33:49,826 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 5
2023-02-17 06:33:49,988 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 2
2023-02-17 06:33:50,042 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 3
2023-02-17 06:33:50,283 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 0
2023-02-17 06:33:50,601 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 6
2023-02-17 06:33:50,836 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 7
2023-02-17 06:33:54,175 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 1
2023-02-17 06:33:54,620 DEBUG CV Batch 2/800 loss 28.583496 loss_att 33.955212 loss_ctc 41.939705 loss_rnnt 25.524105 hw_loss 0.382917 history loss 20.261765 rank 4
2023-02-17 06:34:06,106 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 2
2023-02-17 06:34:06,119 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 5
2023-02-17 06:34:06,320 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 3
2023-02-17 06:34:06,717 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 6
2023-02-17 06:34:06,818 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 0
2023-02-17 06:34:07,065 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 7
2023-02-17 06:34:10,721 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 1
2023-02-17 06:34:11,361 DEBUG CV Batch 2/900 loss 48.809528 loss_att 92.556221 loss_ctc 64.191849 loss_rnnt 37.871346 hw_loss 0.258502 history loss 19.995155 rank 4
2023-02-17 06:34:20,062 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 5
2023-02-17 06:34:20,526 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 2
2023-02-17 06:34:20,569 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 3
2023-02-17 06:34:20,603 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354516 history loss 19.566970 rank 6
2023-02-17 06:34:21,190 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 0
2023-02-17 06:34:21,473 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 7
2023-02-17 06:34:25,211 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 1
2023-02-17 06:34:25,646 DEBUG CV Batch 2/1000 loss 16.343739 loss_att 24.896179 loss_ctc 19.739117 loss_rnnt 13.991461 hw_loss 0.354515 history loss 19.566970 rank 4
2023-02-17 06:34:34,461 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 5
2023-02-17 06:34:34,466 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 3
2023-02-17 06:34:34,632 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 6
2023-02-17 06:34:34,774 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 2
2023-02-17 06:34:35,403 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 0
2023-02-17 06:34:35,612 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 7
2023-02-17 06:34:40,825 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 1
2023-02-17 06:34:40,841 DEBUG CV Batch 2/1100 loss 9.699955 loss_att 9.667805 loss_ctc 13.402979 loss_rnnt 9.026147 hw_loss 0.349692 history loss 19.547423 rank 4
2023-02-17 06:34:47,870 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 5
2023-02-17 06:34:48,109 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 3
2023-02-17 06:34:48,210 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 2
2023-02-17 06:34:48,826 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 7
2023-02-17 06:34:48,883 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 6
2023-02-17 06:34:49,261 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 0
2023-02-17 06:34:54,212 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 1
2023-02-17 06:34:54,532 DEBUG CV Batch 2/1200 loss 30.444431 loss_att 32.464130 loss_ctc 43.443115 loss_rnnt 28.129358 hw_loss 0.333705 history loss 20.109112 rank 4
2023-02-17 06:35:01,924 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 5
2023-02-17 06:35:02,608 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 2
2023-02-17 06:35:02,739 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 3
2023-02-17 06:35:03,469 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 7
2023-02-17 06:35:03,616 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 6
2023-02-17 06:35:03,926 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 0
2023-02-17 06:35:09,409 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 1
2023-02-17 06:35:09,503 DEBUG CV Batch 2/1300 loss 17.313248 loss_att 19.551767 loss_ctc 24.627552 loss_rnnt 15.702551 hw_loss 0.352034 history loss 20.605992 rank 4
2023-02-17 06:35:16,568 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 5
2023-02-17 06:35:18,246 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 2
2023-02-17 06:35:18,490 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 3
2023-02-17 06:35:19,033 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 6
2023-02-17 06:35:19,100 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 7
2023-02-17 06:35:19,858 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 0
2023-02-17 06:35:24,783 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 1
2023-02-17 06:35:25,018 DEBUG CV Batch 2/1400 loss 48.290302 loss_att 99.202682 loss_ctc 62.470364 loss_rnnt 36.095585 hw_loss 0.227938 history loss 21.192349 rank 4
2023-02-17 06:35:32,611 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 5
2023-02-17 06:35:33,519 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 2
2023-02-17 06:35:33,797 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 3
2023-02-17 06:35:34,951 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 6
2023-02-17 06:35:35,308 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 7
2023-02-17 06:35:35,409 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 0
2023-02-17 06:35:41,032 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 4
2023-02-17 06:35:41,136 DEBUG CV Batch 2/1500 loss 24.684221 loss_att 28.611996 loss_ctc 32.615360 loss_rnnt 22.670561 hw_loss 0.319915 history loss 20.764716 rank 1
2023-02-17 06:35:48,603 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 5
2023-02-17 06:35:49,731 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 3
2023-02-17 06:35:50,094 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 2
2023-02-17 06:35:51,158 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 7
2023-02-17 06:35:51,193 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 6
2023-02-17 06:35:51,740 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 0
2023-02-17 06:35:57,794 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 1
2023-02-17 06:35:57,801 DEBUG CV Batch 2/1600 loss 29.154875 loss_att 60.603500 loss_ctc 44.299561 loss_rnnt 20.699469 hw_loss 0.274482 history loss 20.649567 rank 4
2023-02-17 06:36:02,893 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 5
2023-02-17 06:36:04,299 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 3
2023-02-17 06:36:04,901 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 2
2023-02-17 06:36:05,268 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 6
2023-02-17 06:36:06,163 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 0
2023-02-17 06:36:06,243 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 7
2023-02-17 06:36:11,819 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 1
2023-02-17 06:36:11,939 DEBUG CV Batch 2/1700 loss 25.160215 loss_att 29.510258 loss_ctc 35.559563 loss_rnnt 22.706383 hw_loss 0.369837 history loss 20.405593 rank 4
2023-02-17 06:36:13,417 INFO Epoch 2 CV info cv_loss 20.363190185749765
2023-02-17 06:36:13,418 INFO Epoch 3 TRAIN info lr 0.0009993007341435492
2023-02-17 06:36:13,422 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:14,571 INFO Epoch 2 CV info cv_loss 20.363190177858762
2023-02-17 06:36:14,571 INFO Epoch 3 TRAIN info lr 0.000999680153518126
2023-02-17 06:36:14,577 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:15,217 INFO Epoch 2 CV info cv_loss 20.36319018216608
2023-02-17 06:36:15,218 INFO Epoch 3 TRAIN info lr 0.0009988420145056651
2023-02-17 06:36:15,222 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:15,639 INFO Epoch 2 CV info cv_loss 20.363190184681553
2023-02-17 06:36:15,640 INFO Epoch 3 TRAIN info lr 0.0009998
2023-02-17 06:36:15,644 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:16,285 INFO Epoch 2 CV info cv_loss 20.36319018685244
2023-02-17 06:36:16,286 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/2.pt
2023-02-17 06:36:16,387 INFO Epoch 2 CV info cv_loss 20.363190183751172
2023-02-17 06:36:16,388 INFO Epoch 3 TRAIN info lr 0.00099872
2023-02-17 06:36:16,390 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:17,359 INFO Epoch 3 TRAIN info lr 0.0009995403171568558
2023-02-17 06:36:17,363 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:21,465 INFO Epoch 2 CV info cv_loss 20.363190184647095
2023-02-17 06:36:21,465 INFO Epoch 3 TRAIN info lr 0.000999161056920415
2023-02-17 06:36:21,468 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:36:21,663 INFO Epoch 2 CV info cv_loss 20.363190185715307
2023-02-17 06:36:21,663 INFO Epoch 3 TRAIN info lr 0.00099928
2023-02-17 06:36:21,668 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 06:37:39,045 DEBUG TRAIN Batch 3/0 loss 16.608852 loss_att 17.875732 loss_ctc 21.486887 loss_rnnt 15.508310 hw_loss 0.368922 lr 0.00099984 rank 6
2023-02-17 06:37:39,058 DEBUG TRAIN Batch 3/0 loss 23.553070 loss_att 25.221178 loss_ctc 27.522221 loss_rnnt 22.487604 hw_loss 0.379921 lr 0.00099882 rank 2
2023-02-17 06:37:39,067 DEBUG TRAIN Batch 3/0 loss 19.095200 loss_att 18.936817 loss_ctc 23.731279 loss_rnnt 18.343519 hw_loss 0.309773 lr 0.00099966 rank 3
2023-02-17 06:37:39,069 DEBUG TRAIN Batch 3/0 loss 14.126387 loss_att 17.111626 loss_ctc 18.432602 loss_rnnt 12.726693 hw_loss 0.428408 lr 0.00099876 rank 7
2023-02-17 06:37:39,069 DEBUG TRAIN Batch 3/0 loss 17.930845 loss_att 18.415756 loss_ctc 22.255991 loss_rnnt 17.044783 hw_loss 0.398239 lr 0.00099928 rank 5
2023-02-17 06:37:39,071 DEBUG TRAIN Batch 3/0 loss 17.457115 loss_att 18.466127 loss_ctc 21.804867 loss_rnnt 16.475603 hw_loss 0.375018 lr 0.00099952 rank 0
2023-02-17 06:37:39,089 DEBUG TRAIN Batch 3/0 loss 18.832336 loss_att 20.728167 loss_ctc 25.356380 loss_rnnt 17.376234 hw_loss 0.388244 lr 0.00099932 rank 4
2023-02-17 06:37:39,098 DEBUG TRAIN Batch 3/0 loss 17.739220 loss_att 17.978306 loss_ctc 21.192738 loss_rnnt 17.054729 hw_loss 0.330382 lr 0.00099914 rank 1
2023-02-17 06:38:54,913 DEBUG TRAIN Batch 3/100 loss 52.250610 loss_att 65.312218 loss_ctc 69.328369 loss_rnnt 47.267696 hw_loss 0.175419 lr 0.00099834 rank 4
2023-02-17 06:38:54,916 DEBUG TRAIN Batch 3/100 loss 24.708271 loss_att 39.226322 loss_ctc 35.604057 loss_rnnt 20.213783 hw_loss 0.258949 lr 0.00099862 rank 7
2023-02-17 06:38:54,918 DEBUG TRAIN Batch 3/100 loss 47.831596 loss_att 57.048187 loss_ctc 66.766663 loss_rnnt 43.369392 hw_loss 0.176642 lr 0.00099753 rank 0
2023-02-17 06:38:54,953 DEBUG TRAIN Batch 3/100 loss 51.600449 loss_att 64.680359 loss_ctc 75.532074 loss_rnnt 45.676880 hw_loss 0.218817 lr 0.00099684 rank 2
2023-02-17 06:38:54,953 DEBUG TRAIN Batch 3/100 loss 44.984879 loss_att 59.354374 loss_ctc 69.732872 loss_rnnt 38.679947 hw_loss 0.246190 lr 0.00099767 rank 3
2023-02-17 06:38:54,965 DEBUG TRAIN Batch 3/100 loss 29.906437 loss_att 38.394257 loss_ctc 35.899002 loss_rnnt 27.302986 hw_loss 0.200394 lr 0.00099809 rank 6
2023-02-17 06:38:54,986 DEBUG TRAIN Batch 3/100 loss 33.462311 loss_att 42.528660 loss_ctc 48.028805 loss_rnnt 29.529453 hw_loss 0.332608 lr 0.00099729 rank 5
2023-02-17 06:38:54,988 DEBUG TRAIN Batch 3/100 loss 38.781532 loss_att 43.218334 loss_ctc 48.381157 loss_rnnt 36.442699 hw_loss 0.321610 lr 0.00099715 rank 1
2023-02-17 06:40:11,485 DEBUG TRAIN Batch 3/200 loss 63.290783 loss_att 68.058495 loss_ctc 92.921387 loss_rnnt 58.247559 hw_loss 0.260510 lr 0.00099569 rank 3
2023-02-17 06:40:11,489 DEBUG TRAIN Batch 3/200 loss 38.636852 loss_att 50.259926 loss_ctc 55.856628 loss_rnnt 33.791576 hw_loss 0.421298 lr 0.00099664 rank 7
2023-02-17 06:40:11,490 DEBUG TRAIN Batch 3/200 loss 59.576630 loss_att 64.187241 loss_ctc 73.367485 loss_rnnt 56.665253 hw_loss 0.282134 lr 0.00099555 rank 0
2023-02-17 06:40:11,490 DEBUG TRAIN Batch 3/200 loss 16.794716 loss_att 25.013786 loss_ctc 30.360884 loss_rnnt 13.230156 hw_loss 0.209857 lr 0.00099518 rank 1
2023-02-17 06:40:11,490 DEBUG TRAIN Batch 3/200 loss 52.455658 loss_att 62.024017 loss_ctc 69.442818 loss_rnnt 48.164074 hw_loss 0.211796 lr 0.00099531 rank 5
2023-02-17 06:40:11,491 DEBUG TRAIN Batch 3/200 loss 27.055769 loss_att 36.033936 loss_ctc 46.875710 loss_rnnt 22.460112 hw_loss 0.295063 lr 0.00099486 rank 2
2023-02-17 06:40:11,492 DEBUG TRAIN Batch 3/200 loss 37.417427 loss_att 47.820702 loss_ctc 52.223541 loss_rnnt 33.227150 hw_loss 0.254014 lr 0.00099610 rank 6
2023-02-17 06:40:11,493 DEBUG TRAIN Batch 3/200 loss 56.898716 loss_att 60.815266 loss_ctc 81.049576 loss_rnnt 52.732689 hw_loss 0.304884 lr 0.00099636 rank 4
2023-02-17 06:41:32,544 DEBUG TRAIN Batch 3/300 loss 28.093212 loss_att 42.764683 loss_ctc 43.018776 loss_rnnt 22.999359 hw_loss 0.317778 lr 0.00099358 rank 0
2023-02-17 06:41:32,554 DEBUG TRAIN Batch 3/300 loss 30.563543 loss_att 38.821342 loss_ctc 50.754807 loss_rnnt 26.073563 hw_loss 0.274219 lr 0.00099413 rank 6
2023-02-17 06:41:32,555 DEBUG TRAIN Batch 3/300 loss 25.265299 loss_att 36.812698 loss_ctc 35.239807 loss_rnnt 21.466539 hw_loss 0.298771 lr 0.00099372 rank 3
2023-02-17 06:41:32,556 DEBUG TRAIN Batch 3/300 loss 40.364441 loss_att 46.779636 loss_ctc 49.026318 loss_rnnt 37.825302 hw_loss 0.189723 lr 0.00099335 rank 5
2023-02-17 06:41:32,559 DEBUG TRAIN Batch 3/300 loss 25.406237 loss_att 38.066608 loss_ctc 37.465622 loss_rnnt 21.138382 hw_loss 0.239738 lr 0.00099439 rank 4
2023-02-17 06:41:32,564 DEBUG TRAIN Batch 3/300 loss 30.493355 loss_att 39.652828 loss_ctc 42.682007 loss_rnnt 26.879539 hw_loss 0.293931 lr 0.00099466 rank 7
2023-02-17 06:41:32,632 DEBUG TRAIN Batch 3/300 loss 42.137928 loss_att 56.115131 loss_ctc 61.324142 loss_rnnt 36.582783 hw_loss 0.377888 lr 0.00099321 rank 1
2023-02-17 06:41:32,632 DEBUG TRAIN Batch 3/300 loss 21.115973 loss_att 40.450874 loss_ctc 31.857014 loss_rnnt 15.658113 hw_loss 0.297635 lr 0.00099290 rank 2
2023-02-17 06:43:01,758 DEBUG TRAIN Batch 3/400 loss 40.250530 loss_att 50.412441 loss_ctc 57.087250 loss_rnnt 35.859028 hw_loss 0.214171 lr 0.00099094 rank 2
2023-02-17 06:43:01,770 DEBUG TRAIN Batch 3/400 loss 43.085079 loss_att 52.171474 loss_ctc 63.280865 loss_rnnt 38.458412 hw_loss 0.218661 lr 0.00099139 rank 5
2023-02-17 06:43:01,773 DEBUG TRAIN Batch 3/400 loss 22.549072 loss_att 34.558762 loss_ctc 35.642921 loss_rnnt 18.291073 hw_loss 0.206652 lr 0.00099163 rank 0
2023-02-17 06:43:01,777 DEBUG TRAIN Batch 3/400 loss 31.758257 loss_att 42.506512 loss_ctc 50.667351 loss_rnnt 26.956713 hw_loss 0.245023 lr 0.00099126 rank 1
2023-02-17 06:43:01,778 DEBUG TRAIN Batch 3/400 loss 28.331669 loss_att 37.145622 loss_ctc 38.567593 loss_rnnt 25.011841 hw_loss 0.360462 lr 0.00099243 rank 4
2023-02-17 06:43:01,794 DEBUG TRAIN Batch 3/400 loss 25.381947 loss_att 30.937811 loss_ctc 44.554680 loss_rnnt 21.606304 hw_loss 0.202704 lr 0.00099270 rank 7
2023-02-17 06:43:01,801 DEBUG TRAIN Batch 3/400 loss 37.934303 loss_att 48.505978 loss_ctc 52.073418 loss_rnnt 33.826023 hw_loss 0.203871 lr 0.00099176 rank 3
2023-02-17 06:43:01,803 DEBUG TRAIN Batch 3/400 loss 38.098118 loss_att 45.101944 loss_ctc 57.002472 loss_rnnt 34.042465 hw_loss 0.251825 lr 0.00099217 rank 6
2023-02-17 06:44:17,161 DEBUG TRAIN Batch 3/500 loss 36.230511 loss_att 49.229927 loss_ctc 53.312969 loss_rnnt 31.207150 hw_loss 0.273409 lr 0.00098982 rank 3
2023-02-17 06:44:17,163 DEBUG TRAIN Batch 3/500 loss 23.174082 loss_att 28.410515 loss_ctc 34.536591 loss_rnnt 20.390495 hw_loss 0.414932 lr 0.00099075 rank 7
2023-02-17 06:44:17,163 DEBUG TRAIN Batch 3/500 loss 25.029728 loss_att 32.087666 loss_ctc 39.248169 loss_rnnt 21.541615 hw_loss 0.338875 lr 0.00098968 rank 0
2023-02-17 06:44:17,164 DEBUG TRAIN Batch 3/500 loss 26.425087 loss_att 31.274006 loss_ctc 32.994316 loss_rnnt 24.436182 hw_loss 0.268546 lr 0.00099048 rank 4
2023-02-17 06:44:17,165 DEBUG TRAIN Batch 3/500 loss 33.421143 loss_att 46.570229 loss_ctc 50.725845 loss_rnnt 28.305819 hw_loss 0.334147 lr 0.00098945 rank 5
2023-02-17 06:44:17,165 DEBUG TRAIN Batch 3/500 loss 36.004383 loss_att 42.379234 loss_ctc 53.078369 loss_rnnt 32.314644 hw_loss 0.259201 lr 0.00098931 rank 1
2023-02-17 06:44:17,169 DEBUG TRAIN Batch 3/500 loss 25.466940 loss_att 30.580910 loss_ctc 40.918697 loss_rnnt 22.203863 hw_loss 0.337589 lr 0.00099023 rank 6
2023-02-17 06:44:17,170 DEBUG TRAIN Batch 3/500 loss 34.187542 loss_att 40.203796 loss_ctc 48.597393 loss_rnnt 30.885202 hw_loss 0.333325 lr 0.00098900 rank 2
2023-02-17 06:45:33,707 DEBUG TRAIN Batch 3/600 loss 25.688919 loss_att 42.971283 loss_ctc 48.045341 loss_rnnt 19.021753 hw_loss 0.430944 lr 0.00098707 rank 2
2023-02-17 06:45:33,707 DEBUG TRAIN Batch 3/600 loss 20.063591 loss_att 21.650301 loss_ctc 28.779337 loss_rnnt 18.415701 hw_loss 0.315844 lr 0.00098738 rank 1
2023-02-17 06:45:33,713 DEBUG TRAIN Batch 3/600 loss 30.984293 loss_att 32.714825 loss_ctc 42.826118 loss_rnnt 28.827002 hw_loss 0.435519 lr 0.00098775 rank 0
2023-02-17 06:45:33,716 DEBUG TRAIN Batch 3/600 loss 32.953007 loss_att 40.671505 loss_ctc 48.853134 loss_rnnt 29.142258 hw_loss 0.275691 lr 0.00098881 rank 7
2023-02-17 06:45:33,740 DEBUG TRAIN Batch 3/600 loss 16.607296 loss_att 18.423958 loss_ctc 22.860287 loss_rnnt 15.176990 hw_loss 0.437325 lr 0.00098829 rank 6
2023-02-17 06:45:33,747 DEBUG TRAIN Batch 3/600 loss 38.100662 loss_att 41.355301 loss_ctc 52.023563 loss_rnnt 35.404541 hw_loss 0.354017 lr 0.00098854 rank 4
2023-02-17 06:45:33,749 DEBUG TRAIN Batch 3/600 loss 34.238056 loss_att 38.155975 loss_ctc 48.047314 loss_rnnt 31.459509 hw_loss 0.288249 lr 0.00098752 rank 5
2023-02-17 06:45:33,750 DEBUG TRAIN Batch 3/600 loss 36.136398 loss_att 38.694321 loss_ctc 47.283337 loss_rnnt 33.951786 hw_loss 0.350193 lr 0.00098788 rank 3
2023-02-17 06:46:58,459 DEBUG TRAIN Batch 3/700 loss 42.551136 loss_att 56.749680 loss_ctc 64.558746 loss_rnnt 36.623806 hw_loss 0.287392 lr 0.00098546 rank 1
2023-02-17 06:46:58,461 DEBUG TRAIN Batch 3/700 loss 26.230362 loss_att 41.612411 loss_ctc 52.541809 loss_rnnt 19.494877 hw_loss 0.282905 lr 0.00098516 rank 2
2023-02-17 06:46:58,462 DEBUG TRAIN Batch 3/700 loss 43.199699 loss_att 48.802685 loss_ctc 51.652569 loss_rnnt 40.822151 hw_loss 0.243559 lr 0.00098636 rank 6
2023-02-17 06:46:58,486 DEBUG TRAIN Batch 3/700 loss 46.485374 loss_att 65.343277 loss_ctc 73.528519 loss_rnnt 38.954456 hw_loss 0.287975 lr 0.00098583 rank 0
2023-02-17 06:46:58,503 DEBUG TRAIN Batch 3/700 loss 24.106119 loss_att 33.198997 loss_ctc 31.431751 loss_rnnt 21.166868 hw_loss 0.269856 lr 0.00098661 rank 4
2023-02-17 06:46:58,502 DEBUG TRAIN Batch 3/700 loss 17.306263 loss_att 26.603292 loss_ctc 28.982956 loss_rnnt 13.717934 hw_loss 0.322558 lr 0.00098688 rank 7
2023-02-17 06:46:58,529 DEBUG TRAIN Batch 3/700 loss 38.720524 loss_att 48.179680 loss_ctc 56.179550 loss_rnnt 34.339478 hw_loss 0.302528 lr 0.00098596 rank 3
2023-02-17 06:46:58,534 DEBUG TRAIN Batch 3/700 loss 31.440285 loss_att 41.313766 loss_ctc 50.822922 loss_rnnt 26.707611 hw_loss 0.325548 lr 0.00098560 rank 5
2023-02-17 06:48:23,433 DEBUG TRAIN Batch 3/800 loss 52.245880 loss_att 61.928585 loss_ctc 76.076569 loss_rnnt 46.997139 hw_loss 0.252699 lr 0.00098445 rank 6
2023-02-17 06:48:23,435 DEBUG TRAIN Batch 3/800 loss 20.131367 loss_att 28.283306 loss_ctc 35.248489 loss_rnnt 16.367731 hw_loss 0.220554 lr 0.00098392 rank 0
2023-02-17 06:48:23,438 DEBUG TRAIN Batch 3/800 loss 34.453819 loss_att 39.166744 loss_ctc 53.344429 loss_rnnt 30.869894 hw_loss 0.229860 lr 0.00098497 rank 7
2023-02-17 06:48:23,442 DEBUG TRAIN Batch 3/800 loss 46.959805 loss_att 64.612686 loss_ctc 64.849197 loss_rnnt 40.895866 hw_loss 0.277709 lr 0.00098355 rank 1
2023-02-17 06:48:23,443 DEBUG TRAIN Batch 3/800 loss 22.840931 loss_att 32.671860 loss_ctc 37.673332 loss_rnnt 18.721281 hw_loss 0.329642 lr 0.00098369 rank 5
2023-02-17 06:48:23,445 DEBUG TRAIN Batch 3/800 loss 37.042366 loss_att 45.719780 loss_ctc 59.445602 loss_rnnt 32.172218 hw_loss 0.276684 lr 0.00098470 rank 4
2023-02-17 06:48:23,491 DEBUG TRAIN Batch 3/800 loss 33.437332 loss_att 37.567432 loss_ctc 38.823574 loss_rnnt 31.723532 hw_loss 0.318031 lr 0.00098405 rank 3
2023-02-17 06:48:23,492 DEBUG TRAIN Batch 3/800 loss 21.255886 loss_att 30.179981 loss_ctc 35.615036 loss_rnnt 17.361961 hw_loss 0.364780 lr 0.00098325 rank 2
2023-02-17 06:49:39,112 DEBUG TRAIN Batch 3/900 loss 44.262798 loss_att 56.335182 loss_ctc 59.398678 loss_rnnt 39.669601 hw_loss 0.301122 lr 0.00098279 rank 4
2023-02-17 06:49:39,126 DEBUG TRAIN Batch 3/900 loss 29.131199 loss_att 34.977417 loss_ctc 37.228691 loss_rnnt 26.714043 hw_loss 0.315461 lr 0.00098255 rank 6
2023-02-17 06:49:39,126 DEBUG TRAIN Batch 3/900 loss 35.117653 loss_att 44.868423 loss_ctc 52.121338 loss_rnnt 30.746853 hw_loss 0.287790 lr 0.00098306 rank 7
2023-02-17 06:49:39,127 DEBUG TRAIN Batch 3/900 loss 46.355381 loss_att 56.826256 loss_ctc 66.374161 loss_rnnt 41.447334 hw_loss 0.271315 lr 0.00098179 rank 5
2023-02-17 06:49:39,129 DEBUG TRAIN Batch 3/900 loss 49.704494 loss_att 59.228439 loss_ctc 78.348206 loss_rnnt 43.827660 hw_loss 0.286665 lr 0.00098135 rank 2
2023-02-17 06:49:39,130 DEBUG TRAIN Batch 3/900 loss 33.561920 loss_att 43.903080 loss_ctc 50.304062 loss_rnnt 29.099504 hw_loss 0.303558 lr 0.00098202 rank 0
2023-02-17 06:49:39,203 DEBUG TRAIN Batch 3/900 loss 23.212343 loss_att 28.702461 loss_ctc 36.422302 loss_rnnt 20.219898 hw_loss 0.249552 lr 0.00098166 rank 1
2023-02-17 06:49:39,204 DEBUG TRAIN Batch 3/900 loss 27.686802 loss_att 36.772499 loss_ctc 37.462906 loss_rnnt 24.385906 hw_loss 0.338021 lr 0.00098215 rank 3
2023-02-17 06:50:58,549 DEBUG TRAIN Batch 3/1000 loss 17.001106 loss_att 28.149376 loss_ctc 28.811274 loss_rnnt 13.020698 hw_loss 0.330123 lr 0.00097977 rank 1
2023-02-17 06:50:58,551 DEBUG TRAIN Batch 3/1000 loss 45.677048 loss_att 45.284103 loss_ctc 54.008385 loss_rnnt 44.448154 hw_loss 0.368690 lr 0.00098117 rank 7
2023-02-17 06:50:58,554 DEBUG TRAIN Batch 3/1000 loss 27.430204 loss_att 30.558044 loss_ctc 41.273067 loss_rnnt 24.844803 hw_loss 0.213970 lr 0.00098090 rank 4
2023-02-17 06:50:58,554 DEBUG TRAIN Batch 3/1000 loss 52.075710 loss_att 65.497864 loss_ctc 76.543823 loss_rnnt 46.003994 hw_loss 0.234139 lr 0.00097990 rank 5
2023-02-17 06:50:58,585 DEBUG TRAIN Batch 3/1000 loss 40.713566 loss_att 49.105923 loss_ctc 61.279434 loss_rnnt 36.096622 hw_loss 0.368163 lr 0.00098026 rank 3
2023-02-17 06:50:58,589 DEBUG TRAIN Batch 3/1000 loss 35.115864 loss_att 42.950577 loss_ctc 53.490959 loss_rnnt 30.987953 hw_loss 0.208044 lr 0.00098066 rank 6
2023-02-17 06:50:58,629 DEBUG TRAIN Batch 3/1000 loss 18.694624 loss_att 27.383362 loss_ctc 34.264618 loss_rnnt 14.743043 hw_loss 0.258435 lr 0.00098013 rank 0
2023-02-17 06:50:58,630 DEBUG TRAIN Batch 3/1000 loss 30.827354 loss_att 41.762680 loss_ctc 45.283688 loss_rnnt 26.552702 hw_loss 0.300144 lr 0.00097947 rank 2
2023-02-17 06:52:30,943 DEBUG TRAIN Batch 3/1100 loss 33.301086 loss_att 39.460129 loss_ctc 52.271034 loss_rnnt 29.433790 hw_loss 0.199055 lr 0.00097790 rank 1
2023-02-17 06:52:30,948 DEBUG TRAIN Batch 3/1100 loss 29.561413 loss_att 34.274387 loss_ctc 38.655396 loss_rnnt 27.253342 hw_loss 0.286772 lr 0.00097878 rank 6
2023-02-17 06:52:30,954 DEBUG TRAIN Batch 3/1100 loss 50.134239 loss_att 55.561729 loss_ctc 69.034050 loss_rnnt 46.317329 hw_loss 0.396441 lr 0.00097928 rank 7
2023-02-17 06:52:30,954 DEBUG TRAIN Batch 3/1100 loss 24.069332 loss_att 28.089714 loss_ctc 43.333275 loss_rnnt 20.520168 hw_loss 0.331056 lr 0.00097760 rank 2
2023-02-17 06:52:30,955 DEBUG TRAIN Batch 3/1100 loss 19.027054 loss_att 25.949974 loss_ctc 29.885111 loss_rnnt 16.090073 hw_loss 0.196229 lr 0.00097838 rank 3
2023-02-17 06:52:30,963 DEBUG TRAIN Batch 3/1100 loss 29.195652 loss_att 33.713264 loss_ctc 43.297474 loss_rnnt 26.241758 hw_loss 0.318990 lr 0.00097803 rank 5
2023-02-17 06:52:30,972 DEBUG TRAIN Batch 3/1100 loss 35.125202 loss_att 35.384659 loss_ctc 40.744728 loss_rnnt 34.132141 hw_loss 0.359811 lr 0.00097825 rank 0
2023-02-17 06:52:31,001 DEBUG TRAIN Batch 3/1100 loss 31.630873 loss_att 36.523235 loss_ctc 47.490067 loss_rnnt 28.409159 hw_loss 0.241278 lr 0.00097902 rank 4
2023-02-17 06:53:47,521 DEBUG TRAIN Batch 3/1200 loss 34.175526 loss_att 43.345627 loss_ctc 51.149811 loss_rnnt 29.943281 hw_loss 0.253092 lr 0.00097741 rank 7
2023-02-17 06:53:47,522 DEBUG TRAIN Batch 3/1200 loss 22.922810 loss_att 27.093498 loss_ctc 32.812485 loss_rnnt 20.590748 hw_loss 0.336190 lr 0.00097715 rank 4
2023-02-17 06:53:47,522 DEBUG TRAIN Batch 3/1200 loss 28.724630 loss_att 32.134354 loss_ctc 43.691765 loss_rnnt 25.890779 hw_loss 0.293041 lr 0.00097573 rank 2
2023-02-17 06:53:47,524 DEBUG TRAIN Batch 3/1200 loss 24.517326 loss_att 26.077024 loss_ctc 35.901455 loss_rnnt 22.537228 hw_loss 0.281769 lr 0.00097691 rank 6
2023-02-17 06:53:47,525 DEBUG TRAIN Batch 3/1200 loss 25.942373 loss_att 35.953369 loss_ctc 38.766399 loss_rnnt 22.041803 hw_loss 0.353439 lr 0.00097638 rank 0
2023-02-17 06:53:47,529 DEBUG TRAIN Batch 3/1200 loss 21.723152 loss_att 25.410797 loss_ctc 33.420139 loss_rnnt 19.231316 hw_loss 0.365076 lr 0.00097616 rank 5
2023-02-17 06:53:47,572 DEBUG TRAIN Batch 3/1200 loss 22.921406 loss_att 28.328465 loss_ctc 32.723675 loss_rnnt 20.338135 hw_loss 0.365421 lr 0.00097651 rank 3
2023-02-17 06:53:47,597 DEBUG TRAIN Batch 3/1200 loss 34.289024 loss_att 42.138474 loss_ctc 47.817703 loss_rnnt 30.706907 hw_loss 0.390756 lr 0.00097603 rank 1
2023-02-17 06:55:03,499 DEBUG TRAIN Batch 3/1300 loss 27.442915 loss_att 31.081215 loss_ctc 36.395821 loss_rnnt 25.351408 hw_loss 0.318988 lr 0.00097555 rank 7
2023-02-17 06:55:03,512 DEBUG TRAIN Batch 3/1300 loss 32.311947 loss_att 38.973259 loss_ctc 50.556362 loss_rnnt 28.391256 hw_loss 0.292195 lr 0.00097529 rank 4
2023-02-17 06:55:03,517 DEBUG TRAIN Batch 3/1300 loss 30.676929 loss_att 39.286522 loss_ctc 56.668114 loss_rnnt 25.342321 hw_loss 0.275996 lr 0.00097466 rank 3
2023-02-17 06:55:03,519 DEBUG TRAIN Batch 3/1300 loss 19.169607 loss_att 38.160641 loss_ctc 39.211746 loss_rnnt 12.564757 hw_loss 0.251919 lr 0.00097505 rank 6
2023-02-17 06:55:03,521 DEBUG TRAIN Batch 3/1300 loss 18.722042 loss_att 20.253035 loss_ctc 24.381510 loss_rnnt 17.473160 hw_loss 0.352662 lr 0.00097453 rank 0
2023-02-17 06:55:03,525 DEBUG TRAIN Batch 3/1300 loss 32.128460 loss_att 38.826069 loss_ctc 41.978218 loss_rnnt 29.353832 hw_loss 0.228384 lr 0.00097388 rank 2
2023-02-17 06:55:03,618 DEBUG TRAIN Batch 3/1300 loss 16.135593 loss_att 18.594368 loss_ctc 20.202070 loss_rnnt 14.917244 hw_loss 0.345745 lr 0.00097418 rank 1
2023-02-17 06:55:03,619 DEBUG TRAIN Batch 3/1300 loss 36.403961 loss_att 57.750015 loss_ctc 50.945526 loss_rnnt 30.085266 hw_loss 0.207387 lr 0.00097431 rank 5
2023-02-17 06:56:28,082 DEBUG TRAIN Batch 3/1400 loss 30.251282 loss_att 38.644684 loss_ctc 42.806938 loss_rnnt 26.792587 hw_loss 0.198613 lr 0.00097320 rank 6
2023-02-17 06:56:28,086 DEBUG TRAIN Batch 3/1400 loss 39.935909 loss_att 52.248394 loss_ctc 62.714336 loss_rnnt 34.340347 hw_loss 0.179894 lr 0.00097204 rank 2
2023-02-17 06:56:28,089 DEBUG TRAIN Batch 3/1400 loss 14.358890 loss_att 19.562815 loss_ctc 23.691589 loss_rnnt 11.885132 hw_loss 0.353648 lr 0.00097233 rank 1
2023-02-17 06:56:28,129 DEBUG TRAIN Batch 3/1400 loss 46.526684 loss_att 53.846027 loss_ctc 71.817772 loss_rnnt 41.506058 hw_loss 0.346142 lr 0.00097246 rank 5
2023-02-17 06:56:28,131 DEBUG TRAIN Batch 3/1400 loss 24.171562 loss_att 31.160769 loss_ctc 51.440819 loss_rnnt 18.986898 hw_loss 0.282977 lr 0.00097344 rank 4
2023-02-17 06:56:28,148 DEBUG TRAIN Batch 3/1400 loss 45.655857 loss_att 56.254158 loss_ctc 75.313728 loss_rnnt 39.438324 hw_loss 0.269039 lr 0.00097370 rank 7
2023-02-17 06:56:28,158 DEBUG TRAIN Batch 3/1400 loss 22.918715 loss_att 32.072262 loss_ctc 37.554962 loss_rnnt 18.998768 hw_loss 0.258261 lr 0.00097268 rank 0
2023-02-17 06:56:28,185 DEBUG TRAIN Batch 3/1400 loss 11.655622 loss_att 20.287605 loss_ctc 23.231453 loss_rnnt 8.250502 hw_loss 0.253648 lr 0.00097281 rank 3
2023-02-17 06:57:54,490 DEBUG TRAIN Batch 3/1500 loss 22.805120 loss_att 32.159302 loss_ctc 28.416784 loss_rnnt 20.023565 hw_loss 0.304685 lr 0.00097050 rank 1
2023-02-17 06:57:54,496 DEBUG TRAIN Batch 3/1500 loss 24.802011 loss_att 32.230171 loss_ctc 36.857040 loss_rnnt 21.589264 hw_loss 0.224587 lr 0.00097136 rank 6
2023-02-17 06:57:54,498 DEBUG TRAIN Batch 3/1500 loss 40.393833 loss_att 44.666874 loss_ctc 49.361526 loss_rnnt 38.162521 hw_loss 0.339393 lr 0.00097097 rank 3
2023-02-17 06:57:54,499 DEBUG TRAIN Batch 3/1500 loss 30.100382 loss_att 39.366467 loss_ctc 43.652962 loss_rnnt 26.327965 hw_loss 0.210351 lr 0.00097021 rank 2
2023-02-17 06:57:54,500 DEBUG TRAIN Batch 3/1500 loss 33.568027 loss_att 37.993462 loss_ctc 47.122337 loss_rnnt 30.740101 hw_loss 0.254235 lr 0.00097085 rank 0
2023-02-17 06:57:54,501 DEBUG TRAIN Batch 3/1500 loss 25.640785 loss_att 35.621506 loss_ctc 44.729309 loss_rnnt 20.987740 hw_loss 0.209558 lr 0.00097160 rank 4
2023-02-17 06:57:54,502 DEBUG TRAIN Batch 3/1500 loss 27.216967 loss_att 34.040989 loss_ctc 44.394768 loss_rnnt 23.460918 hw_loss 0.189135 lr 0.00097185 rank 7
2023-02-17 06:57:54,502 DEBUG TRAIN Batch 3/1500 loss 35.795177 loss_att 44.155746 loss_ctc 55.327286 loss_rnnt 31.396376 hw_loss 0.229512 lr 0.00097063 rank 5
2023-02-17 06:59:11,305 DEBUG TRAIN Batch 3/1600 loss 22.435843 loss_att 35.510986 loss_ctc 35.207657 loss_rnnt 18.009651 hw_loss 0.202974 lr 0.00096902 rank 0
2023-02-17 06:59:11,306 DEBUG TRAIN Batch 3/1600 loss 32.009106 loss_att 41.315548 loss_ctc 50.134426 loss_rnnt 27.610920 hw_loss 0.225351 lr 0.00096880 rank 5
2023-02-17 06:59:11,307 DEBUG TRAIN Batch 3/1600 loss 30.592678 loss_att 36.059956 loss_ctc 46.850044 loss_rnnt 27.192234 hw_loss 0.261258 lr 0.00096915 rank 3
2023-02-17 06:59:11,308 DEBUG TRAIN Batch 3/1600 loss 40.319427 loss_att 44.343147 loss_ctc 61.274105 loss_rnnt 36.585564 hw_loss 0.253430 lr 0.00096953 rank 6
2023-02-17 06:59:11,312 DEBUG TRAIN Batch 3/1600 loss 30.427748 loss_att 38.401417 loss_ctc 46.677708 loss_rnnt 26.477909 hw_loss 0.353331 lr 0.00097002 rank 7
2023-02-17 06:59:11,334 DEBUG TRAIN Batch 3/1600 loss 25.074354 loss_att 35.111477 loss_ctc 40.135601 loss_rnnt 20.949039 hw_loss 0.205731 lr 0.00096977 rank 4
2023-02-17 06:59:11,338 DEBUG TRAIN Batch 3/1600 loss 37.934093 loss_att 49.849098 loss_ctc 65.467484 loss_rnnt 31.696341 hw_loss 0.344311 lr 0.00096868 rank 1
2023-02-17 06:59:11,358 DEBUG TRAIN Batch 3/1600 loss 32.763176 loss_att 38.599854 loss_ctc 47.947666 loss_rnnt 29.421455 hw_loss 0.280849 lr 0.00096839 rank 2
2023-02-17 07:00:30,020 DEBUG TRAIN Batch 3/1700 loss 40.299351 loss_att 53.125336 loss_ctc 56.279076 loss_rnnt 35.501034 hw_loss 0.192174 lr 0.00096699 rank 5
2023-02-17 07:00:30,039 DEBUG TRAIN Batch 3/1700 loss 55.164322 loss_att 57.629845 loss_ctc 80.128914 loss_rnnt 51.220043 hw_loss 0.229802 lr 0.00096771 rank 6
2023-02-17 07:00:30,038 DEBUG TRAIN Batch 3/1700 loss 39.773880 loss_att 44.420044 loss_ctc 55.362328 loss_rnnt 36.622391 hw_loss 0.269620 lr 0.00096657 rank 2
2023-02-17 07:00:30,040 DEBUG TRAIN Batch 3/1700 loss 22.033922 loss_att 29.319073 loss_ctc 38.322170 loss_rnnt 18.249590 hw_loss 0.291627 lr 0.00096686 rank 1
2023-02-17 07:00:30,040 DEBUG TRAIN Batch 3/1700 loss 25.578375 loss_att 31.068291 loss_ctc 36.325813 loss_rnnt 22.935425 hw_loss 0.209950 lr 0.00096795 rank 4
2023-02-17 07:00:30,066 DEBUG TRAIN Batch 3/1700 loss 16.314085 loss_att 25.459904 loss_ctc 27.616209 loss_rnnt 12.886669 hw_loss 0.171195 lr 0.00096733 rank 3
2023-02-17 07:00:30,068 DEBUG TRAIN Batch 3/1700 loss 42.167435 loss_att 51.266853 loss_ctc 60.877083 loss_rnnt 37.698906 hw_loss 0.288795 lr 0.00096820 rank 7
2023-02-17 07:00:30,070 DEBUG TRAIN Batch 3/1700 loss 41.854092 loss_att 41.635208 loss_ctc 55.526760 loss_rnnt 39.906391 hw_loss 0.315845 lr 0.00096721 rank 0
2023-02-17 07:02:02,585 DEBUG TRAIN Batch 3/1800 loss 33.715618 loss_att 39.227272 loss_ctc 47.783661 loss_rnnt 30.566307 hw_loss 0.321076 lr 0.00096506 rank 1
2023-02-17 07:02:02,585 DEBUG TRAIN Batch 3/1800 loss 26.874884 loss_att 27.877754 loss_ctc 33.647224 loss_rnnt 25.541935 hw_loss 0.430112 lr 0.00096477 rank 2
2023-02-17 07:02:02,588 DEBUG TRAIN Batch 3/1800 loss 26.957251 loss_att 34.350235 loss_ctc 41.065380 loss_rnnt 23.463953 hw_loss 0.250526 lr 0.00096591 rank 6
2023-02-17 07:02:02,589 DEBUG TRAIN Batch 3/1800 loss 36.086620 loss_att 38.041473 loss_ctc 46.671185 loss_rnnt 34.115139 hw_loss 0.317315 lr 0.00096553 rank 3
2023-02-17 07:02:02,591 DEBUG TRAIN Batch 3/1800 loss 27.242727 loss_att 31.059675 loss_ctc 45.967831 loss_rnnt 23.863020 hw_loss 0.224325 lr 0.00096519 rank 5
2023-02-17 07:02:02,591 DEBUG TRAIN Batch 3/1800 loss 33.193790 loss_att 38.842041 loss_ctc 45.400581 loss_rnnt 30.296604 hw_loss 0.262430 lr 0.00096639 rank 7
2023-02-17 07:02:02,624 DEBUG TRAIN Batch 3/1800 loss 26.631058 loss_att 34.065178 loss_ctc 40.966927 loss_rnnt 23.077536 hw_loss 0.291088 lr 0.00096614 rank 4
2023-02-17 07:02:02,653 DEBUG TRAIN Batch 3/1800 loss 23.364601 loss_att 28.610992 loss_ctc 35.048752 loss_rnnt 20.642113 hw_loss 0.216232 lr 0.00096540 rank 0
2023-02-17 07:03:18,830 DEBUG TRAIN Batch 3/1900 loss 24.482382 loss_att 28.637934 loss_ctc 35.989876 loss_rnnt 21.909943 hw_loss 0.388116 lr 0.00096361 rank 0
2023-02-17 07:03:18,831 DEBUG TRAIN Batch 3/1900 loss 13.676019 loss_att 14.978837 loss_ctc 18.134541 loss_rnnt 12.647320 hw_loss 0.325624 lr 0.00096339 rank 5
2023-02-17 07:03:18,831 DEBUG TRAIN Batch 3/1900 loss 20.962900 loss_att 21.994524 loss_ctc 28.085604 loss_rnnt 19.596500 hw_loss 0.394463 lr 0.00096459 rank 7
2023-02-17 07:03:18,831 DEBUG TRAIN Batch 3/1900 loss 16.112644 loss_att 15.948724 loss_ctc 19.091465 loss_rnnt 15.523577 hw_loss 0.421268 lr 0.00096434 rank 4
2023-02-17 07:03:18,831 DEBUG TRAIN Batch 3/1900 loss 24.053240 loss_att 37.959572 loss_ctc 47.374413 loss_rnnt 17.983719 hw_loss 0.335183 lr 0.00096298 rank 2
2023-02-17 07:03:18,832 DEBUG TRAIN Batch 3/1900 loss 18.976151 loss_att 21.709728 loss_ctc 31.038183 loss_rnnt 16.625341 hw_loss 0.367164 lr 0.00096327 rank 1
2023-02-17 07:03:18,833 DEBUG TRAIN Batch 3/1900 loss 23.373096 loss_att 24.895004 loss_ctc 33.851490 loss_rnnt 21.436701 hw_loss 0.440426 lr 0.00096411 rank 6
2023-02-17 07:03:18,839 DEBUG TRAIN Batch 3/1900 loss 18.636652 loss_att 29.950790 loss_ctc 34.379402 loss_rnnt 14.188889 hw_loss 0.161068 lr 0.00096373 rank 3
2023-02-17 07:04:33,671 DEBUG TRAIN Batch 3/2000 loss 14.688926 loss_att 25.772999 loss_ctc 29.330078 loss_rnnt 10.335365 hw_loss 0.346109 lr 0.00096280 rank 7
2023-02-17 07:04:33,672 DEBUG TRAIN Batch 3/2000 loss 32.828087 loss_att 41.641487 loss_ctc 53.931786 loss_rnnt 28.129345 hw_loss 0.229190 lr 0.00096149 rank 1
2023-02-17 07:04:33,674 DEBUG TRAIN Batch 3/2000 loss 38.838676 loss_att 42.130871 loss_ctc 49.906502 loss_rnnt 36.591530 hw_loss 0.211878 lr 0.00096120 rank 2
2023-02-17 07:04:33,677 DEBUG TRAIN Batch 3/2000 loss 48.502293 loss_att 63.918205 loss_ctc 75.819595 loss_rnnt 41.631405 hw_loss 0.272622 lr 0.00096255 rank 4
2023-02-17 07:04:33,679 DEBUG TRAIN Batch 3/2000 loss 42.496155 loss_att 55.253387 loss_ctc 73.556274 loss_rnnt 35.628036 hw_loss 0.328720 lr 0.00096182 rank 0
2023-02-17 07:04:33,680 DEBUG TRAIN Batch 3/2000 loss 29.745111 loss_att 45.884689 loss_ctc 52.704025 loss_rnnt 23.334341 hw_loss 0.228123 lr 0.00096195 rank 3
2023-02-17 07:04:33,723 DEBUG TRAIN Batch 3/2000 loss 19.340481 loss_att 29.518665 loss_ctc 31.323059 loss_rnnt 15.532051 hw_loss 0.328341 lr 0.00096232 rank 6
2023-02-17 07:04:33,751 DEBUG TRAIN Batch 3/2000 loss 40.240589 loss_att 51.290283 loss_ctc 60.205334 loss_rnnt 35.184910 hw_loss 0.344572 lr 0.00096161 rank 5
2023-02-17 07:05:59,304 DEBUG TRAIN Batch 3/2100 loss 26.340513 loss_att 31.958290 loss_ctc 44.334435 loss_rnnt 22.663189 hw_loss 0.289843 lr 0.00096077 rank 4
2023-02-17 07:05:59,306 DEBUG TRAIN Batch 3/2100 loss 22.234488 loss_att 35.415173 loss_ctc 31.118313 loss_rnnt 18.243887 hw_loss 0.318665 lr 0.00095943 rank 2
2023-02-17 07:05:59,307 DEBUG TRAIN Batch 3/2100 loss 37.353127 loss_att 42.122952 loss_ctc 55.798004 loss_rnnt 33.786705 hw_loss 0.287136 lr 0.00096005 rank 0
2023-02-17 07:05:59,322 DEBUG TRAIN Batch 3/2100 loss 11.138595 loss_att 19.994881 loss_ctc 20.900723 loss_rnnt 7.914510 hw_loss 0.283519 lr 0.00095984 rank 5
2023-02-17 07:05:59,366 DEBUG TRAIN Batch 3/2100 loss 51.042694 loss_att 65.340500 loss_ctc 75.747368 loss_rnnt 44.775963 hw_loss 0.212274 lr 0.00096102 rank 7
2023-02-17 07:05:59,368 DEBUG TRAIN Batch 3/2100 loss 38.640022 loss_att 49.496452 loss_ctc 58.330147 loss_rnnt 33.730591 hw_loss 0.211484 lr 0.00095971 rank 1
2023-02-17 07:05:59,388 DEBUG TRAIN Batch 3/2100 loss 29.046379 loss_att 36.607285 loss_ctc 44.706360 loss_rnnt 25.344900 hw_loss 0.189938 lr 0.00096054 rank 6
2023-02-17 07:05:59,393 DEBUG TRAIN Batch 3/2100 loss 32.851421 loss_att 40.504841 loss_ctc 45.499687 loss_rnnt 29.488850 hw_loss 0.272726 lr 0.00096017 rank 3
2023-02-17 07:07:25,757 DEBUG TRAIN Batch 3/2200 loss 28.673756 loss_att 35.695827 loss_ctc 44.830734 loss_rnnt 25.009556 hw_loss 0.197852 lr 0.00095878 rank 6
2023-02-17 07:07:25,757 DEBUG TRAIN Batch 3/2200 loss 27.105782 loss_att 35.467941 loss_ctc 44.431747 loss_rnnt 23.047276 hw_loss 0.142395 lr 0.00095841 rank 3
2023-02-17 07:07:25,775 DEBUG TRAIN Batch 3/2200 loss 33.721684 loss_att 40.978024 loss_ctc 49.568542 loss_rnnt 30.021769 hw_loss 0.254498 lr 0.00095807 rank 5
2023-02-17 07:07:25,781 DEBUG TRAIN Batch 3/2200 loss 26.068743 loss_att 33.145367 loss_ctc 46.576408 loss_rnnt 21.816402 hw_loss 0.192490 lr 0.00095795 rank 1
2023-02-17 07:07:25,802 DEBUG TRAIN Batch 3/2200 loss 23.653961 loss_att 32.361061 loss_ctc 37.049500 loss_rnnt 19.995686 hw_loss 0.245219 lr 0.00095901 rank 4
2023-02-17 07:07:25,818 DEBUG TRAIN Batch 3/2200 loss 33.934658 loss_att 43.949345 loss_ctc 52.610119 loss_rnnt 29.233730 hw_loss 0.389862 lr 0.00095925 rank 7
2023-02-17 07:07:25,819 DEBUG TRAIN Batch 3/2200 loss 24.801609 loss_att 32.893963 loss_ctc 36.179375 loss_rnnt 21.490791 hw_loss 0.328715 lr 0.00095767 rank 2
2023-02-17 07:07:25,840 DEBUG TRAIN Batch 3/2200 loss 40.256607 loss_att 49.063484 loss_ctc 54.660225 loss_rnnt 36.408905 hw_loss 0.310954 lr 0.00095828 rank 0
2023-02-17 07:08:41,677 DEBUG TRAIN Batch 3/2300 loss 45.072666 loss_att 52.897476 loss_ctc 62.095806 loss_rnnt 41.114380 hw_loss 0.231696 lr 0.00095620 rank 1
2023-02-17 07:08:41,677 DEBUG TRAIN Batch 3/2300 loss 66.553215 loss_att 65.718307 loss_ctc 91.038345 loss_rnnt 63.313026 hw_loss 0.267161 lr 0.00095653 rank 0
2023-02-17 07:08:41,678 DEBUG TRAIN Batch 3/2300 loss 23.193306 loss_att 29.356306 loss_ctc 34.445366 loss_rnnt 20.314596 hw_loss 0.273441 lr 0.00095665 rank 3
2023-02-17 07:08:41,680 DEBUG TRAIN Batch 3/2300 loss 33.193314 loss_att 38.029533 loss_ctc 48.705887 loss_rnnt 29.992083 hw_loss 0.310584 lr 0.00095632 rank 5
2023-02-17 07:08:41,681 DEBUG TRAIN Batch 3/2300 loss 45.391575 loss_att 51.810844 loss_ctc 66.919800 loss_rnnt 41.059151 hw_loss 0.334012 lr 0.00095725 rank 4
2023-02-17 07:08:41,681 DEBUG TRAIN Batch 3/2300 loss 21.987350 loss_att 33.041882 loss_ctc 35.026573 loss_rnnt 17.906971 hw_loss 0.245457 lr 0.00095702 rank 6
2023-02-17 07:08:41,683 DEBUG TRAIN Batch 3/2300 loss 28.162231 loss_att 32.414654 loss_ctc 43.372681 loss_rnnt 25.171055 hw_loss 0.211180 lr 0.00095592 rank 2
2023-02-17 07:08:41,723 DEBUG TRAIN Batch 3/2300 loss 32.314636 loss_att 40.353359 loss_ctc 55.646976 loss_rnnt 27.426479 hw_loss 0.317684 lr 0.00095749 rank 7
2023-02-17 07:09:58,969 DEBUG TRAIN Batch 3/2400 loss 27.723650 loss_att 33.100464 loss_ctc 44.698105 loss_rnnt 24.188961 hw_loss 0.367629 lr 0.00095478 rank 0
2023-02-17 07:09:58,974 DEBUG TRAIN Batch 3/2400 loss 44.684669 loss_att 48.631607 loss_ctc 63.109158 loss_rnnt 41.328789 hw_loss 0.206050 lr 0.00095445 rank 1
2023-02-17 07:09:58,981 DEBUG TRAIN Batch 3/2400 loss 34.450020 loss_att 40.370399 loss_ctc 52.475029 loss_rnnt 30.747463 hw_loss 0.215904 lr 0.00095490 rank 3
2023-02-17 07:09:58,982 DEBUG TRAIN Batch 3/2400 loss 44.268066 loss_att 49.143127 loss_ctc 63.808937 loss_rnnt 40.535145 hw_loss 0.285855 lr 0.00095574 rank 7
2023-02-17 07:09:59,002 DEBUG TRAIN Batch 3/2400 loss 37.858109 loss_att 47.893848 loss_ctc 51.617393 loss_rnnt 33.857079 hw_loss 0.298713 lr 0.00095457 rank 5
2023-02-17 07:09:59,041 DEBUG TRAIN Batch 3/2400 loss 30.599131 loss_att 38.953079 loss_ctc 45.743279 loss_rnnt 26.751785 hw_loss 0.295003 lr 0.00095527 rank 6
2023-02-17 07:09:59,048 DEBUG TRAIN Batch 3/2400 loss 32.827541 loss_att 39.121067 loss_ctc 44.121597 loss_rnnt 29.900560 hw_loss 0.304495 lr 0.00095550 rank 4
2023-02-17 07:09:59,066 DEBUG TRAIN Batch 3/2400 loss 20.146341 loss_att 24.857861 loss_ctc 32.069199 loss_rnnt 17.464886 hw_loss 0.280196 lr 0.00095417 rank 2
2023-02-17 07:11:31,939 DEBUG TRAIN Batch 3/2500 loss 19.308802 loss_att 22.213264 loss_ctc 27.422941 loss_rnnt 17.455215 hw_loss 0.357768 lr 0.00095317 rank 3
2023-02-17 07:11:31,941 DEBUG TRAIN Batch 3/2500 loss 21.785753 loss_att 24.301670 loss_ctc 28.554510 loss_rnnt 20.204073 hw_loss 0.329989 lr 0.00095305 rank 0
2023-02-17 07:11:31,941 DEBUG TRAIN Batch 3/2500 loss 36.744774 loss_att 36.885948 loss_ctc 49.510284 loss_rnnt 34.825897 hw_loss 0.353572 lr 0.00095353 rank 6
2023-02-17 07:11:31,945 DEBUG TRAIN Batch 3/2500 loss 39.877846 loss_att 50.813110 loss_ctc 54.879677 loss_rnnt 35.525055 hw_loss 0.310295 lr 0.00095272 rank 1
2023-02-17 07:11:31,946 DEBUG TRAIN Batch 3/2500 loss 54.769405 loss_att 62.608589 loss_ctc 88.146362 loss_rnnt 48.560436 hw_loss 0.357881 lr 0.00095244 rank 2
2023-02-17 07:11:31,948 DEBUG TRAIN Batch 3/2500 loss 34.261120 loss_att 38.066868 loss_ctc 46.112473 loss_rnnt 31.703705 hw_loss 0.405156 lr 0.00095400 rank 7
2023-02-17 07:11:31,950 DEBUG TRAIN Batch 3/2500 loss 21.844660 loss_att 24.849627 loss_ctc 35.479744 loss_rnnt 19.241873 hw_loss 0.344597 lr 0.00095284 rank 5
2023-02-17 07:11:31,991 DEBUG TRAIN Batch 3/2500 loss 13.736403 loss_att 19.599661 loss_ctc 22.728920 loss_rnnt 11.146828 hw_loss 0.408603 lr 0.00095376 rank 4
2023-02-17 07:12:48,192 DEBUG TRAIN Batch 3/2600 loss 38.340916 loss_att 47.915016 loss_ctc 58.122185 loss_rnnt 33.635414 hw_loss 0.287208 lr 0.00095227 rank 7
2023-02-17 07:12:48,201 DEBUG TRAIN Batch 3/2600 loss 31.539579 loss_att 43.267944 loss_ctc 52.031456 loss_rnnt 26.362356 hw_loss 0.186186 lr 0.00095180 rank 6
2023-02-17 07:12:48,203 DEBUG TRAIN Batch 3/2600 loss 52.900028 loss_att 59.539955 loss_ctc 77.228363 loss_rnnt 48.243084 hw_loss 0.159711 lr 0.00095132 rank 0
2023-02-17 07:12:48,206 DEBUG TRAIN Batch 3/2600 loss 37.883411 loss_att 49.694191 loss_ctc 59.210114 loss_rnnt 32.514198 hw_loss 0.306555 lr 0.00095203 rank 4
2023-02-17 07:12:48,206 DEBUG TRAIN Batch 3/2600 loss 36.573517 loss_att 47.287407 loss_ctc 52.526043 loss_rnnt 32.135448 hw_loss 0.315529 lr 0.00095111 rank 5
2023-02-17 07:12:48,213 DEBUG TRAIN Batch 3/2600 loss 22.581770 loss_att 32.731232 loss_ctc 35.318489 loss_rnnt 18.740353 hw_loss 0.212433 lr 0.00095072 rank 2
2023-02-17 07:12:48,214 DEBUG TRAIN Batch 3/2600 loss 20.439051 loss_att 29.987713 loss_ctc 35.143425 loss_rnnt 16.390259 hw_loss 0.334646 lr 0.00095099 rank 1
2023-02-17 07:12:48,216 DEBUG TRAIN Batch 3/2600 loss 62.942234 loss_att 72.663269 loss_ctc 94.506042 loss_rnnt 56.665833 hw_loss 0.231915 lr 0.00095144 rank 3
2023-02-17 07:14:04,094 DEBUG TRAIN Batch 3/2700 loss 19.254026 loss_att 30.559929 loss_ctc 26.365482 loss_rnnt 15.919047 hw_loss 0.235510 lr 0.00094960 rank 0
2023-02-17 07:14:04,094 DEBUG TRAIN Batch 3/2700 loss 36.230526 loss_att 42.759766 loss_ctc 52.445854 loss_rnnt 32.639000 hw_loss 0.231808 lr 0.00094928 rank 1
2023-02-17 07:14:04,096 DEBUG TRAIN Batch 3/2700 loss 56.688728 loss_att 68.910568 loss_ctc 70.691238 loss_rnnt 52.262169 hw_loss 0.215989 lr 0.00095031 rank 4
2023-02-17 07:14:04,099 DEBUG TRAIN Batch 3/2700 loss 22.782288 loss_att 27.952053 loss_ctc 33.810768 loss_rnnt 20.114079 hw_loss 0.307106 lr 0.00095008 rank 6
2023-02-17 07:14:04,100 DEBUG TRAIN Batch 3/2700 loss 40.889706 loss_att 54.215309 loss_ctc 56.868790 loss_rnnt 35.936302 hw_loss 0.295751 lr 0.00094972 rank 3
2023-02-17 07:14:04,104 DEBUG TRAIN Batch 3/2700 loss 42.611889 loss_att 49.370949 loss_ctc 46.983788 loss_rnnt 40.547638 hw_loss 0.242846 lr 0.00094900 rank 2
2023-02-17 07:14:04,106 DEBUG TRAIN Batch 3/2700 loss 26.622274 loss_att 39.935020 loss_ctc 35.428650 loss_rnnt 22.669041 hw_loss 0.218433 lr 0.00095055 rank 7
2023-02-17 07:14:04,131 DEBUG TRAIN Batch 3/2700 loss 19.721989 loss_att 27.784885 loss_ctc 33.322617 loss_rnnt 16.171669 hw_loss 0.233106 lr 0.00094940 rank 5
2023-02-17 07:15:26,328 DEBUG TRAIN Batch 3/2800 loss 29.962202 loss_att 32.904686 loss_ctc 43.694908 loss_rnnt 27.393591 hw_loss 0.279537 lr 0.00094730 rank 2
2023-02-17 07:15:26,347 DEBUG TRAIN Batch 3/2800 loss 40.361935 loss_att 49.695763 loss_ctc 63.241066 loss_rnnt 35.354794 hw_loss 0.168415 lr 0.00094789 rank 0
2023-02-17 07:15:26,352 DEBUG TRAIN Batch 3/2800 loss 37.286205 loss_att 41.635757 loss_ctc 50.492393 loss_rnnt 34.505508 hw_loss 0.281174 lr 0.00094801 rank 3
2023-02-17 07:15:26,366 DEBUG TRAIN Batch 3/2800 loss 26.617212 loss_att 39.357685 loss_ctc 43.552547 loss_rnnt 21.696697 hw_loss 0.214457 lr 0.00094769 rank 5
2023-02-17 07:15:26,380 DEBUG TRAIN Batch 3/2800 loss 22.012575 loss_att 32.778980 loss_ctc 41.237778 loss_rnnt 17.122398 hw_loss 0.325379 lr 0.00094859 rank 4
2023-02-17 07:15:26,382 DEBUG TRAIN Batch 3/2800 loss 18.632086 loss_att 29.891659 loss_ctc 37.924892 loss_rnnt 13.693707 hw_loss 0.213917 lr 0.00094883 rank 7
2023-02-17 07:15:26,392 DEBUG TRAIN Batch 3/2800 loss 29.825880 loss_att 39.014523 loss_ctc 45.414989 loss_rnnt 25.719028 hw_loss 0.357324 lr 0.00094757 rank 1
2023-02-17 07:15:26,398 DEBUG TRAIN Batch 3/2800 loss 44.746994 loss_att 51.910892 loss_ctc 64.628563 loss_rnnt 40.525631 hw_loss 0.258211 lr 0.00094837 rank 6
2023-02-17 07:16:55,325 DEBUG TRAIN Batch 3/2900 loss 32.416828 loss_att 36.869919 loss_ctc 41.652809 loss_rnnt 30.201511 hw_loss 0.174810 lr 0.00094689 rank 4
2023-02-17 07:16:55,328 DEBUG TRAIN Batch 3/2900 loss 37.607639 loss_att 52.963528 loss_ctc 60.304840 loss_rnnt 31.387428 hw_loss 0.230133 lr 0.00094713 rank 7
2023-02-17 07:16:55,328 DEBUG TRAIN Batch 3/2900 loss 46.025478 loss_att 52.585335 loss_ctc 54.299572 loss_rnnt 43.502831 hw_loss 0.201495 lr 0.00094560 rank 2
2023-02-17 07:16:55,330 DEBUG TRAIN Batch 3/2900 loss 42.242226 loss_att 50.672962 loss_ctc 67.771698 loss_rnnt 36.967937 hw_loss 0.345397 lr 0.00094599 rank 5
2023-02-17 07:16:55,332 DEBUG TRAIN Batch 3/2900 loss 53.539440 loss_att 67.494591 loss_ctc 79.241234 loss_rnnt 47.185429 hw_loss 0.255136 lr 0.00094631 rank 3
2023-02-17 07:16:55,336 DEBUG TRAIN Batch 3/2900 loss 17.901546 loss_att 27.866684 loss_ctc 33.927353 loss_rnnt 13.637211 hw_loss 0.252250 lr 0.00094667 rank 6
2023-02-17 07:16:55,340 DEBUG TRAIN Batch 3/2900 loss 33.394371 loss_att 40.090290 loss_ctc 56.163662 loss_rnnt 28.882826 hw_loss 0.255861 lr 0.00094620 rank 0
2023-02-17 07:16:55,375 DEBUG TRAIN Batch 3/2900 loss 38.852863 loss_att 49.981384 loss_ctc 56.218895 loss_rnnt 34.211212 hw_loss 0.188391 lr 0.00094587 rank 1
2023-02-17 07:18:10,995 DEBUG TRAIN Batch 3/3000 loss 34.568199 loss_att 48.645256 loss_ctc 57.068344 loss_rnnt 28.654652 hw_loss 0.183961 lr 0.00094543 rank 7
2023-02-17 07:18:11,002 DEBUG TRAIN Batch 3/3000 loss 31.381102 loss_att 33.771755 loss_ctc 47.552990 loss_rnnt 28.591125 hw_loss 0.291738 lr 0.00094462 rank 3
2023-02-17 07:18:11,004 DEBUG TRAIN Batch 3/3000 loss 29.521873 loss_att 44.533268 loss_ctc 44.690304 loss_rnnt 24.298557 hw_loss 0.372335 lr 0.00094498 rank 6
2023-02-17 07:18:11,008 DEBUG TRAIN Batch 3/3000 loss 41.289425 loss_att 46.593655 loss_ctc 57.491364 loss_rnnt 37.967133 hw_loss 0.189727 lr 0.00094419 rank 1
2023-02-17 07:18:11,009 DEBUG TRAIN Batch 3/3000 loss 31.629745 loss_att 40.152954 loss_ctc 52.431881 loss_rnnt 26.978075 hw_loss 0.325140 lr 0.00094430 rank 5
2023-02-17 07:18:11,022 DEBUG TRAIN Batch 3/3000 loss 25.208229 loss_att 32.378639 loss_ctc 35.195492 loss_rnnt 22.328844 hw_loss 0.213129 lr 0.00094451 rank 0
2023-02-17 07:18:11,034 DEBUG TRAIN Batch 3/3000 loss 24.608828 loss_att 29.931307 loss_ctc 34.252171 loss_rnnt 22.074080 hw_loss 0.345890 lr 0.00094392 rank 2
2023-02-17 07:18:11,049 DEBUG TRAIN Batch 3/3000 loss 32.426109 loss_att 37.118652 loss_ctc 42.945580 loss_rnnt 29.892622 hw_loss 0.360718 lr 0.00094520 rank 4
2023-02-17 07:19:27,296 DEBUG TRAIN Batch 3/3100 loss 23.699608 loss_att 29.064011 loss_ctc 37.977600 loss_rnnt 20.570646 hw_loss 0.285654 lr 0.00094283 rank 0
2023-02-17 07:19:27,299 DEBUG TRAIN Batch 3/3100 loss 31.309900 loss_att 36.277321 loss_ctc 48.304367 loss_rnnt 27.842840 hw_loss 0.389337 lr 0.00094375 rank 7
2023-02-17 07:19:27,301 DEBUG TRAIN Batch 3/3100 loss 28.423676 loss_att 30.301540 loss_ctc 40.416737 loss_rnnt 26.275766 hw_loss 0.324863 lr 0.00094251 rank 1
2023-02-17 07:19:27,309 DEBUG TRAIN Batch 3/3100 loss 20.353727 loss_att 23.453066 loss_ctc 29.977579 loss_rnnt 18.278589 hw_loss 0.322673 lr 0.00094224 rank 2
2023-02-17 07:19:27,326 DEBUG TRAIN Batch 3/3100 loss 37.222729 loss_att 46.017899 loss_ctc 53.767250 loss_rnnt 33.057125 hw_loss 0.376193 lr 0.00094330 rank 6
2023-02-17 07:19:27,345 DEBUG TRAIN Batch 3/3100 loss 31.957291 loss_att 35.280804 loss_ctc 46.595402 loss_rnnt 29.202717 hw_loss 0.258978 lr 0.00094351 rank 4
2023-02-17 07:19:27,348 DEBUG TRAIN Batch 3/3100 loss 42.599812 loss_att 53.306602 loss_ctc 60.101189 loss_rnnt 37.951157 hw_loss 0.325836 lr 0.00094262 rank 5
2023-02-17 07:19:27,359 DEBUG TRAIN Batch 3/3100 loss 30.217836 loss_att 35.474228 loss_ctc 47.282356 loss_rnnt 26.729380 hw_loss 0.303579 lr 0.00094294 rank 3
2023-02-17 07:21:02,336 DEBUG TRAIN Batch 3/3200 loss 24.774811 loss_att 27.210192 loss_ctc 31.171650 loss_rnnt 23.197145 hw_loss 0.445642 lr 0.00094162 rank 6
2023-02-17 07:21:02,348 DEBUG TRAIN Batch 3/3200 loss 18.225468 loss_att 30.086773 loss_ctc 28.637144 loss_rnnt 14.365237 hw_loss 0.187024 lr 0.00094127 rank 3
2023-02-17 07:21:02,357 DEBUG TRAIN Batch 3/3200 loss 43.875195 loss_att 58.777397 loss_ctc 62.814812 loss_rnnt 38.279518 hw_loss 0.168654 lr 0.00094084 rank 1
2023-02-17 07:21:02,361 DEBUG TRAIN Batch 3/3200 loss 20.747683 loss_att 29.677315 loss_ctc 33.414886 loss_rnnt 17.160065 hw_loss 0.211371 lr 0.00094207 rank 7
2023-02-17 07:21:02,365 DEBUG TRAIN Batch 3/3200 loss 34.912266 loss_att 41.512836 loss_ctc 52.597069 loss_rnnt 31.077702 hw_loss 0.293391 lr 0.00094115 rank 0
2023-02-17 07:21:02,366 DEBUG TRAIN Batch 3/3200 loss 19.603783 loss_att 21.398460 loss_ctc 23.716846 loss_rnnt 18.506535 hw_loss 0.356073 lr 0.00094095 rank 5
2023-02-17 07:21:02,367 DEBUG TRAIN Batch 3/3200 loss 61.340088 loss_att 59.953327 loss_ctc 84.014030 loss_rnnt 58.415802 hw_loss 0.334573 lr 0.00094057 rank 2
2023-02-17 07:21:02,377 DEBUG TRAIN Batch 3/3200 loss 17.185493 loss_att 16.206972 loss_ctc 20.719816 loss_rnnt 16.700708 hw_loss 0.392338 lr 0.00094184 rank 4
2023-02-17 07:22:18,697 DEBUG TRAIN Batch 3/3300 loss 21.092833 loss_att 29.431473 loss_ctc 35.508286 loss_rnnt 17.357412 hw_loss 0.273062 lr 0.00093949 rank 0
2023-02-17 07:22:18,700 DEBUG TRAIN Batch 3/3300 loss 31.916052 loss_att 44.572208 loss_ctc 55.849426 loss_rnnt 26.056057 hw_loss 0.258091 lr 0.00093996 rank 6
2023-02-17 07:22:18,702 DEBUG TRAIN Batch 3/3300 loss 37.716736 loss_att 41.552528 loss_ctc 56.445114 loss_rnnt 34.277321 hw_loss 0.328385 lr 0.00093891 rank 2
2023-02-17 07:22:18,703 DEBUG TRAIN Batch 3/3300 loss 24.469057 loss_att 26.602623 loss_ctc 32.725101 loss_rnnt 22.799084 hw_loss 0.267101 lr 0.00094040 rank 7
2023-02-17 07:22:18,705 DEBUG TRAIN Batch 3/3300 loss 25.612627 loss_att 34.053066 loss_ctc 45.573620 loss_rnnt 21.142771 hw_loss 0.225570 lr 0.00093961 rank 3
2023-02-17 07:22:18,725 DEBUG TRAIN Batch 3/3300 loss 39.407101 loss_att 45.605515 loss_ctc 65.275558 loss_rnnt 34.553291 hw_loss 0.309368 lr 0.00093929 rank 5
2023-02-17 07:22:18,733 DEBUG TRAIN Batch 3/3300 loss 41.211182 loss_att 49.126450 loss_ctc 68.103653 loss_rnnt 35.882450 hw_loss 0.300022 lr 0.00094017 rank 4
2023-02-17 07:22:18,749 DEBUG TRAIN Batch 3/3300 loss 23.290010 loss_att 30.175419 loss_ctc 33.106133 loss_rnnt 20.485960 hw_loss 0.221535 lr 0.00093918 rank 1
2023-02-17 07:23:35,147 DEBUG TRAIN Batch 3/3400 loss 28.880568 loss_att 39.886196 loss_ctc 47.142097 loss_rnnt 24.137032 hw_loss 0.201635 lr 0.00093795 rank 3
2023-02-17 07:23:35,147 DEBUG TRAIN Batch 3/3400 loss 42.753555 loss_att 52.709709 loss_ctc 58.882774 loss_rnnt 38.399773 hw_loss 0.397481 lr 0.00093851 rank 4
2023-02-17 07:23:35,147 DEBUG TRAIN Batch 3/3400 loss 27.215313 loss_att 32.344090 loss_ctc 40.751755 loss_rnnt 24.274450 hw_loss 0.206713 lr 0.00093830 rank 6
2023-02-17 07:23:35,151 DEBUG TRAIN Batch 3/3400 loss 33.101070 loss_att 39.152546 loss_ctc 57.009773 loss_rnnt 28.505880 hw_loss 0.369502 lr 0.00093784 rank 0
2023-02-17 07:23:35,153 DEBUG TRAIN Batch 3/3400 loss 22.869673 loss_att 31.974670 loss_ctc 33.580486 loss_rnnt 19.458916 hw_loss 0.303094 lr 0.00093726 rank 2
2023-02-17 07:23:35,152 DEBUG TRAIN Batch 3/3400 loss 33.950260 loss_att 41.233158 loss_ctc 50.452019 loss_rnnt 30.182281 hw_loss 0.208436 lr 0.00093752 rank 1
2023-02-17 07:23:35,154 DEBUG TRAIN Batch 3/3400 loss 25.635468 loss_att 33.447906 loss_ctc 41.466953 loss_rnnt 21.795261 hw_loss 0.312844 lr 0.00093875 rank 7
2023-02-17 07:23:35,203 DEBUG TRAIN Batch 3/3400 loss 46.413120 loss_att 49.469131 loss_ctc 67.512901 loss_rnnt 42.879620 hw_loss 0.204364 lr 0.00093764 rank 5
2023-02-17 07:24:55,933 DEBUG TRAIN Batch 3/3500 loss 33.264324 loss_att 42.813278 loss_ctc 53.630722 loss_rnnt 28.502266 hw_loss 0.256398 lr 0.00093619 rank 0
2023-02-17 07:24:55,949 DEBUG TRAIN Batch 3/3500 loss 46.942608 loss_att 59.893707 loss_ctc 73.960876 loss_rnnt 40.593025 hw_loss 0.294235 lr 0.00093599 rank 5
2023-02-17 07:24:55,969 DEBUG TRAIN Batch 3/3500 loss 38.549591 loss_att 43.482018 loss_ctc 55.600391 loss_rnnt 35.150444 hw_loss 0.261038 lr 0.00093710 rank 7
2023-02-17 07:24:55,987 DEBUG TRAIN Batch 3/3500 loss 33.617138 loss_att 40.481773 loss_ctc 47.741531 loss_rnnt 30.200768 hw_loss 0.300362 lr 0.00093631 rank 3
2023-02-17 07:24:55,990 DEBUG TRAIN Batch 3/3500 loss 35.033569 loss_att 36.236267 loss_ctc 54.214050 loss_rnnt 32.125465 hw_loss 0.206567 lr 0.00093588 rank 1
2023-02-17 07:24:55,992 DEBUG TRAIN Batch 3/3500 loss 49.360947 loss_att 54.512993 loss_ctc 66.605270 loss_rnnt 45.893665 hw_loss 0.258056 lr 0.00093687 rank 4
2023-02-17 07:24:55,994 DEBUG TRAIN Batch 3/3500 loss 22.446867 loss_att 28.827566 loss_ctc 41.911392 loss_rnnt 18.478716 hw_loss 0.181388 lr 0.00093665 rank 6
2023-02-17 07:24:56,001 DEBUG TRAIN Batch 3/3500 loss 19.118780 loss_att 24.417927 loss_ctc 26.626930 loss_rnnt 16.884411 hw_loss 0.325228 lr 0.00093562 rank 2
2023-02-17 07:26:27,790 DEBUG TRAIN Batch 3/3600 loss 21.891956 loss_att 28.281828 loss_ctc 38.447914 loss_rnnt 18.268620 hw_loss 0.258558 lr 0.00093501 rank 6
2023-02-17 07:26:27,793 DEBUG TRAIN Batch 3/3600 loss 29.266123 loss_att 33.324921 loss_ctc 47.653580 loss_rnnt 25.934368 hw_loss 0.128131 lr 0.00093424 rank 1
2023-02-17 07:26:27,795 DEBUG TRAIN Batch 3/3600 loss 23.395742 loss_att 32.332901 loss_ctc 42.110844 loss_rnnt 18.968159 hw_loss 0.271511 lr 0.00093436 rank 5
2023-02-17 07:26:27,796 DEBUG TRAIN Batch 3/3600 loss 29.780893 loss_att 33.869953 loss_ctc 41.416405 loss_rnnt 27.287968 hw_loss 0.231960 lr 0.00093522 rank 4
2023-02-17 07:26:27,796 DEBUG TRAIN Batch 3/3600 loss 26.268881 loss_att 31.469698 loss_ctc 43.003922 loss_rnnt 22.886946 hw_loss 0.207061 lr 0.00093545 rank 7
2023-02-17 07:26:27,797 DEBUG TRAIN Batch 3/3600 loss 24.852129 loss_att 34.847816 loss_ctc 39.792412 loss_rnnt 20.710991 hw_loss 0.281180 lr 0.00093455 rank 0
2023-02-17 07:26:27,801 DEBUG TRAIN Batch 3/3600 loss 22.862001 loss_att 26.501314 loss_ctc 34.237892 loss_rnnt 20.484211 hw_loss 0.249643 lr 0.00093398 rank 2
2023-02-17 07:26:27,847 DEBUG TRAIN Batch 3/3600 loss 16.238995 loss_att 23.078159 loss_ctc 22.714785 loss_rnnt 13.857069 hw_loss 0.282476 lr 0.00093467 rank 3
2023-02-17 07:27:43,924 DEBUG TRAIN Batch 3/3700 loss 27.824289 loss_att 36.251984 loss_ctc 48.586975 loss_rnnt 23.225670 hw_loss 0.271353 lr 0.00093338 rank 6
2023-02-17 07:27:43,926 DEBUG TRAIN Batch 3/3700 loss 32.766315 loss_att 37.322617 loss_ctc 51.579983 loss_rnnt 29.185133 hw_loss 0.302683 lr 0.00093304 rank 3
2023-02-17 07:27:43,927 DEBUG TRAIN Batch 3/3700 loss 26.203569 loss_att 38.097679 loss_ctc 46.367416 loss_rnnt 20.987993 hw_loss 0.277952 lr 0.00093273 rank 5
2023-02-17 07:27:43,929 DEBUG TRAIN Batch 3/3700 loss 15.640119 loss_att 23.502480 loss_ctc 27.208735 loss_rnnt 12.367381 hw_loss 0.295842 lr 0.00093262 rank 1
2023-02-17 07:27:43,930 DEBUG TRAIN Batch 3/3700 loss 25.420286 loss_att 29.741581 loss_ctc 43.034492 loss_rnnt 22.125437 hw_loss 0.153806 lr 0.00093236 rank 2
2023-02-17 07:27:43,930 DEBUG TRAIN Batch 3/3700 loss 30.025297 loss_att 38.029037 loss_ctc 47.885193 loss_rnnt 25.899933 hw_loss 0.268681 lr 0.00093293 rank 0
2023-02-17 07:27:43,935 DEBUG TRAIN Batch 3/3700 loss 25.628891 loss_att 29.240814 loss_ctc 43.732704 loss_rnnt 22.338907 hw_loss 0.288296 lr 0.00093359 rank 4
2023-02-17 07:27:43,939 DEBUG TRAIN Batch 3/3700 loss 32.145679 loss_att 41.420288 loss_ctc 46.782967 loss_rnnt 28.214081 hw_loss 0.234440 lr 0.00093382 rank 7
2023-02-17 07:28:59,478 DEBUG TRAIN Batch 3/3800 loss 28.269814 loss_att 33.189232 loss_ctc 37.352028 loss_rnnt 25.899593 hw_loss 0.328826 lr 0.00093176 rank 6
2023-02-17 07:28:59,481 DEBUG TRAIN Batch 3/3800 loss 12.401552 loss_att 12.386791 loss_ctc 16.498735 loss_rnnt 11.677926 hw_loss 0.338036 lr 0.00093220 rank 7
2023-02-17 07:28:59,480 DEBUG TRAIN Batch 3/3800 loss 21.038853 loss_att 24.142729 loss_ctc 29.610750 loss_rnnt 19.137644 hw_loss 0.257836 lr 0.00093131 rank 0
2023-02-17 07:28:59,480 DEBUG TRAIN Batch 3/3800 loss 13.148606 loss_att 12.994018 loss_ctc 16.920580 loss_rnnt 12.443424 hw_loss 0.437196 lr 0.00093100 rank 1
2023-02-17 07:28:59,486 DEBUG TRAIN Batch 3/3800 loss 20.148891 loss_att 25.139215 loss_ctc 30.616751 loss_rnnt 17.596334 hw_loss 0.297710 lr 0.00093111 rank 5
2023-02-17 07:28:59,487 DEBUG TRAIN Batch 3/3800 loss 17.964699 loss_att 17.491421 loss_ctc 23.377417 loss_rnnt 17.152355 hw_loss 0.347445 lr 0.00093074 rank 2
2023-02-17 07:28:59,490 DEBUG TRAIN Batch 3/3800 loss 16.178886 loss_att 15.760199 loss_ctc 23.573961 loss_rnnt 15.057553 hw_loss 0.410736 lr 0.00093142 rank 3
2023-02-17 07:28:59,491 DEBUG TRAIN Batch 3/3800 loss 33.450428 loss_att 37.134331 loss_ctc 42.813683 loss_rnnt 31.276711 hw_loss 0.353442 lr 0.00093197 rank 4
2023-02-17 07:30:24,358 DEBUG TRAIN Batch 3/3900 loss 25.965885 loss_att 38.572495 loss_ctc 48.386223 loss_rnnt 20.248497 hw_loss 0.387537 lr 0.00093036 rank 4
2023-02-17 07:30:24,371 DEBUG TRAIN Batch 3/3900 loss 39.918266 loss_att 47.947670 loss_ctc 47.879173 loss_rnnt 37.093613 hw_loss 0.294976 lr 0.00092970 rank 0
2023-02-17 07:30:24,374 DEBUG TRAIN Batch 3/3900 loss 25.597996 loss_att 34.120651 loss_ctc 41.589050 loss_rnnt 21.585300 hw_loss 0.330045 lr 0.00092981 rank 3
2023-02-17 07:30:24,384 DEBUG TRAIN Batch 3/3900 loss 34.130527 loss_att 46.723660 loss_ctc 55.255291 loss_rnnt 28.633057 hw_loss 0.304137 lr 0.00092950 rank 5
2023-02-17 07:30:24,406 DEBUG TRAIN Batch 3/3900 loss 50.008217 loss_att 64.179474 loss_ctc 73.812660 loss_rnnt 43.881516 hw_loss 0.222229 lr 0.00092939 rank 1
2023-02-17 07:30:24,406 DEBUG TRAIN Batch 3/3900 loss 23.541754 loss_att 31.559465 loss_ctc 41.092236 loss_rnnt 19.408834 hw_loss 0.354962 lr 0.00093058 rank 7
2023-02-17 07:30:24,412 DEBUG TRAIN Batch 3/3900 loss 18.698236 loss_att 25.951075 loss_ctc 32.778210 loss_rnnt 15.204771 hw_loss 0.310438 lr 0.00092913 rank 2
2023-02-17 07:30:24,426 DEBUG TRAIN Batch 3/3900 loss 52.594624 loss_att 59.316879 loss_ctc 78.757202 loss_rnnt 47.591820 hw_loss 0.318766 lr 0.00093015 rank 6
2023-02-17 07:31:50,779 DEBUG TRAIN Batch 3/4000 loss 17.501099 loss_att 26.201843 loss_ctc 34.128860 loss_rnnt 13.408778 hw_loss 0.253382 lr 0.00092854 rank 6
2023-02-17 07:31:50,782 DEBUG TRAIN Batch 3/4000 loss 46.022129 loss_att 56.291924 loss_ctc 65.351028 loss_rnnt 41.226089 hw_loss 0.309173 lr 0.00092779 rank 1
2023-02-17 07:31:50,782 DEBUG TRAIN Batch 3/4000 loss 34.688782 loss_att 39.516834 loss_ctc 43.066505 loss_rnnt 32.458405 hw_loss 0.277000 lr 0.00092820 rank 3
2023-02-17 07:31:50,782 DEBUG TRAIN Batch 3/4000 loss 49.859829 loss_att 56.258072 loss_ctc 84.623878 loss_rnnt 43.816952 hw_loss 0.240048 lr 0.00092809 rank 0
2023-02-17 07:31:50,784 DEBUG TRAIN Batch 3/4000 loss 28.169552 loss_att 32.346306 loss_ctc 38.562653 loss_rnnt 25.832130 hw_loss 0.218105 lr 0.00092753 rank 2
2023-02-17 07:31:50,784 DEBUG TRAIN Batch 3/4000 loss 38.057503 loss_att 47.011284 loss_ctc 64.717491 loss_rnnt 32.528172 hw_loss 0.344833 lr 0.00092790 rank 5
2023-02-17 07:31:50,807 DEBUG TRAIN Batch 3/4000 loss 28.764492 loss_att 40.189659 loss_ctc 46.903229 loss_rnnt 23.904900 hw_loss 0.292615 lr 0.00092897 rank 7
2023-02-17 07:31:50,813 DEBUG TRAIN Batch 3/4000 loss 31.486897 loss_att 36.008057 loss_ctc 47.260651 loss_rnnt 28.361984 hw_loss 0.220336 lr 0.00092875 rank 4
2023-02-17 07:33:06,298 DEBUG TRAIN Batch 3/4100 loss 28.156404 loss_att 35.890572 loss_ctc 40.514973 loss_rnnt 24.840866 hw_loss 0.226676 lr 0.00092631 rank 5
2023-02-17 07:33:06,298 DEBUG TRAIN Batch 3/4100 loss 24.492237 loss_att 29.088961 loss_ctc 33.843040 loss_rnnt 22.237268 hw_loss 0.166595 lr 0.00092661 rank 3
2023-02-17 07:33:06,299 DEBUG TRAIN Batch 3/4100 loss 26.834461 loss_att 37.324860 loss_ctc 52.466702 loss_rnnt 21.178495 hw_loss 0.262976 lr 0.00092650 rank 0
2023-02-17 07:33:06,300 DEBUG TRAIN Batch 3/4100 loss 21.654871 loss_att 32.632698 loss_ctc 34.076252 loss_rnnt 17.672039 hw_loss 0.245776 lr 0.00092620 rank 1
2023-02-17 07:33:06,301 DEBUG TRAIN Batch 3/4100 loss 37.208172 loss_att 48.850426 loss_ctc 49.296257 loss_rnnt 33.133106 hw_loss 0.252881 lr 0.00092694 rank 6
2023-02-17 07:33:06,303 DEBUG TRAIN Batch 3/4100 loss 43.074497 loss_att 49.171421 loss_ctc 65.101303 loss_rnnt 38.765102 hw_loss 0.287062 lr 0.00092715 rank 4
2023-02-17 07:33:06,302 DEBUG TRAIN Batch 3/4100 loss 31.415922 loss_att 38.653122 loss_ctc 56.686176 loss_rnnt 26.424276 hw_loss 0.327825 lr 0.00092594 rank 2
2023-02-17 07:33:06,305 DEBUG TRAIN Batch 3/4100 loss 29.461657 loss_att 39.551971 loss_ctc 52.615429 loss_rnnt 24.253716 hw_loss 0.192573 lr 0.00092737 rank 7
2023-02-17 07:34:25,696 DEBUG TRAIN Batch 3/4200 loss 46.081383 loss_att 54.929447 loss_ctc 73.305344 loss_rnnt 40.517746 hw_loss 0.307794 lr 0.00092491 rank 0
2023-02-17 07:34:25,706 DEBUG TRAIN Batch 3/4200 loss 25.193539 loss_att 29.848942 loss_ctc 37.013302 loss_rnnt 22.518940 hw_loss 0.314148 lr 0.00092535 rank 6
2023-02-17 07:34:25,710 DEBUG TRAIN Batch 3/4200 loss 16.558189 loss_att 22.116684 loss_ctc 26.338043 loss_rnnt 14.011703 hw_loss 0.245266 lr 0.00092461 rank 1
2023-02-17 07:34:25,749 DEBUG TRAIN Batch 3/4200 loss 24.620533 loss_att 32.768101 loss_ctc 37.941460 loss_rnnt 21.103056 hw_loss 0.209703 lr 0.00092578 rank 7
2023-02-17 07:34:25,754 DEBUG TRAIN Batch 3/4200 loss 29.568176 loss_att 35.914177 loss_ctc 41.736721 loss_rnnt 26.563694 hw_loss 0.211515 lr 0.00092472 rank 5
2023-02-17 07:34:25,758 DEBUG TRAIN Batch 3/4200 loss 31.652821 loss_att 35.819077 loss_ctc 45.283947 loss_rnnt 28.909628 hw_loss 0.173359 lr 0.00092502 rank 3
2023-02-17 07:34:25,781 DEBUG TRAIN Batch 3/4200 loss 50.816040 loss_att 57.230793 loss_ctc 77.512917 loss_rnnt 45.711372 hw_loss 0.491485 lr 0.00092556 rank 4
2023-02-17 07:34:25,783 DEBUG TRAIN Batch 3/4200 loss 16.970785 loss_att 20.393120 loss_ctc 30.101793 loss_rnnt 14.401285 hw_loss 0.251685 lr 0.00092436 rank 2
2023-02-17 07:35:58,375 DEBUG TRAIN Batch 3/4300 loss 28.942440 loss_att 36.278534 loss_ctc 49.128761 loss_rnnt 24.644428 hw_loss 0.261154 lr 0.00092303 rank 1
2023-02-17 07:35:58,377 DEBUG TRAIN Batch 3/4300 loss 34.963142 loss_att 43.419518 loss_ctc 54.186630 loss_rnnt 30.599068 hw_loss 0.205628 lr 0.00092314 rank 5
2023-02-17 07:35:58,377 DEBUG TRAIN Batch 3/4300 loss 39.481995 loss_att 47.998215 loss_ctc 61.749939 loss_rnnt 34.685322 hw_loss 0.233187 lr 0.00092333 rank 0
2023-02-17 07:35:58,378 DEBUG TRAIN Batch 3/4300 loss 26.639978 loss_att 33.600670 loss_ctc 43.295609 loss_rnnt 22.791794 hw_loss 0.441179 lr 0.00092377 rank 6
2023-02-17 07:35:58,379 DEBUG TRAIN Batch 3/4300 loss 19.186012 loss_att 25.111074 loss_ctc 31.804737 loss_rnnt 16.132694 hw_loss 0.348390 lr 0.00092278 rank 2
2023-02-17 07:35:58,379 DEBUG TRAIN Batch 3/4300 loss 25.309841 loss_att 36.772568 loss_ctc 46.151337 loss_rnnt 20.124180 hw_loss 0.214215 lr 0.00092398 rank 4
2023-02-17 07:35:58,379 DEBUG TRAIN Batch 3/4300 loss 24.290762 loss_att 27.837559 loss_ctc 39.233032 loss_rnnt 21.397293 hw_loss 0.359634 lr 0.00092420 rank 7
2023-02-17 07:35:58,427 DEBUG TRAIN Batch 3/4300 loss 26.104395 loss_att 37.798347 loss_ctc 40.864563 loss_rnnt 21.612682 hw_loss 0.346690 lr 0.00092344 rank 3
2023-02-17 07:37:14,264 DEBUG TRAIN Batch 3/4400 loss 31.325676 loss_att 32.303402 loss_ctc 42.563679 loss_rnnt 29.408667 hw_loss 0.418244 lr 0.00092241 rank 4
2023-02-17 07:37:14,265 DEBUG TRAIN Batch 3/4400 loss 29.578871 loss_att 30.468477 loss_ctc 41.675381 loss_rnnt 27.625280 hw_loss 0.305252 lr 0.00092176 rank 0
2023-02-17 07:37:14,265 DEBUG TRAIN Batch 3/4400 loss 33.356228 loss_att 39.885166 loss_ctc 49.478718 loss_rnnt 29.775167 hw_loss 0.235509 lr 0.00092220 rank 6
2023-02-17 07:37:14,267 DEBUG TRAIN Batch 3/4400 loss 33.764641 loss_att 34.125729 loss_ctc 48.100079 loss_rnnt 31.626183 hw_loss 0.290349 lr 0.00092263 rank 7
2023-02-17 07:37:14,267 DEBUG TRAIN Batch 3/4400 loss 37.244186 loss_att 38.088486 loss_ctc 50.210861 loss_rnnt 35.211079 hw_loss 0.253790 lr 0.00092157 rank 5
2023-02-17 07:37:14,268 DEBUG TRAIN Batch 3/4400 loss 15.642141 loss_att 16.512295 loss_ctc 23.031454 loss_rnnt 14.284436 hw_loss 0.372062 lr 0.00092122 rank 2
2023-02-17 07:37:14,298 DEBUG TRAIN Batch 3/4400 loss 16.404833 loss_att 20.067787 loss_ctc 24.969997 loss_rnnt 14.392021 hw_loss 0.259121 lr 0.00092147 rank 1
2023-02-17 07:37:14,313 DEBUG TRAIN Batch 3/4400 loss 25.412855 loss_att 29.466661 loss_ctc 46.513943 loss_rnnt 21.592335 hw_loss 0.368024 lr 0.00092187 rank 3
2023-02-17 07:38:29,795 DEBUG TRAIN Batch 3/4500 loss 28.190027 loss_att 39.468323 loss_ctc 39.295769 loss_rnnt 24.360710 hw_loss 0.174173 lr 0.00092064 rank 6
2023-02-17 07:38:29,803 DEBUG TRAIN Batch 3/4500 loss 30.684732 loss_att 37.402576 loss_ctc 53.067017 loss_rnnt 26.257483 hw_loss 0.186333 lr 0.00092031 rank 3
2023-02-17 07:38:29,804 DEBUG TRAIN Batch 3/4500 loss 31.131401 loss_att 38.983360 loss_ctc 45.264339 loss_rnnt 27.511253 hw_loss 0.310061 lr 0.00091966 rank 2
2023-02-17 07:38:29,808 DEBUG TRAIN Batch 3/4500 loss 28.607332 loss_att 41.649391 loss_ctc 46.309402 loss_rnnt 23.479153 hw_loss 0.299049 lr 0.00091990 rank 1
2023-02-17 07:38:29,829 DEBUG TRAIN Batch 3/4500 loss 22.096918 loss_att 21.123260 loss_ctc 27.730631 loss_rnnt 21.362946 hw_loss 0.332892 lr 0.00092001 rank 5
2023-02-17 07:38:29,833 DEBUG TRAIN Batch 3/4500 loss 21.721096 loss_att 29.540169 loss_ctc 35.210663 loss_rnnt 18.220173 hw_loss 0.259686 lr 0.00092106 rank 7
2023-02-17 07:38:29,848 DEBUG TRAIN Batch 3/4500 loss 28.137445 loss_att 45.012360 loss_ctc 46.049255 loss_rnnt 22.314400 hw_loss 0.112164 lr 0.00092084 rank 4
2023-02-17 07:38:29,849 DEBUG TRAIN Batch 3/4500 loss 22.538387 loss_att 22.589291 loss_ctc 25.494169 loss_rnnt 21.925068 hw_loss 0.391936 lr 0.00092020 rank 0
2023-02-17 07:39:53,535 DEBUG TRAIN Batch 3/4600 loss 14.318529 loss_att 22.196363 loss_ctc 24.650787 loss_rnnt 11.164856 hw_loss 0.375886 lr 0.00091875 rank 3
2023-02-17 07:39:53,535 DEBUG TRAIN Batch 3/4600 loss 34.499825 loss_att 41.802692 loss_ctc 64.933899 loss_rnnt 28.838240 hw_loss 0.268372 lr 0.00091846 rank 5
2023-02-17 07:39:53,551 DEBUG TRAIN Batch 3/4600 loss 14.621214 loss_att 20.227840 loss_ctc 19.929657 loss_rnnt 12.638296 hw_loss 0.288378 lr 0.00091865 rank 0
2023-02-17 07:39:53,567 DEBUG TRAIN Batch 3/4600 loss 29.280443 loss_att 37.459724 loss_ctc 43.610130 loss_rnnt 25.586731 hw_loss 0.276054 lr 0.00091810 rank 2
2023-02-17 07:39:53,570 DEBUG TRAIN Batch 3/4600 loss 14.622261 loss_att 23.962177 loss_ctc 28.902233 loss_rnnt 10.673953 hw_loss 0.330616 lr 0.00091950 rank 7
2023-02-17 07:39:53,586 DEBUG TRAIN Batch 3/4600 loss 27.531429 loss_att 38.531857 loss_ctc 44.026134 loss_rnnt 22.956429 hw_loss 0.329295 lr 0.00091835 rank 1
2023-02-17 07:39:53,602 DEBUG TRAIN Batch 3/4600 loss 22.307194 loss_att 28.870239 loss_ctc 35.633179 loss_rnnt 19.050503 hw_loss 0.313659 lr 0.00091908 rank 6
2023-02-17 07:39:53,603 DEBUG TRAIN Batch 3/4600 loss 29.526781 loss_att 38.506355 loss_ctc 44.595428 loss_rnnt 25.655684 hw_loss 0.123807 lr 0.00091928 rank 4
2023-02-17 07:41:20,492 DEBUG TRAIN Batch 3/4700 loss 34.508701 loss_att 44.009331 loss_ctc 52.525436 loss_rnnt 30.005188 hw_loss 0.377164 lr 0.00091656 rank 2
2023-02-17 07:41:20,493 DEBUG TRAIN Batch 3/4700 loss 38.892094 loss_att 48.264004 loss_ctc 62.651501 loss_rnnt 33.745506 hw_loss 0.195533 lr 0.00091681 rank 1
2023-02-17 07:41:20,493 DEBUG TRAIN Batch 3/4700 loss 39.850109 loss_att 43.969063 loss_ctc 63.622101 loss_rnnt 35.789490 hw_loss 0.126053 lr 0.00091721 rank 3
2023-02-17 07:41:20,500 DEBUG TRAIN Batch 3/4700 loss 39.584515 loss_att 47.234791 loss_ctc 57.443230 loss_rnnt 35.592331 hw_loss 0.151807 lr 0.00091773 rank 4
2023-02-17 07:41:20,519 DEBUG TRAIN Batch 3/4700 loss 21.101603 loss_att 32.362347 loss_ctc 38.930744 loss_rnnt 16.361120 hw_loss 0.208343 lr 0.00091753 rank 6
2023-02-17 07:41:20,527 DEBUG TRAIN Batch 3/4700 loss 32.743481 loss_att 35.337555 loss_ctc 43.643993 loss_rnnt 30.613544 hw_loss 0.295723 lr 0.00091795 rank 7
2023-02-17 07:41:20,544 DEBUG TRAIN Batch 3/4700 loss 23.146244 loss_att 31.980864 loss_ctc 36.928814 loss_rnnt 19.358898 hw_loss 0.342644 lr 0.00091710 rank 0
2023-02-17 07:41:20,548 DEBUG TRAIN Batch 3/4700 loss 21.829685 loss_att 28.357353 loss_ctc 39.090500 loss_rnnt 18.110390 hw_loss 0.210601 lr 0.00091691 rank 5
2023-02-17 07:42:35,261 DEBUG TRAIN Batch 3/4800 loss 31.942356 loss_att 43.703957 loss_ctc 45.972416 loss_rnnt 27.609003 hw_loss 0.206917 lr 0.00091599 rank 6
2023-02-17 07:42:35,264 DEBUG TRAIN Batch 3/4800 loss 28.236973 loss_att 34.510963 loss_ctc 48.644257 loss_rnnt 24.099739 hw_loss 0.302746 lr 0.00091556 rank 0
2023-02-17 07:42:35,264 DEBUG TRAIN Batch 3/4800 loss 18.288635 loss_att 28.012651 loss_ctc 35.868542 loss_rnnt 13.823526 hw_loss 0.330595 lr 0.00091538 rank 5
2023-02-17 07:42:35,266 DEBUG TRAIN Batch 3/4800 loss 24.488598 loss_att 31.744343 loss_ctc 40.034935 loss_rnnt 20.872934 hw_loss 0.171876 lr 0.00091641 rank 7
2023-02-17 07:42:35,288 DEBUG TRAIN Batch 3/4800 loss 33.850754 loss_att 40.625702 loss_ctc 50.987541 loss_rnnt 30.127777 hw_loss 0.155776 lr 0.00091502 rank 2
2023-02-17 07:42:35,289 DEBUG TRAIN Batch 3/4800 loss 25.170113 loss_att 35.888008 loss_ctc 46.997467 loss_rnnt 19.977800 hw_loss 0.259535 lr 0.00091619 rank 4
2023-02-17 07:42:35,294 DEBUG TRAIN Batch 3/4800 loss 44.834461 loss_att 48.098026 loss_ctc 66.982178 loss_rnnt 41.094700 hw_loss 0.251284 lr 0.00091527 rank 1
2023-02-17 07:42:35,328 DEBUG TRAIN Batch 3/4800 loss 25.061722 loss_att 31.770823 loss_ctc 33.789917 loss_rnnt 22.416380 hw_loss 0.262056 lr 0.00091567 rank 3
2023-02-17 07:43:52,330 DEBUG TRAIN Batch 3/4900 loss 25.177629 loss_att 37.744644 loss_ctc 47.530441 loss_rnnt 19.483852 hw_loss 0.374999 lr 0.00091466 rank 4
2023-02-17 07:43:52,335 DEBUG TRAIN Batch 3/4900 loss 15.084523 loss_att 20.810337 loss_ctc 30.136026 loss_rnnt 11.751774 hw_loss 0.338849 lr 0.00091414 rank 3
2023-02-17 07:43:52,352 DEBUG TRAIN Batch 3/4900 loss 27.958509 loss_att 37.206688 loss_ctc 45.177467 loss_rnnt 23.691029 hw_loss 0.228718 lr 0.00091350 rank 2
2023-02-17 07:43:52,358 DEBUG TRAIN Batch 3/4900 loss 22.283993 loss_att 25.284149 loss_ctc 34.420357 loss_rnnt 19.937338 hw_loss 0.240831 lr 0.00091403 rank 0
2023-02-17 07:43:52,392 DEBUG TRAIN Batch 3/4900 loss 34.586582 loss_att 41.970894 loss_ctc 44.986797 loss_rnnt 31.582413 hw_loss 0.263643 lr 0.00091374 rank 1
2023-02-17 07:43:52,399 DEBUG TRAIN Batch 3/4900 loss 23.057537 loss_att 29.115524 loss_ctc 37.654301 loss_rnnt 19.785757 hw_loss 0.213650 lr 0.00091385 rank 5
2023-02-17 07:43:52,416 DEBUG TRAIN Batch 3/4900 loss 29.104286 loss_att 31.037252 loss_ctc 44.653332 loss_rnnt 26.500797 hw_loss 0.269412 lr 0.00091487 rank 7
2023-02-17 07:43:52,423 DEBUG TRAIN Batch 3/4900 loss 38.567486 loss_att 46.436546 loss_ctc 66.831787 loss_rnnt 33.050041 hw_loss 0.328231 lr 0.00091446 rank 6
2023-02-17 07:45:25,873 DEBUG TRAIN Batch 3/5000 loss 29.789862 loss_att 39.114483 loss_ctc 47.237591 loss_rnnt 25.512655 hw_loss 0.161101 lr 0.00091251 rank 0
2023-02-17 07:45:25,876 DEBUG TRAIN Batch 3/5000 loss 20.844614 loss_att 30.484585 loss_ctc 42.658722 loss_rnnt 15.854252 hw_loss 0.288415 lr 0.00091261 rank 3
2023-02-17 07:45:25,876 DEBUG TRAIN Batch 3/5000 loss 23.495485 loss_att 27.967342 loss_ctc 32.331142 loss_rnnt 21.268255 hw_loss 0.290200 lr 0.00091313 rank 4
2023-02-17 07:45:25,876 DEBUG TRAIN Batch 3/5000 loss 24.367533 loss_att 26.601364 loss_ctc 34.013882 loss_rnnt 22.481070 hw_loss 0.287844 lr 0.00091222 rank 1
2023-02-17 07:45:25,877 DEBUG TRAIN Batch 3/5000 loss 20.959457 loss_att 24.200378 loss_ctc 31.871140 loss_rnnt 18.663139 hw_loss 0.362328 lr 0.00091197 rank 2
2023-02-17 07:45:25,907 DEBUG TRAIN Batch 3/5000 loss 40.942657 loss_att 49.320168 loss_ctc 64.147476 loss_rnnt 36.001854 hw_loss 0.321239 lr 0.00091293 rank 6
2023-02-17 07:45:25,908 DEBUG TRAIN Batch 3/5000 loss 26.362041 loss_att 29.920269 loss_ctc 35.162472 loss_rnnt 24.346657 hw_loss 0.244401 lr 0.00091232 rank 5
2023-02-17 07:45:25,923 DEBUG TRAIN Batch 3/5000 loss 17.870302 loss_att 22.841644 loss_ctc 24.187603 loss_rnnt 15.897596 hw_loss 0.255248 lr 0.00091334 rank 7
2023-02-17 07:46:42,322 DEBUG TRAIN Batch 3/5100 loss 18.730469 loss_att 19.642269 loss_ctc 25.276606 loss_rnnt 17.484808 hw_loss 0.357157 lr 0.00091141 rank 6
2023-02-17 07:46:42,326 DEBUG TRAIN Batch 3/5100 loss 16.066854 loss_att 18.658447 loss_ctc 22.905499 loss_rnnt 14.390536 hw_loss 0.461587 lr 0.00091070 rank 1
2023-02-17 07:46:42,327 DEBUG TRAIN Batch 3/5100 loss 34.167633 loss_att 38.628372 loss_ctc 55.886456 loss_rnnt 30.213072 hw_loss 0.312316 lr 0.00091046 rank 2
2023-02-17 07:46:42,328 DEBUG TRAIN Batch 3/5100 loss 20.215836 loss_att 22.176239 loss_ctc 31.408285 loss_rnnt 18.149683 hw_loss 0.340765 lr 0.00091099 rank 0
2023-02-17 07:46:42,327 DEBUG TRAIN Batch 3/5100 loss 16.975767 loss_att 21.883604 loss_ctc 27.229824 loss_rnnt 14.446716 hw_loss 0.338017 lr 0.00091081 rank 5
2023-02-17 07:46:42,331 DEBUG TRAIN Batch 3/5100 loss 41.198013 loss_att 50.571949 loss_ctc 56.176262 loss_rnnt 37.223083 hw_loss 0.193206 lr 0.00091182 rank 7
2023-02-17 07:46:42,336 DEBUG TRAIN Batch 3/5100 loss 15.406434 loss_att 14.163754 loss_ctc 19.410572 loss_rnnt 14.965792 hw_loss 0.291172 lr 0.00091161 rank 4
2023-02-17 07:46:42,344 DEBUG TRAIN Batch 3/5100 loss 12.140733 loss_att 13.026169 loss_ctc 16.153801 loss_rnnt 11.262366 hw_loss 0.311631 lr 0.00091110 rank 3
2023-02-17 07:47:57,643 DEBUG TRAIN Batch 3/5200 loss 33.729736 loss_att 40.572151 loss_ctc 53.067436 loss_rnnt 29.648184 hw_loss 0.252585 lr 0.00090930 rank 5
2023-02-17 07:47:57,648 DEBUG TRAIN Batch 3/5200 loss 30.215216 loss_att 42.871731 loss_ctc 48.737503 loss_rnnt 25.116722 hw_loss 0.182910 lr 0.00090896 rank 2
2023-02-17 07:47:57,650 DEBUG TRAIN Batch 3/5200 loss 31.846643 loss_att 38.448479 loss_ctc 48.374187 loss_rnnt 28.179115 hw_loss 0.269036 lr 0.00091031 rank 7
2023-02-17 07:47:57,658 DEBUG TRAIN Batch 3/5200 loss 20.139246 loss_att 25.802010 loss_ctc 29.314079 loss_rnnt 17.607218 hw_loss 0.330307 lr 0.00090948 rank 0
2023-02-17 07:47:57,662 DEBUG TRAIN Batch 3/5200 loss 22.940742 loss_att 29.893082 loss_ctc 32.142227 loss_rnnt 20.173986 hw_loss 0.280171 lr 0.00090990 rank 6
2023-02-17 07:47:57,669 DEBUG TRAIN Batch 3/5200 loss 17.606728 loss_att 24.604588 loss_ctc 33.264412 loss_rnnt 14.030482 hw_loss 0.166842 lr 0.00091010 rank 4
2023-02-17 07:47:57,698 DEBUG TRAIN Batch 3/5200 loss 33.998512 loss_att 38.894073 loss_ctc 47.613159 loss_rnnt 31.047771 hw_loss 0.293137 lr 0.00090959 rank 3
2023-02-17 07:47:57,709 DEBUG TRAIN Batch 3/5200 loss 12.277143 loss_att 20.699919 loss_ctc 20.289917 loss_rnnt 9.407825 hw_loss 0.218234 lr 0.00090920 rank 1
2023-02-17 07:49:20,644 DEBUG TRAIN Batch 3/5300 loss 29.758673 loss_att 31.539713 loss_ctc 39.002590 loss_rnnt 28.042568 hw_loss 0.238828 lr 0.00090746 rank 2
2023-02-17 07:49:20,647 DEBUG TRAIN Batch 3/5300 loss 66.511124 loss_att 70.487610 loss_ctc 99.282532 loss_rnnt 61.172050 hw_loss 0.326720 lr 0.00090798 rank 0
2023-02-17 07:49:20,669 DEBUG TRAIN Batch 3/5300 loss 26.082172 loss_att 40.971649 loss_ctc 47.130287 loss_rnnt 20.119461 hw_loss 0.334502 lr 0.00090809 rank 3
2023-02-17 07:49:20,673 DEBUG TRAIN Batch 3/5300 loss 21.942474 loss_att 23.433880 loss_ctc 27.817947 loss_rnnt 20.745768 hw_loss 0.215680 lr 0.00090780 rank 5
2023-02-17 07:49:20,682 DEBUG TRAIN Batch 3/5300 loss 20.256584 loss_att 24.822899 loss_ctc 30.223974 loss_rnnt 17.913954 hw_loss 0.188218 lr 0.00090840 rank 6
2023-02-17 07:49:20,684 DEBUG TRAIN Batch 3/5300 loss 79.172432 loss_att 95.343719 loss_ctc 102.860222 loss_rnnt 72.559219 hw_loss 0.413582 lr 0.00090770 rank 1
2023-02-17 07:49:20,709 DEBUG TRAIN Batch 3/5300 loss 36.971481 loss_att 42.101093 loss_ctc 52.343803 loss_rnnt 33.754730 hw_loss 0.264718 lr 0.00090860 rank 4
2023-02-17 07:49:20,709 DEBUG TRAIN Batch 3/5300 loss 36.024750 loss_att 40.465050 loss_ctc 54.702816 loss_rnnt 32.516598 hw_loss 0.243159 lr 0.00090881 rank 7
2023-02-17 07:50:46,501 DEBUG TRAIN Batch 3/5400 loss 36.740025 loss_att 44.558319 loss_ctc 54.733208 loss_rnnt 32.600594 hw_loss 0.331281 lr 0.00090631 rank 5
2023-02-17 07:50:46,506 DEBUG TRAIN Batch 3/5400 loss 14.582336 loss_att 22.928997 loss_ctc 27.818541 loss_rnnt 11.000973 hw_loss 0.276007 lr 0.00090690 rank 6
2023-02-17 07:50:46,506 DEBUG TRAIN Batch 3/5400 loss 25.134737 loss_att 35.237999 loss_ctc 41.526726 loss_rnnt 20.778301 hw_loss 0.281597 lr 0.00090649 rank 0
2023-02-17 07:50:46,507 DEBUG TRAIN Batch 3/5400 loss 23.131495 loss_att 32.487625 loss_ctc 40.561775 loss_rnnt 18.801250 hw_loss 0.253085 lr 0.00090620 rank 1
2023-02-17 07:50:46,511 DEBUG TRAIN Batch 3/5400 loss 17.655697 loss_att 27.825024 loss_ctc 28.512283 loss_rnnt 13.993174 hw_loss 0.339585 lr 0.00090597 rank 2
2023-02-17 07:50:46,536 DEBUG TRAIN Batch 3/5400 loss 24.221638 loss_att 28.486399 loss_ctc 43.584675 loss_rnnt 20.660786 hw_loss 0.236548 lr 0.00090710 rank 4
2023-02-17 07:50:46,544 DEBUG TRAIN Batch 3/5400 loss 22.105583 loss_att 26.826441 loss_ctc 30.039703 loss_rnnt 19.987215 hw_loss 0.218087 lr 0.00090731 rank 7
2023-02-17 07:50:46,561 DEBUG TRAIN Batch 3/5400 loss 25.684958 loss_att 30.413671 loss_ctc 40.119007 loss_rnnt 22.682865 hw_loss 0.247138 lr 0.00090659 rank 3
2023-02-17 07:52:02,036 DEBUG TRAIN Batch 3/5500 loss 36.886250 loss_att 43.693745 loss_ctc 60.848198 loss_rnnt 32.230331 hw_loss 0.186544 lr 0.00090542 rank 6
2023-02-17 07:52:02,038 DEBUG TRAIN Batch 3/5500 loss 19.572756 loss_att 27.289307 loss_ctc 29.986282 loss_rnnt 16.521008 hw_loss 0.224939 lr 0.00090511 rank 3
2023-02-17 07:52:02,041 DEBUG TRAIN Batch 3/5500 loss 29.183760 loss_att 36.117928 loss_ctc 47.363998 loss_rnnt 25.195517 hw_loss 0.332581 lr 0.00090448 rank 2
2023-02-17 07:52:02,041 DEBUG TRAIN Batch 3/5500 loss 26.508314 loss_att 30.318010 loss_ctc 32.537815 loss_rnnt 24.835724 hw_loss 0.200094 lr 0.00090500 rank 0
2023-02-17 07:52:02,040 DEBUG TRAIN Batch 3/5500 loss 32.768387 loss_att 35.942295 loss_ctc 51.354034 loss_rnnt 29.514616 hw_loss 0.264188 lr 0.00090561 rank 4
2023-02-17 07:52:02,041 DEBUG TRAIN Batch 3/5500 loss 31.160202 loss_att 33.416004 loss_ctc 47.078182 loss_rnnt 28.459167 hw_loss 0.239018 lr 0.00090472 rank 1
2023-02-17 07:52:02,043 DEBUG TRAIN Batch 3/5500 loss 27.001284 loss_att 31.665504 loss_ctc 36.296196 loss_rnnt 24.617298 hw_loss 0.397156 lr 0.00090482 rank 5
2023-02-17 07:52:02,088 DEBUG TRAIN Batch 3/5500 loss 18.201252 loss_att 25.807896 loss_ctc 29.403040 loss_rnnt 15.078352 hw_loss 0.202499 lr 0.00090582 rank 7
2023-02-17 07:53:19,149 DEBUG TRAIN Batch 3/5600 loss 25.241243 loss_att 28.485001 loss_ctc 36.633949 loss_rnnt 22.948254 hw_loss 0.234770 lr 0.00090394 rank 6
2023-02-17 07:53:19,153 DEBUG TRAIN Batch 3/5600 loss 22.354694 loss_att 25.391270 loss_ctc 32.345131 loss_rnnt 20.258404 hw_loss 0.294217 lr 0.00090434 rank 7
2023-02-17 07:53:19,179 DEBUG TRAIN Batch 3/5600 loss 32.574116 loss_att 39.828682 loss_ctc 54.687439 loss_rnnt 27.972994 hw_loss 0.378313 lr 0.00090324 rank 1
2023-02-17 07:53:19,188 DEBUG TRAIN Batch 3/5600 loss 17.249434 loss_att 24.513180 loss_ctc 33.980053 loss_rnnt 13.364251 hw_loss 0.378157 lr 0.00090413 rank 4
2023-02-17 07:53:19,194 DEBUG TRAIN Batch 3/5600 loss 20.551767 loss_att 25.904898 loss_ctc 32.127457 loss_rnnt 17.777021 hw_loss 0.301302 lr 0.00090363 rank 3
2023-02-17 07:53:19,196 DEBUG TRAIN Batch 3/5600 loss 28.159372 loss_att 34.323669 loss_ctc 46.399632 loss_rnnt 24.301737 hw_loss 0.361389 lr 0.00090335 rank 5
2023-02-17 07:53:19,197 DEBUG TRAIN Batch 3/5600 loss 39.097851 loss_att 44.614674 loss_ctc 61.280407 loss_rnnt 34.882324 hw_loss 0.289665 lr 0.00090301 rank 2
2023-02-17 07:53:19,202 DEBUG TRAIN Batch 3/5600 loss 38.210258 loss_att 44.580791 loss_ctc 63.211586 loss_rnnt 33.410679 hw_loss 0.359931 lr 0.00090352 rank 0
2023-02-17 07:54:52,743 DEBUG TRAIN Batch 3/5700 loss 33.821575 loss_att 43.711792 loss_ctc 45.653290 loss_rnnt 30.094696 hw_loss 0.321143 lr 0.00090154 rank 2
2023-02-17 07:54:52,744 DEBUG TRAIN Batch 3/5700 loss 19.321989 loss_att 22.934174 loss_ctc 28.801834 loss_rnnt 17.177034 hw_loss 0.297260 lr 0.00090205 rank 0
2023-02-17 07:54:52,744 DEBUG TRAIN Batch 3/5700 loss 23.624727 loss_att 22.581635 loss_ctc 31.242640 loss_rnnt 22.596022 hw_loss 0.415505 lr 0.00090265 rank 4
2023-02-17 07:54:52,750 DEBUG TRAIN Batch 3/5700 loss 15.761149 loss_att 25.212278 loss_ctc 29.146902 loss_rnnt 11.973311 hw_loss 0.211584 lr 0.00090286 rank 7
2023-02-17 07:54:52,771 DEBUG TRAIN Batch 3/5700 loss 25.724581 loss_att 29.971607 loss_ctc 38.174297 loss_rnnt 23.011511 hw_loss 0.381940 lr 0.00090215 rank 3
2023-02-17 07:54:52,775 DEBUG TRAIN Batch 3/5700 loss 25.180660 loss_att 31.087158 loss_ctc 39.871471 loss_rnnt 21.860558 hw_loss 0.337555 lr 0.00090187 rank 5
2023-02-17 07:54:52,776 DEBUG TRAIN Batch 3/5700 loss 23.609133 loss_att 24.639042 loss_ctc 30.604605 loss_rnnt 22.276516 hw_loss 0.363576 lr 0.00090246 rank 6
2023-02-17 07:54:52,811 DEBUG TRAIN Batch 3/5700 loss 20.804567 loss_att 22.060251 loss_ctc 34.158237 loss_rnnt 18.636219 hw_loss 0.256352 lr 0.00090177 rank 1
2023-02-17 07:56:08,791 DEBUG TRAIN Batch 3/5800 loss 18.558107 loss_att 26.381582 loss_ctc 30.726501 loss_rnnt 15.266737 hw_loss 0.195416 lr 0.00090100 rank 6
2023-02-17 07:56:08,801 DEBUG TRAIN Batch 3/5800 loss 14.505484 loss_att 14.776545 loss_ctc 17.314838 loss_rnnt 13.913782 hw_loss 0.305452 lr 0.00090041 rank 5
2023-02-17 07:56:08,804 DEBUG TRAIN Batch 3/5800 loss 39.089870 loss_att 49.226524 loss_ctc 62.749840 loss_rnnt 33.740021 hw_loss 0.314733 lr 0.00090069 rank 3
2023-02-17 07:56:08,809 DEBUG TRAIN Batch 3/5800 loss 35.700668 loss_att 39.214428 loss_ctc 54.900909 loss_rnnt 32.318314 hw_loss 0.224194 lr 0.00090008 rank 2
2023-02-17 07:56:08,809 DEBUG TRAIN Batch 3/5800 loss 45.396404 loss_att 56.627335 loss_ctc 64.951767 loss_rnnt 40.418919 hw_loss 0.232347 lr 0.00090059 rank 0
2023-02-17 07:56:08,811 DEBUG TRAIN Batch 3/5800 loss 17.105818 loss_att 26.339481 loss_ctc 40.338028 loss_rnnt 12.018261 hw_loss 0.268493 lr 0.00090031 rank 1
2023-02-17 07:56:08,810 DEBUG TRAIN Batch 3/5800 loss 33.863213 loss_att 36.975227 loss_ctc 46.556602 loss_rnnt 31.382824 hw_loss 0.310371 lr 0.00090139 rank 7
2023-02-17 07:56:08,834 DEBUG TRAIN Batch 3/5800 loss 20.019794 loss_att 27.351269 loss_ctc 37.727509 loss_rnnt 16.121199 hw_loss 0.133641 lr 0.00090119 rank 4
2023-02-17 07:57:24,086 DEBUG TRAIN Batch 3/5900 loss 18.804089 loss_att 25.157707 loss_ctc 30.087917 loss_rnnt 15.865901 hw_loss 0.305537 lr 0.00089862 rank 2
2023-02-17 07:57:24,087 DEBUG TRAIN Batch 3/5900 loss 20.183611 loss_att 28.871225 loss_ctc 40.716980 loss_rnnt 15.596985 hw_loss 0.208727 lr 0.00089954 rank 6
2023-02-17 07:57:24,087 DEBUG TRAIN Batch 3/5900 loss 14.939946 loss_att 18.202999 loss_ctc 24.027691 loss_rnnt 12.902667 hw_loss 0.324319 lr 0.00089973 rank 4
2023-02-17 07:57:24,092 DEBUG TRAIN Batch 3/5900 loss 17.082314 loss_att 24.674488 loss_ctc 31.590992 loss_rnnt 13.478808 hw_loss 0.282337 lr 0.00089885 rank 1
2023-02-17 07:57:24,092 DEBUG TRAIN Batch 3/5900 loss 33.074123 loss_att 44.850754 loss_ctc 55.659081 loss_rnnt 27.640102 hw_loss 0.126317 lr 0.00089895 rank 5
2023-02-17 07:57:24,095 DEBUG TRAIN Batch 3/5900 loss 20.115101 loss_att 23.255188 loss_ctc 29.401505 loss_rnnt 18.123573 hw_loss 0.234977 lr 0.00089993 rank 7
2023-02-17 07:57:24,115 DEBUG TRAIN Batch 3/5900 loss 18.880930 loss_att 27.991020 loss_ctc 35.727898 loss_rnnt 14.689514 hw_loss 0.230878 lr 0.00089923 rank 3
2023-02-17 07:57:24,150 DEBUG TRAIN Batch 3/5900 loss 20.940859 loss_att 27.556438 loss_ctc 32.193966 loss_rnnt 17.995411 hw_loss 0.228599 lr 0.00089913 rank 0
2023-02-17 07:58:47,039 DEBUG TRAIN Batch 3/6000 loss 41.281578 loss_att 43.934021 loss_ctc 59.053715 loss_rnnt 38.241051 hw_loss 0.263288 lr 0.00089848 rank 7
2023-02-17 07:58:47,043 DEBUG TRAIN Batch 3/6000 loss 45.790668 loss_att 50.968632 loss_ctc 62.353100 loss_rnnt 42.419064 hw_loss 0.239424 lr 0.00089740 rank 1
2023-02-17 07:58:47,074 DEBUG TRAIN Batch 3/6000 loss 18.620382 loss_att 26.235516 loss_ctc 29.559895 loss_rnnt 15.481524 hw_loss 0.294806 lr 0.00089808 rank 6
2023-02-17 07:58:47,085 DEBUG TRAIN Batch 3/6000 loss 21.882030 loss_att 33.902550 loss_ctc 44.621056 loss_rnnt 16.258972 hw_loss 0.350786 lr 0.00089751 rank 5
2023-02-17 07:58:47,092 DEBUG TRAIN Batch 3/6000 loss 35.949493 loss_att 40.568790 loss_ctc 57.129456 loss_rnnt 32.008308 hw_loss 0.362490 lr 0.00089778 rank 3
2023-02-17 07:58:47,104 DEBUG TRAIN Batch 3/6000 loss 49.601704 loss_att 49.685596 loss_ctc 68.936195 loss_rnnt 46.811596 hw_loss 0.366373 lr 0.00089717 rank 2
2023-02-17 07:58:47,107 DEBUG TRAIN Batch 3/6000 loss 27.904902 loss_att 35.653191 loss_ctc 42.314312 loss_rnnt 24.240719 hw_loss 0.362384 lr 0.00089768 rank 0
2023-02-17 07:58:47,126 DEBUG TRAIN Batch 3/6000 loss 14.244754 loss_att 18.479431 loss_ctc 24.881321 loss_rnnt 11.837260 hw_loss 0.266905 lr 0.00089827 rank 4
2023-02-17 08:00:13,118 DEBUG TRAIN Batch 3/6100 loss 18.067600 loss_att 27.153580 loss_ctc 31.275898 loss_rnnt 14.384745 hw_loss 0.196036 lr 0.00089624 rank 0
2023-02-17 08:00:13,120 DEBUG TRAIN Batch 3/6100 loss 28.290367 loss_att 36.660759 loss_ctc 48.303925 loss_rnnt 23.813429 hw_loss 0.251971 lr 0.00089596 rank 1
2023-02-17 08:00:13,121 DEBUG TRAIN Batch 3/6100 loss 13.326185 loss_att 23.381985 loss_ctc 29.500916 loss_rnnt 8.988755 hw_loss 0.318075 lr 0.00089634 rank 3
2023-02-17 08:00:13,123 DEBUG TRAIN Batch 3/6100 loss 46.508434 loss_att 46.902477 loss_ctc 64.194092 loss_rnnt 43.927864 hw_loss 0.269384 lr 0.00089573 rank 2
2023-02-17 08:00:13,125 DEBUG TRAIN Batch 3/6100 loss 19.134682 loss_att 29.332478 loss_ctc 27.157482 loss_rnnt 15.931402 hw_loss 0.176279 lr 0.00089683 rank 4
2023-02-17 08:00:13,129 DEBUG TRAIN Batch 3/6100 loss 25.668661 loss_att 34.214008 loss_ctc 42.604378 loss_rnnt 21.550230 hw_loss 0.283623 lr 0.00089606 rank 5
2023-02-17 08:00:13,133 DEBUG TRAIN Batch 3/6100 loss 23.526669 loss_att 26.224777 loss_ctc 40.695152 loss_rnnt 20.500793 hw_loss 0.369608 lr 0.00089664 rank 6
2023-02-17 08:00:13,148 DEBUG TRAIN Batch 3/6100 loss 28.143253 loss_att 32.294720 loss_ctc 43.095837 loss_rnnt 25.173107 hw_loss 0.274081 lr 0.00089703 rank 7
2023-02-17 08:01:28,650 DEBUG TRAIN Batch 3/6200 loss 26.469757 loss_att 30.953892 loss_ctc 44.915863 loss_rnnt 22.970999 hw_loss 0.267094 lr 0.00089520 rank 6
2023-02-17 08:01:28,663 DEBUG TRAIN Batch 3/6200 loss 19.615023 loss_att 29.348534 loss_ctc 31.117754 loss_rnnt 16.017712 hw_loss 0.219206 lr 0.00089453 rank 1
2023-02-17 08:01:28,667 DEBUG TRAIN Batch 3/6200 loss 29.914322 loss_att 38.046608 loss_ctc 46.520851 loss_rnnt 25.931044 hw_loss 0.267408 lr 0.00089539 rank 4
2023-02-17 08:01:28,668 DEBUG TRAIN Batch 3/6200 loss 31.770220 loss_att 36.976788 loss_ctc 42.722252 loss_rnnt 29.127180 hw_loss 0.265227 lr 0.00089559 rank 7
2023-02-17 08:01:28,670 DEBUG TRAIN Batch 3/6200 loss 28.404657 loss_att 39.314861 loss_ctc 47.212509 loss_rnnt 23.598145 hw_loss 0.218922 lr 0.00089490 rank 3
2023-02-17 08:01:28,675 DEBUG TRAIN Batch 3/6200 loss 22.928665 loss_att 28.225548 loss_ctc 35.695564 loss_rnnt 20.036221 hw_loss 0.245277 lr 0.00089480 rank 0
2023-02-17 08:01:28,701 DEBUG TRAIN Batch 3/6200 loss 31.875067 loss_att 37.978577 loss_ctc 43.168518 loss_rnnt 28.986444 hw_loss 0.303982 lr 0.00089430 rank 2
2023-02-17 08:01:28,721 DEBUG TRAIN Batch 3/6200 loss 13.465619 loss_att 18.375599 loss_ctc 22.861423 loss_rnnt 11.116831 hw_loss 0.213782 lr 0.00089463 rank 5
2023-02-17 08:02:44,663 DEBUG TRAIN Batch 3/6300 loss 31.907387 loss_att 34.549137 loss_ctc 45.346352 loss_rnnt 29.440765 hw_loss 0.274514 lr 0.00089377 rank 6
2023-02-17 08:02:44,667 DEBUG TRAIN Batch 3/6300 loss 27.088814 loss_att 31.064651 loss_ctc 38.726021 loss_rnnt 24.631966 hw_loss 0.206348 lr 0.00089396 rank 4
2023-02-17 08:02:44,680 DEBUG TRAIN Batch 3/6300 loss 23.770283 loss_att 27.322145 loss_ctc 40.426041 loss_rnnt 20.709171 hw_loss 0.243699 lr 0.00089310 rank 1
2023-02-17 08:02:44,683 DEBUG TRAIN Batch 3/6300 loss 12.389095 loss_att 18.504425 loss_ctc 19.821358 loss_rnnt 10.103363 hw_loss 0.134435 lr 0.00089287 rank 2
2023-02-17 08:02:44,684 DEBUG TRAIN Batch 3/6300 loss 38.949425 loss_att 40.475048 loss_ctc 47.516479 loss_rnnt 37.343822 hw_loss 0.296630 lr 0.00089347 rank 3
2023-02-17 08:02:44,686 DEBUG TRAIN Batch 3/6300 loss 39.412823 loss_att 46.768658 loss_ctc 60.976959 loss_rnnt 34.954861 hw_loss 0.209207 lr 0.00089320 rank 5
2023-02-17 08:02:44,711 DEBUG TRAIN Batch 3/6300 loss 19.068691 loss_att 21.498663 loss_ctc 27.453598 loss_rnnt 17.263329 hw_loss 0.377589 lr 0.00089416 rank 7
2023-02-17 08:02:44,718 DEBUG TRAIN Batch 3/6300 loss 31.329695 loss_att 38.396023 loss_ctc 45.829113 loss_rnnt 27.785383 hw_loss 0.370862 lr 0.00089337 rank 0
2023-02-17 08:04:19,732 DEBUG TRAIN Batch 3/6400 loss 47.555084 loss_att 54.016689 loss_ctc 66.128578 loss_rnnt 43.665367 hw_loss 0.226747 lr 0.00089273 rank 7
2023-02-17 08:04:19,735 DEBUG TRAIN Batch 3/6400 loss 23.138367 loss_att 32.620815 loss_ctc 45.080353 loss_rnnt 18.228016 hw_loss 0.165491 lr 0.00089145 rank 2
2023-02-17 08:04:19,736 DEBUG TRAIN Batch 3/6400 loss 34.409954 loss_att 35.629238 loss_ctc 44.368301 loss_rnnt 32.665909 hw_loss 0.323263 lr 0.00089195 rank 0
2023-02-17 08:04:19,737 DEBUG TRAIN Batch 3/6400 loss 34.128170 loss_att 38.369865 loss_ctc 49.781506 loss_rnnt 31.009607 hw_loss 0.343333 lr 0.00089178 rank 5
2023-02-17 08:04:19,743 DEBUG TRAIN Batch 3/6400 loss 14.276275 loss_att 15.327898 loss_ctc 20.234724 loss_rnnt 13.045313 hw_loss 0.424081 lr 0.00089253 rank 4
2023-02-17 08:04:19,753 DEBUG TRAIN Batch 3/6400 loss 28.205160 loss_att 32.466698 loss_ctc 44.005230 loss_rnnt 25.123947 hw_loss 0.229179 lr 0.00089168 rank 1
2023-02-17 08:04:19,755 DEBUG TRAIN Batch 3/6400 loss 31.585777 loss_att 42.540714 loss_ctc 49.894085 loss_rnnt 26.796936 hw_loss 0.293901 lr 0.00089205 rank 3
2023-02-17 08:04:19,767 DEBUG TRAIN Batch 3/6400 loss 45.988823 loss_att 52.837219 loss_ctc 63.073803 loss_rnnt 42.227760 hw_loss 0.212604 lr 0.00089235 rank 6
2023-02-17 08:05:35,439 DEBUG TRAIN Batch 3/6500 loss 18.363441 loss_att 28.206100 loss_ctc 26.929794 loss_rnnt 15.130717 hw_loss 0.228773 lr 0.00089053 rank 0
2023-02-17 08:05:35,440 DEBUG TRAIN Batch 3/6500 loss 16.420572 loss_att 22.841923 loss_ctc 22.131430 loss_rnnt 14.278706 hw_loss 0.180278 lr 0.00089093 rank 6
2023-02-17 08:05:35,440 DEBUG TRAIN Batch 3/6500 loss 30.880045 loss_att 35.485603 loss_ctc 51.335125 loss_rnnt 27.113766 hw_loss 0.220922 lr 0.00089026 rank 1
2023-02-17 08:05:35,447 DEBUG TRAIN Batch 3/6500 loss 24.231901 loss_att 30.140553 loss_ctc 41.171112 loss_rnnt 20.598698 hw_loss 0.361704 lr 0.00089004 rank 2
2023-02-17 08:05:35,447 DEBUG TRAIN Batch 3/6500 loss 27.087465 loss_att 43.396893 loss_ctc 35.167297 loss_rnnt 22.548218 hw_loss 0.375095 lr 0.00089111 rank 4
2023-02-17 08:05:35,457 DEBUG TRAIN Batch 3/6500 loss 24.819958 loss_att 33.181358 loss_ctc 39.871838 loss_rnnt 20.916088 hw_loss 0.421261 lr 0.00089131 rank 7
2023-02-17 08:05:35,466 DEBUG TRAIN Batch 3/6500 loss 52.126614 loss_att 66.292122 loss_ctc 82.107651 loss_rnnt 45.199898 hw_loss 0.180258 lr 0.00089036 rank 5
2023-02-17 08:05:35,467 DEBUG TRAIN Batch 3/6500 loss 28.005167 loss_att 34.319633 loss_ctc 46.051208 loss_rnnt 24.193197 hw_loss 0.268008 lr 0.00089063 rank 3
2023-02-17 08:06:51,293 DEBUG TRAIN Batch 3/6600 loss 30.875582 loss_att 31.255247 loss_ctc 48.985138 loss_rnnt 28.259466 hw_loss 0.235453 lr 0.00088912 rank 0
2023-02-17 08:06:51,299 DEBUG TRAIN Batch 3/6600 loss 26.849451 loss_att 34.694374 loss_ctc 41.289795 loss_rnnt 23.212719 hw_loss 0.266945 lr 0.00088952 rank 6
2023-02-17 08:06:51,299 DEBUG TRAIN Batch 3/6600 loss 17.003420 loss_att 21.419224 loss_ctc 27.118847 loss_rnnt 14.609350 hw_loss 0.304100 lr 0.00088970 rank 4
2023-02-17 08:06:51,299 DEBUG TRAIN Batch 3/6600 loss 44.091320 loss_att 51.334408 loss_ctc 64.461639 loss_rnnt 39.831749 hw_loss 0.177960 lr 0.00088886 rank 1
2023-02-17 08:06:51,300 DEBUG TRAIN Batch 3/6600 loss 8.381158 loss_att 14.615728 loss_ctc 14.345195 loss_rnnt 6.227732 hw_loss 0.208699 lr 0.00088922 rank 3
2023-02-17 08:06:51,304 DEBUG TRAIN Batch 3/6600 loss 33.502068 loss_att 42.994625 loss_ctc 53.762585 loss_rnnt 28.763882 hw_loss 0.259262 lr 0.00088863 rank 2
2023-02-17 08:06:51,304 DEBUG TRAIN Batch 3/6600 loss 24.340570 loss_att 28.724905 loss_ctc 39.242905 loss_rnnt 21.325428 hw_loss 0.283686 lr 0.00088895 rank 5
2023-02-17 08:06:51,338 DEBUG TRAIN Batch 3/6600 loss 18.863148 loss_att 25.680153 loss_ctc 30.350206 loss_rnnt 15.785864 hw_loss 0.341767 lr 0.00088990 rank 7
2023-02-17 08:08:11,770 DEBUG TRAIN Batch 3/6700 loss 25.371275 loss_att 30.365767 loss_ctc 36.586456 loss_rnnt 22.772316 hw_loss 0.196317 lr 0.00088811 rank 6
2023-02-17 08:08:11,771 DEBUG TRAIN Batch 3/6700 loss 20.231577 loss_att 26.402309 loss_ctc 35.311722 loss_rnnt 16.841084 hw_loss 0.273115 lr 0.00088829 rank 4
2023-02-17 08:08:11,773 DEBUG TRAIN Batch 3/6700 loss 26.527493 loss_att 33.400558 loss_ctc 51.182190 loss_rnnt 21.701454 hw_loss 0.307744 lr 0.00088723 rank 2
2023-02-17 08:08:11,776 DEBUG TRAIN Batch 3/6700 loss 10.465860 loss_att 15.937571 loss_ctc 19.355942 loss_rnnt 8.071842 hw_loss 0.214371 lr 0.00088782 rank 3
2023-02-17 08:08:11,780 DEBUG TRAIN Batch 3/6700 loss 25.281471 loss_att 34.699623 loss_ctc 36.201046 loss_rnnt 21.776360 hw_loss 0.310384 lr 0.00088755 rank 5
2023-02-17 08:08:11,795 DEBUG TRAIN Batch 3/6700 loss 27.350454 loss_att 31.196280 loss_ctc 51.102863 loss_rnnt 23.298349 hw_loss 0.217407 lr 0.00088849 rank 7
2023-02-17 08:08:11,797 DEBUG TRAIN Batch 3/6700 loss 34.434994 loss_att 43.789330 loss_ctc 58.890991 loss_rnnt 29.175354 hw_loss 0.239948 lr 0.00088745 rank 1
2023-02-17 08:08:11,809 DEBUG TRAIN Batch 3/6700 loss 25.138695 loss_att 25.562799 loss_ctc 37.713272 loss_rnnt 23.197573 hw_loss 0.336917 lr 0.00088772 rank 0
2023-02-17 08:09:41,229 DEBUG TRAIN Batch 3/6800 loss 24.670891 loss_att 30.066814 loss_ctc 44.312462 loss_rnnt 20.803507 hw_loss 0.317477 lr 0.00088632 rank 0
2023-02-17 08:09:41,230 DEBUG TRAIN Batch 3/6800 loss 46.457561 loss_att 52.766563 loss_ctc 67.012619 loss_rnnt 42.269089 hw_loss 0.348736 lr 0.00088616 rank 5
2023-02-17 08:09:41,232 DEBUG TRAIN Batch 3/6800 loss 20.308691 loss_att 24.066105 loss_ctc 32.564835 loss_rnnt 17.754780 hw_loss 0.315514 lr 0.00088690 rank 4
2023-02-17 08:09:41,233 DEBUG TRAIN Batch 3/6800 loss 14.990733 loss_att 25.493971 loss_ctc 20.967583 loss_rnnt 11.953423 hw_loss 0.262033 lr 0.00088606 rank 1
2023-02-17 08:09:41,233 DEBUG TRAIN Batch 3/6800 loss 27.812906 loss_att 37.235519 loss_ctc 44.436111 loss_rnnt 23.559731 hw_loss 0.285419 lr 0.00088642 rank 3
2023-02-17 08:09:41,234 DEBUG TRAIN Batch 3/6800 loss 22.150208 loss_att 32.004982 loss_ctc 32.058357 loss_rnnt 18.726923 hw_loss 0.246083 lr 0.00088584 rank 2
2023-02-17 08:09:41,245 DEBUG TRAIN Batch 3/6800 loss 37.549091 loss_att 44.811741 loss_ctc 59.464096 loss_rnnt 33.042286 hw_loss 0.248011 lr 0.00088671 rank 6
2023-02-17 08:09:41,252 DEBUG TRAIN Batch 3/6800 loss 28.299009 loss_att 32.080067 loss_ctc 44.470894 loss_rnnt 25.201429 hw_loss 0.347093 lr 0.00088709 rank 7
2023-02-17 08:10:58,294 DEBUG TRAIN Batch 3/6900 loss 44.740196 loss_att 45.602085 loss_ctc 53.446693 loss_rnnt 43.279530 hw_loss 0.238914 lr 0.00088570 rank 7
2023-02-17 08:10:58,294 DEBUG TRAIN Batch 3/6900 loss 29.491043 loss_att 38.024265 loss_ctc 49.128033 loss_rnnt 25.027737 hw_loss 0.259496 lr 0.00088493 rank 0
2023-02-17 08:10:58,294 DEBUG TRAIN Batch 3/6900 loss 37.190491 loss_att 38.180550 loss_ctc 55.252758 loss_rnnt 34.440407 hw_loss 0.269566 lr 0.00088445 rank 2
2023-02-17 08:10:58,295 DEBUG TRAIN Batch 3/6900 loss 26.410393 loss_att 29.683807 loss_ctc 39.051167 loss_rnnt 23.944181 hw_loss 0.236423 lr 0.00088503 rank 3
2023-02-17 08:10:58,297 DEBUG TRAIN Batch 3/6900 loss 29.715895 loss_att 35.823971 loss_ctc 51.453682 loss_rnnt 25.419634 hw_loss 0.330511 lr 0.00088477 rank 5
2023-02-17 08:10:58,298 DEBUG TRAIN Batch 3/6900 loss 22.632502 loss_att 26.850567 loss_ctc 34.536434 loss_rnnt 20.036732 hw_loss 0.309310 lr 0.00088532 rank 6
2023-02-17 08:10:58,301 DEBUG TRAIN Batch 3/6900 loss 23.390314 loss_att 25.547606 loss_ctc 41.957996 loss_rnnt 20.385508 hw_loss 0.183098 lr 0.00088467 rank 1
2023-02-17 08:10:58,302 DEBUG TRAIN Batch 3/6900 loss 33.355076 loss_att 38.104790 loss_ctc 48.365864 loss_rnnt 30.254713 hw_loss 0.279338 lr 0.00088550 rank 4
2023-02-17 08:12:14,344 DEBUG TRAIN Batch 3/7000 loss 29.370909 loss_att 28.477169 loss_ctc 42.099422 loss_rnnt 27.653824 hw_loss 0.372560 lr 0.00088394 rank 6
2023-02-17 08:12:14,352 DEBUG TRAIN Batch 3/7000 loss 20.082472 loss_att 20.207140 loss_ctc 27.923222 loss_rnnt 18.816942 hw_loss 0.365930 lr 0.00088412 rank 4
2023-02-17 08:12:14,354 DEBUG TRAIN Batch 3/7000 loss 23.870771 loss_att 27.240795 loss_ctc 35.273510 loss_rnnt 21.534296 hw_loss 0.266440 lr 0.00088329 rank 1
2023-02-17 08:12:14,354 DEBUG TRAIN Batch 3/7000 loss 14.679751 loss_att 16.444311 loss_ctc 20.849363 loss_rnnt 13.333334 hw_loss 0.320424 lr 0.00088365 rank 3
2023-02-17 08:12:14,373 DEBUG TRAIN Batch 3/7000 loss 57.938541 loss_att 59.965263 loss_ctc 70.057465 loss_rnnt 55.781036 hw_loss 0.255571 lr 0.00088307 rank 2
2023-02-17 08:12:14,378 DEBUG TRAIN Batch 3/7000 loss 22.031012 loss_att 23.615555 loss_ctc 28.811514 loss_rnnt 20.685341 hw_loss 0.233804 lr 0.00088339 rank 5
2023-02-17 08:12:14,381 DEBUG TRAIN Batch 3/7000 loss 26.572964 loss_att 39.956936 loss_ctc 38.733837 loss_rnnt 22.113457 hw_loss 0.302362 lr 0.00088431 rank 7
2023-02-17 08:12:14,407 DEBUG TRAIN Batch 3/7000 loss 24.439087 loss_att 29.161533 loss_ctc 39.421902 loss_rnnt 21.346283 hw_loss 0.282381 lr 0.00088355 rank 0
2023-02-17 08:13:45,706 DEBUG TRAIN Batch 3/7100 loss 41.329052 loss_att 43.854870 loss_ctc 59.036850 loss_rnnt 38.380630 hw_loss 0.154147 lr 0.00088274 rank 4
2023-02-17 08:13:45,706 DEBUG TRAIN Batch 3/7100 loss 21.462988 loss_att 25.412743 loss_ctc 30.057199 loss_rnnt 19.341253 hw_loss 0.348544 lr 0.00088192 rank 1
2023-02-17 08:13:45,707 DEBUG TRAIN Batch 3/7100 loss 18.029127 loss_att 34.078186 loss_ctc 32.494820 loss_rnnt 12.737548 hw_loss 0.286888 lr 0.00088218 rank 0
2023-02-17 08:13:45,715 DEBUG TRAIN Batch 3/7100 loss 23.923828 loss_att 31.122103 loss_ctc 35.209007 loss_rnnt 20.873112 hw_loss 0.199442 lr 0.00088293 rank 7
2023-02-17 08:13:45,730 DEBUG TRAIN Batch 3/7100 loss 29.732414 loss_att 32.900711 loss_ctc 42.851372 loss_rnnt 27.231926 hw_loss 0.220567 lr 0.00088227 rank 3
2023-02-17 08:13:45,739 DEBUG TRAIN Batch 3/7100 loss 32.844391 loss_att 40.373070 loss_ctc 54.024334 loss_rnnt 28.391197 hw_loss 0.231490 lr 0.00088256 rank 6
2023-02-17 08:13:45,752 DEBUG TRAIN Batch 3/7100 loss 34.356728 loss_att 37.829529 loss_ctc 48.394081 loss_rnnt 31.614773 hw_loss 0.329519 lr 0.00088170 rank 2
2023-02-17 08:13:45,772 DEBUG TRAIN Batch 3/7100 loss 13.956746 loss_att 14.345478 loss_ctc 19.122440 loss_rnnt 12.971700 hw_loss 0.409763 lr 0.00088201 rank 5
2023-02-17 08:15:05,421 DEBUG TRAIN Batch 3/7200 loss 31.374773 loss_att 36.381256 loss_ctc 45.583252 loss_rnnt 28.328787 hw_loss 0.281674 lr 0.00088119 rank 6
2023-02-17 08:15:05,426 DEBUG TRAIN Batch 3/7200 loss 27.090322 loss_att 32.392605 loss_ctc 45.157513 loss_rnnt 23.459959 hw_loss 0.301780 lr 0.00088055 rank 1
2023-02-17 08:15:05,426 DEBUG TRAIN Batch 3/7200 loss 22.414198 loss_att 31.311779 loss_ctc 34.152462 loss_rnnt 18.981833 hw_loss 0.164525 lr 0.00088033 rank 2
2023-02-17 08:15:05,426 DEBUG TRAIN Batch 3/7200 loss 30.771677 loss_att 34.374969 loss_ctc 49.369644 loss_rnnt 27.476788 hw_loss 0.177194 lr 0.00088156 rank 7
2023-02-17 08:15:05,427 DEBUG TRAIN Batch 3/7200 loss 20.788929 loss_att 32.418530 loss_ctc 40.914307 loss_rnnt 15.686995 hw_loss 0.173685 lr 0.00088137 rank 4
2023-02-17 08:15:05,428 DEBUG TRAIN Batch 3/7200 loss 30.028809 loss_att 33.948727 loss_ctc 50.420376 loss_rnnt 26.420357 hw_loss 0.197985 lr 0.00088081 rank 0
2023-02-17 08:15:05,454 DEBUG TRAIN Batch 3/7200 loss 30.578991 loss_att 39.834564 loss_ctc 52.678341 loss_rnnt 25.676613 hw_loss 0.196281 lr 0.00088090 rank 3
2023-02-17 08:15:05,461 DEBUG TRAIN Batch 3/7200 loss 27.879057 loss_att 32.180115 loss_ctc 38.583675 loss_rnnt 25.455801 hw_loss 0.254554 lr 0.00088064 rank 5
2023-02-17 08:16:20,807 DEBUG TRAIN Batch 3/7300 loss 54.342541 loss_att 53.058788 loss_ctc 77.580750 loss_rnnt 51.325607 hw_loss 0.328604 lr 0.00087982 rank 6
2023-02-17 08:16:20,807 DEBUG TRAIN Batch 3/7300 loss 32.134106 loss_att 38.707916 loss_ctc 40.847328 loss_rnnt 29.448883 hw_loss 0.391307 lr 0.00087928 rank 5
2023-02-17 08:16:20,807 DEBUG TRAIN Batch 3/7300 loss 10.430389 loss_att 19.602781 loss_ctc 21.845654 loss_rnnt 6.926057 hw_loss 0.277159 lr 0.00088000 rank 4
2023-02-17 08:16:20,808 DEBUG TRAIN Batch 3/7300 loss 25.900772 loss_att 32.115192 loss_ctc 41.415054 loss_rnnt 22.471409 hw_loss 0.221070 lr 0.00088019 rank 7
2023-02-17 08:16:20,813 DEBUG TRAIN Batch 3/7300 loss 23.132336 loss_att 26.271877 loss_ctc 35.659023 loss_rnnt 20.677826 hw_loss 0.293200 lr 0.00087944 rank 0
2023-02-17 08:16:20,856 DEBUG TRAIN Batch 3/7300 loss 39.325916 loss_att 43.916756 loss_ctc 57.828625 loss_rnnt 35.797546 hw_loss 0.268454 lr 0.00087918 rank 1
2023-02-17 08:16:20,856 DEBUG TRAIN Batch 3/7300 loss 25.947678 loss_att 33.399937 loss_ctc 43.415260 loss_rnnt 21.989067 hw_loss 0.260902 lr 0.00087954 rank 3
2023-02-17 08:16:20,862 DEBUG TRAIN Batch 3/7300 loss 35.539364 loss_att 47.027084 loss_ctc 61.537254 loss_rnnt 29.592468 hw_loss 0.343066 lr 0.00087897 rank 2
2023-02-17 08:17:39,511 DEBUG TRAIN Batch 3/7400 loss 21.061007 loss_att 28.365744 loss_ctc 35.218613 loss_rnnt 17.564049 hw_loss 0.278117 lr 0.00087783 rank 1
2023-02-17 08:17:39,527 DEBUG TRAIN Batch 3/7400 loss 10.623571 loss_att 17.201124 loss_ctc 19.420080 loss_rnnt 8.069779 hw_loss 0.122651 lr 0.00087792 rank 5
2023-02-17 08:17:39,562 DEBUG TRAIN Batch 3/7400 loss 21.817863 loss_att 24.075087 loss_ctc 30.235256 loss_rnnt 20.114792 hw_loss 0.242453 lr 0.00087761 rank 2
2023-02-17 08:17:39,572 DEBUG TRAIN Batch 3/7400 loss 24.390665 loss_att 30.386089 loss_ctc 37.872974 loss_rnnt 21.228062 hw_loss 0.311015 lr 0.00087818 rank 3
2023-02-17 08:17:39,571 DEBUG TRAIN Batch 3/7400 loss 27.634382 loss_att 37.690670 loss_ctc 45.064114 loss_rnnt 23.151197 hw_loss 0.277431 lr 0.00087864 rank 4
2023-02-17 08:17:39,595 DEBUG TRAIN Batch 3/7400 loss 48.146839 loss_att 50.611835 loss_ctc 71.993439 loss_rnnt 44.284664 hw_loss 0.355558 lr 0.00087846 rank 6
2023-02-17 08:17:39,598 DEBUG TRAIN Batch 3/7400 loss 19.125587 loss_att 27.368633 loss_ctc 33.888824 loss_rnnt 15.356956 hw_loss 0.284234 lr 0.00087883 rank 7
2023-02-17 08:17:39,599 DEBUG TRAIN Batch 3/7400 loss 30.232655 loss_att 38.917999 loss_ctc 51.068001 loss_rnnt 25.569429 hw_loss 0.277705 lr 0.00087809 rank 0
2023-02-17 08:19:10,462 DEBUG TRAIN Batch 3/7500 loss 21.196499 loss_att 23.688242 loss_ctc 25.804865 loss_rnnt 19.926128 hw_loss 0.295450 lr 0.00087711 rank 6
2023-02-17 08:19:10,463 DEBUG TRAIN Batch 3/7500 loss 31.595024 loss_att 36.300705 loss_ctc 48.251797 loss_rnnt 28.273243 hw_loss 0.299512 lr 0.00087626 rank 2
2023-02-17 08:19:10,465 DEBUG TRAIN Batch 3/7500 loss 32.220913 loss_att 32.827370 loss_ctc 44.886475 loss_rnnt 30.279903 hw_loss 0.245585 lr 0.00087648 rank 1
2023-02-17 08:19:10,468 DEBUG TRAIN Batch 3/7500 loss 29.002529 loss_att 32.392078 loss_ctc 41.488731 loss_rnnt 26.496845 hw_loss 0.305522 lr 0.00087748 rank 7
2023-02-17 08:19:10,469 DEBUG TRAIN Batch 3/7500 loss 29.547234 loss_att 35.403721 loss_ctc 42.945457 loss_rnnt 26.432697 hw_loss 0.294015 lr 0.00087673 rank 0
2023-02-17 08:19:10,469 DEBUG TRAIN Batch 3/7500 loss 29.916723 loss_att 37.982597 loss_ctc 44.116859 loss_rnnt 26.300316 hw_loss 0.206025 lr 0.00087657 rank 5
2023-02-17 08:19:10,472 DEBUG TRAIN Batch 3/7500 loss 25.617968 loss_att 31.331627 loss_ctc 37.613007 loss_rnnt 22.720802 hw_loss 0.290801 lr 0.00087729 rank 4
2023-02-17 08:19:10,511 DEBUG TRAIN Batch 3/7500 loss 18.196638 loss_att 20.681662 loss_ctc 29.306494 loss_rnnt 16.075571 hw_loss 0.267650 lr 0.00087683 rank 3
2023-02-17 08:20:27,299 DEBUG TRAIN Batch 3/7600 loss 15.659469 loss_att 16.059319 loss_ctc 24.237619 loss_rnnt 14.289478 hw_loss 0.274250 lr 0.00087492 rank 2
2023-02-17 08:20:27,299 DEBUG TRAIN Batch 3/7600 loss 13.759552 loss_att 18.266005 loss_ctc 24.190762 loss_rnnt 11.368128 hw_loss 0.186198 lr 0.00087523 rank 5
2023-02-17 08:20:27,301 DEBUG TRAIN Batch 3/7600 loss 26.153273 loss_att 25.778574 loss_ctc 35.545963 loss_rnnt 24.810984 hw_loss 0.309130 lr 0.00087577 rank 6
2023-02-17 08:20:27,302 DEBUG TRAIN Batch 3/7600 loss 18.967203 loss_att 17.703815 loss_ctc 25.873545 loss_rnnt 18.102406 hw_loss 0.368683 lr 0.00087613 rank 7
2023-02-17 08:20:27,302 DEBUG TRAIN Batch 3/7600 loss 13.355660 loss_att 17.180607 loss_ctc 22.936890 loss_rnnt 11.182839 hw_loss 0.244376 lr 0.00087513 rank 1
2023-02-17 08:20:27,306 DEBUG TRAIN Batch 3/7600 loss 23.738176 loss_att 29.032768 loss_ctc 36.788849 loss_rnnt 20.774984 hw_loss 0.307846 lr 0.00087539 rank 0
2023-02-17 08:20:27,305 DEBUG TRAIN Batch 3/7600 loss 19.746798 loss_att 23.163704 loss_ctc 28.444237 loss_rnnt 17.735336 hw_loss 0.315790 lr 0.00087594 rank 4
2023-02-17 08:20:27,351 DEBUG TRAIN Batch 3/7600 loss 12.395555 loss_att 13.833653 loss_ctc 17.169815 loss_rnnt 11.359559 hw_loss 0.209638 lr 0.00087548 rank 3
2023-02-17 08:21:42,997 DEBUG TRAIN Batch 3/7700 loss 30.959612 loss_att 45.158718 loss_ctc 48.466881 loss_rnnt 25.639250 hw_loss 0.274197 lr 0.00087443 rank 6
2023-02-17 08:21:43,004 DEBUG TRAIN Batch 3/7700 loss 28.287764 loss_att 39.092480 loss_ctc 41.542114 loss_rnnt 24.275131 hw_loss 0.158330 lr 0.00087405 rank 0
2023-02-17 08:21:43,016 DEBUG TRAIN Batch 3/7700 loss 71.575348 loss_att 72.043320 loss_ctc 98.803612 loss_rnnt 67.688683 hw_loss 0.304940 lr 0.00087414 rank 3
2023-02-17 08:21:43,019 DEBUG TRAIN Batch 3/7700 loss 31.067591 loss_att 39.060787 loss_ctc 53.019463 loss_rnnt 26.426977 hw_loss 0.215737 lr 0.00087460 rank 4
2023-02-17 08:21:43,037 DEBUG TRAIN Batch 3/7700 loss 18.775042 loss_att 19.336636 loss_ctc 27.400124 loss_rnnt 17.377850 hw_loss 0.252865 lr 0.00087389 rank 5
2023-02-17 08:21:43,062 DEBUG TRAIN Batch 3/7700 loss 25.044743 loss_att 33.870041 loss_ctc 36.619820 loss_rnnt 21.604954 hw_loss 0.246343 lr 0.00087479 rank 7
2023-02-17 08:21:43,062 DEBUG TRAIN Batch 3/7700 loss 28.975565 loss_att 35.001312 loss_ctc 40.821499 loss_rnnt 26.069641 hw_loss 0.227470 lr 0.00087358 rank 2
2023-02-17 08:21:43,063 DEBUG TRAIN Batch 3/7700 loss 23.347460 loss_att 28.177837 loss_ctc 38.662621 loss_rnnt 20.200996 hw_loss 0.259433 lr 0.00087380 rank 1
2023-02-17 08:23:06,870 DEBUG TRAIN Batch 3/7800 loss 26.684090 loss_att 32.135521 loss_ctc 45.539978 loss_rnnt 22.954422 hw_loss 0.234870 lr 0.00087272 rank 0
2023-02-17 08:23:06,876 DEBUG TRAIN Batch 3/7800 loss 38.009510 loss_att 46.251015 loss_ctc 60.156792 loss_rnnt 33.185101 hw_loss 0.418389 lr 0.00087281 rank 3
2023-02-17 08:23:06,904 DEBUG TRAIN Batch 3/7800 loss 41.768040 loss_att 51.099472 loss_ctc 69.891624 loss_rnnt 36.061741 hw_loss 0.169126 lr 0.00087247 rank 1
2023-02-17 08:23:06,906 DEBUG TRAIN Batch 3/7800 loss 16.377029 loss_att 22.947502 loss_ctc 26.629570 loss_rnnt 13.558052 hw_loss 0.258519 lr 0.00087225 rank 2
2023-02-17 08:23:06,911 DEBUG TRAIN Batch 3/7800 loss 37.776047 loss_att 57.055267 loss_ctc 61.554810 loss_rnnt 30.637426 hw_loss 0.210511 lr 0.00087256 rank 5
2023-02-17 08:23:06,911 DEBUG TRAIN Batch 3/7800 loss 23.411451 loss_att 29.789259 loss_ctc 36.166828 loss_rnnt 20.315887 hw_loss 0.223658 lr 0.00087309 rank 6
2023-02-17 08:23:06,939 DEBUG TRAIN Batch 3/7800 loss 29.654566 loss_att 37.660294 loss_ctc 52.047981 loss_rnnt 24.918516 hw_loss 0.279591 lr 0.00087345 rank 7
2023-02-17 08:23:06,939 DEBUG TRAIN Batch 3/7800 loss 61.237110 loss_att 67.624283 loss_ctc 86.315353 loss_rnnt 56.452545 hw_loss 0.306314 lr 0.00087326 rank 4
2023-02-17 08:24:32,967 DEBUG TRAIN Batch 3/7900 loss 14.050890 loss_att 19.824322 loss_ctc 18.792744 loss_rnnt 12.093832 hw_loss 0.318984 lr 0.00087123 rank 5
2023-02-17 08:24:32,967 DEBUG TRAIN Batch 3/7900 loss 20.753260 loss_att 28.712185 loss_ctc 33.214565 loss_rnnt 17.335707 hw_loss 0.307991 lr 0.00087093 rank 2
2023-02-17 08:24:32,969 DEBUG TRAIN Batch 3/7900 loss 11.473820 loss_att 16.153948 loss_ctc 22.438305 loss_rnnt 8.918677 hw_loss 0.294724 lr 0.00087148 rank 3
2023-02-17 08:24:32,972 DEBUG TRAIN Batch 3/7900 loss 35.164192 loss_att 48.716263 loss_ctc 56.160992 loss_rnnt 29.548006 hw_loss 0.199123 lr 0.00087139 rank 0
2023-02-17 08:24:32,972 DEBUG TRAIN Batch 3/7900 loss 25.502781 loss_att 35.015549 loss_ctc 49.830288 loss_rnnt 20.203032 hw_loss 0.287868 lr 0.00087194 rank 4
2023-02-17 08:24:32,996 DEBUG TRAIN Batch 3/7900 loss 40.515575 loss_att 47.159859 loss_ctc 59.872623 loss_rnnt 36.502724 hw_loss 0.193238 lr 0.00087212 rank 7
2023-02-17 08:24:32,996 DEBUG TRAIN Batch 3/7900 loss 26.857788 loss_att 34.944553 loss_ctc 54.389050 loss_rnnt 21.443161 hw_loss 0.237072 lr 0.00087176 rank 6
2023-02-17 08:24:33,007 DEBUG TRAIN Batch 3/7900 loss 36.339375 loss_att 42.595398 loss_ctc 60.562195 loss_rnnt 31.729803 hw_loss 0.241233 lr 0.00087114 rank 1
2023-02-17 08:25:48,984 DEBUG TRAIN Batch 3/8000 loss 13.084090 loss_att 22.391766 loss_ctc 27.039562 loss_rnnt 9.188900 hw_loss 0.324236 lr 0.00087044 rank 6
2023-02-17 08:25:48,985 DEBUG TRAIN Batch 3/8000 loss 33.372185 loss_att 43.297573 loss_ctc 54.119194 loss_rnnt 28.468321 hw_loss 0.285973 lr 0.00087016 rank 3
2023-02-17 08:25:48,987 DEBUG TRAIN Batch 3/8000 loss 15.121616 loss_att 21.788021 loss_ctc 28.189762 loss_rnnt 11.946901 hw_loss 0.185653 lr 0.00086961 rank 2
2023-02-17 08:25:48,990 DEBUG TRAIN Batch 3/8000 loss 19.138147 loss_att 24.018467 loss_ctc 30.707447 loss_rnnt 16.451960 hw_loss 0.314153 lr 0.00087007 rank 0
2023-02-17 08:25:48,993 DEBUG TRAIN Batch 3/8000 loss 14.217228 loss_att 21.037928 loss_ctc 25.784687 loss_rnnt 11.202064 hw_loss 0.203805 lr 0.00087061 rank 4
2023-02-17 08:25:48,994 DEBUG TRAIN Batch 3/8000 loss 32.253559 loss_att 43.363239 loss_ctc 54.978439 loss_rnnt 26.885220 hw_loss 0.218288 lr 0.00086991 rank 5
2023-02-17 08:25:49,007 DEBUG TRAIN Batch 3/8000 loss 33.417519 loss_att 38.910118 loss_ctc 54.288857 loss_rnnt 29.461229 hw_loss 0.140473 lr 0.00087080 rank 7
2023-02-17 08:25:49,007 DEBUG TRAIN Batch 3/8000 loss 43.676399 loss_att 57.051651 loss_ctc 61.402603 loss_rnnt 38.431412 hw_loss 0.387085 lr 0.00086982 rank 1
2023-02-17 08:27:06,469 DEBUG TRAIN Batch 3/8100 loss 31.896006 loss_att 35.773178 loss_ctc 49.106956 loss_rnnt 28.678972 hw_loss 0.275261 lr 0.00086913 rank 6
2023-02-17 08:27:06,470 DEBUG TRAIN Batch 3/8100 loss 32.163929 loss_att 37.860882 loss_ctc 49.682636 loss_rnnt 28.548634 hw_loss 0.262639 lr 0.00086885 rank 3
2023-02-17 08:27:06,484 DEBUG TRAIN Batch 3/8100 loss 25.075186 loss_att 32.275917 loss_ctc 43.678528 loss_rnnt 21.021221 hw_loss 0.250077 lr 0.00086948 rank 7
2023-02-17 08:27:06,494 DEBUG TRAIN Batch 3/8100 loss 29.092184 loss_att 31.500706 loss_ctc 40.497917 loss_rnnt 26.973995 hw_loss 0.216975 lr 0.00086851 rank 1
2023-02-17 08:27:06,497 DEBUG TRAIN Batch 3/8100 loss 25.473129 loss_att 32.036781 loss_ctc 38.265450 loss_rnnt 22.370529 hw_loss 0.157926 lr 0.00086860 rank 5
2023-02-17 08:27:06,507 DEBUG TRAIN Batch 3/8100 loss 20.694763 loss_att 26.262493 loss_ctc 32.620094 loss_rnnt 17.849516 hw_loss 0.265610 lr 0.00086830 rank 2
2023-02-17 08:27:06,581 DEBUG TRAIN Batch 3/8100 loss 14.941539 loss_att 21.768309 loss_ctc 25.769306 loss_rnnt 11.962780 hw_loss 0.318190 lr 0.00086930 rank 4
2023-02-17 08:27:06,587 DEBUG TRAIN Batch 3/8100 loss 27.583664 loss_att 36.426617 loss_ctc 41.802238 loss_rnnt 23.795963 hw_loss 0.231186 lr 0.00086876 rank 0
2023-02-17 08:28:25,345 DEBUG TRAIN Batch 3/8200 loss 21.750053 loss_att 26.646091 loss_ctc 38.986099 loss_rnnt 18.337740 hw_loss 0.253063 lr 0.00086720 rank 1
2023-02-17 08:28:25,358 DEBUG TRAIN Batch 3/8200 loss 21.530752 loss_att 27.488449 loss_ctc 35.345959 loss_rnnt 18.324982 hw_loss 0.322878 lr 0.00086729 rank 5
2023-02-17 08:28:25,363 DEBUG TRAIN Batch 3/8200 loss 17.872841 loss_att 20.986650 loss_ctc 27.386446 loss_rnnt 15.795209 hw_loss 0.349481 lr 0.00086817 rank 7
2023-02-17 08:28:25,365 DEBUG TRAIN Batch 3/8200 loss 29.297884 loss_att 32.153557 loss_ctc 45.469418 loss_rnnt 26.416777 hw_loss 0.288317 lr 0.00086754 rank 3
2023-02-17 08:28:25,366 DEBUG TRAIN Batch 3/8200 loss 22.383482 loss_att 24.839630 loss_ctc 33.920288 loss_rnnt 20.241238 hw_loss 0.211453 lr 0.00086798 rank 4
2023-02-17 08:28:25,368 DEBUG TRAIN Batch 3/8200 loss 24.811375 loss_att 26.889160 loss_ctc 35.167778 loss_rnnt 22.797138 hw_loss 0.408422 lr 0.00086745 rank 0
2023-02-17 08:28:25,370 DEBUG TRAIN Batch 3/8200 loss 14.871797 loss_att 18.156544 loss_ctc 24.116035 loss_rnnt 12.852015 hw_loss 0.244247 lr 0.00086699 rank 2
2023-02-17 08:28:25,408 DEBUG TRAIN Batch 3/8200 loss 20.770519 loss_att 25.268890 loss_ctc 36.865532 loss_rnnt 17.572931 hw_loss 0.284832 lr 0.00086781 rank 6
2023-02-17 08:29:41,026 DEBUG TRAIN Batch 3/8300 loss 26.488014 loss_att 34.594711 loss_ctc 43.068314 loss_rnnt 22.544769 hw_loss 0.208498 lr 0.00086624 rank 3
2023-02-17 08:29:41,026 DEBUG TRAIN Batch 3/8300 loss 22.396490 loss_att 28.598820 loss_ctc 39.792740 loss_rnnt 18.718903 hw_loss 0.220539 lr 0.00086615 rank 0
2023-02-17 08:29:41,029 DEBUG TRAIN Batch 3/8300 loss 30.843416 loss_att 35.129055 loss_ctc 42.798706 loss_rnnt 28.217159 hw_loss 0.328296 lr 0.00086651 rank 6
2023-02-17 08:29:41,030 DEBUG TRAIN Batch 3/8300 loss 12.403793 loss_att 20.554808 loss_ctc 20.012669 loss_rnnt 9.555016 hw_loss 0.382610 lr 0.00086668 rank 4
2023-02-17 08:29:41,033 DEBUG TRAIN Batch 3/8300 loss 41.317421 loss_att 50.869736 loss_ctc 68.284561 loss_rnnt 35.635422 hw_loss 0.329841 lr 0.00086686 rank 7
2023-02-17 08:29:41,035 DEBUG TRAIN Batch 3/8300 loss 20.298649 loss_att 24.379498 loss_ctc 29.590914 loss_rnnt 18.104099 hw_loss 0.261393 lr 0.00086599 rank 5
2023-02-17 08:29:41,035 DEBUG TRAIN Batch 3/8300 loss 29.598082 loss_att 39.670570 loss_ctc 39.959381 loss_rnnt 26.077438 hw_loss 0.233698 lr 0.00086569 rank 2
2023-02-17 08:29:41,036 DEBUG TRAIN Batch 3/8300 loss 33.438564 loss_att 40.072350 loss_ctc 46.902122 loss_rnnt 30.176016 hw_loss 0.263719 lr 0.00086590 rank 1
2023-02-17 08:30:27,431 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 7
2023-02-17 08:30:27,436 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 0
2023-02-17 08:30:27,436 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 6
2023-02-17 08:30:27,446 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 4
2023-02-17 08:30:27,451 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 5
2023-02-17 08:30:27,457 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 3
2023-02-17 08:30:27,458 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 2
2023-02-17 08:30:27,461 DEBUG CV Batch 3/0 loss 4.328170 loss_att 4.430267 loss_ctc 6.948191 loss_rnnt 3.735149 hw_loss 0.418623 history loss 4.167868 rank 1
2023-02-17 08:30:40,461 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 5
2023-02-17 08:30:40,495 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 0
2023-02-17 08:30:40,627 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 6
2023-02-17 08:30:40,672 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 7
2023-02-17 08:30:40,794 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 1
2023-02-17 08:30:41,202 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 4
2023-02-17 08:30:41,331 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 2
2023-02-17 08:30:41,598 DEBUG CV Batch 3/100 loss 16.256485 loss_att 21.215473 loss_ctc 24.727125 loss_rnnt 13.905879 hw_loss 0.430109 history loss 7.845575 rank 3
2023-02-17 08:30:56,558 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 6
2023-02-17 08:30:56,837 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 0
2023-02-17 08:30:57,326 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 4
2023-02-17 08:30:57,395 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 7
2023-02-17 08:30:57,427 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 1
2023-02-17 08:30:57,798 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 5
2023-02-17 08:30:57,922 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 3
2023-02-17 08:30:58,164 DEBUG CV Batch 3/200 loss 18.807402 loss_att 36.894615 loss_ctc 26.470772 loss_rnnt 14.038321 hw_loss 0.243478 history loss 8.753894 rank 2
2023-02-17 08:31:10,376 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 6
2023-02-17 08:31:11,163 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 0
2023-02-17 08:31:11,273 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 4
2023-02-17 08:31:11,426 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 7
2023-02-17 08:31:11,792 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 1
2023-02-17 08:31:11,862 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 5
2023-02-17 08:31:12,203 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 3
2023-02-17 08:31:12,720 DEBUG CV Batch 3/300 loss 10.007765 loss_att 11.096899 loss_ctc 18.292843 loss_rnnt 8.467052 hw_loss 0.409140 history loss 8.899284 rank 2
2023-02-17 08:31:24,188 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 6
2023-02-17 08:31:25,172 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 0
2023-02-17 08:31:25,267 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 4
2023-02-17 08:31:25,737 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 7
2023-02-17 08:31:26,110 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 1
2023-02-17 08:31:26,327 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 3
2023-02-17 08:31:27,041 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 2
2023-02-17 08:31:28,913 DEBUG CV Batch 3/400 loss 44.906815 loss_att 145.852737 loss_ctc 55.397072 loss_rnnt 23.241884 hw_loss 0.144459 history loss 10.189282 rank 5
2023-02-17 08:31:37,238 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 6
2023-02-17 08:31:37,880 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 0
2023-02-17 08:31:37,944 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 4
2023-02-17 08:31:38,845 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 1
2023-02-17 08:31:39,275 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 7
2023-02-17 08:31:39,616 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 3
2023-02-17 08:31:39,934 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 2
2023-02-17 08:31:43,057 DEBUG CV Batch 3/500 loss 13.194683 loss_att 16.495113 loss_ctc 23.722185 loss_rnnt 10.968859 hw_loss 0.303881 history loss 11.328621 rank 5
2023-02-17 08:31:52,475 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 6
2023-02-17 08:31:53,018 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 4
2023-02-17 08:31:53,056 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 0
2023-02-17 08:31:53,412 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 1
2023-02-17 08:31:54,262 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 7
2023-02-17 08:31:54,792 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 3
2023-02-17 08:31:55,438 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 2
2023-02-17 08:31:59,647 DEBUG CV Batch 3/600 loss 13.113750 loss_att 14.170828 loss_ctc 19.510721 loss_rnnt 11.807187 hw_loss 0.454157 history loss 12.516551 rank 5
2023-02-17 08:32:07,332 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 0
2023-02-17 08:32:07,706 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 6
2023-02-17 08:32:08,037 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 4
2023-02-17 08:32:08,117 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 1
2023-02-17 08:32:09,249 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 7
2023-02-17 08:32:09,848 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 3
2023-02-17 08:32:10,922 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 2
2023-02-17 08:32:15,104 DEBUG CV Batch 3/700 loss 46.871155 loss_att 128.180389 loss_ctc 68.852165 loss_rnnt 27.649874 hw_loss 0.053679 history loss 13.446449 rank 5
2023-02-17 08:32:22,002 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 6
2023-02-17 08:32:22,120 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 0
2023-02-17 08:32:22,909 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 1
2023-02-17 08:32:22,997 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 4
2023-02-17 08:32:24,460 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 7
2023-02-17 08:32:24,686 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 3
2023-02-17 08:32:26,020 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 2
2023-02-17 08:32:31,487 DEBUG CV Batch 3/800 loss 17.061846 loss_att 19.572605 loss_ctc 26.292690 loss_rnnt 15.173658 hw_loss 0.291108 history loss 12.677937 rank 5
2023-02-17 08:32:38,498 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 0
2023-02-17 08:32:38,835 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 6
2023-02-17 08:32:38,935 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 4
2023-02-17 08:32:39,960 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 1
2023-02-17 08:32:41,098 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 3
2023-02-17 08:32:41,143 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 7
2023-02-17 08:32:43,047 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 2
2023-02-17 08:32:49,956 DEBUG CV Batch 3/900 loss 33.749760 loss_att 51.686436 loss_ctc 48.752686 loss_rnnt 28.059994 hw_loss 0.191327 history loss 12.434864 rank 5
2023-02-17 08:32:52,983 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 4
2023-02-17 08:32:53,141 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 0
2023-02-17 08:32:53,247 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 6
2023-02-17 08:32:54,073 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 1
2023-02-17 08:32:55,178 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 7
2023-02-17 08:32:55,237 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 3
2023-02-17 08:32:57,221 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 2
2023-02-17 08:33:04,338 DEBUG CV Batch 3/1000 loss 5.688089 loss_att 8.180874 loss_ctc 11.879060 loss_rnnt 4.220525 hw_loss 0.269146 history loss 12.112680 rank 5
2023-02-17 08:33:06,794 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 4
2023-02-17 08:33:07,134 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 0
2023-02-17 08:33:07,235 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 6
2023-02-17 08:33:07,992 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 1
2023-02-17 08:33:09,188 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 3
2023-02-17 08:33:09,919 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 7
2023-02-17 08:33:11,400 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 2
2023-02-17 08:33:19,517 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 0
2023-02-17 08:33:20,321 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 4
2023-02-17 08:33:20,536 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 6
2023-02-17 08:33:20,852 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 1
2023-02-17 08:33:20,902 DEBUG CV Batch 3/1100 loss 10.184176 loss_att 9.329519 loss_ctc 13.954700 loss_rnnt 9.664343 hw_loss 0.352552 history loss 12.075273 rank 5
2023-02-17 08:33:22,511 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 3
2023-02-17 08:33:23,441 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 7
2023-02-17 08:33:24,291 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 2
2023-02-17 08:33:34,059 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 0
2023-02-17 08:33:34,263 DEBUG CV Batch 3/1200 loss 17.789581 loss_att 21.044027 loss_ctc 29.817846 loss_rnnt 15.369662 hw_loss 0.309864 history loss 12.548669 rank 5
2023-02-17 08:33:34,562 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 4
2023-02-17 08:33:34,751 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 6
2023-02-17 08:33:35,111 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 1
2023-02-17 08:33:37,128 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 3
2023-02-17 08:33:38,547 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 7
2023-02-17 08:33:39,163 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 2
2023-02-17 08:33:48,404 DEBUG CV Batch 3/1300 loss 9.824759 loss_att 11.001246 loss_ctc 15.425858 loss_rnnt 8.664628 hw_loss 0.333788 history loss 12.941337 rank 5
2023-02-17 08:33:49,089 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 0
2023-02-17 08:33:49,230 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 6
2023-02-17 08:33:50,012 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 4
2023-02-17 08:33:50,214 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 1
2023-02-17 08:33:52,025 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 3
2023-02-17 08:33:54,125 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 2
2023-02-17 08:33:54,289 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 7
2023-02-17 08:34:02,910 DEBUG CV Batch 3/1400 loss 18.790771 loss_att 56.348534 loss_ctc 26.919241 loss_rnnt 10.097679 hw_loss 0.183266 history loss 13.396542 rank 5
2023-02-17 08:34:04,375 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 0
2023-02-17 08:34:04,911 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 6
2023-02-17 08:34:05,535 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 1
2023-02-17 08:34:05,612 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 4
2023-02-17 08:34:07,696 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 3
2023-02-17 08:34:09,282 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 2
2023-02-17 08:34:09,758 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 7
2023-02-17 08:34:18,307 DEBUG CV Batch 3/1500 loss 16.779490 loss_att 16.164612 loss_ctc 22.021513 loss_rnnt 16.024155 hw_loss 0.336329 history loss 13.116394 rank 5
2023-02-17 08:34:20,916 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 0
2023-02-17 08:34:21,457 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 1
2023-02-17 08:34:21,512 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 4
2023-02-17 08:34:21,644 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 6
2023-02-17 08:34:23,876 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 3
2023-02-17 08:34:25,433 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 2
2023-02-17 08:34:26,050 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 7
2023-02-17 08:34:34,324 DEBUG CV Batch 3/1600 loss 16.714708 loss_att 42.616383 loss_ctc 30.568617 loss_rnnt 9.506904 hw_loss 0.338031 history loss 13.017393 rank 5
2023-02-17 08:34:35,691 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 0
2023-02-17 08:34:35,863 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 6
2023-02-17 08:34:36,054 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 1
2023-02-17 08:34:36,358 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 4
2023-02-17 08:34:38,295 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 3
2023-02-17 08:34:40,310 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 2
2023-02-17 08:34:40,549 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 7
2023-02-17 08:34:45,954 INFO Epoch 3 CV info cv_loss 12.795806410099354
2023-02-17 08:34:45,955 INFO Epoch 4 TRAIN info lr 0.0008660167436603024
2023-02-17 08:34:45,960 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:46,059 INFO Epoch 3 CV info cv_loss 12.795806413200623
2023-02-17 08:34:46,059 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/3.pt
2023-02-17 08:34:46,491 INFO Epoch 3 CV info cv_loss 12.795806410754066
2023-02-17 08:34:46,491 INFO Epoch 4 TRAIN info lr 0.0008661466728080577
2023-02-17 08:34:46,495 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:46,611 INFO Epoch 3 CV info cv_loss 12.795806412373617
2023-02-17 08:34:46,612 INFO Epoch 4 TRAIN info lr 0.0008656402795311361
2023-02-17 08:34:46,617 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:46,653 INFO Epoch 4 TRAIN info lr 0.0008656792013305546
2023-02-17 08:34:46,658 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:47,681 DEBUG CV Batch 3/1700 loss 13.907338 loss_att 15.170641 loss_ctc 22.681667 loss_rnnt 12.269106 hw_loss 0.404364 history loss 12.857681 rank 5
2023-02-17 08:34:48,201 INFO Epoch 3 CV info cv_loss 12.7958064123047
2023-02-17 08:34:48,202 INFO Epoch 4 TRAIN info lr 0.000865731105230912
2023-02-17 08:34:48,206 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:50,329 INFO Epoch 3 CV info cv_loss 12.79580640996152
2023-02-17 08:34:50,329 INFO Epoch 4 TRAIN info lr 0.0008650311892618034
2023-02-17 08:34:50,333 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:50,407 INFO Epoch 3 CV info cv_loss 12.795806408721013
2023-02-17 08:34:50,408 INFO Epoch 4 TRAIN info lr 0.0008661726656559234
2023-02-17 08:34:50,413 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:34:57,556 INFO Epoch 3 CV info cv_loss 12.795806410512856
2023-02-17 08:34:57,557 INFO Epoch 4 TRAIN info lr 0.0008656921764304352
2023-02-17 08:34:57,562 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 08:36:05,081 DEBUG TRAIN Batch 4/0 loss 14.179090 loss_att 14.094070 loss_ctc 19.386438 loss_rnnt 13.280909 hw_loss 0.414133 lr 0.00086567 rank 0
2023-02-17 08:36:05,083 DEBUG TRAIN Batch 4/0 loss 10.839894 loss_att 11.854280 loss_ctc 14.402627 loss_rnnt 9.960914 hw_loss 0.377010 lr 0.00086572 rank 3
2023-02-17 08:36:05,085 DEBUG TRAIN Batch 4/0 loss 19.560091 loss_att 18.775486 loss_ctc 24.171251 loss_rnnt 18.883663 hw_loss 0.409739 lr 0.00086613 rank 4
2023-02-17 08:36:05,088 DEBUG TRAIN Batch 4/0 loss 12.254733 loss_att 11.646695 loss_ctc 14.104536 loss_rnnt 11.965711 hw_loss 0.307479 lr 0.00086563 rank 1
2023-02-17 08:36:05,108 DEBUG TRAIN Batch 4/0 loss 10.605323 loss_att 11.705973 loss_ctc 14.902386 loss_rnnt 9.660513 hw_loss 0.284511 lr 0.00086502 rank 2
2023-02-17 08:36:05,122 DEBUG TRAIN Batch 4/0 loss 12.435182 loss_att 11.570805 loss_ctc 15.997086 loss_rnnt 11.912718 hw_loss 0.413287 lr 0.00086568 rank 5
2023-02-17 08:36:05,126 DEBUG TRAIN Batch 4/0 loss 13.987843 loss_att 14.177067 loss_ctc 19.115894 loss_rnnt 13.099161 hw_loss 0.313303 lr 0.00086600 rank 6
2023-02-17 08:36:05,134 DEBUG TRAIN Batch 4/0 loss 17.239567 loss_att 15.817543 loss_ctc 21.770792 loss_rnnt 16.729813 hw_loss 0.356240 lr 0.00086616 rank 7
2023-02-17 08:37:20,516 DEBUG TRAIN Batch 4/100 loss 10.442493 loss_att 14.049080 loss_ctc 15.231292 loss_rnnt 8.935815 hw_loss 0.275351 lr 0.00086433 rank 1
2023-02-17 08:37:20,516 DEBUG TRAIN Batch 4/100 loss 24.723524 loss_att 30.176056 loss_ctc 50.625908 loss_rnnt 19.946892 hw_loss 0.435887 lr 0.00086442 rank 3
2023-02-17 08:37:20,517 DEBUG TRAIN Batch 4/100 loss 27.763224 loss_att 37.332718 loss_ctc 41.628189 loss_rnnt 23.867466 hw_loss 0.249744 lr 0.00086471 rank 6
2023-02-17 08:37:20,520 DEBUG TRAIN Batch 4/100 loss 25.424589 loss_att 33.898643 loss_ctc 38.764389 loss_rnnt 21.787834 hw_loss 0.306196 lr 0.00086437 rank 0
2023-02-17 08:37:20,520 DEBUG TRAIN Batch 4/100 loss 33.460934 loss_att 38.242561 loss_ctc 50.661560 loss_rnnt 30.069134 hw_loss 0.266358 lr 0.00086438 rank 5
2023-02-17 08:37:20,526 DEBUG TRAIN Batch 4/100 loss 15.003490 loss_att 20.964657 loss_ctc 17.732624 loss_rnnt 13.298758 hw_loss 0.278654 lr 0.00086373 rank 2
2023-02-17 08:37:20,544 DEBUG TRAIN Batch 4/100 loss 26.101885 loss_att 36.741776 loss_ctc 41.946465 loss_rnnt 21.726706 hw_loss 0.252355 lr 0.00086486 rank 7
2023-02-17 08:37:20,568 DEBUG TRAIN Batch 4/100 loss 29.269382 loss_att 34.214123 loss_ctc 40.162849 loss_rnnt 26.663511 hw_loss 0.308366 lr 0.00086484 rank 4
2023-02-17 08:38:36,253 DEBUG TRAIN Batch 4/200 loss 8.878626 loss_att 13.254766 loss_ctc 16.665054 loss_rnnt 6.866046 hw_loss 0.185928 lr 0.00086342 rank 6
2023-02-17 08:38:36,254 DEBUG TRAIN Batch 4/200 loss 46.559540 loss_att 54.700119 loss_ctc 74.721802 loss_rnnt 41.056213 hw_loss 0.225459 lr 0.00086310 rank 5
2023-02-17 08:38:36,256 DEBUG TRAIN Batch 4/200 loss 36.196068 loss_att 44.076057 loss_ctc 57.022881 loss_rnnt 31.711775 hw_loss 0.246357 lr 0.00086244 rank 2
2023-02-17 08:38:36,256 DEBUG TRAIN Batch 4/200 loss 25.269953 loss_att 32.330704 loss_ctc 37.376301 loss_rnnt 22.040901 hw_loss 0.380100 lr 0.00086357 rank 7
2023-02-17 08:38:36,263 DEBUG TRAIN Batch 4/200 loss 36.524269 loss_att 43.619278 loss_ctc 52.474861 loss_rnnt 32.896263 hw_loss 0.154242 lr 0.00086304 rank 1
2023-02-17 08:38:36,267 DEBUG TRAIN Batch 4/200 loss 16.728197 loss_att 21.566334 loss_ctc 28.690786 loss_rnnt 14.016041 hw_loss 0.280341 lr 0.00086308 rank 0
2023-02-17 08:38:36,280 DEBUG TRAIN Batch 4/200 loss 17.696379 loss_att 22.693878 loss_ctc 25.875235 loss_rnnt 15.465678 hw_loss 0.263785 lr 0.00086355 rank 4
2023-02-17 08:38:36,319 DEBUG TRAIN Batch 4/200 loss 37.660503 loss_att 49.834194 loss_ctc 50.367210 loss_rnnt 33.404980 hw_loss 0.237298 lr 0.00086313 rank 3
2023-02-17 08:39:57,045 DEBUG TRAIN Batch 4/300 loss 21.630724 loss_att 27.937626 loss_ctc 34.180443 loss_rnnt 18.586220 hw_loss 0.205923 lr 0.00086116 rank 2
2023-02-17 08:39:57,070 DEBUG TRAIN Batch 4/300 loss 22.190332 loss_att 29.827717 loss_ctc 40.364315 loss_rnnt 18.131725 hw_loss 0.202376 lr 0.00086176 rank 1
2023-02-17 08:39:57,080 DEBUG TRAIN Batch 4/300 loss 21.178596 loss_att 27.035622 loss_ctc 40.836586 loss_rnnt 17.202404 hw_loss 0.344478 lr 0.00086185 rank 3
2023-02-17 08:39:57,084 DEBUG TRAIN Batch 4/300 loss 37.249435 loss_att 47.937061 loss_ctc 60.939281 loss_rnnt 31.787121 hw_loss 0.311516 lr 0.00086180 rank 0
2023-02-17 08:39:57,099 DEBUG TRAIN Batch 4/300 loss 20.930986 loss_att 26.414921 loss_ctc 28.447922 loss_rnnt 18.704645 hw_loss 0.238683 lr 0.00086226 rank 4
2023-02-17 08:39:57,099 DEBUG TRAIN Batch 4/300 loss 27.489639 loss_att 34.143284 loss_ctc 41.175396 loss_rnnt 24.182327 hw_loss 0.284651 lr 0.00086229 rank 7
2023-02-17 08:39:57,102 DEBUG TRAIN Batch 4/300 loss 19.106770 loss_att 25.758316 loss_ctc 34.967094 loss_rnnt 15.520546 hw_loss 0.264757 lr 0.00086181 rank 5
2023-02-17 08:39:57,114 DEBUG TRAIN Batch 4/300 loss 28.616505 loss_att 34.347542 loss_ctc 43.209797 loss_rnnt 25.408955 hw_loss 0.216691 lr 0.00086213 rank 6
2023-02-17 08:41:25,699 DEBUG TRAIN Batch 4/400 loss 17.883469 loss_att 24.847460 loss_ctc 32.999359 loss_rnnt 14.351576 hw_loss 0.231828 lr 0.00086085 rank 6
2023-02-17 08:41:25,701 DEBUG TRAIN Batch 4/400 loss 40.549767 loss_att 48.630939 loss_ctc 71.575104 loss_rnnt 34.527817 hw_loss 0.504394 lr 0.00086048 rank 1
2023-02-17 08:41:25,700 DEBUG TRAIN Batch 4/400 loss 47.952255 loss_att 54.097649 loss_ctc 68.279182 loss_rnnt 43.827507 hw_loss 0.347654 lr 0.00086057 rank 3
2023-02-17 08:41:25,708 DEBUG TRAIN Batch 4/400 loss 21.848738 loss_att 30.458050 loss_ctc 40.559433 loss_rnnt 17.529686 hw_loss 0.192057 lr 0.00086054 rank 5
2023-02-17 08:41:25,712 DEBUG TRAIN Batch 4/400 loss 18.856297 loss_att 21.425137 loss_ctc 26.280634 loss_rnnt 17.163342 hw_loss 0.354893 lr 0.00086101 rank 7
2023-02-17 08:41:25,724 DEBUG TRAIN Batch 4/400 loss 24.007715 loss_att 28.827499 loss_ctc 43.301216 loss_rnnt 20.285980 hw_loss 0.347455 lr 0.00085989 rank 2
2023-02-17 08:41:25,728 DEBUG TRAIN Batch 4/400 loss 23.828651 loss_att 30.406874 loss_ctc 38.144108 loss_rnnt 20.474882 hw_loss 0.242620 lr 0.00086052 rank 0
2023-02-17 08:41:25,751 DEBUG TRAIN Batch 4/400 loss 28.315226 loss_att 31.929701 loss_ctc 43.274559 loss_rnnt 25.449926 hw_loss 0.277174 lr 0.00086098 rank 4
2023-02-17 08:42:41,028 DEBUG TRAIN Batch 4/500 loss 21.165764 loss_att 30.244308 loss_ctc 37.907303 loss_rnnt 16.942131 hw_loss 0.329476 lr 0.00085973 rank 7
2023-02-17 08:42:41,029 DEBUG TRAIN Batch 4/500 loss 24.770887 loss_att 33.377914 loss_ctc 42.740330 loss_rnnt 20.448652 hw_loss 0.384197 lr 0.00085926 rank 5
2023-02-17 08:42:41,030 DEBUG TRAIN Batch 4/500 loss 44.962112 loss_att 48.124676 loss_ctc 63.043041 loss_rnnt 41.792259 hw_loss 0.237282 lr 0.00085958 rank 6
2023-02-17 08:42:41,029 DEBUG TRAIN Batch 4/500 loss 32.075947 loss_att 35.751335 loss_ctc 52.916500 loss_rnnt 28.375530 hw_loss 0.349870 lr 0.00085921 rank 1
2023-02-17 08:42:41,031 DEBUG TRAIN Batch 4/500 loss 31.011549 loss_att 36.272228 loss_ctc 43.960468 loss_rnnt 28.046259 hw_loss 0.349930 lr 0.00085930 rank 3
2023-02-17 08:42:41,032 DEBUG TRAIN Batch 4/500 loss 17.802540 loss_att 24.199360 loss_ctc 28.198362 loss_rnnt 15.025564 hw_loss 0.209066 lr 0.00085925 rank 0
2023-02-17 08:42:41,072 DEBUG TRAIN Batch 4/500 loss 13.918869 loss_att 18.601522 loss_ctc 24.310526 loss_rnnt 11.451288 hw_loss 0.272803 lr 0.00085862 rank 2
2023-02-17 08:42:41,084 DEBUG TRAIN Batch 4/500 loss 20.041191 loss_att 24.223885 loss_ctc 31.119509 loss_rnnt 17.513620 hw_loss 0.401105 lr 0.00085971 rank 4
2023-02-17 08:43:57,648 DEBUG TRAIN Batch 4/600 loss 22.061373 loss_att 23.129717 loss_ctc 27.778677 loss_rnnt 20.882328 hw_loss 0.380754 lr 0.00085795 rank 1
2023-02-17 08:43:57,658 DEBUG TRAIN Batch 4/600 loss 10.979965 loss_att 12.167131 loss_ctc 14.619820 loss_rnnt 10.039371 hw_loss 0.408466 lr 0.00085735 rank 2
2023-02-17 08:43:57,660 DEBUG TRAIN Batch 4/600 loss 14.813548 loss_att 21.649357 loss_ctc 27.817453 loss_rnnt 11.605338 hw_loss 0.200989 lr 0.00085831 rank 6
2023-02-17 08:43:57,660 DEBUG TRAIN Batch 4/600 loss 24.068485 loss_att 30.319691 loss_ctc 38.594982 loss_rnnt 20.757769 hw_loss 0.231764 lr 0.00085799 rank 0
2023-02-17 08:43:57,662 DEBUG TRAIN Batch 4/600 loss 21.644829 loss_att 23.812019 loss_ctc 30.261236 loss_rnnt 19.878277 hw_loss 0.345486 lr 0.00085847 rank 7
2023-02-17 08:43:57,673 DEBUG TRAIN Batch 4/600 loss 15.811623 loss_att 15.492491 loss_ctc 21.018517 loss_rnnt 14.964784 hw_loss 0.405775 lr 0.00085844 rank 4
2023-02-17 08:43:57,685 DEBUG TRAIN Batch 4/600 loss 21.105564 loss_att 24.338501 loss_ctc 35.985508 loss_rnnt 18.292746 hw_loss 0.341698 lr 0.00085804 rank 3
2023-02-17 08:43:57,741 DEBUG TRAIN Batch 4/600 loss 17.335541 loss_att 17.492704 loss_ctc 21.256327 loss_rnnt 16.567535 hw_loss 0.400878 lr 0.00085800 rank 5
2023-02-17 08:45:23,400 DEBUG TRAIN Batch 4/700 loss 21.202862 loss_att 31.133327 loss_ctc 39.490196 loss_rnnt 16.590376 hw_loss 0.352648 lr 0.00085720 rank 7
2023-02-17 08:45:23,403 DEBUG TRAIN Batch 4/700 loss 27.826628 loss_att 36.120842 loss_ctc 52.277981 loss_rnnt 22.711174 hw_loss 0.368306 lr 0.00085669 rank 1
2023-02-17 08:45:23,415 DEBUG TRAIN Batch 4/700 loss 30.485138 loss_att 37.760319 loss_ctc 50.498779 loss_rnnt 26.225712 hw_loss 0.254822 lr 0.00085678 rank 3
2023-02-17 08:45:23,420 DEBUG TRAIN Batch 4/700 loss 19.756279 loss_att 29.985157 loss_ctc 31.358418 loss_rnnt 16.006147 hw_loss 0.295133 lr 0.00085610 rank 2
2023-02-17 08:45:23,420 DEBUG TRAIN Batch 4/700 loss 26.002258 loss_att 37.475121 loss_ctc 38.075325 loss_rnnt 21.928436 hw_loss 0.317820 lr 0.00085674 rank 5
2023-02-17 08:45:23,456 DEBUG TRAIN Batch 4/700 loss 27.408493 loss_att 38.993729 loss_ctc 45.745941 loss_rnnt 22.500591 hw_loss 0.273492 lr 0.00085705 rank 6
2023-02-17 08:45:23,489 DEBUG TRAIN Batch 4/700 loss 27.143614 loss_att 35.726116 loss_ctc 42.847729 loss_rnnt 23.166725 hw_loss 0.312200 lr 0.00085718 rank 4
2023-02-17 08:45:23,494 DEBUG TRAIN Batch 4/700 loss 16.745594 loss_att 22.895720 loss_ctc 26.164663 loss_rnnt 14.142639 hw_loss 0.219475 lr 0.00085672 rank 0
2023-02-17 08:46:48,021 DEBUG TRAIN Batch 4/800 loss 15.910893 loss_att 22.229576 loss_ctc 28.656860 loss_rnnt 12.796251 hw_loss 0.283954 lr 0.00085547 rank 0
2023-02-17 08:46:48,024 DEBUG TRAIN Batch 4/800 loss 31.720413 loss_att 38.353714 loss_ctc 47.423119 loss_rnnt 28.127378 hw_loss 0.323777 lr 0.00085595 rank 7
2023-02-17 08:46:48,026 DEBUG TRAIN Batch 4/800 loss 27.997110 loss_att 32.518803 loss_ctc 39.339546 loss_rnnt 25.480156 hw_loss 0.188044 lr 0.00085484 rank 2
2023-02-17 08:46:48,030 DEBUG TRAIN Batch 4/800 loss 29.248255 loss_att 33.914116 loss_ctc 36.830452 loss_rnnt 27.174011 hw_loss 0.243961 lr 0.00085592 rank 4
2023-02-17 08:46:48,030 DEBUG TRAIN Batch 4/800 loss 16.497889 loss_att 22.513115 loss_ctc 28.202682 loss_rnnt 13.613785 hw_loss 0.225787 lr 0.00085552 rank 3
2023-02-17 08:46:48,034 DEBUG TRAIN Batch 4/800 loss 18.280437 loss_att 22.875790 loss_ctc 31.034904 loss_rnnt 15.518917 hw_loss 0.265976 lr 0.00085548 rank 5
2023-02-17 08:46:48,052 DEBUG TRAIN Batch 4/800 loss 14.497757 loss_att 19.775772 loss_ctc 26.491074 loss_rnnt 11.684367 hw_loss 0.297522 lr 0.00085543 rank 1
2023-02-17 08:46:48,058 DEBUG TRAIN Batch 4/800 loss 32.787521 loss_att 40.978493 loss_ctc 49.682457 loss_rnnt 28.783773 hw_loss 0.211675 lr 0.00085580 rank 6
2023-02-17 08:48:03,824 DEBUG TRAIN Batch 4/900 loss 24.822796 loss_att 34.514557 loss_ctc 35.166775 loss_rnnt 21.346142 hw_loss 0.298325 lr 0.00085423 rank 5
2023-02-17 08:48:03,825 DEBUG TRAIN Batch 4/900 loss 19.839350 loss_att 25.752789 loss_ctc 36.214951 loss_rnnt 16.332142 hw_loss 0.264572 lr 0.00085427 rank 3
2023-02-17 08:48:03,825 DEBUG TRAIN Batch 4/900 loss 17.637226 loss_att 31.501656 loss_ctc 40.010780 loss_rnnt 11.781955 hw_loss 0.186087 lr 0.00085454 rank 6
2023-02-17 08:48:03,831 DEBUG TRAIN Batch 4/900 loss 25.833971 loss_att 33.355839 loss_ctc 44.579697 loss_rnnt 21.706039 hw_loss 0.232738 lr 0.00085469 rank 7
2023-02-17 08:48:03,830 DEBUG TRAIN Batch 4/900 loss 14.389608 loss_att 23.703650 loss_ctc 29.944447 loss_rnnt 10.275595 hw_loss 0.332300 lr 0.00085360 rank 2
2023-02-17 08:48:03,834 DEBUG TRAIN Batch 4/900 loss 24.469538 loss_att 29.598106 loss_ctc 36.348171 loss_rnnt 21.726934 hw_loss 0.249512 lr 0.00085422 rank 0
2023-02-17 08:48:03,839 DEBUG TRAIN Batch 4/900 loss 18.956902 loss_att 29.227428 loss_ctc 37.363743 loss_rnnt 14.295492 hw_loss 0.286989 lr 0.00085418 rank 1
2023-02-17 08:48:03,887 DEBUG TRAIN Batch 4/900 loss 31.967695 loss_att 36.722950 loss_ctc 49.379753 loss_rnnt 28.620737 hw_loss 0.139308 lr 0.00085467 rank 4
2023-02-17 08:49:23,349 DEBUG TRAIN Batch 4/1000 loss 22.000818 loss_att 27.328445 loss_ctc 35.505920 loss_rnnt 18.997747 hw_loss 0.256622 lr 0.00085298 rank 0
2023-02-17 08:49:23,372 DEBUG TRAIN Batch 4/1000 loss 26.114435 loss_att 29.234501 loss_ctc 37.259525 loss_rnnt 23.913395 hw_loss 0.170650 lr 0.00085342 rank 4
2023-02-17 08:49:23,384 DEBUG TRAIN Batch 4/1000 loss 19.969736 loss_att 23.995167 loss_ctc 38.508801 loss_rnnt 16.489212 hw_loss 0.381679 lr 0.00085303 rank 3
2023-02-17 08:49:23,392 DEBUG TRAIN Batch 4/1000 loss 12.064268 loss_att 15.041748 loss_ctc 21.926155 loss_rnnt 9.984645 hw_loss 0.317268 lr 0.00085299 rank 5
2023-02-17 08:49:23,394 DEBUG TRAIN Batch 4/1000 loss 15.813382 loss_att 18.816376 loss_ctc 27.016521 loss_rnnt 13.633259 hw_loss 0.160823 lr 0.00085294 rank 1
2023-02-17 08:49:23,400 DEBUG TRAIN Batch 4/1000 loss 17.115150 loss_att 20.825977 loss_ctc 31.072693 loss_rnnt 14.334393 hw_loss 0.332975 lr 0.00085345 rank 7
2023-02-17 08:49:23,423 DEBUG TRAIN Batch 4/1000 loss 29.036707 loss_att 34.841530 loss_ctc 41.008076 loss_rnnt 26.131599 hw_loss 0.277429 lr 0.00085236 rank 2
2023-02-17 08:49:23,423 DEBUG TRAIN Batch 4/1000 loss 32.134449 loss_att 40.666069 loss_ctc 52.068802 loss_rnnt 27.618114 hw_loss 0.285177 lr 0.00085330 rank 6
2023-02-17 08:50:54,739 DEBUG TRAIN Batch 4/1100 loss 18.110794 loss_att 22.172733 loss_ctc 31.328314 loss_rnnt 15.373168 hw_loss 0.305443 lr 0.00085218 rank 4
2023-02-17 08:50:54,742 DEBUG TRAIN Batch 4/1100 loss 15.078416 loss_att 18.233904 loss_ctc 25.162125 loss_rnnt 12.999140 hw_loss 0.194406 lr 0.00085112 rank 2
2023-02-17 08:50:54,742 DEBUG TRAIN Batch 4/1100 loss 36.444218 loss_att 39.789551 loss_ctc 54.832939 loss_rnnt 33.156097 hw_loss 0.313553 lr 0.00085221 rank 7
2023-02-17 08:50:54,743 DEBUG TRAIN Batch 4/1100 loss 17.331030 loss_att 22.737486 loss_ctc 29.974983 loss_rnnt 14.412110 hw_loss 0.284564 lr 0.00085170 rank 1
2023-02-17 08:50:54,747 DEBUG TRAIN Batch 4/1100 loss 17.449829 loss_att 24.764227 loss_ctc 33.508286 loss_rnnt 13.664081 hw_loss 0.340768 lr 0.00085174 rank 0
2023-02-17 08:50:54,749 DEBUG TRAIN Batch 4/1100 loss 20.186935 loss_att 24.422960 loss_ctc 33.072311 loss_rnnt 17.514046 hw_loss 0.201813 lr 0.00085179 rank 3
2023-02-17 08:50:54,763 DEBUG TRAIN Batch 4/1100 loss 31.607212 loss_att 35.047928 loss_ctc 52.247784 loss_rnnt 28.075630 hw_loss 0.171304 lr 0.00085175 rank 5
2023-02-17 08:50:54,773 DEBUG TRAIN Batch 4/1100 loss 25.768829 loss_att 29.284336 loss_ctc 43.743324 loss_rnnt 22.543453 hw_loss 0.235641 lr 0.00085206 rank 6
2023-02-17 08:52:10,870 DEBUG TRAIN Batch 4/1200 loss 17.985981 loss_att 23.000820 loss_ctc 28.088522 loss_rnnt 15.465672 hw_loss 0.319375 lr 0.00085052 rank 5
2023-02-17 08:52:10,876 DEBUG TRAIN Batch 4/1200 loss 13.294448 loss_att 17.327179 loss_ctc 22.497969 loss_rnnt 11.112607 hw_loss 0.277796 lr 0.00085083 rank 6
2023-02-17 08:52:10,877 DEBUG TRAIN Batch 4/1200 loss 23.825575 loss_att 24.243162 loss_ctc 35.745495 loss_rnnt 21.996527 hw_loss 0.292894 lr 0.00085055 rank 3
2023-02-17 08:52:10,880 DEBUG TRAIN Batch 4/1200 loss 22.613115 loss_att 26.150898 loss_ctc 37.022820 loss_rnnt 19.818462 hw_loss 0.310885 lr 0.00085047 rank 1
2023-02-17 08:52:10,883 DEBUG TRAIN Batch 4/1200 loss 34.549976 loss_att 38.282677 loss_ctc 52.159481 loss_rnnt 31.267435 hw_loss 0.352624 lr 0.00084989 rank 2
2023-02-17 08:52:10,882 DEBUG TRAIN Batch 4/1200 loss 24.085745 loss_att 29.568542 loss_ctc 41.540283 loss_rnnt 20.466627 hw_loss 0.366161 lr 0.00085050 rank 0
2023-02-17 08:52:10,925 DEBUG TRAIN Batch 4/1200 loss 21.207384 loss_att 28.775951 loss_ctc 32.636787 loss_rnnt 18.041121 hw_loss 0.241180 lr 0.00085097 rank 7
2023-02-17 08:52:10,928 DEBUG TRAIN Batch 4/1200 loss 29.210258 loss_att 30.610933 loss_ctc 39.417622 loss_rnnt 27.361887 hw_loss 0.388601 lr 0.00085095 rank 4
2023-02-17 08:53:27,659 DEBUG TRAIN Batch 4/1300 loss 28.594297 loss_att 36.084831 loss_ctc 41.810242 loss_rnnt 25.222919 hw_loss 0.208392 lr 0.00084974 rank 7
2023-02-17 08:53:27,664 DEBUG TRAIN Batch 4/1300 loss 12.767111 loss_att 20.532188 loss_ctc 22.365952 loss_rnnt 9.783202 hw_loss 0.283215 lr 0.00084924 rank 1
2023-02-17 08:53:27,663 DEBUG TRAIN Batch 4/1300 loss 25.509960 loss_att 39.052742 loss_ctc 47.310516 loss_rnnt 19.754400 hw_loss 0.262991 lr 0.00084960 rank 6
2023-02-17 08:53:27,663 DEBUG TRAIN Batch 4/1300 loss 20.358736 loss_att 23.720139 loss_ctc 34.573223 loss_rnnt 17.644073 hw_loss 0.275841 lr 0.00084867 rank 2
2023-02-17 08:53:27,664 DEBUG TRAIN Batch 4/1300 loss 40.848866 loss_att 47.451195 loss_ctc 54.617706 loss_rnnt 37.568455 hw_loss 0.232689 lr 0.00084972 rank 4
2023-02-17 08:53:27,665 DEBUG TRAIN Batch 4/1300 loss 14.373981 loss_att 22.799023 loss_ctc 19.813419 loss_rnnt 11.890582 hw_loss 0.137123 lr 0.00084933 rank 3
2023-02-17 08:53:27,666 DEBUG TRAIN Batch 4/1300 loss 16.248245 loss_att 17.990088 loss_ctc 23.395081 loss_rnnt 14.763586 hw_loss 0.343840 lr 0.00084928 rank 0
2023-02-17 08:53:27,687 DEBUG TRAIN Batch 4/1300 loss 22.088089 loss_att 29.107037 loss_ctc 42.333385 loss_rnnt 17.896549 hw_loss 0.165711 lr 0.00084929 rank 5
2023-02-17 08:54:51,663 DEBUG TRAIN Batch 4/1400 loss 18.665407 loss_att 23.595623 loss_ctc 32.046867 loss_rnnt 15.779198 hw_loss 0.217448 lr 0.00084807 rank 5
2023-02-17 08:54:51,666 DEBUG TRAIN Batch 4/1400 loss 13.699290 loss_att 22.375000 loss_ctc 25.286991 loss_rnnt 10.241155 hw_loss 0.333689 lr 0.00084805 rank 0
2023-02-17 08:54:51,684 DEBUG TRAIN Batch 4/1400 loss 24.854097 loss_att 29.475920 loss_ctc 36.948997 loss_rnnt 22.177444 hw_loss 0.261813 lr 0.00084745 rank 2
2023-02-17 08:54:51,704 DEBUG TRAIN Batch 4/1400 loss 21.870123 loss_att 29.263086 loss_ctc 39.326565 loss_rnnt 17.959568 hw_loss 0.195819 lr 0.00084852 rank 7
2023-02-17 08:54:51,703 DEBUG TRAIN Batch 4/1400 loss 21.115248 loss_att 31.604404 loss_ctc 31.594328 loss_rnnt 17.487080 hw_loss 0.249608 lr 0.00084837 rank 6
2023-02-17 08:54:51,703 DEBUG TRAIN Batch 4/1400 loss 32.114235 loss_att 33.795959 loss_ctc 47.820091 loss_rnnt 29.515608 hw_loss 0.315315 lr 0.00084849 rank 4
2023-02-17 08:54:51,713 DEBUG TRAIN Batch 4/1400 loss 29.526686 loss_att 40.598572 loss_ctc 50.464088 loss_rnnt 24.407368 hw_loss 0.212406 lr 0.00084802 rank 1
2023-02-17 08:54:51,721 DEBUG TRAIN Batch 4/1400 loss 12.536561 loss_att 18.843540 loss_ctc 17.057961 loss_rnnt 10.566859 hw_loss 0.197724 lr 0.00084810 rank 3
2023-02-17 08:56:18,616 DEBUG TRAIN Batch 4/1500 loss 28.253975 loss_att 32.979710 loss_ctc 41.069748 loss_rnnt 25.519726 hw_loss 0.150621 lr 0.00084730 rank 7
2023-02-17 08:56:18,616 DEBUG TRAIN Batch 4/1500 loss 13.577541 loss_att 19.496063 loss_ctc 29.340622 loss_rnnt 10.149521 hw_loss 0.267322 lr 0.00084715 rank 6
2023-02-17 08:56:18,617 DEBUG TRAIN Batch 4/1500 loss 23.826748 loss_att 28.153526 loss_ctc 35.398537 loss_rnnt 21.278259 hw_loss 0.262927 lr 0.00084685 rank 5
2023-02-17 08:56:18,618 DEBUG TRAIN Batch 4/1500 loss 19.443365 loss_att 24.687000 loss_ctc 28.322285 loss_rnnt 17.076651 hw_loss 0.251499 lr 0.00084728 rank 4
2023-02-17 08:56:18,619 DEBUG TRAIN Batch 4/1500 loss 42.793560 loss_att 51.181820 loss_ctc 65.618713 loss_rnnt 37.938190 hw_loss 0.251933 lr 0.00084680 rank 1
2023-02-17 08:56:18,621 DEBUG TRAIN Batch 4/1500 loss 41.758221 loss_att 43.591667 loss_ctc 59.279762 loss_rnnt 38.909943 hw_loss 0.272601 lr 0.00084623 rank 2
2023-02-17 08:56:18,621 DEBUG TRAIN Batch 4/1500 loss 15.110140 loss_att 20.666912 loss_ctc 26.778877 loss_rnnt 12.313514 hw_loss 0.242701 lr 0.00084689 rank 3
2023-02-17 08:56:18,669 DEBUG TRAIN Batch 4/1500 loss 15.079562 loss_att 20.814007 loss_ctc 28.828995 loss_rnnt 11.924701 hw_loss 0.327592 lr 0.00084684 rank 0
2023-02-17 08:57:35,398 DEBUG TRAIN Batch 4/1600 loss 29.572184 loss_att 35.958035 loss_ctc 43.144531 loss_rnnt 26.329819 hw_loss 0.291653 lr 0.00084606 rank 4
2023-02-17 08:57:35,398 DEBUG TRAIN Batch 4/1600 loss 20.098192 loss_att 28.528847 loss_ctc 37.870800 loss_rnnt 15.917091 hw_loss 0.234914 lr 0.00084502 rank 2
2023-02-17 08:57:35,398 DEBUG TRAIN Batch 4/1600 loss 15.543870 loss_att 23.783083 loss_ctc 27.391863 loss_rnnt 12.181848 hw_loss 0.252086 lr 0.00084559 rank 1
2023-02-17 08:57:35,399 DEBUG TRAIN Batch 4/1600 loss 20.063356 loss_att 27.746521 loss_ctc 40.489277 loss_rnnt 15.642440 hw_loss 0.301553 lr 0.00084609 rank 7
2023-02-17 08:57:35,399 DEBUG TRAIN Batch 4/1600 loss 34.873562 loss_att 42.360233 loss_ctc 54.423721 loss_rnnt 30.656183 hw_loss 0.212546 lr 0.00084567 rank 3
2023-02-17 08:57:35,404 DEBUG TRAIN Batch 4/1600 loss 22.724514 loss_att 26.366474 loss_ctc 31.442692 loss_rnnt 20.703339 hw_loss 0.244422 lr 0.00084563 rank 0
2023-02-17 08:57:35,437 DEBUG TRAIN Batch 4/1600 loss 17.840656 loss_att 28.213837 loss_ctc 40.854649 loss_rnnt 12.578910 hw_loss 0.222332 lr 0.00084564 rank 5
2023-02-17 08:57:35,442 DEBUG TRAIN Batch 4/1600 loss 12.748569 loss_att 19.854202 loss_ctc 22.967552 loss_rnnt 9.874117 hw_loss 0.170237 lr 0.00084594 rank 6
2023-02-17 08:58:52,817 DEBUG TRAIN Batch 4/1700 loss 20.386587 loss_att 29.324661 loss_ctc 35.606941 loss_rnnt 16.421322 hw_loss 0.278007 lr 0.00084442 rank 0
2023-02-17 08:58:52,834 DEBUG TRAIN Batch 4/1700 loss 26.470152 loss_att 32.572712 loss_ctc 43.631821 loss_rnnt 22.777042 hw_loss 0.345704 lr 0.00084447 rank 3
2023-02-17 08:58:52,871 DEBUG TRAIN Batch 4/1700 loss 37.490437 loss_att 47.332096 loss_ctc 62.319023 loss_rnnt 32.108124 hw_loss 0.194062 lr 0.00084443 rank 5
2023-02-17 08:58:52,872 DEBUG TRAIN Batch 4/1700 loss 16.955103 loss_att 21.821501 loss_ctc 25.529205 loss_rnnt 14.728339 hw_loss 0.206757 lr 0.00084473 rank 6
2023-02-17 08:58:52,874 DEBUG TRAIN Batch 4/1700 loss 24.662020 loss_att 29.542925 loss_ctc 47.540520 loss_rnnt 20.515686 hw_loss 0.224413 lr 0.00084438 rank 1
2023-02-17 08:58:52,878 DEBUG TRAIN Batch 4/1700 loss 23.943838 loss_att 29.807335 loss_ctc 38.786884 loss_rnnt 20.621704 hw_loss 0.319426 lr 0.00084485 rank 4
2023-02-17 08:58:52,907 DEBUG TRAIN Batch 4/1700 loss 21.167925 loss_att 26.378048 loss_ctc 39.163803 loss_rnnt 17.570757 hw_loss 0.291923 lr 0.00084488 rank 7
2023-02-17 08:58:52,908 DEBUG TRAIN Batch 4/1700 loss 25.813480 loss_att 30.635267 loss_ctc 43.081684 loss_rnnt 22.421337 hw_loss 0.235051 lr 0.00084382 rank 2
2023-02-17 09:00:24,588 DEBUG TRAIN Batch 4/1800 loss 17.772255 loss_att 22.035454 loss_ctc 29.120209 loss_rnnt 15.292246 hw_loss 0.214327 lr 0.00084318 rank 1
2023-02-17 09:00:24,591 DEBUG TRAIN Batch 4/1800 loss 19.389515 loss_att 24.062000 loss_ctc 30.872541 loss_rnnt 16.758244 hw_loss 0.310692 lr 0.00084367 rank 7
2023-02-17 09:00:24,590 DEBUG TRAIN Batch 4/1800 loss 29.728518 loss_att 30.974400 loss_ctc 45.198322 loss_rnnt 27.271124 hw_loss 0.272957 lr 0.00084353 rank 6
2023-02-17 09:00:24,593 DEBUG TRAIN Batch 4/1800 loss 22.280670 loss_att 22.659706 loss_ctc 30.631180 loss_rnnt 20.878189 hw_loss 0.399886 lr 0.00084365 rank 4
2023-02-17 09:00:24,594 DEBUG TRAIN Batch 4/1800 loss 27.401121 loss_att 34.873184 loss_ctc 43.563709 loss_rnnt 23.588503 hw_loss 0.305986 lr 0.00084262 rank 2
2023-02-17 09:00:24,593 DEBUG TRAIN Batch 4/1800 loss 19.450333 loss_att 24.112803 loss_ctc 35.726059 loss_rnnt 16.128819 hw_loss 0.410481 lr 0.00084323 rank 5
2023-02-17 09:00:24,594 DEBUG TRAIN Batch 4/1800 loss 24.290457 loss_att 30.170616 loss_ctc 37.525085 loss_rnnt 21.186333 hw_loss 0.306517 lr 0.00084322 rank 0
2023-02-17 09:00:24,596 DEBUG TRAIN Batch 4/1800 loss 22.571218 loss_att 26.020908 loss_ctc 37.267101 loss_rnnt 19.762571 hw_loss 0.298609 lr 0.00084327 rank 3
2023-02-17 09:01:41,109 DEBUG TRAIN Batch 4/1900 loss 17.284967 loss_att 26.615353 loss_ctc 30.349836 loss_rnnt 13.601181 hw_loss 0.141987 lr 0.00084207 rank 3
2023-02-17 09:01:41,110 DEBUG TRAIN Batch 4/1900 loss 17.094358 loss_att 17.412943 loss_ctc 25.315216 loss_rnnt 15.758686 hw_loss 0.329698 lr 0.00084142 rank 2
2023-02-17 09:01:41,120 DEBUG TRAIN Batch 4/1900 loss 10.162528 loss_att 10.517903 loss_ctc 14.082729 loss_rnnt 9.396878 hw_loss 0.322277 lr 0.00084233 rank 6
2023-02-17 09:01:41,124 DEBUG TRAIN Batch 4/1900 loss 18.173088 loss_att 18.681614 loss_ctc 26.414497 loss_rnnt 16.759705 hw_loss 0.399047 lr 0.00084247 rank 7
2023-02-17 09:01:41,126 DEBUG TRAIN Batch 4/1900 loss 27.660061 loss_att 30.342421 loss_ctc 35.452160 loss_rnnt 25.954128 hw_loss 0.244715 lr 0.00084245 rank 4
2023-02-17 09:01:41,127 DEBUG TRAIN Batch 4/1900 loss 27.731480 loss_att 30.863094 loss_ctc 44.429970 loss_rnnt 24.784048 hw_loss 0.177459 lr 0.00084198 rank 1
2023-02-17 09:01:41,130 DEBUG TRAIN Batch 4/1900 loss 14.532647 loss_att 18.556087 loss_ctc 21.057713 loss_rnnt 12.669521 hw_loss 0.353306 lr 0.00084202 rank 0
2023-02-17 09:01:41,164 DEBUG TRAIN Batch 4/1900 loss 13.586361 loss_att 13.570086 loss_ctc 16.978870 loss_rnnt 12.933832 hw_loss 0.381464 lr 0.00084203 rank 5
2023-02-17 09:02:56,244 DEBUG TRAIN Batch 4/2000 loss 34.440125 loss_att 40.914749 loss_ctc 53.153393 loss_rnnt 30.492056 hw_loss 0.296330 lr 0.00084024 rank 2
2023-02-17 09:02:56,244 DEBUG TRAIN Batch 4/2000 loss 22.152802 loss_att 28.086222 loss_ctc 29.452398 loss_rnnt 19.839748 hw_loss 0.287044 lr 0.00084128 rank 7
2023-02-17 09:02:56,244 DEBUG TRAIN Batch 4/2000 loss 21.510870 loss_att 30.368443 loss_ctc 30.569098 loss_rnnt 18.360804 hw_loss 0.320230 lr 0.00084083 rank 0
2023-02-17 09:02:56,245 DEBUG TRAIN Batch 4/2000 loss 23.239290 loss_att 27.097191 loss_ctc 36.492889 loss_rnnt 20.568014 hw_loss 0.248532 lr 0.00084114 rank 6
2023-02-17 09:02:56,245 DEBUG TRAIN Batch 4/2000 loss 31.954678 loss_att 35.147354 loss_ctc 50.043148 loss_rnnt 28.768942 hw_loss 0.253885 lr 0.00084084 rank 5
2023-02-17 09:02:56,246 DEBUG TRAIN Batch 4/2000 loss 22.257433 loss_att 27.600571 loss_ctc 38.040329 loss_rnnt 18.999990 hw_loss 0.158306 lr 0.00084088 rank 3
2023-02-17 09:02:56,247 DEBUG TRAIN Batch 4/2000 loss 9.755635 loss_att 14.328307 loss_ctc 12.774452 loss_rnnt 8.354177 hw_loss 0.158278 lr 0.00084079 rank 1
2023-02-17 09:02:56,267 DEBUG TRAIN Batch 4/2000 loss 21.513197 loss_att 23.170979 loss_ctc 31.767956 loss_rnnt 19.670343 hw_loss 0.269989 lr 0.00084126 rank 4
2023-02-17 09:04:18,742 DEBUG TRAIN Batch 4/2100 loss 22.689800 loss_att 28.651756 loss_ctc 39.180523 loss_rnnt 19.107798 hw_loss 0.357836 lr 0.00083965 rank 5
2023-02-17 09:04:18,764 DEBUG TRAIN Batch 4/2100 loss 12.585551 loss_att 17.794392 loss_ctc 24.658827 loss_rnnt 9.777789 hw_loss 0.292919 lr 0.00083961 rank 1
2023-02-17 09:04:18,786 DEBUG TRAIN Batch 4/2100 loss 21.484379 loss_att 24.512888 loss_ctc 28.802902 loss_rnnt 19.707642 hw_loss 0.366059 lr 0.00083969 rank 3
2023-02-17 09:04:18,797 DEBUG TRAIN Batch 4/2100 loss 24.927727 loss_att 30.959358 loss_ctc 40.809582 loss_rnnt 21.439056 hw_loss 0.308928 lr 0.00084007 rank 4
2023-02-17 09:04:18,826 DEBUG TRAIN Batch 4/2100 loss 30.693502 loss_att 38.595409 loss_ctc 54.038002 loss_rnnt 25.842192 hw_loss 0.296863 lr 0.00083905 rank 2
2023-02-17 09:04:18,830 DEBUG TRAIN Batch 4/2100 loss 43.336285 loss_att 45.785614 loss_ctc 58.163677 loss_rnnt 40.749683 hw_loss 0.224524 lr 0.00083995 rank 6
2023-02-17 09:04:18,831 DEBUG TRAIN Batch 4/2100 loss 34.848038 loss_att 40.812897 loss_ctc 58.396435 loss_rnnt 30.398926 hw_loss 0.218159 lr 0.00084009 rank 7
2023-02-17 09:04:18,836 DEBUG TRAIN Batch 4/2100 loss 24.872955 loss_att 32.587029 loss_ctc 44.963707 loss_rnnt 20.483061 hw_loss 0.315590 lr 0.00083964 rank 0
2023-02-17 09:05:44,848 DEBUG TRAIN Batch 4/2200 loss 23.957966 loss_att 27.503292 loss_ctc 39.489922 loss_rnnt 21.052494 hw_loss 0.235271 lr 0.00083877 rank 6
2023-02-17 09:05:44,849 DEBUG TRAIN Batch 4/2200 loss 19.117559 loss_att 22.887203 loss_ctc 32.186283 loss_rnnt 16.459362 hw_loss 0.303321 lr 0.00083851 rank 3
2023-02-17 09:05:44,852 DEBUG TRAIN Batch 4/2200 loss 36.823917 loss_att 40.757431 loss_ctc 61.426453 loss_rnnt 32.586456 hw_loss 0.319543 lr 0.00083889 rank 4
2023-02-17 09:05:44,852 DEBUG TRAIN Batch 4/2200 loss 21.193020 loss_att 27.089657 loss_ctc 38.792931 loss_rnnt 17.541592 hw_loss 0.235214 lr 0.00083847 rank 5
2023-02-17 09:05:44,855 DEBUG TRAIN Batch 4/2200 loss 20.794231 loss_att 26.526035 loss_ctc 32.450283 loss_rnnt 17.905815 hw_loss 0.352345 lr 0.00083846 rank 0
2023-02-17 09:05:44,866 DEBUG TRAIN Batch 4/2200 loss 25.010649 loss_att 29.982567 loss_ctc 37.442451 loss_rnnt 22.186201 hw_loss 0.323420 lr 0.00083891 rank 7
2023-02-17 09:05:44,881 DEBUG TRAIN Batch 4/2200 loss 23.336788 loss_att 27.787762 loss_ctc 40.214489 loss_rnnt 20.052971 hw_loss 0.268624 lr 0.00083787 rank 2
2023-02-17 09:05:44,898 DEBUG TRAIN Batch 4/2200 loss 46.985508 loss_att 52.493225 loss_ctc 74.480217 loss_rnnt 42.059097 hw_loss 0.297950 lr 0.00083843 rank 1
2023-02-17 09:07:00,737 DEBUG TRAIN Batch 4/2300 loss 14.115737 loss_att 20.145620 loss_ctc 26.638575 loss_rnnt 11.123677 hw_loss 0.218197 lr 0.00083670 rank 2
2023-02-17 09:07:00,738 DEBUG TRAIN Batch 4/2300 loss 21.150934 loss_att 28.605324 loss_ctc 37.725739 loss_rnnt 17.304407 hw_loss 0.273140 lr 0.00083728 rank 0
2023-02-17 09:07:00,739 DEBUG TRAIN Batch 4/2300 loss 19.701300 loss_att 23.042755 loss_ctc 31.830189 loss_rnnt 17.270723 hw_loss 0.272059 lr 0.00083771 rank 4
2023-02-17 09:07:00,741 DEBUG TRAIN Batch 4/2300 loss 29.510921 loss_att 33.924690 loss_ctc 45.819870 loss_rnnt 26.352913 hw_loss 0.188864 lr 0.00083733 rank 3
2023-02-17 09:07:00,741 DEBUG TRAIN Batch 4/2300 loss 24.385014 loss_att 25.100939 loss_ctc 40.119865 loss_rnnt 22.028118 hw_loss 0.217001 lr 0.00083773 rank 7
2023-02-17 09:07:00,742 DEBUG TRAIN Batch 4/2300 loss 35.461498 loss_att 38.903267 loss_ctc 54.229614 loss_rnnt 32.127068 hw_loss 0.269368 lr 0.00083730 rank 5
2023-02-17 09:07:00,746 DEBUG TRAIN Batch 4/2300 loss 21.454906 loss_att 28.634064 loss_ctc 38.723083 loss_rnnt 17.575241 hw_loss 0.265143 lr 0.00083725 rank 1
2023-02-17 09:07:00,785 DEBUG TRAIN Batch 4/2300 loss 26.521486 loss_att 31.123421 loss_ctc 39.524673 loss_rnnt 23.741329 hw_loss 0.236277 lr 0.00083759 rank 6
2023-02-17 09:08:17,169 DEBUG TRAIN Batch 4/2400 loss 35.198967 loss_att 40.289394 loss_ctc 42.682632 loss_rnnt 33.049660 hw_loss 0.250131 lr 0.00083613 rank 5
2023-02-17 09:08:17,175 DEBUG TRAIN Batch 4/2400 loss 17.644545 loss_att 23.160013 loss_ctc 28.291386 loss_rnnt 14.940252 hw_loss 0.340540 lr 0.00083608 rank 1
2023-02-17 09:08:17,174 DEBUG TRAIN Batch 4/2400 loss 22.179230 loss_att 25.856663 loss_ctc 38.068180 loss_rnnt 19.137897 hw_loss 0.351224 lr 0.00083611 rank 0
2023-02-17 09:08:17,175 DEBUG TRAIN Batch 4/2400 loss 41.129593 loss_att 45.192589 loss_ctc 67.336899 loss_rnnt 36.718330 hw_loss 0.195664 lr 0.00083656 rank 7
2023-02-17 09:08:17,179 DEBUG TRAIN Batch 4/2400 loss 51.172020 loss_att 55.833351 loss_ctc 84.186394 loss_rnnt 45.750278 hw_loss 0.164165 lr 0.00083553 rank 2
2023-02-17 09:08:17,183 DEBUG TRAIN Batch 4/2400 loss 30.813942 loss_att 36.952957 loss_ctc 43.954437 loss_rnnt 27.710743 hw_loss 0.231244 lr 0.00083653 rank 4
2023-02-17 09:08:17,214 DEBUG TRAIN Batch 4/2400 loss 24.506985 loss_att 31.038090 loss_ctc 42.093342 loss_rnnt 20.699173 hw_loss 0.293889 lr 0.00083616 rank 3
2023-02-17 09:08:17,246 DEBUG TRAIN Batch 4/2400 loss 19.904078 loss_att 24.781055 loss_ctc 34.946999 loss_rnnt 16.814648 hw_loss 0.203080 lr 0.00083642 rank 6
2023-02-17 09:09:50,656 DEBUG TRAIN Batch 4/2500 loss 12.049262 loss_att 13.872673 loss_ctc 18.084080 loss_rnnt 10.698913 hw_loss 0.339422 lr 0.00083499 rank 3
2023-02-17 09:09:50,666 DEBUG TRAIN Batch 4/2500 loss 24.999344 loss_att 26.880741 loss_ctc 37.844326 loss_rnnt 22.720852 hw_loss 0.355405 lr 0.00083525 rank 6
2023-02-17 09:09:50,667 DEBUG TRAIN Batch 4/2500 loss 20.969542 loss_att 24.278770 loss_ctc 28.703972 loss_rnnt 19.074522 hw_loss 0.378595 lr 0.00083491 rank 1
2023-02-17 09:09:50,701 DEBUG TRAIN Batch 4/2500 loss 32.552277 loss_att 36.463100 loss_ctc 49.142696 loss_rnnt 29.399738 hw_loss 0.296840 lr 0.00083539 rank 7
2023-02-17 09:09:50,708 DEBUG TRAIN Batch 4/2500 loss 15.848219 loss_att 17.559170 loss_ctc 23.809994 loss_rnnt 14.243202 hw_loss 0.377354 lr 0.00083496 rank 5
2023-02-17 09:09:50,713 DEBUG TRAIN Batch 4/2500 loss 15.178336 loss_att 18.903225 loss_ctc 25.430031 loss_rnnt 12.944096 hw_loss 0.229444 lr 0.00083495 rank 0
2023-02-17 09:09:50,715 DEBUG TRAIN Batch 4/2500 loss 19.375492 loss_att 26.821409 loss_ctc 34.722206 loss_rnnt 15.701159 hw_loss 0.260477 lr 0.00083537 rank 4
2023-02-17 09:09:50,737 DEBUG TRAIN Batch 4/2500 loss 21.647264 loss_att 22.968134 loss_ctc 30.078007 loss_rnnt 20.061729 hw_loss 0.369862 lr 0.00083437 rank 2
2023-02-17 09:11:07,132 DEBUG TRAIN Batch 4/2600 loss 15.439505 loss_att 20.993212 loss_ctc 24.384903 loss_rnnt 12.992256 hw_loss 0.269601 lr 0.00083321 rank 2
2023-02-17 09:11:07,135 DEBUG TRAIN Batch 4/2600 loss 17.487196 loss_att 23.917164 loss_ctc 34.343876 loss_rnnt 13.801436 hw_loss 0.285390 lr 0.00083420 rank 4
2023-02-17 09:11:07,135 DEBUG TRAIN Batch 4/2600 loss 31.122200 loss_att 36.481750 loss_ctc 50.540737 loss_rnnt 27.343809 hw_loss 0.220017 lr 0.00083383 rank 3
2023-02-17 09:11:07,136 DEBUG TRAIN Batch 4/2600 loss 16.284256 loss_att 24.149155 loss_ctc 27.148016 loss_rnnt 13.165443 hw_loss 0.182495 lr 0.00083409 rank 6
2023-02-17 09:11:07,140 DEBUG TRAIN Batch 4/2600 loss 16.369087 loss_att 19.593681 loss_ctc 23.730429 loss_rnnt 14.582251 hw_loss 0.300761 lr 0.00083380 rank 5
2023-02-17 09:11:07,141 DEBUG TRAIN Batch 4/2600 loss 18.678099 loss_att 25.571278 loss_ctc 36.109570 loss_rnnt 14.791652 hw_loss 0.344277 lr 0.00083423 rank 7
2023-02-17 09:11:07,142 DEBUG TRAIN Batch 4/2600 loss 11.945646 loss_att 20.019806 loss_ctc 22.880871 loss_rnnt 8.748809 hw_loss 0.232455 lr 0.00083375 rank 1
2023-02-17 09:11:07,143 DEBUG TRAIN Batch 4/2600 loss 14.535883 loss_att 15.940343 loss_ctc 19.541092 loss_rnnt 13.427855 hw_loss 0.299579 lr 0.00083379 rank 0
2023-02-17 09:12:22,585 DEBUG TRAIN Batch 4/2700 loss 24.910484 loss_att 33.672318 loss_ctc 35.522530 loss_rnnt 21.652237 hw_loss 0.170512 lr 0.00083263 rank 0
2023-02-17 09:12:22,588 DEBUG TRAIN Batch 4/2700 loss 16.745840 loss_att 25.205406 loss_ctc 28.516516 loss_rnnt 13.362483 hw_loss 0.228787 lr 0.00083307 rank 7
2023-02-17 09:12:22,589 DEBUG TRAIN Batch 4/2700 loss 11.165474 loss_att 18.197311 loss_ctc 20.502285 loss_rnnt 8.380729 hw_loss 0.250255 lr 0.00083259 rank 1
2023-02-17 09:12:22,589 DEBUG TRAIN Batch 4/2700 loss 26.794203 loss_att 29.392574 loss_ctc 40.456741 loss_rnnt 24.328640 hw_loss 0.232912 lr 0.00083304 rank 4
2023-02-17 09:12:22,590 DEBUG TRAIN Batch 4/2700 loss 20.634048 loss_att 27.951733 loss_ctc 29.864243 loss_rnnt 17.845552 hw_loss 0.176749 lr 0.00083205 rank 2
2023-02-17 09:12:22,591 DEBUG TRAIN Batch 4/2700 loss 28.892469 loss_att 35.148342 loss_ctc 54.354759 loss_rnnt 24.190769 hw_loss 0.104161 lr 0.00083267 rank 3
2023-02-17 09:12:22,600 DEBUG TRAIN Batch 4/2700 loss 19.733263 loss_att 26.642712 loss_ctc 32.611450 loss_rnnt 16.482552 hw_loss 0.284494 lr 0.00083293 rank 6
2023-02-17 09:12:22,631 DEBUG TRAIN Batch 4/2700 loss 32.303944 loss_att 39.529243 loss_ctc 59.900040 loss_rnnt 27.046438 hw_loss 0.249309 lr 0.00083264 rank 5
2023-02-17 09:13:44,636 DEBUG TRAIN Batch 4/2800 loss 17.094458 loss_att 21.419125 loss_ctc 28.481056 loss_rnnt 14.531788 hw_loss 0.336602 lr 0.00083178 rank 6
2023-02-17 09:13:44,657 DEBUG TRAIN Batch 4/2800 loss 32.553490 loss_att 39.241623 loss_ctc 56.942665 loss_rnnt 27.882404 hw_loss 0.152927 lr 0.00083152 rank 3
2023-02-17 09:13:44,664 DEBUG TRAIN Batch 4/2800 loss 18.146088 loss_att 23.692751 loss_ctc 24.318996 loss_rnnt 16.034662 hw_loss 0.335693 lr 0.00083191 rank 7
2023-02-17 09:13:44,669 DEBUG TRAIN Batch 4/2800 loss 57.594425 loss_att 61.362583 loss_ctc 79.122620 loss_rnnt 53.806450 hw_loss 0.307354 lr 0.00083144 rank 1
2023-02-17 09:13:44,673 DEBUG TRAIN Batch 4/2800 loss 59.952293 loss_att 55.028694 loss_ctc 86.850189 loss_rnnt 57.153038 hw_loss 0.370484 lr 0.00083148 rank 0
2023-02-17 09:13:44,679 DEBUG TRAIN Batch 4/2800 loss 50.259228 loss_att 55.241550 loss_ctc 73.642029 loss_rnnt 45.933319 hw_loss 0.397002 lr 0.00083090 rank 2
2023-02-17 09:13:44,719 DEBUG TRAIN Batch 4/2800 loss 23.934219 loss_att 31.755905 loss_ctc 40.368473 loss_rnnt 20.002560 hw_loss 0.330165 lr 0.00083149 rank 5
2023-02-17 09:13:44,722 DEBUG TRAIN Batch 4/2800 loss 19.856726 loss_att 23.310335 loss_ctc 29.792828 loss_rnnt 17.650229 hw_loss 0.358057 lr 0.00083189 rank 4
2023-02-17 09:15:12,995 DEBUG TRAIN Batch 4/2900 loss 36.074795 loss_att 36.085087 loss_ctc 47.052078 loss_rnnt 34.533009 hw_loss 0.142667 lr 0.00083063 rank 6
2023-02-17 09:15:13,004 DEBUG TRAIN Batch 4/2900 loss 19.932846 loss_att 24.771055 loss_ctc 35.182320 loss_rnnt 16.822203 hw_loss 0.205762 lr 0.00083076 rank 7
2023-02-17 09:15:13,006 DEBUG TRAIN Batch 4/2900 loss 55.584286 loss_att 61.046928 loss_ctc 82.395058 loss_rnnt 50.771095 hw_loss 0.273542 lr 0.00083033 rank 0
2023-02-17 09:15:13,010 DEBUG TRAIN Batch 4/2900 loss 33.452785 loss_att 34.807381 loss_ctc 52.850563 loss_rnnt 30.480934 hw_loss 0.214800 lr 0.00083074 rank 4
2023-02-17 09:15:13,012 DEBUG TRAIN Batch 4/2900 loss 28.884232 loss_att 35.889072 loss_ctc 45.730156 loss_rnnt 25.123837 hw_loss 0.212450 lr 0.00082976 rank 2
2023-02-17 09:15:13,033 DEBUG TRAIN Batch 4/2900 loss 32.958748 loss_att 38.851166 loss_ctc 52.689743 loss_rnnt 29.061348 hw_loss 0.165215 lr 0.00083037 rank 3
2023-02-17 09:15:13,044 DEBUG TRAIN Batch 4/2900 loss 36.772987 loss_att 45.357746 loss_ctc 54.359100 loss_rnnt 32.600258 hw_loss 0.208049 lr 0.00083034 rank 5
2023-02-17 09:15:13,052 DEBUG TRAIN Batch 4/2900 loss 31.657633 loss_att 40.461124 loss_ctc 48.366669 loss_rnnt 27.570255 hw_loss 0.185263 lr 0.00083029 rank 1
2023-02-17 09:16:29,038 DEBUG TRAIN Batch 4/3000 loss 28.309189 loss_att 34.857239 loss_ctc 43.101257 loss_rnnt 24.917995 hw_loss 0.204956 lr 0.00082919 rank 0
2023-02-17 09:16:29,038 DEBUG TRAIN Batch 4/3000 loss 17.417139 loss_att 24.431704 loss_ctc 24.959440 loss_rnnt 14.839730 hw_loss 0.316604 lr 0.00082862 rank 2
2023-02-17 09:16:29,039 DEBUG TRAIN Batch 4/3000 loss 18.859850 loss_att 26.069891 loss_ctc 31.868622 loss_rnnt 15.567737 hw_loss 0.216756 lr 0.00082962 rank 7
2023-02-17 09:16:29,040 DEBUG TRAIN Batch 4/3000 loss 14.885575 loss_att 17.053320 loss_ctc 22.779627 loss_rnnt 13.270504 hw_loss 0.241839 lr 0.00082920 rank 5
2023-02-17 09:16:29,045 DEBUG TRAIN Batch 4/3000 loss 21.313898 loss_att 25.919109 loss_ctc 32.953766 loss_rnnt 18.724857 hw_loss 0.217530 lr 0.00082915 rank 1
2023-02-17 09:16:29,044 DEBUG TRAIN Batch 4/3000 loss 12.427150 loss_att 18.007311 loss_ctc 27.333454 loss_rnnt 9.193796 hw_loss 0.243402 lr 0.00082923 rank 3
2023-02-17 09:16:29,069 DEBUG TRAIN Batch 4/3000 loss 16.876383 loss_att 20.707863 loss_ctc 23.313847 loss_rnnt 15.094290 hw_loss 0.295251 lr 0.00082948 rank 6
2023-02-17 09:16:29,095 DEBUG TRAIN Batch 4/3000 loss 21.977392 loss_att 25.875639 loss_ctc 35.838646 loss_rnnt 19.222805 hw_loss 0.237695 lr 0.00082960 rank 4
2023-02-17 09:17:45,672 DEBUG TRAIN Batch 4/3100 loss 17.558582 loss_att 23.974949 loss_ctc 28.076080 loss_rnnt 14.748583 hw_loss 0.233236 lr 0.00082834 rank 6
2023-02-17 09:17:45,672 DEBUG TRAIN Batch 4/3100 loss 17.634802 loss_att 21.534199 loss_ctc 25.508501 loss_rnnt 15.649400 hw_loss 0.291930 lr 0.00082846 rank 4
2023-02-17 09:17:45,672 DEBUG TRAIN Batch 4/3100 loss 29.234600 loss_att 26.797586 loss_ctc 41.417835 loss_rnnt 27.916962 hw_loss 0.338642 lr 0.00082801 rank 1
2023-02-17 09:17:45,674 DEBUG TRAIN Batch 4/3100 loss 26.647606 loss_att 31.337864 loss_ctc 44.681599 loss_rnnt 23.185682 hw_loss 0.223761 lr 0.00082805 rank 0
2023-02-17 09:17:45,677 DEBUG TRAIN Batch 4/3100 loss 19.727402 loss_att 23.356745 loss_ctc 33.244705 loss_rnnt 17.053858 hw_loss 0.272566 lr 0.00082748 rank 2
2023-02-17 09:17:45,691 DEBUG TRAIN Batch 4/3100 loss 26.357767 loss_att 32.978050 loss_ctc 39.844120 loss_rnnt 23.079321 hw_loss 0.292896 lr 0.00082806 rank 5
2023-02-17 09:17:45,724 DEBUG TRAIN Batch 4/3100 loss 16.491064 loss_att 19.334694 loss_ctc 27.874571 loss_rnnt 14.329020 hw_loss 0.141590 lr 0.00082809 rank 3
2023-02-17 09:17:45,731 DEBUG TRAIN Batch 4/3100 loss 24.367292 loss_att 27.472385 loss_ctc 36.831310 loss_rnnt 21.895130 hw_loss 0.354885 lr 0.00082848 rank 7
2023-02-17 09:19:18,126 DEBUG TRAIN Batch 4/3200 loss 22.418133 loss_att 33.891872 loss_ctc 32.594666 loss_rnnt 18.650959 hw_loss 0.216667 lr 0.00082688 rank 1
2023-02-17 09:19:18,130 DEBUG TRAIN Batch 4/3200 loss 20.981497 loss_att 20.849121 loss_ctc 28.227383 loss_rnnt 19.830414 hw_loss 0.396448 lr 0.00082735 rank 7
2023-02-17 09:19:18,131 DEBUG TRAIN Batch 4/3200 loss 22.500042 loss_att 22.872482 loss_ctc 31.052656 loss_rnnt 21.090672 hw_loss 0.364752 lr 0.00082693 rank 5
2023-02-17 09:19:18,134 DEBUG TRAIN Batch 4/3200 loss 19.070250 loss_att 34.481972 loss_ctc 28.201675 loss_rnnt 14.672865 hw_loss 0.182842 lr 0.00082732 rank 4
2023-02-17 09:19:18,136 DEBUG TRAIN Batch 4/3200 loss 26.976599 loss_att 33.710560 loss_ctc 43.763840 loss_rnnt 23.234793 hw_loss 0.293840 lr 0.00082721 rank 6
2023-02-17 09:19:18,139 DEBUG TRAIN Batch 4/3200 loss 17.660664 loss_att 19.427834 loss_ctc 29.824087 loss_rnnt 15.499688 hw_loss 0.348281 lr 0.00082696 rank 3
2023-02-17 09:19:18,173 DEBUG TRAIN Batch 4/3200 loss 14.160429 loss_att 16.529482 loss_ctc 18.010584 loss_rnnt 12.976211 hw_loss 0.369475 lr 0.00082692 rank 0
2023-02-17 09:19:18,175 DEBUG TRAIN Batch 4/3200 loss 26.383413 loss_att 35.049793 loss_ctc 46.340057 loss_rnnt 21.865917 hw_loss 0.231253 lr 0.00082635 rank 2
2023-02-17 09:20:33,177 DEBUG TRAIN Batch 4/3300 loss 17.741196 loss_att 24.775928 loss_ctc 27.760834 loss_rnnt 14.831085 hw_loss 0.313523 lr 0.00082580 rank 5
2023-02-17 09:20:33,177 DEBUG TRAIN Batch 4/3300 loss 18.804737 loss_att 21.267056 loss_ctc 31.512487 loss_rnnt 16.500603 hw_loss 0.219946 lr 0.00082621 rank 7
2023-02-17 09:20:33,178 DEBUG TRAIN Batch 4/3300 loss 34.169827 loss_att 41.720608 loss_ctc 48.054062 loss_rnnt 30.665922 hw_loss 0.267217 lr 0.00082583 rank 3
2023-02-17 09:20:33,180 DEBUG TRAIN Batch 4/3300 loss 16.772135 loss_att 22.582518 loss_ctc 25.544579 loss_rnnt 14.277287 hw_loss 0.305841 lr 0.00082608 rank 6
2023-02-17 09:20:33,184 DEBUG TRAIN Batch 4/3300 loss 27.049639 loss_att 29.098209 loss_ctc 41.248775 loss_rnnt 24.583431 hw_loss 0.306143 lr 0.00082579 rank 0
2023-02-17 09:20:33,186 DEBUG TRAIN Batch 4/3300 loss 22.661377 loss_att 28.068148 loss_ctc 34.166763 loss_rnnt 19.926821 hw_loss 0.223410 lr 0.00082522 rank 2
2023-02-17 09:20:33,186 DEBUG TRAIN Batch 4/3300 loss 28.692631 loss_att 34.731853 loss_ctc 50.420204 loss_rnnt 24.436409 hw_loss 0.283813 lr 0.00082575 rank 1
2023-02-17 09:20:33,189 DEBUG TRAIN Batch 4/3300 loss 35.436394 loss_att 40.021168 loss_ctc 53.815575 loss_rnnt 31.941916 hw_loss 0.238069 lr 0.00082619 rank 4
2023-02-17 09:21:49,559 DEBUG TRAIN Batch 4/3400 loss 27.240316 loss_att 29.264565 loss_ctc 36.188522 loss_rnnt 25.430660 hw_loss 0.396962 lr 0.00082509 rank 7
2023-02-17 09:21:49,560 DEBUG TRAIN Batch 4/3400 loss 11.557755 loss_att 18.150896 loss_ctc 23.003174 loss_rnnt 8.569727 hw_loss 0.268768 lr 0.00082467 rank 5
2023-02-17 09:21:49,561 DEBUG TRAIN Batch 4/3400 loss 20.353283 loss_att 27.757435 loss_ctc 37.504650 loss_rnnt 16.372532 hw_loss 0.399512 lr 0.00082466 rank 0
2023-02-17 09:21:49,562 DEBUG TRAIN Batch 4/3400 loss 42.108482 loss_att 45.801201 loss_ctc 62.453152 loss_rnnt 38.534771 hw_loss 0.229773 lr 0.00082410 rank 2
2023-02-17 09:21:49,563 DEBUG TRAIN Batch 4/3400 loss 18.227703 loss_att 26.352442 loss_ctc 31.595541 loss_rnnt 14.657763 hw_loss 0.304899 lr 0.00082463 rank 1
2023-02-17 09:21:49,565 DEBUG TRAIN Batch 4/3400 loss 18.748510 loss_att 22.204058 loss_ctc 30.042400 loss_rnnt 16.446762 hw_loss 0.196478 lr 0.00082495 rank 6
2023-02-17 09:21:49,564 DEBUG TRAIN Batch 4/3400 loss 38.308041 loss_att 51.402893 loss_ctc 62.314178 loss_rnnt 32.411560 hw_loss 0.143799 lr 0.00082507 rank 4
2023-02-17 09:21:49,565 DEBUG TRAIN Batch 4/3400 loss 12.513275 loss_att 21.120382 loss_ctc 23.959854 loss_rnnt 9.139133 hw_loss 0.237204 lr 0.00082471 rank 3
2023-02-17 09:23:10,095 DEBUG TRAIN Batch 4/3500 loss 24.827978 loss_att 27.996311 loss_ctc 40.414593 loss_rnnt 21.978987 hw_loss 0.257077 lr 0.00082351 rank 1
2023-02-17 09:23:10,095 DEBUG TRAIN Batch 4/3500 loss 17.036003 loss_att 20.747068 loss_ctc 26.985409 loss_rnnt 14.848497 hw_loss 0.222572 lr 0.00082397 rank 7
2023-02-17 09:23:10,115 DEBUG TRAIN Batch 4/3500 loss 36.026001 loss_att 38.459930 loss_ctc 49.109566 loss_rnnt 33.671013 hw_loss 0.231994 lr 0.00082395 rank 4
2023-02-17 09:23:10,117 DEBUG TRAIN Batch 4/3500 loss 14.629132 loss_att 22.418276 loss_ctc 23.951950 loss_rnnt 11.700000 hw_loss 0.240491 lr 0.00082359 rank 3
2023-02-17 09:23:10,134 DEBUG TRAIN Batch 4/3500 loss 26.512054 loss_att 32.810349 loss_ctc 45.281540 loss_rnnt 22.574223 hw_loss 0.329201 lr 0.00082355 rank 5
2023-02-17 09:23:10,134 DEBUG TRAIN Batch 4/3500 loss 28.834143 loss_att 35.065037 loss_ctc 44.997009 loss_rnnt 25.309952 hw_loss 0.230556 lr 0.00082383 rank 6
2023-02-17 09:23:10,137 DEBUG TRAIN Batch 4/3500 loss 28.003983 loss_att 30.765957 loss_ctc 46.079277 loss_rnnt 24.844595 hw_loss 0.369293 lr 0.00082299 rank 2
2023-02-17 09:23:10,144 DEBUG TRAIN Batch 4/3500 loss 27.289982 loss_att 30.746258 loss_ctc 42.083515 loss_rnnt 24.505550 hw_loss 0.226326 lr 0.00082354 rank 0
2023-02-17 09:24:40,514 DEBUG TRAIN Batch 4/3600 loss 22.090929 loss_att 25.331249 loss_ctc 29.520866 loss_rnnt 20.352623 hw_loss 0.186716 lr 0.00082240 rank 1
2023-02-17 09:24:40,514 DEBUG TRAIN Batch 4/3600 loss 25.427973 loss_att 29.016617 loss_ctc 34.972885 loss_rnnt 23.270399 hw_loss 0.313484 lr 0.00082285 rank 7
2023-02-17 09:24:40,515 DEBUG TRAIN Batch 4/3600 loss 27.062580 loss_att 29.071175 loss_ctc 44.371460 loss_rnnt 24.198463 hw_loss 0.289773 lr 0.00082187 rank 2
2023-02-17 09:24:40,523 DEBUG TRAIN Batch 4/3600 loss 32.354931 loss_att 40.338421 loss_ctc 45.974678 loss_rnnt 28.846029 hw_loss 0.180443 lr 0.00082243 rank 0
2023-02-17 09:24:40,548 DEBUG TRAIN Batch 4/3600 loss 14.316298 loss_att 18.772387 loss_ctc 26.046066 loss_rnnt 11.755109 hw_loss 0.198754 lr 0.00082244 rank 5
2023-02-17 09:24:40,553 DEBUG TRAIN Batch 4/3600 loss 17.993034 loss_att 23.621811 loss_ctc 29.987123 loss_rnnt 15.100409 hw_loss 0.314362 lr 0.00082247 rank 3
2023-02-17 09:24:40,562 DEBUG TRAIN Batch 4/3600 loss 24.622953 loss_att 30.509262 loss_ctc 34.742821 loss_rnnt 21.957926 hw_loss 0.259597 lr 0.00082283 rank 4
2023-02-17 09:24:40,572 DEBUG TRAIN Batch 4/3600 loss 16.841642 loss_att 21.313288 loss_ctc 30.328140 loss_rnnt 14.066526 hw_loss 0.154852 lr 0.00082272 rank 6
2023-02-17 09:25:56,061 DEBUG TRAIN Batch 4/3700 loss 22.209888 loss_att 34.598610 loss_ctc 45.991764 loss_rnnt 16.351337 hw_loss 0.393545 lr 0.00082161 rank 6
2023-02-17 09:25:56,073 DEBUG TRAIN Batch 4/3700 loss 17.076668 loss_att 22.762318 loss_ctc 38.042980 loss_rnnt 12.991296 hw_loss 0.286374 lr 0.00082136 rank 3
2023-02-17 09:25:56,075 DEBUG TRAIN Batch 4/3700 loss 30.664669 loss_att 35.559261 loss_ctc 48.655125 loss_rnnt 27.125959 hw_loss 0.301994 lr 0.00082128 rank 1
2023-02-17 09:25:56,077 DEBUG TRAIN Batch 4/3700 loss 35.253429 loss_att 38.156811 loss_ctc 43.903744 loss_rnnt 33.361347 hw_loss 0.296306 lr 0.00082172 rank 4
2023-02-17 09:25:56,078 DEBUG TRAIN Batch 4/3700 loss 12.874904 loss_att 16.996628 loss_ctc 24.979755 loss_rnnt 10.346588 hw_loss 0.168732 lr 0.00082076 rank 2
2023-02-17 09:25:56,083 DEBUG TRAIN Batch 4/3700 loss 31.465826 loss_att 35.490959 loss_ctc 46.719711 loss_rnnt 28.481167 hw_loss 0.273341 lr 0.00082133 rank 5
2023-02-17 09:25:56,088 DEBUG TRAIN Batch 4/3700 loss 22.092474 loss_att 26.682041 loss_ctc 32.714775 loss_rnnt 19.601959 hw_loss 0.293056 lr 0.00082132 rank 0
2023-02-17 09:25:56,135 DEBUG TRAIN Batch 4/3700 loss 25.536072 loss_att 26.600357 loss_ctc 35.303921 loss_rnnt 23.854273 hw_loss 0.312303 lr 0.00082174 rank 7
2023-02-17 09:27:12,203 DEBUG TRAIN Batch 4/3800 loss 20.047331 loss_att 20.975290 loss_ctc 28.609364 loss_rnnt 18.536221 hw_loss 0.344840 lr 0.00082050 rank 6
2023-02-17 09:27:12,203 DEBUG TRAIN Batch 4/3800 loss 17.791580 loss_att 24.205139 loss_ctc 27.305145 loss_rnnt 15.085372 hw_loss 0.290664 lr 0.00082021 rank 0
2023-02-17 09:27:12,208 DEBUG TRAIN Batch 4/3800 loss 21.220724 loss_att 25.763235 loss_ctc 33.654751 loss_rnnt 18.523138 hw_loss 0.246029 lr 0.00082022 rank 5
2023-02-17 09:27:12,210 DEBUG TRAIN Batch 4/3800 loss 12.510488 loss_att 16.207968 loss_ctc 18.930178 loss_rnnt 10.777354 hw_loss 0.258147 lr 0.00082018 rank 1
2023-02-17 09:27:12,214 DEBUG TRAIN Batch 4/3800 loss 24.309479 loss_att 31.115103 loss_ctc 47.397095 loss_rnnt 19.710714 hw_loss 0.298671 lr 0.00081966 rank 2
2023-02-17 09:27:12,218 DEBUG TRAIN Batch 4/3800 loss 18.188311 loss_att 27.659765 loss_ctc 26.942905 loss_rnnt 14.995005 hw_loss 0.247007 lr 0.00082061 rank 4
2023-02-17 09:27:12,237 DEBUG TRAIN Batch 4/3800 loss 21.378498 loss_att 23.193588 loss_ctc 33.207989 loss_rnnt 19.310148 hw_loss 0.240124 lr 0.00082063 rank 7
2023-02-17 09:27:12,251 DEBUG TRAIN Batch 4/3800 loss 32.806705 loss_att 33.821857 loss_ctc 46.182842 loss_rnnt 30.574745 hw_loss 0.460203 lr 0.00082026 rank 3
2023-02-17 09:28:37,871 DEBUG TRAIN Batch 4/3900 loss 25.262402 loss_att 32.274715 loss_ctc 47.820343 loss_rnnt 20.704380 hw_loss 0.277186 lr 0.00081912 rank 5
2023-02-17 09:28:37,887 DEBUG TRAIN Batch 4/3900 loss 31.430420 loss_att 36.817123 loss_ctc 49.186825 loss_rnnt 27.867390 hw_loss 0.221566 lr 0.00081908 rank 1
2023-02-17 09:28:37,912 DEBUG TRAIN Batch 4/3900 loss 9.251409 loss_att 17.127922 loss_ctc 15.150019 loss_rnnt 6.741010 hw_loss 0.278652 lr 0.00081915 rank 3
2023-02-17 09:28:37,917 DEBUG TRAIN Batch 4/3900 loss 23.513662 loss_att 31.058025 loss_ctc 38.072433 loss_rnnt 19.898024 hw_loss 0.310495 lr 0.00081951 rank 4
2023-02-17 09:28:37,919 DEBUG TRAIN Batch 4/3900 loss 25.067781 loss_att 28.306007 loss_ctc 28.505569 loss_rnnt 23.834440 hw_loss 0.238732 lr 0.00081940 rank 6
2023-02-17 09:28:37,921 DEBUG TRAIN Batch 4/3900 loss 31.142664 loss_att 35.142590 loss_ctc 42.743523 loss_rnnt 28.643169 hw_loss 0.286364 lr 0.00081856 rank 2
2023-02-17 09:28:37,922 DEBUG TRAIN Batch 4/3900 loss 12.463881 loss_att 20.568851 loss_ctc 26.364187 loss_rnnt 8.853432 hw_loss 0.255149 lr 0.00081953 rank 7
2023-02-17 09:28:37,940 DEBUG TRAIN Batch 4/3900 loss 18.294582 loss_att 24.772455 loss_ctc 30.003691 loss_rnnt 15.306849 hw_loss 0.245517 lr 0.00081911 rank 0
2023-02-17 09:30:02,814 DEBUG TRAIN Batch 4/4000 loss 17.223776 loss_att 25.893473 loss_ctc 29.646124 loss_rnnt 13.713524 hw_loss 0.224997 lr 0.00081801 rank 0
2023-02-17 09:30:02,815 DEBUG TRAIN Batch 4/4000 loss 25.281425 loss_att 30.537165 loss_ctc 36.087151 loss_rnnt 22.647160 hw_loss 0.266916 lr 0.00081798 rank 1
2023-02-17 09:30:02,815 DEBUG TRAIN Batch 4/4000 loss 6.437677 loss_att 13.104189 loss_ctc 16.108091 loss_rnnt 3.726460 hw_loss 0.165986 lr 0.00081802 rank 5
2023-02-17 09:30:02,815 DEBUG TRAIN Batch 4/4000 loss 31.711445 loss_att 38.771545 loss_ctc 37.662395 loss_rnnt 29.363827 hw_loss 0.266515 lr 0.00081747 rank 2
2023-02-17 09:30:02,817 DEBUG TRAIN Batch 4/4000 loss 20.999441 loss_att 28.088036 loss_ctc 32.659950 loss_rnnt 17.905827 hw_loss 0.227176 lr 0.00081806 rank 3
2023-02-17 09:30:02,821 DEBUG TRAIN Batch 4/4000 loss 31.593239 loss_att 38.278214 loss_ctc 49.023560 loss_rnnt 27.871519 hw_loss 0.113777 lr 0.00081843 rank 7
2023-02-17 09:30:02,849 DEBUG TRAIN Batch 4/4000 loss 44.327698 loss_att 52.062489 loss_ctc 65.168793 loss_rnnt 39.879028 hw_loss 0.230436 lr 0.00081841 rank 4
2023-02-17 09:30:02,877 DEBUG TRAIN Batch 4/4000 loss 29.821184 loss_att 31.118959 loss_ctc 48.916286 loss_rnnt 26.917700 hw_loss 0.183594 lr 0.00081830 rank 6
2023-02-17 09:31:18,096 DEBUG TRAIN Batch 4/4100 loss 37.214298 loss_att 45.224041 loss_ctc 57.349194 loss_rnnt 32.776459 hw_loss 0.283566 lr 0.00081734 rank 7
2023-02-17 09:31:18,098 DEBUG TRAIN Batch 4/4100 loss 18.610342 loss_att 24.902954 loss_ctc 29.662134 loss_rnnt 15.778347 hw_loss 0.187312 lr 0.00081689 rank 1
2023-02-17 09:31:18,097 DEBUG TRAIN Batch 4/4100 loss 26.589859 loss_att 35.217999 loss_ctc 41.842621 loss_rnnt 22.707470 hw_loss 0.230740 lr 0.00081638 rank 2
2023-02-17 09:31:18,101 DEBUG TRAIN Batch 4/4100 loss 15.810013 loss_att 20.307535 loss_ctc 26.558168 loss_rnnt 13.354896 hw_loss 0.229736 lr 0.00081731 rank 4
2023-02-17 09:31:18,123 DEBUG TRAIN Batch 4/4100 loss 21.283024 loss_att 25.479967 loss_ctc 36.333622 loss_rnnt 18.329279 hw_loss 0.201767 lr 0.00081692 rank 0
2023-02-17 09:31:18,125 DEBUG TRAIN Batch 4/4100 loss 20.180586 loss_att 24.807468 loss_ctc 27.278606 loss_rnnt 18.220919 hw_loss 0.164785 lr 0.00081721 rank 6
2023-02-17 09:31:18,138 DEBUG TRAIN Batch 4/4100 loss 21.150185 loss_att 35.575336 loss_ctc 35.956818 loss_rnnt 16.184244 hw_loss 0.200045 lr 0.00081697 rank 3
2023-02-17 09:31:18,147 DEBUG TRAIN Batch 4/4100 loss 27.851400 loss_att 34.766731 loss_ctc 41.448647 loss_rnnt 24.543472 hw_loss 0.209802 lr 0.00081693 rank 5
2023-02-17 09:32:37,324 DEBUG TRAIN Batch 4/4200 loss 27.029905 loss_att 30.136909 loss_ctc 44.025860 loss_rnnt 23.989388 hw_loss 0.286857 lr 0.00081588 rank 3
2023-02-17 09:32:37,329 DEBUG TRAIN Batch 4/4200 loss 24.604214 loss_att 34.315376 loss_ctc 39.778900 loss_rnnt 20.496584 hw_loss 0.266445 lr 0.00081625 rank 7
2023-02-17 09:32:37,330 DEBUG TRAIN Batch 4/4200 loss 25.538635 loss_att 29.438513 loss_ctc 42.831512 loss_rnnt 22.313725 hw_loss 0.261030 lr 0.00081584 rank 5
2023-02-17 09:32:37,352 DEBUG TRAIN Batch 4/4200 loss 19.555397 loss_att 24.985729 loss_ctc 30.763432 loss_rnnt 16.771353 hw_loss 0.381697 lr 0.00081580 rank 1
2023-02-17 09:32:37,361 DEBUG TRAIN Batch 4/4200 loss 25.262110 loss_att 32.585693 loss_ctc 38.991417 loss_rnnt 21.821819 hw_loss 0.271879 lr 0.00081612 rank 6
2023-02-17 09:32:37,372 DEBUG TRAIN Batch 4/4200 loss 25.440464 loss_att 35.449062 loss_ctc 50.172363 loss_rnnt 19.998745 hw_loss 0.267025 lr 0.00081622 rank 4
2023-02-17 09:32:37,373 DEBUG TRAIN Batch 4/4200 loss 16.331049 loss_att 20.446476 loss_ctc 27.748419 loss_rnnt 13.890686 hw_loss 0.178051 lr 0.00081529 rank 2
2023-02-17 09:32:37,413 DEBUG TRAIN Batch 4/4200 loss 26.625187 loss_att 33.137096 loss_ctc 41.090630 loss_rnnt 23.308233 hw_loss 0.160959 lr 0.00081583 rank 0
2023-02-17 09:34:08,359 DEBUG TRAIN Batch 4/4300 loss 24.728334 loss_att 28.259865 loss_ctc 42.757488 loss_rnnt 21.440170 hw_loss 0.333694 lr 0.00081421 rank 2
2023-02-17 09:34:08,361 DEBUG TRAIN Batch 4/4300 loss 25.763779 loss_att 32.098816 loss_ctc 42.228104 loss_rnnt 22.185411 hw_loss 0.217714 lr 0.00081479 rank 3
2023-02-17 09:34:08,364 DEBUG TRAIN Batch 4/4300 loss 16.229130 loss_att 21.969791 loss_ctc 22.401260 loss_rnnt 14.082464 hw_loss 0.329217 lr 0.00081475 rank 0
2023-02-17 09:34:08,370 DEBUG TRAIN Batch 4/4300 loss 21.045389 loss_att 26.025639 loss_ctc 32.651367 loss_rnnt 18.342739 hw_loss 0.298379 lr 0.00081472 rank 1
2023-02-17 09:34:08,395 DEBUG TRAIN Batch 4/4300 loss 18.571289 loss_att 23.428774 loss_ctc 29.916126 loss_rnnt 15.986712 hw_loss 0.188319 lr 0.00081503 rank 6
2023-02-17 09:34:08,405 DEBUG TRAIN Batch 4/4300 loss 26.190430 loss_att 28.940639 loss_ctc 35.262676 loss_rnnt 24.295408 hw_loss 0.253771 lr 0.00081514 rank 4
2023-02-17 09:34:08,415 DEBUG TRAIN Batch 4/4300 loss 28.808649 loss_att 31.285450 loss_ctc 42.793304 loss_rnnt 26.304585 hw_loss 0.270150 lr 0.00081476 rank 5
2023-02-17 09:34:08,432 DEBUG TRAIN Batch 4/4300 loss 28.002087 loss_att 32.696510 loss_ctc 45.689327 loss_rnnt 24.620781 hw_loss 0.157727 lr 0.00081516 rank 7
2023-02-17 09:35:25,383 DEBUG TRAIN Batch 4/4400 loss 9.962347 loss_att 10.678401 loss_ctc 13.745561 loss_rnnt 9.109685 hw_loss 0.384420 lr 0.00081313 rank 2
2023-02-17 09:35:25,383 DEBUG TRAIN Batch 4/4400 loss 22.485575 loss_att 26.175686 loss_ctc 34.958164 loss_rnnt 19.936792 hw_loss 0.277025 lr 0.00081395 rank 6
2023-02-17 09:35:25,386 DEBUG TRAIN Batch 4/4400 loss 44.000946 loss_att 53.082100 loss_ctc 65.044632 loss_rnnt 39.227440 hw_loss 0.283977 lr 0.00081406 rank 4
2023-02-17 09:35:25,386 DEBUG TRAIN Batch 4/4400 loss 18.865761 loss_att 21.332863 loss_ctc 31.151340 loss_rnnt 16.577257 hw_loss 0.294387 lr 0.00081371 rank 3
2023-02-17 09:35:25,387 DEBUG TRAIN Batch 4/4400 loss 24.593676 loss_att 29.610378 loss_ctc 38.695004 loss_rnnt 21.592999 hw_loss 0.219674 lr 0.00081408 rank 7
2023-02-17 09:35:25,388 DEBUG TRAIN Batch 4/4400 loss 23.083439 loss_att 24.854103 loss_ctc 37.083916 loss_rnnt 20.751839 hw_loss 0.207633 lr 0.00081364 rank 1
2023-02-17 09:35:25,391 DEBUG TRAIN Batch 4/4400 loss 18.919910 loss_att 25.355083 loss_ctc 32.897785 loss_rnnt 15.640021 hw_loss 0.242131 lr 0.00081368 rank 5
2023-02-17 09:35:25,415 DEBUG TRAIN Batch 4/4400 loss 38.497856 loss_att 39.701244 loss_ctc 50.739967 loss_rnnt 36.492870 hw_loss 0.247551 lr 0.00081367 rank 0
2023-02-17 09:36:40,963 DEBUG TRAIN Batch 4/4500 loss 24.346840 loss_att 28.627811 loss_ctc 41.165459 loss_rnnt 21.122757 hw_loss 0.235134 lr 0.00081287 rank 6
2023-02-17 09:36:40,965 DEBUG TRAIN Batch 4/4500 loss 35.432541 loss_att 41.824741 loss_ctc 56.075745 loss_rnnt 31.276646 hw_loss 0.234421 lr 0.00081256 rank 1
2023-02-17 09:36:40,967 DEBUG TRAIN Batch 4/4500 loss 19.649588 loss_att 29.293499 loss_ctc 32.397331 loss_rnnt 15.916306 hw_loss 0.196500 lr 0.00081264 rank 3
2023-02-17 09:36:40,968 DEBUG TRAIN Batch 4/4500 loss 24.454611 loss_att 31.619820 loss_ctc 37.540138 loss_rnnt 21.123713 hw_loss 0.287099 lr 0.00081298 rank 4
2023-02-17 09:36:40,972 DEBUG TRAIN Batch 4/4500 loss 11.756102 loss_att 13.054443 loss_ctc 16.476274 loss_rnnt 10.673331 hw_loss 0.363275 lr 0.00081259 rank 0
2023-02-17 09:36:41,010 DEBUG TRAIN Batch 4/4500 loss 30.753321 loss_att 31.234514 loss_ctc 38.781292 loss_rnnt 29.368568 hw_loss 0.408971 lr 0.00081300 rank 7
2023-02-17 09:36:41,035 DEBUG TRAIN Batch 4/4500 loss 37.603474 loss_att 43.829948 loss_ctc 54.545422 loss_rnnt 33.984062 hw_loss 0.215979 lr 0.00081206 rank 2
2023-02-17 09:36:41,044 DEBUG TRAIN Batch 4/4500 loss 12.798154 loss_att 15.863000 loss_ctc 20.341316 loss_rnnt 11.015850 hw_loss 0.306711 lr 0.00081261 rank 5
2023-02-17 09:38:05,518 DEBUG TRAIN Batch 4/4600 loss 18.711781 loss_att 25.597097 loss_ctc 28.455107 loss_rnnt 15.901812 hw_loss 0.250862 lr 0.00081193 rank 7
2023-02-17 09:38:05,532 DEBUG TRAIN Batch 4/4600 loss 12.804472 loss_att 19.611954 loss_ctc 22.830242 loss_rnnt 9.954365 hw_loss 0.284701 lr 0.00081180 rank 6
2023-02-17 09:38:05,536 DEBUG TRAIN Batch 4/4600 loss 26.639610 loss_att 39.795841 loss_ctc 62.069458 loss_rnnt 19.097486 hw_loss 0.350437 lr 0.00081152 rank 0
2023-02-17 09:38:05,551 DEBUG TRAIN Batch 4/4600 loss 19.686850 loss_att 26.163002 loss_ctc 29.211956 loss_rnnt 16.959507 hw_loss 0.303933 lr 0.00081149 rank 1
2023-02-17 09:38:05,555 DEBUG TRAIN Batch 4/4600 loss 40.168610 loss_att 43.365192 loss_ctc 60.989426 loss_rnnt 36.558105 hw_loss 0.365769 lr 0.00081157 rank 3
2023-02-17 09:38:05,563 DEBUG TRAIN Batch 4/4600 loss 15.718946 loss_att 17.908220 loss_ctc 25.396378 loss_rnnt 13.840850 hw_loss 0.281096 lr 0.00081099 rank 2
2023-02-17 09:38:05,577 DEBUG TRAIN Batch 4/4600 loss 13.735899 loss_att 20.626423 loss_ctc 24.555334 loss_rnnt 10.796791 hw_loss 0.222019 lr 0.00081153 rank 5
2023-02-17 09:38:05,619 DEBUG TRAIN Batch 4/4600 loss 33.925831 loss_att 44.530060 loss_ctc 58.137924 loss_rnnt 28.449820 hw_loss 0.237917 lr 0.00081191 rank 4
2023-02-17 09:39:31,919 DEBUG TRAIN Batch 4/4700 loss 19.131725 loss_att 23.421368 loss_ctc 29.011002 loss_rnnt 16.821705 hw_loss 0.252852 lr 0.00081073 rank 6
2023-02-17 09:39:31,919 DEBUG TRAIN Batch 4/4700 loss 25.950787 loss_att 34.453987 loss_ctc 40.608910 loss_rnnt 22.183182 hw_loss 0.211030 lr 0.00081086 rank 7
2023-02-17 09:39:31,920 DEBUG TRAIN Batch 4/4700 loss 22.792582 loss_att 30.058281 loss_ctc 35.924889 loss_rnnt 19.416582 hw_loss 0.322278 lr 0.00081042 rank 1
2023-02-17 09:39:31,924 DEBUG TRAIN Batch 4/4700 loss 28.145443 loss_att 31.614964 loss_ctc 37.585556 loss_rnnt 26.060627 hw_loss 0.247925 lr 0.00081046 rank 0
2023-02-17 09:39:31,926 DEBUG TRAIN Batch 4/4700 loss 17.771441 loss_att 22.568689 loss_ctc 31.009087 loss_rnnt 14.891378 hw_loss 0.291741 lr 0.00080993 rank 2
2023-02-17 09:39:31,928 DEBUG TRAIN Batch 4/4700 loss 35.281223 loss_att 43.202271 loss_ctc 54.172390 loss_rnnt 30.988869 hw_loss 0.354987 lr 0.00081084 rank 4
2023-02-17 09:39:31,957 DEBUG TRAIN Batch 4/4700 loss 29.219725 loss_att 35.189938 loss_ctc 48.646797 loss_rnnt 25.323475 hw_loss 0.209872 lr 0.00081050 rank 3
2023-02-17 09:39:31,974 DEBUG TRAIN Batch 4/4700 loss 12.819282 loss_att 19.371647 loss_ctc 21.050810 loss_rnnt 10.220996 hw_loss 0.356766 lr 0.00081047 rank 5
2023-02-17 09:40:47,961 DEBUG TRAIN Batch 4/4800 loss 33.031673 loss_att 40.822166 loss_ctc 56.621586 loss_rnnt 28.218105 hw_loss 0.206530 lr 0.00080944 rank 3
2023-02-17 09:40:47,969 DEBUG TRAIN Batch 4/4800 loss 18.885180 loss_att 32.883408 loss_ctc 29.954155 loss_rnnt 14.497042 hw_loss 0.211176 lr 0.00080967 rank 6
2023-02-17 09:40:47,970 DEBUG TRAIN Batch 4/4800 loss 24.586384 loss_att 29.966784 loss_ctc 46.274467 loss_rnnt 20.532047 hw_loss 0.162212 lr 0.00080940 rank 5
2023-02-17 09:40:47,972 DEBUG TRAIN Batch 4/4800 loss 25.495516 loss_att 31.314674 loss_ctc 44.063873 loss_rnnt 21.700577 hw_loss 0.291237 lr 0.00080980 rank 7
2023-02-17 09:40:47,972 DEBUG TRAIN Batch 4/4800 loss 33.395248 loss_att 35.773552 loss_ctc 43.465839 loss_rnnt 31.404211 hw_loss 0.323687 lr 0.00080978 rank 4
2023-02-17 09:40:47,985 DEBUG TRAIN Batch 4/4800 loss 32.554737 loss_att 32.953354 loss_ctc 44.495106 loss_rnnt 30.744234 hw_loss 0.260116 lr 0.00080936 rank 1
2023-02-17 09:40:48,000 DEBUG TRAIN Batch 4/4800 loss 19.436239 loss_att 23.600574 loss_ctc 35.315445 loss_rnnt 16.329981 hw_loss 0.292808 lr 0.00080939 rank 0
2023-02-17 09:40:48,002 DEBUG TRAIN Batch 4/4800 loss 20.624550 loss_att 25.370701 loss_ctc 28.719038 loss_rnnt 18.471699 hw_loss 0.233168 lr 0.00080886 rank 2
2023-02-17 09:42:06,405 DEBUG TRAIN Batch 4/4900 loss 16.294319 loss_att 24.668608 loss_ctc 32.315239 loss_rnnt 12.320650 hw_loss 0.305043 lr 0.00080874 rank 7
2023-02-17 09:42:06,405 DEBUG TRAIN Batch 4/4900 loss 21.242712 loss_att 27.092615 loss_ctc 35.506145 loss_rnnt 18.018734 hw_loss 0.285388 lr 0.00080838 rank 3
2023-02-17 09:42:06,426 DEBUG TRAIN Batch 4/4900 loss 22.825117 loss_att 30.650517 loss_ctc 43.197845 loss_rnnt 18.394535 hw_loss 0.279631 lr 0.00080781 rank 2
2023-02-17 09:42:06,447 DEBUG TRAIN Batch 4/4900 loss 27.030355 loss_att 28.054007 loss_ctc 38.226181 loss_rnnt 25.179846 hw_loss 0.286881 lr 0.00080861 rank 6
2023-02-17 09:42:06,451 DEBUG TRAIN Batch 4/4900 loss 12.220765 loss_att 18.594614 loss_ctc 22.912201 loss_rnnt 9.340763 hw_loss 0.336951 lr 0.00080830 rank 1
2023-02-17 09:42:06,454 DEBUG TRAIN Batch 4/4900 loss 18.641279 loss_att 22.721991 loss_ctc 31.428244 loss_rnnt 16.051880 hw_loss 0.128118 lr 0.00080834 rank 0
2023-02-17 09:42:06,481 DEBUG TRAIN Batch 4/4900 loss 21.150206 loss_att 26.030691 loss_ctc 39.759148 loss_rnnt 17.537903 hw_loss 0.290652 lr 0.00080835 rank 5
2023-02-17 09:42:06,483 DEBUG TRAIN Batch 4/4900 loss 24.909138 loss_att 27.767117 loss_ctc 39.374779 loss_rnnt 22.269712 hw_loss 0.260772 lr 0.00080872 rank 4
2023-02-17 09:43:38,553 DEBUG TRAIN Batch 4/5000 loss 17.519833 loss_att 23.594616 loss_ctc 32.126167 loss_rnnt 14.184159 hw_loss 0.324758 lr 0.00080728 rank 0
2023-02-17 09:43:38,555 DEBUG TRAIN Batch 4/5000 loss 21.619116 loss_att 29.249866 loss_ctc 32.742851 loss_rnnt 18.453810 hw_loss 0.292486 lr 0.00080756 rank 6
2023-02-17 09:43:38,579 DEBUG TRAIN Batch 4/5000 loss 26.069788 loss_att 29.185226 loss_ctc 42.270710 loss_rnnt 23.229187 hw_loss 0.107601 lr 0.00080768 rank 7
2023-02-17 09:43:38,580 DEBUG TRAIN Batch 4/5000 loss 20.530128 loss_att 23.148254 loss_ctc 34.001060 loss_rnnt 18.042585 hw_loss 0.314611 lr 0.00080725 rank 1
2023-02-17 09:43:38,581 DEBUG TRAIN Batch 4/5000 loss 23.438124 loss_att 26.904591 loss_ctc 36.370682 loss_rnnt 20.866842 hw_loss 0.288090 lr 0.00080676 rank 2
2023-02-17 09:43:38,587 DEBUG TRAIN Batch 4/5000 loss 17.923729 loss_att 22.944038 loss_ctc 29.280264 loss_rnnt 15.255420 hw_loss 0.281329 lr 0.00080732 rank 3
2023-02-17 09:43:38,594 DEBUG TRAIN Batch 4/5000 loss 15.211220 loss_att 17.858650 loss_ctc 21.844784 loss_rnnt 13.605729 hw_loss 0.359117 lr 0.00080766 rank 4
2023-02-17 09:43:38,612 DEBUG TRAIN Batch 4/5000 loss 25.100346 loss_att 27.965681 loss_ctc 40.153721 loss_rnnt 22.395140 hw_loss 0.234417 lr 0.00080729 rank 5
2023-02-17 09:44:54,839 DEBUG TRAIN Batch 4/5100 loss 28.187944 loss_att 27.873108 loss_ctc 43.852261 loss_rnnt 26.038752 hw_loss 0.231717 lr 0.00080624 rank 5
2023-02-17 09:44:54,840 DEBUG TRAIN Batch 4/5100 loss 13.354772 loss_att 14.371167 loss_ctc 17.247423 loss_rnnt 12.454787 hw_loss 0.333160 lr 0.00080650 rank 6
2023-02-17 09:44:54,840 DEBUG TRAIN Batch 4/5100 loss 21.409161 loss_att 22.993389 loss_ctc 31.300602 loss_rnnt 19.588554 hw_loss 0.346691 lr 0.00080623 rank 0
2023-02-17 09:44:54,841 DEBUG TRAIN Batch 4/5100 loss 19.749763 loss_att 23.733467 loss_ctc 31.737720 loss_rnnt 17.180113 hw_loss 0.327214 lr 0.00080663 rank 7
2023-02-17 09:44:54,841 DEBUG TRAIN Batch 4/5100 loss 21.993727 loss_att 28.859406 loss_ctc 39.007034 loss_rnnt 18.232910 hw_loss 0.223576 lr 0.00080571 rank 2
2023-02-17 09:44:54,867 DEBUG TRAIN Batch 4/5100 loss 19.383286 loss_att 25.861973 loss_ctc 32.561119 loss_rnnt 16.222237 hw_loss 0.203000 lr 0.00080620 rank 1
2023-02-17 09:44:54,872 DEBUG TRAIN Batch 4/5100 loss 24.065260 loss_att 23.173023 loss_ctc 35.707405 loss_rnnt 22.550306 hw_loss 0.264592 lr 0.00080661 rank 4
2023-02-17 09:44:54,881 DEBUG TRAIN Batch 4/5100 loss 16.754005 loss_att 17.455902 loss_ctc 26.588064 loss_rnnt 15.089069 hw_loss 0.400026 lr 0.00080627 rank 3
2023-02-17 09:46:10,409 DEBUG TRAIN Batch 4/5200 loss 24.864754 loss_att 33.261230 loss_ctc 42.810925 loss_rnnt 20.662344 hw_loss 0.244293 lr 0.00080546 rank 6
2023-02-17 09:46:10,420 DEBUG TRAIN Batch 4/5200 loss 8.566260 loss_att 17.052914 loss_ctc 19.717354 loss_rnnt 5.207794 hw_loss 0.326855 lr 0.00080556 rank 4
2023-02-17 09:46:10,422 DEBUG TRAIN Batch 4/5200 loss 42.562202 loss_att 49.065819 loss_ctc 63.760742 loss_rnnt 38.324978 hw_loss 0.206303 lr 0.00080523 rank 3
2023-02-17 09:46:10,425 DEBUG TRAIN Batch 4/5200 loss 18.916437 loss_att 25.561735 loss_ctc 28.761429 loss_rnnt 16.157856 hw_loss 0.219103 lr 0.00080520 rank 5
2023-02-17 09:46:10,426 DEBUG TRAIN Batch 4/5200 loss 24.908506 loss_att 31.460337 loss_ctc 32.900265 loss_rnnt 22.391079 hw_loss 0.265301 lr 0.00080519 rank 0
2023-02-17 09:46:10,429 DEBUG TRAIN Batch 4/5200 loss 14.446888 loss_att 18.624285 loss_ctc 19.908642 loss_rnnt 12.732574 hw_loss 0.282376 lr 0.00080515 rank 1
2023-02-17 09:46:10,463 DEBUG TRAIN Batch 4/5200 loss 24.315933 loss_att 29.565464 loss_ctc 35.889019 loss_rnnt 21.599958 hw_loss 0.230610 lr 0.00080466 rank 2
2023-02-17 09:46:10,470 DEBUG TRAIN Batch 4/5200 loss 22.609352 loss_att 27.587185 loss_ctc 37.059380 loss_rnnt 19.577782 hw_loss 0.205000 lr 0.00080558 rank 7
2023-02-17 09:47:34,024 DEBUG TRAIN Batch 4/5300 loss 12.624704 loss_att 19.984066 loss_ctc 21.848850 loss_rnnt 9.742893 hw_loss 0.337597 lr 0.00080362 rank 2
2023-02-17 09:47:34,024 DEBUG TRAIN Batch 4/5300 loss 23.831324 loss_att 31.108198 loss_ctc 45.265221 loss_rnnt 19.377987 hw_loss 0.262702 lr 0.00080441 rank 6
2023-02-17 09:47:34,027 DEBUG TRAIN Batch 4/5300 loss 25.564238 loss_att 28.742201 loss_ctc 40.544998 loss_rnnt 22.794460 hw_loss 0.256405 lr 0.00080418 rank 3
2023-02-17 09:47:34,044 DEBUG TRAIN Batch 4/5300 loss 21.219128 loss_att 28.221481 loss_ctc 39.728531 loss_rnnt 17.215919 hw_loss 0.252783 lr 0.00080415 rank 5
2023-02-17 09:47:34,063 DEBUG TRAIN Batch 4/5300 loss 15.304100 loss_att 17.185192 loss_ctc 19.145798 loss_rnnt 14.234576 hw_loss 0.339525 lr 0.00080411 rank 1
2023-02-17 09:47:34,067 DEBUG TRAIN Batch 4/5300 loss 22.085447 loss_att 24.473045 loss_ctc 29.389709 loss_rnnt 20.505718 hw_loss 0.240574 lr 0.00080414 rank 0
2023-02-17 09:47:34,089 DEBUG TRAIN Batch 4/5300 loss 27.422939 loss_att 37.388187 loss_ctc 44.992371 loss_rnnt 22.909569 hw_loss 0.333242 lr 0.00080454 rank 7
2023-02-17 09:47:34,096 DEBUG TRAIN Batch 4/5300 loss 27.664692 loss_att 28.948740 loss_ctc 43.028690 loss_rnnt 25.224712 hw_loss 0.252443 lr 0.00080452 rank 4
2023-02-17 09:49:03,228 DEBUG TRAIN Batch 4/5400 loss 25.595734 loss_att 32.634842 loss_ctc 39.623398 loss_rnnt 22.191168 hw_loss 0.236980 lr 0.00080350 rank 7
2023-02-17 09:49:03,228 DEBUG TRAIN Batch 4/5400 loss 21.704992 loss_att 22.868591 loss_ctc 32.468643 loss_rnnt 19.895521 hw_loss 0.265498 lr 0.00080307 rank 1
2023-02-17 09:49:03,229 DEBUG TRAIN Batch 4/5400 loss 33.613171 loss_att 42.950813 loss_ctc 50.262009 loss_rnnt 29.392241 hw_loss 0.250416 lr 0.00080311 rank 0
2023-02-17 09:49:03,231 DEBUG TRAIN Batch 4/5400 loss 24.592604 loss_att 28.509686 loss_ctc 34.973114 loss_rnnt 22.292622 hw_loss 0.248430 lr 0.00080337 rank 6
2023-02-17 09:49:03,232 DEBUG TRAIN Batch 4/5400 loss 21.894670 loss_att 29.793745 loss_ctc 37.315342 loss_rnnt 18.134739 hw_loss 0.232550 lr 0.00080259 rank 2
2023-02-17 09:49:03,232 DEBUG TRAIN Batch 4/5400 loss 25.896536 loss_att 32.579990 loss_ctc 39.936989 loss_rnnt 22.532583 hw_loss 0.291006 lr 0.00080348 rank 4
2023-02-17 09:49:03,237 DEBUG TRAIN Batch 4/5400 loss 23.775824 loss_att 29.473446 loss_ctc 41.299328 loss_rnnt 20.116440 hw_loss 0.343860 lr 0.00080312 rank 5
2023-02-17 09:49:03,279 DEBUG TRAIN Batch 4/5400 loss 21.617010 loss_att 26.405811 loss_ctc 29.145052 loss_rnnt 19.559731 hw_loss 0.179588 lr 0.00080315 rank 3
2023-02-17 09:50:19,736 DEBUG TRAIN Batch 4/5500 loss 24.759405 loss_att 28.243214 loss_ctc 37.997093 loss_rnnt 22.166004 hw_loss 0.246773 lr 0.00080244 rank 4
2023-02-17 09:50:19,737 DEBUG TRAIN Batch 4/5500 loss 15.836333 loss_att 20.978123 loss_ctc 28.331440 loss_rnnt 12.977402 hw_loss 0.308551 lr 0.00080211 rank 3
2023-02-17 09:50:19,741 DEBUG TRAIN Batch 4/5500 loss 27.858654 loss_att 33.036308 loss_ctc 41.606400 loss_rnnt 24.897045 hw_loss 0.174461 lr 0.00080246 rank 7
2023-02-17 09:50:19,743 DEBUG TRAIN Batch 4/5500 loss 19.147270 loss_att 27.487331 loss_ctc 38.730717 loss_rnnt 14.740406 hw_loss 0.239485 lr 0.00080156 rank 2
2023-02-17 09:50:19,752 DEBUG TRAIN Batch 4/5500 loss 18.313450 loss_att 24.447903 loss_ctc 29.858585 loss_rnnt 15.422064 hw_loss 0.234649 lr 0.00080208 rank 5
2023-02-17 09:50:19,763 DEBUG TRAIN Batch 4/5500 loss 19.176445 loss_att 24.200043 loss_ctc 30.606358 loss_rnnt 16.551456 hw_loss 0.180525 lr 0.00080234 rank 6
2023-02-17 09:50:19,763 DEBUG TRAIN Batch 4/5500 loss 26.378105 loss_att 27.150345 loss_ctc 34.513500 loss_rnnt 24.982033 hw_loss 0.294199 lr 0.00080207 rank 0
2023-02-17 09:50:19,789 DEBUG TRAIN Batch 4/5500 loss 17.118872 loss_att 22.089373 loss_ctc 29.275925 loss_rnnt 14.386331 hw_loss 0.220312 lr 0.00080204 rank 1
2023-02-17 09:51:37,122 DEBUG TRAIN Batch 4/5600 loss 25.359266 loss_att 27.017366 loss_ctc 39.158295 loss_rnnt 23.046717 hw_loss 0.264488 lr 0.00080053 rank 2
2023-02-17 09:51:37,137 DEBUG TRAIN Batch 4/5600 loss 35.826336 loss_att 44.084770 loss_ctc 54.437889 loss_rnnt 31.581734 hw_loss 0.208822 lr 0.00080108 rank 3
2023-02-17 09:51:37,141 DEBUG TRAIN Batch 4/5600 loss 22.684711 loss_att 24.352051 loss_ctc 36.339344 loss_rnnt 20.398945 hw_loss 0.246899 lr 0.00080141 rank 4
2023-02-17 09:51:37,144 DEBUG TRAIN Batch 4/5600 loss 25.407991 loss_att 28.302412 loss_ctc 39.809845 loss_rnnt 22.801691 hw_loss 0.200942 lr 0.00080131 rank 6
2023-02-17 09:51:37,170 DEBUG TRAIN Batch 4/5600 loss 14.596749 loss_att 21.006756 loss_ctc 23.650654 loss_rnnt 11.915740 hw_loss 0.359664 lr 0.00080101 rank 1
2023-02-17 09:51:37,174 DEBUG TRAIN Batch 4/5600 loss 26.466530 loss_att 28.637280 loss_ctc 42.804024 loss_rnnt 23.720251 hw_loss 0.250863 lr 0.00080104 rank 0
2023-02-17 09:51:37,200 DEBUG TRAIN Batch 4/5600 loss 25.461975 loss_att 29.542126 loss_ctc 42.437252 loss_rnnt 22.225355 hw_loss 0.294790 lr 0.00080105 rank 5
2023-02-17 09:51:37,206 DEBUG TRAIN Batch 4/5600 loss 17.121029 loss_att 24.236191 loss_ctc 28.587591 loss_rnnt 14.029538 hw_loss 0.261715 lr 0.00080143 rank 7
2023-02-17 09:53:10,631 DEBUG TRAIN Batch 4/5700 loss 18.206394 loss_att 18.942650 loss_ctc 25.306847 loss_rnnt 17.014668 hw_loss 0.183277 lr 0.00080040 rank 7
2023-02-17 09:53:10,637 DEBUG TRAIN Batch 4/5700 loss 14.608274 loss_att 18.333836 loss_ctc 28.762396 loss_rnnt 11.764728 hw_loss 0.396035 lr 0.00080006 rank 3
2023-02-17 09:53:10,637 DEBUG TRAIN Batch 4/5700 loss 29.889574 loss_att 28.594696 loss_ctc 35.963741 loss_rnnt 29.171432 hw_loss 0.313554 lr 0.00080002 rank 0
2023-02-17 09:53:10,648 DEBUG TRAIN Batch 4/5700 loss 26.991434 loss_att 33.690369 loss_ctc 38.346462 loss_rnnt 23.968292 hw_loss 0.317533 lr 0.00080028 rank 6
2023-02-17 09:53:10,649 DEBUG TRAIN Batch 4/5700 loss 10.255538 loss_att 15.900097 loss_ctc 16.782803 loss_rnnt 8.087411 hw_loss 0.316713 lr 0.00079998 rank 1
2023-02-17 09:53:10,651 DEBUG TRAIN Batch 4/5700 loss 33.008064 loss_att 29.004204 loss_ctc 47.132977 loss_rnnt 31.843102 hw_loss 0.154526 lr 0.00080038 rank 4
2023-02-17 09:53:10,653 DEBUG TRAIN Batch 4/5700 loss 11.455828 loss_att 14.678851 loss_ctc 22.374750 loss_rnnt 9.267341 hw_loss 0.165047 lr 0.00080003 rank 5
2023-02-17 09:53:10,674 DEBUG TRAIN Batch 4/5700 loss 12.645219 loss_att 19.300142 loss_ctc 22.368170 loss_rnnt 9.888208 hw_loss 0.243061 lr 0.00079950 rank 2
2023-02-17 09:54:26,543 DEBUG TRAIN Batch 4/5800 loss 17.292521 loss_att 20.677389 loss_ctc 24.406776 loss_rnnt 15.598949 hw_loss 0.127554 lr 0.00079926 rank 6
2023-02-17 09:54:26,547 DEBUG TRAIN Batch 4/5800 loss 17.852659 loss_att 26.053890 loss_ctc 31.216625 loss_rnnt 14.280823 hw_loss 0.280743 lr 0.00079936 rank 4
2023-02-17 09:54:26,550 DEBUG TRAIN Batch 4/5800 loss 34.335491 loss_att 40.212395 loss_ctc 50.778206 loss_rnnt 30.852901 hw_loss 0.215338 lr 0.00079848 rank 2
2023-02-17 09:54:26,552 DEBUG TRAIN Batch 4/5800 loss 13.551188 loss_att 13.802858 loss_ctc 16.943878 loss_rnnt 12.893146 hw_loss 0.291280 lr 0.00079938 rank 7
2023-02-17 09:54:26,551 DEBUG TRAIN Batch 4/5800 loss 26.366657 loss_att 38.892212 loss_ctc 35.093102 loss_rnnt 22.544460 hw_loss 0.287928 lr 0.00079896 rank 1
2023-02-17 09:54:26,557 DEBUG TRAIN Batch 4/5800 loss 21.731890 loss_att 23.731615 loss_ctc 29.747961 loss_rnnt 20.111675 hw_loss 0.283990 lr 0.00079900 rank 5
2023-02-17 09:54:26,568 DEBUG TRAIN Batch 4/5800 loss 19.803213 loss_att 26.876385 loss_ctc 28.034262 loss_rnnt 17.190611 hw_loss 0.188429 lr 0.00079903 rank 3
2023-02-17 09:54:26,601 DEBUG TRAIN Batch 4/5800 loss 20.138460 loss_att 29.961781 loss_ctc 34.016182 loss_rnnt 16.147297 hw_loss 0.330251 lr 0.00079899 rank 0
2023-02-17 09:55:42,317 DEBUG TRAIN Batch 4/5900 loss 12.879196 loss_att 15.847116 loss_ctc 24.432835 loss_rnnt 10.622056 hw_loss 0.230756 lr 0.00079824 rank 6
2023-02-17 09:55:42,321 DEBUG TRAIN Batch 4/5900 loss 20.872034 loss_att 32.467941 loss_ctc 27.905516 loss_rnnt 17.417522 hw_loss 0.370372 lr 0.00079747 rank 2
2023-02-17 09:55:42,321 DEBUG TRAIN Batch 4/5900 loss 26.245924 loss_att 33.295456 loss_ctc 45.461071 loss_rnnt 22.139975 hw_loss 0.251295 lr 0.00079794 rank 1
2023-02-17 09:55:42,321 DEBUG TRAIN Batch 4/5900 loss 21.107374 loss_att 26.068085 loss_ctc 35.265724 loss_rnnt 18.107204 hw_loss 0.225465 lr 0.00079836 rank 7
2023-02-17 09:55:42,328 DEBUG TRAIN Batch 4/5900 loss 24.671583 loss_att 31.382109 loss_ctc 42.281693 loss_rnnt 20.870026 hw_loss 0.208947 lr 0.00079834 rank 4
2023-02-17 09:55:42,329 DEBUG TRAIN Batch 4/5900 loss 33.666409 loss_att 42.682030 loss_ctc 45.146942 loss_rnnt 30.174519 hw_loss 0.296295 lr 0.00079799 rank 5
2023-02-17 09:55:42,336 DEBUG TRAIN Batch 4/5900 loss 27.769075 loss_att 29.526257 loss_ctc 43.410332 loss_rnnt 25.235510 hw_loss 0.181173 lr 0.00079802 rank 3
2023-02-17 09:55:42,353 DEBUG TRAIN Batch 4/5900 loss 24.878212 loss_att 30.381741 loss_ctc 40.957466 loss_rnnt 21.482830 hw_loss 0.282706 lr 0.00079798 rank 0
2023-02-17 09:57:04,083 DEBUG TRAIN Batch 4/6000 loss 35.009472 loss_att 42.399269 loss_ctc 52.689682 loss_rnnt 31.013115 hw_loss 0.301947 lr 0.00079735 rank 7
2023-02-17 09:57:04,082 DEBUG TRAIN Batch 4/6000 loss 29.990425 loss_att 34.221134 loss_ctc 42.189308 loss_rnnt 27.369268 hw_loss 0.278434 lr 0.00079696 rank 0
2023-02-17 09:57:04,097 DEBUG TRAIN Batch 4/6000 loss 31.306149 loss_att 40.507942 loss_ctc 47.575562 loss_rnnt 27.146629 hw_loss 0.281070 lr 0.00079646 rank 2
2023-02-17 09:57:04,120 DEBUG TRAIN Batch 4/6000 loss 26.648861 loss_att 31.338448 loss_ctc 33.596603 loss_rnnt 24.666128 hw_loss 0.222089 lr 0.00079722 rank 6
2023-02-17 09:57:04,121 DEBUG TRAIN Batch 4/6000 loss 19.667513 loss_att 28.706455 loss_ctc 42.783379 loss_rnnt 14.646164 hw_loss 0.246464 lr 0.00079693 rank 1
2023-02-17 09:57:04,154 DEBUG TRAIN Batch 4/6000 loss 26.415268 loss_att 32.423782 loss_ctc 45.378761 loss_rnnt 22.534340 hw_loss 0.282673 lr 0.00079697 rank 5
2023-02-17 09:57:04,155 DEBUG TRAIN Batch 4/6000 loss 34.809055 loss_att 45.314266 loss_ctc 53.501762 loss_rnnt 30.159069 hw_loss 0.106097 lr 0.00079700 rank 3
2023-02-17 09:57:04,174 DEBUG TRAIN Batch 4/6000 loss 21.147202 loss_att 28.214329 loss_ctc 31.689243 loss_rnnt 18.169003 hw_loss 0.298437 lr 0.00079733 rank 4
2023-02-17 09:58:32,565 DEBUG TRAIN Batch 4/6100 loss 16.935446 loss_att 19.605089 loss_ctc 27.832695 loss_rnnt 14.839174 hw_loss 0.205077 lr 0.00079592 rank 1
2023-02-17 09:58:32,573 DEBUG TRAIN Batch 4/6100 loss 20.996368 loss_att 22.157726 loss_ctc 34.440872 loss_rnnt 18.860111 hw_loss 0.208844 lr 0.00079621 rank 6
2023-02-17 09:58:32,575 DEBUG TRAIN Batch 4/6100 loss 17.651957 loss_att 22.344578 loss_ctc 26.355230 loss_rnnt 15.432913 hw_loss 0.225160 lr 0.00079599 rank 3
2023-02-17 09:58:32,576 DEBUG TRAIN Batch 4/6100 loss 30.899137 loss_att 33.355080 loss_ctc 46.348141 loss_rnnt 28.225018 hw_loss 0.230746 lr 0.00079596 rank 5
2023-02-17 09:58:32,576 DEBUG TRAIN Batch 4/6100 loss 28.419224 loss_att 30.427893 loss_ctc 42.000481 loss_rnnt 26.030119 hw_loss 0.331007 lr 0.00079633 rank 7
2023-02-17 09:58:32,578 DEBUG TRAIN Batch 4/6100 loss 21.507614 loss_att 24.549940 loss_ctc 29.667374 loss_rnnt 19.683319 hw_loss 0.239740 lr 0.00079545 rank 2
2023-02-17 09:58:32,580 DEBUG TRAIN Batch 4/6100 loss 23.728239 loss_att 27.238876 loss_ctc 35.025391 loss_rnnt 21.366299 hw_loss 0.287861 lr 0.00079595 rank 0
2023-02-17 09:58:32,585 DEBUG TRAIN Batch 4/6100 loss 19.422117 loss_att 24.524914 loss_ctc 34.679337 loss_rnnt 16.209547 hw_loss 0.295714 lr 0.00079631 rank 4
2023-02-17 09:59:48,055 DEBUG TRAIN Batch 4/6200 loss 31.272245 loss_att 33.449600 loss_ctc 38.975193 loss_rnnt 29.714090 hw_loss 0.179291 lr 0.00079521 rank 6
2023-02-17 09:59:48,058 DEBUG TRAIN Batch 4/6200 loss 23.637508 loss_att 26.860874 loss_ctc 37.394165 loss_rnnt 21.033649 hw_loss 0.234307 lr 0.00079531 rank 4
2023-02-17 09:59:48,060 DEBUG TRAIN Batch 4/6200 loss 12.639906 loss_att 14.434448 loss_ctc 20.827280 loss_rnnt 11.062354 hw_loss 0.238114 lr 0.00079495 rank 5
2023-02-17 09:59:48,061 DEBUG TRAIN Batch 4/6200 loss 21.395649 loss_att 24.085678 loss_ctc 31.635225 loss_rnnt 19.349995 hw_loss 0.266946 lr 0.00079498 rank 3
2023-02-17 09:59:48,062 DEBUG TRAIN Batch 4/6200 loss 28.379675 loss_att 32.853683 loss_ctc 37.646824 loss_rnnt 26.105503 hw_loss 0.269529 lr 0.00079491 rank 1
2023-02-17 09:59:48,066 DEBUG TRAIN Batch 4/6200 loss 19.034143 loss_att 23.925919 loss_ctc 30.064417 loss_rnnt 16.458668 hw_loss 0.237030 lr 0.00079444 rank 2
2023-02-17 09:59:48,087 DEBUG TRAIN Batch 4/6200 loss 13.014813 loss_att 14.410450 loss_ctc 18.964439 loss_rnnt 11.810024 hw_loss 0.248209 lr 0.00079494 rank 0
2023-02-17 09:59:48,115 DEBUG TRAIN Batch 4/6200 loss 35.237396 loss_att 45.994949 loss_ctc 61.078171 loss_rnnt 29.513605 hw_loss 0.237823 lr 0.00079533 rank 7
2023-02-17 10:01:03,812 DEBUG TRAIN Batch 4/6300 loss 37.580696 loss_att 44.533684 loss_ctc 57.037544 loss_rnnt 33.440300 hw_loss 0.291667 lr 0.00079432 rank 7
2023-02-17 10:01:03,817 DEBUG TRAIN Batch 4/6300 loss 22.430248 loss_att 26.379770 loss_ctc 39.148170 loss_rnnt 19.267490 hw_loss 0.269617 lr 0.00079394 rank 0
2023-02-17 10:01:03,827 DEBUG TRAIN Batch 4/6300 loss 33.025826 loss_att 36.441833 loss_ctc 41.777679 loss_rnnt 31.109917 hw_loss 0.123355 lr 0.00079420 rank 6
2023-02-17 10:01:03,829 DEBUG TRAIN Batch 4/6300 loss 20.377819 loss_att 24.393625 loss_ctc 35.790085 loss_rnnt 17.377239 hw_loss 0.267093 lr 0.00079398 rank 3
2023-02-17 10:01:03,830 DEBUG TRAIN Batch 4/6300 loss 15.923773 loss_att 15.830338 loss_ctc 23.131622 loss_rnnt 14.807742 hw_loss 0.325635 lr 0.00079430 rank 4
2023-02-17 10:01:03,835 DEBUG TRAIN Batch 4/6300 loss 22.246134 loss_att 21.864029 loss_ctc 34.304588 loss_rnnt 20.563429 hw_loss 0.283753 lr 0.00079391 rank 1
2023-02-17 10:01:03,847 DEBUG TRAIN Batch 4/6300 loss 18.089636 loss_att 26.126892 loss_ctc 30.622215 loss_rnnt 14.623324 hw_loss 0.352214 lr 0.00079395 rank 5
2023-02-17 10:01:03,874 DEBUG TRAIN Batch 4/6300 loss 11.176804 loss_att 10.597551 loss_ctc 15.242337 loss_rnnt 10.583355 hw_loss 0.313552 lr 0.00079344 rank 2
2023-02-17 10:02:37,061 DEBUG TRAIN Batch 4/6400 loss 23.934437 loss_att 24.601770 loss_ctc 30.474670 loss_rnnt 22.779938 hw_loss 0.279380 lr 0.00079295 rank 5
2023-02-17 10:02:37,061 DEBUG TRAIN Batch 4/6400 loss 19.474869 loss_att 19.913042 loss_ctc 25.927032 loss_rnnt 18.344242 hw_loss 0.342567 lr 0.00079298 rank 3
2023-02-17 10:02:37,061 DEBUG TRAIN Batch 4/6400 loss 16.930775 loss_att 17.868843 loss_ctc 22.234503 loss_rnnt 15.829655 hw_loss 0.386890 lr 0.00079320 rank 6
2023-02-17 10:02:37,062 DEBUG TRAIN Batch 4/6400 loss 12.793697 loss_att 13.312608 loss_ctc 15.978169 loss_rnnt 12.069014 hw_loss 0.368076 lr 0.00079332 rank 7
2023-02-17 10:02:37,084 DEBUG TRAIN Batch 4/6400 loss 13.527793 loss_att 16.163918 loss_ctc 20.072264 loss_rnnt 12.028982 hw_loss 0.185607 lr 0.00079244 rank 2
2023-02-17 10:02:37,116 DEBUG TRAIN Batch 4/6400 loss 31.564831 loss_att 35.598099 loss_ctc 44.455261 loss_rnnt 28.924194 hw_loss 0.216112 lr 0.00079330 rank 4
2023-02-17 10:02:37,119 DEBUG TRAIN Batch 4/6400 loss 11.869320 loss_att 18.599901 loss_ctc 18.859718 loss_rnnt 9.497322 hw_loss 0.175929 lr 0.00079294 rank 0
2023-02-17 10:02:37,124 DEBUG TRAIN Batch 4/6400 loss 25.086843 loss_att 32.701004 loss_ctc 40.068920 loss_rnnt 21.411430 hw_loss 0.290571 lr 0.00079291 rank 1
2023-02-17 10:03:52,660 DEBUG TRAIN Batch 4/6500 loss 24.696285 loss_att 29.752594 loss_ctc 37.421776 loss_rnnt 21.831366 hw_loss 0.294235 lr 0.00079221 rank 6
2023-02-17 10:03:52,664 DEBUG TRAIN Batch 4/6500 loss 15.215540 loss_att 21.144972 loss_ctc 20.845953 loss_rnnt 13.158824 hw_loss 0.225202 lr 0.00079196 rank 5
2023-02-17 10:03:52,665 DEBUG TRAIN Batch 4/6500 loss 29.067875 loss_att 39.034981 loss_ctc 44.789486 loss_rnnt 24.834305 hw_loss 0.269876 lr 0.00079195 rank 0
2023-02-17 10:03:52,666 DEBUG TRAIN Batch 4/6500 loss 30.174475 loss_att 36.818871 loss_ctc 46.056633 loss_rnnt 26.580236 hw_loss 0.277008 lr 0.00079230 rank 4
2023-02-17 10:03:52,667 DEBUG TRAIN Batch 4/6500 loss 23.568420 loss_att 25.175190 loss_ctc 35.832310 loss_rnnt 21.455017 hw_loss 0.294119 lr 0.00079145 rank 2
2023-02-17 10:03:52,668 DEBUG TRAIN Batch 4/6500 loss 17.914715 loss_att 27.437550 loss_ctc 29.127388 loss_rnnt 14.340495 hw_loss 0.327432 lr 0.00079199 rank 3
2023-02-17 10:03:52,670 DEBUG TRAIN Batch 4/6500 loss 29.035627 loss_att 32.821312 loss_ctc 40.687889 loss_rnnt 26.581730 hw_loss 0.268360 lr 0.00079192 rank 1
2023-02-17 10:03:52,715 DEBUG TRAIN Batch 4/6500 loss 34.201740 loss_att 41.357368 loss_ctc 51.659180 loss_rnnt 30.291573 hw_loss 0.283844 lr 0.00079232 rank 7
2023-02-17 10:05:09,589 DEBUG TRAIN Batch 4/6600 loss 32.512531 loss_att 41.175995 loss_ctc 69.440262 loss_rnnt 25.713943 hw_loss 0.266618 lr 0.00079099 rank 3
2023-02-17 10:05:09,589 DEBUG TRAIN Batch 4/6600 loss 33.585762 loss_att 46.228447 loss_ctc 57.715130 loss_rnnt 27.679817 hw_loss 0.300294 lr 0.00079096 rank 0
2023-02-17 10:05:09,617 DEBUG TRAIN Batch 4/6600 loss 19.862638 loss_att 20.993488 loss_ctc 40.142784 loss_rnnt 16.781200 hw_loss 0.283592 lr 0.00079096 rank 5
2023-02-17 10:05:09,633 DEBUG TRAIN Batch 4/6600 loss 26.084831 loss_att 35.219681 loss_ctc 35.498344 loss_rnnt 22.901052 hw_loss 0.190642 lr 0.00079121 rank 6
2023-02-17 10:05:09,644 DEBUG TRAIN Batch 4/6600 loss 20.780222 loss_att 26.895290 loss_ctc 36.657310 loss_rnnt 17.344376 hw_loss 0.179785 lr 0.00079046 rank 2
2023-02-17 10:05:09,655 DEBUG TRAIN Batch 4/6600 loss 31.781395 loss_att 37.611618 loss_ctc 56.485115 loss_rnnt 27.151703 hw_loss 0.318409 lr 0.00079133 rank 7
2023-02-17 10:05:09,658 DEBUG TRAIN Batch 4/6600 loss 23.207432 loss_att 29.539680 loss_ctc 33.442535 loss_rnnt 20.448322 hw_loss 0.239957 lr 0.00079093 rank 1
2023-02-17 10:05:09,692 DEBUG TRAIN Batch 4/6600 loss 24.918947 loss_att 30.431452 loss_ctc 36.386665 loss_rnnt 22.165625 hw_loss 0.228363 lr 0.00079131 rank 4
2023-02-17 10:06:30,017 DEBUG TRAIN Batch 4/6700 loss 17.668938 loss_att 21.040575 loss_ctc 26.588257 loss_rnnt 15.661962 hw_loss 0.268885 lr 0.00078997 rank 0
2023-02-17 10:06:30,028 DEBUG TRAIN Batch 4/6700 loss 13.598615 loss_att 21.094545 loss_ctc 34.362812 loss_rnnt 9.230619 hw_loss 0.187969 lr 0.00079034 rank 7
2023-02-17 10:06:30,040 DEBUG TRAIN Batch 4/6700 loss 27.368111 loss_att 34.184647 loss_ctc 37.334305 loss_rnnt 24.543365 hw_loss 0.248647 lr 0.00079022 rank 6
2023-02-17 10:06:30,044 DEBUG TRAIN Batch 4/6700 loss 17.416372 loss_att 19.410585 loss_ctc 31.099298 loss_rnnt 15.051914 hw_loss 0.264798 lr 0.00078947 rank 2
2023-02-17 10:06:30,046 DEBUG TRAIN Batch 4/6700 loss 21.860769 loss_att 24.742075 loss_ctc 38.112152 loss_rnnt 18.996473 hw_loss 0.227221 lr 0.00079032 rank 4
2023-02-17 10:06:30,046 DEBUG TRAIN Batch 4/6700 loss 21.870985 loss_att 25.940363 loss_ctc 36.511715 loss_rnnt 18.918549 hw_loss 0.349622 lr 0.00079001 rank 3
2023-02-17 10:06:30,073 DEBUG TRAIN Batch 4/6700 loss 27.665615 loss_att 31.762293 loss_ctc 38.571873 loss_rnnt 25.191818 hw_loss 0.375546 lr 0.00078994 rank 1
2023-02-17 10:06:30,088 DEBUG TRAIN Batch 4/6700 loss 23.916500 loss_att 30.917231 loss_ctc 39.296387 loss_rnnt 20.349596 hw_loss 0.217698 lr 0.00078998 rank 5
2023-02-17 10:08:00,689 DEBUG TRAIN Batch 4/6800 loss 20.779371 loss_att 28.786781 loss_ctc 30.483232 loss_rnnt 17.757944 hw_loss 0.236430 lr 0.00078934 rank 4
2023-02-17 10:08:00,690 DEBUG TRAIN Batch 4/6800 loss 29.825298 loss_att 38.442181 loss_ctc 40.350994 loss_rnnt 26.571178 hw_loss 0.238726 lr 0.00078899 rank 5
2023-02-17 10:08:00,690 DEBUG TRAIN Batch 4/6800 loss 19.605530 loss_att 29.613388 loss_ctc 26.647610 loss_rnnt 16.515945 hw_loss 0.279502 lr 0.00078898 rank 0
2023-02-17 10:08:00,693 DEBUG TRAIN Batch 4/6800 loss 12.551802 loss_att 17.672668 loss_ctc 22.784939 loss_rnnt 9.982165 hw_loss 0.339461 lr 0.00078936 rank 7
2023-02-17 10:08:00,693 DEBUG TRAIN Batch 4/6800 loss 30.619497 loss_att 34.140236 loss_ctc 58.619915 loss_rnnt 26.088619 hw_loss 0.175014 lr 0.00078924 rank 6
2023-02-17 10:08:00,693 DEBUG TRAIN Batch 4/6800 loss 22.802059 loss_att 30.115005 loss_ctc 41.497562 loss_rnnt 18.758347 hw_loss 0.165731 lr 0.00078895 rank 1
2023-02-17 10:08:00,694 DEBUG TRAIN Batch 4/6800 loss 38.223675 loss_att 40.800056 loss_ctc 58.135830 loss_rnnt 34.906017 hw_loss 0.276427 lr 0.00078902 rank 3
2023-02-17 10:08:00,738 DEBUG TRAIN Batch 4/6800 loss 11.941210 loss_att 14.270222 loss_ctc 25.459930 loss_rnnt 9.504627 hw_loss 0.315533 lr 0.00078849 rank 2
2023-02-17 10:09:17,167 DEBUG TRAIN Batch 4/6900 loss 39.943268 loss_att 45.484818 loss_ctc 61.308887 loss_rnnt 35.885357 hw_loss 0.189090 lr 0.00078800 rank 0
2023-02-17 10:09:17,179 DEBUG TRAIN Batch 4/6900 loss 22.446001 loss_att 24.976168 loss_ctc 35.177212 loss_rnnt 20.103912 hw_loss 0.259800 lr 0.00078836 rank 4
2023-02-17 10:09:17,180 DEBUG TRAIN Batch 4/6900 loss 13.239591 loss_att 15.877648 loss_ctc 20.385193 loss_rnnt 11.573939 hw_loss 0.347427 lr 0.00078751 rank 2
2023-02-17 10:09:17,182 DEBUG TRAIN Batch 4/6900 loss 34.593708 loss_att 39.854988 loss_ctc 52.073116 loss_rnnt 31.067774 hw_loss 0.268300 lr 0.00078837 rank 7
2023-02-17 10:09:17,184 DEBUG TRAIN Batch 4/6900 loss 28.245794 loss_att 34.547344 loss_ctc 44.939671 loss_rnnt 24.629217 hw_loss 0.244529 lr 0.00078804 rank 3
2023-02-17 10:09:17,185 DEBUG TRAIN Batch 4/6900 loss 27.859232 loss_att 32.451393 loss_ctc 46.734207 loss_rnnt 24.219816 hw_loss 0.383095 lr 0.00078797 rank 1
2023-02-17 10:09:17,185 DEBUG TRAIN Batch 4/6900 loss 21.469175 loss_att 27.940296 loss_ctc 37.557930 loss_rnnt 17.893923 hw_loss 0.254741 lr 0.00078801 rank 5
2023-02-17 10:09:17,257 DEBUG TRAIN Batch 4/6900 loss 16.368538 loss_att 19.268826 loss_ctc 22.686928 loss_rnnt 14.796968 hw_loss 0.279486 lr 0.00078826 rank 6
2023-02-17 10:10:32,972 DEBUG TRAIN Batch 4/7000 loss 12.424404 loss_att 17.483515 loss_ctc 21.699457 loss_rnnt 10.019624 hw_loss 0.293032 lr 0.00078728 rank 6
2023-02-17 10:10:32,973 DEBUG TRAIN Batch 4/7000 loss 33.270264 loss_att 43.350559 loss_ctc 46.848843 loss_rnnt 29.309668 hw_loss 0.251358 lr 0.00078700 rank 1
2023-02-17 10:10:32,974 DEBUG TRAIN Batch 4/7000 loss 18.160881 loss_att 19.014538 loss_ctc 25.959349 loss_rnnt 16.790308 hw_loss 0.300082 lr 0.00078703 rank 0
2023-02-17 10:10:32,974 DEBUG TRAIN Batch 4/7000 loss 18.234425 loss_att 20.830168 loss_ctc 26.114584 loss_rnnt 16.483391 hw_loss 0.339740 lr 0.00078740 rank 7
2023-02-17 10:10:32,976 DEBUG TRAIN Batch 4/7000 loss 19.325167 loss_att 24.637327 loss_ctc 31.384247 loss_rnnt 16.480827 hw_loss 0.326306 lr 0.00078704 rank 5
2023-02-17 10:10:32,978 DEBUG TRAIN Batch 4/7000 loss 30.217295 loss_att 36.479527 loss_ctc 41.752319 loss_rnnt 27.296652 hw_loss 0.244110 lr 0.00078738 rank 4
2023-02-17 10:10:32,980 DEBUG TRAIN Batch 4/7000 loss 13.380595 loss_att 15.509512 loss_ctc 21.445702 loss_rnnt 11.713440 hw_loss 0.311297 lr 0.00078706 rank 3
2023-02-17 10:10:32,989 DEBUG TRAIN Batch 4/7000 loss 16.851175 loss_att 22.610600 loss_ctc 28.450981 loss_rnnt 14.013618 hw_loss 0.260683 lr 0.00078654 rank 2
2023-02-17 10:11:59,065 DEBUG TRAIN Batch 4/7100 loss 24.138454 loss_att 35.050270 loss_ctc 41.748375 loss_rnnt 19.485809 hw_loss 0.229302 lr 0.00078631 rank 6
2023-02-17 10:11:59,067 DEBUG TRAIN Batch 4/7100 loss 30.047997 loss_att 34.344433 loss_ctc 51.338573 loss_rnnt 26.220900 hw_loss 0.242001 lr 0.00078609 rank 3
2023-02-17 10:11:59,084 DEBUG TRAIN Batch 4/7100 loss 36.609875 loss_att 39.987289 loss_ctc 55.616493 loss_rnnt 33.236534 hw_loss 0.306828 lr 0.00078642 rank 7
2023-02-17 10:11:59,086 DEBUG TRAIN Batch 4/7100 loss 32.045883 loss_att 39.278679 loss_ctc 52.459846 loss_rnnt 27.698425 hw_loss 0.335691 lr 0.00078606 rank 5
2023-02-17 10:11:59,099 DEBUG TRAIN Batch 4/7100 loss 10.198246 loss_att 14.805647 loss_ctc 22.041882 loss_rnnt 7.581279 hw_loss 0.218129 lr 0.00078640 rank 4
2023-02-17 10:11:59,098 DEBUG TRAIN Batch 4/7100 loss 9.820814 loss_att 14.749283 loss_ctc 16.201721 loss_rnnt 7.855006 hw_loss 0.242489 lr 0.00078605 rank 0
2023-02-17 10:11:59,125 DEBUG TRAIN Batch 4/7100 loss 15.401725 loss_att 23.005142 loss_ctc 28.969021 loss_rnnt 11.925895 hw_loss 0.274076 lr 0.00078557 rank 2
2023-02-17 10:11:59,146 DEBUG TRAIN Batch 4/7100 loss 32.841385 loss_att 33.909424 loss_ctc 47.543968 loss_rnnt 30.495934 hw_loss 0.321557 lr 0.00078602 rank 1
2023-02-17 10:13:23,857 DEBUG TRAIN Batch 4/7200 loss 22.231827 loss_att 25.777363 loss_ctc 31.874443 loss_rnnt 20.123894 hw_loss 0.212145 lr 0.00078543 rank 4
2023-02-17 10:13:23,867 DEBUG TRAIN Batch 4/7200 loss 26.358164 loss_att 28.214687 loss_ctc 38.101067 loss_rnnt 24.316578 hw_loss 0.196051 lr 0.00078460 rank 2
2023-02-17 10:13:23,869 DEBUG TRAIN Batch 4/7200 loss 26.593737 loss_att 30.925442 loss_ctc 50.493164 loss_rnnt 22.414183 hw_loss 0.237417 lr 0.00078505 rank 1
2023-02-17 10:13:23,871 DEBUG TRAIN Batch 4/7200 loss 19.739210 loss_att 22.762730 loss_ctc 24.911438 loss_rnnt 18.342903 hw_loss 0.191198 lr 0.00078509 rank 5
2023-02-17 10:13:23,877 DEBUG TRAIN Batch 4/7200 loss 23.637033 loss_att 30.663338 loss_ctc 43.937492 loss_rnnt 19.362539 hw_loss 0.304699 lr 0.00078545 rank 7
2023-02-17 10:13:23,878 DEBUG TRAIN Batch 4/7200 loss 23.906073 loss_att 28.962757 loss_ctc 35.183334 loss_rnnt 21.246113 hw_loss 0.271849 lr 0.00078533 rank 6
2023-02-17 10:13:23,877 DEBUG TRAIN Batch 4/7200 loss 14.577479 loss_att 15.439004 loss_ctc 22.491156 loss_rnnt 13.170780 hw_loss 0.336069 lr 0.00078508 rank 0
2023-02-17 10:13:23,920 DEBUG TRAIN Batch 4/7200 loss 58.530781 loss_att 53.318321 loss_ctc 67.683838 loss_rnnt 58.179394 hw_loss 0.325256 lr 0.00078512 rank 3
2023-02-17 10:14:39,970 DEBUG TRAIN Batch 4/7300 loss 22.293221 loss_att 27.573484 loss_ctc 38.229012 loss_rnnt 18.952168 hw_loss 0.300428 lr 0.00078448 rank 7
2023-02-17 10:14:39,971 DEBUG TRAIN Batch 4/7300 loss 9.811028 loss_att 16.399868 loss_ctc 17.984489 loss_rnnt 7.208283 hw_loss 0.365967 lr 0.00078409 rank 1
2023-02-17 10:14:39,973 DEBUG TRAIN Batch 4/7300 loss 17.061357 loss_att 23.439779 loss_ctc 35.754662 loss_rnnt 13.171256 hw_loss 0.228706 lr 0.00078413 rank 5
2023-02-17 10:14:39,973 DEBUG TRAIN Batch 4/7300 loss 15.609276 loss_att 21.120773 loss_ctc 28.238708 loss_rnnt 12.623840 hw_loss 0.373521 lr 0.00078416 rank 3
2023-02-17 10:14:39,975 DEBUG TRAIN Batch 4/7300 loss 17.971552 loss_att 21.884912 loss_ctc 30.655071 loss_rnnt 15.352986 hw_loss 0.271419 lr 0.00078364 rank 2
2023-02-17 10:14:39,976 DEBUG TRAIN Batch 4/7300 loss 25.739134 loss_att 26.861471 loss_ctc 37.257637 loss_rnnt 23.901262 hw_loss 0.145509 lr 0.00078412 rank 0
2023-02-17 10:14:39,988 DEBUG TRAIN Batch 4/7300 loss 38.565105 loss_att 50.591141 loss_ctc 65.795456 loss_rnnt 32.441586 hw_loss 0.164254 lr 0.00078437 rank 6
2023-02-17 10:14:40,001 DEBUG TRAIN Batch 4/7300 loss 14.586623 loss_att 20.193043 loss_ctc 23.684805 loss_rnnt 12.122563 hw_loss 0.243158 lr 0.00078446 rank 4
2023-02-17 10:15:59,401 DEBUG TRAIN Batch 4/7400 loss 13.557920 loss_att 16.339643 loss_ctc 22.860737 loss_rnnt 11.609192 hw_loss 0.285016 lr 0.00078315 rank 0
2023-02-17 10:15:59,404 DEBUG TRAIN Batch 4/7400 loss 9.447954 loss_att 16.294024 loss_ctc 19.883001 loss_rnnt 6.571450 hw_loss 0.217405 lr 0.00078352 rank 7
2023-02-17 10:15:59,405 DEBUG TRAIN Batch 4/7400 loss 19.323019 loss_att 23.185156 loss_ctc 33.831581 loss_rnnt 16.508415 hw_loss 0.201941 lr 0.00078350 rank 4
2023-02-17 10:15:59,458 DEBUG TRAIN Batch 4/7400 loss 26.612228 loss_att 32.477219 loss_ctc 40.121601 loss_rnnt 23.470808 hw_loss 0.313455 lr 0.00078319 rank 3
2023-02-17 10:15:59,461 DEBUG TRAIN Batch 4/7400 loss 19.708035 loss_att 27.142906 loss_ctc 33.666405 loss_rnnt 16.250065 hw_loss 0.206020 lr 0.00078340 rank 6
2023-02-17 10:15:59,462 DEBUG TRAIN Batch 4/7400 loss 33.728191 loss_att 38.330490 loss_ctc 50.795830 loss_rnnt 30.375725 hw_loss 0.293099 lr 0.00078313 rank 1
2023-02-17 10:15:59,464 DEBUG TRAIN Batch 4/7400 loss 18.246712 loss_att 24.755348 loss_ctc 32.115562 loss_rnnt 14.939037 hw_loss 0.293937 lr 0.00078316 rank 5
2023-02-17 10:15:59,506 DEBUG TRAIN Batch 4/7400 loss 27.292614 loss_att 30.981703 loss_ctc 40.125702 loss_rnnt 24.669785 hw_loss 0.326126 lr 0.00078267 rank 2
2023-02-17 10:17:31,029 DEBUG TRAIN Batch 4/7500 loss 24.201342 loss_att 26.085707 loss_ctc 40.628963 loss_rnnt 21.451576 hw_loss 0.342269 lr 0.00078254 rank 4
2023-02-17 10:17:31,031 DEBUG TRAIN Batch 4/7500 loss 21.632294 loss_att 28.162754 loss_ctc 36.601559 loss_rnnt 18.222494 hw_loss 0.202138 lr 0.00078221 rank 5
2023-02-17 10:17:31,031 DEBUG TRAIN Batch 4/7500 loss 14.409123 loss_att 19.031124 loss_ctc 22.633129 loss_rnnt 12.268381 hw_loss 0.224639 lr 0.00078217 rank 1
2023-02-17 10:17:31,034 DEBUG TRAIN Batch 4/7500 loss 18.236246 loss_att 22.723349 loss_ctc 28.935400 loss_rnnt 15.771402 hw_loss 0.264128 lr 0.00078256 rank 7
2023-02-17 10:17:31,052 DEBUG TRAIN Batch 4/7500 loss 37.362247 loss_att 42.607727 loss_ctc 53.448830 loss_rnnt 34.009514 hw_loss 0.297672 lr 0.00078223 rank 3
2023-02-17 10:17:31,062 DEBUG TRAIN Batch 4/7500 loss 20.430908 loss_att 19.311302 loss_ctc 26.377621 loss_rnnt 19.697884 hw_loss 0.307593 lr 0.00078172 rank 2
2023-02-17 10:17:31,063 DEBUG TRAIN Batch 4/7500 loss 29.605593 loss_att 30.282825 loss_ctc 42.848839 loss_rnnt 27.522976 hw_loss 0.340133 lr 0.00078244 rank 6
2023-02-17 10:17:31,072 DEBUG TRAIN Batch 4/7500 loss 30.318407 loss_att 36.585224 loss_ctc 52.533340 loss_rnnt 25.920715 hw_loss 0.341883 lr 0.00078220 rank 0
2023-02-17 10:18:46,408 DEBUG TRAIN Batch 4/7600 loss 32.950218 loss_att 46.468903 loss_ctc 58.660183 loss_rnnt 26.653679 hw_loss 0.309006 lr 0.00078076 rank 2
2023-02-17 10:18:46,409 DEBUG TRAIN Batch 4/7600 loss 18.070559 loss_att 23.655386 loss_ctc 30.665792 loss_rnnt 15.107996 hw_loss 0.311686 lr 0.00078149 rank 6
2023-02-17 10:18:46,409 DEBUG TRAIN Batch 4/7600 loss 15.933762 loss_att 18.576712 loss_ctc 24.365791 loss_rnnt 14.067823 hw_loss 0.399523 lr 0.00078124 rank 0
2023-02-17 10:18:46,410 DEBUG TRAIN Batch 4/7600 loss 18.131802 loss_att 17.875244 loss_ctc 27.539577 loss_rnnt 16.843185 hw_loss 0.160416 lr 0.00078121 rank 1
2023-02-17 10:18:46,412 DEBUG TRAIN Batch 4/7600 loss 14.940700 loss_att 16.631517 loss_ctc 19.359461 loss_rnnt 13.843236 hw_loss 0.318996 lr 0.00078158 rank 4
2023-02-17 10:18:46,435 DEBUG TRAIN Batch 4/7600 loss 21.875347 loss_att 25.435459 loss_ctc 32.564972 loss_rnnt 19.578159 hw_loss 0.299777 lr 0.00078160 rank 7
2023-02-17 10:18:46,443 DEBUG TRAIN Batch 4/7600 loss 34.240009 loss_att 38.654041 loss_ctc 45.996445 loss_rnnt 31.620478 hw_loss 0.317252 lr 0.00078128 rank 3
2023-02-17 10:18:46,450 DEBUG TRAIN Batch 4/7600 loss 18.449823 loss_att 20.253109 loss_ctc 33.338055 loss_rnnt 15.960573 hw_loss 0.269053 lr 0.00078125 rank 5
2023-02-17 10:20:02,320 DEBUG TRAIN Batch 4/7700 loss 49.205692 loss_att 59.027664 loss_ctc 70.228745 loss_rnnt 44.321953 hw_loss 0.218023 lr 0.00078065 rank 7
2023-02-17 10:20:02,322 DEBUG TRAIN Batch 4/7700 loss 15.350891 loss_att 14.961367 loss_ctc 21.425064 loss_rnnt 14.441372 hw_loss 0.332876 lr 0.00078054 rank 6
2023-02-17 10:20:02,323 DEBUG TRAIN Batch 4/7700 loss 21.391006 loss_att 22.804581 loss_ctc 31.316984 loss_rnnt 19.668854 hw_loss 0.217450 lr 0.00078029 rank 0
2023-02-17 10:20:02,324 DEBUG TRAIN Batch 4/7700 loss 18.677473 loss_att 24.172092 loss_ctc 36.995323 loss_rnnt 14.989609 hw_loss 0.274802 lr 0.00078033 rank 3
2023-02-17 10:20:02,325 DEBUG TRAIN Batch 4/7700 loss 11.042551 loss_att 12.834260 loss_ctc 16.579464 loss_rnnt 9.754501 hw_loss 0.358973 lr 0.00078030 rank 5
2023-02-17 10:20:02,326 DEBUG TRAIN Batch 4/7700 loss 30.268753 loss_att 32.076706 loss_ctc 44.586876 loss_rnnt 27.867077 hw_loss 0.245632 lr 0.00078026 rank 1
2023-02-17 10:20:02,329 DEBUG TRAIN Batch 4/7700 loss 22.061205 loss_att 25.006063 loss_ctc 34.509521 loss_rnnt 19.669075 hw_loss 0.268841 lr 0.00077981 rank 2
2023-02-17 10:20:02,330 DEBUG TRAIN Batch 4/7700 loss 27.359514 loss_att 36.785896 loss_ctc 45.999474 loss_rnnt 22.860600 hw_loss 0.240581 lr 0.00078063 rank 4
2023-02-17 10:21:26,786 DEBUG TRAIN Batch 4/7800 loss 17.288023 loss_att 20.260544 loss_ctc 23.494717 loss_rnnt 15.781996 hw_loss 0.157431 lr 0.00077934 rank 0
2023-02-17 10:21:26,812 DEBUG TRAIN Batch 4/7800 loss 19.018431 loss_att 28.123653 loss_ctc 29.923851 loss_rnnt 15.592742 hw_loss 0.282354 lr 0.00077938 rank 3
2023-02-17 10:21:26,814 DEBUG TRAIN Batch 4/7800 loss 27.744089 loss_att 29.963205 loss_ctc 46.246445 loss_rnnt 24.727530 hw_loss 0.198291 lr 0.00077968 rank 4
2023-02-17 10:21:26,819 DEBUG TRAIN Batch 4/7800 loss 14.837674 loss_att 24.518951 loss_ctc 20.298168 loss_rnnt 12.012660 hw_loss 0.301300 lr 0.00077931 rank 1
2023-02-17 10:21:26,836 DEBUG TRAIN Batch 4/7800 loss 22.298225 loss_att 27.588001 loss_ctc 38.199707 loss_rnnt 18.926376 hw_loss 0.363178 lr 0.00077959 rank 6
2023-02-17 10:21:26,842 DEBUG TRAIN Batch 4/7800 loss 16.300016 loss_att 22.854534 loss_ctc 30.598801 loss_rnnt 12.922091 hw_loss 0.300969 lr 0.00077887 rank 2
2023-02-17 10:21:26,843 DEBUG TRAIN Batch 4/7800 loss 30.325838 loss_att 37.801952 loss_ctc 55.780743 loss_rnnt 25.226141 hw_loss 0.394663 lr 0.00077935 rank 5
2023-02-17 10:21:26,854 DEBUG TRAIN Batch 4/7800 loss 16.504974 loss_att 21.904963 loss_ctc 27.480133 loss_rnnt 13.819395 hw_loss 0.266674 lr 0.00077970 rank 7
2023-02-17 10:22:53,636 DEBUG TRAIN Batch 4/7900 loss 26.280991 loss_att 29.267216 loss_ctc 47.410690 loss_rnnt 22.743717 hw_loss 0.230120 lr 0.00077840 rank 5
2023-02-17 10:22:53,636 DEBUG TRAIN Batch 4/7900 loss 21.345282 loss_att 26.273521 loss_ctc 30.788303 loss_rnnt 18.963779 hw_loss 0.256472 lr 0.00077873 rank 4
2023-02-17 10:22:53,638 DEBUG TRAIN Batch 4/7900 loss 22.218060 loss_att 24.127365 loss_ctc 28.967127 loss_rnnt 20.850945 hw_loss 0.160084 lr 0.00077792 rank 2
2023-02-17 10:22:53,644 DEBUG TRAIN Batch 4/7900 loss 11.484796 loss_att 14.506760 loss_ctc 18.699654 loss_rnnt 9.785275 hw_loss 0.249648 lr 0.00077840 rank 0
2023-02-17 10:22:53,646 DEBUG TRAIN Batch 4/7900 loss 13.527796 loss_att 22.152891 loss_ctc 24.843945 loss_rnnt 10.135529 hw_loss 0.297052 lr 0.00077864 rank 6
2023-02-17 10:22:53,676 DEBUG TRAIN Batch 4/7900 loss 15.177799 loss_att 19.076023 loss_ctc 20.594265 loss_rnnt 13.570839 hw_loss 0.197102 lr 0.00077837 rank 1
2023-02-17 10:22:53,676 DEBUG TRAIN Batch 4/7900 loss 18.302256 loss_att 23.953711 loss_ctc 26.610037 loss_rnnt 15.987796 hw_loss 0.143372 lr 0.00077843 rank 3
2023-02-17 10:22:53,698 DEBUG TRAIN Batch 4/7900 loss 36.703228 loss_att 45.351585 loss_ctc 63.297333 loss_rnnt 31.314213 hw_loss 0.212733 lr 0.00077875 rank 7
2023-02-17 10:24:08,878 DEBUG TRAIN Batch 4/8000 loss 15.460266 loss_att 19.690687 loss_ctc 26.557747 loss_rnnt 13.007497 hw_loss 0.238161 lr 0.00077743 rank 1
2023-02-17 10:24:08,911 DEBUG TRAIN Batch 4/8000 loss 21.532791 loss_att 29.709955 loss_ctc 35.599010 loss_rnnt 17.875046 hw_loss 0.275281 lr 0.00077746 rank 5
2023-02-17 10:24:08,911 DEBUG TRAIN Batch 4/8000 loss 16.081142 loss_att 18.849211 loss_ctc 28.787766 loss_rnnt 13.690163 hw_loss 0.268405 lr 0.00077781 rank 7
2023-02-17 10:24:08,916 DEBUG TRAIN Batch 4/8000 loss 36.863831 loss_att 43.850052 loss_ctc 57.951630 loss_rnnt 32.480263 hw_loss 0.327404 lr 0.00077745 rank 0
2023-02-17 10:24:08,942 DEBUG TRAIN Batch 4/8000 loss 19.115707 loss_att 24.408287 loss_ctc 33.933571 loss_rnnt 15.985662 hw_loss 0.179653 lr 0.00077770 rank 6
2023-02-17 10:24:08,941 DEBUG TRAIN Batch 4/8000 loss 17.055914 loss_att 20.176701 loss_ctc 27.381256 loss_rnnt 14.955384 hw_loss 0.186863 lr 0.00077779 rank 4
2023-02-17 10:24:08,944 DEBUG TRAIN Batch 4/8000 loss 12.960737 loss_att 18.470451 loss_ctc 25.322842 loss_rnnt 10.069077 hw_loss 0.265194 lr 0.00077749 rank 3
2023-02-17 10:24:08,945 DEBUG TRAIN Batch 4/8000 loss 20.073912 loss_att 28.894634 loss_ctc 32.032104 loss_rnnt 16.603497 hw_loss 0.209708 lr 0.00077698 rank 2
2023-02-17 10:25:27,154 DEBUG TRAIN Batch 4/8100 loss 25.625256 loss_att 28.703636 loss_ctc 33.869534 loss_rnnt 23.791794 hw_loss 0.222277 lr 0.00077655 rank 3
2023-02-17 10:25:27,190 DEBUG TRAIN Batch 4/8100 loss 22.893353 loss_att 24.220242 loss_ctc 37.249855 loss_rnnt 20.588020 hw_loss 0.235793 lr 0.00077685 rank 4
2023-02-17 10:25:27,198 DEBUG TRAIN Batch 4/8100 loss 25.677967 loss_att 31.544163 loss_ctc 39.942078 loss_rnnt 22.446991 hw_loss 0.292228 lr 0.00077652 rank 0
2023-02-17 10:25:27,209 DEBUG TRAIN Batch 4/8100 loss 25.967756 loss_att 26.305222 loss_ctc 37.975838 loss_rnnt 24.122227 hw_loss 0.331793 lr 0.00077649 rank 1
2023-02-17 10:25:27,221 DEBUG TRAIN Batch 4/8100 loss 20.809999 loss_att 24.571697 loss_ctc 31.432060 loss_rnnt 18.522442 hw_loss 0.223018 lr 0.00077652 rank 5
2023-02-17 10:25:27,228 DEBUG TRAIN Batch 4/8100 loss 15.126978 loss_att 19.169395 loss_ctc 27.042023 loss_rnnt 12.547749 hw_loss 0.341386 lr 0.00077687 rank 7
2023-02-17 10:25:27,230 DEBUG TRAIN Batch 4/8100 loss 6.422539 loss_att 12.431759 loss_ctc 11.754444 loss_rnnt 4.349490 hw_loss 0.300533 lr 0.00077676 rank 6
2023-02-17 10:25:27,256 DEBUG TRAIN Batch 4/8100 loss 16.109331 loss_att 18.562637 loss_ctc 25.137030 loss_rnnt 14.305814 hw_loss 0.204676 lr 0.00077605 rank 2
2023-02-17 10:26:46,028 DEBUG TRAIN Batch 4/8200 loss 12.475435 loss_att 16.320145 loss_ctc 18.108232 loss_rnnt 10.832598 hw_loss 0.230353 lr 0.00077594 rank 7
2023-02-17 10:26:46,031 DEBUG TRAIN Batch 4/8200 loss 25.761549 loss_att 27.346874 loss_ctc 40.343983 loss_rnnt 23.340492 hw_loss 0.299375 lr 0.00077582 rank 6
2023-02-17 10:26:46,034 DEBUG TRAIN Batch 4/8200 loss 16.237272 loss_att 19.169638 loss_ctc 27.254648 loss_rnnt 14.052490 hw_loss 0.242484 lr 0.00077555 rank 1
2023-02-17 10:26:46,038 DEBUG TRAIN Batch 4/8200 loss 22.133324 loss_att 25.265020 loss_ctc 30.998672 loss_rnnt 20.233704 hw_loss 0.171067 lr 0.00077562 rank 3
2023-02-17 10:26:46,039 DEBUG TRAIN Batch 4/8200 loss 30.333069 loss_att 27.511898 loss_ctc 40.877266 loss_rnnt 29.306105 hw_loss 0.347446 lr 0.00077558 rank 0
2023-02-17 10:26:46,040 DEBUG TRAIN Batch 4/8200 loss 41.037792 loss_att 44.841412 loss_ctc 66.861984 loss_rnnt 36.680473 hw_loss 0.287573 lr 0.00077559 rank 5
2023-02-17 10:26:46,042 DEBUG TRAIN Batch 4/8200 loss 12.442568 loss_att 13.236185 loss_ctc 17.601219 loss_rnnt 11.383344 hw_loss 0.398775 lr 0.00077511 rank 2
2023-02-17 10:26:46,043 DEBUG TRAIN Batch 4/8200 loss 16.987749 loss_att 18.582239 loss_ctc 25.183558 loss_rnnt 15.417598 hw_loss 0.297147 lr 0.00077592 rank 4
2023-02-17 10:28:00,933 DEBUG TRAIN Batch 4/8300 loss 15.744576 loss_att 22.569109 loss_ctc 26.685482 loss_rnnt 12.813046 hw_loss 0.202191 lr 0.00077498 rank 4
2023-02-17 10:28:00,935 DEBUG TRAIN Batch 4/8300 loss 23.841303 loss_att 27.903931 loss_ctc 37.257721 loss_rnnt 21.098101 hw_loss 0.265907 lr 0.00077489 rank 6
2023-02-17 10:28:00,935 DEBUG TRAIN Batch 4/8300 loss 12.739803 loss_att 15.412051 loss_ctc 20.403400 loss_rnnt 11.008312 hw_loss 0.328553 lr 0.00077465 rank 0
2023-02-17 10:28:00,937 DEBUG TRAIN Batch 4/8300 loss 12.142087 loss_att 16.744122 loss_ctc 19.414284 loss_rnnt 10.118551 hw_loss 0.250315 lr 0.00077466 rank 5
2023-02-17 10:28:00,938 DEBUG TRAIN Batch 4/8300 loss 12.786439 loss_att 18.166771 loss_ctc 27.188892 loss_rnnt 9.674973 hw_loss 0.215759 lr 0.00077469 rank 3
2023-02-17 10:28:00,943 DEBUG TRAIN Batch 4/8300 loss 38.155350 loss_att 42.680279 loss_ctc 55.948936 loss_rnnt 34.745270 hw_loss 0.248651 lr 0.00077418 rank 2
2023-02-17 10:28:00,973 DEBUG TRAIN Batch 4/8300 loss 13.238645 loss_att 13.335192 loss_ctc 18.528694 loss_rnnt 12.360933 hw_loss 0.286991 lr 0.00077500 rank 7
2023-02-17 10:28:00,981 DEBUG TRAIN Batch 4/8300 loss 19.037361 loss_att 28.129389 loss_ctc 31.256012 loss_rnnt 15.409937 hw_loss 0.337250 lr 0.00077462 rank 1
2023-02-17 10:28:50,169 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 2
2023-02-17 10:28:50,171 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 5
2023-02-17 10:28:50,172 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 4
2023-02-17 10:28:50,172 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 7
2023-02-17 10:28:50,173 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 6
2023-02-17 10:28:50,174 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 1
2023-02-17 10:28:50,182 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 0
2023-02-17 10:28:50,189 DEBUG CV Batch 4/0 loss 3.818997 loss_att 3.832987 loss_ctc 6.073077 loss_rnnt 3.292484 hw_loss 0.418447 history loss 3.677553 rank 3
2023-02-17 10:29:03,180 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 5
2023-02-17 10:29:03,199 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 1
2023-02-17 10:29:03,202 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 4
2023-02-17 10:29:03,345 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 0
2023-02-17 10:29:03,478 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 3
2023-02-17 10:29:03,841 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 7
2023-02-17 10:29:04,737 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 6
2023-02-17 10:29:05,454 DEBUG CV Batch 4/100 loss 15.649966 loss_att 17.554729 loss_ctc 24.463558 loss_rnnt 13.957518 hw_loss 0.255658 history loss 6.555096 rank 2
2023-02-17 10:29:19,134 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 5
2023-02-17 10:29:19,395 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 0
2023-02-17 10:29:19,566 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 4
2023-02-17 10:29:19,721 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 7
2023-02-17 10:29:19,743 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 1
2023-02-17 10:29:20,052 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 3
2023-02-17 10:29:22,735 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 6
2023-02-17 10:29:23,398 DEBUG CV Batch 4/200 loss 24.412184 loss_att 29.621315 loss_ctc 41.174194 loss_rnnt 21.073925 hw_loss 0.115307 history loss 7.373648 rank 2
2023-02-17 10:29:33,122 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 5
2023-02-17 10:29:33,438 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 0
2023-02-17 10:29:33,665 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 4
2023-02-17 10:29:33,776 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 7
2023-02-17 10:29:34,217 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 1
2023-02-17 10:29:34,371 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 3
2023-02-17 10:29:36,886 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 6
2023-02-17 10:29:38,126 DEBUG CV Batch 4/300 loss 8.304491 loss_att 9.463690 loss_ctc 15.805546 loss_rnnt 6.893055 hw_loss 0.336478 history loss 7.477592 rank 2
2023-02-17 10:29:47,121 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 5
2023-02-17 10:29:47,578 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 4
2023-02-17 10:29:47,652 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 0
2023-02-17 10:29:47,857 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 7
2023-02-17 10:29:47,984 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 1
2023-02-17 10:29:49,074 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 3
2023-02-17 10:29:51,458 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 6
2023-02-17 10:29:55,915 DEBUG CV Batch 4/400 loss 41.763138 loss_att 154.215027 loss_ctc 36.404900 loss_rnnt 19.799582 hw_loss 0.351777 history loss 8.678474 rank 2
2023-02-17 10:30:00,130 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 5
2023-02-17 10:30:00,385 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 4
2023-02-17 10:30:00,952 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 7
2023-02-17 10:30:01,093 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 0
2023-02-17 10:30:01,815 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 1
2023-02-17 10:30:02,759 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 3
2023-02-17 10:30:04,861 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 6
2023-02-17 10:30:10,678 DEBUG CV Batch 4/500 loss 11.678717 loss_att 12.026594 loss_ctc 14.923076 loss_rnnt 11.005656 hw_loss 0.320444 history loss 9.555615 rank 2
2023-02-17 10:30:15,152 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 5
2023-02-17 10:30:15,546 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 4
2023-02-17 10:30:16,374 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 0
2023-02-17 10:30:16,450 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 7
2023-02-17 10:30:16,940 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 1
2023-02-17 10:30:18,105 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 3
2023-02-17 10:30:19,633 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 6
2023-02-17 10:30:27,413 DEBUG CV Batch 4/600 loss 10.216548 loss_att 10.302818 loss_ctc 14.511427 loss_rnnt 9.443652 hw_loss 0.343109 history loss 10.660407 rank 2
2023-02-17 10:30:29,677 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 5
2023-02-17 10:30:30,130 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 4
2023-02-17 10:30:30,885 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 0
2023-02-17 10:30:31,019 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 7
2023-02-17 10:30:31,815 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 1
2023-02-17 10:30:33,058 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 3
2023-02-17 10:30:34,539 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 6
2023-02-17 10:30:43,529 DEBUG CV Batch 4/700 loss 39.082455 loss_att 73.387314 loss_ctc 51.494457 loss_rnnt 30.500017 hw_loss 0.124737 history loss 11.519371 rank 2
2023-02-17 10:30:44,179 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 5
2023-02-17 10:30:45,059 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 4
2023-02-17 10:30:45,712 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 7
2023-02-17 10:30:45,798 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 0
2023-02-17 10:30:46,142 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 1
2023-02-17 10:30:48,164 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 3
2023-02-17 10:30:49,670 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 6
2023-02-17 10:31:00,230 DEBUG CV Batch 4/800 loss 16.852362 loss_att 18.058197 loss_ctc 27.383968 loss_rnnt 15.049903 hw_loss 0.294520 history loss 10.846854 rank 2
2023-02-17 10:31:00,281 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 5
2023-02-17 10:31:01,403 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 4
2023-02-17 10:31:02,087 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 1
2023-02-17 10:31:02,387 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 0
2023-02-17 10:31:02,898 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 7
2023-02-17 10:31:04,808 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 3
2023-02-17 10:31:06,305 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 6
2023-02-17 10:31:14,295 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 5
2023-02-17 10:31:15,257 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 4
2023-02-17 10:31:15,904 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 1
2023-02-17 10:31:16,402 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 0
2023-02-17 10:31:17,090 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 7
2023-02-17 10:31:18,433 DEBUG CV Batch 4/900 loss 17.858101 loss_att 26.972012 loss_ctc 37.386017 loss_rnnt 13.326960 hw_loss 0.196189 history loss 10.626718 rank 2
2023-02-17 10:31:19,441 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 3
2023-02-17 10:31:20,468 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307191 history loss 10.337692 rank 6
2023-02-17 10:31:28,474 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 5
2023-02-17 10:31:28,940 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 4
2023-02-17 10:31:30,027 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 1
2023-02-17 10:31:30,663 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 0
2023-02-17 10:31:31,039 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 7
2023-02-17 10:31:33,221 DEBUG CV Batch 4/1000 loss 8.045516 loss_att 9.267308 loss_ctc 11.042765 loss_rnnt 7.237689 hw_loss 0.307192 history loss 10.337692 rank 2
2023-02-17 10:31:33,598 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 3
2023-02-17 10:31:34,586 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 6
2023-02-17 10:31:41,744 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 5
2023-02-17 10:31:41,899 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 4
2023-02-17 10:31:43,352 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 1
2023-02-17 10:31:44,319 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 0
2023-02-17 10:31:44,679 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 7
2023-02-17 10:31:47,036 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 3
2023-02-17 10:31:48,194 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 6
2023-02-17 10:31:48,842 DEBUG CV Batch 4/1100 loss 7.817453 loss_att 7.543442 loss_ctc 11.688736 loss_rnnt 7.193751 hw_loss 0.304373 history loss 10.342929 rank 2
2023-02-17 10:31:56,424 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 5
2023-02-17 10:31:56,927 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 4
2023-02-17 10:31:57,521 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 1
2023-02-17 10:31:58,881 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 0
2023-02-17 10:31:58,967 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 7
2023-02-17 10:32:01,540 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 3
2023-02-17 10:32:02,397 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 6
2023-02-17 10:32:03,611 DEBUG CV Batch 4/1200 loss 14.407136 loss_att 17.344175 loss_ctc 20.763359 loss_rnnt 12.810697 hw_loss 0.302878 history loss 10.696061 rank 2
2023-02-17 10:32:11,094 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 5
2023-02-17 10:32:11,743 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 1
2023-02-17 10:32:11,819 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 4
2023-02-17 10:32:13,767 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 0
2023-02-17 10:32:14,026 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 7
2023-02-17 10:32:16,681 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 3
2023-02-17 10:32:17,758 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 6
2023-02-17 10:32:19,006 DEBUG CV Batch 4/1300 loss 9.008170 loss_att 8.999085 loss_ctc 13.813448 loss_rnnt 8.190684 hw_loss 0.334873 history loss 11.048299 rank 2
2023-02-17 10:32:26,303 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 5
2023-02-17 10:32:26,508 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 4
2023-02-17 10:32:27,666 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 1
2023-02-17 10:32:28,919 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 0
2023-02-17 10:32:29,787 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 7
2023-02-17 10:32:31,875 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 3
2023-02-17 10:32:33,198 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 6
2023-02-17 10:32:35,629 DEBUG CV Batch 4/1400 loss 17.340450 loss_att 34.729279 loss_ctc 27.034393 loss_rnnt 12.399893 hw_loss 0.319249 history loss 11.479337 rank 2
2023-02-17 10:32:42,735 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 4
2023-02-17 10:32:43,057 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 5
2023-02-17 10:32:43,672 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 1
2023-02-17 10:32:45,063 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 0
2023-02-17 10:32:46,683 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 7
2023-02-17 10:32:47,945 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 3
2023-02-17 10:32:49,451 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 6
2023-02-17 10:32:53,396 DEBUG CV Batch 4/1500 loss 13.507089 loss_att 13.406450 loss_ctc 15.877913 loss_rnnt 13.093585 hw_loss 0.220354 history loss 11.236301 rank 2
2023-02-17 10:32:57,011 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 4
2023-02-17 10:32:57,597 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 5
2023-02-17 10:32:58,157 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 1
2023-02-17 10:32:59,845 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 0
2023-02-17 10:33:01,237 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 7
2023-02-17 10:33:02,528 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 3
2023-02-17 10:33:03,520 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 6
2023-02-17 10:33:07,514 INFO Epoch 4 CV info cv_loss 10.968385986422748
2023-02-17 10:33:07,514 INFO Epoch 5 TRAIN info lr 0.0007745099289866192
2023-02-17 10:33:07,519 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:07,921 INFO Epoch 4 CV info cv_loss 10.968385985388991
2023-02-17 10:33:07,921 INFO Epoch 5 TRAIN info lr 0.0007743427264669895
2023-02-17 10:33:07,925 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:08,349 INFO Epoch 4 CV info cv_loss 10.968385987938923
2023-02-17 10:33:08,350 INFO Epoch 5 TRAIN info lr 0.000774231318261756
2023-02-17 10:33:08,355 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:09,377 DEBUG CV Batch 4/1600 loss 17.966255 loss_att 22.607094 loss_ctc 27.658409 loss_rnnt 15.647329 hw_loss 0.184636 history loss 11.143435 rank 2
2023-02-17 10:33:10,035 INFO Epoch 4 CV info cv_loss 10.968385982253265
2023-02-17 10:33:10,035 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/4.pt
2023-02-17 10:33:10,664 INFO Epoch 5 TRAIN info lr 0.0007743334406129372
2023-02-17 10:33:10,669 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:11,328 INFO Epoch 4 CV info cv_loss 10.96838598566466
2023-02-17 10:33:11,329 INFO Epoch 5 TRAIN info lr 0.0007746772398643196
2023-02-17 10:33:11,332 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:12,515 INFO Epoch 4 CV info cv_loss 10.968385985992017
2023-02-17 10:33:12,515 INFO Epoch 5 TRAIN info lr 0.0007744541827794007
2023-02-17 10:33:12,521 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:13,276 INFO Epoch 4 CV info cv_loss 10.96838598547514
2023-02-17 10:33:13,276 INFO Epoch 5 TRAIN info lr 0.0007746865380925978
2023-02-17 10:33:13,279 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:33:22,688 DEBUG CV Batch 4/1700 loss 12.153644 loss_att 12.107375 loss_ctc 19.743099 loss_rnnt 10.924203 hw_loss 0.425189 history loss 11.001495 rank 2
2023-02-17 10:33:32,897 INFO Epoch 4 CV info cv_loss 10.968385983614377
2023-02-17 10:33:32,898 INFO Epoch 5 TRAIN info lr 0.0007736564742601974
2023-02-17 10:33:32,903 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 10:34:42,968 DEBUG TRAIN Batch 5/0 loss 10.364916 loss_att 10.303772 loss_ctc 14.699864 loss_rnnt 9.594359 hw_loss 0.383984 lr 0.00077450 rank 4
2023-02-17 10:34:42,970 DEBUG TRAIN Batch 5/0 loss 11.886934 loss_att 11.129971 loss_ctc 14.665462 loss_rnnt 11.454945 hw_loss 0.399210 lr 0.00077468 rank 6
2023-02-17 10:34:42,971 DEBUG TRAIN Batch 5/0 loss 10.462595 loss_att 10.020628 loss_ctc 13.714198 loss_rnnt 9.916179 hw_loss 0.377368 lr 0.00077432 rank 0
2023-02-17 10:34:42,976 DEBUG TRAIN Batch 5/0 loss 12.474229 loss_att 12.987170 loss_ctc 16.134298 loss_rnnt 11.695024 hw_loss 0.353637 lr 0.00077422 rank 1
2023-02-17 10:34:42,981 DEBUG TRAIN Batch 5/0 loss 14.463301 loss_att 15.074018 loss_ctc 22.253698 loss_rnnt 13.131389 hw_loss 0.320715 lr 0.00077444 rank 3
2023-02-17 10:34:42,989 DEBUG TRAIN Batch 5/0 loss 13.926159 loss_att 13.942507 loss_ctc 16.911369 loss_rnnt 13.369251 hw_loss 0.291767 lr 0.00077433 rank 5
2023-02-17 10:34:42,998 DEBUG TRAIN Batch 5/0 loss 13.428413 loss_att 14.020608 loss_ctc 18.029068 loss_rnnt 12.535633 hw_loss 0.301727 lr 0.00077365 rank 2
2023-02-17 10:34:43,008 DEBUG TRAIN Batch 5/0 loss 9.783405 loss_att 10.247191 loss_ctc 13.337484 loss_rnnt 9.058899 hw_loss 0.296009 lr 0.00077467 rank 7
2023-02-17 10:35:58,511 DEBUG TRAIN Batch 5/100 loss 20.654274 loss_att 29.409386 loss_ctc 26.723137 loss_rnnt 17.977413 hw_loss 0.218734 lr 0.00077272 rank 2
2023-02-17 10:35:58,511 DEBUG TRAIN Batch 5/100 loss 36.301250 loss_att 43.112904 loss_ctc 59.171154 loss_rnnt 31.701925 hw_loss 0.351888 lr 0.00077352 rank 3
2023-02-17 10:35:58,511 DEBUG TRAIN Batch 5/100 loss 14.802084 loss_att 24.203087 loss_ctc 29.181351 loss_rnnt 10.882895 hw_loss 0.228287 lr 0.00077341 rank 5
2023-02-17 10:35:58,518 DEBUG TRAIN Batch 5/100 loss 23.225464 loss_att 29.670345 loss_ctc 45.860012 loss_rnnt 18.770773 hw_loss 0.277078 lr 0.00077340 rank 0
2023-02-17 10:35:58,540 DEBUG TRAIN Batch 5/100 loss 33.443577 loss_att 37.573082 loss_ctc 44.695694 loss_rnnt 31.028305 hw_loss 0.167038 lr 0.00077375 rank 6
2023-02-17 10:35:58,551 DEBUG TRAIN Batch 5/100 loss 26.900074 loss_att 34.416374 loss_ctc 48.076336 loss_rnnt 22.459282 hw_loss 0.213802 lr 0.00077330 rank 1
2023-02-17 10:35:58,556 DEBUG TRAIN Batch 5/100 loss 14.344925 loss_att 23.596445 loss_ctc 24.026688 loss_rnnt 11.013301 hw_loss 0.357035 lr 0.00077357 rank 4
2023-02-17 10:35:58,561 DEBUG TRAIN Batch 5/100 loss 15.511201 loss_att 19.043068 loss_ctc 23.813858 loss_rnnt 13.504699 hw_loss 0.362076 lr 0.00077374 rank 7
2023-02-17 10:37:14,015 DEBUG TRAIN Batch 5/200 loss 18.603502 loss_att 26.857702 loss_ctc 37.028008 loss_rnnt 14.368422 hw_loss 0.239324 lr 0.00077247 rank 0
2023-02-17 10:37:14,027 DEBUG TRAIN Batch 5/200 loss 16.462532 loss_att 20.882784 loss_ctc 26.886532 loss_rnnt 14.104660 hw_loss 0.157415 lr 0.00077180 rank 2
2023-02-17 10:37:14,027 DEBUG TRAIN Batch 5/200 loss 20.217989 loss_att 22.077982 loss_ctc 32.377441 loss_rnnt 18.107941 hw_loss 0.218981 lr 0.00077282 rank 7
2023-02-17 10:37:14,029 DEBUG TRAIN Batch 5/200 loss 34.607098 loss_att 38.358112 loss_ctc 50.691925 loss_rnnt 31.594662 hw_loss 0.220477 lr 0.00077265 rank 4
2023-02-17 10:37:14,031 DEBUG TRAIN Batch 5/200 loss 49.233536 loss_att 56.139820 loss_ctc 68.322205 loss_rnnt 45.168835 hw_loss 0.259283 lr 0.00077282 rank 6
2023-02-17 10:37:14,031 DEBUG TRAIN Batch 5/200 loss 13.135006 loss_att 20.009014 loss_ctc 25.684052 loss_rnnt 9.939777 hw_loss 0.276038 lr 0.00077259 rank 3
2023-02-17 10:37:14,031 DEBUG TRAIN Batch 5/200 loss 26.962563 loss_att 34.102268 loss_ctc 37.943100 loss_rnnt 23.955307 hw_loss 0.216083 lr 0.00077237 rank 1
2023-02-17 10:37:14,078 DEBUG TRAIN Batch 5/200 loss 13.687764 loss_att 19.793232 loss_ctc 28.204651 loss_rnnt 10.379978 hw_loss 0.283327 lr 0.00077248 rank 5
2023-02-17 10:38:35,066 DEBUG TRAIN Batch 5/300 loss 32.094673 loss_att 34.098568 loss_ctc 49.344887 loss_rnnt 29.269928 hw_loss 0.232380 lr 0.00077167 rank 3
2023-02-17 10:38:35,081 DEBUG TRAIN Batch 5/300 loss 18.299929 loss_att 23.834419 loss_ctc 29.901714 loss_rnnt 15.497319 hw_loss 0.279014 lr 0.00077189 rank 7
2023-02-17 10:38:35,098 DEBUG TRAIN Batch 5/300 loss 29.821001 loss_att 33.829247 loss_ctc 35.884430 loss_rnnt 28.100039 hw_loss 0.207854 lr 0.00077190 rank 6
2023-02-17 10:38:35,119 DEBUG TRAIN Batch 5/300 loss 16.934736 loss_att 24.231911 loss_ctc 28.786953 loss_rnnt 13.743839 hw_loss 0.283436 lr 0.00077156 rank 5
2023-02-17 10:38:35,121 DEBUG TRAIN Batch 5/300 loss 29.645113 loss_att 38.475952 loss_ctc 53.817421 loss_rnnt 24.523148 hw_loss 0.249044 lr 0.00077088 rank 2
2023-02-17 10:38:35,125 DEBUG TRAIN Batch 5/300 loss 21.447781 loss_att 26.972589 loss_ctc 29.896912 loss_rnnt 19.050020 hw_loss 0.311715 lr 0.00077173 rank 4
2023-02-17 10:38:35,126 DEBUG TRAIN Batch 5/300 loss 30.444736 loss_att 37.021969 loss_ctc 56.566486 loss_rnnt 25.518936 hw_loss 0.238978 lr 0.00077155 rank 0
2023-02-17 10:38:35,182 DEBUG TRAIN Batch 5/300 loss 18.577776 loss_att 26.491037 loss_ctc 33.747757 loss_rnnt 14.786205 hw_loss 0.349228 lr 0.00077145 rank 1
2023-02-17 10:40:06,961 DEBUG TRAIN Batch 5/400 loss 24.170052 loss_att 32.140358 loss_ctc 49.677769 loss_rnnt 19.098385 hw_loss 0.143581 lr 0.00077064 rank 0
2023-02-17 10:40:06,962 DEBUG TRAIN Batch 5/400 loss 23.575066 loss_att 27.000044 loss_ctc 39.725616 loss_rnnt 20.600185 hw_loss 0.255896 lr 0.00077076 rank 3
2023-02-17 10:40:06,963 DEBUG TRAIN Batch 5/400 loss 21.907160 loss_att 26.870651 loss_ctc 38.834248 loss_rnnt 18.533869 hw_loss 0.231837 lr 0.00077054 rank 1
2023-02-17 10:40:06,964 DEBUG TRAIN Batch 5/400 loss 22.774441 loss_att 27.485670 loss_ctc 30.019592 loss_rnnt 20.719257 hw_loss 0.275468 lr 0.00077098 rank 7
2023-02-17 10:40:06,966 DEBUG TRAIN Batch 5/400 loss 19.481207 loss_att 21.574924 loss_ctc 28.357693 loss_rnnt 17.776463 hw_loss 0.192131 lr 0.00077081 rank 4
2023-02-17 10:40:06,968 DEBUG TRAIN Batch 5/400 loss 49.174397 loss_att 55.057335 loss_ctc 65.246788 loss_rnnt 45.666298 hw_loss 0.353475 lr 0.00077065 rank 5
2023-02-17 10:40:06,997 DEBUG TRAIN Batch 5/400 loss 21.399446 loss_att 25.675102 loss_ctc 34.732960 loss_rnnt 18.661919 hw_loss 0.196117 lr 0.00076997 rank 2
2023-02-17 10:40:07,012 DEBUG TRAIN Batch 5/400 loss 14.172639 loss_att 17.384550 loss_ctc 23.341171 loss_rnnt 12.181661 hw_loss 0.236485 lr 0.00077098 rank 6
2023-02-17 10:41:23,172 DEBUG TRAIN Batch 5/500 loss 28.707293 loss_att 35.708229 loss_ctc 49.267719 loss_rnnt 24.397058 hw_loss 0.316222 lr 0.00076984 rank 3
2023-02-17 10:41:23,173 DEBUG TRAIN Batch 5/500 loss 16.473410 loss_att 19.729177 loss_ctc 30.073511 loss_rnnt 13.881176 hw_loss 0.239503 lr 0.00077006 rank 7
2023-02-17 10:41:23,175 DEBUG TRAIN Batch 5/500 loss 19.983624 loss_att 22.459290 loss_ctc 26.447908 loss_rnnt 18.498791 hw_loss 0.239612 lr 0.00076990 rank 4
2023-02-17 10:41:23,175 DEBUG TRAIN Batch 5/500 loss 13.916522 loss_att 18.272387 loss_ctc 28.131321 loss_rnnt 10.958873 hw_loss 0.358443 lr 0.00076972 rank 0
2023-02-17 10:41:23,177 DEBUG TRAIN Batch 5/500 loss 26.724535 loss_att 30.299990 loss_ctc 38.081791 loss_rnnt 24.409277 hw_loss 0.160999 lr 0.00076973 rank 5
2023-02-17 10:41:23,208 DEBUG TRAIN Batch 5/500 loss 46.402256 loss_att 48.229366 loss_ctc 78.170052 loss_rnnt 41.695461 hw_loss 0.198124 lr 0.00077007 rank 6
2023-02-17 10:41:23,217 DEBUG TRAIN Batch 5/500 loss 20.341358 loss_att 25.065582 loss_ctc 31.469244 loss_rnnt 17.731171 hw_loss 0.340543 lr 0.00076906 rank 2
2023-02-17 10:41:23,251 DEBUG TRAIN Batch 5/500 loss 20.915161 loss_att 26.591846 loss_ctc 33.663155 loss_rnnt 17.895802 hw_loss 0.345547 lr 0.00076962 rank 1
2023-02-17 10:42:39,776 DEBUG TRAIN Batch 5/600 loss 15.169265 loss_att 15.440947 loss_ctc 24.218178 loss_rnnt 13.754871 hw_loss 0.287878 lr 0.00076916 rank 6
2023-02-17 10:42:39,776 DEBUG TRAIN Batch 5/600 loss 25.419868 loss_att 28.083210 loss_ctc 38.909233 loss_rnnt 22.955093 hw_loss 0.250356 lr 0.00076893 rank 3
2023-02-17 10:42:39,777 DEBUG TRAIN Batch 5/600 loss 11.904784 loss_att 13.603064 loss_ctc 18.201046 loss_rnnt 10.587695 hw_loss 0.258622 lr 0.00076815 rank 2
2023-02-17 10:42:39,777 DEBUG TRAIN Batch 5/600 loss 9.505134 loss_att 12.104352 loss_ctc 14.320887 loss_rnnt 8.201374 hw_loss 0.265903 lr 0.00076882 rank 5
2023-02-17 10:42:39,778 DEBUG TRAIN Batch 5/600 loss 22.557264 loss_att 26.874702 loss_ctc 33.892891 loss_rnnt 20.031841 hw_loss 0.282220 lr 0.00076871 rank 1
2023-02-17 10:42:39,781 DEBUG TRAIN Batch 5/600 loss 17.981344 loss_att 18.859850 loss_ctc 29.027378 loss_rnnt 16.140743 hw_loss 0.360177 lr 0.00076915 rank 7
2023-02-17 10:42:39,783 DEBUG TRAIN Batch 5/600 loss 10.148314 loss_att 14.204239 loss_ctc 18.137726 loss_rnnt 8.086811 hw_loss 0.346993 lr 0.00076881 rank 0
2023-02-17 10:42:39,784 DEBUG TRAIN Batch 5/600 loss 19.939430 loss_att 23.720766 loss_ctc 29.350697 loss_rnnt 17.785563 hw_loss 0.267680 lr 0.00076899 rank 4
2023-02-17 10:44:05,225 DEBUG TRAIN Batch 5/700 loss 22.720562 loss_att 28.340759 loss_ctc 37.686729 loss_rnnt 19.456541 hw_loss 0.270926 lr 0.00076781 rank 1
2023-02-17 10:44:05,226 DEBUG TRAIN Batch 5/700 loss 7.840695 loss_att 12.280529 loss_ctc 15.759616 loss_rnnt 5.720993 hw_loss 0.329775 lr 0.00076824 rank 7
2023-02-17 10:44:05,225 DEBUG TRAIN Batch 5/700 loss 15.137496 loss_att 24.414421 loss_ctc 34.773739 loss_rnnt 10.587788 hw_loss 0.142797 lr 0.00076791 rank 5
2023-02-17 10:44:05,232 DEBUG TRAIN Batch 5/700 loss 19.883442 loss_att 25.213215 loss_ctc 31.280394 loss_rnnt 17.152143 hw_loss 0.273283 lr 0.00076802 rank 3
2023-02-17 10:44:05,256 DEBUG TRAIN Batch 5/700 loss 20.243097 loss_att 27.313732 loss_ctc 33.187271 loss_rnnt 16.993576 hw_loss 0.205323 lr 0.00076724 rank 2
2023-02-17 10:44:05,263 DEBUG TRAIN Batch 5/700 loss 17.247921 loss_att 23.781025 loss_ctc 34.563366 loss_rnnt 13.471481 hw_loss 0.302047 lr 0.00076791 rank 0
2023-02-17 10:44:05,308 DEBUG TRAIN Batch 5/700 loss 22.729034 loss_att 29.204367 loss_ctc 29.377075 loss_rnnt 20.385017 hw_loss 0.304770 lr 0.00076825 rank 6
2023-02-17 10:44:05,320 DEBUG TRAIN Batch 5/700 loss 45.546932 loss_att 47.761101 loss_ctc 63.612747 loss_rnnt 42.586666 hw_loss 0.203730 lr 0.00076808 rank 4
2023-02-17 10:45:31,117 DEBUG TRAIN Batch 5/800 loss 40.949738 loss_att 54.927124 loss_ctc 70.675522 loss_rnnt 34.091446 hw_loss 0.186333 lr 0.00076734 rank 6
2023-02-17 10:45:31,122 DEBUG TRAIN Batch 5/800 loss 31.082027 loss_att 37.097286 loss_ctc 51.487251 loss_rnnt 27.050299 hw_loss 0.202466 lr 0.00076717 rank 4
2023-02-17 10:45:31,122 DEBUG TRAIN Batch 5/800 loss 20.325844 loss_att 27.891844 loss_ctc 28.922644 loss_rnnt 17.588421 hw_loss 0.146217 lr 0.00076700 rank 0
2023-02-17 10:45:31,122 DEBUG TRAIN Batch 5/800 loss 13.434000 loss_att 19.072887 loss_ctc 23.902603 loss_rnnt 10.789555 hw_loss 0.226601 lr 0.00076701 rank 5
2023-02-17 10:45:31,122 DEBUG TRAIN Batch 5/800 loss 16.776142 loss_att 19.746384 loss_ctc 22.760925 loss_rnnt 15.242064 hw_loss 0.266359 lr 0.00076734 rank 7
2023-02-17 10:45:31,123 DEBUG TRAIN Batch 5/800 loss 24.958750 loss_att 33.064182 loss_ctc 45.565731 loss_rnnt 20.469910 hw_loss 0.225290 lr 0.00076690 rank 1
2023-02-17 10:45:31,125 DEBUG TRAIN Batch 5/800 loss 15.057304 loss_att 19.828102 loss_ctc 24.720953 loss_rnnt 12.694522 hw_loss 0.225256 lr 0.00076634 rank 2
2023-02-17 10:45:31,126 DEBUG TRAIN Batch 5/800 loss 32.292686 loss_att 35.314697 loss_ctc 49.787292 loss_rnnt 29.215042 hw_loss 0.263673 lr 0.00076712 rank 3
2023-02-17 10:46:48,228 DEBUG TRAIN Batch 5/900 loss 18.660715 loss_att 24.309734 loss_ctc 36.349571 loss_rnnt 15.039573 hw_loss 0.249049 lr 0.00076643 rank 7
2023-02-17 10:46:48,229 DEBUG TRAIN Batch 5/900 loss 16.272623 loss_att 20.586445 loss_ctc 38.085346 loss_rnnt 12.365988 hw_loss 0.254076 lr 0.00076600 rank 1
2023-02-17 10:46:48,229 DEBUG TRAIN Batch 5/900 loss 44.617149 loss_att 51.193352 loss_ctc 64.254440 loss_rnnt 40.564346 hw_loss 0.223601 lr 0.00076644 rank 6
2023-02-17 10:46:48,229 DEBUG TRAIN Batch 5/900 loss 19.201504 loss_att 20.421337 loss_ctc 22.320614 loss_rnnt 18.411634 hw_loss 0.243793 lr 0.00076610 rank 0
2023-02-17 10:46:48,269 DEBUG TRAIN Batch 5/900 loss 26.343809 loss_att 28.309261 loss_ctc 42.819691 loss_rnnt 23.649086 hw_loss 0.196589 lr 0.00076544 rank 2
2023-02-17 10:46:48,284 DEBUG TRAIN Batch 5/900 loss 25.689032 loss_att 30.186808 loss_ctc 40.832260 loss_rnnt 22.654280 hw_loss 0.217685 lr 0.00076627 rank 4
2023-02-17 10:46:48,284 DEBUG TRAIN Batch 5/900 loss 20.854647 loss_att 21.869875 loss_ctc 29.613348 loss_rnnt 19.350430 hw_loss 0.250022 lr 0.00076622 rank 3
2023-02-17 10:46:48,286 DEBUG TRAIN Batch 5/900 loss 14.199760 loss_att 21.371346 loss_ctc 23.853546 loss_rnnt 11.354217 hw_loss 0.232604 lr 0.00076611 rank 5
2023-02-17 10:48:09,319 DEBUG TRAIN Batch 5/1000 loss 12.881774 loss_att 14.986723 loss_ctc 16.413235 loss_rnnt 11.852427 hw_loss 0.257804 lr 0.00076510 rank 1
2023-02-17 10:48:09,329 DEBUG TRAIN Batch 5/1000 loss 26.538038 loss_att 29.909363 loss_ctc 36.765118 loss_rnnt 24.344206 hw_loss 0.292423 lr 0.00076520 rank 0
2023-02-17 10:48:09,332 DEBUG TRAIN Batch 5/1000 loss 25.933064 loss_att 27.911766 loss_ctc 36.643112 loss_rnnt 24.022606 hw_loss 0.162583 lr 0.00076521 rank 5
2023-02-17 10:48:09,332 DEBUG TRAIN Batch 5/1000 loss 14.192511 loss_att 17.822401 loss_ctc 27.092571 loss_rnnt 11.590810 hw_loss 0.291964 lr 0.00076532 rank 3
2023-02-17 10:48:09,359 DEBUG TRAIN Batch 5/1000 loss 18.596962 loss_att 19.999186 loss_ctc 30.176226 loss_rnnt 16.635067 hw_loss 0.257904 lr 0.00076537 rank 4
2023-02-17 10:48:09,388 DEBUG TRAIN Batch 5/1000 loss 23.734964 loss_att 26.622499 loss_ctc 35.803802 loss_rnnt 21.401203 hw_loss 0.275763 lr 0.00076455 rank 2
2023-02-17 10:48:09,402 DEBUG TRAIN Batch 5/1000 loss 20.010029 loss_att 25.721073 loss_ctc 38.753510 loss_rnnt 16.304441 hw_loss 0.120465 lr 0.00076554 rank 6
2023-02-17 10:48:09,405 DEBUG TRAIN Batch 5/1000 loss 38.708263 loss_att 44.406132 loss_ctc 51.367462 loss_rnnt 35.740765 hw_loss 0.262569 lr 0.00076553 rank 7
2023-02-17 10:49:42,649 DEBUG TRAIN Batch 5/1100 loss 23.618938 loss_att 26.864292 loss_ctc 39.883682 loss_rnnt 20.652464 hw_loss 0.278945 lr 0.00076442 rank 3
2023-02-17 10:49:42,650 DEBUG TRAIN Batch 5/1100 loss 16.600977 loss_att 21.961351 loss_ctc 25.881245 loss_rnnt 14.097145 hw_loss 0.364482 lr 0.00076421 rank 1
2023-02-17 10:49:42,651 DEBUG TRAIN Batch 5/1100 loss 16.791725 loss_att 23.150883 loss_ctc 29.024181 loss_rnnt 13.786572 hw_loss 0.191866 lr 0.00076432 rank 5
2023-02-17 10:49:42,654 DEBUG TRAIN Batch 5/1100 loss 15.468199 loss_att 24.905209 loss_ctc 26.580267 loss_rnnt 11.978889 hw_loss 0.225560 lr 0.00076431 rank 0
2023-02-17 10:49:42,654 DEBUG TRAIN Batch 5/1100 loss 25.262184 loss_att 26.771673 loss_ctc 37.078079 loss_rnnt 23.273968 hw_loss 0.207878 lr 0.00076465 rank 6
2023-02-17 10:49:42,664 DEBUG TRAIN Batch 5/1100 loss 28.299881 loss_att 35.288860 loss_ctc 51.087891 loss_rnnt 23.746315 hw_loss 0.220065 lr 0.00076464 rank 7
2023-02-17 10:49:42,696 DEBUG TRAIN Batch 5/1100 loss 28.352259 loss_att 32.186687 loss_ctc 48.174660 loss_rnnt 24.713083 hw_loss 0.429942 lr 0.00076366 rank 2
2023-02-17 10:49:42,710 DEBUG TRAIN Batch 5/1100 loss 23.678213 loss_att 29.191748 loss_ctc 44.580666 loss_rnnt 19.651310 hw_loss 0.257260 lr 0.00076448 rank 4
2023-02-17 10:50:58,652 DEBUG TRAIN Batch 5/1200 loss 23.848192 loss_att 25.953197 loss_ctc 34.833725 loss_rnnt 21.758936 hw_loss 0.381594 lr 0.00076375 rank 7
2023-02-17 10:50:58,652 DEBUG TRAIN Batch 5/1200 loss 18.093307 loss_att 17.263136 loss_ctc 28.088287 loss_rnnt 16.824492 hw_loss 0.191601 lr 0.00076353 rank 3
2023-02-17 10:50:58,652 DEBUG TRAIN Batch 5/1200 loss 17.541077 loss_att 20.341951 loss_ctc 29.308857 loss_rnnt 15.288986 hw_loss 0.230397 lr 0.00076332 rank 1
2023-02-17 10:50:58,652 DEBUG TRAIN Batch 5/1200 loss 16.201883 loss_att 21.801376 loss_ctc 24.759827 loss_rnnt 13.743832 hw_loss 0.369548 lr 0.00076342 rank 0
2023-02-17 10:50:58,654 DEBUG TRAIN Batch 5/1200 loss 12.819802 loss_att 16.322456 loss_ctc 19.024578 loss_rnnt 11.152400 hw_loss 0.261691 lr 0.00076277 rank 2
2023-02-17 10:50:58,655 DEBUG TRAIN Batch 5/1200 loss 15.463978 loss_att 17.141937 loss_ctc 22.562269 loss_rnnt 14.050483 hw_loss 0.246496 lr 0.00076375 rank 6
2023-02-17 10:50:58,656 DEBUG TRAIN Batch 5/1200 loss 16.149462 loss_att 18.732170 loss_ctc 27.177649 loss_rnnt 14.022000 hw_loss 0.263430 lr 0.00076343 rank 5
2023-02-17 10:50:58,661 DEBUG TRAIN Batch 5/1200 loss 12.190145 loss_att 21.464479 loss_ctc 26.820667 loss_rnnt 8.248240 hw_loss 0.255568 lr 0.00076359 rank 4
2023-02-17 10:52:13,739 DEBUG TRAIN Batch 5/1300 loss 22.072083 loss_att 27.515030 loss_ctc 28.250402 loss_rnnt 20.037380 hw_loss 0.229382 lr 0.00076287 rank 6
2023-02-17 10:52:13,740 DEBUG TRAIN Batch 5/1300 loss 15.806326 loss_att 19.420317 loss_ctc 20.829845 loss_rnnt 14.237231 hw_loss 0.330926 lr 0.00076254 rank 5
2023-02-17 10:52:13,741 DEBUG TRAIN Batch 5/1300 loss 20.634993 loss_att 21.563286 loss_ctc 26.990215 loss_rnnt 19.463196 hw_loss 0.260200 lr 0.00076188 rank 2
2023-02-17 10:52:13,741 DEBUG TRAIN Batch 5/1300 loss 18.012854 loss_att 22.886665 loss_ctc 31.061092 loss_rnnt 15.135717 hw_loss 0.304888 lr 0.00076286 rank 7
2023-02-17 10:52:13,742 DEBUG TRAIN Batch 5/1300 loss 10.600376 loss_att 12.740782 loss_ctc 15.593158 loss_rnnt 9.344160 hw_loss 0.304557 lr 0.00076253 rank 0
2023-02-17 10:52:13,747 DEBUG TRAIN Batch 5/1300 loss 14.575709 loss_att 16.313906 loss_ctc 21.973497 loss_rnnt 13.027098 hw_loss 0.402376 lr 0.00076243 rank 1
2023-02-17 10:52:13,752 DEBUG TRAIN Batch 5/1300 loss 40.612591 loss_att 48.482655 loss_ctc 64.615814 loss_rnnt 35.681610 hw_loss 0.293501 lr 0.00076264 rank 3
2023-02-17 10:52:13,753 DEBUG TRAIN Batch 5/1300 loss 14.491404 loss_att 14.306950 loss_ctc 20.283102 loss_rnnt 13.555099 hw_loss 0.376814 lr 0.00076270 rank 4
2023-02-17 10:53:37,779 DEBUG TRAIN Batch 5/1400 loss 21.299824 loss_att 29.602905 loss_ctc 43.435394 loss_rnnt 16.558403 hw_loss 0.242614 lr 0.00076100 rank 2
2023-02-17 10:53:37,798 DEBUG TRAIN Batch 5/1400 loss 20.438389 loss_att 28.049112 loss_ctc 33.967758 loss_rnnt 17.014915 hw_loss 0.182647 lr 0.00076155 rank 1
2023-02-17 10:53:37,820 DEBUG TRAIN Batch 5/1400 loss 14.939637 loss_att 18.859245 loss_ctc 28.790771 loss_rnnt 12.244757 hw_loss 0.120267 lr 0.00076197 rank 7
2023-02-17 10:53:37,844 DEBUG TRAIN Batch 5/1400 loss 23.343800 loss_att 33.069275 loss_ctc 44.135239 loss_rnnt 18.493536 hw_loss 0.249327 lr 0.00076165 rank 5
2023-02-17 10:53:37,866 DEBUG TRAIN Batch 5/1400 loss 13.236473 loss_att 19.807970 loss_ctc 24.313354 loss_rnnt 10.305721 hw_loss 0.261627 lr 0.00076164 rank 0
2023-02-17 10:53:37,869 DEBUG TRAIN Batch 5/1400 loss 27.800779 loss_att 34.223011 loss_ctc 49.063915 loss_rnnt 23.547588 hw_loss 0.250616 lr 0.00076181 rank 4
2023-02-17 10:53:37,904 DEBUG TRAIN Batch 5/1400 loss 22.775976 loss_att 27.392424 loss_ctc 35.909779 loss_rnnt 20.010853 hw_loss 0.169989 lr 0.00076176 rank 3
2023-02-17 10:53:37,910 DEBUG TRAIN Batch 5/1400 loss 10.882284 loss_att 18.480419 loss_ctc 20.662197 loss_rnnt 7.958570 hw_loss 0.187684 lr 0.00076198 rank 6
2023-02-17 10:55:04,627 DEBUG TRAIN Batch 5/1500 loss 20.510036 loss_att 23.887514 loss_ctc 30.065405 loss_rnnt 18.444553 hw_loss 0.217381 lr 0.00076077 rank 5
2023-02-17 10:55:04,630 DEBUG TRAIN Batch 5/1500 loss 15.436989 loss_att 19.619274 loss_ctc 29.159107 loss_rnnt 12.667999 hw_loss 0.192968 lr 0.00076093 rank 4
2023-02-17 10:55:04,629 DEBUG TRAIN Batch 5/1500 loss 18.562206 loss_att 24.615284 loss_ctc 33.355900 loss_rnnt 15.278526 hw_loss 0.188572 lr 0.00076110 rank 6
2023-02-17 10:55:04,630 DEBUG TRAIN Batch 5/1500 loss 39.857281 loss_att 48.020378 loss_ctc 66.097626 loss_rnnt 34.548958 hw_loss 0.331855 lr 0.00076066 rank 1
2023-02-17 10:55:04,630 DEBUG TRAIN Batch 5/1500 loss 10.159523 loss_att 15.821242 loss_ctc 23.320736 loss_rnnt 7.160831 hw_loss 0.209097 lr 0.00076076 rank 0
2023-02-17 10:55:04,631 DEBUG TRAIN Batch 5/1500 loss 20.970453 loss_att 28.458399 loss_ctc 38.924347 loss_rnnt 16.961521 hw_loss 0.220293 lr 0.00076012 rank 2
2023-02-17 10:55:04,632 DEBUG TRAIN Batch 5/1500 loss 22.495886 loss_att 28.521847 loss_ctc 37.526722 loss_rnnt 19.168175 hw_loss 0.222009 lr 0.00076109 rank 7
2023-02-17 10:55:04,632 DEBUG TRAIN Batch 5/1500 loss 15.036799 loss_att 23.765429 loss_ctc 37.450928 loss_rnnt 10.222815 hw_loss 0.149452 lr 0.00076088 rank 3
2023-02-17 10:56:19,401 DEBUG TRAIN Batch 5/1600 loss 17.248732 loss_att 24.650913 loss_ctc 30.106949 loss_rnnt 13.884627 hw_loss 0.317321 lr 0.00076021 rank 7
2023-02-17 10:56:19,401 DEBUG TRAIN Batch 5/1600 loss 20.236464 loss_att 23.867464 loss_ctc 36.497753 loss_rnnt 17.219242 hw_loss 0.230341 lr 0.00075989 rank 5
2023-02-17 10:56:19,403 DEBUG TRAIN Batch 5/1600 loss 30.075975 loss_att 35.343800 loss_ctc 44.150528 loss_rnnt 27.038235 hw_loss 0.201698 lr 0.00076005 rank 4
2023-02-17 10:56:19,404 DEBUG TRAIN Batch 5/1600 loss 21.964180 loss_att 23.628601 loss_ctc 37.149292 loss_rnnt 19.442490 hw_loss 0.307730 lr 0.00075924 rank 2
2023-02-17 10:56:19,431 DEBUG TRAIN Batch 5/1600 loss 18.178978 loss_att 24.909252 loss_ctc 34.425644 loss_rnnt 14.595830 hw_loss 0.132883 lr 0.00076000 rank 3
2023-02-17 10:56:19,432 DEBUG TRAIN Batch 5/1600 loss 36.636810 loss_att 41.457054 loss_ctc 55.155060 loss_rnnt 33.074139 hw_loss 0.242858 lr 0.00076022 rank 6
2023-02-17 10:56:19,435 DEBUG TRAIN Batch 5/1600 loss 15.932079 loss_att 19.385365 loss_ctc 26.951809 loss_rnnt 13.642097 hw_loss 0.243806 lr 0.00075979 rank 1
2023-02-17 10:56:19,448 DEBUG TRAIN Batch 5/1600 loss 21.393524 loss_att 25.350174 loss_ctc 31.532787 loss_rnnt 19.132845 hw_loss 0.220213 lr 0.00075988 rank 0
2023-02-17 10:57:37,196 DEBUG TRAIN Batch 5/1700 loss 43.277969 loss_att 46.919415 loss_ctc 67.089828 loss_rnnt 39.216537 hw_loss 0.296673 lr 0.00075934 rank 6
2023-02-17 10:57:37,210 DEBUG TRAIN Batch 5/1700 loss 13.657545 loss_att 18.295116 loss_ctc 23.355843 loss_rnnt 11.278767 hw_loss 0.296544 lr 0.00075933 rank 7
2023-02-17 10:57:37,216 DEBUG TRAIN Batch 5/1700 loss 17.443380 loss_att 22.027542 loss_ctc 27.996128 loss_rnnt 14.950710 hw_loss 0.316512 lr 0.00075917 rank 4
2023-02-17 10:57:37,224 DEBUG TRAIN Batch 5/1700 loss 23.462410 loss_att 28.843834 loss_ctc 40.067055 loss_rnnt 20.018681 hw_loss 0.287799 lr 0.00075837 rank 2
2023-02-17 10:57:37,224 DEBUG TRAIN Batch 5/1700 loss 22.813515 loss_att 23.216972 loss_ctc 33.441708 loss_rnnt 21.211977 hw_loss 0.194536 lr 0.00075901 rank 0
2023-02-17 10:57:37,249 DEBUG TRAIN Batch 5/1700 loss 17.503353 loss_att 21.612326 loss_ctc 27.956146 loss_rnnt 15.146379 hw_loss 0.265263 lr 0.00075901 rank 5
2023-02-17 10:57:37,252 DEBUG TRAIN Batch 5/1700 loss 20.589741 loss_att 23.444817 loss_ctc 30.439510 loss_rnnt 18.604557 hw_loss 0.189125 lr 0.00075912 rank 3
2023-02-17 10:57:37,262 DEBUG TRAIN Batch 5/1700 loss 24.540009 loss_att 30.820263 loss_ctc 41.361248 loss_rnnt 20.948711 hw_loss 0.173279 lr 0.00075891 rank 1
2023-02-17 10:59:11,949 DEBUG TRAIN Batch 5/1800 loss 15.490977 loss_att 21.563171 loss_ctc 26.019598 loss_rnnt 12.739590 hw_loss 0.249622 lr 0.00075814 rank 5
2023-02-17 10:59:11,949 DEBUG TRAIN Batch 5/1800 loss 20.372532 loss_att 25.900000 loss_ctc 33.330273 loss_rnnt 17.341413 hw_loss 0.371114 lr 0.00075825 rank 3
2023-02-17 10:59:11,949 DEBUG TRAIN Batch 5/1800 loss 17.596394 loss_att 20.969090 loss_ctc 23.015882 loss_rnnt 16.041035 hw_loss 0.296664 lr 0.00075750 rank 2
2023-02-17 10:59:11,949 DEBUG TRAIN Batch 5/1800 loss 25.644268 loss_att 29.404192 loss_ctc 35.243332 loss_rnnt 23.436361 hw_loss 0.330088 lr 0.00075846 rank 6
2023-02-17 10:59:11,950 DEBUG TRAIN Batch 5/1800 loss 16.961567 loss_att 21.786772 loss_ctc 30.056236 loss_rnnt 14.099531 hw_loss 0.283199 lr 0.00075830 rank 4
2023-02-17 10:59:11,951 DEBUG TRAIN Batch 5/1800 loss 14.806816 loss_att 18.546890 loss_ctc 24.647856 loss_rnnt 12.543104 hw_loss 0.381672 lr 0.00075846 rank 7
2023-02-17 10:59:11,955 DEBUG TRAIN Batch 5/1800 loss 17.785583 loss_att 18.743399 loss_ctc 25.394125 loss_rnnt 16.394218 hw_loss 0.347497 lr 0.00075804 rank 1
2023-02-17 10:59:12,018 DEBUG TRAIN Batch 5/1800 loss 34.397373 loss_att 36.004040 loss_ctc 54.947567 loss_rnnt 31.181927 hw_loss 0.288913 lr 0.00075813 rank 0
2023-02-17 11:00:28,691 DEBUG TRAIN Batch 5/1900 loss 15.967248 loss_att 19.519218 loss_ctc 26.221294 loss_rnnt 13.711134 hw_loss 0.334713 lr 0.00075738 rank 3
2023-02-17 11:00:28,692 DEBUG TRAIN Batch 5/1900 loss 15.756760 loss_att 14.872398 loss_ctc 23.563604 loss_rnnt 14.718673 hw_loss 0.326339 lr 0.00075726 rank 0
2023-02-17 11:00:28,696 DEBUG TRAIN Batch 5/1900 loss 12.891735 loss_att 14.899709 loss_ctc 19.884216 loss_rnnt 11.435846 hw_loss 0.228679 lr 0.00075663 rank 2
2023-02-17 11:00:28,696 DEBUG TRAIN Batch 5/1900 loss 10.564978 loss_att 11.706687 loss_ctc 15.607552 loss_rnnt 9.521332 hw_loss 0.268053 lr 0.00075727 rank 5
2023-02-17 11:00:28,697 DEBUG TRAIN Batch 5/1900 loss 35.109406 loss_att 42.122246 loss_ctc 52.061180 loss_rnnt 31.336447 hw_loss 0.206538 lr 0.00075758 rank 7
2023-02-17 11:00:28,698 DEBUG TRAIN Batch 5/1900 loss 14.177462 loss_att 13.934095 loss_ctc 19.364639 loss_rnnt 13.352979 hw_loss 0.340371 lr 0.00075717 rank 1
2023-02-17 11:00:28,703 DEBUG TRAIN Batch 5/1900 loss 20.070803 loss_att 20.139931 loss_ctc 28.501377 loss_rnnt 18.782782 hw_loss 0.281471 lr 0.00075743 rank 4
2023-02-17 11:00:28,738 DEBUG TRAIN Batch 5/1900 loss 14.580983 loss_att 15.472411 loss_ctc 21.938293 loss_rnnt 13.249254 hw_loss 0.323377 lr 0.00075759 rank 6
2023-02-17 11:01:44,263 DEBUG TRAIN Batch 5/2000 loss 27.466084 loss_att 33.519119 loss_ctc 40.732346 loss_rnnt 24.272720 hw_loss 0.401101 lr 0.00075640 rank 0
2023-02-17 11:01:44,264 DEBUG TRAIN Batch 5/2000 loss 18.031750 loss_att 24.083324 loss_ctc 39.313667 loss_rnnt 13.863417 hw_loss 0.225807 lr 0.00075672 rank 7
2023-02-17 11:01:44,267 DEBUG TRAIN Batch 5/2000 loss 21.569559 loss_att 24.532444 loss_ctc 32.780182 loss_rnnt 19.351934 hw_loss 0.244309 lr 0.00075673 rank 6
2023-02-17 11:01:44,270 DEBUG TRAIN Batch 5/2000 loss 17.986904 loss_att 22.035889 loss_ctc 28.477697 loss_rnnt 15.642210 hw_loss 0.255236 lr 0.00075651 rank 3
2023-02-17 11:01:44,270 DEBUG TRAIN Batch 5/2000 loss 11.883699 loss_att 18.036406 loss_ctc 22.750473 loss_rnnt 9.088610 hw_loss 0.216833 lr 0.00075640 rank 5
2023-02-17 11:01:44,269 DEBUG TRAIN Batch 5/2000 loss 46.403862 loss_att 54.261391 loss_ctc 73.574394 loss_rnnt 41.088902 hw_loss 0.226336 lr 0.00075656 rank 4
2023-02-17 11:01:44,274 DEBUG TRAIN Batch 5/2000 loss 24.311584 loss_att 32.163933 loss_ctc 38.233917 loss_rnnt 20.750105 hw_loss 0.252560 lr 0.00075576 rank 2
2023-02-17 11:01:44,320 DEBUG TRAIN Batch 5/2000 loss 12.527899 loss_att 15.719252 loss_ctc 16.515532 loss_rnnt 11.244759 hw_loss 0.212221 lr 0.00075630 rank 1
2023-02-17 11:03:07,773 DEBUG TRAIN Batch 5/2100 loss 14.051416 loss_att 16.993652 loss_ctc 18.766327 loss_rnnt 12.681876 hw_loss 0.285821 lr 0.00075544 rank 1
2023-02-17 11:03:07,777 DEBUG TRAIN Batch 5/2100 loss 14.078506 loss_att 20.591839 loss_ctc 21.913742 loss_rnnt 11.600398 hw_loss 0.245148 lr 0.00075564 rank 3
2023-02-17 11:03:07,782 DEBUG TRAIN Batch 5/2100 loss 8.379419 loss_att 14.665340 loss_ctc 18.192532 loss_rnnt 5.663785 hw_loss 0.281314 lr 0.00075554 rank 5
2023-02-17 11:03:07,794 DEBUG TRAIN Batch 5/2100 loss 27.665709 loss_att 29.969769 loss_ctc 42.543251 loss_rnnt 25.046751 hw_loss 0.327135 lr 0.00075586 rank 6
2023-02-17 11:03:07,804 DEBUG TRAIN Batch 5/2100 loss 22.648119 loss_att 25.018328 loss_ctc 27.940571 loss_rnnt 21.403454 hw_loss 0.121808 lr 0.00075490 rank 2
2023-02-17 11:03:07,808 DEBUG TRAIN Batch 5/2100 loss 15.498931 loss_att 22.435680 loss_ctc 27.312103 loss_rnnt 12.458696 hw_loss 0.145861 lr 0.00075553 rank 0
2023-02-17 11:03:07,818 DEBUG TRAIN Batch 5/2100 loss 29.259785 loss_att 30.018452 loss_ctc 41.145813 loss_rnnt 27.408936 hw_loss 0.214335 lr 0.00075570 rank 4
2023-02-17 11:03:07,818 DEBUG TRAIN Batch 5/2100 loss 9.681742 loss_att 13.291940 loss_ctc 17.029598 loss_rnnt 7.867230 hw_loss 0.211419 lr 0.00075585 rank 7
2023-02-17 11:04:37,970 DEBUG TRAIN Batch 5/2200 loss 14.420576 loss_att 21.230240 loss_ctc 29.023739 loss_rnnt 10.978007 hw_loss 0.250402 lr 0.00075468 rank 5
2023-02-17 11:04:37,978 DEBUG TRAIN Batch 5/2200 loss 25.413319 loss_att 29.593102 loss_ctc 40.446968 loss_rnnt 22.455788 hw_loss 0.219540 lr 0.00075500 rank 6
2023-02-17 11:04:37,979 DEBUG TRAIN Batch 5/2200 loss 17.939043 loss_att 22.275368 loss_ctc 27.170822 loss_rnnt 15.698216 hw_loss 0.267482 lr 0.00075478 rank 3
2023-02-17 11:04:37,980 DEBUG TRAIN Batch 5/2200 loss 15.807341 loss_att 20.210192 loss_ctc 29.969450 loss_rnnt 12.882967 hw_loss 0.291601 lr 0.00075483 rank 4
2023-02-17 11:04:37,980 DEBUG TRAIN Batch 5/2200 loss 10.288322 loss_att 14.064863 loss_ctc 22.519295 loss_rnnt 7.747522 hw_loss 0.290054 lr 0.00075404 rank 2
2023-02-17 11:04:37,985 DEBUG TRAIN Batch 5/2200 loss 26.284311 loss_att 30.851494 loss_ctc 40.236198 loss_rnnt 23.396490 hw_loss 0.214001 lr 0.00075499 rank 7
2023-02-17 11:04:37,986 DEBUG TRAIN Batch 5/2200 loss 20.046917 loss_att 26.720022 loss_ctc 32.749107 loss_rnnt 16.985233 hw_loss 0.062693 lr 0.00075458 rank 1
2023-02-17 11:04:37,986 DEBUG TRAIN Batch 5/2200 loss 8.973401 loss_att 13.275421 loss_ctc 18.385902 loss_rnnt 6.754804 hw_loss 0.193486 lr 0.00075467 rank 0
2023-02-17 11:05:53,193 DEBUG TRAIN Batch 5/2300 loss 22.487209 loss_att 27.427805 loss_ctc 37.943470 loss_rnnt 19.239519 hw_loss 0.372630 lr 0.00075414 rank 6
2023-02-17 11:05:53,196 DEBUG TRAIN Batch 5/2300 loss 21.451254 loss_att 23.892605 loss_ctc 37.101906 loss_rnnt 18.748058 hw_loss 0.240323 lr 0.00075319 rank 2
2023-02-17 11:05:53,197 DEBUG TRAIN Batch 5/2300 loss 21.194866 loss_att 27.501417 loss_ctc 32.835995 loss_rnnt 18.240608 hw_loss 0.263994 lr 0.00075413 rank 7
2023-02-17 11:05:53,198 DEBUG TRAIN Batch 5/2300 loss 40.288776 loss_att 51.404530 loss_ctc 66.536377 loss_rnnt 34.407066 hw_loss 0.297901 lr 0.00075372 rank 1
2023-02-17 11:05:53,199 DEBUG TRAIN Batch 5/2300 loss 20.130596 loss_att 29.032394 loss_ctc 34.341522 loss_rnnt 16.314148 hw_loss 0.264929 lr 0.00075381 rank 0
2023-02-17 11:05:53,201 DEBUG TRAIN Batch 5/2300 loss 22.946196 loss_att 25.356480 loss_ctc 34.836063 loss_rnnt 20.777658 hw_loss 0.189682 lr 0.00075398 rank 4
2023-02-17 11:05:53,202 DEBUG TRAIN Batch 5/2300 loss 26.146278 loss_att 29.758888 loss_ctc 44.405769 loss_rnnt 22.791569 hw_loss 0.370479 lr 0.00075382 rank 5
2023-02-17 11:05:53,252 DEBUG TRAIN Batch 5/2300 loss 18.573784 loss_att 25.829262 loss_ctc 31.169674 loss_rnnt 15.255363 hw_loss 0.352264 lr 0.00075392 rank 3
2023-02-17 11:07:10,310 DEBUG TRAIN Batch 5/2400 loss 14.852062 loss_att 18.108463 loss_ctc 29.470695 loss_rnnt 12.114544 hw_loss 0.257037 lr 0.00075296 rank 0
2023-02-17 11:07:10,322 DEBUG TRAIN Batch 5/2400 loss 19.183954 loss_att 25.069712 loss_ctc 25.259977 loss_rnnt 17.096142 hw_loss 0.188482 lr 0.00075328 rank 6
2023-02-17 11:07:10,325 DEBUG TRAIN Batch 5/2400 loss 18.873785 loss_att 25.318428 loss_ctc 30.377869 loss_rnnt 15.934262 hw_loss 0.218846 lr 0.00075312 rank 4
2023-02-17 11:07:10,327 DEBUG TRAIN Batch 5/2400 loss 11.361465 loss_att 15.428108 loss_ctc 16.298840 loss_rnnt 9.764214 hw_loss 0.235512 lr 0.00075297 rank 5
2023-02-17 11:07:10,330 DEBUG TRAIN Batch 5/2400 loss 18.258211 loss_att 22.732006 loss_ctc 29.678501 loss_rnnt 15.671667 hw_loss 0.317027 lr 0.00075233 rank 2
2023-02-17 11:07:10,346 DEBUG TRAIN Batch 5/2400 loss 22.710014 loss_att 26.305605 loss_ctc 33.725136 loss_rnnt 20.419334 hw_loss 0.192899 lr 0.00075286 rank 1
2023-02-17 11:07:10,355 DEBUG TRAIN Batch 5/2400 loss 5.832109 loss_att 9.826116 loss_ctc 12.765074 loss_rnnt 3.959473 hw_loss 0.280199 lr 0.00075327 rank 7
2023-02-17 11:07:10,367 DEBUG TRAIN Batch 5/2400 loss 32.493870 loss_att 39.154087 loss_ctc 51.800545 loss_rnnt 28.432152 hw_loss 0.291472 lr 0.00075307 rank 3
2023-02-17 11:08:47,570 DEBUG TRAIN Batch 5/2500 loss 19.893446 loss_att 21.837498 loss_ctc 32.905350 loss_rnnt 17.563366 hw_loss 0.386907 lr 0.00075148 rank 2
2023-02-17 11:08:47,572 DEBUG TRAIN Batch 5/2500 loss 20.397449 loss_att 23.064651 loss_ctc 32.329094 loss_rnnt 18.143406 hw_loss 0.243221 lr 0.00075242 rank 7
2023-02-17 11:08:47,572 DEBUG TRAIN Batch 5/2500 loss 14.347459 loss_att 17.350204 loss_ctc 22.010941 loss_rnnt 12.598136 hw_loss 0.238081 lr 0.00075211 rank 5
2023-02-17 11:08:47,576 DEBUG TRAIN Batch 5/2500 loss 30.927946 loss_att 32.294598 loss_ctc 42.780159 loss_rnnt 28.971167 hw_loss 0.193421 lr 0.00075201 rank 1
2023-02-17 11:08:47,576 DEBUG TRAIN Batch 5/2500 loss 20.090765 loss_att 24.575619 loss_ctc 36.636219 loss_rnnt 16.900141 hw_loss 0.164238 lr 0.00075227 rank 4
2023-02-17 11:08:47,586 DEBUG TRAIN Batch 5/2500 loss 15.785602 loss_att 15.732176 loss_ctc 21.624155 loss_rnnt 14.820111 hw_loss 0.370690 lr 0.00075243 rank 6
2023-02-17 11:08:47,596 DEBUG TRAIN Batch 5/2500 loss 17.716167 loss_att 18.334408 loss_ctc 23.367674 loss_rnnt 16.639542 hw_loss 0.373954 lr 0.00075211 rank 0
2023-02-17 11:08:47,643 DEBUG TRAIN Batch 5/2500 loss 13.712869 loss_att 13.716289 loss_ctc 19.159113 loss_rnnt 12.798155 hw_loss 0.352245 lr 0.00075222 rank 3
2023-02-17 11:10:03,327 DEBUG TRAIN Batch 5/2600 loss 14.235253 loss_att 22.132517 loss_ctc 26.602562 loss_rnnt 10.861307 hw_loss 0.272846 lr 0.00075126 rank 5
2023-02-17 11:10:03,329 DEBUG TRAIN Batch 5/2600 loss 10.677285 loss_att 14.931104 loss_ctc 18.693773 loss_rnnt 8.644144 hw_loss 0.212835 lr 0.00075158 rank 6
2023-02-17 11:10:03,330 DEBUG TRAIN Batch 5/2600 loss 33.192772 loss_att 36.443398 loss_ctc 53.283752 loss_rnnt 29.724049 hw_loss 0.262124 lr 0.00075116 rank 1
2023-02-17 11:10:03,330 DEBUG TRAIN Batch 5/2600 loss 33.428642 loss_att 33.960854 loss_ctc 50.018433 loss_rnnt 30.976315 hw_loss 0.251091 lr 0.00075137 rank 3
2023-02-17 11:10:03,331 DEBUG TRAIN Batch 5/2600 loss 16.569704 loss_att 24.337872 loss_ctc 28.610046 loss_rnnt 13.249777 hw_loss 0.301712 lr 0.00075126 rank 0
2023-02-17 11:10:03,338 DEBUG TRAIN Batch 5/2600 loss 15.252355 loss_att 15.508535 loss_ctc 19.615482 loss_rnnt 14.418479 hw_loss 0.376668 lr 0.00075142 rank 4
2023-02-17 11:10:03,340 DEBUG TRAIN Batch 5/2600 loss 14.521420 loss_att 22.536732 loss_ctc 25.053001 loss_rnnt 11.326874 hw_loss 0.351132 lr 0.00075064 rank 2
2023-02-17 11:10:03,375 DEBUG TRAIN Batch 5/2600 loss 10.660940 loss_att 13.368525 loss_ctc 18.193413 loss_rnnt 8.994461 hw_loss 0.226185 lr 0.00075157 rank 7
2023-02-17 11:11:19,612 DEBUG TRAIN Batch 5/2700 loss 7.889659 loss_att 12.824223 loss_ctc 14.741374 loss_rnnt 5.864681 hw_loss 0.233444 lr 0.00075073 rank 6
2023-02-17 11:11:19,615 DEBUG TRAIN Batch 5/2700 loss 32.322605 loss_att 38.314384 loss_ctc 51.908051 loss_rnnt 28.384520 hw_loss 0.240631 lr 0.00075032 rank 1
2023-02-17 11:11:19,615 DEBUG TRAIN Batch 5/2700 loss 15.142578 loss_att 16.882750 loss_ctc 27.365459 loss_rnnt 13.052475 hw_loss 0.210659 lr 0.00075052 rank 3
2023-02-17 11:11:19,616 DEBUG TRAIN Batch 5/2700 loss 9.107754 loss_att 15.241174 loss_ctc 18.689882 loss_rnnt 6.458086 hw_loss 0.272565 lr 0.00075057 rank 4
2023-02-17 11:11:19,617 DEBUG TRAIN Batch 5/2700 loss 16.583603 loss_att 16.525919 loss_ctc 25.574570 loss_rnnt 15.311981 hw_loss 0.158183 lr 0.00075042 rank 5
2023-02-17 11:11:19,619 DEBUG TRAIN Batch 5/2700 loss 18.105055 loss_att 24.941257 loss_ctc 28.698538 loss_rnnt 15.189358 hw_loss 0.254982 lr 0.00075041 rank 0
2023-02-17 11:11:19,620 DEBUG TRAIN Batch 5/2700 loss 15.875854 loss_att 20.000956 loss_ctc 23.482498 loss_rnnt 13.862293 hw_loss 0.326853 lr 0.00074979 rank 2
2023-02-17 11:11:19,705 DEBUG TRAIN Batch 5/2700 loss 36.682747 loss_att 38.195549 loss_ctc 55.354675 loss_rnnt 33.804321 hw_loss 0.161765 lr 0.00075072 rank 7
2023-02-17 11:12:40,770 DEBUG TRAIN Batch 5/2800 loss 11.464179 loss_att 14.564273 loss_ctc 19.002132 loss_rnnt 9.708330 hw_loss 0.245192 lr 0.00074957 rank 0
2023-02-17 11:12:40,772 DEBUG TRAIN Batch 5/2800 loss 24.042032 loss_att 32.056671 loss_ctc 36.105755 loss_rnnt 20.718761 hw_loss 0.209715 lr 0.00074947 rank 1
2023-02-17 11:12:40,774 DEBUG TRAIN Batch 5/2800 loss 12.221502 loss_att 17.645559 loss_ctc 19.336906 loss_rnnt 10.070692 hw_loss 0.219898 lr 0.00074988 rank 7
2023-02-17 11:12:40,773 DEBUG TRAIN Batch 5/2800 loss 18.439934 loss_att 25.139809 loss_ctc 30.583027 loss_rnnt 15.331182 hw_loss 0.280683 lr 0.00074957 rank 5
2023-02-17 11:12:40,787 DEBUG TRAIN Batch 5/2800 loss 16.041592 loss_att 21.355459 loss_ctc 26.027580 loss_rnnt 13.541753 hw_loss 0.197995 lr 0.00074967 rank 3
2023-02-17 11:12:40,800 DEBUG TRAIN Batch 5/2800 loss 15.396532 loss_att 20.885389 loss_ctc 21.890789 loss_rnnt 13.319613 hw_loss 0.212336 lr 0.00074973 rank 4
2023-02-17 11:12:40,806 DEBUG TRAIN Batch 5/2800 loss 16.520491 loss_att 22.122381 loss_ctc 25.326941 loss_rnnt 14.071043 hw_loss 0.290392 lr 0.00074895 rank 2
2023-02-17 11:12:40,822 DEBUG TRAIN Batch 5/2800 loss 22.679342 loss_att 27.344753 loss_ctc 37.980465 loss_rnnt 19.561987 hw_loss 0.270233 lr 0.00074989 rank 6
2023-02-17 11:14:10,978 DEBUG TRAIN Batch 5/2900 loss 19.387777 loss_att 24.824366 loss_ctc 29.347370 loss_rnnt 16.808817 hw_loss 0.306931 lr 0.00074904 rank 7
2023-02-17 11:14:10,979 DEBUG TRAIN Batch 5/2900 loss 26.456757 loss_att 29.397612 loss_ctc 41.204231 loss_rnnt 23.791487 hw_loss 0.207689 lr 0.00074888 rank 4
2023-02-17 11:14:10,981 DEBUG TRAIN Batch 5/2900 loss 20.064541 loss_att 26.748446 loss_ctc 37.956482 loss_rnnt 16.179142 hw_loss 0.305673 lr 0.00074873 rank 5
2023-02-17 11:14:10,981 DEBUG TRAIN Batch 5/2900 loss 15.865523 loss_att 20.919989 loss_ctc 30.397064 loss_rnnt 12.815351 hw_loss 0.190763 lr 0.00074904 rank 6
2023-02-17 11:14:10,986 DEBUG TRAIN Batch 5/2900 loss 12.495267 loss_att 20.462013 loss_ctc 19.821527 loss_rnnt 9.818341 hw_loss 0.200141 lr 0.00074811 rank 2
2023-02-17 11:14:10,987 DEBUG TRAIN Batch 5/2900 loss 22.950649 loss_att 26.614010 loss_ctc 39.618774 loss_rnnt 19.886463 hw_loss 0.204560 lr 0.00074883 rank 3
2023-02-17 11:14:10,990 DEBUG TRAIN Batch 5/2900 loss 29.210032 loss_att 36.106640 loss_ctc 42.035469 loss_rnnt 25.993530 hw_loss 0.238351 lr 0.00074863 rank 1
2023-02-17 11:14:10,990 DEBUG TRAIN Batch 5/2900 loss 19.870321 loss_att 24.094776 loss_ctc 33.338402 loss_rnnt 17.083431 hw_loss 0.274228 lr 0.00074872 rank 0
2023-02-17 11:15:27,410 DEBUG TRAIN Batch 5/3000 loss 37.942215 loss_att 41.934357 loss_ctc 59.182793 loss_rnnt 34.121201 hw_loss 0.357196 lr 0.00074820 rank 6
2023-02-17 11:15:27,422 DEBUG TRAIN Batch 5/3000 loss 24.610239 loss_att 31.360554 loss_ctc 39.750744 loss_rnnt 21.138832 hw_loss 0.192390 lr 0.00074820 rank 7
2023-02-17 11:15:27,430 DEBUG TRAIN Batch 5/3000 loss 31.536486 loss_att 34.749718 loss_ctc 43.485855 loss_rnnt 29.156174 hw_loss 0.270780 lr 0.00074728 rank 2
2023-02-17 11:15:27,431 DEBUG TRAIN Batch 5/3000 loss 20.708183 loss_att 21.187719 loss_ctc 25.097202 loss_rnnt 19.938530 hw_loss 0.166022 lr 0.00074779 rank 1
2023-02-17 11:15:27,433 DEBUG TRAIN Batch 5/3000 loss 9.570595 loss_att 13.667723 loss_ctc 15.795832 loss_rnnt 7.774189 hw_loss 0.275527 lr 0.00074789 rank 5
2023-02-17 11:15:27,435 DEBUG TRAIN Batch 5/3000 loss 33.752796 loss_att 38.701923 loss_ctc 52.061848 loss_rnnt 30.169811 hw_loss 0.284914 lr 0.00074800 rank 3
2023-02-17 11:15:27,436 DEBUG TRAIN Batch 5/3000 loss 10.795676 loss_att 15.956314 loss_ctc 16.779799 loss_rnnt 8.888239 hw_loss 0.145175 lr 0.00074789 rank 0
2023-02-17 11:15:27,438 DEBUG TRAIN Batch 5/3000 loss 28.694500 loss_att 36.560684 loss_ctc 47.578468 loss_rnnt 24.484579 hw_loss 0.222793 lr 0.00074805 rank 4
2023-02-17 11:16:43,990 DEBUG TRAIN Batch 5/3100 loss 17.528631 loss_att 20.348608 loss_ctc 28.549381 loss_rnnt 15.347112 hw_loss 0.277669 lr 0.00074705 rank 0
2023-02-17 11:16:43,992 DEBUG TRAIN Batch 5/3100 loss 21.607836 loss_att 25.649645 loss_ctc 35.551022 loss_rnnt 18.791779 hw_loss 0.278635 lr 0.00074706 rank 5
2023-02-17 11:16:43,993 DEBUG TRAIN Batch 5/3100 loss 17.020634 loss_att 19.847393 loss_ctc 24.776936 loss_rnnt 15.288293 hw_loss 0.249030 lr 0.00074736 rank 7
2023-02-17 11:16:43,994 DEBUG TRAIN Batch 5/3100 loss 20.225914 loss_att 19.310680 loss_ctc 29.488338 loss_rnnt 18.994726 hw_loss 0.336087 lr 0.00074737 rank 6
2023-02-17 11:16:43,997 DEBUG TRAIN Batch 5/3100 loss 19.775591 loss_att 26.558784 loss_ctc 29.023045 loss_rnnt 17.019405 hw_loss 0.312285 lr 0.00074721 rank 4
2023-02-17 11:16:44,016 DEBUG TRAIN Batch 5/3100 loss 10.950830 loss_att 13.188803 loss_ctc 19.410721 loss_rnnt 9.296870 hw_loss 0.146965 lr 0.00074716 rank 3
2023-02-17 11:16:44,026 DEBUG TRAIN Batch 5/3100 loss 25.456083 loss_att 30.118069 loss_ctc 42.808464 loss_rnnt 22.014610 hw_loss 0.366414 lr 0.00074644 rank 2
2023-02-17 11:16:44,046 DEBUG TRAIN Batch 5/3100 loss 10.950801 loss_att 13.010673 loss_ctc 14.838992 loss_rnnt 9.845054 hw_loss 0.328777 lr 0.00074696 rank 1
2023-02-17 11:18:19,317 DEBUG TRAIN Batch 5/3200 loss 29.565514 loss_att 34.739983 loss_ctc 50.611248 loss_rnnt 25.552160 hw_loss 0.323180 lr 0.00074633 rank 3
2023-02-17 11:18:19,318 DEBUG TRAIN Batch 5/3200 loss 35.908195 loss_att 42.508629 loss_ctc 57.156853 loss_rnnt 31.622252 hw_loss 0.248827 lr 0.00074653 rank 7
2023-02-17 11:18:19,324 DEBUG TRAIN Batch 5/3200 loss 21.942476 loss_att 25.721415 loss_ctc 33.285187 loss_rnnt 19.500441 hw_loss 0.326043 lr 0.00074638 rank 4
2023-02-17 11:18:19,325 DEBUG TRAIN Batch 5/3200 loss 17.796156 loss_att 16.951756 loss_ctc 23.143003 loss_rnnt 17.058403 hw_loss 0.363225 lr 0.00074623 rank 5
2023-02-17 11:18:19,337 DEBUG TRAIN Batch 5/3200 loss 11.336725 loss_att 16.148777 loss_ctc 17.433287 loss_rnnt 9.480704 hw_loss 0.151379 lr 0.00074653 rank 6
2023-02-17 11:18:19,343 DEBUG TRAIN Batch 5/3200 loss 7.668431 loss_att 14.775934 loss_ctc 12.426943 loss_rnnt 5.519336 hw_loss 0.174613 lr 0.00074622 rank 0
2023-02-17 11:18:19,362 DEBUG TRAIN Batch 5/3200 loss 18.190113 loss_att 23.910858 loss_ctc 29.531250 loss_rnnt 15.418124 hw_loss 0.216911 lr 0.00074613 rank 1
2023-02-17 11:18:19,369 DEBUG TRAIN Batch 5/3200 loss 19.048113 loss_att 24.873421 loss_ctc 36.357414 loss_rnnt 15.464581 hw_loss 0.207307 lr 0.00074561 rank 2
2023-02-17 11:19:37,168 DEBUG TRAIN Batch 5/3300 loss 11.576170 loss_att 18.548042 loss_ctc 27.577841 loss_rnnt 7.889753 hw_loss 0.297163 lr 0.00074539 rank 0
2023-02-17 11:19:37,169 DEBUG TRAIN Batch 5/3300 loss 19.593204 loss_att 25.699650 loss_ctc 30.288223 loss_rnnt 16.798981 hw_loss 0.275498 lr 0.00074550 rank 3
2023-02-17 11:19:37,171 DEBUG TRAIN Batch 5/3300 loss 9.440773 loss_att 10.274677 loss_ctc 12.810503 loss_rnnt 8.650757 hw_loss 0.326133 lr 0.00074555 rank 4
2023-02-17 11:19:37,173 DEBUG TRAIN Batch 5/3300 loss 24.497511 loss_att 25.330765 loss_ctc 33.197220 loss_rnnt 23.060719 hw_loss 0.206591 lr 0.00074530 rank 1
2023-02-17 11:19:37,173 DEBUG TRAIN Batch 5/3300 loss 11.692913 loss_att 16.282745 loss_ctc 17.713360 loss_rnnt 9.848598 hw_loss 0.231791 lr 0.00074540 rank 5
2023-02-17 11:19:37,181 DEBUG TRAIN Batch 5/3300 loss 27.060310 loss_att 33.371986 loss_ctc 41.652405 loss_rnnt 23.690361 hw_loss 0.303751 lr 0.00074570 rank 7
2023-02-17 11:19:37,201 DEBUG TRAIN Batch 5/3300 loss 21.750227 loss_att 26.617477 loss_ctc 38.926338 loss_rnnt 18.343575 hw_loss 0.268224 lr 0.00074570 rank 6
2023-02-17 11:19:37,213 DEBUG TRAIN Batch 5/3300 loss 14.817634 loss_att 18.938181 loss_ctc 24.822601 loss_rnnt 12.548107 hw_loss 0.208915 lr 0.00074479 rank 2
2023-02-17 11:20:53,320 DEBUG TRAIN Batch 5/3400 loss 21.559017 loss_att 25.232618 loss_ctc 25.507271 loss_rnnt 20.154083 hw_loss 0.269583 lr 0.00074396 rank 2
2023-02-17 11:20:53,325 DEBUG TRAIN Batch 5/3400 loss 15.435434 loss_att 21.335058 loss_ctc 25.186762 loss_rnnt 12.786423 hw_loss 0.316703 lr 0.00074488 rank 6
2023-02-17 11:20:53,326 DEBUG TRAIN Batch 5/3400 loss 27.211964 loss_att 37.966019 loss_ctc 51.860031 loss_rnnt 21.629879 hw_loss 0.271626 lr 0.00074467 rank 3
2023-02-17 11:20:53,329 DEBUG TRAIN Batch 5/3400 loss 19.364223 loss_att 23.854958 loss_ctc 35.217194 loss_rnnt 16.242731 hw_loss 0.205533 lr 0.00074447 rank 1
2023-02-17 11:20:53,330 DEBUG TRAIN Batch 5/3400 loss 33.053577 loss_att 41.595783 loss_ctc 54.228821 loss_rnnt 28.361139 hw_loss 0.301181 lr 0.00074487 rank 7
2023-02-17 11:20:53,330 DEBUG TRAIN Batch 5/3400 loss 19.659622 loss_att 26.472061 loss_ctc 36.468605 loss_rnnt 15.951838 hw_loss 0.195182 lr 0.00074456 rank 0
2023-02-17 11:20:53,332 DEBUG TRAIN Batch 5/3400 loss 14.886979 loss_att 20.462502 loss_ctc 25.544304 loss_rnnt 12.205340 hw_loss 0.272923 lr 0.00074472 rank 4
2023-02-17 11:20:53,336 DEBUG TRAIN Batch 5/3400 loss 13.947561 loss_att 17.092117 loss_ctc 22.097097 loss_rnnt 12.054655 hw_loss 0.332607 lr 0.00074457 rank 5
2023-02-17 11:22:13,849 DEBUG TRAIN Batch 5/3500 loss 13.238809 loss_att 17.037560 loss_ctc 20.799921 loss_rnnt 11.387765 hw_loss 0.155896 lr 0.00074385 rank 3
2023-02-17 11:22:13,871 DEBUG TRAIN Batch 5/3500 loss 15.964025 loss_att 19.902225 loss_ctc 28.170437 loss_rnnt 13.449505 hw_loss 0.186294 lr 0.00074405 rank 6
2023-02-17 11:22:13,882 DEBUG TRAIN Batch 5/3500 loss 17.606899 loss_att 24.180958 loss_ctc 23.578705 loss_rnnt 15.339535 hw_loss 0.293084 lr 0.00074365 rank 1
2023-02-17 11:22:13,904 DEBUG TRAIN Batch 5/3500 loss 27.124653 loss_att 30.747755 loss_ctc 41.910370 loss_rnnt 24.290810 hw_loss 0.258360 lr 0.00074389 rank 4
2023-02-17 11:22:13,907 DEBUG TRAIN Batch 5/3500 loss 45.396858 loss_att 48.383736 loss_ctc 65.613266 loss_rnnt 41.978294 hw_loss 0.235623 lr 0.00074314 rank 2
2023-02-17 11:22:13,908 DEBUG TRAIN Batch 5/3500 loss 16.223673 loss_att 19.290279 loss_ctc 25.776867 loss_rnnt 14.213400 hw_loss 0.230988 lr 0.00074375 rank 5
2023-02-17 11:22:13,920 DEBUG TRAIN Batch 5/3500 loss 18.420099 loss_att 26.135403 loss_ctc 34.374855 loss_rnnt 14.613184 hw_loss 0.256040 lr 0.00074374 rank 0
2023-02-17 11:22:13,924 DEBUG TRAIN Batch 5/3500 loss 25.454075 loss_att 26.286816 loss_ctc 33.391182 loss_rnnt 24.073479 hw_loss 0.292064 lr 0.00074404 rank 7
2023-02-17 11:23:45,440 DEBUG TRAIN Batch 5/3600 loss 16.731951 loss_att 22.057487 loss_ctc 30.942627 loss_rnnt 13.615754 hw_loss 0.293119 lr 0.00074307 rank 4
2023-02-17 11:23:45,443 DEBUG TRAIN Batch 5/3600 loss 17.384327 loss_att 19.414997 loss_ctc 27.024981 loss_rnnt 15.552181 hw_loss 0.263607 lr 0.00074323 rank 6
2023-02-17 11:23:45,446 DEBUG TRAIN Batch 5/3600 loss 21.007406 loss_att 27.207403 loss_ctc 34.582649 loss_rnnt 17.754877 hw_loss 0.379682 lr 0.00074292 rank 0
2023-02-17 11:23:45,446 DEBUG TRAIN Batch 5/3600 loss 9.854469 loss_att 15.022042 loss_ctc 19.823738 loss_rnnt 7.315208 hw_loss 0.330958 lr 0.00074302 rank 3
2023-02-17 11:23:45,448 DEBUG TRAIN Batch 5/3600 loss 22.644880 loss_att 25.920218 loss_ctc 31.251940 loss_rnnt 20.696529 hw_loss 0.273144 lr 0.00074283 rank 1
2023-02-17 11:23:45,477 DEBUG TRAIN Batch 5/3600 loss 20.194174 loss_att 23.928114 loss_ctc 34.681713 loss_rnnt 17.412895 hw_loss 0.192779 lr 0.00074232 rank 2
2023-02-17 11:23:45,486 DEBUG TRAIN Batch 5/3600 loss 33.453300 loss_att 40.691631 loss_ctc 56.971062 loss_rnnt 28.696779 hw_loss 0.324660 lr 0.00074292 rank 5
2023-02-17 11:23:45,493 DEBUG TRAIN Batch 5/3600 loss 32.810558 loss_att 40.656715 loss_ctc 54.123901 loss_rnnt 28.268711 hw_loss 0.245314 lr 0.00074322 rank 7
2023-02-17 11:25:02,012 DEBUG TRAIN Batch 5/3700 loss 20.456047 loss_att 23.527975 loss_ctc 32.530289 loss_rnnt 18.103157 hw_loss 0.241135 lr 0.00074241 rank 6
2023-02-17 11:25:02,015 DEBUG TRAIN Batch 5/3700 loss 18.853144 loss_att 22.873547 loss_ctc 31.181015 loss_rnnt 16.261127 hw_loss 0.270408 lr 0.00074211 rank 5
2023-02-17 11:25:02,016 DEBUG TRAIN Batch 5/3700 loss 12.723907 loss_att 15.986904 loss_ctc 26.647871 loss_rnnt 10.088926 hw_loss 0.235971 lr 0.00074220 rank 3
2023-02-17 11:25:02,017 DEBUG TRAIN Batch 5/3700 loss 12.358496 loss_att 12.014828 loss_ctc 19.771612 loss_rnnt 11.270284 hw_loss 0.315992 lr 0.00074240 rank 7
2023-02-17 11:25:02,017 DEBUG TRAIN Batch 5/3700 loss 22.860624 loss_att 26.857807 loss_ctc 33.935696 loss_rnnt 20.468931 hw_loss 0.216715 lr 0.00074150 rank 2
2023-02-17 11:25:02,019 DEBUG TRAIN Batch 5/3700 loss 23.896145 loss_att 26.060310 loss_ctc 32.753555 loss_rnnt 22.155975 hw_loss 0.236900 lr 0.00074210 rank 0
2023-02-17 11:25:02,022 DEBUG TRAIN Batch 5/3700 loss 25.346416 loss_att 30.621647 loss_ctc 37.825996 loss_rnnt 22.477367 hw_loss 0.281364 lr 0.00074225 rank 4
2023-02-17 11:25:02,077 DEBUG TRAIN Batch 5/3700 loss 13.731798 loss_att 18.936125 loss_ctc 24.989328 loss_rnnt 11.036666 hw_loss 0.287369 lr 0.00074201 rank 1
2023-02-17 11:26:19,259 DEBUG TRAIN Batch 5/3800 loss 18.114313 loss_att 19.344604 loss_ctc 24.994617 loss_rnnt 16.828375 hw_loss 0.229698 lr 0.00074139 rank 3
2023-02-17 11:26:19,261 DEBUG TRAIN Batch 5/3800 loss 16.511257 loss_att 25.895613 loss_ctc 26.562674 loss_rnnt 13.174919 hw_loss 0.223647 lr 0.00074158 rank 7
2023-02-17 11:26:19,264 DEBUG TRAIN Batch 5/3800 loss 11.328753 loss_att 18.898233 loss_ctc 23.947098 loss_rnnt 8.077765 hw_loss 0.102461 lr 0.00074069 rank 2
2023-02-17 11:26:19,264 DEBUG TRAIN Batch 5/3800 loss 18.470020 loss_att 19.579437 loss_ctc 28.656481 loss_rnnt 16.734039 hw_loss 0.292313 lr 0.00074129 rank 5
2023-02-17 11:26:19,268 DEBUG TRAIN Batch 5/3800 loss 19.307411 loss_att 25.858253 loss_ctc 33.865376 loss_rnnt 15.949633 hw_loss 0.199776 lr 0.00074144 rank 4
2023-02-17 11:26:19,269 DEBUG TRAIN Batch 5/3800 loss 19.299423 loss_att 21.447126 loss_ctc 27.489101 loss_rnnt 17.618633 hw_loss 0.298671 lr 0.00074128 rank 0
2023-02-17 11:26:19,280 DEBUG TRAIN Batch 5/3800 loss 18.571983 loss_att 20.500620 loss_ctc 29.172009 loss_rnnt 16.701824 hw_loss 0.133307 lr 0.00074159 rank 6
2023-02-17 11:26:19,288 DEBUG TRAIN Batch 5/3800 loss 19.787626 loss_att 25.279745 loss_ctc 43.654736 loss_rnnt 15.366539 hw_loss 0.263215 lr 0.00074119 rank 1
2023-02-17 11:27:50,956 DEBUG TRAIN Batch 5/3900 loss 18.658112 loss_att 19.287632 loss_ctc 25.406870 loss_rnnt 17.518499 hw_loss 0.213512 lr 0.00074062 rank 4
2023-02-17 11:27:50,956 DEBUG TRAIN Batch 5/3900 loss 9.858529 loss_att 13.621808 loss_ctc 13.221972 loss_rnnt 8.483898 hw_loss 0.325342 lr 0.00074077 rank 7
2023-02-17 11:27:50,980 DEBUG TRAIN Batch 5/3900 loss 22.552036 loss_att 30.657078 loss_ctc 44.502319 loss_rnnt 17.907425 hw_loss 0.181683 lr 0.00074038 rank 1
2023-02-17 11:27:50,999 DEBUG TRAIN Batch 5/3900 loss 15.600705 loss_att 20.167519 loss_ctc 25.842743 loss_rnnt 13.213306 hw_loss 0.203308 lr 0.00074048 rank 5
2023-02-17 11:27:51,003 DEBUG TRAIN Batch 5/3900 loss 22.006474 loss_att 26.379269 loss_ctc 32.668026 loss_rnnt 19.607347 hw_loss 0.193179 lr 0.00074057 rank 3
2023-02-17 11:27:51,006 DEBUG TRAIN Batch 5/3900 loss 19.318312 loss_att 27.590084 loss_ctc 32.342957 loss_rnnt 15.737400 hw_loss 0.356136 lr 0.00073988 rank 2
2023-02-17 11:27:51,027 DEBUG TRAIN Batch 5/3900 loss 25.897100 loss_att 30.847816 loss_ctc 28.104778 loss_rnnt 24.508177 hw_loss 0.195796 lr 0.00074078 rank 6
2023-02-17 11:27:51,030 DEBUG TRAIN Batch 5/3900 loss 13.065381 loss_att 17.111057 loss_ctc 18.099789 loss_rnnt 11.453380 hw_loss 0.246773 lr 0.00074047 rank 0
2023-02-17 11:29:11,112 DEBUG TRAIN Batch 5/4000 loss 18.238829 loss_att 19.937971 loss_ctc 30.709202 loss_rnnt 16.144787 hw_loss 0.171555 lr 0.00073966 rank 0
2023-02-17 11:29:11,113 DEBUG TRAIN Batch 5/4000 loss 24.323336 loss_att 28.613165 loss_ctc 36.595215 loss_rnnt 21.679203 hw_loss 0.281089 lr 0.00073967 rank 5
2023-02-17 11:29:11,114 DEBUG TRAIN Batch 5/4000 loss 22.429508 loss_att 30.095690 loss_ctc 38.237198 loss_rnnt 18.655148 hw_loss 0.250189 lr 0.00073981 rank 4
2023-02-17 11:29:11,113 DEBUG TRAIN Batch 5/4000 loss 33.054317 loss_att 37.317722 loss_ctc 60.347145 loss_rnnt 28.423803 hw_loss 0.260226 lr 0.00073907 rank 2
2023-02-17 11:29:11,119 DEBUG TRAIN Batch 5/4000 loss 5.536926 loss_att 10.034039 loss_ctc 15.300265 loss_rnnt 3.221367 hw_loss 0.214420 lr 0.00073976 rank 3
2023-02-17 11:29:11,143 DEBUG TRAIN Batch 5/4000 loss 11.489577 loss_att 16.152407 loss_ctc 22.164558 loss_rnnt 9.055040 hw_loss 0.147451 lr 0.00073957 rank 1
2023-02-17 11:29:11,148 DEBUG TRAIN Batch 5/4000 loss 16.666460 loss_att 16.255865 loss_ctc 23.951782 loss_rnnt 15.694831 hw_loss 0.154446 lr 0.00073996 rank 7
2023-02-17 11:29:11,159 DEBUG TRAIN Batch 5/4000 loss 15.241991 loss_att 24.856014 loss_ctc 30.928623 loss_rnnt 11.138039 hw_loss 0.167995 lr 0.00073997 rank 6
2023-02-17 11:30:26,690 DEBUG TRAIN Batch 5/4100 loss 24.762678 loss_att 30.046453 loss_ctc 37.337379 loss_rnnt 21.854809 hw_loss 0.327167 lr 0.00073915 rank 7
2023-02-17 11:30:26,702 DEBUG TRAIN Batch 5/4100 loss 21.480145 loss_att 27.396011 loss_ctc 29.846001 loss_rnnt 19.024254 hw_loss 0.294882 lr 0.00073885 rank 0
2023-02-17 11:30:26,704 DEBUG TRAIN Batch 5/4100 loss 29.342693 loss_att 31.804764 loss_ctc 43.567242 loss_rnnt 26.811529 hw_loss 0.266523 lr 0.00073900 rank 4
2023-02-17 11:30:26,703 DEBUG TRAIN Batch 5/4100 loss 29.358885 loss_att 33.063404 loss_ctc 43.408279 loss_rnnt 26.610094 hw_loss 0.252439 lr 0.00073876 rank 1
2023-02-17 11:30:26,705 DEBUG TRAIN Batch 5/4100 loss 17.521469 loss_att 23.523405 loss_ctc 32.859474 loss_rnnt 14.155825 hw_loss 0.225353 lr 0.00073895 rank 3
2023-02-17 11:30:26,706 DEBUG TRAIN Batch 5/4100 loss 28.826113 loss_att 38.488808 loss_ctc 49.921188 loss_rnnt 23.933113 hw_loss 0.277095 lr 0.00073826 rank 2
2023-02-17 11:30:26,707 DEBUG TRAIN Batch 5/4100 loss 25.507389 loss_att 30.206291 loss_ctc 47.406128 loss_rnnt 21.574028 hw_loss 0.138272 lr 0.00073916 rank 6
2023-02-17 11:30:26,750 DEBUG TRAIN Batch 5/4100 loss 15.719335 loss_att 20.216045 loss_ctc 32.238087 loss_rnnt 12.527933 hw_loss 0.167924 lr 0.00073886 rank 5
2023-02-17 11:31:44,552 DEBUG TRAIN Batch 5/4200 loss 22.055090 loss_att 24.781977 loss_ctc 38.196079 loss_rnnt 19.180462 hw_loss 0.332100 lr 0.00073834 rank 7
2023-02-17 11:31:44,561 DEBUG TRAIN Batch 5/4200 loss 27.184402 loss_att 28.036909 loss_ctc 44.938690 loss_rnnt 24.499331 hw_loss 0.276245 lr 0.00073805 rank 5
2023-02-17 11:31:44,562 DEBUG TRAIN Batch 5/4200 loss 19.469500 loss_att 25.508190 loss_ctc 35.651138 loss_rnnt 15.994032 hw_loss 0.206585 lr 0.00073746 rank 2
2023-02-17 11:31:44,563 DEBUG TRAIN Batch 5/4200 loss 29.668310 loss_att 31.598700 loss_ctc 41.602219 loss_rnnt 27.586746 hw_loss 0.195561 lr 0.00073835 rank 6
2023-02-17 11:31:44,566 DEBUG TRAIN Batch 5/4200 loss 23.192793 loss_att 26.370516 loss_ctc 33.501194 loss_rnnt 21.099094 hw_loss 0.156935 lr 0.00073804 rank 0
2023-02-17 11:31:44,576 DEBUG TRAIN Batch 5/4200 loss 14.957306 loss_att 19.421562 loss_ctc 28.424965 loss_rnnt 12.126963 hw_loss 0.265879 lr 0.00073820 rank 4
2023-02-17 11:31:44,576 DEBUG TRAIN Batch 5/4200 loss 13.546734 loss_att 14.012822 loss_ctc 18.261921 loss_rnnt 12.694454 hw_loss 0.244446 lr 0.00073796 rank 1
2023-02-17 11:31:44,594 DEBUG TRAIN Batch 5/4200 loss 30.607136 loss_att 36.805553 loss_ctc 51.024376 loss_rnnt 26.507072 hw_loss 0.258903 lr 0.00073815 rank 3
2023-02-17 11:33:03,100 DEBUG TRAIN Batch 5/4300 loss 18.703697 loss_att 23.253178 loss_ctc 32.401398 loss_rnnt 15.860656 hw_loss 0.200222 lr 0.00073724 rank 0
2023-02-17 11:33:03,103 DEBUG TRAIN Batch 5/4300 loss 38.167881 loss_att 45.312328 loss_ctc 62.390999 loss_rnnt 33.369923 hw_loss 0.261225 lr 0.00073739 rank 4
2023-02-17 11:33:03,105 DEBUG TRAIN Batch 5/4300 loss 18.581545 loss_att 23.085485 loss_ctc 31.295429 loss_rnnt 15.833948 hw_loss 0.284296 lr 0.00073754 rank 7
2023-02-17 11:33:03,107 DEBUG TRAIN Batch 5/4300 loss 24.212893 loss_att 28.240856 loss_ctc 41.081482 loss_rnnt 21.040375 hw_loss 0.220838 lr 0.00073755 rank 6
2023-02-17 11:33:03,107 DEBUG TRAIN Batch 5/4300 loss 18.165344 loss_att 20.530184 loss_ctc 24.107096 loss_rnnt 16.733137 hw_loss 0.313138 lr 0.00073735 rank 3
2023-02-17 11:33:03,108 DEBUG TRAIN Batch 5/4300 loss 23.329313 loss_att 27.202076 loss_ctc 33.049313 loss_rnnt 21.090200 hw_loss 0.316055 lr 0.00073725 rank 5
2023-02-17 11:33:03,109 DEBUG TRAIN Batch 5/4300 loss 22.462868 loss_att 27.227203 loss_ctc 35.185040 loss_rnnt 19.667662 hw_loss 0.273846 lr 0.00073715 rank 1
2023-02-17 11:33:03,110 DEBUG TRAIN Batch 5/4300 loss 28.634634 loss_att 32.455482 loss_ctc 41.114056 loss_rnnt 26.050018 hw_loss 0.293480 lr 0.00073666 rank 2
2023-02-17 11:34:18,692 DEBUG TRAIN Batch 5/4400 loss 17.356098 loss_att 18.037500 loss_ctc 22.126501 loss_rnnt 16.397064 hw_loss 0.350059 lr 0.00073645 rank 5
2023-02-17 11:34:18,693 DEBUG TRAIN Batch 5/4400 loss 11.419942 loss_att 19.693516 loss_ctc 12.680282 loss_rnnt 9.455984 hw_loss 0.264747 lr 0.00073675 rank 6
2023-02-17 11:34:18,694 DEBUG TRAIN Batch 5/4400 loss 18.761406 loss_att 18.623362 loss_ctc 27.190809 loss_rnnt 17.487286 hw_loss 0.333388 lr 0.00073644 rank 0
2023-02-17 11:34:18,695 DEBUG TRAIN Batch 5/4400 loss 15.908105 loss_att 17.769833 loss_ctc 21.043344 loss_rnnt 14.647079 hw_loss 0.382465 lr 0.00073635 rank 1
2023-02-17 11:34:18,697 DEBUG TRAIN Batch 5/4400 loss 10.241115 loss_att 10.601121 loss_ctc 14.447717 loss_rnnt 9.433802 hw_loss 0.327060 lr 0.00073655 rank 3
2023-02-17 11:34:18,698 DEBUG TRAIN Batch 5/4400 loss 36.323246 loss_att 43.080864 loss_ctc 69.840179 loss_rnnt 30.372280 hw_loss 0.244726 lr 0.00073674 rank 7
2023-02-17 11:34:18,704 DEBUG TRAIN Batch 5/4400 loss 20.160145 loss_att 22.088379 loss_ctc 29.751036 loss_rnnt 18.385183 hw_loss 0.207243 lr 0.00073586 rank 2
2023-02-17 11:34:18,705 DEBUG TRAIN Batch 5/4400 loss 27.926926 loss_att 29.619278 loss_ctc 40.930893 loss_rnnt 25.753139 hw_loss 0.190225 lr 0.00073659 rank 4
2023-02-17 11:35:33,025 DEBUG TRAIN Batch 5/4500 loss 18.831213 loss_att 22.418221 loss_ctc 31.573071 loss_rnnt 16.294334 hw_loss 0.226053 lr 0.00073506 rank 2
2023-02-17 11:35:33,027 DEBUG TRAIN Batch 5/4500 loss 13.137749 loss_att 17.388660 loss_ctc 22.692450 loss_rnnt 10.862783 hw_loss 0.282794 lr 0.00073575 rank 3
2023-02-17 11:35:33,028 DEBUG TRAIN Batch 5/4500 loss 20.970383 loss_att 27.610218 loss_ctc 30.019827 loss_rnnt 18.324736 hw_loss 0.208293 lr 0.00073556 rank 1
2023-02-17 11:35:33,028 DEBUG TRAIN Batch 5/4500 loss 19.383427 loss_att 20.393738 loss_ctc 31.179289 loss_rnnt 17.435848 hw_loss 0.323875 lr 0.00073580 rank 4
2023-02-17 11:35:33,029 DEBUG TRAIN Batch 5/4500 loss 26.649313 loss_att 27.232950 loss_ctc 49.915203 loss_rnnt 23.305431 hw_loss 0.234448 lr 0.00073595 rank 6
2023-02-17 11:35:33,030 DEBUG TRAIN Batch 5/4500 loss 25.431402 loss_att 30.555267 loss_ctc 41.569714 loss_rnnt 22.171080 hw_loss 0.157081 lr 0.00073564 rank 0
2023-02-17 11:35:33,031 DEBUG TRAIN Batch 5/4500 loss 19.538271 loss_att 26.188665 loss_ctc 32.894665 loss_rnnt 16.288109 hw_loss 0.261056 lr 0.00073594 rank 7
2023-02-17 11:35:33,035 DEBUG TRAIN Batch 5/4500 loss 25.329168 loss_att 29.120663 loss_ctc 42.622601 loss_rnnt 22.149857 hw_loss 0.216043 lr 0.00073565 rank 5
2023-02-17 11:36:50,179 DEBUG TRAIN Batch 5/4600 loss 12.642247 loss_att 17.052719 loss_ctc 22.101530 loss_rnnt 10.332619 hw_loss 0.311804 lr 0.00073427 rank 2
2023-02-17 11:36:50,181 DEBUG TRAIN Batch 5/4600 loss 18.074963 loss_att 20.017899 loss_ctc 27.041998 loss_rnnt 16.345169 hw_loss 0.273005 lr 0.00073495 rank 3
2023-02-17 11:36:50,183 DEBUG TRAIN Batch 5/4600 loss 15.694139 loss_att 21.504351 loss_ctc 24.825619 loss_rnnt 13.206599 hw_loss 0.202438 lr 0.00073515 rank 6
2023-02-17 11:36:50,183 DEBUG TRAIN Batch 5/4600 loss 11.695559 loss_att 20.232187 loss_ctc 28.468847 loss_rnnt 7.611187 hw_loss 0.263639 lr 0.00073476 rank 1
2023-02-17 11:36:50,186 DEBUG TRAIN Batch 5/4600 loss 16.647858 loss_att 22.494595 loss_ctc 22.216423 loss_rnnt 14.636370 hw_loss 0.186872 lr 0.00073514 rank 7
2023-02-17 11:36:50,188 DEBUG TRAIN Batch 5/4600 loss 20.252382 loss_att 23.847555 loss_ctc 25.662161 loss_rnnt 18.704987 hw_loss 0.200734 lr 0.00073486 rank 5
2023-02-17 11:36:50,190 DEBUG TRAIN Batch 5/4600 loss 14.292055 loss_att 25.563400 loss_ctc 29.728302 loss_rnnt 9.808282 hw_loss 0.321257 lr 0.00073485 rank 0
2023-02-17 11:36:50,231 DEBUG TRAIN Batch 5/4600 loss 18.290087 loss_att 20.039450 loss_ctc 29.827032 loss_rnnt 16.327450 hw_loss 0.139701 lr 0.00073500 rank 4
2023-02-17 11:38:05,603 DEBUG TRAIN Batch 5/4700 loss 17.829533 loss_att 23.131235 loss_ctc 31.151119 loss_rnnt 14.872967 hw_loss 0.225023 lr 0.00073406 rank 5
2023-02-17 11:38:05,604 DEBUG TRAIN Batch 5/4700 loss 14.185423 loss_att 19.144444 loss_ctc 24.116970 loss_rnnt 11.765817 hw_loss 0.194239 lr 0.00073436 rank 6
2023-02-17 11:38:05,607 DEBUG TRAIN Batch 5/4700 loss 16.452940 loss_att 22.770771 loss_ctc 32.119789 loss_rnnt 12.993391 hw_loss 0.200760 lr 0.00073416 rank 3
2023-02-17 11:38:05,608 DEBUG TRAIN Batch 5/4700 loss 13.463665 loss_att 17.635736 loss_ctc 25.061432 loss_rnnt 10.949207 hw_loss 0.250640 lr 0.00073348 rank 2
2023-02-17 11:38:05,608 DEBUG TRAIN Batch 5/4700 loss 14.213328 loss_att 18.884840 loss_ctc 25.087553 loss_rnnt 11.646761 hw_loss 0.341938 lr 0.00073406 rank 0
2023-02-17 11:38:05,611 DEBUG TRAIN Batch 5/4700 loss 33.284584 loss_att 41.155449 loss_ctc 56.383453 loss_rnnt 28.501846 hw_loss 0.241337 lr 0.00073397 rank 1
2023-02-17 11:38:05,614 DEBUG TRAIN Batch 5/4700 loss 15.135310 loss_att 20.391823 loss_ctc 26.594437 loss_rnnt 12.471983 hw_loss 0.157764 lr 0.00073421 rank 4
2023-02-17 11:38:05,617 DEBUG TRAIN Batch 5/4700 loss 10.193805 loss_att 14.848799 loss_ctc 19.484978 loss_rnnt 7.909261 hw_loss 0.215105 lr 0.00073435 rank 7
2023-02-17 11:39:21,223 DEBUG TRAIN Batch 5/4800 loss 20.007710 loss_att 24.469769 loss_ctc 38.025124 loss_rnnt 16.556318 hw_loss 0.293730 lr 0.00073357 rank 6
2023-02-17 11:39:21,224 DEBUG TRAIN Batch 5/4800 loss 17.091236 loss_att 19.616043 loss_ctc 26.697325 loss_rnnt 15.193009 hw_loss 0.210848 lr 0.00073269 rank 2
2023-02-17 11:39:21,224 DEBUG TRAIN Batch 5/4800 loss 21.270124 loss_att 30.058529 loss_ctc 35.898022 loss_rnnt 17.427885 hw_loss 0.251577 lr 0.00073327 rank 5
2023-02-17 11:39:21,225 DEBUG TRAIN Batch 5/4800 loss 16.237865 loss_att 21.564177 loss_ctc 24.551952 loss_rnnt 13.923138 hw_loss 0.264226 lr 0.00073337 rank 3
2023-02-17 11:39:21,225 DEBUG TRAIN Batch 5/4800 loss 16.776735 loss_att 18.792019 loss_ctc 24.900494 loss_rnnt 15.152746 hw_loss 0.258309 lr 0.00073342 rank 4
2023-02-17 11:39:21,228 DEBUG TRAIN Batch 5/4800 loss 23.192001 loss_att 27.489935 loss_ctc 38.083817 loss_rnnt 20.231697 hw_loss 0.215893 lr 0.00073356 rank 7
2023-02-17 11:39:21,232 DEBUG TRAIN Batch 5/4800 loss 23.717180 loss_att 26.122120 loss_ctc 31.355906 loss_rnnt 22.072140 hw_loss 0.272923 lr 0.00073327 rank 0
2023-02-17 11:39:21,234 DEBUG TRAIN Batch 5/4800 loss 16.468464 loss_att 23.378677 loss_ctc 30.786255 loss_rnnt 13.053543 hw_loss 0.232198 lr 0.00073318 rank 1
2023-02-17 11:40:39,901 DEBUG TRAIN Batch 5/4900 loss 14.958162 loss_att 20.660023 loss_ctc 21.616930 loss_rnnt 12.752539 hw_loss 0.332654 lr 0.00073239 rank 1
2023-02-17 11:40:39,905 DEBUG TRAIN Batch 5/4900 loss 21.125130 loss_att 26.459362 loss_ctc 37.351105 loss_rnnt 17.766350 hw_loss 0.240878 lr 0.00073278 rank 6
2023-02-17 11:40:39,911 DEBUG TRAIN Batch 5/4900 loss 10.576838 loss_att 15.111319 loss_ctc 18.774933 loss_rnnt 8.480240 hw_loss 0.181170 lr 0.00073263 rank 4
2023-02-17 11:40:39,913 DEBUG TRAIN Batch 5/4900 loss 16.697548 loss_att 21.460987 loss_ctc 23.540222 loss_rnnt 14.651722 hw_loss 0.338965 lr 0.00073191 rank 2
2023-02-17 11:40:39,914 DEBUG TRAIN Batch 5/4900 loss 18.744713 loss_att 22.915937 loss_ctc 29.095715 loss_rnnt 16.377447 hw_loss 0.286666 lr 0.00073249 rank 5
2023-02-17 11:40:39,915 DEBUG TRAIN Batch 5/4900 loss 16.361383 loss_att 24.288036 loss_ctc 36.118279 loss_rnnt 12.014925 hw_loss 0.237889 lr 0.00073277 rank 7
2023-02-17 11:40:39,925 DEBUG TRAIN Batch 5/4900 loss 32.348419 loss_att 38.157623 loss_ctc 51.827522 loss_rnnt 28.455414 hw_loss 0.251161 lr 0.00073248 rank 0
2023-02-17 11:40:39,965 DEBUG TRAIN Batch 5/4900 loss 41.694763 loss_att 43.866310 loss_ctc 58.430794 loss_rnnt 38.928185 hw_loss 0.188989 lr 0.00073258 rank 3
2023-02-17 11:41:59,423 DEBUG TRAIN Batch 5/5000 loss 31.672707 loss_att 33.206009 loss_ctc 43.635342 loss_rnnt 29.647751 hw_loss 0.231143 lr 0.00073180 rank 3
2023-02-17 11:41:59,426 DEBUG TRAIN Batch 5/5000 loss 19.902519 loss_att 22.267193 loss_ctc 32.543667 loss_rnnt 17.578083 hw_loss 0.311281 lr 0.00073170 rank 5
2023-02-17 11:41:59,430 DEBUG TRAIN Batch 5/5000 loss 13.636586 loss_att 16.530146 loss_ctc 22.491508 loss_rnnt 11.702548 hw_loss 0.327505 lr 0.00073199 rank 6
2023-02-17 11:41:59,431 DEBUG TRAIN Batch 5/5000 loss 22.601767 loss_att 27.822113 loss_ctc 40.425426 loss_rnnt 19.058325 hw_loss 0.230408 lr 0.00073184 rank 4
2023-02-17 11:41:59,433 DEBUG TRAIN Batch 5/5000 loss 28.038015 loss_att 32.676418 loss_ctc 47.200745 loss_rnnt 24.402332 hw_loss 0.286824 lr 0.00073112 rank 2
2023-02-17 11:41:59,434 DEBUG TRAIN Batch 5/5000 loss 11.631520 loss_att 13.871635 loss_ctc 20.194294 loss_rnnt 9.917484 hw_loss 0.233080 lr 0.00073161 rank 1
2023-02-17 11:41:59,434 DEBUG TRAIN Batch 5/5000 loss 19.551544 loss_att 19.677052 loss_ctc 29.799437 loss_rnnt 17.991507 hw_loss 0.316031 lr 0.00073170 rank 0
2023-02-17 11:41:59,436 DEBUG TRAIN Batch 5/5000 loss 13.651693 loss_att 15.005733 loss_ctc 20.903189 loss_rnnt 12.207223 hw_loss 0.387742 lr 0.00073199 rank 7
2023-02-17 11:43:16,114 DEBUG TRAIN Batch 5/5100 loss 14.468868 loss_att 18.052065 loss_ctc 25.664536 loss_rnnt 12.099955 hw_loss 0.299097 lr 0.00073121 rank 6
2023-02-17 11:43:16,116 DEBUG TRAIN Batch 5/5100 loss 16.069485 loss_att 22.466799 loss_ctc 26.154575 loss_rnnt 13.382148 hw_loss 0.118495 lr 0.00073120 rank 7
2023-02-17 11:43:16,116 DEBUG TRAIN Batch 5/5100 loss 18.468613 loss_att 21.909649 loss_ctc 31.105352 loss_rnnt 15.937790 hw_loss 0.295716 lr 0.00073106 rank 4
2023-02-17 11:43:16,118 DEBUG TRAIN Batch 5/5100 loss 8.141688 loss_att 15.099424 loss_ctc 20.275873 loss_rnnt 4.970430 hw_loss 0.303411 lr 0.00073092 rank 5
2023-02-17 11:43:16,121 DEBUG TRAIN Batch 5/5100 loss 25.759766 loss_att 26.966162 loss_ctc 32.002472 loss_rnnt 24.559345 hw_loss 0.237713 lr 0.00073083 rank 1
2023-02-17 11:43:16,123 DEBUG TRAIN Batch 5/5100 loss 16.371613 loss_att 25.855917 loss_ctc 32.963531 loss_rnnt 12.065604 hw_loss 0.369172 lr 0.00073091 rank 0
2023-02-17 11:43:16,123 DEBUG TRAIN Batch 5/5100 loss 14.323828 loss_att 15.094593 loss_ctc 20.580959 loss_rnnt 13.154905 hw_loss 0.338411 lr 0.00073034 rank 2
2023-02-17 11:43:16,124 DEBUG TRAIN Batch 5/5100 loss 27.808949 loss_att 32.484222 loss_ctc 48.181343 loss_rnnt 24.079636 hw_loss 0.146137 lr 0.00073101 rank 3
2023-02-17 11:44:33,641 DEBUG TRAIN Batch 5/5200 loss 15.798882 loss_att 24.427830 loss_ctc 24.538681 loss_rnnt 12.780074 hw_loss 0.239458 lr 0.00073043 rank 6
2023-02-17 11:44:33,642 DEBUG TRAIN Batch 5/5200 loss 23.239393 loss_att 27.792141 loss_ctc 40.524906 loss_rnnt 19.857689 hw_loss 0.312037 lr 0.00072957 rank 2
2023-02-17 11:44:33,644 DEBUG TRAIN Batch 5/5200 loss 32.362473 loss_att 40.984550 loss_ctc 50.474922 loss_rnnt 28.066238 hw_loss 0.294047 lr 0.00073013 rank 0
2023-02-17 11:44:33,645 DEBUG TRAIN Batch 5/5200 loss 13.823200 loss_att 16.652382 loss_ctc 24.929996 loss_rnnt 11.576261 hw_loss 0.375368 lr 0.00073028 rank 4
2023-02-17 11:44:33,645 DEBUG TRAIN Batch 5/5200 loss 13.127737 loss_att 16.617910 loss_ctc 23.361217 loss_rnnt 10.947378 hw_loss 0.220990 lr 0.00073014 rank 5
2023-02-17 11:44:33,646 DEBUG TRAIN Batch 5/5200 loss 17.439547 loss_att 18.980007 loss_ctc 24.544739 loss_rnnt 16.041027 hw_loss 0.268255 lr 0.00073023 rank 3
2023-02-17 11:44:33,647 DEBUG TRAIN Batch 5/5200 loss 29.459770 loss_att 37.936291 loss_ctc 40.997231 loss_rnnt 26.087154 hw_loss 0.260592 lr 0.00073042 rank 7
2023-02-17 11:44:33,651 DEBUG TRAIN Batch 5/5200 loss 11.361594 loss_att 15.285541 loss_ctc 20.928520 loss_rnnt 9.144066 hw_loss 0.294654 lr 0.00073005 rank 1
2023-02-17 11:45:51,659 DEBUG TRAIN Batch 5/5300 loss 16.234781 loss_att 27.174417 loss_ctc 27.889603 loss_rnnt 12.354105 hw_loss 0.260195 lr 0.00072946 rank 3
2023-02-17 11:45:51,664 DEBUG TRAIN Batch 5/5300 loss 9.501648 loss_att 15.999077 loss_ctc 15.266302 loss_rnnt 7.318223 hw_loss 0.216221 lr 0.00072965 rank 6
2023-02-17 11:45:51,665 DEBUG TRAIN Batch 5/5300 loss 20.870834 loss_att 23.549536 loss_ctc 32.519600 loss_rnnt 18.631260 hw_loss 0.282498 lr 0.00072936 rank 5
2023-02-17 11:45:51,666 DEBUG TRAIN Batch 5/5300 loss 12.518062 loss_att 19.244907 loss_ctc 20.529751 loss_rnnt 9.973344 hw_loss 0.245854 lr 0.00072950 rank 4
2023-02-17 11:45:51,669 DEBUG TRAIN Batch 5/5300 loss 7.521070 loss_att 12.018071 loss_ctc 18.446365 loss_rnnt 5.073739 hw_loss 0.171046 lr 0.00072879 rank 2
2023-02-17 11:45:51,677 DEBUG TRAIN Batch 5/5300 loss 11.023759 loss_att 14.176354 loss_ctc 16.426796 loss_rnnt 9.557884 hw_loss 0.215533 lr 0.00072927 rank 1
2023-02-17 11:45:51,712 DEBUG TRAIN Batch 5/5300 loss 30.025656 loss_att 36.883392 loss_ctc 51.314331 loss_rnnt 25.729162 hw_loss 0.162107 lr 0.00072936 rank 0
2023-02-17 11:45:51,749 DEBUG TRAIN Batch 5/5300 loss 27.544857 loss_att 30.231209 loss_ctc 42.243874 loss_rnnt 24.919613 hw_loss 0.240198 lr 0.00072964 rank 7
2023-02-17 11:47:08,433 DEBUG TRAIN Batch 5/5400 loss 24.444284 loss_att 29.180563 loss_ctc 40.014675 loss_rnnt 21.337263 hw_loss 0.156966 lr 0.00072858 rank 0
2023-02-17 11:47:08,435 DEBUG TRAIN Batch 5/5400 loss 17.110487 loss_att 21.890812 loss_ctc 26.960464 loss_rnnt 14.687567 hw_loss 0.287856 lr 0.00072873 rank 4
2023-02-17 11:47:08,435 DEBUG TRAIN Batch 5/5400 loss 21.449015 loss_att 25.809942 loss_ctc 30.154142 loss_rnnt 19.245079 hw_loss 0.320754 lr 0.00072888 rank 6
2023-02-17 11:47:08,435 DEBUG TRAIN Batch 5/5400 loss 18.086571 loss_att 25.971619 loss_ctc 29.510662 loss_rnnt 14.872766 hw_loss 0.212967 lr 0.00072887 rank 7
2023-02-17 11:47:08,437 DEBUG TRAIN Batch 5/5400 loss 32.460114 loss_att 37.367283 loss_ctc 51.063023 loss_rnnt 28.876358 hw_loss 0.228622 lr 0.00072802 rank 2
2023-02-17 11:47:08,438 DEBUG TRAIN Batch 5/5400 loss 27.752140 loss_att 30.682487 loss_ctc 45.337517 loss_rnnt 24.684792 hw_loss 0.256052 lr 0.00072850 rank 1
2023-02-17 11:47:08,440 DEBUG TRAIN Batch 5/5400 loss 18.255959 loss_att 22.500460 loss_ctc 30.464073 loss_rnnt 15.635186 hw_loss 0.270227 lr 0.00072868 rank 3
2023-02-17 11:47:08,442 DEBUG TRAIN Batch 5/5400 loss 19.670479 loss_att 22.857277 loss_ctc 30.526716 loss_rnnt 17.448402 hw_loss 0.257280 lr 0.00072859 rank 5
2023-02-17 11:48:23,953 DEBUG TRAIN Batch 5/5500 loss 16.940716 loss_att 21.671009 loss_ctc 25.471357 loss_rnnt 14.753767 hw_loss 0.194006 lr 0.00072810 rank 6
2023-02-17 11:48:23,957 DEBUG TRAIN Batch 5/5500 loss 14.636912 loss_att 18.446346 loss_ctc 21.521000 loss_rnnt 12.817379 hw_loss 0.262063 lr 0.00072782 rank 5
2023-02-17 11:48:23,958 DEBUG TRAIN Batch 5/5500 loss 13.794240 loss_att 19.218300 loss_ctc 21.728905 loss_rnnt 11.556041 hw_loss 0.178936 lr 0.00072772 rank 1
2023-02-17 11:48:23,958 DEBUG TRAIN Batch 5/5500 loss 27.065350 loss_att 28.198442 loss_ctc 38.333809 loss_rnnt 25.171352 hw_loss 0.309220 lr 0.00072791 rank 3
2023-02-17 11:48:23,958 DEBUG TRAIN Batch 5/5500 loss 27.226173 loss_att 34.039368 loss_ctc 40.397175 loss_rnnt 23.974653 hw_loss 0.248906 lr 0.00072809 rank 7
2023-02-17 11:48:23,959 DEBUG TRAIN Batch 5/5500 loss 19.450968 loss_att 21.935488 loss_ctc 37.578506 loss_rnnt 16.387548 hw_loss 0.280331 lr 0.00072781 rank 0
2023-02-17 11:48:23,959 DEBUG TRAIN Batch 5/5500 loss 29.251404 loss_att 37.343021 loss_ctc 46.025909 loss_rnnt 25.231754 hw_loss 0.308856 lr 0.00072725 rank 2
2023-02-17 11:48:23,961 DEBUG TRAIN Batch 5/5500 loss 25.728994 loss_att 28.185490 loss_ctc 37.086014 loss_rnnt 23.625486 hw_loss 0.183641 lr 0.00072796 rank 4
2023-02-17 11:49:39,393 DEBUG TRAIN Batch 5/5600 loss 16.947889 loss_att 20.309500 loss_ctc 21.599401 loss_rnnt 15.481837 hw_loss 0.325364 lr 0.00072648 rank 2
2023-02-17 11:49:39,394 DEBUG TRAIN Batch 5/5600 loss 23.169199 loss_att 25.664848 loss_ctc 40.596680 loss_rnnt 20.184134 hw_loss 0.304260 lr 0.00072733 rank 6
2023-02-17 11:49:39,394 DEBUG TRAIN Batch 5/5600 loss 27.745180 loss_att 33.163956 loss_ctc 45.674103 loss_rnnt 24.172396 hw_loss 0.184694 lr 0.00072704 rank 0
2023-02-17 11:49:39,395 DEBUG TRAIN Batch 5/5600 loss 13.909872 loss_att 17.380823 loss_ctc 21.200214 loss_rnnt 12.071194 hw_loss 0.323330 lr 0.00072719 rank 4
2023-02-17 11:49:39,396 DEBUG TRAIN Batch 5/5600 loss 9.812286 loss_att 15.072410 loss_ctc 17.275043 loss_rnnt 7.644162 hw_loss 0.226995 lr 0.00072714 rank 3
2023-02-17 11:49:39,397 DEBUG TRAIN Batch 5/5600 loss 13.499463 loss_att 17.026466 loss_ctc 22.741590 loss_rnnt 11.381846 hw_loss 0.337371 lr 0.00072732 rank 7
2023-02-17 11:49:39,402 DEBUG TRAIN Batch 5/5600 loss 20.771278 loss_att 23.208668 loss_ctc 33.027756 loss_rnnt 18.549438 hw_loss 0.187807 lr 0.00072705 rank 5
2023-02-17 11:49:39,410 DEBUG TRAIN Batch 5/5600 loss 15.815039 loss_att 18.145136 loss_ctc 21.207788 loss_rnnt 14.516217 hw_loss 0.213317 lr 0.00072695 rank 1
2023-02-17 11:50:59,069 DEBUG TRAIN Batch 5/5700 loss 21.655579 loss_att 21.369368 loss_ctc 31.051878 loss_rnnt 20.368069 hw_loss 0.172340 lr 0.00072656 rank 6
2023-02-17 11:50:59,071 DEBUG TRAIN Batch 5/5700 loss 8.518447 loss_att 11.036338 loss_ctc 13.736884 loss_rnnt 7.160682 hw_loss 0.296992 lr 0.00072627 rank 0
2023-02-17 11:50:59,071 DEBUG TRAIN Batch 5/5700 loss 18.777439 loss_att 27.322899 loss_ctc 30.009298 loss_rnnt 15.431244 hw_loss 0.261601 lr 0.00072628 rank 5
2023-02-17 11:50:59,072 DEBUG TRAIN Batch 5/5700 loss 17.409464 loss_att 17.037695 loss_ctc 22.248911 loss_rnnt 16.658245 hw_loss 0.338088 lr 0.00072637 rank 3
2023-02-17 11:50:59,074 DEBUG TRAIN Batch 5/5700 loss 11.830379 loss_att 11.659836 loss_ctc 16.456442 loss_rnnt 11.073650 hw_loss 0.326302 lr 0.00072571 rank 2
2023-02-17 11:50:59,076 DEBUG TRAIN Batch 5/5700 loss 18.636066 loss_att 24.677555 loss_ctc 34.107433 loss_rnnt 15.268006 hw_loss 0.181711 lr 0.00072642 rank 4
2023-02-17 11:50:59,078 DEBUG TRAIN Batch 5/5700 loss 20.772877 loss_att 26.765778 loss_ctc 37.799210 loss_rnnt 17.167339 hw_loss 0.256461 lr 0.00072619 rank 1
2023-02-17 11:50:59,080 DEBUG TRAIN Batch 5/5700 loss 15.552703 loss_att 21.167610 loss_ctc 29.278248 loss_rnnt 12.415957 hw_loss 0.344422 lr 0.00072656 rank 7
2023-02-17 11:52:14,867 DEBUG TRAIN Batch 5/5800 loss 15.188011 loss_att 19.077408 loss_ctc 23.434433 loss_rnnt 13.211586 hw_loss 0.185669 lr 0.00072551 rank 0
2023-02-17 11:52:14,867 DEBUG TRAIN Batch 5/5800 loss 17.733936 loss_att 22.899628 loss_ctc 28.054165 loss_rnnt 15.167144 hw_loss 0.295542 lr 0.00072495 rank 2
2023-02-17 11:52:14,869 DEBUG TRAIN Batch 5/5800 loss 24.056387 loss_att 25.467234 loss_ctc 27.948324 loss_rnnt 23.123817 hw_loss 0.246516 lr 0.00072580 rank 6
2023-02-17 11:52:14,873 DEBUG TRAIN Batch 5/5800 loss 15.987025 loss_att 18.153751 loss_ctc 26.370459 loss_rnnt 14.026138 hw_loss 0.268282 lr 0.00072565 rank 4
2023-02-17 11:52:14,873 DEBUG TRAIN Batch 5/5800 loss 16.744129 loss_att 20.418474 loss_ctc 21.643269 loss_rnnt 15.194231 hw_loss 0.303390 lr 0.00072561 rank 3
2023-02-17 11:52:14,875 DEBUG TRAIN Batch 5/5800 loss 15.522877 loss_att 18.836458 loss_ctc 25.735218 loss_rnnt 13.420833 hw_loss 0.145654 lr 0.00072542 rank 1
2023-02-17 11:52:14,879 DEBUG TRAIN Batch 5/5800 loss 22.667189 loss_att 34.792198 loss_ctc 33.378296 loss_rnnt 18.713997 hw_loss 0.187577 lr 0.00072579 rank 7
2023-02-17 11:52:14,886 DEBUG TRAIN Batch 5/5800 loss 21.078400 loss_att 29.894709 loss_ctc 33.782944 loss_rnnt 17.445772 hw_loss 0.328920 lr 0.00072551 rank 5
2023-02-17 11:53:31,148 DEBUG TRAIN Batch 5/5900 loss 19.146864 loss_att 19.590597 loss_ctc 37.968197 loss_rnnt 16.441601 hw_loss 0.200635 lr 0.00072503 rank 6
2023-02-17 11:53:31,150 DEBUG TRAIN Batch 5/5900 loss 27.352184 loss_att 32.252052 loss_ctc 47.506489 loss_rnnt 23.520046 hw_loss 0.309233 lr 0.00072466 rank 1
2023-02-17 11:53:31,151 DEBUG TRAIN Batch 5/5900 loss 14.482595 loss_att 21.545464 loss_ctc 27.888000 loss_rnnt 11.175600 hw_loss 0.200687 lr 0.00072474 rank 0
2023-02-17 11:53:31,152 DEBUG TRAIN Batch 5/5900 loss 12.181672 loss_att 18.670624 loss_ctc 24.960567 loss_rnnt 9.043801 hw_loss 0.255426 lr 0.00072419 rank 2
2023-02-17 11:53:31,152 DEBUG TRAIN Batch 5/5900 loss 15.446768 loss_att 21.747276 loss_ctc 21.407635 loss_rnnt 13.282772 hw_loss 0.204584 lr 0.00072489 rank 4
2023-02-17 11:53:31,154 DEBUG TRAIN Batch 5/5900 loss 29.214134 loss_att 36.437233 loss_ctc 46.647957 loss_rnnt 25.265598 hw_loss 0.336383 lr 0.00072484 rank 3
2023-02-17 11:53:31,156 DEBUG TRAIN Batch 5/5900 loss 10.722136 loss_att 16.549442 loss_ctc 19.203703 loss_rnnt 8.302502 hw_loss 0.231182 lr 0.00072475 rank 5
2023-02-17 11:53:31,197 DEBUG TRAIN Batch 5/5900 loss 16.965836 loss_att 21.823818 loss_ctc 30.837219 loss_rnnt 14.023528 hw_loss 0.227235 lr 0.00072503 rank 7
2023-02-17 11:54:49,411 DEBUG TRAIN Batch 5/6000 loss 26.485678 loss_att 31.723829 loss_ctc 51.676041 loss_rnnt 21.955948 hw_loss 0.231348 lr 0.00072427 rank 6
2023-02-17 11:54:49,415 DEBUG TRAIN Batch 5/6000 loss 12.512483 loss_att 15.140294 loss_ctc 24.954391 loss_rnnt 10.227386 hw_loss 0.188649 lr 0.00072413 rank 4
2023-02-17 11:54:49,416 DEBUG TRAIN Batch 5/6000 loss 25.223688 loss_att 31.527519 loss_ctc 38.050575 loss_rnnt 22.129356 hw_loss 0.231213 lr 0.00072399 rank 5
2023-02-17 11:54:49,417 DEBUG TRAIN Batch 5/6000 loss 32.005692 loss_att 35.912449 loss_ctc 62.501705 loss_rnnt 27.039345 hw_loss 0.222861 lr 0.00072343 rank 2
2023-02-17 11:54:49,417 DEBUG TRAIN Batch 5/6000 loss 19.942547 loss_att 26.994669 loss_ctc 32.035534 loss_rnnt 16.775106 hw_loss 0.271157 lr 0.00072408 rank 3
2023-02-17 11:54:49,422 DEBUG TRAIN Batch 5/6000 loss 17.347710 loss_att 22.946800 loss_ctc 28.730593 loss_rnnt 14.547873 hw_loss 0.304318 lr 0.00072390 rank 1
2023-02-17 11:54:49,438 DEBUG TRAIN Batch 5/6000 loss 26.852926 loss_att 33.675423 loss_ctc 42.128609 loss_rnnt 23.276821 hw_loss 0.327846 lr 0.00072398 rank 0
2023-02-17 11:54:49,441 DEBUG TRAIN Batch 5/6000 loss 23.601942 loss_att 26.944031 loss_ctc 43.131920 loss_rnnt 20.167576 hw_loss 0.303661 lr 0.00072427 rank 7
2023-02-17 11:56:07,009 DEBUG TRAIN Batch 5/6100 loss 19.413330 loss_att 23.105324 loss_ctc 31.385008 loss_rnnt 16.979687 hw_loss 0.185662 lr 0.00072332 rank 3
2023-02-17 11:56:07,013 DEBUG TRAIN Batch 5/6100 loss 15.277941 loss_att 18.386257 loss_ctc 25.553234 loss_rnnt 13.164490 hw_loss 0.228280 lr 0.00072351 rank 6
2023-02-17 11:56:07,014 DEBUG TRAIN Batch 5/6100 loss 25.410160 loss_att 25.574104 loss_ctc 46.160507 loss_rnnt 22.495504 hw_loss 0.215914 lr 0.00072267 rank 2
2023-02-17 11:56:07,015 DEBUG TRAIN Batch 5/6100 loss 17.540375 loss_att 23.039558 loss_ctc 33.052429 loss_rnnt 14.288286 hw_loss 0.157458 lr 0.00072323 rank 5
2023-02-17 11:56:07,016 DEBUG TRAIN Batch 5/6100 loss 27.455708 loss_att 39.092049 loss_ctc 44.491837 loss_rnnt 22.701870 hw_loss 0.290783 lr 0.00072337 rank 4
2023-02-17 11:56:07,018 DEBUG TRAIN Batch 5/6100 loss 16.454422 loss_att 21.475832 loss_ctc 28.125483 loss_rnnt 13.741985 hw_loss 0.285024 lr 0.00072351 rank 7
2023-02-17 11:56:07,020 DEBUG TRAIN Batch 5/6100 loss 17.736271 loss_att 24.492649 loss_ctc 30.162027 loss_rnnt 14.653540 hw_loss 0.140041 lr 0.00072323 rank 0
2023-02-17 11:56:07,025 DEBUG TRAIN Batch 5/6100 loss 15.527903 loss_att 18.818546 loss_ctc 25.436962 loss_rnnt 13.359110 hw_loss 0.355229 lr 0.00072314 rank 1
2023-02-17 11:57:22,353 DEBUG TRAIN Batch 5/6200 loss 17.780294 loss_att 19.839430 loss_ctc 26.134760 loss_rnnt 16.125698 hw_loss 0.241574 lr 0.00072276 rank 6
2023-02-17 11:57:22,354 DEBUG TRAIN Batch 5/6200 loss 6.627457 loss_att 9.417145 loss_ctc 11.265201 loss_rnnt 5.341165 hw_loss 0.206228 lr 0.00072247 rank 0
2023-02-17 11:57:22,355 DEBUG TRAIN Batch 5/6200 loss 16.321138 loss_att 18.780428 loss_ctc 27.227222 loss_rnnt 14.212270 hw_loss 0.305371 lr 0.00072257 rank 3
2023-02-17 11:57:22,356 DEBUG TRAIN Batch 5/6200 loss 16.447857 loss_att 21.197807 loss_ctc 24.256529 loss_rnnt 14.283524 hw_loss 0.324725 lr 0.00072192 rank 2
2023-02-17 11:57:22,358 DEBUG TRAIN Batch 5/6200 loss 20.808414 loss_att 27.428440 loss_ctc 28.471869 loss_rnnt 18.319126 hw_loss 0.269041 lr 0.00072275 rank 7
2023-02-17 11:57:22,358 DEBUG TRAIN Batch 5/6200 loss 31.552910 loss_att 34.717152 loss_ctc 44.397774 loss_rnnt 29.117161 hw_loss 0.169218 lr 0.00072248 rank 5
2023-02-17 11:57:22,359 DEBUG TRAIN Batch 5/6200 loss 16.322723 loss_att 19.960455 loss_ctc 24.767134 loss_rnnt 14.293590 hw_loss 0.329374 lr 0.00072261 rank 4
2023-02-17 11:57:22,359 DEBUG TRAIN Batch 5/6200 loss 21.403959 loss_att 24.961296 loss_ctc 31.633749 loss_rnnt 19.192783 hw_loss 0.254506 lr 0.00072239 rank 1
2023-02-17 11:58:38,669 DEBUG TRAIN Batch 5/6300 loss 15.485507 loss_att 17.014158 loss_ctc 25.305889 loss_rnnt 13.714282 hw_loss 0.292706 lr 0.00072182 rank 3
2023-02-17 11:58:38,670 DEBUG TRAIN Batch 5/6300 loss 24.900898 loss_att 29.462332 loss_ctc 36.013008 loss_rnnt 22.325171 hw_loss 0.340922 lr 0.00072200 rank 6
2023-02-17 11:58:38,670 DEBUG TRAIN Batch 5/6300 loss 30.820259 loss_att 30.445799 loss_ctc 43.615559 loss_rnnt 29.028713 hw_loss 0.300747 lr 0.00072186 rank 4
2023-02-17 11:58:38,671 DEBUG TRAIN Batch 5/6300 loss 8.866821 loss_att 8.940630 loss_ctc 11.282169 loss_rnnt 8.315203 hw_loss 0.402770 lr 0.00072117 rank 2
2023-02-17 11:58:38,674 DEBUG TRAIN Batch 5/6300 loss 20.995718 loss_att 27.411922 loss_ctc 35.551590 loss_rnnt 17.638937 hw_loss 0.248925 lr 0.00072164 rank 1
2023-02-17 11:58:38,675 DEBUG TRAIN Batch 5/6300 loss 19.087915 loss_att 21.239178 loss_ctc 28.092422 loss_rnnt 17.294804 hw_loss 0.304235 lr 0.00072173 rank 5
2023-02-17 11:58:38,676 DEBUG TRAIN Batch 5/6300 loss 14.556545 loss_att 14.611591 loss_ctc 18.251587 loss_rnnt 13.856735 hw_loss 0.367740 lr 0.00072172 rank 0
2023-02-17 11:58:38,676 DEBUG TRAIN Batch 5/6300 loss 22.553488 loss_att 22.731018 loss_ctc 38.489410 loss_rnnt 20.195942 hw_loss 0.369846 lr 0.00072200 rank 7
2023-02-17 11:59:56,963 DEBUG TRAIN Batch 5/6400 loss 9.782253 loss_att 11.525465 loss_ctc 13.762444 loss_rnnt 8.776406 hw_loss 0.237209 lr 0.00072097 rank 5
2023-02-17 11:59:56,966 DEBUG TRAIN Batch 5/6400 loss 11.515939 loss_att 18.181274 loss_ctc 21.971992 loss_rnnt 8.704542 hw_loss 0.157855 lr 0.00072106 rank 3
2023-02-17 11:59:56,967 DEBUG TRAIN Batch 5/6400 loss 15.384439 loss_att 16.436005 loss_ctc 23.441137 loss_rnnt 13.954099 hw_loss 0.273373 lr 0.00072111 rank 4
2023-02-17 11:59:56,968 DEBUG TRAIN Batch 5/6400 loss 18.284061 loss_att 23.877758 loss_ctc 30.600323 loss_rnnt 15.316795 hw_loss 0.386926 lr 0.00072042 rank 2
2023-02-17 11:59:56,971 DEBUG TRAIN Batch 5/6400 loss 22.485722 loss_att 22.861446 loss_ctc 36.997585 loss_rnnt 20.327406 hw_loss 0.277980 lr 0.00072124 rank 7
2023-02-17 11:59:56,972 DEBUG TRAIN Batch 5/6400 loss 16.426308 loss_att 17.347507 loss_ctc 22.143074 loss_rnnt 15.334055 hw_loss 0.273333 lr 0.00072088 rank 1
2023-02-17 11:59:56,975 DEBUG TRAIN Batch 5/6400 loss 15.375067 loss_att 21.993973 loss_ctc 18.167416 loss_rnnt 13.546039 hw_loss 0.249249 lr 0.00072125 rank 6
2023-02-17 11:59:56,990 DEBUG TRAIN Batch 5/6400 loss 12.310817 loss_att 19.132351 loss_ctc 19.374823 loss_rnnt 9.871227 hw_loss 0.250152 lr 0.00072097 rank 0
2023-02-17 12:01:12,455 DEBUG TRAIN Batch 5/6500 loss 3.145019 loss_att 5.674454 loss_ctc 5.197381 loss_rnnt 2.222986 hw_loss 0.267184 lr 0.00072014 rank 1
2023-02-17 12:01:12,457 DEBUG TRAIN Batch 5/6500 loss 21.414862 loss_att 20.437481 loss_ctc 27.419426 loss_rnnt 20.611538 hw_loss 0.371611 lr 0.00072036 rank 4
2023-02-17 12:01:12,458 DEBUG TRAIN Batch 5/6500 loss 24.780464 loss_att 25.854988 loss_ctc 36.358868 loss_rnnt 22.916725 hw_loss 0.196961 lr 0.00072050 rank 6
2023-02-17 12:01:12,460 DEBUG TRAIN Batch 5/6500 loss 15.387896 loss_att 24.318293 loss_ctc 23.180410 loss_rnnt 12.443281 hw_loss 0.224125 lr 0.00072050 rank 7
2023-02-17 12:01:12,460 DEBUG TRAIN Batch 5/6500 loss 25.451260 loss_att 26.464399 loss_ctc 48.304062 loss_rnnt 22.083130 hw_loss 0.222117 lr 0.00072032 rank 3
2023-02-17 12:01:12,464 DEBUG TRAIN Batch 5/6500 loss 18.578396 loss_att 23.071121 loss_ctc 29.168928 loss_rnnt 16.087156 hw_loss 0.338670 lr 0.00072022 rank 0
2023-02-17 12:01:12,465 DEBUG TRAIN Batch 5/6500 loss 15.777750 loss_att 17.098560 loss_ctc 21.547108 loss_rnnt 14.606873 hw_loss 0.257752 lr 0.00071967 rank 2
2023-02-17 12:01:12,466 DEBUG TRAIN Batch 5/6500 loss 28.825064 loss_att 34.367634 loss_ctc 43.752895 loss_rnnt 25.550650 hw_loss 0.329103 lr 0.00072023 rank 5
2023-02-17 12:02:27,051 DEBUG TRAIN Batch 5/6600 loss 43.876507 loss_att 45.180019 loss_ctc 66.847221 loss_rnnt 40.377777 hw_loss 0.328618 lr 0.00071975 rank 7
2023-02-17 12:02:27,067 DEBUG TRAIN Batch 5/6600 loss 18.693213 loss_att 26.327188 loss_ctc 40.591473 loss_rnnt 14.153164 hw_loss 0.175283 lr 0.00071957 rank 3
2023-02-17 12:02:27,068 DEBUG TRAIN Batch 5/6600 loss 20.762184 loss_att 28.139755 loss_ctc 32.825577 loss_rnnt 17.576363 hw_loss 0.190976 lr 0.00071976 rank 6
2023-02-17 12:02:27,068 DEBUG TRAIN Batch 5/6600 loss 12.581328 loss_att 14.017117 loss_ctc 20.914833 loss_rnnt 11.022171 hw_loss 0.301623 lr 0.00071947 rank 0
2023-02-17 12:02:27,072 DEBUG TRAIN Batch 5/6600 loss 18.135782 loss_att 24.967716 loss_ctc 32.510197 loss_rnnt 14.688978 hw_loss 0.307180 lr 0.00071948 rank 5
2023-02-17 12:02:27,072 DEBUG TRAIN Batch 5/6600 loss 21.521378 loss_att 24.713478 loss_ctc 36.035713 loss_rnnt 18.762104 hw_loss 0.348014 lr 0.00071961 rank 4
2023-02-17 12:02:27,075 DEBUG TRAIN Batch 5/6600 loss 18.832300 loss_att 24.966970 loss_ctc 26.092514 loss_rnnt 16.481848 hw_loss 0.291545 lr 0.00071939 rank 1
2023-02-17 12:02:27,076 DEBUG TRAIN Batch 5/6600 loss 11.593338 loss_att 15.877746 loss_ctc 18.535192 loss_rnnt 9.653643 hw_loss 0.294814 lr 0.00071893 rank 2
2023-02-17 12:03:43,286 DEBUG TRAIN Batch 5/6700 loss 32.612293 loss_att 37.220860 loss_ctc 49.644489 loss_rnnt 29.205976 hw_loss 0.400578 lr 0.00071900 rank 7
2023-02-17 12:03:43,289 DEBUG TRAIN Batch 5/6700 loss 13.346829 loss_att 15.252432 loss_ctc 19.028435 loss_rnnt 12.077468 hw_loss 0.245051 lr 0.00071873 rank 0
2023-02-17 12:03:43,291 DEBUG TRAIN Batch 5/6700 loss 24.893152 loss_att 29.290659 loss_ctc 40.650139 loss_rnnt 21.776075 hw_loss 0.256207 lr 0.00071887 rank 4
2023-02-17 12:03:43,291 DEBUG TRAIN Batch 5/6700 loss 12.759418 loss_att 19.101831 loss_ctc 25.681782 loss_rnnt 9.622240 hw_loss 0.273211 lr 0.00071901 rank 6
2023-02-17 12:03:43,292 DEBUG TRAIN Batch 5/6700 loss 11.211525 loss_att 16.258080 loss_ctc 23.257793 loss_rnnt 8.493586 hw_loss 0.192111 lr 0.00071883 rank 3
2023-02-17 12:03:43,292 DEBUG TRAIN Batch 5/6700 loss 29.126530 loss_att 32.271393 loss_ctc 40.116524 loss_rnnt 26.907951 hw_loss 0.233009 lr 0.00071819 rank 2
2023-02-17 12:03:43,295 DEBUG TRAIN Batch 5/6700 loss 22.962011 loss_att 25.709375 loss_ctc 38.899067 loss_rnnt 20.164068 hw_loss 0.231618 lr 0.00071874 rank 5
2023-02-17 12:03:43,321 DEBUG TRAIN Batch 5/6700 loss 13.669634 loss_att 17.256628 loss_ctc 25.273445 loss_rnnt 11.261904 hw_loss 0.268417 lr 0.00071865 rank 1
2023-02-17 12:05:00,256 DEBUG TRAIN Batch 5/6800 loss 23.246885 loss_att 26.224163 loss_ctc 39.326977 loss_rnnt 20.325285 hw_loss 0.341500 lr 0.00071826 rank 7
2023-02-17 12:05:00,259 DEBUG TRAIN Batch 5/6800 loss 21.377226 loss_att 28.578621 loss_ctc 39.356159 loss_rnnt 17.388577 hw_loss 0.283462 lr 0.00071827 rank 6
2023-02-17 12:05:00,262 DEBUG TRAIN Batch 5/6800 loss 28.028311 loss_att 30.574493 loss_ctc 37.863544 loss_rnnt 26.050522 hw_loss 0.294728 lr 0.00071813 rank 4
2023-02-17 12:05:00,264 DEBUG TRAIN Batch 5/6800 loss 17.805325 loss_att 24.031223 loss_ctc 31.315626 loss_rnnt 14.652039 hw_loss 0.200125 lr 0.00071808 rank 3
2023-02-17 12:05:00,266 DEBUG TRAIN Batch 5/6800 loss 28.787085 loss_att 29.422457 loss_ctc 35.963043 loss_rnnt 27.540257 hw_loss 0.305550 lr 0.00071745 rank 2
2023-02-17 12:05:00,267 DEBUG TRAIN Batch 5/6800 loss 18.098021 loss_att 23.080770 loss_ctc 28.137135 loss_rnnt 15.593526 hw_loss 0.317616 lr 0.00071800 rank 5
2023-02-17 12:05:00,266 DEBUG TRAIN Batch 5/6800 loss 11.214490 loss_att 13.548071 loss_ctc 20.215881 loss_rnnt 9.357090 hw_loss 0.357182 lr 0.00071791 rank 1
2023-02-17 12:05:00,268 DEBUG TRAIN Batch 5/6800 loss 16.265841 loss_att 17.771980 loss_ctc 25.804859 loss_rnnt 14.575360 hw_loss 0.220092 lr 0.00071799 rank 0
2023-02-17 12:06:15,293 DEBUG TRAIN Batch 5/6900 loss 20.324955 loss_att 23.816599 loss_ctc 33.737225 loss_rnnt 17.684364 hw_loss 0.288673 lr 0.00071725 rank 0
2023-02-17 12:06:15,294 DEBUG TRAIN Batch 5/6900 loss 15.157013 loss_att 18.095919 loss_ctc 22.507332 loss_rnnt 13.451880 hw_loss 0.257456 lr 0.00071752 rank 7
2023-02-17 12:06:15,295 DEBUG TRAIN Batch 5/6900 loss 10.690610 loss_att 12.757036 loss_ctc 21.309479 loss_rnnt 8.724752 hw_loss 0.256358 lr 0.00071671 rank 2
2023-02-17 12:06:15,294 DEBUG TRAIN Batch 5/6900 loss 13.022122 loss_att 13.893082 loss_ctc 20.659372 loss_rnnt 11.638807 hw_loss 0.357796 lr 0.00071717 rank 1
2023-02-17 12:06:15,295 DEBUG TRAIN Batch 5/6900 loss 14.882261 loss_att 17.001923 loss_ctc 23.207575 loss_rnnt 13.230270 hw_loss 0.221282 lr 0.00071734 rank 3
2023-02-17 12:06:15,299 DEBUG TRAIN Batch 5/6900 loss 13.098052 loss_att 16.579931 loss_ctc 20.485603 loss_rnnt 11.304967 hw_loss 0.209441 lr 0.00071739 rank 4
2023-02-17 12:06:15,299 DEBUG TRAIN Batch 5/6900 loss 13.322481 loss_att 18.380854 loss_ctc 22.551680 loss_rnnt 10.951695 hw_loss 0.241031 lr 0.00071753 rank 6
2023-02-17 12:06:15,304 DEBUG TRAIN Batch 5/6900 loss 21.129927 loss_att 26.529659 loss_ctc 34.239227 loss_rnnt 18.179173 hw_loss 0.230443 lr 0.00071726 rank 5
2023-02-17 12:07:31,082 DEBUG TRAIN Batch 5/7000 loss 15.122144 loss_att 16.138626 loss_ctc 24.605576 loss_rnnt 13.470970 hw_loss 0.343911 lr 0.00071679 rank 6
2023-02-17 12:07:31,084 DEBUG TRAIN Batch 5/7000 loss 14.836426 loss_att 15.894438 loss_ctc 19.709309 loss_rnnt 13.823505 hw_loss 0.284250 lr 0.00071661 rank 3
2023-02-17 12:07:31,088 DEBUG TRAIN Batch 5/7000 loss 18.194778 loss_att 26.453148 loss_ctc 34.683945 loss_rnnt 14.210986 hw_loss 0.250431 lr 0.00071643 rank 1
2023-02-17 12:07:31,088 DEBUG TRAIN Batch 5/7000 loss 12.638796 loss_att 11.627307 loss_ctc 16.807053 loss_rnnt 12.108722 hw_loss 0.331136 lr 0.00071652 rank 5
2023-02-17 12:07:31,090 DEBUG TRAIN Batch 5/7000 loss 31.783419 loss_att 35.412640 loss_ctc 44.797573 loss_rnnt 29.245926 hw_loss 0.143303 lr 0.00071598 rank 2
2023-02-17 12:07:31,091 DEBUG TRAIN Batch 5/7000 loss 13.211796 loss_att 18.674578 loss_ctc 26.742466 loss_rnnt 10.194614 hw_loss 0.226003 lr 0.00071665 rank 4
2023-02-17 12:07:31,091 DEBUG TRAIN Batch 5/7000 loss 23.950159 loss_att 33.395931 loss_ctc 42.857445 loss_rnnt 19.404675 hw_loss 0.253797 lr 0.00071651 rank 0
2023-02-17 12:07:31,094 DEBUG TRAIN Batch 5/7000 loss 20.512894 loss_att 25.255516 loss_ctc 35.397842 loss_rnnt 17.473143 hw_loss 0.199815 lr 0.00071678 rank 7
2023-02-17 12:08:49,280 DEBUG TRAIN Batch 5/7100 loss 18.945648 loss_att 20.758293 loss_ctc 24.887524 loss_rnnt 17.591038 hw_loss 0.374682 lr 0.00071605 rank 7
2023-02-17 12:08:49,285 DEBUG TRAIN Batch 5/7100 loss 11.993592 loss_att 16.540373 loss_ctc 22.406963 loss_rnnt 9.608905 hw_loss 0.162903 lr 0.00071606 rank 6
2023-02-17 12:08:49,290 DEBUG TRAIN Batch 5/7100 loss 20.156311 loss_att 21.338634 loss_ctc 29.057995 loss_rnnt 18.503630 hw_loss 0.429983 lr 0.00071592 rank 4
2023-02-17 12:08:49,290 DEBUG TRAIN Batch 5/7100 loss 16.266817 loss_att 24.888317 loss_ctc 32.295830 loss_rnnt 12.302928 hw_loss 0.191982 lr 0.00071524 rank 2
2023-02-17 12:08:49,292 DEBUG TRAIN Batch 5/7100 loss 16.003647 loss_att 23.942255 loss_ctc 27.877274 loss_rnnt 12.712969 hw_loss 0.224634 lr 0.00071587 rank 3
2023-02-17 12:08:49,293 DEBUG TRAIN Batch 5/7100 loss 11.860433 loss_att 17.994230 loss_ctc 17.992767 loss_rnnt 9.716607 hw_loss 0.186414 lr 0.00071578 rank 5
2023-02-17 12:08:49,302 DEBUG TRAIN Batch 5/7100 loss 13.741836 loss_att 16.850079 loss_ctc 28.986107 loss_rnnt 10.953639 hw_loss 0.251210 lr 0.00071578 rank 0
2023-02-17 12:08:49,304 DEBUG TRAIN Batch 5/7100 loss 31.492294 loss_att 33.051521 loss_ctc 46.190151 loss_rnnt 29.101574 hw_loss 0.223426 lr 0.00071570 rank 1
2023-02-17 12:10:05,353 DEBUG TRAIN Batch 5/7200 loss 11.219812 loss_att 16.867439 loss_ctc 18.592838 loss_rnnt 8.993202 hw_loss 0.213777 lr 0.00071451 rank 2
2023-02-17 12:10:05,355 DEBUG TRAIN Batch 5/7200 loss 16.053856 loss_att 20.410786 loss_ctc 28.685741 loss_rnnt 13.443228 hw_loss 0.103106 lr 0.00071532 rank 6
2023-02-17 12:10:05,358 DEBUG TRAIN Batch 5/7200 loss 13.958318 loss_att 18.186081 loss_ctc 26.727102 loss_rnnt 11.334175 hw_loss 0.142660 lr 0.00071518 rank 4
2023-02-17 12:10:05,358 DEBUG TRAIN Batch 5/7200 loss 22.966400 loss_att 27.935757 loss_ctc 31.311142 loss_rnnt 20.709721 hw_loss 0.281583 lr 0.00071514 rank 3
2023-02-17 12:10:05,360 DEBUG TRAIN Batch 5/7200 loss 29.170860 loss_att 36.132828 loss_ctc 49.227913 loss_rnnt 24.946480 hw_loss 0.295708 lr 0.00071532 rank 7
2023-02-17 12:10:05,362 DEBUG TRAIN Batch 5/7200 loss 23.162741 loss_att 24.461559 loss_ctc 29.871572 loss_rnnt 21.904165 hw_loss 0.195565 lr 0.00071504 rank 0
2023-02-17 12:10:05,362 DEBUG TRAIN Batch 5/7200 loss 16.378332 loss_att 21.792244 loss_ctc 29.885540 loss_rnnt 13.387164 hw_loss 0.201419 lr 0.00071496 rank 1
2023-02-17 12:10:05,373 DEBUG TRAIN Batch 5/7200 loss 37.715759 loss_att 42.492149 loss_ctc 60.995689 loss_rnnt 33.473339 hw_loss 0.343411 lr 0.00071505 rank 5
2023-02-17 12:11:19,968 DEBUG TRAIN Batch 5/7300 loss 30.708681 loss_att 34.354034 loss_ctc 49.844616 loss_rnnt 27.301725 hw_loss 0.237053 lr 0.00071423 rank 1
2023-02-17 12:11:19,980 DEBUG TRAIN Batch 5/7300 loss 23.968088 loss_att 32.258095 loss_ctc 42.322353 loss_rnnt 19.762054 hw_loss 0.188995 lr 0.00071459 rank 6
2023-02-17 12:11:19,981 DEBUG TRAIN Batch 5/7300 loss 14.276477 loss_att 18.003490 loss_ctc 22.682209 loss_rnnt 12.282162 hw_loss 0.240278 lr 0.00071378 rank 2
2023-02-17 12:11:19,982 DEBUG TRAIN Batch 5/7300 loss 18.052475 loss_att 23.127405 loss_ctc 35.777023 loss_rnnt 14.536913 hw_loss 0.257438 lr 0.00071441 rank 3
2023-02-17 12:11:19,982 DEBUG TRAIN Batch 5/7300 loss 19.224348 loss_att 23.231459 loss_ctc 28.125843 loss_rnnt 17.107965 hw_loss 0.240178 lr 0.00071458 rank 7
2023-02-17 12:11:19,987 DEBUG TRAIN Batch 5/7300 loss 21.977627 loss_att 26.913527 loss_ctc 33.928047 loss_rnnt 19.224361 hw_loss 0.323808 lr 0.00071445 rank 4
2023-02-17 12:11:19,987 DEBUG TRAIN Batch 5/7300 loss 13.722265 loss_att 18.679127 loss_ctc 22.602619 loss_rnnt 11.423738 hw_loss 0.230826 lr 0.00071432 rank 5
2023-02-17 12:11:20,043 DEBUG TRAIN Batch 5/7300 loss 16.495304 loss_att 17.676256 loss_ctc 26.058983 loss_rnnt 14.875704 hw_loss 0.202974 lr 0.00071431 rank 0
2023-02-17 12:12:35,483 DEBUG TRAIN Batch 5/7400 loss 9.760080 loss_att 13.315566 loss_ctc 16.663363 loss_rnnt 7.968782 hw_loss 0.299555 lr 0.00071386 rank 7
2023-02-17 12:12:35,493 DEBUG TRAIN Batch 5/7400 loss 24.242556 loss_att 31.009838 loss_ctc 42.820770 loss_rnnt 20.257671 hw_loss 0.289373 lr 0.00071368 rank 3
2023-02-17 12:12:35,496 DEBUG TRAIN Batch 5/7400 loss 22.624907 loss_att 25.642334 loss_ctc 30.897015 loss_rnnt 20.725887 hw_loss 0.361098 lr 0.00071359 rank 5
2023-02-17 12:12:35,496 DEBUG TRAIN Batch 5/7400 loss 25.773708 loss_att 27.677341 loss_ctc 38.068184 loss_rnnt 23.579010 hw_loss 0.327580 lr 0.00071306 rank 2
2023-02-17 12:12:35,506 DEBUG TRAIN Batch 5/7400 loss 12.836473 loss_att 19.200220 loss_ctc 30.067879 loss_rnnt 9.176687 hw_loss 0.167840 lr 0.00071359 rank 0
2023-02-17 12:12:35,510 DEBUG TRAIN Batch 5/7400 loss 18.295624 loss_att 23.196480 loss_ctc 32.733009 loss_rnnt 15.303854 hw_loss 0.162400 lr 0.00071351 rank 1
2023-02-17 12:12:35,524 DEBUG TRAIN Batch 5/7400 loss 15.613860 loss_att 18.970234 loss_ctc 28.431793 loss_rnnt 13.114301 hw_loss 0.223548 lr 0.00071373 rank 4
2023-02-17 12:12:35,525 DEBUG TRAIN Batch 5/7400 loss 13.997325 loss_att 18.265835 loss_ctc 20.011475 loss_rnnt 12.164614 hw_loss 0.332106 lr 0.00071386 rank 6
2023-02-17 12:13:53,819 DEBUG TRAIN Batch 5/7500 loss 10.515135 loss_att 17.040482 loss_ctc 18.809406 loss_rnnt 8.002115 hw_loss 0.191338 lr 0.00071287 rank 5
2023-02-17 12:13:53,821 DEBUG TRAIN Batch 5/7500 loss 8.871533 loss_att 15.593789 loss_ctc 18.086473 loss_rnnt 6.149287 hw_loss 0.279632 lr 0.00071296 rank 3
2023-02-17 12:13:53,821 DEBUG TRAIN Batch 5/7500 loss 14.043744 loss_att 14.627266 loss_ctc 21.435030 loss_rnnt 12.804426 hw_loss 0.257079 lr 0.00071278 rank 1
2023-02-17 12:13:53,822 DEBUG TRAIN Batch 5/7500 loss 10.986560 loss_att 12.180237 loss_ctc 17.493469 loss_rnnt 9.704753 hw_loss 0.329031 lr 0.00071286 rank 0
2023-02-17 12:13:53,826 DEBUG TRAIN Batch 5/7500 loss 18.834475 loss_att 21.260981 loss_ctc 25.420305 loss_rnnt 17.342182 hw_loss 0.241651 lr 0.00071314 rank 6
2023-02-17 12:13:53,827 DEBUG TRAIN Batch 5/7500 loss 18.480450 loss_att 20.702496 loss_ctc 31.829456 loss_rnnt 16.086555 hw_loss 0.318029 lr 0.00071233 rank 2
2023-02-17 12:13:53,828 DEBUG TRAIN Batch 5/7500 loss 17.651545 loss_att 22.714087 loss_ctc 26.183016 loss_rnnt 15.386659 hw_loss 0.215341 lr 0.00071300 rank 4
2023-02-17 12:13:53,829 DEBUG TRAIN Batch 5/7500 loss 20.474476 loss_att 23.873192 loss_ctc 33.699791 loss_rnnt 17.894520 hw_loss 0.256571 lr 0.00071313 rank 7
2023-02-17 12:15:09,500 DEBUG TRAIN Batch 5/7600 loss 19.058474 loss_att 20.098816 loss_ctc 28.505993 loss_rnnt 17.397715 hw_loss 0.361913 lr 0.00071241 rank 6
2023-02-17 12:15:09,501 DEBUG TRAIN Batch 5/7600 loss 12.473052 loss_att 13.663533 loss_ctc 16.963831 loss_rnnt 11.511576 hw_loss 0.233640 lr 0.00071223 rank 3
2023-02-17 12:15:09,502 DEBUG TRAIN Batch 5/7600 loss 11.777220 loss_att 12.930748 loss_ctc 18.005249 loss_rnnt 10.556211 hw_loss 0.299811 lr 0.00071161 rank 2
2023-02-17 12:15:09,502 DEBUG TRAIN Batch 5/7600 loss 8.043106 loss_att 12.767271 loss_ctc 14.858938 loss_rnnt 6.050409 hw_loss 0.260788 lr 0.00071206 rank 1
2023-02-17 12:15:09,503 DEBUG TRAIN Batch 5/7600 loss 20.207848 loss_att 21.181797 loss_ctc 27.549892 loss_rnnt 18.900890 hw_loss 0.249804 lr 0.00071215 rank 5
2023-02-17 12:15:09,504 DEBUG TRAIN Batch 5/7600 loss 20.719929 loss_att 26.020313 loss_ctc 30.131454 loss_rnnt 18.253740 hw_loss 0.283575 lr 0.00071241 rank 7
2023-02-17 12:15:09,505 DEBUG TRAIN Batch 5/7600 loss 22.812019 loss_att 23.782028 loss_ctc 36.684856 loss_rnnt 20.641003 hw_loss 0.238692 lr 0.00071228 rank 4
2023-02-17 12:15:09,505 DEBUG TRAIN Batch 5/7600 loss 13.847718 loss_att 14.107050 loss_ctc 20.117237 loss_rnnt 12.796038 hw_loss 0.307271 lr 0.00071214 rank 0
2023-02-17 12:16:24,449 DEBUG TRAIN Batch 5/7700 loss 15.626059 loss_att 18.778130 loss_ctc 21.964239 loss_rnnt 14.011976 hw_loss 0.259833 lr 0.00071142 rank 5
2023-02-17 12:16:24,450 DEBUG TRAIN Batch 5/7700 loss 16.695751 loss_att 23.097851 loss_ctc 28.226826 loss_rnnt 13.735071 hw_loss 0.267716 lr 0.00071168 rank 7
2023-02-17 12:16:24,450 DEBUG TRAIN Batch 5/7700 loss 12.775779 loss_att 19.780027 loss_ctc 23.417189 loss_rnnt 9.770125 hw_loss 0.348657 lr 0.00071169 rank 6
2023-02-17 12:16:24,452 DEBUG TRAIN Batch 5/7700 loss 12.069852 loss_att 16.036488 loss_ctc 19.075562 loss_rnnt 10.179810 hw_loss 0.304912 lr 0.00071155 rank 4
2023-02-17 12:16:24,455 DEBUG TRAIN Batch 5/7700 loss 18.628090 loss_att 26.379971 loss_ctc 31.874493 loss_rnnt 15.184240 hw_loss 0.238661 lr 0.00071151 rank 3
2023-02-17 12:16:24,456 DEBUG TRAIN Batch 5/7700 loss 18.353666 loss_att 20.955812 loss_ctc 29.009106 loss_rnnt 16.248262 hw_loss 0.307964 lr 0.00071142 rank 0
2023-02-17 12:16:24,457 DEBUG TRAIN Batch 5/7700 loss 17.115299 loss_att 21.268850 loss_ctc 34.206528 loss_rnnt 13.866842 hw_loss 0.260468 lr 0.00071089 rank 2
2023-02-17 12:16:24,457 DEBUG TRAIN Batch 5/7700 loss 15.269515 loss_att 20.925264 loss_ctc 28.694767 loss_rnnt 12.211403 hw_loss 0.256742 lr 0.00071134 rank 1
2023-02-17 12:17:42,431 DEBUG TRAIN Batch 5/7800 loss 15.463686 loss_att 19.313454 loss_ctc 27.209560 loss_rnnt 13.004321 hw_loss 0.231177 lr 0.00071079 rank 3
2023-02-17 12:17:42,432 DEBUG TRAIN Batch 5/7800 loss 12.891036 loss_att 18.481045 loss_ctc 20.631161 loss_rnnt 10.587707 hw_loss 0.287458 lr 0.00071097 rank 6
2023-02-17 12:17:42,437 DEBUG TRAIN Batch 5/7800 loss 19.911331 loss_att 26.246479 loss_ctc 28.929077 loss_rnnt 17.344311 hw_loss 0.183043 lr 0.00071070 rank 5
2023-02-17 12:17:42,436 DEBUG TRAIN Batch 5/7800 loss 14.083995 loss_att 21.121233 loss_ctc 29.163155 loss_rnnt 10.559051 hw_loss 0.200515 lr 0.00071017 rank 2
2023-02-17 12:17:42,438 DEBUG TRAIN Batch 5/7800 loss 38.969059 loss_att 41.818031 loss_ctc 54.424522 loss_rnnt 36.203796 hw_loss 0.252641 lr 0.00071062 rank 1
2023-02-17 12:17:42,438 DEBUG TRAIN Batch 5/7800 loss 22.188795 loss_att 31.856701 loss_ctc 35.430027 loss_rnnt 18.358047 hw_loss 0.246880 lr 0.00071083 rank 4
2023-02-17 12:17:42,448 DEBUG TRAIN Batch 5/7800 loss 24.252226 loss_att 31.241503 loss_ctc 35.650242 loss_rnnt 21.148466 hw_loss 0.349067 lr 0.00071070 rank 0
2023-02-17 12:17:42,462 DEBUG TRAIN Batch 5/7800 loss 5.826526 loss_att 9.047426 loss_ctc 9.649620 loss_rnnt 4.505633 hw_loss 0.313064 lr 0.00071096 rank 7
2023-02-17 12:18:58,098 DEBUG TRAIN Batch 5/7900 loss 28.328091 loss_att 33.494678 loss_ctc 45.102154 loss_rnnt 24.988737 hw_loss 0.130299 lr 0.00070999 rank 5
2023-02-17 12:18:58,109 DEBUG TRAIN Batch 5/7900 loss 25.567705 loss_att 28.343731 loss_ctc 43.335594 loss_rnnt 22.529633 hw_loss 0.213406 lr 0.00071007 rank 3
2023-02-17 12:18:58,111 DEBUG TRAIN Batch 5/7900 loss 23.807034 loss_att 26.374332 loss_ctc 42.578949 loss_rnnt 20.663301 hw_loss 0.238780 lr 0.00070946 rank 2
2023-02-17 12:18:58,113 DEBUG TRAIN Batch 5/7900 loss 44.243450 loss_att 40.985920 loss_ctc 52.304592 loss_rnnt 43.708916 hw_loss 0.208543 lr 0.00070998 rank 0
2023-02-17 12:18:58,113 DEBUG TRAIN Batch 5/7900 loss 27.156496 loss_att 31.587795 loss_ctc 41.291698 loss_rnnt 24.246216 hw_loss 0.261237 lr 0.00070990 rank 1
2023-02-17 12:18:58,116 DEBUG TRAIN Batch 5/7900 loss 11.390053 loss_att 15.604285 loss_ctc 21.657700 loss_rnnt 9.083212 hw_loss 0.178078 lr 0.00071012 rank 4
2023-02-17 12:18:58,115 DEBUG TRAIN Batch 5/7900 loss 18.779316 loss_att 25.190273 loss_ctc 33.616997 loss_rnnt 15.407592 hw_loss 0.208456 lr 0.00071025 rank 6
2023-02-17 12:18:58,163 DEBUG TRAIN Batch 5/7900 loss 15.910164 loss_att 16.795612 loss_ctc 27.534307 loss_rnnt 14.063093 hw_loss 0.225176 lr 0.00071025 rank 7
2023-02-17 12:20:13,352 DEBUG TRAIN Batch 5/8000 loss 22.747768 loss_att 27.451981 loss_ctc 32.833733 loss_rnnt 20.332714 hw_loss 0.242653 lr 0.00070953 rank 7
2023-02-17 12:20:13,356 DEBUG TRAIN Batch 5/8000 loss 21.214659 loss_att 26.048473 loss_ctc 31.561115 loss_rnnt 18.741259 hw_loss 0.238334 lr 0.00070927 rank 5
2023-02-17 12:20:13,357 DEBUG TRAIN Batch 5/8000 loss 16.152309 loss_att 20.603266 loss_ctc 27.812637 loss_rnnt 13.552994 hw_loss 0.289528 lr 0.00070940 rank 4
2023-02-17 12:20:13,357 DEBUG TRAIN Batch 5/8000 loss 20.910641 loss_att 21.425241 loss_ctc 30.401522 loss_rnnt 19.428169 hw_loss 0.213938 lr 0.00070927 rank 0
2023-02-17 12:20:13,358 DEBUG TRAIN Batch 5/8000 loss 25.360586 loss_att 27.464176 loss_ctc 38.652115 loss_rnnt 23.035984 hw_loss 0.246899 lr 0.00070954 rank 6
2023-02-17 12:20:13,360 DEBUG TRAIN Batch 5/8000 loss 22.306498 loss_att 26.961281 loss_ctc 32.827656 loss_rnnt 19.830502 hw_loss 0.266661 lr 0.00070919 rank 1
2023-02-17 12:20:13,361 DEBUG TRAIN Batch 5/8000 loss 24.858664 loss_att 30.275391 loss_ctc 38.572258 loss_rnnt 21.785686 hw_loss 0.302162 lr 0.00070936 rank 3
2023-02-17 12:20:13,365 DEBUG TRAIN Batch 5/8000 loss 13.974322 loss_att 19.110210 loss_ctc 28.017105 loss_rnnt 10.975883 hw_loss 0.185421 lr 0.00070875 rank 2
2023-02-17 12:21:28,934 DEBUG TRAIN Batch 5/8100 loss 16.379349 loss_att 19.907793 loss_ctc 29.464283 loss_rnnt 13.800143 hw_loss 0.241608 lr 0.00070882 rank 7
2023-02-17 12:21:28,937 DEBUG TRAIN Batch 5/8100 loss 27.905390 loss_att 33.099648 loss_ctc 46.153942 loss_rnnt 24.269899 hw_loss 0.306560 lr 0.00070882 rank 6
2023-02-17 12:21:28,940 DEBUG TRAIN Batch 5/8100 loss 12.224933 loss_att 15.661033 loss_ctc 24.564976 loss_rnnt 9.738224 hw_loss 0.289029 lr 0.00070856 rank 5
2023-02-17 12:21:28,944 DEBUG TRAIN Batch 5/8100 loss 18.420181 loss_att 24.123596 loss_ctc 35.177177 loss_rnnt 14.924706 hw_loss 0.225988 lr 0.00070803 rank 2
2023-02-17 12:21:28,945 DEBUG TRAIN Batch 5/8100 loss 11.508225 loss_att 15.296373 loss_ctc 20.906693 loss_rnnt 9.338260 hw_loss 0.298512 lr 0.00070869 rank 4
2023-02-17 12:21:28,946 DEBUG TRAIN Batch 5/8100 loss 12.660899 loss_att 14.759441 loss_ctc 18.669077 loss_rnnt 11.294277 hw_loss 0.273415 lr 0.00070865 rank 3
2023-02-17 12:21:28,947 DEBUG TRAIN Batch 5/8100 loss 11.922459 loss_att 14.727047 loss_ctc 19.436304 loss_rnnt 10.189765 hw_loss 0.318620 lr 0.00070848 rank 1
2023-02-17 12:21:28,965 DEBUG TRAIN Batch 5/8100 loss 26.587528 loss_att 29.538141 loss_ctc 43.119926 loss_rnnt 23.634327 hw_loss 0.297671 lr 0.00070855 rank 0
2023-02-17 12:22:45,473 DEBUG TRAIN Batch 5/8200 loss 17.280016 loss_att 20.218727 loss_ctc 28.669422 loss_rnnt 14.984390 hw_loss 0.354927 lr 0.00070733 rank 2
2023-02-17 12:22:45,472 DEBUG TRAIN Batch 5/8200 loss 13.322508 loss_att 14.323694 loss_ctc 21.232164 loss_rnnt 11.884390 hw_loss 0.343614 lr 0.00070777 rank 1
2023-02-17 12:22:45,475 DEBUG TRAIN Batch 5/8200 loss 18.212187 loss_att 18.637165 loss_ctc 30.686531 loss_rnnt 16.313992 hw_loss 0.281163 lr 0.00070811 rank 7
2023-02-17 12:22:45,477 DEBUG TRAIN Batch 5/8200 loss 11.780218 loss_att 13.164265 loss_ctc 21.551567 loss_rnnt 10.037450 hw_loss 0.305835 lr 0.00070784 rank 0
2023-02-17 12:22:45,476 DEBUG TRAIN Batch 5/8200 loss 15.626466 loss_att 17.924337 loss_ctc 26.285215 loss_rnnt 13.585045 hw_loss 0.301272 lr 0.00070811 rank 6
2023-02-17 12:22:45,477 DEBUG TRAIN Batch 5/8200 loss 16.011801 loss_att 19.211182 loss_ctc 22.234331 loss_rnnt 14.432343 hw_loss 0.206081 lr 0.00070794 rank 3
2023-02-17 12:22:45,478 DEBUG TRAIN Batch 5/8200 loss 22.606075 loss_att 28.630270 loss_ctc 38.332542 loss_rnnt 19.144880 hw_loss 0.299053 lr 0.00070785 rank 5
2023-02-17 12:22:45,482 DEBUG TRAIN Batch 5/8200 loss 9.870112 loss_att 12.404756 loss_ctc 18.297758 loss_rnnt 8.102922 hw_loss 0.256079 lr 0.00070798 rank 4
2023-02-17 12:24:01,337 DEBUG TRAIN Batch 5/8300 loss 17.454866 loss_att 24.731947 loss_ctc 34.539261 loss_rnnt 13.584787 hw_loss 0.256391 lr 0.00070727 rank 4
2023-02-17 12:24:01,348 DEBUG TRAIN Batch 5/8300 loss 31.805151 loss_att 36.329941 loss_ctc 49.535320 loss_rnnt 28.377752 hw_loss 0.297030 lr 0.00070706 rank 1
2023-02-17 12:24:01,350 DEBUG TRAIN Batch 5/8300 loss 9.155807 loss_att 12.883991 loss_ctc 14.004566 loss_rnnt 7.618786 hw_loss 0.271658 lr 0.00070740 rank 6
2023-02-17 12:24:01,352 DEBUG TRAIN Batch 5/8300 loss 21.832281 loss_att 25.405956 loss_ctc 36.502968 loss_rnnt 19.075129 hw_loss 0.161860 lr 0.00070740 rank 7
2023-02-17 12:24:01,353 DEBUG TRAIN Batch 5/8300 loss 7.044477 loss_att 7.171520 loss_ctc 10.794438 loss_rnnt 6.338153 hw_loss 0.339225 lr 0.00070723 rank 3
2023-02-17 12:24:01,353 DEBUG TRAIN Batch 5/8300 loss 33.067863 loss_att 35.033386 loss_ctc 49.888947 loss_rnnt 30.295074 hw_loss 0.256638 lr 0.00070714 rank 0
2023-02-17 12:24:01,356 DEBUG TRAIN Batch 5/8300 loss 13.094765 loss_att 15.438295 loss_ctc 21.319813 loss_rnnt 11.376952 hw_loss 0.285810 lr 0.00070714 rank 5
2023-02-17 12:24:01,357 DEBUG TRAIN Batch 5/8300 loss 25.833134 loss_att 28.052258 loss_ctc 36.696255 loss_rnnt 23.808805 hw_loss 0.247661 lr 0.00070662 rank 2
2023-02-17 12:24:50,872 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 7
2023-02-17 12:24:50,874 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 0
2023-02-17 12:24:50,877 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 2
2023-02-17 12:24:50,879 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 4
2023-02-17 12:24:50,884 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 5
2023-02-17 12:24:50,884 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 1
2023-02-17 12:24:50,890 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 6
2023-02-17 12:24:50,891 DEBUG CV Batch 5/0 loss 2.837048 loss_att 3.112581 loss_ctc 4.660731 loss_rnnt 2.330503 hw_loss 0.390526 history loss 2.731972 rank 3
2023-02-17 12:25:02,304 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 7
2023-02-17 12:25:02,305 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 0
2023-02-17 12:25:02,749 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 6
2023-02-17 12:25:02,818 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 3
2023-02-17 12:25:02,896 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 2
2023-02-17 12:25:02,896 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 4
2023-02-17 12:25:02,917 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 5
2023-02-17 12:25:03,162 DEBUG CV Batch 5/100 loss 13.731789 loss_att 13.283510 loss_ctc 23.630154 loss_rnnt 12.360090 hw_loss 0.265447 history loss 5.278357 rank 1
2023-02-17 12:25:15,898 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 0
2023-02-17 12:25:15,940 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 7
2023-02-17 12:25:16,686 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 2
2023-02-17 12:25:16,722 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 3
2023-02-17 12:25:16,789 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 4
2023-02-17 12:25:16,822 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 5
2023-02-17 12:25:16,825 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 6
2023-02-17 12:25:17,205 DEBUG CV Batch 5/200 loss 13.277768 loss_att 23.893417 loss_ctc 17.614866 loss_rnnt 10.446459 hw_loss 0.243560 history loss 5.976199 rank 1
2023-02-17 12:25:28,111 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 0
2023-02-17 12:25:28,348 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 7
2023-02-17 12:25:29,044 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 3
2023-02-17 12:25:29,318 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 2
2023-02-17 12:25:29,330 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 6
2023-02-17 12:25:29,460 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 5
2023-02-17 12:25:29,493 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 4
2023-02-17 12:25:29,811 DEBUG CV Batch 5/300 loss 8.355331 loss_att 8.438674 loss_ctc 14.604710 loss_rnnt 7.370399 hw_loss 0.253150 history loss 6.095924 rank 1
2023-02-17 12:25:40,204 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 0
2023-02-17 12:25:40,592 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 7
2023-02-17 12:25:41,447 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 3
2023-02-17 12:25:41,802 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 6
2023-02-17 12:25:41,958 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 2
2023-02-17 12:25:42,103 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 4
2023-02-17 12:25:42,182 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 5
2023-02-17 12:25:42,595 DEBUG CV Batch 5/400 loss 37.135281 loss_att 151.709854 loss_ctc 17.577528 loss_rnnt 16.699083 hw_loss 0.241844 history loss 7.209438 rank 1
2023-02-17 12:25:50,942 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 7
2023-02-17 12:25:51,061 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 0
2023-02-17 12:25:52,146 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 3
2023-02-17 12:25:52,717 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 6
2023-02-17 12:25:52,771 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 4
2023-02-17 12:25:53,044 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 5
2023-02-17 12:25:53,045 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 2
2023-02-17 12:25:53,474 DEBUG CV Batch 5/500 loss 8.098442 loss_att 9.587070 loss_ctc 12.931521 loss_rnnt 7.021758 hw_loss 0.252277 history loss 8.120532 rank 1
2023-02-17 12:26:03,061 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 7
2023-02-17 12:26:03,237 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 0
2023-02-17 12:26:04,741 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 3
2023-02-17 12:26:05,131 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 6
2023-02-17 12:26:05,578 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 4
2023-02-17 12:26:05,689 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 5
2023-02-17 12:26:05,935 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 2
2023-02-17 12:26:06,015 DEBUG CV Batch 5/600 loss 9.596271 loss_att 10.150869 loss_ctc 13.811447 loss_rnnt 8.694029 hw_loss 0.429934 history loss 9.091613 rank 1
2023-02-17 12:26:14,746 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 0
2023-02-17 12:26:14,750 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 7
2023-02-17 12:26:16,869 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 3
2023-02-17 12:26:16,931 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 6
2023-02-17 12:26:17,305 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 5
2023-02-17 12:26:17,368 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 4
2023-02-17 12:26:17,701 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 1
2023-02-17 12:26:17,959 DEBUG CV Batch 5/700 loss 23.283785 loss_att 66.269836 loss_ctc 41.337105 loss_rnnt 12.158530 hw_loss 0.226754 history loss 9.815245 rank 2
2023-02-17 12:26:26,642 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 0
2023-02-17 12:26:26,885 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 7
2023-02-17 12:26:28,397 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 3
2023-02-17 12:26:28,469 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 6
2023-02-17 12:26:28,817 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 5
2023-02-17 12:26:28,870 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 4
2023-02-17 12:26:29,459 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 1
2023-02-17 12:26:29,776 DEBUG CV Batch 5/800 loss 15.493886 loss_att 15.036537 loss_ctc 27.201820 loss_rnnt 13.824799 hw_loss 0.374059 history loss 9.200400 rank 2
2023-02-17 12:26:40,462 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 0
2023-02-17 12:26:40,669 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 7
2023-02-17 12:26:42,290 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 6
2023-02-17 12:26:42,369 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 3
2023-02-17 12:26:42,587 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 5
2023-02-17 12:26:42,781 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 4
2023-02-17 12:26:43,309 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 1
2023-02-17 12:26:43,907 DEBUG CV Batch 5/900 loss 19.486244 loss_att 31.199310 loss_ctc 33.800064 loss_rnnt 15.149027 hw_loss 0.161422 history loss 8.981977 rank 2
2023-02-17 12:26:52,845 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 0
2023-02-17 12:26:52,901 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 7
2023-02-17 12:26:55,176 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 3
2023-02-17 12:26:55,191 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 6
2023-02-17 12:26:55,223 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 4
2023-02-17 12:26:55,229 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 5
2023-02-17 12:26:55,930 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 1
2023-02-17 12:26:56,859 DEBUG CV Batch 5/1000 loss 4.189603 loss_att 5.544401 loss_ctc 6.960273 loss_rnnt 3.377153 hw_loss 0.322629 history loss 8.720257 rank 2
2023-02-17 12:27:05,109 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 0
2023-02-17 12:27:05,287 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 7
2023-02-17 12:27:07,449 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 3
2023-02-17 12:27:07,614 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 6
2023-02-17 12:27:07,631 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 5
2023-02-17 12:27:07,777 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 4
2023-02-17 12:27:08,376 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 1
2023-02-17 12:27:09,516 DEBUG CV Batch 5/1100 loss 6.785234 loss_att 7.001323 loss_ctc 10.541430 loss_rnnt 6.045218 hw_loss 0.367447 history loss 8.712800 rank 2
2023-02-17 12:27:15,853 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 0
2023-02-17 12:27:16,545 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 7
2023-02-17 12:27:18,356 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 3
2023-02-17 12:27:18,539 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 5
2023-02-17 12:27:18,691 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 4
2023-02-17 12:27:18,872 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 6
2023-02-17 12:27:19,042 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 1
2023-02-17 12:27:20,856 DEBUG CV Batch 5/1200 loss 13.210610 loss_att 12.857544 loss_ctc 19.760733 loss_rnnt 12.303513 hw_loss 0.195678 history loss 9.102772 rank 2
2023-02-17 12:27:27,982 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 0
2023-02-17 12:27:28,587 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 7
2023-02-17 12:27:30,658 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 3
2023-02-17 12:27:31,154 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 4
2023-02-17 12:27:31,201 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 5
2023-02-17 12:27:31,400 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 6
2023-02-17 12:27:31,412 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 1
2023-02-17 12:27:33,770 DEBUG CV Batch 5/1300 loss 6.895421 loss_att 7.406426 loss_ctc 11.502644 loss_rnnt 6.001121 hw_loss 0.333380 history loss 9.419941 rank 2
2023-02-17 12:27:39,284 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 0
2023-02-17 12:27:40,195 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 7
2023-02-17 12:27:42,387 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 3
2023-02-17 12:27:42,605 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 4
2023-02-17 12:27:42,792 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 1
2023-02-17 12:27:42,947 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 5
2023-02-17 12:27:43,190 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 6
2023-02-17 12:27:45,855 DEBUG CV Batch 5/1400 loss 17.984991 loss_att 45.930611 loss_ctc 28.185440 loss_rnnt 10.946821 hw_loss 0.166849 history loss 9.782871 rank 2
2023-02-17 12:27:50,860 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 0
2023-02-17 12:27:53,222 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 7
2023-02-17 12:27:54,095 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 3
2023-02-17 12:27:54,693 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 5
2023-02-17 12:27:54,696 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 4
2023-02-17 12:27:54,754 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 1
2023-02-17 12:27:55,198 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 6
2023-02-17 12:27:58,043 DEBUG CV Batch 5/1500 loss 10.990183 loss_att 11.920862 loss_ctc 13.478271 loss_rnnt 10.303416 hw_loss 0.316661 history loss 9.562362 rank 2
2023-02-17 12:28:04,283 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 0
2023-02-17 12:28:07,137 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 7
2023-02-17 12:28:07,797 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 3
2023-02-17 12:28:08,368 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 5
2023-02-17 12:28:08,375 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 4
2023-02-17 12:28:08,478 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 1
2023-02-17 12:28:08,706 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 6
2023-02-17 12:28:11,848 DEBUG CV Batch 5/1600 loss 10.876321 loss_att 23.309752 loss_ctc 19.625904 loss_rnnt 7.112857 hw_loss 0.206560 history loss 9.472481 rank 2
2023-02-17 12:28:17,081 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 0
2023-02-17 12:28:20,182 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 7
2023-02-17 12:28:20,567 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 3
2023-02-17 12:28:21,103 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 5
2023-02-17 12:28:21,103 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 1
2023-02-17 12:28:21,164 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 4
2023-02-17 12:28:21,480 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 6
2023-02-17 12:28:24,669 DEBUG CV Batch 5/1700 loss 9.426417 loss_att 10.606400 loss_ctc 17.218012 loss_rnnt 7.996614 hw_loss 0.290490 history loss 9.350639 rank 2
2023-02-17 12:28:26,629 INFO Epoch 5 CV info cv_loss 9.30783616032003
2023-02-17 12:28:26,629 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class/5.pt
2023-02-17 12:28:27,193 INFO Epoch 6 TRAIN info lr 0.0007067958593961651
2023-02-17 12:28:27,197 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:29,479 INFO Epoch 5 CV info cv_loss 9.30783615937242
2023-02-17 12:28:29,479 INFO Epoch 6 TRAIN info lr 0.0007069866036781422
2023-02-17 12:28:29,485 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:29,779 INFO Epoch 5 CV info cv_loss 9.307836162301394
2023-02-17 12:28:29,780 INFO Epoch 6 TRAIN info lr 0.0007069654022425799
2023-02-17 12:28:29,785 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:30,195 INFO Epoch 5 CV info cv_loss 9.307836159665317
2023-02-17 12:28:30,196 INFO Epoch 6 TRAIN info lr 0.0007068947445639024
2023-02-17 12:28:30,202 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:30,434 INFO Epoch 5 CV info cv_loss 9.307836162008497
2023-02-17 12:28:30,434 INFO Epoch 6 TRAIN info lr 0.0007071350671549643
2023-02-17 12:28:30,437 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:30,645 INFO Epoch 5 CV info cv_loss 9.307836160871366
2023-02-17 12:28:30,646 INFO Epoch 6 TRAIN info lr 0.0007066617244488842
2023-02-17 12:28:30,651 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:30,723 INFO Epoch 5 CV info cv_loss 9.307836161198722
2023-02-17 12:28:30,724 INFO Epoch 6 TRAIN info lr 0.0007070643585976712
2023-02-17 12:28:30,726 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:28:34,037 INFO Epoch 5 CV info cv_loss 9.307836161870663
2023-02-17 12:28:34,037 INFO Epoch 6 TRAIN info lr 0.0007063161496050108
2023-02-17 12:28:34,039 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 12:29:33,766 DEBUG TRAIN Batch 6/0 loss 10.975821 loss_att 11.624693 loss_ctc 15.203653 loss_rnnt 10.116424 hw_loss 0.311083 lr 0.00070706 rank 6
2023-02-17 12:29:33,772 DEBUG TRAIN Batch 6/0 loss 10.818448 loss_att 10.188095 loss_ctc 14.899367 loss_rnnt 10.214643 hw_loss 0.348288 lr 0.00070696 rank 3
2023-02-17 12:29:33,776 DEBUG TRAIN Batch 6/0 loss 11.576279 loss_att 11.396202 loss_ctc 15.723514 loss_rnnt 10.885419 hw_loss 0.326080 lr 0.00070679 rank 0
2023-02-17 12:29:33,793 DEBUG TRAIN Batch 6/0 loss 10.438864 loss_att 10.180012 loss_ctc 14.773236 loss_rnnt 9.720211 hw_loss 0.360951 lr 0.00070713 rank 4
2023-02-17 12:29:33,794 DEBUG TRAIN Batch 6/0 loss 11.905218 loss_att 12.064589 loss_ctc 16.630547 loss_rnnt 11.042918 hw_loss 0.375716 lr 0.00070665 rank 1
2023-02-17 12:29:33,799 DEBUG TRAIN Batch 6/0 loss 14.253356 loss_att 13.327744 loss_ctc 18.343296 loss_rnnt 13.754415 hw_loss 0.260133 lr 0.00070689 rank 5
2023-02-17 12:29:33,810 DEBUG TRAIN Batch 6/0 loss 13.102737 loss_att 13.254421 loss_ctc 18.224398 loss_rnnt 12.225767 hw_loss 0.307023 lr 0.00070631 rank 2
2023-02-17 12:29:33,825 DEBUG TRAIN Batch 6/0 loss 14.577532 loss_att 14.808549 loss_ctc 20.077007 loss_rnnt 13.570607 hw_loss 0.426483 lr 0.00070698 rank 7
2023-02-17 12:30:48,729 DEBUG TRAIN Batch 6/100 loss 13.989882 loss_att 18.680031 loss_ctc 23.877045 loss_rnnt 11.557104 hw_loss 0.330862 lr 0.00070635 rank 6
2023-02-17 12:30:48,730 DEBUG TRAIN Batch 6/100 loss 15.044345 loss_att 19.227745 loss_ctc 26.636106 loss_rnnt 12.504774 hw_loss 0.294980 lr 0.00070618 rank 5
2023-02-17 12:30:48,730 DEBUG TRAIN Batch 6/100 loss 30.117420 loss_att 33.842396 loss_ctc 41.761944 loss_rnnt 27.730444 hw_loss 0.167582 lr 0.00070625 rank 3
2023-02-17 12:30:48,731 DEBUG TRAIN Batch 6/100 loss 17.271414 loss_att 19.651777 loss_ctc 27.294674 loss_rnnt 15.286761 hw_loss 0.322771 lr 0.00070627 rank 7
2023-02-17 12:30:48,733 DEBUG TRAIN Batch 6/100 loss 10.202900 loss_att 18.102821 loss_ctc 26.663731 loss_rnnt 6.297840 hw_loss 0.244308 lr 0.00070561 rank 2
2023-02-17 12:30:48,734 DEBUG TRAIN Batch 6/100 loss 19.714190 loss_att 28.086876 loss_ctc 37.370770 loss_rnnt 15.568174 hw_loss 0.219876 lr 0.00070595 rank 1
2023-02-17 12:30:48,737 DEBUG TRAIN Batch 6/100 loss 8.275111 loss_att 12.946033 loss_ctc 15.956527 loss_rnnt 6.214925 hw_loss 0.190900 lr 0.00070608 rank 0
2023-02-17 12:30:48,743 DEBUG TRAIN Batch 6/100 loss 16.362831 loss_att 18.846109 loss_ctc 32.508324 loss_rnnt 13.518161 hw_loss 0.366153 lr 0.00070642 rank 4
2023-02-17 12:32:04,349 DEBUG TRAIN Batch 6/200 loss 20.998119 loss_att 27.110245 loss_ctc 42.934753 loss_rnnt 16.705042 hw_loss 0.273312 lr 0.00070538 rank 0
2023-02-17 12:32:04,350 DEBUG TRAIN Batch 6/200 loss 11.506804 loss_att 14.011057 loss_ctc 17.450294 loss_rnnt 10.076491 hw_loss 0.256867 lr 0.00070565 rank 6
2023-02-17 12:32:04,352 DEBUG TRAIN Batch 6/200 loss 11.906504 loss_att 16.742357 loss_ctc 25.831089 loss_rnnt 8.904172 hw_loss 0.334781 lr 0.00070525 rank 1
2023-02-17 12:32:04,354 DEBUG TRAIN Batch 6/200 loss 10.298752 loss_att 18.789865 loss_ctc 21.581795 loss_rnnt 6.902989 hw_loss 0.362124 lr 0.00070548 rank 5
2023-02-17 12:32:04,356 DEBUG TRAIN Batch 6/200 loss 18.276503 loss_att 19.085442 loss_ctc 24.590004 loss_rnnt 17.153534 hw_loss 0.223837 lr 0.00070572 rank 4
2023-02-17 12:32:04,356 DEBUG TRAIN Batch 6/200 loss 29.156384 loss_att 33.455231 loss_ctc 45.381828 loss_rnnt 25.981764 hw_loss 0.283984 lr 0.00070490 rank 2
2023-02-17 12:32:04,359 DEBUG TRAIN Batch 6/200 loss 24.512598 loss_att 27.490156 loss_ctc 45.191948 loss_rnnt 21.079628 hw_loss 0.150397 lr 0.00070555 rank 3
2023-02-17 12:32:04,413 DEBUG TRAIN Batch 6/200 loss 22.993103 loss_att 25.054379 loss_ctc 36.233295 loss_rnnt 20.709393 hw_loss 0.198927 lr 0.00070557 rank 7
run_2_14_rnnt_bias.sh: line 166: 15940 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:29899: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:46294: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:25014
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:20882
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:59074
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:10140
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:39313
