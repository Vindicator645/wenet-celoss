/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_20_rnnt_bias_0-3word_finetune_22.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_20_rnnt_bias_loss_2_class_0-3word_22/ddp_init
2023-02-20 20:46:38,091 INFO training on multiple gpus, this gpu 7
2023-02-20 20:46:38,092 INFO training on multiple gpus, this gpu 2
2023-02-20 20:46:38,092 INFO training on multiple gpus, this gpu 3
2023-02-20 20:46:38,093 INFO training on multiple gpus, this gpu 0
2023-02-20 20:46:38,093 INFO training on multiple gpus, this gpu 5
2023-02-20 20:46:38,094 INFO training on multiple gpus, this gpu 6
2023-02-20 20:46:38,107 INFO training on multiple gpus, this gpu 1
2023-02-20 20:46:38,159 INFO training on multiple gpus, this gpu 4
2023-02-20 20:48:13,065 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-20 20:48:13,066 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-20 20:48:13,067 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-20 20:48:13,070 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-20 20:48:13,071 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-20 20:48:14,109 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-20 20:48:14,140 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-20 20:48:17,149 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-20 20:48:17,151 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:17,154 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:17,244 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:17,370 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:17,664 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:17,817 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:19,833 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:20,379 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 20:48:29,772 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,119 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,149 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,181 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,213 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,244 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:30,274 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:31,884 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/10.pt for GPU
2023-02-20 20:48:40,035 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,038 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-20 20:48:40,038 INFO Epoch 11 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,040 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,151 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,153 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,159 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,164 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,192 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,194 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,235 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,237 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,274 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 20:48:40,277 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-20 20:48:40,285 INFO Epoch 11 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:48:40,289 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-20 20:50:01,928 DEBUG TRAIN Batch 11/0 loss 229.466568 loss_att 70.045303 loss_ctc 918.113708 loss_rnnt 167.885864 hw_loss 3.085009 lr 0.00052195 rank 5
2023-02-20 20:50:01,935 DEBUG TRAIN Batch 11/0 loss 257.887817 loss_att 58.008163 loss_ctc 474.123505 loss_rnnt 267.285309 hw_loss 3.275637 lr 0.00052195 rank 7
2023-02-20 20:50:01,936 DEBUG TRAIN Batch 11/0 loss 262.335236 loss_att 61.791447 loss_ctc 240.605377 loss_rnnt 303.415955 hw_loss 3.610053 lr 0.00052195 rank 1
2023-02-20 20:50:01,939 DEBUG TRAIN Batch 11/0 loss 370.572266 loss_att 63.831017 loss_ctc 963.466675 loss_rnnt 351.163818 hw_loss 3.195222 lr 0.00052195 rank 6
2023-02-20 20:50:01,945 DEBUG TRAIN Batch 11/0 loss 169.068558 loss_att 72.251045 loss_ctc 463.085419 loss_rnnt 147.391693 hw_loss 3.446482 lr 0.00052195 rank 3
2023-02-20 20:50:01,978 DEBUG TRAIN Batch 11/0 loss 300.709290 loss_att 67.674530 loss_ctc 824.685303 loss_rnnt 275.373108 hw_loss 3.899397 lr 0.00052195 rank 0
2023-02-20 20:50:02,015 DEBUG TRAIN Batch 11/0 loss 259.573761 loss_att 72.832428 loss_ctc 1002.460144 loss_rnnt 196.143845 hw_loss 3.237523 lr 0.00052195 rank 4
2023-02-20 20:50:02,016 DEBUG TRAIN Batch 11/0 loss 281.059723 loss_att 86.691437 loss_ctc 1159.141235 loss_rnnt 201.192902 hw_loss 3.117974 lr 0.00052195 rank 2
2023-02-20 20:51:00,663 DEBUG TRAIN Batch 11/100 loss 355.066925 loss_att 323.866455 loss_ctc 458.105652 loss_rnnt 347.517761 hw_loss 0.095174 lr 0.00052181 rank 0
2023-02-20 20:51:00,674 DEBUG TRAIN Batch 11/100 loss 306.369781 loss_att 277.478363 loss_ctc 392.083069 loss_rnnt 300.501465 hw_loss 0.409007 lr 0.00052181 rank 3
2023-02-20 20:51:00,675 DEBUG TRAIN Batch 11/100 loss 347.517639 loss_att 318.466217 loss_ctc 446.478851 loss_rnnt 340.001648 hw_loss 0.246543 lr 0.00052181 rank 6
2023-02-20 20:51:00,676 DEBUG TRAIN Batch 11/100 loss 272.141296 loss_att 247.690582 loss_ctc 398.067780 loss_rnnt 260.144745 hw_loss 0.180959 lr 0.00052181 rank 2
2023-02-20 20:51:00,682 DEBUG TRAIN Batch 11/100 loss 289.584442 loss_att 261.584473 loss_ctc 387.675171 loss_rnnt 281.923462 hw_loss 0.341646 lr 0.00052181 rank 1
2023-02-20 20:51:00,682 DEBUG TRAIN Batch 11/100 loss 271.731995 loss_att 245.639420 loss_ctc 358.094299 loss_rnnt 265.260773 hw_loss 0.327651 lr 0.00052181 rank 4
2023-02-20 20:51:00,692 DEBUG TRAIN Batch 11/100 loss 327.686584 loss_att 286.596649 loss_ctc 437.398407 loss_rnnt 321.149750 hw_loss 0.237354 lr 0.00052181 rank 7
2023-02-20 20:51:00,738 DEBUG TRAIN Batch 11/100 loss 288.990997 loss_att 256.348297 loss_ctc 390.579956 loss_rnnt 281.713593 hw_loss 0.488876 lr 0.00052181 rank 5
2023-02-20 20:52:00,512 DEBUG TRAIN Batch 11/200 loss 249.477066 loss_att 310.862762 loss_ctc 230.190643 loss_rnnt 239.586670 hw_loss 0.346497 lr 0.00052167 rank 0
2023-02-20 20:52:00,517 DEBUG TRAIN Batch 11/200 loss 222.299118 loss_att 255.642517 loss_ctc 215.828064 loss_rnnt 216.257050 hw_loss 0.442896 lr 0.00052167 rank 3
2023-02-20 20:52:00,520 DEBUG TRAIN Batch 11/200 loss 196.777908 loss_att 260.663513 loss_ctc 166.134445 loss_rnnt 188.048859 hw_loss 0.070682 lr 0.00052167 rank 5
2023-02-20 20:52:00,522 DEBUG TRAIN Batch 11/200 loss 227.975250 loss_att 243.692108 loss_ctc 242.857819 loss_rnnt 222.705734 hw_loss 0.265892 lr 0.00052167 rank 1
2023-02-20 20:52:00,525 DEBUG TRAIN Batch 11/200 loss 227.885986 loss_att 274.715576 loss_ctc 200.858673 loss_rnnt 222.027924 hw_loss 0.179616 lr 0.00052167 rank 7
2023-02-20 20:52:00,527 DEBUG TRAIN Batch 11/200 loss 257.065155 loss_att 303.837891 loss_ctc 258.151520 loss_rnnt 247.325073 hw_loss 0.451259 lr 0.00052167 rank 2
2023-02-20 20:52:00,534 DEBUG TRAIN Batch 11/200 loss 271.610870 loss_att 312.654663 loss_ctc 271.993744 loss_rnnt 263.207977 hw_loss 0.268293 lr 0.00052167 rank 6
2023-02-20 20:52:00,577 DEBUG TRAIN Batch 11/200 loss 305.255737 loss_att 334.000244 loss_ctc 278.823242 loss_rnnt 302.993469 hw_loss 0.070681 lr 0.00052167 rank 4
2023-02-20 20:52:57,936 DEBUG TRAIN Batch 11/300 loss 95.029617 loss_att 132.548141 loss_ctc 82.925171 loss_rnnt 88.945366 hw_loss 0.364655 lr 0.00052153 rank 0
2023-02-20 20:52:57,939 DEBUG TRAIN Batch 11/300 loss 121.293289 loss_att 172.701126 loss_ctc 103.301323 loss_rnnt 113.267639 hw_loss 0.268135 lr 0.00052153 rank 3
2023-02-20 20:52:57,939 DEBUG TRAIN Batch 11/300 loss 110.712715 loss_att 157.674973 loss_ctc 108.926697 loss_rnnt 101.324219 hw_loss 0.439082 lr 0.00052153 rank 6
2023-02-20 20:52:57,946 DEBUG TRAIN Batch 11/300 loss 119.359581 loss_att 173.181717 loss_ctc 108.421600 loss_rnnt 109.870804 hw_loss 0.342640 lr 0.00052153 rank 4
2023-02-20 20:52:57,947 DEBUG TRAIN Batch 11/300 loss 79.114525 loss_att 103.255447 loss_ctc 77.768326 loss_rnnt 74.278061 hw_loss 0.352083 lr 0.00052153 rank 2
2023-02-20 20:52:57,947 DEBUG TRAIN Batch 11/300 loss 91.853622 loss_att 113.348595 loss_ctc 89.706459 loss_rnnt 87.647606 hw_loss 0.362473 lr 0.00052153 rank 5
2023-02-20 20:52:57,961 DEBUG TRAIN Batch 11/300 loss 86.839302 loss_att 118.258469 loss_ctc 77.724831 loss_rnnt 81.609154 hw_loss 0.302967 lr 0.00052153 rank 7
2023-02-20 20:52:58,019 DEBUG TRAIN Batch 11/300 loss 121.088921 loss_att 166.905365 loss_ctc 116.950500 loss_rnnt 112.285019 hw_loss 0.360764 lr 0.00052153 rank 1
2023-02-20 20:53:56,412 DEBUG TRAIN Batch 11/400 loss 147.561707 loss_att 238.209427 loss_ctc 129.253128 loss_rnnt 131.627625 hw_loss 0.460654 lr 0.00052139 rank 5
2023-02-20 20:53:56,412 DEBUG TRAIN Batch 11/400 loss 118.469017 loss_att 213.321671 loss_ctc 112.129601 loss_rnnt 100.135727 hw_loss 0.390039 lr 0.00052139 rank 0
2023-02-20 20:53:56,413 DEBUG TRAIN Batch 11/400 loss 185.007690 loss_att 285.723022 loss_ctc 170.072754 loss_rnnt 166.745956 hw_loss 0.206205 lr 0.00052139 rank 4
2023-02-20 20:53:56,415 DEBUG TRAIN Batch 11/400 loss 182.615509 loss_att 272.946594 loss_ctc 172.122101 loss_rnnt 165.774246 hw_loss 0.326576 lr 0.00052139 rank 6
2023-02-20 20:53:56,416 DEBUG TRAIN Batch 11/400 loss 202.883789 loss_att 297.316956 loss_ctc 203.048523 loss_rnnt 183.809967 hw_loss 0.309741 lr 0.00052139 rank 2
2023-02-20 20:53:56,420 DEBUG TRAIN Batch 11/400 loss 137.893982 loss_att 252.681305 loss_ctc 119.232712 loss_rnnt 117.321350 hw_loss 0.193750 lr 0.00052139 rank 3
2023-02-20 20:53:56,420 DEBUG TRAIN Batch 11/400 loss 115.473602 loss_att 234.317566 loss_ctc 97.270233 loss_rnnt 93.952721 hw_loss 0.336023 lr 0.00052139 rank 1
2023-02-20 20:53:56,425 DEBUG TRAIN Batch 11/400 loss 100.741791 loss_att 205.204529 loss_ctc 79.067924 loss_rnnt 82.554375 hw_loss 0.346347 lr 0.00052139 rank 7
2023-02-20 20:54:54,693 DEBUG TRAIN Batch 11/500 loss 157.808777 loss_att 313.512146 loss_ctc 118.471176 loss_rnnt 131.799973 hw_loss 0.212182 lr 0.00052124 rank 0
2023-02-20 20:54:54,697 DEBUG TRAIN Batch 11/500 loss 181.251663 loss_att 305.208862 loss_ctc 162.733383 loss_rnnt 158.889740 hw_loss 0.074219 lr 0.00052124 rank 2
2023-02-20 20:54:54,697 DEBUG TRAIN Batch 11/500 loss 182.021500 loss_att 321.111328 loss_ctc 151.198944 loss_rnnt 158.157272 hw_loss 0.292393 lr 0.00052124 rank 6
2023-02-20 20:54:54,699 DEBUG TRAIN Batch 11/500 loss 100.910622 loss_att 293.662384 loss_ctc 63.106377 loss_rnnt 67.206390 hw_loss 0.364588 lr 0.00052124 rank 1
2023-02-20 20:54:54,700 DEBUG TRAIN Batch 11/500 loss 140.760101 loss_att 287.713318 loss_ctc 115.686600 loss_rnnt 114.593758 hw_loss 0.222819 lr 0.00052124 rank 3
2023-02-20 20:54:54,703 DEBUG TRAIN Batch 11/500 loss 118.322151 loss_att 239.399689 loss_ctc 108.409134 loss_rnnt 95.266808 hw_loss 0.302945 lr 0.00052124 rank 5
2023-02-20 20:54:54,704 DEBUG TRAIN Batch 11/500 loss 151.050064 loss_att 306.584137 loss_ctc 119.937546 loss_rnnt 123.912506 hw_loss 0.335771 lr 0.00052124 rank 4
2023-02-20 20:54:54,777 DEBUG TRAIN Batch 11/500 loss 183.000626 loss_att 317.857452 loss_ctc 180.779816 loss_rnnt 156.175049 hw_loss 0.281822 lr 0.00052124 rank 7
2023-02-20 20:55:51,195 DEBUG TRAIN Batch 11/600 loss 109.108803 loss_att 186.881226 loss_ctc 86.472427 loss_rnnt 96.415688 hw_loss 0.294021 lr 0.00052110 rank 0
2023-02-20 20:55:51,198 DEBUG TRAIN Batch 11/600 loss 68.954788 loss_att 146.002258 loss_ctc 55.360283 loss_rnnt 55.186737 hw_loss 0.320912 lr 0.00052110 rank 2
2023-02-20 20:55:51,199 DEBUG TRAIN Batch 11/600 loss 48.036831 loss_att 118.805222 loss_ctc 34.507484 loss_rnnt 35.584190 hw_loss 0.192893 lr 0.00052110 rank 5
2023-02-20 20:55:51,202 DEBUG TRAIN Batch 11/600 loss 82.869812 loss_att 166.886688 loss_ctc 64.694786 loss_rnnt 68.360092 hw_loss 0.243140 lr 0.00052110 rank 4
2023-02-20 20:55:51,203 DEBUG TRAIN Batch 11/600 loss 111.243149 loss_att 189.486206 loss_ctc 99.308441 loss_rnnt 96.944305 hw_loss 0.452841 lr 0.00052110 rank 6
2023-02-20 20:55:51,202 DEBUG TRAIN Batch 11/600 loss 94.478912 loss_att 173.585938 loss_ctc 78.682549 loss_rnnt 80.525658 hw_loss 0.446320 lr 0.00052110 rank 3
2023-02-20 20:55:51,204 DEBUG TRAIN Batch 11/600 loss 117.410172 loss_att 226.548676 loss_ctc 102.122734 loss_rnnt 97.473618 hw_loss 0.275950 lr 0.00052110 rank 1
2023-02-20 20:55:51,210 DEBUG TRAIN Batch 11/600 loss 123.578087 loss_att 182.603363 loss_ctc 109.983757 loss_rnnt 113.290314 hw_loss 0.553690 lr 0.00052110 rank 7
2023-02-20 20:56:49,509 DEBUG TRAIN Batch 11/700 loss 102.988205 loss_att 225.060303 loss_ctc 88.237709 loss_rnnt 80.463638 hw_loss 0.144144 lr 0.00052096 rank 0
2023-02-20 20:56:49,510 DEBUG TRAIN Batch 11/700 loss 115.610077 loss_att 245.346771 loss_ctc 101.712921 loss_rnnt 91.454269 hw_loss 0.115171 lr 0.00052096 rank 5
2023-02-20 20:56:49,510 DEBUG TRAIN Batch 11/700 loss 94.905037 loss_att 241.352966 loss_ctc 66.839424 loss_rnnt 69.198738 hw_loss 0.297729 lr 0.00052096 rank 3
2023-02-20 20:56:49,513 DEBUG TRAIN Batch 11/700 loss 111.419563 loss_att 262.547760 loss_ctc 84.314835 loss_rnnt 84.759010 hw_loss 0.091643 lr 0.00052096 rank 6
2023-02-20 20:56:49,515 DEBUG TRAIN Batch 11/700 loss 121.559402 loss_att 295.970978 loss_ctc 91.123367 loss_rnnt 90.536812 hw_loss 0.372015 lr 0.00052096 rank 2
2023-02-20 20:56:49,516 DEBUG TRAIN Batch 11/700 loss 204.220642 loss_att 344.599548 loss_ctc 193.114517 loss_rnnt 177.451584 hw_loss 0.326470 lr 0.00052096 rank 7
2023-02-20 20:56:49,518 DEBUG TRAIN Batch 11/700 loss 172.168503 loss_att 311.312012 loss_ctc 156.320892 loss_rnnt 146.278687 hw_loss 0.326496 lr 0.00052096 rank 1
2023-02-20 20:56:49,524 DEBUG TRAIN Batch 11/700 loss 104.855194 loss_att 248.158417 loss_ctc 89.551064 loss_rnnt 78.161072 hw_loss 0.138790 lr 0.00052096 rank 4
2023-02-20 20:57:48,295 DEBUG TRAIN Batch 11/800 loss 134.004913 loss_att 314.434235 loss_ctc 136.332901 loss_rnnt 97.501221 hw_loss 0.201408 lr 0.00052082 rank 0
2023-02-20 20:57:48,303 DEBUG TRAIN Batch 11/800 loss 75.807037 loss_att 234.905426 loss_ctc 47.645107 loss_rnnt 47.567173 hw_loss 0.328336 lr 0.00052082 rank 3
2023-02-20 20:57:48,306 DEBUG TRAIN Batch 11/800 loss 72.323677 loss_att 203.761230 loss_ctc 56.628704 loss_rnnt 48.020905 hw_loss 0.202349 lr 0.00052082 rank 6
2023-02-20 20:57:48,306 DEBUG TRAIN Batch 11/800 loss 86.372894 loss_att 183.187149 loss_ctc 86.441124 loss_rnnt 66.874985 hw_loss 0.236160 lr 0.00052082 rank 2
2023-02-20 20:57:48,309 DEBUG TRAIN Batch 11/800 loss 122.353218 loss_att 276.338745 loss_ctc 112.026703 loss_rnnt 92.750946 hw_loss 0.341322 lr 0.00052082 rank 7
2023-02-20 20:57:48,311 DEBUG TRAIN Batch 11/800 loss 186.052948 loss_att 377.101959 loss_ctc 176.192902 loss_rnnt 148.942780 hw_loss 0.403214 lr 0.00052082 rank 1
2023-02-20 20:57:48,342 DEBUG TRAIN Batch 11/800 loss 89.979630 loss_att 214.415344 loss_ctc 75.964043 loss_rnnt 66.917442 hw_loss 0.082098 lr 0.00052082 rank 4
2023-02-20 20:57:48,388 DEBUG TRAIN Batch 11/800 loss 142.632385 loss_att 295.015564 loss_ctc 127.532265 loss_rnnt 113.966873 hw_loss 0.379195 lr 0.00052082 rank 5
2023-02-20 20:59:07,590 DEBUG TRAIN Batch 11/900 loss 71.850769 loss_att 193.405182 loss_ctc 56.607285 loss_rnnt 49.448631 hw_loss 0.231969 lr 0.00052068 rank 0
2023-02-20 20:59:07,597 DEBUG TRAIN Batch 11/900 loss 94.943611 loss_att 182.730042 loss_ctc 93.506378 loss_rnnt 77.407013 hw_loss 0.320511 lr 0.00052068 rank 5
2023-02-20 20:59:07,598 DEBUG TRAIN Batch 11/900 loss 75.537521 loss_att 162.815659 loss_ctc 59.000977 loss_rnnt 60.237198 hw_loss 0.092945 lr 0.00052068 rank 2
2023-02-20 20:59:07,600 DEBUG TRAIN Batch 11/900 loss 74.186241 loss_att 171.559464 loss_ctc 56.313900 loss_rnnt 56.892742 hw_loss 0.378430 lr 0.00052068 rank 6
2023-02-20 20:59:07,605 DEBUG TRAIN Batch 11/900 loss 94.468712 loss_att 201.198288 loss_ctc 60.568897 loss_rnnt 77.486984 hw_loss 0.292101 lr 0.00052068 rank 3
2023-02-20 20:59:07,610 DEBUG TRAIN Batch 11/900 loss 112.493057 loss_att 254.280014 loss_ctc 91.558563 loss_rnnt 86.839180 hw_loss 0.164525 lr 0.00052068 rank 7
2023-02-20 20:59:07,617 DEBUG TRAIN Batch 11/900 loss 104.424553 loss_att 201.164612 loss_ctc 86.284302 loss_rnnt 87.298767 hw_loss 0.368391 lr 0.00052068 rank 4
2023-02-20 20:59:07,622 DEBUG TRAIN Batch 11/900 loss 118.580109 loss_att 256.764618 loss_ctc 102.771149 loss_rnnt 92.885551 hw_loss 0.310351 lr 0.00052068 rank 1
2023-02-20 21:00:07,240 DEBUG TRAIN Batch 11/1000 loss 143.360977 loss_att 249.388031 loss_ctc 151.102814 loss_rnnt 120.849434 hw_loss 0.513536 lr 0.00052054 rank 0
2023-02-20 21:00:07,242 DEBUG TRAIN Batch 11/1000 loss 128.568085 loss_att 270.046021 loss_ctc 123.684891 loss_rnnt 100.754059 hw_loss 0.317842 lr 0.00052054 rank 5
2023-02-20 21:00:07,243 DEBUG TRAIN Batch 11/1000 loss 103.526009 loss_att 214.619598 loss_ctc 95.477142 loss_rnnt 82.269218 hw_loss 0.208616 lr 0.00052054 rank 2
2023-02-20 21:00:07,246 DEBUG TRAIN Batch 11/1000 loss 117.388092 loss_att 233.839279 loss_ctc 119.278152 loss_rnnt 93.622070 hw_loss 0.419581 lr 0.00052054 rank 6
2023-02-20 21:00:07,248 DEBUG TRAIN Batch 11/1000 loss 111.371696 loss_att 243.262573 loss_ctc 100.453568 loss_rnnt 86.424110 hw_loss 0.047167 lr 0.00052054 rank 3
2023-02-20 21:00:07,252 DEBUG TRAIN Batch 11/1000 loss 84.067062 loss_att 205.815720 loss_ctc 60.379749 loss_rnnt 62.737587 hw_loss 0.258842 lr 0.00052054 rank 4
2023-02-20 21:00:07,252 DEBUG TRAIN Batch 11/1000 loss 175.459717 loss_att 289.855713 loss_ctc 178.398300 loss_rnnt 152.041351 hw_loss 0.276243 lr 0.00052054 rank 1
2023-02-20 21:00:07,255 DEBUG TRAIN Batch 11/1000 loss 107.167892 loss_att 245.725296 loss_ctc 99.440163 loss_rnnt 80.151947 hw_loss 0.627808 lr 0.00052054 rank 7
2023-02-20 21:01:04,532 DEBUG TRAIN Batch 11/1100 loss 71.162918 loss_att 207.956818 loss_ctc 58.768494 loss_rnnt 45.364143 hw_loss 0.173583 lr 0.00052040 rank 3
2023-02-20 21:01:04,533 DEBUG TRAIN Batch 11/1100 loss 158.261063 loss_att 325.917969 loss_ctc 140.718155 loss_rnnt 126.944778 hw_loss 0.232384 lr 0.00052040 rank 0
2023-02-20 21:01:04,533 DEBUG TRAIN Batch 11/1100 loss 99.465149 loss_att 249.435303 loss_ctc 82.651672 loss_rnnt 71.499763 hw_loss 0.399650 lr 0.00052040 rank 2
2023-02-20 21:01:04,539 DEBUG TRAIN Batch 11/1100 loss 93.495369 loss_att 269.020081 loss_ctc 69.374008 loss_rnnt 61.522465 hw_loss 0.157766 lr 0.00052040 rank 5
2023-02-20 21:01:04,541 DEBUG TRAIN Batch 11/1100 loss 71.743607 loss_att 244.178009 loss_ctc 57.970066 loss_rnnt 39.052895 hw_loss 0.075560 lr 0.00052040 rank 7
2023-02-20 21:01:04,546 DEBUG TRAIN Batch 11/1100 loss 108.838791 loss_att 247.333954 loss_ctc 91.338783 loss_rnnt 83.432777 hw_loss 0.075560 lr 0.00052040 rank 6
2023-02-20 21:01:04,552 DEBUG TRAIN Batch 11/1100 loss 135.290070 loss_att 263.221252 loss_ctc 136.153107 loss_rnnt 109.461700 hw_loss 0.238220 lr 0.00052040 rank 1
2023-02-20 21:01:04,599 DEBUG TRAIN Batch 11/1100 loss 79.357971 loss_att 272.393860 loss_ctc 54.552029 loss_rnnt 43.871681 hw_loss 0.349803 lr 0.00052040 rank 4
2023-02-20 21:02:02,279 DEBUG TRAIN Batch 11/1200 loss 99.797783 loss_att 173.226791 loss_ctc 106.400452 loss_rnnt 84.140503 hw_loss 0.170834 lr 0.00052026 rank 0
2023-02-20 21:02:02,286 DEBUG TRAIN Batch 11/1200 loss 64.973976 loss_att 182.447037 loss_ctc 52.249588 loss_rnnt 43.064751 hw_loss 0.208492 lr 0.00052026 rank 6
2023-02-20 21:02:02,285 DEBUG TRAIN Batch 11/1200 loss 73.520920 loss_att 164.017670 loss_ctc 58.478909 loss_rnnt 57.265789 hw_loss 0.302589 lr 0.00052026 rank 2
2023-02-20 21:02:02,286 DEBUG TRAIN Batch 11/1200 loss 87.083809 loss_att 203.995117 loss_ctc 78.146240 loss_rnnt 64.666656 hw_loss 0.424802 lr 0.00052026 rank 4
2023-02-20 21:02:02,290 DEBUG TRAIN Batch 11/1200 loss 97.162025 loss_att 193.392654 loss_ctc 90.286003 loss_rnnt 78.771225 hw_loss 0.115270 lr 0.00052026 rank 7
2023-02-20 21:02:02,295 DEBUG TRAIN Batch 11/1200 loss 53.721622 loss_att 117.972839 loss_ctc 40.226433 loss_rnnt 42.435585 hw_loss 0.440913 lr 0.00052026 rank 1
2023-02-20 21:02:02,298 DEBUG TRAIN Batch 11/1200 loss 56.750008 loss_att 151.484131 loss_ctc 38.127090 loss_rnnt 40.061241 hw_loss 0.421868 lr 0.00052026 rank 3
2023-02-20 21:02:02,345 DEBUG TRAIN Batch 11/1200 loss 114.312912 loss_att 200.442017 loss_ctc 105.803116 loss_rnnt 97.994301 hw_loss 0.426399 lr 0.00052026 rank 5
2023-02-20 21:03:01,793 DEBUG TRAIN Batch 11/1300 loss 51.893673 loss_att 161.400238 loss_ctc 35.416260 loss_rnnt 32.067600 hw_loss 0.228271 lr 0.00052011 rank 0
2023-02-20 21:03:01,800 DEBUG TRAIN Batch 11/1300 loss 90.347519 loss_att 221.283905 loss_ctc 64.023285 loss_rnnt 67.630295 hw_loss 0.074694 lr 0.00052011 rank 2
2023-02-20 21:03:01,805 DEBUG TRAIN Batch 11/1300 loss 74.017189 loss_att 159.625595 loss_ctc 57.852261 loss_rnnt 58.951637 hw_loss 0.185989 lr 0.00052011 rank 5
2023-02-20 21:03:01,809 DEBUG TRAIN Batch 11/1300 loss 47.000710 loss_att 161.284180 loss_ctc 34.615433 loss_rnnt 25.755547 hw_loss 0.074694 lr 0.00052011 rank 1
2023-02-20 21:03:01,809 DEBUG TRAIN Batch 11/1300 loss 94.283844 loss_att 207.875824 loss_ctc 75.226089 loss_rnnt 73.935410 hw_loss 0.320759 lr 0.00052011 rank 7
2023-02-20 21:03:01,814 DEBUG TRAIN Batch 11/1300 loss 110.103157 loss_att 201.552155 loss_ctc 76.053589 loss_rnnt 96.040558 hw_loss 0.586369 lr 0.00052011 rank 3
2023-02-20 21:03:01,815 DEBUG TRAIN Batch 11/1300 loss 57.767841 loss_att 206.498993 loss_ctc 47.918636 loss_rnnt 29.195719 hw_loss 0.260839 lr 0.00052011 rank 6
2023-02-20 21:03:01,818 DEBUG TRAIN Batch 11/1300 loss 52.010216 loss_att 142.547653 loss_ctc 36.043850 loss_rnnt 35.991737 hw_loss 0.074694 lr 0.00052011 rank 4
2023-02-20 21:03:57,780 DEBUG TRAIN Batch 11/1400 loss 47.930779 loss_att 143.770782 loss_ctc 31.268631 loss_rnnt 30.885183 hw_loss 0.186027 lr 0.00051997 rank 0
2023-02-20 21:03:57,787 DEBUG TRAIN Batch 11/1400 loss 126.923325 loss_att 270.562592 loss_ctc 92.960129 loss_rnnt 102.616890 hw_loss 0.200636 lr 0.00051997 rank 6
2023-02-20 21:03:57,788 DEBUG TRAIN Batch 11/1400 loss 93.949234 loss_att 258.135742 loss_ctc 52.445145 loss_rnnt 66.561172 hw_loss 0.158701 lr 0.00051997 rank 2
2023-02-20 21:03:57,792 DEBUG TRAIN Batch 11/1400 loss 64.350540 loss_att 189.434525 loss_ctc 42.255798 loss_rnnt 42.096252 hw_loss 0.343969 lr 0.00051997 rank 4
2023-02-20 21:03:57,794 DEBUG TRAIN Batch 11/1400 loss 52.582848 loss_att 149.168015 loss_ctc 34.890148 loss_rnnt 35.460953 hw_loss 0.307293 lr 0.00051997 rank 7
2023-02-20 21:03:57,794 DEBUG TRAIN Batch 11/1400 loss 59.722332 loss_att 185.586563 loss_ctc 44.405743 loss_rnnt 36.476002 hw_loss 0.216923 lr 0.00051997 rank 3
2023-02-20 21:03:57,798 DEBUG TRAIN Batch 11/1400 loss 110.693428 loss_att 291.687439 loss_ctc 91.248535 loss_rnnt 77.047356 hw_loss 0.074869 lr 0.00051997 rank 1
2023-02-20 21:03:57,852 DEBUG TRAIN Batch 11/1400 loss 91.941765 loss_att 229.879730 loss_ctc 67.554680 loss_rnnt 67.455887 hw_loss 0.281046 lr 0.00051997 rank 5
2023-02-20 21:04:55,923 DEBUG TRAIN Batch 11/1500 loss 62.752037 loss_att 139.986664 loss_ctc 48.087379 loss_rnnt 49.020287 hw_loss 0.450207 lr 0.00051983 rank 0
2023-02-20 21:04:55,932 DEBUG TRAIN Batch 11/1500 loss 46.980202 loss_att 113.688766 loss_ctc 33.139412 loss_rnnt 35.316505 hw_loss 0.313913 lr 0.00051983 rank 3
2023-02-20 21:04:55,933 DEBUG TRAIN Batch 11/1500 loss 78.444969 loss_att 160.433441 loss_ctc 74.026077 loss_rnnt 62.463562 hw_loss 0.324197 lr 0.00051983 rank 2
2023-02-20 21:04:55,935 DEBUG TRAIN Batch 11/1500 loss 53.881634 loss_att 147.300049 loss_ctc 39.837852 loss_rnnt 36.935345 hw_loss 0.253324 lr 0.00051983 rank 1
2023-02-20 21:04:55,935 DEBUG TRAIN Batch 11/1500 loss 91.632683 loss_att 185.517059 loss_ctc 90.463974 loss_rnnt 72.914368 hw_loss 0.182352 lr 0.00051983 rank 6
2023-02-20 21:04:55,936 DEBUG TRAIN Batch 11/1500 loss 69.600845 loss_att 190.729004 loss_ctc 60.544872 loss_rnnt 46.463837 hw_loss 0.222817 lr 0.00051983 rank 7
2023-02-20 21:04:55,939 DEBUG TRAIN Batch 11/1500 loss 54.805489 loss_att 126.396080 loss_ctc 55.091682 loss_rnnt 40.270317 hw_loss 0.335428 lr 0.00051983 rank 4
2023-02-20 21:04:55,993 DEBUG TRAIN Batch 11/1500 loss 95.229858 loss_att 169.343384 loss_ctc 80.754837 loss_rnnt 82.131409 hw_loss 0.385782 lr 0.00051983 rank 5
2023-02-20 21:05:56,163 DEBUG TRAIN Batch 11/1600 loss 50.019276 loss_att 135.544510 loss_ctc 38.953136 loss_rnnt 34.180855 hw_loss 0.391603 lr 0.00051969 rank 0
2023-02-20 21:05:56,164 DEBUG TRAIN Batch 11/1600 loss 70.550430 loss_att 195.263458 loss_ctc 55.301529 loss_rnnt 47.583424 hw_loss 0.107984 lr 0.00051969 rank 5
2023-02-20 21:05:56,168 DEBUG TRAIN Batch 11/1600 loss 67.656052 loss_att 167.244202 loss_ctc 61.864002 loss_rnnt 48.473446 hw_loss 0.069850 lr 0.00051969 rank 6
2023-02-20 21:05:56,171 DEBUG TRAIN Batch 11/1600 loss 112.157990 loss_att 211.149124 loss_ctc 122.193596 loss_rnnt 90.742828 hw_loss 0.522848 lr 0.00051969 rank 2
2023-02-20 21:05:56,171 DEBUG TRAIN Batch 11/1600 loss 116.938950 loss_att 244.052231 loss_ctc 104.597374 loss_rnnt 93.124573 hw_loss 0.069850 lr 0.00051969 rank 4
2023-02-20 21:05:56,172 DEBUG TRAIN Batch 11/1600 loss 60.117977 loss_att 146.485977 loss_ctc 50.612041 loss_rnnt 44.021675 hw_loss 0.169044 lr 0.00051969 rank 3
2023-02-20 21:05:56,173 DEBUG TRAIN Batch 11/1600 loss 84.878807 loss_att 213.342224 loss_ctc 71.959373 loss_rnnt 60.693096 hw_loss 0.404283 lr 0.00051969 rank 7
2023-02-20 21:05:56,241 DEBUG TRAIN Batch 11/1600 loss 65.396164 loss_att 147.050781 loss_ctc 51.617538 loss_rnnt 50.668941 hw_loss 0.437713 lr 0.00051969 rank 1
2023-02-20 21:06:55,186 DEBUG TRAIN Batch 11/1700 loss 64.654655 loss_att 156.771240 loss_ctc 56.274666 loss_rnnt 47.149254 hw_loss 0.373899 lr 0.00051955 rank 0
2023-02-20 21:06:55,193 DEBUG TRAIN Batch 11/1700 loss 116.418892 loss_att 237.956970 loss_ctc 104.850471 loss_rnnt 93.579468 hw_loss 0.139262 lr 0.00051955 rank 6
2023-02-20 21:06:55,195 DEBUG TRAIN Batch 11/1700 loss 79.124573 loss_att 169.427948 loss_ctc 77.783401 loss_rnnt 61.206081 hw_loss 0.068712 lr 0.00051955 rank 3
2023-02-20 21:06:55,195 DEBUG TRAIN Batch 11/1700 loss 41.917500 loss_att 90.442657 loss_ctc 54.922768 loss_rnnt 30.290648 hw_loss 0.352095 lr 0.00051955 rank 2
2023-02-20 21:06:55,201 DEBUG TRAIN Batch 11/1700 loss 89.126152 loss_att 165.341599 loss_ctc 79.324806 loss_rnnt 75.055977 hw_loss 0.251130 lr 0.00051955 rank 7
2023-02-20 21:06:55,201 DEBUG TRAIN Batch 11/1700 loss 53.706944 loss_att 128.916229 loss_ctc 48.277485 loss_rnnt 39.258320 hw_loss 0.245050 lr 0.00051955 rank 5
2023-02-20 21:06:55,230 DEBUG TRAIN Batch 11/1700 loss 133.543777 loss_att 241.824997 loss_ctc 142.804016 loss_rnnt 110.292557 hw_loss 0.675514 lr 0.00051955 rank 1
2023-02-20 21:06:55,240 DEBUG TRAIN Batch 11/1700 loss 68.788956 loss_att 166.861786 loss_ctc 61.841721 loss_rnnt 50.064049 hw_loss 0.068712 lr 0.00051955 rank 4
2023-02-20 21:08:13,287 DEBUG TRAIN Batch 11/1800 loss 34.730362 loss_att 84.578194 loss_ctc 26.847622 loss_rnnt 25.679649 hw_loss 0.247831 lr 0.00051941 rank 0
2023-02-20 21:08:13,289 DEBUG TRAIN Batch 11/1800 loss 55.998020 loss_att 116.622940 loss_ctc 59.274353 loss_rnnt 43.295074 hw_loss 0.264591 lr 0.00051941 rank 5
2023-02-20 21:08:13,292 DEBUG TRAIN Batch 11/1800 loss 80.386276 loss_att 183.604111 loss_ctc 69.462982 loss_rnnt 61.016953 hw_loss 0.341621 lr 0.00051941 rank 3
2023-02-20 21:08:13,292 DEBUG TRAIN Batch 11/1800 loss 40.693718 loss_att 98.157547 loss_ctc 37.352470 loss_rnnt 29.533356 hw_loss 0.212055 lr 0.00051941 rank 2
2023-02-20 21:08:13,294 DEBUG TRAIN Batch 11/1800 loss 101.872910 loss_att 202.416000 loss_ctc 95.738358 loss_rnnt 82.440674 hw_loss 0.265402 lr 0.00051941 rank 6
2023-02-20 21:08:13,297 DEBUG TRAIN Batch 11/1800 loss 63.029659 loss_att 151.863770 loss_ctc 51.556870 loss_rnnt 46.635654 hw_loss 0.294157 lr 0.00051941 rank 4
2023-02-20 21:08:13,298 DEBUG TRAIN Batch 11/1800 loss 44.790497 loss_att 108.677811 loss_ctc 34.797272 loss_rnnt 33.192062 hw_loss 0.287627 lr 0.00051941 rank 1
2023-02-20 21:08:13,301 DEBUG TRAIN Batch 11/1800 loss 87.608101 loss_att 148.256744 loss_ctc 88.096741 loss_rnnt 75.267555 hw_loss 0.273122 lr 0.00051941 rank 7
2023-02-20 21:09:13,244 DEBUG TRAIN Batch 11/1900 loss 22.145800 loss_att 69.412788 loss_ctc 9.590771 loss_rnnt 14.143626 hw_loss 0.417715 lr 0.00051927 rank 0
2023-02-20 21:09:13,250 DEBUG TRAIN Batch 11/1900 loss 60.714592 loss_att 121.924103 loss_ctc 56.326351 loss_rnnt 48.642601 hw_loss 0.778478 lr 0.00051927 rank 2
2023-02-20 21:09:13,256 DEBUG TRAIN Batch 11/1900 loss 26.142969 loss_att 71.195580 loss_ctc 23.127623 loss_rnnt 17.205137 hw_loss 0.617544 lr 0.00051927 rank 6
2023-02-20 21:09:13,256 DEBUG TRAIN Batch 11/1900 loss 58.358204 loss_att 165.313705 loss_ctc 50.807541 loss_rnnt 37.754105 hw_loss 0.412034 lr 0.00051927 rank 4
2023-02-20 21:09:13,257 DEBUG TRAIN Batch 11/1900 loss 33.031239 loss_att 109.292870 loss_ctc 26.685539 loss_rnnt 18.508179 hw_loss 0.219052 lr 0.00051927 rank 3
2023-02-20 21:09:13,259 DEBUG TRAIN Batch 11/1900 loss 51.393745 loss_att 115.922363 loss_ctc 68.647575 loss_rnnt 36.148872 hw_loss 0.072451 lr 0.00051927 rank 7
2023-02-20 21:09:13,261 DEBUG TRAIN Batch 11/1900 loss 61.219780 loss_att 97.098038 loss_ctc 72.197189 loss_rnnt 52.369251 hw_loss 0.396046 lr 0.00051927 rank 5
2023-02-20 21:09:13,274 DEBUG TRAIN Batch 11/1900 loss 63.890701 loss_att 156.271637 loss_ctc 56.829670 loss_rnnt 46.147739 hw_loss 0.390451 lr 0.00051927 rank 1
2023-02-20 21:10:10,161 DEBUG TRAIN Batch 11/2000 loss 31.213194 loss_att 65.322296 loss_ctc 30.119602 loss_rnnt 24.437378 hw_loss 0.187135 lr 0.00051913 rank 0
2023-02-20 21:10:10,169 DEBUG TRAIN Batch 11/2000 loss 48.462837 loss_att 127.441132 loss_ctc 47.570583 loss_rnnt 32.587200 hw_loss 0.373021 lr 0.00051913 rank 3
2023-02-20 21:10:10,172 DEBUG TRAIN Batch 11/2000 loss 16.299814 loss_att 31.556883 loss_ctc 15.299469 loss_rnnt 13.149986 hw_loss 0.434611 lr 0.00051913 rank 1
2023-02-20 21:10:10,179 DEBUG TRAIN Batch 11/2000 loss 27.464224 loss_att 76.323814 loss_ctc 25.701996 loss_rnnt 17.794197 hw_loss 0.249510 lr 0.00051913 rank 5
2023-02-20 21:10:10,180 DEBUG TRAIN Batch 11/2000 loss 289.884827 loss_att 329.314148 loss_ctc 325.822693 loss_rnnt 277.173950 hw_loss 0.062372 lr 0.00051913 rank 7
2023-02-20 21:10:10,180 DEBUG TRAIN Batch 11/2000 loss 68.738914 loss_att 136.085861 loss_ctc 68.873085 loss_rnnt 55.115646 hw_loss 0.254975 lr 0.00051913 rank 2
2023-02-20 21:10:10,186 DEBUG TRAIN Batch 11/2000 loss 24.430706 loss_att 33.026230 loss_ctc 24.825918 loss_rnnt 22.260336 hw_loss 0.747319 lr 0.00051913 rank 6
2023-02-20 21:10:10,243 DEBUG TRAIN Batch 11/2000 loss 26.010511 loss_att 43.267883 loss_ctc 25.938334 loss_rnnt 22.421062 hw_loss 0.276749 lr 0.00051913 rank 4
2023-02-20 21:11:07,097 DEBUG TRAIN Batch 11/2100 loss 49.213032 loss_att 103.990501 loss_ctc 49.350151 loss_rnnt 38.121529 hw_loss 0.220740 lr 0.00051899 rank 0
2023-02-20 21:11:07,108 DEBUG TRAIN Batch 11/2100 loss 66.791191 loss_att 166.947083 loss_ctc 49.997154 loss_rnnt 48.884285 hw_loss 0.215498 lr 0.00051899 rank 1
2023-02-20 21:11:07,110 DEBUG TRAIN Batch 11/2100 loss 44.988373 loss_att 111.227097 loss_ctc 39.020737 loss_rnnt 32.483421 hw_loss 0.099177 lr 0.00051899 rank 3
2023-02-20 21:11:07,110 DEBUG TRAIN Batch 11/2100 loss 70.516342 loss_att 135.268890 loss_ctc 72.413841 loss_rnnt 57.199436 hw_loss 0.212618 lr 0.00051899 rank 4
2023-02-20 21:11:07,114 DEBUG TRAIN Batch 11/2100 loss 25.797270 loss_att 68.897858 loss_ctc 19.852751 loss_rnnt 17.734909 hw_loss 0.440338 lr 0.00051899 rank 2
2023-02-20 21:11:07,117 DEBUG TRAIN Batch 11/2100 loss 77.365974 loss_att 133.086761 loss_ctc 75.942696 loss_rnnt 66.236061 hw_loss 0.329119 lr 0.00051899 rank 5
2023-02-20 21:11:07,119 DEBUG TRAIN Batch 11/2100 loss 39.714653 loss_att 76.938919 loss_ctc 34.144566 loss_rnnt 32.971016 hw_loss 0.077742 lr 0.00051899 rank 6
2023-02-20 21:11:07,187 DEBUG TRAIN Batch 11/2100 loss 86.055260 loss_att 154.664703 loss_ctc 79.173615 loss_rnnt 73.059387 hw_loss 0.359137 lr 0.00051899 rank 7
2023-02-20 21:12:06,953 DEBUG TRAIN Batch 11/2200 loss 103.964958 loss_att 156.446640 loss_ctc 122.240349 loss_rnnt 90.968521 hw_loss 0.118835 lr 0.00051885 rank 0
2023-02-20 21:12:06,959 DEBUG TRAIN Batch 11/2200 loss 106.842659 loss_att 197.114349 loss_ctc 109.840065 loss_rnnt 88.262657 hw_loss 0.236272 lr 0.00051885 rank 6
2023-02-20 21:12:06,967 DEBUG TRAIN Batch 11/2200 loss 46.480114 loss_att 120.794891 loss_ctc 39.710297 loss_rnnt 32.296856 hw_loss 0.418019 lr 0.00051885 rank 1
2023-02-20 21:12:06,967 DEBUG TRAIN Batch 11/2200 loss 34.286545 loss_att 70.475327 loss_ctc 39.223938 loss_rnnt 26.245770 hw_loss 0.271307 lr 0.00051885 rank 3
2023-02-20 21:12:06,968 DEBUG TRAIN Batch 11/2200 loss 52.194359 loss_att 145.027573 loss_ctc 48.154942 loss_rnnt 33.993729 hw_loss 0.323584 lr 0.00051885 rank 2
2023-02-20 21:12:06,970 DEBUG TRAIN Batch 11/2200 loss 38.443562 loss_att 83.631531 loss_ctc 32.333706 loss_rnnt 30.009880 hw_loss 0.395125 lr 0.00051885 rank 4
2023-02-20 21:12:06,974 DEBUG TRAIN Batch 11/2200 loss 34.664169 loss_att 88.845093 loss_ctc 35.864433 loss_rnnt 23.298199 hw_loss 0.693286 lr 0.00051885 rank 7
2023-02-20 21:12:07,026 DEBUG TRAIN Batch 11/2200 loss 73.698929 loss_att 142.307632 loss_ctc 65.415245 loss_rnnt 60.920845 hw_loss 0.301553 lr 0.00051885 rank 5
2023-02-20 21:13:04,641 DEBUG TRAIN Batch 11/2300 loss 63.029980 loss_att 89.734009 loss_ctc 75.333496 loss_rnnt 55.794136 hw_loss 0.477316 lr 0.00051871 rank 0
2023-02-20 21:13:04,642 DEBUG TRAIN Batch 11/2300 loss 29.523184 loss_att 51.188091 loss_ctc 34.964302 loss_rnnt 24.335751 hw_loss 0.241816 lr 0.00051871 rank 6
2023-02-20 21:13:04,642 DEBUG TRAIN Batch 11/2300 loss 15.425724 loss_att 45.093056 loss_ctc 14.501454 loss_rnnt 9.389330 hw_loss 0.424055 lr 0.00051871 rank 2
2023-02-20 21:13:04,647 DEBUG TRAIN Batch 11/2300 loss 82.985519 loss_att 165.212799 loss_ctc 76.293915 loss_rnnt 67.322617 hw_loss 0.205617 lr 0.00051871 rank 5
2023-02-20 21:13:04,649 DEBUG TRAIN Batch 11/2300 loss 35.520000 loss_att 48.836723 loss_ctc 38.070679 loss_rnnt 32.286217 hw_loss 0.431895 lr 0.00051871 rank 4
2023-02-20 21:13:04,651 DEBUG TRAIN Batch 11/2300 loss 27.708733 loss_att 43.246140 loss_ctc 33.621525 loss_rnnt 23.519554 hw_loss 0.549985 lr 0.00051871 rank 7
2023-02-20 21:13:04,655 DEBUG TRAIN Batch 11/2300 loss 113.763985 loss_att 244.504959 loss_ctc 138.240555 loss_rnnt 84.295303 hw_loss 0.106761 lr 0.00051871 rank 3
2023-02-20 21:13:04,710 DEBUG TRAIN Batch 11/2300 loss 39.357735 loss_att 53.993019 loss_ctc 43.105694 loss_rnnt 35.716286 hw_loss 0.402486 lr 0.00051871 rank 1
2023-02-20 21:14:02,902 DEBUG TRAIN Batch 11/2400 loss 46.670021 loss_att 92.060730 loss_ctc 39.809532 loss_rnnt 38.326813 hw_loss 0.337122 lr 0.00051857 rank 0
2023-02-20 21:14:02,904 DEBUG TRAIN Batch 11/2400 loss 43.685268 loss_att 64.306915 loss_ctc 41.261444 loss_rnnt 39.555706 hw_loss 0.615766 lr 0.00051857 rank 5
2023-02-20 21:14:02,908 DEBUG TRAIN Batch 11/2400 loss 29.255455 loss_att 73.936363 loss_ctc 29.004005 loss_rnnt 20.179947 hw_loss 0.324096 lr 0.00051857 rank 6
2023-02-20 21:14:02,910 DEBUG TRAIN Batch 11/2400 loss 34.003830 loss_att 54.114155 loss_ctc 31.728420 loss_rnnt 30.171764 hw_loss 0.212604 lr 0.00051857 rank 3
2023-02-20 21:14:02,916 DEBUG TRAIN Batch 11/2400 loss 54.324184 loss_att 91.159760 loss_ctc 58.138535 loss_rnnt 46.418243 hw_loss 0.056701 lr 0.00051857 rank 2
2023-02-20 21:14:02,918 DEBUG TRAIN Batch 11/2400 loss 62.523693 loss_att 111.812019 loss_ctc 55.915192 loss_rnnt 53.266968 hw_loss 0.525372 lr 0.00051857 rank 7
2023-02-20 21:14:02,919 DEBUG TRAIN Batch 11/2400 loss 36.891766 loss_att 67.265099 loss_ctc 29.825575 loss_rnnt 31.608351 hw_loss 0.282950 lr 0.00051857 rank 4
2023-02-20 21:14:02,921 DEBUG TRAIN Batch 11/2400 loss 36.622452 loss_att 69.829910 loss_ctc 42.123978 loss_rnnt 28.990150 hw_loss 0.482387 lr 0.00051857 rank 1
2023-02-20 21:15:02,600 DEBUG TRAIN Batch 11/2500 loss 91.641068 loss_att 134.149399 loss_ctc 93.045547 loss_rnnt 82.847389 hw_loss 0.196388 lr 0.00051843 rank 0
2023-02-20 21:15:02,600 DEBUG TRAIN Batch 11/2500 loss 57.093510 loss_att 79.646149 loss_ctc 53.339260 loss_rnnt 53.077980 hw_loss 0.010434 lr 0.00051843 rank 6
2023-02-20 21:15:02,607 DEBUG TRAIN Batch 11/2500 loss 54.126099 loss_att 118.016663 loss_ctc 51.320538 loss_rnnt 41.644299 hw_loss 0.145809 lr 0.00051843 rank 2
2023-02-20 21:15:02,610 DEBUG TRAIN Batch 11/2500 loss 62.282238 loss_att 97.876205 loss_ctc 70.102051 loss_rnnt 53.909233 hw_loss 0.396703 lr 0.00051843 rank 5
2023-02-20 21:15:02,610 DEBUG TRAIN Batch 11/2500 loss 37.507660 loss_att 53.952160 loss_ctc 46.145844 loss_rnnt 32.962921 hw_loss 0.195154 lr 0.00051843 rank 1
2023-02-20 21:15:02,613 DEBUG TRAIN Batch 11/2500 loss 109.790123 loss_att 124.226700 loss_ctc 116.260605 loss_rnnt 105.680428 hw_loss 0.674337 lr 0.00051843 rank 7
2023-02-20 21:15:02,620 DEBUG TRAIN Batch 11/2500 loss 100.669090 loss_att 182.147537 loss_ctc 115.732216 loss_rnnt 82.318893 hw_loss 0.086419 lr 0.00051843 rank 4
2023-02-20 21:15:02,625 DEBUG TRAIN Batch 11/2500 loss 87.660683 loss_att 190.232880 loss_ctc 75.296829 loss_rnnt 68.683540 hw_loss 0.208527 lr 0.00051843 rank 3
2023-02-20 21:16:21,331 DEBUG TRAIN Batch 11/2600 loss 92.181618 loss_att 174.037201 loss_ctc 80.998375 loss_rnnt 77.125511 hw_loss 0.330158 lr 0.00051829 rank 0
2023-02-20 21:16:21,338 DEBUG TRAIN Batch 11/2600 loss 37.571716 loss_att 58.132462 loss_ctc 35.150188 loss_rnnt 33.708664 hw_loss 0.138324 lr 0.00051829 rank 6
2023-02-20 21:16:21,338 DEBUG TRAIN Batch 11/2600 loss 52.330025 loss_att 92.359077 loss_ctc 52.090225 loss_rnnt 44.258354 hw_loss 0.183441 lr 0.00051829 rank 1
2023-02-20 21:16:21,343 DEBUG TRAIN Batch 11/2600 loss 22.502022 loss_att 30.615435 loss_ctc 26.076168 loss_rnnt 20.187721 hw_loss 0.403247 lr 0.00051829 rank 2
2023-02-20 21:16:21,343 DEBUG TRAIN Batch 11/2600 loss 50.174099 loss_att 62.109226 loss_ctc 43.663528 loss_rnnt 48.616383 hw_loss 0.072692 lr 0.00051829 rank 5
2023-02-20 21:16:21,346 DEBUG TRAIN Batch 11/2600 loss 23.053886 loss_att 32.438126 loss_ctc 25.032963 loss_rnnt 20.701948 hw_loss 0.396029 lr 0.00051829 rank 3
2023-02-20 21:16:21,349 DEBUG TRAIN Batch 11/2600 loss 27.420397 loss_att 41.144329 loss_ctc 32.149879 loss_rnnt 23.910080 hw_loss 0.252994 lr 0.00051829 rank 7
2023-02-20 21:16:21,351 DEBUG TRAIN Batch 11/2600 loss 54.788101 loss_att 66.330933 loss_ctc 61.197273 loss_rnnt 51.460434 hw_loss 0.308518 lr 0.00051829 rank 4
2023-02-20 21:17:19,614 DEBUG TRAIN Batch 11/2700 loss 53.179035 loss_att 93.580002 loss_ctc 56.164101 loss_rnnt 44.570549 hw_loss 0.244279 lr 0.00051816 rank 0
2023-02-20 21:17:19,619 DEBUG TRAIN Batch 11/2700 loss 90.198647 loss_att 155.269363 loss_ctc 86.800659 loss_rnnt 77.447586 hw_loss 0.356218 lr 0.00051816 rank 6
2023-02-20 21:17:19,620 DEBUG TRAIN Batch 11/2700 loss 41.119091 loss_att 66.812172 loss_ctc 43.952946 loss_rnnt 35.377518 hw_loss 0.422083 lr 0.00051816 rank 3
2023-02-20 21:17:19,622 DEBUG TRAIN Batch 11/2700 loss 44.882488 loss_att 91.285461 loss_ctc 41.412796 loss_rnnt 35.844376 hw_loss 0.412773 lr 0.00051816 rank 2
2023-02-20 21:17:19,626 DEBUG TRAIN Batch 11/2700 loss 60.275620 loss_att 94.506111 loss_ctc 56.475132 loss_rnnt 53.720642 hw_loss 0.404261 lr 0.00051816 rank 4
2023-02-20 21:17:19,627 DEBUG TRAIN Batch 11/2700 loss 46.167171 loss_att 80.597755 loss_ctc 47.169079 loss_rnnt 38.981979 hw_loss 0.310287 lr 0.00051816 rank 5
2023-02-20 21:17:19,626 DEBUG TRAIN Batch 11/2700 loss 91.237823 loss_att 152.063766 loss_ctc 88.685158 loss_rnnt 79.366501 hw_loss 0.087168 lr 0.00051816 rank 7
2023-02-20 21:17:19,685 DEBUG TRAIN Batch 11/2700 loss 39.329994 loss_att 78.461845 loss_ctc 31.061474 loss_rnnt 32.423630 hw_loss 0.342123 lr 0.00051816 rank 1
2023-02-20 21:18:18,046 DEBUG TRAIN Batch 11/2800 loss 125.291298 loss_att 161.034805 loss_ctc 136.885422 loss_rnnt 116.500252 hw_loss 0.180853 lr 0.00051802 rank 0
2023-02-20 21:18:18,051 DEBUG TRAIN Batch 11/2800 loss 58.170139 loss_att 118.748642 loss_ctc 48.646408 loss_rnnt 47.174717 hw_loss 0.280407 lr 0.00051802 rank 2
2023-02-20 21:18:18,053 DEBUG TRAIN Batch 11/2800 loss 26.794495 loss_att 45.558933 loss_ctc 21.644909 loss_rnnt 23.565479 hw_loss 0.305126 lr 0.00051802 rank 3
2023-02-20 21:18:18,053 DEBUG TRAIN Batch 11/2800 loss 49.213566 loss_att 123.903374 loss_ctc 50.823112 loss_rnnt 33.828865 hw_loss 0.435248 lr 0.00051802 rank 1
2023-02-20 21:18:18,059 DEBUG TRAIN Batch 11/2800 loss 45.639950 loss_att 72.297501 loss_ctc 40.133659 loss_rnnt 40.824806 hw_loss 0.408395 lr 0.00051802 rank 4
2023-02-20 21:18:18,060 DEBUG TRAIN Batch 11/2800 loss 33.243690 loss_att 85.611923 loss_ctc 26.999495 loss_rnnt 23.409704 hw_loss 0.361694 lr 0.00051802 rank 5
2023-02-20 21:18:18,062 DEBUG TRAIN Batch 11/2800 loss 63.685280 loss_att 125.258102 loss_ctc 76.646973 loss_rnnt 49.436253 hw_loss 0.386698 lr 0.00051802 rank 7
2023-02-20 21:18:18,121 DEBUG TRAIN Batch 11/2800 loss 26.444748 loss_att 56.345104 loss_ctc 27.821671 loss_rnnt 20.171690 hw_loss 0.205114 lr 0.00051802 rank 6
2023-02-20 21:19:16,288 DEBUG TRAIN Batch 11/2900 loss 30.090208 loss_att 43.513866 loss_ctc 26.276915 loss_rnnt 27.719910 hw_loss 0.363759 lr 0.00051788 rank 0
2023-02-20 21:19:16,290 DEBUG TRAIN Batch 11/2900 loss 52.378838 loss_att 99.302017 loss_ctc 55.425640 loss_rnnt 42.547035 hw_loss 0.076743 lr 0.00051788 rank 5
2023-02-20 21:19:16,292 DEBUG TRAIN Batch 11/2900 loss 61.446823 loss_att 97.158211 loss_ctc 70.458435 loss_rnnt 52.901527 hw_loss 0.377748 lr 0.00051788 rank 4
2023-02-20 21:19:16,294 DEBUG TRAIN Batch 11/2900 loss 33.714626 loss_att 55.874710 loss_ctc 34.632378 loss_rnnt 29.080811 hw_loss 0.148935 lr 0.00051788 rank 7
2023-02-20 21:19:16,296 DEBUG TRAIN Batch 11/2900 loss 19.385788 loss_att 24.741966 loss_ctc 21.183765 loss_rnnt 18.005962 hw_loss 0.129106 lr 0.00051788 rank 3
2023-02-20 21:19:16,297 DEBUG TRAIN Batch 11/2900 loss 43.669598 loss_att 62.722713 loss_ctc 51.336975 loss_rnnt 38.632854 hw_loss 0.382124 lr 0.00051788 rank 2
2023-02-20 21:19:16,300 DEBUG TRAIN Batch 11/2900 loss 30.538355 loss_att 47.398628 loss_ctc 25.027473 loss_rnnt 27.687656 hw_loss 0.400177 lr 0.00051788 rank 1
2023-02-20 21:19:16,355 DEBUG TRAIN Batch 11/2900 loss 43.793354 loss_att 61.777241 loss_ctc 43.873814 loss_rnnt 39.944386 hw_loss 0.452741 lr 0.00051788 rank 6
2023-02-20 21:20:16,111 DEBUG TRAIN Batch 11/3000 loss 39.785381 loss_att 63.554199 loss_ctc 33.155910 loss_rnnt 35.835857 hw_loss 0.149408 lr 0.00051774 rank 0
2023-02-20 21:20:16,116 DEBUG TRAIN Batch 11/3000 loss 42.587814 loss_att 63.328979 loss_ctc 41.897575 loss_rnnt 38.453896 hw_loss 0.145725 lr 0.00051774 rank 1
2023-02-20 21:20:16,118 DEBUG TRAIN Batch 11/3000 loss 15.329396 loss_att 29.178844 loss_ctc 16.359814 loss_rnnt 12.297390 hw_loss 0.233863 lr 0.00051774 rank 2
2023-02-20 21:20:16,127 DEBUG TRAIN Batch 11/3000 loss 44.618729 loss_att 79.879265 loss_ctc 41.948570 loss_rnnt 37.762650 hw_loss 0.299987 lr 0.00051774 rank 6
2023-02-20 21:20:16,128 DEBUG TRAIN Batch 11/3000 loss 51.614746 loss_att 66.855606 loss_ctc 50.361221 loss_rnnt 48.604561 hw_loss 0.242154 lr 0.00051774 rank 3
2023-02-20 21:20:16,127 DEBUG TRAIN Batch 11/3000 loss 15.497495 loss_att 44.168304 loss_ctc 12.758743 loss_rnnt 10.041079 hw_loss 0.163912 lr 0.00051774 rank 5
2023-02-20 21:20:16,140 DEBUG TRAIN Batch 11/3000 loss 32.699451 loss_att 54.792999 loss_ctc 37.446117 loss_rnnt 27.506714 hw_loss 0.264633 lr 0.00051774 rank 4
2023-02-20 21:20:16,140 DEBUG TRAIN Batch 11/3000 loss 50.467884 loss_att 93.487656 loss_ctc 62.302994 loss_rnnt 40.137665 hw_loss 0.277965 lr 0.00051774 rank 7
2023-02-20 21:21:14,523 DEBUG TRAIN Batch 11/3100 loss 48.668484 loss_att 95.148941 loss_ctc 39.874031 loss_rnnt 40.516605 hw_loss 0.053208 lr 0.00051760 rank 0
2023-02-20 21:21:14,536 DEBUG TRAIN Batch 11/3100 loss 36.982189 loss_att 79.042717 loss_ctc 28.854805 loss_rnnt 29.471533 hw_loss 0.341625 lr 0.00051760 rank 3
2023-02-20 21:21:14,538 DEBUG TRAIN Batch 11/3100 loss 23.533342 loss_att 52.345943 loss_ctc 18.646397 loss_rnnt 18.122015 hw_loss 0.563251 lr 0.00051760 rank 7
2023-02-20 21:21:14,540 DEBUG TRAIN Batch 11/3100 loss 26.857025 loss_att 65.062401 loss_ctc 27.170511 loss_rnnt 18.968803 hw_loss 0.385026 lr 0.00051760 rank 2
2023-02-20 21:21:14,545 DEBUG TRAIN Batch 11/3100 loss 40.189716 loss_att 65.244461 loss_ctc 36.131721 loss_rnnt 35.594830 hw_loss 0.234382 lr 0.00051760 rank 5
2023-02-20 21:21:14,546 DEBUG TRAIN Batch 11/3100 loss 29.716518 loss_att 54.983089 loss_ctc 13.136494 loss_rnnt 26.779694 hw_loss 0.176584 lr 0.00051760 rank 6
2023-02-20 21:21:14,584 DEBUG TRAIN Batch 11/3100 loss 91.624878 loss_att 129.050262 loss_ctc 97.574242 loss_rnnt 83.162392 hw_loss 0.345305 lr 0.00051760 rank 4
2023-02-20 21:21:14,628 DEBUG TRAIN Batch 11/3100 loss 39.470955 loss_att 67.046783 loss_ctc 47.071648 loss_rnnt 32.913986 hw_loss 0.053208 lr 0.00051760 rank 1
2023-02-20 21:22:12,266 DEBUG TRAIN Batch 11/3200 loss 25.194807 loss_att 34.960144 loss_ctc 34.428055 loss_rnnt 21.817139 hw_loss 0.362812 lr 0.00051746 rank 0
2023-02-20 21:22:12,275 DEBUG TRAIN Batch 11/3200 loss 26.133764 loss_att 35.343231 loss_ctc 22.646805 loss_rnnt 24.589436 hw_loss 0.313804 lr 0.00051746 rank 2
2023-02-20 21:22:12,277 DEBUG TRAIN Batch 11/3200 loss 62.590115 loss_att 103.718933 loss_ctc 71.489044 loss_rnnt 53.147133 hw_loss 0.057555 lr 0.00051746 rank 7
2023-02-20 21:22:12,278 DEBUG TRAIN Batch 11/3200 loss 26.976982 loss_att 44.383495 loss_ctc 27.624411 loss_rnnt 23.268818 hw_loss 0.263506 lr 0.00051746 rank 1
2023-02-20 21:22:12,279 DEBUG TRAIN Batch 11/3200 loss 41.923336 loss_att 53.558174 loss_ctc 45.667908 loss_rnnt 38.846317 hw_loss 0.470203 lr 0.00051746 rank 3
2023-02-20 21:22:12,280 DEBUG TRAIN Batch 11/3200 loss 22.199371 loss_att 23.926273 loss_ctc 24.989555 loss_rnnt 21.207157 hw_loss 0.515264 lr 0.00051746 rank 5
2023-02-20 21:22:12,282 DEBUG TRAIN Batch 11/3200 loss 30.543903 loss_att 48.869175 loss_ctc 30.327423 loss_rnnt 26.813984 hw_loss 0.175744 lr 0.00051746 rank 4
2023-02-20 21:22:12,338 DEBUG TRAIN Batch 11/3200 loss 22.247528 loss_att 41.348022 loss_ctc 27.883171 loss_rnnt 17.417149 hw_loss 0.485362 lr 0.00051746 rank 6
2023-02-20 21:23:12,695 DEBUG TRAIN Batch 11/3300 loss 43.568359 loss_att 64.272858 loss_ctc 58.147263 loss_rnnt 37.382225 hw_loss 0.190089 lr 0.00051732 rank 0
2023-02-20 21:23:12,703 DEBUG TRAIN Batch 11/3300 loss 47.343163 loss_att 84.271515 loss_ctc 43.162605 loss_rnnt 40.428375 hw_loss 0.162232 lr 0.00051732 rank 4
2023-02-20 21:23:12,705 DEBUG TRAIN Batch 11/3300 loss 65.706078 loss_att 105.655411 loss_ctc 73.955353 loss_rnnt 56.458496 hw_loss 0.295879 lr 0.00051732 rank 1
2023-02-20 21:23:12,705 DEBUG TRAIN Batch 11/3300 loss 41.872875 loss_att 72.746353 loss_ctc 43.698288 loss_rnnt 35.434174 hw_loss 0.038648 lr 0.00051732 rank 2
2023-02-20 21:23:12,708 DEBUG TRAIN Batch 11/3300 loss 65.908775 loss_att 83.020432 loss_ctc 49.356476 loss_rnnt 64.504143 hw_loss 0.354903 lr 0.00051732 rank 6
2023-02-20 21:23:12,712 DEBUG TRAIN Batch 11/3300 loss 15.939552 loss_att 36.060867 loss_ctc 14.202958 loss_rnnt 12.069732 hw_loss 0.144569 lr 0.00051732 rank 3
2023-02-20 21:23:12,715 DEBUG TRAIN Batch 11/3300 loss 44.445671 loss_att 84.293381 loss_ctc 46.156475 loss_rnnt 36.102730 hw_loss 0.272418 lr 0.00051732 rank 7
2023-02-20 21:23:12,760 DEBUG TRAIN Batch 11/3300 loss 37.634308 loss_att 68.425423 loss_ctc 46.244133 loss_rnnt 30.185665 hw_loss 0.267081 lr 0.00051732 rank 5
2023-02-20 21:24:11,647 DEBUG TRAIN Batch 11/3400 loss 40.217682 loss_att 63.789932 loss_ctc 45.780315 loss_rnnt 34.636337 hw_loss 0.234770 lr 0.00051718 rank 2
2023-02-20 21:24:11,647 DEBUG TRAIN Batch 11/3400 loss 51.158810 loss_att 60.089088 loss_ctc 44.694462 loss_rnnt 50.135540 hw_loss 0.185873 lr 0.00051718 rank 0
2023-02-20 21:24:11,652 DEBUG TRAIN Batch 11/3400 loss 34.971603 loss_att 54.423836 loss_ctc 31.822227 loss_rnnt 31.321167 hw_loss 0.337327 lr 0.00051718 rank 3
2023-02-20 21:24:11,654 DEBUG TRAIN Batch 11/3400 loss 23.117920 loss_att 36.262444 loss_ctc 25.537685 loss_rnnt 19.850197 hw_loss 0.592844 lr 0.00051718 rank 5
2023-02-20 21:24:11,677 DEBUG TRAIN Batch 11/3400 loss 49.187397 loss_att 93.754868 loss_ctc 48.760601 loss_rnnt 40.301884 hw_loss 0.054237 lr 0.00051718 rank 6
2023-02-20 21:24:11,681 DEBUG TRAIN Batch 11/3400 loss 20.414461 loss_att 36.902771 loss_ctc 22.905014 loss_rnnt 16.563519 hw_loss 0.414762 lr 0.00051718 rank 1
2023-02-20 21:24:11,693 DEBUG TRAIN Batch 11/3400 loss 47.733761 loss_att 66.055458 loss_ctc 56.915604 loss_rnnt 42.630764 hw_loss 0.402025 lr 0.00051718 rank 4
2023-02-20 21:24:11,702 DEBUG TRAIN Batch 11/3400 loss 29.584997 loss_att 47.310444 loss_ctc 36.774082 loss_rnnt 25.052437 hw_loss 0.054237 lr 0.00051718 rank 7
2023-02-20 21:25:30,495 DEBUG TRAIN Batch 11/3500 loss 46.261505 loss_att 60.248726 loss_ctc 56.950584 loss_rnnt 41.937828 hw_loss 0.189422 lr 0.00051705 rank 5
2023-02-20 21:25:30,496 DEBUG TRAIN Batch 11/3500 loss 50.113274 loss_att 69.489273 loss_ctc 52.331680 loss_rnnt 45.647041 hw_loss 0.553570 lr 0.00051705 rank 0
2023-02-20 21:25:30,497 DEBUG TRAIN Batch 11/3500 loss 16.943439 loss_att 23.829540 loss_ctc 15.894302 loss_rnnt 15.514568 hw_loss 0.359128 lr 0.00051705 rank 1
2023-02-20 21:25:30,500 DEBUG TRAIN Batch 11/3500 loss 39.981537 loss_att 66.240593 loss_ctc 49.401131 loss_rnnt 33.385620 hw_loss 0.165301 lr 0.00051705 rank 7
2023-02-20 21:25:30,501 DEBUG TRAIN Batch 11/3500 loss 27.770187 loss_att 36.957733 loss_ctc 25.920021 loss_rnnt 25.954845 hw_loss 0.420981 lr 0.00051705 rank 2
2023-02-20 21:25:30,500 DEBUG TRAIN Batch 11/3500 loss 61.811829 loss_att 104.888832 loss_ctc 62.306946 loss_rnnt 52.896500 hw_loss 0.438587 lr 0.00051705 rank 6
2023-02-20 21:25:30,502 DEBUG TRAIN Batch 11/3500 loss 32.301590 loss_att 51.351368 loss_ctc 31.592010 loss_rnnt 28.540514 hw_loss 0.085747 lr 0.00051705 rank 3
2023-02-20 21:25:30,502 DEBUG TRAIN Batch 11/3500 loss 23.698671 loss_att 36.428261 loss_ctc 20.816008 loss_rnnt 21.378160 hw_loss 0.298028 lr 0.00051705 rank 4
2023-02-20 21:26:30,434 DEBUG TRAIN Batch 11/3600 loss 54.453945 loss_att 91.967560 loss_ctc 61.160889 loss_rnnt 45.793922 hw_loss 0.493193 lr 0.00051691 rank 0
2023-02-20 21:26:30,441 DEBUG TRAIN Batch 11/3600 loss 16.872826 loss_att 39.057838 loss_ctc 19.244408 loss_rnnt 11.935734 hw_loss 0.344775 lr 0.00051691 rank 4
2023-02-20 21:26:30,443 DEBUG TRAIN Batch 11/3600 loss 37.455669 loss_att 73.603363 loss_ctc 38.613571 loss_rnnt 30.048552 hw_loss 0.043484 lr 0.00051691 rank 6
2023-02-20 21:26:30,447 DEBUG TRAIN Batch 11/3600 loss 71.782867 loss_att 99.116005 loss_ctc 90.983833 loss_rnnt 63.713387 hw_loss 0.080118 lr 0.00051691 rank 3
2023-02-20 21:26:30,454 DEBUG TRAIN Batch 11/3600 loss 20.785351 loss_att 40.909039 loss_ctc 21.465271 loss_rnnt 16.646767 hw_loss 0.043484 lr 0.00051691 rank 1
2023-02-20 21:26:30,458 DEBUG TRAIN Batch 11/3600 loss 33.111252 loss_att 60.531567 loss_ctc 31.677948 loss_rnnt 27.560905 hw_loss 0.482609 lr 0.00051691 rank 2
2023-02-20 21:26:30,460 DEBUG TRAIN Batch 11/3600 loss 28.547455 loss_att 53.803604 loss_ctc 29.455166 loss_rnnt 23.177319 hw_loss 0.371024 lr 0.00051691 rank 7
2023-02-20 21:26:30,501 DEBUG TRAIN Batch 11/3600 loss 37.418896 loss_att 57.802345 loss_ctc 41.060944 loss_rnnt 32.713600 hw_loss 0.268124 lr 0.00051691 rank 5
2023-02-20 21:27:27,387 DEBUG TRAIN Batch 11/3700 loss 69.172768 loss_att 106.605125 loss_ctc 75.378510 loss_rnnt 60.787262 hw_loss 0.134267 lr 0.00051677 rank 0
2023-02-20 21:27:27,394 DEBUG TRAIN Batch 11/3700 loss 29.289583 loss_att 72.985008 loss_ctc 24.062748 loss_rnnt 21.065334 hw_loss 0.341389 lr 0.00051677 rank 5
2023-02-20 21:27:27,400 DEBUG TRAIN Batch 11/3700 loss 19.986044 loss_att 43.931351 loss_ctc 18.697041 loss_rnnt 15.307709 hw_loss 0.114635 lr 0.00051677 rank 3
2023-02-20 21:27:27,401 DEBUG TRAIN Batch 11/3700 loss 28.085657 loss_att 48.959129 loss_ctc 25.156647 loss_rnnt 24.270378 hw_loss 0.058345 lr 0.00051677 rank 1
2023-02-20 21:27:27,402 DEBUG TRAIN Batch 11/3700 loss 36.837234 loss_att 52.170555 loss_ctc 29.256975 loss_rnnt 34.620770 hw_loss 0.300937 lr 0.00051677 rank 2
2023-02-20 21:27:27,403 DEBUG TRAIN Batch 11/3700 loss 47.536209 loss_att 58.441551 loss_ctc 56.128437 loss_rnnt 44.029095 hw_loss 0.338286 lr 0.00051677 rank 6
2023-02-20 21:27:27,403 DEBUG TRAIN Batch 11/3700 loss 64.253014 loss_att 121.941727 loss_ctc 46.158482 loss_rnnt 55.096764 hw_loss 0.058345 lr 0.00051677 rank 4
2023-02-20 21:27:27,472 DEBUG TRAIN Batch 11/3700 loss 185.540161 loss_att 181.263367 loss_ctc 226.141937 loss_rnnt 180.859406 hw_loss 0.229714 lr 0.00051677 rank 7
2023-02-20 21:28:25,886 DEBUG TRAIN Batch 11/3800 loss 48.806149 loss_att 67.591171 loss_ctc 50.959675 loss_rnnt 44.662952 hw_loss 0.185726 lr 0.00051663 rank 2
2023-02-20 21:28:25,887 DEBUG TRAIN Batch 11/3800 loss 25.531649 loss_att 45.401535 loss_ctc 26.770554 loss_rnnt 21.308779 hw_loss 0.156946 lr 0.00051663 rank 3
2023-02-20 21:28:25,889 DEBUG TRAIN Batch 11/3800 loss 25.461363 loss_att 30.175220 loss_ctc 20.425200 loss_rnnt 25.166380 hw_loss 0.044438 lr 0.00051663 rank 0
2023-02-20 21:28:25,893 DEBUG TRAIN Batch 11/3800 loss 26.026270 loss_att 39.400963 loss_ctc 30.422779 loss_rnnt 22.741428 hw_loss 0.044437 lr 0.00051663 rank 6
2023-02-20 21:28:25,894 DEBUG TRAIN Batch 11/3800 loss 34.267361 loss_att 41.429123 loss_ctc 34.084698 loss_rnnt 32.672783 hw_loss 0.349833 lr 0.00051663 rank 5
2023-02-20 21:28:25,903 DEBUG TRAIN Batch 11/3800 loss 32.564426 loss_att 43.555496 loss_ctc 36.965984 loss_rnnt 29.628122 hw_loss 0.283525 lr 0.00051663 rank 7
2023-02-20 21:28:25,905 DEBUG TRAIN Batch 11/3800 loss 47.239388 loss_att 64.161133 loss_ctc 54.120461 loss_rnnt 42.749229 hw_loss 0.353126 lr 0.00051663 rank 4
2023-02-20 21:28:25,962 DEBUG TRAIN Batch 11/3800 loss 46.437775 loss_att 76.866272 loss_ctc 50.643524 loss_rnnt 39.528984 hw_loss 0.491858 lr 0.00051663 rank 1
2023-02-20 21:29:25,457 DEBUG TRAIN Batch 11/3900 loss 61.825958 loss_att 62.202095 loss_ctc 71.188736 loss_rnnt 60.320210 hw_loss 0.341526 lr 0.00051649 rank 0
2023-02-20 21:29:25,464 DEBUG TRAIN Batch 11/3900 loss 29.991665 loss_att 42.009464 loss_ctc 31.327332 loss_rnnt 27.319870 hw_loss 0.169022 lr 0.00051649 rank 1
2023-02-20 21:29:25,464 DEBUG TRAIN Batch 11/3900 loss 21.399427 loss_att 25.776287 loss_ctc 28.722727 loss_rnnt 19.354603 hw_loss 0.361895 lr 0.00051649 rank 2
2023-02-20 21:29:25,464 DEBUG TRAIN Batch 11/3900 loss 61.715748 loss_att 110.310623 loss_ctc 57.851265 loss_rnnt 52.219204 hw_loss 0.549056 lr 0.00051649 rank 6
2023-02-20 21:29:25,464 DEBUG TRAIN Batch 11/3900 loss 48.532585 loss_att 68.073410 loss_ctc 43.725208 loss_rnnt 45.191750 hw_loss 0.138096 lr 0.00051649 rank 3
2023-02-20 21:29:25,469 DEBUG TRAIN Batch 11/3900 loss 37.191139 loss_att 72.455856 loss_ctc 35.982563 loss_rnnt 30.045710 hw_loss 0.475554 lr 0.00051649 rank 4
2023-02-20 21:29:25,469 DEBUG TRAIN Batch 11/3900 loss 41.609882 loss_att 65.205757 loss_ctc 34.517242 loss_rnnt 37.765041 hw_loss 0.133786 lr 0.00051649 rank 5
2023-02-20 21:29:25,469 DEBUG TRAIN Batch 11/3900 loss 36.785397 loss_att 39.745995 loss_ctc 43.679119 loss_rnnt 35.065495 hw_loss 0.391159 lr 0.00051649 rank 7
2023-02-20 21:30:21,432 DEBUG TRAIN Batch 11/4000 loss 40.899498 loss_att 76.698700 loss_ctc 34.062241 loss_rnnt 34.559273 hw_loss 0.172536 lr 0.00051636 rank 0
2023-02-20 21:30:21,440 DEBUG TRAIN Batch 11/4000 loss 45.534889 loss_att 54.955189 loss_ctc 50.575397 loss_rnnt 42.795006 hw_loss 0.344548 lr 0.00051636 rank 2
2023-02-20 21:30:21,443 DEBUG TRAIN Batch 11/4000 loss 36.589211 loss_att 63.024029 loss_ctc 23.851465 loss_rnnt 32.988029 hw_loss 0.023598 lr 0.00051636 rank 5
2023-02-20 21:30:21,447 DEBUG TRAIN Batch 11/4000 loss 115.479622 loss_att 152.867111 loss_ctc 144.643814 loss_rnnt 104.100983 hw_loss 0.023598 lr 0.00051636 rank 6
2023-02-20 21:30:21,448 DEBUG TRAIN Batch 11/4000 loss 9.200424 loss_att 16.313976 loss_ctc 8.301963 loss_rnnt 7.780188 hw_loss 0.219978 lr 0.00051636 rank 7
2023-02-20 21:30:21,449 DEBUG TRAIN Batch 11/4000 loss 64.443726 loss_att 85.513145 loss_ctc 62.895245 loss_rnnt 60.346245 hw_loss 0.168874 lr 0.00051636 rank 1
2023-02-20 21:30:21,449 DEBUG TRAIN Batch 11/4000 loss 69.819290 loss_att 99.404991 loss_ctc 67.284462 loss_rnnt 64.099907 hw_loss 0.262904 lr 0.00051636 rank 4
2023-02-20 21:30:21,452 DEBUG TRAIN Batch 11/4000 loss 49.433155 loss_att 89.906738 loss_ctc 48.578690 loss_rnnt 41.439781 hw_loss 0.023598 lr 0.00051636 rank 3
2023-02-20 21:31:19,153 DEBUG TRAIN Batch 11/4100 loss 54.556454 loss_att 80.516678 loss_ctc 53.607376 loss_rnnt 49.474899 hw_loss 0.030099 lr 0.00051622 rank 0
2023-02-20 21:31:19,156 DEBUG TRAIN Batch 11/4100 loss 31.847198 loss_att 44.635662 loss_ctc 32.321407 loss_rnnt 29.034950 hw_loss 0.358733 lr 0.00051622 rank 2
2023-02-20 21:31:19,157 DEBUG TRAIN Batch 11/4100 loss 29.984957 loss_att 63.606003 loss_ctc 31.285236 loss_rnnt 22.780664 hw_loss 0.575085 lr 0.00051622 rank 3
2023-02-20 21:31:19,159 DEBUG TRAIN Batch 11/4100 loss 35.485836 loss_att 53.486713 loss_ctc 36.200089 loss_rnnt 31.656057 hw_loss 0.251941 lr 0.00051622 rank 6
2023-02-20 21:31:19,163 DEBUG TRAIN Batch 11/4100 loss 69.915375 loss_att 93.410080 loss_ctc 71.433151 loss_rnnt 64.814804 hw_loss 0.373606 lr 0.00051622 rank 7
2023-02-20 21:31:19,164 DEBUG TRAIN Batch 11/4100 loss 21.865444 loss_att 35.596027 loss_ctc 21.684929 loss_rnnt 19.105934 hw_loss 0.070238 lr 0.00051622 rank 4
2023-02-20 21:31:19,206 DEBUG TRAIN Batch 11/4100 loss 17.995844 loss_att 27.698185 loss_ctc 14.799139 loss_rnnt 16.318111 hw_loss 0.306546 lr 0.00051622 rank 5
2023-02-20 21:31:19,243 DEBUG TRAIN Batch 11/4100 loss 26.164650 loss_att 36.425354 loss_ctc 29.846214 loss_rnnt 23.496025 hw_loss 0.235520 lr 0.00051622 rank 1
2023-02-20 21:32:19,154 DEBUG TRAIN Batch 11/4200 loss 82.255592 loss_att 103.526840 loss_ctc 108.146408 loss_rnnt 74.532539 hw_loss 0.031279 lr 0.00051608 rank 0
2023-02-20 21:32:19,164 DEBUG TRAIN Batch 11/4200 loss 39.198502 loss_att 43.230274 loss_ctc 32.893513 loss_rnnt 39.216125 hw_loss 0.031279 lr 0.00051608 rank 2
2023-02-20 21:32:19,166 DEBUG TRAIN Batch 11/4200 loss 31.585751 loss_att 42.218502 loss_ctc 28.034517 loss_rnnt 29.790108 hw_loss 0.267358 lr 0.00051608 rank 1
2023-02-20 21:32:19,167 DEBUG TRAIN Batch 11/4200 loss 33.772900 loss_att 66.752167 loss_ctc 29.048256 loss_rnnt 27.675079 hw_loss 0.247355 lr 0.00051608 rank 5
2023-02-20 21:32:19,170 DEBUG TRAIN Batch 11/4200 loss 62.773403 loss_att 87.659737 loss_ctc 48.417759 loss_rnnt 59.561378 hw_loss 0.279079 lr 0.00051608 rank 3
2023-02-20 21:32:19,173 DEBUG TRAIN Batch 11/4200 loss 41.369175 loss_att 67.344604 loss_ctc 46.503826 loss_rnnt 35.375496 hw_loss 0.213695 lr 0.00051608 rank 7
2023-02-20 21:32:19,178 DEBUG TRAIN Batch 11/4200 loss 21.516531 loss_att 35.997841 loss_ctc 25.474861 loss_rnnt 17.884666 hw_loss 0.389670 lr 0.00051608 rank 4
2023-02-20 21:32:19,239 DEBUG TRAIN Batch 11/4200 loss 33.538509 loss_att 59.584476 loss_ctc 32.433025 loss_rnnt 28.160423 hw_loss 0.593046 lr 0.00051608 rank 6
2023-02-20 21:33:39,269 DEBUG TRAIN Batch 11/4300 loss 64.781570 loss_att 91.186638 loss_ctc 72.526672 loss_rnnt 58.313812 hw_loss 0.288874 lr 0.00051594 rank 2
2023-02-20 21:33:39,273 DEBUG TRAIN Batch 11/4300 loss 13.522629 loss_att 22.319899 loss_ctc 12.333474 loss_rnnt 11.540266 hw_loss 0.715245 lr 0.00051594 rank 3
2023-02-20 21:33:39,278 DEBUG TRAIN Batch 11/4300 loss 13.300385 loss_att 18.872877 loss_ctc 16.698540 loss_rnnt 11.468947 hw_loss 0.494723 lr 0.00051594 rank 1
2023-02-20 21:33:39,281 DEBUG TRAIN Batch 11/4300 loss 27.348770 loss_att 64.617752 loss_ctc 36.380386 loss_rnnt 18.541214 hw_loss 0.280396 lr 0.00051594 rank 7
2023-02-20 21:33:39,286 DEBUG TRAIN Batch 11/4300 loss 91.658333 loss_att 123.737213 loss_ctc 117.030365 loss_rnnt 81.807831 hw_loss 0.097110 lr 0.00051594 rank 4
2023-02-20 21:33:39,289 DEBUG TRAIN Batch 11/4300 loss 14.414226 loss_att 18.066776 loss_ctc 17.001343 loss_rnnt 13.049512 hw_loss 0.542352 lr 0.00051594 rank 6
2023-02-20 21:33:39,294 DEBUG TRAIN Batch 11/4300 loss 39.487309 loss_att 67.815742 loss_ctc 31.081768 loss_rnnt 34.755230 hw_loss 0.350865 lr 0.00051594 rank 5
2023-02-20 21:33:39,301 DEBUG TRAIN Batch 11/4300 loss 47.419895 loss_att 79.744965 loss_ctc 48.912750 loss_rnnt 40.641907 hw_loss 0.213614 lr 0.00051594 rank 0
2023-02-20 21:34:37,906 DEBUG TRAIN Batch 11/4400 loss 24.059708 loss_att 34.063412 loss_ctc 28.302002 loss_rnnt 21.365158 hw_loss 0.240318 lr 0.00051581 rank 5
2023-02-20 21:34:37,907 DEBUG TRAIN Batch 11/4400 loss 17.420628 loss_att 29.354820 loss_ctc 18.920315 loss_rnnt 14.700448 hw_loss 0.250093 lr 0.00051581 rank 0
2023-02-20 21:34:37,908 DEBUG TRAIN Batch 11/4400 loss 29.811510 loss_att 48.742531 loss_ctc 30.521894 loss_rnnt 25.704689 hw_loss 0.423561 lr 0.00051581 rank 2
2023-02-20 21:34:37,909 DEBUG TRAIN Batch 11/4400 loss 21.889551 loss_att 37.796177 loss_ctc 22.729870 loss_rnnt 18.489044 hw_loss 0.200887 lr 0.00051581 rank 6
2023-02-20 21:34:37,910 DEBUG TRAIN Batch 11/4400 loss 14.240896 loss_att 32.371613 loss_ctc 5.986751 loss_rnnt 11.510212 hw_loss 0.384551 lr 0.00051581 rank 3
2023-02-20 21:34:37,913 DEBUG TRAIN Batch 11/4400 loss 21.164480 loss_att 45.808815 loss_ctc 21.143677 loss_rnnt 16.087563 hw_loss 0.282797 lr 0.00051581 rank 1
2023-02-20 21:34:37,914 DEBUG TRAIN Batch 11/4400 loss 23.940845 loss_att 44.220146 loss_ctc 23.950953 loss_rnnt 19.756935 hw_loss 0.237565 lr 0.00051581 rank 4
2023-02-20 21:34:37,917 DEBUG TRAIN Batch 11/4400 loss 28.288084 loss_att 43.586292 loss_ctc 27.196377 loss_rnnt 25.223995 hw_loss 0.281267 lr 0.00051581 rank 7
2023-02-20 21:35:37,783 DEBUG TRAIN Batch 11/4500 loss 19.942537 loss_att 35.636749 loss_ctc 16.718071 loss_rnnt 16.921408 hw_loss 0.585409 lr 0.00051567 rank 0
2023-02-20 21:35:37,790 DEBUG TRAIN Batch 11/4500 loss 97.008644 loss_att 142.772125 loss_ctc 101.975693 loss_rnnt 86.966492 hw_loss 0.425974 lr 0.00051567 rank 3
2023-02-20 21:35:37,791 DEBUG TRAIN Batch 11/4500 loss 24.014421 loss_att 39.982868 loss_ctc 22.577456 loss_rnnt 20.729649 hw_loss 0.530026 lr 0.00051567 rank 6
2023-02-20 21:35:37,792 DEBUG TRAIN Batch 11/4500 loss 15.744754 loss_att 28.772339 loss_ctc 20.766211 loss_rnnt 12.462345 hw_loss 0.013808 lr 0.00051567 rank 2
2023-02-20 21:35:37,792 DEBUG TRAIN Batch 11/4500 loss 32.583614 loss_att 35.282455 loss_ctc 31.461187 loss_rnnt 32.092415 hw_loss 0.189543 lr 0.00051567 rank 5
2023-02-20 21:35:37,793 DEBUG TRAIN Batch 11/4500 loss 63.632141 loss_att 81.697296 loss_ctc 66.144722 loss_rnnt 59.607460 hw_loss 0.143703 lr 0.00051567 rank 4
2023-02-20 21:35:37,797 DEBUG TRAIN Batch 11/4500 loss 27.670759 loss_att 34.374767 loss_ctc 29.434860 loss_rnnt 26.040001 hw_loss 0.102645 lr 0.00051567 rank 1
2023-02-20 21:35:37,805 DEBUG TRAIN Batch 11/4500 loss 32.481243 loss_att 43.289383 loss_ctc 33.345188 loss_rnnt 30.130131 hw_loss 0.139293 lr 0.00051567 rank 7
2023-02-20 21:36:34,678 DEBUG TRAIN Batch 11/4600 loss 23.026234 loss_att 25.522366 loss_ctc 23.617582 loss_rnnt 22.161957 hw_loss 0.536628 lr 0.00051553 rank 0
2023-02-20 21:36:34,688 DEBUG TRAIN Batch 11/4600 loss 72.691544 loss_att 80.257378 loss_ctc 79.437820 loss_rnnt 70.273529 hw_loss 0.010021 lr 0.00051553 rank 5
2023-02-20 21:36:34,689 DEBUG TRAIN Batch 11/4600 loss 23.699160 loss_att 26.393780 loss_ctc 22.940680 loss_rnnt 23.078375 hw_loss 0.343112 lr 0.00051553 rank 6
2023-02-20 21:36:34,691 DEBUG TRAIN Batch 11/4600 loss 39.084957 loss_att 51.645508 loss_ctc 49.555237 loss_rnnt 35.057846 hw_loss 0.223054 lr 0.00051553 rank 1
2023-02-20 21:36:34,692 DEBUG TRAIN Batch 11/4600 loss 22.388128 loss_att 25.772501 loss_ctc 26.818317 loss_rnnt 20.923431 hw_loss 0.369622 lr 0.00051553 rank 2
2023-02-20 21:36:34,694 DEBUG TRAIN Batch 11/4600 loss 24.935068 loss_att 57.047840 loss_ctc 18.928564 loss_rnnt 19.130692 hw_loss 0.342541 lr 0.00051553 rank 3
2023-02-20 21:36:34,695 DEBUG TRAIN Batch 11/4600 loss 44.336639 loss_att 78.555099 loss_ctc 52.815304 loss_rnnt 36.143269 hw_loss 0.410974 lr 0.00051553 rank 7
2023-02-20 21:36:34,758 DEBUG TRAIN Batch 11/4600 loss 20.497456 loss_att 26.765873 loss_ctc 23.976027 loss_rnnt 18.601843 hw_loss 0.333978 lr 0.00051553 rank 4
2023-02-20 21:37:32,184 DEBUG TRAIN Batch 11/4700 loss 32.623184 loss_att 48.686928 loss_ctc 32.806934 loss_rnnt 29.220245 hw_loss 0.310668 lr 0.00051540 rank 0
2023-02-20 21:37:32,201 DEBUG TRAIN Batch 11/4700 loss 36.587376 loss_att 59.976395 loss_ctc 40.318375 loss_rnnt 31.406906 hw_loss 0.009743 lr 0.00051540 rank 1
2023-02-20 21:37:32,201 DEBUG TRAIN Batch 11/4700 loss 5.438908 loss_att 18.958200 loss_ctc 4.225826 loss_rnnt 2.756811 hw_loss 0.262465 lr 0.00051540 rank 5
2023-02-20 21:37:32,202 DEBUG TRAIN Batch 11/4700 loss 23.789423 loss_att 43.652657 loss_ctc 27.401756 loss_rnnt 19.192619 hw_loss 0.267210 lr 0.00051540 rank 7
2023-02-20 21:37:32,203 DEBUG TRAIN Batch 11/4700 loss 23.942692 loss_att 38.458221 loss_ctc 20.612713 loss_rnnt 21.321753 hw_loss 0.303430 lr 0.00051540 rank 3
2023-02-20 21:37:32,204 DEBUG TRAIN Batch 11/4700 loss 30.709023 loss_att 43.808208 loss_ctc 32.636398 loss_rnnt 27.644308 hw_loss 0.352301 lr 0.00051540 rank 2
2023-02-20 21:37:32,204 DEBUG TRAIN Batch 11/4700 loss 48.304104 loss_att 53.099846 loss_ctc 50.839119 loss_rnnt 46.767715 hw_loss 0.448572 lr 0.00051540 rank 4
2023-02-20 21:37:32,212 DEBUG TRAIN Batch 11/4700 loss 54.236187 loss_att 70.081024 loss_ctc 58.635612 loss_rnnt 50.366280 hw_loss 0.214401 lr 0.00051540 rank 6
2023-02-20 21:38:32,455 DEBUG TRAIN Batch 11/4800 loss 20.402571 loss_att 36.206772 loss_ctc 22.328133 loss_rnnt 16.769417 hw_loss 0.404199 lr 0.00051526 rank 0
2023-02-20 21:38:32,459 DEBUG TRAIN Batch 11/4800 loss 52.669144 loss_att 70.208427 loss_ctc 52.282696 loss_rnnt 49.164478 hw_loss 0.090614 lr 0.00051526 rank 5
2023-02-20 21:38:32,462 DEBUG TRAIN Batch 11/4800 loss 29.806564 loss_att 50.608471 loss_ctc 33.987885 loss_rnnt 24.993986 hw_loss 0.177540 lr 0.00051526 rank 6
2023-02-20 21:38:32,464 DEBUG TRAIN Batch 11/4800 loss 31.507298 loss_att 46.033806 loss_ctc 28.496458 loss_rnnt 28.997847 hw_loss 0.010491 lr 0.00051526 rank 2
2023-02-20 21:38:32,465 DEBUG TRAIN Batch 11/4800 loss 96.681854 loss_att 114.442162 loss_ctc 102.953178 loss_rnnt 92.074333 hw_loss 0.411157 lr 0.00051526 rank 3
2023-02-20 21:38:32,474 DEBUG TRAIN Batch 11/4800 loss 17.574177 loss_att 29.117897 loss_ctc 16.692255 loss_rnnt 15.166098 hw_loss 0.406730 lr 0.00051526 rank 7
2023-02-20 21:38:32,481 DEBUG TRAIN Batch 11/4800 loss 51.653160 loss_att 102.826721 loss_ctc 58.863556 loss_rnnt 40.451466 hw_loss 0.010491 lr 0.00051526 rank 4
2023-02-20 21:38:32,530 DEBUG TRAIN Batch 11/4800 loss 25.807617 loss_att 57.975838 loss_ctc 24.377207 loss_rnnt 19.559097 hw_loss 0.010491 lr 0.00051526 rank 1
2023-02-20 21:39:29,738 DEBUG TRAIN Batch 11/4900 loss 22.048664 loss_att 30.387915 loss_ctc 23.053404 loss_rnnt 20.006805 hw_loss 0.450082 lr 0.00051512 rank 0
2023-02-20 21:39:29,745 DEBUG TRAIN Batch 11/4900 loss 19.924009 loss_att 33.488117 loss_ctc 20.764538 loss_rnnt 16.903471 hw_loss 0.366837 lr 0.00051512 rank 6
2023-02-20 21:39:29,750 DEBUG TRAIN Batch 11/4900 loss 50.287754 loss_att 59.725346 loss_ctc 46.790688 loss_rnnt 48.686783 hw_loss 0.336994 lr 0.00051512 rank 5
2023-02-20 21:39:29,750 DEBUG TRAIN Batch 11/4900 loss 20.702782 loss_att 20.029755 loss_ctc 22.361364 loss_rnnt 20.406958 hw_loss 0.392409 lr 0.00051512 rank 2
2023-02-20 21:39:29,752 DEBUG TRAIN Batch 11/4900 loss 20.083935 loss_att 19.555288 loss_ctc 22.288040 loss_rnnt 19.580641 hw_loss 0.590897 lr 0.00051512 rank 3
2023-02-20 21:39:29,757 DEBUG TRAIN Batch 11/4900 loss 23.756542 loss_att 26.742390 loss_ctc 18.040031 loss_rnnt 23.918310 hw_loss 0.006122 lr 0.00051512 rank 4
2023-02-20 21:39:29,766 DEBUG TRAIN Batch 11/4900 loss 23.476032 loss_att 25.167524 loss_ctc 22.291325 loss_rnnt 23.196503 hw_loss 0.185987 lr 0.00051512 rank 7
2023-02-20 21:39:29,805 DEBUG TRAIN Batch 11/4900 loss 25.443069 loss_att 38.110756 loss_ctc 26.184700 loss_rnnt 22.643631 hw_loss 0.313159 lr 0.00051512 rank 1
2023-02-20 21:40:28,557 DEBUG TRAIN Batch 11/5000 loss 23.616480 loss_att 31.798306 loss_ctc 21.186663 loss_rnnt 22.056412 hw_loss 0.464398 lr 0.00051499 rank 0
2023-02-20 21:40:28,563 DEBUG TRAIN Batch 11/5000 loss 64.195633 loss_att 87.166832 loss_ctc 72.712029 loss_rnnt 58.271183 hw_loss 0.365044 lr 0.00051499 rank 2
2023-02-20 21:40:28,565 DEBUG TRAIN Batch 11/5000 loss 22.591253 loss_att 31.188015 loss_ctc 27.285543 loss_rnnt 20.093723 hw_loss 0.285508 lr 0.00051499 rank 6
2023-02-20 21:40:28,567 DEBUG TRAIN Batch 11/5000 loss 41.534023 loss_att 51.651089 loss_ctc 40.432198 loss_rnnt 39.388107 hw_loss 0.505152 lr 0.00051499 rank 1
2023-02-20 21:40:28,569 DEBUG TRAIN Batch 11/5000 loss 9.811194 loss_att 14.783529 loss_ctc 10.231572 loss_rnnt 8.629187 hw_loss 0.246543 lr 0.00051499 rank 4
2023-02-20 21:40:28,571 DEBUG TRAIN Batch 11/5000 loss 40.072556 loss_att 51.674088 loss_ctc 45.173641 loss_rnnt 36.976555 hw_loss 0.179159 lr 0.00051499 rank 3
2023-02-20 21:40:28,575 DEBUG TRAIN Batch 11/5000 loss 19.123146 loss_att 23.881321 loss_ctc 17.635405 loss_rnnt 18.312733 hw_loss 0.107145 lr 0.00051499 rank 7
2023-02-20 21:40:28,629 DEBUG TRAIN Batch 11/5000 loss 35.901722 loss_att 53.752083 loss_ctc 38.160534 loss_rnnt 31.951593 hw_loss 0.147906 lr 0.00051499 rank 5
2023-02-20 21:41:27,689 DEBUG TRAIN Batch 11/5100 loss 17.961861 loss_att 30.400089 loss_ctc 16.583637 loss_rnnt 15.536827 hw_loss 0.227158 lr 0.00051485 rank 0
2023-02-20 21:41:27,692 DEBUG TRAIN Batch 11/5100 loss 28.161461 loss_att 43.820026 loss_ctc 37.809689 loss_rnnt 23.583935 hw_loss 0.298842 lr 0.00051485 rank 6
2023-02-20 21:41:27,695 DEBUG TRAIN Batch 11/5100 loss 54.391830 loss_att 74.647560 loss_ctc 51.367416 loss_rnnt 50.640339 hw_loss 0.194249 lr 0.00051485 rank 3
2023-02-20 21:41:27,696 DEBUG TRAIN Batch 11/5100 loss 27.944149 loss_att 41.244652 loss_ctc 20.328012 loss_rnnt 26.115429 hw_loss 0.345200 lr 0.00051485 rank 5
2023-02-20 21:41:27,696 DEBUG TRAIN Batch 11/5100 loss 40.439827 loss_att 59.749020 loss_ctc 47.276314 loss_rnnt 35.579025 hw_loss 0.163934 lr 0.00051485 rank 2
2023-02-20 21:41:27,699 DEBUG TRAIN Batch 11/5100 loss 37.089680 loss_att 50.527412 loss_ctc 42.320328 loss_rnnt 33.458614 hw_loss 0.461431 lr 0.00051485 rank 1
2023-02-20 21:41:27,702 DEBUG TRAIN Batch 11/5100 loss 24.531969 loss_att 33.455925 loss_ctc 35.030483 loss_rnnt 21.114845 hw_loss 0.435996 lr 0.00051485 rank 4
2023-02-20 21:41:27,768 DEBUG TRAIN Batch 11/5100 loss 31.568247 loss_att 43.266808 loss_ctc 32.635765 loss_rnnt 28.823435 hw_loss 0.492681 lr 0.00051485 rank 7
2023-02-20 21:42:47,577 DEBUG TRAIN Batch 11/5200 loss 28.997046 loss_att 37.845661 loss_ctc 37.244614 loss_rnnt 25.938095 hw_loss 0.355411 lr 0.00051471 rank 0
2023-02-20 21:42:47,588 DEBUG TRAIN Batch 11/5200 loss 40.640713 loss_att 54.930008 loss_ctc 41.740414 loss_rnnt 37.424046 hw_loss 0.397833 lr 0.00051471 rank 1
2023-02-20 21:42:47,588 DEBUG TRAIN Batch 11/5200 loss 18.881540 loss_att 24.437614 loss_ctc 24.521149 loss_rnnt 16.945837 hw_loss 0.136011 lr 0.00051471 rank 6
2023-02-20 21:42:47,589 DEBUG TRAIN Batch 11/5200 loss 35.938343 loss_att 49.032085 loss_ctc 39.732983 loss_rnnt 32.588211 hw_loss 0.422681 lr 0.00051471 rank 2
2023-02-20 21:42:47,591 DEBUG TRAIN Batch 11/5200 loss 32.010796 loss_att 39.962177 loss_ctc 35.147629 loss_rnnt 29.923113 hw_loss 0.148428 lr 0.00051471 rank 4
2023-02-20 21:42:47,593 DEBUG TRAIN Batch 11/5200 loss 18.729319 loss_att 22.999458 loss_ctc 18.901768 loss_rnnt 17.768856 hw_loss 0.156452 lr 0.00051471 rank 3
2023-02-20 21:42:47,603 DEBUG TRAIN Batch 11/5200 loss 32.093990 loss_att 29.104712 loss_ctc 28.417229 loss_rnnt 33.039944 hw_loss 0.266512 lr 0.00051471 rank 7
2023-02-20 21:42:47,634 DEBUG TRAIN Batch 11/5200 loss 34.623226 loss_att 54.585117 loss_ctc 35.679874 loss_rnnt 30.487312 hw_loss 0.004964 lr 0.00051471 rank 5
2023-02-20 21:43:46,678 DEBUG TRAIN Batch 11/5300 loss 90.625435 loss_att 117.958069 loss_ctc 104.593689 loss_rnnt 83.021286 hw_loss 0.515978 lr 0.00051458 rank 0
2023-02-20 21:43:46,686 DEBUG TRAIN Batch 11/5300 loss 29.871574 loss_att 44.050522 loss_ctc 32.986961 loss_rnnt 26.418800 hw_loss 0.377997 lr 0.00051458 rank 6
2023-02-20 21:43:46,690 DEBUG TRAIN Batch 11/5300 loss 78.612701 loss_att 81.031265 loss_ctc 80.203903 loss_rnnt 77.813622 hw_loss 0.193510 lr 0.00051458 rank 3
2023-02-20 21:43:46,693 DEBUG TRAIN Batch 11/5300 loss 34.645927 loss_att 53.414093 loss_ctc 41.269768 loss_rnnt 29.880894 hw_loss 0.240411 lr 0.00051458 rank 2
2023-02-20 21:43:46,697 DEBUG TRAIN Batch 11/5300 loss 19.867954 loss_att 35.699783 loss_ctc 17.043427 loss_rnnt 16.940931 hw_loss 0.257365 lr 0.00051458 rank 7
2023-02-20 21:43:46,697 DEBUG TRAIN Batch 11/5300 loss 76.915344 loss_att 98.349510 loss_ctc 74.582230 loss_rnnt 72.846191 hw_loss 0.175141 lr 0.00051458 rank 1
2023-02-20 21:43:46,703 DEBUG TRAIN Batch 11/5300 loss 15.762877 loss_att 33.014202 loss_ctc 14.109607 loss_rnnt 12.361582 hw_loss 0.321498 lr 0.00051458 rank 4
2023-02-20 21:43:46,752 DEBUG TRAIN Batch 11/5300 loss 20.088451 loss_att 36.727032 loss_ctc 21.482414 loss_rnnt 16.444118 hw_loss 0.245162 lr 0.00051458 rank 5
2023-02-20 21:44:44,298 DEBUG TRAIN Batch 11/5400 loss 29.120773 loss_att 42.035614 loss_ctc 28.368462 loss_rnnt 26.634962 hw_loss 0.005911 lr 0.00051444 rank 0
2023-02-20 21:44:44,307 DEBUG TRAIN Batch 11/5400 loss 6.575858 loss_att 18.100224 loss_ctc 6.347101 loss_rnnt 4.200242 hw_loss 0.189831 lr 0.00051444 rank 3
2023-02-20 21:44:44,308 DEBUG TRAIN Batch 11/5400 loss 27.596699 loss_att 68.812828 loss_ctc 13.715508 loss_rnnt 21.201145 hw_loss 0.005911 lr 0.00051444 rank 5
2023-02-20 21:44:44,310 DEBUG TRAIN Batch 11/5400 loss 87.689491 loss_att 117.593918 loss_ctc 99.841431 loss_rnnt 79.935516 hw_loss 0.286535 lr 0.00051444 rank 2
2023-02-20 21:44:44,310 DEBUG TRAIN Batch 11/5400 loss 74.579460 loss_att 101.204254 loss_ctc 79.743576 loss_rnnt 68.360451 hw_loss 0.385304 lr 0.00051444 rank 6
2023-02-20 21:44:44,313 DEBUG TRAIN Batch 11/5400 loss 61.735352 loss_att 91.876068 loss_ctc 56.016708 loss_rnnt 56.353359 hw_loss 0.218134 lr 0.00051444 rank 7
2023-02-20 21:44:44,312 DEBUG TRAIN Batch 11/5400 loss 13.009692 loss_att 22.033249 loss_ctc 15.565774 loss_rnnt 10.861018 hw_loss 0.005911 lr 0.00051444 rank 1
2023-02-20 21:44:44,315 DEBUG TRAIN Batch 11/5400 loss 19.131481 loss_att 27.320580 loss_ctc 16.970087 loss_rnnt 17.778694 hw_loss 0.005911 lr 0.00051444 rank 4
2023-02-20 21:45:41,930 DEBUG TRAIN Batch 11/5500 loss 59.569584 loss_att 53.877243 loss_ctc 68.166214 loss_rnnt 59.421467 hw_loss 0.263183 lr 0.00051430 rank 0
2023-02-20 21:45:41,937 DEBUG TRAIN Batch 11/5500 loss 31.217382 loss_att 34.308521 loss_ctc 32.724403 loss_rnnt 30.214588 hw_loss 0.344307 lr 0.00051430 rank 6
2023-02-20 21:45:41,938 DEBUG TRAIN Batch 11/5500 loss 27.072119 loss_att 32.904240 loss_ctc 30.733803 loss_rnnt 25.267012 hw_loss 0.282113 lr 0.00051430 rank 7
2023-02-20 21:45:41,941 DEBUG TRAIN Batch 11/5500 loss 21.331758 loss_att 21.310425 loss_ctc 21.307053 loss_rnnt 21.160994 hw_loss 0.334364 lr 0.00051430 rank 5
2023-02-20 21:45:41,943 DEBUG TRAIN Batch 11/5500 loss 18.073111 loss_att 28.466976 loss_ctc 27.386263 loss_rnnt 14.750708 hw_loss 0.003521 lr 0.00051430 rank 4
2023-02-20 21:45:41,943 DEBUG TRAIN Batch 11/5500 loss 27.446260 loss_att 36.243320 loss_ctc 30.570602 loss_rnnt 25.004501 hw_loss 0.498312 lr 0.00051430 rank 2
2023-02-20 21:45:41,947 DEBUG TRAIN Batch 11/5500 loss 44.992619 loss_att 39.248161 loss_ctc 47.362156 loss_rnnt 45.598633 hw_loss 0.425522 lr 0.00051430 rank 3
2023-02-20 21:45:41,957 DEBUG TRAIN Batch 11/5500 loss 44.208805 loss_att 56.028202 loss_ctc 53.915802 loss_rnnt 40.386772 hw_loss 0.307283 lr 0.00051430 rank 1
2023-02-20 21:46:40,858 DEBUG TRAIN Batch 11/5600 loss 37.292099 loss_att 47.720131 loss_ctc 42.763931 loss_rnnt 34.281143 hw_loss 0.367072 lr 0.00051417 rank 0
2023-02-20 21:46:40,868 DEBUG TRAIN Batch 11/5600 loss 26.054674 loss_att 45.281532 loss_ctc 31.770996 loss_rnnt 21.250202 hw_loss 0.369228 lr 0.00051417 rank 2
2023-02-20 21:46:40,870 DEBUG TRAIN Batch 11/5600 loss 18.819576 loss_att 34.721916 loss_ctc 15.951315 loss_rnnt 15.773507 hw_loss 0.465064 lr 0.00051417 rank 5
2023-02-20 21:46:40,873 DEBUG TRAIN Batch 11/5600 loss 55.726856 loss_att 67.725128 loss_ctc 56.111996 loss_rnnt 53.100487 hw_loss 0.328819 lr 0.00051417 rank 4
2023-02-20 21:46:40,873 DEBUG TRAIN Batch 11/5600 loss 16.040737 loss_att 25.415977 loss_ctc 18.094500 loss_rnnt 13.771704 hw_loss 0.225280 lr 0.00051417 rank 3
2023-02-20 21:46:40,877 DEBUG TRAIN Batch 11/5600 loss 20.610310 loss_att 35.980083 loss_ctc 19.198921 loss_rnnt 17.723919 hw_loss 0.001162 lr 0.00051417 rank 7
2023-02-20 21:46:40,884 DEBUG TRAIN Batch 11/5600 loss 30.120312 loss_att 41.221622 loss_ctc 33.724926 loss_rnnt 27.221466 hw_loss 0.371188 lr 0.00051417 rank 6
2023-02-20 21:46:40,938 DEBUG TRAIN Batch 11/5600 loss 8.931330 loss_att 18.198189 loss_ctc 9.086685 loss_rnnt 6.986741 hw_loss 0.132194 lr 0.00051417 rank 1
2023-02-20 21:47:38,163 DEBUG TRAIN Batch 11/5700 loss 37.323009 loss_att 44.773201 loss_ctc 35.438923 loss_rnnt 35.894394 hw_loss 0.355857 lr 0.00051403 rank 0
2023-02-20 21:47:38,165 DEBUG TRAIN Batch 11/5700 loss 27.707985 loss_att 52.144020 loss_ctc 27.258440 loss_rnnt 22.709583 hw_loss 0.320872 lr 0.00051403 rank 2
2023-02-20 21:47:38,168 DEBUG TRAIN Batch 11/5700 loss 24.412361 loss_att 33.699089 loss_ctc 13.136785 loss_rnnt 23.751190 hw_loss 0.576067 lr 0.00051403 rank 1
2023-02-20 21:47:38,170 DEBUG TRAIN Batch 11/5700 loss 37.333160 loss_att 59.649864 loss_ctc 47.861675 loss_rnnt 31.310127 hw_loss 0.292287 lr 0.00051403 rank 5
2023-02-20 21:47:38,175 DEBUG TRAIN Batch 11/5700 loss 76.045509 loss_att 93.916557 loss_ctc 59.371735 loss_rnnt 74.498352 hw_loss 0.367724 lr 0.00051403 rank 3
2023-02-20 21:47:38,181 DEBUG TRAIN Batch 11/5700 loss 15.898954 loss_att 24.555649 loss_ctc 11.968559 loss_rnnt 14.638790 hw_loss 0.099145 lr 0.00051403 rank 4
2023-02-20 21:47:38,187 DEBUG TRAIN Batch 11/5700 loss 5.991621 loss_att 21.182919 loss_ctc 0.903837 loss_rnnt 3.417390 hw_loss 0.401894 lr 0.00051403 rank 7
2023-02-20 21:47:38,233 DEBUG TRAIN Batch 11/5700 loss 21.427696 loss_att 42.791420 loss_ctc 15.732247 loss_rnnt 17.913689 hw_loss 0.001228 lr 0.00051403 rank 6
2023-02-20 21:48:34,752 DEBUG TRAIN Batch 11/5800 loss 20.733713 loss_att 32.256660 loss_ctc 18.360601 loss_rnnt 18.591259 hw_loss 0.289274 lr 0.00051390 rank 0
2023-02-20 21:48:34,758 DEBUG TRAIN Batch 11/5800 loss 21.867552 loss_att 41.740952 loss_ctc 24.417744 loss_rnnt 17.258389 hw_loss 0.552107 lr 0.00051390 rank 2
2023-02-20 21:48:34,763 DEBUG TRAIN Batch 11/5800 loss 12.603011 loss_att 19.632572 loss_ctc 17.565296 loss_rnnt 10.348999 hw_loss 0.349615 lr 0.00051390 rank 1
2023-02-20 21:48:34,763 DEBUG TRAIN Batch 11/5800 loss 47.939964 loss_att 56.133087 loss_ctc 58.254112 loss_rnnt 44.645195 hw_loss 0.526737 lr 0.00051390 rank 5
2023-02-20 21:48:34,764 DEBUG TRAIN Batch 11/5800 loss 12.995173 loss_att 25.781847 loss_ctc 21.331961 loss_rnnt 9.120232 hw_loss 0.386314 lr 0.00051390 rank 3
2023-02-20 21:48:34,765 DEBUG TRAIN Batch 11/5800 loss 38.780922 loss_att 49.478851 loss_ctc 47.065468 loss_rnnt 35.461365 hw_loss 0.141304 lr 0.00051390 rank 6
2023-02-20 21:48:34,766 DEBUG TRAIN Batch 11/5800 loss 31.698210 loss_att 42.292572 loss_ctc 29.757807 loss_rnnt 29.769722 hw_loss 0.128129 lr 0.00051390 rank 4
2023-02-20 21:48:34,769 DEBUG TRAIN Batch 11/5800 loss 17.598024 loss_att 26.864868 loss_ctc 19.771927 loss_rnnt 15.356688 hw_loss 0.183963 lr 0.00051390 rank 7
2023-02-20 21:49:34,335 DEBUG TRAIN Batch 11/5900 loss 10.506174 loss_att 23.905289 loss_ctc 15.655464 loss_rnnt 7.138790 hw_loss 0.001854 lr 0.00051376 rank 0
2023-02-20 21:49:34,344 DEBUG TRAIN Batch 11/5900 loss 23.713959 loss_att 41.927029 loss_ctc 30.491642 loss_rnnt 19.024343 hw_loss 0.268708 lr 0.00051376 rank 3
2023-02-20 21:49:34,346 DEBUG TRAIN Batch 11/5900 loss 32.508469 loss_att 43.312996 loss_ctc 31.882313 loss_rnnt 30.320591 hw_loss 0.207107 lr 0.00051376 rank 2
2023-02-20 21:49:34,351 DEBUG TRAIN Batch 11/5900 loss 33.478832 loss_att 46.424957 loss_ctc 41.794868 loss_rnnt 29.728115 hw_loss 0.098793 lr 0.00051376 rank 6
2023-02-20 21:49:34,351 DEBUG TRAIN Batch 11/5900 loss 24.660746 loss_att 35.916344 loss_ctc 24.204197 loss_rnnt 22.311214 hw_loss 0.298664 lr 0.00051376 rank 7
2023-02-20 21:49:34,352 DEBUG TRAIN Batch 11/5900 loss 30.321705 loss_att 45.659641 loss_ctc 30.090820 loss_rnnt 27.225311 hw_loss 0.111733 lr 0.00051376 rank 1
2023-02-20 21:49:34,356 DEBUG TRAIN Batch 11/5900 loss 54.174522 loss_att 69.856956 loss_ctc 57.960548 loss_rnnt 50.333900 hw_loss 0.373740 lr 0.00051376 rank 5
2023-02-20 21:49:34,357 DEBUG TRAIN Batch 11/5900 loss 32.671272 loss_att 32.669273 loss_ctc 33.025047 loss_rnnt 32.623516 hw_loss 0.001854 lr 0.00051376 rank 4
2023-02-20 21:50:33,936 DEBUG TRAIN Batch 11/6000 loss 10.467665 loss_att 17.326967 loss_ctc 3.142454 loss_rnnt 9.866840 hw_loss 0.385609 lr 0.00051362 rank 0
2023-02-20 21:50:33,936 DEBUG TRAIN Batch 11/6000 loss 22.101831 loss_att 36.406372 loss_ctc 20.438501 loss_rnnt 19.255333 hw_loss 0.388811 lr 0.00051362 rank 1
2023-02-20 21:50:33,937 DEBUG TRAIN Batch 11/6000 loss 42.522610 loss_att 43.048363 loss_ctc 42.346130 loss_rnnt 42.192513 hw_loss 0.465893 lr 0.00051362 rank 3
2023-02-20 21:50:33,937 DEBUG TRAIN Batch 11/6000 loss 23.922993 loss_att 48.203701 loss_ctc 19.159332 loss_rnnt 19.518082 hw_loss 0.344857 lr 0.00051362 rank 2
2023-02-20 21:50:33,938 DEBUG TRAIN Batch 11/6000 loss 29.229254 loss_att 55.381905 loss_ctc 36.640614 loss_rnnt 22.755535 hw_loss 0.478138 lr 0.00051362 rank 5
2023-02-20 21:50:33,942 DEBUG TRAIN Batch 11/6000 loss 23.347361 loss_att 35.499741 loss_ctc 25.949125 loss_rnnt 20.332769 hw_loss 0.444775 lr 0.00051362 rank 7
2023-02-20 21:50:33,942 DEBUG TRAIN Batch 11/6000 loss 32.969467 loss_att 41.087601 loss_ctc 25.399519 loss_rnnt 32.164146 hw_loss 0.358165 lr 0.00051362 rank 6
2023-02-20 21:50:33,947 DEBUG TRAIN Batch 11/6000 loss 24.432114 loss_att 36.447201 loss_ctc 25.925135 loss_rnnt 21.734221 hw_loss 0.179634 lr 0.00051362 rank 4
2023-02-20 21:51:54,376 DEBUG TRAIN Batch 11/6100 loss 16.953705 loss_att 27.299782 loss_ctc 16.993126 loss_rnnt 14.730242 hw_loss 0.279360 lr 0.00051349 rank 5
2023-02-20 21:51:54,381 DEBUG TRAIN Batch 11/6100 loss 35.044231 loss_att 51.850388 loss_ctc 34.464703 loss_rnnt 31.628872 hw_loss 0.246375 lr 0.00051349 rank 3
2023-02-20 21:51:54,381 DEBUG TRAIN Batch 11/6100 loss 14.718273 loss_att 22.745514 loss_ctc 18.970161 loss_rnnt 12.431298 hw_loss 0.214889 lr 0.00051349 rank 4
2023-02-20 21:51:54,382 DEBUG TRAIN Batch 11/6100 loss 23.529190 loss_att 30.062969 loss_ctc 26.475075 loss_rnnt 21.688904 hw_loss 0.263903 lr 0.00051349 rank 0
2023-02-20 21:51:54,383 DEBUG TRAIN Batch 11/6100 loss 45.346912 loss_att 52.451195 loss_ctc 57.013275 loss_rnnt 42.196289 hw_loss 0.326715 lr 0.00051349 rank 1
2023-02-20 21:51:54,388 DEBUG TRAIN Batch 11/6100 loss 55.334225 loss_att 59.538456 loss_ctc 54.909164 loss_rnnt 54.386734 hw_loss 0.306219 lr 0.00051349 rank 6
2023-02-20 21:51:54,389 DEBUG TRAIN Batch 11/6100 loss 27.873934 loss_att 41.512505 loss_ctc 22.948706 loss_rnnt 25.718786 hw_loss 0.157746 lr 0.00051349 rank 2
2023-02-20 21:51:54,396 DEBUG TRAIN Batch 11/6100 loss 40.933556 loss_att 50.390850 loss_ctc 48.182289 loss_rnnt 37.909554 hw_loss 0.311333 lr 0.00051349 rank 7
2023-02-20 21:52:55,352 DEBUG TRAIN Batch 11/6200 loss 32.723602 loss_att 32.696804 loss_ctc 33.172951 loss_rnnt 32.434879 hw_loss 0.439058 lr 0.00051335 rank 0
2023-02-20 21:52:55,354 DEBUG TRAIN Batch 11/6200 loss 17.900698 loss_att 27.825750 loss_ctc 17.475248 loss_rnnt 15.859319 hw_loss 0.212048 lr 0.00051335 rank 6
2023-02-20 21:52:55,355 DEBUG TRAIN Batch 11/6200 loss 18.898842 loss_att 31.124943 loss_ctc 22.689480 loss_rnnt 15.781352 hw_loss 0.312847 lr 0.00051335 rank 3
2023-02-20 21:52:55,363 DEBUG TRAIN Batch 11/6200 loss 32.849953 loss_att 43.431541 loss_ctc 35.046051 loss_rnnt 30.192757 hw_loss 0.465129 lr 0.00051335 rank 5
2023-02-20 21:52:55,364 DEBUG TRAIN Batch 11/6200 loss 20.395285 loss_att 25.052919 loss_ctc 22.247128 loss_rnnt 19.146591 hw_loss 0.131725 lr 0.00051335 rank 2
2023-02-20 21:52:55,373 DEBUG TRAIN Batch 11/6200 loss 19.328014 loss_att 28.978729 loss_ctc 21.411777 loss_rnnt 17.016203 hw_loss 0.194687 lr 0.00051335 rank 4
2023-02-20 21:52:55,377 DEBUG TRAIN Batch 11/6200 loss 12.763949 loss_att 28.767090 loss_ctc 10.513792 loss_rnnt 9.673134 hw_loss 0.356640 lr 0.00051335 rank 7
2023-02-20 21:52:55,427 DEBUG TRAIN Batch 11/6200 loss 19.691822 loss_att 33.925774 loss_ctc 21.572405 loss_rnnt 16.504772 hw_loss 0.167839 lr 0.00051335 rank 1
2023-02-20 21:53:56,472 DEBUG TRAIN Batch 11/6300 loss 73.091034 loss_att 76.982437 loss_ctc 72.138962 loss_rnnt 72.169586 hw_loss 0.506458 lr 0.00051322 rank 3
2023-02-20 21:53:56,472 DEBUG TRAIN Batch 11/6300 loss 27.412094 loss_att 46.507217 loss_ctc 32.754059 loss_rnnt 22.724121 hw_loss 0.293790 lr 0.00051322 rank 0
2023-02-20 21:53:56,475 DEBUG TRAIN Batch 11/6300 loss 55.375927 loss_att 78.138863 loss_ctc 60.273605 loss_rnnt 49.985237 hw_loss 0.347025 lr 0.00051322 rank 2
2023-02-20 21:53:56,479 DEBUG TRAIN Batch 11/6300 loss 71.452370 loss_att 73.830605 loss_ctc 78.211472 loss_rnnt 69.990471 hw_loss 0.159461 lr 0.00051322 rank 5
2023-02-20 21:53:56,481 DEBUG TRAIN Batch 11/6300 loss 9.693254 loss_att 20.200687 loss_ctc 3.496502 loss_rnnt 8.204452 hw_loss 0.400405 lr 0.00051322 rank 6
2023-02-20 21:53:56,483 DEBUG TRAIN Batch 11/6300 loss 17.357632 loss_att 21.839127 loss_ctc 17.614315 loss_rnnt 16.209841 hw_loss 0.407377 lr 0.00051322 rank 7
2023-02-20 21:53:56,483 DEBUG TRAIN Batch 11/6300 loss 46.062210 loss_att 66.164261 loss_ctc 39.236645 loss_rnnt 42.868019 hw_loss 0.157218 lr 0.00051322 rank 1
2023-02-20 21:53:56,540 DEBUG TRAIN Batch 11/6300 loss 21.149330 loss_att 29.348850 loss_ctc 14.844317 loss_rnnt 20.072184 hw_loss 0.521080 lr 0.00051322 rank 4
2023-02-20 21:54:54,315 DEBUG TRAIN Batch 11/6400 loss 44.617264 loss_att 53.689404 loss_ctc 49.867409 loss_rnnt 41.962696 hw_loss 0.262730 lr 0.00051308 rank 0
2023-02-20 21:54:54,320 DEBUG TRAIN Batch 11/6400 loss 22.462236 loss_att 33.485439 loss_ctc 28.932892 loss_rnnt 19.232204 hw_loss 0.304947 lr 0.00051308 rank 2
2023-02-20 21:54:54,323 DEBUG TRAIN Batch 11/6400 loss 26.874102 loss_att 30.605589 loss_ctc 24.869732 loss_rnnt 26.182735 hw_loss 0.398097 lr 0.00051308 rank 6
2023-02-20 21:54:54,325 DEBUG TRAIN Batch 11/6400 loss 28.108356 loss_att 32.045845 loss_ctc 24.147659 loss_rnnt 27.737085 hw_loss 0.209747 lr 0.00051308 rank 7
2023-02-20 21:54:54,328 DEBUG TRAIN Batch 11/6400 loss 18.490175 loss_att 21.276375 loss_ctc 18.099575 loss_rnnt 17.772324 hw_loss 0.398800 lr 0.00051308 rank 1
2023-02-20 21:54:54,328 DEBUG TRAIN Batch 11/6400 loss 31.362244 loss_att 37.609711 loss_ctc 26.986532 loss_rnnt 30.565214 hw_loss 0.245563 lr 0.00051308 rank 3
2023-02-20 21:54:54,333 DEBUG TRAIN Batch 11/6400 loss 28.437725 loss_att 33.429436 loss_ctc 33.029682 loss_rnnt 26.724331 hw_loss 0.192733 lr 0.00051308 rank 4
2023-02-20 21:54:54,387 DEBUG TRAIN Batch 11/6400 loss 26.877014 loss_att 34.894283 loss_ctc 27.464705 loss_rnnt 25.069361 hw_loss 0.235947 lr 0.00051308 rank 5
2023-02-20 21:55:55,228 DEBUG TRAIN Batch 11/6500 loss 19.677765 loss_att 21.425486 loss_ctc 18.606760 loss_rnnt 19.470333 hw_loss 0.001288 lr 0.00051295 rank 0
2023-02-20 21:55:55,236 DEBUG TRAIN Batch 11/6500 loss 29.188828 loss_att 42.654865 loss_ctc 30.815792 loss_rnnt 26.170036 hw_loss 0.203726 lr 0.00051295 rank 2
2023-02-20 21:55:55,236 DEBUG TRAIN Batch 11/6500 loss 11.492090 loss_att 14.806321 loss_ctc 13.104196 loss_rnnt 10.331183 hw_loss 0.530836 lr 0.00051295 rank 3
2023-02-20 21:55:55,238 DEBUG TRAIN Batch 11/6500 loss 33.804413 loss_att 34.822914 loss_ctc 37.680866 loss_rnnt 32.804657 hw_loss 0.523487 lr 0.00051295 rank 4
2023-02-20 21:55:55,240 DEBUG TRAIN Batch 11/6500 loss 25.677296 loss_att 44.496437 loss_ctc 30.297396 loss_rnnt 21.296768 hw_loss 0.001287 lr 0.00051295 rank 5
2023-02-20 21:55:55,240 DEBUG TRAIN Batch 11/6500 loss 23.213573 loss_att 26.928381 loss_ctc 20.099615 loss_rnnt 22.789162 hw_loss 0.181210 lr 0.00051295 rank 6
2023-02-20 21:55:55,244 DEBUG TRAIN Batch 11/6500 loss 17.690632 loss_att 26.657850 loss_ctc 13.952353 loss_rnnt 16.324486 hw_loss 0.133388 lr 0.00051295 rank 7
2023-02-20 21:55:55,299 DEBUG TRAIN Batch 11/6500 loss 29.650635 loss_att 42.352776 loss_ctc 26.406006 loss_rnnt 27.542137 hw_loss 0.001287 lr 0.00051295 rank 1
2023-02-20 21:56:51,894 DEBUG TRAIN Batch 11/6600 loss 15.465047 loss_att 19.667501 loss_ctc 16.921795 loss_rnnt 14.217666 hw_loss 0.398734 lr 0.00051281 rank 0
2023-02-20 21:56:51,904 DEBUG TRAIN Batch 11/6600 loss 14.367229 loss_att 28.942942 loss_ctc 13.890428 loss_rnnt 11.413530 hw_loss 0.191491 lr 0.00051281 rank 6
2023-02-20 21:56:51,906 DEBUG TRAIN Batch 11/6600 loss 12.405122 loss_att 22.556946 loss_ctc 8.459828 loss_rnnt 10.900536 hw_loss 0.000488 lr 0.00051281 rank 2
2023-02-20 21:56:51,907 DEBUG TRAIN Batch 11/6600 loss 16.841858 loss_att 31.852654 loss_ctc 15.646536 loss_rnnt 13.903252 hw_loss 0.179667 lr 0.00051281 rank 4
2023-02-20 21:56:51,910 DEBUG TRAIN Batch 11/6600 loss 11.976779 loss_att 25.845285 loss_ctc 12.916229 loss_rnnt 8.960358 hw_loss 0.220236 lr 0.00051281 rank 5
2023-02-20 21:56:51,910 DEBUG TRAIN Batch 11/6600 loss 30.121367 loss_att 47.183125 loss_ctc 30.634550 loss_rnnt 26.595081 hw_loss 0.085323 lr 0.00051281 rank 3
2023-02-20 21:56:51,914 DEBUG TRAIN Batch 11/6600 loss 19.485992 loss_att 42.954132 loss_ctc 23.842999 loss_rnnt 13.972689 hw_loss 0.447633 lr 0.00051281 rank 7
2023-02-20 21:56:51,974 DEBUG TRAIN Batch 11/6600 loss 95.320335 loss_att 90.294113 loss_ctc 100.797173 loss_rnnt 95.595062 hw_loss 0.000488 lr 0.00051281 rank 1
2023-02-20 21:57:50,360 DEBUG TRAIN Batch 11/6700 loss 40.144642 loss_att 57.514885 loss_ctc 41.919426 loss_rnnt 36.334957 hw_loss 0.185625 lr 0.00051268 rank 0
2023-02-20 21:57:50,365 DEBUG TRAIN Batch 11/6700 loss 18.494253 loss_att 30.282059 loss_ctc 20.051743 loss_rnnt 15.809692 hw_loss 0.223749 lr 0.00051268 rank 2
2023-02-20 21:57:50,365 DEBUG TRAIN Batch 11/6700 loss 13.875227 loss_att 29.877640 loss_ctc 14.164268 loss_rnnt 10.561217 hw_loss 0.140606 lr 0.00051268 rank 1
2023-02-20 21:57:50,366 DEBUG TRAIN Batch 11/6700 loss 31.089376 loss_att 43.855743 loss_ctc 38.747807 loss_rnnt 27.465351 hw_loss 0.093048 lr 0.00051268 rank 5
2023-02-20 21:57:50,370 DEBUG TRAIN Batch 11/6700 loss 33.487591 loss_att 42.305405 loss_ctc 36.976292 loss_rnnt 31.188774 hw_loss 0.131428 lr 0.00051268 rank 6
2023-02-20 21:57:50,370 DEBUG TRAIN Batch 11/6700 loss 20.413277 loss_att 34.892052 loss_ctc 15.370620 loss_rnnt 18.080593 hw_loss 0.204907 lr 0.00051268 rank 4
2023-02-20 21:57:50,370 DEBUG TRAIN Batch 11/6700 loss 31.552259 loss_att 52.109985 loss_ctc 27.156456 loss_rnnt 27.802376 hw_loss 0.420837 lr 0.00051268 rank 3
2023-02-20 21:57:50,437 DEBUG TRAIN Batch 11/6700 loss 11.656498 loss_att 21.815369 loss_ctc 16.108995 loss_rnnt 8.806140 hw_loss 0.421720 lr 0.00051268 rank 7
2023-02-20 21:58:50,811 DEBUG TRAIN Batch 11/6800 loss 35.879425 loss_att 40.065323 loss_ctc 38.407291 loss_rnnt 34.509678 hw_loss 0.366591 lr 0.00051254 rank 0
2023-02-20 21:58:50,818 DEBUG TRAIN Batch 11/6800 loss 22.710882 loss_att 32.240425 loss_ctc 20.486303 loss_rnnt 20.994930 hw_loss 0.199975 lr 0.00051254 rank 4
2023-02-20 21:58:50,819 DEBUG TRAIN Batch 11/6800 loss 23.662418 loss_att 33.660233 loss_ctc 26.802448 loss_rnnt 21.240574 hw_loss 0.006770 lr 0.00051254 rank 5
2023-02-20 21:58:50,820 DEBUG TRAIN Batch 11/6800 loss 35.847168 loss_att 54.579254 loss_ctc 50.574993 loss_rnnt 29.962727 hw_loss 0.326835 lr 0.00051254 rank 3
2023-02-20 21:58:50,820 DEBUG TRAIN Batch 11/6800 loss 22.382345 loss_att 29.866777 loss_ctc 26.695986 loss_rnnt 20.306694 hw_loss 0.006770 lr 0.00051254 rank 6
2023-02-20 21:58:50,822 DEBUG TRAIN Batch 11/6800 loss 47.008274 loss_att 59.665489 loss_ctc 39.555099 loss_rnnt 45.370815 hw_loss 0.187083 lr 0.00051254 rank 2
2023-02-20 21:58:50,824 DEBUG TRAIN Batch 11/6800 loss 37.649586 loss_att 58.540443 loss_ctc 44.732880 loss_rnnt 32.425720 hw_loss 0.189850 lr 0.00051254 rank 7
2023-02-20 21:58:50,824 DEBUG TRAIN Batch 11/6800 loss 38.858883 loss_att 37.175079 loss_ctc 44.061176 loss_rnnt 38.328110 hw_loss 0.326050 lr 0.00051254 rank 1
2023-02-20 22:00:07,808 DEBUG TRAIN Batch 11/6900 loss 20.644915 loss_att 35.823940 loss_ctc 20.935184 loss_rnnt 17.568554 hw_loss 0.003476 lr 0.00051241 rank 2
2023-02-20 22:00:07,812 DEBUG TRAIN Batch 11/6900 loss 10.073135 loss_att 11.104078 loss_ctc 10.293669 loss_rnnt 9.502192 hw_loss 0.628781 lr 0.00051241 rank 0
2023-02-20 22:00:07,813 DEBUG TRAIN Batch 11/6900 loss 11.059989 loss_att 16.615074 loss_ctc 15.682107 loss_rnnt 9.330836 hw_loss 0.003476 lr 0.00051241 rank 3
2023-02-20 22:00:07,815 DEBUG TRAIN Batch 11/6900 loss 20.308819 loss_att 33.518559 loss_ctc 17.934460 loss_rnnt 17.981598 hw_loss 0.003476 lr 0.00051241 rank 5
2023-02-20 22:00:07,817 DEBUG TRAIN Batch 11/6900 loss 14.724829 loss_att 29.811211 loss_ctc 15.386258 loss_rnnt 11.470469 hw_loss 0.279175 lr 0.00051241 rank 1
2023-02-20 22:00:07,817 DEBUG TRAIN Batch 11/6900 loss 30.485546 loss_att 48.913826 loss_ctc 33.554249 loss_rnnt 26.296366 hw_loss 0.176931 lr 0.00051241 rank 7
2023-02-20 22:00:07,818 DEBUG TRAIN Batch 11/6900 loss 17.803686 loss_att 37.306747 loss_ctc 19.379272 loss_rnnt 13.583665 hw_loss 0.204996 lr 0.00051241 rank 6
2023-02-20 22:00:07,821 DEBUG TRAIN Batch 11/6900 loss 18.326069 loss_att 27.582039 loss_ctc 24.207495 loss_rnnt 15.573565 hw_loss 0.219598 lr 0.00051241 rank 4
2023-02-20 22:01:06,811 DEBUG TRAIN Batch 11/7000 loss 33.973644 loss_att 40.059864 loss_ctc 40.566051 loss_rnnt 31.877167 hw_loss 0.000458 lr 0.00051228 rank 0
2023-02-20 22:01:06,812 DEBUG TRAIN Batch 11/7000 loss 30.780615 loss_att 47.873665 loss_ctc 33.685669 loss_rnnt 26.845745 hw_loss 0.241729 lr 0.00051228 rank 1
2023-02-20 22:01:06,815 DEBUG TRAIN Batch 11/7000 loss 27.177917 loss_att 38.605560 loss_ctc 28.312325 loss_rnnt 24.553814 hw_loss 0.351230 lr 0.00051228 rank 2
2023-02-20 22:01:06,819 DEBUG TRAIN Batch 11/7000 loss 38.443645 loss_att 44.110420 loss_ctc 42.709511 loss_rnnt 36.560562 hw_loss 0.339275 lr 0.00051228 rank 5
2023-02-20 22:01:06,820 DEBUG TRAIN Batch 11/7000 loss 17.883131 loss_att 28.339598 loss_ctc 22.293755 loss_rnnt 15.057856 hw_loss 0.273555 lr 0.00051228 rank 6
2023-02-20 22:01:06,822 DEBUG TRAIN Batch 11/7000 loss 45.385231 loss_att 50.419098 loss_ctc 50.504074 loss_rnnt 43.508728 hw_loss 0.351024 lr 0.00051228 rank 7
2023-02-20 22:01:06,824 DEBUG TRAIN Batch 11/7000 loss 18.033636 loss_att 41.670639 loss_ctc 19.972950 loss_rnnt 12.892969 hw_loss 0.290044 lr 0.00051228 rank 4
2023-02-20 22:01:06,827 DEBUG TRAIN Batch 11/7000 loss 38.510323 loss_att 44.006065 loss_ctc 42.793404 loss_rnnt 36.724918 hw_loss 0.215952 lr 0.00051228 rank 3
2023-02-20 22:02:06,726 DEBUG TRAIN Batch 11/7100 loss 33.806908 loss_att 36.534447 loss_ctc 37.113842 loss_rnnt 32.647354 hw_loss 0.324602 lr 0.00051214 rank 0
2023-02-20 22:02:06,734 DEBUG TRAIN Batch 11/7100 loss 17.197544 loss_att 31.172558 loss_ctc 19.012650 loss_rnnt 13.912666 hw_loss 0.464741 lr 0.00051214 rank 5
2023-02-20 22:02:06,734 DEBUG TRAIN Batch 11/7100 loss 77.340187 loss_att 105.162003 loss_ctc 103.877365 loss_rnnt 67.999512 hw_loss 0.446290 lr 0.00051214 rank 3
2023-02-20 22:02:06,735 DEBUG TRAIN Batch 11/7100 loss 8.058558 loss_att 15.339007 loss_ctc 5.160836 loss_rnnt 6.988424 hw_loss 0.000763 lr 0.00051214 rank 6
2023-02-20 22:02:06,736 DEBUG TRAIN Batch 11/7100 loss 7.841878 loss_att 23.323669 loss_ctc 4.999635 loss_rnnt 4.860866 hw_loss 0.494288 lr 0.00051214 rank 1
2023-02-20 22:02:06,736 DEBUG TRAIN Batch 11/7100 loss 31.327568 loss_att 29.476755 loss_ctc 29.981554 loss_rnnt 31.876791 hw_loss 0.000763 lr 0.00051214 rank 2
2023-02-20 22:02:06,736 DEBUG TRAIN Batch 11/7100 loss 66.663925 loss_att 75.953415 loss_ctc 61.332867 loss_rnnt 65.516434 hw_loss 0.000763 lr 0.00051214 rank 7
2023-02-20 22:02:06,736 DEBUG TRAIN Batch 11/7100 loss 13.991175 loss_att 33.305630 loss_ctc 16.324907 loss_rnnt 9.587470 hw_loss 0.430591 lr 0.00051214 rank 4
2023-02-20 22:03:03,277 DEBUG TRAIN Batch 11/7200 loss 17.185888 loss_att 20.419666 loss_ctc 22.020557 loss_rnnt 15.794083 hw_loss 0.188298 lr 0.00051201 rank 0
2023-02-20 22:03:03,283 DEBUG TRAIN Batch 11/7200 loss 48.881210 loss_att 61.457977 loss_ctc 48.158989 loss_rnnt 46.287750 hw_loss 0.327008 lr 0.00051201 rank 6
2023-02-20 22:03:03,285 DEBUG TRAIN Batch 11/7200 loss 13.616125 loss_att 23.490543 loss_ctc 18.700468 loss_rnnt 10.690463 hw_loss 0.511624 lr 0.00051201 rank 5
2023-02-20 22:03:03,286 DEBUG TRAIN Batch 11/7200 loss 52.704617 loss_att 55.445248 loss_ctc 56.370506 loss_rnnt 51.459381 hw_loss 0.390606 lr 0.00051201 rank 4
2023-02-20 22:03:03,287 DEBUG TRAIN Batch 11/7200 loss 36.777870 loss_att 50.984550 loss_ctc 39.670780 loss_rnnt 33.476940 hw_loss 0.138515 lr 0.00051201 rank 3
2023-02-20 22:03:03,288 DEBUG TRAIN Batch 11/7200 loss 17.460115 loss_att 21.525944 loss_ctc 20.361616 loss_rnnt 15.984171 hw_loss 0.517336 lr 0.00051201 rank 2
2023-02-20 22:03:03,295 DEBUG TRAIN Batch 11/7200 loss 33.718758 loss_att 42.952175 loss_ctc 31.491371 loss_rnnt 32.168659 hw_loss 0.000740 lr 0.00051201 rank 7
2023-02-20 22:03:03,346 DEBUG TRAIN Batch 11/7200 loss 36.140640 loss_att 45.449554 loss_ctc 41.893402 loss_rnnt 33.511425 hw_loss 0.000740 lr 0.00051201 rank 1
2023-02-20 22:04:02,663 DEBUG TRAIN Batch 11/7300 loss 43.895748 loss_att 55.490299 loss_ctc 48.947899 loss_rnnt 40.718678 hw_loss 0.346007 lr 0.00051187 rank 0
2023-02-20 22:04:02,667 DEBUG TRAIN Batch 11/7300 loss 23.152899 loss_att 39.269897 loss_ctc 29.569294 loss_rnnt 18.900984 hw_loss 0.324367 lr 0.00051187 rank 6
2023-02-20 22:04:02,670 DEBUG TRAIN Batch 11/7300 loss 17.958145 loss_att 20.891207 loss_ctc 21.374485 loss_rnnt 16.751959 hw_loss 0.307615 lr 0.00051187 rank 4
2023-02-20 22:04:02,673 DEBUG TRAIN Batch 11/7300 loss 18.167393 loss_att 30.217978 loss_ctc 23.306244 loss_rnnt 14.917993 hw_loss 0.288946 lr 0.00051187 rank 2
2023-02-20 22:04:02,674 DEBUG TRAIN Batch 11/7300 loss 31.250156 loss_att 45.102692 loss_ctc 45.395840 loss_rnnt 26.438057 hw_loss 0.291568 lr 0.00051187 rank 3
2023-02-20 22:04:02,675 DEBUG TRAIN Batch 11/7300 loss 31.367332 loss_att 45.271210 loss_ctc 31.072201 loss_rnnt 28.408667 hw_loss 0.407329 lr 0.00051187 rank 5
2023-02-20 22:04:02,681 DEBUG TRAIN Batch 11/7300 loss 5.783648 loss_att 15.406158 loss_ctc 3.319826 loss_rnnt 4.039611 hw_loss 0.277584 lr 0.00051187 rank 7
2023-02-20 22:04:02,729 DEBUG TRAIN Batch 11/7300 loss 25.590513 loss_att 36.751900 loss_ctc 23.728912 loss_rnnt 23.341106 hw_loss 0.497521 lr 0.00051187 rank 1
2023-02-20 22:05:02,077 DEBUG TRAIN Batch 11/7400 loss 14.079940 loss_att 19.442303 loss_ctc 12.525250 loss_rnnt 13.044803 hw_loss 0.318669 lr 0.00051174 rank 6
2023-02-20 22:05:02,080 DEBUG TRAIN Batch 11/7400 loss 25.118700 loss_att 33.425552 loss_ctc 26.004637 loss_rnnt 23.338478 hw_loss 0.001359 lr 0.00051174 rank 0
2023-02-20 22:05:02,080 DEBUG TRAIN Batch 11/7400 loss 17.519335 loss_att 29.319084 loss_ctc 16.246090 loss_rnnt 14.967861 hw_loss 0.677420 lr 0.00051174 rank 7
2023-02-20 22:05:02,081 DEBUG TRAIN Batch 11/7400 loss 13.356697 loss_att 16.443230 loss_ctc 18.349895 loss_rnnt 12.072906 hw_loss 0.001359 lr 0.00051174 rank 3
2023-02-20 22:05:02,081 DEBUG TRAIN Batch 11/7400 loss 15.425184 loss_att 32.412865 loss_ctc 15.506610 loss_rnnt 11.855556 hw_loss 0.302318 lr 0.00051174 rank 5
2023-02-20 22:05:02,085 DEBUG TRAIN Batch 11/7400 loss 23.169146 loss_att 28.440290 loss_ctc 27.075624 loss_rnnt 21.593328 hw_loss 0.001359 lr 0.00051174 rank 2
2023-02-20 22:05:02,090 DEBUG TRAIN Batch 11/7400 loss 16.526148 loss_att 26.152601 loss_ctc 18.922863 loss_rnnt 14.195563 hw_loss 0.160746 lr 0.00051174 rank 1
2023-02-20 22:05:02,091 DEBUG TRAIN Batch 11/7400 loss 31.330996 loss_att 41.205849 loss_ctc 29.658970 loss_rnnt 29.432215 hw_loss 0.275151 lr 0.00051174 rank 4
2023-02-20 22:05:58,674 DEBUG TRAIN Batch 11/7500 loss 22.351110 loss_att 24.674988 loss_ctc 21.472038 loss_rnnt 21.916740 hw_loss 0.162759 lr 0.00051160 rank 0
2023-02-20 22:05:58,680 DEBUG TRAIN Batch 11/7500 loss 23.398384 loss_att 26.943872 loss_ctc 27.973997 loss_rnnt 21.862988 hw_loss 0.405406 lr 0.00051160 rank 1
2023-02-20 22:05:58,681 DEBUG TRAIN Batch 11/7500 loss 17.554071 loss_att 22.487473 loss_ctc 19.445494 loss_rnnt 16.075672 hw_loss 0.449117 lr 0.00051160 rank 3
2023-02-20 22:05:58,686 DEBUG TRAIN Batch 11/7500 loss 29.269144 loss_att 28.947216 loss_ctc 37.506348 loss_rnnt 28.137421 hw_loss 0.183400 lr 0.00051160 rank 2
2023-02-20 22:05:58,687 DEBUG TRAIN Batch 11/7500 loss 17.858505 loss_att 15.342212 loss_ctc 20.689205 loss_rnnt 17.682552 hw_loss 0.565845 lr 0.00051160 rank 5
2023-02-20 22:05:58,687 DEBUG TRAIN Batch 11/7500 loss 9.531513 loss_att 16.274918 loss_ctc 12.353109 loss_rnnt 7.690938 hw_loss 0.216901 lr 0.00051160 rank 6
2023-02-20 22:05:58,689 DEBUG TRAIN Batch 11/7500 loss 26.564196 loss_att 26.833891 loss_ctc 29.652763 loss_rnnt 25.989307 hw_loss 0.204633 lr 0.00051160 rank 4
2023-02-20 22:05:58,746 DEBUG TRAIN Batch 11/7500 loss 15.039721 loss_att 26.718563 loss_ctc 12.652514 loss_rnnt 12.817129 hw_loss 0.384595 lr 0.00051160 rank 7
2023-02-20 22:06:58,869 DEBUG TRAIN Batch 11/7600 loss 20.956615 loss_att 35.038158 loss_ctc 22.402704 loss_rnnt 17.882061 hw_loss 0.122688 lr 0.00051147 rank 0
2023-02-20 22:06:58,875 DEBUG TRAIN Batch 11/7600 loss 23.923363 loss_att 25.442829 loss_ctc 29.987459 loss_rnnt 22.810352 hw_loss 0.001070 lr 0.00051147 rank 3
2023-02-20 22:06:58,881 DEBUG TRAIN Batch 11/7600 loss 24.975565 loss_att 33.077991 loss_ctc 25.066250 loss_rnnt 23.221800 hw_loss 0.227232 lr 0.00051147 rank 1
2023-02-20 22:06:58,883 DEBUG TRAIN Batch 11/7600 loss 16.232031 loss_att 22.281273 loss_ctc 18.262352 loss_rnnt 14.686526 hw_loss 0.121775 lr 0.00051147 rank 6
2023-02-20 22:06:58,887 DEBUG TRAIN Batch 11/7600 loss 41.243252 loss_att 48.279854 loss_ctc 42.445560 loss_rnnt 39.476246 hw_loss 0.373833 lr 0.00051147 rank 2
2023-02-20 22:06:58,891 DEBUG TRAIN Batch 11/7600 loss 54.084602 loss_att 58.304070 loss_ctc 71.200890 loss_rnnt 50.957970 hw_loss 0.001070 lr 0.00051147 rank 4
2023-02-20 22:06:58,894 DEBUG TRAIN Batch 11/7600 loss 30.504143 loss_att 39.433334 loss_ctc 38.596367 loss_rnnt 27.526180 hw_loss 0.212179 lr 0.00051147 rank 7
2023-02-20 22:06:58,943 DEBUG TRAIN Batch 11/7600 loss 23.936132 loss_att 38.040527 loss_ctc 28.930790 loss_rnnt 20.254852 hw_loss 0.364587 lr 0.00051147 rank 5
2023-02-20 22:07:59,436 DEBUG TRAIN Batch 11/7700 loss 16.844288 loss_att 28.744717 loss_ctc 23.616858 loss_rnnt 13.293005 hw_loss 0.502856 lr 0.00051134 rank 0
2023-02-20 22:07:59,440 DEBUG TRAIN Batch 11/7700 loss 8.813387 loss_att 16.503012 loss_ctc 12.188191 loss_rnnt 6.585111 hw_loss 0.450706 lr 0.00051134 rank 1
2023-02-20 22:07:59,445 DEBUG TRAIN Batch 11/7700 loss 9.475938 loss_att 23.654322 loss_ctc 8.241289 loss_rnnt 6.633639 hw_loss 0.321078 lr 0.00051134 rank 6
2023-02-20 22:07:59,447 DEBUG TRAIN Batch 11/7700 loss 32.219952 loss_att 46.817764 loss_ctc 32.647366 loss_rnnt 28.945463 hw_loss 0.558634 lr 0.00051134 rank 5
2023-02-20 22:07:59,453 DEBUG TRAIN Batch 11/7700 loss 43.720135 loss_att 44.448936 loss_ctc 46.460197 loss_rnnt 43.078491 hw_loss 0.244773 lr 0.00051134 rank 2
2023-02-20 22:07:59,459 DEBUG TRAIN Batch 11/7700 loss 28.481291 loss_att 31.989948 loss_ctc 31.463961 loss_rnnt 27.133713 hw_loss 0.465291 lr 0.00051134 rank 3
2023-02-20 22:07:59,500 DEBUG TRAIN Batch 11/7700 loss 23.969904 loss_att 36.226776 loss_ctc 34.347458 loss_rnnt 19.943207 hw_loss 0.359340 lr 0.00051134 rank 7
2023-02-20 22:07:59,501 DEBUG TRAIN Batch 11/7700 loss 26.185715 loss_att 43.040497 loss_ctc 26.361406 loss_rnnt 22.666000 hw_loss 0.234996 lr 0.00051134 rank 4
2023-02-20 22:09:19,016 DEBUG TRAIN Batch 11/7800 loss 41.208790 loss_att 59.925385 loss_ctc 52.453274 loss_rnnt 35.797054 hw_loss 0.317165 lr 0.00051120 rank 0
2023-02-20 22:09:19,021 DEBUG TRAIN Batch 11/7800 loss 16.250750 loss_att 16.547144 loss_ctc 20.040060 loss_rnnt 15.386617 hw_loss 0.561771 lr 0.00051120 rank 6
2023-02-20 22:09:19,022 DEBUG TRAIN Batch 11/7800 loss 23.979921 loss_att 31.418098 loss_ctc 23.843201 loss_rnnt 22.317083 hw_loss 0.362686 lr 0.00051120 rank 2
2023-02-20 22:09:19,028 DEBUG TRAIN Batch 11/7800 loss 21.008942 loss_att 23.416372 loss_ctc 20.313843 loss_rnnt 20.406872 hw_loss 0.399866 lr 0.00051120 rank 1
2023-02-20 22:09:19,033 DEBUG TRAIN Batch 11/7800 loss 24.940060 loss_att 30.550058 loss_ctc 30.090748 loss_rnnt 22.917049 hw_loss 0.401724 lr 0.00051120 rank 4
2023-02-20 22:09:19,033 DEBUG TRAIN Batch 11/7800 loss 23.076731 loss_att 25.384289 loss_ctc 28.237906 loss_rnnt 21.740658 hw_loss 0.349506 lr 0.00051120 rank 5
2023-02-20 22:09:19,034 DEBUG TRAIN Batch 11/7800 loss 20.179686 loss_att 26.855778 loss_ctc 22.013010 loss_rnnt 18.476467 hw_loss 0.231673 lr 0.00051120 rank 3
2023-02-20 22:09:19,087 DEBUG TRAIN Batch 11/7800 loss 33.573425 loss_att 36.098839 loss_ctc 40.531048 loss_rnnt 31.911413 hw_loss 0.429837 lr 0.00051120 rank 7
2023-02-20 22:10:19,109 DEBUG TRAIN Batch 11/7900 loss 59.411243 loss_att 70.019569 loss_ctc 68.384384 loss_rnnt 55.999077 hw_loss 0.176400 lr 0.00051107 rank 0
2023-02-20 22:10:19,113 DEBUG TRAIN Batch 11/7900 loss 27.643740 loss_att 37.732826 loss_ctc 27.615589 loss_rnnt 25.492889 hw_loss 0.256477 lr 0.00051107 rank 2
2023-02-20 22:10:19,125 DEBUG TRAIN Batch 11/7900 loss 32.493427 loss_att 38.404396 loss_ctc 34.053139 loss_rnnt 30.955736 hw_loss 0.276633 lr 0.00051107 rank 5
2023-02-20 22:10:19,125 DEBUG TRAIN Batch 11/7900 loss 24.276157 loss_att 29.521935 loss_ctc 25.504095 loss_rnnt 22.795088 hw_loss 0.502852 lr 0.00051107 rank 6
2023-02-20 22:10:19,126 DEBUG TRAIN Batch 11/7900 loss 30.141796 loss_att 30.898365 loss_ctc 28.765480 loss_rnnt 30.061485 hw_loss 0.210944 lr 0.00051107 rank 3
2023-02-20 22:10:19,173 DEBUG TRAIN Batch 11/7900 loss 12.281107 loss_att 20.508575 loss_ctc 14.095191 loss_rnnt 10.106745 hw_loss 0.538109 lr 0.00051107 rank 4
2023-02-20 22:10:19,178 DEBUG TRAIN Batch 11/7900 loss 9.742768 loss_att 16.189327 loss_ctc 8.760103 loss_rnnt 8.424393 hw_loss 0.300163 lr 0.00051107 rank 7
2023-02-20 22:10:19,180 DEBUG TRAIN Batch 11/7900 loss 21.929396 loss_att 21.467834 loss_ctc 23.824594 loss_rnnt 21.706684 hw_loss 0.116867 lr 0.00051107 rank 1
2023-02-20 22:11:17,610 DEBUG TRAIN Batch 11/8000 loss 15.175963 loss_att 25.359198 loss_ctc 19.053181 loss_rnnt 12.535988 hw_loss 0.161937 lr 0.00051094 rank 0
2023-02-20 22:11:17,614 DEBUG TRAIN Batch 11/8000 loss 18.585747 loss_att 22.250715 loss_ctc 19.477875 loss_rnnt 17.554651 hw_loss 0.335906 lr 0.00051094 rank 2
2023-02-20 22:11:17,616 DEBUG TRAIN Batch 11/8000 loss 41.765640 loss_att 54.642006 loss_ctc 46.987560 loss_rnnt 38.380398 hw_loss 0.213210 lr 0.00051094 rank 3
2023-02-20 22:11:17,620 DEBUG TRAIN Batch 11/8000 loss 46.778877 loss_att 54.934181 loss_ctc 51.456757 loss_rnnt 44.522423 hw_loss 0.003151 lr 0.00051094 rank 6
2023-02-20 22:11:17,620 DEBUG TRAIN Batch 11/8000 loss 22.252861 loss_att 40.729561 loss_ctc 30.137663 loss_rnnt 17.364159 hw_loss 0.266356 lr 0.00051094 rank 7
2023-02-20 22:11:17,657 DEBUG TRAIN Batch 11/8000 loss 23.281708 loss_att 20.396145 loss_ctc 23.247904 loss_rnnt 23.640131 hw_loss 0.418491 lr 0.00051094 rank 4
2023-02-20 22:11:17,658 DEBUG TRAIN Batch 11/8000 loss 33.899662 loss_att 45.739094 loss_ctc 50.200462 loss_rnnt 29.356657 hw_loss 0.003151 lr 0.00051094 rank 5
2023-02-20 22:11:17,674 DEBUG TRAIN Batch 11/8000 loss 12.676929 loss_att 27.668068 loss_ctc 19.340219 loss_rnnt 8.588608 hw_loss 0.378101 lr 0.00051094 rank 1
2023-02-20 22:12:15,818 DEBUG TRAIN Batch 11/8100 loss 9.882578 loss_att 15.690937 loss_ctc 6.470936 loss_rnnt 8.928155 hw_loss 0.464318 lr 0.00051080 rank 0
2023-02-20 22:12:15,826 DEBUG TRAIN Batch 11/8100 loss 21.519535 loss_att 22.869362 loss_ctc 20.936853 loss_rnnt 21.160324 hw_loss 0.313005 lr 0.00051080 rank 5
2023-02-20 22:12:15,828 DEBUG TRAIN Batch 11/8100 loss 19.179415 loss_att 29.670994 loss_ctc 25.257591 loss_rnnt 16.139563 hw_loss 0.245836 lr 0.00051080 rank 1
2023-02-20 22:12:15,828 DEBUG TRAIN Batch 11/8100 loss 14.173201 loss_att 17.095715 loss_ctc 11.651107 loss_rnnt 13.794241 hw_loss 0.245132 lr 0.00051080 rank 7
2023-02-20 22:12:15,830 DEBUG TRAIN Batch 11/8100 loss 29.679102 loss_att 31.205284 loss_ctc 39.723461 loss_rnnt 27.880903 hw_loss 0.288213 lr 0.00051080 rank 2
2023-02-20 22:12:15,832 DEBUG TRAIN Batch 11/8100 loss 14.623847 loss_att 23.926920 loss_ctc 11.943688 loss_rnnt 13.008116 hw_loss 0.210886 lr 0.00051080 rank 3
2023-02-20 22:12:15,837 DEBUG TRAIN Batch 11/8100 loss 37.990948 loss_att 55.953575 loss_ctc 38.814484 loss_rnnt 34.067776 hw_loss 0.414075 lr 0.00051080 rank 4
2023-02-20 22:12:15,840 DEBUG TRAIN Batch 11/8100 loss 31.474447 loss_att 46.934078 loss_ctc 42.706795 loss_rnnt 26.796316 hw_loss 0.166047 lr 0.00051080 rank 6
2023-02-20 22:13:15,829 DEBUG TRAIN Batch 11/8200 loss 41.180180 loss_att 53.684319 loss_ctc 39.883224 loss_rnnt 38.787968 hw_loss 0.120569 lr 0.00051067 rank 2
2023-02-20 22:13:15,829 DEBUG TRAIN Batch 11/8200 loss 13.226377 loss_att 20.830654 loss_ctc 12.493776 loss_rnnt 11.654001 hw_loss 0.279750 lr 0.00051067 rank 5
2023-02-20 22:13:15,831 DEBUG TRAIN Batch 11/8200 loss 45.755047 loss_att 57.216446 loss_ctc 52.969536 loss_rnnt 42.419575 hw_loss 0.152362 lr 0.00051067 rank 3
2023-02-20 22:13:15,831 DEBUG TRAIN Batch 11/8200 loss 20.266611 loss_att 35.447632 loss_ctc 22.197697 loss_rnnt 16.737814 hw_loss 0.440841 lr 0.00051067 rank 6
2023-02-20 22:13:15,833 DEBUG TRAIN Batch 11/8200 loss 14.475952 loss_att 30.522024 loss_ctc 12.583308 loss_rnnt 11.503565 hw_loss 0.029105 lr 0.00051067 rank 0
2023-02-20 22:13:15,833 DEBUG TRAIN Batch 11/8200 loss 12.711501 loss_att 20.932510 loss_ctc 14.779202 loss_rnnt 10.480341 hw_loss 0.583623 lr 0.00051067 rank 7
2023-02-20 22:13:15,835 DEBUG TRAIN Batch 11/8200 loss 10.648553 loss_att 17.718355 loss_ctc 13.249287 loss_rnnt 8.676701 hw_loss 0.395862 lr 0.00051067 rank 4
2023-02-20 22:13:15,842 DEBUG TRAIN Batch 11/8200 loss 10.229322 loss_att 19.805408 loss_ctc 8.374154 loss_rnnt 8.420977 hw_loss 0.263410 lr 0.00051067 rank 1
2023-02-20 22:14:14,637 DEBUG TRAIN Batch 11/8300 loss 18.636782 loss_att 29.741926 loss_ctc 21.634573 loss_rnnt 15.857109 hw_loss 0.298010 lr 0.00051054 rank 0
2023-02-20 22:14:14,641 DEBUG TRAIN Batch 11/8300 loss 38.891453 loss_att 58.150639 loss_ctc 30.654774 loss_rnnt 35.956970 hw_loss 0.339125 lr 0.00051054 rank 5
2023-02-20 22:14:14,642 DEBUG TRAIN Batch 11/8300 loss 23.677729 loss_att 35.033836 loss_ctc 39.545269 loss_rnnt 19.161867 hw_loss 0.241814 lr 0.00051054 rank 2
2023-02-20 22:14:14,642 DEBUG TRAIN Batch 11/8300 loss 10.698155 loss_att 29.886845 loss_ctc 10.678258 loss_rnnt 6.723239 hw_loss 0.262181 lr 0.00051054 rank 6
2023-02-20 22:14:14,647 DEBUG TRAIN Batch 11/8300 loss 21.182116 loss_att 23.417904 loss_ctc 27.803793 loss_rnnt 19.843538 hw_loss 0.015991 lr 0.00051054 rank 1
2023-02-20 22:14:14,647 DEBUG TRAIN Batch 11/8300 loss 11.683184 loss_att 14.376437 loss_ctc 10.538590 loss_rnnt 11.288618 hw_loss 0.015991 lr 0.00051054 rank 7
2023-02-20 22:14:14,649 DEBUG TRAIN Batch 11/8300 loss 19.711182 loss_att 26.878965 loss_ctc 21.738249 loss_rnnt 17.903141 hw_loss 0.195388 lr 0.00051054 rank 4
2023-02-20 22:14:14,650 DEBUG TRAIN Batch 11/8300 loss 27.170126 loss_att 39.654961 loss_ctc 28.394619 loss_rnnt 24.303692 hw_loss 0.386627 lr 0.00051054 rank 3
2023-02-20 22:15:12,226 DEBUG TRAIN Batch 11/8400 loss 20.227810 loss_att 26.888863 loss_ctc 17.736401 loss_rnnt 19.074917 hw_loss 0.286629 lr 0.00051040 rank 0
2023-02-20 22:15:12,232 DEBUG TRAIN Batch 11/8400 loss 29.227419 loss_att 29.366077 loss_ctc 38.921257 loss_rnnt 27.838102 hw_loss 0.129514 lr 0.00051040 rank 2
2023-02-20 22:15:12,235 DEBUG TRAIN Batch 11/8400 loss 11.841551 loss_att 21.121023 loss_ctc 14.030321 loss_rnnt 9.512093 hw_loss 0.340739 lr 0.00051040 rank 5
2023-02-20 22:15:12,236 DEBUG TRAIN Batch 11/8400 loss 29.286339 loss_att 36.935841 loss_ctc 36.354412 loss_rnnt 26.566525 hw_loss 0.464074 lr 0.00051040 rank 7
2023-02-20 22:15:12,240 DEBUG TRAIN Batch 11/8400 loss 17.805412 loss_att 28.516493 loss_ctc 16.705986 loss_rnnt 15.648347 hw_loss 0.302698 lr 0.00051040 rank 1
2023-02-20 22:15:12,242 DEBUG TRAIN Batch 11/8400 loss 35.874851 loss_att 42.485271 loss_ctc 46.607498 loss_rnnt 32.947475 hw_loss 0.326767 lr 0.00051040 rank 3
2023-02-20 22:15:12,247 DEBUG TRAIN Batch 11/8400 loss 34.021362 loss_att 40.405640 loss_ctc 32.825634 loss_rnnt 32.807610 hw_loss 0.180607 lr 0.00051040 rank 4
2023-02-20 22:15:12,292 DEBUG TRAIN Batch 11/8400 loss 30.115046 loss_att 29.744995 loss_ctc 36.297951 loss_rnnt 29.205002 hw_loss 0.299374 lr 0.00051040 rank 6
2023-02-20 22:16:13,423 DEBUG TRAIN Batch 11/8500 loss 50.427113 loss_att 66.777168 loss_ctc 59.183880 loss_rnnt 45.806686 hw_loss 0.342835 lr 0.00051027 rank 0
2023-02-20 22:16:13,428 DEBUG TRAIN Batch 11/8500 loss 27.643881 loss_att 30.945721 loss_ctc 19.187759 loss_rnnt 27.945927 hw_loss 0.309500 lr 0.00051027 rank 2
2023-02-20 22:16:13,432 DEBUG TRAIN Batch 11/8500 loss 25.802425 loss_att 42.237534 loss_ctc 30.815666 loss_rnnt 21.650135 hw_loss 0.369068 lr 0.00051027 rank 7
2023-02-20 22:16:13,433 DEBUG TRAIN Batch 11/8500 loss 38.330383 loss_att 71.899872 loss_ctc 46.087532 loss_rnnt 30.338560 hw_loss 0.456830 lr 0.00051027 rank 4
2023-02-20 22:16:13,435 DEBUG TRAIN Batch 11/8500 loss 15.150745 loss_att 13.887877 loss_ctc 8.692811 loss_rnnt 16.069664 hw_loss 0.365087 lr 0.00051027 rank 3
2023-02-20 22:16:13,436 DEBUG TRAIN Batch 11/8500 loss 14.361264 loss_att 26.974102 loss_ctc 10.769121 loss_rnnt 12.201547 hw_loss 0.217693 lr 0.00051027 rank 5
2023-02-20 22:16:13,437 DEBUG TRAIN Batch 11/8500 loss 17.497292 loss_att 26.328058 loss_ctc 19.542839 loss_rnnt 15.316393 hw_loss 0.266261 lr 0.00051027 rank 6
2023-02-20 22:16:13,491 DEBUG TRAIN Batch 11/8500 loss 26.492043 loss_att 35.417023 loss_ctc 33.339336 loss_rnnt 23.469248 hw_loss 0.609051 lr 0.00051027 rank 1
2023-02-20 22:17:31,844 DEBUG TRAIN Batch 11/8600 loss 29.207684 loss_att 47.547394 loss_ctc 28.913710 loss_rnnt 25.442869 hw_loss 0.255133 lr 0.00051014 rank 1
2023-02-20 22:17:31,846 DEBUG TRAIN Batch 11/8600 loss 30.531572 loss_att 28.760933 loss_ctc 31.096426 loss_rnnt 30.579782 hw_loss 0.432380 lr 0.00051014 rank 0
2023-02-20 22:17:31,849 DEBUG TRAIN Batch 11/8600 loss 23.690516 loss_att 31.706091 loss_ctc 22.491327 loss_rnnt 22.123550 hw_loss 0.232017 lr 0.00051014 rank 5
2023-02-20 22:17:31,850 DEBUG TRAIN Batch 11/8600 loss 74.428612 loss_att 92.446083 loss_ctc 98.375908 loss_rnnt 67.509605 hw_loss 0.229750 lr 0.00051014 rank 3
2023-02-20 22:17:31,853 DEBUG TRAIN Batch 11/8600 loss 30.044008 loss_att 33.177345 loss_ctc 41.826778 loss_rnnt 27.731953 hw_loss 0.214406 lr 0.00051014 rank 2
2023-02-20 22:17:31,855 DEBUG TRAIN Batch 11/8600 loss 19.071629 loss_att 25.847960 loss_ctc 26.076305 loss_rnnt 16.599979 hw_loss 0.342049 lr 0.00051014 rank 4
2023-02-20 22:17:31,856 DEBUG TRAIN Batch 11/8600 loss 41.646511 loss_att 38.182800 loss_ctc 38.689426 loss_rnnt 42.589497 hw_loss 0.270060 lr 0.00051014 rank 6
2023-02-20 22:17:31,856 DEBUG TRAIN Batch 11/8600 loss 16.300426 loss_att 25.428736 loss_ctc 19.563082 loss_rnnt 13.869215 hw_loss 0.319743 lr 0.00051014 rank 7
2023-02-20 22:18:34,917 DEBUG TRAIN Batch 11/8700 loss 25.662262 loss_att 28.293880 loss_ctc 34.463821 loss_rnnt 23.783333 hw_loss 0.335743 lr 0.00051000 rank 0
2023-02-20 22:18:34,918 DEBUG TRAIN Batch 11/8700 loss 11.402357 loss_att 15.533806 loss_ctc 14.199841 loss_rnnt 10.160778 hw_loss 0.079298 lr 0.00051000 rank 3
2023-02-20 22:18:34,919 DEBUG TRAIN Batch 11/8700 loss 13.618051 loss_att 23.643873 loss_ctc 13.614897 loss_rnnt 11.497841 hw_loss 0.216499 lr 0.00051000 rank 5
2023-02-20 22:18:34,923 DEBUG TRAIN Batch 11/8700 loss 18.776930 loss_att 23.057007 loss_ctc 24.307606 loss_rnnt 16.993393 hw_loss 0.356437 lr 0.00051000 rank 6
2023-02-20 22:18:34,923 DEBUG TRAIN Batch 11/8700 loss 9.195162 loss_att 9.865178 loss_ctc 7.443542 loss_rnnt 9.106046 hw_loss 0.353742 lr 0.00051000 rank 2
2023-02-20 22:18:34,923 DEBUG TRAIN Batch 11/8700 loss 23.239706 loss_att 35.358704 loss_ctc 27.251034 loss_rnnt 20.192276 hw_loss 0.166471 lr 0.00051000 rank 4
2023-02-20 22:18:34,930 DEBUG TRAIN Batch 11/8700 loss 16.517225 loss_att 23.971859 loss_ctc 18.373230 loss_rnnt 14.561672 hw_loss 0.407170 lr 0.00051000 rank 1
2023-02-20 22:18:34,987 DEBUG TRAIN Batch 11/8700 loss 18.912043 loss_att 25.068321 loss_ctc 21.439499 loss_rnnt 17.146149 hw_loss 0.370582 lr 0.00051000 rank 7
2023-02-20 22:19:35,557 DEBUG TRAIN Batch 11/8800 loss 21.507324 loss_att 28.855055 loss_ctc 18.827629 loss_rnnt 20.263350 hw_loss 0.246980 lr 0.00050987 rank 0
2023-02-20 22:19:35,559 DEBUG TRAIN Batch 11/8800 loss 16.746696 loss_att 35.146896 loss_ctc 19.503122 loss_rnnt 12.450264 hw_loss 0.466626 lr 0.00050987 rank 3
2023-02-20 22:19:35,560 DEBUG TRAIN Batch 11/8800 loss 6.011755 loss_att 17.348719 loss_ctc 7.638792 loss_rnnt 3.358257 hw_loss 0.317189 lr 0.00050987 rank 6
2023-02-20 22:19:35,563 DEBUG TRAIN Batch 11/8800 loss 38.565475 loss_att 58.795490 loss_ctc 49.911583 loss_rnnt 32.827618 hw_loss 0.335699 lr 0.00050987 rank 5
2023-02-20 22:19:35,564 DEBUG TRAIN Batch 11/8800 loss 13.780199 loss_att 19.378727 loss_ctc 23.514881 loss_rnnt 11.229223 hw_loss 0.249963 lr 0.00050987 rank 2
2023-02-20 22:19:35,565 DEBUG TRAIN Batch 11/8800 loss 43.864758 loss_att 64.967987 loss_ctc 58.627422 loss_rnnt 37.593666 hw_loss 0.153927 lr 0.00050987 rank 4
2023-02-20 22:19:35,604 DEBUG TRAIN Batch 11/8800 loss 27.612576 loss_att 44.006935 loss_ctc 32.922523 loss_rnnt 23.540215 hw_loss 0.160310 lr 0.00050987 rank 7
2023-02-20 22:19:35,609 DEBUG TRAIN Batch 11/8800 loss 12.211052 loss_att 23.628981 loss_ctc 22.187660 loss_rnnt 8.298261 hw_loss 0.560609 lr 0.00050987 rank 1
2023-02-20 22:20:32,513 DEBUG TRAIN Batch 11/8900 loss 19.908541 loss_att 21.843386 loss_ctc 18.591612 loss_rnnt 19.482517 hw_loss 0.402462 lr 0.00050974 rank 0
2023-02-20 22:20:32,519 DEBUG TRAIN Batch 11/8900 loss 30.879646 loss_att 37.342693 loss_ctc 29.263407 loss_rnnt 29.663664 hw_loss 0.260385 lr 0.00050974 rank 5
2023-02-20 22:20:32,520 DEBUG TRAIN Batch 11/8900 loss 30.366713 loss_att 47.335617 loss_ctc 37.353474 loss_rnnt 25.991755 hw_loss 0.093015 lr 0.00050974 rank 2
2023-02-20 22:20:32,522 DEBUG TRAIN Batch 11/8900 loss 39.026459 loss_att 53.318466 loss_ctc 42.809395 loss_rnnt 35.512001 hw_loss 0.284367 lr 0.00050974 rank 6
2023-02-20 22:20:32,523 DEBUG TRAIN Batch 11/8900 loss 22.771008 loss_att 33.420372 loss_ctc 17.881155 loss_rnnt 21.057808 hw_loss 0.441205 lr 0.00050974 rank 1
2023-02-20 22:20:32,526 DEBUG TRAIN Batch 11/8900 loss 8.090372 loss_att 11.655652 loss_ctc 11.342718 loss_rnnt 6.641164 hw_loss 0.567199 lr 0.00050974 rank 4
2023-02-20 22:20:32,531 DEBUG TRAIN Batch 11/8900 loss 7.909504 loss_att 14.653806 loss_ctc 8.326195 loss_rnnt 6.502904 hw_loss 0.004087 lr 0.00050974 rank 3
2023-02-20 22:20:32,533 DEBUG TRAIN Batch 11/8900 loss 23.205858 loss_att 37.755466 loss_ctc 19.098066 loss_rnnt 20.696915 hw_loss 0.275110 lr 0.00050974 rank 7
2023-02-20 22:21:31,458 DEBUG TRAIN Batch 11/9000 loss 32.838043 loss_att 38.407673 loss_ctc 35.754906 loss_rnnt 31.149536 hw_loss 0.348123 lr 0.00050961 rank 0
2023-02-20 22:21:31,462 DEBUG TRAIN Batch 11/9000 loss 28.167040 loss_att 30.948185 loss_ctc 27.115561 loss_rnnt 27.748362 hw_loss 0.004965 lr 0.00050961 rank 2
2023-02-20 22:21:31,463 DEBUG TRAIN Batch 11/9000 loss 10.169559 loss_att 16.839155 loss_ctc 17.200069 loss_rnnt 7.679978 hw_loss 0.409237 lr 0.00050961 rank 6
2023-02-20 22:21:31,467 DEBUG TRAIN Batch 11/9000 loss 28.059280 loss_att 39.138870 loss_ctc 35.363277 loss_rnnt 24.715492 hw_loss 0.288759 lr 0.00050961 rank 4
2023-02-20 22:21:31,469 DEBUG TRAIN Batch 11/9000 loss 15.897993 loss_att 26.467339 loss_ctc 15.462193 loss_rnnt 13.839584 hw_loss 0.004965 lr 0.00050961 rank 3
2023-02-20 22:21:31,470 DEBUG TRAIN Batch 11/9000 loss 27.841908 loss_att 33.859016 loss_ctc 32.448639 loss_rnnt 25.971376 hw_loss 0.099146 lr 0.00050961 rank 5
2023-02-20 22:21:31,472 DEBUG TRAIN Batch 11/9000 loss 22.843504 loss_att 31.222385 loss_ctc 26.745224 loss_rnnt 20.536633 hw_loss 0.207878 lr 0.00050961 rank 1
2023-02-20 22:21:31,477 DEBUG TRAIN Batch 11/9000 loss 4.463701 loss_att 11.578314 loss_ctc 6.315351 loss_rnnt 2.631569 hw_loss 0.304354 lr 0.00050961 rank 7
2023-02-20 22:22:31,498 DEBUG TRAIN Batch 11/9100 loss 13.406718 loss_att 20.453516 loss_ctc 12.976501 loss_rnnt 11.880695 hw_loss 0.326296 lr 0.00050948 rank 1
2023-02-20 22:22:31,498 DEBUG TRAIN Batch 11/9100 loss 8.243483 loss_att 19.423155 loss_ctc 6.984874 loss_rnnt 6.119090 hw_loss 0.105512 lr 0.00050948 rank 4
2023-02-20 22:22:31,504 DEBUG TRAIN Batch 11/9100 loss 21.155638 loss_att 26.347923 loss_ctc 25.202883 loss_rnnt 19.475410 hw_loss 0.191506 lr 0.00050948 rank 0
2023-02-20 22:22:31,508 DEBUG TRAIN Batch 11/9100 loss 26.875879 loss_att 39.076035 loss_ctc 28.700903 loss_rnnt 24.023117 hw_loss 0.317619 lr 0.00050948 rank 3
2023-02-20 22:22:31,509 DEBUG TRAIN Batch 11/9100 loss 19.424961 loss_att 23.269676 loss_ctc 20.315319 loss_rnnt 18.439651 hw_loss 0.183096 lr 0.00050948 rank 2
2023-02-20 22:22:31,512 DEBUG TRAIN Batch 11/9100 loss 17.454840 loss_att 30.302643 loss_ctc 14.076501 loss_rnnt 15.063989 hw_loss 0.509505 lr 0.00050948 rank 7
2023-02-20 22:22:31,545 DEBUG TRAIN Batch 11/9100 loss 10.088527 loss_att 16.704290 loss_ctc 3.030338 loss_rnnt 9.704144 hw_loss 0.004354 lr 0.00050948 rank 6
2023-02-20 22:22:31,589 DEBUG TRAIN Batch 11/9100 loss 30.310188 loss_att 43.813690 loss_ctc 26.059570 loss_rnnt 28.135012 hw_loss 0.077298 lr 0.00050948 rank 5
2023-02-20 22:23:28,934 DEBUG TRAIN Batch 11/9200 loss 13.960282 loss_att 19.270052 loss_ctc 12.956481 loss_rnnt 12.889907 hw_loss 0.266742 lr 0.00050934 rank 0
2023-02-20 22:23:28,941 DEBUG TRAIN Batch 11/9200 loss 11.837280 loss_att 12.840324 loss_ctc 12.689365 loss_rnnt 11.265193 hw_loss 0.483504 lr 0.00050934 rank 3
2023-02-20 22:23:28,942 DEBUG TRAIN Batch 11/9200 loss 15.498100 loss_att 25.055637 loss_ctc 17.733238 loss_rnnt 13.237877 hw_loss 0.095057 lr 0.00050934 rank 5
2023-02-20 22:23:28,943 DEBUG TRAIN Batch 11/9200 loss 20.434084 loss_att 26.637903 loss_ctc 22.115181 loss_rnnt 18.966749 hw_loss 0.004550 lr 0.00050934 rank 1
2023-02-20 22:23:28,945 DEBUG TRAIN Batch 11/9200 loss 17.996712 loss_att 17.659386 loss_ctc 21.528126 loss_rnnt 17.391060 hw_loss 0.379240 lr 0.00050934 rank 2
2023-02-20 22:23:28,946 DEBUG TRAIN Batch 11/9200 loss 52.572212 loss_att 57.010300 loss_ctc 54.746742 loss_rnnt 51.351540 hw_loss 0.080842 lr 0.00050934 rank 4
2023-02-20 22:23:28,950 DEBUG TRAIN Batch 11/9200 loss 10.353534 loss_att 14.750372 loss_ctc 8.826815 loss_rnnt 9.675302 hw_loss 0.004550 lr 0.00050934 rank 6
2023-02-20 22:23:28,956 DEBUG TRAIN Batch 11/9200 loss 48.880695 loss_att 53.949650 loss_ctc 48.155682 loss_rnnt 47.747787 hw_loss 0.404599 lr 0.00050934 rank 7
2023-02-20 22:24:27,849 DEBUG TRAIN Batch 11/9300 loss 20.715866 loss_att 27.096111 loss_ctc 21.999269 loss_rnnt 19.155495 hw_loss 0.212254 lr 0.00050921 rank 0
2023-02-20 22:24:27,851 DEBUG TRAIN Batch 11/9300 loss 40.255043 loss_att 43.969116 loss_ctc 60.654442 loss_rnnt 36.652180 hw_loss 0.262738 lr 0.00050921 rank 3
2023-02-20 22:24:27,853 DEBUG TRAIN Batch 11/9300 loss 6.167842 loss_att 12.750905 loss_ctc 6.349025 loss_rnnt 4.767909 hw_loss 0.110931 lr 0.00050921 rank 5
2023-02-20 22:24:27,853 DEBUG TRAIN Batch 11/9300 loss 18.413555 loss_att 32.768620 loss_ctc 23.636511 loss_rnnt 14.698313 hw_loss 0.277189 lr 0.00050921 rank 2
2023-02-20 22:24:27,860 DEBUG TRAIN Batch 11/9300 loss 18.446194 loss_att 26.825382 loss_ctc 25.163723 loss_rnnt 15.714400 hw_loss 0.300530 lr 0.00050921 rank 7
2023-02-20 22:24:27,861 DEBUG TRAIN Batch 11/9300 loss 10.710176 loss_att 19.978521 loss_ctc 11.321672 loss_rnnt 8.641239 hw_loss 0.250754 lr 0.00050921 rank 6
2023-02-20 22:24:27,868 DEBUG TRAIN Batch 11/9300 loss 11.518916 loss_att 15.211215 loss_ctc 10.569670 loss_rnnt 10.846000 hw_loss 0.114420 lr 0.00050921 rank 4
2023-02-20 22:24:27,917 DEBUG TRAIN Batch 11/9300 loss 19.607870 loss_att 23.370134 loss_ctc 22.742048 loss_rnnt 18.303318 hw_loss 0.251639 lr 0.00050921 rank 1
2023-02-20 22:25:28,072 DEBUG TRAIN Batch 11/9400 loss 19.596081 loss_att 26.203499 loss_ctc 27.918550 loss_rnnt 16.832088 hw_loss 0.624085 lr 0.00050908 rank 0
2023-02-20 22:25:28,073 DEBUG TRAIN Batch 11/9400 loss 15.009023 loss_att 31.005810 loss_ctc 15.140713 loss_rnnt 11.612404 hw_loss 0.336940 lr 0.00050908 rank 3
2023-02-20 22:25:28,073 DEBUG TRAIN Batch 11/9400 loss 29.331347 loss_att 40.795509 loss_ctc 32.583340 loss_rnnt 26.384909 hw_loss 0.412516 lr 0.00050908 rank 2
2023-02-20 22:25:28,080 DEBUG TRAIN Batch 11/9400 loss 26.348112 loss_att 30.693701 loss_ctc 35.659019 loss_rnnt 24.233751 hw_loss 0.007102 lr 0.00050908 rank 6
2023-02-20 22:25:28,084 DEBUG TRAIN Batch 11/9400 loss 31.198420 loss_att 27.868198 loss_ctc 32.240902 loss_rnnt 31.721680 hw_loss 0.007102 lr 0.00050908 rank 5
2023-02-20 22:25:28,084 DEBUG TRAIN Batch 11/9400 loss 15.356608 loss_att 22.127247 loss_ctc 16.678572 loss_rnnt 13.822432 hw_loss 0.007102 lr 0.00050908 rank 7
2023-02-20 22:25:28,086 DEBUG TRAIN Batch 11/9400 loss 25.981148 loss_att 24.479301 loss_ctc 30.303404 loss_rnnt 25.701429 hw_loss 0.007102 lr 0.00050908 rank 4
2023-02-20 22:25:28,087 DEBUG TRAIN Batch 11/9400 loss 32.349483 loss_att 39.950665 loss_ctc 41.672112 loss_rnnt 29.347725 hw_loss 0.447193 lr 0.00050908 rank 1
2023-02-20 22:26:46,504 DEBUG TRAIN Batch 11/9500 loss 22.240408 loss_att 29.650141 loss_ctc 26.646612 loss_rnnt 19.971039 hw_loss 0.374866 lr 0.00050895 rank 0
2023-02-20 22:26:46,515 DEBUG TRAIN Batch 11/9500 loss 15.923265 loss_att 17.045807 loss_ctc 15.832573 loss_rnnt 15.426402 hw_loss 0.533335 lr 0.00050895 rank 2
2023-02-20 22:26:46,517 DEBUG TRAIN Batch 11/9500 loss 44.864220 loss_att 41.360077 loss_ctc 50.582752 loss_rnnt 44.575829 hw_loss 0.425168 lr 0.00050895 rank 1
2023-02-20 22:26:46,518 DEBUG TRAIN Batch 11/9500 loss 15.054029 loss_att 38.822254 loss_ctc 17.158098 loss_rnnt 9.703276 hw_loss 0.593559 lr 0.00050895 rank 6
2023-02-20 22:26:46,519 DEBUG TRAIN Batch 11/9500 loss 19.339378 loss_att 23.509930 loss_ctc 28.670116 loss_rnnt 17.008551 hw_loss 0.473664 lr 0.00050895 rank 5
2023-02-20 22:26:46,519 DEBUG TRAIN Batch 11/9500 loss 13.397771 loss_att 17.559290 loss_ctc 16.974339 loss_rnnt 11.904922 hw_loss 0.344384 lr 0.00050895 rank 3
2023-02-20 22:26:46,523 DEBUG TRAIN Batch 11/9500 loss 24.390638 loss_att 19.791912 loss_ctc 23.423677 loss_rnnt 25.435175 hw_loss 0.007756 lr 0.00050895 rank 4
2023-02-20 22:26:46,532 DEBUG TRAIN Batch 11/9500 loss 66.053223 loss_att 112.487305 loss_ctc 75.212128 loss_rnnt 55.541077 hw_loss 0.007756 lr 0.00050895 rank 7
2023-02-20 22:27:46,250 DEBUG TRAIN Batch 11/9600 loss 23.052917 loss_att 27.356911 loss_ctc 24.797644 loss_rnnt 21.957867 hw_loss 0.003041 lr 0.00050882 rank 0
2023-02-20 22:27:46,255 DEBUG TRAIN Batch 11/9600 loss 18.401196 loss_att 22.602184 loss_ctc 23.450251 loss_rnnt 16.784969 hw_loss 0.192792 lr 0.00050882 rank 2
2023-02-20 22:27:46,257 DEBUG TRAIN Batch 11/9600 loss 18.395796 loss_att 23.810394 loss_ctc 22.258434 loss_rnnt 16.738956 hw_loss 0.110438 lr 0.00050882 rank 5
2023-02-20 22:27:46,258 DEBUG TRAIN Batch 11/9600 loss 10.736429 loss_att 21.616282 loss_ctc 10.437856 loss_rnnt 8.472352 hw_loss 0.239842 lr 0.00050882 rank 1
2023-02-20 22:27:46,262 DEBUG TRAIN Batch 11/9600 loss 20.112246 loss_att 25.820673 loss_ctc 25.942390 loss_rnnt 17.962036 hw_loss 0.433451 lr 0.00050882 rank 7
2023-02-20 22:27:46,264 DEBUG TRAIN Batch 11/9600 loss 40.123844 loss_att 58.915794 loss_ctc 47.629074 loss_rnnt 35.192387 hw_loss 0.323188 lr 0.00050882 rank 3
2023-02-20 22:27:46,265 DEBUG TRAIN Batch 11/9600 loss 31.790758 loss_att 41.260284 loss_ctc 33.986168 loss_rnnt 29.473742 hw_loss 0.244477 lr 0.00050882 rank 6
2023-02-20 22:27:46,265 DEBUG TRAIN Batch 11/9600 loss 34.582928 loss_att 38.713989 loss_ctc 49.012169 loss_rnnt 31.750584 hw_loss 0.154185 lr 0.00050882 rank 4
2023-02-20 22:28:45,610 DEBUG TRAIN Batch 11/9700 loss 34.585300 loss_att 35.383762 loss_ctc 31.753588 loss_rnnt 34.801117 hw_loss 0.003856 lr 0.00050868 rank 0
2023-02-20 22:28:45,614 DEBUG TRAIN Batch 11/9700 loss 8.049633 loss_att 12.936640 loss_ctc 3.216221 loss_rnnt 7.714630 hw_loss 0.003856 lr 0.00050868 rank 1
2023-02-20 22:28:45,614 DEBUG TRAIN Batch 11/9700 loss 33.887051 loss_att 38.194958 loss_ctc 39.105953 loss_rnnt 32.327560 hw_loss 0.003856 lr 0.00050868 rank 3
2023-02-20 22:28:45,619 DEBUG TRAIN Batch 11/9700 loss 7.923126 loss_att 11.360971 loss_ctc 9.114928 loss_rnnt 7.074594 hw_loss 0.003856 lr 0.00050868 rank 2
2023-02-20 22:28:45,620 DEBUG TRAIN Batch 11/9700 loss 17.222797 loss_att 39.540192 loss_ctc 16.360823 loss_rnnt 12.678270 hw_loss 0.367458 lr 0.00050868 rank 4
2023-02-20 22:28:45,621 DEBUG TRAIN Batch 11/9700 loss 15.102314 loss_att 30.695002 loss_ctc 9.242520 loss_rnnt 12.675144 hw_loss 0.168634 lr 0.00050868 rank 5
2023-02-20 22:28:45,624 DEBUG TRAIN Batch 11/9700 loss 5.615010 loss_att 15.393876 loss_ctc 5.480781 loss_rnnt 3.576543 hw_loss 0.188609 lr 0.00050868 rank 7
2023-02-20 22:28:45,625 DEBUG TRAIN Batch 11/9700 loss 13.759403 loss_att 28.966953 loss_ctc 26.238174 loss_rnnt 8.931145 hw_loss 0.230460 lr 0.00050868 rank 6
2023-02-20 22:29:43,512 DEBUG TRAIN Batch 11/9800 loss 11.221714 loss_att 20.509560 loss_ctc 16.831842 loss_rnnt 8.432144 hw_loss 0.344967 lr 0.00050855 rank 0
2023-02-20 22:29:43,515 DEBUG TRAIN Batch 11/9800 loss 63.731659 loss_att 57.915661 loss_ctc 65.455612 loss_rnnt 64.562920 hw_loss 0.191402 lr 0.00050855 rank 3
2023-02-20 22:29:43,518 DEBUG TRAIN Batch 11/9800 loss 21.546476 loss_att 25.358852 loss_ctc 23.103458 loss_rnnt 20.575207 hw_loss 0.002248 lr 0.00050855 rank 1
2023-02-20 22:29:43,519 DEBUG TRAIN Batch 11/9800 loss 19.584911 loss_att 21.493660 loss_ctc 20.131432 loss_rnnt 18.927965 hw_loss 0.379360 lr 0.00050855 rank 2
2023-02-20 22:29:43,523 DEBUG TRAIN Batch 11/9800 loss 10.683091 loss_att 22.625523 loss_ctc 16.912920 loss_rnnt 7.184430 hw_loss 0.524121 lr 0.00050855 rank 5
2023-02-20 22:29:43,523 DEBUG TRAIN Batch 11/9800 loss 38.700653 loss_att 47.966957 loss_ctc 42.186485 loss_rnnt 36.337070 hw_loss 0.085396 lr 0.00050855 rank 4
2023-02-20 22:29:43,526 DEBUG TRAIN Batch 11/9800 loss 5.749487 loss_att 18.684393 loss_ctc 6.153545 loss_rnnt 2.859571 hw_loss 0.466988 lr 0.00050855 rank 6
2023-02-20 22:29:43,585 DEBUG TRAIN Batch 11/9800 loss 18.463120 loss_att 23.622313 loss_ctc 26.106781 loss_rnnt 16.136162 hw_loss 0.517436 lr 0.00050855 rank 7
2023-02-20 22:30:43,601 DEBUG TRAIN Batch 11/9900 loss 16.014456 loss_att 21.335297 loss_ctc 12.957354 loss_rnnt 15.139691 hw_loss 0.409144 lr 0.00050842 rank 0
2023-02-20 22:30:43,611 DEBUG TRAIN Batch 11/9900 loss 11.638456 loss_att 20.867760 loss_ctc 10.886172 loss_rnnt 9.757051 hw_loss 0.254718 lr 0.00050842 rank 5
2023-02-20 22:30:43,614 DEBUG TRAIN Batch 11/9900 loss 12.266861 loss_att 16.763470 loss_ctc 9.184875 loss_rnnt 11.719231 hw_loss 0.111075 lr 0.00050842 rank 3
2023-02-20 22:30:43,615 DEBUG TRAIN Batch 11/9900 loss 21.468130 loss_att 29.217987 loss_ctc 20.916254 loss_rnnt 19.866667 hw_loss 0.234517 lr 0.00050842 rank 2
2023-02-20 22:30:43,615 DEBUG TRAIN Batch 11/9900 loss 29.293728 loss_att 38.744080 loss_ctc 37.395515 loss_rnnt 26.097282 hw_loss 0.424001 lr 0.00050842 rank 1
2023-02-20 22:30:43,618 DEBUG TRAIN Batch 11/9900 loss 47.034523 loss_att 55.318237 loss_ctc 55.579533 loss_rnnt 44.112366 hw_loss 0.236395 lr 0.00050842 rank 6
2023-02-20 22:30:43,619 DEBUG TRAIN Batch 11/9900 loss 39.604759 loss_att 55.731117 loss_ctc 42.749916 loss_rnnt 35.849068 hw_loss 0.208242 lr 0.00050842 rank 7
2023-02-20 22:30:43,623 DEBUG TRAIN Batch 11/9900 loss 29.788681 loss_att 48.178116 loss_ctc 38.635052 loss_rnnt 24.929466 hw_loss 0.003398 lr 0.00050842 rank 4
2023-02-20 22:31:42,766 DEBUG TRAIN Batch 11/10000 loss 23.630348 loss_att 34.205994 loss_ctc 43.709110 loss_rnnt 18.837551 hw_loss 0.000936 lr 0.00050829 rank 0
2023-02-20 22:31:42,770 DEBUG TRAIN Batch 11/10000 loss 15.126939 loss_att 30.100182 loss_ctc 16.351658 loss_rnnt 11.968493 hw_loss 0.000936 lr 0.00050829 rank 1
2023-02-20 22:31:42,770 DEBUG TRAIN Batch 11/10000 loss 35.574295 loss_att 30.940905 loss_ctc 41.839119 loss_rnnt 35.515953 hw_loss 0.280702 lr 0.00050829 rank 5
2023-02-20 22:31:42,776 DEBUG TRAIN Batch 11/10000 loss 25.493910 loss_att 23.843519 loss_ctc 19.363794 loss_rnnt 26.430622 hw_loss 0.395088 lr 0.00050829 rank 3
2023-02-20 22:31:42,777 DEBUG TRAIN Batch 11/10000 loss 9.525034 loss_att 10.444125 loss_ctc 8.354470 loss_rnnt 9.427460 hw_loss 0.130934 lr 0.00050829 rank 2
2023-02-20 22:31:42,778 DEBUG TRAIN Batch 11/10000 loss 36.679886 loss_att 46.540031 loss_ctc 42.381176 loss_rnnt 33.947186 hw_loss 0.000936 lr 0.00050829 rank 6
2023-02-20 22:31:42,785 DEBUG TRAIN Batch 11/10000 loss 19.563396 loss_att 19.934105 loss_ctc 23.853575 loss_rnnt 18.729450 hw_loss 0.352085 lr 0.00050829 rank 7
2023-02-20 22:31:42,839 DEBUG TRAIN Batch 11/10000 loss 36.524937 loss_att 52.677425 loss_ctc 24.331497 loss_rnnt 34.691826 hw_loss 0.428261 lr 0.00050829 rank 4
2023-02-20 22:32:40,381 DEBUG TRAIN Batch 11/10100 loss 7.104456 loss_att 16.583881 loss_ctc 6.605481 loss_rnnt 5.067004 hw_loss 0.390182 lr 0.00050816 rank 0
2023-02-20 22:32:40,391 DEBUG TRAIN Batch 11/10100 loss 1.357685 loss_att 5.367800 loss_ctc 0.426623 loss_rnnt 0.595241 hw_loss 0.158555 lr 0.00050816 rank 1
2023-02-20 22:32:40,391 DEBUG TRAIN Batch 11/10100 loss 16.554716 loss_att 16.212076 loss_ctc 20.687029 loss_rnnt 15.911435 hw_loss 0.301560 lr 0.00050816 rank 3
2023-02-20 22:32:40,391 DEBUG TRAIN Batch 11/10100 loss 19.752747 loss_att 21.822956 loss_ctc 21.380178 loss_rnnt 18.975498 hw_loss 0.274152 lr 0.00050816 rank 2
2023-02-20 22:32:40,392 DEBUG TRAIN Batch 11/10100 loss 16.028208 loss_att 14.025116 loss_ctc 16.573669 loss_rnnt 16.153191 hw_loss 0.380454 lr 0.00050816 rank 5
2023-02-20 22:32:40,395 DEBUG TRAIN Batch 11/10100 loss 13.967821 loss_att 14.954054 loss_ctc 16.139923 loss_rnnt 13.386806 hw_loss 0.176539 lr 0.00050816 rank 4
2023-02-20 22:32:40,403 DEBUG TRAIN Batch 11/10100 loss 17.535435 loss_att 18.468195 loss_ctc 21.962158 loss_rnnt 16.591333 hw_loss 0.313721 lr 0.00050816 rank 7
2023-02-20 22:32:40,463 DEBUG TRAIN Batch 11/10100 loss 15.584342 loss_att 14.982248 loss_ctc 18.315342 loss_rnnt 15.168892 hw_loss 0.322003 lr 0.00050816 rank 6
2023-02-20 22:33:39,913 DEBUG TRAIN Batch 11/10200 loss 35.761307 loss_att 40.800537 loss_ctc 36.945339 loss_rnnt 34.372387 hw_loss 0.418508 lr 0.00050803 rank 0
2023-02-20 22:33:39,920 DEBUG TRAIN Batch 11/10200 loss 34.619999 loss_att 41.258553 loss_ctc 46.768288 loss_rnnt 31.671295 hw_loss 0.002292 lr 0.00050803 rank 3
2023-02-20 22:33:39,920 DEBUG TRAIN Batch 11/10200 loss 16.162348 loss_att 17.009808 loss_ctc 16.607979 loss_rnnt 15.777839 hw_loss 0.291750 lr 0.00050803 rank 2
2023-02-20 22:33:39,921 DEBUG TRAIN Batch 11/10200 loss 12.325394 loss_att 17.161957 loss_ctc 12.286030 loss_rnnt 11.160293 hw_loss 0.380693 lr 0.00050803 rank 5
2023-02-20 22:33:39,921 DEBUG TRAIN Batch 11/10200 loss 3.771742 loss_att 10.434074 loss_ctc 2.498488 loss_rnnt 2.471862 hw_loss 0.257214 lr 0.00050803 rank 6
2023-02-20 22:33:39,923 DEBUG TRAIN Batch 11/10200 loss 15.108415 loss_att 19.605698 loss_ctc 14.284693 loss_rnnt 14.208987 hw_loss 0.205875 lr 0.00050803 rank 1
2023-02-20 22:33:39,925 DEBUG TRAIN Batch 11/10200 loss 16.465883 loss_att 58.425480 loss_ctc 12.258170 loss_rnnt 8.462545 hw_loss 0.323340 lr 0.00050803 rank 4
2023-02-20 22:33:39,932 DEBUG TRAIN Batch 11/10200 loss 33.083023 loss_att 39.090778 loss_ctc 39.648796 loss_rnnt 30.888031 hw_loss 0.221252 lr 0.00050803 rank 7
2023-02-20 22:34:59,473 DEBUG TRAIN Batch 11/10300 loss 3.067464 loss_att 9.157907 loss_ctc 2.067507 loss_rnnt 1.911728 hw_loss 0.133078 lr 0.00050790 rank 2
2023-02-20 22:34:59,476 DEBUG TRAIN Batch 11/10300 loss 5.046304 loss_att 8.103117 loss_ctc 3.599485 loss_rnnt 4.463844 hw_loss 0.307512 lr 0.00050790 rank 0
2023-02-20 22:34:59,482 DEBUG TRAIN Batch 11/10300 loss 38.365700 loss_att 37.594982 loss_ctc 42.410103 loss_rnnt 37.729626 hw_loss 0.470563 lr 0.00050790 rank 6
2023-02-20 22:34:59,482 DEBUG TRAIN Batch 11/10300 loss 10.612628 loss_att 14.810745 loss_ctc 11.975651 loss_rnnt 9.489916 hw_loss 0.190035 lr 0.00050790 rank 1
2023-02-20 22:34:59,486 DEBUG TRAIN Batch 11/10300 loss 37.178402 loss_att 41.059170 loss_ctc 46.835667 loss_rnnt 35.113388 hw_loss 0.002301 lr 0.00050790 rank 5
2023-02-20 22:34:59,491 DEBUG TRAIN Batch 11/10300 loss 22.081459 loss_att 29.184397 loss_ctc 19.396894 loss_rnnt 20.882555 hw_loss 0.255479 lr 0.00050790 rank 3
2023-02-20 22:34:59,491 DEBUG TRAIN Batch 11/10300 loss 44.345905 loss_att 53.555393 loss_ctc 57.746365 loss_rnnt 40.510452 hw_loss 0.387802 lr 0.00050790 rank 7
2023-02-20 22:34:59,501 DEBUG TRAIN Batch 11/10300 loss 17.830547 loss_att 17.577158 loss_ctc 20.412580 loss_rnnt 17.288677 hw_loss 0.465515 lr 0.00050790 rank 4
2023-02-20 22:35:59,833 DEBUG TRAIN Batch 11/10400 loss 20.099815 loss_att 21.685480 loss_ctc 19.232641 loss_rnnt 19.803425 hw_loss 0.177900 lr 0.00050776 rank 0
2023-02-20 22:35:59,836 DEBUG TRAIN Batch 11/10400 loss 31.303850 loss_att 35.783852 loss_ctc 33.324127 loss_rnnt 29.916565 hw_loss 0.416086 lr 0.00050776 rank 2
2023-02-20 22:35:59,839 DEBUG TRAIN Batch 11/10400 loss 15.774314 loss_att 20.617453 loss_ctc 19.427610 loss_rnnt 14.223388 hw_loss 0.178485 lr 0.00050776 rank 6
2023-02-20 22:35:59,840 DEBUG TRAIN Batch 11/10400 loss 16.591232 loss_att 19.877525 loss_ctc 18.025917 loss_rnnt 15.552451 hw_loss 0.356685 lr 0.00050776 rank 5
2023-02-20 22:35:59,841 DEBUG TRAIN Batch 11/10400 loss 6.784105 loss_att 19.778446 loss_ctc 6.718127 loss_rnnt 4.013463 hw_loss 0.338568 lr 0.00050776 rank 4
2023-02-20 22:35:59,841 DEBUG TRAIN Batch 11/10400 loss 13.879449 loss_att 15.426512 loss_ctc 12.657912 loss_rnnt 13.618739 hw_loss 0.214068 lr 0.00050776 rank 3
2023-02-20 22:35:59,842 DEBUG TRAIN Batch 11/10400 loss 15.406777 loss_att 19.897385 loss_ctc 20.736715 loss_rnnt 13.745293 hw_loss 0.098821 lr 0.00050776 rank 1
2023-02-20 22:35:59,843 DEBUG TRAIN Batch 11/10400 loss 18.812485 loss_att 26.383604 loss_ctc 25.200460 loss_rnnt 16.267746 hw_loss 0.335223 lr 0.00050776 rank 7
2023-02-20 22:36:59,908 DEBUG TRAIN Batch 11/10500 loss 7.472008 loss_att 12.885539 loss_ctc 9.579322 loss_rnnt 5.999491 hw_loss 0.204067 lr 0.00050763 rank 0
2023-02-20 22:36:59,916 DEBUG TRAIN Batch 11/10500 loss 29.766031 loss_att 38.047188 loss_ctc 34.860542 loss_rnnt 27.332783 hw_loss 0.183278 lr 0.00050763 rank 5
2023-02-20 22:36:59,917 DEBUG TRAIN Batch 11/10500 loss 10.253759 loss_att 20.837353 loss_ctc 13.003289 loss_rnnt 7.540242 hw_loss 0.431615 lr 0.00050763 rank 3
2023-02-20 22:36:59,920 DEBUG TRAIN Batch 11/10500 loss 44.319027 loss_att 55.278175 loss_ctc 48.327980 loss_rnnt 41.550861 hw_loss 0.078386 lr 0.00050763 rank 4
2023-02-20 22:36:59,920 DEBUG TRAIN Batch 11/10500 loss 40.498371 loss_att 38.881042 loss_ctc 48.645782 loss_rnnt 39.658695 hw_loss 0.144031 lr 0.00050763 rank 7
2023-02-20 22:36:59,922 DEBUG TRAIN Batch 11/10500 loss 22.019548 loss_att 26.667690 loss_ctc 18.425838 loss_rnnt 21.406803 hw_loss 0.304265 lr 0.00050763 rank 2
2023-02-20 22:36:59,927 DEBUG TRAIN Batch 11/10500 loss 34.500797 loss_att 39.284565 loss_ctc 40.913078 loss_rnnt 32.553268 hw_loss 0.254629 lr 0.00050763 rank 1
2023-02-20 22:36:59,975 DEBUG TRAIN Batch 11/10500 loss 23.007856 loss_att 33.224957 loss_ctc 34.872177 loss_rnnt 19.243279 hw_loss 0.261093 lr 0.00050763 rank 6
2023-02-20 22:37:58,879 DEBUG TRAIN Batch 11/10600 loss 15.178082 loss_att 18.870586 loss_ctc 20.688772 loss_rnnt 13.453702 hw_loss 0.470850 lr 0.00050750 rank 2
2023-02-20 22:37:58,882 DEBUG TRAIN Batch 11/10600 loss 113.393204 loss_att 101.218414 loss_ctc 118.643661 loss_rnnt 115.041153 hw_loss 0.163029 lr 0.00050750 rank 6
2023-02-20 22:37:58,885 DEBUG TRAIN Batch 11/10600 loss 7.221411 loss_att 14.672526 loss_ctc 8.199421 loss_rnnt 5.421236 hw_loss 0.336656 lr 0.00050750 rank 5
2023-02-20 22:37:58,887 DEBUG TRAIN Batch 11/10600 loss 16.706440 loss_att 21.989214 loss_ctc 10.386509 loss_rnnt 16.491184 hw_loss 0.002547 lr 0.00050750 rank 3
2023-02-20 22:37:58,890 DEBUG TRAIN Batch 11/10600 loss 13.934747 loss_att 29.729782 loss_ctc 22.137377 loss_rnnt 9.298292 hw_loss 0.719555 lr 0.00050750 rank 7
2023-02-20 22:37:58,891 DEBUG TRAIN Batch 11/10600 loss 12.529290 loss_att 14.447409 loss_ctc 14.448786 loss_rnnt 11.704696 hw_loss 0.346947 lr 0.00050750 rank 4
2023-02-20 22:37:58,911 DEBUG TRAIN Batch 11/10600 loss 21.884260 loss_att 27.281225 loss_ctc 17.222301 loss_rnnt 21.218220 hw_loss 0.390452 lr 0.00050750 rank 0
2023-02-20 22:37:58,929 DEBUG TRAIN Batch 11/10600 loss 27.093714 loss_att 25.308428 loss_ctc 35.042416 loss_rnnt 26.295250 hw_loss 0.179423 lr 0.00050750 rank 1
2023-02-20 22:38:56,749 DEBUG TRAIN Batch 11/10700 loss 10.843498 loss_att 15.078008 loss_ctc 12.744240 loss_rnnt 9.634081 hw_loss 0.204530 lr 0.00050737 rank 0
2023-02-20 22:38:56,750 DEBUG TRAIN Batch 11/10700 loss 20.898756 loss_att 21.984777 loss_ctc 23.471224 loss_rnnt 20.119621 hw_loss 0.410506 lr 0.00050737 rank 2
2023-02-20 22:38:56,755 DEBUG TRAIN Batch 11/10700 loss 6.726795 loss_att 10.758892 loss_ctc 7.571615 loss_rnnt 5.670430 hw_loss 0.257442 lr 0.00050737 rank 6
2023-02-20 22:38:56,761 DEBUG TRAIN Batch 11/10700 loss 26.691557 loss_att 23.871471 loss_ctc 29.559170 loss_rnnt 26.714939 hw_loss 0.296784 lr 0.00050737 rank 5
2023-02-20 22:38:56,761 DEBUG TRAIN Batch 11/10700 loss 31.377420 loss_att 37.782417 loss_ctc 38.393822 loss_rnnt 29.129129 hw_loss 0.059571 lr 0.00050737 rank 3
2023-02-20 22:38:56,767 DEBUG TRAIN Batch 11/10700 loss 45.484329 loss_att 62.746490 loss_ctc 44.874702 loss_rnnt 41.998909 hw_loss 0.214249 lr 0.00050737 rank 1
2023-02-20 22:38:56,768 DEBUG TRAIN Batch 11/10700 loss 11.323164 loss_att 24.494135 loss_ctc 11.734573 loss_rnnt 8.632011 hw_loss 0.003943 lr 0.00050737 rank 4
2023-02-20 22:38:56,772 DEBUG TRAIN Batch 11/10700 loss 29.752920 loss_att 31.798767 loss_ctc 32.706177 loss_rnnt 28.856377 hw_loss 0.175507 lr 0.00050737 rank 7
2023-02-20 22:39:57,369 DEBUG TRAIN Batch 11/10800 loss 32.781200 loss_att 55.377586 loss_ctc 43.239574 loss_rnnt 26.697628 hw_loss 0.318461 lr 0.00050724 rank 0
2023-02-20 22:39:57,381 DEBUG TRAIN Batch 11/10800 loss 30.395222 loss_att 35.062775 loss_ctc 37.752140 loss_rnnt 28.257042 hw_loss 0.419526 lr 0.00050724 rank 2
2023-02-20 22:39:57,381 DEBUG TRAIN Batch 11/10800 loss 1.526240 loss_att 6.219646 loss_ctc 1.458062 loss_rnnt 0.595448 hw_loss 0.002252 lr 0.00050724 rank 3
2023-02-20 22:39:57,385 DEBUG TRAIN Batch 11/10800 loss 23.331154 loss_att 28.606823 loss_ctc 24.671812 loss_rnnt 21.976408 hw_loss 0.226613 lr 0.00050724 rank 4
2023-02-20 22:39:57,386 DEBUG TRAIN Batch 11/10800 loss 42.285397 loss_att 44.022884 loss_ctc 41.973423 loss_rnnt 41.841068 hw_loss 0.259549 lr 0.00050724 rank 6
2023-02-20 22:39:57,393 DEBUG TRAIN Batch 11/10800 loss 23.293854 loss_att 31.179564 loss_ctc 31.405231 loss_rnnt 20.370346 hw_loss 0.496589 lr 0.00050724 rank 7
2023-02-20 22:39:57,398 DEBUG TRAIN Batch 11/10800 loss 9.583710 loss_att 18.950069 loss_ctc 7.203283 loss_rnnt 7.906897 hw_loss 0.226747 lr 0.00050724 rank 5
2023-02-20 22:39:57,445 DEBUG TRAIN Batch 11/10800 loss 32.580673 loss_att 37.446320 loss_ctc 28.511694 loss_rnnt 32.012619 hw_loss 0.257730 lr 0.00050724 rank 1
2023-02-20 22:40:56,265 DEBUG TRAIN Batch 11/10900 loss 22.334661 loss_att 32.347153 loss_ctc 21.626898 loss_rnnt 20.227444 hw_loss 0.373287 lr 0.00050711 rank 0
2023-02-20 22:40:56,273 DEBUG TRAIN Batch 11/10900 loss 21.780125 loss_att 32.937939 loss_ctc 32.294300 loss_rnnt 17.972191 hw_loss 0.327152 lr 0.00050711 rank 5
2023-02-20 22:40:56,277 DEBUG TRAIN Batch 11/10900 loss 30.595581 loss_att 29.417206 loss_ctc 34.478218 loss_rnnt 30.312931 hw_loss 0.001198 lr 0.00050711 rank 3
2023-02-20 22:40:56,281 DEBUG TRAIN Batch 11/10900 loss 10.141031 loss_att 10.242208 loss_ctc 10.512553 loss_rnnt 10.070621 hw_loss 0.001198 lr 0.00050711 rank 7
2023-02-20 22:40:56,282 DEBUG TRAIN Batch 11/10900 loss 13.796995 loss_att 27.140738 loss_ctc 9.996131 loss_rnnt 11.634391 hw_loss 0.001198 lr 0.00050711 rank 2
2023-02-20 22:40:56,282 DEBUG TRAIN Batch 11/10900 loss 83.493919 loss_att 92.958252 loss_ctc 103.800697 loss_rnnt 78.671623 hw_loss 0.415976 lr 0.00050711 rank 6
2023-02-20 22:40:56,283 DEBUG TRAIN Batch 11/10900 loss 11.924472 loss_att 18.528374 loss_ctc 11.504751 loss_rnnt 10.498728 hw_loss 0.301737 lr 0.00050711 rank 4
2023-02-20 22:40:56,342 DEBUG TRAIN Batch 11/10900 loss 25.073622 loss_att 36.925652 loss_ctc 26.751070 loss_rnnt 22.478918 hw_loss 0.001198 lr 0.00050711 rank 1
2023-02-20 22:41:54,386 DEBUG TRAIN Batch 11/11000 loss 11.756952 loss_att 15.914768 loss_ctc 12.062241 loss_rnnt 10.756809 hw_loss 0.239764 lr 0.00050698 rank 6
2023-02-20 22:41:54,389 DEBUG TRAIN Batch 11/11000 loss 11.124849 loss_att 15.830755 loss_ctc 10.826698 loss_rnnt 10.081457 hw_loss 0.266185 lr 0.00050698 rank 5
2023-02-20 22:41:54,391 DEBUG TRAIN Batch 11/11000 loss 10.216307 loss_att 22.313553 loss_ctc 10.604984 loss_rnnt 7.500865 hw_loss 0.457817 lr 0.00050698 rank 0
2023-02-20 22:41:54,393 DEBUG TRAIN Batch 11/11000 loss 41.868416 loss_att 48.195602 loss_ctc 50.969482 loss_rnnt 39.389305 hw_loss 0.000373 lr 0.00050698 rank 4
2023-02-20 22:41:54,393 DEBUG TRAIN Batch 11/11000 loss 35.394272 loss_att 44.809978 loss_ctc 38.839787 loss_rnnt 32.876293 hw_loss 0.328946 lr 0.00050698 rank 3
2023-02-20 22:41:54,396 DEBUG TRAIN Batch 11/11000 loss 5.733693 loss_att 11.295847 loss_ctc 8.007526 loss_rnnt 4.163229 hw_loss 0.290352 lr 0.00050698 rank 7
2023-02-20 22:41:54,397 DEBUG TRAIN Batch 11/11000 loss 23.395960 loss_att 37.218704 loss_ctc 31.275072 loss_rnnt 19.372829 hw_loss 0.390064 lr 0.00050698 rank 2
2023-02-20 22:41:54,460 DEBUG TRAIN Batch 11/11000 loss 35.408855 loss_att 46.984615 loss_ctc 28.149849 loss_rnnt 33.897903 hw_loss 0.306874 lr 0.00050698 rank 1
2023-02-20 22:42:54,675 DEBUG TRAIN Batch 11/11100 loss 20.375166 loss_att 35.904762 loss_ctc 18.839731 loss_rnnt 17.345636 hw_loss 0.240628 lr 0.00050685 rank 0
2023-02-20 22:42:54,678 DEBUG TRAIN Batch 11/11100 loss 20.033119 loss_att 28.028959 loss_ctc 17.109604 loss_rnnt 18.700260 hw_loss 0.231549 lr 0.00050685 rank 6
2023-02-20 22:42:54,680 DEBUG TRAIN Batch 11/11100 loss 4.140545 loss_att 7.901193 loss_ctc 1.646126 loss_rnnt 3.572349 hw_loss 0.278729 lr 0.00050685 rank 2
2023-02-20 22:42:54,681 DEBUG TRAIN Batch 11/11100 loss 17.479645 loss_att 29.338856 loss_ctc 20.187654 loss_rnnt 14.536934 hw_loss 0.393376 lr 0.00050685 rank 3
2023-02-20 22:42:54,682 DEBUG TRAIN Batch 11/11100 loss 10.705856 loss_att 16.869120 loss_ctc 10.179616 loss_rnnt 9.350190 hw_loss 0.362210 lr 0.00050685 rank 5
2023-02-20 22:42:54,687 DEBUG TRAIN Batch 11/11100 loss 13.468484 loss_att 22.536839 loss_ctc 20.609472 loss_rnnt 10.546350 hw_loss 0.293121 lr 0.00050685 rank 1
2023-02-20 22:42:54,688 DEBUG TRAIN Batch 11/11100 loss 25.246113 loss_att 31.562531 loss_ctc 36.785583 loss_rnnt 22.333698 hw_loss 0.207254 lr 0.00050685 rank 7
2023-02-20 22:42:54,699 DEBUG TRAIN Batch 11/11100 loss 32.390049 loss_att 42.924168 loss_ctc 35.136261 loss_rnnt 29.916615 hw_loss 0.000840 lr 0.00050685 rank 4
2023-02-20 22:44:11,886 DEBUG TRAIN Batch 11/11200 loss 9.280861 loss_att 14.778170 loss_ctc 12.337583 loss_rnnt 7.773378 hw_loss 0.000858 lr 0.00050672 rank 5
2023-02-20 22:44:11,893 DEBUG TRAIN Batch 11/11200 loss 27.606928 loss_att 35.178612 loss_ctc 25.065941 loss_rnnt 26.430931 hw_loss 0.000858 lr 0.00050672 rank 0
2023-02-20 22:44:11,901 DEBUG TRAIN Batch 11/11200 loss 13.836780 loss_att 30.943386 loss_ctc 13.287615 loss_rnnt 10.411486 hw_loss 0.144739 lr 0.00050672 rank 7
2023-02-20 22:44:11,905 DEBUG TRAIN Batch 11/11200 loss 4.355268 loss_att 10.845240 loss_ctc 6.572322 loss_rnnt 2.761209 hw_loss 0.000858 lr 0.00050672 rank 6
2023-02-20 22:44:11,904 DEBUG TRAIN Batch 11/11200 loss 42.217770 loss_att 56.545322 loss_ctc 48.184788 loss_rnnt 38.556198 hw_loss 0.000858 lr 0.00050672 rank 3
2023-02-20 22:44:11,924 DEBUG TRAIN Batch 11/11200 loss 29.496557 loss_att 33.297642 loss_ctc 32.060047 loss_rnnt 28.180199 hw_loss 0.401892 lr 0.00050672 rank 2
2023-02-20 22:44:11,946 DEBUG TRAIN Batch 11/11200 loss 13.469273 loss_att 12.238109 loss_ctc 15.020569 loss_rnnt 13.242724 hw_loss 0.498640 lr 0.00050672 rank 1
2023-02-20 22:44:11,963 DEBUG TRAIN Batch 11/11200 loss 21.432791 loss_att 16.901876 loss_ctc 23.184584 loss_rnnt 22.104942 hw_loss 0.000858 lr 0.00050672 rank 4
2023-02-20 22:45:14,921 DEBUG TRAIN Batch 11/11300 loss 5.756959 loss_att 14.346781 loss_ctc 5.966519 loss_rnnt 3.888268 hw_loss 0.230224 lr 0.00050659 rank 3
2023-02-20 22:45:14,922 DEBUG TRAIN Batch 11/11300 loss 17.806177 loss_att 21.940271 loss_ctc 19.954697 loss_rnnt 16.600754 hw_loss 0.172754 lr 0.00050659 rank 6
2023-02-20 22:45:14,923 DEBUG TRAIN Batch 11/11300 loss 7.604168 loss_att 11.802242 loss_ctc 11.262022 loss_rnnt 6.118372 hw_loss 0.297124 lr 0.00050659 rank 0
2023-02-20 22:45:14,924 DEBUG TRAIN Batch 11/11300 loss 22.594036 loss_att 30.612289 loss_ctc 23.450245 loss_rnnt 20.786434 hw_loss 0.168354 lr 0.00050659 rank 1
2023-02-20 22:45:14,929 DEBUG TRAIN Batch 11/11300 loss 16.261263 loss_att 17.928764 loss_ctc 16.341213 loss_rnnt 15.916827 hw_loss 0.000518 lr 0.00050659 rank 5
2023-02-20 22:45:14,932 DEBUG TRAIN Batch 11/11300 loss 21.370613 loss_att 26.075262 loss_ctc 21.142899 loss_rnnt 20.359728 hw_loss 0.188094 lr 0.00050659 rank 2
2023-02-20 22:45:14,936 DEBUG TRAIN Batch 11/11300 loss 27.132570 loss_att 32.199284 loss_ctc 34.220512 loss_rnnt 24.979477 hw_loss 0.365046 lr 0.00050659 rank 4
2023-02-20 22:45:14,992 DEBUG TRAIN Batch 11/11300 loss 22.388727 loss_att 23.875565 loss_ctc 21.452990 loss_rnnt 21.983528 hw_loss 0.436116 lr 0.00050659 rank 7
2023-02-20 22:46:16,017 DEBUG TRAIN Batch 11/11400 loss 6.627799 loss_att 11.418990 loss_ctc 9.240351 loss_rnnt 5.193441 hw_loss 0.239586 lr 0.00050646 rank 0
2023-02-20 22:46:16,017 DEBUG TRAIN Batch 11/11400 loss 22.795225 loss_att 23.585995 loss_ctc 26.419207 loss_rnnt 22.021633 hw_loss 0.247955 lr 0.00050646 rank 3
2023-02-20 22:46:16,021 DEBUG TRAIN Batch 11/11400 loss 17.940649 loss_att 33.404739 loss_ctc 27.325499 loss_rnnt 13.386039 hw_loss 0.394648 lr 0.00050646 rank 5
2023-02-20 22:46:16,025 DEBUG TRAIN Batch 11/11400 loss 39.858456 loss_att 55.232494 loss_ctc 51.964691 loss_rnnt 34.987808 hw_loss 0.340643 lr 0.00050646 rank 2
2023-02-20 22:46:16,025 DEBUG TRAIN Batch 11/11400 loss 20.844227 loss_att 26.905844 loss_ctc 22.749081 loss_rnnt 19.186958 hw_loss 0.358055 lr 0.00050646 rank 7
2023-02-20 22:46:16,026 DEBUG TRAIN Batch 11/11400 loss 11.095941 loss_att 17.216604 loss_ctc 15.926129 loss_rnnt 9.082752 hw_loss 0.271930 lr 0.00050646 rank 4
2023-02-20 22:46:16,028 DEBUG TRAIN Batch 11/11400 loss 19.292904 loss_att 28.994730 loss_ctc 16.043221 loss_rnnt 17.784609 hw_loss 0.002289 lr 0.00050646 rank 1
2023-02-20 22:46:16,029 DEBUG TRAIN Batch 11/11400 loss 4.852519 loss_att 13.060032 loss_ctc 5.716520 loss_rnnt 3.060901 hw_loss 0.065466 lr 0.00050646 rank 6
2023-02-20 22:47:13,642 DEBUG TRAIN Batch 11/11500 loss 26.800200 loss_att 27.339964 loss_ctc 30.547541 loss_rnnt 25.961187 hw_loss 0.433904 lr 0.00050633 rank 0
2023-02-20 22:47:13,659 DEBUG TRAIN Batch 11/11500 loss 26.042274 loss_att 27.757771 loss_ctc 28.623768 loss_rnnt 25.121834 hw_loss 0.437139 lr 0.00050633 rank 1
2023-02-20 22:47:13,659 DEBUG TRAIN Batch 11/11500 loss 14.050492 loss_att 18.785730 loss_ctc 16.752981 loss_rnnt 12.573468 hw_loss 0.318083 lr 0.00050633 rank 2
2023-02-20 22:47:13,660 DEBUG TRAIN Batch 11/11500 loss 21.582991 loss_att 28.645161 loss_ctc 21.170189 loss_rnnt 20.037769 hw_loss 0.352177 lr 0.00050633 rank 4
2023-02-20 22:47:13,660 DEBUG TRAIN Batch 11/11500 loss 42.785782 loss_att 59.657318 loss_ctc 43.807938 loss_rnnt 39.274193 hw_loss 0.001872 lr 0.00050633 rank 6
2023-02-20 22:47:13,661 DEBUG TRAIN Batch 11/11500 loss 18.017912 loss_att 17.748926 loss_ctc 23.834723 loss_rnnt 17.096836 hw_loss 0.373684 lr 0.00050633 rank 3
2023-02-20 22:47:13,664 DEBUG TRAIN Batch 11/11500 loss 5.542435 loss_att 18.981321 loss_ctc 3.216834 loss_rnnt 2.946058 hw_loss 0.410023 lr 0.00050633 rank 7
2023-02-20 22:47:13,717 DEBUG TRAIN Batch 11/11500 loss 63.282310 loss_att 74.609154 loss_ctc 63.619122 loss_rnnt 60.971035 hw_loss 0.001872 lr 0.00050633 rank 5
2023-02-20 22:48:12,283 DEBUG TRAIN Batch 11/11600 loss 11.671033 loss_att 21.455866 loss_ctc 16.827398 loss_rnnt 8.889079 hw_loss 0.257759 lr 0.00050620 rank 0
2023-02-20 22:48:12,294 DEBUG TRAIN Batch 11/11600 loss 10.747963 loss_att 12.978236 loss_ctc 13.595469 loss_rnnt 9.800672 hw_loss 0.227943 lr 0.00050620 rank 6
2023-02-20 22:48:12,295 DEBUG TRAIN Batch 11/11600 loss 22.281107 loss_att 30.188763 loss_ctc 20.088972 loss_rnnt 20.837116 hw_loss 0.290145 lr 0.00050620 rank 3
2023-02-20 22:48:12,297 DEBUG TRAIN Batch 11/11600 loss 31.117191 loss_att 45.089863 loss_ctc 40.019913 loss_rnnt 27.013317 hw_loss 0.229333 lr 0.00050620 rank 4
2023-02-20 22:48:12,301 DEBUG TRAIN Batch 11/11600 loss 33.148293 loss_att 29.209835 loss_ctc 35.161674 loss_rnnt 33.537617 hw_loss 0.243592 lr 0.00050620 rank 2
2023-02-20 22:48:12,308 DEBUG TRAIN Batch 11/11600 loss 17.778721 loss_att 23.556858 loss_ctc 19.664858 loss_rnnt 16.202101 hw_loss 0.317826 lr 0.00050620 rank 7
2023-02-20 22:48:12,308 DEBUG TRAIN Batch 11/11600 loss 26.423899 loss_att 28.902988 loss_ctc 30.896515 loss_rnnt 25.159138 hw_loss 0.323616 lr 0.00050620 rank 1
2023-02-20 22:48:12,356 DEBUG TRAIN Batch 11/11600 loss 22.460886 loss_att 32.871330 loss_ctc 27.141270 loss_rnnt 19.590193 hw_loss 0.308538 lr 0.00050620 rank 5
2023-02-20 22:49:13,399 DEBUG TRAIN Batch 11/11700 loss 24.779226 loss_att 23.882702 loss_ctc 32.674793 loss_rnnt 23.706728 hw_loss 0.373240 lr 0.00050607 rank 0
2023-02-20 22:49:13,400 DEBUG TRAIN Batch 11/11700 loss 14.422797 loss_att 22.394436 loss_ctc 18.845833 loss_rnnt 12.161210 hw_loss 0.145353 lr 0.00050607 rank 6
2023-02-20 22:49:13,403 DEBUG TRAIN Batch 11/11700 loss 9.229910 loss_att 14.203600 loss_ctc 4.940971 loss_rnnt 8.620948 hw_loss 0.348905 lr 0.00050607 rank 2
2023-02-20 22:49:13,405 DEBUG TRAIN Batch 11/11700 loss 24.961542 loss_att 34.066864 loss_ctc 25.315746 loss_rnnt 23.031593 hw_loss 0.115607 lr 0.00050607 rank 1
2023-02-20 22:49:13,406 DEBUG TRAIN Batch 11/11700 loss 41.380505 loss_att 49.669159 loss_ctc 59.658333 loss_rnnt 37.003387 hw_loss 0.529399 lr 0.00050607 rank 4
2023-02-20 22:49:13,405 DEBUG TRAIN Batch 11/11700 loss 3.086430 loss_att 8.663856 loss_ctc 4.101624 loss_rnnt 1.630864 hw_loss 0.383852 lr 0.00050607 rank 3
2023-02-20 22:49:13,413 DEBUG TRAIN Batch 11/11700 loss 34.323997 loss_att 42.147186 loss_ctc 37.135124 loss_rnnt 32.214188 hw_loss 0.319415 lr 0.00050607 rank 7
2023-02-20 22:49:13,466 DEBUG TRAIN Batch 11/11700 loss 19.740747 loss_att 18.650059 loss_ctc 18.965313 loss_rnnt 20.058239 hw_loss 0.007571 lr 0.00050607 rank 5
2023-02-20 22:50:11,054 DEBUG TRAIN Batch 11/11800 loss 19.734653 loss_att 21.600895 loss_ctc 22.505148 loss_rnnt 18.724419 hw_loss 0.501722 lr 0.00050594 rank 0
2023-02-20 22:50:11,056 DEBUG TRAIN Batch 11/11800 loss 27.377953 loss_att 27.219307 loss_ctc 27.035879 loss_rnnt 27.295891 hw_loss 0.298873 lr 0.00050594 rank 2
2023-02-20 22:50:11,057 DEBUG TRAIN Batch 11/11800 loss 34.319000 loss_att 34.827263 loss_ctc 34.097153 loss_rnnt 34.098240 hw_loss 0.278783 lr 0.00050594 rank 5
2023-02-20 22:50:11,064 DEBUG TRAIN Batch 11/11800 loss 14.304378 loss_att 16.137009 loss_ctc 16.344017 loss_rnnt 13.464480 hw_loss 0.377661 lr 0.00050594 rank 3
2023-02-20 22:50:11,066 DEBUG TRAIN Batch 11/11800 loss 39.202419 loss_att 56.610115 loss_ctc 46.815773 loss_rnnt 34.495335 hw_loss 0.394556 lr 0.00050594 rank 6
2023-02-20 22:50:11,069 DEBUG TRAIN Batch 11/11800 loss 14.157509 loss_att 18.641016 loss_ctc 11.120934 loss_rnnt 13.664272 hw_loss 0.002647 lr 0.00050594 rank 4
2023-02-20 22:50:11,070 DEBUG TRAIN Batch 11/11800 loss 4.162175 loss_att 8.473207 loss_ctc 5.891397 loss_rnnt 2.897995 hw_loss 0.321394 lr 0.00050594 rank 7
2023-02-20 22:50:11,116 DEBUG TRAIN Batch 11/11800 loss 27.957273 loss_att 28.041653 loss_ctc 27.804375 loss_rnnt 27.773962 hw_loss 0.350289 lr 0.00050594 rank 1
2023-02-20 22:51:10,825 DEBUG TRAIN Batch 11/11900 loss 11.771852 loss_att 14.922878 loss_ctc 13.219288 loss_rnnt 10.801073 hw_loss 0.276716 lr 0.00050581 rank 0
2023-02-20 22:51:10,830 DEBUG TRAIN Batch 11/11900 loss 11.469802 loss_att 17.437263 loss_ctc 11.651784 loss_rnnt 10.034707 hw_loss 0.407507 lr 0.00050581 rank 1
2023-02-20 22:51:10,832 DEBUG TRAIN Batch 11/11900 loss 25.627645 loss_att 32.762478 loss_ctc 31.238531 loss_rnnt 23.452332 hw_loss 0.000428 lr 0.00050581 rank 2
2023-02-20 22:51:10,836 DEBUG TRAIN Batch 11/11900 loss 17.103334 loss_att 27.771927 loss_ctc 23.373026 loss_rnnt 14.031504 hw_loss 0.191537 lr 0.00050581 rank 3
2023-02-20 22:51:10,841 DEBUG TRAIN Batch 11/11900 loss 17.811541 loss_att 17.255068 loss_ctc 28.864491 loss_rnnt 16.448879 hw_loss 0.000428 lr 0.00050581 rank 5
2023-02-20 22:51:10,843 DEBUG TRAIN Batch 11/11900 loss 25.035795 loss_att 31.363359 loss_ctc 23.889107 loss_rnnt 23.686508 hw_loss 0.443750 lr 0.00050581 rank 7
2023-02-20 22:51:10,844 DEBUG TRAIN Batch 11/11900 loss 13.773376 loss_att 20.486874 loss_ctc 14.590288 loss_rnnt 12.121992 hw_loss 0.374555 lr 0.00050581 rank 4
2023-02-20 22:51:10,845 DEBUG TRAIN Batch 11/11900 loss 39.562527 loss_att 41.685238 loss_ctc 46.530113 loss_rnnt 38.180275 hw_loss 0.053818 lr 0.00050581 rank 6
2023-02-20 22:52:12,457 DEBUG TRAIN Batch 11/12000 loss 21.517157 loss_att 32.107975 loss_ctc 23.885374 loss_rnnt 18.879114 hw_loss 0.382716 lr 0.00050568 rank 6
2023-02-20 22:52:12,460 DEBUG TRAIN Batch 11/12000 loss 8.279348 loss_att 21.758633 loss_ctc 9.377340 loss_rnnt 5.435845 hw_loss 0.002336 lr 0.00050568 rank 1
2023-02-20 22:52:12,462 DEBUG TRAIN Batch 11/12000 loss 21.666637 loss_att 25.843960 loss_ctc 26.712149 loss_rnnt 20.157192 hw_loss 0.002336 lr 0.00050568 rank 0
2023-02-20 22:52:12,464 DEBUG TRAIN Batch 11/12000 loss 25.139891 loss_att 31.750271 loss_ctc 34.214996 loss_rnnt 22.406813 hw_loss 0.376853 lr 0.00050568 rank 4
2023-02-20 22:52:12,464 DEBUG TRAIN Batch 11/12000 loss 10.052883 loss_att 11.305804 loss_ctc 9.928937 loss_rnnt 9.611252 hw_loss 0.389202 lr 0.00050568 rank 3
2023-02-20 22:52:12,466 DEBUG TRAIN Batch 11/12000 loss 16.158510 loss_att 22.322088 loss_ctc 22.182528 loss_rnnt 14.121346 hw_loss 0.002336 lr 0.00050568 rank 7
2023-02-20 22:52:12,474 DEBUG TRAIN Batch 11/12000 loss 20.466869 loss_att 28.210600 loss_ctc 22.346811 loss_rnnt 18.666218 hw_loss 0.002336 lr 0.00050568 rank 2
2023-02-20 22:52:12,518 DEBUG TRAIN Batch 11/12000 loss 25.357574 loss_att 41.599415 loss_ctc 22.897102 loss_rnnt 22.351011 hw_loss 0.161733 lr 0.00050568 rank 5
2023-02-20 22:53:31,123 DEBUG TRAIN Batch 11/12100 loss 9.003042 loss_att 17.350613 loss_ctc 6.365609 loss_rnnt 7.534705 hw_loss 0.282151 lr 0.00050555 rank 6
2023-02-20 22:53:31,123 DEBUG TRAIN Batch 11/12100 loss 33.769012 loss_att 41.045738 loss_ctc 41.117996 loss_rnnt 31.229750 hw_loss 0.195099 lr 0.00050555 rank 1
2023-02-20 22:53:31,125 DEBUG TRAIN Batch 11/12100 loss 11.304315 loss_att 13.918557 loss_ctc 9.035076 loss_rnnt 10.895413 hw_loss 0.353657 lr 0.00050555 rank 0
2023-02-20 22:53:31,125 DEBUG TRAIN Batch 11/12100 loss 12.867345 loss_att 20.070332 loss_ctc 16.882513 loss_rnnt 10.730918 hw_loss 0.300888 lr 0.00050555 rank 2
2023-02-20 22:53:31,133 DEBUG TRAIN Batch 11/12100 loss 11.648234 loss_att 17.487345 loss_ctc 13.778312 loss_rnnt 10.025515 hw_loss 0.320413 lr 0.00050555 rank 4
2023-02-20 22:53:31,134 DEBUG TRAIN Batch 11/12100 loss 9.459390 loss_att 11.339058 loss_ctc 9.953828 loss_rnnt 8.762957 hw_loss 0.477326 lr 0.00050555 rank 5
2023-02-20 22:53:31,135 DEBUG TRAIN Batch 11/12100 loss 23.148687 loss_att 28.266014 loss_ctc 28.539371 loss_rnnt 21.171574 hw_loss 0.440423 lr 0.00050555 rank 3
2023-02-20 22:53:31,195 DEBUG TRAIN Batch 11/12100 loss 4.470277 loss_att 8.440916 loss_ctc 6.594930 loss_rnnt 3.335104 hw_loss 0.108297 lr 0.00050555 rank 7
2023-02-20 22:54:44,497 DEBUG TRAIN Batch 11/12200 loss 31.113960 loss_att 45.334854 loss_ctc 33.128719 loss_rnnt 27.936787 hw_loss 0.120672 lr 0.00050542 rank 0
2023-02-20 22:54:44,504 DEBUG TRAIN Batch 11/12200 loss 21.268871 loss_att 33.915680 loss_ctc 18.884537 loss_rnnt 18.885704 hw_loss 0.321964 lr 0.00050542 rank 2
2023-02-20 22:54:44,505 DEBUG TRAIN Batch 11/12200 loss 20.908651 loss_att 22.544691 loss_ctc 24.658684 loss_rnnt 20.007095 hw_loss 0.139394 lr 0.00050542 rank 3
2023-02-20 22:54:44,506 DEBUG TRAIN Batch 11/12200 loss 25.217407 loss_att 26.790707 loss_ctc 21.700182 loss_rnnt 25.204838 hw_loss 0.312885 lr 0.00050542 rank 6
2023-02-20 22:54:44,506 DEBUG TRAIN Batch 11/12200 loss 17.353973 loss_att 20.930525 loss_ctc 17.194057 loss_rnnt 16.402388 hw_loss 0.482996 lr 0.00050542 rank 5
2023-02-20 22:54:44,506 DEBUG TRAIN Batch 11/12200 loss 18.346918 loss_att 26.080765 loss_ctc 21.247250 loss_rnnt 16.237078 hw_loss 0.330672 lr 0.00050542 rank 1
2023-02-20 22:54:44,517 DEBUG TRAIN Batch 11/12200 loss 13.935861 loss_att 20.702362 loss_ctc 13.907596 loss_rnnt 12.332127 hw_loss 0.476629 lr 0.00050542 rank 4
2023-02-20 22:54:44,525 DEBUG TRAIN Batch 11/12200 loss 27.997734 loss_att 28.075163 loss_ctc 29.261374 loss_rnnt 27.665672 hw_loss 0.277669 lr 0.00050542 rank 7
2023-02-20 22:55:45,033 DEBUG TRAIN Batch 11/12300 loss 15.439431 loss_att 21.708832 loss_ctc 15.048812 loss_rnnt 14.236709 hw_loss 0.001735 lr 0.00050530 rank 0
2023-02-20 22:55:45,038 DEBUG TRAIN Batch 11/12300 loss 23.638365 loss_att 30.359867 loss_ctc 28.267292 loss_rnnt 21.506311 hw_loss 0.319806 lr 0.00050530 rank 5
2023-02-20 22:55:45,037 DEBUG TRAIN Batch 11/12300 loss 15.798271 loss_att 19.487041 loss_ctc 19.306486 loss_rnnt 14.391744 hw_loss 0.376895 lr 0.00050530 rank 2
2023-02-20 22:55:45,039 DEBUG TRAIN Batch 11/12300 loss 20.528624 loss_att 24.479603 loss_ctc 15.254226 loss_rnnt 20.238073 hw_loss 0.381767 lr 0.00050530 rank 3
2023-02-20 22:55:45,041 DEBUG TRAIN Batch 11/12300 loss 35.514473 loss_att 32.984684 loss_ctc 39.005394 loss_rnnt 35.554047 hw_loss 0.001735 lr 0.00050530 rank 4
2023-02-20 22:55:45,042 DEBUG TRAIN Batch 11/12300 loss 25.025906 loss_att 25.676956 loss_ctc 33.518177 loss_rnnt 23.762466 hw_loss 0.001735 lr 0.00050530 rank 7
2023-02-20 22:55:45,044 DEBUG TRAIN Batch 11/12300 loss 19.193565 loss_att 30.766708 loss_ctc 18.456150 loss_rnnt 16.976334 hw_loss 0.001735 lr 0.00050530 rank 6
2023-02-20 22:55:45,099 DEBUG TRAIN Batch 11/12300 loss 2.675978 loss_att 7.298489 loss_ctc 3.024114 loss_rnnt 1.554316 hw_loss 0.282641 lr 0.00050530 rank 1
2023-02-20 22:56:43,208 DEBUG TRAIN Batch 11/12400 loss 9.285324 loss_att 11.371065 loss_ctc 10.242182 loss_rnnt 8.557168 hw_loss 0.343924 lr 0.00050517 rank 0
2023-02-20 22:56:43,211 DEBUG TRAIN Batch 11/12400 loss 9.815688 loss_att 11.861212 loss_ctc 8.059549 loss_rnnt 9.640144 hw_loss 0.001109 lr 0.00050517 rank 2
2023-02-20 22:56:43,218 DEBUG TRAIN Batch 11/12400 loss 11.859322 loss_att 21.224503 loss_ctc 14.012918 loss_rnnt 9.508796 hw_loss 0.356893 lr 0.00050517 rank 4
2023-02-20 22:56:43,218 DEBUG TRAIN Batch 11/12400 loss 20.308096 loss_att 24.368382 loss_ctc 20.413296 loss_rnnt 19.276899 hw_loss 0.384585 lr 0.00050517 rank 1
2023-02-20 22:56:43,221 DEBUG TRAIN Batch 11/12400 loss 30.412771 loss_att 38.605671 loss_ctc 33.097622 loss_rnnt 28.178684 hw_loss 0.445362 lr 0.00050517 rank 3
2023-02-20 22:56:43,221 DEBUG TRAIN Batch 11/12400 loss 24.446123 loss_att 34.328026 loss_ctc 22.531403 loss_rnnt 22.605190 hw_loss 0.224720 lr 0.00050517 rank 7
2023-02-20 22:56:43,222 DEBUG TRAIN Batch 11/12400 loss 14.874860 loss_att 19.479460 loss_ctc 21.963291 loss_rnnt 12.925500 hw_loss 0.156216 lr 0.00050517 rank 6
2023-02-20 22:56:43,225 DEBUG TRAIN Batch 11/12400 loss 18.646057 loss_att 21.753078 loss_ctc 20.208239 loss_rnnt 17.631298 hw_loss 0.346993 lr 0.00050517 rank 5
2023-02-20 22:57:43,281 DEBUG TRAIN Batch 11/12500 loss 31.049288 loss_att 35.601036 loss_ctc 37.098450 loss_rnnt 29.213707 hw_loss 0.222518 lr 0.00050504 rank 0
2023-02-20 22:57:43,292 DEBUG TRAIN Batch 11/12500 loss 29.073166 loss_att 36.317352 loss_ctc 37.569534 loss_rnnt 26.417944 hw_loss 0.137876 lr 0.00050504 rank 5
2023-02-20 22:57:43,295 DEBUG TRAIN Batch 11/12500 loss 8.625425 loss_att 10.953073 loss_ctc 7.682843 loss_rnnt 8.147596 hw_loss 0.258707 lr 0.00050504 rank 3
2023-02-20 22:57:43,296 DEBUG TRAIN Batch 11/12500 loss 21.682287 loss_att 25.418163 loss_ctc 27.566925 loss_rnnt 19.948307 hw_loss 0.379098 lr 0.00050504 rank 2
2023-02-20 22:57:43,297 DEBUG TRAIN Batch 11/12500 loss 13.877368 loss_att 19.744543 loss_ctc 16.107615 loss_rnnt 12.129859 hw_loss 0.518828 lr 0.00050504 rank 7
2023-02-20 22:57:43,300 DEBUG TRAIN Batch 11/12500 loss 15.904172 loss_att 18.304029 loss_ctc 19.689188 loss_rnnt 14.745366 hw_loss 0.326559 lr 0.00050504 rank 6
2023-02-20 22:57:43,303 DEBUG TRAIN Batch 11/12500 loss 65.790451 loss_att 98.878334 loss_ctc 87.992973 loss_rnnt 56.025528 hw_loss 0.350647 lr 0.00050504 rank 1
2023-02-20 22:57:43,357 DEBUG TRAIN Batch 11/12500 loss 37.627754 loss_att 36.117325 loss_ctc 32.220718 loss_rnnt 38.650620 hw_loss 0.000294 lr 0.00050504 rank 4
2023-02-20 22:58:43,252 DEBUG TRAIN Batch 11/12600 loss 22.020493 loss_att 31.702122 loss_ctc 26.259424 loss_rnnt 19.285469 hw_loss 0.437821 lr 0.00050491 rank 0
2023-02-20 22:58:43,253 DEBUG TRAIN Batch 11/12600 loss 6.296895 loss_att 12.668726 loss_ctc 8.703784 loss_rnnt 4.699446 hw_loss 0.004057 lr 0.00050491 rank 1
2023-02-20 22:58:43,257 DEBUG TRAIN Batch 11/12600 loss 38.324276 loss_att 37.300026 loss_ctc 62.338089 loss_rnnt 35.187027 hw_loss 0.262973 lr 0.00050491 rank 2
2023-02-20 22:58:43,260 DEBUG TRAIN Batch 11/12600 loss 10.572065 loss_att 15.415962 loss_ctc 11.466053 loss_rnnt 9.434000 hw_loss 0.093915 lr 0.00050491 rank 7
2023-02-20 22:58:43,263 DEBUG TRAIN Batch 11/12600 loss 17.467205 loss_att 17.929914 loss_ctc 18.754730 loss_rnnt 17.200829 hw_loss 0.004057 lr 0.00050491 rank 5
2023-02-20 22:58:43,263 DEBUG TRAIN Batch 11/12600 loss 32.970104 loss_att 37.686089 loss_ctc 46.421486 loss_rnnt 30.058298 hw_loss 0.328289 lr 0.00050491 rank 3
2023-02-20 22:58:43,265 DEBUG TRAIN Batch 11/12600 loss 6.982601 loss_att 6.423383 loss_ctc 7.513035 loss_rnnt 6.689585 hw_loss 0.626503 lr 0.00050491 rank 4
2023-02-20 22:58:43,265 DEBUG TRAIN Batch 11/12600 loss 32.146263 loss_att 33.514660 loss_ctc 31.824816 loss_rnnt 31.609440 hw_loss 0.573752 lr 0.00050491 rank 6
2023-02-20 22:59:40,605 DEBUG TRAIN Batch 11/12700 loss 18.154211 loss_att 27.656088 loss_ctc 23.467178 loss_rnnt 15.397238 hw_loss 0.277882 lr 0.00050478 rank 0
2023-02-20 22:59:40,612 DEBUG TRAIN Batch 11/12700 loss 9.948082 loss_att 14.939867 loss_ctc 9.665442 loss_rnnt 8.820729 hw_loss 0.312525 lr 0.00050478 rank 2
2023-02-20 22:59:40,615 DEBUG TRAIN Batch 11/12700 loss 13.571935 loss_att 17.412615 loss_ctc 15.438415 loss_rnnt 12.315437 hw_loss 0.449058 lr 0.00050478 rank 5
2023-02-20 22:59:40,617 DEBUG TRAIN Batch 11/12700 loss 30.361509 loss_att 41.621998 loss_ctc 31.319267 loss_rnnt 27.892933 hw_loss 0.166455 lr 0.00050478 rank 4
2023-02-20 22:59:40,618 DEBUG TRAIN Batch 11/12700 loss 13.061525 loss_att 18.332104 loss_ctc 19.605614 loss_rnnt 11.033798 hw_loss 0.189498 lr 0.00050478 rank 3
2023-02-20 22:59:40,619 DEBUG TRAIN Batch 11/12700 loss 17.070580 loss_att 16.013874 loss_ctc 22.155865 loss_rnnt 16.396233 hw_loss 0.389344 lr 0.00050478 rank 7
2023-02-20 22:59:40,625 DEBUG TRAIN Batch 11/12700 loss 11.925454 loss_att 15.163940 loss_ctc 13.821439 loss_rnnt 10.793709 hw_loss 0.433594 lr 0.00050478 rank 6
2023-02-20 22:59:40,630 DEBUG TRAIN Batch 11/12700 loss 5.403678 loss_att 8.901484 loss_ctc 8.744658 loss_rnnt 4.058598 hw_loss 0.375104 lr 0.00050478 rank 1
2023-02-20 23:00:41,252 DEBUG TRAIN Batch 11/12800 loss 20.543814 loss_att 26.820929 loss_ctc 32.328232 loss_rnnt 17.586849 hw_loss 0.244285 lr 0.00050465 rank 0
2023-02-20 23:00:41,265 DEBUG TRAIN Batch 11/12800 loss 16.749146 loss_att 23.204636 loss_ctc 19.945671 loss_rnnt 14.847930 hw_loss 0.344837 lr 0.00050465 rank 2
2023-02-20 23:00:41,270 DEBUG TRAIN Batch 11/12800 loss 28.501612 loss_att 27.868275 loss_ctc 30.265032 loss_rnnt 28.249239 hw_loss 0.269846 lr 0.00050465 rank 1
2023-02-20 23:00:41,271 DEBUG TRAIN Batch 11/12800 loss 15.402333 loss_att 24.015734 loss_ctc 19.049133 loss_rnnt 13.039820 hw_loss 0.287987 lr 0.00050465 rank 5
2023-02-20 23:00:41,273 DEBUG TRAIN Batch 11/12800 loss 19.803316 loss_att 25.125317 loss_ctc 27.195253 loss_rnnt 17.549866 hw_loss 0.381483 lr 0.00050465 rank 3
2023-02-20 23:00:41,275 DEBUG TRAIN Batch 11/12800 loss 26.959837 loss_att 38.857670 loss_ctc 38.929092 loss_rnnt 22.983974 hw_loss 0.000741 lr 0.00050465 rank 7
2023-02-20 23:00:41,276 DEBUG TRAIN Batch 11/12800 loss 15.123379 loss_att 18.305706 loss_ctc 17.709864 loss_rnnt 14.106824 hw_loss 0.066044 lr 0.00050465 rank 6
2023-02-20 23:00:41,278 DEBUG TRAIN Batch 11/12800 loss 45.102776 loss_att 49.100582 loss_ctc 68.056488 loss_rnnt 41.096371 hw_loss 0.274414 lr 0.00050465 rank 4
2023-02-20 23:01:57,532 DEBUG TRAIN Batch 11/12900 loss 75.757446 loss_att 76.133644 loss_ctc 77.343391 loss_rnnt 75.470619 hw_loss 0.000252 lr 0.00050452 rank 0
2023-02-20 23:01:57,542 DEBUG TRAIN Batch 11/12900 loss 43.288860 loss_att 39.647453 loss_ctc 50.055035 loss_rnnt 43.114853 hw_loss 0.000252 lr 0.00050452 rank 3
2023-02-20 23:01:57,542 DEBUG TRAIN Batch 11/12900 loss 20.471249 loss_att 30.178516 loss_ctc 22.601709 loss_rnnt 18.146824 hw_loss 0.185456 lr 0.00050452 rank 5
2023-02-20 23:01:57,543 DEBUG TRAIN Batch 11/12900 loss 34.452072 loss_att 41.050888 loss_ctc 41.826694 loss_rnnt 32.020443 hw_loss 0.241087 lr 0.00050452 rank 6
2023-02-20 23:01:57,548 DEBUG TRAIN Batch 11/12900 loss 23.013546 loss_att 26.935463 loss_ctc 27.138973 loss_rnnt 21.406410 hw_loss 0.511305 lr 0.00050452 rank 7
2023-02-20 23:01:57,548 DEBUG TRAIN Batch 11/12900 loss 10.564130 loss_att 22.315147 loss_ctc 9.116735 loss_rnnt 8.181171 hw_loss 0.423264 lr 0.00050452 rank 2
2023-02-20 23:01:57,548 DEBUG TRAIN Batch 11/12900 loss 23.449499 loss_att 26.947691 loss_ctc 23.479353 loss_rnnt 22.506569 hw_loss 0.448711 lr 0.00050452 rank 4
2023-02-20 23:01:57,556 DEBUG TRAIN Batch 11/12900 loss 10.627621 loss_att 13.953058 loss_ctc 8.062344 loss_rnnt 10.247645 hw_loss 0.106735 lr 0.00050452 rank 1
2023-02-20 23:03:17,129 DEBUG TRAIN Batch 11/13000 loss 7.846751 loss_att 15.928092 loss_ctc 10.188867 loss_rnnt 5.774994 hw_loss 0.268514 lr 0.00050439 rank 0
2023-02-20 23:03:17,131 DEBUG TRAIN Batch 11/13000 loss 8.583144 loss_att 17.977581 loss_ctc 12.002892 loss_rnnt 6.092422 hw_loss 0.292256 lr 0.00050439 rank 1
2023-02-20 23:03:17,135 DEBUG TRAIN Batch 11/13000 loss 18.841042 loss_att 17.777695 loss_ctc 19.257084 loss_rnnt 18.767471 hw_loss 0.432685 lr 0.00050439 rank 6
2023-02-20 23:03:17,135 DEBUG TRAIN Batch 11/13000 loss 14.976864 loss_att 18.495037 loss_ctc 20.176899 loss_rnnt 13.359464 hw_loss 0.413300 lr 0.00050439 rank 2
2023-02-20 23:03:17,136 DEBUG TRAIN Batch 11/13000 loss 27.577168 loss_att 31.612808 loss_ctc 36.006264 loss_rnnt 25.530312 hw_loss 0.217218 lr 0.00050439 rank 4
2023-02-20 23:03:17,138 DEBUG TRAIN Batch 11/13000 loss 15.300654 loss_att 17.972385 loss_ctc 16.686029 loss_rnnt 14.581165 hw_loss 0.000799 lr 0.00050439 rank 3
2023-02-20 23:03:17,146 DEBUG TRAIN Batch 11/13000 loss 12.856425 loss_att 14.643979 loss_ctc 12.772003 loss_rnnt 12.254277 hw_loss 0.479797 lr 0.00050439 rank 7
2023-02-20 23:03:17,148 DEBUG TRAIN Batch 11/13000 loss 31.594681 loss_att 29.911926 loss_ctc 44.716164 loss_rnnt 30.181273 hw_loss 0.000799 lr 0.00050439 rank 5
2023-02-20 23:04:18,414 DEBUG TRAIN Batch 11/13100 loss 17.724421 loss_att 25.805773 loss_ctc 18.242033 loss_rnnt 15.848207 hw_loss 0.357989 lr 0.00050427 rank 0
2023-02-20 23:04:18,419 DEBUG TRAIN Batch 11/13100 loss 19.802816 loss_att 20.881533 loss_ctc 19.998888 loss_rnnt 19.353952 hw_loss 0.388078 lr 0.00050427 rank 5
2023-02-20 23:04:18,421 DEBUG TRAIN Batch 11/13100 loss 10.253190 loss_att 16.321478 loss_ctc 12.343050 loss_rnnt 8.760059 hw_loss 0.001547 lr 0.00050427 rank 6
2023-02-20 23:04:18,422 DEBUG TRAIN Batch 11/13100 loss 38.313808 loss_att 39.476074 loss_ctc 34.583687 loss_rnnt 38.383057 hw_loss 0.366828 lr 0.00050427 rank 1
2023-02-20 23:04:18,424 DEBUG TRAIN Batch 11/13100 loss 27.205524 loss_att 33.414608 loss_ctc 27.094837 loss_rnnt 25.838463 hw_loss 0.262505 lr 0.00050427 rank 2
2023-02-20 23:04:18,424 DEBUG TRAIN Batch 11/13100 loss 2.430372 loss_att 5.821465 loss_ctc 2.658969 loss_rnnt 1.720848 hw_loss 0.001547 lr 0.00050427 rank 3
2023-02-20 23:04:18,424 DEBUG TRAIN Batch 11/13100 loss 59.918060 loss_att 59.832741 loss_ctc 65.565369 loss_rnnt 59.047012 hw_loss 0.253382 lr 0.00050427 rank 4
2023-02-20 23:04:18,436 DEBUG TRAIN Batch 11/13100 loss 12.898800 loss_att 11.677964 loss_ctc 11.964273 loss_rnnt 13.041278 hw_loss 0.424298 lr 0.00050427 rank 7
2023-02-20 23:05:17,719 DEBUG TRAIN Batch 11/13200 loss 17.976969 loss_att 28.746080 loss_ctc 19.137026 loss_rnnt 15.668304 hw_loss 0.000314 lr 0.00050414 rank 0
2023-02-20 23:05:17,721 DEBUG TRAIN Batch 11/13200 loss 8.578137 loss_att 15.313496 loss_ctc 12.463068 loss_rnnt 6.587958 hw_loss 0.234595 lr 0.00050414 rank 6
2023-02-20 23:05:17,724 DEBUG TRAIN Batch 11/13200 loss 19.751579 loss_att 23.219669 loss_ctc 18.161682 loss_rnnt 19.174429 hw_loss 0.179101 lr 0.00050414 rank 5
2023-02-20 23:05:17,725 DEBUG TRAIN Batch 11/13200 loss 10.684680 loss_att 20.220007 loss_ctc 19.329062 loss_rnnt 7.624862 hw_loss 0.000314 lr 0.00050414 rank 2
2023-02-20 23:05:17,725 DEBUG TRAIN Batch 11/13200 loss 21.598339 loss_att 23.224430 loss_ctc 23.498703 loss_rnnt 20.801807 hw_loss 0.408623 lr 0.00050414 rank 1
2023-02-20 23:05:17,729 DEBUG TRAIN Batch 11/13200 loss 45.581856 loss_att 58.626862 loss_ctc 72.688019 loss_rnnt 39.068047 hw_loss 0.544978 lr 0.00050414 rank 3
2023-02-20 23:05:17,729 DEBUG TRAIN Batch 11/13200 loss 28.509668 loss_att 27.508146 loss_ctc 41.708694 loss_rnnt 26.766397 hw_loss 0.344441 lr 0.00050414 rank 4
2023-02-20 23:05:17,734 DEBUG TRAIN Batch 11/13200 loss 20.009909 loss_att 37.396378 loss_ctc 22.248253 loss_rnnt 16.234001 hw_loss 0.000314 lr 0.00050414 rank 7
2023-02-20 23:06:16,856 DEBUG TRAIN Batch 11/13300 loss 15.617724 loss_att 22.210081 loss_ctc 22.769976 loss_rnnt 13.173074 hw_loss 0.323522 lr 0.00050401 rank 0
2023-02-20 23:06:16,861 DEBUG TRAIN Batch 11/13300 loss 33.219765 loss_att 36.625546 loss_ctc 34.529747 loss_rnnt 32.317356 hw_loss 0.087349 lr 0.00050401 rank 3
2023-02-20 23:06:16,864 DEBUG TRAIN Batch 11/13300 loss 26.117643 loss_att 30.677561 loss_ctc 30.444069 loss_rnnt 24.628551 hw_loss 0.000474 lr 0.00050401 rank 2
2023-02-20 23:06:16,864 DEBUG TRAIN Batch 11/13300 loss 30.696217 loss_att 28.065237 loss_ctc 37.050171 loss_rnnt 30.140162 hw_loss 0.440732 lr 0.00050401 rank 5
2023-02-20 23:06:16,866 DEBUG TRAIN Batch 11/13300 loss 10.998727 loss_att 13.679571 loss_ctc 11.584762 loss_rnnt 10.384167 hw_loss 0.000474 lr 0.00050401 rank 4
2023-02-20 23:06:16,866 DEBUG TRAIN Batch 11/13300 loss 19.330362 loss_att 22.512016 loss_ctc 18.811630 loss_rnnt 18.542316 hw_loss 0.414145 lr 0.00050401 rank 1
2023-02-20 23:06:16,903 DEBUG TRAIN Batch 11/13300 loss 14.217728 loss_att 19.508160 loss_ctc 17.207235 loss_rnnt 12.596243 hw_loss 0.308996 lr 0.00050401 rank 7
2023-02-20 23:06:16,945 DEBUG TRAIN Batch 11/13300 loss 39.511364 loss_att 63.698788 loss_ctc 48.178276 loss_rnnt 33.408554 hw_loss 0.205755 lr 0.00050401 rank 6
2023-02-20 23:07:16,947 DEBUG TRAIN Batch 11/13400 loss 4.904420 loss_att 9.535407 loss_ctc 5.006643 loss_rnnt 3.963536 hw_loss 0.001982 lr 0.00050388 rank 0
2023-02-20 23:07:16,949 DEBUG TRAIN Batch 11/13400 loss 16.595579 loss_att 23.565567 loss_ctc 20.525230 loss_rnnt 14.513414 hw_loss 0.307898 lr 0.00050388 rank 2
2023-02-20 23:07:16,953 DEBUG TRAIN Batch 11/13400 loss 36.413940 loss_att 44.302803 loss_ctc 44.014484 loss_rnnt 33.652550 hw_loss 0.319144 lr 0.00050388 rank 5
2023-02-20 23:07:16,956 DEBUG TRAIN Batch 11/13400 loss 17.449007 loss_att 22.687967 loss_ctc 18.046356 loss_rnnt 16.205439 hw_loss 0.217745 lr 0.00050388 rank 3
2023-02-20 23:07:16,957 DEBUG TRAIN Batch 11/13400 loss 22.482784 loss_att 34.096008 loss_ctc 28.290695 loss_rnnt 19.384693 hw_loss 0.001982 lr 0.00050388 rank 4
2023-02-20 23:07:16,958 DEBUG TRAIN Batch 11/13400 loss 28.488171 loss_att 25.614599 loss_ctc 27.126461 loss_rnnt 29.112110 hw_loss 0.248127 lr 0.00050388 rank 6
2023-02-20 23:07:16,961 DEBUG TRAIN Batch 11/13400 loss 12.424675 loss_att 11.221674 loss_ctc 16.904808 loss_rnnt 12.066867 hw_loss 0.001982 lr 0.00050388 rank 1
2023-02-20 23:07:16,968 DEBUG TRAIN Batch 11/13400 loss 24.325964 loss_att 33.179897 loss_ctc 26.079079 loss_rnnt 22.216171 hw_loss 0.197357 lr 0.00050388 rank 7
2023-02-20 23:08:15,529 DEBUG TRAIN Batch 11/13500 loss 22.089380 loss_att 32.331310 loss_ctc 22.398045 loss_rnnt 19.996264 hw_loss 0.006701 lr 0.00050375 rank 2
2023-02-20 23:08:15,530 DEBUG TRAIN Batch 11/13500 loss 19.867949 loss_att 22.465433 loss_ctc 20.398949 loss_rnnt 19.217896 hw_loss 0.112044 lr 0.00050375 rank 0
2023-02-20 23:08:15,533 DEBUG TRAIN Batch 11/13500 loss 14.332373 loss_att 19.244375 loss_ctc 13.487064 loss_rnnt 13.232078 hw_loss 0.432383 lr 0.00050375 rank 3
2023-02-20 23:08:15,537 DEBUG TRAIN Batch 11/13500 loss 57.732426 loss_att 72.661629 loss_ctc 80.130760 loss_rnnt 51.563011 hw_loss 0.369612 lr 0.00050375 rank 7
2023-02-20 23:08:15,543 DEBUG TRAIN Batch 11/13500 loss 1.176980 loss_att 4.520760 loss_ctc 0.424713 loss_rnnt 0.414833 hw_loss 0.363177 lr 0.00050375 rank 5
2023-02-20 23:08:15,548 DEBUG TRAIN Batch 11/13500 loss 24.076771 loss_att 30.706472 loss_ctc 24.962112 loss_rnnt 22.504322 hw_loss 0.240862 lr 0.00050375 rank 4
2023-02-20 23:08:15,550 DEBUG TRAIN Batch 11/13500 loss 11.014847 loss_att 11.654906 loss_ctc 13.938128 loss_rnnt 10.354123 hw_loss 0.268014 lr 0.00050375 rank 1
2023-02-20 23:08:15,588 DEBUG TRAIN Batch 11/13500 loss 9.048350 loss_att 15.698013 loss_ctc 11.488651 loss_rnnt 7.158757 hw_loss 0.439289 lr 0.00050375 rank 6
2023-02-20 23:09:15,200 DEBUG TRAIN Batch 11/13600 loss 27.510359 loss_att 34.174778 loss_ctc 31.301716 loss_rnnt 25.598467 hw_loss 0.137799 lr 0.00050363 rank 0
2023-02-20 23:09:15,209 DEBUG TRAIN Batch 11/13600 loss 21.483297 loss_att 24.329193 loss_ctc 21.057077 loss_rnnt 20.823425 hw_loss 0.276603 lr 0.00050363 rank 2
2023-02-20 23:09:15,213 DEBUG TRAIN Batch 11/13600 loss 23.063538 loss_att 29.673817 loss_ctc 23.664818 loss_rnnt 21.533813 hw_loss 0.239056 lr 0.00050363 rank 3
2023-02-20 23:09:15,216 DEBUG TRAIN Batch 11/13600 loss 38.600384 loss_att 48.181892 loss_ctc 43.706787 loss_rnnt 35.940559 hw_loss 0.117500 lr 0.00050363 rank 1
2023-02-20 23:09:15,217 DEBUG TRAIN Batch 11/13600 loss 33.459694 loss_att 36.753723 loss_ctc 41.427925 loss_rnnt 31.666349 hw_loss 0.135198 lr 0.00050363 rank 5
2023-02-20 23:09:15,221 DEBUG TRAIN Batch 11/13600 loss 26.434340 loss_att 29.397121 loss_ctc 34.476013 loss_rnnt 24.576099 hw_loss 0.362735 lr 0.00050363 rank 7
2023-02-20 23:09:15,234 DEBUG TRAIN Batch 11/13600 loss 21.378706 loss_att 23.734396 loss_ctc 24.409084 loss_rnnt 20.273561 hw_loss 0.431164 lr 0.00050363 rank 6
2023-02-20 23:09:15,250 DEBUG TRAIN Batch 11/13600 loss 49.734692 loss_att 85.026573 loss_ctc 62.023552 loss_rnnt 40.809479 hw_loss 0.428107 lr 0.00050363 rank 4
2023-02-20 23:10:15,403 DEBUG TRAIN Batch 11/13700 loss 4.390695 loss_att 5.927942 loss_ctc 4.021884 loss_rnnt 4.131773 hw_loss 0.001215 lr 0.00050350 rank 0
2023-02-20 23:10:15,410 DEBUG TRAIN Batch 11/13700 loss 2.272641 loss_att 6.228155 loss_ctc 7.660632 loss_rnnt 0.657972 hw_loss 0.197188 lr 0.00050350 rank 2
2023-02-20 23:10:15,411 DEBUG TRAIN Batch 11/13700 loss 8.672039 loss_att 12.195892 loss_ctc 9.015164 loss_rnnt 7.768806 hw_loss 0.286335 lr 0.00050350 rank 3
2023-02-20 23:10:15,412 DEBUG TRAIN Batch 11/13700 loss 89.867821 loss_att 89.429832 loss_ctc 88.083221 loss_rnnt 90.033386 hw_loss 0.299968 lr 0.00050350 rank 5
2023-02-20 23:10:15,416 DEBUG TRAIN Batch 11/13700 loss 25.948997 loss_att 27.088457 loss_ctc 33.702480 loss_rnnt 24.625546 hw_loss 0.115806 lr 0.00050350 rank 6
2023-02-20 23:10:15,420 DEBUG TRAIN Batch 11/13700 loss 24.683678 loss_att 26.603750 loss_ctc 33.848537 loss_rnnt 22.824715 hw_loss 0.474316 lr 0.00050350 rank 7
2023-02-20 23:10:15,420 DEBUG TRAIN Batch 11/13700 loss 7.229043 loss_att 19.608097 loss_ctc 9.900558 loss_rnnt 4.251526 hw_loss 0.272821 lr 0.00050350 rank 4
2023-02-20 23:10:15,476 DEBUG TRAIN Batch 11/13700 loss 25.793821 loss_att 40.243336 loss_ctc 32.331261 loss_rnnt 21.828676 hw_loss 0.381718 lr 0.00050350 rank 1
2023-02-20 23:11:34,882 DEBUG TRAIN Batch 11/13800 loss 9.881021 loss_att 8.482505 loss_ctc 10.593360 loss_rnnt 9.823436 hw_loss 0.454334 lr 0.00050337 rank 3
2023-02-20 23:11:34,882 DEBUG TRAIN Batch 11/13800 loss 32.153320 loss_att 34.255188 loss_ctc 31.401335 loss_rnnt 31.833130 hw_loss 0.000149 lr 0.00050337 rank 0
2023-02-20 23:11:34,883 DEBUG TRAIN Batch 11/13800 loss 14.299911 loss_att 21.383694 loss_ctc 13.746049 loss_rnnt 12.757895 hw_loss 0.373327 lr 0.00050337 rank 2
2023-02-20 23:11:34,887 DEBUG TRAIN Batch 11/13800 loss 1.690526 loss_att 7.064232 loss_ctc 1.670340 loss_rnnt 0.618396 hw_loss 0.000149 lr 0.00050337 rank 7
2023-02-20 23:11:34,889 DEBUG TRAIN Batch 11/13800 loss 27.748552 loss_att 36.166752 loss_ctc 30.772699 loss_rnnt 25.459490 hw_loss 0.379126 lr 0.00050337 rank 1
2023-02-20 23:11:34,890 DEBUG TRAIN Batch 11/13800 loss 17.345680 loss_att 19.469025 loss_ctc 18.358587 loss_rnnt 16.640654 hw_loss 0.272447 lr 0.00050337 rank 4
2023-02-20 23:11:34,898 DEBUG TRAIN Batch 11/13800 loss 10.729991 loss_att 20.251719 loss_ctc 13.868984 loss_rnnt 8.244358 hw_loss 0.305164 lr 0.00050337 rank 6
2023-02-20 23:11:34,920 DEBUG TRAIN Batch 11/13800 loss 4.778731 loss_att 10.171930 loss_ctc 5.617007 loss_rnnt 3.315971 hw_loss 0.510655 lr 0.00050337 rank 5
2023-02-20 23:12:52,974 DEBUG TRAIN Batch 11/13900 loss 25.268814 loss_att 27.547808 loss_ctc 26.885878 loss_rnnt 24.445959 hw_loss 0.283967 lr 0.00050324 rank 0
2023-02-20 23:12:52,984 DEBUG TRAIN Batch 11/13900 loss 19.222054 loss_att 25.027077 loss_ctc 23.825169 loss_rnnt 17.357773 hw_loss 0.167865 lr 0.00050324 rank 3
2023-02-20 23:12:52,986 DEBUG TRAIN Batch 11/13900 loss 9.788840 loss_att 9.494833 loss_ctc 8.766170 loss_rnnt 9.891998 hw_loss 0.172498 lr 0.00050324 rank 6
2023-02-20 23:12:52,987 DEBUG TRAIN Batch 11/13900 loss 30.733555 loss_att 38.832146 loss_ctc 30.741390 loss_rnnt 28.886837 hw_loss 0.423669 lr 0.00050324 rank 2
2023-02-20 23:12:52,993 DEBUG TRAIN Batch 11/13900 loss 56.026817 loss_att 67.825691 loss_ctc 75.693810 loss_rnnt 50.998161 hw_loss 0.087409 lr 0.00050324 rank 4
2023-02-20 23:12:52,998 DEBUG TRAIN Batch 11/13900 loss 23.734274 loss_att 24.035973 loss_ctc 28.961514 loss_rnnt 22.846272 hw_loss 0.245056 lr 0.00050324 rank 5
2023-02-20 23:12:53,007 DEBUG TRAIN Batch 11/13900 loss 18.261003 loss_att 20.723169 loss_ctc 20.205772 loss_rnnt 17.268349 hw_loss 0.451721 lr 0.00050324 rank 7
2023-02-20 23:12:53,060 DEBUG TRAIN Batch 11/13900 loss 14.826408 loss_att 21.694456 loss_ctc 16.976864 loss_rnnt 12.909307 hw_loss 0.481435 lr 0.00050324 rank 1
2023-02-20 23:13:52,568 DEBUG TRAIN Batch 11/14000 loss 23.098133 loss_att 27.063305 loss_ctc 35.872299 loss_rnnt 20.447954 hw_loss 0.288602 lr 0.00050312 rank 0
2023-02-20 23:13:52,570 DEBUG TRAIN Batch 11/14000 loss 14.348026 loss_att 31.048756 loss_ctc 19.795128 loss_rnnt 10.213263 hw_loss 0.128134 lr 0.00050312 rank 1
2023-02-20 23:13:52,572 DEBUG TRAIN Batch 11/14000 loss 42.847603 loss_att 47.087906 loss_ctc 92.545654 loss_rnnt 35.217308 hw_loss 0.292178 lr 0.00050312 rank 2
2023-02-20 23:13:52,574 DEBUG TRAIN Batch 11/14000 loss 11.451524 loss_att 14.579984 loss_ctc 14.640989 loss_rnnt 10.194813 hw_loss 0.385795 lr 0.00050312 rank 6
2023-02-20 23:13:52,577 DEBUG TRAIN Batch 11/14000 loss 6.702765 loss_att 11.881357 loss_ctc 2.352656 loss_rnnt 5.951973 hw_loss 0.553287 lr 0.00050312 rank 3
2023-02-20 23:13:52,579 DEBUG TRAIN Batch 11/14000 loss 26.788101 loss_att 41.914764 loss_ctc 32.527344 loss_rnnt 22.997368 hw_loss 0.000315 lr 0.00050312 rank 5
2023-02-20 23:13:52,580 DEBUG TRAIN Batch 11/14000 loss 17.989077 loss_att 20.573862 loss_ctc 31.394764 loss_rnnt 15.544355 hw_loss 0.263137 lr 0.00050312 rank 4
2023-02-20 23:13:52,584 DEBUG TRAIN Batch 11/14000 loss 19.634817 loss_att 30.972425 loss_ctc 20.451052 loss_rnnt 17.206127 hw_loss 0.098132 lr 0.00050312 rank 7
2023-02-20 23:14:50,943 DEBUG TRAIN Batch 11/14100 loss 11.484289 loss_att 11.765943 loss_ctc 13.044443 loss_rnnt 10.930475 hw_loss 0.542741 lr 0.00050299 rank 0
2023-02-20 23:14:50,949 DEBUG TRAIN Batch 11/14100 loss 2.121340 loss_att 4.024835 loss_ctc 2.071083 loss_rnnt 1.614629 hw_loss 0.248837 lr 0.00050299 rank 2
2023-02-20 23:14:50,950 DEBUG TRAIN Batch 11/14100 loss 50.551868 loss_att 69.377434 loss_ctc 44.355408 loss_rnnt 47.430145 hw_loss 0.342760 lr 0.00050299 rank 6
2023-02-20 23:14:50,950 DEBUG TRAIN Batch 11/14100 loss 8.805251 loss_att 16.053680 loss_ctc 16.243322 loss_rnnt 6.119602 hw_loss 0.457915 lr 0.00050299 rank 7
2023-02-20 23:14:50,951 DEBUG TRAIN Batch 11/14100 loss 12.492416 loss_att 22.543797 loss_ctc 13.450006 loss_rnnt 10.354082 hw_loss 0.000710 lr 0.00050299 rank 5
2023-02-20 23:14:50,953 DEBUG TRAIN Batch 11/14100 loss 17.008121 loss_att 19.274502 loss_ctc 18.771273 loss_rnnt 16.168631 hw_loss 0.283364 lr 0.00050299 rank 3
2023-02-20 23:14:50,954 DEBUG TRAIN Batch 11/14100 loss 5.577355 loss_att 11.916091 loss_ctc 5.937499 loss_rnnt 4.021922 hw_loss 0.449374 lr 0.00050299 rank 4
2023-02-20 23:14:51,012 DEBUG TRAIN Batch 11/14100 loss 11.758033 loss_att 15.254652 loss_ctc 12.610164 loss_rnnt 10.882481 hw_loss 0.117395 lr 0.00050299 rank 1
2023-02-20 23:15:50,277 DEBUG TRAIN Batch 11/14200 loss 10.498725 loss_att 16.772871 loss_ctc 12.555639 loss_rnnt 8.824705 hw_loss 0.271752 lr 0.00050286 rank 0
2023-02-20 23:15:50,279 DEBUG TRAIN Batch 11/14200 loss 31.596096 loss_att 31.287897 loss_ctc 31.330936 loss_rnnt 31.518778 hw_loss 0.326835 lr 0.00050286 rank 6
2023-02-20 23:15:50,282 DEBUG TRAIN Batch 11/14200 loss 9.636778 loss_att 20.380091 loss_ctc 12.859494 loss_rnnt 6.871992 hw_loss 0.349553 lr 0.00050286 rank 3
2023-02-20 23:15:50,283 DEBUG TRAIN Batch 11/14200 loss 19.719297 loss_att 27.005543 loss_ctc 21.978622 loss_rnnt 17.892115 hw_loss 0.128789 lr 0.00050286 rank 5
2023-02-20 23:15:50,283 DEBUG TRAIN Batch 11/14200 loss 28.131716 loss_att 23.042120 loss_ctc 33.140388 loss_rnnt 28.302971 hw_loss 0.335328 lr 0.00050286 rank 2
2023-02-20 23:15:50,287 DEBUG TRAIN Batch 11/14200 loss 13.841991 loss_att 19.722324 loss_ctc 13.651705 loss_rnnt 12.570225 hw_loss 0.227007 lr 0.00050286 rank 1
2023-02-20 23:15:50,294 DEBUG TRAIN Batch 11/14200 loss 15.025751 loss_att 26.009609 loss_ctc 22.309185 loss_rnnt 11.677856 hw_loss 0.337495 lr 0.00050286 rank 7
2023-02-20 23:15:50,344 DEBUG TRAIN Batch 11/14200 loss 34.199844 loss_att 45.576359 loss_ctc 45.853020 loss_rnnt 30.370596 hw_loss 0.000353 lr 0.00050286 rank 4
2023-02-20 23:16:50,375 DEBUG TRAIN Batch 11/14300 loss 69.456848 loss_att 80.782082 loss_ctc 61.373844 loss_rnnt 68.015450 hw_loss 0.476419 lr 0.00050273 rank 0
2023-02-20 23:16:50,378 DEBUG TRAIN Batch 11/14300 loss 46.341633 loss_att 65.133751 loss_ctc 51.963696 loss_rnnt 41.682194 hw_loss 0.283881 lr 0.00050273 rank 2
2023-02-20 23:16:50,387 DEBUG TRAIN Batch 11/14300 loss 6.666057 loss_att 12.926468 loss_ctc 5.339283 loss_rnnt 5.376600 hw_loss 0.401770 lr 0.00050273 rank 1
2023-02-20 23:16:50,391 DEBUG TRAIN Batch 11/14300 loss 29.557127 loss_att 42.121990 loss_ctc 37.541710 loss_rnnt 25.979416 hw_loss 0.000237 lr 0.00050273 rank 5
2023-02-20 23:16:50,393 DEBUG TRAIN Batch 11/14300 loss 15.358644 loss_att 21.750446 loss_ctc 17.926615 loss_rnnt 13.648196 hw_loss 0.168172 lr 0.00050273 rank 6
2023-02-20 23:16:50,394 DEBUG TRAIN Batch 11/14300 loss 41.898487 loss_att 37.553093 loss_ctc 45.340794 loss_rnnt 42.104294 hw_loss 0.383051 lr 0.00050273 rank 3
2023-02-20 23:16:50,396 DEBUG TRAIN Batch 11/14300 loss 23.941599 loss_att 24.850214 loss_ctc 25.558159 loss_rnnt 23.354912 hw_loss 0.355167 lr 0.00050273 rank 7
2023-02-20 23:16:50,398 DEBUG TRAIN Batch 11/14300 loss 30.146566 loss_att 49.155167 loss_ctc 37.146626 loss_rnnt 25.147879 hw_loss 0.494299 lr 0.00050273 rank 4
2023-02-20 23:17:47,269 DEBUG TRAIN Batch 11/14400 loss 7.864207 loss_att 10.120650 loss_ctc 8.872251 loss_rnnt 7.089893 hw_loss 0.353661 lr 0.00050261 rank 0
2023-02-20 23:17:47,281 DEBUG TRAIN Batch 11/14400 loss 16.581013 loss_att 23.340439 loss_ctc 21.900555 loss_rnnt 14.470385 hw_loss 0.092756 lr 0.00050261 rank 6
2023-02-20 23:17:47,282 DEBUG TRAIN Batch 11/14400 loss 18.329325 loss_att 21.001047 loss_ctc 21.439281 loss_rnnt 17.080650 hw_loss 0.561876 lr 0.00050261 rank 5
2023-02-20 23:17:47,284 DEBUG TRAIN Batch 11/14400 loss 24.165634 loss_att 27.765293 loss_ctc 26.937748 loss_rnnt 22.888191 hw_loss 0.352304 lr 0.00050261 rank 3
2023-02-20 23:17:47,286 DEBUG TRAIN Batch 11/14400 loss 27.496052 loss_att 27.137312 loss_ctc 30.977005 loss_rnnt 26.983734 hw_loss 0.224888 lr 0.00050261 rank 1
2023-02-20 23:17:47,286 DEBUG TRAIN Batch 11/14400 loss 20.814447 loss_att 29.700268 loss_ctc 26.793459 loss_rnnt 18.162781 hw_loss 0.144941 lr 0.00050261 rank 4
2023-02-20 23:17:47,288 DEBUG TRAIN Batch 11/14400 loss 18.002209 loss_att 18.803841 loss_ctc 25.799322 loss_rnnt 16.675907 hw_loss 0.236925 lr 0.00050261 rank 2
2023-02-20 23:17:47,296 DEBUG TRAIN Batch 11/14400 loss 4.552093 loss_att 10.299337 loss_ctc 5.107323 loss_rnnt 3.157397 hw_loss 0.321029 lr 0.00050261 rank 7
2023-02-20 23:18:46,094 DEBUG TRAIN Batch 11/14500 loss 16.053738 loss_att 25.027977 loss_ctc 18.840317 loss_rnnt 13.835258 hw_loss 0.097667 lr 0.00050248 rank 0
2023-02-20 23:18:46,095 DEBUG TRAIN Batch 11/14500 loss 15.895109 loss_att 29.499149 loss_ctc 17.507799 loss_rnnt 12.785628 hw_loss 0.325589 lr 0.00050248 rank 2
2023-02-20 23:18:46,099 DEBUG TRAIN Batch 11/14500 loss 27.423040 loss_att 33.057678 loss_ctc 30.722084 loss_rnnt 25.652485 hw_loss 0.382036 lr 0.00050248 rank 1
2023-02-20 23:18:46,100 DEBUG TRAIN Batch 11/14500 loss 34.557182 loss_att 38.644669 loss_ctc 38.880878 loss_rnnt 32.961712 hw_loss 0.377783 lr 0.00050248 rank 6
2023-02-20 23:18:46,102 DEBUG TRAIN Batch 11/14500 loss 14.684341 loss_att 21.976547 loss_ctc 18.649195 loss_rnnt 12.529724 hw_loss 0.314116 lr 0.00050248 rank 7
2023-02-20 23:18:46,102 DEBUG TRAIN Batch 11/14500 loss 12.831120 loss_att 20.669142 loss_ctc 14.266447 loss_rnnt 10.900640 hw_loss 0.321561 lr 0.00050248 rank 3
2023-02-20 23:18:46,112 DEBUG TRAIN Batch 11/14500 loss 47.414532 loss_att 52.289070 loss_ctc 71.106857 loss_rnnt 43.280540 hw_loss 0.000196 lr 0.00050248 rank 4
2023-02-20 23:18:46,162 DEBUG TRAIN Batch 11/14500 loss 12.966288 loss_att 12.805008 loss_ctc 13.822826 loss_rnnt 12.522714 hw_loss 0.678045 lr 0.00050248 rank 5
2023-02-20 23:19:47,121 DEBUG TRAIN Batch 11/14600 loss 17.795259 loss_att 21.422134 loss_ctc 19.366976 loss_rnnt 16.714701 hw_loss 0.273038 lr 0.00050235 rank 0
2023-02-20 23:19:47,120 DEBUG TRAIN Batch 11/14600 loss 7.115056 loss_att 19.248796 loss_ctc 5.468664 loss_rnnt 4.907119 hw_loss 0.001327 lr 0.00050235 rank 2
2023-02-20 23:19:47,124 DEBUG TRAIN Batch 11/14600 loss 8.069076 loss_att 10.132959 loss_ctc 10.848496 loss_rnnt 7.193528 hw_loss 0.172838 lr 0.00050235 rank 5
2023-02-20 23:19:47,127 DEBUG TRAIN Batch 11/14600 loss 26.758924 loss_att 42.539917 loss_ctc 28.334732 loss_rnnt 23.391911 hw_loss 0.001327 lr 0.00050235 rank 1
2023-02-20 23:19:47,127 DEBUG TRAIN Batch 11/14600 loss 13.377780 loss_att 16.394567 loss_ctc 10.257566 loss_rnnt 13.014598 hw_loss 0.329726 lr 0.00050235 rank 4
2023-02-20 23:19:47,128 DEBUG TRAIN Batch 11/14600 loss 11.491079 loss_att 18.311945 loss_ctc 18.882607 loss_rnnt 9.140661 hw_loss 0.001327 lr 0.00050235 rank 7
2023-02-20 23:19:47,135 DEBUG TRAIN Batch 11/14600 loss 12.491313 loss_att 25.007027 loss_ctc 9.348648 loss_rnnt 10.360933 hw_loss 0.086736 lr 0.00050235 rank 3
2023-02-20 23:19:47,182 DEBUG TRAIN Batch 11/14600 loss 43.289986 loss_att 54.925316 loss_ctc 50.793083 loss_rnnt 39.790642 hw_loss 0.322249 lr 0.00050235 rank 6
2023-02-20 23:21:05,537 DEBUG TRAIN Batch 11/14700 loss 33.675545 loss_att 41.995144 loss_ctc 41.131752 loss_rnnt 30.799152 hw_loss 0.409338 lr 0.00050223 rank 0
2023-02-20 23:21:05,537 DEBUG TRAIN Batch 11/14700 loss 15.360931 loss_att 18.805925 loss_ctc 13.667153 loss_rnnt 14.825283 hw_loss 0.135914 lr 0.00050223 rank 6
2023-02-20 23:21:05,541 DEBUG TRAIN Batch 11/14700 loss 20.220791 loss_att 20.285841 loss_ctc 23.524321 loss_rnnt 19.661991 hw_loss 0.197476 lr 0.00050223 rank 2
2023-02-20 23:21:05,542 DEBUG TRAIN Batch 11/14700 loss 15.679455 loss_att 23.239235 loss_ctc 14.293470 loss_rnnt 14.321940 hw_loss 0.056917 lr 0.00050223 rank 4
2023-02-20 23:21:05,543 DEBUG TRAIN Batch 11/14700 loss 11.618088 loss_att 18.973185 loss_ctc 14.499062 loss_rnnt 9.660019 hw_loss 0.192974 lr 0.00050223 rank 3
2023-02-20 23:21:05,546 DEBUG TRAIN Batch 11/14700 loss 5.528380 loss_att 11.009050 loss_ctc 7.702350 loss_rnnt 3.979299 hw_loss 0.305783 lr 0.00050223 rank 7
2023-02-20 23:21:05,591 DEBUG TRAIN Batch 11/14700 loss 7.222363 loss_att 12.061931 loss_ctc 8.572805 loss_rnnt 5.962047 hw_loss 0.210644 lr 0.00050223 rank 1
2023-02-20 23:21:05,600 DEBUG TRAIN Batch 11/14700 loss 6.325417 loss_att 12.910077 loss_ctc 7.035328 loss_rnnt 4.837709 hw_loss 0.142727 lr 0.00050223 rank 5
2023-02-20 23:22:24,619 DEBUG TRAIN Batch 11/14800 loss 28.509388 loss_att 35.666592 loss_ctc 33.905930 loss_rnnt 26.278038 hw_loss 0.150692 lr 0.00050210 rank 5
2023-02-20 23:22:24,626 DEBUG TRAIN Batch 11/14800 loss 12.532919 loss_att 16.700804 loss_ctc 12.487913 loss_rnnt 11.499912 hw_loss 0.385180 lr 0.00050210 rank 7
2023-02-20 23:22:24,623 DEBUG TRAIN Batch 11/14800 loss 9.083951 loss_att 11.029597 loss_ctc 12.168263 loss_rnnt 8.234197 hw_loss 0.092592 lr 0.00050210 rank 0
2023-02-20 23:22:24,627 DEBUG TRAIN Batch 11/14800 loss 9.662672 loss_att 17.319721 loss_ctc 13.986074 loss_rnnt 7.381821 hw_loss 0.324351 lr 0.00050210 rank 3
2023-02-20 23:22:24,630 DEBUG TRAIN Batch 11/14800 loss 21.030327 loss_att 26.929478 loss_ctc 25.774677 loss_rnnt 19.070377 hw_loss 0.276634 lr 0.00050210 rank 2
2023-02-20 23:22:24,634 DEBUG TRAIN Batch 11/14800 loss 20.060011 loss_att 24.291901 loss_ctc 28.113712 loss_rnnt 17.958323 hw_loss 0.340279 lr 0.00050210 rank 1
2023-02-20 23:22:24,636 DEBUG TRAIN Batch 11/14800 loss 17.557571 loss_att 23.897373 loss_ctc 17.955404 loss_rnnt 15.893490 hw_loss 0.643266 lr 0.00050210 rank 4
2023-02-20 23:22:24,682 DEBUG TRAIN Batch 11/14800 loss 13.056209 loss_att 14.626562 loss_ctc 15.905638 loss_rnnt 12.136830 hw_loss 0.422596 lr 0.00050210 rank 6
2023-02-20 23:23:24,358 DEBUG TRAIN Batch 11/14900 loss 10.644243 loss_att 10.663634 loss_ctc 10.440115 loss_rnnt 10.562051 hw_loss 0.197870 lr 0.00050197 rank 2
2023-02-20 23:23:24,359 DEBUG TRAIN Batch 11/14900 loss 18.842705 loss_att 22.709963 loss_ctc 28.290651 loss_rnnt 16.623371 hw_loss 0.349040 lr 0.00050197 rank 0
2023-02-20 23:23:24,364 DEBUG TRAIN Batch 11/14900 loss 60.388752 loss_att 63.185078 loss_ctc 65.898438 loss_rnnt 58.846786 hw_loss 0.465137 lr 0.00050197 rank 6
2023-02-20 23:23:24,364 DEBUG TRAIN Batch 11/14900 loss 15.597960 loss_att 32.762802 loss_ctc 13.800794 loss_rnnt 12.301847 hw_loss 0.192688 lr 0.00050197 rank 5
2023-02-20 23:23:24,365 DEBUG TRAIN Batch 11/14900 loss 3.170995 loss_att 9.629061 loss_ctc 4.737852 loss_rnnt 1.670333 hw_loss 0.000251 lr 0.00050197 rank 3
2023-02-20 23:23:24,370 DEBUG TRAIN Batch 11/14900 loss 19.425236 loss_att 15.891886 loss_ctc 20.301720 loss_rnnt 19.747963 hw_loss 0.500768 lr 0.00050197 rank 4
2023-02-20 23:23:24,372 DEBUG TRAIN Batch 11/14900 loss 21.617928 loss_att 22.083023 loss_ctc 25.184631 loss_rnnt 21.049212 hw_loss 0.000251 lr 0.00050197 rank 1
2023-02-20 23:23:24,373 DEBUG TRAIN Batch 11/14900 loss 11.034576 loss_att 19.855946 loss_ctc 13.995946 loss_rnnt 8.724655 hw_loss 0.282746 lr 0.00050197 rank 7
2023-02-20 23:24:22,098 DEBUG TRAIN Batch 11/15000 loss 24.019396 loss_att 33.682755 loss_ctc 24.517529 loss_rnnt 21.890875 hw_loss 0.242682 lr 0.00050185 rank 0
2023-02-20 23:24:22,105 DEBUG TRAIN Batch 11/15000 loss 21.484800 loss_att 33.277981 loss_ctc 28.073503 loss_rnnt 18.120399 hw_loss 0.238633 lr 0.00050185 rank 6
2023-02-20 23:24:22,105 DEBUG TRAIN Batch 11/15000 loss 12.017256 loss_att 12.568669 loss_ctc 13.030177 loss_rnnt 11.517657 hw_loss 0.476738 lr 0.00050185 rank 5
2023-02-20 23:24:22,106 DEBUG TRAIN Batch 11/15000 loss 29.695076 loss_att 34.558189 loss_ctc 39.886066 loss_rnnt 27.314251 hw_loss 0.092628 lr 0.00050185 rank 4
2023-02-20 23:24:22,107 DEBUG TRAIN Batch 11/15000 loss 30.838140 loss_att 33.302612 loss_ctc 28.361244 loss_rnnt 30.494179 hw_loss 0.339975 lr 0.00050185 rank 2
2023-02-20 23:24:22,108 DEBUG TRAIN Batch 11/15000 loss 8.115710 loss_att 14.475559 loss_ctc 8.252055 loss_rnnt 6.623121 hw_loss 0.379574 lr 0.00050185 rank 1
2023-02-20 23:24:22,111 DEBUG TRAIN Batch 11/15000 loss 21.333168 loss_att 24.461491 loss_ctc 23.400789 loss_rnnt 20.249329 hw_loss 0.342176 lr 0.00050185 rank 3
2023-02-20 23:24:22,116 DEBUG TRAIN Batch 11/15000 loss 8.480960 loss_att 14.147655 loss_ctc 10.035517 loss_rnnt 7.139889 hw_loss 0.000859 lr 0.00050185 rank 7
2023-02-20 23:25:21,203 DEBUG TRAIN Batch 11/15100 loss 54.916042 loss_att 53.250767 loss_ctc 67.896263 loss_rnnt 53.455223 hw_loss 0.118461 lr 0.00050172 rank 6
2023-02-20 23:25:21,204 DEBUG TRAIN Batch 11/15100 loss 12.312391 loss_att 16.645372 loss_ctc 13.337569 loss_rnnt 11.200579 hw_loss 0.203483 lr 0.00050172 rank 5
2023-02-20 23:25:21,204 DEBUG TRAIN Batch 11/15100 loss 36.579731 loss_att 39.626556 loss_ctc 49.152271 loss_rnnt 34.038418 hw_loss 0.479264 lr 0.00050172 rank 0
2023-02-20 23:25:21,207 DEBUG TRAIN Batch 11/15100 loss 6.071803 loss_att 14.172934 loss_ctc 8.805750 loss_rnnt 4.019609 hw_loss 0.126452 lr 0.00050172 rank 2
2023-02-20 23:25:21,209 DEBUG TRAIN Batch 11/15100 loss 30.770554 loss_att 49.874733 loss_ctc 39.056767 loss_rnnt 25.844555 hw_loss 0.000627 lr 0.00050172 rank 1
2023-02-20 23:25:21,216 DEBUG TRAIN Batch 11/15100 loss 24.980898 loss_att 23.220232 loss_ctc 20.286682 loss_rnnt 25.819614 hw_loss 0.261213 lr 0.00050172 rank 7
2023-02-20 23:25:21,217 DEBUG TRAIN Batch 11/15100 loss 7.429629 loss_att 15.124322 loss_ctc 7.812260 loss_rnnt 5.609569 hw_loss 0.431448 lr 0.00050172 rank 4
2023-02-20 23:25:21,217 DEBUG TRAIN Batch 11/15100 loss 36.555157 loss_att 50.553322 loss_ctc 38.138119 loss_rnnt 33.443008 hw_loss 0.190218 lr 0.00050172 rank 3
2023-02-20 23:26:21,410 DEBUG TRAIN Batch 11/15200 loss 21.373453 loss_att 27.383190 loss_ctc 23.553211 loss_rnnt 19.570110 hw_loss 0.582675 lr 0.00050160 rank 0
2023-02-20 23:26:21,413 DEBUG TRAIN Batch 11/15200 loss 16.334627 loss_att 18.053844 loss_ctc 21.000040 loss_rnnt 15.276598 hw_loss 0.172744 lr 0.00050160 rank 6
2023-02-20 23:26:21,417 DEBUG TRAIN Batch 11/15200 loss 8.240886 loss_att 14.025065 loss_ctc 9.131227 loss_rnnt 6.704409 hw_loss 0.489241 lr 0.00050160 rank 2
2023-02-20 23:26:21,420 DEBUG TRAIN Batch 11/15200 loss 24.597313 loss_att 43.854980 loss_ctc 35.243793 loss_rnnt 19.324644 hw_loss 0.003010 lr 0.00050160 rank 1
2023-02-20 23:26:21,422 DEBUG TRAIN Batch 11/15200 loss 20.216095 loss_att 31.967449 loss_ctc 25.830299 loss_rnnt 17.115660 hw_loss 0.003010 lr 0.00050160 rank 5
2023-02-20 23:26:21,424 DEBUG TRAIN Batch 11/15200 loss 12.390197 loss_att 24.620136 loss_ctc 12.607590 loss_rnnt 9.662960 hw_loss 0.472992 lr 0.00050160 rank 3
2023-02-20 23:26:21,424 DEBUG TRAIN Batch 11/15200 loss 19.526035 loss_att 22.591438 loss_ctc 19.037773 loss_rnnt 18.758171 hw_loss 0.412281 lr 0.00050160 rank 4
2023-02-20 23:26:21,427 DEBUG TRAIN Batch 11/15200 loss 8.538069 loss_att 14.717896 loss_ctc 11.922813 loss_rnnt 6.849200 hw_loss 0.003010 lr 0.00050160 rank 7
2023-02-20 23:27:19,070 DEBUG TRAIN Batch 11/15300 loss 6.442687 loss_att 11.227688 loss_ctc 6.587932 loss_rnnt 5.270736 hw_loss 0.366723 lr 0.00050147 rank 0
2023-02-20 23:27:19,075 DEBUG TRAIN Batch 11/15300 loss 12.851298 loss_att 17.441381 loss_ctc 15.919773 loss_rnnt 11.360708 hw_loss 0.306458 lr 0.00050147 rank 2
2023-02-20 23:27:19,077 DEBUG TRAIN Batch 11/15300 loss 1.958011 loss_att 8.973217 loss_ctc 0.624028 loss_rnnt 0.489622 hw_loss 0.456024 lr 0.00050147 rank 6
2023-02-20 23:27:19,079 DEBUG TRAIN Batch 11/15300 loss 14.380351 loss_att 15.985497 loss_ctc 20.480219 loss_rnnt 13.077664 hw_loss 0.315641 lr 0.00050147 rank 3
2023-02-20 23:27:19,079 DEBUG TRAIN Batch 11/15300 loss 24.139080 loss_att 31.546192 loss_ctc 26.962225 loss_rnnt 22.190771 hw_loss 0.169628 lr 0.00050147 rank 7
2023-02-20 23:27:19,079 DEBUG TRAIN Batch 11/15300 loss 29.273195 loss_att 35.835590 loss_ctc 30.896982 loss_rnnt 27.553022 hw_loss 0.358481 lr 0.00050147 rank 4
2023-02-20 23:27:19,087 DEBUG TRAIN Batch 11/15300 loss 13.277458 loss_att 19.082962 loss_ctc 15.966696 loss_rnnt 11.570381 hw_loss 0.351395 lr 0.00050147 rank 5
2023-02-20 23:27:19,140 DEBUG TRAIN Batch 11/15300 loss 12.386201 loss_att 19.556402 loss_ctc 15.948151 loss_rnnt 10.476470 hw_loss 0.001433 lr 0.00050147 rank 1
2023-02-20 23:28:19,158 DEBUG TRAIN Batch 11/15400 loss 25.700401 loss_att 27.231136 loss_ctc 25.251791 loss_rnnt 25.316809 hw_loss 0.257362 lr 0.00050134 rank 0
2023-02-20 23:28:19,169 DEBUG TRAIN Batch 11/15400 loss 19.515877 loss_att 26.686136 loss_ctc 29.524014 loss_rnnt 16.744926 hw_loss 0.004651 lr 0.00050134 rank 1
2023-02-20 23:28:19,174 DEBUG TRAIN Batch 11/15400 loss 14.022742 loss_att 15.690241 loss_ctc 14.209731 loss_rnnt 13.457190 hw_loss 0.388354 lr 0.00050134 rank 2
2023-02-20 23:28:19,175 DEBUG TRAIN Batch 11/15400 loss 24.821701 loss_att 31.810677 loss_ctc 29.261658 loss_rnnt 22.593706 hw_loss 0.446635 lr 0.00050134 rank 5
2023-02-20 23:28:19,177 DEBUG TRAIN Batch 11/15400 loss 50.203220 loss_att 43.858776 loss_ctc 58.142078 loss_rnnt 50.274364 hw_loss 0.261056 lr 0.00050134 rank 3
2023-02-20 23:28:19,180 DEBUG TRAIN Batch 11/15400 loss 14.055106 loss_att 20.045082 loss_ctc 13.425905 loss_rnnt 12.797724 hw_loss 0.268650 lr 0.00050134 rank 7
2023-02-20 23:28:19,182 DEBUG TRAIN Batch 11/15400 loss 25.644920 loss_att 26.931364 loss_ctc 27.282526 loss_rnnt 24.901194 hw_loss 0.502666 lr 0.00050134 rank 4
2023-02-20 23:28:19,203 DEBUG TRAIN Batch 11/15400 loss 8.061959 loss_att 12.489724 loss_ctc 10.949627 loss_rnnt 6.601557 hw_loss 0.355925 lr 0.00050134 rank 6
2023-02-20 23:29:36,599 DEBUG TRAIN Batch 11/15500 loss 39.278652 loss_att 41.792877 loss_ctc 35.651455 loss_rnnt 39.120487 hw_loss 0.260527 lr 0.00050122 rank 0
2023-02-20 23:29:36,604 DEBUG TRAIN Batch 11/15500 loss 26.877928 loss_att 26.509514 loss_ctc 20.641768 loss_rnnt 27.731970 hw_loss 0.095866 lr 0.00050122 rank 3
2023-02-20 23:29:36,604 DEBUG TRAIN Batch 11/15500 loss 51.582706 loss_att 56.556091 loss_ctc 46.594311 loss_rnnt 51.033607 hw_loss 0.411638 lr 0.00050122 rank 2
2023-02-20 23:29:36,610 DEBUG TRAIN Batch 11/15500 loss 4.231615 loss_att 12.796568 loss_ctc 8.783012 loss_rnnt 1.911249 hw_loss 0.000979 lr 0.00050122 rank 1
2023-02-20 23:29:36,612 DEBUG TRAIN Batch 11/15500 loss 17.287094 loss_att 21.826006 loss_ctc 9.135705 loss_rnnt 17.227396 hw_loss 0.447687 lr 0.00050122 rank 5
2023-02-20 23:29:36,614 DEBUG TRAIN Batch 11/15500 loss 15.249861 loss_att 28.077530 loss_ctc 19.189730 loss_rnnt 11.955297 hw_loss 0.381963 lr 0.00050122 rank 6
2023-02-20 23:29:36,632 DEBUG TRAIN Batch 11/15500 loss 4.100912 loss_att 8.835348 loss_ctc 8.716462 loss_rnnt 2.327545 hw_loss 0.395762 lr 0.00050122 rank 7
2023-02-20 23:29:36,675 DEBUG TRAIN Batch 11/15500 loss 23.718552 loss_att 31.085194 loss_ctc 18.609898 loss_rnnt 22.752003 hw_loss 0.326946 lr 0.00050122 rank 4
2023-02-20 23:30:54,335 DEBUG TRAIN Batch 11/15600 loss 36.829105 loss_att 43.474625 loss_ctc 41.886818 loss_rnnt 34.757603 hw_loss 0.127560 lr 0.00050109 rank 0
2023-02-20 23:30:54,338 DEBUG TRAIN Batch 11/15600 loss 33.501194 loss_att 37.764236 loss_ctc 37.818970 loss_rnnt 31.926449 hw_loss 0.274562 lr 0.00050109 rank 2
2023-02-20 23:30:54,340 DEBUG TRAIN Batch 11/15600 loss 15.297693 loss_att 15.821746 loss_ctc 21.753593 loss_rnnt 14.146379 hw_loss 0.348220 lr 0.00050109 rank 1
2023-02-20 23:30:54,342 DEBUG TRAIN Batch 11/15600 loss 38.766472 loss_att 50.039040 loss_ctc 44.424496 loss_rnnt 35.580463 hw_loss 0.332041 lr 0.00050109 rank 3
2023-02-20 23:30:54,342 DEBUG TRAIN Batch 11/15600 loss 18.697838 loss_att 32.625702 loss_ctc 22.018347 loss_rnnt 15.468601 hw_loss 0.001743 lr 0.00050109 rank 6
2023-02-20 23:30:54,347 DEBUG TRAIN Batch 11/15600 loss 15.855042 loss_att 15.715296 loss_ctc 20.140341 loss_rnnt 15.161853 hw_loss 0.280813 lr 0.00050109 rank 7
2023-02-20 23:30:54,348 DEBUG TRAIN Batch 11/15600 loss 27.249647 loss_att 25.190029 loss_ctc 28.403625 loss_rnnt 27.423719 hw_loss 0.157475 lr 0.00050109 rank 4
2023-02-20 23:30:54,401 DEBUG TRAIN Batch 11/15600 loss 6.007189 loss_att 9.204821 loss_ctc 8.522775 loss_rnnt 4.835717 hw_loss 0.368502 lr 0.00050109 rank 5
2023-02-20 23:31:55,344 DEBUG TRAIN Batch 11/15700 loss 38.457180 loss_att 39.075512 loss_ctc 49.788643 loss_rnnt 36.688576 hw_loss 0.251385 lr 0.00050097 rank 0
2023-02-20 23:31:55,348 DEBUG TRAIN Batch 11/15700 loss 3.646171 loss_att 7.750173 loss_ctc 6.056273 loss_rnnt 2.263350 hw_loss 0.451263 lr 0.00050097 rank 1
2023-02-20 23:31:55,354 DEBUG TRAIN Batch 11/15700 loss 26.476366 loss_att 34.471123 loss_ctc 34.680954 loss_rnnt 23.685139 hw_loss 0.184370 lr 0.00050097 rank 5
2023-02-20 23:31:55,355 DEBUG TRAIN Batch 11/15700 loss 34.701523 loss_att 59.299393 loss_ctc 35.726311 loss_rnnt 29.644293 hw_loss 0.001906 lr 0.00050097 rank 2
2023-02-20 23:31:55,355 DEBUG TRAIN Batch 11/15700 loss 28.637527 loss_att 31.638288 loss_ctc 32.460861 loss_rnnt 27.319616 hw_loss 0.389965 lr 0.00050097 rank 6
2023-02-20 23:31:55,362 DEBUG TRAIN Batch 11/15700 loss 4.947204 loss_att 11.407321 loss_ctc 11.113241 loss_rnnt 2.570781 hw_loss 0.491738 lr 0.00050097 rank 4
2023-02-20 23:31:55,363 DEBUG TRAIN Batch 11/15700 loss 23.199326 loss_att 23.114985 loss_ctc 27.288525 loss_rnnt 22.580873 hw_loss 0.168923 lr 0.00050097 rank 3
2023-02-20 23:31:55,365 DEBUG TRAIN Batch 11/15700 loss 13.826703 loss_att 18.340752 loss_ctc 17.919182 loss_rnnt 12.239966 hw_loss 0.259245 lr 0.00050097 rank 7
2023-02-20 23:32:54,792 DEBUG TRAIN Batch 11/15800 loss 14.279590 loss_att 25.410084 loss_ctc 15.596332 loss_rnnt 11.715393 hw_loss 0.304747 lr 0.00050084 rank 0
2023-02-20 23:32:54,798 DEBUG TRAIN Batch 11/15800 loss 12.905186 loss_att 14.555021 loss_ctc 14.588536 loss_rnnt 12.107825 hw_loss 0.455524 lr 0.00050084 rank 1
2023-02-20 23:32:54,800 DEBUG TRAIN Batch 11/15800 loss 19.031281 loss_att 17.943892 loss_ctc 20.902122 loss_rnnt 18.890995 hw_loss 0.203095 lr 0.00050084 rank 6
2023-02-20 23:32:54,800 DEBUG TRAIN Batch 11/15800 loss 9.931307 loss_att 19.239574 loss_ctc 8.903373 loss_rnnt 8.206236 hw_loss 0.000893 lr 0.00050084 rank 3
2023-02-20 23:32:54,800 DEBUG TRAIN Batch 11/15800 loss 9.300973 loss_att 14.220963 loss_ctc 14.943881 loss_rnnt 7.380641 hw_loss 0.344899 lr 0.00050084 rank 2
2023-02-20 23:32:54,801 DEBUG TRAIN Batch 11/15800 loss 11.110756 loss_att 17.740686 loss_ctc 12.296480 loss_rnnt 9.416024 hw_loss 0.394967 lr 0.00050084 rank 7
2023-02-20 23:32:54,802 DEBUG TRAIN Batch 11/15800 loss 17.949848 loss_att 17.721949 loss_ctc 17.854139 loss_rnnt 17.882223 hw_loss 0.236189 lr 0.00050084 rank 5
2023-02-20 23:32:54,809 DEBUG TRAIN Batch 11/15800 loss 15.209890 loss_att 20.163652 loss_ctc 17.888147 loss_rnnt 13.705353 hw_loss 0.293784 lr 0.00050084 rank 4
2023-02-20 23:33:53,044 DEBUG TRAIN Batch 11/15900 loss 15.617372 loss_att 23.596371 loss_ctc 14.268112 loss_rnnt 14.011820 hw_loss 0.355598 lr 0.00050071 rank 0
2023-02-20 23:33:53,046 DEBUG TRAIN Batch 11/15900 loss 15.393694 loss_att 23.687893 loss_ctc 20.120939 loss_rnnt 13.023355 hw_loss 0.152250 lr 0.00050071 rank 3
2023-02-20 23:33:53,047 DEBUG TRAIN Batch 11/15900 loss 19.041510 loss_att 18.556381 loss_ctc 20.387814 loss_rnnt 18.765831 hw_loss 0.362244 lr 0.00050071 rank 6
2023-02-20 23:33:53,048 DEBUG TRAIN Batch 11/15900 loss 22.995974 loss_att 21.088211 loss_ctc 22.938793 loss_rnnt 23.138687 hw_loss 0.462117 lr 0.00050071 rank 7
2023-02-20 23:33:53,049 DEBUG TRAIN Batch 11/15900 loss 12.724707 loss_att 16.468639 loss_ctc 11.925689 loss_rnnt 11.998209 hw_loss 0.157962 lr 0.00050071 rank 1
2023-02-20 23:33:53,051 DEBUG TRAIN Batch 11/15900 loss 13.289806 loss_att 22.858301 loss_ctc 17.899872 loss_rnnt 10.554431 hw_loss 0.388127 lr 0.00050071 rank 2
2023-02-20 23:33:53,055 DEBUG TRAIN Batch 11/15900 loss 29.352207 loss_att 34.158695 loss_ctc 36.521309 loss_rnnt 27.287188 hw_loss 0.277204 lr 0.00050071 rank 4
2023-02-20 23:33:53,107 DEBUG TRAIN Batch 11/15900 loss 39.385490 loss_att 58.609840 loss_ctc 52.200050 loss_rnnt 33.718678 hw_loss 0.212498 lr 0.00050071 rank 5
2023-02-20 23:34:53,206 DEBUG TRAIN Batch 11/16000 loss 24.459024 loss_att 30.603947 loss_ctc 27.021893 loss_rnnt 22.751043 hw_loss 0.257399 lr 0.00050059 rank 2
2023-02-20 23:34:53,209 DEBUG TRAIN Batch 11/16000 loss 4.006430 loss_att 11.663166 loss_ctc 7.557316 loss_rnnt 1.883926 hw_loss 0.220699 lr 0.00050059 rank 5
2023-02-20 23:34:53,211 DEBUG TRAIN Batch 11/16000 loss 15.148152 loss_att 14.812485 loss_ctc 12.717158 loss_rnnt 15.282068 hw_loss 0.482529 lr 0.00050059 rank 0
2023-02-20 23:34:53,213 DEBUG TRAIN Batch 11/16000 loss 19.081219 loss_att 26.986383 loss_ctc 14.988795 loss_rnnt 18.045637 hw_loss 0.000385 lr 0.00050059 rank 1
2023-02-20 23:34:53,215 DEBUG TRAIN Batch 11/16000 loss 13.700611 loss_att 24.246752 loss_ctc 19.002399 loss_rnnt 10.699075 hw_loss 0.347629 lr 0.00050059 rank 6
2023-02-20 23:34:53,221 DEBUG TRAIN Batch 11/16000 loss 68.962234 loss_att 74.843498 loss_ctc 86.871613 loss_rnnt 65.230713 hw_loss 0.313781 lr 0.00050059 rank 3
2023-02-20 23:34:53,221 DEBUG TRAIN Batch 11/16000 loss 11.434800 loss_att 23.966480 loss_ctc 13.169771 loss_rnnt 8.538954 hw_loss 0.296590 lr 0.00050059 rank 7
2023-02-20 23:34:53,275 DEBUG TRAIN Batch 11/16000 loss 20.991014 loss_att 26.023439 loss_ctc 20.315956 loss_rnnt 20.074329 hw_loss 0.000385 lr 0.00050059 rank 4
2023-02-20 23:35:50,359 DEBUG TRAIN Batch 11/16100 loss 9.520894 loss_att 10.182656 loss_ctc 12.231364 loss_rnnt 8.765858 hw_loss 0.489914 lr 0.00050046 rank 0
2023-02-20 23:35:50,366 DEBUG TRAIN Batch 11/16100 loss 11.730725 loss_att 12.603924 loss_ctc 16.520336 loss_rnnt 10.767518 hw_loss 0.281159 lr 0.00050046 rank 1
2023-02-20 23:35:50,369 DEBUG TRAIN Batch 11/16100 loss 14.033790 loss_att 13.307300 loss_ctc 15.380991 loss_rnnt 13.782837 hw_loss 0.406169 lr 0.00050046 rank 2
2023-02-20 23:35:50,370 DEBUG TRAIN Batch 11/16100 loss 33.509998 loss_att 35.859440 loss_ctc 47.540222 loss_rnnt 31.169224 hw_loss 0.000351 lr 0.00050046 rank 5
2023-02-20 23:35:50,375 DEBUG TRAIN Batch 11/16100 loss 12.133247 loss_att 12.540342 loss_ctc 10.206630 loss_rnnt 12.173403 hw_loss 0.253700 lr 0.00050046 rank 4
2023-02-20 23:35:50,376 DEBUG TRAIN Batch 11/16100 loss 9.209774 loss_att 9.816624 loss_ctc 14.341283 loss_rnnt 8.216499 hw_loss 0.351944 lr 0.00050046 rank 3
2023-02-20 23:35:50,377 DEBUG TRAIN Batch 11/16100 loss 6.587809 loss_att 14.823666 loss_ctc 6.637338 loss_rnnt 4.663211 hw_loss 0.507793 lr 0.00050046 rank 7
2023-02-20 23:35:50,382 DEBUG TRAIN Batch 11/16100 loss 5.102578 loss_att 8.819168 loss_ctc 5.004018 loss_rnnt 4.372213 hw_loss 0.000351 lr 0.00050046 rank 6
2023-02-20 23:36:49,243 DEBUG TRAIN Batch 11/16200 loss 20.419035 loss_att 20.958717 loss_ctc 21.741774 loss_rnnt 19.910851 hw_loss 0.419781 lr 0.00050034 rank 2
2023-02-20 23:36:49,247 DEBUG TRAIN Batch 11/16200 loss 12.001213 loss_att 21.589687 loss_ctc 14.524053 loss_rnnt 9.667874 hw_loss 0.148623 lr 0.00050034 rank 0
2023-02-20 23:36:49,253 DEBUG TRAIN Batch 11/16200 loss 15.985742 loss_att 19.828562 loss_ctc 20.578371 loss_rnnt 14.435871 hw_loss 0.316791 lr 0.00050034 rank 3
2023-02-20 23:36:49,253 DEBUG TRAIN Batch 11/16200 loss 0.912943 loss_att 3.684195 loss_ctc 0.698271 loss_rnnt 0.164692 hw_loss 0.417417 lr 0.00050034 rank 4
2023-02-20 23:36:49,254 DEBUG TRAIN Batch 11/16200 loss 14.329194 loss_att 16.346220 loss_ctc 14.720968 loss_rnnt 13.697971 hw_loss 0.329216 lr 0.00050034 rank 6
2023-02-20 23:36:49,259 DEBUG TRAIN Batch 11/16200 loss 25.038530 loss_att 30.219852 loss_ctc 39.222515 loss_rnnt 21.981543 hw_loss 0.242864 lr 0.00050034 rank 7
2023-02-20 23:36:49,293 DEBUG TRAIN Batch 11/16200 loss 27.155470 loss_att 30.962742 loss_ctc 24.438980 loss_rnnt 26.704273 hw_loss 0.097387 lr 0.00050034 rank 1
2023-02-20 23:36:49,294 DEBUG TRAIN Batch 11/16200 loss 18.162857 loss_att 23.007151 loss_ctc 18.280746 loss_rnnt 17.077137 hw_loss 0.189645 lr 0.00050034 rank 5
2023-02-20 23:37:50,094 DEBUG TRAIN Batch 11/16300 loss 12.486409 loss_att 33.606323 loss_ctc 15.109646 loss_rnnt 7.912456 hw_loss 0.000386 lr 0.00050021 rank 1
2023-02-20 23:37:50,094 DEBUG TRAIN Batch 11/16300 loss 19.965313 loss_att 23.620251 loss_ctc 21.466240 loss_rnnt 18.983673 hw_loss 0.094745 lr 0.00050021 rank 3
2023-02-20 23:37:50,096 DEBUG TRAIN Batch 11/16300 loss 8.399612 loss_att 12.913877 loss_ctc 8.509748 loss_rnnt 7.220500 hw_loss 0.490453 lr 0.00050021 rank 0
2023-02-20 23:37:50,096 DEBUG TRAIN Batch 11/16300 loss 41.442707 loss_att 37.631916 loss_ctc 79.732376 loss_rnnt 37.099377 hw_loss 0.000386 lr 0.00050021 rank 5
2023-02-20 23:37:50,097 DEBUG TRAIN Batch 11/16300 loss 28.838966 loss_att 33.788803 loss_ctc 42.796101 loss_rnnt 25.987841 hw_loss 0.000386 lr 0.00050021 rank 2
2023-02-20 23:37:50,105 DEBUG TRAIN Batch 11/16300 loss 14.509148 loss_att 20.094437 loss_ctc 19.801922 loss_rnnt 12.575250 hw_loss 0.208381 lr 0.00050021 rank 7
2023-02-20 23:37:50,107 DEBUG TRAIN Batch 11/16300 loss 12.774279 loss_att 15.965504 loss_ctc 17.362396 loss_rnnt 11.329207 hw_loss 0.365770 lr 0.00050021 rank 6
2023-02-20 23:37:50,170 DEBUG TRAIN Batch 11/16300 loss 34.696079 loss_att 36.551105 loss_ctc 36.464687 loss_rnnt 33.968975 hw_loss 0.225536 lr 0.00050021 rank 4
2023-02-20 23:39:10,615 DEBUG TRAIN Batch 11/16400 loss 32.275154 loss_att 37.303852 loss_ctc 32.737072 loss_rnnt 31.012318 hw_loss 0.366573 lr 0.00050009 rank 0
2023-02-20 23:39:10,620 DEBUG TRAIN Batch 11/16400 loss 36.041615 loss_att 34.292004 loss_ctc 39.494240 loss_rnnt 35.772751 hw_loss 0.297059 lr 0.00050009 rank 2
2023-02-20 23:39:10,629 DEBUG TRAIN Batch 11/16400 loss 16.837601 loss_att 19.706209 loss_ctc 17.995441 loss_rnnt 15.966578 hw_loss 0.267976 lr 0.00050009 rank 3
2023-02-20 23:39:10,631 DEBUG TRAIN Batch 11/16400 loss 20.385021 loss_att 26.437962 loss_ctc 27.986521 loss_rnnt 17.888815 hw_loss 0.510156 lr 0.00050009 rank 6
2023-02-20 23:39:10,634 DEBUG TRAIN Batch 11/16400 loss 32.478226 loss_att 50.334511 loss_ctc 37.424648 loss_rnnt 28.113628 hw_loss 0.250912 lr 0.00050009 rank 5
2023-02-20 23:39:10,635 DEBUG TRAIN Batch 11/16400 loss 17.480961 loss_att 26.552607 loss_ctc 21.677174 loss_rnnt 14.911325 hw_loss 0.367144 lr 0.00050009 rank 7
2023-02-20 23:39:10,637 DEBUG TRAIN Batch 11/16400 loss 7.870407 loss_att 14.295208 loss_ctc 5.897427 loss_rnnt 6.632456 hw_loss 0.405102 lr 0.00050009 rank 4
2023-02-20 23:39:10,686 DEBUG TRAIN Batch 11/16400 loss 21.546623 loss_att 19.825447 loss_ctc 26.492353 loss_rnnt 21.017448 hw_loss 0.401212 lr 0.00050009 rank 1
2023-02-20 23:40:22,082 DEBUG TRAIN Batch 11/16500 loss 21.754463 loss_att 35.546600 loss_ctc 30.789204 loss_rnnt 17.712925 hw_loss 0.147153 lr 0.00049996 rank 0
2023-02-20 23:40:22,092 DEBUG TRAIN Batch 11/16500 loss 17.570669 loss_att 19.996107 loss_ctc 21.313643 loss_rnnt 16.504341 hw_loss 0.154081 lr 0.00049996 rank 6
2023-02-20 23:40:22,092 DEBUG TRAIN Batch 11/16500 loss 14.877156 loss_att 17.495787 loss_ctc 12.778253 loss_rnnt 14.524633 hw_loss 0.203721 lr 0.00049996 rank 4
2023-02-20 23:40:22,093 DEBUG TRAIN Batch 11/16500 loss 26.715580 loss_att 31.286186 loss_ctc 29.509270 loss_rnnt 25.327274 hw_loss 0.190666 lr 0.00049996 rank 3
2023-02-20 23:40:22,094 DEBUG TRAIN Batch 11/16500 loss 25.314823 loss_att 30.035809 loss_ctc 25.201900 loss_rnnt 24.263973 hw_loss 0.228209 lr 0.00049996 rank 1
2023-02-20 23:40:22,096 DEBUG TRAIN Batch 11/16500 loss 4.369108 loss_att 13.403899 loss_ctc 4.927319 loss_rnnt 2.187331 hw_loss 0.563231 lr 0.00049996 rank 2
2023-02-20 23:40:22,099 DEBUG TRAIN Batch 11/16500 loss 13.492258 loss_att 17.489157 loss_ctc 17.810398 loss_rnnt 11.997113 hw_loss 0.225024 lr 0.00049996 rank 7
2023-02-20 23:40:22,148 DEBUG TRAIN Batch 11/16500 loss 3.512511 loss_att 7.590033 loss_ctc 4.633213 loss_rnnt 2.351569 hw_loss 0.367522 lr 0.00049996 rank 5
2023-02-20 23:41:22,484 DEBUG TRAIN Batch 11/16600 loss 11.884096 loss_att 14.139737 loss_ctc 12.818852 loss_rnnt 10.976885 hw_loss 0.621466 lr 0.00049984 rank 2
2023-02-20 23:41:22,487 DEBUG TRAIN Batch 11/16600 loss 4.339849 loss_att 6.862104 loss_ctc 8.367295 loss_rnnt 3.298207 hw_loss 0.000373 lr 0.00049984 rank 5
2023-02-20 23:41:22,489 DEBUG TRAIN Batch 11/16600 loss 24.001949 loss_att 26.181442 loss_ctc 14.918779 loss_rnnt 24.776943 hw_loss 0.000373 lr 0.00049984 rank 0
2023-02-20 23:41:22,489 DEBUG TRAIN Batch 11/16600 loss 11.974460 loss_att 15.734361 loss_ctc 12.968039 loss_rnnt 10.844035 hw_loss 0.461190 lr 0.00049984 rank 1
2023-02-20 23:41:22,491 DEBUG TRAIN Batch 11/16600 loss 2.635298 loss_att 7.215253 loss_ctc 2.295039 loss_rnnt 1.567694 hw_loss 0.369339 lr 0.00049984 rank 3
2023-02-20 23:41:22,491 DEBUG TRAIN Batch 11/16600 loss 13.768993 loss_att 27.557663 loss_ctc 18.345661 loss_rnnt 10.338256 hw_loss 0.117715 lr 0.00049984 rank 6
2023-02-20 23:41:22,497 DEBUG TRAIN Batch 11/16600 loss 31.700388 loss_att 37.027611 loss_ctc 30.520725 loss_rnnt 30.792034 hw_loss 0.000373 lr 0.00049984 rank 4
2023-02-20 23:41:22,505 DEBUG TRAIN Batch 11/16600 loss 11.531285 loss_att 21.206306 loss_ctc 18.048023 loss_rnnt 8.664831 hw_loss 0.117283 lr 0.00049984 rank 7
2023-02-20 23:42:20,263 DEBUG TRAIN Batch 11/16700 loss 16.946751 loss_att 13.751154 loss_ctc 21.048653 loss_rnnt 16.763134 hw_loss 0.517154 lr 0.00049971 rank 0
2023-02-20 23:42:20,269 DEBUG TRAIN Batch 11/16700 loss 31.619131 loss_att 41.002125 loss_ctc 36.311714 loss_rnnt 29.115759 hw_loss 0.002054 lr 0.00049971 rank 5
2023-02-20 23:42:20,271 DEBUG TRAIN Batch 11/16700 loss 31.887356 loss_att 34.077499 loss_ctc 35.414116 loss_rnnt 30.816889 hw_loss 0.304129 lr 0.00049971 rank 2
2023-02-20 23:42:20,277 DEBUG TRAIN Batch 11/16700 loss 30.396090 loss_att 33.522984 loss_ctc 31.696207 loss_rnnt 29.596266 hw_loss 0.002054 lr 0.00049971 rank 6
2023-02-20 23:42:20,276 DEBUG TRAIN Batch 11/16700 loss 19.941710 loss_att 36.216442 loss_ctc 30.332396 loss_rnnt 15.300243 hw_loss 0.002054 lr 0.00049971 rank 7
2023-02-20 23:42:20,280 DEBUG TRAIN Batch 11/16700 loss 10.205441 loss_att 14.853312 loss_ctc 12.086042 loss_rnnt 8.907722 hw_loss 0.220123 lr 0.00049971 rank 3
2023-02-20 23:42:20,285 DEBUG TRAIN Batch 11/16700 loss 13.653013 loss_att 22.112679 loss_ctc 18.200169 loss_rnnt 11.142340 hw_loss 0.398349 lr 0.00049971 rank 4
2023-02-20 23:42:20,333 DEBUG TRAIN Batch 11/16700 loss 24.769020 loss_att 37.769279 loss_ctc 23.477089 loss_rnnt 22.121122 hw_loss 0.412692 lr 0.00049971 rank 1
2023-02-20 23:43:19,478 DEBUG TRAIN Batch 11/16800 loss 19.894238 loss_att 24.636793 loss_ctc 21.725723 loss_rnnt 18.537893 hw_loss 0.306817 lr 0.00049959 rank 2
2023-02-20 23:43:19,480 DEBUG TRAIN Batch 11/16800 loss 24.155798 loss_att 29.098074 loss_ctc 23.253542 loss_rnnt 23.156126 hw_loss 0.246597 lr 0.00049959 rank 0
2023-02-20 23:43:19,485 DEBUG TRAIN Batch 11/16800 loss 35.576702 loss_att 35.732956 loss_ctc 40.638474 loss_rnnt 34.688229 hw_loss 0.341857 lr 0.00049959 rank 3
2023-02-20 23:43:19,491 DEBUG TRAIN Batch 11/16800 loss 11.690697 loss_att 14.219266 loss_ctc 12.854124 loss_rnnt 10.821093 hw_loss 0.391436 lr 0.00049959 rank 6
2023-02-20 23:43:19,492 DEBUG TRAIN Batch 11/16800 loss 11.779766 loss_att 10.708002 loss_ctc 13.895586 loss_rnnt 11.674456 hw_loss 0.070412 lr 0.00049959 rank 5
2023-02-20 23:43:19,492 DEBUG TRAIN Batch 11/16800 loss 16.850531 loss_att 18.273411 loss_ctc 17.225079 loss_rnnt 16.374186 hw_loss 0.265925 lr 0.00049959 rank 7
2023-02-20 23:43:19,494 DEBUG TRAIN Batch 11/16800 loss 21.051640 loss_att 25.807215 loss_ctc 25.666384 loss_rnnt 19.341976 hw_loss 0.268595 lr 0.00049959 rank 1
2023-02-20 23:43:19,549 DEBUG TRAIN Batch 11/16800 loss 10.119800 loss_att 14.099782 loss_ctc 13.358467 loss_rnnt 8.891540 hw_loss 0.000826 lr 0.00049959 rank 4
2023-02-20 23:44:19,717 DEBUG TRAIN Batch 11/16900 loss 20.916109 loss_att 31.346157 loss_ctc 18.366222 loss_rnnt 18.989538 hw_loss 0.338524 lr 0.00049946 rank 0
2023-02-20 23:44:19,723 DEBUG TRAIN Batch 11/16900 loss 8.673826 loss_att 16.855572 loss_ctc 9.867151 loss_rnnt 6.653985 hw_loss 0.420717 lr 0.00049946 rank 2
2023-02-20 23:44:19,733 DEBUG TRAIN Batch 11/16900 loss 6.394515 loss_att 8.446277 loss_ctc 7.930809 loss_rnnt 5.526085 hw_loss 0.474823 lr 0.00049946 rank 4
2023-02-20 23:44:19,734 DEBUG TRAIN Batch 11/16900 loss 11.876951 loss_att 15.793029 loss_ctc 11.779001 loss_rnnt 10.987668 hw_loss 0.223362 lr 0.00049946 rank 1
2023-02-20 23:44:19,737 DEBUG TRAIN Batch 11/16900 loss 14.949619 loss_att 14.627207 loss_ctc 21.116507 loss_rnnt 13.968809 hw_loss 0.418203 lr 0.00049946 rank 3
2023-02-20 23:44:19,737 DEBUG TRAIN Batch 11/16900 loss 18.153229 loss_att 20.461874 loss_ctc 20.475657 loss_rnnt 17.243296 hw_loss 0.259774 lr 0.00049946 rank 6
2023-02-20 23:44:19,737 DEBUG TRAIN Batch 11/16900 loss 4.624765 loss_att 10.181011 loss_ctc 3.174629 loss_rnnt 3.406905 hw_loss 0.562428 lr 0.00049946 rank 7
2023-02-20 23:44:19,738 DEBUG TRAIN Batch 11/16900 loss 22.354345 loss_att 30.127396 loss_ctc 31.575554 loss_rnnt 19.381575 hw_loss 0.353749 lr 0.00049946 rank 5
2023-02-20 23:45:17,244 DEBUG TRAIN Batch 11/17000 loss 12.329496 loss_att 13.631393 loss_ctc 16.422054 loss_rnnt 11.398565 hw_loss 0.234147 lr 0.00049934 rank 0
2023-02-20 23:45:17,254 DEBUG TRAIN Batch 11/17000 loss 12.437377 loss_att 18.184938 loss_ctc 16.331335 loss_rnnt 10.555645 hw_loss 0.399419 lr 0.00049934 rank 1
2023-02-20 23:45:17,255 DEBUG TRAIN Batch 11/17000 loss 19.027500 loss_att 18.201679 loss_ctc 23.454443 loss_rnnt 18.342464 hw_loss 0.487387 lr 0.00049934 rank 5
2023-02-20 23:45:17,257 DEBUG TRAIN Batch 11/17000 loss 23.931784 loss_att 25.747244 loss_ctc 30.170147 loss_rnnt 22.571064 hw_loss 0.310955 lr 0.00049934 rank 3
2023-02-20 23:45:17,258 DEBUG TRAIN Batch 11/17000 loss 25.459053 loss_att 25.677568 loss_ctc 31.959572 loss_rnnt 24.383020 hw_loss 0.310488 lr 0.00049934 rank 2
2023-02-20 23:45:17,263 DEBUG TRAIN Batch 11/17000 loss 18.362183 loss_att 19.345058 loss_ctc 16.616714 loss_rnnt 18.397827 hw_loss 0.000953 lr 0.00049934 rank 6
2023-02-20 23:45:17,267 DEBUG TRAIN Batch 11/17000 loss 11.729162 loss_att 16.732651 loss_ctc 13.307021 loss_rnnt 10.517574 hw_loss 0.000953 lr 0.00049934 rank 7
2023-02-20 23:45:17,270 DEBUG TRAIN Batch 11/17000 loss 11.017653 loss_att 19.123974 loss_ctc 12.097544 loss_rnnt 9.086643 hw_loss 0.310803 lr 0.00049934 rank 4
2023-02-20 23:46:17,522 DEBUG TRAIN Batch 11/17100 loss 1.562456 loss_att 6.750540 loss_ctc 1.432645 loss_rnnt 0.385664 hw_loss 0.293407 lr 0.00049921 rank 0
2023-02-20 23:46:17,530 DEBUG TRAIN Batch 11/17100 loss 31.326822 loss_att 26.981680 loss_ctc 38.196457 loss_rnnt 31.031904 hw_loss 0.464991 lr 0.00049921 rank 5
2023-02-20 23:46:17,532 DEBUG TRAIN Batch 11/17100 loss 28.657169 loss_att 37.139984 loss_ctc 31.927898 loss_rnnt 26.331383 hw_loss 0.362115 lr 0.00049921 rank 2
2023-02-20 23:46:17,535 DEBUG TRAIN Batch 11/17100 loss 10.573082 loss_att 13.738845 loss_ctc 14.572447 loss_rnnt 9.405478 hw_loss 0.002255 lr 0.00049921 rank 1
2023-02-20 23:46:17,536 DEBUG TRAIN Batch 11/17100 loss 27.896193 loss_att 27.736359 loss_ctc 32.117130 loss_rnnt 27.119350 hw_loss 0.461282 lr 0.00049921 rank 6
2023-02-20 23:46:17,535 DEBUG TRAIN Batch 11/17100 loss 22.845173 loss_att 30.426491 loss_ctc 30.948139 loss_rnnt 20.095123 hw_loss 0.287605 lr 0.00049921 rank 3
2023-02-20 23:46:17,535 DEBUG TRAIN Batch 11/17100 loss 15.837005 loss_att 24.876320 loss_ctc 16.756014 loss_rnnt 13.651356 hw_loss 0.478596 lr 0.00049921 rank 4
2023-02-20 23:46:17,596 DEBUG TRAIN Batch 11/17100 loss 20.757820 loss_att 28.027458 loss_ctc 31.824543 loss_rnnt 17.666843 hw_loss 0.302786 lr 0.00049921 rank 7
2023-02-20 23:47:35,459 DEBUG TRAIN Batch 11/17200 loss 23.392382 loss_att 54.138313 loss_ctc 21.307444 loss_rnnt 17.313379 hw_loss 0.389637 lr 0.00049909 rank 0
2023-02-20 23:47:35,460 DEBUG TRAIN Batch 11/17200 loss 12.869346 loss_att 17.396166 loss_ctc 10.917188 loss_rnnt 12.224073 hw_loss 0.000370 lr 0.00049909 rank 1
2023-02-20 23:47:35,461 DEBUG TRAIN Batch 11/17200 loss 52.185970 loss_att 53.342285 loss_ctc 51.422211 loss_rnnt 51.836029 hw_loss 0.413467 lr 0.00049909 rank 2
2023-02-20 23:47:35,462 DEBUG TRAIN Batch 11/17200 loss 11.911842 loss_att 19.736603 loss_ctc 12.585296 loss_rnnt 10.256900 hw_loss 0.000370 lr 0.00049909 rank 6
2023-02-20 23:47:35,463 DEBUG TRAIN Batch 11/17200 loss 33.286209 loss_att 43.533836 loss_ctc 53.433937 loss_rnnt 28.307251 hw_loss 0.455754 lr 0.00049909 rank 3
2023-02-20 23:47:35,464 DEBUG TRAIN Batch 11/17200 loss 11.680661 loss_att 18.527365 loss_ctc 11.812248 loss_rnnt 10.159567 hw_loss 0.251643 lr 0.00049909 rank 4
2023-02-20 23:47:35,468 DEBUG TRAIN Batch 11/17200 loss 12.614134 loss_att 17.762644 loss_ctc 17.765434 loss_rnnt 10.849821 hw_loss 0.089568 lr 0.00049909 rank 5
2023-02-20 23:47:35,475 DEBUG TRAIN Batch 11/17200 loss 7.622022 loss_att 12.253277 loss_ctc 7.077147 loss_rnnt 6.657028 hw_loss 0.208861 lr 0.00049909 rank 7
2023-02-20 23:48:51,236 DEBUG TRAIN Batch 11/17300 loss 61.530121 loss_att 45.509769 loss_ctc 71.191177 loss_rnnt 63.322197 hw_loss 0.232230 lr 0.00049897 rank 6
2023-02-20 23:48:51,238 DEBUG TRAIN Batch 11/17300 loss 23.277582 loss_att 29.118343 loss_ctc 25.951069 loss_rnnt 21.569447 hw_loss 0.344097 lr 0.00049897 rank 0
2023-02-20 23:48:51,239 DEBUG TRAIN Batch 11/17300 loss 20.937450 loss_att 25.497101 loss_ctc 22.555473 loss_rnnt 19.719370 hw_loss 0.169525 lr 0.00049897 rank 1
2023-02-20 23:48:51,240 DEBUG TRAIN Batch 11/17300 loss 6.090098 loss_att 11.047215 loss_ctc 7.838701 loss_rnnt 4.781775 hw_loss 0.157037 lr 0.00049897 rank 2
2023-02-20 23:48:51,245 DEBUG TRAIN Batch 11/17300 loss 18.388025 loss_att 17.069931 loss_ctc 22.929035 loss_rnnt 17.682421 hw_loss 0.682041 lr 0.00049897 rank 7
2023-02-20 23:48:51,246 DEBUG TRAIN Batch 11/17300 loss 32.900517 loss_att 34.863995 loss_ctc 40.531555 loss_rnnt 31.294128 hw_loss 0.367908 lr 0.00049897 rank 5
2023-02-20 23:48:51,247 DEBUG TRAIN Batch 11/17300 loss 4.599378 loss_att 6.933723 loss_ctc 2.930524 loss_rnnt 4.181350 hw_loss 0.325636 lr 0.00049897 rank 3
2023-02-20 23:48:51,248 DEBUG TRAIN Batch 11/17300 loss 6.819699 loss_att 11.127243 loss_ctc 8.177856 loss_rnnt 5.626995 hw_loss 0.281452 lr 0.00049897 rank 4
2023-02-20 23:49:51,170 DEBUG TRAIN Batch 11/17400 loss 9.533563 loss_att 13.382016 loss_ctc 9.931767 loss_rnnt 8.710531 hw_loss 0.000461 lr 0.00049884 rank 0
2023-02-20 23:49:51,177 DEBUG TRAIN Batch 11/17400 loss 4.257780 loss_att 8.645878 loss_ctc 4.021550 loss_rnnt 3.361709 hw_loss 0.093654 lr 0.00049884 rank 2
2023-02-20 23:49:51,180 DEBUG TRAIN Batch 11/17400 loss 42.987488 loss_att 44.375050 loss_ctc 52.720863 loss_rnnt 41.135025 hw_loss 0.519687 lr 0.00049884 rank 6
2023-02-20 23:49:51,180 DEBUG TRAIN Batch 11/17400 loss 13.742549 loss_att 22.020428 loss_ctc 10.234374 loss_rnnt 12.431908 hw_loss 0.230290 lr 0.00049884 rank 4
2023-02-20 23:49:51,182 DEBUG TRAIN Batch 11/17400 loss 35.087257 loss_att 31.649897 loss_ctc 43.196556 loss_rnnt 34.523998 hw_loss 0.317797 lr 0.00049884 rank 3
2023-02-20 23:49:51,182 DEBUG TRAIN Batch 11/17400 loss 18.740675 loss_att 23.433506 loss_ctc 21.215586 loss_rnnt 17.471874 hw_loss 0.000461 lr 0.00049884 rank 1
2023-02-20 23:49:51,184 DEBUG TRAIN Batch 11/17400 loss 17.879028 loss_att 25.942511 loss_ctc 21.852121 loss_rnnt 15.518597 hw_loss 0.408729 lr 0.00049884 rank 7
2023-02-20 23:49:51,186 DEBUG TRAIN Batch 11/17400 loss 30.740837 loss_att 43.565460 loss_ctc 38.258205 loss_rnnt 26.972626 hw_loss 0.376820 lr 0.00049884 rank 5
2023-02-20 23:50:49,986 DEBUG TRAIN Batch 11/17500 loss 9.153100 loss_att 13.927135 loss_ctc 10.423226 loss_rnnt 7.849170 hw_loss 0.337074 lr 0.00049872 rank 0
2023-02-20 23:50:49,997 DEBUG TRAIN Batch 11/17500 loss 16.645369 loss_att 33.289276 loss_ctc 18.287157 loss_rnnt 12.862772 hw_loss 0.440457 lr 0.00049872 rank 2
2023-02-20 23:50:50,000 DEBUG TRAIN Batch 11/17500 loss 30.515511 loss_att 42.735123 loss_ctc 40.845535 loss_rnnt 26.435772 hw_loss 0.484652 lr 0.00049872 rank 6
2023-02-20 23:50:50,000 DEBUG TRAIN Batch 11/17500 loss 16.993055 loss_att 33.207748 loss_ctc 22.095589 loss_rnnt 12.928645 hw_loss 0.264627 lr 0.00049872 rank 7
2023-02-20 23:50:50,001 DEBUG TRAIN Batch 11/17500 loss 47.780621 loss_att 55.699806 loss_ctc 58.230247 loss_rnnt 44.613571 hw_loss 0.356115 lr 0.00049872 rank 3
2023-02-20 23:50:50,001 DEBUG TRAIN Batch 11/17500 loss 7.552043 loss_att 8.822395 loss_ctc 7.664486 loss_rnnt 7.109882 hw_loss 0.324560 lr 0.00049872 rank 5
2023-02-20 23:50:50,009 DEBUG TRAIN Batch 11/17500 loss 22.865692 loss_att 24.504520 loss_ctc 30.952307 loss_rnnt 21.213387 hw_loss 0.461861 lr 0.00049872 rank 4
2023-02-20 23:50:50,062 DEBUG TRAIN Batch 11/17500 loss 39.255451 loss_att 45.783073 loss_ctc 49.795010 loss_rnnt 36.458385 hw_loss 0.161750 lr 0.00049872 rank 1
2023-02-20 23:51:48,275 DEBUG TRAIN Batch 11/17600 loss 21.062950 loss_att 25.326195 loss_ctc 20.115955 loss_rnnt 20.165234 hw_loss 0.321248 lr 0.00049859 rank 0
2023-02-20 23:51:48,279 DEBUG TRAIN Batch 11/17600 loss 19.525383 loss_att 22.326508 loss_ctc 19.026178 loss_rnnt 18.836773 hw_loss 0.365524 lr 0.00049859 rank 5
2023-02-20 23:51:48,279 DEBUG TRAIN Batch 11/17600 loss 15.766005 loss_att 19.310823 loss_ctc 17.433918 loss_rnnt 14.581201 hw_loss 0.475221 lr 0.00049859 rank 2
2023-02-20 23:51:48,282 DEBUG TRAIN Batch 11/17600 loss 9.909016 loss_att 14.321120 loss_ctc 9.221153 loss_rnnt 8.869199 hw_loss 0.467080 lr 0.00049859 rank 3
2023-02-20 23:51:48,287 DEBUG TRAIN Batch 11/17600 loss 26.344923 loss_att 25.549778 loss_ctc 33.819550 loss_rnnt 25.336956 hw_loss 0.319468 lr 0.00049859 rank 4
2023-02-20 23:51:48,288 DEBUG TRAIN Batch 11/17600 loss 21.453560 loss_att 22.998007 loss_ctc 21.029957 loss_rnnt 21.003922 hw_loss 0.369804 lr 0.00049859 rank 7
2023-02-20 23:51:48,289 DEBUG TRAIN Batch 11/17600 loss 16.678299 loss_att 25.363472 loss_ctc 23.207752 loss_rnnt 13.933392 hw_loss 0.257399 lr 0.00049859 rank 6
2023-02-20 23:51:48,302 DEBUG TRAIN Batch 11/17600 loss 10.119850 loss_att 20.714174 loss_ctc 16.368628 loss_rnnt 7.081685 hw_loss 0.161494 lr 0.00049859 rank 1
2023-02-20 23:52:48,565 DEBUG TRAIN Batch 11/17700 loss 50.113434 loss_att 55.739510 loss_ctc 54.632362 loss_rnnt 48.204628 hw_loss 0.339492 lr 0.00049847 rank 0
2023-02-20 23:52:48,567 DEBUG TRAIN Batch 11/17700 loss 28.525513 loss_att 29.996172 loss_ctc 37.189503 loss_rnnt 26.930990 hw_loss 0.272236 lr 0.00049847 rank 2
2023-02-20 23:52:48,567 DEBUG TRAIN Batch 11/17700 loss 11.459891 loss_att 20.396414 loss_ctc 11.586336 loss_rnnt 9.463095 hw_loss 0.361186 lr 0.00049847 rank 5
2023-02-20 23:52:48,569 DEBUG TRAIN Batch 11/17700 loss 29.684717 loss_att 36.303093 loss_ctc 28.790306 loss_rnnt 28.310394 hw_loss 0.318571 lr 0.00049847 rank 3
2023-02-20 23:52:48,570 DEBUG TRAIN Batch 11/17700 loss 11.977680 loss_att 20.164059 loss_ctc 14.901765 loss_rnnt 9.751957 hw_loss 0.372319 lr 0.00049847 rank 4
2023-02-20 23:52:48,572 DEBUG TRAIN Batch 11/17700 loss 7.682857 loss_att 12.840666 loss_ctc 7.561911 loss_rnnt 6.558389 hw_loss 0.204434 lr 0.00049847 rank 6
2023-02-20 23:52:48,617 DEBUG TRAIN Batch 11/17700 loss 14.626873 loss_att 19.133139 loss_ctc 16.649387 loss_rnnt 13.372766 hw_loss 0.155973 lr 0.00049847 rank 7
2023-02-20 23:52:48,654 DEBUG TRAIN Batch 11/17700 loss 3.542065 loss_att 9.135401 loss_ctc 6.843838 loss_rnnt 1.718413 hw_loss 0.496404 lr 0.00049847 rank 1
2023-02-20 23:53:47,045 DEBUG TRAIN Batch 11/17800 loss 8.723620 loss_att 10.447984 loss_ctc 9.650217 loss_rnnt 7.987252 hw_loss 0.502404 lr 0.00049835 rank 0
2023-02-20 23:53:47,055 DEBUG TRAIN Batch 11/17800 loss 16.692736 loss_att 28.727053 loss_ctc 16.989939 loss_rnnt 14.245640 hw_loss 0.001136 lr 0.00049835 rank 1
2023-02-20 23:53:47,056 DEBUG TRAIN Batch 11/17800 loss 30.533810 loss_att 47.341805 loss_ctc 36.741108 loss_rnnt 26.104521 hw_loss 0.450096 lr 0.00049835 rank 2
2023-02-20 23:53:47,057 DEBUG TRAIN Batch 11/17800 loss 5.619283 loss_att 12.376062 loss_ctc 5.181935 loss_rnnt 4.241565 hw_loss 0.158765 lr 0.00049835 rank 6
2023-02-20 23:53:47,060 DEBUG TRAIN Batch 11/17800 loss 40.308083 loss_att 45.062248 loss_ctc 52.671207 loss_rnnt 37.458054 hw_loss 0.470212 lr 0.00049835 rank 7
2023-02-20 23:53:47,062 DEBUG TRAIN Batch 11/17800 loss 2.454178 loss_att 7.693561 loss_ctc 1.681698 loss_rnnt 1.300879 hw_loss 0.390787 lr 0.00049835 rank 5
2023-02-20 23:53:47,062 DEBUG TRAIN Batch 11/17800 loss 14.142835 loss_att 17.791162 loss_ctc 19.553526 loss_rnnt 12.610014 hw_loss 0.153243 lr 0.00049835 rank 3
2023-02-20 23:53:47,063 DEBUG TRAIN Batch 11/17800 loss 12.701358 loss_att 14.249453 loss_ctc 12.604288 loss_rnnt 12.231460 hw_loss 0.324793 lr 0.00049835 rank 4
2023-02-20 23:54:46,400 DEBUG TRAIN Batch 11/17900 loss 18.307051 loss_att 18.932308 loss_ctc 16.542334 loss_rnnt 18.358660 hw_loss 0.109939 lr 0.00049822 rank 0
2023-02-20 23:54:46,410 DEBUG TRAIN Batch 11/17900 loss 30.937433 loss_att 23.886745 loss_ctc 34.993366 loss_rnnt 31.604908 hw_loss 0.378509 lr 0.00049822 rank 3
2023-02-20 23:54:46,414 DEBUG TRAIN Batch 11/17900 loss 26.402071 loss_att 35.137367 loss_ctc 28.402794 loss_rnnt 24.186544 hw_loss 0.378200 lr 0.00049822 rank 4
2023-02-20 23:54:46,416 DEBUG TRAIN Batch 11/17900 loss 30.836740 loss_att 28.323997 loss_ctc 31.932594 loss_rnnt 30.884998 hw_loss 0.577828 lr 0.00049822 rank 6
2023-02-20 23:54:46,417 DEBUG TRAIN Batch 11/17900 loss 11.400815 loss_att 16.988585 loss_ctc 13.570178 loss_rnnt 9.854683 hw_loss 0.261243 lr 0.00049822 rank 5
2023-02-20 23:54:46,420 DEBUG TRAIN Batch 11/17900 loss 24.121241 loss_att 27.423161 loss_ctc 24.958408 loss_rnnt 23.277660 hw_loss 0.134206 lr 0.00049822 rank 2
2023-02-20 23:54:46,422 DEBUG TRAIN Batch 11/17900 loss 27.240864 loss_att 30.105701 loss_ctc 31.998510 loss_rnnt 25.733627 hw_loss 0.562341 lr 0.00049822 rank 7
2023-02-20 23:54:46,471 DEBUG TRAIN Batch 11/17900 loss 28.373804 loss_att 25.862579 loss_ctc 33.416237 loss_rnnt 28.083527 hw_loss 0.225373 lr 0.00049822 rank 1
2023-02-20 23:55:46,496 DEBUG TRAIN Batch 11/18000 loss 11.349396 loss_att 20.469162 loss_ctc 11.175653 loss_rnnt 9.413042 hw_loss 0.254186 lr 0.00049810 rank 0
2023-02-20 23:55:46,502 DEBUG TRAIN Batch 11/18000 loss 28.366463 loss_att 25.555576 loss_ctc 28.766985 loss_rnnt 28.874964 hw_loss 0.000513 lr 0.00049810 rank 2
2023-02-20 23:55:46,506 DEBUG TRAIN Batch 11/18000 loss 11.086616 loss_att 16.920721 loss_ctc 8.532067 loss_rnnt 10.126261 hw_loss 0.251509 lr 0.00049810 rank 5
2023-02-20 23:55:46,505 DEBUG TRAIN Batch 11/18000 loss 23.136829 loss_att 28.037247 loss_ctc 21.593454 loss_rnnt 22.274046 hw_loss 0.165904 lr 0.00049810 rank 6
2023-02-20 23:55:46,505 DEBUG TRAIN Batch 11/18000 loss 12.596571 loss_att 11.343809 loss_ctc 16.515882 loss_rnnt 12.144733 hw_loss 0.337154 lr 0.00049810 rank 1
2023-02-20 23:55:46,506 DEBUG TRAIN Batch 11/18000 loss 23.097967 loss_att 36.463322 loss_ctc 44.150414 loss_rnnt 17.467575 hw_loss 0.281863 lr 0.00049810 rank 3
2023-02-20 23:55:46,509 DEBUG TRAIN Batch 11/18000 loss 34.367413 loss_att 43.949299 loss_ctc 39.052994 loss_rnnt 31.660954 hw_loss 0.309999 lr 0.00049810 rank 7
2023-02-20 23:55:46,512 DEBUG TRAIN Batch 11/18000 loss 24.589781 loss_att 37.310623 loss_ctc 38.742077 loss_rnnt 20.072882 hw_loss 0.160796 lr 0.00049810 rank 4
2023-02-20 23:57:04,330 DEBUG TRAIN Batch 11/18100 loss 15.163968 loss_att 21.003807 loss_ctc 21.471260 loss_rnnt 13.000093 hw_loss 0.290503 lr 0.00049797 rank 3
2023-02-20 23:57:04,331 DEBUG TRAIN Batch 11/18100 loss 8.307790 loss_att 18.251526 loss_ctc 12.471848 loss_rnnt 5.763682 hw_loss 0.000285 lr 0.00049797 rank 2
2023-02-20 23:57:04,337 DEBUG TRAIN Batch 11/18100 loss 25.144243 loss_att 20.252878 loss_ctc 35.149620 loss_rnnt 24.613577 hw_loss 0.327920 lr 0.00049797 rank 0
2023-02-20 23:57:04,339 DEBUG TRAIN Batch 11/18100 loss 9.172127 loss_att 20.774891 loss_ctc 11.133465 loss_rnnt 6.589910 hw_loss 0.000285 lr 0.00049797 rank 6
2023-02-20 23:57:04,339 DEBUG TRAIN Batch 11/18100 loss 18.943409 loss_att 23.365643 loss_ctc 28.919029 loss_rnnt 16.534973 hw_loss 0.363574 lr 0.00049797 rank 5
2023-02-20 23:57:04,346 DEBUG TRAIN Batch 11/18100 loss 23.412817 loss_att 26.365635 loss_ctc 24.101223 loss_rnnt 22.616821 hw_loss 0.213086 lr 0.00049797 rank 4
2023-02-20 23:57:04,348 DEBUG TRAIN Batch 11/18100 loss 34.798958 loss_att 53.844170 loss_ctc 38.226246 loss_rnnt 30.341125 hw_loss 0.359659 lr 0.00049797 rank 1
2023-02-20 23:57:04,408 DEBUG TRAIN Batch 11/18100 loss 10.149656 loss_att 12.112963 loss_ctc 14.950114 loss_rnnt 8.936540 hw_loss 0.338240 lr 0.00049797 rank 7
2023-02-20 23:58:22,526 DEBUG TRAIN Batch 11/18200 loss 14.736998 loss_att 24.722460 loss_ctc 27.448652 loss_rnnt 10.982246 hw_loss 0.117695 lr 0.00049785 rank 0
2023-02-20 23:58:22,533 DEBUG TRAIN Batch 11/18200 loss 8.254672 loss_att 14.835049 loss_ctc 8.075596 loss_rnnt 6.859557 hw_loss 0.192968 lr 0.00049785 rank 3
2023-02-20 23:58:22,534 DEBUG TRAIN Batch 11/18200 loss 33.087326 loss_att 36.073727 loss_ctc 33.701550 loss_rnnt 32.315731 hw_loss 0.173281 lr 0.00049785 rank 5
2023-02-20 23:58:22,534 DEBUG TRAIN Batch 11/18200 loss 11.125072 loss_att 13.820789 loss_ctc 13.927050 loss_rnnt 10.062305 hw_loss 0.281301 lr 0.00049785 rank 7
2023-02-20 23:58:22,540 DEBUG TRAIN Batch 11/18200 loss 21.476423 loss_att 22.267365 loss_ctc 26.922445 loss_rnnt 20.439079 hw_loss 0.286913 lr 0.00049785 rank 2
2023-02-20 23:58:22,542 DEBUG TRAIN Batch 11/18200 loss 14.749514 loss_att 20.440495 loss_ctc 18.287979 loss_rnnt 13.000197 hw_loss 0.261232 lr 0.00049785 rank 6
2023-02-20 23:58:22,545 DEBUG TRAIN Batch 11/18200 loss 2.927518 loss_att 5.824915 loss_ctc 1.535985 loss_rnnt 2.258044 hw_loss 0.516624 lr 0.00049785 rank 4
2023-02-20 23:58:22,596 DEBUG TRAIN Batch 11/18200 loss 25.571123 loss_att 32.946308 loss_ctc 27.098892 loss_rnnt 23.785412 hw_loss 0.200575 lr 0.00049785 rank 1
2023-02-20 23:59:23,203 DEBUG TRAIN Batch 11/18300 loss 13.414749 loss_att 23.151272 loss_ctc 20.867212 loss_rnnt 10.244875 hw_loss 0.429204 lr 0.00049773 rank 0
2023-02-20 23:59:23,209 DEBUG TRAIN Batch 11/18300 loss 27.726954 loss_att 25.995823 loss_ctc 34.350773 loss_rnnt 27.188890 hw_loss 0.002086 lr 0.00049773 rank 2
2023-02-20 23:59:23,220 DEBUG TRAIN Batch 11/18300 loss 36.123707 loss_att 46.441639 loss_ctc 39.915974 loss_rnnt 33.336082 hw_loss 0.409497 lr 0.00049773 rank 4
2023-02-20 23:59:23,223 DEBUG TRAIN Batch 11/18300 loss 2.986778 loss_att 7.849302 loss_ctc 5.120315 loss_rnnt 1.539282 hw_loss 0.357226 lr 0.00049773 rank 3
2023-02-20 23:59:23,223 DEBUG TRAIN Batch 11/18300 loss 14.672272 loss_att 21.337059 loss_ctc 16.885981 loss_rnnt 12.943213 hw_loss 0.189266 lr 0.00049773 rank 7
2023-02-20 23:59:23,223 DEBUG TRAIN Batch 11/18300 loss 6.259482 loss_att 10.608573 loss_ctc 7.490305 loss_rnnt 5.137495 hw_loss 0.165111 lr 0.00049773 rank 1
2023-02-20 23:59:23,226 DEBUG TRAIN Batch 11/18300 loss 17.231718 loss_att 20.894352 loss_ctc 21.063343 loss_rnnt 15.883984 hw_loss 0.195606 lr 0.00049773 rank 5
2023-02-20 23:59:23,289 DEBUG TRAIN Batch 11/18300 loss 37.132328 loss_att 45.188145 loss_ctc 49.007607 loss_rnnt 33.811726 hw_loss 0.236380 lr 0.00049773 rank 6
2023-02-21 00:00:21,462 DEBUG TRAIN Batch 11/18400 loss 6.123246 loss_att 12.420565 loss_ctc 9.824847 loss_rnnt 4.185800 hw_loss 0.345816 lr 0.00049760 rank 0
2023-02-21 00:00:21,468 DEBUG TRAIN Batch 11/18400 loss 9.859974 loss_att 10.468976 loss_ctc 12.776056 loss_rnnt 9.249690 hw_loss 0.186883 lr 0.00049760 rank 1
2023-02-21 00:00:21,469 DEBUG TRAIN Batch 11/18400 loss 9.297457 loss_att 20.128397 loss_ctc 11.714518 loss_rnnt 6.730681 hw_loss 0.146836 lr 0.00049760 rank 2
2023-02-21 00:00:21,470 DEBUG TRAIN Batch 11/18400 loss 4.443944 loss_att 6.679417 loss_ctc 2.095282 loss_rnnt 4.156984 hw_loss 0.286912 lr 0.00049760 rank 5
2023-02-21 00:00:21,469 DEBUG TRAIN Batch 11/18400 loss 15.987532 loss_att 22.368607 loss_ctc 22.479996 loss_rnnt 13.745068 hw_loss 0.188599 lr 0.00049760 rank 3
2023-02-21 00:00:21,470 DEBUG TRAIN Batch 11/18400 loss 13.831948 loss_att 17.584555 loss_ctc 17.108242 loss_rnnt 12.542624 hw_loss 0.191182 lr 0.00049760 rank 4
2023-02-21 00:00:21,475 DEBUG TRAIN Batch 11/18400 loss 15.945654 loss_att 25.579523 loss_ctc 18.397806 loss_rnnt 13.455341 hw_loss 0.443598 lr 0.00049760 rank 6
2023-02-21 00:00:21,483 DEBUG TRAIN Batch 11/18400 loss 83.911179 loss_att 100.396828 loss_ctc 110.801430 loss_rnnt 77.028343 hw_loss 0.000643 lr 0.00049760 rank 7
2023-02-21 00:01:20,647 DEBUG TRAIN Batch 11/18500 loss 6.555077 loss_att 9.865189 loss_ctc 2.781429 loss_rnnt 6.092446 hw_loss 0.569552 lr 0.00049748 rank 0
2023-02-21 00:01:20,649 DEBUG TRAIN Batch 11/18500 loss 41.922977 loss_att 45.797104 loss_ctc 45.388844 loss_rnnt 40.376663 hw_loss 0.580068 lr 0.00049748 rank 2
2023-02-21 00:01:20,650 DEBUG TRAIN Batch 11/18500 loss 19.700546 loss_att 31.446882 loss_ctc 19.491293 loss_rnnt 17.274538 hw_loss 0.196199 lr 0.00049748 rank 3
2023-02-21 00:01:20,650 DEBUG TRAIN Batch 11/18500 loss 25.452881 loss_att 27.412985 loss_ctc 28.374960 loss_rnnt 24.462616 hw_loss 0.391185 lr 0.00049748 rank 6
2023-02-21 00:01:20,652 DEBUG TRAIN Batch 11/18500 loss 19.533033 loss_att 25.588409 loss_ctc 20.646027 loss_rnnt 18.035227 hw_loss 0.259370 lr 0.00049748 rank 1
2023-02-21 00:01:20,655 DEBUG TRAIN Batch 11/18500 loss 6.223765 loss_att 13.685458 loss_ctc 7.797593 loss_rnnt 4.315275 hw_loss 0.386829 lr 0.00049748 rank 7
2023-02-21 00:01:20,655 DEBUG TRAIN Batch 11/18500 loss 9.312697 loss_att 12.470432 loss_ctc 14.023708 loss_rnnt 7.921511 hw_loss 0.246569 lr 0.00049748 rank 4
2023-02-21 00:01:20,713 DEBUG TRAIN Batch 11/18500 loss 17.176952 loss_att 21.425419 loss_ctc 15.427358 loss_rnnt 16.415653 hw_loss 0.271661 lr 0.00049748 rank 5
2023-02-21 00:02:21,336 DEBUG TRAIN Batch 11/18600 loss 12.768196 loss_att 13.323734 loss_ctc 12.074586 loss_rnnt 12.749348 hw_loss 0.000418 lr 0.00049736 rank 0
2023-02-21 00:02:21,344 DEBUG TRAIN Batch 11/18600 loss 16.353651 loss_att 26.805328 loss_ctc 25.488148 loss_rnnt 12.941818 hw_loss 0.194184 lr 0.00049736 rank 2
2023-02-21 00:02:21,345 DEBUG TRAIN Batch 11/18600 loss 6.010686 loss_att 8.217880 loss_ctc 10.735148 loss_rnnt 4.538168 hw_loss 0.752157 lr 0.00049736 rank 3
2023-02-21 00:02:21,348 DEBUG TRAIN Batch 11/18600 loss 28.824682 loss_att 38.926132 loss_ctc 28.682203 loss_rnnt 26.745441 hw_loss 0.146148 lr 0.00049736 rank 1
2023-02-21 00:02:21,349 DEBUG TRAIN Batch 11/18600 loss 6.238294 loss_att 12.896417 loss_ctc 4.725761 loss_rnnt 4.936236 hw_loss 0.322694 lr 0.00049736 rank 4
2023-02-21 00:02:21,350 DEBUG TRAIN Batch 11/18600 loss 47.528904 loss_att 40.306370 loss_ctc 52.112305 loss_rnnt 48.249207 hw_loss 0.212034 lr 0.00049736 rank 5
2023-02-21 00:02:21,361 DEBUG TRAIN Batch 11/18600 loss 15.729298 loss_att 16.635290 loss_ctc 15.907485 loss_rnnt 15.362860 hw_loss 0.302777 lr 0.00049736 rank 7
2023-02-21 00:02:21,403 DEBUG TRAIN Batch 11/18600 loss 27.522118 loss_att 48.330719 loss_ctc 30.378288 loss_rnnt 22.979351 hw_loss 0.000418 lr 0.00049736 rank 6
2023-02-21 00:03:19,509 DEBUG TRAIN Batch 11/18700 loss 16.868322 loss_att 15.313933 loss_ctc 19.786686 loss_rnnt 16.674603 hw_loss 0.216531 lr 0.00049724 rank 2
2023-02-21 00:03:19,511 DEBUG TRAIN Batch 11/18700 loss 36.404446 loss_att 37.752560 loss_ctc 35.909191 loss_rnnt 36.200729 hw_loss 0.000240 lr 0.00049724 rank 6
2023-02-21 00:03:19,510 DEBUG TRAIN Batch 11/18700 loss 18.325224 loss_att 17.317852 loss_ctc 21.094378 loss_rnnt 18.073355 hw_loss 0.157731 lr 0.00049724 rank 1
2023-02-21 00:03:19,512 DEBUG TRAIN Batch 11/18700 loss 23.424726 loss_att 24.444103 loss_ctc 30.785912 loss_rnnt 22.084099 hw_loss 0.291114 lr 0.00049724 rank 5
2023-02-21 00:03:19,513 DEBUG TRAIN Batch 11/18700 loss 6.328368 loss_att 10.449267 loss_ctc 8.794266 loss_rnnt 5.026241 hw_loss 0.279676 lr 0.00049724 rank 0
2023-02-21 00:03:19,514 DEBUG TRAIN Batch 11/18700 loss 6.071101 loss_att 12.095804 loss_ctc 10.225595 loss_rnnt 4.142432 hw_loss 0.318367 lr 0.00049724 rank 4
2023-02-21 00:03:19,521 DEBUG TRAIN Batch 11/18700 loss 10.131005 loss_att 11.954210 loss_ctc 12.241670 loss_rnnt 9.287819 hw_loss 0.369605 lr 0.00049724 rank 3
2023-02-21 00:03:19,582 DEBUG TRAIN Batch 11/18700 loss 6.595791 loss_att 17.300852 loss_ctc 10.202068 loss_rnnt 3.785478 hw_loss 0.353369 lr 0.00049724 rank 7
2023-02-21 00:04:19,227 DEBUG TRAIN Batch 11/18800 loss 4.375985 loss_att 5.955568 loss_ctc 2.001713 loss_rnnt 4.028390 hw_loss 0.652965 lr 0.00049711 rank 0
2023-02-21 00:04:19,234 DEBUG TRAIN Batch 11/18800 loss 28.549841 loss_att 34.204899 loss_ctc 35.104393 loss_rnnt 26.450535 hw_loss 0.176910 lr 0.00049711 rank 5
2023-02-21 00:04:19,237 DEBUG TRAIN Batch 11/18800 loss 12.324876 loss_att 17.860844 loss_ctc 15.337631 loss_rnnt 10.694500 hw_loss 0.227777 lr 0.00049711 rank 2
2023-02-21 00:04:19,243 DEBUG TRAIN Batch 11/18800 loss 26.328377 loss_att 32.040115 loss_ctc 30.837944 loss_rnnt 24.387041 hw_loss 0.370713 lr 0.00049711 rank 1
2023-02-21 00:04:19,244 DEBUG TRAIN Batch 11/18800 loss 35.379654 loss_att 37.666748 loss_ctc 44.861088 loss_rnnt 33.491371 hw_loss 0.312507 lr 0.00049711 rank 4
2023-02-21 00:04:19,246 DEBUG TRAIN Batch 11/18800 loss 14.107620 loss_att 20.214928 loss_ctc 17.525072 loss_rnnt 12.304718 hw_loss 0.235840 lr 0.00049711 rank 3
2023-02-21 00:04:19,254 DEBUG TRAIN Batch 11/18800 loss 26.081261 loss_att 29.192123 loss_ctc 30.727768 loss_rnnt 24.652458 hw_loss 0.350804 lr 0.00049711 rank 7
2023-02-21 00:04:19,296 DEBUG TRAIN Batch 11/18800 loss 23.335150 loss_att 29.247261 loss_ctc 24.988356 loss_rnnt 21.932014 hw_loss 0.000537 lr 0.00049711 rank 6
2023-02-21 00:05:20,823 DEBUG TRAIN Batch 11/18900 loss 11.153773 loss_att 13.832890 loss_ctc 8.213701 loss_rnnt 11.009579 hw_loss 0.000714 lr 0.00049699 rank 0
2023-02-21 00:05:20,828 DEBUG TRAIN Batch 11/18900 loss 26.959562 loss_att 24.696615 loss_ctc 22.520935 loss_rnnt 27.717754 hw_loss 0.536650 lr 0.00049699 rank 2
2023-02-21 00:05:20,829 DEBUG TRAIN Batch 11/18900 loss 11.640948 loss_att 16.097744 loss_ctc 12.883602 loss_rnnt 10.305676 hw_loss 0.521671 lr 0.00049699 rank 3
2023-02-21 00:05:20,831 DEBUG TRAIN Batch 11/18900 loss 48.305641 loss_att 59.626156 loss_ctc 63.377796 loss_rnnt 44.031536 hw_loss 0.000714 lr 0.00049699 rank 6
2023-02-21 00:05:20,840 DEBUG TRAIN Batch 11/18900 loss 12.824946 loss_att 18.638138 loss_ctc 17.354134 loss_rnnt 10.756590 hw_loss 0.565924 lr 0.00049699 rank 5
2023-02-21 00:05:20,845 DEBUG TRAIN Batch 11/18900 loss 24.636911 loss_att 26.539127 loss_ctc 25.010620 loss_rnnt 23.911863 hw_loss 0.552706 lr 0.00049699 rank 4
2023-02-21 00:05:20,849 DEBUG TRAIN Batch 11/18900 loss 2.330746 loss_att 10.026917 loss_ctc 1.005689 loss_rnnt 0.742177 hw_loss 0.423767 lr 0.00049699 rank 1
2023-02-21 00:05:20,899 DEBUG TRAIN Batch 11/18900 loss 21.050283 loss_att 26.933525 loss_ctc 23.878967 loss_rnnt 19.362604 hw_loss 0.251007 lr 0.00049699 rank 7
2023-02-21 00:06:40,551 DEBUG TRAIN Batch 11/19000 loss 26.647543 loss_att 23.023903 loss_ctc 26.264328 loss_rnnt 27.155476 hw_loss 0.502298 lr 0.00049687 rank 2
2023-02-21 00:06:40,551 DEBUG TRAIN Batch 11/19000 loss 15.625813 loss_att 25.730492 loss_ctc 19.975161 loss_rnnt 13.024544 hw_loss 0.000790 lr 0.00049687 rank 6
2023-02-21 00:06:40,552 DEBUG TRAIN Batch 11/19000 loss 26.072922 loss_att 32.675140 loss_ctc 30.576042 loss_rnnt 23.927662 hw_loss 0.420745 lr 0.00049687 rank 0
2023-02-21 00:06:40,553 DEBUG TRAIN Batch 11/19000 loss 11.194784 loss_att 14.161522 loss_ctc 13.350031 loss_rnnt 10.105642 hw_loss 0.390801 lr 0.00049687 rank 3
2023-02-21 00:06:40,553 DEBUG TRAIN Batch 11/19000 loss 15.374521 loss_att 20.018133 loss_ctc 20.131840 loss_rnnt 13.604772 hw_loss 0.387598 lr 0.00049687 rank 1
2023-02-21 00:06:40,555 DEBUG TRAIN Batch 11/19000 loss 15.087068 loss_att 14.648667 loss_ctc 18.115614 loss_rnnt 14.601330 hw_loss 0.318021 lr 0.00049687 rank 4
2023-02-21 00:06:40,562 DEBUG TRAIN Batch 11/19000 loss 47.832062 loss_att 56.476273 loss_ctc 52.291199 loss_rnnt 45.253345 hw_loss 0.478726 lr 0.00049687 rank 7
2023-02-21 00:06:40,581 DEBUG TRAIN Batch 11/19000 loss 10.925597 loss_att 12.354422 loss_ctc 9.257096 loss_rnnt 10.861877 hw_loss 0.000790 lr 0.00049687 rank 5
2023-02-21 00:07:56,582 DEBUG TRAIN Batch 11/19100 loss 17.532230 loss_att 22.507778 loss_ctc 22.743578 loss_rnnt 15.727821 hw_loss 0.214603 lr 0.00049674 rank 0
2023-02-21 00:07:56,589 DEBUG TRAIN Batch 11/19100 loss 13.067647 loss_att 17.429438 loss_ctc 11.447070 loss_rnnt 12.324804 hw_loss 0.162305 lr 0.00049674 rank 5
2023-02-21 00:07:56,589 DEBUG TRAIN Batch 11/19100 loss 14.173512 loss_att 19.757748 loss_ctc 16.239334 loss_rnnt 12.567574 hw_loss 0.400589 lr 0.00049674 rank 4
2023-02-21 00:07:56,592 DEBUG TRAIN Batch 11/19100 loss 18.137987 loss_att 24.149660 loss_ctc 21.636204 loss_rnnt 16.367100 hw_loss 0.191479 lr 0.00049674 rank 3
2023-02-21 00:07:56,593 DEBUG TRAIN Batch 11/19100 loss 14.245647 loss_att 18.929140 loss_ctc 17.704964 loss_rnnt 12.633770 hw_loss 0.401130 lr 0.00049674 rank 6
2023-02-21 00:07:56,592 DEBUG TRAIN Batch 11/19100 loss 12.540419 loss_att 16.933273 loss_ctc 17.102173 loss_rnnt 10.961020 hw_loss 0.173614 lr 0.00049674 rank 2
2023-02-21 00:07:56,595 DEBUG TRAIN Batch 11/19100 loss 29.979687 loss_att 40.011650 loss_ctc 40.892326 loss_rnnt 26.349323 hw_loss 0.316781 lr 0.00049674 rank 1
2023-02-21 00:07:56,601 DEBUG TRAIN Batch 11/19100 loss 28.201248 loss_att 27.795979 loss_ctc 34.834064 loss_rnnt 27.397589 hw_loss 0.000636 lr 0.00049674 rank 7
2023-02-21 00:08:55,986 DEBUG TRAIN Batch 11/19200 loss 25.402437 loss_att 36.194412 loss_ctc 29.606386 loss_rnnt 22.682341 hw_loss 0.002210 lr 0.00049662 rank 0
2023-02-21 00:08:55,993 DEBUG TRAIN Batch 11/19200 loss 20.090027 loss_att 20.461721 loss_ctc 19.926998 loss_rnnt 19.931475 hw_loss 0.198656 lr 0.00049662 rank 5
2023-02-21 00:08:55,996 DEBUG TRAIN Batch 11/19200 loss 9.737885 loss_att 24.292381 loss_ctc 8.837009 loss_rnnt 6.724680 hw_loss 0.417040 lr 0.00049662 rank 2
2023-02-21 00:08:55,998 DEBUG TRAIN Batch 11/19200 loss 24.444735 loss_att 23.632505 loss_ctc 28.610044 loss_rnnt 23.985168 hw_loss 0.124945 lr 0.00049662 rank 3
2023-02-21 00:08:56,002 DEBUG TRAIN Batch 11/19200 loss 12.631155 loss_att 14.413028 loss_ctc 9.820872 loss_rnnt 12.595341 hw_loss 0.101521 lr 0.00049662 rank 1
2023-02-21 00:08:56,008 DEBUG TRAIN Batch 11/19200 loss 5.674070 loss_att 11.645933 loss_ctc 8.930706 loss_rnnt 3.918683 hw_loss 0.237742 lr 0.00049662 rank 7
2023-02-21 00:08:56,009 DEBUG TRAIN Batch 11/19200 loss 4.845212 loss_att 5.420932 loss_ctc 6.155034 loss_rnnt 4.381535 hw_loss 0.326044 lr 0.00049662 rank 4
2023-02-21 00:08:56,063 DEBUG TRAIN Batch 11/19200 loss 18.287884 loss_att 15.622230 loss_ctc 19.421467 loss_rnnt 18.608841 hw_loss 0.114433 lr 0.00049662 rank 6
2023-02-21 00:09:54,202 DEBUG TRAIN Batch 11/19300 loss 22.827425 loss_att 26.540937 loss_ctc 27.872068 loss_rnnt 21.239742 hw_loss 0.323177 lr 0.00049650 rank 0
2023-02-21 00:09:54,206 DEBUG TRAIN Batch 11/19300 loss 17.266066 loss_att 18.415810 loss_ctc 18.709549 loss_rnnt 16.666842 hw_loss 0.331522 lr 0.00049650 rank 2
2023-02-21 00:09:54,206 DEBUG TRAIN Batch 11/19300 loss 15.211436 loss_att 16.034468 loss_ctc 16.258261 loss_rnnt 14.694980 hw_loss 0.398014 lr 0.00049650 rank 5
2023-02-21 00:09:54,217 DEBUG TRAIN Batch 11/19300 loss 22.969353 loss_att 31.334574 loss_ctc 32.319874 loss_rnnt 19.772287 hw_loss 0.519912 lr 0.00049650 rank 6
2023-02-21 00:09:54,218 DEBUG TRAIN Batch 11/19300 loss 18.131191 loss_att 20.264767 loss_ctc 24.870090 loss_rnnt 16.611683 hw_loss 0.364261 lr 0.00049650 rank 3
2023-02-21 00:09:54,218 DEBUG TRAIN Batch 11/19300 loss 7.590747 loss_att 7.678400 loss_ctc 8.207500 loss_rnnt 7.299164 hw_loss 0.359658 lr 0.00049650 rank 7
2023-02-21 00:09:54,231 DEBUG TRAIN Batch 11/19300 loss 5.096753 loss_att 10.426733 loss_ctc 6.851425 loss_rnnt 3.662228 hw_loss 0.252323 lr 0.00049650 rank 4
2023-02-21 00:09:54,274 DEBUG TRAIN Batch 11/19300 loss 9.970572 loss_att 12.558915 loss_ctc 12.747175 loss_rnnt 9.019709 hw_loss 0.118089 lr 0.00049650 rank 1
2023-02-21 00:10:54,731 DEBUG TRAIN Batch 11/19400 loss 52.385303 loss_att 69.936203 loss_ctc 73.252930 loss_rnnt 45.842888 hw_loss 0.468527 lr 0.00049638 rank 0
2023-02-21 00:10:54,732 DEBUG TRAIN Batch 11/19400 loss 24.919821 loss_att 28.993355 loss_ctc 20.192804 loss_rnnt 24.734676 hw_loss 0.001332 lr 0.00049638 rank 1
2023-02-21 00:10:54,737 DEBUG TRAIN Batch 11/19400 loss 15.868381 loss_att 22.414017 loss_ctc 19.043510 loss_rnnt 13.892672 hw_loss 0.456057 lr 0.00049638 rank 2
2023-02-21 00:10:54,738 DEBUG TRAIN Batch 11/19400 loss 9.302159 loss_att 8.032017 loss_ctc 7.857173 loss_rnnt 9.636056 hw_loss 0.211494 lr 0.00049638 rank 4
2023-02-21 00:10:54,740 DEBUG TRAIN Batch 11/19400 loss 37.052582 loss_att 37.370216 loss_ctc 35.307335 loss_rnnt 37.075336 hw_loss 0.274532 lr 0.00049638 rank 6
2023-02-21 00:10:54,739 DEBUG TRAIN Batch 11/19400 loss 5.394261 loss_att 9.140641 loss_ctc 8.283919 loss_rnnt 4.197457 hw_loss 0.116700 lr 0.00049638 rank 3
2023-02-21 00:10:54,740 DEBUG TRAIN Batch 11/19400 loss 19.004866 loss_att 26.732328 loss_ctc 23.216494 loss_rnnt 16.740559 hw_loss 0.294873 lr 0.00049638 rank 5
2023-02-21 00:10:54,803 DEBUG TRAIN Batch 11/19400 loss 24.045828 loss_att 29.906124 loss_ctc 27.908962 loss_rnnt 22.114819 hw_loss 0.457252 lr 0.00049638 rank 7
2023-02-21 00:11:54,750 DEBUG TRAIN Batch 11/19500 loss 14.385317 loss_att 15.969352 loss_ctc 16.373142 loss_rnnt 13.588030 hw_loss 0.403944 lr 0.00049625 rank 0
2023-02-21 00:11:54,754 DEBUG TRAIN Batch 11/19500 loss 26.837011 loss_att 28.964203 loss_ctc 27.099016 loss_rnnt 26.376530 hw_loss 0.000201 lr 0.00049625 rank 5
2023-02-21 00:11:54,762 DEBUG TRAIN Batch 11/19500 loss 9.457843 loss_att 10.746607 loss_ctc 12.670813 loss_rnnt 8.615635 hw_loss 0.292611 lr 0.00049625 rank 4
2023-02-21 00:11:54,763 DEBUG TRAIN Batch 11/19500 loss 29.063665 loss_att 44.691780 loss_ctc 53.144955 loss_rnnt 22.727097 hw_loss 0.000201 lr 0.00049625 rank 2
2023-02-21 00:11:54,765 DEBUG TRAIN Batch 11/19500 loss 29.171005 loss_att 32.225929 loss_ctc 29.627583 loss_rnnt 28.404150 hw_loss 0.178112 lr 0.00049625 rank 1
2023-02-21 00:11:54,768 DEBUG TRAIN Batch 11/19500 loss 18.275980 loss_att 24.894096 loss_ctc 15.073750 loss_rnnt 17.135979 hw_loss 0.456268 lr 0.00049625 rank 3
2023-02-21 00:11:54,772 DEBUG TRAIN Batch 11/19500 loss 16.695951 loss_att 22.293016 loss_ctc 19.214785 loss_rnnt 15.077021 hw_loss 0.306889 lr 0.00049625 rank 7
2023-02-21 00:11:54,818 DEBUG TRAIN Batch 11/19500 loss 22.583622 loss_att 32.122139 loss_ctc 26.370316 loss_rnnt 20.066128 hw_loss 0.196686 lr 0.00049625 rank 6
2023-02-21 00:12:53,649 DEBUG TRAIN Batch 11/19600 loss 18.086658 loss_att 24.911457 loss_ctc 19.283852 loss_rnnt 16.561966 hw_loss 0.000199 lr 0.00049613 rank 0
2023-02-21 00:12:53,654 DEBUG TRAIN Batch 11/19600 loss 15.215388 loss_att 21.231819 loss_ctc 19.765026 loss_rnnt 13.208376 hw_loss 0.369575 lr 0.00049613 rank 2
2023-02-21 00:12:53,661 DEBUG TRAIN Batch 11/19600 loss 40.719315 loss_att 55.223320 loss_ctc 44.078381 loss_rnnt 37.187050 hw_loss 0.344215 lr 0.00049613 rank 4
2023-02-21 00:12:53,662 DEBUG TRAIN Batch 11/19600 loss 11.384135 loss_att 14.892749 loss_ctc 17.619352 loss_rnnt 9.655466 hw_loss 0.366721 lr 0.00049613 rank 5
2023-02-21 00:12:53,664 DEBUG TRAIN Batch 11/19600 loss 11.829930 loss_att 13.801819 loss_ctc 13.544653 loss_rnnt 11.014330 hw_loss 0.361111 lr 0.00049613 rank 3
2023-02-21 00:12:53,667 DEBUG TRAIN Batch 11/19600 loss 12.405758 loss_att 14.129737 loss_ctc 13.529881 loss_rnnt 11.742896 hw_loss 0.315341 lr 0.00049613 rank 6
2023-02-21 00:12:53,668 DEBUG TRAIN Batch 11/19600 loss 18.731052 loss_att 17.625099 loss_ctc 23.639294 loss_rnnt 18.125998 hw_loss 0.322153 lr 0.00049613 rank 7
2023-02-21 00:12:53,720 DEBUG TRAIN Batch 11/19600 loss 15.228764 loss_att 20.139133 loss_ctc 19.029627 loss_rnnt 13.603361 hw_loss 0.256027 lr 0.00049613 rank 1
2023-02-21 00:13:53,998 DEBUG TRAIN Batch 11/19700 loss 3.326218 loss_att 9.421102 loss_ctc 4.290144 loss_rnnt 1.796191 hw_loss 0.342237 lr 0.00049601 rank 0
2023-02-21 00:13:53,999 DEBUG TRAIN Batch 11/19700 loss 2.635797 loss_att 9.044971 loss_ctc 3.078499 loss_rnnt 1.294856 hw_loss 0.000147 lr 0.00049601 rank 1
2023-02-21 00:13:54,004 DEBUG TRAIN Batch 11/19700 loss 14.779253 loss_att 21.170185 loss_ctc 22.492575 loss_rnnt 12.384274 hw_loss 0.165653 lr 0.00049601 rank 2
2023-02-21 00:13:54,004 DEBUG TRAIN Batch 11/19700 loss 15.479105 loss_att 22.812752 loss_ctc 16.631765 loss_rnnt 13.672895 hw_loss 0.348359 lr 0.00049601 rank 3
2023-02-21 00:13:54,013 DEBUG TRAIN Batch 11/19700 loss 36.735168 loss_att 39.402275 loss_ctc 38.303749 loss_rnnt 35.816597 hw_loss 0.330014 lr 0.00049601 rank 5
2023-02-21 00:13:54,014 DEBUG TRAIN Batch 11/19700 loss 11.885012 loss_att 14.763887 loss_ctc 13.674610 loss_rnnt 11.012335 hw_loss 0.109291 lr 0.00049601 rank 7
2023-02-21 00:13:54,014 DEBUG TRAIN Batch 11/19700 loss 26.929838 loss_att 38.215065 loss_ctc 33.553806 loss_rnnt 23.570042 hw_loss 0.411661 lr 0.00049601 rank 4
2023-02-21 00:13:54,067 DEBUG TRAIN Batch 11/19700 loss 20.820354 loss_att 23.231335 loss_ctc 23.164894 loss_rnnt 19.923347 hw_loss 0.191630 lr 0.00049601 rank 6
2023-02-21 00:14:53,956 DEBUG TRAIN Batch 11/19800 loss 15.480741 loss_att 16.649160 loss_ctc 16.500141 loss_rnnt 14.820675 hw_loss 0.544616 lr 0.00049589 rank 0
2023-02-21 00:14:53,958 DEBUG TRAIN Batch 11/19800 loss 5.916133 loss_att 9.568285 loss_ctc 5.865884 loss_rnnt 5.066703 hw_loss 0.235686 lr 0.00049589 rank 5
2023-02-21 00:14:53,958 DEBUG TRAIN Batch 11/19800 loss 37.320370 loss_att 43.291672 loss_ctc 40.224445 loss_rnnt 35.577934 hw_loss 0.301807 lr 0.00049589 rank 2
2023-02-21 00:14:53,960 DEBUG TRAIN Batch 11/19800 loss 22.045294 loss_att 26.353676 loss_ctc 28.816341 loss_rnnt 20.156685 hw_loss 0.232738 lr 0.00049589 rank 4
2023-02-21 00:14:53,962 DEBUG TRAIN Batch 11/19800 loss 5.061509 loss_att 11.969459 loss_ctc 8.661360 loss_rnnt 3.197493 hw_loss 0.004586 lr 0.00049589 rank 3
2023-02-21 00:14:53,963 DEBUG TRAIN Batch 11/19800 loss 5.415163 loss_att 10.690456 loss_ctc 4.266684 loss_rnnt 4.294383 hw_loss 0.410347 lr 0.00049589 rank 6
2023-02-21 00:14:53,966 DEBUG TRAIN Batch 11/19800 loss 17.229021 loss_att 21.518246 loss_ctc 16.854872 loss_rnnt 16.137650 hw_loss 0.531402 lr 0.00049589 rank 7
2023-02-21 00:14:54,022 DEBUG TRAIN Batch 11/19800 loss 7.751526 loss_att 10.001404 loss_ctc 12.481090 loss_rnnt 6.202375 hw_loss 0.878564 lr 0.00049589 rank 1
2023-02-21 00:15:53,140 DEBUG TRAIN Batch 11/19900 loss 8.089550 loss_att 8.631430 loss_ctc 7.750223 loss_rnnt 7.857474 hw_loss 0.316768 lr 0.00049577 rank 0
2023-02-21 00:15:53,142 DEBUG TRAIN Batch 11/19900 loss 9.275315 loss_att 14.593315 loss_ctc 10.561357 loss_rnnt 7.773612 hw_loss 0.499934 lr 0.00049577 rank 5
2023-02-21 00:15:53,144 DEBUG TRAIN Batch 11/19900 loss 3.554374 loss_att 7.266336 loss_ctc 3.954647 loss_rnnt 2.525853 hw_loss 0.436421 lr 0.00049577 rank 3
2023-02-21 00:15:53,147 DEBUG TRAIN Batch 11/19900 loss 5.369026 loss_att 9.235018 loss_ctc 3.953889 loss_rnnt 4.628502 hw_loss 0.292518 lr 0.00049577 rank 1
2023-02-21 00:15:53,147 DEBUG TRAIN Batch 11/19900 loss 18.345800 loss_att 19.257206 loss_ctc 22.220909 loss_rnnt 17.397459 hw_loss 0.467586 lr 0.00049577 rank 7
2023-02-21 00:15:53,149 DEBUG TRAIN Batch 11/19900 loss 45.170959 loss_att 58.577972 loss_ctc 51.193779 loss_rnnt 41.594772 hw_loss 0.172013 lr 0.00049577 rank 4
2023-02-21 00:15:53,190 DEBUG TRAIN Batch 11/19900 loss 16.263638 loss_att 15.072172 loss_ctc 14.410026 loss_rnnt 16.588551 hw_loss 0.300989 lr 0.00049577 rank 6
2023-02-21 00:15:53,201 DEBUG TRAIN Batch 11/19900 loss 20.347502 loss_att 27.482965 loss_ctc 31.348722 loss_rnnt 17.400072 hw_loss 0.100324 lr 0.00049577 rank 2
2023-02-21 00:16:53,931 DEBUG TRAIN Batch 11/20000 loss 38.909332 loss_att 40.661366 loss_ctc 51.780098 loss_rnnt 36.793144 hw_loss 0.093147 lr 0.00049565 rank 2
2023-02-21 00:16:53,940 DEBUG TRAIN Batch 11/20000 loss 15.450215 loss_att 12.117119 loss_ctc 15.188944 loss_rnnt 16.151247 hw_loss 0.000793 lr 0.00049565 rank 0
2023-02-21 00:16:53,949 DEBUG TRAIN Batch 11/20000 loss 47.938980 loss_att 50.926476 loss_ctc 56.989536 loss_rnnt 45.990623 hw_loss 0.270223 lr 0.00049565 rank 1
2023-02-21 00:16:53,949 DEBUG TRAIN Batch 11/20000 loss 24.028931 loss_att 33.103279 loss_ctc 33.508564 loss_rnnt 20.850948 hw_loss 0.185926 lr 0.00049565 rank 3
2023-02-21 00:16:53,950 DEBUG TRAIN Batch 11/20000 loss 4.679508 loss_att 7.811177 loss_ctc 8.319799 loss_rnnt 3.516136 hw_loss 0.096875 lr 0.00049565 rank 5
2023-02-21 00:16:53,951 DEBUG TRAIN Batch 11/20000 loss 16.476959 loss_att 23.087566 loss_ctc 17.407909 loss_rnnt 14.927027 hw_loss 0.194405 lr 0.00049565 rank 4
2023-02-21 00:16:53,958 DEBUG TRAIN Batch 11/20000 loss 18.985586 loss_att 25.683969 loss_ctc 23.574703 loss_rnnt 16.828384 hw_loss 0.385579 lr 0.00049565 rank 7
2023-02-21 00:16:54,010 DEBUG TRAIN Batch 11/20000 loss 15.286073 loss_att 17.746094 loss_ctc 18.092089 loss_rnnt 14.352753 hw_loss 0.125963 lr 0.00049565 rank 6
2023-02-21 00:17:51,882 DEBUG TRAIN Batch 11/20100 loss 13.117002 loss_att 21.008163 loss_ctc 18.067097 loss_rnnt 10.820117 hw_loss 0.109954 lr 0.00049552 rank 0
2023-02-21 00:17:51,886 DEBUG TRAIN Batch 11/20100 loss 26.508327 loss_att 43.946747 loss_ctc 31.454416 loss_rnnt 22.360735 hw_loss 0.000806 lr 0.00049552 rank 5
2023-02-21 00:17:51,889 DEBUG TRAIN Batch 11/20100 loss 47.224201 loss_att 37.730911 loss_ctc 33.965496 loss_rnnt 50.773052 hw_loss 0.220570 lr 0.00049552 rank 3
2023-02-21 00:17:51,891 DEBUG TRAIN Batch 11/20100 loss 1.362749 loss_att 4.931550 loss_ctc 0.712629 loss_rnnt 0.735242 hw_loss 0.000806 lr 0.00049552 rank 2
2023-02-21 00:17:51,893 DEBUG TRAIN Batch 11/20100 loss 9.982280 loss_att 17.878864 loss_ctc 11.876297 loss_rnnt 7.991713 hw_loss 0.297590 lr 0.00049552 rank 4
2023-02-21 00:17:51,893 DEBUG TRAIN Batch 11/20100 loss 14.052368 loss_att 17.237518 loss_ctc 17.078310 loss_rnnt 12.927097 hw_loss 0.158966 lr 0.00049552 rank 6
2023-02-21 00:17:51,894 DEBUG TRAIN Batch 11/20100 loss 15.516496 loss_att 16.500851 loss_ctc 24.755558 loss_rnnt 13.915611 hw_loss 0.322759 lr 0.00049552 rank 7
2023-02-21 00:17:51,944 DEBUG TRAIN Batch 11/20100 loss 29.426640 loss_att 37.736710 loss_ctc 34.471161 loss_rnnt 26.913124 hw_loss 0.335429 lr 0.00049552 rank 1
2023-02-21 00:18:50,784 DEBUG TRAIN Batch 11/20200 loss 5.569905 loss_att 9.623837 loss_ctc 6.649699 loss_rnnt 4.614848 hw_loss 0.000558 lr 0.00049540 rank 2
2023-02-21 00:18:50,787 DEBUG TRAIN Batch 11/20200 loss 41.433628 loss_att 43.702175 loss_ctc 41.720203 loss_rnnt 40.788151 hw_loss 0.287927 lr 0.00049540 rank 6
2023-02-21 00:18:50,788 DEBUG TRAIN Batch 11/20200 loss 7.693382 loss_att 11.051254 loss_ctc 8.977844 loss_rnnt 6.583076 hw_loss 0.501506 lr 0.00049540 rank 3
2023-02-21 00:18:50,788 DEBUG TRAIN Batch 11/20200 loss 14.891740 loss_att 17.156168 loss_ctc 25.119532 loss_rnnt 12.905187 hw_loss 0.318678 lr 0.00049540 rank 5
2023-02-21 00:18:50,792 DEBUG TRAIN Batch 11/20200 loss 24.261522 loss_att 29.322800 loss_ctc 26.672665 loss_rnnt 22.699457 hw_loss 0.428102 lr 0.00049540 rank 7
2023-02-21 00:19:40,909 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 1
2023-02-21 00:19:40,911 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 6
2023-02-21 00:19:40,911 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 0
2023-02-21 00:19:40,914 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 5
2023-02-21 00:19:40,916 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 7
2023-02-21 00:19:40,921 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 4
2023-02-21 00:19:40,936 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 3
2023-02-21 00:19:40,946 DEBUG CV Batch 11/0 loss 3.747281 loss_att 3.606697 loss_ctc 4.041034 loss_rnnt 3.455653 hw_loss 0.526084 history loss 3.497462 rank 2
2023-02-21 00:19:51,201 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 0
2023-02-21 00:19:51,399 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 2
2023-02-21 00:19:51,484 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 3
2023-02-21 00:19:51,495 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 6
2023-02-21 00:19:51,676 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 5
2023-02-21 00:19:51,812 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 1
2023-02-21 00:19:51,978 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 7
2023-02-21 00:19:52,013 DEBUG CV Batch 11/100 loss 11.579613 loss_att 16.471069 loss_ctc 16.562510 loss_rnnt 9.785554 hw_loss 0.283839 history loss 5.990740 rank 4
2023-02-21 00:20:03,672 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 2
2023-02-21 00:20:03,740 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 0
2023-02-21 00:20:04,054 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 6
2023-02-21 00:20:04,156 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 3
2023-02-21 00:20:04,354 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 5
2023-02-21 00:20:04,790 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 7
2023-02-21 00:20:04,799 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 1
2023-02-21 00:20:05,219 DEBUG CV Batch 11/200 loss 3.456083 loss_att 4.349604 loss_ctc 2.621175 loss_rnnt 3.173932 hw_loss 0.402689 history loss 5.645752 rank 4
2023-02-21 00:20:17,065 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 2
2023-02-21 00:20:17,404 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 0
2023-02-21 00:20:17,538 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 3
2023-02-21 00:20:17,929 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 6
2023-02-21 00:20:18,754 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 5
2023-02-21 00:20:18,786 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 1
2023-02-21 00:20:18,968 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 7
2023-02-21 00:20:19,205 DEBUG CV Batch 11/300 loss 3.647428 loss_att 3.841519 loss_ctc 5.129145 loss_rnnt 3.250619 hw_loss 0.300802 history loss 6.330253 rank 4
2023-02-21 00:20:27,403 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 2
2023-02-21 00:20:27,730 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 0
2023-02-21 00:20:27,948 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 3
2023-02-21 00:20:28,542 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 6
2023-02-21 00:20:29,660 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 1
2023-02-21 00:20:30,159 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 4
2023-02-21 00:20:30,260 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 7
2023-02-21 00:20:30,304 DEBUG CV Batch 11/400 loss 23.944633 loss_att 21.110893 loss_ctc 23.019283 loss_rnnt 24.428457 hw_loss 0.386826 history loss 6.348251 rank 5
2023-02-21 00:20:42,889 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 2
2023-02-21 00:20:42,999 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 0
2023-02-21 00:20:43,412 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 3
2023-02-21 00:20:44,070 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 6
2023-02-21 00:20:45,525 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 1
2023-02-21 00:20:45,985 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 4
2023-02-21 00:20:46,059 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 7
2023-02-21 00:20:47,785 DEBUG CV Batch 11/500 loss 5.036909 loss_att 7.702845 loss_ctc 6.392206 loss_rnnt 4.112114 hw_loss 0.395440 history loss 6.467073 rank 5
2023-02-21 00:20:55,534 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 2
2023-02-21 00:20:55,838 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 0
2023-02-21 00:20:56,366 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 3
2023-02-21 00:20:57,133 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 6
2023-02-21 00:20:58,971 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 1
2023-02-21 00:20:59,302 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 4
2023-02-21 00:20:59,341 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 7
2023-02-21 00:21:01,233 DEBUG CV Batch 11/600 loss 13.432467 loss_att 40.461174 loss_ctc 8.702996 loss_rnnt 8.575512 hw_loss 0.153395 history loss 6.774423 rank 5
2023-02-21 00:21:07,099 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 0
2023-02-21 00:21:07,109 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 2
2023-02-21 00:21:07,789 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 3
2023-02-21 00:21:08,640 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 6
2023-02-21 00:21:10,910 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 1
2023-02-21 00:21:11,136 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 7
2023-02-21 00:21:11,158 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 4
2023-02-21 00:21:13,057 DEBUG CV Batch 11/700 loss 27.250471 loss_att 31.927050 loss_ctc 36.858395 loss_rnnt 25.003325 hw_loss 0.057700 history loss 6.924025 rank 5
2023-02-21 00:21:18,228 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 2
2023-02-21 00:21:18,426 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 0
2023-02-21 00:21:19,109 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 3
2023-02-21 00:21:21,341 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 6
2023-02-21 00:21:22,562 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 1
2023-02-21 00:21:22,698 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 7
2023-02-21 00:21:22,883 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 4
2023-02-21 00:21:24,867 DEBUG CV Batch 11/800 loss 7.922338 loss_att 6.852408 loss_ctc 7.097843 loss_rnnt 8.058932 hw_loss 0.351233 history loss 7.359375 rank 5
2023-02-21 00:21:30,554 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 2
2023-02-21 00:21:30,615 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 0
2023-02-21 00:21:31,425 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 3
2023-02-21 00:21:34,133 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 6
2023-02-21 00:21:35,300 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 1
2023-02-21 00:21:35,542 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 7
2023-02-21 00:21:36,085 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 4
2023-02-21 00:21:37,449 DEBUG CV Batch 11/900 loss 16.670706 loss_att 14.487156 loss_ctc 19.353922 loss_rnnt 16.500614 hw_loss 0.466949 history loss 8.254669 rank 5
2023-02-21 00:21:41,434 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 2
2023-02-21 00:21:41,583 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 0
2023-02-21 00:21:42,098 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 3
2023-02-21 00:21:45,312 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 6
2023-02-21 00:21:46,719 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 1
2023-02-21 00:21:47,088 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 7
2023-02-21 00:21:47,647 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 4
2023-02-21 00:21:48,694 DEBUG CV Batch 11/1000 loss 73.852051 loss_att 59.640614 loss_ctc 85.827332 loss_rnnt 75.097313 hw_loss 0.000613 history loss 9.329407 rank 5
2023-02-21 00:21:52,512 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 2
2023-02-21 00:21:52,642 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 0
2023-02-21 00:21:52,923 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 3
2023-02-21 00:21:56,427 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 6
2023-02-21 00:21:58,557 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 7
2023-02-21 00:21:59,105 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 1
2023-02-21 00:21:59,132 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 4
2023-02-21 00:22:00,203 DEBUG CV Batch 11/1100 loss 1.656283 loss_att 4.536643 loss_ctc 0.570703 loss_rnnt 0.986894 hw_loss 0.446366 history loss 9.288764 rank 5
2023-02-21 00:22:04,423 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 2
2023-02-21 00:22:04,628 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 0
2023-02-21 00:22:05,034 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 3
2023-02-21 00:22:08,543 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 6
2023-02-21 00:22:11,144 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 7
2023-02-21 00:22:11,198 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 1
2023-02-21 00:22:11,277 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 4
2023-02-21 00:22:12,522 DEBUG CV Batch 11/1200 loss 14.657043 loss_att 13.280396 loss_ctc 13.766575 loss_rnnt 14.855465 hw_loss 0.366820 history loss 9.707075 rank 5
2023-02-21 00:22:17,380 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 2
2023-02-21 00:22:17,485 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 0
2023-02-21 00:22:17,988 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 3
2023-02-21 00:22:21,713 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 6
2023-02-21 00:22:24,378 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 1
2023-02-21 00:22:24,498 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 7
2023-02-21 00:22:25,486 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 4
2023-02-21 00:22:26,036 DEBUG CV Batch 11/1300 loss 7.688432 loss_att 6.430123 loss_ctc 8.405921 loss_rnnt 7.640780 hw_loss 0.381840 history loss 10.450957 rank 5
2023-02-21 00:22:28,120 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 0
2023-02-21 00:22:28,128 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 2
2023-02-21 00:22:28,867 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 3
2023-02-21 00:22:32,735 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 6
2023-02-21 00:22:35,415 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 1
2023-02-21 00:22:36,069 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 7
2023-02-21 00:22:36,724 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 4
2023-02-21 00:22:37,048 DEBUG CV Batch 11/1400 loss 186.227264 loss_att 213.253540 loss_ctc 226.892838 loss_rnnt 175.288391 hw_loss 0.209108 history loss 11.146023 rank 5
2023-02-21 00:22:38,040 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 2
2023-02-21 00:22:38,267 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 0
2023-02-21 00:22:39,024 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 3
2023-02-21 00:22:42,746 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 6
2023-02-21 00:22:45,631 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 1
2023-02-21 00:22:46,471 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 7
2023-02-21 00:22:47,365 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 4
2023-02-21 00:22:47,381 DEBUG CV Batch 11/1500 loss 3.665688 loss_att 8.037927 loss_ctc 6.074099 loss_rnnt 2.305080 hw_loss 0.309449 history loss 11.384299 rank 5
2023-02-21 00:22:49,957 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 2
2023-02-21 00:22:50,319 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 0
2023-02-21 00:22:51,201 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 3
2023-02-21 00:22:54,888 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 6
2023-02-21 00:22:57,842 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 1
2023-02-21 00:22:59,341 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 7
2023-02-21 00:22:59,832 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 4
2023-02-21 00:23:00,881 DEBUG CV Batch 11/1600 loss 18.439060 loss_att 18.470995 loss_ctc 22.932837 loss_rnnt 17.689312 hw_loss 0.270357 history loss 11.066467 rank 5
2023-02-21 00:23:02,910 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 2
2023-02-21 00:23:03,191 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 0
2023-02-21 00:23:04,111 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 3
2023-02-21 00:23:08,025 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 6
2023-02-21 00:23:10,904 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 1
2023-02-21 00:23:12,672 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 7
2023-02-21 00:23:13,222 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 4
2023-02-21 00:23:14,055 DEBUG CV Batch 11/1700 loss 7.864838 loss_att 7.533423 loss_ctc 7.113061 loss_rnnt 7.804550 hw_loss 0.425265 history loss 10.795989 rank 5
2023-02-21 00:23:14,449 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 2
2023-02-21 00:23:14,888 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 0
2023-02-21 00:23:15,822 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 3
2023-02-21 00:23:19,621 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 6
2023-02-21 00:23:22,807 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 1
2023-02-21 00:23:24,969 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 7
2023-02-21 00:23:25,337 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 4
2023-02-21 00:23:25,722 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 2
2023-02-21 00:23:26,067 DEBUG CV Batch 11/1800 loss 26.820967 loss_att 31.435139 loss_ctc 37.609295 loss_rnnt 24.459360 hw_loss 0.000613 history loss 10.715502 rank 5
2023-02-21 00:23:26,361 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 0
2023-02-21 00:23:27,174 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 3
2023-02-21 00:23:30,955 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 6
2023-02-21 00:23:34,112 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 1
2023-02-21 00:23:36,776 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 7
2023-02-21 00:23:36,970 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 4
2023-02-21 00:23:37,852 DEBUG CV Batch 11/1900 loss 5.789302 loss_att 5.566697 loss_ctc 6.005568 loss_rnnt 5.639392 hw_loss 0.310492 history loss 10.378159 rank 5
2023-02-21 00:23:41,572 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 2
2023-02-21 00:23:42,230 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 0
2023-02-21 00:23:43,008 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 3
2023-02-21 00:23:46,884 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 6
2023-02-21 00:23:50,256 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 1
2023-02-21 00:23:52,405 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 2
2023-02-21 00:23:53,025 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 7
2023-02-21 00:23:53,145 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 0
2023-02-21 00:23:53,670 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 3
2023-02-21 00:23:53,938 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 5
2023-02-21 00:23:53,983 DEBUG CV Batch 11/2000 loss 9.028506 loss_att 8.535667 loss_ctc 8.567440 loss_rnnt 8.989725 hw_loss 0.372795 history loss 10.264140 rank 4
2023-02-21 00:23:57,750 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 6
2023-02-21 00:24:01,191 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 1
2023-02-21 00:24:04,167 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 7
2023-02-21 00:24:05,118 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 5
2023-02-21 00:24:05,489 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 2
2023-02-21 00:24:06,258 DEBUG CV Batch 11/2100 loss 10.991920 loss_att 19.066809 loss_ctc 13.254536 loss_rnnt 8.900024 hw_loss 0.328567 history loss 10.209278 rank 4
2023-02-21 00:24:06,352 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 0
2023-02-21 00:24:06,791 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 3
2023-02-21 00:24:10,851 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 6
2023-02-21 00:24:14,486 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 1
2023-02-21 00:24:17,156 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 2
2023-02-21 00:24:17,778 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 7
2023-02-21 00:24:18,074 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 0
2023-02-21 00:24:18,150 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 3
2023-02-21 00:24:18,476 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 5
2023-02-21 00:24:20,458 DEBUG CV Batch 11/2200 loss 5.487373 loss_att 10.158175 loss_ctc 6.406285 loss_rnnt 4.045720 hw_loss 0.721821 history loss 10.066999 rank 4
2023-02-21 00:24:22,715 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 6
2023-02-21 00:24:27,319 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 1
2023-02-21 00:24:29,421 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 2
2023-02-21 00:24:29,857 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 7
2023-02-21 00:24:30,190 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 3
2023-02-21 00:24:30,238 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 0
2023-02-21 00:24:30,540 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 5
2023-02-21 00:24:32,479 DEBUG CV Batch 11/2300 loss 3.299160 loss_att 3.548112 loss_ctc 3.263603 loss_rnnt 3.014865 hw_loss 0.448585 history loss 9.966481 rank 4
2023-02-21 00:24:34,938 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 6
2023-02-21 00:24:39,579 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 2
2023-02-21 00:24:40,040 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 1
2023-02-21 00:24:40,109 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 3
2023-02-21 00:24:40,468 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 0
2023-02-21 00:24:42,660 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 7
2023-02-21 00:24:42,957 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 5
2023-02-21 00:24:44,963 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 6
2023-02-21 00:24:45,161 DEBUG CV Batch 11/2400 loss 8.016654 loss_att 7.259813 loss_ctc 10.660432 loss_rnnt 7.509109 hw_loss 0.574518 history loss 10.057940 rank 4
2023-02-21 00:24:50,299 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 1
2023-02-21 00:24:50,828 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 2
2023-02-21 00:24:51,268 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 3
2023-02-21 00:24:51,811 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 0
2023-02-21 00:24:53,059 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 7
2023-02-21 00:24:53,277 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 5
2023-02-21 00:24:55,833 DEBUG CV Batch 11/2500 loss 30.093302 loss_att 34.124657 loss_ctc 41.390690 loss_rnnt 27.780384 hw_loss 0.000613 history loss 10.475905 rank 4
2023-02-21 00:24:57,051 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 6
2023-02-21 00:25:01,893 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 1
2023-02-21 00:25:03,464 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 2
2023-02-21 00:25:03,902 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 3
2023-02-21 00:25:04,493 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 0
2023-02-21 00:25:04,554 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 7
2023-02-21 00:25:04,648 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 5
2023-02-21 00:25:07,731 DEBUG CV Batch 11/2600 loss 6.214333 loss_att 7.467640 loss_ctc 7.688243 loss_rnnt 5.515302 hw_loss 0.472215 history loss 10.532955 rank 4
2023-02-21 00:25:09,740 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 6
2023-02-21 00:25:14,088 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 2
2023-02-21 00:25:14,377 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 3
2023-02-21 00:25:14,925 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 1
2023-02-21 00:25:15,065 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 0
2023-02-21 00:25:17,583 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 7
2023-02-21 00:25:18,187 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 5
2023-02-21 00:25:20,032 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 6
2023-02-21 00:25:21,354 DEBUG CV Batch 11/2700 loss 21.285158 loss_att 15.907495 loss_ctc 26.705439 loss_rnnt 21.396978 hw_loss 0.451895 history loss 10.592353 rank 4
2023-02-21 00:25:25,386 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 2
2023-02-21 00:25:25,712 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 3
2023-02-21 00:25:25,957 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 1
2023-02-21 00:25:26,205 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 0
2023-02-21 00:25:28,544 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 7
2023-02-21 00:25:28,907 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 5
2023-02-21 00:25:31,034 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 6
2023-02-21 00:25:32,264 DEBUG CV Batch 11/2800 loss 23.692085 loss_att 20.558187 loss_ctc 25.441618 loss_rnnt 23.929680 hw_loss 0.292339 history loss 10.866240 rank 4
2023-02-21 00:25:36,500 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 2
2023-02-21 00:25:36,968 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 3
2023-02-21 00:25:37,494 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 0
2023-02-21 00:25:37,552 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 1
2023-02-21 00:25:40,376 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 5
2023-02-21 00:25:40,572 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 7
2023-02-21 00:25:42,531 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 6
2023-02-21 00:25:44,211 DEBUG CV Batch 11/2900 loss 40.819523 loss_att 33.489937 loss_ctc 50.426193 loss_rnnt 40.902565 hw_loss 0.191222 history loss 11.134619 rank 4
2023-02-21 00:25:48,849 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 2
2023-02-21 00:25:49,048 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 1
2023-02-21 00:25:49,242 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 3
2023-02-21 00:25:49,754 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 0
2023-02-21 00:25:51,836 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 5
2023-02-21 00:25:52,153 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 7
2023-02-21 00:25:55,002 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 6
2023-02-21 00:25:55,797 DEBUG CV Batch 11/3000 loss 6.385027 loss_att 6.522929 loss_ctc 6.403992 loss_rnnt 6.133319 hw_loss 0.415500 history loss 11.295092 rank 4
2023-02-21 00:26:01,577 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 2
2023-02-21 00:26:01,744 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 3
2023-02-21 00:26:02,008 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 1
2023-02-21 00:26:02,407 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 0
2023-02-21 00:26:04,294 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 5
2023-02-21 00:26:04,843 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 7
2023-02-21 00:26:07,852 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 6
2023-02-21 00:26:08,679 DEBUG CV Batch 11/3100 loss 8.451079 loss_att 9.639409 loss_ctc 14.686438 loss_rnnt 7.145042 hw_loss 0.444357 history loss 11.285321 rank 4
2023-02-21 00:26:12,178 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 2
2023-02-21 00:26:12,182 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 3
2023-02-21 00:26:13,021 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 0
2023-02-21 00:26:15,342 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 1
2023-02-21 00:26:17,097 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 5
2023-02-21 00:26:18,021 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 7
2023-02-21 00:26:18,425 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 6
2023-02-21 00:26:21,730 DEBUG CV Batch 11/3200 loss 1.720122 loss_att 2.074410 loss_ctc 1.052885 loss_rnnt 1.492535 hw_loss 0.460677 history loss 11.224984 rank 4
2023-02-21 00:26:24,651 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 2
2023-02-21 00:26:24,766 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 3
2023-02-21 00:26:25,706 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 0
2023-02-21 00:26:26,044 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 1
2023-02-21 00:26:27,646 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 5
2023-02-21 00:26:29,131 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 7
2023-02-21 00:26:31,257 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 6
2023-02-21 00:26:32,828 DEBUG CV Batch 11/3300 loss 11.974298 loss_att 16.800003 loss_ctc 8.422156 loss_rnnt 11.402235 hw_loss 0.151015 history loss 11.138875 rank 4
2023-02-21 00:26:39,162 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 1
2023-02-21 00:26:40,266 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 2
2023-02-21 00:26:40,575 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 3
2023-02-21 00:26:40,594 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 5
2023-02-21 00:26:41,285 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 0
2023-02-21 00:26:42,160 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 7
2023-02-21 00:26:46,020 DEBUG CV Batch 11/3400 loss 12.674649 loss_att 11.047585 loss_ctc 10.511573 loss_rnnt 13.055499 hw_loss 0.436825 history loss 11.009218 rank 4
2023-02-21 00:26:46,785 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 6
2023-02-21 00:26:50,895 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 2
2023-02-21 00:26:51,047 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 3
2023-02-21 00:26:51,874 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 0
2023-02-21 00:26:55,199 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 1
2023-02-21 00:26:56,667 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 5
2023-02-21 00:26:57,248 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 6
2023-02-21 00:26:58,283 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 7
2023-02-21 00:27:01,932 DEBUG CV Batch 11/3500 loss 89.064720 loss_att 162.734909 loss_ctc 92.481453 loss_rnnt 73.755020 hw_loss 0.225172 history loss 11.041626 rank 4
2023-02-21 00:27:04,440 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 2
2023-02-21 00:27:04,773 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 3
2023-02-21 00:27:05,598 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 0
2023-02-21 00:27:05,889 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 1
2023-02-21 00:27:07,345 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 5
2023-02-21 00:27:09,229 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 7
2023-02-21 00:27:11,544 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 6
2023-02-21 00:27:13,086 DEBUG CV Batch 11/3600 loss 9.583853 loss_att 17.802834 loss_ctc 6.340000 loss_rnnt 8.328214 hw_loss 0.083168 history loss 10.912760 rank 4
2023-02-21 00:27:15,724 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 2
2023-02-21 00:27:15,881 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 3
2023-02-21 00:27:16,696 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 0
2023-02-21 00:27:19,888 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 1
2023-02-21 00:27:21,228 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 5
2023-02-21 00:27:22,846 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 6
2023-02-21 00:27:23,473 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 7
2023-02-21 00:27:24,415 INFO Epoch 11 CV info cv_loss 10.803045052378694
2023-02-21 00:27:24,415 INFO Epoch 12 TRAIN info lr 0.0004953555243664854
2023-02-21 00:27:24,420 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:24,441 INFO Epoch 11 CV info cv_loss 10.803045053481366
2023-02-21 00:27:24,441 INFO Epoch 12 TRAIN info lr 0.0004953312163758942
2023-02-21 00:27:24,446 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:25,374 INFO Epoch 11 CV info cv_loss 10.80304505305925
2023-02-21 00:27:25,374 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/11.pt
2023-02-21 00:27:26,401 INFO Epoch 12 TRAIN info lr 0.000495443062773352
2023-02-21 00:27:26,403 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:27,471 DEBUG CV Batch 11/3700 loss 13.132625 loss_att 12.011097 loss_ctc 13.200684 loss_rnnt 13.186858 hw_loss 0.301871 history loss 10.857760 rank 4
2023-02-21 00:27:31,554 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 1
2023-02-21 00:27:31,588 INFO Epoch 11 CV info cv_loss 10.803045053550283
2023-02-21 00:27:31,589 INFO Epoch 12 TRAIN info lr 0.0004953360776877317
2023-02-21 00:27:31,593 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:32,606 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 5
2023-02-21 00:27:35,021 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 7
2023-02-21 00:27:38,903 DEBUG CV Batch 11/3800 loss 5.332067 loss_att 5.608809 loss_ctc 3.521175 loss_rnnt 5.343400 hw_loss 0.327695 history loss 10.811992 rank 4
2023-02-21 00:27:40,759 INFO Epoch 11 CV info cv_loss 10.803045053886255
2023-02-21 00:27:40,760 INFO Epoch 12 TRAIN info lr 0.00049540415108566
2023-02-21 00:27:40,765 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:41,488 INFO Epoch 11 CV info cv_loss 10.80304505196519
2023-02-21 00:27:41,489 INFO Epoch 12 TRAIN info lr 0.0004953409391427023
2023-02-21 00:27:41,494 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:43,996 INFO Epoch 11 CV info cv_loss 10.803045052568216
2023-02-21 00:27:43,997 INFO Epoch 12 TRAIN info lr 0.0004953093422437138
2023-02-21 00:27:44,001 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:27:48,061 INFO Epoch 11 CV info cv_loss 10.803045053266
2023-02-21 00:27:48,061 INFO Epoch 12 TRAIN info lr 0.0004954600895201771
2023-02-21 00:27:48,065 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 00:29:06,264 DEBUG TRAIN Batch 12/0 loss 9.575777 loss_att 8.135948 loss_ctc 9.882228 loss_rnnt 9.603879 hw_loss 0.410632 lr 0.00049533 rank 3
2023-02-21 00:29:06,265 DEBUG TRAIN Batch 12/0 loss 17.498936 loss_att 16.955950 loss_ctc 19.119226 loss_rnnt 17.142994 hw_loss 0.465935 lr 0.00049544 rank 0
2023-02-21 00:29:06,267 DEBUG TRAIN Batch 12/0 loss 15.930862 loss_att 12.849363 loss_ctc 15.690811 loss_rnnt 16.294853 hw_loss 0.533092 lr 0.00049533 rank 6
2023-02-21 00:29:06,269 DEBUG TRAIN Batch 12/0 loss 17.238699 loss_att 16.863880 loss_ctc 20.676123 loss_rnnt 16.548475 hw_loss 0.575373 lr 0.00049535 rank 2
2023-02-21 00:29:06,273 DEBUG TRAIN Batch 12/0 loss 16.278431 loss_att 16.248102 loss_ctc 19.117092 loss_rnnt 15.612877 hw_loss 0.549623 lr 0.00049540 rank 1
2023-02-21 00:29:06,283 DEBUG TRAIN Batch 12/0 loss 14.570288 loss_att 13.007127 loss_ctc 16.303812 loss_rnnt 14.390101 hw_loss 0.490654 lr 0.00049534 rank 5
2023-02-21 00:29:06,287 DEBUG TRAIN Batch 12/0 loss 13.334591 loss_att 12.596020 loss_ctc 14.259323 loss_rnnt 13.090253 hw_loss 0.503914 lr 0.00049531 rank 7
2023-02-21 00:29:06,418 DEBUG TRAIN Batch 12/0 loss 19.847593 loss_att 18.095116 loss_ctc 22.941145 loss_rnnt 19.543247 hw_loss 0.454439 lr 0.00049546 rank 4
2023-02-21 00:30:03,135 DEBUG TRAIN Batch 12/100 loss 28.319929 loss_att 30.919621 loss_ctc 32.208870 loss_rnnt 27.232906 hw_loss 0.091052 lr 0.00049521 rank 6
2023-02-21 00:30:03,135 DEBUG TRAIN Batch 12/100 loss 17.409624 loss_att 19.249722 loss_ctc 19.939674 loss_rnnt 16.446802 hw_loss 0.482740 lr 0.00049532 rank 0
2023-02-21 00:30:03,142 DEBUG TRAIN Batch 12/100 loss 25.854269 loss_att 26.885729 loss_ctc 27.332968 loss_rnnt 25.253948 hw_loss 0.369131 lr 0.00049523 rank 2
2023-02-21 00:30:03,143 DEBUG TRAIN Batch 12/100 loss 37.914288 loss_att 52.561745 loss_ctc 58.113880 loss_rnnt 32.027176 hw_loss 0.495638 lr 0.00049522 rank 5
2023-02-21 00:30:03,148 DEBUG TRAIN Batch 12/100 loss 6.169454 loss_att 11.656772 loss_ctc 5.429904 loss_rnnt 5.041263 hw_loss 0.242500 lr 0.00049534 rank 4
2023-02-21 00:30:03,149 DEBUG TRAIN Batch 12/100 loss 10.974288 loss_att 18.630451 loss_ctc 13.965666 loss_rnnt 8.884657 hw_loss 0.299151 lr 0.00049521 rank 3
2023-02-21 00:30:03,152 DEBUG TRAIN Batch 12/100 loss 27.431917 loss_att 29.572495 loss_ctc 32.338100 loss_rnnt 26.251625 hw_loss 0.183780 lr 0.00049519 rank 7
2023-02-21 00:30:03,201 DEBUG TRAIN Batch 12/100 loss 10.545031 loss_att 18.148455 loss_ctc 10.720039 loss_rnnt 8.826323 hw_loss 0.327542 lr 0.00049528 rank 1
2023-02-21 00:31:01,334 DEBUG TRAIN Batch 12/200 loss 17.220890 loss_att 18.704367 loss_ctc 16.180788 loss_rnnt 16.872425 hw_loss 0.357094 lr 0.00049520 rank 0
2023-02-21 00:31:01,338 DEBUG TRAIN Batch 12/200 loss 11.118335 loss_att 21.607132 loss_ctc 11.234424 loss_rnnt 8.953222 hw_loss 0.097264 lr 0.00049509 rank 6
2023-02-21 00:31:01,345 DEBUG TRAIN Batch 12/200 loss 7.691941 loss_att 12.959560 loss_ctc 6.216691 loss_rnnt 6.694008 hw_loss 0.264578 lr 0.00049521 rank 4
2023-02-21 00:31:01,346 DEBUG TRAIN Batch 12/200 loss 2.790923 loss_att 5.062318 loss_ctc 2.374932 loss_rnnt 2.184392 hw_loss 0.389471 lr 0.00049509 rank 3
2023-02-21 00:31:01,346 DEBUG TRAIN Batch 12/200 loss 15.465797 loss_att 23.826653 loss_ctc 16.762711 loss_rnnt 13.537107 hw_loss 0.156746 lr 0.00049511 rank 2
2023-02-21 00:31:01,348 DEBUG TRAIN Batch 12/200 loss 76.482819 loss_att 79.964333 loss_ctc 83.784767 loss_rnnt 74.594009 hw_loss 0.410465 lr 0.00049516 rank 1
2023-02-21 00:31:01,351 DEBUG TRAIN Batch 12/200 loss 26.037615 loss_att 30.273886 loss_ctc 27.305048 loss_rnnt 24.868237 hw_loss 0.287126 lr 0.00049506 rank 7
2023-02-21 00:31:01,352 DEBUG TRAIN Batch 12/200 loss 23.950237 loss_att 27.770508 loss_ctc 29.061180 loss_rnnt 22.317211 hw_loss 0.351585 lr 0.00049510 rank 5
2023-02-21 00:31:58,172 DEBUG TRAIN Batch 12/300 loss 19.152542 loss_att 17.070129 loss_ctc 24.847404 loss_rnnt 18.631737 hw_loss 0.333697 lr 0.00049508 rank 0
2023-02-21 00:31:58,184 DEBUG TRAIN Batch 12/300 loss 33.252586 loss_att 34.620373 loss_ctc 42.129921 loss_rnnt 31.583296 hw_loss 0.397659 lr 0.00049509 rank 4
2023-02-21 00:31:58,186 DEBUG TRAIN Batch 12/300 loss 14.976212 loss_att 15.870398 loss_ctc 16.240334 loss_rnnt 14.529953 hw_loss 0.185386 lr 0.00049499 rank 2
2023-02-21 00:31:58,187 DEBUG TRAIN Batch 12/300 loss 5.158476 loss_att 6.364513 loss_ctc 5.011139 loss_rnnt 4.699733 hw_loss 0.444713 lr 0.00049497 rank 5
2023-02-21 00:31:58,187 DEBUG TRAIN Batch 12/300 loss 14.660050 loss_att 15.507613 loss_ctc 16.399973 loss_rnnt 13.932444 hw_loss 0.611444 lr 0.00049504 rank 1
2023-02-21 00:31:58,190 DEBUG TRAIN Batch 12/300 loss 18.313210 loss_att 17.980650 loss_ctc 17.065781 loss_rnnt 18.320992 hw_loss 0.421975 lr 0.00049496 rank 3
2023-02-21 00:31:58,191 DEBUG TRAIN Batch 12/300 loss 35.680332 loss_att 38.255619 loss_ctc 44.117317 loss_rnnt 33.838974 hw_loss 0.377568 lr 0.00049497 rank 6
2023-02-21 00:31:58,254 DEBUG TRAIN Batch 12/300 loss 14.935998 loss_att 13.057961 loss_ctc 16.571793 loss_rnnt 14.916552 hw_loss 0.331777 lr 0.00049494 rank 7
2023-02-21 00:32:55,955 DEBUG TRAIN Batch 12/400 loss 23.611687 loss_att 22.618839 loss_ctc 29.071075 loss_rnnt 22.961365 hw_loss 0.226823 lr 0.00049495 rank 0
2023-02-21 00:32:55,964 DEBUG TRAIN Batch 12/400 loss 16.877224 loss_att 23.021223 loss_ctc 20.758318 loss_rnnt 14.958838 hw_loss 0.322702 lr 0.00049497 rank 4
2023-02-21 00:32:55,966 DEBUG TRAIN Batch 12/400 loss 14.022826 loss_att 21.721210 loss_ctc 18.562202 loss_rnnt 11.654264 hw_loss 0.419315 lr 0.00049484 rank 3
2023-02-21 00:32:55,967 DEBUG TRAIN Batch 12/400 loss 28.486612 loss_att 32.880527 loss_ctc 33.556252 loss_rnnt 26.752075 hw_loss 0.337131 lr 0.00049485 rank 6
2023-02-21 00:32:55,967 DEBUG TRAIN Batch 12/400 loss 5.151896 loss_att 8.939369 loss_ctc 5.505171 loss_rnnt 4.129235 hw_loss 0.408869 lr 0.00049485 rank 5
2023-02-21 00:32:55,968 DEBUG TRAIN Batch 12/400 loss 21.854006 loss_att 24.259357 loss_ctc 20.336508 loss_rnnt 21.543894 hw_loss 0.058821 lr 0.00049492 rank 1
2023-02-21 00:32:55,968 DEBUG TRAIN Batch 12/400 loss 4.396747 loss_att 7.540341 loss_ctc 4.130156 loss_rnnt 3.655428 hw_loss 0.277772 lr 0.00049487 rank 2
2023-02-21 00:32:55,978 DEBUG TRAIN Batch 12/400 loss 6.967362 loss_att 14.863255 loss_ctc 7.833796 loss_rnnt 5.127592 hw_loss 0.272003 lr 0.00049482 rank 7
2023-02-21 00:33:53,971 DEBUG TRAIN Batch 12/500 loss 32.843040 loss_att 36.042179 loss_ctc 37.941307 loss_rnnt 31.367565 hw_loss 0.292273 lr 0.00049473 rank 5
2023-02-21 00:33:53,971 DEBUG TRAIN Batch 12/500 loss 17.475979 loss_att 28.571850 loss_ctc 21.764725 loss_rnnt 14.446967 hw_loss 0.446259 lr 0.00049483 rank 0
2023-02-21 00:33:53,971 DEBUG TRAIN Batch 12/500 loss 6.183353 loss_att 6.818455 loss_ctc 6.516376 loss_rnnt 5.757445 hw_loss 0.477158 lr 0.00049475 rank 2
2023-02-21 00:33:53,973 DEBUG TRAIN Batch 12/500 loss 17.913965 loss_att 16.067467 loss_ctc 12.691139 loss_rnnt 18.666122 hw_loss 0.587849 lr 0.00049470 rank 7
2023-02-21 00:33:53,978 DEBUG TRAIN Batch 12/500 loss 1.400230 loss_att 4.758735 loss_ctc 0.575541 loss_rnnt 0.546652 hw_loss 0.547190 lr 0.00049473 rank 6
2023-02-21 00:33:53,980 DEBUG TRAIN Batch 12/500 loss 4.823984 loss_att 11.115945 loss_ctc 4.979551 loss_rnnt 3.388175 hw_loss 0.293766 lr 0.00049479 rank 1
2023-02-21 00:33:53,981 DEBUG TRAIN Batch 12/500 loss 5.501037 loss_att 12.378172 loss_ctc 2.781398 loss_rnnt 4.484884 hw_loss 0.006269 lr 0.00049472 rank 3
2023-02-21 00:33:53,983 DEBUG TRAIN Batch 12/500 loss 7.526581 loss_att 9.465438 loss_ctc 8.260437 loss_rnnt 6.901376 hw_loss 0.261725 lr 0.00049485 rank 4
2023-02-21 00:34:50,262 DEBUG TRAIN Batch 12/600 loss 32.395630 loss_att 26.514135 loss_ctc 38.407665 loss_rnnt 32.603634 hw_loss 0.312547 lr 0.00049461 rank 5
2023-02-21 00:34:50,264 DEBUG TRAIN Batch 12/600 loss 15.600163 loss_att 19.136837 loss_ctc 15.011734 loss_rnnt 14.849922 hw_loss 0.227557 lr 0.00049471 rank 0
2023-02-21 00:34:50,269 DEBUG TRAIN Batch 12/600 loss 4.530322 loss_att 9.061401 loss_ctc 2.574336 loss_rnnt 3.745075 hw_loss 0.262179 lr 0.00049458 rank 7
2023-02-21 00:34:50,271 DEBUG TRAIN Batch 12/600 loss 11.375025 loss_att 15.498198 loss_ctc 14.728172 loss_rnnt 9.864512 hw_loss 0.447735 lr 0.00049473 rank 4
2023-02-21 00:34:50,272 DEBUG TRAIN Batch 12/600 loss 21.112926 loss_att 29.274588 loss_ctc 20.143921 loss_rnnt 19.403408 hw_loss 0.386977 lr 0.00049463 rank 2
2023-02-21 00:34:50,273 DEBUG TRAIN Batch 12/600 loss 18.658121 loss_att 21.299557 loss_ctc 23.025801 loss_rnnt 17.417410 hw_loss 0.243877 lr 0.00049461 rank 6
2023-02-21 00:34:50,273 DEBUG TRAIN Batch 12/600 loss 17.500237 loss_att 19.418913 loss_ctc 19.152893 loss_rnnt 16.721922 hw_loss 0.326670 lr 0.00049460 rank 3
2023-02-21 00:34:50,278 DEBUG TRAIN Batch 12/600 loss 26.423866 loss_att 26.246964 loss_ctc 27.731245 loss_rnnt 26.282864 hw_loss 0.003878 lr 0.00049467 rank 1
2023-02-21 00:35:49,738 DEBUG TRAIN Batch 12/700 loss 11.025332 loss_att 20.543320 loss_ctc 17.424915 loss_rnnt 8.202973 hw_loss 0.122782 lr 0.00049459 rank 0
2023-02-21 00:35:49,740 DEBUG TRAIN Batch 12/700 loss 26.861263 loss_att 29.609009 loss_ctc 28.831873 loss_rnnt 25.907898 hw_loss 0.264509 lr 0.00049449 rank 5
2023-02-21 00:35:49,742 DEBUG TRAIN Batch 12/700 loss 22.694927 loss_att 23.584881 loss_ctc 30.020660 loss_rnnt 21.449694 hw_loss 0.169645 lr 0.00049448 rank 3
2023-02-21 00:35:49,748 DEBUG TRAIN Batch 12/700 loss 19.008677 loss_att 31.096910 loss_ctc 17.158161 loss_rnnt 16.695694 hw_loss 0.266381 lr 0.00049450 rank 2
2023-02-21 00:35:49,750 DEBUG TRAIN Batch 12/700 loss 23.020918 loss_att 24.008394 loss_ctc 28.559593 loss_rnnt 21.967014 hw_loss 0.221093 lr 0.00049446 rank 7
2023-02-21 00:35:49,752 DEBUG TRAIN Batch 12/700 loss 8.036045 loss_att 12.134439 loss_ctc 12.191821 loss_rnnt 6.506250 hw_loss 0.292522 lr 0.00049455 rank 1
2023-02-21 00:35:49,756 DEBUG TRAIN Batch 12/700 loss 13.269274 loss_att 18.685865 loss_ctc 12.153746 loss_rnnt 12.249964 hw_loss 0.158864 lr 0.00049461 rank 4
2023-02-21 00:35:49,811 DEBUG TRAIN Batch 12/700 loss 14.434194 loss_att 22.421869 loss_ctc 16.170856 loss_rnnt 12.501815 hw_loss 0.193668 lr 0.00049449 rank 6
2023-02-21 00:36:48,317 DEBUG TRAIN Batch 12/800 loss 36.263626 loss_att 36.153118 loss_ctc 50.929745 loss_rnnt 34.329819 hw_loss 0.000798 lr 0.00049443 rank 1
2023-02-21 00:36:48,319 DEBUG TRAIN Batch 12/800 loss 10.933987 loss_att 20.894411 loss_ctc 8.040009 loss_rnnt 9.201601 hw_loss 0.236559 lr 0.00049447 rank 0
2023-02-21 00:36:48,321 DEBUG TRAIN Batch 12/800 loss 16.904505 loss_att 25.429638 loss_ctc 22.693186 loss_rnnt 14.210991 hw_loss 0.406242 lr 0.00049449 rank 4
2023-02-21 00:36:48,322 DEBUG TRAIN Batch 12/800 loss 3.477070 loss_att 9.384277 loss_ctc 3.662881 loss_rnnt 2.270633 hw_loss 0.000413 lr 0.00049438 rank 2
2023-02-21 00:36:48,323 DEBUG TRAIN Batch 12/800 loss 5.902264 loss_att 16.276659 loss_ctc 5.230878 loss_rnnt 3.704201 hw_loss 0.398816 lr 0.00049437 rank 5
2023-02-21 00:36:48,324 DEBUG TRAIN Batch 12/800 loss 17.904942 loss_att 23.242119 loss_ctc 30.406328 loss_rnnt 15.038973 hw_loss 0.246900 lr 0.00049436 rank 6
2023-02-21 00:36:48,330 DEBUG TRAIN Batch 12/800 loss 21.959564 loss_att 18.352058 loss_ctc 22.362047 loss_rnnt 22.515078 hw_loss 0.210606 lr 0.00049436 rank 3
2023-02-21 00:36:48,333 DEBUG TRAIN Batch 12/800 loss 33.831215 loss_att 39.874622 loss_ctc 52.424862 loss_rnnt 29.985497 hw_loss 0.296033 lr 0.00049434 rank 7
2023-02-21 00:38:09,870 DEBUG TRAIN Batch 12/900 loss 18.704760 loss_att 24.278351 loss_ctc 24.834713 loss_rnnt 16.771856 hw_loss 0.001613 lr 0.00049426 rank 2
2023-02-21 00:38:09,872 DEBUG TRAIN Batch 12/900 loss 9.316555 loss_att 14.199739 loss_ctc 11.547424 loss_rnnt 7.938505 hw_loss 0.194930 lr 0.00049435 rank 0
2023-02-21 00:38:09,877 DEBUG TRAIN Batch 12/900 loss 19.928652 loss_att 23.791004 loss_ctc 21.585546 loss_rnnt 18.743290 hw_loss 0.359943 lr 0.00049425 rank 5
2023-02-21 00:38:09,879 DEBUG TRAIN Batch 12/900 loss 9.455231 loss_att 10.393063 loss_ctc 8.956923 loss_rnnt 9.142149 hw_loss 0.359916 lr 0.00049424 rank 6
2023-02-21 00:38:09,881 DEBUG TRAIN Batch 12/900 loss 20.873241 loss_att 27.462685 loss_ctc 26.739094 loss_rnnt 18.700516 hw_loss 0.136357 lr 0.00049424 rank 3
2023-02-21 00:38:09,882 DEBUG TRAIN Batch 12/900 loss 22.744762 loss_att 26.240236 loss_ctc 29.630234 loss_rnnt 21.058540 hw_loss 0.129496 lr 0.00049437 rank 4
2023-02-21 00:38:09,883 DEBUG TRAIN Batch 12/900 loss 15.332196 loss_att 15.601783 loss_ctc 14.513854 loss_rnnt 15.181291 hw_loss 0.386439 lr 0.00049431 rank 1
2023-02-21 00:38:09,947 DEBUG TRAIN Batch 12/900 loss 19.009697 loss_att 22.264120 loss_ctc 24.520947 loss_rnnt 17.466366 hw_loss 0.295522 lr 0.00049422 rank 7
2023-02-21 00:39:09,918 DEBUG TRAIN Batch 12/1000 loss 12.487365 loss_att 19.453930 loss_ctc 16.464312 loss_rnnt 10.438584 hw_loss 0.234763 lr 0.00049423 rank 0
2023-02-21 00:39:09,927 DEBUG TRAIN Batch 12/1000 loss 31.493914 loss_att 33.150970 loss_ctc 34.004288 loss_rnnt 30.761765 hw_loss 0.123790 lr 0.00049412 rank 6
2023-02-21 00:39:09,928 DEBUG TRAIN Batch 12/1000 loss 12.181623 loss_att 16.066612 loss_ctc 17.667271 loss_rnnt 10.548632 hw_loss 0.233577 lr 0.00049413 rank 5
2023-02-21 00:39:09,929 DEBUG TRAIN Batch 12/1000 loss 50.927895 loss_att 64.064346 loss_ctc 83.289238 loss_rnnt 43.816532 hw_loss 0.317302 lr 0.00049419 rank 1
2023-02-21 00:39:09,932 DEBUG TRAIN Batch 12/1000 loss 11.348485 loss_att 17.584372 loss_ctc 14.256529 loss_rnnt 9.614188 hw_loss 0.186341 lr 0.00049414 rank 2
2023-02-21 00:39:09,933 DEBUG TRAIN Batch 12/1000 loss 8.014729 loss_att 13.848602 loss_ctc 7.285013 loss_rnnt 6.737994 hw_loss 0.388606 lr 0.00049425 rank 4
2023-02-21 00:39:09,934 DEBUG TRAIN Batch 12/1000 loss 24.094681 loss_att 26.862593 loss_ctc 20.983219 loss_rnnt 23.855595 hw_loss 0.188182 lr 0.00049412 rank 3
2023-02-21 00:39:09,997 DEBUG TRAIN Batch 12/1000 loss 12.129762 loss_att 13.614431 loss_ctc 8.334528 loss_rnnt 12.260908 hw_loss 0.146158 lr 0.00049410 rank 7
2023-02-21 00:40:07,151 DEBUG TRAIN Batch 12/1100 loss 8.558519 loss_att 10.521062 loss_ctc 10.671984 loss_rnnt 7.778879 hw_loss 0.197507 lr 0.00049407 rank 1
2023-02-21 00:40:07,151 DEBUG TRAIN Batch 12/1100 loss 19.983276 loss_att 32.619171 loss_ctc 19.048681 loss_rnnt 17.389917 hw_loss 0.357733 lr 0.00049402 rank 2
2023-02-21 00:40:07,151 DEBUG TRAIN Batch 12/1100 loss 16.962921 loss_att 20.809006 loss_ctc 15.438644 loss_rnnt 16.302860 hw_loss 0.176402 lr 0.00049411 rank 0
2023-02-21 00:40:07,152 DEBUG TRAIN Batch 12/1100 loss 12.768050 loss_att 18.542030 loss_ctc 17.044870 loss_rnnt 10.914060 hw_loss 0.241784 lr 0.00049400 rank 6
2023-02-21 00:40:07,156 DEBUG TRAIN Batch 12/1100 loss 3.649283 loss_att 10.407896 loss_ctc 7.197777 loss_rnnt 1.562349 hw_loss 0.491399 lr 0.00049400 rank 3
2023-02-21 00:40:07,159 DEBUG TRAIN Batch 12/1100 loss 27.542341 loss_att 34.232868 loss_ctc 29.930172 loss_rnnt 25.879137 hw_loss 0.012605 lr 0.00049401 rank 5
2023-02-21 00:40:07,160 DEBUG TRAIN Batch 12/1100 loss 20.553045 loss_att 20.176809 loss_ctc 18.423023 loss_rnnt 20.812588 hw_loss 0.186949 lr 0.00049398 rank 7
2023-02-21 00:40:07,222 DEBUG TRAIN Batch 12/1100 loss 15.597263 loss_att 18.723579 loss_ctc 11.960202 loss_rnnt 15.270011 hw_loss 0.350494 lr 0.00049413 rank 4
2023-02-21 00:41:03,740 DEBUG TRAIN Batch 12/1200 loss 37.842949 loss_att 40.036808 loss_ctc 37.116425 loss_rnnt 37.370914 hw_loss 0.243995 lr 0.00049399 rank 0
2023-02-21 00:41:03,746 DEBUG TRAIN Batch 12/1200 loss 24.647064 loss_att 26.592937 loss_ctc 30.355284 loss_rnnt 23.289593 hw_loss 0.388504 lr 0.00049389 rank 5
2023-02-21 00:41:03,748 DEBUG TRAIN Batch 12/1200 loss 4.626190 loss_att 6.994375 loss_ctc 3.902707 loss_rnnt 4.116668 hw_loss 0.248155 lr 0.00049390 rank 2
2023-02-21 00:41:03,750 DEBUG TRAIN Batch 12/1200 loss 22.059681 loss_att 27.644230 loss_ctc 26.127853 loss_rnnt 20.302456 hw_loss 0.183547 lr 0.00049395 rank 1
2023-02-21 00:41:03,751 DEBUG TRAIN Batch 12/1200 loss 17.994928 loss_att 19.876701 loss_ctc 21.734915 loss_rnnt 17.066872 hw_loss 0.099442 lr 0.00049400 rank 4
2023-02-21 00:41:03,752 DEBUG TRAIN Batch 12/1200 loss 7.936366 loss_att 11.974731 loss_ctc 8.650877 loss_rnnt 6.908014 hw_loss 0.235146 lr 0.00049388 rank 3
2023-02-21 00:41:03,759 DEBUG TRAIN Batch 12/1200 loss 13.219008 loss_att 18.668837 loss_ctc 16.121489 loss_rnnt 11.525213 hw_loss 0.406561 lr 0.00049386 rank 7
2023-02-21 00:41:03,760 DEBUG TRAIN Batch 12/1200 loss 5.983589 loss_att 10.878890 loss_ctc 8.944393 loss_rnnt 4.438647 hw_loss 0.320827 lr 0.00049388 rank 6
2023-02-21 00:42:03,610 DEBUG TRAIN Batch 12/1300 loss 10.042833 loss_att 17.913944 loss_ctc 12.427709 loss_rnnt 7.878218 hw_loss 0.510768 lr 0.00049387 rank 0
2023-02-21 00:42:03,613 DEBUG TRAIN Batch 12/1300 loss 12.784513 loss_att 17.555069 loss_ctc 11.799429 loss_rnnt 11.834324 hw_loss 0.238916 lr 0.00049377 rank 5
2023-02-21 00:42:03,617 DEBUG TRAIN Batch 12/1300 loss 32.997066 loss_att 41.674770 loss_ctc 39.250511 loss_rnnt 30.285181 hw_loss 0.267289 lr 0.00049376 rank 3
2023-02-21 00:42:03,619 DEBUG TRAIN Batch 12/1300 loss 8.849665 loss_att 12.047546 loss_ctc 11.736296 loss_rnnt 7.595606 hw_loss 0.430496 lr 0.00049376 rank 6
2023-02-21 00:42:03,619 DEBUG TRAIN Batch 12/1300 loss 16.200512 loss_att 19.087477 loss_ctc 20.560747 loss_rnnt 14.853958 hw_loss 0.352117 lr 0.00049378 rank 2
2023-02-21 00:42:03,620 DEBUG TRAIN Batch 12/1300 loss 8.119602 loss_att 13.652914 loss_ctc 3.801340 loss_rnnt 7.588371 hw_loss 0.000632 lr 0.00049373 rank 7
2023-02-21 00:42:03,624 DEBUG TRAIN Batch 12/1300 loss 13.002316 loss_att 17.390064 loss_ctc 16.070301 loss_rnnt 11.609137 hw_loss 0.199811 lr 0.00049388 rank 4
2023-02-21 00:42:03,676 DEBUG TRAIN Batch 12/1300 loss 9.127184 loss_att 11.031382 loss_ctc 8.581765 loss_rnnt 8.818704 hw_loss 0.000679 lr 0.00049383 rank 1
2023-02-21 00:43:01,078 DEBUG TRAIN Batch 12/1400 loss 3.271118 loss_att 7.722261 loss_ctc 6.588397 loss_rnnt 1.861799 hw_loss 0.143973 lr 0.00049364 rank 3
2023-02-21 00:43:01,080 DEBUG TRAIN Batch 12/1400 loss 20.516830 loss_att 28.098013 loss_ctc 28.459921 loss_rnnt 17.735434 hw_loss 0.386398 lr 0.00049375 rank 0
2023-02-21 00:43:01,082 DEBUG TRAIN Batch 12/1400 loss 40.314083 loss_att 27.743021 loss_ctc 43.729900 loss_rnnt 42.371655 hw_loss 0.002248 lr 0.00049364 rank 6
2023-02-21 00:43:01,087 DEBUG TRAIN Batch 12/1400 loss 9.285684 loss_att 14.126041 loss_ctc 11.750638 loss_rnnt 7.988402 hw_loss 0.001031 lr 0.00049361 rank 7
2023-02-21 00:43:01,090 DEBUG TRAIN Batch 12/1400 loss 10.018402 loss_att 14.796684 loss_ctc 15.231449 loss_rnnt 8.191379 hw_loss 0.330553 lr 0.00049376 rank 4
2023-02-21 00:43:01,090 DEBUG TRAIN Batch 12/1400 loss 37.467430 loss_att 40.451347 loss_ctc 38.376873 loss_rnnt 36.567783 hw_loss 0.340506 lr 0.00049366 rank 2
2023-02-21 00:43:01,132 DEBUG TRAIN Batch 12/1400 loss 29.041718 loss_att 34.758244 loss_ctc 25.740076 loss_rnnt 28.219904 hw_loss 0.222615 lr 0.00049371 rank 1
2023-02-21 00:43:01,137 DEBUG TRAIN Batch 12/1400 loss 2.571188 loss_att 5.247614 loss_ctc 2.380430 loss_rnnt 1.817328 hw_loss 0.457517 lr 0.00049365 rank 5
2023-02-21 00:43:58,324 DEBUG TRAIN Batch 12/1500 loss 8.160509 loss_att 14.272144 loss_ctc 10.404069 loss_rnnt 6.480361 hw_loss 0.297525 lr 0.00049363 rank 0
2023-02-21 00:43:58,329 DEBUG TRAIN Batch 12/1500 loss 33.968628 loss_att 36.865269 loss_ctc 34.283527 loss_rnnt 33.346748 hw_loss 0.001062 lr 0.00049359 rank 1
2023-02-21 00:43:58,329 DEBUG TRAIN Batch 12/1500 loss 19.741850 loss_att 23.083580 loss_ctc 22.442284 loss_rnnt 18.542082 hw_loss 0.321309 lr 0.00049353 rank 5
2023-02-21 00:43:58,332 DEBUG TRAIN Batch 12/1500 loss 42.763828 loss_att 34.981953 loss_ctc 46.409920 loss_rnnt 43.725288 hw_loss 0.203939 lr 0.00049352 rank 6
2023-02-21 00:43:58,333 DEBUG TRAIN Batch 12/1500 loss 13.710765 loss_att 17.426731 loss_ctc 16.029356 loss_rnnt 12.591159 hw_loss 0.126125 lr 0.00049349 rank 7
2023-02-21 00:43:58,336 DEBUG TRAIN Batch 12/1500 loss 14.689635 loss_att 22.554087 loss_ctc 20.700312 loss_rnnt 12.071030 hw_loss 0.458048 lr 0.00049354 rank 2
2023-02-21 00:43:58,339 DEBUG TRAIN Batch 12/1500 loss 65.840538 loss_att 91.346123 loss_ctc 73.083542 loss_rnnt 59.547302 hw_loss 0.424480 lr 0.00049364 rank 4
2023-02-21 00:43:58,341 DEBUG TRAIN Batch 12/1500 loss 18.727322 loss_att 22.106531 loss_ctc 31.358356 loss_rnnt 16.191256 hw_loss 0.330162 lr 0.00049352 rank 3
2023-02-21 00:44:58,013 DEBUG TRAIN Batch 12/1600 loss 49.116554 loss_att 63.851582 loss_ctc 75.433479 loss_rnnt 42.487179 hw_loss 0.325219 lr 0.00049351 rank 0
2023-02-21 00:44:58,021 DEBUG TRAIN Batch 12/1600 loss 15.574173 loss_att 28.505623 loss_ctc 25.008518 loss_rnnt 11.615646 hw_loss 0.214357 lr 0.00049340 rank 6
2023-02-21 00:44:58,023 DEBUG TRAIN Batch 12/1600 loss 26.253471 loss_att 32.285023 loss_ctc 31.056360 loss_rnnt 24.302778 hw_loss 0.195000 lr 0.00049342 rank 2
2023-02-21 00:44:58,024 DEBUG TRAIN Batch 12/1600 loss 16.752438 loss_att 19.624405 loss_ctc 21.753162 loss_rnnt 15.312881 hw_loss 0.371999 lr 0.00049341 rank 5
2023-02-21 00:44:58,027 DEBUG TRAIN Batch 12/1600 loss 38.161694 loss_att 49.136467 loss_ctc 53.973858 loss_rnnt 33.849762 hw_loss 0.016291 lr 0.00049340 rank 3
2023-02-21 00:44:58,029 DEBUG TRAIN Batch 12/1600 loss 9.884175 loss_att 12.051590 loss_ctc 13.799561 loss_rnnt 8.926324 hw_loss 0.004345 lr 0.00049347 rank 1
2023-02-21 00:44:58,032 DEBUG TRAIN Batch 12/1600 loss 10.993892 loss_att 15.123107 loss_ctc 15.707093 loss_rnnt 9.421458 hw_loss 0.221558 lr 0.00049352 rank 4
2023-02-21 00:44:58,085 DEBUG TRAIN Batch 12/1600 loss 12.724996 loss_att 13.401552 loss_ctc 12.376942 loss_rnnt 12.634161 hw_loss 0.003621 lr 0.00049337 rank 7
2023-02-21 00:45:57,376 DEBUG TRAIN Batch 12/1700 loss 11.829064 loss_att 18.753613 loss_ctc 19.663504 loss_rnnt 9.309337 hw_loss 0.169175 lr 0.00049339 rank 0
2023-02-21 00:45:57,381 DEBUG TRAIN Batch 12/1700 loss 7.321145 loss_att 9.665121 loss_ctc 8.774688 loss_rnnt 6.658011 hw_loss 0.000999 lr 0.00049329 rank 5
2023-02-21 00:45:57,383 DEBUG TRAIN Batch 12/1700 loss 20.459740 loss_att 22.056009 loss_ctc 17.282427 loss_rnnt 20.446041 hw_loss 0.221415 lr 0.00049330 rank 2
2023-02-21 00:45:57,385 DEBUG TRAIN Batch 12/1700 loss 37.313862 loss_att 42.872135 loss_ctc 38.783237 loss_rnnt 36.006054 hw_loss 0.000450 lr 0.00049325 rank 7
2023-02-21 00:45:57,388 DEBUG TRAIN Batch 12/1700 loss 61.770267 loss_att 59.369514 loss_ctc 73.348053 loss_rnnt 60.705093 hw_loss 0.003043 lr 0.00049328 rank 3
2023-02-21 00:45:57,388 DEBUG TRAIN Batch 12/1700 loss 19.283182 loss_att 17.522848 loss_ctc 23.904308 loss_rnnt 19.018656 hw_loss 0.000832 lr 0.00049328 rank 6
2023-02-21 00:45:57,414 DEBUG TRAIN Batch 12/1700 loss 12.677990 loss_att 11.857058 loss_ctc 11.289256 loss_rnnt 12.802523 hw_loss 0.421533 lr 0.00049335 rank 1
2023-02-21 00:45:57,417 DEBUG TRAIN Batch 12/1700 loss 9.393592 loss_att 14.853384 loss_ctc 6.478151 loss_rnnt 8.690016 hw_loss 0.000642 lr 0.00049340 rank 4
2023-02-21 00:47:15,970 DEBUG TRAIN Batch 12/1800 loss 23.516521 loss_att 37.152031 loss_ctc 27.539019 loss_rnnt 20.068790 hw_loss 0.345552 lr 0.00049316 rank 6
2023-02-21 00:47:15,970 DEBUG TRAIN Batch 12/1800 loss 41.925663 loss_att 46.374641 loss_ctc 56.244221 loss_rnnt 38.845539 hw_loss 0.527227 lr 0.00049327 rank 0
2023-02-21 00:47:15,971 DEBUG TRAIN Batch 12/1800 loss 16.252153 loss_att 16.644108 loss_ctc 17.441162 loss_rnnt 15.941734 hw_loss 0.137802 lr 0.00049318 rank 2
2023-02-21 00:47:15,976 DEBUG TRAIN Batch 12/1800 loss 42.419579 loss_att 40.192852 loss_ctc 44.988914 loss_rnnt 42.485161 hw_loss 0.069722 lr 0.00049317 rank 5
2023-02-21 00:47:15,979 DEBUG TRAIN Batch 12/1800 loss 15.748077 loss_att 18.034000 loss_ctc 17.579985 loss_rnnt 14.926744 hw_loss 0.224802 lr 0.00049313 rank 7
2023-02-21 00:47:15,981 DEBUG TRAIN Batch 12/1800 loss 24.892458 loss_att 28.021612 loss_ctc 25.313030 loss_rnnt 23.982559 hw_loss 0.427484 lr 0.00049316 rank 3
2023-02-21 00:47:15,984 DEBUG TRAIN Batch 12/1800 loss 15.846128 loss_att 19.028904 loss_ctc 20.083225 loss_rnnt 14.467043 hw_loss 0.332966 lr 0.00049328 rank 4
2023-02-21 00:47:16,036 DEBUG TRAIN Batch 12/1800 loss 35.267857 loss_att 41.743271 loss_ctc 36.715504 loss_rnnt 33.645958 hw_loss 0.250865 lr 0.00049323 rank 1
2023-02-21 00:48:15,239 DEBUG TRAIN Batch 12/1900 loss 8.124905 loss_att 11.539399 loss_ctc 6.240516 loss_rnnt 7.503844 hw_loss 0.355151 lr 0.00049306 rank 2
2023-02-21 00:48:15,246 DEBUG TRAIN Batch 12/1900 loss 9.731838 loss_att 11.143093 loss_ctc 11.463922 loss_rnnt 8.999583 hw_loss 0.410737 lr 0.00049305 rank 5
2023-02-21 00:48:15,248 DEBUG TRAIN Batch 12/1900 loss 4.088733 loss_att 10.212263 loss_ctc 4.571211 loss_rnnt 2.638041 hw_loss 0.303104 lr 0.00049304 rank 6
2023-02-21 00:48:15,247 DEBUG TRAIN Batch 12/1900 loss 36.620888 loss_att 50.548386 loss_ctc 38.337322 loss_rnnt 33.506397 hw_loss 0.187742 lr 0.00049315 rank 0
2023-02-21 00:48:15,248 DEBUG TRAIN Batch 12/1900 loss 18.982254 loss_att 21.768509 loss_ctc 24.569925 loss_rnnt 17.569767 hw_loss 0.206653 lr 0.00049304 rank 3
2023-02-21 00:48:15,251 DEBUG TRAIN Batch 12/1900 loss 82.777039 loss_att 79.392120 loss_ctc 97.912773 loss_rnnt 81.342484 hw_loss 0.175190 lr 0.00049316 rank 4
2023-02-21 00:48:15,254 DEBUG TRAIN Batch 12/1900 loss 18.170809 loss_att 22.294983 loss_ctc 21.614553 loss_rnnt 16.875774 hw_loss 0.020692 lr 0.00049311 rank 1
2023-02-21 00:48:15,321 DEBUG TRAIN Batch 12/1900 loss 20.017788 loss_att 22.480934 loss_ctc 25.007450 loss_rnnt 18.698339 hw_loss 0.302868 lr 0.00049301 rank 7
2023-02-21 00:49:11,453 DEBUG TRAIN Batch 12/2000 loss 26.198511 loss_att 32.151306 loss_ctc 30.168726 loss_rnnt 24.478403 hw_loss 0.000349 lr 0.00049303 rank 0
2023-02-21 00:49:11,466 DEBUG TRAIN Batch 12/2000 loss 5.901016 loss_att 16.274368 loss_ctc 6.705037 loss_rnnt 3.718934 hw_loss 0.000391 lr 0.00049293 rank 5
2023-02-21 00:49:11,467 DEBUG TRAIN Batch 12/2000 loss 23.849319 loss_att 32.187515 loss_ctc 25.062687 loss_rnnt 22.019707 hw_loss 0.000357 lr 0.00049299 rank 1
2023-02-21 00:49:11,467 DEBUG TRAIN Batch 12/2000 loss 15.749501 loss_att 12.169934 loss_ctc 15.900891 loss_rnnt 16.114000 hw_loss 0.621055 lr 0.00049292 rank 6
2023-02-21 00:49:11,470 DEBUG TRAIN Batch 12/2000 loss 9.470629 loss_att 19.039341 loss_ctc 10.555738 loss_rnnt 7.082713 hw_loss 0.617797 lr 0.00049292 rank 3
2023-02-21 00:49:11,471 DEBUG TRAIN Batch 12/2000 loss 11.946726 loss_att 28.401611 loss_ctc 21.460506 loss_rnnt 7.386582 hw_loss 0.001240 lr 0.00049294 rank 2
2023-02-21 00:49:11,474 DEBUG TRAIN Batch 12/2000 loss 29.171263 loss_att 28.793987 loss_ctc 31.705976 loss_rnnt 28.742699 hw_loss 0.311361 lr 0.00049289 rank 7
2023-02-21 00:49:11,475 DEBUG TRAIN Batch 12/2000 loss 12.297660 loss_att 21.445190 loss_ctc 12.362732 loss_rnnt 10.260515 hw_loss 0.373054 lr 0.00049304 rank 4
2023-02-21 00:50:09,050 DEBUG TRAIN Batch 12/2100 loss 29.811529 loss_att 38.708908 loss_ctc 52.102528 loss_rnnt 24.973145 hw_loss 0.162702 lr 0.00049291 rank 0
2023-02-21 00:50:09,054 DEBUG TRAIN Batch 12/2100 loss 21.125166 loss_att 22.784548 loss_ctc 17.937019 loss_rnnt 21.096329 hw_loss 0.228837 lr 0.00049282 rank 2
2023-02-21 00:50:09,055 DEBUG TRAIN Batch 12/2100 loss 13.745609 loss_att 14.429152 loss_ctc 14.348703 loss_rnnt 13.469869 hw_loss 0.109911 lr 0.00049280 rank 3
2023-02-21 00:50:09,059 DEBUG TRAIN Batch 12/2100 loss 11.622918 loss_att 17.205801 loss_ctc 10.423700 loss_rnnt 10.534708 hw_loss 0.246617 lr 0.00049287 rank 1
2023-02-21 00:50:09,062 DEBUG TRAIN Batch 12/2100 loss 8.265959 loss_att 10.514482 loss_ctc 8.899635 loss_rnnt 7.676739 hw_loss 0.103172 lr 0.00049292 rank 4
2023-02-21 00:50:09,068 DEBUG TRAIN Batch 12/2100 loss 10.315465 loss_att 13.211404 loss_ctc 12.366680 loss_rnnt 9.346393 hw_loss 0.218231 lr 0.00049281 rank 5
2023-02-21 00:50:09,071 DEBUG TRAIN Batch 12/2100 loss 14.514794 loss_att 20.873720 loss_ctc 17.262184 loss_rnnt 12.705345 hw_loss 0.321270 lr 0.00049277 rank 7
2023-02-21 00:50:09,121 DEBUG TRAIN Batch 12/2100 loss 25.424570 loss_att 27.401655 loss_ctc 29.731684 loss_rnnt 24.269213 hw_loss 0.348107 lr 0.00049280 rank 6
2023-02-21 00:51:09,025 DEBUG TRAIN Batch 12/2200 loss 2.040946 loss_att 6.447734 loss_ctc 1.405486 loss_rnnt 1.095619 hw_loss 0.278807 lr 0.00049279 rank 0
2023-02-21 00:51:09,028 DEBUG TRAIN Batch 12/2200 loss 2.154300 loss_att 7.700763 loss_ctc 2.411775 loss_rnnt 0.918779 hw_loss 0.172309 lr 0.00049268 rank 6
2023-02-21 00:51:09,033 DEBUG TRAIN Batch 12/2200 loss 1.988819 loss_att 5.782402 loss_ctc 1.143744 loss_rnnt 1.116856 hw_loss 0.423605 lr 0.00049266 rank 7
2023-02-21 00:51:09,035 DEBUG TRAIN Batch 12/2200 loss 11.622186 loss_att 19.117985 loss_ctc 13.437164 loss_rnnt 9.880272 hw_loss 0.001419 lr 0.00049275 rank 1
2023-02-21 00:51:09,036 DEBUG TRAIN Batch 12/2200 loss 25.212414 loss_att 30.616596 loss_ctc 24.713886 loss_rnnt 24.197807 hw_loss 0.000452 lr 0.00049270 rank 2
2023-02-21 00:51:09,038 DEBUG TRAIN Batch 12/2200 loss 1.809739 loss_att 9.080273 loss_ctc 0.741968 loss_rnnt 0.495745 hw_loss 0.004232 lr 0.00049268 rank 3
2023-02-21 00:51:09,100 DEBUG TRAIN Batch 12/2200 loss 11.225904 loss_att 16.429543 loss_ctc 15.284191 loss_rnnt 9.542828 hw_loss 0.189831 lr 0.00049269 rank 5
2023-02-21 00:51:09,142 DEBUG TRAIN Batch 12/2200 loss 49.672863 loss_att 41.414719 loss_ctc 52.843590 loss_rnnt 50.901508 hw_loss 0.000414 lr 0.00049280 rank 4
2023-02-21 00:52:05,462 DEBUG TRAIN Batch 12/2300 loss 24.140011 loss_att 32.009869 loss_ctc 20.546991 loss_rnnt 23.044773 hw_loss 0.000630 lr 0.00049258 rank 2
2023-02-21 00:52:05,462 DEBUG TRAIN Batch 12/2300 loss 37.114273 loss_att 52.370079 loss_ctc 57.737881 loss_rnnt 31.167210 hw_loss 0.273910 lr 0.00049267 rank 0
2023-02-21 00:52:05,465 DEBUG TRAIN Batch 12/2300 loss 39.426544 loss_att 38.009453 loss_ctc 49.283264 loss_rnnt 38.176216 hw_loss 0.411588 lr 0.00049256 rank 6
2023-02-21 00:52:05,466 DEBUG TRAIN Batch 12/2300 loss 13.614251 loss_att 18.427368 loss_ctc 7.014488 loss_rnnt 13.286888 hw_loss 0.458829 lr 0.00049256 rank 3
2023-02-21 00:52:05,466 DEBUG TRAIN Batch 12/2300 loss 7.383493 loss_att 15.368542 loss_ctc 17.965384 loss_rnnt 4.251194 hw_loss 0.233196 lr 0.00049257 rank 5
2023-02-21 00:52:05,472 DEBUG TRAIN Batch 12/2300 loss 21.185318 loss_att 35.384918 loss_ctc 31.411297 loss_rnnt 16.880289 hw_loss 0.190581 lr 0.00049268 rank 4
2023-02-21 00:52:05,477 DEBUG TRAIN Batch 12/2300 loss 18.645195 loss_att 21.622803 loss_ctc 20.561325 loss_rnnt 17.568508 hw_loss 0.423157 lr 0.00049263 rank 1
2023-02-21 00:52:05,479 DEBUG TRAIN Batch 12/2300 loss 19.088081 loss_att 18.535271 loss_ctc 20.982794 loss_rnnt 18.817957 hw_loss 0.240105 lr 0.00049254 rank 7
2023-02-21 00:53:02,546 DEBUG TRAIN Batch 12/2400 loss 7.320297 loss_att 12.921640 loss_ctc 9.655719 loss_rnnt 5.760623 hw_loss 0.240029 lr 0.00049255 rank 0
2023-02-21 00:53:02,554 DEBUG TRAIN Batch 12/2400 loss 5.691696 loss_att 12.467264 loss_ctc 10.219600 loss_rnnt 3.683254 hw_loss 0.093015 lr 0.00049246 rank 2
2023-02-21 00:53:02,558 DEBUG TRAIN Batch 12/2400 loss 11.632992 loss_att 13.305693 loss_ctc 13.877818 loss_rnnt 10.863437 hw_loss 0.254446 lr 0.00049245 rank 5
2023-02-21 00:53:02,558 DEBUG TRAIN Batch 12/2400 loss 12.583989 loss_att 14.698361 loss_ctc 11.932354 loss_rnnt 12.064256 hw_loss 0.344517 lr 0.00049251 rank 1
2023-02-21 00:53:02,559 DEBUG TRAIN Batch 12/2400 loss 21.494089 loss_att 20.748199 loss_ctc 18.901722 loss_rnnt 21.886194 hw_loss 0.192598 lr 0.00049244 rank 6
2023-02-21 00:53:02,560 DEBUG TRAIN Batch 12/2400 loss 12.721948 loss_att 13.399532 loss_ctc 12.528210 loss_rnnt 12.415922 hw_loss 0.368139 lr 0.00049256 rank 4
2023-02-21 00:53:02,561 DEBUG TRAIN Batch 12/2400 loss 18.774134 loss_att 23.654068 loss_ctc 34.734024 loss_rnnt 15.488083 hw_loss 0.341396 lr 0.00049244 rank 3
2023-02-21 00:53:02,568 DEBUG TRAIN Batch 12/2400 loss 18.603102 loss_att 22.872469 loss_ctc 25.257185 loss_rnnt 16.777737 hw_loss 0.158025 lr 0.00049242 rank 7
2023-02-21 00:54:01,543 DEBUG TRAIN Batch 12/2500 loss 6.303779 loss_att 9.990159 loss_ctc 3.175929 loss_rnnt 5.838904 hw_loss 0.271210 lr 0.00049243 rank 0
2023-02-21 00:54:01,545 DEBUG TRAIN Batch 12/2500 loss 9.556110 loss_att 16.446453 loss_ctc 8.706990 loss_rnnt 8.181190 hw_loss 0.206375 lr 0.00049232 rank 6
2023-02-21 00:54:01,546 DEBUG TRAIN Batch 12/2500 loss 30.113506 loss_att 34.494392 loss_ctc 29.592955 loss_rnnt 29.178482 hw_loss 0.240481 lr 0.00049239 rank 1
2023-02-21 00:54:01,552 DEBUG TRAIN Batch 12/2500 loss 18.427612 loss_att 29.648998 loss_ctc 24.296751 loss_rnnt 15.308994 hw_loss 0.172105 lr 0.00049230 rank 7
2023-02-21 00:54:01,554 DEBUG TRAIN Batch 12/2500 loss 26.419966 loss_att 31.331078 loss_ctc 28.365692 loss_rnnt 25.023842 hw_loss 0.289637 lr 0.00049233 rank 5
2023-02-21 00:54:01,554 DEBUG TRAIN Batch 12/2500 loss 54.901108 loss_att 65.686371 loss_ctc 64.199471 loss_rnnt 51.377655 hw_loss 0.237405 lr 0.00049232 rank 3
2023-02-21 00:54:01,555 DEBUG TRAIN Batch 12/2500 loss 10.839105 loss_att 11.731079 loss_ctc 13.769061 loss_rnnt 10.194957 hw_loss 0.140799 lr 0.00049244 rank 4
2023-02-21 00:54:01,554 DEBUG TRAIN Batch 12/2500 loss 12.556027 loss_att 31.554087 loss_ctc 13.721029 loss_rnnt 8.474565 hw_loss 0.237218 lr 0.00049234 rank 2
2023-02-21 00:55:23,786 DEBUG TRAIN Batch 12/2600 loss 20.484558 loss_att 17.943691 loss_ctc 23.694855 loss_rnnt 20.254997 hw_loss 0.580676 lr 0.00049222 rank 2
2023-02-21 00:55:23,788 DEBUG TRAIN Batch 12/2600 loss 11.120873 loss_att 9.970594 loss_ctc 12.430949 loss_rnnt 10.960574 hw_loss 0.404396 lr 0.00049231 rank 0
2023-02-21 00:55:23,791 DEBUG TRAIN Batch 12/2600 loss 4.223991 loss_att 8.317692 loss_ctc 5.013401 loss_rnnt 3.126600 hw_loss 0.325119 lr 0.00049220 rank 3
2023-02-21 00:55:23,794 DEBUG TRAIN Batch 12/2600 loss 12.253101 loss_att 23.485212 loss_ctc 12.089349 loss_rnnt 9.800341 hw_loss 0.427821 lr 0.00049221 rank 5
2023-02-21 00:55:23,802 DEBUG TRAIN Batch 12/2600 loss 14.251507 loss_att 18.042076 loss_ctc 19.877266 loss_rnnt 12.593124 hw_loss 0.281565 lr 0.00049233 rank 4
2023-02-21 00:55:23,825 DEBUG TRAIN Batch 12/2600 loss 14.402827 loss_att 12.896401 loss_ctc 15.665068 loss_rnnt 14.436122 hw_loss 0.186923 lr 0.00049220 rank 6
2023-02-21 00:55:23,826 DEBUG TRAIN Batch 12/2600 loss 9.483171 loss_att 13.405691 loss_ctc 10.655704 loss_rnnt 8.311916 hw_loss 0.432022 lr 0.00049227 rank 1
2023-02-21 00:55:23,860 DEBUG TRAIN Batch 12/2600 loss 11.296383 loss_att 14.878254 loss_ctc 10.578497 loss_rnnt 10.559413 hw_loss 0.218088 lr 0.00049218 rank 7
2023-02-21 00:56:23,351 DEBUG TRAIN Batch 12/2700 loss 12.519828 loss_att 18.230618 loss_ctc 18.683516 loss_rnnt 10.382947 hw_loss 0.324182 lr 0.00049219 rank 0
2023-02-21 00:56:23,356 DEBUG TRAIN Batch 12/2700 loss 11.187239 loss_att 16.951797 loss_ctc 10.929457 loss_rnnt 10.068128 hw_loss 0.001069 lr 0.00049210 rank 2
2023-02-21 00:56:23,362 DEBUG TRAIN Batch 12/2700 loss 10.499484 loss_att 16.229023 loss_ctc 14.021741 loss_rnnt 8.808896 hw_loss 0.140710 lr 0.00049208 rank 3
2023-02-21 00:56:23,363 DEBUG TRAIN Batch 12/2700 loss 23.720285 loss_att 27.634142 loss_ctc 30.057753 loss_rnnt 21.913597 hw_loss 0.335479 lr 0.00049208 rank 6
2023-02-21 00:56:23,365 DEBUG TRAIN Batch 12/2700 loss 16.180910 loss_att 17.529266 loss_ctc 20.843987 loss_rnnt 15.164700 hw_loss 0.233989 lr 0.00049221 rank 4
2023-02-21 00:56:23,369 DEBUG TRAIN Batch 12/2700 loss 29.996868 loss_att 34.257969 loss_ctc 35.540615 loss_rnnt 28.324669 hw_loss 0.151521 lr 0.00049206 rank 7
2023-02-21 00:56:23,377 DEBUG TRAIN Batch 12/2700 loss 18.653114 loss_att 20.219553 loss_ctc 17.786396 loss_rnnt 18.384521 hw_loss 0.132877 lr 0.00049215 rank 1
2023-02-21 00:56:23,419 DEBUG TRAIN Batch 12/2700 loss 12.326716 loss_att 18.014896 loss_ctc 13.232310 loss_rnnt 10.991127 hw_loss 0.144765 lr 0.00049209 rank 5
2023-02-21 00:57:22,588 DEBUG TRAIN Batch 12/2800 loss 15.312462 loss_att 20.081932 loss_ctc 25.047302 loss_rnnt 12.886650 hw_loss 0.326134 lr 0.00049198 rank 2
2023-02-21 00:57:22,589 DEBUG TRAIN Batch 12/2800 loss 1.754161 loss_att 4.732079 loss_ctc 0.367959 loss_rnnt 1.228744 hw_loss 0.214988 lr 0.00049207 rank 0
2023-02-21 00:57:22,591 DEBUG TRAIN Batch 12/2800 loss 6.906639 loss_att 12.711257 loss_ctc 4.579682 loss_rnnt 5.871244 hw_loss 0.346373 lr 0.00049203 rank 1
2023-02-21 00:57:22,590 DEBUG TRAIN Batch 12/2800 loss 11.044348 loss_att 21.111895 loss_ctc 17.626854 loss_rnnt 7.999818 hw_loss 0.287537 lr 0.00049196 rank 3
2023-02-21 00:57:22,593 DEBUG TRAIN Batch 12/2800 loss 19.562902 loss_att 24.596214 loss_ctc 30.010448 loss_rnnt 17.162937 hw_loss 0.000551 lr 0.00049209 rank 4
2023-02-21 00:57:22,594 DEBUG TRAIN Batch 12/2800 loss 31.183046 loss_att 45.082443 loss_ctc 42.280052 loss_rnnt 26.863005 hw_loss 0.113551 lr 0.00049194 rank 7
2023-02-21 00:57:22,596 DEBUG TRAIN Batch 12/2800 loss 5.595472 loss_att 12.823568 loss_ctc 4.625989 loss_rnnt 4.194600 hw_loss 0.158470 lr 0.00049197 rank 5
2023-02-21 00:57:22,654 DEBUG TRAIN Batch 12/2800 loss 1.899356 loss_att 6.798303 loss_ctc 3.992709 loss_rnnt 0.567086 hw_loss 0.137563 lr 0.00049197 rank 6
2023-02-21 00:58:19,915 DEBUG TRAIN Batch 12/2900 loss 24.748232 loss_att 23.813881 loss_ctc 26.850471 loss_rnnt 24.459225 hw_loss 0.366712 lr 0.00049195 rank 0
2023-02-21 00:58:19,916 DEBUG TRAIN Batch 12/2900 loss 20.769611 loss_att 22.483595 loss_ctc 21.544802 loss_rnnt 20.029654 hw_loss 0.550882 lr 0.00049191 rank 1
2023-02-21 00:58:19,916 DEBUG TRAIN Batch 12/2900 loss 16.636414 loss_att 16.852007 loss_ctc 18.622978 loss_rnnt 16.223118 hw_loss 0.197438 lr 0.00049185 rank 5
2023-02-21 00:58:19,924 DEBUG TRAIN Batch 12/2900 loss 19.489071 loss_att 21.843231 loss_ctc 26.303043 loss_rnnt 17.969282 hw_loss 0.263300 lr 0.00049184 rank 3
2023-02-21 00:58:19,926 DEBUG TRAIN Batch 12/2900 loss 15.432569 loss_att 14.254810 loss_ctc 29.030697 loss_rnnt 13.664518 hw_loss 0.357222 lr 0.00049182 rank 7
2023-02-21 00:58:19,926 DEBUG TRAIN Batch 12/2900 loss 25.606096 loss_att 24.105604 loss_ctc 24.310417 loss_rnnt 26.078716 hw_loss 0.000442 lr 0.00049185 rank 6
2023-02-21 00:58:19,929 DEBUG TRAIN Batch 12/2900 loss 15.379803 loss_att 17.317785 loss_ctc 20.728834 loss_rnnt 14.112846 hw_loss 0.311542 lr 0.00049187 rank 2
2023-02-21 00:58:19,989 DEBUG TRAIN Batch 12/2900 loss 30.561144 loss_att 28.718212 loss_ctc 35.151680 loss_rnnt 30.080938 hw_loss 0.443853 lr 0.00049197 rank 4
2023-02-21 00:59:18,648 DEBUG TRAIN Batch 12/3000 loss 15.637633 loss_att 20.559710 loss_ctc 23.235344 loss_rnnt 13.432514 hw_loss 0.389390 lr 0.00049183 rank 0
2023-02-21 00:59:18,661 DEBUG TRAIN Batch 12/3000 loss 13.682502 loss_att 21.296961 loss_ctc 14.691006 loss_rnnt 11.889972 hw_loss 0.253445 lr 0.00049172 rank 3
2023-02-21 00:59:18,662 DEBUG TRAIN Batch 12/3000 loss 14.909138 loss_att 21.486101 loss_ctc 17.646400 loss_rnnt 12.996763 hw_loss 0.435026 lr 0.00049173 rank 5
2023-02-21 00:59:18,664 DEBUG TRAIN Batch 12/3000 loss 22.249609 loss_att 31.640999 loss_ctc 27.779966 loss_rnnt 19.513973 hw_loss 0.224956 lr 0.00049179 rank 1
2023-02-21 00:59:18,664 DEBUG TRAIN Batch 12/3000 loss 15.078507 loss_att 22.862160 loss_ctc 14.557291 loss_rnnt 13.412979 hw_loss 0.334299 lr 0.00049175 rank 2
2023-02-21 00:59:18,665 DEBUG TRAIN Batch 12/3000 loss 21.726461 loss_att 31.599100 loss_ctc 24.680794 loss_rnnt 19.087242 hw_loss 0.507714 lr 0.00049173 rank 6
2023-02-21 00:59:18,669 DEBUG TRAIN Batch 12/3000 loss 9.102431 loss_att 10.278506 loss_ctc 7.623759 loss_rnnt 8.980801 hw_loss 0.156698 lr 0.00049185 rank 4
2023-02-21 00:59:18,728 DEBUG TRAIN Batch 12/3000 loss 7.073936 loss_att 12.568797 loss_ctc 7.968795 loss_rnnt 5.791829 hw_loss 0.119663 lr 0.00049170 rank 7
2023-02-21 01:00:16,731 DEBUG TRAIN Batch 12/3100 loss 9.562109 loss_att 20.291525 loss_ctc 8.433105 loss_rnnt 7.418075 hw_loss 0.278785 lr 0.00049171 rank 0
2023-02-21 01:00:16,735 DEBUG TRAIN Batch 12/3100 loss 5.541867 loss_att 8.847980 loss_ctc 4.697871 loss_rnnt 4.758709 hw_loss 0.439628 lr 0.00049163 rank 2
2023-02-21 01:00:16,740 DEBUG TRAIN Batch 12/3100 loss 23.727055 loss_att 38.700851 loss_ctc 36.885197 loss_rnnt 18.636448 hw_loss 0.640177 lr 0.00049161 rank 6
2023-02-21 01:00:16,742 DEBUG TRAIN Batch 12/3100 loss 11.445716 loss_att 14.412730 loss_ctc 11.780966 loss_rnnt 10.807025 hw_loss 0.001104 lr 0.00049160 rank 3
2023-02-21 01:00:16,742 DEBUG TRAIN Batch 12/3100 loss 2.520502 loss_att 6.352276 loss_ctc 1.738660 loss_rnnt 1.763935 hw_loss 0.177110 lr 0.00049158 rank 7
2023-02-21 01:00:16,743 DEBUG TRAIN Batch 12/3100 loss 9.279528 loss_att 18.725288 loss_ctc 8.625768 loss_rnnt 7.421244 hw_loss 0.105562 lr 0.00049173 rank 4
2023-02-21 01:00:16,745 DEBUG TRAIN Batch 12/3100 loss 4.407708 loss_att 11.044963 loss_ctc 5.580837 loss_rnnt 2.773664 hw_loss 0.281578 lr 0.00049161 rank 5
2023-02-21 01:00:16,748 DEBUG TRAIN Batch 12/3100 loss 9.877475 loss_att 17.514912 loss_ctc 14.120691 loss_rnnt 7.546811 hw_loss 0.445151 lr 0.00049168 rank 1
2023-02-21 01:01:14,299 DEBUG TRAIN Batch 12/3200 loss 9.218315 loss_att 12.125098 loss_ctc 9.643514 loss_rnnt 8.418206 hw_loss 0.303861 lr 0.00049156 rank 1
2023-02-21 01:01:14,303 DEBUG TRAIN Batch 12/3200 loss 4.008763 loss_att 6.607599 loss_ctc 4.637486 loss_rnnt 3.315723 hw_loss 0.167706 lr 0.00049149 rank 3
2023-02-21 01:01:14,304 DEBUG TRAIN Batch 12/3200 loss 17.960995 loss_att 21.290537 loss_ctc 29.129707 loss_rnnt 15.721203 hw_loss 0.158853 lr 0.00049159 rank 0
2023-02-21 01:01:14,306 DEBUG TRAIN Batch 12/3200 loss 18.866388 loss_att 17.043453 loss_ctc 22.479383 loss_rnnt 18.526592 hw_loss 0.417467 lr 0.00049149 rank 5
2023-02-21 01:01:14,308 DEBUG TRAIN Batch 12/3200 loss 12.793315 loss_att 15.802042 loss_ctc 21.154682 loss_rnnt 10.913989 hw_loss 0.305121 lr 0.00049151 rank 2
2023-02-21 01:01:14,310 DEBUG TRAIN Batch 12/3200 loss 20.013548 loss_att 25.288330 loss_ctc 25.283970 loss_rnnt 18.155476 hw_loss 0.188234 lr 0.00049149 rank 6
2023-02-21 01:01:14,310 DEBUG TRAIN Batch 12/3200 loss 11.606988 loss_att 14.468590 loss_ctc 16.614716 loss_rnnt 10.210358 hw_loss 0.293648 lr 0.00049146 rank 7
2023-02-21 01:01:14,317 DEBUG TRAIN Batch 12/3200 loss 27.108385 loss_att 29.253235 loss_ctc 31.400074 loss_rnnt 25.837631 hw_loss 0.505423 lr 0.00049161 rank 4
2023-02-21 01:02:13,782 DEBUG TRAIN Batch 12/3300 loss 22.717669 loss_att 27.886623 loss_ctc 32.214054 loss_rnnt 20.334045 hw_loss 0.156836 lr 0.00049148 rank 0
2023-02-21 01:02:13,782 DEBUG TRAIN Batch 12/3300 loss 5.172009 loss_att 9.723158 loss_ctc 7.063337 loss_rnnt 3.878368 hw_loss 0.246062 lr 0.00049137 rank 3
2023-02-21 01:02:13,784 DEBUG TRAIN Batch 12/3300 loss 16.626488 loss_att 23.122192 loss_ctc 22.069149 loss_rnnt 14.434866 hw_loss 0.312739 lr 0.00049139 rank 2
2023-02-21 01:02:13,790 DEBUG TRAIN Batch 12/3300 loss 15.886445 loss_att 24.675198 loss_ctc 22.599154 loss_rnnt 13.022884 hw_loss 0.395217 lr 0.00049137 rank 6
2023-02-21 01:02:13,793 DEBUG TRAIN Batch 12/3300 loss 16.034107 loss_att 19.609377 loss_ctc 20.504412 loss_rnnt 14.528645 hw_loss 0.364442 lr 0.00049144 rank 1
2023-02-21 01:02:13,794 DEBUG TRAIN Batch 12/3300 loss 12.806518 loss_att 18.285263 loss_ctc 19.514393 loss_rnnt 10.672029 hw_loss 0.270668 lr 0.00049138 rank 5
2023-02-21 01:02:13,793 DEBUG TRAIN Batch 12/3300 loss 13.387451 loss_att 12.137501 loss_ctc 16.901535 loss_rnnt 13.025954 hw_loss 0.268018 lr 0.00049149 rank 4
2023-02-21 01:02:13,798 DEBUG TRAIN Batch 12/3300 loss 23.288670 loss_att 38.991402 loss_ctc 25.106604 loss_rnnt 19.732559 hw_loss 0.324697 lr 0.00049135 rank 7
2023-02-21 01:03:13,171 DEBUG TRAIN Batch 12/3400 loss 18.403137 loss_att 26.962215 loss_ctc 19.759148 loss_rnnt 16.460499 hw_loss 0.093789 lr 0.00049136 rank 0
2023-02-21 01:03:13,184 DEBUG TRAIN Batch 12/3400 loss 20.910475 loss_att 23.583118 loss_ctc 20.604952 loss_rnnt 20.416468 hw_loss 0.000399 lr 0.00049127 rank 2
2023-02-21 01:03:13,191 DEBUG TRAIN Batch 12/3400 loss 53.409584 loss_att 54.360901 loss_ctc 53.465195 loss_rnnt 53.135628 hw_loss 0.143015 lr 0.00049125 rank 3
2023-02-21 01:03:13,191 DEBUG TRAIN Batch 12/3400 loss 4.287266 loss_att 10.278315 loss_ctc 3.617567 loss_rnnt 3.027582 hw_loss 0.282689 lr 0.00049125 rank 6
2023-02-21 01:03:13,202 DEBUG TRAIN Batch 12/3400 loss 28.538389 loss_att 29.114658 loss_ctc 31.907475 loss_rnnt 27.973267 hw_loss 0.001228 lr 0.00049132 rank 1
2023-02-21 01:03:13,203 DEBUG TRAIN Batch 12/3400 loss 8.649212 loss_att 14.273960 loss_ctc 17.302696 loss_rnnt 6.370210 hw_loss 0.000476 lr 0.00049137 rank 4
2023-02-21 01:03:13,232 DEBUG TRAIN Batch 12/3400 loss 17.529680 loss_att 23.596560 loss_ctc 27.559061 loss_rnnt 14.857447 hw_loss 0.228009 lr 0.00049126 rank 5
2023-02-21 01:03:13,235 DEBUG TRAIN Batch 12/3400 loss 6.332875 loss_att 12.556482 loss_ctc 8.356352 loss_rnnt 4.693347 hw_loss 0.234394 lr 0.00049123 rank 7
2023-02-21 01:04:32,804 DEBUG TRAIN Batch 12/3500 loss 14.651845 loss_att 16.515409 loss_ctc 17.043058 loss_rnnt 13.799479 hw_loss 0.301545 lr 0.00049124 rank 0
2023-02-21 01:04:32,809 DEBUG TRAIN Batch 12/3500 loss 25.486784 loss_att 25.683222 loss_ctc 30.101345 loss_rnnt 24.714861 hw_loss 0.220051 lr 0.00049120 rank 1
2023-02-21 01:04:32,809 DEBUG TRAIN Batch 12/3500 loss 30.524366 loss_att 31.873760 loss_ctc 39.101009 loss_rnnt 28.928402 hw_loss 0.342251 lr 0.00049115 rank 2
2023-02-21 01:04:32,811 DEBUG TRAIN Batch 12/3500 loss 6.320426 loss_att 8.863017 loss_ctc 9.433248 loss_rnnt 5.329293 hw_loss 0.126700 lr 0.00049113 rank 6
2023-02-21 01:04:32,815 DEBUG TRAIN Batch 12/3500 loss 2.621554 loss_att 4.849202 loss_ctc 3.450622 loss_rnnt 1.855658 hw_loss 0.393419 lr 0.00049114 rank 5
2023-02-21 01:04:32,818 DEBUG TRAIN Batch 12/3500 loss 6.595005 loss_att 10.371983 loss_ctc 7.171112 loss_rnnt 5.642028 hw_loss 0.226439 lr 0.00049125 rank 4
2023-02-21 01:04:32,818 DEBUG TRAIN Batch 12/3500 loss 8.136865 loss_att 9.719242 loss_ctc 15.944400 loss_rnnt 6.628787 hw_loss 0.282371 lr 0.00049113 rank 3
2023-02-21 01:04:32,877 DEBUG TRAIN Batch 12/3500 loss 12.983870 loss_att 19.049713 loss_ctc 15.668489 loss_rnnt 11.256155 hw_loss 0.293618 lr 0.00049111 rank 7
2023-02-21 01:05:33,942 DEBUG TRAIN Batch 12/3600 loss 3.565025 loss_att 7.180779 loss_ctc 7.219755 loss_rnnt 2.353502 hw_loss 0.002015 lr 0.00049112 rank 0
2023-02-21 01:05:33,947 DEBUG TRAIN Batch 12/3600 loss 5.654360 loss_att 6.935427 loss_ctc 3.791861 loss_rnnt 5.548449 hw_loss 0.183806 lr 0.00049102 rank 6
2023-02-21 01:05:33,952 DEBUG TRAIN Batch 12/3600 loss 21.628433 loss_att 22.083923 loss_ctc 27.807180 loss_rnnt 20.638561 hw_loss 0.140513 lr 0.00049101 rank 3
2023-02-21 01:05:33,959 DEBUG TRAIN Batch 12/3600 loss 13.761638 loss_att 16.191277 loss_ctc 17.364004 loss_rnnt 12.619934 hw_loss 0.328986 lr 0.00049102 rank 5
2023-02-21 01:05:33,959 DEBUG TRAIN Batch 12/3600 loss 22.724163 loss_att 30.850517 loss_ctc 29.753342 loss_rnnt 20.046066 hw_loss 0.216749 lr 0.00049103 rank 2
2023-02-21 01:05:33,962 DEBUG TRAIN Batch 12/3600 loss 13.964771 loss_att 18.754606 loss_ctc 19.361080 loss_rnnt 12.195225 hw_loss 0.172634 lr 0.00049114 rank 4
2023-02-21 01:05:33,981 DEBUG TRAIN Batch 12/3600 loss 9.197825 loss_att 17.052233 loss_ctc 10.193601 loss_rnnt 7.493032 hw_loss 0.002140 lr 0.00049108 rank 1
2023-02-21 01:05:33,981 DEBUG TRAIN Batch 12/3600 loss 3.258790 loss_att 3.788934 loss_ctc 0.975652 loss_rnnt 3.195455 hw_loss 0.490735 lr 0.00049099 rank 7
2023-02-21 01:06:31,034 DEBUG TRAIN Batch 12/3700 loss 5.828648 loss_att 8.189543 loss_ctc 7.933231 loss_rnnt 4.970567 hw_loss 0.197420 lr 0.00049102 rank 4
2023-02-21 01:06:31,036 DEBUG TRAIN Batch 12/3700 loss 29.022028 loss_att 30.203651 loss_ctc 23.713428 loss_rnnt 29.313864 hw_loss 0.336848 lr 0.00049100 rank 0
2023-02-21 01:06:31,038 DEBUG TRAIN Batch 12/3700 loss 15.977747 loss_att 15.134559 loss_ctc 12.938287 loss_rnnt 16.463676 hw_loss 0.164942 lr 0.00049089 rank 3
2023-02-21 01:06:31,041 DEBUG TRAIN Batch 12/3700 loss 15.415305 loss_att 13.425733 loss_ctc 13.399566 loss_rnnt 16.081837 hw_loss 0.000279 lr 0.00049092 rank 2
2023-02-21 01:06:31,041 DEBUG TRAIN Batch 12/3700 loss 7.293068 loss_att 14.435678 loss_ctc 8.214818 loss_rnnt 5.573471 hw_loss 0.315331 lr 0.00049087 rank 7
2023-02-21 01:06:31,046 DEBUG TRAIN Batch 12/3700 loss 20.682322 loss_att 25.311550 loss_ctc 30.506084 loss_rnnt 18.240799 hw_loss 0.385958 lr 0.00049096 rank 1
2023-02-21 01:06:31,046 DEBUG TRAIN Batch 12/3700 loss 8.769395 loss_att 10.611471 loss_ctc 14.811906 loss_rnnt 7.373686 hw_loss 0.415547 lr 0.00049090 rank 6
2023-02-21 01:06:31,049 DEBUG TRAIN Batch 12/3700 loss 14.536423 loss_att 15.378634 loss_ctc 12.595942 loss_rnnt 14.383011 hw_loss 0.456938 lr 0.00049090 rank 5
2023-02-21 01:07:27,697 DEBUG TRAIN Batch 12/3800 loss 43.644379 loss_att 46.598145 loss_ctc 50.811859 loss_rnnt 41.964314 hw_loss 0.250586 lr 0.00049088 rank 0
2023-02-21 01:07:27,703 DEBUG TRAIN Batch 12/3800 loss 26.060009 loss_att 31.637936 loss_ctc 34.440361 loss_rnnt 23.609179 hw_loss 0.408498 lr 0.00049078 rank 6
2023-02-21 01:07:27,705 DEBUG TRAIN Batch 12/3800 loss 8.632584 loss_att 13.119151 loss_ctc 8.033723 loss_rnnt 7.634789 hw_loss 0.338115 lr 0.00049085 rank 1
2023-02-21 01:07:27,706 DEBUG TRAIN Batch 12/3800 loss 43.552296 loss_att 44.857689 loss_ctc 55.961430 loss_rnnt 41.576424 hw_loss 0.112952 lr 0.00049078 rank 5
2023-02-21 01:07:27,706 DEBUG TRAIN Batch 12/3800 loss 18.518208 loss_att 24.645039 loss_ctc 22.866390 loss_rnnt 16.572720 hw_loss 0.263182 lr 0.00049077 rank 3
2023-02-21 01:07:27,706 DEBUG TRAIN Batch 12/3800 loss 8.077273 loss_att 14.257172 loss_ctc 8.229443 loss_rnnt 6.758395 hw_loss 0.117393 lr 0.00049075 rank 7
2023-02-21 01:07:27,708 DEBUG TRAIN Batch 12/3800 loss 25.646473 loss_att 33.975140 loss_ctc 27.723412 loss_rnnt 23.587940 hw_loss 0.217259 lr 0.00049080 rank 2
2023-02-21 01:07:27,767 DEBUG TRAIN Batch 12/3800 loss 13.268068 loss_att 16.132595 loss_ctc 14.896591 loss_rnnt 12.252934 hw_loss 0.422049 lr 0.00049090 rank 4
2023-02-21 01:08:28,689 DEBUG TRAIN Batch 12/3900 loss 8.941443 loss_att 11.823160 loss_ctc 17.844101 loss_rnnt 7.122810 hw_loss 0.103628 lr 0.00049076 rank 0
2023-02-21 01:08:28,692 DEBUG TRAIN Batch 12/3900 loss 21.592066 loss_att 27.003408 loss_ctc 31.526115 loss_rnnt 19.002909 hw_loss 0.341908 lr 0.00049067 rank 5
2023-02-21 01:08:28,696 DEBUG TRAIN Batch 12/3900 loss 14.825987 loss_att 18.905067 loss_ctc 17.831985 loss_rnnt 13.609049 hw_loss 0.000602 lr 0.00049066 rank 6
2023-02-21 01:08:28,697 DEBUG TRAIN Batch 12/3900 loss 31.633839 loss_att 36.579815 loss_ctc 41.797131 loss_rnnt 29.130611 hw_loss 0.297984 lr 0.00049066 rank 3
2023-02-21 01:08:28,700 DEBUG TRAIN Batch 12/3900 loss 29.197758 loss_att 38.418602 loss_ctc 38.450336 loss_rnnt 26.119736 hw_loss 0.000329 lr 0.00049068 rank 2
2023-02-21 01:08:28,700 DEBUG TRAIN Batch 12/3900 loss 8.019161 loss_att 12.087166 loss_ctc 8.339045 loss_rnnt 6.879616 hw_loss 0.531176 lr 0.00049078 rank 4
2023-02-21 01:08:28,703 DEBUG TRAIN Batch 12/3900 loss 8.086763 loss_att 11.889263 loss_ctc 14.491410 loss_rnnt 6.472047 hw_loss 0.000493 lr 0.00049073 rank 1
2023-02-21 01:08:28,707 DEBUG TRAIN Batch 12/3900 loss 9.423693 loss_att 21.012520 loss_ctc 9.506879 loss_rnnt 6.947565 hw_loss 0.276133 lr 0.00049063 rank 7
2023-02-21 01:09:24,984 DEBUG TRAIN Batch 12/4000 loss 11.695333 loss_att 18.626291 loss_ctc 12.383621 loss_rnnt 10.215423 hw_loss 0.003653 lr 0.00049065 rank 0
2023-02-21 01:09:24,990 DEBUG TRAIN Batch 12/4000 loss 26.339870 loss_att 31.313190 loss_ctc 33.062984 loss_rnnt 24.362125 hw_loss 0.162496 lr 0.00049066 rank 4
2023-02-21 01:09:24,992 DEBUG TRAIN Batch 12/4000 loss 15.588727 loss_att 22.974899 loss_ctc 19.259888 loss_rnnt 13.516223 hw_loss 0.198341 lr 0.00049056 rank 2
2023-02-21 01:09:24,993 DEBUG TRAIN Batch 12/4000 loss 16.859169 loss_att 22.838650 loss_ctc 20.535759 loss_rnnt 15.169762 hw_loss 0.006183 lr 0.00049054 rank 3
2023-02-21 01:09:24,996 DEBUG TRAIN Batch 12/4000 loss 18.456625 loss_att 18.328081 loss_ctc 18.442194 loss_rnnt 18.314953 hw_loss 0.317444 lr 0.00049052 rank 7
2023-02-21 01:09:25,001 DEBUG TRAIN Batch 12/4000 loss 11.349791 loss_att 15.887207 loss_ctc 14.130370 loss_rnnt 10.070207 hw_loss 0.002542 lr 0.00049054 rank 6
2023-02-21 01:09:25,004 DEBUG TRAIN Batch 12/4000 loss 3.435574 loss_att 6.398806 loss_ctc 2.123433 loss_rnnt 2.850139 hw_loss 0.314513 lr 0.00049061 rank 1
2023-02-21 01:09:25,049 DEBUG TRAIN Batch 12/4000 loss 19.660124 loss_att 26.186857 loss_ctc 13.693584 loss_rnnt 19.004389 hw_loss 0.273614 lr 0.00049055 rank 5
2023-02-21 01:10:22,391 DEBUG TRAIN Batch 12/4100 loss 21.193720 loss_att 26.822206 loss_ctc 27.344494 loss_rnnt 19.213634 hw_loss 0.064283 lr 0.00049053 rank 0
2023-02-21 01:10:22,397 DEBUG TRAIN Batch 12/4100 loss 12.305058 loss_att 16.732990 loss_ctc 14.798613 loss_rnnt 10.974427 hw_loss 0.211066 lr 0.00049044 rank 2
2023-02-21 01:10:22,409 DEBUG TRAIN Batch 12/4100 loss 6.133828 loss_att 7.694411 loss_ctc 8.718823 loss_rnnt 5.328161 hw_loss 0.279156 lr 0.00049042 rank 3
2023-02-21 01:10:22,409 DEBUG TRAIN Batch 12/4100 loss 13.996215 loss_att 15.815310 loss_ctc 18.404799 loss_rnnt 12.920216 hw_loss 0.233193 lr 0.00049055 rank 4
2023-02-21 01:10:22,410 DEBUG TRAIN Batch 12/4100 loss 11.764907 loss_att 14.234653 loss_ctc 16.688145 loss_rnnt 10.419184 hw_loss 0.366266 lr 0.00049049 rank 1
2023-02-21 01:10:22,415 DEBUG TRAIN Batch 12/4100 loss 25.498039 loss_att 28.137711 loss_ctc 36.081039 loss_rnnt 23.473087 hw_loss 0.161152 lr 0.00049040 rank 7
2023-02-21 01:10:22,415 DEBUG TRAIN Batch 12/4100 loss 26.870977 loss_att 29.476490 loss_ctc 36.421238 loss_rnnt 24.828012 hw_loss 0.465925 lr 0.00049042 rank 6
2023-02-21 01:10:22,465 DEBUG TRAIN Batch 12/4100 loss 21.411316 loss_att 19.314297 loss_ctc 17.827705 loss_rnnt 22.136244 hw_loss 0.323048 lr 0.00049043 rank 5
2023-02-21 01:11:23,287 DEBUG TRAIN Batch 12/4200 loss 33.375679 loss_att 32.896202 loss_ctc 44.601883 loss_rnnt 31.966612 hw_loss 0.015256 lr 0.00049041 rank 0
2023-02-21 01:11:23,295 DEBUG TRAIN Batch 12/4200 loss 47.563503 loss_att 56.836334 loss_ctc 47.499958 loss_rnnt 45.481049 hw_loss 0.443173 lr 0.00049037 rank 1
2023-02-21 01:11:23,296 DEBUG TRAIN Batch 12/4200 loss 4.191617 loss_att 7.167665 loss_ctc 10.408393 loss_rnnt 2.602357 hw_loss 0.309650 lr 0.00049031 rank 6
2023-02-21 01:11:23,297 DEBUG TRAIN Batch 12/4200 loss 1.326953 loss_att 6.450515 loss_ctc 0.983109 loss_rnnt 0.136018 hw_loss 0.397628 lr 0.00049033 rank 2
2023-02-21 01:11:23,299 DEBUG TRAIN Batch 12/4200 loss 3.190401 loss_att 6.768489 loss_ctc 3.925769 loss_rnnt 2.165745 hw_loss 0.395604 lr 0.00049031 rank 5
2023-02-21 01:11:23,300 DEBUG TRAIN Batch 12/4200 loss 23.441395 loss_att 28.625628 loss_ctc 32.331390 loss_rnnt 21.044674 hw_loss 0.327268 lr 0.00049030 rank 3
2023-02-21 01:11:23,305 DEBUG TRAIN Batch 12/4200 loss 15.778518 loss_att 16.523642 loss_ctc 13.626641 loss_rnnt 15.719961 hw_loss 0.368343 lr 0.00049028 rank 7
2023-02-21 01:11:23,312 DEBUG TRAIN Batch 12/4200 loss 7.079391 loss_att 11.885287 loss_ctc 4.326774 loss_rnnt 6.479669 hw_loss 0.010423 lr 0.00049043 rank 4
2023-02-21 01:12:46,523 DEBUG TRAIN Batch 12/4300 loss 18.726255 loss_att 18.570461 loss_ctc 16.096712 loss_rnnt 18.922531 hw_loss 0.347793 lr 0.00049029 rank 0
2023-02-21 01:12:46,527 DEBUG TRAIN Batch 12/4300 loss 8.542928 loss_att 16.422451 loss_ctc 6.029267 loss_rnnt 7.302030 hw_loss 0.000276 lr 0.00049018 rank 3
2023-02-21 01:12:46,530 DEBUG TRAIN Batch 12/4300 loss 16.841328 loss_att 21.275494 loss_ctc 19.935242 loss_rnnt 15.541821 hw_loss 0.000286 lr 0.00049019 rank 5
2023-02-21 01:12:46,533 DEBUG TRAIN Batch 12/4300 loss 18.990669 loss_att 27.424732 loss_ctc 16.147369 loss_rnnt 17.435820 hw_loss 0.463396 lr 0.00049031 rank 4
2023-02-21 01:12:46,536 DEBUG TRAIN Batch 12/4300 loss 9.162698 loss_att 12.166994 loss_ctc 12.777012 loss_rnnt 7.801781 hw_loss 0.521528 lr 0.00049016 rank 7
2023-02-21 01:12:46,536 DEBUG TRAIN Batch 12/4300 loss 3.536555 loss_att 5.980769 loss_ctc 2.440858 loss_rnnt 3.094134 hw_loss 0.186883 lr 0.00049025 rank 1
2023-02-21 01:12:46,536 DEBUG TRAIN Batch 12/4300 loss 7.204916 loss_att 12.595558 loss_ctc 8.064248 loss_rnnt 5.861028 hw_loss 0.283466 lr 0.00049021 rank 2
2023-02-21 01:12:46,543 DEBUG TRAIN Batch 12/4300 loss 12.571706 loss_att 12.697268 loss_ctc 13.621778 loss_rnnt 12.270999 hw_loss 0.254221 lr 0.00049019 rank 6
2023-02-21 01:13:45,651 DEBUG TRAIN Batch 12/4400 loss 10.227571 loss_att 13.482775 loss_ctc 15.549462 loss_rnnt 8.719167 hw_loss 0.277086 lr 0.00049008 rank 5
2023-02-21 01:13:45,651 DEBUG TRAIN Batch 12/4400 loss 28.598511 loss_att 37.220009 loss_ctc 31.235935 loss_rnnt 26.209108 hw_loss 0.587705 lr 0.00049014 rank 1
2023-02-21 01:13:45,654 DEBUG TRAIN Batch 12/4400 loss 21.907526 loss_att 23.402885 loss_ctc 21.557529 loss_rnnt 21.654877 hw_loss 0.000455 lr 0.00049007 rank 6
2023-02-21 01:13:45,655 DEBUG TRAIN Batch 12/4400 loss 16.329967 loss_att 20.289036 loss_ctc 25.864649 loss_rnnt 14.095430 hw_loss 0.321435 lr 0.00049019 rank 4
2023-02-21 01:13:45,657 DEBUG TRAIN Batch 12/4400 loss 19.232206 loss_att 21.364269 loss_ctc 29.230467 loss_rnnt 17.345459 hw_loss 0.238559 lr 0.00049009 rank 2
2023-02-21 01:13:45,659 DEBUG TRAIN Batch 12/4400 loss 25.294336 loss_att 26.352852 loss_ctc 29.853918 loss_rnnt 24.365873 hw_loss 0.204028 lr 0.00049017 rank 0
2023-02-21 01:13:45,669 DEBUG TRAIN Batch 12/4400 loss 7.683751 loss_att 14.515877 loss_ctc 7.508711 loss_rnnt 6.340443 hw_loss 0.000414 lr 0.00049005 rank 7
2023-02-21 01:13:45,674 DEBUG TRAIN Batch 12/4400 loss 18.063749 loss_att 20.244335 loss_ctc 20.196507 loss_rnnt 17.221014 hw_loss 0.229221 lr 0.00049007 rank 3
2023-02-21 01:14:44,373 DEBUG TRAIN Batch 12/4500 loss 12.064910 loss_att 12.345631 loss_ctc 13.287083 loss_rnnt 11.667302 hw_loss 0.334699 lr 0.00049006 rank 0
2023-02-21 01:14:44,381 DEBUG TRAIN Batch 12/4500 loss 12.453913 loss_att 18.870895 loss_ctc 22.531242 loss_rnnt 9.826725 hw_loss 0.000276 lr 0.00048995 rank 6
2023-02-21 01:14:44,383 DEBUG TRAIN Batch 12/4500 loss 20.809862 loss_att 25.450409 loss_ctc 18.372780 loss_rnnt 20.113857 hw_loss 0.174076 lr 0.00048997 rank 2
2023-02-21 01:14:44,384 DEBUG TRAIN Batch 12/4500 loss 9.661139 loss_att 12.190723 loss_ctc 9.305144 loss_rnnt 9.202425 hw_loss 0.000492 lr 0.00049007 rank 4
2023-02-21 01:14:44,386 DEBUG TRAIN Batch 12/4500 loss 13.816915 loss_att 21.439764 loss_ctc 18.016518 loss_rnnt 11.732237 hw_loss 0.000303 lr 0.00048996 rank 5
2023-02-21 01:14:44,388 DEBUG TRAIN Batch 12/4500 loss 0.873669 loss_att 3.784801 loss_ctc 0.363386 loss_rnnt 0.221793 hw_loss 0.258164 lr 0.00048995 rank 3
2023-02-21 01:14:44,391 DEBUG TRAIN Batch 12/4500 loss 15.120296 loss_att 11.945694 loss_ctc 12.581356 loss_rnnt 16.093542 hw_loss 0.000376 lr 0.00049002 rank 1
2023-02-21 01:14:44,402 DEBUG TRAIN Batch 12/4500 loss 25.445280 loss_att 27.087818 loss_ctc 24.865213 loss_rnnt 24.918989 hw_loss 0.515862 lr 0.00048993 rank 7
2023-02-21 01:15:40,693 DEBUG TRAIN Batch 12/4600 loss 27.864384 loss_att 22.639925 loss_ctc 25.848694 loss_rnnt 29.107590 hw_loss 0.132084 lr 0.00048994 rank 0
2023-02-21 01:15:40,699 DEBUG TRAIN Batch 12/4600 loss 5.783739 loss_att 7.755948 loss_ctc 5.226604 loss_rnnt 5.463233 hw_loss 0.000654 lr 0.00048983 rank 3
2023-02-21 01:15:40,701 DEBUG TRAIN Batch 12/4600 loss 11.964046 loss_att 24.059923 loss_ctc 10.740398 loss_rnnt 9.707882 hw_loss 0.000265 lr 0.00048985 rank 2
2023-02-21 01:15:40,702 DEBUG TRAIN Batch 12/4600 loss 23.989838 loss_att 22.518856 loss_ctc 26.366043 loss_rnnt 23.790380 hw_loss 0.331549 lr 0.00048984 rank 6
2023-02-21 01:15:40,709 DEBUG TRAIN Batch 12/4600 loss 7.037317 loss_att 11.549862 loss_ctc 7.881064 loss_rnnt 6.022186 hw_loss 0.000228 lr 0.00048996 rank 4
2023-02-21 01:15:40,715 DEBUG TRAIN Batch 12/4600 loss 27.556019 loss_att 34.752762 loss_ctc 35.789787 loss_rnnt 24.942253 hw_loss 0.143584 lr 0.00048990 rank 1
2023-02-21 01:15:40,764 DEBUG TRAIN Batch 12/4600 loss 10.858397 loss_att 12.507616 loss_ctc 12.727747 loss_rnnt 10.116720 hw_loss 0.304850 lr 0.00048981 rank 7
2023-02-21 01:15:40,792 DEBUG TRAIN Batch 12/4600 loss 16.671736 loss_att 18.977186 loss_ctc 16.891846 loss_rnnt 15.959236 hw_loss 0.416365 lr 0.00048984 rank 5
2023-02-21 01:16:40,141 DEBUG TRAIN Batch 12/4700 loss 30.239256 loss_att 33.470512 loss_ctc 31.519838 loss_rnnt 29.270298 hw_loss 0.284927 lr 0.00048972 rank 5
2023-02-21 01:16:40,141 DEBUG TRAIN Batch 12/4700 loss 20.265450 loss_att 22.420574 loss_ctc 27.003677 loss_rnnt 18.800522 hw_loss 0.254010 lr 0.00048982 rank 0
2023-02-21 01:16:40,142 DEBUG TRAIN Batch 12/4700 loss 19.098282 loss_att 23.628338 loss_ctc 30.446682 loss_rnnt 16.493597 hw_loss 0.347917 lr 0.00048971 rank 3
2023-02-21 01:16:40,145 DEBUG TRAIN Batch 12/4700 loss 14.918861 loss_att 16.818670 loss_ctc 14.396675 loss_rnnt 14.418554 hw_loss 0.356195 lr 0.00048974 rank 2
2023-02-21 01:16:40,149 DEBUG TRAIN Batch 12/4700 loss 3.465551 loss_att 8.346044 loss_ctc 4.405156 loss_rnnt 2.292949 hw_loss 0.133543 lr 0.00048984 rank 4
2023-02-21 01:16:40,155 DEBUG TRAIN Batch 12/4700 loss 27.310495 loss_att 28.852863 loss_ctc 37.712502 loss_rnnt 25.614647 hw_loss 0.000825 lr 0.00048969 rank 7
2023-02-21 01:16:40,195 DEBUG TRAIN Batch 12/4700 loss 17.082678 loss_att 26.568527 loss_ctc 22.886810 loss_rnnt 14.280855 hw_loss 0.245188 lr 0.00048978 rank 1
2023-02-21 01:16:40,239 DEBUG TRAIN Batch 12/4700 loss 12.502542 loss_att 19.465927 loss_ctc 14.699646 loss_rnnt 10.655965 hw_loss 0.301787 lr 0.00048972 rank 6
2023-02-21 01:17:40,277 DEBUG TRAIN Batch 12/4800 loss 18.748274 loss_att 24.275848 loss_ctc 32.440777 loss_rnnt 15.536982 hw_loss 0.525204 lr 0.00048970 rank 0
2023-02-21 01:17:40,283 DEBUG TRAIN Batch 12/4800 loss 47.132763 loss_att 47.083290 loss_ctc 61.715778 loss_rnnt 44.929276 hw_loss 0.504344 lr 0.00048960 rank 3
2023-02-21 01:17:40,288 DEBUG TRAIN Batch 12/4800 loss 8.136162 loss_att 15.542047 loss_ctc 15.301706 loss_rnnt 5.699421 hw_loss 0.000293 lr 0.00048962 rank 2
2023-02-21 01:17:40,288 DEBUG TRAIN Batch 12/4800 loss 30.011305 loss_att 33.884033 loss_ctc 47.336643 loss_rnnt 26.926449 hw_loss 0.000496 lr 0.00048961 rank 5
2023-02-21 01:17:40,295 DEBUG TRAIN Batch 12/4800 loss 22.907633 loss_att 30.547646 loss_ctc 38.082581 loss_rnnt 19.356129 hw_loss 0.000326 lr 0.00048960 rank 6
2023-02-21 01:17:40,295 DEBUG TRAIN Batch 12/4800 loss 17.241220 loss_att 18.587851 loss_ctc 20.774622 loss_rnnt 16.356987 hw_loss 0.269599 lr 0.00048972 rank 4
2023-02-21 01:17:40,298 DEBUG TRAIN Batch 12/4800 loss 34.898502 loss_att 42.477619 loss_ctc 45.145931 loss_rnnt 31.969887 hw_loss 0.087132 lr 0.00048967 rank 1
2023-02-21 01:17:40,352 DEBUG TRAIN Batch 12/4800 loss 12.712231 loss_att 14.711607 loss_ctc 10.521823 loss_rnnt 12.537352 hw_loss 0.125735 lr 0.00048958 rank 7
2023-02-21 01:18:37,368 DEBUG TRAIN Batch 12/4900 loss 18.386892 loss_att 18.194109 loss_ctc 19.891438 loss_rnnt 18.036739 hw_loss 0.352691 lr 0.00048960 rank 4
2023-02-21 01:18:37,371 DEBUG TRAIN Batch 12/4900 loss 13.184121 loss_att 15.741861 loss_ctc 17.767298 loss_rnnt 11.923199 hw_loss 0.259283 lr 0.00048948 rank 3
2023-02-21 01:18:37,375 DEBUG TRAIN Batch 12/4900 loss 13.838280 loss_att 20.544369 loss_ctc 18.344957 loss_rnnt 11.633390 hw_loss 0.492715 lr 0.00048959 rank 0
2023-02-21 01:18:37,375 DEBUG TRAIN Batch 12/4900 loss 10.652706 loss_att 21.528824 loss_ctc 16.317413 loss_rnnt 7.721819 hw_loss 0.000693 lr 0.00048949 rank 5
2023-02-21 01:18:37,376 DEBUG TRAIN Batch 12/4900 loss 12.973127 loss_att 15.882392 loss_ctc 13.992629 loss_rnnt 12.012218 hw_loss 0.455857 lr 0.00048950 rank 2
2023-02-21 01:18:37,377 DEBUG TRAIN Batch 12/4900 loss 22.088945 loss_att 24.799419 loss_ctc 33.777740 loss_rnnt 19.884190 hw_loss 0.195291 lr 0.00048948 rank 6
2023-02-21 01:18:37,381 DEBUG TRAIN Batch 12/4900 loss 26.050800 loss_att 24.702934 loss_ctc 29.884560 loss_rnnt 25.693083 hw_loss 0.217729 lr 0.00048946 rank 7
2023-02-21 01:18:37,384 DEBUG TRAIN Batch 12/4900 loss 16.526043 loss_att 16.348043 loss_ctc 19.105467 loss_rnnt 15.925214 hw_loss 0.548449 lr 0.00048955 rank 1
2023-02-21 01:19:35,993 DEBUG TRAIN Batch 12/5000 loss 12.015123 loss_att 11.043912 loss_ctc 14.188206 loss_rnnt 11.737495 hw_loss 0.341487 lr 0.00048937 rank 5
2023-02-21 01:19:35,995 DEBUG TRAIN Batch 12/5000 loss 19.122383 loss_att 24.182070 loss_ctc 23.119675 loss_rnnt 17.388723 hw_loss 0.353910 lr 0.00048947 rank 0
2023-02-21 01:19:36,000 DEBUG TRAIN Batch 12/5000 loss 15.449533 loss_att 15.440796 loss_ctc 16.191013 loss_rnnt 15.191793 hw_loss 0.301167 lr 0.00048943 rank 1
2023-02-21 01:19:36,002 DEBUG TRAIN Batch 12/5000 loss 20.585966 loss_att 20.406588 loss_ctc 21.607300 loss_rnnt 20.385094 hw_loss 0.188571 lr 0.00048936 rank 3
2023-02-21 01:19:36,002 DEBUG TRAIN Batch 12/5000 loss 3.997427 loss_att 9.132752 loss_ctc 5.214352 loss_rnnt 2.638481 hw_loss 0.318046 lr 0.00048937 rank 6
2023-02-21 01:19:36,010 DEBUG TRAIN Batch 12/5000 loss 12.522948 loss_att 15.584867 loss_ctc 15.284115 loss_rnnt 11.348459 hw_loss 0.363653 lr 0.00048949 rank 4
2023-02-21 01:19:36,010 DEBUG TRAIN Batch 12/5000 loss 21.732618 loss_att 24.230377 loss_ctc 27.979706 loss_rnnt 20.283731 hw_loss 0.218235 lr 0.00048934 rank 7
2023-02-21 01:19:36,010 DEBUG TRAIN Batch 12/5000 loss 12.577928 loss_att 13.848044 loss_ctc 18.676594 loss_rnnt 11.510058 hw_loss 0.001295 lr 0.00048939 rank 2
2023-02-21 01:20:35,910 DEBUG TRAIN Batch 12/5100 loss 12.611163 loss_att 19.226643 loss_ctc 19.979713 loss_rnnt 10.013525 hw_loss 0.547631 lr 0.00048935 rank 0
2023-02-21 01:20:35,913 DEBUG TRAIN Batch 12/5100 loss 12.431836 loss_att 23.252165 loss_ctc 17.412138 loss_rnnt 9.379142 hw_loss 0.421103 lr 0.00048925 rank 6
2023-02-21 01:20:35,916 DEBUG TRAIN Batch 12/5100 loss 4.848344 loss_att 7.708923 loss_ctc 6.424473 loss_rnnt 4.065217 hw_loss 0.001615 lr 0.00048927 rank 2
2023-02-21 01:20:35,917 DEBUG TRAIN Batch 12/5100 loss 6.203998 loss_att 10.671432 loss_ctc 7.822861 loss_rnnt 4.949445 hw_loss 0.272284 lr 0.00048924 rank 3
2023-02-21 01:20:35,918 DEBUG TRAIN Batch 12/5100 loss 30.281578 loss_att 36.543236 loss_ctc 36.141415 loss_rnnt 28.247152 hw_loss 0.001470 lr 0.00048932 rank 1
2023-02-21 01:20:35,918 DEBUG TRAIN Batch 12/5100 loss 10.761025 loss_att 19.761763 loss_ctc 19.060072 loss_rnnt 7.604126 hw_loss 0.469149 lr 0.00048922 rank 7
2023-02-21 01:20:35,920 DEBUG TRAIN Batch 12/5100 loss 24.815357 loss_att 29.699425 loss_ctc 36.037731 loss_rnnt 22.239916 hw_loss 0.191829 lr 0.00048937 rank 4
2023-02-21 01:20:35,920 DEBUG TRAIN Batch 12/5100 loss 14.114635 loss_att 18.864204 loss_ctc 35.911346 loss_rnnt 10.031436 hw_loss 0.425732 lr 0.00048925 rank 5
2023-02-21 01:21:57,479 DEBUG TRAIN Batch 12/5200 loss 17.364422 loss_att 20.840412 loss_ctc 28.130152 loss_rnnt 15.105612 hw_loss 0.240339 lr 0.00048924 rank 0
2023-02-21 01:21:57,482 DEBUG TRAIN Batch 12/5200 loss 27.803289 loss_att 25.198805 loss_ctc 31.283819 loss_rnnt 27.605795 hw_loss 0.476853 lr 0.00048915 rank 2
2023-02-21 01:21:57,487 DEBUG TRAIN Batch 12/5200 loss 9.508810 loss_att 12.925298 loss_ctc 9.569605 loss_rnnt 8.669431 hw_loss 0.277452 lr 0.00048920 rank 1
2023-02-21 01:21:57,487 DEBUG TRAIN Batch 12/5200 loss 18.705744 loss_att 26.402977 loss_ctc 27.590599 loss_rnnt 15.799067 hw_loss 0.342341 lr 0.00048911 rank 7
2023-02-21 01:21:57,490 DEBUG TRAIN Batch 12/5200 loss 23.255615 loss_att 20.924799 loss_ctc 24.607161 loss_rnnt 23.263285 hw_loss 0.521789 lr 0.00048914 rank 5
2023-02-21 01:21:57,491 DEBUG TRAIN Batch 12/5200 loss 21.856976 loss_att 29.197071 loss_ctc 33.094757 loss_rnnt 18.890465 hw_loss 0.000228 lr 0.00048913 rank 6
2023-02-21 01:21:57,490 DEBUG TRAIN Batch 12/5200 loss 2.024031 loss_att 5.386047 loss_ctc 2.608832 loss_rnnt 0.986823 hw_loss 0.537808 lr 0.00048913 rank 3
2023-02-21 01:21:57,490 DEBUG TRAIN Batch 12/5200 loss 10.955081 loss_att 11.581777 loss_ctc 10.782596 loss_rnnt 10.739671 hw_loss 0.212004 lr 0.00048925 rank 4
2023-02-21 01:22:56,817 DEBUG TRAIN Batch 12/5300 loss 36.839741 loss_att 41.474174 loss_ctc 31.198191 loss_rnnt 36.302536 hw_loss 0.679733 lr 0.00048912 rank 0
2023-02-21 01:22:56,823 DEBUG TRAIN Batch 12/5300 loss 6.399675 loss_att 8.760873 loss_ctc 5.641656 loss_rnnt 5.868214 hw_loss 0.300544 lr 0.00048902 rank 5
2023-02-21 01:22:56,826 DEBUG TRAIN Batch 12/5300 loss 7.490850 loss_att 12.061928 loss_ctc 8.636856 loss_rnnt 6.266185 hw_loss 0.295594 lr 0.00048908 rank 1
2023-02-21 01:22:56,829 DEBUG TRAIN Batch 12/5300 loss 25.678667 loss_att 29.565062 loss_ctc 33.497353 loss_rnnt 23.629457 hw_loss 0.430198 lr 0.00048901 rank 3
2023-02-21 01:22:56,831 DEBUG TRAIN Batch 12/5300 loss 14.233994 loss_att 20.696804 loss_ctc 20.764727 loss_rnnt 11.987076 hw_loss 0.156735 lr 0.00048903 rank 2
2023-02-21 01:22:56,832 DEBUG TRAIN Batch 12/5300 loss 14.088284 loss_att 20.012337 loss_ctc 16.503452 loss_rnnt 12.538546 hw_loss 0.080446 lr 0.00048913 rank 4
2023-02-21 01:22:56,833 DEBUG TRAIN Batch 12/5300 loss 29.768793 loss_att 31.061203 loss_ctc 34.808128 loss_rnnt 28.838230 hw_loss 0.000320 lr 0.00048899 rank 7
2023-02-21 01:22:56,838 DEBUG TRAIN Batch 12/5300 loss 14.178346 loss_att 19.835253 loss_ctc 15.258821 loss_rnnt 12.902734 hw_loss 0.000313 lr 0.00048902 rank 6
2023-02-21 01:23:55,706 DEBUG TRAIN Batch 12/5400 loss 9.772897 loss_att 18.635593 loss_ctc 8.701834 loss_rnnt 8.030418 hw_loss 0.211400 lr 0.00048889 rank 3
2023-02-21 01:23:55,707 DEBUG TRAIN Batch 12/5400 loss 13.872601 loss_att 12.604630 loss_ctc 12.349325 loss_rnnt 14.023407 hw_loss 0.573545 lr 0.00048900 rank 0
2023-02-21 01:23:55,708 DEBUG TRAIN Batch 12/5400 loss 24.605455 loss_att 24.046114 loss_ctc 28.997240 loss_rnnt 24.131565 hw_loss 0.000354 lr 0.00048892 rank 2
2023-02-21 01:23:55,708 DEBUG TRAIN Batch 12/5400 loss 15.544872 loss_att 24.794682 loss_ctc 23.625029 loss_rnnt 12.492809 hw_loss 0.233900 lr 0.00048902 rank 4
2023-02-21 01:23:55,709 DEBUG TRAIN Batch 12/5400 loss 7.508061 loss_att 8.928371 loss_ctc 9.111043 loss_rnnt 6.720484 hw_loss 0.543346 lr 0.00048890 rank 5
2023-02-21 01:23:55,714 DEBUG TRAIN Batch 12/5400 loss 15.629737 loss_att 25.090080 loss_ctc 21.356663 loss_rnnt 12.973821 hw_loss 0.000484 lr 0.00048896 rank 1
2023-02-21 01:23:55,719 DEBUG TRAIN Batch 12/5400 loss 9.549972 loss_att 20.076355 loss_ctc 16.394297 loss_rnnt 6.458804 hw_loss 0.137466 lr 0.00048890 rank 6
2023-02-21 01:23:55,720 DEBUG TRAIN Batch 12/5400 loss 23.688503 loss_att 31.604710 loss_ctc 24.676601 loss_rnnt 21.894495 hw_loss 0.148163 lr 0.00048887 rank 7
2023-02-21 01:24:53,824 DEBUG TRAIN Batch 12/5500 loss 18.603422 loss_att 23.868195 loss_ctc 22.745943 loss_rnnt 16.814180 hw_loss 0.344907 lr 0.00048888 rank 0
2023-02-21 01:24:53,825 DEBUG TRAIN Batch 12/5500 loss 4.825548 loss_att 9.508204 loss_ctc 5.866696 loss_rnnt 3.591674 hw_loss 0.297232 lr 0.00048878 rank 3
2023-02-21 01:24:53,825 DEBUG TRAIN Batch 12/5500 loss 23.640291 loss_att 23.664000 loss_ctc 26.848820 loss_rnnt 23.058847 hw_loss 0.279182 lr 0.00048885 rank 1
2023-02-21 01:24:53,829 DEBUG TRAIN Batch 12/5500 loss 14.952673 loss_att 12.797441 loss_ctc 14.823042 loss_rnnt 15.066458 hw_loss 0.627272 lr 0.00048880 rank 2
2023-02-21 01:24:53,833 DEBUG TRAIN Batch 12/5500 loss 7.432026 loss_att 10.948291 loss_ctc 8.820555 loss_rnnt 6.324879 hw_loss 0.410171 lr 0.00048878 rank 6
2023-02-21 01:24:53,841 DEBUG TRAIN Batch 12/5500 loss 28.780180 loss_att 26.063110 loss_ctc 37.834541 loss_rnnt 27.940872 hw_loss 0.329013 lr 0.00048876 rank 7
2023-02-21 01:24:53,870 DEBUG TRAIN Batch 12/5500 loss 15.961884 loss_att 18.008368 loss_ctc 19.128738 loss_rnnt 14.950672 hw_loss 0.336876 lr 0.00048879 rank 5
2023-02-21 01:24:53,924 DEBUG TRAIN Batch 12/5500 loss 30.820826 loss_att 28.581854 loss_ctc 33.948750 loss_rnnt 30.683643 hw_loss 0.314851 lr 0.00048890 rank 4
2023-02-21 01:25:54,159 DEBUG TRAIN Batch 12/5600 loss 20.007360 loss_att 24.520031 loss_ctc 19.154011 loss_rnnt 18.989033 hw_loss 0.430452 lr 0.00048877 rank 0
2023-02-21 01:25:54,165 DEBUG TRAIN Batch 12/5600 loss 6.670689 loss_att 9.070750 loss_ctc 7.603553 loss_rnnt 5.861299 hw_loss 0.384366 lr 0.00048866 rank 3
2023-02-21 01:25:54,165 DEBUG TRAIN Batch 12/5600 loss 9.136394 loss_att 12.475464 loss_ctc 9.673644 loss_rnnt 8.194008 hw_loss 0.380508 lr 0.00048868 rank 2
2023-02-21 01:25:54,165 DEBUG TRAIN Batch 12/5600 loss 5.958990 loss_att 9.390955 loss_ctc 8.711654 loss_rnnt 4.824503 hw_loss 0.152011 lr 0.00048866 rank 6
2023-02-21 01:25:54,166 DEBUG TRAIN Batch 12/5600 loss 18.681940 loss_att 22.258682 loss_ctc 24.609455 loss_rnnt 17.106342 hw_loss 0.131088 lr 0.00048873 rank 1
2023-02-21 01:25:54,172 DEBUG TRAIN Batch 12/5600 loss 34.683422 loss_att 42.491325 loss_ctc 28.266914 loss_rnnt 33.696918 hw_loss 0.525853 lr 0.00048864 rank 7
2023-02-21 01:25:54,192 DEBUG TRAIN Batch 12/5600 loss 7.791412 loss_att 10.458681 loss_ctc 10.752596 loss_rnnt 6.798298 hw_loss 0.121567 lr 0.00048867 rank 5
2023-02-21 01:25:54,250 DEBUG TRAIN Batch 12/5600 loss 15.891858 loss_att 23.143871 loss_ctc 23.923801 loss_rnnt 13.243462 hw_loss 0.238253 lr 0.00048878 rank 4
2023-02-21 01:26:51,786 DEBUG TRAIN Batch 12/5700 loss 44.937553 loss_att 60.247387 loss_ctc 49.135506 loss_rnnt 41.315697 hw_loss 0.000304 lr 0.00048855 rank 6
2023-02-21 01:26:51,787 DEBUG TRAIN Batch 12/5700 loss 15.719847 loss_att 24.162998 loss_ctc 24.890974 loss_rnnt 12.749840 hw_loss 0.109798 lr 0.00048865 rank 0
2023-02-21 01:26:51,788 DEBUG TRAIN Batch 12/5700 loss 9.968156 loss_att 16.005806 loss_ctc 17.411682 loss_rnnt 7.767896 hw_loss 0.000486 lr 0.00048855 rank 5
2023-02-21 01:26:51,790 DEBUG TRAIN Batch 12/5700 loss 12.151175 loss_att 16.585659 loss_ctc 13.336142 loss_rnnt 11.106092 hw_loss 0.000356 lr 0.00048857 rank 2
2023-02-21 01:26:51,790 DEBUG TRAIN Batch 12/5700 loss 12.144093 loss_att 19.702721 loss_ctc 18.164289 loss_rnnt 9.636417 hw_loss 0.362356 lr 0.00048861 rank 1
2023-02-21 01:26:51,792 DEBUG TRAIN Batch 12/5700 loss 44.445839 loss_att 46.685223 loss_ctc 63.587906 loss_rnnt 41.256161 hw_loss 0.355359 lr 0.00048852 rank 7
2023-02-21 01:26:51,792 DEBUG TRAIN Batch 12/5700 loss 20.165598 loss_att 27.367842 loss_ctc 23.699453 loss_rnnt 18.031853 hw_loss 0.416465 lr 0.00048867 rank 4
2023-02-21 01:26:51,797 DEBUG TRAIN Batch 12/5700 loss 8.065206 loss_att 14.355490 loss_ctc 11.954436 loss_rnnt 6.019345 hw_loss 0.504824 lr 0.00048854 rank 3
2023-02-21 01:27:51,074 DEBUG TRAIN Batch 12/5800 loss 15.615111 loss_att 21.566341 loss_ctc 13.997890 loss_rnnt 14.468708 hw_loss 0.322102 lr 0.00048853 rank 0
2023-02-21 01:27:51,083 DEBUG TRAIN Batch 12/5800 loss 12.107477 loss_att 15.572927 loss_ctc 12.932475 loss_rnnt 11.207973 hw_loss 0.180778 lr 0.00048844 rank 5
2023-02-21 01:27:51,084 DEBUG TRAIN Batch 12/5800 loss 12.532763 loss_att 16.411667 loss_ctc 16.512417 loss_rnnt 11.061295 hw_loss 0.309503 lr 0.00048845 rank 2
2023-02-21 01:27:51,089 DEBUG TRAIN Batch 12/5800 loss 14.194233 loss_att 33.363358 loss_ctc 15.973783 loss_rnnt 9.912418 hw_loss 0.395093 lr 0.00048843 rank 3
2023-02-21 01:27:51,091 DEBUG TRAIN Batch 12/5800 loss 29.706341 loss_att 30.207043 loss_ctc 37.772491 loss_rnnt 28.338490 hw_loss 0.360422 lr 0.00048843 rank 6
2023-02-21 01:27:51,093 DEBUG TRAIN Batch 12/5800 loss 12.718806 loss_att 17.732994 loss_ctc 16.091660 loss_rnnt 11.096651 hw_loss 0.318006 lr 0.00048841 rank 7
2023-02-21 01:27:51,096 DEBUG TRAIN Batch 12/5800 loss 21.544029 loss_att 22.723291 loss_ctc 26.140480 loss_rnnt 20.476978 hw_loss 0.409385 lr 0.00048855 rank 4
2023-02-21 01:27:51,154 DEBUG TRAIN Batch 12/5800 loss 29.324427 loss_att 25.657166 loss_ctc 31.338160 loss_rnnt 29.700510 hw_loss 0.166631 lr 0.00048850 rank 1
2023-02-21 01:28:51,368 DEBUG TRAIN Batch 12/5900 loss 17.144037 loss_att 21.491669 loss_ctc 18.906349 loss_rnnt 15.985368 hw_loss 0.101567 lr 0.00048842 rank 0
2023-02-21 01:28:51,377 DEBUG TRAIN Batch 12/5900 loss 8.605151 loss_att 12.522961 loss_ctc 9.163832 loss_rnnt 7.571346 hw_loss 0.329535 lr 0.00048838 rank 1
2023-02-21 01:28:51,379 DEBUG TRAIN Batch 12/5900 loss 44.882137 loss_att 53.714561 loss_ctc 53.721001 loss_rnnt 41.882633 hw_loss 0.102182 lr 0.00048833 rank 2
2023-02-21 01:28:51,382 DEBUG TRAIN Batch 12/5900 loss 3.807186 loss_att 9.392139 loss_ctc 4.263471 loss_rnnt 2.574503 hw_loss 0.102851 lr 0.00048832 rank 6
2023-02-21 01:28:51,383 DEBUG TRAIN Batch 12/5900 loss 38.850807 loss_att 49.970459 loss_ctc 45.205376 loss_rnnt 35.653271 hw_loss 0.236871 lr 0.00048832 rank 5
2023-02-21 01:28:51,383 DEBUG TRAIN Batch 12/5900 loss 10.755250 loss_att 12.276654 loss_ctc 13.436129 loss_rnnt 9.883458 hw_loss 0.393863 lr 0.00048831 rank 3
2023-02-21 01:28:51,386 DEBUG TRAIN Batch 12/5900 loss 12.067694 loss_att 20.275461 loss_ctc 12.151573 loss_rnnt 10.176569 hw_loss 0.446979 lr 0.00048843 rank 4
2023-02-21 01:28:51,386 DEBUG TRAIN Batch 12/5900 loss 13.857006 loss_att 20.324932 loss_ctc 16.510071 loss_rnnt 11.969960 hw_loss 0.449470 lr 0.00048829 rank 7
2023-02-21 01:30:09,489 DEBUG TRAIN Batch 12/6000 loss 16.472420 loss_att 23.004448 loss_ctc 22.135504 loss_rnnt 14.352301 hw_loss 0.109938 lr 0.00048830 rank 0
2023-02-21 01:30:09,492 DEBUG TRAIN Batch 12/6000 loss 50.901928 loss_att 63.079922 loss_ctc 67.337868 loss_rnnt 46.274757 hw_loss 0.000200 lr 0.00048820 rank 5
2023-02-21 01:30:09,495 DEBUG TRAIN Batch 12/6000 loss 8.824226 loss_att 17.623774 loss_ctc 7.476291 loss_rnnt 7.150023 hw_loss 0.176284 lr 0.00048822 rank 2
2023-02-21 01:30:09,496 DEBUG TRAIN Batch 12/6000 loss 22.792196 loss_att 27.711777 loss_ctc 29.622692 loss_rnnt 20.801321 hw_loss 0.180427 lr 0.00048826 rank 1
2023-02-21 01:30:09,496 DEBUG TRAIN Batch 12/6000 loss 4.578048 loss_att 13.881916 loss_ctc 4.427731 loss_rnnt 2.737211 hw_loss 0.000200 lr 0.00048819 rank 3
2023-02-21 01:30:09,498 DEBUG TRAIN Batch 12/6000 loss 10.495271 loss_att 15.193687 loss_ctc 15.859885 loss_rnnt 8.658266 hw_loss 0.341322 lr 0.00048820 rank 6
2023-02-21 01:30:09,499 DEBUG TRAIN Batch 12/6000 loss 15.688764 loss_att 15.709412 loss_ctc 17.527391 loss_rnnt 15.228037 hw_loss 0.396461 lr 0.00048817 rank 7
2023-02-21 01:30:09,503 DEBUG TRAIN Batch 12/6000 loss 24.910841 loss_att 27.708591 loss_ctc 23.460789 loss_rnnt 24.544527 hw_loss 0.000195 lr 0.00048832 rank 4
2023-02-21 01:31:28,248 DEBUG TRAIN Batch 12/6100 loss 23.208101 loss_att 29.109104 loss_ctc 33.993076 loss_rnnt 20.490074 hw_loss 0.187181 lr 0.00048818 rank 0
2023-02-21 01:31:28,249 DEBUG TRAIN Batch 12/6100 loss 20.948559 loss_att 23.266701 loss_ctc 29.069012 loss_rnnt 19.373476 hw_loss 0.053864 lr 0.00048809 rank 5
2023-02-21 01:31:28,252 DEBUG TRAIN Batch 12/6100 loss 7.371991 loss_att 11.501432 loss_ctc 8.537890 loss_rnnt 6.305625 hw_loss 0.159421 lr 0.00048808 rank 6
2023-02-21 01:31:28,252 DEBUG TRAIN Batch 12/6100 loss 16.161198 loss_att 14.080777 loss_ctc 18.892706 loss_rnnt 16.009653 hw_loss 0.381426 lr 0.00048808 rank 3
2023-02-21 01:31:28,255 DEBUG TRAIN Batch 12/6100 loss 7.396360 loss_att 8.123933 loss_ctc 7.036228 loss_rnnt 7.037078 hw_loss 0.490848 lr 0.00048810 rank 2
2023-02-21 01:31:28,256 DEBUG TRAIN Batch 12/6100 loss 7.495850 loss_att 8.165526 loss_ctc 9.931665 loss_rnnt 6.854310 hw_loss 0.342804 lr 0.00048806 rank 7
2023-02-21 01:31:28,258 DEBUG TRAIN Batch 12/6100 loss 15.619121 loss_att 15.929429 loss_ctc 22.252743 loss_rnnt 14.512299 hw_loss 0.300520 lr 0.00048815 rank 1
2023-02-21 01:31:28,321 DEBUG TRAIN Batch 12/6100 loss 9.782715 loss_att 13.448162 loss_ctc 13.065413 loss_rnnt 8.611511 hw_loss 0.000789 lr 0.00048820 rank 4
2023-02-21 01:32:27,221 DEBUG TRAIN Batch 12/6200 loss 12.344987 loss_att 16.161333 loss_ctc 22.657368 loss_rnnt 9.984059 hw_loss 0.417513 lr 0.00048807 rank 0
2023-02-21 01:32:27,231 DEBUG TRAIN Batch 12/6200 loss 23.216528 loss_att 26.606312 loss_ctc 35.994198 loss_rnnt 20.739323 hw_loss 0.179173 lr 0.00048798 rank 2
2023-02-21 01:32:27,232 DEBUG TRAIN Batch 12/6200 loss 23.720465 loss_att 27.596586 loss_ctc 30.303505 loss_rnnt 21.966101 hw_loss 0.190130 lr 0.00048797 rank 5
2023-02-21 01:32:27,234 DEBUG TRAIN Batch 12/6200 loss 13.377711 loss_att 14.945130 loss_ctc 13.253481 loss_rnnt 12.886893 hw_loss 0.363559 lr 0.00048808 rank 4
2023-02-21 01:32:27,239 DEBUG TRAIN Batch 12/6200 loss 16.396448 loss_att 20.559149 loss_ctc 22.392179 loss_rnnt 14.640399 hw_loss 0.232650 lr 0.00048796 rank 3
2023-02-21 01:32:27,241 DEBUG TRAIN Batch 12/6200 loss 15.546995 loss_att 22.388542 loss_ctc 21.380060 loss_rnnt 13.346288 hw_loss 0.102480 lr 0.00048797 rank 6
2023-02-21 01:32:27,241 DEBUG TRAIN Batch 12/6200 loss 6.373895 loss_att 11.476387 loss_ctc 16.044901 loss_rnnt 3.862721 hw_loss 0.377265 lr 0.00048803 rank 1
2023-02-21 01:32:27,307 DEBUG TRAIN Batch 12/6200 loss 2.004830 loss_att 6.623459 loss_ctc 3.839818 loss_rnnt 0.750543 hw_loss 0.161056 lr 0.00048794 rank 7
2023-02-21 01:33:25,135 DEBUG TRAIN Batch 12/6300 loss 17.405514 loss_att 33.072170 loss_ctc 19.546062 loss_rnnt 13.876541 hw_loss 0.206692 lr 0.00048795 rank 0
2023-02-21 01:33:25,141 DEBUG TRAIN Batch 12/6300 loss 58.257133 loss_att 65.060791 loss_ctc 75.687157 loss_rnnt 54.320534 hw_loss 0.472236 lr 0.00048787 rank 2
2023-02-21 01:33:25,142 DEBUG TRAIN Batch 12/6300 loss 13.524166 loss_att 19.825178 loss_ctc 13.343699 loss_rnnt 12.150760 hw_loss 0.257375 lr 0.00048785 rank 3
2023-02-21 01:33:25,144 DEBUG TRAIN Batch 12/6300 loss 25.797222 loss_att 22.779819 loss_ctc 18.007589 loss_rnnt 27.226391 hw_loss 0.399245 lr 0.00048797 rank 4
2023-02-21 01:33:25,143 DEBUG TRAIN Batch 12/6300 loss 34.894035 loss_att 39.933746 loss_ctc 37.231987 loss_rnnt 33.574287 hw_loss 0.000154 lr 0.00048785 rank 5
2023-02-21 01:33:25,146 DEBUG TRAIN Batch 12/6300 loss 7.617279 loss_att 8.978148 loss_ctc 8.192848 loss_rnnt 7.159622 hw_loss 0.203890 lr 0.00048785 rank 6
2023-02-21 01:33:25,151 DEBUG TRAIN Batch 12/6300 loss 19.734121 loss_att 23.879753 loss_ctc 25.472219 loss_rnnt 17.956644 hw_loss 0.343631 lr 0.00048782 rank 7
2023-02-21 01:33:25,202 DEBUG TRAIN Batch 12/6300 loss 8.073435 loss_att 17.398109 loss_ctc 4.310079 loss_rnnt 6.539604 hw_loss 0.320019 lr 0.00048792 rank 1
2023-02-21 01:34:22,922 DEBUG TRAIN Batch 12/6400 loss 5.932114 loss_att 8.500074 loss_ctc 4.742718 loss_rnnt 5.410140 hw_loss 0.313065 lr 0.00048784 rank 0
2023-02-21 01:34:22,928 DEBUG TRAIN Batch 12/6400 loss 15.351197 loss_att 19.151188 loss_ctc 16.258970 loss_rnnt 14.261038 hw_loss 0.392109 lr 0.00048774 rank 5
2023-02-21 01:34:22,930 DEBUG TRAIN Batch 12/6400 loss 8.899076 loss_att 12.988371 loss_ctc 10.601900 loss_rnnt 7.704545 hw_loss 0.280555 lr 0.00048775 rank 2
2023-02-21 01:34:22,931 DEBUG TRAIN Batch 12/6400 loss 19.280647 loss_att 20.031498 loss_ctc 21.810833 loss_rnnt 18.674515 hw_loss 0.222381 lr 0.00048773 rank 3
2023-02-21 01:34:22,932 DEBUG TRAIN Batch 12/6400 loss 26.331774 loss_att 34.436787 loss_ctc 33.482327 loss_rnnt 23.538960 hw_loss 0.409510 lr 0.00048773 rank 6
2023-02-21 01:34:22,935 DEBUG TRAIN Batch 12/6400 loss 15.669600 loss_att 17.312326 loss_ctc 19.492760 loss_rnnt 14.624775 hw_loss 0.387235 lr 0.00048780 rank 1
2023-02-21 01:34:22,935 DEBUG TRAIN Batch 12/6400 loss 11.784426 loss_att 14.681013 loss_ctc 12.711157 loss_rnnt 10.999002 hw_loss 0.154765 lr 0.00048785 rank 4
2023-02-21 01:34:22,941 DEBUG TRAIN Batch 12/6400 loss 12.260329 loss_att 16.613834 loss_ctc 10.925296 loss_rnnt 11.353802 hw_loss 0.400934 lr 0.00048771 rank 7
2023-02-21 01:35:22,601 DEBUG TRAIN Batch 12/6500 loss 5.579695 loss_att 8.205462 loss_ctc 11.164402 loss_rnnt 4.096294 hw_loss 0.400537 lr 0.00048764 rank 2
2023-02-21 01:35:22,605 DEBUG TRAIN Batch 12/6500 loss 11.400147 loss_att 16.471565 loss_ctc 9.700524 loss_rnnt 10.447177 hw_loss 0.309943 lr 0.00048772 rank 0
2023-02-21 01:35:22,608 DEBUG TRAIN Batch 12/6500 loss 12.361929 loss_att 15.830579 loss_ctc 13.563067 loss_rnnt 11.310379 hw_loss 0.370627 lr 0.00048762 rank 6
2023-02-21 01:35:22,617 DEBUG TRAIN Batch 12/6500 loss 3.933851 loss_att 10.431467 loss_ctc 3.977991 loss_rnnt 2.487463 hw_loss 0.264336 lr 0.00048774 rank 4
2023-02-21 01:35:22,618 DEBUG TRAIN Batch 12/6500 loss 8.496703 loss_att 15.267008 loss_ctc 10.121048 loss_rnnt 6.741291 hw_loss 0.346446 lr 0.00048761 rank 3
2023-02-21 01:35:22,625 DEBUG TRAIN Batch 12/6500 loss 1.849509 loss_att 6.711938 loss_ctc 1.400388 loss_rnnt 0.850761 hw_loss 0.161522 lr 0.00048759 rank 7
2023-02-21 01:35:22,631 DEBUG TRAIN Batch 12/6500 loss 12.692103 loss_att 17.803589 loss_ctc 15.704716 loss_rnnt 11.078337 hw_loss 0.355851 lr 0.00048762 rank 5
2023-02-21 01:35:22,672 DEBUG TRAIN Batch 12/6500 loss 2.130253 loss_att 5.244675 loss_ctc 1.619308 loss_rnnt 1.416752 hw_loss 0.297642 lr 0.00048768 rank 1
2023-02-21 01:36:20,246 DEBUG TRAIN Batch 12/6600 loss 27.277481 loss_att 27.859297 loss_ctc 30.579533 loss_rnnt 26.474426 hw_loss 0.462037 lr 0.00048760 rank 0
2023-02-21 01:36:20,247 DEBUG TRAIN Batch 12/6600 loss 45.537170 loss_att 39.925941 loss_ctc 66.025566 loss_rnnt 43.825462 hw_loss 0.191569 lr 0.00048751 rank 5
2023-02-21 01:36:20,255 DEBUG TRAIN Batch 12/6600 loss 11.262080 loss_att 9.924620 loss_ctc 12.983021 loss_rnnt 11.056870 hw_loss 0.456084 lr 0.00048750 rank 3
2023-02-21 01:36:20,257 DEBUG TRAIN Batch 12/6600 loss 9.498629 loss_att 11.163341 loss_ctc 5.055544 loss_rnnt 9.551748 hw_loss 0.386907 lr 0.00048757 rank 1
2023-02-21 01:36:20,258 DEBUG TRAIN Batch 12/6600 loss 21.070599 loss_att 21.439066 loss_ctc 25.583992 loss_rnnt 20.168383 hw_loss 0.425129 lr 0.00048750 rank 6
2023-02-21 01:36:20,261 DEBUG TRAIN Batch 12/6600 loss 7.057387 loss_att 16.654749 loss_ctc 5.848374 loss_rnnt 5.066805 hw_loss 0.435584 lr 0.00048752 rank 2
2023-02-21 01:36:20,263 DEBUG TRAIN Batch 12/6600 loss 15.112059 loss_att 13.977729 loss_ctc 17.233912 loss_rnnt 14.912738 hw_loss 0.268636 lr 0.00048748 rank 7
2023-02-21 01:36:20,265 DEBUG TRAIN Batch 12/6600 loss 13.882854 loss_att 19.296034 loss_ctc 15.704344 loss_rnnt 12.295247 hw_loss 0.491445 lr 0.00048762 rank 4
2023-02-21 01:37:19,135 DEBUG TRAIN Batch 12/6700 loss 23.749649 loss_att 32.638866 loss_ctc 28.324467 loss_rnnt 21.269152 hw_loss 0.173767 lr 0.00048749 rank 0
2023-02-21 01:37:19,141 DEBUG TRAIN Batch 12/6700 loss 12.066115 loss_att 15.866220 loss_ctc 11.614998 loss_rnnt 11.287062 hw_loss 0.148465 lr 0.00048739 rank 6
2023-02-21 01:37:19,143 DEBUG TRAIN Batch 12/6700 loss 19.669596 loss_att 19.167479 loss_ctc 19.265984 loss_rnnt 19.645962 hw_loss 0.333510 lr 0.00048740 rank 2
2023-02-21 01:37:19,145 DEBUG TRAIN Batch 12/6700 loss 16.804386 loss_att 17.794798 loss_ctc 18.999800 loss_rnnt 16.160912 hw_loss 0.286259 lr 0.00048745 rank 1
2023-02-21 01:37:19,146 DEBUG TRAIN Batch 12/6700 loss 19.380732 loss_att 25.123951 loss_ctc 23.130188 loss_rnnt 17.491652 hw_loss 0.450955 lr 0.00048738 rank 3
2023-02-21 01:37:19,147 DEBUG TRAIN Batch 12/6700 loss 32.965092 loss_att 30.943258 loss_ctc 37.372829 loss_rnnt 32.596138 hw_loss 0.348039 lr 0.00048739 rank 5
2023-02-21 01:37:19,149 DEBUG TRAIN Batch 12/6700 loss 22.100122 loss_att 23.097757 loss_ctc 27.243656 loss_rnnt 20.919825 hw_loss 0.553061 lr 0.00048750 rank 4
2023-02-21 01:37:19,153 DEBUG TRAIN Batch 12/6700 loss 10.979308 loss_att 16.670414 loss_ctc 15.080589 loss_rnnt 9.293829 hw_loss 0.000788 lr 0.00048736 rank 7
2023-02-21 01:38:19,973 DEBUG TRAIN Batch 12/6800 loss 15.328698 loss_att 21.359112 loss_ctc 15.724612 loss_rnnt 13.896439 hw_loss 0.325102 lr 0.00048737 rank 0
2023-02-21 01:38:19,978 DEBUG TRAIN Batch 12/6800 loss 3.440440 loss_att 5.674254 loss_ctc 3.700658 loss_rnnt 2.839656 hw_loss 0.223735 lr 0.00048728 rank 5
2023-02-21 01:38:19,980 DEBUG TRAIN Batch 12/6800 loss 33.194752 loss_att 33.302452 loss_ctc 32.608593 loss_rnnt 33.095108 hw_loss 0.292991 lr 0.00048734 rank 1
2023-02-21 01:38:19,982 DEBUG TRAIN Batch 12/6800 loss 14.583570 loss_att 8.728357 loss_ctc 17.944502 loss_rnnt 15.243752 hw_loss 0.117634 lr 0.00048729 rank 2
2023-02-21 01:38:19,989 DEBUG TRAIN Batch 12/6800 loss 21.676413 loss_att 18.925528 loss_ctc 25.105392 loss_rnnt 21.768730 hw_loss 0.001242 lr 0.00048727 rank 3
2023-02-21 01:38:19,993 DEBUG TRAIN Batch 12/6800 loss 18.975868 loss_att 20.877769 loss_ctc 30.746048 loss_rnnt 17.025370 hw_loss 0.001427 lr 0.00048725 rank 7
2023-02-21 01:38:19,993 DEBUG TRAIN Batch 12/6800 loss 10.860436 loss_att 10.174938 loss_ctc 16.475803 loss_rnnt 10.136515 hw_loss 0.210573 lr 0.00048739 rank 4
2023-02-21 01:38:20,042 DEBUG TRAIN Batch 12/6800 loss 14.435493 loss_att 19.544903 loss_ctc 17.514469 loss_rnnt 12.771359 hw_loss 0.434476 lr 0.00048727 rank 6
2023-02-21 01:39:38,384 DEBUG TRAIN Batch 12/6900 loss 15.122996 loss_att 19.853943 loss_ctc 16.582418 loss_rnnt 13.981968 hw_loss 0.000468 lr 0.00048726 rank 0
2023-02-21 01:39:38,394 DEBUG TRAIN Batch 12/6900 loss 46.205917 loss_att 41.474533 loss_ctc 44.621716 loss_rnnt 47.188557 hw_loss 0.327861 lr 0.00048722 rank 1
2023-02-21 01:39:38,393 DEBUG TRAIN Batch 12/6900 loss 22.793884 loss_att 20.565788 loss_ctc 20.318871 loss_rnnt 23.569298 hw_loss 0.000392 lr 0.00048716 rank 5
2023-02-21 01:39:38,397 DEBUG TRAIN Batch 12/6900 loss 7.882097 loss_att 9.181269 loss_ctc 9.585619 loss_rnnt 7.232323 hw_loss 0.305257 lr 0.00048717 rank 2
2023-02-21 01:39:38,399 DEBUG TRAIN Batch 12/6900 loss 7.447588 loss_att 10.191850 loss_ctc 7.741061 loss_rnnt 6.798436 hw_loss 0.114693 lr 0.00048715 rank 3
2023-02-21 01:39:38,403 DEBUG TRAIN Batch 12/6900 loss 16.075890 loss_att 18.478577 loss_ctc 15.020996 loss_rnnt 15.540049 hw_loss 0.367413 lr 0.00048713 rank 7
2023-02-21 01:39:38,406 DEBUG TRAIN Batch 12/6900 loss 17.313320 loss_att 16.268515 loss_ctc 20.978180 loss_rnnt 16.830885 hw_loss 0.380155 lr 0.00048715 rank 6
2023-02-21 01:39:38,405 DEBUG TRAIN Batch 12/6900 loss 11.205698 loss_att 10.717006 loss_ctc 13.386075 loss_rnnt 10.762488 hw_loss 0.469185 lr 0.00048727 rank 4
2023-02-21 01:40:54,633 DEBUG TRAIN Batch 12/7000 loss 19.740950 loss_att 24.680735 loss_ctc 21.720104 loss_rnnt 18.382917 hw_loss 0.199104 lr 0.00048714 rank 0
2023-02-21 01:40:54,636 DEBUG TRAIN Batch 12/7000 loss 7.156879 loss_att 8.514892 loss_ctc 12.128932 loss_rnnt 6.099650 hw_loss 0.230034 lr 0.00048704 rank 5
2023-02-21 01:40:54,637 DEBUG TRAIN Batch 12/7000 loss 22.254187 loss_att 21.391855 loss_ctc 24.685272 loss_rnnt 21.932623 hw_loss 0.318535 lr 0.00048703 rank 3
2023-02-21 01:40:54,638 DEBUG TRAIN Batch 12/7000 loss 18.213217 loss_att 24.387697 loss_ctc 24.895258 loss_rnnt 16.086792 hw_loss 0.001105 lr 0.00048706 rank 2
2023-02-21 01:40:54,639 DEBUG TRAIN Batch 12/7000 loss 17.069052 loss_att 19.096531 loss_ctc 23.526745 loss_rnnt 15.752786 hw_loss 0.093270 lr 0.00048701 rank 7
2023-02-21 01:40:54,639 DEBUG TRAIN Batch 12/7000 loss 15.499574 loss_att 18.790951 loss_ctc 18.528975 loss_rnnt 14.346257 hw_loss 0.170850 lr 0.00048704 rank 6
2023-02-21 01:40:54,639 DEBUG TRAIN Batch 12/7000 loss 20.530087 loss_att 23.838226 loss_ctc 26.783451 loss_rnnt 18.889605 hw_loss 0.272005 lr 0.00048716 rank 4
2023-02-21 01:40:54,648 DEBUG TRAIN Batch 12/7000 loss 10.966916 loss_att 19.120358 loss_ctc 13.737310 loss_rnnt 8.797061 hw_loss 0.318338 lr 0.00048710 rank 1
2023-02-21 01:41:54,039 DEBUG TRAIN Batch 12/7100 loss 35.509007 loss_att 34.041550 loss_ctc 40.828552 loss_rnnt 35.093124 hw_loss 0.000192 lr 0.00048703 rank 0
2023-02-21 01:41:54,042 DEBUG TRAIN Batch 12/7100 loss 3.192907 loss_att 7.237724 loss_ctc 5.430541 loss_rnnt 1.770073 hw_loss 0.591598 lr 0.00048694 rank 2
2023-02-21 01:41:54,044 DEBUG TRAIN Batch 12/7100 loss 1.210903 loss_att 5.584731 loss_ctc 0.537853 loss_rnnt 0.253629 hw_loss 0.322966 lr 0.00048692 rank 6
2023-02-21 01:41:54,046 DEBUG TRAIN Batch 12/7100 loss 59.936661 loss_att 72.400749 loss_ctc 67.204285 loss_rnnt 56.427448 hw_loss 0.088829 lr 0.00048704 rank 4
2023-02-21 01:41:54,047 DEBUG TRAIN Batch 12/7100 loss 27.334478 loss_att 32.005859 loss_ctc 31.970718 loss_rnnt 25.598534 hw_loss 0.344073 lr 0.00048692 rank 3
2023-02-21 01:41:54,049 DEBUG TRAIN Batch 12/7100 loss 3.435014 loss_att 11.196282 loss_ctc 5.998082 loss_rnnt 1.540910 hw_loss 0.000205 lr 0.00048693 rank 5
2023-02-21 01:41:54,062 DEBUG TRAIN Batch 12/7100 loss 1.927126 loss_att 7.397944 loss_ctc 3.784407 loss_rnnt 0.466394 hw_loss 0.222996 lr 0.00048699 rank 1
2023-02-21 01:41:54,067 DEBUG TRAIN Batch 12/7100 loss 24.717628 loss_att 31.400059 loss_ctc 26.240641 loss_rnnt 23.046986 hw_loss 0.245792 lr 0.00048690 rank 7
2023-02-21 01:42:51,958 DEBUG TRAIN Batch 12/7200 loss 22.963951 loss_att 21.943596 loss_ctc 25.152092 loss_rnnt 22.725973 hw_loss 0.281804 lr 0.00048691 rank 0
2023-02-21 01:42:51,961 DEBUG TRAIN Batch 12/7200 loss 10.979061 loss_att 19.024454 loss_ctc 10.179425 loss_rnnt 9.296228 hw_loss 0.338196 lr 0.00048681 rank 5
2023-02-21 01:42:51,965 DEBUG TRAIN Batch 12/7200 loss 4.117769 loss_att 15.338427 loss_ctc 5.122764 loss_rnnt 1.659466 hw_loss 0.150322 lr 0.00048683 rank 2
2023-02-21 01:42:51,968 DEBUG TRAIN Batch 12/7200 loss 16.082994 loss_att 22.890768 loss_ctc 12.826043 loss_rnnt 14.972345 hw_loss 0.343786 lr 0.00048680 rank 3
2023-02-21 01:42:51,974 DEBUG TRAIN Batch 12/7200 loss 7.264897 loss_att 10.925381 loss_ctc 8.485756 loss_rnnt 6.196690 hw_loss 0.324993 lr 0.00048693 rank 4
2023-02-21 01:42:52,006 DEBUG TRAIN Batch 12/7200 loss 8.830624 loss_att 8.802507 loss_ctc 12.916225 loss_rnnt 8.023606 hw_loss 0.502301 lr 0.00048681 rank 6
2023-02-21 01:42:52,016 DEBUG TRAIN Batch 12/7200 loss 18.327291 loss_att 23.568775 loss_ctc 17.002209 loss_rnnt 17.335125 hw_loss 0.226023 lr 0.00048678 rank 7
2023-02-21 01:42:52,023 DEBUG TRAIN Batch 12/7200 loss 10.409898 loss_att 12.885197 loss_ctc 11.618430 loss_rnnt 9.618225 hw_loss 0.254015 lr 0.00048687 rank 1
2023-02-21 01:43:50,387 DEBUG TRAIN Batch 12/7300 loss 26.235313 loss_att 24.829140 loss_ctc 25.189011 loss_rnnt 26.509661 hw_loss 0.274485 lr 0.00048679 rank 0
2023-02-21 01:43:50,390 DEBUG TRAIN Batch 12/7300 loss 36.810184 loss_att 45.162643 loss_ctc 40.245213 loss_rnnt 34.507004 hw_loss 0.327535 lr 0.00048671 rank 2
2023-02-21 01:43:50,394 DEBUG TRAIN Batch 12/7300 loss 20.008221 loss_att 20.763508 loss_ctc 21.428497 loss_rnnt 19.596087 hw_loss 0.134448 lr 0.00048669 rank 6
2023-02-21 01:43:50,395 DEBUG TRAIN Batch 12/7300 loss 7.586069 loss_att 10.262621 loss_ctc 9.830905 loss_rnnt 6.628866 hw_loss 0.229840 lr 0.00048670 rank 5
2023-02-21 01:43:50,398 DEBUG TRAIN Batch 12/7300 loss 11.952081 loss_att 13.541759 loss_ctc 13.872725 loss_rnnt 11.299307 hw_loss 0.147661 lr 0.00048669 rank 3
2023-02-21 01:43:50,400 DEBUG TRAIN Batch 12/7300 loss 11.297645 loss_att 15.553411 loss_ctc 17.334734 loss_rnnt 9.565430 hw_loss 0.142717 lr 0.00048681 rank 4
2023-02-21 01:43:50,412 DEBUG TRAIN Batch 12/7300 loss 24.576157 loss_att 30.051249 loss_ctc 29.584784 loss_rnnt 22.756750 hw_loss 0.106069 lr 0.00048667 rank 7
2023-02-21 01:43:50,456 DEBUG TRAIN Batch 12/7300 loss 7.235516 loss_att 11.885676 loss_ctc 8.902183 loss_rnnt 5.916883 hw_loss 0.311958 lr 0.00048676 rank 1
2023-02-21 01:44:49,457 DEBUG TRAIN Batch 12/7400 loss 18.113028 loss_att 30.641939 loss_ctc 22.508068 loss_rnnt 14.819534 hw_loss 0.378199 lr 0.00048658 rank 5
2023-02-21 01:44:49,458 DEBUG TRAIN Batch 12/7400 loss 12.272373 loss_att 14.953184 loss_ctc 12.891336 loss_rnnt 11.561456 hw_loss 0.172926 lr 0.00048658 rank 6
2023-02-21 01:44:49,459 DEBUG TRAIN Batch 12/7400 loss 10.506400 loss_att 11.240694 loss_ctc 16.564615 loss_rnnt 9.393211 hw_loss 0.297314 lr 0.00048660 rank 2
2023-02-21 01:44:49,460 DEBUG TRAIN Batch 12/7400 loss 6.537079 loss_att 10.114126 loss_ctc 7.330022 loss_rnnt 5.715822 hw_loss 0.000228 lr 0.00048670 rank 4
2023-02-21 01:44:49,462 DEBUG TRAIN Batch 12/7400 loss 24.535616 loss_att 28.678040 loss_ctc 32.974216 loss_rnnt 22.440193 hw_loss 0.265857 lr 0.00048657 rank 3
2023-02-21 01:44:49,462 DEBUG TRAIN Batch 12/7400 loss 2.363622 loss_att 6.095901 loss_ctc 1.998773 loss_rnnt 1.333467 hw_loss 0.623148 lr 0.00048668 rank 0
2023-02-21 01:44:49,469 DEBUG TRAIN Batch 12/7400 loss 12.546773 loss_att 15.239045 loss_ctc 13.686299 loss_rnnt 11.693789 hw_loss 0.304862 lr 0.00048655 rank 7
2023-02-21 01:44:49,527 DEBUG TRAIN Batch 12/7400 loss 5.252409 loss_att 7.931355 loss_ctc 2.786451 loss_rnnt 4.880354 hw_loss 0.309487 lr 0.00048664 rank 1
2023-02-21 01:45:46,678 DEBUG TRAIN Batch 12/7500 loss 14.319384 loss_att 11.727461 loss_ctc 18.100122 loss_rnnt 14.169559 hw_loss 0.307707 lr 0.00048656 rank 0
2023-02-21 01:45:46,678 DEBUG TRAIN Batch 12/7500 loss 23.715805 loss_att 24.541534 loss_ctc 28.434933 loss_rnnt 22.753065 hw_loss 0.315710 lr 0.00048658 rank 4
2023-02-21 01:45:46,680 DEBUG TRAIN Batch 12/7500 loss 14.337055 loss_att 18.697943 loss_ctc 11.667315 loss_rnnt 13.693598 hw_loss 0.238583 lr 0.00048653 rank 1
2023-02-21 01:45:46,681 DEBUG TRAIN Batch 12/7500 loss 71.481941 loss_att 73.493553 loss_ctc 66.597809 loss_rnnt 71.654831 hw_loss 0.142506 lr 0.00048647 rank 5
2023-02-21 01:45:46,683 DEBUG TRAIN Batch 12/7500 loss 5.690458 loss_att 10.593314 loss_ctc 7.032786 loss_rnnt 4.199879 hw_loss 0.620683 lr 0.00048646 rank 3
2023-02-21 01:45:46,684 DEBUG TRAIN Batch 12/7500 loss 20.091312 loss_att 18.622925 loss_ctc 23.175917 loss_rnnt 19.732483 hw_loss 0.452300 lr 0.00048646 rank 6
2023-02-21 01:45:46,687 DEBUG TRAIN Batch 12/7500 loss 29.143696 loss_att 33.352154 loss_ctc 36.360359 loss_rnnt 27.339472 hw_loss 0.000584 lr 0.00048648 rank 2
2023-02-21 01:45:46,695 DEBUG TRAIN Batch 12/7500 loss 23.557581 loss_att 31.625397 loss_ctc 31.605366 loss_rnnt 20.639450 hw_loss 0.434116 lr 0.00048644 rank 7
2023-02-21 01:46:46,401 DEBUG TRAIN Batch 12/7600 loss 15.195223 loss_att 17.719694 loss_ctc 17.471512 loss_rnnt 14.386681 hw_loss 0.000268 lr 0.00048645 rank 0
2023-02-21 01:46:46,407 DEBUG TRAIN Batch 12/7600 loss 13.055001 loss_att 19.246124 loss_ctc 16.876226 loss_rnnt 11.127510 hw_loss 0.337066 lr 0.00048635 rank 6
2023-02-21 01:46:46,409 DEBUG TRAIN Batch 12/7600 loss 15.405120 loss_att 20.738712 loss_ctc 18.271309 loss_rnnt 13.777706 hw_loss 0.334755 lr 0.00048641 rank 1
2023-02-21 01:46:46,410 DEBUG TRAIN Batch 12/7600 loss 20.227547 loss_att 27.569721 loss_ctc 21.947063 loss_rnnt 18.345558 hw_loss 0.345537 lr 0.00048634 rank 3
2023-02-21 01:46:46,413 DEBUG TRAIN Batch 12/7600 loss 17.673962 loss_att 24.728449 loss_ctc 19.423584 loss_rnnt 15.842793 hw_loss 0.350603 lr 0.00048637 rank 2
2023-02-21 01:46:46,418 DEBUG TRAIN Batch 12/7600 loss 21.556387 loss_att 25.593571 loss_ctc 25.677895 loss_rnnt 20.100933 hw_loss 0.184653 lr 0.00048647 rank 4
2023-02-21 01:46:46,427 DEBUG TRAIN Batch 12/7600 loss 10.719748 loss_att 19.999802 loss_ctc 12.126639 loss_rnnt 8.516989 hw_loss 0.298429 lr 0.00048632 rank 7
2023-02-21 01:46:46,469 DEBUG TRAIN Batch 12/7600 loss 21.500546 loss_att 21.408478 loss_ctc 19.919231 loss_rnnt 21.538773 hw_loss 0.358182 lr 0.00048635 rank 5
2023-02-21 01:47:46,242 DEBUG TRAIN Batch 12/7700 loss 3.359224 loss_att 5.569614 loss_ctc 2.104052 loss_rnnt 2.908433 hw_loss 0.330129 lr 0.00048633 rank 0
2023-02-21 01:47:46,245 DEBUG TRAIN Batch 12/7700 loss 57.563766 loss_att 56.775967 loss_ctc 54.632408 loss_rnnt 57.926029 hw_loss 0.349015 lr 0.00048623 rank 6
2023-02-21 01:47:46,246 DEBUG TRAIN Batch 12/7700 loss 17.188766 loss_att 19.450434 loss_ctc 19.264694 loss_rnnt 16.459457 hw_loss 0.000349 lr 0.00048630 rank 1
2023-02-21 01:47:46,249 DEBUG TRAIN Batch 12/7700 loss 36.532738 loss_att 48.960419 loss_ctc 52.826023 loss_rnnt 31.715803 hw_loss 0.298054 lr 0.00048625 rank 2
2023-02-21 01:47:46,250 DEBUG TRAIN Batch 12/7700 loss 9.046595 loss_att 14.379852 loss_ctc 11.342991 loss_rnnt 7.617592 hw_loss 0.105310 lr 0.00048624 rank 5
2023-02-21 01:47:46,252 DEBUG TRAIN Batch 12/7700 loss 22.322023 loss_att 27.162329 loss_ctc 24.768621 loss_rnnt 20.934595 hw_loss 0.174659 lr 0.00048623 rank 3
2023-02-21 01:47:46,256 DEBUG TRAIN Batch 12/7700 loss 9.533622 loss_att 16.382296 loss_ctc 14.501348 loss_rnnt 7.501159 hw_loss 0.000685 lr 0.00048621 rank 7
2023-02-21 01:47:46,319 DEBUG TRAIN Batch 12/7700 loss 13.190827 loss_att 17.512724 loss_ctc 17.626398 loss_rnnt 11.734889 hw_loss 0.000279 lr 0.00048635 rank 4
2023-02-21 01:49:03,495 DEBUG TRAIN Batch 12/7800 loss 13.533763 loss_att 17.806606 loss_ctc 19.795738 loss_rnnt 11.726615 hw_loss 0.220593 lr 0.00048611 rank 3
2023-02-21 01:49:03,496 DEBUG TRAIN Batch 12/7800 loss 12.536648 loss_att 13.830593 loss_ctc 15.441707 loss_rnnt 11.890118 hw_loss 0.000751 lr 0.00048622 rank 0
2023-02-21 01:49:03,497 DEBUG TRAIN Batch 12/7800 loss 59.056561 loss_att 81.681412 loss_ctc 69.157326 loss_rnnt 53.009605 hw_loss 0.328530 lr 0.00048614 rank 2
2023-02-21 01:49:03,498 DEBUG TRAIN Batch 12/7800 loss 19.232346 loss_att 21.565725 loss_ctc 24.084639 loss_rnnt 17.996712 hw_loss 0.228721 lr 0.00048612 rank 6
2023-02-21 01:49:03,500 DEBUG TRAIN Batch 12/7800 loss 17.470770 loss_att 18.390858 loss_ctc 20.024094 loss_rnnt 16.730900 hw_loss 0.403890 lr 0.00048624 rank 4
2023-02-21 01:49:03,503 DEBUG TRAIN Batch 12/7800 loss 5.131863 loss_att 7.442479 loss_ctc 5.265800 loss_rnnt 4.513126 hw_loss 0.260167 lr 0.00048618 rank 1
2023-02-21 01:49:03,504 DEBUG TRAIN Batch 12/7800 loss 16.654131 loss_att 15.831263 loss_ctc 19.081274 loss_rnnt 16.267511 hw_loss 0.426697 lr 0.00048612 rank 5
2023-02-21 01:49:03,509 DEBUG TRAIN Batch 12/7800 loss 8.529993 loss_att 11.362524 loss_ctc 12.644875 loss_rnnt 7.414145 hw_loss 0.001294 lr 0.00048609 rank 7
2023-02-21 01:50:20,075 DEBUG TRAIN Batch 12/7900 loss 31.962042 loss_att 37.950832 loss_ctc 31.164841 loss_rnnt 30.742851 hw_loss 0.239485 lr 0.00048610 rank 0
2023-02-21 01:50:20,079 DEBUG TRAIN Batch 12/7900 loss 58.230122 loss_att 63.446400 loss_ctc 76.446152 loss_rnnt 54.757778 hw_loss 0.000530 lr 0.00048600 rank 6
2023-02-21 01:50:20,080 DEBUG TRAIN Batch 12/7900 loss 9.717629 loss_att 14.112947 loss_ctc 12.840938 loss_rnnt 8.228922 hw_loss 0.362254 lr 0.00048612 rank 4
2023-02-21 01:50:20,080 DEBUG TRAIN Batch 12/7900 loss 33.222240 loss_att 35.539932 loss_ctc 39.006306 loss_rnnt 31.937157 hw_loss 0.094387 lr 0.00048601 rank 5
2023-02-21 01:50:20,082 DEBUG TRAIN Batch 12/7900 loss 21.245941 loss_att 21.693171 loss_ctc 22.251869 loss_rnnt 20.775379 hw_loss 0.463112 lr 0.00048602 rank 2
2023-02-21 01:50:20,085 DEBUG TRAIN Batch 12/7900 loss 24.246103 loss_att 27.731081 loss_ctc 28.114344 loss_rnnt 22.846790 hw_loss 0.349785 lr 0.00048607 rank 1
2023-02-21 01:50:20,087 DEBUG TRAIN Batch 12/7900 loss 10.823733 loss_att 14.672554 loss_ctc 14.146892 loss_rnnt 9.409586 hw_loss 0.377429 lr 0.00048598 rank 7
2023-02-21 01:50:20,091 DEBUG TRAIN Batch 12/7900 loss 6.230876 loss_att 10.961787 loss_ctc 8.387518 loss_rnnt 4.964169 hw_loss 0.061826 lr 0.00048600 rank 3
2023-02-21 01:51:18,375 DEBUG TRAIN Batch 12/8000 loss 31.339701 loss_att 33.975281 loss_ctc 40.534050 loss_rnnt 29.408377 hw_loss 0.334301 lr 0.00048599 rank 0
2023-02-21 01:51:18,377 DEBUG TRAIN Batch 12/8000 loss 2.735985 loss_att 7.334309 loss_ctc 5.315548 loss_rnnt 1.240427 hw_loss 0.434909 lr 0.00048591 rank 2
2023-02-21 01:51:18,377 DEBUG TRAIN Batch 12/8000 loss 10.401742 loss_att 12.987989 loss_ctc 8.142514 loss_rnnt 10.084403 hw_loss 0.189975 lr 0.00048589 rank 5
2023-02-21 01:51:18,379 DEBUG TRAIN Batch 12/8000 loss 30.701653 loss_att 32.860542 loss_ctc 33.428482 loss_rnnt 29.750217 hw_loss 0.292651 lr 0.00048588 rank 3
2023-02-21 01:51:18,378 DEBUG TRAIN Batch 12/8000 loss 2.077232 loss_att 7.387868 loss_ctc 2.290481 loss_rnnt 0.937710 hw_loss 0.091804 lr 0.00048589 rank 6
2023-02-21 01:51:18,380 DEBUG TRAIN Batch 12/8000 loss 8.748382 loss_att 13.111565 loss_ctc 13.097641 loss_rnnt 7.153028 hw_loss 0.267776 lr 0.00048601 rank 4
2023-02-21 01:51:18,381 DEBUG TRAIN Batch 12/8000 loss 10.072482 loss_att 19.555954 loss_ctc 12.214803 loss_rnnt 7.621981 hw_loss 0.502809 lr 0.00048586 rank 7
2023-02-21 01:51:18,383 DEBUG TRAIN Batch 12/8000 loss 25.805399 loss_att 26.709469 loss_ctc 28.322594 loss_rnnt 25.022812 hw_loss 0.499026 lr 0.00048595 rank 1
2023-02-21 01:52:15,076 DEBUG TRAIN Batch 12/8100 loss 5.331594 loss_att 9.908955 loss_ctc 6.636435 loss_rnnt 4.046731 hw_loss 0.366397 lr 0.00048587 rank 0
2023-02-21 01:52:15,082 DEBUG TRAIN Batch 12/8100 loss 40.004696 loss_att 46.379665 loss_ctc 46.292198 loss_rnnt 37.740978 hw_loss 0.281987 lr 0.00048577 rank 6
2023-02-21 01:52:15,083 DEBUG TRAIN Batch 12/8100 loss 11.953026 loss_att 11.635070 loss_ctc 12.723997 loss_rnnt 11.684947 hw_loss 0.429139 lr 0.00048579 rank 2
2023-02-21 01:52:15,087 DEBUG TRAIN Batch 12/8100 loss 20.509010 loss_att 21.299337 loss_ctc 26.986088 loss_rnnt 19.338554 hw_loss 0.278964 lr 0.00048575 rank 7
2023-02-21 01:52:15,089 DEBUG TRAIN Batch 12/8100 loss 14.572034 loss_att 16.493505 loss_ctc 21.077635 loss_rnnt 13.171420 hw_loss 0.279201 lr 0.00048589 rank 4
2023-02-21 01:52:15,090 DEBUG TRAIN Batch 12/8100 loss 13.212379 loss_att 16.124180 loss_ctc 14.578006 loss_rnnt 12.261005 hw_loss 0.350495 lr 0.00048577 rank 3
2023-02-21 01:52:15,090 DEBUG TRAIN Batch 12/8100 loss 4.358762 loss_att 6.226817 loss_ctc 5.316246 loss_rnnt 3.677336 hw_loss 0.337783 lr 0.00048584 rank 1
2023-02-21 01:52:15,091 DEBUG TRAIN Batch 12/8100 loss 20.573345 loss_att 19.922535 loss_ctc 19.151896 loss_rnnt 20.737404 hw_loss 0.291804 lr 0.00048578 rank 5
2023-02-21 01:53:15,516 DEBUG TRAIN Batch 12/8200 loss 12.963401 loss_att 19.921562 loss_ctc 19.443001 loss_rnnt 10.604448 hw_loss 0.193824 lr 0.00048576 rank 0
2023-02-21 01:53:15,527 DEBUG TRAIN Batch 12/8200 loss 23.113323 loss_att 24.125820 loss_ctc 32.925648 loss_rnnt 21.601728 hw_loss 0.001472 lr 0.00048572 rank 1
2023-02-21 01:53:15,528 DEBUG TRAIN Batch 12/8200 loss 11.254460 loss_att 12.703784 loss_ctc 11.065649 loss_rnnt 10.876124 hw_loss 0.213085 lr 0.00048568 rank 2
2023-02-21 01:53:15,529 DEBUG TRAIN Batch 12/8200 loss 6.659531 loss_att 9.848650 loss_ctc 5.637843 loss_rnnt 5.953341 hw_loss 0.383610 lr 0.00048578 rank 4
2023-02-21 01:53:15,534 DEBUG TRAIN Batch 12/8200 loss 4.132054 loss_att 8.102369 loss_ctc 3.717465 loss_rnnt 3.198817 hw_loss 0.364599 lr 0.00048566 rank 6
2023-02-21 01:53:15,533 DEBUG TRAIN Batch 12/8200 loss 10.900935 loss_att 16.969027 loss_ctc 12.869997 loss_rnnt 9.207439 hw_loss 0.407505 lr 0.00048566 rank 5
2023-02-21 01:53:15,535 DEBUG TRAIN Batch 12/8200 loss 32.421249 loss_att 32.836708 loss_ctc 34.238770 loss_rnnt 32.094807 hw_loss 0.001900 lr 0.00048565 rank 3
2023-02-21 01:53:15,541 DEBUG TRAIN Batch 12/8200 loss 3.324851 loss_att 6.487885 loss_ctc 5.687490 loss_rnnt 2.325593 hw_loss 0.096812 lr 0.00048563 rank 7
2023-02-21 01:54:13,664 DEBUG TRAIN Batch 12/8300 loss 37.242722 loss_att 37.342175 loss_ctc 37.730644 loss_rnnt 36.992752 hw_loss 0.309415 lr 0.00048565 rank 0
2023-02-21 01:54:13,672 DEBUG TRAIN Batch 12/8300 loss 9.676140 loss_att 20.674793 loss_ctc 12.319421 loss_rnnt 6.922783 hw_loss 0.377229 lr 0.00048555 rank 5
2023-02-21 01:54:13,672 DEBUG TRAIN Batch 12/8300 loss 5.372985 loss_att 5.962236 loss_ctc 4.733312 loss_rnnt 5.196661 hw_loss 0.269558 lr 0.00048554 rank 3
2023-02-21 01:54:13,672 DEBUG TRAIN Batch 12/8300 loss 9.580894 loss_att 16.644161 loss_ctc 10.348206 loss_rnnt 7.916822 hw_loss 0.279584 lr 0.00048561 rank 1
2023-02-21 01:54:13,676 DEBUG TRAIN Batch 12/8300 loss 18.590118 loss_att 27.466267 loss_ctc 27.985945 loss_rnnt 15.403540 hw_loss 0.297324 lr 0.00048554 rank 6
2023-02-21 01:54:13,677 DEBUG TRAIN Batch 12/8300 loss 24.214436 loss_att 29.340454 loss_ctc 20.920387 loss_rnnt 23.628334 hw_loss 0.000195 lr 0.00048556 rank 2
2023-02-21 01:54:13,678 DEBUG TRAIN Batch 12/8300 loss 13.973166 loss_att 19.280140 loss_ctc 11.053459 loss_rnnt 13.247053 hw_loss 0.101273 lr 0.00048566 rank 4
2023-02-21 01:54:13,684 DEBUG TRAIN Batch 12/8300 loss 2.160484 loss_att 4.386598 loss_ctc 1.200791 loss_rnnt 1.776006 hw_loss 0.126027 lr 0.00048552 rank 7
2023-02-21 01:55:11,256 DEBUG TRAIN Batch 12/8400 loss 23.642164 loss_att 26.426231 loss_ctc 24.927168 loss_rnnt 22.791933 hw_loss 0.228906 lr 0.00048553 rank 0
2023-02-21 01:55:11,259 DEBUG TRAIN Batch 12/8400 loss 50.426350 loss_att 50.932961 loss_ctc 59.443893 loss_rnnt 48.977520 hw_loss 0.272184 lr 0.00048545 rank 2
2023-02-21 01:55:11,265 DEBUG TRAIN Batch 12/8400 loss 10.687059 loss_att 12.732162 loss_ctc 12.273513 loss_rnnt 9.948466 hw_loss 0.221337 lr 0.00048549 rank 1
2023-02-21 01:55:11,267 DEBUG TRAIN Batch 12/8400 loss 22.204296 loss_att 25.549648 loss_ctc 24.139175 loss_rnnt 21.123436 hw_loss 0.288384 lr 0.00048543 rank 5
2023-02-21 01:55:11,269 DEBUG TRAIN Batch 12/8400 loss 15.327402 loss_att 15.273106 loss_ctc 15.828371 loss_rnnt 15.064112 hw_loss 0.388786 lr 0.00048543 rank 3
2023-02-21 01:55:11,270 DEBUG TRAIN Batch 12/8400 loss 11.461922 loss_att 18.606558 loss_ctc 12.122303 loss_rnnt 9.715225 hw_loss 0.430723 lr 0.00048543 rank 6
2023-02-21 01:55:11,274 DEBUG TRAIN Batch 12/8400 loss 17.746384 loss_att 21.108444 loss_ctc 24.995829 loss_rnnt 15.886377 hw_loss 0.414378 lr 0.00048540 rank 7
2023-02-21 01:55:11,279 DEBUG TRAIN Batch 12/8400 loss 5.242777 loss_att 8.723735 loss_ctc 3.299591 loss_rnnt 4.517036 hw_loss 0.541202 lr 0.00048555 rank 4
2023-02-21 01:56:11,565 DEBUG TRAIN Batch 12/8500 loss 4.491113 loss_att 12.135549 loss_ctc 7.562383 loss_rnnt 2.435626 hw_loss 0.219556 lr 0.00048532 rank 6
2023-02-21 01:56:11,566 DEBUG TRAIN Batch 12/8500 loss 6.628581 loss_att 9.856703 loss_ctc 10.193822 loss_rnnt 5.507390 hw_loss 0.000378 lr 0.00048542 rank 0
2023-02-21 01:56:11,569 DEBUG TRAIN Batch 12/8500 loss 33.230499 loss_att 49.253757 loss_ctc 39.139866 loss_rnnt 29.237661 hw_loss 0.000504 lr 0.00048538 rank 1
2023-02-21 01:56:11,569 DEBUG TRAIN Batch 12/8500 loss 29.585283 loss_att 35.216373 loss_ctc 30.239109 loss_rnnt 28.248436 hw_loss 0.231473 lr 0.00048533 rank 2
2023-02-21 01:56:11,569 DEBUG TRAIN Batch 12/8500 loss 6.428018 loss_att 7.505757 loss_ctc 6.068336 loss_rnnt 6.006097 hw_loss 0.476869 lr 0.00048529 rank 7
2023-02-21 01:56:11,574 DEBUG TRAIN Batch 12/8500 loss 13.863471 loss_att 15.205326 loss_ctc 17.095644 loss_rnnt 13.163762 hw_loss 0.000714 lr 0.00048532 rank 5
2023-02-21 01:56:11,576 DEBUG TRAIN Batch 12/8500 loss 9.976462 loss_att 12.201882 loss_ctc 13.979883 loss_rnnt 8.935612 hw_loss 0.116207 lr 0.00048531 rank 3
2023-02-21 01:56:11,582 DEBUG TRAIN Batch 12/8500 loss 24.144272 loss_att 28.685619 loss_ctc 31.837132 loss_rnnt 22.210133 hw_loss 0.000287 lr 0.00048543 rank 4
2023-02-21 01:57:29,527 DEBUG TRAIN Batch 12/8600 loss 32.627338 loss_att 46.651199 loss_ctc 41.913261 loss_rnnt 28.479637 hw_loss 0.196516 lr 0.00048530 rank 0
2023-02-21 01:57:29,531 DEBUG TRAIN Batch 12/8600 loss 6.718582 loss_att 9.858950 loss_ctc 16.737831 loss_rnnt 4.629300 hw_loss 0.234955 lr 0.00048522 rank 2
2023-02-21 01:57:29,532 DEBUG TRAIN Batch 12/8600 loss 7.200034 loss_att 11.252930 loss_ctc 11.696550 loss_rnnt 5.654807 hw_loss 0.253336 lr 0.00048520 rank 3
2023-02-21 01:57:29,532 DEBUG TRAIN Batch 12/8600 loss 24.046999 loss_att 20.659349 loss_ctc 25.111389 loss_rnnt 24.467873 hw_loss 0.215133 lr 0.00048521 rank 5
2023-02-21 01:57:29,536 DEBUG TRAIN Batch 12/8600 loss 15.670403 loss_att 33.325058 loss_ctc 22.338633 loss_rnnt 11.250289 hw_loss 0.000162 lr 0.00048520 rank 6
2023-02-21 01:57:29,537 DEBUG TRAIN Batch 12/8600 loss 19.230814 loss_att 18.465042 loss_ctc 19.615982 loss_rnnt 19.100008 hw_loss 0.436134 lr 0.00048527 rank 1
2023-02-21 01:57:29,537 DEBUG TRAIN Batch 12/8600 loss 6.371307 loss_att 9.219025 loss_ctc 8.686843 loss_rnnt 5.242968 hw_loss 0.468856 lr 0.00048518 rank 7
2023-02-21 01:57:29,540 DEBUG TRAIN Batch 12/8600 loss 25.345701 loss_att 30.981342 loss_ctc 33.540787 loss_rnnt 23.049057 hw_loss 0.144070 lr 0.00048532 rank 4
2023-02-21 01:58:28,833 DEBUG TRAIN Batch 12/8700 loss 6.444212 loss_att 8.815682 loss_ctc 6.108473 loss_rnnt 5.843214 hw_loss 0.321505 lr 0.00048519 rank 0
2023-02-21 01:58:28,842 DEBUG TRAIN Batch 12/8700 loss 31.652788 loss_att 33.358299 loss_ctc 38.480377 loss_rnnt 30.234709 hw_loss 0.312436 lr 0.00048508 rank 3
2023-02-21 01:58:28,844 DEBUG TRAIN Batch 12/8700 loss 12.953199 loss_att 17.543432 loss_ctc 15.258092 loss_rnnt 11.557384 hw_loss 0.319595 lr 0.00048515 rank 1
2023-02-21 01:58:28,844 DEBUG TRAIN Batch 12/8700 loss 10.087878 loss_att 15.484299 loss_ctc 14.096987 loss_rnnt 8.210212 hw_loss 0.494689 lr 0.00048509 rank 6
2023-02-21 01:58:28,845 DEBUG TRAIN Batch 12/8700 loss 11.870827 loss_att 16.581583 loss_ctc 18.028351 loss_rnnt 9.926235 hw_loss 0.340195 lr 0.00048511 rank 2
2023-02-21 01:58:28,846 DEBUG TRAIN Batch 12/8700 loss 17.310480 loss_att 20.467880 loss_ctc 21.345713 loss_rnnt 16.027330 hw_loss 0.213069 lr 0.00048509 rank 5
2023-02-21 01:58:28,847 DEBUG TRAIN Batch 12/8700 loss 7.361071 loss_att 10.316851 loss_ctc 8.327196 loss_rnnt 6.403421 hw_loss 0.445642 lr 0.00048520 rank 4
2023-02-21 01:58:28,852 DEBUG TRAIN Batch 12/8700 loss 7.457218 loss_att 12.100869 loss_ctc 12.046864 loss_rnnt 5.692431 hw_loss 0.420196 lr 0.00048506 rank 7
2023-02-21 01:59:28,076 DEBUG TRAIN Batch 12/8800 loss 53.447651 loss_att 84.398193 loss_ctc 63.689350 loss_rnnt 45.751938 hw_loss 0.262582 lr 0.00048507 rank 0
2023-02-21 01:59:28,080 DEBUG TRAIN Batch 12/8800 loss 29.978443 loss_att 39.078346 loss_ctc 42.231148 loss_rnnt 26.422379 hw_loss 0.191985 lr 0.00048498 rank 5
2023-02-21 01:59:28,082 DEBUG TRAIN Batch 12/8800 loss 7.124655 loss_att 15.183634 loss_ctc 8.903935 loss_rnnt 5.075394 hw_loss 0.375426 lr 0.00048504 rank 1
2023-02-21 01:59:28,083 DEBUG TRAIN Batch 12/8800 loss 4.287381 loss_att 8.260472 loss_ctc 2.665925 loss_rnnt 3.588125 hw_loss 0.226560 lr 0.00048499 rank 2
2023-02-21 01:59:28,086 DEBUG TRAIN Batch 12/8800 loss 14.237889 loss_att 19.136675 loss_ctc 13.131778 loss_rnnt 13.197785 hw_loss 0.389676 lr 0.00048509 rank 4
2023-02-21 01:59:28,088 DEBUG TRAIN Batch 12/8800 loss 19.697628 loss_att 24.647657 loss_ctc 21.973743 loss_rnnt 18.158789 hw_loss 0.460035 lr 0.00048495 rank 7
2023-02-21 01:59:28,095 DEBUG TRAIN Batch 12/8800 loss 11.942354 loss_att 24.126169 loss_ctc 11.196699 loss_rnnt 9.392395 hw_loss 0.398654 lr 0.00048497 rank 3
2023-02-21 01:59:28,144 DEBUG TRAIN Batch 12/8800 loss 3.536960 loss_att 4.390107 loss_ctc 3.028720 loss_rnnt 3.214090 hw_loss 0.412511 lr 0.00048497 rank 6
2023-02-21 02:00:25,416 DEBUG TRAIN Batch 12/8900 loss 20.807981 loss_att 17.339722 loss_ctc 20.211260 loss_rnnt 21.454432 hw_loss 0.237683 lr 0.00048496 rank 0
2023-02-21 02:00:25,418 DEBUG TRAIN Batch 12/8900 loss 16.065325 loss_att 18.149517 loss_ctc 15.191422 loss_rnnt 15.707573 hw_loss 0.107691 lr 0.00048485 rank 3
2023-02-21 02:00:25,419 DEBUG TRAIN Batch 12/8900 loss 3.551948 loss_att 8.613786 loss_ctc 5.827930 loss_rnnt 2.077538 hw_loss 0.297335 lr 0.00048486 rank 5
2023-02-21 02:00:25,419 DEBUG TRAIN Batch 12/8900 loss 13.774597 loss_att 20.351931 loss_ctc 19.005693 loss_rnnt 11.579346 hw_loss 0.341823 lr 0.00048488 rank 2
2023-02-21 02:00:25,421 DEBUG TRAIN Batch 12/8900 loss 19.865578 loss_att 18.386633 loss_ctc 23.089022 loss_rnnt 19.518532 hw_loss 0.399452 lr 0.00048486 rank 6
2023-02-21 02:00:25,422 DEBUG TRAIN Batch 12/8900 loss 13.291696 loss_att 15.171814 loss_ctc 12.893829 loss_rnnt 12.816971 hw_loss 0.284530 lr 0.00048483 rank 7
2023-02-21 02:00:25,427 DEBUG TRAIN Batch 12/8900 loss 14.395605 loss_att 20.582056 loss_ctc 11.599143 loss_rnnt 13.448803 hw_loss 0.154451 lr 0.00048498 rank 4
2023-02-21 02:00:25,427 DEBUG TRAIN Batch 12/8900 loss 3.687106 loss_att 6.253117 loss_ctc 4.495275 loss_rnnt 2.830849 hw_loss 0.441186 lr 0.00048492 rank 1
2023-02-21 02:01:23,428 DEBUG TRAIN Batch 12/9000 loss 7.059927 loss_att 11.484430 loss_ctc 9.189600 loss_rnnt 5.785470 hw_loss 0.198000 lr 0.00048485 rank 0
2023-02-21 02:01:23,434 DEBUG TRAIN Batch 12/9000 loss 8.223320 loss_att 12.865320 loss_ctc 8.582829 loss_rnnt 7.115648 hw_loss 0.246259 lr 0.00048475 rank 6
2023-02-21 02:01:23,438 DEBUG TRAIN Batch 12/9000 loss 16.844671 loss_att 17.537434 loss_ctc 16.079483 loss_rnnt 16.666386 hw_loss 0.265792 lr 0.00048474 rank 3
2023-02-21 02:01:23,439 DEBUG TRAIN Batch 12/9000 loss 24.354279 loss_att 34.033070 loss_ctc 28.428261 loss_rnnt 21.674442 hw_loss 0.376643 lr 0.00048481 rank 1
2023-02-21 02:01:23,440 DEBUG TRAIN Batch 12/9000 loss 9.878239 loss_att 15.688492 loss_ctc 9.288858 loss_rnnt 8.794696 hw_loss 0.000144 lr 0.00048476 rank 2
2023-02-21 02:01:23,444 DEBUG TRAIN Batch 12/9000 loss 25.971458 loss_att 27.888624 loss_ctc 28.253727 loss_rnnt 25.080601 hw_loss 0.380852 lr 0.00048475 rank 5
2023-02-21 02:01:23,447 DEBUG TRAIN Batch 12/9000 loss 8.243446 loss_att 15.634768 loss_ctc 9.464121 loss_rnnt 6.505145 hw_loss 0.182400 lr 0.00048472 rank 7
2023-02-21 02:01:23,447 DEBUG TRAIN Batch 12/9000 loss 5.195392 loss_att 9.553602 loss_ctc 9.817440 loss_rnnt 3.707407 hw_loss 0.000129 lr 0.00048486 rank 4
2023-02-21 02:02:22,847 DEBUG TRAIN Batch 12/9100 loss 15.021452 loss_att 17.885904 loss_ctc 18.561897 loss_rnnt 13.976295 hw_loss 0.000390 lr 0.00048473 rank 0
2023-02-21 02:02:22,848 DEBUG TRAIN Batch 12/9100 loss 11.339314 loss_att 18.818691 loss_ctc 21.383299 loss_rnnt 8.324237 hw_loss 0.337506 lr 0.00048465 rank 2
2023-02-21 02:02:22,854 DEBUG TRAIN Batch 12/9100 loss 8.325760 loss_att 11.003505 loss_ctc 8.392979 loss_rnnt 7.657735 hw_loss 0.231589 lr 0.00048464 rank 5
2023-02-21 02:02:22,855 DEBUG TRAIN Batch 12/9100 loss 6.196347 loss_att 6.727889 loss_ctc 4.876382 loss_rnnt 6.087322 hw_loss 0.335085 lr 0.00048463 rank 3
2023-02-21 02:02:22,857 DEBUG TRAIN Batch 12/9100 loss 17.834160 loss_att 22.024191 loss_ctc 31.511986 loss_rnnt 14.973175 hw_loss 0.373631 lr 0.00048463 rank 6
2023-02-21 02:02:22,861 DEBUG TRAIN Batch 12/9100 loss 25.693609 loss_att 28.894524 loss_ctc 46.962063 loss_rnnt 21.998276 hw_loss 0.411292 lr 0.00048470 rank 1
2023-02-21 02:02:22,864 DEBUG TRAIN Batch 12/9100 loss 13.112247 loss_att 24.381290 loss_ctc 18.590643 loss_rnnt 9.864267 hw_loss 0.494471 lr 0.00048475 rank 4
2023-02-21 02:02:22,864 DEBUG TRAIN Batch 12/9100 loss 17.182634 loss_att 18.675182 loss_ctc 20.127695 loss_rnnt 16.324486 hw_loss 0.313060 lr 0.00048461 rank 7
2023-02-21 02:03:21,128 DEBUG TRAIN Batch 12/9200 loss 4.019259 loss_att 9.214566 loss_ctc 5.320708 loss_rnnt 2.712514 hw_loss 0.176544 lr 0.00048462 rank 0
2023-02-21 02:03:21,129 DEBUG TRAIN Batch 12/9200 loss 8.387078 loss_att 18.555773 loss_ctc 8.515847 loss_rnnt 6.218738 hw_loss 0.220186 lr 0.00048454 rank 2
2023-02-21 02:03:21,134 DEBUG TRAIN Batch 12/9200 loss 26.595631 loss_att 28.489388 loss_ctc 39.291027 loss_rnnt 24.523663 hw_loss 0.000934 lr 0.00048452 rank 5
2023-02-21 02:03:21,134 DEBUG TRAIN Batch 12/9200 loss 19.383268 loss_att 24.012260 loss_ctc 18.397482 loss_rnnt 18.311604 hw_loss 0.519945 lr 0.00048451 rank 3
2023-02-21 02:03:21,136 DEBUG TRAIN Batch 12/9200 loss 16.986139 loss_att 19.483723 loss_ctc 25.829350 loss_rnnt 15.154341 hw_loss 0.287226 lr 0.00048452 rank 6
2023-02-21 02:03:21,137 DEBUG TRAIN Batch 12/9200 loss 3.603331 loss_att 4.685553 loss_ctc 4.153963 loss_rnnt 3.100857 hw_loss 0.398649 lr 0.00048463 rank 4
2023-02-21 02:03:21,138 DEBUG TRAIN Batch 12/9200 loss 37.255859 loss_att 35.310059 loss_ctc 44.388176 loss_rnnt 36.399765 hw_loss 0.551773 lr 0.00048458 rank 1
2023-02-21 02:03:21,154 DEBUG TRAIN Batch 12/9200 loss 8.467590 loss_att 12.489098 loss_ctc 8.323554 loss_rnnt 7.489507 hw_loss 0.361850 lr 0.00048449 rank 7
2023-02-21 02:04:20,464 DEBUG TRAIN Batch 12/9300 loss 22.827982 loss_att 26.294090 loss_ctc 25.670475 loss_rnnt 21.624208 hw_loss 0.246663 lr 0.00048450 rank 0
2023-02-21 02:04:20,466 DEBUG TRAIN Batch 12/9300 loss 8.898100 loss_att 12.713518 loss_ctc 10.953424 loss_rnnt 7.642433 hw_loss 0.409760 lr 0.00048441 rank 5
2023-02-21 02:04:20,466 DEBUG TRAIN Batch 12/9300 loss 3.104992 loss_att 5.196257 loss_ctc 3.692303 loss_rnnt 2.382844 hw_loss 0.422976 lr 0.00048440 rank 3
2023-02-21 02:04:20,468 DEBUG TRAIN Batch 12/9300 loss 15.716389 loss_att 29.081205 loss_ctc 23.660872 loss_rnnt 11.983470 hw_loss 0.001298 lr 0.00048440 rank 6
2023-02-21 02:04:20,468 DEBUG TRAIN Batch 12/9300 loss 21.845070 loss_att 29.230644 loss_ctc 22.366497 loss_rnnt 20.119141 hw_loss 0.336170 lr 0.00048442 rank 2
2023-02-21 02:04:20,473 DEBUG TRAIN Batch 12/9300 loss 6.873559 loss_att 13.577555 loss_ctc 9.067803 loss_rnnt 5.188649 hw_loss 0.096649 lr 0.00048452 rank 4
2023-02-21 02:04:20,475 DEBUG TRAIN Batch 12/9300 loss 21.543211 loss_att 25.019974 loss_ctc 25.241539 loss_rnnt 20.149994 hw_loss 0.383912 lr 0.00048447 rank 1
2023-02-21 02:04:20,535 DEBUG TRAIN Batch 12/9300 loss 18.023197 loss_att 26.041748 loss_ctc 24.881031 loss_rnnt 15.504383 hw_loss 0.001363 lr 0.00048438 rank 7
2023-02-21 02:05:19,782 DEBUG TRAIN Batch 12/9400 loss 10.904510 loss_att 14.112631 loss_ctc 12.245073 loss_rnnt 9.939138 hw_loss 0.271882 lr 0.00048439 rank 0
2023-02-21 02:05:19,790 DEBUG TRAIN Batch 12/9400 loss 29.207014 loss_att 33.492836 loss_ctc 32.911961 loss_rnnt 27.627586 hw_loss 0.428004 lr 0.00048429 rank 5
2023-02-21 02:05:19,791 DEBUG TRAIN Batch 12/9400 loss 8.585060 loss_att 9.185669 loss_ctc 9.911962 loss_rnnt 8.287937 hw_loss 0.000153 lr 0.00048429 rank 3
2023-02-21 02:05:19,793 DEBUG TRAIN Batch 12/9400 loss 20.525635 loss_att 14.083740 loss_ctc 25.897678 loss_rnnt 20.847324 hw_loss 0.469532 lr 0.00048431 rank 2
2023-02-21 02:05:19,794 DEBUG TRAIN Batch 12/9400 loss 18.695421 loss_att 26.889301 loss_ctc 17.839275 loss_rnnt 17.044353 hw_loss 0.237085 lr 0.00048441 rank 4
2023-02-21 02:05:19,795 DEBUG TRAIN Batch 12/9400 loss 35.907707 loss_att 35.366871 loss_ctc 39.528107 loss_rnnt 35.321140 hw_loss 0.397531 lr 0.00048427 rank 7
2023-02-21 02:05:19,798 DEBUG TRAIN Batch 12/9400 loss 57.911514 loss_att 70.326965 loss_ctc 82.334419 loss_rnnt 52.096447 hw_loss 0.141729 lr 0.00048429 rank 6
2023-02-21 02:05:19,860 DEBUG TRAIN Batch 12/9400 loss 2.967333 loss_att 6.835725 loss_ctc 3.694123 loss_rnnt 2.096646 hw_loss 0.000192 lr 0.00048435 rank 1
2023-02-21 02:06:40,099 DEBUG TRAIN Batch 12/9500 loss 7.655489 loss_att 11.382523 loss_ctc 9.445992 loss_rnnt 6.549367 hw_loss 0.228716 lr 0.00048428 rank 0
2023-02-21 02:06:40,106 DEBUG TRAIN Batch 12/9500 loss 19.541550 loss_att 20.754307 loss_ctc 25.392445 loss_rnnt 18.442196 hw_loss 0.143783 lr 0.00048424 rank 1
2023-02-21 02:06:40,105 DEBUG TRAIN Batch 12/9500 loss 23.345257 loss_att 23.122011 loss_ctc 31.235891 loss_rnnt 22.174482 hw_loss 0.306261 lr 0.00048418 rank 5
2023-02-21 02:06:40,108 DEBUG TRAIN Batch 12/9500 loss 12.508123 loss_att 12.471364 loss_ctc 12.900562 loss_rnnt 12.295365 hw_loss 0.314596 lr 0.00048418 rank 6
2023-02-21 02:06:40,111 DEBUG TRAIN Batch 12/9500 loss 18.709162 loss_att 27.062500 loss_ctc 24.138111 loss_rnnt 16.032497 hw_loss 0.529001 lr 0.00048419 rank 2
2023-02-21 02:06:40,114 DEBUG TRAIN Batch 12/9500 loss 48.358089 loss_att 48.892456 loss_ctc 61.633896 loss_rnnt 46.235928 hw_loss 0.459705 lr 0.00048417 rank 3
2023-02-21 02:06:40,117 DEBUG TRAIN Batch 12/9500 loss 39.378792 loss_att 37.791084 loss_ctc 46.678116 loss_rnnt 38.555618 hw_loss 0.314009 lr 0.00048415 rank 7
2023-02-21 02:06:40,167 DEBUG TRAIN Batch 12/9500 loss 20.773821 loss_att 20.554577 loss_ctc 26.991791 loss_rnnt 19.847374 hw_loss 0.264810 lr 0.00048429 rank 4
2023-02-21 02:07:56,984 DEBUG TRAIN Batch 12/9600 loss 25.176605 loss_att 35.295143 loss_ctc 29.724773 loss_rnnt 22.364494 hw_loss 0.341215 lr 0.00048416 rank 0
2023-02-21 02:07:56,990 DEBUG TRAIN Batch 12/9600 loss 5.001167 loss_att 11.312243 loss_ctc 6.728818 loss_rnnt 3.434271 hw_loss 0.139364 lr 0.00048408 rank 2
2023-02-21 02:07:56,992 DEBUG TRAIN Batch 12/9600 loss 12.812431 loss_att 17.245415 loss_ctc 17.979382 loss_rnnt 11.168757 hw_loss 0.127782 lr 0.00048406 rank 3
2023-02-21 02:07:56,993 DEBUG TRAIN Batch 12/9600 loss 28.981108 loss_att 35.808212 loss_ctc 33.426292 loss_rnnt 26.893246 hw_loss 0.243286 lr 0.00048407 rank 5
2023-02-21 02:07:56,994 DEBUG TRAIN Batch 12/9600 loss 51.979542 loss_att 61.472389 loss_ctc 63.020805 loss_rnnt 48.510223 hw_loss 0.184830 lr 0.00048406 rank 6
2023-02-21 02:07:56,996 DEBUG TRAIN Batch 12/9600 loss 8.145120 loss_att 11.302864 loss_ctc 11.288992 loss_rnnt 6.921210 hw_loss 0.324709 lr 0.00048413 rank 1
2023-02-21 02:07:56,999 DEBUG TRAIN Batch 12/9600 loss 20.711437 loss_att 24.077135 loss_ctc 23.089455 loss_rnnt 19.513813 hw_loss 0.388906 lr 0.00048418 rank 4
2023-02-21 02:07:57,002 DEBUG TRAIN Batch 12/9600 loss 21.020241 loss_att 26.917553 loss_ctc 31.002504 loss_rnnt 18.353888 hw_loss 0.292360 lr 0.00048404 rank 7
2023-02-21 02:08:56,330 DEBUG TRAIN Batch 12/9700 loss 8.026477 loss_att 10.979446 loss_ctc 8.762503 loss_rnnt 7.052444 hw_loss 0.534942 lr 0.00048405 rank 0
2023-02-21 02:08:56,343 DEBUG TRAIN Batch 12/9700 loss 21.435169 loss_att 17.762547 loss_ctc 19.286461 loss_rnnt 22.349348 hw_loss 0.200331 lr 0.00048395 rank 6
2023-02-21 02:08:56,345 DEBUG TRAIN Batch 12/9700 loss 10.634660 loss_att 12.130104 loss_ctc 14.261814 loss_rnnt 9.633134 hw_loss 0.410277 lr 0.00048395 rank 3
2023-02-21 02:08:56,345 DEBUG TRAIN Batch 12/9700 loss 21.025106 loss_att 20.321037 loss_ctc 22.503391 loss_rnnt 20.968134 hw_loss 0.001274 lr 0.00048397 rank 2
2023-02-21 02:08:56,348 DEBUG TRAIN Batch 12/9700 loss 44.546440 loss_att 42.953827 loss_ctc 55.742752 loss_rnnt 43.292786 hw_loss 0.148751 lr 0.00048407 rank 4
2023-02-21 02:08:56,349 DEBUG TRAIN Batch 12/9700 loss 11.506121 loss_att 11.405343 loss_ctc 12.735562 loss_rnnt 11.260862 hw_loss 0.190293 lr 0.00048401 rank 1
2023-02-21 02:08:56,349 DEBUG TRAIN Batch 12/9700 loss 19.229084 loss_att 15.514994 loss_ctc 20.156200 loss_rnnt 19.846741 hw_loss 0.002899 lr 0.00048392 rank 7
2023-02-21 02:08:56,399 DEBUG TRAIN Batch 12/9700 loss 9.328692 loss_att 16.442234 loss_ctc 13.797441 loss_rnnt 7.091253 hw_loss 0.410432 lr 0.00048395 rank 5
2023-02-21 02:09:53,288 DEBUG TRAIN Batch 12/9800 loss 17.846180 loss_att 18.340086 loss_ctc 20.820063 loss_rnnt 17.113686 hw_loss 0.444743 lr 0.00048394 rank 0
2023-02-21 02:09:53,295 DEBUG TRAIN Batch 12/9800 loss 7.549823 loss_att 8.242909 loss_ctc 7.152506 loss_rnnt 7.246879 hw_loss 0.407444 lr 0.00048384 rank 5
2023-02-21 02:09:53,295 DEBUG TRAIN Batch 12/9800 loss 12.260144 loss_att 13.131169 loss_ctc 13.420317 loss_rnnt 11.858729 hw_loss 0.135976 lr 0.00048395 rank 4
2023-02-21 02:09:53,296 DEBUG TRAIN Batch 12/9800 loss 11.583311 loss_att 13.737858 loss_ctc 17.721001 loss_rnnt 10.075368 hw_loss 0.485014 lr 0.00048381 rank 7
2023-02-21 02:09:53,297 DEBUG TRAIN Batch 12/9800 loss 13.662133 loss_att 12.980743 loss_ctc 14.355014 loss_rnnt 13.489069 hw_loss 0.406796 lr 0.00048385 rank 2
2023-02-21 02:09:53,299 DEBUG TRAIN Batch 12/9800 loss 17.815290 loss_att 28.925961 loss_ctc 26.853767 loss_rnnt 14.285004 hw_loss 0.193168 lr 0.00048384 rank 6
2023-02-21 02:09:53,301 DEBUG TRAIN Batch 12/9800 loss 19.648079 loss_att 26.805508 loss_ctc 26.553099 loss_rnnt 17.110470 hw_loss 0.347727 lr 0.00048390 rank 1
2023-02-21 02:09:53,302 DEBUG TRAIN Batch 12/9800 loss 22.720041 loss_att 28.560329 loss_ctc 26.887323 loss_rnnt 20.874102 hw_loss 0.229204 lr 0.00048383 rank 3
2023-02-21 02:10:53,023 DEBUG TRAIN Batch 12/9900 loss 7.591226 loss_att 12.509912 loss_ctc 11.025150 loss_rnnt 6.035953 hw_loss 0.213149 lr 0.00048382 rank 0
2023-02-21 02:10:53,033 DEBUG TRAIN Batch 12/9900 loss 15.832893 loss_att 20.908230 loss_ctc 13.260666 loss_rnnt 14.843931 hw_loss 0.594110 lr 0.00048372 rank 6
2023-02-21 02:10:53,033 DEBUG TRAIN Batch 12/9900 loss 10.674170 loss_att 13.614959 loss_ctc 9.695158 loss_rnnt 10.017697 hw_loss 0.372842 lr 0.00048373 rank 5
2023-02-21 02:10:53,035 DEBUG TRAIN Batch 12/9900 loss 23.213459 loss_att 39.503899 loss_ctc 24.527855 loss_rnnt 19.629417 hw_loss 0.282564 lr 0.00048374 rank 2
2023-02-21 02:10:53,036 DEBUG TRAIN Batch 12/9900 loss 23.012798 loss_att 30.507313 loss_ctc 34.509922 loss_rnnt 19.791229 hw_loss 0.355717 lr 0.00048372 rank 3
2023-02-21 02:10:53,041 DEBUG TRAIN Batch 12/9900 loss 14.397297 loss_att 18.160736 loss_ctc 14.620193 loss_rnnt 13.454702 hw_loss 0.300352 lr 0.00048384 rank 4
2023-02-21 02:10:53,041 DEBUG TRAIN Batch 12/9900 loss 13.952187 loss_att 14.915967 loss_ctc 11.845743 loss_rnnt 13.820772 hw_loss 0.411594 lr 0.00048379 rank 1
2023-02-21 02:10:53,045 DEBUG TRAIN Batch 12/9900 loss 4.737891 loss_att 7.560546 loss_ctc 6.289080 loss_rnnt 3.965757 hw_loss 0.001457 lr 0.00048370 rank 7
2023-02-21 02:11:51,307 DEBUG TRAIN Batch 12/10000 loss 14.256140 loss_att 21.992889 loss_ctc 14.938951 loss_rnnt 12.394219 hw_loss 0.419116 lr 0.00048363 rank 2
2023-02-21 02:11:51,313 DEBUG TRAIN Batch 12/10000 loss 28.218740 loss_att 29.349974 loss_ctc 33.648495 loss_rnnt 26.994120 hw_loss 0.514510 lr 0.00048361 rank 6
2023-02-21 02:11:51,313 DEBUG TRAIN Batch 12/10000 loss 24.666149 loss_att 29.250984 loss_ctc 31.278831 loss_rnnt 22.779434 hw_loss 0.165104 lr 0.00048371 rank 0
2023-02-21 02:11:51,313 DEBUG TRAIN Batch 12/10000 loss 4.235137 loss_att 6.599776 loss_ctc 4.680917 loss_rnnt 3.702632 hw_loss 0.000262 lr 0.00048367 rank 1
2023-02-21 02:11:51,314 DEBUG TRAIN Batch 12/10000 loss 16.177702 loss_att 19.605530 loss_ctc 17.879217 loss_rnnt 15.081404 hw_loss 0.344744 lr 0.00048361 rank 3
2023-02-21 02:11:51,316 DEBUG TRAIN Batch 12/10000 loss 4.510443 loss_att 11.713809 loss_ctc 9.964106 loss_rnnt 2.184193 hw_loss 0.297040 lr 0.00048361 rank 5
2023-02-21 02:11:51,321 DEBUG TRAIN Batch 12/10000 loss 10.325359 loss_att 15.986790 loss_ctc 9.872018 loss_rnnt 9.142914 hw_loss 0.207383 lr 0.00048373 rank 4
2023-02-21 02:11:51,332 DEBUG TRAIN Batch 12/10000 loss 11.061433 loss_att 16.348669 loss_ctc 23.447630 loss_rnnt 8.175952 hw_loss 0.331014 lr 0.00048359 rank 7
2023-02-21 02:12:49,627 DEBUG TRAIN Batch 12/10100 loss 72.261818 loss_att 78.266556 loss_ctc 97.709747 loss_rnnt 67.667641 hw_loss 0.000320 lr 0.00048349 rank 3
2023-02-21 02:12:49,630 DEBUG TRAIN Batch 12/10100 loss 14.273618 loss_att 15.401751 loss_ctc 18.114727 loss_rnnt 13.392844 hw_loss 0.268124 lr 0.00048352 rank 2
2023-02-21 02:12:49,631 DEBUG TRAIN Batch 12/10100 loss 11.639830 loss_att 17.781698 loss_ctc 11.971509 loss_rnnt 10.104036 hw_loss 0.493493 lr 0.00048360 rank 0
2023-02-21 02:12:49,632 DEBUG TRAIN Batch 12/10100 loss 24.841692 loss_att 24.792318 loss_ctc 27.195808 loss_rnnt 24.297157 hw_loss 0.450987 lr 0.00048350 rank 5
2023-02-21 02:12:49,637 DEBUG TRAIN Batch 12/10100 loss 18.733583 loss_att 23.813576 loss_ctc 26.121904 loss_rnnt 16.619322 hw_loss 0.212162 lr 0.00048361 rank 4
2023-02-21 02:12:49,639 DEBUG TRAIN Batch 12/10100 loss 10.151646 loss_att 11.554090 loss_ctc 9.758197 loss_rnnt 9.709763 hw_loss 0.400977 lr 0.00048356 rank 1
2023-02-21 02:12:49,640 DEBUG TRAIN Batch 12/10100 loss 18.521925 loss_att 16.718224 loss_ctc 21.981524 loss_rnnt 18.313147 hw_loss 0.202948 lr 0.00048350 rank 6
2023-02-21 02:12:49,643 DEBUG TRAIN Batch 12/10100 loss 10.756903 loss_att 15.812622 loss_ctc 17.939476 loss_rnnt 8.755914 hw_loss 0.060315 lr 0.00048347 rank 7
2023-02-21 02:13:49,002 DEBUG TRAIN Batch 12/10200 loss 10.844056 loss_att 13.103159 loss_ctc 22.781919 loss_rnnt 8.662692 hw_loss 0.258428 lr 0.00048348 rank 0
2023-02-21 02:13:49,013 DEBUG TRAIN Batch 12/10200 loss 14.947510 loss_att 15.982388 loss_ctc 14.780793 loss_rnnt 14.510724 hw_loss 0.472575 lr 0.00048340 rank 2
2023-02-21 02:13:49,013 DEBUG TRAIN Batch 12/10200 loss 2.831608 loss_att 5.483031 loss_ctc 2.629133 loss_rnnt 2.150346 hw_loss 0.333701 lr 0.00048339 rank 5
2023-02-21 02:13:49,016 DEBUG TRAIN Batch 12/10200 loss 13.786257 loss_att 19.403780 loss_ctc 14.750676 loss_rnnt 12.440616 hw_loss 0.175402 lr 0.00048338 rank 3
2023-02-21 02:13:49,016 DEBUG TRAIN Batch 12/10200 loss 6.013312 loss_att 12.126799 loss_ctc 3.967315 loss_rnnt 5.063229 hw_loss 0.000348 lr 0.00048338 rank 6
2023-02-21 02:13:49,019 DEBUG TRAIN Batch 12/10200 loss 2.732672 loss_att 8.804150 loss_ctc 3.591437 loss_rnnt 1.403367 hw_loss 0.000951 lr 0.00048336 rank 7
2023-02-21 02:13:49,019 DEBUG TRAIN Batch 12/10200 loss 31.233547 loss_att 34.944527 loss_ctc 44.579361 loss_rnnt 28.711651 hw_loss 0.000486 lr 0.00048345 rank 1
2023-02-21 02:13:49,079 DEBUG TRAIN Batch 12/10200 loss 15.185616 loss_att 16.179983 loss_ctc 19.274794 loss_rnnt 14.306597 hw_loss 0.252978 lr 0.00048350 rank 4
2023-02-21 02:14:48,353 DEBUG TRAIN Batch 12/10300 loss 1.610883 loss_att 5.837227 loss_ctc 1.748602 loss_rnnt 0.747027 hw_loss 0.000422 lr 0.00048337 rank 0
2023-02-21 02:14:48,362 DEBUG TRAIN Batch 12/10300 loss 4.420938 loss_att 16.535402 loss_ctc 2.921304 loss_rnnt 2.197660 hw_loss 0.000630 lr 0.00048333 rank 1
2023-02-21 02:14:48,364 DEBUG TRAIN Batch 12/10300 loss 7.544292 loss_att 11.495989 loss_ctc 7.971375 loss_rnnt 6.473187 hw_loss 0.419664 lr 0.00048329 rank 2
2023-02-21 02:14:48,366 DEBUG TRAIN Batch 12/10300 loss 22.289906 loss_att 23.214600 loss_ctc 18.710129 loss_rnnt 22.581955 hw_loss 0.000589 lr 0.00048328 rank 5
2023-02-21 02:14:48,366 DEBUG TRAIN Batch 12/10300 loss 12.191401 loss_att 12.121063 loss_ctc 8.640974 loss_rnnt 12.430064 hw_loss 0.466490 lr 0.00048327 rank 3
2023-02-21 02:14:48,372 DEBUG TRAIN Batch 12/10300 loss 10.468529 loss_att 22.656569 loss_ctc 10.601557 loss_rnnt 7.924204 hw_loss 0.166835 lr 0.00048339 rank 4
2023-02-21 02:14:48,377 DEBUG TRAIN Batch 12/10300 loss 16.938320 loss_att 15.573446 loss_ctc 21.117064 loss_rnnt 16.433056 hw_loss 0.414511 lr 0.00048325 rank 7
2023-02-21 02:14:48,427 DEBUG TRAIN Batch 12/10300 loss 16.873230 loss_att 21.173069 loss_ctc 20.993025 loss_rnnt 15.463720 hw_loss 0.000443 lr 0.00048327 rank 6
2023-02-21 02:16:08,777 DEBUG TRAIN Batch 12/10400 loss 19.878391 loss_att 21.829941 loss_ctc 26.962627 loss_rnnt 18.349083 hw_loss 0.364561 lr 0.00048326 rank 0
2023-02-21 02:16:08,783 DEBUG TRAIN Batch 12/10400 loss 12.756162 loss_att 15.876865 loss_ctc 17.623470 loss_rnnt 11.370788 hw_loss 0.210484 lr 0.00048318 rank 2
2023-02-21 02:16:08,785 DEBUG TRAIN Batch 12/10400 loss 10.921666 loss_att 17.795326 loss_ctc 15.539806 loss_rnnt 8.622154 hw_loss 0.579426 lr 0.00048316 rank 5
2023-02-21 02:16:08,789 DEBUG TRAIN Batch 12/10400 loss 43.212700 loss_att 54.133175 loss_ctc 49.059521 loss_rnnt 40.061153 hw_loss 0.352269 lr 0.00048327 rank 4
2023-02-21 02:16:08,790 DEBUG TRAIN Batch 12/10400 loss 11.456881 loss_att 11.692156 loss_ctc 18.236221 loss_rnnt 10.260928 hw_loss 0.459349 lr 0.00048315 rank 3
2023-02-21 02:16:08,790 DEBUG TRAIN Batch 12/10400 loss 25.654621 loss_att 27.931931 loss_ctc 31.691694 loss_rnnt 24.306137 hw_loss 0.165149 lr 0.00048313 rank 7
2023-02-21 02:16:08,792 DEBUG TRAIN Batch 12/10400 loss 14.354209 loss_att 15.926561 loss_ctc 15.783718 loss_rnnt 13.583963 hw_loss 0.497200 lr 0.00048316 rank 6
2023-02-21 02:16:08,794 DEBUG TRAIN Batch 12/10400 loss 28.161448 loss_att 27.394495 loss_ctc 39.704834 loss_rnnt 26.567869 hw_loss 0.389717 lr 0.00048322 rank 1
2023-02-21 02:17:09,628 DEBUG TRAIN Batch 12/10500 loss 18.051186 loss_att 16.440491 loss_ctc 16.183416 loss_rnnt 18.480015 hw_loss 0.266899 lr 0.00048314 rank 0
2023-02-21 02:17:09,635 DEBUG TRAIN Batch 12/10500 loss 13.664631 loss_att 15.607197 loss_ctc 14.810432 loss_rnnt 13.003518 hw_loss 0.224674 lr 0.00048306 rank 2
2023-02-21 02:17:09,639 DEBUG TRAIN Batch 12/10500 loss 10.031137 loss_att 14.069452 loss_ctc 14.572140 loss_rnnt 8.496872 hw_loss 0.227128 lr 0.00048302 rank 7
2023-02-21 02:17:09,642 DEBUG TRAIN Batch 12/10500 loss 5.309895 loss_att 12.606846 loss_ctc 4.275925 loss_rnnt 3.750762 hw_loss 0.445509 lr 0.00048304 rank 3
2023-02-21 02:17:09,642 DEBUG TRAIN Batch 12/10500 loss 54.993969 loss_att 56.316505 loss_ctc 72.417320 loss_rnnt 52.250332 hw_loss 0.292529 lr 0.00048305 rank 6
2023-02-21 02:17:09,643 DEBUG TRAIN Batch 12/10500 loss 16.697260 loss_att 21.026672 loss_ctc 21.120413 loss_rnnt 15.074975 hw_loss 0.312467 lr 0.00048311 rank 1
2023-02-21 02:17:09,643 DEBUG TRAIN Batch 12/10500 loss 12.551650 loss_att 16.008520 loss_ctc 15.275887 loss_rnnt 11.339739 hw_loss 0.294946 lr 0.00048305 rank 5
2023-02-21 02:17:09,651 DEBUG TRAIN Batch 12/10500 loss 28.631817 loss_att 31.594198 loss_ctc 32.559963 loss_rnnt 27.463329 hw_loss 0.097984 lr 0.00048316 rank 4
2023-02-21 02:18:06,917 DEBUG TRAIN Batch 12/10600 loss 4.407268 loss_att 7.496765 loss_ctc 5.520683 loss_rnnt 3.430837 hw_loss 0.393892 lr 0.00048303 rank 0
2023-02-21 02:18:06,925 DEBUG TRAIN Batch 12/10600 loss 29.130690 loss_att 33.176697 loss_ctc 30.211035 loss_rnnt 27.988440 hw_loss 0.354378 lr 0.00048294 rank 5
2023-02-21 02:18:06,927 DEBUG TRAIN Batch 12/10600 loss 10.243962 loss_att 14.379080 loss_ctc 14.818561 loss_rnnt 8.637959 hw_loss 0.316936 lr 0.00048293 rank 6
2023-02-21 02:18:06,931 DEBUG TRAIN Batch 12/10600 loss 23.565144 loss_att 30.126381 loss_ctc 32.825127 loss_rnnt 21.018007 hw_loss 0.000422 lr 0.00048293 rank 3
2023-02-21 02:18:06,933 DEBUG TRAIN Batch 12/10600 loss 16.944008 loss_att 20.566301 loss_ctc 26.836912 loss_rnnt 14.742994 hw_loss 0.295314 lr 0.00048305 rank 4
2023-02-21 02:18:06,935 DEBUG TRAIN Batch 12/10600 loss 19.366402 loss_att 22.670004 loss_ctc 22.464443 loss_rnnt 18.291954 hw_loss 0.001225 lr 0.00048291 rank 7
2023-02-21 02:18:06,935 DEBUG TRAIN Batch 12/10600 loss 22.981472 loss_att 24.132559 loss_ctc 38.396576 loss_rnnt 20.573160 hw_loss 0.230152 lr 0.00048295 rank 2
2023-02-21 02:18:06,989 DEBUG TRAIN Batch 12/10600 loss 13.927284 loss_att 21.479752 loss_ctc 17.468798 loss_rnnt 11.616282 hw_loss 0.615576 lr 0.00048300 rank 1
2023-02-21 02:19:04,958 DEBUG TRAIN Batch 12/10700 loss 10.485407 loss_att 10.417720 loss_ctc 12.439083 loss_rnnt 10.070018 hw_loss 0.315819 lr 0.00048292 rank 0
2023-02-21 02:19:04,968 DEBUG TRAIN Batch 12/10700 loss 12.486516 loss_att 15.735397 loss_ctc 10.323591 loss_rnnt 11.987756 hw_loss 0.257575 lr 0.00048282 rank 3
2023-02-21 02:19:04,969 DEBUG TRAIN Batch 12/10700 loss 29.326744 loss_att 30.112684 loss_ctc 30.369202 loss_rnnt 28.849993 hw_loss 0.338564 lr 0.00048282 rank 6
2023-02-21 02:19:04,970 DEBUG TRAIN Batch 12/10700 loss 16.259735 loss_att 19.422909 loss_ctc 22.048439 loss_rnnt 14.590483 hw_loss 0.496482 lr 0.00048288 rank 1
2023-02-21 02:19:04,971 DEBUG TRAIN Batch 12/10700 loss 8.674473 loss_att 11.370957 loss_ctc 7.259370 loss_rnnt 8.070622 hw_loss 0.474814 lr 0.00048294 rank 4
2023-02-21 02:19:04,974 DEBUG TRAIN Batch 12/10700 loss 24.392168 loss_att 27.041607 loss_ctc 27.276402 loss_rnnt 23.284195 hw_loss 0.362848 lr 0.00048284 rank 2
2023-02-21 02:19:04,981 DEBUG TRAIN Batch 12/10700 loss 11.369843 loss_att 15.125664 loss_ctc 15.901701 loss_rnnt 9.971175 hw_loss 0.081107 lr 0.00048280 rank 7
2023-02-21 02:19:05,027 DEBUG TRAIN Batch 12/10700 loss 39.237247 loss_att 41.500641 loss_ctc 40.520176 loss_rnnt 38.388512 hw_loss 0.421868 lr 0.00048282 rank 5
2023-02-21 02:20:04,567 DEBUG TRAIN Batch 12/10800 loss 12.026712 loss_att 15.724993 loss_ctc 15.922188 loss_rnnt 10.613476 hw_loss 0.289097 lr 0.00048281 rank 0
2023-02-21 02:20:04,575 DEBUG TRAIN Batch 12/10800 loss 17.152687 loss_att 23.307823 loss_ctc 24.505676 loss_rnnt 14.826251 hw_loss 0.215643 lr 0.00048273 rank 2
2023-02-21 02:20:04,575 DEBUG TRAIN Batch 12/10800 loss 11.586776 loss_att 20.961739 loss_ctc 20.770075 loss_rnnt 8.306644 hw_loss 0.338810 lr 0.00048271 rank 5
2023-02-21 02:20:04,576 DEBUG TRAIN Batch 12/10800 loss 4.865647 loss_att 9.403455 loss_ctc 4.614108 loss_rnnt 3.732197 hw_loss 0.486425 lr 0.00048277 rank 1
2023-02-21 02:20:04,579 DEBUG TRAIN Batch 12/10800 loss 36.473309 loss_att 39.533047 loss_ctc 48.925148 loss_rnnt 34.018787 hw_loss 0.341866 lr 0.00048268 rank 7
2023-02-21 02:20:04,585 DEBUG TRAIN Batch 12/10800 loss 30.458637 loss_att 31.170795 loss_ctc 42.315372 loss_rnnt 28.602123 hw_loss 0.249724 lr 0.00048270 rank 3
2023-02-21 02:20:04,585 DEBUG TRAIN Batch 12/10800 loss 21.099777 loss_att 23.455376 loss_ctc 19.337534 loss_rnnt 20.630590 hw_loss 0.436936 lr 0.00048282 rank 4
2023-02-21 02:20:04,586 DEBUG TRAIN Batch 12/10800 loss 26.705317 loss_att 41.274170 loss_ctc 45.518509 loss_rnnt 21.091412 hw_loss 0.359454 lr 0.00048271 rank 6
2023-02-21 02:21:02,738 DEBUG TRAIN Batch 12/10900 loss 9.769725 loss_att 10.843411 loss_ctc 10.143584 loss_rnnt 9.504971 hw_loss 0.000317 lr 0.00048269 rank 0
2023-02-21 02:21:02,738 DEBUG TRAIN Batch 12/10900 loss 5.783158 loss_att 10.948031 loss_ctc 8.299034 loss_rnnt 4.325771 hw_loss 0.166804 lr 0.00048261 rank 2
2023-02-21 02:21:02,741 DEBUG TRAIN Batch 12/10900 loss 13.320835 loss_att 12.815598 loss_ctc 17.503254 loss_rnnt 12.572411 hw_loss 0.547155 lr 0.00048260 rank 6
2023-02-21 02:21:02,744 DEBUG TRAIN Batch 12/10900 loss 14.065366 loss_att 13.027681 loss_ctc 16.104641 loss_rnnt 13.752212 hw_loss 0.466477 lr 0.00048266 rank 1
2023-02-21 02:21:02,745 DEBUG TRAIN Batch 12/10900 loss 4.521322 loss_att 9.750338 loss_ctc 5.826922 loss_rnnt 3.158065 hw_loss 0.268825 lr 0.00048271 rank 4
2023-02-21 02:21:02,747 DEBUG TRAIN Batch 12/10900 loss 6.695584 loss_att 13.516994 loss_ctc 8.838385 loss_rnnt 4.892573 hw_loss 0.286916 lr 0.00048260 rank 5
2023-02-21 02:21:02,747 DEBUG TRAIN Batch 12/10900 loss 25.037369 loss_att 29.928421 loss_ctc 27.693062 loss_rnnt 23.600063 hw_loss 0.196881 lr 0.00048259 rank 3
2023-02-21 02:21:02,810 DEBUG TRAIN Batch 12/10900 loss 9.005311 loss_att 10.309201 loss_ctc 10.187528 loss_rnnt 8.358384 hw_loss 0.428474 lr 0.00048257 rank 7
2023-02-21 02:22:00,317 DEBUG TRAIN Batch 12/11000 loss 12.171514 loss_att 15.174619 loss_ctc 12.581701 loss_rnnt 11.321102 hw_loss 0.365810 lr 0.00048258 rank 0
2023-02-21 02:22:00,327 DEBUG TRAIN Batch 12/11000 loss 5.796369 loss_att 12.928040 loss_ctc 4.973408 loss_rnnt 4.343803 hw_loss 0.254924 lr 0.00048249 rank 5
2023-02-21 02:22:00,329 DEBUG TRAIN Batch 12/11000 loss 9.687299 loss_att 10.159978 loss_ctc 12.690769 loss_rnnt 9.122972 hw_loss 0.129991 lr 0.00048250 rank 2
2023-02-21 02:22:00,330 DEBUG TRAIN Batch 12/11000 loss 12.782741 loss_att 20.086895 loss_ctc 20.183868 loss_rnnt 10.134911 hw_loss 0.375341 lr 0.00048260 rank 4
2023-02-21 02:22:00,331 DEBUG TRAIN Batch 12/11000 loss 18.410978 loss_att 24.582701 loss_ctc 21.466528 loss_rnnt 16.572189 hw_loss 0.369444 lr 0.00048248 rank 6
2023-02-21 02:22:00,335 DEBUG TRAIN Batch 12/11000 loss 9.222021 loss_att 11.887463 loss_ctc 6.770892 loss_rnnt 8.777054 hw_loss 0.447554 lr 0.00048255 rank 1
2023-02-21 02:22:00,337 DEBUG TRAIN Batch 12/11000 loss 2.763036 loss_att 5.048665 loss_ctc 3.478927 loss_rnnt 1.981505 hw_loss 0.429288 lr 0.00048248 rank 3
2023-02-21 02:22:00,344 DEBUG TRAIN Batch 12/11000 loss 19.838987 loss_att 24.481165 loss_ctc 24.048576 loss_rnnt 18.150257 hw_loss 0.373154 lr 0.00048246 rank 7
2023-02-21 02:23:00,444 DEBUG TRAIN Batch 12/11100 loss 8.395976 loss_att 16.419012 loss_ctc 8.027282 loss_rnnt 6.631098 hw_loss 0.392683 lr 0.00048247 rank 0
2023-02-21 02:23:00,450 DEBUG TRAIN Batch 12/11100 loss 27.652050 loss_att 34.313839 loss_ctc 35.171825 loss_rnnt 25.079588 hw_loss 0.445251 lr 0.00048239 rank 2
2023-02-21 02:23:00,452 DEBUG TRAIN Batch 12/11100 loss 14.477422 loss_att 19.247438 loss_ctc 28.001223 loss_rnnt 11.629354 hw_loss 0.170419 lr 0.00048237 rank 6
2023-02-21 02:23:00,452 DEBUG TRAIN Batch 12/11100 loss 20.151272 loss_att 28.764927 loss_ctc 32.478733 loss_rnnt 16.708853 hw_loss 0.142552 lr 0.00048243 rank 1
2023-02-21 02:23:00,452 DEBUG TRAIN Batch 12/11100 loss 27.073629 loss_att 26.735950 loss_ctc 44.283272 loss_rnnt 24.845829 hw_loss 0.001338 lr 0.00048235 rank 7
2023-02-21 02:23:00,453 DEBUG TRAIN Batch 12/11100 loss 11.809395 loss_att 14.163459 loss_ctc 16.604244 loss_rnnt 10.547166 hw_loss 0.285192 lr 0.00048237 rank 3
2023-02-21 02:23:00,454 DEBUG TRAIN Batch 12/11100 loss 1.895489 loss_att 6.052577 loss_ctc 3.048408 loss_rnnt 0.832400 hw_loss 0.146153 lr 0.00048249 rank 4
2023-02-21 02:23:00,455 DEBUG TRAIN Batch 12/11100 loss 2.925657 loss_att 7.036252 loss_ctc 2.428600 loss_rnnt 2.009041 hw_loss 0.301445 lr 0.00048238 rank 5
2023-02-21 02:24:20,743 DEBUG TRAIN Batch 12/11200 loss 58.334007 loss_att 53.987671 loss_ctc 72.050819 loss_rnnt 57.332100 hw_loss 0.079244 lr 0.00048236 rank 0
2023-02-21 02:24:20,749 DEBUG TRAIN Batch 12/11200 loss 14.291063 loss_att 20.573311 loss_ctc 21.658064 loss_rnnt 12.052145 hw_loss 0.000379 lr 0.00048228 rank 2
2023-02-21 02:24:20,751 DEBUG TRAIN Batch 12/11200 loss 14.994586 loss_att 14.848493 loss_ctc 18.974037 loss_rnnt 14.492977 hw_loss 0.000438 lr 0.00048232 rank 1
2023-02-21 02:24:20,753 DEBUG TRAIN Batch 12/11200 loss 10.422649 loss_att 16.015923 loss_ctc 20.566711 loss_rnnt 7.951258 hw_loss 0.000365 lr 0.00048237 rank 4
2023-02-21 02:24:20,754 DEBUG TRAIN Batch 12/11200 loss 5.966444 loss_att 10.541746 loss_ctc 4.986863 loss_rnnt 5.181678 hw_loss 0.000594 lr 0.00048226 rank 5
2023-02-21 02:24:20,757 DEBUG TRAIN Batch 12/11200 loss 14.059591 loss_att 16.738657 loss_ctc 19.832735 loss_rnnt 12.753800 hw_loss 0.000421 lr 0.00048225 rank 3
2023-02-21 02:24:20,760 DEBUG TRAIN Batch 12/11200 loss 16.571644 loss_att 19.074076 loss_ctc 20.086580 loss_rnnt 15.340357 hw_loss 0.491516 lr 0.00048226 rank 6
2023-02-21 02:24:20,761 DEBUG TRAIN Batch 12/11200 loss 15.205314 loss_att 18.615612 loss_ctc 19.849205 loss_rnnt 13.750725 hw_loss 0.287519 lr 0.00048223 rank 7
2023-02-21 02:25:20,814 DEBUG TRAIN Batch 12/11300 loss 6.957285 loss_att 10.936039 loss_ctc 8.065860 loss_rnnt 5.944739 hw_loss 0.129348 lr 0.00048225 rank 0
2023-02-21 02:25:20,817 DEBUG TRAIN Batch 12/11300 loss 18.304123 loss_att 25.918047 loss_ctc 20.488171 loss_rnnt 16.242117 hw_loss 0.465029 lr 0.00048215 rank 5
2023-02-21 02:25:20,818 DEBUG TRAIN Batch 12/11300 loss 7.817943 loss_att 11.816292 loss_ctc 8.178337 loss_rnnt 6.805640 hw_loss 0.308589 lr 0.00048216 rank 2
2023-02-21 02:25:20,821 DEBUG TRAIN Batch 12/11300 loss 22.260216 loss_att 23.558006 loss_ctc 31.422514 loss_rnnt 20.652021 hw_loss 0.238120 lr 0.00048221 rank 1
2023-02-21 02:25:20,826 DEBUG TRAIN Batch 12/11300 loss 12.115374 loss_att 14.844661 loss_ctc 13.619926 loss_rnnt 11.224099 hw_loss 0.271518 lr 0.00048226 rank 4
2023-02-21 02:25:20,827 DEBUG TRAIN Batch 12/11300 loss 16.368420 loss_att 19.872908 loss_ctc 18.218634 loss_rnnt 15.258830 hw_loss 0.303747 lr 0.00048212 rank 7
2023-02-21 02:25:20,829 DEBUG TRAIN Batch 12/11300 loss 16.809706 loss_att 18.650555 loss_ctc 26.807938 loss_rnnt 14.937706 hw_loss 0.320122 lr 0.00048214 rank 3
2023-02-21 02:25:20,881 DEBUG TRAIN Batch 12/11300 loss 9.080615 loss_att 14.487231 loss_ctc 10.173520 loss_rnnt 7.796582 hw_loss 0.106854 lr 0.00048215 rank 6
2023-02-21 02:26:20,899 DEBUG TRAIN Batch 12/11400 loss 25.206821 loss_att 25.868073 loss_ctc 27.818308 loss_rnnt 24.518799 hw_loss 0.389205 lr 0.00048213 rank 0
2023-02-21 02:26:20,899 DEBUG TRAIN Batch 12/11400 loss 3.686996 loss_att 7.482940 loss_ctc 2.845689 loss_rnnt 2.839037 hw_loss 0.376771 lr 0.00048205 rank 2
2023-02-21 02:26:20,909 DEBUG TRAIN Batch 12/11400 loss 1.565214 loss_att 5.120453 loss_ctc 1.128471 loss_rnnt 0.853400 hw_loss 0.110622 lr 0.00048203 rank 3
2023-02-21 02:26:20,911 DEBUG TRAIN Batch 12/11400 loss 11.857523 loss_att 10.930994 loss_ctc 18.813992 loss_rnnt 10.989861 hw_loss 0.235197 lr 0.00048203 rank 6
2023-02-21 02:26:20,912 DEBUG TRAIN Batch 12/11400 loss 2.022422 loss_att 7.911440 loss_ctc 1.041734 loss_rnnt 0.872169 hw_loss 0.193515 lr 0.00048210 rank 1
2023-02-21 02:26:20,913 DEBUG TRAIN Batch 12/11400 loss 9.942946 loss_att 11.709888 loss_ctc 9.258955 loss_rnnt 9.680590 hw_loss 0.000315 lr 0.00048215 rank 4
2023-02-21 02:26:20,919 DEBUG TRAIN Batch 12/11400 loss 43.387722 loss_att 49.273987 loss_ctc 54.648960 loss_rnnt 40.604866 hw_loss 0.195195 lr 0.00048201 rank 7
2023-02-21 02:26:20,921 DEBUG TRAIN Batch 12/11400 loss 4.790418 loss_att 8.142563 loss_ctc 5.695763 loss_rnnt 3.742555 hw_loss 0.481353 lr 0.00048204 rank 5
2023-02-21 02:27:17,537 DEBUG TRAIN Batch 12/11500 loss 2.704514 loss_att 10.860544 loss_ctc 3.176330 loss_rnnt 1.010345 hw_loss 0.000102 lr 0.00048202 rank 0
2023-02-21 02:27:17,539 DEBUG TRAIN Batch 12/11500 loss 14.703755 loss_att 15.868477 loss_ctc 18.464987 loss_rnnt 13.697813 hw_loss 0.509064 lr 0.00048192 rank 6
2023-02-21 02:27:17,550 DEBUG TRAIN Batch 12/11500 loss 8.487853 loss_att 14.912504 loss_ctc 6.014827 loss_rnnt 7.449222 hw_loss 0.156445 lr 0.00048204 rank 4
2023-02-21 02:27:17,551 DEBUG TRAIN Batch 12/11500 loss 6.238942 loss_att 9.158253 loss_ctc 10.184481 loss_rnnt 4.945658 hw_loss 0.343782 lr 0.00048192 rank 3
2023-02-21 02:27:17,555 DEBUG TRAIN Batch 12/11500 loss 11.093436 loss_att 18.185589 loss_ctc 10.550193 loss_rnnt 9.566238 hw_loss 0.339750 lr 0.00048199 rank 1
2023-02-21 02:27:17,555 DEBUG TRAIN Batch 12/11500 loss 9.723915 loss_att 11.877948 loss_ctc 13.516809 loss_rnnt 8.599636 hw_loss 0.352038 lr 0.00048190 rank 7
2023-02-21 02:27:17,562 DEBUG TRAIN Batch 12/11500 loss 57.423454 loss_att 68.178864 loss_ctc 85.423218 loss_rnnt 51.293884 hw_loss 0.459727 lr 0.00048194 rank 2
2023-02-21 02:27:17,601 DEBUG TRAIN Batch 12/11500 loss 32.400684 loss_att 32.303535 loss_ctc 39.958591 loss_rnnt 31.412338 hw_loss 0.000108 lr 0.00048193 rank 5
2023-02-21 02:28:15,509 DEBUG TRAIN Batch 12/11600 loss 32.967117 loss_att 33.514771 loss_ctc 36.341675 loss_rnnt 32.175911 hw_loss 0.434500 lr 0.00048191 rank 0
2023-02-21 02:28:15,509 DEBUG TRAIN Batch 12/11600 loss 20.196173 loss_att 28.935701 loss_ctc 26.166359 loss_rnnt 17.523046 hw_loss 0.242243 lr 0.00048187 rank 1
2023-02-21 02:28:15,512 DEBUG TRAIN Batch 12/11600 loss 15.969167 loss_att 20.663218 loss_ctc 19.439127 loss_rnnt 14.567631 hw_loss 0.000121 lr 0.00048183 rank 2
2023-02-21 02:28:15,516 DEBUG TRAIN Batch 12/11600 loss 15.084730 loss_att 22.243847 loss_ctc 20.095589 loss_rnnt 12.756377 hw_loss 0.428279 lr 0.00048181 rank 6
2023-02-21 02:28:15,517 DEBUG TRAIN Batch 12/11600 loss 16.843388 loss_att 18.145452 loss_ctc 21.050449 loss_rnnt 15.911496 hw_loss 0.207255 lr 0.00048181 rank 3
2023-02-21 02:28:15,519 DEBUG TRAIN Batch 12/11600 loss 8.470194 loss_att 13.306774 loss_ctc 10.154139 loss_rnnt 7.168257 hw_loss 0.206427 lr 0.00048181 rank 5
2023-02-21 02:28:15,522 DEBUG TRAIN Batch 12/11600 loss 8.198338 loss_att 12.327315 loss_ctc 9.393757 loss_rnnt 7.044892 hw_loss 0.315487 lr 0.00048179 rank 7
2023-02-21 02:28:15,525 DEBUG TRAIN Batch 12/11600 loss 9.236573 loss_att 12.616362 loss_ctc 8.226988 loss_rnnt 8.560741 hw_loss 0.252161 lr 0.00048192 rank 4
2023-02-21 02:29:15,078 DEBUG TRAIN Batch 12/11700 loss 10.849293 loss_att 14.133238 loss_ctc 13.591238 loss_rnnt 9.628757 hw_loss 0.371538 lr 0.00048180 rank 0
2023-02-21 02:29:15,079 DEBUG TRAIN Batch 12/11700 loss 5.529309 loss_att 11.897699 loss_ctc 6.228370 loss_rnnt 4.106205 hw_loss 0.105406 lr 0.00048170 rank 6
2023-02-21 02:29:15,080 DEBUG TRAIN Batch 12/11700 loss 13.164633 loss_att 14.935480 loss_ctc 18.070259 loss_rnnt 12.156187 hw_loss 0.000362 lr 0.00048169 rank 3
2023-02-21 02:29:15,081 DEBUG TRAIN Batch 12/11700 loss 28.675722 loss_att 37.965775 loss_ctc 35.246452 loss_rnnt 25.779936 hw_loss 0.303147 lr 0.00048170 rank 5
2023-02-21 02:29:15,087 DEBUG TRAIN Batch 12/11700 loss 16.916113 loss_att 27.043070 loss_ctc 24.158453 loss_rnnt 13.795924 hw_loss 0.242158 lr 0.00048172 rank 2
2023-02-21 02:29:15,089 DEBUG TRAIN Batch 12/11700 loss 4.574891 loss_att 12.068417 loss_ctc 5.365707 loss_rnnt 2.923638 hw_loss 0.088322 lr 0.00048176 rank 1
2023-02-21 02:29:15,094 DEBUG TRAIN Batch 12/11700 loss 28.420551 loss_att 34.569996 loss_ctc 32.727509 loss_rnnt 26.477436 hw_loss 0.260557 lr 0.00048167 rank 7
2023-02-21 02:29:15,097 DEBUG TRAIN Batch 12/11700 loss 8.571659 loss_att 14.102760 loss_ctc 16.386137 loss_rnnt 6.282503 hw_loss 0.264388 lr 0.00048181 rank 4
2023-02-21 02:30:12,466 DEBUG TRAIN Batch 12/11800 loss 2.240707 loss_att 4.373412 loss_ctc 3.674031 loss_rnnt 1.449449 hw_loss 0.325512 lr 0.00048169 rank 0
2023-02-21 02:30:12,469 DEBUG TRAIN Batch 12/11800 loss 6.576604 loss_att 9.935669 loss_ctc 7.274267 loss_rnnt 5.714932 hw_loss 0.181571 lr 0.00048159 rank 5
2023-02-21 02:30:12,470 DEBUG TRAIN Batch 12/11800 loss 16.997614 loss_att 13.521195 loss_ctc 16.331600 loss_rnnt 17.715149 hw_loss 0.124782 lr 0.00048159 rank 6
2023-02-21 02:30:12,471 DEBUG TRAIN Batch 12/11800 loss 6.532365 loss_att 5.239980 loss_ctc 1.262544 loss_rnnt 7.306712 hw_loss 0.350199 lr 0.00048160 rank 2
2023-02-21 02:30:12,472 DEBUG TRAIN Batch 12/11800 loss 14.709914 loss_att 17.934940 loss_ctc 20.456890 loss_rnnt 13.033792 hw_loss 0.496599 lr 0.00048165 rank 1
2023-02-21 02:30:12,474 DEBUG TRAIN Batch 12/11800 loss 1.326851 loss_att 5.598735 loss_ctc 2.565863 loss_rnnt 0.307166 hw_loss 0.000200 lr 0.00048158 rank 3
2023-02-21 02:30:12,477 DEBUG TRAIN Batch 12/11800 loss 10.370094 loss_att 13.437346 loss_ctc 15.277740 loss_rnnt 8.930318 hw_loss 0.322449 lr 0.00048156 rank 7
2023-02-21 02:30:12,478 DEBUG TRAIN Batch 12/11800 loss 19.464787 loss_att 19.499430 loss_ctc 20.081722 loss_rnnt 19.162601 hw_loss 0.399372 lr 0.00048170 rank 4
2023-02-21 02:31:12,210 DEBUG TRAIN Batch 12/11900 loss 6.419863 loss_att 9.548294 loss_ctc 8.214054 loss_rnnt 5.479037 hw_loss 0.142340 lr 0.00048147 rank 3
2023-02-21 02:31:12,210 DEBUG TRAIN Batch 12/11900 loss 8.445537 loss_att 12.546680 loss_ctc 6.802590 loss_rnnt 7.744562 hw_loss 0.187134 lr 0.00048157 rank 0
2023-02-21 02:31:12,214 DEBUG TRAIN Batch 12/11900 loss 3.004376 loss_att 5.487992 loss_ctc 2.043306 loss_rnnt 2.520392 hw_loss 0.216382 lr 0.00048148 rank 5
2023-02-21 02:31:12,214 DEBUG TRAIN Batch 12/11900 loss 18.464523 loss_att 20.697590 loss_ctc 26.515930 loss_rnnt 16.780615 hw_loss 0.307081 lr 0.00048149 rank 2
2023-02-21 02:31:12,219 DEBUG TRAIN Batch 12/11900 loss 9.293303 loss_att 13.533980 loss_ctc 8.780127 loss_rnnt 8.451496 hw_loss 0.116428 lr 0.00048148 rank 6
2023-02-21 02:31:12,220 DEBUG TRAIN Batch 12/11900 loss 20.504400 loss_att 28.272863 loss_ctc 25.342197 loss_rnnt 18.156784 hw_loss 0.279155 lr 0.00048159 rank 4
2023-02-21 02:31:12,222 DEBUG TRAIN Batch 12/11900 loss 9.563705 loss_att 13.743320 loss_ctc 12.078862 loss_rnnt 8.315834 hw_loss 0.143615 lr 0.00048154 rank 1
2023-02-21 02:31:12,224 DEBUG TRAIN Batch 12/11900 loss 3.027528 loss_att 7.660439 loss_ctc 4.099360 loss_rnnt 1.957855 hw_loss 0.000338 lr 0.00048145 rank 7
2023-02-21 02:32:12,260 DEBUG TRAIN Batch 12/12000 loss 5.542449 loss_att 9.419859 loss_ctc 5.917912 loss_rnnt 4.716799 hw_loss 0.000200 lr 0.00048146 rank 0
2023-02-21 02:32:12,270 DEBUG TRAIN Batch 12/12000 loss 9.538139 loss_att 14.144901 loss_ctc 10.627755 loss_rnnt 8.308620 hw_loss 0.305408 lr 0.00048138 rank 2
2023-02-21 02:32:12,271 DEBUG TRAIN Batch 12/12000 loss 16.885715 loss_att 17.046200 loss_ctc 22.185829 loss_rnnt 15.843714 hw_loss 0.568543 lr 0.00048137 rank 5
2023-02-21 02:32:12,272 DEBUG TRAIN Batch 12/12000 loss 32.695610 loss_att 32.499657 loss_ctc 45.866341 loss_rnnt 30.813284 hw_loss 0.310168 lr 0.00048148 rank 4
2023-02-21 02:32:12,273 DEBUG TRAIN Batch 12/12000 loss 35.445339 loss_att 50.423756 loss_ctc 51.355492 loss_rnnt 30.159735 hw_loss 0.316066 lr 0.00048136 rank 6
2023-02-21 02:32:12,277 DEBUG TRAIN Batch 12/12000 loss 24.057625 loss_att 21.911018 loss_ctc 22.109507 loss_rnnt 24.555901 hw_loss 0.357735 lr 0.00048143 rank 1
2023-02-21 02:32:12,279 DEBUG TRAIN Batch 12/12000 loss 15.322936 loss_att 17.800495 loss_ctc 22.085690 loss_rnnt 13.925642 hw_loss 0.000154 lr 0.00048136 rank 3
2023-02-21 02:32:12,285 DEBUG TRAIN Batch 12/12000 loss 8.240602 loss_att 17.644522 loss_ctc 21.825994 loss_rnnt 4.497938 hw_loss 0.094676 lr 0.00048134 rank 7
2023-02-21 02:33:33,939 DEBUG TRAIN Batch 12/12100 loss 17.265417 loss_att 18.477776 loss_ctc 16.651802 loss_rnnt 17.011427 hw_loss 0.175005 lr 0.00048137 rank 4
2023-02-21 02:33:33,938 DEBUG TRAIN Batch 12/12100 loss 2.549947 loss_att 6.840585 loss_ctc 1.298031 loss_rnnt 1.858678 hw_loss 0.000120 lr 0.00048135 rank 0
2023-02-21 02:33:33,943 DEBUG TRAIN Batch 12/12100 loss 14.107882 loss_att 12.420835 loss_ctc 17.631599 loss_rnnt 13.762026 hw_loss 0.400193 lr 0.00048126 rank 5
2023-02-21 02:33:33,943 DEBUG TRAIN Batch 12/12100 loss 14.370355 loss_att 14.768147 loss_ctc 16.668249 loss_rnnt 13.771617 hw_loss 0.398985 lr 0.00048125 rank 3
2023-02-21 02:33:33,945 DEBUG TRAIN Batch 12/12100 loss 21.508606 loss_att 22.749788 loss_ctc 23.677979 loss_rnnt 20.802410 hw_loss 0.316333 lr 0.00048125 rank 6
2023-02-21 02:33:33,946 DEBUG TRAIN Batch 12/12100 loss 5.300103 loss_att 9.907881 loss_ctc 5.055387 loss_rnnt 4.411098 hw_loss 0.000146 lr 0.00048127 rank 2
2023-02-21 02:33:33,949 DEBUG TRAIN Batch 12/12100 loss 10.572957 loss_att 16.958612 loss_ctc 12.896372 loss_rnnt 8.985939 hw_loss 0.000183 lr 0.00048123 rank 7
2023-02-21 02:33:34,012 DEBUG TRAIN Batch 12/12100 loss 12.182533 loss_att 16.005619 loss_ctc 15.740288 loss_rnnt 10.706697 hw_loss 0.444098 lr 0.00048131 rank 1
2023-02-21 02:34:34,421 DEBUG TRAIN Batch 12/12200 loss 33.704460 loss_att 32.831207 loss_ctc 39.245670 loss_rnnt 33.139969 hw_loss 0.000589 lr 0.00048124 rank 0
2023-02-21 02:34:34,429 DEBUG TRAIN Batch 12/12200 loss 20.624384 loss_att 22.108719 loss_ctc 28.043550 loss_rnnt 19.159887 hw_loss 0.334517 lr 0.00048116 rank 2
2023-02-21 02:34:34,431 DEBUG TRAIN Batch 12/12200 loss 14.570259 loss_att 18.547024 loss_ctc 18.056612 loss_rnnt 13.087167 hw_loss 0.417922 lr 0.00048114 rank 6
2023-02-21 02:34:34,436 DEBUG TRAIN Batch 12/12200 loss 11.453667 loss_att 16.765060 loss_ctc 14.786208 loss_rnnt 9.867333 hw_loss 0.149468 lr 0.00048114 rank 3
2023-02-21 02:34:34,437 DEBUG TRAIN Batch 12/12200 loss 11.203966 loss_att 16.860964 loss_ctc 13.862078 loss_rnnt 9.677452 hw_loss 0.076312 lr 0.00048125 rank 4
2023-02-21 02:34:34,437 DEBUG TRAIN Batch 12/12200 loss 14.886422 loss_att 16.085873 loss_ctc 16.144264 loss_rnnt 14.437449 hw_loss 0.077569 lr 0.00048115 rank 5
2023-02-21 02:34:34,445 DEBUG TRAIN Batch 12/12200 loss 37.972527 loss_att 41.392284 loss_ctc 53.329857 loss_rnnt 34.982475 hw_loss 0.484598 lr 0.00048112 rank 7
2023-02-21 02:34:34,497 DEBUG TRAIN Batch 12/12200 loss 12.413701 loss_att 13.490226 loss_ctc 11.308680 loss_rnnt 12.152197 hw_loss 0.362879 lr 0.00048120 rank 1
2023-02-21 02:35:33,789 DEBUG TRAIN Batch 12/12300 loss 11.179448 loss_att 8.993977 loss_ctc 6.759359 loss_rnnt 12.205782 hw_loss 0.000198 lr 0.00048113 rank 0
2023-02-21 02:35:33,795 DEBUG TRAIN Batch 12/12300 loss 14.291775 loss_att 11.564093 loss_ctc 14.438747 loss_rnnt 14.673446 hw_loss 0.270505 lr 0.00048103 rank 6
2023-02-21 02:35:33,795 DEBUG TRAIN Batch 12/12300 loss 28.715876 loss_att 30.558117 loss_ctc 30.388285 loss_rnnt 27.861912 hw_loss 0.492238 lr 0.00048109 rank 1
2023-02-21 02:35:33,797 DEBUG TRAIN Batch 12/12300 loss 17.899595 loss_att 16.730795 loss_ctc 28.410833 loss_rnnt 16.731762 hw_loss 0.000178 lr 0.00048105 rank 2
2023-02-21 02:35:33,800 DEBUG TRAIN Batch 12/12300 loss 17.306999 loss_att 22.099119 loss_ctc 19.627880 loss_rnnt 16.038992 hw_loss 0.000247 lr 0.00048114 rank 4
2023-02-21 02:35:33,803 DEBUG TRAIN Batch 12/12300 loss 5.772096 loss_att 14.005186 loss_ctc 4.539919 loss_rnnt 4.157124 hw_loss 0.248708 lr 0.00048100 rank 7
2023-02-21 02:35:33,805 DEBUG TRAIN Batch 12/12300 loss 11.063866 loss_att 11.915409 loss_ctc 9.992443 loss_rnnt 11.036240 hw_loss 0.000325 lr 0.00048103 rank 3
2023-02-21 02:35:33,857 DEBUG TRAIN Batch 12/12300 loss 4.481681 loss_att 9.477647 loss_ctc 7.722012 loss_rnnt 2.909376 hw_loss 0.264504 lr 0.00048103 rank 5
2023-02-21 02:36:33,490 DEBUG TRAIN Batch 12/12400 loss 11.565245 loss_att 14.250242 loss_ctc 16.438894 loss_rnnt 10.241195 hw_loss 0.257307 lr 0.00048102 rank 0
2023-02-21 02:36:33,497 DEBUG TRAIN Batch 12/12400 loss 20.542093 loss_att 22.910915 loss_ctc 20.343788 loss_rnnt 20.094654 hw_loss 0.000215 lr 0.00048094 rank 2
2023-02-21 02:36:33,497 DEBUG TRAIN Batch 12/12400 loss 14.086621 loss_att 20.503107 loss_ctc 18.573959 loss_rnnt 12.085119 hw_loss 0.224799 lr 0.00048098 rank 1
2023-02-21 02:36:33,499 DEBUG TRAIN Batch 12/12400 loss 15.264532 loss_att 16.030823 loss_ctc 18.725262 loss_rnnt 14.356612 hw_loss 0.549808 lr 0.00048103 rank 4
2023-02-21 02:36:33,499 DEBUG TRAIN Batch 12/12400 loss 15.760315 loss_att 17.026646 loss_ctc 20.683493 loss_rnnt 14.753036 hw_loss 0.182982 lr 0.00048092 rank 5
2023-02-21 02:36:33,501 DEBUG TRAIN Batch 12/12400 loss 18.184956 loss_att 16.463184 loss_ctc 18.648804 loss_rnnt 18.192411 hw_loss 0.515725 lr 0.00048091 rank 3
2023-02-21 02:36:33,505 DEBUG TRAIN Batch 12/12400 loss 12.657145 loss_att 12.716578 loss_ctc 19.756725 loss_rnnt 11.570989 hw_loss 0.239359 lr 0.00048089 rank 7
2023-02-21 02:36:33,561 DEBUG TRAIN Batch 12/12400 loss 15.116435 loss_att 17.796423 loss_ctc 17.677746 loss_rnnt 14.082521 hw_loss 0.293261 lr 0.00048092 rank 6
2023-02-21 02:37:33,763 DEBUG TRAIN Batch 12/12500 loss 10.806954 loss_att 15.852885 loss_ctc 11.957967 loss_rnnt 9.615191 hw_loss 0.054581 lr 0.00048082 rank 2
2023-02-21 02:37:33,764 DEBUG TRAIN Batch 12/12500 loss 21.847776 loss_att 31.656536 loss_ctc 23.802876 loss_rnnt 19.578737 hw_loss 0.087392 lr 0.00048090 rank 0
2023-02-21 02:37:33,765 DEBUG TRAIN Batch 12/12500 loss 8.236312 loss_att 13.369772 loss_ctc 10.010754 loss_rnnt 6.873891 hw_loss 0.185881 lr 0.00048080 rank 3
2023-02-21 02:37:33,772 DEBUG TRAIN Batch 12/12500 loss 7.453426 loss_att 18.156368 loss_ctc 9.250726 loss_rnnt 4.847213 hw_loss 0.423721 lr 0.00048081 rank 6
2023-02-21 02:37:33,775 DEBUG TRAIN Batch 12/12500 loss 8.694880 loss_att 13.420863 loss_ctc 8.790248 loss_rnnt 7.686575 hw_loss 0.094485 lr 0.00048081 rank 5
2023-02-21 02:37:33,778 DEBUG TRAIN Batch 12/12500 loss 1.685681 loss_att 3.316183 loss_ctc 0.729368 loss_rnnt 1.336667 hw_loss 0.282042 lr 0.00048092 rank 4
2023-02-21 02:37:33,805 DEBUG TRAIN Batch 12/12500 loss 18.331646 loss_att 21.167673 loss_ctc 19.955606 loss_rnnt 17.412897 hw_loss 0.253151 lr 0.00048087 rank 1
2023-02-21 02:37:33,808 DEBUG TRAIN Batch 12/12500 loss 6.187740 loss_att 14.925056 loss_ctc 13.996571 loss_rnnt 3.202796 hw_loss 0.368069 lr 0.00048078 rank 7
2023-02-21 02:38:31,337 DEBUG TRAIN Batch 12/12600 loss 27.569790 loss_att 31.588783 loss_ctc 37.848614 loss_rnnt 25.211689 hw_loss 0.344610 lr 0.00048079 rank 0
2023-02-21 02:38:31,347 DEBUG TRAIN Batch 12/12600 loss 9.822345 loss_att 10.997393 loss_ctc 12.109894 loss_rnnt 9.114063 hw_loss 0.315499 lr 0.00048071 rank 2
2023-02-21 02:38:31,351 DEBUG TRAIN Batch 12/12600 loss 15.873394 loss_att 29.219467 loss_ctc 19.366997 loss_rnnt 12.738232 hw_loss 0.000251 lr 0.00048070 rank 6
2023-02-21 02:38:31,351 DEBUG TRAIN Batch 12/12600 loss 4.311697 loss_att 6.800347 loss_ctc 6.090861 loss_rnnt 3.576662 hw_loss 0.000156 lr 0.00048069 rank 3
2023-02-21 02:38:31,351 DEBUG TRAIN Batch 12/12600 loss 48.287815 loss_att 46.949257 loss_ctc 63.323841 loss_rnnt 46.512283 hw_loss 0.072066 lr 0.00048070 rank 5
2023-02-21 02:38:31,355 DEBUG TRAIN Batch 12/12600 loss 10.630259 loss_att 12.039561 loss_ctc 10.916315 loss_rnnt 10.105990 hw_loss 0.383001 lr 0.00048081 rank 4
2023-02-21 02:38:31,356 DEBUG TRAIN Batch 12/12600 loss 7.351048 loss_att 10.701003 loss_ctc 8.972947 loss_rnnt 6.370866 hw_loss 0.176133 lr 0.00048067 rank 7
2023-02-21 02:38:31,409 DEBUG TRAIN Batch 12/12600 loss 23.262213 loss_att 23.090298 loss_ctc 32.031410 loss_rnnt 22.022171 hw_loss 0.197250 lr 0.00048076 rank 1
2023-02-21 02:39:28,721 DEBUG TRAIN Batch 12/12700 loss 19.951283 loss_att 20.766029 loss_ctc 20.714458 loss_rnnt 19.483116 hw_loss 0.381490 lr 0.00048068 rank 0
2023-02-21 02:39:28,727 DEBUG TRAIN Batch 12/12700 loss 13.453347 loss_att 14.651222 loss_ctc 15.701761 loss_rnnt 12.804359 hw_loss 0.205547 lr 0.00048065 rank 1
2023-02-21 02:39:28,727 DEBUG TRAIN Batch 12/12700 loss 15.346538 loss_att 18.790136 loss_ctc 22.447693 loss_rnnt 13.531650 hw_loss 0.336277 lr 0.00048058 rank 3
2023-02-21 02:39:28,740 DEBUG TRAIN Batch 12/12700 loss 10.990870 loss_att 12.031868 loss_ctc 11.182243 loss_rnnt 10.604794 hw_loss 0.285679 lr 0.00048058 rank 6
2023-02-21 02:39:28,741 DEBUG TRAIN Batch 12/12700 loss 18.370487 loss_att 18.682150 loss_ctc 28.237820 loss_rnnt 16.849781 hw_loss 0.267616 lr 0.00048070 rank 4
2023-02-21 02:39:28,745 DEBUG TRAIN Batch 12/12700 loss 39.231350 loss_att 44.700089 loss_ctc 51.950111 loss_rnnt 36.211441 hw_loss 0.431861 lr 0.00048056 rank 7
2023-02-21 02:39:28,750 DEBUG TRAIN Batch 12/12700 loss 8.307519 loss_att 9.421640 loss_ctc 8.145690 loss_rnnt 7.870042 hw_loss 0.442932 lr 0.00048060 rank 2
2023-02-21 02:39:28,784 DEBUG TRAIN Batch 12/12700 loss 15.722353 loss_att 16.542637 loss_ctc 18.516548 loss_rnnt 14.856894 hw_loss 0.616582 lr 0.00048059 rank 5
2023-02-21 02:40:28,401 DEBUG TRAIN Batch 12/12800 loss 12.080497 loss_att 13.231497 loss_ctc 16.008518 loss_rnnt 11.179441 hw_loss 0.275847 lr 0.00048057 rank 0
2023-02-21 02:40:28,411 DEBUG TRAIN Batch 12/12800 loss 15.560928 loss_att 16.058128 loss_ctc 25.735739 loss_rnnt 13.911708 hw_loss 0.362137 lr 0.00048048 rank 5
2023-02-21 02:40:28,413 DEBUG TRAIN Batch 12/12800 loss 8.701851 loss_att 8.908632 loss_ctc 18.532440 loss_rnnt 7.231956 hw_loss 0.220862 lr 0.00048049 rank 2
2023-02-21 02:40:28,414 DEBUG TRAIN Batch 12/12800 loss 8.096265 loss_att 9.769100 loss_ctc 13.393128 loss_rnnt 6.891918 hw_loss 0.306620 lr 0.00048047 rank 3
2023-02-21 02:40:28,414 DEBUG TRAIN Batch 12/12800 loss 33.247280 loss_att 45.759449 loss_ctc 44.892746 loss_rnnt 29.033594 hw_loss 0.297229 lr 0.00048047 rank 6
2023-02-21 02:40:28,417 DEBUG TRAIN Batch 12/12800 loss 28.167261 loss_att 31.534018 loss_ctc 33.712856 loss_rnnt 26.692600 hw_loss 0.116053 lr 0.00048059 rank 4
2023-02-21 02:40:28,419 DEBUG TRAIN Batch 12/12800 loss 8.689836 loss_att 11.838634 loss_ctc 10.205689 loss_rnnt 7.733271 hw_loss 0.233797 lr 0.00048045 rank 7
2023-02-21 02:40:28,472 DEBUG TRAIN Batch 12/12800 loss 1.086033 loss_att 4.806555 loss_ctc 1.808372 loss_rnnt 0.245406 hw_loss 0.000396 lr 0.00048054 rank 1
2023-02-21 02:41:28,341 DEBUG TRAIN Batch 12/12900 loss 4.257800 loss_att 7.170996 loss_ctc 2.489932 loss_rnnt 3.661401 hw_loss 0.467766 lr 0.00048046 rank 0
2023-02-21 02:41:28,353 DEBUG TRAIN Batch 12/12900 loss 27.477705 loss_att 27.964554 loss_ctc 29.482618 loss_rnnt 26.818325 hw_loss 0.552543 lr 0.00048036 rank 6
2023-02-21 02:41:28,353 DEBUG TRAIN Batch 12/12900 loss 14.183118 loss_att 14.402466 loss_ctc 10.575667 loss_rnnt 14.386482 hw_loss 0.438300 lr 0.00048048 rank 4
2023-02-21 02:41:28,359 DEBUG TRAIN Batch 12/12900 loss 14.648394 loss_att 14.478418 loss_ctc 13.464643 loss_rnnt 14.702854 hw_loss 0.257565 lr 0.00048043 rank 1
2023-02-21 02:41:28,372 DEBUG TRAIN Batch 12/12900 loss 14.610947 loss_att 12.268834 loss_ctc 7.526042 loss_rnnt 16.023886 hw_loss 0.000257 lr 0.00048034 rank 7
2023-02-21 02:41:28,392 DEBUG TRAIN Batch 12/12900 loss 7.127945 loss_att 10.899828 loss_ctc 9.248322 loss_rnnt 5.940037 hw_loss 0.282777 lr 0.00048038 rank 2
2023-02-21 02:41:28,400 DEBUG TRAIN Batch 12/12900 loss 13.063842 loss_att 11.571255 loss_ctc 17.728195 loss_rnnt 12.463109 hw_loss 0.520006 lr 0.00048036 rank 3
2023-02-21 02:41:28,419 DEBUG TRAIN Batch 12/12900 loss 15.593259 loss_att 23.247059 loss_ctc 20.546898 loss_rnnt 13.401935 hw_loss 0.000151 lr 0.00048037 rank 5
2023-02-21 02:42:50,676 DEBUG TRAIN Batch 12/13000 loss 5.672865 loss_att 12.435468 loss_ctc 11.924747 loss_rnnt 3.399949 hw_loss 0.162772 lr 0.00048035 rank 0
2023-02-21 02:42:50,684 DEBUG TRAIN Batch 12/13000 loss 16.375671 loss_att 20.208784 loss_ctc 17.099821 loss_rnnt 15.305607 hw_loss 0.387915 lr 0.00048037 rank 4
2023-02-21 02:42:50,684 DEBUG TRAIN Batch 12/13000 loss 19.184221 loss_att 25.822729 loss_ctc 23.198978 loss_rnnt 17.063137 hw_loss 0.483905 lr 0.00048027 rank 2
2023-02-21 02:42:50,688 DEBUG TRAIN Batch 12/13000 loss 4.361046 loss_att 9.332783 loss_ctc 5.427216 loss_rnnt 3.074856 hw_loss 0.280663 lr 0.00048026 rank 5
2023-02-21 02:42:50,688 DEBUG TRAIN Batch 12/13000 loss 10.559131 loss_att 13.720913 loss_ctc 16.919025 loss_rnnt 8.896324 hw_loss 0.342122 lr 0.00048025 rank 3
2023-02-21 02:42:50,688 DEBUG TRAIN Batch 12/13000 loss 11.215890 loss_att 16.000795 loss_ctc 11.540176 loss_rnnt 10.069481 hw_loss 0.274106 lr 0.00048025 rank 6
2023-02-21 02:42:50,691 DEBUG TRAIN Batch 12/13000 loss 11.131284 loss_att 14.132484 loss_ctc 12.306372 loss_rnnt 10.209513 hw_loss 0.309098 lr 0.00048031 rank 1
2023-02-21 02:42:50,692 DEBUG TRAIN Batch 12/13000 loss 16.650511 loss_att 21.094728 loss_ctc 21.043446 loss_rnnt 14.966952 hw_loss 0.391859 lr 0.00048023 rank 7
2023-02-21 02:43:51,368 DEBUG TRAIN Batch 12/13100 loss 10.069063 loss_att 12.855057 loss_ctc 12.897790 loss_rnnt 9.057085 hw_loss 0.145529 lr 0.00048024 rank 0
2023-02-21 02:43:51,376 DEBUG TRAIN Batch 12/13100 loss 19.810061 loss_att 22.526119 loss_ctc 25.733101 loss_rnnt 18.323544 hw_loss 0.287940 lr 0.00048016 rank 2
2023-02-21 02:43:51,381 DEBUG TRAIN Batch 12/13100 loss 6.030731 loss_att 12.666843 loss_ctc 6.862191 loss_rnnt 4.591693 hw_loss 0.001789 lr 0.00048014 rank 6
2023-02-21 02:43:51,382 DEBUG TRAIN Batch 12/13100 loss 3.620508 loss_att 5.232037 loss_ctc 0.557470 loss_rnnt 3.557721 hw_loss 0.279163 lr 0.00048012 rank 7
2023-02-21 02:43:51,383 DEBUG TRAIN Batch 12/13100 loss 12.446073 loss_att 18.621469 loss_ctc 17.151173 loss_rnnt 10.494834 hw_loss 0.166524 lr 0.00048014 rank 3
2023-02-21 02:43:51,385 DEBUG TRAIN Batch 12/13100 loss 15.014798 loss_att 19.652164 loss_ctc 16.745602 loss_rnnt 13.735473 hw_loss 0.227022 lr 0.00048015 rank 5
2023-02-21 02:43:51,386 DEBUG TRAIN Batch 12/13100 loss 23.854000 loss_att 28.928877 loss_ctc 31.403589 loss_rnnt 21.831448 hw_loss 0.001813 lr 0.00048020 rank 1
2023-02-21 02:43:51,392 DEBUG TRAIN Batch 12/13100 loss 14.797571 loss_att 15.347889 loss_ctc 12.224155 loss_rnnt 14.781956 hw_loss 0.466264 lr 0.00048025 rank 4
2023-02-21 02:44:49,603 DEBUG TRAIN Batch 12/13200 loss 4.216532 loss_att 9.478992 loss_ctc 7.577290 loss_rnnt 2.458946 hw_loss 0.481861 lr 0.00048013 rank 0
2023-02-21 02:44:49,612 DEBUG TRAIN Batch 12/13200 loss 16.741432 loss_att 15.722023 loss_ctc 22.484484 loss_rnnt 15.981218 hw_loss 0.371912 lr 0.00048004 rank 5
2023-02-21 02:44:49,613 DEBUG TRAIN Batch 12/13200 loss 4.220634 loss_att 10.072338 loss_ctc 14.421230 loss_rnnt 1.690149 hw_loss 0.000119 lr 0.00048009 rank 1
2023-02-21 02:44:49,613 DEBUG TRAIN Batch 12/13200 loss 19.461416 loss_att 25.099972 loss_ctc 25.619982 loss_rnnt 17.391956 hw_loss 0.226134 lr 0.00048005 rank 2
2023-02-21 02:44:49,616 DEBUG TRAIN Batch 12/13200 loss 13.977337 loss_att 16.546631 loss_ctc 20.199764 loss_rnnt 12.633768 hw_loss 0.000098 lr 0.00048001 rank 7
2023-02-21 02:44:49,619 DEBUG TRAIN Batch 12/13200 loss 0.742635 loss_att 2.868019 loss_ctc 1.246477 loss_rnnt 0.137092 hw_loss 0.212414 lr 0.00048003 rank 3
2023-02-21 02:44:49,642 DEBUG TRAIN Batch 12/13200 loss 24.134068 loss_att 24.809917 loss_ctc 24.667747 loss_rnnt 23.759794 hw_loss 0.314903 lr 0.00048003 rank 6
2023-02-21 02:44:49,646 DEBUG TRAIN Batch 12/13200 loss 4.854882 loss_att 10.624504 loss_ctc 3.887105 loss_rnnt 3.576051 hw_loss 0.476144 lr 0.00048014 rank 4
2023-02-21 02:45:45,982 DEBUG TRAIN Batch 12/13300 loss 27.665529 loss_att 34.409462 loss_ctc 39.106895 loss_rnnt 24.739220 hw_loss 0.097514 lr 0.00048002 rank 0
2023-02-21 02:45:45,987 DEBUG TRAIN Batch 12/13300 loss 19.328444 loss_att 24.068855 loss_ctc 22.108936 loss_rnnt 17.906525 hw_loss 0.193319 lr 0.00047994 rank 2
2023-02-21 02:45:45,992 DEBUG TRAIN Batch 12/13300 loss 7.787268 loss_att 8.984002 loss_ctc 6.721157 loss_rnnt 7.497804 hw_loss 0.360497 lr 0.00047998 rank 1
2023-02-21 02:45:45,998 DEBUG TRAIN Batch 12/13300 loss 22.325960 loss_att 25.510818 loss_ctc 24.514959 loss_rnnt 21.289169 hw_loss 0.202414 lr 0.00047992 rank 3
2023-02-21 02:45:45,998 DEBUG TRAIN Batch 12/13300 loss 19.244305 loss_att 18.710186 loss_ctc 20.286472 loss_rnnt 18.991478 hw_loss 0.413803 lr 0.00047992 rank 6
2023-02-21 02:45:46,000 DEBUG TRAIN Batch 12/13300 loss 15.960920 loss_att 19.660950 loss_ctc 29.303234 loss_rnnt 13.347136 hw_loss 0.177759 lr 0.00047992 rank 5
2023-02-21 02:45:46,006 DEBUG TRAIN Batch 12/13300 loss 13.795043 loss_att 15.119847 loss_ctc 14.267050 loss_rnnt 13.413467 hw_loss 0.100649 lr 0.00048003 rank 4
2023-02-21 02:45:46,006 DEBUG TRAIN Batch 12/13300 loss 14.536359 loss_att 15.903699 loss_ctc 18.183081 loss_rnnt 13.701381 hw_loss 0.141151 lr 0.00047990 rank 7
2023-02-21 02:46:46,132 DEBUG TRAIN Batch 12/13400 loss 27.669052 loss_att 34.817852 loss_ctc 33.744137 loss_rnnt 25.328806 hw_loss 0.188391 lr 0.00047991 rank 0
2023-02-21 02:46:46,135 DEBUG TRAIN Batch 12/13400 loss 17.557405 loss_att 20.464357 loss_ctc 15.857658 loss_rnnt 17.026421 hw_loss 0.330424 lr 0.00047987 rank 1
2023-02-21 02:46:46,137 DEBUG TRAIN Batch 12/13400 loss 32.883930 loss_att 34.993462 loss_ctc 43.851482 loss_rnnt 30.873432 hw_loss 0.236708 lr 0.00047981 rank 6
2023-02-21 02:46:46,143 DEBUG TRAIN Batch 12/13400 loss 2.908693 loss_att 6.669410 loss_ctc 4.032440 loss_rnnt 2.005338 hw_loss 0.002584 lr 0.00047981 rank 5
2023-02-21 02:46:46,147 DEBUG TRAIN Batch 12/13400 loss 7.671920 loss_att 11.139348 loss_ctc 9.096439 loss_rnnt 6.647713 hw_loss 0.263973 lr 0.00047983 rank 2
2023-02-21 02:46:46,147 DEBUG TRAIN Batch 12/13400 loss 4.627496 loss_att 7.452443 loss_ctc 5.516421 loss_rnnt 3.942591 hw_loss 0.002611 lr 0.00047979 rank 7
2023-02-21 02:46:46,149 DEBUG TRAIN Batch 12/13400 loss 14.187838 loss_att 24.164318 loss_ctc 19.973480 loss_rnnt 11.318636 hw_loss 0.192162 lr 0.00047981 rank 3
2023-02-21 02:46:46,220 DEBUG TRAIN Batch 12/13400 loss 21.956367 loss_att 23.413843 loss_ctc 28.510191 loss_rnnt 20.634375 hw_loss 0.293727 lr 0.00047992 rank 4
2023-02-21 02:47:43,271 DEBUG TRAIN Batch 12/13500 loss 21.964504 loss_att 19.614332 loss_ctc 21.736477 loss_rnnt 22.308825 hw_loss 0.292721 lr 0.00047980 rank 0
2023-02-21 02:47:43,273 DEBUG TRAIN Batch 12/13500 loss 21.486671 loss_att 22.774593 loss_ctc 26.251955 loss_rnnt 20.412382 hw_loss 0.340001 lr 0.00047972 rank 2
2023-02-21 02:47:43,274 DEBUG TRAIN Batch 12/13500 loss 13.067717 loss_att 16.394854 loss_ctc 12.499032 loss_rnnt 12.477276 hw_loss 0.001568 lr 0.00047970 rank 6
2023-02-21 02:47:43,274 DEBUG TRAIN Batch 12/13500 loss 12.588348 loss_att 11.242784 loss_ctc 12.840811 loss_rnnt 12.574797 hw_loss 0.466882 lr 0.00047976 rank 1
2023-02-21 02:47:43,276 DEBUG TRAIN Batch 12/13500 loss 38.270214 loss_att 42.074684 loss_ctc 51.852642 loss_rnnt 35.575474 hw_loss 0.230348 lr 0.00047981 rank 4
2023-02-21 02:47:43,277 DEBUG TRAIN Batch 12/13500 loss 26.133078 loss_att 26.133335 loss_ctc 27.517534 loss_rnnt 25.765030 hw_loss 0.343880 lr 0.00047969 rank 3
2023-02-21 02:47:43,276 DEBUG TRAIN Batch 12/13500 loss 18.208508 loss_att 16.847647 loss_ctc 20.888067 loss_rnnt 17.938763 hw_loss 0.346201 lr 0.00047968 rank 7
2023-02-21 02:47:43,336 DEBUG TRAIN Batch 12/13500 loss 12.058531 loss_att 15.201789 loss_ctc 18.302591 loss_rnnt 10.421489 hw_loss 0.329716 lr 0.00047970 rank 5
2023-02-21 02:48:41,360 DEBUG TRAIN Batch 12/13600 loss 20.177317 loss_att 24.452110 loss_ctc 20.651489 loss_rnnt 19.258543 hw_loss 0.001112 lr 0.00047969 rank 0
2023-02-21 02:48:41,361 DEBUG TRAIN Batch 12/13600 loss 29.350008 loss_att 37.008415 loss_ctc 41.801765 loss_rnnt 26.026287 hw_loss 0.247138 lr 0.00047961 rank 2
2023-02-21 02:48:41,368 DEBUG TRAIN Batch 12/13600 loss 26.990688 loss_att 23.652996 loss_ctc 32.632526 loss_rnnt 26.762581 hw_loss 0.268878 lr 0.00047959 rank 5
2023-02-21 02:48:41,370 DEBUG TRAIN Batch 12/13600 loss 24.237841 loss_att 29.126801 loss_ctc 33.439327 loss_rnnt 21.876232 hw_loss 0.294283 lr 0.00047959 rank 6
2023-02-21 02:48:41,370 DEBUG TRAIN Batch 12/13600 loss 14.722783 loss_att 16.057434 loss_ctc 19.664402 loss_rnnt 13.666397 hw_loss 0.244824 lr 0.00047965 rank 1
2023-02-21 02:48:41,374 DEBUG TRAIN Batch 12/13600 loss 13.161079 loss_att 17.104534 loss_ctc 15.376658 loss_rnnt 12.076102 hw_loss 0.001644 lr 0.00047970 rank 4
2023-02-21 02:48:41,378 DEBUG TRAIN Batch 12/13600 loss 19.626898 loss_att 22.548805 loss_ctc 22.540859 loss_rnnt 18.512348 hw_loss 0.265573 lr 0.00047958 rank 3
2023-02-21 02:48:41,378 DEBUG TRAIN Batch 12/13600 loss 5.812506 loss_att 14.672462 loss_ctc 9.422186 loss_rnnt 3.558674 hw_loss 0.001032 lr 0.00047956 rank 7
2023-02-21 02:49:39,869 DEBUG TRAIN Batch 12/13700 loss 15.840357 loss_att 17.580894 loss_ctc 19.336834 loss_rnnt 14.832784 hw_loss 0.362376 lr 0.00047950 rank 2
2023-02-21 02:49:39,869 DEBUG TRAIN Batch 12/13700 loss 9.806625 loss_att 19.938519 loss_ctc 15.629377 loss_rnnt 6.737176 hw_loss 0.500069 lr 0.00047958 rank 0
2023-02-21 02:49:39,875 DEBUG TRAIN Batch 12/13700 loss 3.411574 loss_att 7.312469 loss_ctc 2.872976 loss_rnnt 2.479041 hw_loss 0.420312 lr 0.00047947 rank 3
2023-02-21 02:49:39,877 DEBUG TRAIN Batch 12/13700 loss 28.350786 loss_att 31.672922 loss_ctc 34.641277 loss_rnnt 26.696606 hw_loss 0.283164 lr 0.00047954 rank 1
2023-02-21 02:49:39,877 DEBUG TRAIN Batch 12/13700 loss 2.209562 loss_att 5.978362 loss_ctc 2.572320 loss_rnnt 1.094996 hw_loss 0.585822 lr 0.00047948 rank 5
2023-02-21 02:49:39,878 DEBUG TRAIN Batch 12/13700 loss 12.040461 loss_att 16.366516 loss_ctc 10.408263 loss_rnnt 11.137315 hw_loss 0.479177 lr 0.00047948 rank 6
2023-02-21 02:49:39,886 DEBUG TRAIN Batch 12/13700 loss 2.174276 loss_att 6.474181 loss_ctc 1.883686 loss_rnnt 1.164155 hw_loss 0.354159 lr 0.00047945 rank 7
2023-02-21 02:49:39,944 DEBUG TRAIN Batch 12/13700 loss 13.518091 loss_att 16.249832 loss_ctc 13.607947 loss_rnnt 12.790695 hw_loss 0.316999 lr 0.00047959 rank 4
2023-02-21 02:51:01,748 DEBUG TRAIN Batch 12/13800 loss 2.748335 loss_att 7.811750 loss_ctc 4.983822 loss_rnnt 1.386864 hw_loss 0.095106 lr 0.00047939 rank 2
2023-02-21 02:51:01,749 DEBUG TRAIN Batch 12/13800 loss 15.689079 loss_att 20.485804 loss_ctc 11.593361 loss_rnnt 15.275661 hw_loss 0.000321 lr 0.00047937 rank 5
2023-02-21 02:51:01,749 DEBUG TRAIN Batch 12/13800 loss 15.426980 loss_att 19.087851 loss_ctc 17.975285 loss_rnnt 14.202003 hw_loss 0.286931 lr 0.00047937 rank 6
2023-02-21 02:51:01,752 DEBUG TRAIN Batch 12/13800 loss 14.667479 loss_att 15.561301 loss_ctc 16.950516 loss_rnnt 14.055410 hw_loss 0.241688 lr 0.00047943 rank 1
2023-02-21 02:51:01,754 DEBUG TRAIN Batch 12/13800 loss 2.117732 loss_att 10.664102 loss_ctc 0.714526 loss_rnnt 0.425574 hw_loss 0.318709 lr 0.00047948 rank 4
2023-02-21 02:51:01,754 DEBUG TRAIN Batch 12/13800 loss 18.152121 loss_att 16.693264 loss_ctc 16.295868 loss_rnnt 18.434036 hw_loss 0.482542 lr 0.00047934 rank 7
2023-02-21 02:51:01,757 DEBUG TRAIN Batch 12/13800 loss 15.497133 loss_att 23.937883 loss_ctc 28.945778 loss_rnnt 11.732241 hw_loss 0.531727 lr 0.00047936 rank 3
2023-02-21 02:51:01,777 DEBUG TRAIN Batch 12/13800 loss 15.146251 loss_att 14.575578 loss_ctc 24.266933 loss_rnnt 13.845079 hw_loss 0.373529 lr 0.00047947 rank 0
2023-02-21 02:52:00,671 DEBUG TRAIN Batch 12/13900 loss 26.155355 loss_att 27.611841 loss_ctc 31.457291 loss_rnnt 24.955902 hw_loss 0.377306 lr 0.00047936 rank 0
2023-02-21 02:52:00,676 DEBUG TRAIN Batch 12/13900 loss 7.464800 loss_att 12.599076 loss_ctc 9.462645 loss_rnnt 5.950719 hw_loss 0.414087 lr 0.00047926 rank 6
2023-02-21 02:52:00,679 DEBUG TRAIN Batch 12/13900 loss 18.702440 loss_att 20.383322 loss_ctc 24.151005 loss_rnnt 17.596035 hw_loss 0.082036 lr 0.00047928 rank 2
2023-02-21 02:52:00,680 DEBUG TRAIN Batch 12/13900 loss 22.109913 loss_att 24.355085 loss_ctc 31.336287 loss_rnnt 20.248594 hw_loss 0.341439 lr 0.00047926 rank 5
2023-02-21 02:52:00,683 DEBUG TRAIN Batch 12/13900 loss 13.671751 loss_att 14.648122 loss_ctc 18.500010 loss_rnnt 12.658218 hw_loss 0.327167 lr 0.00047937 rank 4
2023-02-21 02:52:00,687 DEBUG TRAIN Batch 12/13900 loss 21.494543 loss_att 25.327974 loss_ctc 33.708931 loss_rnnt 19.030392 hw_loss 0.129150 lr 0.00047925 rank 3
2023-02-21 02:52:00,691 DEBUG TRAIN Batch 12/13900 loss 20.225983 loss_att 19.047564 loss_ctc 28.724052 loss_rnnt 19.216146 hw_loss 0.210835 lr 0.00047932 rank 1
2023-02-21 02:52:00,740 DEBUG TRAIN Batch 12/13900 loss 23.958208 loss_att 33.839745 loss_ctc 36.627357 loss_rnnt 20.172421 hw_loss 0.225487 lr 0.00047923 rank 7
2023-02-21 02:53:00,620 DEBUG TRAIN Batch 12/14000 loss 19.841776 loss_att 19.405869 loss_ctc 18.887644 loss_rnnt 20.056103 hw_loss 0.000135 lr 0.00047925 rank 0
2023-02-21 02:53:00,626 DEBUG TRAIN Batch 12/14000 loss 3.901751 loss_att 8.210743 loss_ctc 9.104749 loss_rnnt 2.081446 hw_loss 0.496450 lr 0.00047917 rank 2
2023-02-21 02:53:00,628 DEBUG TRAIN Batch 12/14000 loss 15.960454 loss_att 18.310881 loss_ctc 17.212950 loss_rnnt 15.261574 hw_loss 0.115865 lr 0.00047915 rank 6
2023-02-21 02:53:00,630 DEBUG TRAIN Batch 12/14000 loss 3.760386 loss_att 6.433343 loss_ctc 5.893491 loss_rnnt 2.879203 hw_loss 0.116582 lr 0.00047915 rank 5
2023-02-21 02:53:00,632 DEBUG TRAIN Batch 12/14000 loss 1.669304 loss_att 5.950660 loss_ctc 1.713930 loss_rnnt 0.634036 hw_loss 0.324462 lr 0.00047921 rank 1
2023-02-21 02:53:00,634 DEBUG TRAIN Batch 12/14000 loss 10.908001 loss_att 15.672853 loss_ctc 11.522857 loss_rnnt 9.654316 hw_loss 0.410125 lr 0.00047914 rank 3
2023-02-21 02:53:00,636 DEBUG TRAIN Batch 12/14000 loss 36.651833 loss_att 43.711533 loss_ctc 47.774399 loss_rnnt 33.563679 hw_loss 0.362264 lr 0.00047926 rank 4
2023-02-21 02:53:00,640 DEBUG TRAIN Batch 12/14000 loss 21.355783 loss_att 24.783363 loss_ctc 21.054750 loss_rnnt 20.710249 hw_loss 0.000294 lr 0.00047912 rank 7
2023-02-21 02:53:58,354 DEBUG TRAIN Batch 12/14100 loss 12.683703 loss_att 11.854776 loss_ctc 13.830497 loss_rnnt 12.416120 hw_loss 0.525869 lr 0.00047914 rank 0
2023-02-21 02:53:58,358 DEBUG TRAIN Batch 12/14100 loss 27.605680 loss_att 31.963745 loss_ctc 43.710819 loss_rnnt 24.475729 hw_loss 0.208103 lr 0.00047904 rank 5
2023-02-21 02:53:58,361 DEBUG TRAIN Batch 12/14100 loss 41.853580 loss_att 58.351841 loss_ctc 55.366833 loss_rnnt 36.636051 hw_loss 0.217701 lr 0.00047906 rank 2
2023-02-21 02:53:58,364 DEBUG TRAIN Batch 12/14100 loss 3.090671 loss_att 5.549714 loss_ctc 0.648077 loss_rnnt 2.829104 hw_loss 0.178946 lr 0.00047903 rank 3
2023-02-21 02:53:58,365 DEBUG TRAIN Batch 12/14100 loss 26.699329 loss_att 33.662300 loss_ctc 42.320026 loss_rnnt 23.080538 hw_loss 0.268947 lr 0.00047901 rank 7
2023-02-21 02:53:58,365 DEBUG TRAIN Batch 12/14100 loss 10.119661 loss_att 16.802877 loss_ctc 11.440129 loss_rnnt 8.532232 hw_loss 0.140107 lr 0.00047915 rank 4
2023-02-21 02:53:58,367 DEBUG TRAIN Batch 12/14100 loss 9.855446 loss_att 12.433984 loss_ctc 10.603186 loss_rnnt 9.085091 hw_loss 0.290529 lr 0.00047904 rank 6
2023-02-21 02:53:58,428 DEBUG TRAIN Batch 12/14100 loss 23.544598 loss_att 22.720703 loss_ctc 22.634542 loss_rnnt 23.616987 hw_loss 0.400744 lr 0.00047910 rank 1
2023-02-21 02:54:56,453 DEBUG TRAIN Batch 12/14200 loss 2.166085 loss_att 6.039003 loss_ctc 3.834102 loss_rnnt 1.034838 hw_loss 0.251739 lr 0.00047892 rank 3
2023-02-21 02:54:56,453 DEBUG TRAIN Batch 12/14200 loss 7.761835 loss_att 10.493162 loss_ctc 11.324315 loss_rnnt 6.501728 hw_loss 0.447833 lr 0.00047903 rank 0
2023-02-21 02:54:56,455 DEBUG TRAIN Batch 12/14200 loss 28.529148 loss_att 31.435713 loss_ctc 31.746826 loss_rnnt 27.368454 hw_loss 0.281920 lr 0.00047895 rank 2
2023-02-21 02:54:56,458 DEBUG TRAIN Batch 12/14200 loss 15.534465 loss_att 20.422083 loss_ctc 22.255499 loss_rnnt 13.363393 hw_loss 0.557644 lr 0.00047904 rank 4
2023-02-21 02:54:56,461 DEBUG TRAIN Batch 12/14200 loss 21.705500 loss_att 26.859793 loss_ctc 26.432339 loss_rnnt 20.044355 hw_loss 0.000075 lr 0.00047893 rank 5
2023-02-21 02:54:56,463 DEBUG TRAIN Batch 12/14200 loss 11.172315 loss_att 12.695787 loss_ctc 10.980059 loss_rnnt 10.779571 hw_loss 0.213155 lr 0.00047890 rank 7
2023-02-21 02:54:56,468 DEBUG TRAIN Batch 12/14200 loss 10.515470 loss_att 13.123636 loss_ctc 13.538976 loss_rnnt 9.369850 hw_loss 0.414097 lr 0.00047893 rank 6
2023-02-21 02:54:56,519 DEBUG TRAIN Batch 12/14200 loss 21.501499 loss_att 28.507368 loss_ctc 24.026102 loss_rnnt 19.605852 hw_loss 0.295984 lr 0.00047899 rank 1
2023-02-21 02:55:55,818 DEBUG TRAIN Batch 12/14300 loss 26.309959 loss_att 28.570950 loss_ctc 35.970345 loss_rnnt 24.569494 hw_loss 0.000405 lr 0.00047892 rank 0
2023-02-21 02:55:55,821 DEBUG TRAIN Batch 12/14300 loss 2.739774 loss_att 9.695877 loss_ctc 2.908417 loss_rnnt 1.232213 hw_loss 0.175976 lr 0.00047881 rank 3
2023-02-21 02:55:55,826 DEBUG TRAIN Batch 12/14300 loss 18.615456 loss_att 29.372002 loss_ctc 21.443325 loss_rnnt 16.039886 hw_loss 0.088520 lr 0.00047882 rank 6
2023-02-21 02:55:55,830 DEBUG TRAIN Batch 12/14300 loss 13.309299 loss_att 18.857374 loss_ctc 17.202120 loss_rnnt 11.358712 hw_loss 0.603616 lr 0.00047882 rank 5
2023-02-21 02:55:55,831 DEBUG TRAIN Batch 12/14300 loss 5.241146 loss_att 8.608559 loss_ctc 7.045197 loss_rnnt 4.087975 hw_loss 0.448402 lr 0.00047888 rank 1
2023-02-21 02:55:55,831 DEBUG TRAIN Batch 12/14300 loss 1.490407 loss_att 5.493904 loss_ctc 1.688882 loss_rnnt 0.663094 hw_loss 0.000282 lr 0.00047884 rank 2
2023-02-21 02:55:55,836 DEBUG TRAIN Batch 12/14300 loss 16.125076 loss_att 19.053833 loss_ctc 15.239309 loss_rnnt 15.521910 hw_loss 0.254098 lr 0.00047893 rank 4
2023-02-21 02:55:55,891 DEBUG TRAIN Batch 12/14300 loss 8.762628 loss_att 11.718137 loss_ctc 15.232535 loss_rnnt 7.308413 hw_loss 0.000861 lr 0.00047879 rank 7
2023-02-21 02:56:53,850 DEBUG TRAIN Batch 12/14400 loss 19.976891 loss_att 18.579000 loss_ctc 21.370697 loss_rnnt 19.750893 hw_loss 0.599506 lr 0.00047881 rank 0
2023-02-21 02:56:53,852 DEBUG TRAIN Batch 12/14400 loss 9.252815 loss_att 8.704408 loss_ctc 9.263329 loss_rnnt 9.194894 hw_loss 0.311628 lr 0.00047871 rank 6
2023-02-21 02:56:53,856 DEBUG TRAIN Batch 12/14400 loss 22.932213 loss_att 24.921696 loss_ctc 29.719402 loss_rnnt 21.351517 hw_loss 0.520950 lr 0.00047870 rank 3
2023-02-21 02:56:53,856 DEBUG TRAIN Batch 12/14400 loss 20.226860 loss_att 32.714371 loss_ctc 14.364235 loss_rnnt 18.510866 hw_loss 0.000322 lr 0.00047873 rank 2
2023-02-21 02:56:53,860 DEBUG TRAIN Batch 12/14400 loss 23.038721 loss_att 34.315895 loss_ctc 28.548119 loss_rnnt 19.903881 hw_loss 0.271535 lr 0.00047882 rank 4
2023-02-21 02:56:53,862 DEBUG TRAIN Batch 12/14400 loss 5.736480 loss_att 9.661227 loss_ctc 10.959023 loss_rnnt 4.074030 hw_loss 0.339678 lr 0.00047877 rank 1
2023-02-21 02:56:53,863 DEBUG TRAIN Batch 12/14400 loss 13.147495 loss_att 18.357206 loss_ctc 13.271862 loss_rnnt 11.906658 hw_loss 0.341835 lr 0.00047868 rank 7
2023-02-21 02:56:53,916 DEBUG TRAIN Batch 12/14400 loss 20.361076 loss_att 20.585400 loss_ctc 21.828278 loss_rnnt 19.936605 hw_loss 0.344965 lr 0.00047871 rank 5
2023-02-21 02:57:52,443 DEBUG TRAIN Batch 12/14500 loss 7.403684 loss_att 9.470147 loss_ctc 6.731478 loss_rnnt 7.053099 hw_loss 0.050476 lr 0.00047870 rank 0
2023-02-21 02:57:52,457 DEBUG TRAIN Batch 12/14500 loss 3.493244 loss_att 6.984967 loss_ctc 3.235933 loss_rnnt 2.727000 hw_loss 0.191637 lr 0.00047860 rank 6
2023-02-21 02:57:52,457 DEBUG TRAIN Batch 12/14500 loss 29.364960 loss_att 39.107430 loss_ctc 39.731644 loss_rnnt 25.834360 hw_loss 0.374776 lr 0.00047859 rank 3
2023-02-21 02:57:52,461 DEBUG TRAIN Batch 12/14500 loss 12.445953 loss_att 12.577662 loss_ctc 10.442167 loss_rnnt 12.533391 hw_loss 0.287609 lr 0.00047866 rank 1
2023-02-21 02:57:52,464 DEBUG TRAIN Batch 12/14500 loss 14.488121 loss_att 17.310066 loss_ctc 15.864043 loss_rnnt 13.545911 hw_loss 0.364432 lr 0.00047862 rank 2
2023-02-21 02:57:52,469 DEBUG TRAIN Batch 12/14500 loss 23.756645 loss_att 26.051138 loss_ctc 26.371811 loss_rnnt 22.751778 hw_loss 0.369900 lr 0.00047858 rank 7
2023-02-21 02:57:52,472 DEBUG TRAIN Batch 12/14500 loss 24.106228 loss_att 25.057117 loss_ctc 29.545609 loss_rnnt 23.076633 hw_loss 0.214056 lr 0.00047871 rank 4
2023-02-21 02:57:52,518 DEBUG TRAIN Batch 12/14500 loss 16.564177 loss_att 17.017305 loss_ctc 18.872147 loss_rnnt 16.131735 hw_loss 0.063914 lr 0.00047860 rank 5
2023-02-21 02:58:52,150 DEBUG TRAIN Batch 12/14600 loss 9.239637 loss_att 18.520435 loss_ctc 18.552673 loss_rnnt 5.860431 hw_loss 0.527454 lr 0.00047851 rank 2
2023-02-21 02:58:52,150 DEBUG TRAIN Batch 12/14600 loss 2.168810 loss_att 4.784863 loss_ctc 3.625668 loss_rnnt 1.130447 hw_loss 0.601697 lr 0.00047859 rank 0
2023-02-21 02:58:52,155 DEBUG TRAIN Batch 12/14600 loss 8.310330 loss_att 16.240719 loss_ctc 13.720275 loss_rnnt 5.885284 hw_loss 0.220580 lr 0.00047849 rank 3
2023-02-21 02:58:52,155 DEBUG TRAIN Batch 12/14600 loss 26.629143 loss_att 33.504601 loss_ctc 47.730747 loss_rnnt 22.141979 hw_loss 0.559737 lr 0.00047849 rank 5
2023-02-21 02:58:52,155 DEBUG TRAIN Batch 12/14600 loss 6.685334 loss_att 10.344800 loss_ctc 9.928157 loss_rnnt 5.477016 hw_loss 0.082590 lr 0.00047855 rank 1
2023-02-21 02:58:52,165 DEBUG TRAIN Batch 12/14600 loss 11.593396 loss_att 15.332493 loss_ctc 16.224428 loss_rnnt 10.047949 hw_loss 0.337794 lr 0.00047849 rank 6
2023-02-21 02:58:52,185 DEBUG TRAIN Batch 12/14600 loss 30.852692 loss_att 29.892332 loss_ctc 37.515064 loss_rnnt 30.156277 hw_loss 0.000319 lr 0.00047847 rank 7
2023-02-21 02:58:52,192 DEBUG TRAIN Batch 12/14600 loss 39.246216 loss_att 42.237587 loss_ctc 43.771210 loss_rnnt 38.044483 hw_loss 0.000232 lr 0.00047860 rank 4
2023-02-21 03:00:13,564 DEBUG TRAIN Batch 12/14700 loss 8.761207 loss_att 11.493474 loss_ctc 9.904924 loss_rnnt 7.877926 hw_loss 0.345622 lr 0.00047848 rank 0
2023-02-21 03:00:13,576 DEBUG TRAIN Batch 12/14700 loss 14.066036 loss_att 14.489401 loss_ctc 16.512053 loss_rnnt 13.526074 hw_loss 0.242162 lr 0.00047840 rank 2
2023-02-21 03:00:13,578 DEBUG TRAIN Batch 12/14700 loss 15.282732 loss_att 17.378269 loss_ctc 15.950661 loss_rnnt 14.623566 hw_loss 0.283127 lr 0.00047838 rank 6
2023-02-21 03:00:13,583 DEBUG TRAIN Batch 12/14700 loss 28.919001 loss_att 27.171104 loss_ctc 33.682877 loss_rnnt 28.436886 hw_loss 0.368457 lr 0.00047838 rank 5
2023-02-21 03:00:13,584 DEBUG TRAIN Batch 12/14700 loss 13.747190 loss_att 13.946443 loss_ctc 14.402449 loss_rnnt 13.500814 hw_loss 0.223421 lr 0.00047838 rank 3
2023-02-21 03:00:13,589 DEBUG TRAIN Batch 12/14700 loss 20.665588 loss_att 19.769520 loss_ctc 23.528776 loss_rnnt 20.270254 hw_loss 0.361481 lr 0.00047844 rank 1
2023-02-21 03:00:13,590 DEBUG TRAIN Batch 12/14700 loss 32.684658 loss_att 35.229855 loss_ctc 47.387619 loss_rnnt 30.117786 hw_loss 0.182694 lr 0.00047836 rank 7
2023-02-21 03:00:13,590 DEBUG TRAIN Batch 12/14700 loss 16.920507 loss_att 20.134954 loss_ctc 22.324306 loss_rnnt 15.445257 hw_loss 0.209728 lr 0.00047849 rank 4
2023-02-21 03:01:13,455 DEBUG TRAIN Batch 12/14800 loss 13.945827 loss_att 14.482685 loss_ctc 18.599926 loss_rnnt 13.113558 hw_loss 0.195653 lr 0.00047837 rank 0
2023-02-21 03:01:13,460 DEBUG TRAIN Batch 12/14800 loss 18.457355 loss_att 22.561831 loss_ctc 18.168652 loss_rnnt 17.488102 hw_loss 0.350348 lr 0.00047828 rank 5
2023-02-21 03:01:13,463 DEBUG TRAIN Batch 12/14800 loss 9.448530 loss_att 12.736092 loss_ctc 10.122840 loss_rnnt 8.543838 hw_loss 0.294885 lr 0.00047827 rank 6
2023-02-21 03:01:13,464 DEBUG TRAIN Batch 12/14800 loss 24.758280 loss_att 24.098543 loss_ctc 27.707306 loss_rnnt 24.431278 hw_loss 0.123269 lr 0.00047838 rank 4
2023-02-21 03:01:13,468 DEBUG TRAIN Batch 12/14800 loss 38.733913 loss_att 43.142021 loss_ctc 47.970570 loss_rnnt 36.490704 hw_loss 0.243820 lr 0.00047829 rank 2
2023-02-21 03:01:13,469 DEBUG TRAIN Batch 12/14800 loss 7.648125 loss_att 12.680091 loss_ctc 12.110888 loss_rnnt 5.870810 hw_loss 0.329788 lr 0.00047827 rank 3
2023-02-21 03:01:13,472 DEBUG TRAIN Batch 12/14800 loss 4.610767 loss_att 10.770552 loss_ctc 8.130188 loss_rnnt 2.751356 hw_loss 0.296621 lr 0.00047833 rank 1
2023-02-21 03:01:13,533 DEBUG TRAIN Batch 12/14800 loss 7.718531 loss_att 10.009354 loss_ctc 12.295681 loss_rnnt 6.510463 hw_loss 0.261782 lr 0.00047825 rank 7
2023-02-21 03:02:11,785 DEBUG TRAIN Batch 12/14900 loss 42.587257 loss_att 36.660324 loss_ctc 47.138824 loss_rnnt 43.165726 hw_loss 0.000078 lr 0.00047826 rank 0
2023-02-21 03:02:11,802 DEBUG TRAIN Batch 12/14900 loss 36.804981 loss_att 33.977753 loss_ctc 32.355347 loss_rnnt 37.829823 hw_loss 0.251034 lr 0.00047818 rank 2
2023-02-21 03:02:11,803 DEBUG TRAIN Batch 12/14900 loss 29.193388 loss_att 31.362095 loss_ctc 31.572802 loss_rnnt 28.442347 hw_loss 0.000080 lr 0.00047816 rank 3
2023-02-21 03:02:11,802 DEBUG TRAIN Batch 12/14900 loss 12.872331 loss_att 20.662842 loss_ctc 14.094965 loss_rnnt 11.043857 hw_loss 0.201290 lr 0.00047814 rank 7
2023-02-21 03:02:11,803 DEBUG TRAIN Batch 12/14900 loss 12.376573 loss_att 15.375519 loss_ctc 18.794958 loss_rnnt 10.800927 hw_loss 0.225134 lr 0.00047817 rank 5
2023-02-21 03:02:11,807 DEBUG TRAIN Batch 12/14900 loss 29.688011 loss_att 27.889860 loss_ctc 44.110085 loss_rnnt 28.124653 hw_loss 0.000082 lr 0.00047816 rank 6
2023-02-21 03:02:11,834 DEBUG TRAIN Batch 12/14900 loss 3.243924 loss_att 10.133606 loss_ctc 6.561511 loss_rnnt 1.423602 hw_loss 0.000076 lr 0.00047827 rank 4
2023-02-21 03:02:11,886 DEBUG TRAIN Batch 12/14900 loss 9.537067 loss_att 18.102318 loss_ctc 9.642962 loss_rnnt 7.647328 hw_loss 0.304818 lr 0.00047822 rank 1
2023-02-21 03:03:09,918 DEBUG TRAIN Batch 12/15000 loss 13.350155 loss_att 12.748559 loss_ctc 16.099026 loss_rnnt 13.021302 hw_loss 0.154979 lr 0.00047815 rank 0
2023-02-21 03:03:09,929 DEBUG TRAIN Batch 12/15000 loss 10.293298 loss_att 14.283854 loss_ctc 15.408617 loss_rnnt 8.621737 hw_loss 0.358890 lr 0.00047807 rank 2
2023-02-21 03:03:09,933 DEBUG TRAIN Batch 12/15000 loss 28.143229 loss_att 27.011452 loss_ctc 29.187012 loss_rnnt 28.064068 hw_loss 0.311901 lr 0.00047805 rank 3
2023-02-21 03:03:09,934 DEBUG TRAIN Batch 12/15000 loss 7.848733 loss_att 7.449699 loss_ctc 7.045364 loss_rnnt 7.904580 hw_loss 0.245766 lr 0.00047806 rank 5
2023-02-21 03:03:09,944 DEBUG TRAIN Batch 12/15000 loss 12.656322 loss_att 16.808290 loss_ctc 17.454800 loss_rnnt 11.037465 hw_loss 0.278746 lr 0.00047805 rank 6
2023-02-21 03:03:09,946 DEBUG TRAIN Batch 12/15000 loss 9.655668 loss_att 10.199055 loss_ctc 12.180938 loss_rnnt 8.938480 hw_loss 0.509640 lr 0.00047816 rank 4
2023-02-21 03:03:09,958 DEBUG TRAIN Batch 12/15000 loss 27.944813 loss_att 29.476097 loss_ctc 36.484806 loss_rnnt 26.406471 hw_loss 0.175158 lr 0.00047811 rank 1
2023-02-21 03:03:09,976 DEBUG TRAIN Batch 12/15000 loss 23.941854 loss_att 33.204952 loss_ctc 33.851460 loss_rnnt 20.683340 hw_loss 0.158655 lr 0.00047803 rank 7
2023-02-21 03:04:10,110 DEBUG TRAIN Batch 12/15100 loss 28.271513 loss_att 35.400764 loss_ctc 31.505682 loss_rnnt 26.336746 hw_loss 0.145680 lr 0.00047804 rank 0
2023-02-21 03:04:10,112 DEBUG TRAIN Batch 12/15100 loss 14.964389 loss_att 18.768234 loss_ctc 22.545313 loss_rnnt 13.018773 hw_loss 0.326354 lr 0.00047795 rank 5
2023-02-21 03:04:10,116 DEBUG TRAIN Batch 12/15100 loss 2.409407 loss_att 7.849918 loss_ctc 5.423031 loss_rnnt 0.773798 hw_loss 0.273168 lr 0.00047794 rank 6
2023-02-21 03:04:10,119 DEBUG TRAIN Batch 12/15100 loss 4.160658 loss_att 9.174773 loss_ctc 2.839032 loss_rnnt 3.300446 hw_loss 0.063012 lr 0.00047800 rank 1
2023-02-21 03:04:10,121 DEBUG TRAIN Batch 12/15100 loss 10.671763 loss_att 20.010536 loss_ctc 17.278301 loss_rnnt 7.749934 hw_loss 0.324758 lr 0.00047796 rank 2
2023-02-21 03:04:10,123 DEBUG TRAIN Batch 12/15100 loss 20.459242 loss_att 28.180738 loss_ctc 29.634766 loss_rnnt 17.604164 hw_loss 0.163825 lr 0.00047805 rank 4
2023-02-21 03:04:10,127 DEBUG TRAIN Batch 12/15100 loss 8.209127 loss_att 11.435953 loss_ctc 12.147253 loss_rnnt 6.906880 hw_loss 0.247120 lr 0.00047794 rank 3
2023-02-21 03:04:10,181 DEBUG TRAIN Batch 12/15100 loss 11.225294 loss_att 16.177902 loss_ctc 15.376027 loss_rnnt 9.434749 hw_loss 0.462363 lr 0.00047792 rank 7
2023-02-21 03:05:08,750 DEBUG TRAIN Batch 12/15200 loss 17.295298 loss_att 20.381834 loss_ctc 29.608927 loss_rnnt 14.929239 hw_loss 0.200500 lr 0.00047793 rank 0
2023-02-21 03:05:08,756 DEBUG TRAIN Batch 12/15200 loss 21.394794 loss_att 18.265728 loss_ctc 24.351015 loss_rnnt 21.545502 hw_loss 0.151773 lr 0.00047784 rank 5
2023-02-21 03:05:08,758 DEBUG TRAIN Batch 12/15200 loss 5.866259 loss_att 8.600611 loss_ctc 11.866591 loss_rnnt 4.284845 hw_loss 0.439684 lr 0.00047783 rank 3
2023-02-21 03:05:08,759 DEBUG TRAIN Batch 12/15200 loss 12.790360 loss_att 13.310575 loss_ctc 11.375902 loss_rnnt 12.617287 hw_loss 0.483044 lr 0.00047785 rank 2
2023-02-21 03:05:08,761 DEBUG TRAIN Batch 12/15200 loss 17.565393 loss_att 18.650398 loss_ctc 25.844948 loss_rnnt 16.244404 hw_loss 0.000089 lr 0.00047783 rank 6
2023-02-21 03:05:08,761 DEBUG TRAIN Batch 12/15200 loss 9.346646 loss_att 10.846420 loss_ctc 15.684443 loss_rnnt 8.201605 hw_loss 0.000088 lr 0.00047795 rank 4
2023-02-21 03:05:08,763 DEBUG TRAIN Batch 12/15200 loss 8.940776 loss_att 17.588747 loss_ctc 9.320008 loss_rnnt 7.160565 hw_loss 0.000098 lr 0.00047781 rank 7
2023-02-21 03:05:08,820 DEBUG TRAIN Batch 12/15200 loss 5.136903 loss_att 6.464088 loss_ctc 3.449447 loss_rnnt 4.867098 hw_loss 0.430052 lr 0.00047789 rank 1
2023-02-21 03:06:07,939 DEBUG TRAIN Batch 12/15300 loss 17.351963 loss_att 22.752525 loss_ctc 23.233727 loss_rnnt 15.433298 hw_loss 0.101841 lr 0.00047773 rank 5
2023-02-21 03:06:07,939 DEBUG TRAIN Batch 12/15300 loss 22.533716 loss_att 24.670828 loss_ctc 31.152325 loss_rnnt 20.837246 hw_loss 0.224808 lr 0.00047774 rank 2
2023-02-21 03:06:07,942 DEBUG TRAIN Batch 12/15300 loss 23.067570 loss_att 28.482162 loss_ctc 35.053967 loss_rnnt 20.247173 hw_loss 0.261173 lr 0.00047782 rank 0
2023-02-21 03:06:07,947 DEBUG TRAIN Batch 12/15300 loss 5.469077 loss_att 10.001093 loss_ctc 12.740864 loss_rnnt 3.385866 hw_loss 0.388568 lr 0.00047770 rank 7
2023-02-21 03:06:07,951 DEBUG TRAIN Batch 12/15300 loss 19.949116 loss_att 26.402012 loss_ctc 19.934948 loss_rnnt 18.477703 hw_loss 0.342604 lr 0.00047772 rank 3
2023-02-21 03:06:07,982 DEBUG TRAIN Batch 12/15300 loss 30.789574 loss_att 31.716246 loss_ctc 39.259686 loss_rnnt 29.342079 hw_loss 0.249021 lr 0.00047772 rank 6
2023-02-21 03:06:07,994 DEBUG TRAIN Batch 12/15300 loss 25.179089 loss_att 32.935822 loss_ctc 26.421814 loss_rnnt 23.176052 hw_loss 0.536240 lr 0.00047779 rank 1
2023-02-21 03:06:08,002 DEBUG TRAIN Batch 12/15300 loss 22.614582 loss_att 20.333002 loss_ctc 25.119766 loss_rnnt 22.587137 hw_loss 0.280755 lr 0.00047784 rank 4
2023-02-21 03:07:08,978 DEBUG TRAIN Batch 12/15400 loss 13.737015 loss_att 20.549007 loss_ctc 18.173210 loss_rnnt 11.672265 hw_loss 0.207858 lr 0.00047771 rank 0
2023-02-21 03:07:08,981 DEBUG TRAIN Batch 12/15400 loss 45.765514 loss_att 54.088039 loss_ctc 32.168659 loss_rnnt 45.832760 hw_loss 0.152192 lr 0.00047768 rank 1
2023-02-21 03:07:08,982 DEBUG TRAIN Batch 12/15400 loss 10.224493 loss_att 14.888239 loss_ctc 13.652689 loss_rnnt 8.768572 hw_loss 0.123897 lr 0.00047762 rank 5
2023-02-21 03:07:08,984 DEBUG TRAIN Batch 12/15400 loss 19.740618 loss_att 23.878689 loss_ctc 22.397711 loss_rnnt 18.422157 hw_loss 0.256063 lr 0.00047761 rank 3
2023-02-21 03:07:08,987 DEBUG TRAIN Batch 12/15400 loss 12.957783 loss_att 18.288788 loss_ctc 11.826572 loss_rnnt 11.891925 hw_loss 0.282158 lr 0.00047763 rank 2
2023-02-21 03:07:08,987 DEBUG TRAIN Batch 12/15400 loss 23.777050 loss_att 26.376257 loss_ctc 29.049868 loss_rnnt 22.346413 hw_loss 0.389538 lr 0.00047773 rank 4
2023-02-21 03:07:08,987 DEBUG TRAIN Batch 12/15400 loss 3.152381 loss_att 7.045233 loss_ctc 5.688311 loss_rnnt 1.881949 hw_loss 0.288259 lr 0.00047759 rank 7
2023-02-21 03:07:08,990 DEBUG TRAIN Batch 12/15400 loss 28.571007 loss_att 25.564384 loss_ctc 36.496269 loss_rnnt 28.113510 hw_loss 0.003971 lr 0.00047762 rank 6
2023-02-21 03:08:07,602 DEBUG TRAIN Batch 12/15500 loss 5.537540 loss_att 8.831137 loss_ctc 6.562516 loss_rnnt 4.742072 hw_loss 0.000162 lr 0.00047752 rank 2
2023-02-21 03:08:07,604 DEBUG TRAIN Batch 12/15500 loss 80.304024 loss_att 99.177460 loss_ctc 104.368065 loss_rnnt 73.146965 hw_loss 0.325939 lr 0.00047750 rank 3
2023-02-21 03:08:07,603 DEBUG TRAIN Batch 12/15500 loss 17.585211 loss_att 33.329384 loss_ctc 29.327726 loss_rnnt 12.796998 hw_loss 0.138204 lr 0.00047751 rank 6
2023-02-21 03:08:07,606 DEBUG TRAIN Batch 12/15500 loss 11.456193 loss_att 15.542048 loss_ctc 14.930059 loss_rnnt 10.123584 hw_loss 0.097980 lr 0.00047762 rank 4
2023-02-21 03:08:07,607 DEBUG TRAIN Batch 12/15500 loss 19.340836 loss_att 21.811787 loss_ctc 31.025919 loss_rnnt 17.201523 hw_loss 0.163332 lr 0.00047751 rank 5
2023-02-21 03:08:07,610 DEBUG TRAIN Batch 12/15500 loss 30.637390 loss_att 39.273415 loss_ctc 33.006847 loss_rnnt 28.546919 hw_loss 0.088765 lr 0.00047760 rank 0
2023-02-21 03:08:07,617 DEBUG TRAIN Batch 12/15500 loss 8.252593 loss_att 10.533797 loss_ctc 8.003664 loss_rnnt 7.829325 hw_loss 0.000409 lr 0.00047748 rank 7
2023-02-21 03:08:07,675 DEBUG TRAIN Batch 12/15500 loss 19.950857 loss_att 24.985474 loss_ctc 19.488651 loss_rnnt 18.777552 hw_loss 0.427511 lr 0.00047757 rank 1
2023-02-21 03:09:28,672 DEBUG TRAIN Batch 12/15600 loss 11.882972 loss_att 19.840025 loss_ctc 20.821344 loss_rnnt 9.039484 hw_loss 0.113050 lr 0.00047749 rank 0
2023-02-21 03:09:28,674 DEBUG TRAIN Batch 12/15600 loss 37.192837 loss_att 38.687943 loss_ctc 49.189148 loss_rnnt 35.079559 hw_loss 0.402650 lr 0.00047742 rank 2
2023-02-21 03:09:28,678 DEBUG TRAIN Batch 12/15600 loss 3.066852 loss_att 6.571659 loss_ctc 2.590265 loss_rnnt 2.267970 hw_loss 0.302746 lr 0.00047746 rank 1
2023-02-21 03:09:28,680 DEBUG TRAIN Batch 12/15600 loss 9.024600 loss_att 10.971761 loss_ctc 11.095035 loss_rnnt 8.250920 hw_loss 0.202855 lr 0.00047739 rank 3
2023-02-21 03:09:28,683 DEBUG TRAIN Batch 12/15600 loss 4.920765 loss_att 7.112323 loss_ctc 4.717387 loss_rnnt 4.377200 hw_loss 0.248195 lr 0.00047740 rank 6
2023-02-21 03:09:28,686 DEBUG TRAIN Batch 12/15600 loss 23.806089 loss_att 26.008261 loss_ctc 24.785812 loss_rnnt 23.037281 hw_loss 0.370768 lr 0.00047737 rank 7
2023-02-21 03:09:28,687 DEBUG TRAIN Batch 12/15600 loss 11.860225 loss_att 13.280478 loss_ctc 9.696099 loss_rnnt 11.679161 hw_loss 0.347928 lr 0.00047751 rank 4
2023-02-21 03:09:28,740 DEBUG TRAIN Batch 12/15600 loss 25.757048 loss_att 30.492664 loss_ctc 35.834160 loss_rnnt 23.392469 hw_loss 0.138447 lr 0.00047740 rank 5
2023-02-21 03:10:30,278 DEBUG TRAIN Batch 12/15700 loss 17.892784 loss_att 22.538481 loss_ctc 25.860613 loss_rnnt 15.729084 hw_loss 0.322841 lr 0.00047728 rank 3
2023-02-21 03:10:30,279 DEBUG TRAIN Batch 12/15700 loss 30.960268 loss_att 30.557056 loss_ctc 34.470165 loss_rnnt 30.371212 hw_loss 0.378215 lr 0.00047738 rank 0
2023-02-21 03:10:30,284 DEBUG TRAIN Batch 12/15700 loss 12.304323 loss_att 14.591423 loss_ctc 9.617294 loss_rnnt 12.010377 hw_loss 0.365242 lr 0.00047731 rank 2
2023-02-21 03:10:30,285 DEBUG TRAIN Batch 12/15700 loss 17.352688 loss_att 19.505127 loss_ctc 21.763395 loss_rnnt 16.333139 hw_loss 0.001806 lr 0.00047735 rank 1
2023-02-21 03:10:30,287 DEBUG TRAIN Batch 12/15700 loss 4.719308 loss_att 7.621710 loss_ctc 6.615230 loss_rnnt 3.598664 hw_loss 0.538826 lr 0.00047729 rank 6
2023-02-21 03:10:30,288 DEBUG TRAIN Batch 12/15700 loss 13.212246 loss_att 11.122665 loss_ctc 14.013490 loss_rnnt 13.288564 hw_loss 0.440185 lr 0.00047729 rank 5
2023-02-21 03:10:30,292 DEBUG TRAIN Batch 12/15700 loss 25.948847 loss_att 29.466043 loss_ctc 30.904833 loss_rnnt 24.583813 hw_loss 0.001498 lr 0.00047740 rank 4
2023-02-21 03:10:30,291 DEBUG TRAIN Batch 12/15700 loss 5.802361 loss_att 11.018593 loss_ctc 6.212702 loss_rnnt 4.551991 hw_loss 0.285774 lr 0.00047727 rank 7
2023-02-21 03:11:28,373 DEBUG TRAIN Batch 12/15800 loss 22.202394 loss_att 26.218819 loss_ctc 28.567513 loss_rnnt 20.550386 hw_loss 0.000076 lr 0.00047728 rank 0
2023-02-21 03:11:28,384 DEBUG TRAIN Batch 12/15800 loss 20.938660 loss_att 23.117697 loss_ctc 24.560705 loss_rnnt 19.771095 hw_loss 0.466531 lr 0.00047718 rank 6
2023-02-21 03:11:28,388 DEBUG TRAIN Batch 12/15800 loss 13.566921 loss_att 15.940992 loss_ctc 18.743149 loss_rnnt 12.182723 hw_loss 0.411039 lr 0.00047718 rank 5
2023-02-21 03:11:28,389 DEBUG TRAIN Batch 12/15800 loss 24.748924 loss_att 30.229252 loss_ctc 22.659452 loss_rnnt 23.761744 hw_loss 0.318205 lr 0.00047718 rank 3
2023-02-21 03:11:28,390 DEBUG TRAIN Batch 12/15800 loss 7.877884 loss_att 7.649613 loss_ctc 9.224383 loss_rnnt 7.460506 hw_loss 0.531561 lr 0.00047716 rank 7
2023-02-21 03:11:28,391 DEBUG TRAIN Batch 12/15800 loss 1.180751 loss_att 4.856548 loss_ctc 1.689940 loss_rnnt 0.256651 hw_loss 0.226968 lr 0.00047720 rank 2
2023-02-21 03:11:28,392 DEBUG TRAIN Batch 12/15800 loss 8.472942 loss_att 12.644568 loss_ctc 6.696769 loss_rnnt 7.748838 hw_loss 0.237379 lr 0.00047729 rank 4
2023-02-21 03:11:28,391 DEBUG TRAIN Batch 12/15800 loss 5.874391 loss_att 10.572340 loss_ctc 11.498254 loss_rnnt 4.184910 hw_loss 0.000080 lr 0.00047724 rank 1
2023-02-21 03:12:26,651 DEBUG TRAIN Batch 12/15900 loss 18.382553 loss_att 18.795137 loss_ctc 22.128880 loss_rnnt 17.632454 hw_loss 0.315136 lr 0.00047717 rank 0
2023-02-21 03:12:26,654 DEBUG TRAIN Batch 12/15900 loss 22.343470 loss_att 23.562246 loss_ctc 26.946095 loss_rnnt 21.357969 hw_loss 0.240122 lr 0.00047707 rank 3
2023-02-21 03:12:26,655 DEBUG TRAIN Batch 12/15900 loss 13.698835 loss_att 15.327549 loss_ctc 12.681713 loss_rnnt 13.334877 hw_loss 0.325936 lr 0.00047708 rank 5
2023-02-21 03:12:26,659 DEBUG TRAIN Batch 12/15900 loss 26.791250 loss_att 25.112740 loss_ctc 22.926338 loss_rnnt 27.512993 hw_loss 0.242403 lr 0.00047718 rank 4
2023-02-21 03:12:26,663 DEBUG TRAIN Batch 12/15900 loss 4.017618 loss_att 6.680229 loss_ctc 7.154202 loss_rnnt 3.038355 hw_loss 0.053491 lr 0.00047707 rank 6
2023-02-21 03:12:26,664 DEBUG TRAIN Batch 12/15900 loss 16.947300 loss_att 25.258121 loss_ctc 26.839441 loss_rnnt 13.875130 hw_loss 0.170720 lr 0.00047709 rank 2
2023-02-21 03:12:26,671 DEBUG TRAIN Batch 12/15900 loss 16.051775 loss_att 19.336800 loss_ctc 14.274208 loss_rnnt 15.475737 hw_loss 0.292580 lr 0.00047705 rank 7
2023-02-21 03:12:26,729 DEBUG TRAIN Batch 12/15900 loss 9.834579 loss_att 10.450964 loss_ctc 10.304338 loss_rnnt 9.574548 hw_loss 0.138971 lr 0.00047713 rank 1
2023-02-21 03:13:28,021 DEBUG TRAIN Batch 12/16000 loss 33.864155 loss_att 42.602226 loss_ctc 32.381123 loss_rnnt 32.216347 hw_loss 0.183617 lr 0.00047706 rank 0
2023-02-21 03:13:28,027 DEBUG TRAIN Batch 12/16000 loss 2.897583 loss_att 9.445533 loss_ctc 6.246329 loss_rnnt 1.140873 hw_loss 0.001163 lr 0.00047702 rank 1
2023-02-21 03:13:28,028 DEBUG TRAIN Batch 12/16000 loss 28.536646 loss_att 27.985233 loss_ctc 40.439548 loss_rnnt 26.972271 hw_loss 0.164258 lr 0.00047696 rank 6
2023-02-21 03:13:28,028 DEBUG TRAIN Batch 12/16000 loss 29.627951 loss_att 30.739773 loss_ctc 35.025547 loss_rnnt 28.685219 hw_loss 0.001295 lr 0.00047697 rank 5
2023-02-21 03:13:28,032 DEBUG TRAIN Batch 12/16000 loss 9.050266 loss_att 13.390155 loss_ctc 14.632759 loss_rnnt 7.437401 hw_loss 0.001040 lr 0.00047707 rank 4
2023-02-21 03:13:28,035 DEBUG TRAIN Batch 12/16000 loss 13.567834 loss_att 19.212601 loss_ctc 17.884045 loss_rnnt 11.712206 hw_loss 0.283460 lr 0.00047698 rank 2
2023-02-21 03:13:28,040 DEBUG TRAIN Batch 12/16000 loss 23.162737 loss_att 27.291327 loss_ctc 26.344160 loss_rnnt 21.839457 hw_loss 0.137573 lr 0.00047696 rank 3
2023-02-21 03:13:28,045 DEBUG TRAIN Batch 12/16000 loss 14.236358 loss_att 19.700487 loss_ctc 27.084846 loss_rnnt 11.216937 hw_loss 0.400243 lr 0.00047694 rank 7
2023-02-21 03:14:25,266 DEBUG TRAIN Batch 12/16100 loss 22.418030 loss_att 28.263325 loss_ctc 19.375441 loss_rnnt 21.386711 hw_loss 0.502386 lr 0.00047687 rank 2
2023-02-21 03:14:25,271 DEBUG TRAIN Batch 12/16100 loss 34.654457 loss_att 35.859962 loss_ctc 43.106754 loss_rnnt 33.200653 hw_loss 0.160742 lr 0.00047695 rank 0
2023-02-21 03:14:25,277 DEBUG TRAIN Batch 12/16100 loss 14.476244 loss_att 16.428520 loss_ctc 17.450272 loss_rnnt 13.509753 hw_loss 0.336559 lr 0.00047692 rank 1
2023-02-21 03:14:25,278 DEBUG TRAIN Batch 12/16100 loss 14.848127 loss_att 14.408187 loss_ctc 18.234787 loss_rnnt 14.310907 hw_loss 0.325599 lr 0.00047685 rank 6
2023-02-21 03:14:25,279 DEBUG TRAIN Batch 12/16100 loss 20.440283 loss_att 20.960316 loss_ctc 23.049561 loss_rnnt 19.895542 hw_loss 0.174058 lr 0.00047683 rank 7
2023-02-21 03:14:25,285 DEBUG TRAIN Batch 12/16100 loss 11.291310 loss_att 13.885680 loss_ctc 20.197027 loss_rnnt 9.363376 hw_loss 0.415558 lr 0.00047685 rank 3
2023-02-21 03:14:25,285 DEBUG TRAIN Batch 12/16100 loss 23.701097 loss_att 31.042128 loss_ctc 42.352180 loss_rnnt 19.746040 hw_loss 0.000077 lr 0.00047686 rank 5
2023-02-21 03:14:25,293 DEBUG TRAIN Batch 12/16100 loss 3.238962 loss_att 7.557702 loss_ctc 1.878026 loss_rnnt 2.474457 hw_loss 0.154153 lr 0.00047697 rank 4
2023-02-21 03:15:23,692 DEBUG TRAIN Batch 12/16200 loss 5.956578 loss_att 12.261400 loss_ctc 8.152239 loss_rnnt 4.244446 hw_loss 0.297025 lr 0.00047684 rank 0
2023-02-21 03:15:23,693 DEBUG TRAIN Batch 12/16200 loss 24.234491 loss_att 22.334894 loss_ctc 24.515518 loss_rnnt 24.426270 hw_loss 0.282506 lr 0.00047681 rank 1
2023-02-21 03:15:23,695 DEBUG TRAIN Batch 12/16200 loss 9.625613 loss_att 13.863007 loss_ctc 19.320103 loss_rnnt 7.484189 hw_loss 0.002526 lr 0.00047675 rank 6
2023-02-21 03:15:23,696 DEBUG TRAIN Batch 12/16200 loss 31.102097 loss_att 34.689922 loss_ctc 35.338638 loss_rnnt 29.708363 hw_loss 0.208683 lr 0.00047686 rank 4
2023-02-21 03:15:23,697 DEBUG TRAIN Batch 12/16200 loss 9.021854 loss_att 10.160666 loss_ctc 13.572612 loss_rnnt 7.957253 hw_loss 0.431385 lr 0.00047676 rank 2
2023-02-21 03:15:23,698 DEBUG TRAIN Batch 12/16200 loss 11.960810 loss_att 13.829294 loss_ctc 11.350612 loss_rnnt 11.605567 hw_loss 0.117949 lr 0.00047674 rank 3
2023-02-21 03:15:23,698 DEBUG TRAIN Batch 12/16200 loss 10.283932 loss_att 12.986486 loss_ctc 10.632952 loss_rnnt 9.587025 hw_loss 0.205987 lr 0.00047675 rank 5
2023-02-21 03:15:23,699 DEBUG TRAIN Batch 12/16200 loss 12.282619 loss_att 16.269745 loss_ctc 18.977186 loss_rnnt 10.434568 hw_loss 0.296280 lr 0.00047672 rank 7
2023-02-21 03:16:24,640 DEBUG TRAIN Batch 12/16300 loss 10.504213 loss_att 12.453522 loss_ctc 11.554291 loss_rnnt 9.706665 hw_loss 0.501892 lr 0.00047673 rank 0
2023-02-21 03:16:24,652 DEBUG TRAIN Batch 12/16300 loss 9.821829 loss_att 14.662867 loss_ctc 15.396320 loss_rnnt 7.880364 hw_loss 0.431233 lr 0.00047666 rank 2
2023-02-21 03:16:24,659 DEBUG TRAIN Batch 12/16300 loss 21.744690 loss_att 27.688423 loss_ctc 28.808641 loss_rnnt 19.614017 hw_loss 0.000126 lr 0.00047663 rank 3
2023-02-21 03:16:24,661 DEBUG TRAIN Batch 12/16300 loss 3.526326 loss_att 8.759983 loss_ctc 4.041353 loss_rnnt 2.410874 hw_loss 0.000093 lr 0.00047664 rank 6
2023-02-21 03:16:24,660 DEBUG TRAIN Batch 12/16300 loss 10.098704 loss_att 14.962788 loss_ctc 15.302739 loss_rnnt 8.382690 hw_loss 0.092484 lr 0.00047664 rank 5
2023-02-21 03:16:24,661 DEBUG TRAIN Batch 12/16300 loss 1.077367 loss_att 5.097984 loss_ctc 1.297001 loss_rnnt 0.243903 hw_loss 0.000105 lr 0.00047670 rank 1
2023-02-21 03:16:24,661 DEBUG TRAIN Batch 12/16300 loss 8.316741 loss_att 17.830393 loss_ctc 10.131694 loss_rnnt 5.975190 hw_loss 0.369050 lr 0.00047661 rank 7
2023-02-21 03:16:24,694 DEBUG TRAIN Batch 12/16300 loss 18.257137 loss_att 28.565538 loss_ctc 22.768806 loss_rnnt 15.351901 hw_loss 0.453748 lr 0.00047675 rank 4
2023-02-21 03:17:53,289 DEBUG TRAIN Batch 12/16400 loss 27.577572 loss_att 25.918264 loss_ctc 30.067211 loss_rnnt 27.380310 hw_loss 0.369697 lr 0.00047653 rank 6
2023-02-21 03:17:53,296 DEBUG TRAIN Batch 12/16400 loss 9.437766 loss_att 11.591551 loss_ctc 8.223102 loss_rnnt 8.931538 hw_loss 0.445175 lr 0.00047664 rank 4
2023-02-21 03:17:53,297 DEBUG TRAIN Batch 12/16400 loss 24.314182 loss_att 24.099417 loss_ctc 29.321690 loss_rnnt 23.612108 hw_loss 0.145051 lr 0.00047651 rank 7
2023-02-21 03:17:53,301 DEBUG TRAIN Batch 12/16400 loss 23.653976 loss_att 22.961210 loss_ctc 27.191343 loss_rnnt 23.197432 hw_loss 0.231471 lr 0.00047659 rank 1
2023-02-21 03:17:53,303 DEBUG TRAIN Batch 12/16400 loss 23.580605 loss_att 25.255037 loss_ctc 30.228775 loss_rnnt 22.160641 hw_loss 0.372482 lr 0.00047663 rank 0
2023-02-21 03:17:53,323 DEBUG TRAIN Batch 12/16400 loss 26.354752 loss_att 22.025394 loss_ctc 31.678949 loss_rnnt 26.429409 hw_loss 0.152478 lr 0.00047655 rank 2
2023-02-21 03:17:53,332 DEBUG TRAIN Batch 12/16400 loss 5.671730 loss_att 11.051750 loss_ctc 6.752246 loss_rnnt 4.451603 hw_loss 0.000100 lr 0.00047653 rank 3
2023-02-21 03:17:53,333 DEBUG TRAIN Batch 12/16400 loss 18.706804 loss_att 17.503847 loss_ctc 20.714121 loss_rnnt 18.605381 hw_loss 0.139444 lr 0.00047653 rank 5
2023-02-21 03:18:53,253 DEBUG TRAIN Batch 12/16500 loss 11.664069 loss_att 19.715469 loss_ctc 13.303220 loss_rnnt 9.746945 hw_loss 0.165544 lr 0.00047644 rank 2
2023-02-21 03:18:53,255 DEBUG TRAIN Batch 12/16500 loss 8.161430 loss_att 19.324177 loss_ctc 13.389523 loss_rnnt 5.122873 hw_loss 0.204243 lr 0.00047648 rank 1
2023-02-21 03:18:53,259 DEBUG TRAIN Batch 12/16500 loss 11.881042 loss_att 14.104092 loss_ctc 20.421371 loss_rnnt 10.052117 hw_loss 0.460506 lr 0.00047652 rank 0
2023-02-21 03:18:53,261 DEBUG TRAIN Batch 12/16500 loss 10.846635 loss_att 16.754223 loss_ctc 16.459435 loss_rnnt 8.747763 hw_loss 0.316841 lr 0.00047643 rank 5
2023-02-21 03:18:53,265 DEBUG TRAIN Batch 12/16500 loss 8.314498 loss_att 9.910990 loss_ctc 8.309071 loss_rnnt 7.835624 hw_loss 0.300563 lr 0.00047642 rank 6
2023-02-21 03:18:53,266 DEBUG TRAIN Batch 12/16500 loss 11.363290 loss_att 13.900955 loss_ctc 14.678646 loss_rnnt 10.257470 hw_loss 0.292947 lr 0.00047642 rank 3
2023-02-21 03:18:53,268 DEBUG TRAIN Batch 12/16500 loss 19.309698 loss_att 32.681061 loss_ctc 29.078644 loss_rnnt 15.224155 hw_loss 0.203894 lr 0.00047653 rank 4
2023-02-21 03:18:53,276 DEBUG TRAIN Batch 12/16500 loss 14.865753 loss_att 22.165119 loss_ctc 23.430824 loss_rnnt 12.176643 hw_loss 0.163550 lr 0.00047640 rank 7
2023-02-21 03:19:53,267 DEBUG TRAIN Batch 12/16600 loss 34.855671 loss_att 40.160156 loss_ctc 49.451054 loss_rnnt 31.567949 hw_loss 0.526448 lr 0.00047641 rank 0
2023-02-21 03:19:53,271 DEBUG TRAIN Batch 12/16600 loss 12.903328 loss_att 16.661533 loss_ctc 11.115773 loss_rnnt 12.189049 hw_loss 0.376834 lr 0.00047633 rank 2
2023-02-21 03:19:53,276 DEBUG TRAIN Batch 12/16600 loss 32.368546 loss_att 34.645905 loss_ctc 40.548809 loss_rnnt 30.822142 hw_loss 0.000440 lr 0.00047631 rank 3
2023-02-21 03:19:53,276 DEBUG TRAIN Batch 12/16600 loss 5.783098 loss_att 9.318787 loss_ctc 3.266776 loss_rnnt 5.266829 hw_loss 0.271201 lr 0.00047642 rank 4
2023-02-21 03:19:53,277 DEBUG TRAIN Batch 12/16600 loss 23.299440 loss_att 20.693331 loss_ctc 26.517433 loss_rnnt 23.240776 hw_loss 0.282786 lr 0.00047629 rank 7
2023-02-21 03:19:53,277 DEBUG TRAIN Batch 12/16600 loss 1.217809 loss_att 5.130750 loss_ctc 1.169780 loss_rnnt 0.441138 hw_loss 0.000913 lr 0.00047631 rank 6
2023-02-21 03:19:53,312 DEBUG TRAIN Batch 12/16600 loss 49.010269 loss_att 46.323792 loss_ctc 54.179646 loss_rnnt 48.706364 hw_loss 0.284897 lr 0.00047637 rank 1
2023-02-21 03:19:53,339 DEBUG TRAIN Batch 12/16600 loss 9.843550 loss_att 12.509231 loss_ctc 16.971090 loss_rnnt 8.102613 hw_loss 0.482740 lr 0.00047632 rank 5
2023-02-21 03:20:52,055 DEBUG TRAIN Batch 12/16700 loss 14.757762 loss_att 13.071703 loss_ctc 20.014629 loss_rnnt 14.197886 hw_loss 0.367822 lr 0.00047630 rank 0
2023-02-21 03:20:52,064 DEBUG TRAIN Batch 12/16700 loss 14.254915 loss_att 15.305906 loss_ctc 16.194990 loss_rnnt 13.667895 hw_loss 0.221521 lr 0.00047621 rank 6
2023-02-21 03:20:52,066 DEBUG TRAIN Batch 12/16700 loss 10.839544 loss_att 26.348946 loss_ctc 15.587465 loss_rnnt 7.104333 hw_loss 0.000514 lr 0.00047622 rank 2
2023-02-21 03:20:52,067 DEBUG TRAIN Batch 12/16700 loss 24.091404 loss_att 25.722248 loss_ctc 25.064383 loss_rnnt 23.463705 hw_loss 0.322121 lr 0.00047621 rank 5
2023-02-21 03:20:52,069 DEBUG TRAIN Batch 12/16700 loss 11.736142 loss_att 19.075598 loss_ctc 16.217913 loss_rnnt 9.529769 hw_loss 0.264213 lr 0.00047627 rank 1
2023-02-21 03:20:52,075 DEBUG TRAIN Batch 12/16700 loss 12.497407 loss_att 15.263058 loss_ctc 22.115826 loss_rnnt 10.594867 hw_loss 0.125540 lr 0.00047618 rank 7
2023-02-21 03:20:52,083 DEBUG TRAIN Batch 12/16700 loss 13.306873 loss_att 14.450400 loss_ctc 13.917232 loss_rnnt 12.789118 hw_loss 0.389375 lr 0.00047620 rank 3
2023-02-21 03:20:52,132 DEBUG TRAIN Batch 12/16700 loss 29.017216 loss_att 34.455284 loss_ctc 40.622662 loss_rnnt 26.142324 hw_loss 0.449783 lr 0.00047632 rank 4
2023-02-21 03:21:53,245 DEBUG TRAIN Batch 12/16800 loss 30.845160 loss_att 29.086983 loss_ctc 34.608940 loss_rnnt 30.486641 hw_loss 0.390590 lr 0.00047619 rank 0
2023-02-21 03:21:53,249 DEBUG TRAIN Batch 12/16800 loss 11.227108 loss_att 15.387942 loss_ctc 12.258170 loss_rnnt 10.257206 hw_loss 0.000488 lr 0.00047611 rank 2
2023-02-21 03:21:53,252 DEBUG TRAIN Batch 12/16800 loss 15.899975 loss_att 18.455507 loss_ctc 18.152180 loss_rnnt 14.844621 hw_loss 0.457410 lr 0.00047609 rank 3
2023-02-21 03:21:53,254 DEBUG TRAIN Batch 12/16800 loss 32.684227 loss_att 35.503304 loss_ctc 32.817665 loss_rnnt 31.998909 hw_loss 0.194452 lr 0.00047616 rank 1
2023-02-21 03:21:53,259 DEBUG TRAIN Batch 12/16800 loss 22.967375 loss_att 32.121231 loss_ctc 21.798620 loss_rnnt 21.292160 hw_loss 0.000518 lr 0.00047607 rank 7
2023-02-21 03:21:53,290 DEBUG TRAIN Batch 12/16800 loss 28.151314 loss_att 35.913177 loss_ctc 33.588959 loss_rnnt 25.830688 hw_loss 0.081061 lr 0.00047610 rank 6
2023-02-21 03:21:53,302 DEBUG TRAIN Batch 12/16800 loss 15.747307 loss_att 19.328077 loss_ctc 25.370613 loss_rnnt 13.575216 hw_loss 0.324053 lr 0.00047621 rank 4
2023-02-21 03:21:53,315 DEBUG TRAIN Batch 12/16800 loss 19.249306 loss_att 24.276628 loss_ctc 30.369669 loss_rnnt 16.610739 hw_loss 0.281976 lr 0.00047610 rank 5
2023-02-21 03:22:51,859 DEBUG TRAIN Batch 12/16900 loss 15.420094 loss_att 16.917828 loss_ctc 18.559717 loss_rnnt 14.483059 hw_loss 0.410385 lr 0.00047608 rank 0
2023-02-21 03:22:51,869 DEBUG TRAIN Batch 12/16900 loss 15.774328 loss_att 19.246149 loss_ctc 21.711784 loss_rnnt 14.188549 hw_loss 0.187038 lr 0.00047599 rank 5
2023-02-21 03:22:51,871 DEBUG TRAIN Batch 12/16900 loss 20.692940 loss_att 23.260572 loss_ctc 20.770102 loss_rnnt 20.018301 hw_loss 0.282791 lr 0.00047605 rank 1
2023-02-21 03:22:51,877 DEBUG TRAIN Batch 12/16900 loss 4.089143 loss_att 7.475955 loss_ctc 6.544207 loss_rnnt 3.084057 hw_loss 0.000717 lr 0.00047597 rank 7
2023-02-21 03:22:51,879 DEBUG TRAIN Batch 12/16900 loss 16.222914 loss_att 23.149122 loss_ctc 22.289770 loss_rnnt 13.814493 hw_loss 0.401745 lr 0.00047610 rank 4
2023-02-21 03:22:51,880 DEBUG TRAIN Batch 12/16900 loss 18.131756 loss_att 16.389687 loss_ctc 21.148310 loss_rnnt 17.815783 hw_loss 0.491585 lr 0.00047599 rank 6
2023-02-21 03:22:51,883 DEBUG TRAIN Batch 12/16900 loss 15.191906 loss_att 17.057350 loss_ctc 25.688065 loss_rnnt 13.183158 hw_loss 0.442822 lr 0.00047601 rank 2
2023-02-21 03:22:51,887 DEBUG TRAIN Batch 12/16900 loss 38.066105 loss_att 42.116901 loss_ctc 54.053558 loss_rnnt 34.958267 hw_loss 0.311287 lr 0.00047599 rank 3
2023-02-21 03:23:49,267 DEBUG TRAIN Batch 12/17000 loss 23.947693 loss_att 25.740265 loss_ctc 31.122700 loss_rnnt 22.403425 hw_loss 0.429533 lr 0.00047598 rank 0
2023-02-21 03:23:49,273 DEBUG TRAIN Batch 12/17000 loss 6.493259 loss_att 8.200067 loss_ctc 8.404385 loss_rnnt 5.798753 hw_loss 0.184366 lr 0.00047588 rank 6
2023-02-21 03:23:49,277 DEBUG TRAIN Batch 12/17000 loss 16.569674 loss_att 16.188314 loss_ctc 19.456669 loss_rnnt 16.034300 hw_loss 0.425088 lr 0.00047589 rank 5
2023-02-21 03:23:49,277 DEBUG TRAIN Batch 12/17000 loss 4.240972 loss_att 4.884097 loss_ctc 5.233957 loss_rnnt 3.719734 hw_loss 0.487904 lr 0.00047590 rank 2
2023-02-21 03:23:49,282 DEBUG TRAIN Batch 12/17000 loss 18.462950 loss_att 21.370064 loss_ctc 25.182125 loss_rnnt 16.891636 hw_loss 0.176255 lr 0.00047588 rank 3
2023-02-21 03:23:49,282 DEBUG TRAIN Batch 12/17000 loss 20.966978 loss_att 27.618120 loss_ctc 25.601360 loss_rnnt 19.017931 hw_loss 0.001681 lr 0.00047594 rank 1
2023-02-21 03:23:49,283 DEBUG TRAIN Batch 12/17000 loss 23.834515 loss_att 25.571331 loss_ctc 30.387634 loss_rnnt 22.431910 hw_loss 0.340298 lr 0.00047586 rank 7
2023-02-21 03:23:49,289 DEBUG TRAIN Batch 12/17000 loss 9.062317 loss_att 17.429699 loss_ctc 21.844156 loss_rnnt 5.683234 hw_loss 0.002552 lr 0.00047599 rank 4
2023-02-21 03:24:49,414 DEBUG TRAIN Batch 12/17100 loss 2.222781 loss_att 5.203524 loss_ctc 2.272776 loss_rnnt 1.538890 hw_loss 0.152019 lr 0.00047587 rank 0
2023-02-21 03:24:49,420 DEBUG TRAIN Batch 12/17100 loss 12.555188 loss_att 12.790354 loss_ctc 16.298437 loss_rnnt 11.703737 hw_loss 0.572469 lr 0.00047579 rank 2
2023-02-21 03:24:49,421 DEBUG TRAIN Batch 12/17100 loss 24.006639 loss_att 24.112066 loss_ctc 32.523312 loss_rnnt 22.746655 hw_loss 0.193772 lr 0.00047588 rank 4
2023-02-21 03:24:49,422 DEBUG TRAIN Batch 12/17100 loss 20.293642 loss_att 21.873337 loss_ctc 18.439257 loss_rnnt 20.163345 hw_loss 0.115520 lr 0.00047575 rank 7
2023-02-21 03:24:49,422 DEBUG TRAIN Batch 12/17100 loss 36.809170 loss_att 48.582161 loss_ctc 42.665878 loss_rnnt 33.605587 hw_loss 0.127671 lr 0.00047577 rank 6
2023-02-21 03:24:49,422 DEBUG TRAIN Batch 12/17100 loss 14.520715 loss_att 21.045317 loss_ctc 20.067381 loss_rnnt 12.360563 hw_loss 0.216891 lr 0.00047577 rank 3
2023-02-21 03:24:49,423 DEBUG TRAIN Batch 12/17100 loss 20.913424 loss_att 20.633947 loss_ctc 28.330639 loss_rnnt 19.980303 hw_loss 0.000099 lr 0.00047578 rank 5
2023-02-21 03:24:49,423 DEBUG TRAIN Batch 12/17100 loss 12.524538 loss_att 20.055447 loss_ctc 13.160838 loss_rnnt 10.933471 hw_loss 0.000087 lr 0.00047583 rank 1
2023-02-21 03:25:49,845 DEBUG TRAIN Batch 12/17200 loss 9.865173 loss_att 14.008595 loss_ctc 13.461327 loss_rnnt 8.474279 hw_loss 0.155103 lr 0.00047568 rank 2
2023-02-21 03:25:49,846 DEBUG TRAIN Batch 12/17200 loss 2.275019 loss_att 9.715136 loss_ctc 4.803802 loss_rnnt 0.350165 hw_loss 0.186861 lr 0.00047567 rank 5
2023-02-21 03:25:49,848 DEBUG TRAIN Batch 12/17200 loss 6.606543 loss_att 12.044664 loss_ctc 6.037096 loss_rnnt 5.541007 hw_loss 0.100947 lr 0.00047566 rank 3
2023-02-21 03:25:49,848 DEBUG TRAIN Batch 12/17200 loss 15.353569 loss_att 21.806427 loss_ctc 18.899698 loss_rnnt 13.414654 hw_loss 0.329113 lr 0.00047576 rank 0
2023-02-21 03:25:49,856 DEBUG TRAIN Batch 12/17200 loss 14.688582 loss_att 11.676074 loss_ctc 11.233432 loss_rnnt 15.751720 hw_loss 0.000093 lr 0.00047578 rank 4
2023-02-21 03:25:49,859 DEBUG TRAIN Batch 12/17200 loss 4.752955 loss_att 8.243876 loss_ctc 6.021899 loss_rnnt 3.701993 hw_loss 0.344223 lr 0.00047573 rank 1
2023-02-21 03:25:49,873 DEBUG TRAIN Batch 12/17200 loss 8.019534 loss_att 13.857062 loss_ctc 7.977702 loss_rnnt 6.698083 hw_loss 0.299105 lr 0.00047567 rank 6
2023-02-21 03:25:49,874 DEBUG TRAIN Batch 12/17200 loss 16.750935 loss_att 29.405888 loss_ctc 14.081340 loss_rnnt 14.385105 hw_loss 0.357725 lr 0.00047564 rank 7
2023-02-21 03:27:12,510 DEBUG TRAIN Batch 12/17300 loss 18.453209 loss_att 18.312180 loss_ctc 20.157492 loss_rnnt 18.190208 hw_loss 0.119943 lr 0.00047565 rank 0
2023-02-21 03:27:12,517 DEBUG TRAIN Batch 12/17300 loss 12.872657 loss_att 12.751595 loss_ctc 15.966097 loss_rnnt 12.277586 hw_loss 0.387794 lr 0.00047555 rank 3
2023-02-21 03:27:12,521 DEBUG TRAIN Batch 12/17300 loss 25.600866 loss_att 26.350224 loss_ctc 35.015701 loss_rnnt 24.044849 hw_loss 0.282814 lr 0.00047562 rank 1
2023-02-21 03:27:12,522 DEBUG TRAIN Batch 12/17300 loss 16.782892 loss_att 18.314703 loss_ctc 17.175320 loss_rnnt 16.188583 hw_loss 0.441794 lr 0.00047567 rank 4
2023-02-21 03:27:12,529 DEBUG TRAIN Batch 12/17300 loss 19.693462 loss_att 22.087696 loss_ctc 31.697495 loss_rnnt 17.358221 hw_loss 0.479733 lr 0.00047556 rank 6
2023-02-21 03:27:12,532 DEBUG TRAIN Batch 12/17300 loss 1.289431 loss_att 3.224128 loss_ctc 2.446381 loss_rnnt 0.655856 hw_loss 0.173205 lr 0.00047558 rank 2
2023-02-21 03:27:12,559 DEBUG TRAIN Batch 12/17300 loss 4.979541 loss_att 8.059798 loss_ctc 9.277343 loss_rnnt 3.790395 hw_loss 0.000101 lr 0.00047556 rank 5
2023-02-21 03:27:12,564 DEBUG TRAIN Batch 12/17300 loss 19.036032 loss_att 20.284157 loss_ctc 16.607637 loss_rnnt 19.110130 hw_loss 0.000113 lr 0.00047554 rank 7
2023-02-21 03:28:13,273 DEBUG TRAIN Batch 12/17400 loss 21.079897 loss_att 26.154060 loss_ctc 30.046209 loss_rnnt 18.667130 hw_loss 0.379549 lr 0.00047555 rank 0
2023-02-21 03:28:13,274 DEBUG TRAIN Batch 12/17400 loss 18.610645 loss_att 24.577927 loss_ctc 21.251804 loss_rnnt 16.982584 hw_loss 0.154595 lr 0.00047547 rank 2
2023-02-21 03:28:13,276 DEBUG TRAIN Batch 12/17400 loss 23.752573 loss_att 32.402592 loss_ctc 28.603718 loss_rnnt 21.221975 hw_loss 0.288329 lr 0.00047545 rank 3
2023-02-21 03:28:13,283 DEBUG TRAIN Batch 12/17400 loss 11.331070 loss_att 17.885620 loss_ctc 16.036203 loss_rnnt 9.254218 hw_loss 0.259857 lr 0.00047546 rank 5
2023-02-21 03:28:13,285 DEBUG TRAIN Batch 12/17400 loss 8.369749 loss_att 13.323702 loss_ctc 8.059452 loss_rnnt 7.420024 hw_loss 0.000574 lr 0.00047545 rank 6
2023-02-21 03:28:13,287 DEBUG TRAIN Batch 12/17400 loss 23.082092 loss_att 22.597569 loss_ctc 34.075451 loss_rnnt 21.614599 hw_loss 0.184909 lr 0.00047551 rank 1
2023-02-21 03:28:13,290 DEBUG TRAIN Batch 12/17400 loss 14.779498 loss_att 15.055660 loss_ctc 17.968184 loss_rnnt 14.175868 hw_loss 0.231072 lr 0.00047556 rank 4
2023-02-21 03:28:13,341 DEBUG TRAIN Batch 12/17400 loss 32.452953 loss_att 31.652538 loss_ctc 46.191124 loss_rnnt 30.573822 hw_loss 0.388986 lr 0.00047543 rank 7
2023-02-21 03:29:12,169 DEBUG TRAIN Batch 12/17500 loss 10.208708 loss_att 19.450096 loss_ctc 10.722047 loss_rnnt 8.140544 hw_loss 0.283953 lr 0.00047544 rank 0
2023-02-21 03:29:12,170 DEBUG TRAIN Batch 12/17500 loss 17.212324 loss_att 19.779802 loss_ctc 24.146549 loss_rnnt 15.572758 hw_loss 0.377824 lr 0.00047534 rank 6
2023-02-21 03:29:12,176 DEBUG TRAIN Batch 12/17500 loss 13.649809 loss_att 18.385851 loss_ctc 18.384125 loss_rnnt 11.918602 hw_loss 0.286416 lr 0.00047535 rank 5
2023-02-21 03:29:12,177 DEBUG TRAIN Batch 12/17500 loss 6.795267 loss_att 9.028406 loss_ctc 10.735511 loss_rnnt 5.628592 hw_loss 0.365026 lr 0.00047536 rank 2
2023-02-21 03:29:12,178 DEBUG TRAIN Batch 12/17500 loss 20.496269 loss_att 27.452946 loss_ctc 22.729630 loss_rnnt 18.566113 hw_loss 0.451945 lr 0.00047532 rank 7
2023-02-21 03:29:12,187 DEBUG TRAIN Batch 12/17500 loss 1.644953 loss_att 3.595073 loss_ctc 2.210501 loss_rnnt 1.104074 hw_loss 0.141466 lr 0.00047545 rank 4
2023-02-21 03:29:12,217 DEBUG TRAIN Batch 12/17500 loss 5.316511 loss_att 7.466767 loss_ctc 8.444705 loss_rnnt 4.231494 hw_loss 0.446011 lr 0.00047534 rank 3
2023-02-21 03:29:12,249 DEBUG TRAIN Batch 12/17500 loss 22.434059 loss_att 40.431160 loss_ctc 23.070015 loss_rnnt 18.559921 hw_loss 0.356105 lr 0.00047540 rank 1
2023-02-21 03:30:09,969 DEBUG TRAIN Batch 12/17600 loss 12.289317 loss_att 20.080025 loss_ctc 17.303051 loss_rnnt 9.910701 hw_loss 0.284958 lr 0.00047533 rank 0
2023-02-21 03:30:09,979 DEBUG TRAIN Batch 12/17600 loss 9.234939 loss_att 12.753314 loss_ctc 17.161997 loss_rnnt 7.334119 hw_loss 0.262881 lr 0.00047524 rank 5
2023-02-21 03:30:09,980 DEBUG TRAIN Batch 12/17600 loss 26.225605 loss_att 26.006159 loss_ctc 30.268410 loss_rnnt 25.538467 hw_loss 0.359976 lr 0.00047523 rank 3
2023-02-21 03:30:09,980 DEBUG TRAIN Batch 12/17600 loss 21.710424 loss_att 26.239382 loss_ctc 24.346395 loss_rnnt 20.269819 hw_loss 0.343785 lr 0.00047535 rank 4
2023-02-21 03:30:09,982 DEBUG TRAIN Batch 12/17600 loss 15.660775 loss_att 16.261433 loss_ctc 17.018082 loss_rnnt 15.298321 hw_loss 0.115029 lr 0.00047521 rank 7
2023-02-21 03:30:09,983 DEBUG TRAIN Batch 12/17600 loss 38.709473 loss_att 46.481190 loss_ctc 45.244873 loss_rnnt 36.053444 hw_loss 0.431808 lr 0.00047524 rank 6
2023-02-21 03:30:09,984 DEBUG TRAIN Batch 12/17600 loss 6.747328 loss_att 8.210397 loss_ctc 6.742428 loss_rnnt 6.306402 hw_loss 0.279310 lr 0.00047525 rank 2
2023-02-21 03:30:09,986 DEBUG TRAIN Batch 12/17600 loss 20.725143 loss_att 19.158985 loss_ctc 31.278145 loss_rnnt 19.476479 hw_loss 0.290303 lr 0.00047530 rank 1
2023-02-21 03:31:11,871 DEBUG TRAIN Batch 12/17700 loss 11.019418 loss_att 13.303945 loss_ctc 12.574411 loss_rnnt 10.168156 hw_loss 0.350667 lr 0.00047522 rank 0
2023-02-21 03:31:11,878 DEBUG TRAIN Batch 12/17700 loss 14.737164 loss_att 16.782669 loss_ctc 17.124336 loss_rnnt 13.924643 hw_loss 0.159620 lr 0.00047513 rank 3
2023-02-21 03:31:11,881 DEBUG TRAIN Batch 12/17700 loss 10.338462 loss_att 10.799685 loss_ctc 15.298176 loss_rnnt 9.377150 hw_loss 0.389572 lr 0.00047524 rank 4
2023-02-21 03:31:11,882 DEBUG TRAIN Batch 12/17700 loss 3.228031 loss_att 7.106002 loss_ctc 5.198826 loss_rnnt 2.188556 hw_loss 0.002077 lr 0.00047513 rank 6
2023-02-21 03:31:11,885 DEBUG TRAIN Batch 12/17700 loss 16.064657 loss_att 17.579203 loss_ctc 19.927742 loss_rnnt 15.084218 hw_loss 0.304597 lr 0.00047515 rank 2
2023-02-21 03:31:11,886 DEBUG TRAIN Batch 12/17700 loss 21.726889 loss_att 21.295687 loss_ctc 26.511116 loss_rnnt 21.040655 hw_loss 0.252332 lr 0.00047511 rank 7
2023-02-21 03:31:11,889 DEBUG TRAIN Batch 12/17700 loss 24.515039 loss_att 24.567326 loss_ctc 23.920326 loss_rnnt 24.363754 hw_loss 0.412729 lr 0.00047513 rank 5
2023-02-21 03:31:11,948 DEBUG TRAIN Batch 12/17700 loss 13.195822 loss_att 21.109423 loss_ctc 15.618430 loss_rnnt 11.054842 hw_loss 0.441083 lr 0.00047519 rank 1
2023-02-21 03:32:08,949 DEBUG TRAIN Batch 12/17800 loss 9.099548 loss_att 16.822765 loss_ctc 10.772646 loss_rnnt 7.331760 hw_loss 0.000121 lr 0.00047512 rank 0
2023-02-21 03:32:08,960 DEBUG TRAIN Batch 12/17800 loss 11.464457 loss_att 17.968201 loss_ctc 13.850953 loss_rnnt 9.739779 hw_loss 0.198242 lr 0.00047508 rank 1
2023-02-21 03:32:08,961 DEBUG TRAIN Batch 12/17800 loss 11.713553 loss_att 14.240679 loss_ctc 16.483225 loss_rnnt 10.572087 hw_loss 0.000159 lr 0.00047502 rank 6
2023-02-21 03:32:08,961 DEBUG TRAIN Batch 12/17800 loss 10.515922 loss_att 14.914302 loss_ctc 10.289907 loss_rnnt 9.445115 hw_loss 0.414871 lr 0.00047504 rank 2
2023-02-21 03:32:08,967 DEBUG TRAIN Batch 12/17800 loss 14.006308 loss_att 17.183582 loss_ctc 17.443586 loss_rnnt 12.752980 hw_loss 0.299188 lr 0.00047513 rank 4
2023-02-21 03:32:08,970 DEBUG TRAIN Batch 12/17800 loss 30.656527 loss_att 36.698071 loss_ctc 40.771225 loss_rnnt 27.943321 hw_loss 0.293000 lr 0.00047502 rank 3
2023-02-21 03:32:08,971 DEBUG TRAIN Batch 12/17800 loss 15.456860 loss_att 17.290157 loss_ctc 23.960812 loss_rnnt 13.847713 hw_loss 0.203673 lr 0.00047500 rank 7
2023-02-21 03:32:09,018 DEBUG TRAIN Batch 12/17800 loss 9.080370 loss_att 11.536152 loss_ctc 8.813061 loss_rnnt 8.624775 hw_loss 0.000150 lr 0.00047503 rank 5
2023-02-21 03:33:06,905 DEBUG TRAIN Batch 12/17900 loss 11.980579 loss_att 14.799268 loss_ctc 11.603685 loss_rnnt 11.466943 hw_loss 0.000281 lr 0.00047501 rank 0
2023-02-21 03:33:06,914 DEBUG TRAIN Batch 12/17900 loss 12.400976 loss_att 20.941614 loss_ctc 16.097509 loss_rnnt 10.055055 hw_loss 0.271730 lr 0.00047497 rank 1
2023-02-21 03:33:06,916 DEBUG TRAIN Batch 12/17900 loss 26.332100 loss_att 33.581623 loss_ctc 27.660164 loss_rnnt 24.501234 hw_loss 0.382290 lr 0.00047493 rank 2
2023-02-21 03:33:06,918 DEBUG TRAIN Batch 12/17900 loss 29.346087 loss_att 30.212334 loss_ctc 38.552544 loss_rnnt 27.945133 hw_loss 0.000326 lr 0.00047492 rank 5
2023-02-21 03:33:06,919 DEBUG TRAIN Batch 12/17900 loss 14.029740 loss_att 14.819879 loss_ctc 13.987038 loss_rnnt 13.587144 hw_loss 0.544240 lr 0.00047502 rank 4
2023-02-21 03:33:06,921 DEBUG TRAIN Batch 12/17900 loss 14.752028 loss_att 16.619236 loss_ctc 19.181150 loss_rnnt 13.534794 hw_loss 0.474831 lr 0.00047491 rank 3
2023-02-21 03:33:06,921 DEBUG TRAIN Batch 12/17900 loss 12.630066 loss_att 16.537197 loss_ctc 15.634598 loss_rnnt 11.382323 hw_loss 0.123213 lr 0.00047489 rank 7
2023-02-21 03:33:06,974 DEBUG TRAIN Batch 12/17900 loss 19.224920 loss_att 20.867104 loss_ctc 24.056232 loss_rnnt 18.064171 hw_loss 0.352760 lr 0.00047491 rank 6
2023-02-21 03:34:07,392 DEBUG TRAIN Batch 12/18000 loss 12.849559 loss_att 11.000551 loss_ctc 16.659134 loss_rnnt 12.711174 hw_loss 0.000455 lr 0.00047490 rank 0
2023-02-21 03:34:07,401 DEBUG TRAIN Batch 12/18000 loss 15.709924 loss_att 22.969488 loss_ctc 19.532595 loss_rnnt 13.747977 hw_loss 0.000645 lr 0.00047483 rank 2
2023-02-21 03:34:07,401 DEBUG TRAIN Batch 12/18000 loss 9.480385 loss_att 15.346245 loss_ctc 8.233379 loss_rnnt 8.277798 hw_loss 0.366904 lr 0.00047481 rank 5
2023-02-21 03:34:07,401 DEBUG TRAIN Batch 12/18000 loss 51.391560 loss_att 69.005875 loss_ctc 69.603806 loss_rnnt 45.304386 hw_loss 0.255019 lr 0.00047481 rank 6
2023-02-21 03:34:07,402 DEBUG TRAIN Batch 12/18000 loss 24.494751 loss_att 27.547684 loss_ctc 34.815609 loss_rnnt 22.359406 hw_loss 0.278704 lr 0.00047480 rank 3
2023-02-21 03:34:07,402 DEBUG TRAIN Batch 12/18000 loss 29.304476 loss_att 39.148922 loss_ctc 28.918884 loss_rnnt 27.233400 hw_loss 0.287993 lr 0.00047487 rank 1
2023-02-21 03:34:07,404 DEBUG TRAIN Batch 12/18000 loss 16.700254 loss_att 22.017574 loss_ctc 21.070711 loss_rnnt 14.895649 hw_loss 0.297024 lr 0.00047492 rank 4
2023-02-21 03:34:07,406 DEBUG TRAIN Batch 12/18000 loss 4.982451 loss_att 6.989649 loss_ctc 10.420255 loss_rnnt 3.626007 hw_loss 0.431183 lr 0.00047478 rank 7
2023-02-21 03:35:27,478 DEBUG TRAIN Batch 12/18100 loss 11.213555 loss_att 15.663759 loss_ctc 12.672386 loss_rnnt 10.024318 hw_loss 0.196287 lr 0.00047480 rank 0
2023-02-21 03:35:27,482 DEBUG TRAIN Batch 12/18100 loss 8.920698 loss_att 11.454704 loss_ctc 9.747526 loss_rnnt 8.118423 hw_loss 0.347310 lr 0.00047472 rank 2
2023-02-21 03:35:27,484 DEBUG TRAIN Batch 12/18100 loss 14.621621 loss_att 21.684824 loss_ctc 13.075845 loss_rnnt 13.221870 hw_loss 0.362275 lr 0.00047470 rank 3
2023-02-21 03:35:27,490 DEBUG TRAIN Batch 12/18100 loss 26.003002 loss_att 22.221989 loss_ctc 29.407743 loss_rnnt 26.305189 hw_loss 0.000095 lr 0.00047470 rank 6
2023-02-21 03:35:27,490 DEBUG TRAIN Batch 12/18100 loss 33.568329 loss_att 29.641268 loss_ctc 33.607082 loss_rnnt 34.187763 hw_loss 0.301516 lr 0.00047471 rank 5
2023-02-21 03:35:27,495 DEBUG TRAIN Batch 12/18100 loss 7.135291 loss_att 10.165572 loss_ctc 13.992273 loss_rnnt 5.400282 hw_loss 0.402539 lr 0.00047481 rank 4
2023-02-21 03:35:27,526 DEBUG TRAIN Batch 12/18100 loss 6.178590 loss_att 14.398371 loss_ctc 7.256434 loss_rnnt 4.234725 hw_loss 0.292868 lr 0.00047476 rank 1
2023-02-21 03:35:27,539 DEBUG TRAIN Batch 12/18100 loss 14.315454 loss_att 14.455173 loss_ctc 17.187244 loss_rnnt 13.736448 hw_loss 0.315296 lr 0.00047468 rank 7
2023-02-21 03:36:26,925 DEBUG TRAIN Batch 12/18200 loss 17.511906 loss_att 19.328705 loss_ctc 16.725422 loss_rnnt 17.029787 hw_loss 0.419291 lr 0.00047459 rank 3
2023-02-21 03:36:26,927 DEBUG TRAIN Batch 12/18200 loss 17.078009 loss_att 20.920227 loss_ctc 24.550272 loss_rnnt 15.235652 hw_loss 0.145523 lr 0.00047459 rank 6
2023-02-21 03:36:26,928 DEBUG TRAIN Batch 12/18200 loss 11.217325 loss_att 13.061306 loss_ctc 15.308756 loss_rnnt 10.121479 hw_loss 0.340362 lr 0.00047469 rank 0
2023-02-21 03:36:26,929 DEBUG TRAIN Batch 12/18200 loss 24.178644 loss_att 27.267347 loss_ctc 28.618221 loss_rnnt 22.789251 hw_loss 0.336957 lr 0.00047461 rank 2
2023-02-21 03:36:26,930 DEBUG TRAIN Batch 12/18200 loss 13.295467 loss_att 18.485966 loss_ctc 17.105881 loss_rnnt 11.660332 hw_loss 0.166837 lr 0.00047470 rank 4
2023-02-21 03:36:26,932 DEBUG TRAIN Batch 12/18200 loss 16.897472 loss_att 16.676714 loss_ctc 15.528740 loss_rnnt 17.124046 hw_loss 0.000143 lr 0.00047457 rank 7
2023-02-21 03:36:26,933 DEBUG TRAIN Batch 12/18200 loss 20.364552 loss_att 25.782993 loss_ctc 21.834904 loss_rnnt 18.971664 hw_loss 0.212161 lr 0.00047460 rank 5
2023-02-21 03:36:26,989 DEBUG TRAIN Batch 12/18200 loss 6.707694 loss_att 10.340343 loss_ctc 10.214748 loss_rnnt 5.371895 hw_loss 0.265616 lr 0.00047465 rank 1
2023-02-21 03:37:27,809 DEBUG TRAIN Batch 12/18300 loss 17.323071 loss_att 18.784498 loss_ctc 25.366011 loss_rnnt 15.807658 hw_loss 0.282627 lr 0.00047458 rank 0
2023-02-21 03:37:27,815 DEBUG TRAIN Batch 12/18300 loss 5.607029 loss_att 7.191319 loss_ctc 8.100074 loss_rnnt 4.702731 hw_loss 0.478191 lr 0.00047450 rank 2
2023-02-21 03:37:27,817 DEBUG TRAIN Batch 12/18300 loss 12.425409 loss_att 14.154687 loss_ctc 13.377772 loss_rnnt 11.952516 hw_loss 0.000105 lr 0.00047448 rank 3
2023-02-21 03:37:27,817 DEBUG TRAIN Batch 12/18300 loss 50.062271 loss_att 47.140793 loss_ctc 50.558132 loss_rnnt 50.346313 hw_loss 0.439012 lr 0.00047449 rank 5
2023-02-21 03:37:27,818 DEBUG TRAIN Batch 12/18300 loss 20.879169 loss_att 17.184420 loss_ctc 22.748018 loss_rnnt 21.210091 hw_loss 0.297843 lr 0.00047455 rank 1
2023-02-21 03:37:27,820 DEBUG TRAIN Batch 12/18300 loss 15.577688 loss_att 14.942873 loss_ctc 13.440813 loss_rnnt 15.929386 hw_loss 0.112840 lr 0.00047460 rank 4
2023-02-21 03:37:27,822 DEBUG TRAIN Batch 12/18300 loss 14.864520 loss_att 20.338923 loss_ctc 18.711937 loss_rnnt 13.074715 hw_loss 0.341128 lr 0.00047449 rank 6
2023-02-21 03:37:27,827 DEBUG TRAIN Batch 12/18300 loss 31.663755 loss_att 27.638067 loss_ctc 35.861908 loss_rnnt 31.783951 hw_loss 0.234726 lr 0.00047446 rank 7
2023-02-21 03:38:25,717 DEBUG TRAIN Batch 12/18400 loss 2.579707 loss_att 6.822584 loss_ctc 4.296773 loss_rnnt 1.297737 hw_loss 0.383348 lr 0.00047447 rank 0
2023-02-21 03:38:25,725 DEBUG TRAIN Batch 12/18400 loss 20.900072 loss_att 20.923475 loss_ctc 24.239447 loss_rnnt 20.299711 hw_loss 0.282060 lr 0.00047438 rank 6
2023-02-21 03:38:25,725 DEBUG TRAIN Batch 12/18400 loss 8.280109 loss_att 14.906219 loss_ctc 16.864239 loss_rnnt 5.810012 hw_loss 0.000610 lr 0.00047449 rank 4
2023-02-21 03:38:25,727 DEBUG TRAIN Batch 12/18400 loss 4.247855 loss_att 12.689854 loss_ctc 4.274010 loss_rnnt 2.555741 hw_loss 0.000424 lr 0.00047438 rank 5
2023-02-21 03:38:25,728 DEBUG TRAIN Batch 12/18400 loss 10.997612 loss_att 10.375107 loss_ctc 12.403708 loss_rnnt 10.745324 hw_loss 0.354955 lr 0.00047438 rank 3
2023-02-21 03:38:25,728 DEBUG TRAIN Batch 12/18400 loss 12.206556 loss_att 12.689470 loss_ctc 13.890320 loss_rnnt 11.642114 hw_loss 0.456295 lr 0.00047440 rank 2
2023-02-21 03:38:25,729 DEBUG TRAIN Batch 12/18400 loss 27.746994 loss_att 27.696468 loss_ctc 31.548466 loss_rnnt 27.117702 hw_loss 0.248505 lr 0.00047436 rank 7
2023-02-21 03:38:25,729 DEBUG TRAIN Batch 12/18400 loss 24.285007 loss_att 29.263248 loss_ctc 28.023962 loss_rnnt 22.433929 hw_loss 0.669192 lr 0.00047444 rank 1
2023-02-21 03:39:23,142 DEBUG TRAIN Batch 12/18500 loss 14.083983 loss_att 16.455288 loss_ctc 17.694525 loss_rnnt 13.022718 hw_loss 0.198001 lr 0.00047433 rank 1
2023-02-21 03:39:23,146 DEBUG TRAIN Batch 12/18500 loss 11.507617 loss_att 12.710629 loss_ctc 15.074978 loss_rnnt 10.628938 hw_loss 0.304554 lr 0.00047437 rank 0
2023-02-21 03:39:23,151 DEBUG TRAIN Batch 12/18500 loss 18.030874 loss_att 21.447559 loss_ctc 21.833811 loss_rnnt 16.745237 hw_loss 0.178580 lr 0.00047429 rank 2
2023-02-21 03:39:23,153 DEBUG TRAIN Batch 12/18500 loss 20.351105 loss_att 23.912483 loss_ctc 28.072844 loss_rnnt 18.544127 hw_loss 0.122137 lr 0.00047428 rank 5
2023-02-21 03:39:23,153 DEBUG TRAIN Batch 12/18500 loss 28.648479 loss_att 26.839436 loss_ctc 35.949524 loss_rnnt 27.915802 hw_loss 0.226903 lr 0.00047427 rank 6
2023-02-21 03:39:23,158 DEBUG TRAIN Batch 12/18500 loss 7.738676 loss_att 14.145582 loss_ctc 9.117606 loss_rnnt 6.207830 hw_loss 0.123011 lr 0.00047425 rank 7
2023-02-21 03:39:23,162 DEBUG TRAIN Batch 12/18500 loss 14.912484 loss_att 17.012985 loss_ctc 16.198095 loss_rnnt 14.320932 hw_loss 0.000070 lr 0.00047427 rank 3
2023-02-21 03:39:23,171 DEBUG TRAIN Batch 12/18500 loss 17.855040 loss_att 22.251469 loss_ctc 29.811962 loss_rnnt 15.328579 hw_loss 0.099219 lr 0.00047438 rank 4
2023-02-21 03:40:22,984 DEBUG TRAIN Batch 12/18600 loss 41.626270 loss_att 46.936607 loss_ctc 61.299343 loss_rnnt 37.731007 hw_loss 0.393967 lr 0.00047426 rank 0
2023-02-21 03:40:22,990 DEBUG TRAIN Batch 12/18600 loss 14.207772 loss_att 21.357813 loss_ctc 10.064053 loss_rnnt 13.081032 hw_loss 0.467302 lr 0.00047418 rank 2
2023-02-21 03:40:22,991 DEBUG TRAIN Batch 12/18600 loss 1.048293 loss_att 3.800476 loss_ctc 0.410635 loss_rnnt 0.403354 hw_loss 0.336606 lr 0.00047423 rank 1
2023-02-21 03:40:22,993 DEBUG TRAIN Batch 12/18600 loss 4.274246 loss_att 6.641747 loss_ctc 3.968905 loss_rnnt 3.841419 hw_loss 0.000074 lr 0.00047428 rank 4
2023-02-21 03:40:22,994 DEBUG TRAIN Batch 12/18600 loss 31.580160 loss_att 34.657623 loss_ctc 38.266079 loss_rnnt 29.984482 hw_loss 0.166365 lr 0.00047416 rank 3
2023-02-21 03:40:22,998 DEBUG TRAIN Batch 12/18600 loss 25.366526 loss_att 30.433823 loss_ctc 26.259750 loss_rnnt 24.017593 hw_loss 0.405705 lr 0.00047417 rank 6
2023-02-21 03:40:23,000 DEBUG TRAIN Batch 12/18600 loss 24.210609 loss_att 34.274445 loss_ctc 30.264828 loss_rnnt 21.333834 hw_loss 0.106460 lr 0.00047414 rank 7
2023-02-21 03:40:23,009 DEBUG TRAIN Batch 12/18600 loss 9.166625 loss_att 8.662168 loss_ctc 7.984810 loss_rnnt 9.131480 hw_loss 0.550523 lr 0.00047417 rank 5
2023-02-21 03:41:20,290 DEBUG TRAIN Batch 12/18700 loss 16.204557 loss_att 17.855667 loss_ctc 20.622433 loss_rnnt 15.088184 hw_loss 0.369563 lr 0.00047415 rank 0
2023-02-21 03:41:20,296 DEBUG TRAIN Batch 12/18700 loss 14.146985 loss_att 15.043212 loss_ctc 16.672680 loss_rnnt 13.534447 hw_loss 0.181000 lr 0.00047408 rank 2
2023-02-21 03:41:20,301 DEBUG TRAIN Batch 12/18700 loss 13.808221 loss_att 14.883295 loss_ctc 15.415985 loss_rnnt 13.262655 hw_loss 0.217844 lr 0.00047406 rank 3
2023-02-21 03:41:20,303 DEBUG TRAIN Batch 12/18700 loss 13.165533 loss_att 15.595751 loss_ctc 18.936590 loss_rnnt 11.717670 hw_loss 0.360646 lr 0.00047406 rank 5
2023-02-21 03:41:20,303 DEBUG TRAIN Batch 12/18700 loss 23.013643 loss_att 23.793085 loss_ctc 28.096270 loss_rnnt 22.122858 hw_loss 0.107275 lr 0.00047417 rank 4
2023-02-21 03:41:20,310 DEBUG TRAIN Batch 12/18700 loss 27.621683 loss_att 26.181955 loss_ctc 31.265417 loss_rnnt 27.198887 hw_loss 0.421707 lr 0.00047406 rank 6
2023-02-21 03:41:20,312 DEBUG TRAIN Batch 12/18700 loss 13.015557 loss_att 18.343273 loss_ctc 14.926058 loss_rnnt 11.626205 hw_loss 0.129516 lr 0.00047404 rank 7
2023-02-21 03:41:20,365 DEBUG TRAIN Batch 12/18700 loss 19.170038 loss_att 18.746010 loss_ctc 23.269644 loss_rnnt 18.460989 hw_loss 0.463575 lr 0.00047412 rank 1
2023-02-21 03:42:20,188 DEBUG TRAIN Batch 12/18800 loss 21.764709 loss_att 24.724205 loss_ctc 25.566610 loss_rnnt 20.514736 hw_loss 0.283412 lr 0.00047405 rank 0
2023-02-21 03:42:20,194 DEBUG TRAIN Batch 12/18800 loss 16.272884 loss_att 18.006140 loss_ctc 18.722477 loss_rnnt 15.382332 hw_loss 0.407417 lr 0.00047395 rank 3
2023-02-21 03:42:20,196 DEBUG TRAIN Batch 12/18800 loss 10.525070 loss_att 12.347388 loss_ctc 12.276281 loss_rnnt 9.723755 hw_loss 0.381295 lr 0.00047397 rank 2
2023-02-21 03:42:20,197 DEBUG TRAIN Batch 12/18800 loss 8.596338 loss_att 12.824024 loss_ctc 15.601702 loss_rnnt 6.580783 hw_loss 0.442443 lr 0.00047395 rank 6
2023-02-21 03:42:20,198 DEBUG TRAIN Batch 12/18800 loss 22.505026 loss_att 23.751225 loss_ctc 26.560394 loss_rnnt 21.546818 hw_loss 0.315469 lr 0.00047396 rank 5
2023-02-21 03:42:20,199 DEBUG TRAIN Batch 12/18800 loss 19.324492 loss_att 18.470842 loss_ctc 21.946295 loss_rnnt 18.943155 hw_loss 0.379672 lr 0.00047406 rank 4
2023-02-21 03:42:20,250 DEBUG TRAIN Batch 12/18800 loss 13.670261 loss_att 16.060223 loss_ctc 15.350557 loss_rnnt 12.921543 hw_loss 0.087534 lr 0.00047401 rank 1
2023-02-21 03:42:20,292 DEBUG TRAIN Batch 12/18800 loss 4.579922 loss_att 8.559231 loss_ctc 5.417739 loss_rnnt 3.465164 hw_loss 0.388475 lr 0.00047393 rank 7
2023-02-21 03:43:21,087 DEBUG TRAIN Batch 12/18900 loss 20.652239 loss_att 21.628555 loss_ctc 30.901184 loss_rnnt 18.923645 hw_loss 0.312759 lr 0.00047394 rank 0
2023-02-21 03:43:21,098 DEBUG TRAIN Batch 12/18900 loss 16.574373 loss_att 13.436566 loss_ctc 16.984354 loss_rnnt 17.007187 hw_loss 0.262657 lr 0.00047386 rank 2
2023-02-21 03:43:21,100 DEBUG TRAIN Batch 12/18900 loss 14.212599 loss_att 21.645483 loss_ctc 22.029346 loss_rnnt 11.561360 hw_loss 0.229554 lr 0.00047384 rank 3
2023-02-21 03:43:21,099 DEBUG TRAIN Batch 12/18900 loss 10.941764 loss_att 15.311348 loss_ctc 12.420535 loss_rnnt 9.815338 hw_loss 0.103759 lr 0.00047385 rank 5
2023-02-21 03:43:21,101 DEBUG TRAIN Batch 12/18900 loss 3.553668 loss_att 5.648949 loss_ctc 1.893133 loss_rnnt 3.355883 hw_loss 0.000251 lr 0.00047391 rank 1
2023-02-21 03:43:21,101 DEBUG TRAIN Batch 12/18900 loss 11.103843 loss_att 15.466007 loss_ctc 11.450013 loss_rnnt 10.072294 hw_loss 0.211799 lr 0.00047385 rank 6
2023-02-21 03:43:21,109 DEBUG TRAIN Batch 12/18900 loss 31.410702 loss_att 37.868477 loss_ctc 39.691841 loss_rnnt 29.014462 hw_loss 0.001002 lr 0.00047382 rank 7
2023-02-21 03:43:21,111 DEBUG TRAIN Batch 12/18900 loss 48.970253 loss_att 51.839783 loss_ctc 44.516010 loss_rnnt 48.644039 hw_loss 0.649138 lr 0.00047396 rank 4
2023-02-21 03:44:43,023 DEBUG TRAIN Batch 12/19000 loss 25.327360 loss_att 26.069557 loss_ctc 27.467039 loss_rnnt 24.710484 hw_loss 0.343396 lr 0.00047383 rank 0
2023-02-21 03:44:43,032 DEBUG TRAIN Batch 12/19000 loss 26.431240 loss_att 33.304565 loss_ctc 30.063843 loss_rnnt 24.402294 hw_loss 0.318629 lr 0.00047376 rank 2
2023-02-21 03:44:43,039 DEBUG TRAIN Batch 12/19000 loss 8.722038 loss_att 9.430773 loss_ctc 10.094395 loss_rnnt 8.396348 hw_loss 0.001806 lr 0.00047374 rank 3
2023-02-21 03:44:43,040 DEBUG TRAIN Batch 12/19000 loss 19.977112 loss_att 24.843714 loss_ctc 23.498180 loss_rnnt 18.366138 hw_loss 0.315334 lr 0.00047374 rank 6
2023-02-21 03:44:43,042 DEBUG TRAIN Batch 12/19000 loss 12.715186 loss_att 12.708604 loss_ctc 17.976517 loss_rnnt 11.894331 hw_loss 0.226241 lr 0.00047375 rank 5
2023-02-21 03:44:43,046 DEBUG TRAIN Batch 12/19000 loss 6.482065 loss_att 7.650196 loss_ctc 9.369130 loss_rnnt 5.672407 hw_loss 0.358292 lr 0.00047385 rank 4
2023-02-21 03:44:43,046 DEBUG TRAIN Batch 12/19000 loss 9.877724 loss_att 10.669030 loss_ctc 11.507845 loss_rnnt 9.391716 hw_loss 0.206996 lr 0.00047372 rank 7
2023-02-21 03:44:43,103 DEBUG TRAIN Batch 12/19000 loss 17.821747 loss_att 20.259909 loss_ctc 18.208700 loss_rnnt 17.098499 hw_loss 0.345037 lr 0.00047380 rank 1
2023-02-21 03:45:42,374 DEBUG TRAIN Batch 12/19100 loss 17.041222 loss_att 24.879076 loss_ctc 24.774166 loss_rnnt 14.357081 hw_loss 0.160330 lr 0.00047373 rank 0
2023-02-21 03:45:42,382 DEBUG TRAIN Batch 12/19100 loss 16.095875 loss_att 18.999624 loss_ctc 21.390694 loss_rnnt 14.694944 hw_loss 0.214135 lr 0.00047363 rank 3
2023-02-21 03:45:42,384 DEBUG TRAIN Batch 12/19100 loss 13.490442 loss_att 18.136890 loss_ctc 20.443724 loss_rnnt 11.559544 hw_loss 0.139696 lr 0.00047363 rank 6
2023-02-21 03:45:42,390 DEBUG TRAIN Batch 12/19100 loss 24.258381 loss_att 22.922535 loss_ctc 33.523811 loss_rnnt 23.141762 hw_loss 0.278243 lr 0.00047364 rank 5
2023-02-21 03:45:42,391 DEBUG TRAIN Batch 12/19100 loss 9.376824 loss_att 12.423301 loss_ctc 13.903800 loss_rnnt 8.090468 hw_loss 0.137746 lr 0.00047365 rank 2
2023-02-21 03:45:42,393 DEBUG TRAIN Batch 12/19100 loss 3.305784 loss_att 7.114840 loss_ctc 4.374233 loss_rnnt 2.198533 hw_loss 0.380589 lr 0.00047361 rank 7
2023-02-21 03:45:42,399 DEBUG TRAIN Batch 12/19100 loss 8.959869 loss_att 12.120699 loss_ctc 10.720942 loss_rnnt 7.960176 hw_loss 0.248845 lr 0.00047374 rank 4
2023-02-21 03:45:42,452 DEBUG TRAIN Batch 12/19100 loss 14.116465 loss_att 18.117188 loss_ctc 17.934086 loss_rnnt 12.682907 hw_loss 0.233246 lr 0.00047369 rank 1
2023-02-21 03:46:42,161 DEBUG TRAIN Batch 12/19200 loss 2.704252 loss_att 8.013111 loss_ctc 8.044586 loss_rnnt 0.744654 hw_loss 0.348340 lr 0.00047362 rank 0
2023-02-21 03:46:42,166 DEBUG TRAIN Batch 12/19200 loss 33.436180 loss_att 47.061882 loss_ctc 55.033993 loss_rnnt 27.752476 hw_loss 0.147849 lr 0.00047353 rank 5
2023-02-21 03:46:42,168 DEBUG TRAIN Batch 12/19200 loss 3.382686 loss_att 7.450866 loss_ctc 6.135708 loss_rnnt 2.201927 hw_loss 0.000101 lr 0.00047353 rank 6
2023-02-21 03:46:42,169 DEBUG TRAIN Batch 12/19200 loss 44.307373 loss_att 51.130939 loss_ctc 45.944279 loss_rnnt 42.454998 hw_loss 0.505143 lr 0.00047355 rank 2
2023-02-21 03:46:42,173 DEBUG TRAIN Batch 12/19200 loss 3.518278 loss_att 10.804342 loss_ctc 5.027079 loss_rnnt 1.762799 hw_loss 0.182050 lr 0.00047352 rank 3
2023-02-21 03:46:42,175 DEBUG TRAIN Batch 12/19200 loss 5.200067 loss_att 8.426987 loss_ctc 3.581392 loss_rnnt 4.770442 hw_loss 0.000120 lr 0.00047351 rank 7
2023-02-21 03:46:42,181 DEBUG TRAIN Batch 12/19200 loss 10.475423 loss_att 19.213871 loss_ctc 14.146194 loss_rnnt 8.173418 hw_loss 0.121649 lr 0.00047364 rank 4
2023-02-21 03:46:42,233 DEBUG TRAIN Batch 12/19200 loss 9.004394 loss_att 12.213551 loss_ctc 10.249956 loss_rnnt 8.012773 hw_loss 0.344466 lr 0.00047359 rank 1
2023-02-21 03:47:40,129 DEBUG TRAIN Batch 12/19300 loss 9.178760 loss_att 11.759231 loss_ctc 11.875959 loss_rnnt 8.175592 hw_loss 0.238962 lr 0.00047352 rank 0
2023-02-21 03:47:40,139 DEBUG TRAIN Batch 12/19300 loss 11.800635 loss_att 11.733887 loss_ctc 14.838550 loss_rnnt 11.128230 hw_loss 0.526311 lr 0.00047353 rank 4
2023-02-21 03:47:40,143 DEBUG TRAIN Batch 12/19300 loss 9.804746 loss_att 11.789419 loss_ctc 12.080898 loss_rnnt 8.888844 hw_loss 0.404024 lr 0.00047342 rank 3
2023-02-21 03:47:40,143 DEBUG TRAIN Batch 12/19300 loss 10.571617 loss_att 12.182051 loss_ctc 13.808405 loss_rnnt 9.622860 hw_loss 0.365810 lr 0.00047348 rank 1
2023-02-21 03:47:40,144 DEBUG TRAIN Batch 12/19300 loss 15.723712 loss_att 20.594803 loss_ctc 25.168114 loss_rnnt 13.263265 hw_loss 0.425579 lr 0.00047344 rank 2
2023-02-21 03:47:40,145 DEBUG TRAIN Batch 12/19300 loss 18.332571 loss_att 22.223677 loss_ctc 18.045425 loss_rnnt 17.536051 hw_loss 0.106095 lr 0.00047342 rank 6
2023-02-21 03:47:40,148 DEBUG TRAIN Batch 12/19300 loss 15.778252 loss_att 21.992926 loss_ctc 20.935236 loss_rnnt 13.702305 hw_loss 0.272652 lr 0.00047340 rank 7
2023-02-21 03:47:40,152 DEBUG TRAIN Batch 12/19300 loss 12.880423 loss_att 14.352988 loss_ctc 15.969584 loss_rnnt 11.998466 hw_loss 0.329167 lr 0.00047343 rank 5
2023-02-21 03:48:40,204 DEBUG TRAIN Batch 12/19400 loss 14.342302 loss_att 18.291605 loss_ctc 17.229050 loss_rnnt 13.167492 hw_loss 0.000093 lr 0.00047341 rank 0
2023-02-21 03:48:40,214 DEBUG TRAIN Batch 12/19400 loss 10.405238 loss_att 14.177567 loss_ctc 15.252612 loss_rnnt 8.918313 hw_loss 0.161516 lr 0.00047333 rank 2
2023-02-21 03:48:40,215 DEBUG TRAIN Batch 12/19400 loss 7.753196 loss_att 9.717224 loss_ctc 11.383062 loss_rnnt 6.721399 hw_loss 0.290641 lr 0.00047338 rank 1
2023-02-21 03:48:40,216 DEBUG TRAIN Batch 12/19400 loss 9.752536 loss_att 11.411325 loss_ctc 14.486116 loss_rnnt 8.581896 hw_loss 0.389510 lr 0.00047329 rank 7
2023-02-21 03:48:40,219 DEBUG TRAIN Batch 12/19400 loss 11.159209 loss_att 15.178708 loss_ctc 15.818182 loss_rnnt 9.594501 hw_loss 0.261771 lr 0.00047331 rank 3
2023-02-21 03:48:40,220 DEBUG TRAIN Batch 12/19400 loss 23.683035 loss_att 26.768124 loss_ctc 23.985188 loss_rnnt 22.901150 hw_loss 0.233588 lr 0.00047332 rank 6
2023-02-21 03:48:40,220 DEBUG TRAIN Batch 12/19400 loss 23.955687 loss_att 25.388117 loss_ctc 24.708929 loss_rnnt 23.457272 hw_loss 0.209056 lr 0.00047342 rank 4
2023-02-21 03:48:40,275 DEBUG TRAIN Batch 12/19400 loss 12.100786 loss_att 11.456421 loss_ctc 14.621979 loss_rnnt 11.672935 hw_loss 0.413560 lr 0.00047332 rank 5
2023-02-21 03:49:38,106 DEBUG TRAIN Batch 12/19500 loss 16.469986 loss_att 17.538015 loss_ctc 19.426645 loss_rnnt 15.650096 hw_loss 0.397621 lr 0.00047330 rank 0
2023-02-21 03:49:38,117 DEBUG TRAIN Batch 12/19500 loss 23.320524 loss_att 22.352480 loss_ctc 24.292824 loss_rnnt 23.383606 hw_loss 0.001662 lr 0.00047323 rank 2
2023-02-21 03:49:38,118 DEBUG TRAIN Batch 12/19500 loss 13.227272 loss_att 17.678463 loss_ctc 14.350870 loss_rnnt 12.186007 hw_loss 0.002275 lr 0.00047321 rank 5
2023-02-21 03:49:38,123 DEBUG TRAIN Batch 12/19500 loss 26.628138 loss_att 24.992222 loss_ctc 35.836979 loss_rnnt 25.508720 hw_loss 0.410166 lr 0.00047321 rank 6
2023-02-21 03:49:38,124 DEBUG TRAIN Batch 12/19500 loss 2.277013 loss_att 5.308758 loss_ctc 3.404954 loss_rnnt 1.339126 hw_loss 0.339649 lr 0.00047319 rank 7
2023-02-21 03:49:38,125 DEBUG TRAIN Batch 12/19500 loss 5.673535 loss_att 9.679922 loss_ctc 10.928398 loss_rnnt 4.072542 hw_loss 0.185751 lr 0.00047321 rank 3
2023-02-21 03:49:38,132 DEBUG TRAIN Batch 12/19500 loss 8.159895 loss_att 13.509970 loss_ctc 6.170516 loss_rnnt 7.154145 hw_loss 0.376849 lr 0.00047332 rank 4
2023-02-21 03:49:38,185 DEBUG TRAIN Batch 12/19500 loss 39.998158 loss_att 40.321102 loss_ctc 59.245537 loss_rnnt 37.262833 hw_loss 0.195794 lr 0.00047327 rank 1
2023-02-21 03:50:36,901 DEBUG TRAIN Batch 12/19600 loss 19.544119 loss_att 29.279785 loss_ctc 17.449436 loss_rnnt 17.780973 hw_loss 0.178696 lr 0.00047320 rank 0
2023-02-21 03:50:36,907 DEBUG TRAIN Batch 12/19600 loss 11.764532 loss_att 15.306070 loss_ctc 18.297953 loss_rnnt 9.969579 hw_loss 0.404106 lr 0.00047310 rank 3
2023-02-21 03:50:36,910 DEBUG TRAIN Batch 12/19600 loss 5.749050 loss_att 7.206847 loss_ctc 6.943167 loss_rnnt 5.098221 hw_loss 0.375101 lr 0.00047312 rank 2
2023-02-21 03:50:36,912 DEBUG TRAIN Batch 12/19600 loss 12.455821 loss_att 16.234367 loss_ctc 14.520505 loss_rnnt 11.188710 hw_loss 0.442707 lr 0.00047310 rank 6
2023-02-21 03:50:36,913 DEBUG TRAIN Batch 12/19600 loss 42.459679 loss_att 47.239105 loss_ctc 48.063560 loss_rnnt 40.695427 hw_loss 0.114720 lr 0.00047316 rank 1
2023-02-21 03:50:36,914 DEBUG TRAIN Batch 12/19600 loss 19.081709 loss_att 23.625710 loss_ctc 28.975727 loss_rnnt 16.680946 hw_loss 0.323927 lr 0.00047308 rank 7
2023-02-21 03:50:36,919 DEBUG TRAIN Batch 12/19600 loss 11.236821 loss_att 10.903230 loss_ctc 13.481733 loss_rnnt 10.788867 hw_loss 0.403783 lr 0.00047321 rank 4
2023-02-21 03:50:36,974 DEBUG TRAIN Batch 12/19600 loss 17.146801 loss_att 19.093782 loss_ctc 25.373852 loss_rnnt 15.660404 hw_loss 0.000112 lr 0.00047311 rank 5
2023-02-21 03:51:37,454 DEBUG TRAIN Batch 12/19700 loss 11.129766 loss_att 17.902115 loss_ctc 12.580247 loss_rnnt 9.334074 hw_loss 0.464671 lr 0.00047309 rank 0
2023-02-21 03:51:37,463 DEBUG TRAIN Batch 12/19700 loss 7.748536 loss_att 18.024128 loss_ctc 13.148982 loss_rnnt 4.805590 hw_loss 0.314565 lr 0.00047302 rank 2
2023-02-21 03:51:37,465 DEBUG TRAIN Batch 12/19700 loss 25.044952 loss_att 28.527607 loss_ctc 34.329124 loss_rnnt 22.907476 hw_loss 0.380724 lr 0.00047300 rank 5
2023-02-21 03:51:37,468 DEBUG TRAIN Batch 12/19700 loss 4.241725 loss_att 9.223643 loss_ctc 9.733704 loss_rnnt 2.355338 hw_loss 0.295762 lr 0.00047298 rank 7
2023-02-21 03:51:37,469 DEBUG TRAIN Batch 12/19700 loss 12.215211 loss_att 15.609835 loss_ctc 19.247311 loss_rnnt 10.445825 hw_loss 0.286588 lr 0.00047300 rank 6
2023-02-21 03:51:37,471 DEBUG TRAIN Batch 12/19700 loss 7.309576 loss_att 9.533183 loss_ctc 7.576668 loss_rnnt 6.676859 hw_loss 0.285718 lr 0.00047299 rank 3
2023-02-21 03:51:37,471 DEBUG TRAIN Batch 12/19700 loss 5.980430 loss_att 19.277637 loss_ctc 8.163088 loss_rnnt 3.029847 hw_loss 0.000227 lr 0.00047306 rank 1
2023-02-21 03:51:37,530 DEBUG TRAIN Batch 12/19700 loss 17.310041 loss_att 21.332945 loss_ctc 18.647999 loss_rnnt 16.113533 hw_loss 0.400377 lr 0.00047311 rank 4
2023-02-21 03:52:37,214 DEBUG TRAIN Batch 12/19800 loss 9.794918 loss_att 12.758233 loss_ctc 11.759264 loss_rnnt 8.766950 hw_loss 0.325109 lr 0.00047289 rank 6
2023-02-21 03:52:37,215 DEBUG TRAIN Batch 12/19800 loss 4.381353 loss_att 8.923729 loss_ctc 4.802693 loss_rnnt 3.315554 hw_loss 0.189646 lr 0.00047295 rank 1
2023-02-21 03:52:37,216 DEBUG TRAIN Batch 12/19800 loss 43.672623 loss_att 43.276993 loss_ctc 55.469509 loss_rnnt 42.084267 hw_loss 0.177312 lr 0.00047299 rank 0
2023-02-21 03:52:37,219 DEBUG TRAIN Batch 12/19800 loss 4.967021 loss_att 12.078694 loss_ctc 8.089939 loss_rnnt 2.967300 hw_loss 0.301868 lr 0.00047287 rank 7
2023-02-21 03:52:37,220 DEBUG TRAIN Batch 12/19800 loss 9.622115 loss_att 12.480045 loss_ctc 16.855042 loss_rnnt 7.882712 hw_loss 0.381425 lr 0.00047291 rank 2
2023-02-21 03:52:37,221 DEBUG TRAIN Batch 12/19800 loss 10.594936 loss_att 16.507362 loss_ctc 14.727549 loss_rnnt 8.730476 hw_loss 0.245550 lr 0.00047300 rank 4
2023-02-21 03:52:37,224 DEBUG TRAIN Batch 12/19800 loss 1.838641 loss_att 6.165644 loss_ctc 2.538314 loss_rnnt 0.879823 hw_loss 0.000239 lr 0.00047289 rank 3
2023-02-21 03:52:37,224 DEBUG TRAIN Batch 12/19800 loss 10.485446 loss_att 18.685484 loss_ctc 8.702579 loss_rnnt 9.039608 hw_loss 0.081649 lr 0.00047290 rank 5
2023-02-21 03:53:34,847 DEBUG TRAIN Batch 12/19900 loss 15.707900 loss_att 19.272699 loss_ctc 19.363251 loss_rnnt 14.507428 hw_loss 0.000245 lr 0.00047288 rank 0
2023-02-21 03:53:34,854 DEBUG TRAIN Batch 12/19900 loss 19.219479 loss_att 21.766724 loss_ctc 25.386932 loss_rnnt 17.704124 hw_loss 0.344208 lr 0.00047279 rank 6
2023-02-21 03:53:34,863 DEBUG TRAIN Batch 12/19900 loss 10.364028 loss_att 17.945959 loss_ctc 10.620825 loss_rnnt 8.673544 hw_loss 0.262233 lr 0.00047278 rank 3
2023-02-21 03:53:34,863 DEBUG TRAIN Batch 12/19900 loss 27.962606 loss_att 35.310860 loss_ctc 47.001141 loss_rnnt 23.779976 hw_loss 0.327206 lr 0.00047280 rank 2
2023-02-21 03:53:34,865 DEBUG TRAIN Batch 12/19900 loss 18.136158 loss_att 18.809708 loss_ctc 17.897800 loss_rnnt 17.899189 hw_loss 0.251325 lr 0.00047276 rank 7
2023-02-21 03:53:34,866 DEBUG TRAIN Batch 12/19900 loss 13.571970 loss_att 14.473961 loss_ctc 15.999397 loss_rnnt 12.885652 hw_loss 0.341745 lr 0.00047285 rank 1
2023-02-21 03:53:34,872 DEBUG TRAIN Batch 12/19900 loss 19.161575 loss_att 16.774971 loss_ctc 19.640060 loss_rnnt 19.492004 hw_loss 0.155803 lr 0.00047289 rank 4
2023-02-21 03:53:34,874 DEBUG TRAIN Batch 12/19900 loss 10.657146 loss_att 14.653464 loss_ctc 12.747044 loss_rnnt 9.523305 hw_loss 0.104858 lr 0.00047279 rank 5
2023-02-21 03:54:35,687 DEBUG TRAIN Batch 12/20000 loss 15.151035 loss_att 15.667688 loss_ctc 14.643744 loss_rnnt 14.962692 hw_loss 0.286221 lr 0.00047277 rank 0
2023-02-21 03:54:35,694 DEBUG TRAIN Batch 12/20000 loss 37.745056 loss_att 52.971386 loss_ctc 29.375492 loss_rnnt 35.815170 hw_loss 0.001053 lr 0.00047269 rank 5
2023-02-21 03:54:35,695 DEBUG TRAIN Batch 12/20000 loss 22.970224 loss_att 20.398335 loss_ctc 21.220209 loss_rnnt 23.475391 hw_loss 0.454775 lr 0.00047268 rank 3
2023-02-21 03:54:35,697 DEBUG TRAIN Batch 12/20000 loss 13.773949 loss_att 21.710014 loss_ctc 19.322128 loss_rnnt 11.192338 hw_loss 0.477451 lr 0.00047268 rank 6
2023-02-21 03:54:35,699 DEBUG TRAIN Batch 12/20000 loss 17.963482 loss_att 22.806145 loss_ctc 27.504465 loss_rnnt 15.722178 hw_loss 0.001200 lr 0.00047270 rank 2
2023-02-21 03:54:35,704 DEBUG TRAIN Batch 12/20000 loss 7.326413 loss_att 8.915366 loss_ctc 9.823616 loss_rnnt 6.520741 hw_loss 0.290473 lr 0.00047279 rank 4
2023-02-21 03:54:35,717 DEBUG TRAIN Batch 12/20000 loss 8.606722 loss_att 10.791039 loss_ctc 13.857749 loss_rnnt 7.468593 hw_loss 0.002115 lr 0.00047266 rank 7
2023-02-21 03:54:35,759 DEBUG TRAIN Batch 12/20000 loss 15.625673 loss_att 21.438534 loss_ctc 23.741066 loss_rnnt 13.297707 hw_loss 0.156266 lr 0.00047274 rank 1
2023-02-21 03:55:33,844 DEBUG TRAIN Batch 12/20100 loss 41.667408 loss_att 50.084751 loss_ctc 54.309040 loss_rnnt 38.194099 hw_loss 0.195546 lr 0.00047267 rank 0
2023-02-21 03:55:33,852 DEBUG TRAIN Batch 12/20100 loss 26.675039 loss_att 39.937206 loss_ctc 36.177547 loss_rnnt 22.581558 hw_loss 0.326337 lr 0.00047258 rank 6
2023-02-21 03:55:33,853 DEBUG TRAIN Batch 12/20100 loss 5.514466 loss_att 13.448782 loss_ctc 12.239009 loss_rnnt 2.851123 hw_loss 0.337265 lr 0.00047259 rank 2
2023-02-21 03:55:33,857 DEBUG TRAIN Batch 12/20100 loss 9.916600 loss_att 16.728456 loss_ctc 8.137705 loss_rnnt 8.580652 hw_loss 0.395181 lr 0.00047257 rank 3
2023-02-21 03:55:33,857 DEBUG TRAIN Batch 12/20100 loss 2.419317 loss_att 4.014101 loss_ctc 1.196064 loss_rnnt 2.038040 hw_loss 0.422661 lr 0.00047268 rank 4
2023-02-21 03:55:33,861 DEBUG TRAIN Batch 12/20100 loss 10.682590 loss_att 10.896948 loss_ctc 13.980964 loss_rnnt 10.108294 hw_loss 0.171826 lr 0.00047258 rank 5
2023-02-21 03:55:33,873 DEBUG TRAIN Batch 12/20100 loss 29.444681 loss_att 37.370163 loss_ctc 40.236809 loss_rnnt 26.420586 hw_loss 0.000090 lr 0.00047255 rank 7
2023-02-21 03:55:33,917 DEBUG TRAIN Batch 12/20100 loss 9.571514 loss_att 16.549143 loss_ctc 12.212461 loss_rnnt 7.701686 hw_loss 0.229079 lr 0.00047263 rank 1
2023-02-21 03:56:34,214 DEBUG TRAIN Batch 12/20200 loss 18.336006 loss_att 18.262589 loss_ctc 24.604742 loss_rnnt 17.334824 hw_loss 0.337561 lr 0.00047256 rank 0
2023-02-21 03:56:34,214 DEBUG TRAIN Batch 12/20200 loss 6.516096 loss_att 12.479585 loss_ctc 7.624311 loss_rnnt 4.963010 hw_loss 0.398676 lr 0.00047249 rank 2
2023-02-21 03:56:34,214 DEBUG TRAIN Batch 12/20200 loss 8.604766 loss_att 10.230178 loss_ctc 8.160434 loss_rnnt 8.182003 hw_loss 0.294235 lr 0.00047247 rank 5
2023-02-21 03:56:34,216 DEBUG TRAIN Batch 12/20200 loss 19.098694 loss_att 21.761768 loss_ctc 27.703562 loss_rnnt 17.418324 hw_loss 0.000827 lr 0.00047253 rank 1
2023-02-21 03:56:34,218 DEBUG TRAIN Batch 12/20200 loss 18.109480 loss_att 22.504423 loss_ctc 25.423937 loss_rnnt 16.254787 hw_loss 0.000831 lr 0.00047247 rank 6
2023-02-21 03:56:34,224 DEBUG TRAIN Batch 12/20200 loss 55.413498 loss_att 54.240654 loss_ctc 72.592163 loss_rnnt 53.357277 hw_loss 0.000555 lr 0.00047247 rank 3
2023-02-21 03:56:34,224 DEBUG TRAIN Batch 12/20200 loss 15.511671 loss_att 20.427792 loss_ctc 11.402475 loss_rnnt 15.011837 hw_loss 0.120943 lr 0.00047258 rank 4
2023-02-21 03:57:16,221 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 7
2023-02-21 03:57:16,225 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 4
2023-02-21 03:57:16,226 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 6
2023-02-21 03:57:16,227 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 5
2023-02-21 03:57:16,228 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 2
2023-02-21 03:57:16,232 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 1
2023-02-21 03:57:16,261 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 3
2023-02-21 03:57:16,265 DEBUG CV Batch 12/0 loss 2.346873 loss_att 2.255711 loss_ctc 2.622456 loss_rnnt 2.053319 hw_loss 0.515704 history loss 2.190415 rank 0
2023-02-21 03:57:26,592 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 6
2023-02-21 03:57:26,805 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 3
2023-02-21 03:57:26,807 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 4
2023-02-21 03:57:26,873 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 5
2023-02-21 03:57:27,066 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 0
2023-02-21 03:57:27,093 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 2
2023-02-21 03:57:27,119 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 1
2023-02-21 03:57:27,497 DEBUG CV Batch 12/100 loss 8.200294 loss_att 12.604586 loss_ctc 11.734631 loss_rnnt 6.689008 hw_loss 0.298470 history loss 5.207796 rank 7
2023-02-21 03:57:39,061 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 6
2023-02-21 03:57:39,431 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 3
2023-02-21 03:57:39,560 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 0
2023-02-21 03:57:39,579 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 4
2023-02-21 03:57:39,627 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 2
2023-02-21 03:57:39,627 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 5
2023-02-21 03:57:39,668 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 1
2023-02-21 03:57:40,407 DEBUG CV Batch 12/200 loss 4.931927 loss_att 4.746162 loss_ctc 2.904249 loss_rnnt 5.044907 hw_loss 0.364744 history loss 4.910743 rank 7
2023-02-21 03:57:53,019 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 6
2023-02-21 03:57:53,050 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 3
2023-02-21 03:57:53,117 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 2
2023-02-21 03:57:53,129 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 0
2023-02-21 03:57:53,351 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 4
2023-02-21 03:57:53,452 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 1
2023-02-21 03:57:53,600 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 5
2023-02-21 03:57:54,122 DEBUG CV Batch 12/300 loss 3.539205 loss_att 3.004493 loss_ctc 4.693987 loss_rnnt 3.205904 hw_loss 0.536759 history loss 5.508769 rank 7
2023-02-21 03:58:03,596 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 6
2023-02-21 03:58:03,610 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 3
2023-02-21 03:58:03,628 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 2
2023-02-21 03:58:03,667 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 0
2023-02-21 03:58:04,142 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 4
2023-02-21 03:58:04,314 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 1
2023-02-21 03:58:04,708 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 5
2023-02-21 03:58:05,179 DEBUG CV Batch 12/400 loss 10.911523 loss_att 14.084041 loss_ctc 15.815296 loss_rnnt 9.505033 hw_loss 0.221532 history loss 5.678206 rank 7
2023-02-21 03:58:18,800 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 0
2023-02-21 03:58:18,896 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 2
2023-02-21 03:58:19,077 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 3
2023-02-21 03:58:19,096 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 6
2023-02-21 03:58:19,757 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 4
2023-02-21 03:58:20,390 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 1
2023-02-21 03:58:20,708 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 7
2023-02-21 03:58:20,731 DEBUG CV Batch 12/500 loss 4.240812 loss_att 5.166996 loss_ctc 5.369129 loss_rnnt 3.616247 hw_loss 0.541661 history loss 5.782953 rank 5
2023-02-21 03:58:31,597 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 0
2023-02-21 03:58:31,739 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 2
2023-02-21 03:58:32,072 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 3
2023-02-21 03:58:32,148 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 6
2023-02-21 03:58:32,883 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 4
2023-02-21 03:58:33,790 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 1
2023-02-21 03:58:33,969 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 7
2023-02-21 03:58:34,287 DEBUG CV Batch 12/600 loss 6.076266 loss_att 29.447334 loss_ctc 2.974662 loss_rnnt 1.697724 hw_loss 0.221015 history loss 6.028629 rank 5
2023-02-21 03:58:42,547 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 0
2023-02-21 03:58:43,132 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 2
2023-02-21 03:58:43,191 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 3
2023-02-21 03:58:43,470 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 6
2023-02-21 03:58:44,655 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 4
2023-02-21 03:58:45,824 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 7
2023-02-21 03:58:45,870 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 1
2023-02-21 03:58:46,167 DEBUG CV Batch 12/700 loss 23.874638 loss_att 24.555943 loss_ctc 35.037506 loss_rnnt 22.124241 hw_loss 0.235788 history loss 6.189576 rank 5
2023-02-21 03:58:53,649 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 0
2023-02-21 03:58:54,460 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 2
2023-02-21 03:58:54,587 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 3
2023-02-21 03:58:54,781 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 6
2023-02-21 03:58:56,577 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 4
2023-02-21 03:58:57,493 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 7
2023-02-21 03:58:57,698 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 1
2023-02-21 03:58:57,895 DEBUG CV Batch 12/800 loss 5.559213 loss_att 5.411087 loss_ctc 7.649209 loss_rnnt 5.091702 hw_loss 0.409630 history loss 6.576337 rank 5
2023-02-21 03:59:05,893 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 0
2023-02-21 03:59:06,950 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 2
2023-02-21 03:59:07,164 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 6
2023-02-21 03:59:07,169 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 3
2023-02-21 03:59:09,280 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 4
2023-02-21 03:59:10,047 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 7
2023-02-21 03:59:10,317 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 1
2023-02-21 03:59:11,072 DEBUG CV Batch 12/900 loss 14.228436 loss_att 12.870320 loss_ctc 15.565562 loss_rnnt 14.115626 hw_loss 0.386531 history loss 7.383384 rank 5
2023-02-21 03:59:16,626 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 0
2023-02-21 03:59:18,024 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 2
2023-02-21 03:59:18,042 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 6
2023-02-21 03:59:18,201 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 3
2023-02-21 03:59:20,669 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 4
2023-02-21 03:59:21,216 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 7
2023-02-21 03:59:21,670 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 1
2023-02-21 03:59:22,512 DEBUG CV Batch 12/1000 loss 93.593971 loss_att 76.373749 loss_ctc 116.931160 loss_rnnt 93.926315 hw_loss 0.000159 history loss 8.355620 rank 5
2023-02-21 03:59:27,457 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 0
2023-02-21 03:59:29,245 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 2
2023-02-21 03:59:29,259 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 6
2023-02-21 03:59:29,285 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 3
2023-02-21 03:59:32,684 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 4
2023-02-21 03:59:32,729 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 7
2023-02-21 03:59:32,967 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 1
2023-02-21 03:59:33,903 DEBUG CV Batch 12/1100 loss 0.842568 loss_att 3.360070 loss_ctc 0.293828 loss_rnnt 0.297277 hw_loss 0.215542 history loss 8.309010 rank 5
2023-02-21 03:59:39,535 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 0
2023-02-21 03:59:41,229 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 3
2023-02-21 03:59:41,303 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 2
2023-02-21 03:59:41,741 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 6
2023-02-21 03:59:44,926 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 4
2023-02-21 03:59:45,047 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 1
2023-02-21 03:59:45,134 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 7
2023-02-21 03:59:46,927 DEBUG CV Batch 12/1200 loss 14.179393 loss_att 12.004666 loss_ctc 14.974443 loss_rnnt 14.351114 hw_loss 0.294780 history loss 8.696865 rank 5
2023-02-21 03:59:52,420 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 0
2023-02-21 03:59:54,290 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 3
2023-02-21 03:59:54,560 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 2
2023-02-21 03:59:55,083 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 6
2023-02-21 03:59:58,218 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 4
2023-02-21 03:59:58,242 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 1
2023-02-21 03:59:58,503 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 7
2023-02-21 04:00:00,309 DEBUG CV Batch 12/1300 loss 5.600568 loss_att 5.688173 loss_ctc 6.724266 loss_rnnt 5.209350 hw_loss 0.419756 history loss 9.400765 rank 5
2023-02-21 04:00:02,966 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 0
2023-02-21 04:00:05,098 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 3
2023-02-21 04:00:05,411 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 2
2023-02-21 04:00:05,826 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 6
2023-02-21 04:00:09,324 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 4
2023-02-21 04:00:09,454 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 1
2023-02-21 04:00:09,574 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 7
2023-02-21 04:00:11,399 DEBUG CV Batch 12/1400 loss 135.575073 loss_att 152.516296 loss_ctc 179.950943 loss_rnnt 126.269958 hw_loss 0.000159 history loss 10.092988 rank 5
2023-02-21 04:00:12,845 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 0
2023-02-21 04:00:15,171 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 3
2023-02-21 04:00:15,475 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 2
2023-02-21 04:00:16,144 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 6
2023-02-21 04:00:19,797 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 4
2023-02-21 04:00:19,920 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 1
2023-02-21 04:00:20,056 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 7
2023-02-21 04:00:21,868 DEBUG CV Batch 12/1500 loss 4.001662 loss_att 7.242760 loss_ctc 5.251705 loss_rnnt 3.043624 hw_loss 0.268398 history loss 10.313483 rank 5
2023-02-21 04:00:24,940 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 0
2023-02-21 04:00:27,377 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 3
2023-02-21 04:00:27,482 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 2
2023-02-21 04:00:28,211 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 6
2023-02-21 04:00:32,173 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 1
2023-02-21 04:00:32,579 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 7
2023-02-21 04:00:32,765 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 4
2023-02-21 04:00:34,483 DEBUG CV Batch 12/1600 loss 14.329405 loss_att 14.108494 loss_ctc 19.306194 loss_rnnt 13.495810 hw_loss 0.401637 history loss 10.034333 rank 5
2023-02-21 04:00:37,895 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 0
2023-02-21 04:00:40,262 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 2
2023-02-21 04:00:40,388 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 3
2023-02-21 04:00:41,303 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 6
2023-02-21 04:00:45,329 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 1
2023-02-21 04:00:45,699 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 7
2023-02-21 04:00:45,953 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 4
2023-02-21 04:00:47,783 DEBUG CV Batch 12/1700 loss 4.930051 loss_att 6.216638 loss_ctc 5.806495 loss_rnnt 4.276415 hw_loss 0.523986 history loss 9.765098 rank 5
2023-02-21 04:00:49,423 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 0
2023-02-21 04:00:51,998 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 2
2023-02-21 04:00:52,409 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 3
2023-02-21 04:00:52,971 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 6
2023-02-21 04:00:57,360 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 1
2023-02-21 04:00:57,811 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 7
2023-02-21 04:00:58,066 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 4
2023-02-21 04:00:59,814 DEBUG CV Batch 12/1800 loss 17.549528 loss_att 20.081240 loss_ctc 30.758532 loss_rnnt 15.170125 hw_loss 0.209737 history loss 9.684565 rank 5
2023-02-21 04:01:00,588 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 0
2023-02-21 04:01:03,429 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 2
2023-02-21 04:01:03,867 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 3
2023-02-21 04:01:04,040 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 6
2023-02-21 04:01:09,077 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 1
2023-02-21 04:01:09,546 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 7
2023-02-21 04:01:09,786 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 4
2023-02-21 04:01:11,563 DEBUG CV Batch 12/1900 loss 5.822827 loss_att 4.877775 loss_ctc 6.750643 loss_rnnt 5.707066 hw_loss 0.339493 history loss 9.404363 rank 5
2023-02-21 04:01:16,512 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 0
2023-02-21 04:01:19,178 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 2
2023-02-21 04:01:19,848 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 3
2023-02-21 04:01:19,896 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 6
2023-02-21 04:01:25,099 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 1
2023-02-21 04:01:25,787 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 7
2023-02-21 04:01:25,858 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 4
2023-02-21 04:01:27,425 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 0
2023-02-21 04:01:27,448 DEBUG CV Batch 12/2000 loss 9.091546 loss_att 8.474422 loss_ctc 10.652798 loss_rnnt 8.799733 hw_loss 0.388258 history loss 9.293810 rank 5
2023-02-21 04:01:29,946 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 2
2023-02-21 04:01:30,618 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 6
2023-02-21 04:01:30,813 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 3
2023-02-21 04:01:37,006 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 4
2023-02-21 04:01:37,062 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 7
2023-02-21 04:01:37,277 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 1
2023-02-21 04:01:38,654 DEBUG CV Batch 12/2100 loss 6.219833 loss_att 10.757164 loss_ctc 6.580154 loss_rnnt 5.264240 hw_loss 0.000159 history loss 9.240451 rank 5
2023-02-21 04:01:40,358 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 0
2023-02-21 04:01:42,946 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 2
2023-02-21 04:01:43,617 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 6
2023-02-21 04:01:44,022 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 3
2023-02-21 04:01:50,430 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 7
2023-02-21 04:01:50,515 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 4
2023-02-21 04:01:50,819 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 1
2023-02-21 04:01:52,025 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 0
2023-02-21 04:01:52,343 DEBUG CV Batch 12/2200 loss 3.574054 loss_att 6.639444 loss_ctc 5.261930 loss_rnnt 2.593233 hw_loss 0.267549 history loss 9.107277 rank 5
2023-02-21 04:01:54,652 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 2
2023-02-21 04:01:55,506 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 6
2023-02-21 04:01:55,859 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 3
2023-02-21 04:02:02,375 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 7
2023-02-21 04:02:02,490 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 4
2023-02-21 04:02:02,620 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 1
2023-02-21 04:02:04,292 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 0
2023-02-21 04:02:04,428 DEBUG CV Batch 12/2300 loss 4.685426 loss_att 4.301150 loss_ctc 4.621738 loss_rnnt 4.496575 hw_loss 0.514119 history loss 9.014279 rank 5
2023-02-21 04:02:06,949 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 2
2023-02-21 04:02:07,759 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 6
2023-02-21 04:02:08,153 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 3
2023-02-21 04:02:14,433 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 0
2023-02-21 04:02:15,033 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 7
2023-02-21 04:02:15,209 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 4
2023-02-21 04:02:15,219 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 1
2023-02-21 04:02:16,845 DEBUG CV Batch 12/2400 loss 8.431815 loss_att 6.830621 loss_ctc 10.751392 loss_rnnt 8.096974 hw_loss 0.648379 history loss 9.080261 rank 5
2023-02-21 04:02:17,156 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 2
2023-02-21 04:02:18,025 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 6
2023-02-21 04:02:18,442 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 3
2023-02-21 04:02:25,536 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 7
2023-02-21 04:02:25,720 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 1
2023-02-21 04:02:25,791 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 4
2023-02-21 04:02:25,810 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 0
2023-02-21 04:02:27,524 DEBUG CV Batch 12/2500 loss 28.403437 loss_att 34.187176 loss_ctc 40.456982 loss_rnnt 25.559547 hw_loss 0.150003 history loss 9.451026 rank 5
2023-02-21 04:02:28,621 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 2
2023-02-21 04:02:29,592 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 6
2023-02-21 04:02:29,689 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 3
2023-02-21 04:02:37,230 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 7
2023-02-21 04:02:37,387 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 1
2023-02-21 04:02:37,472 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 4
2023-02-21 04:02:38,551 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 0
2023-02-21 04:02:39,053 DEBUG CV Batch 12/2600 loss 3.635955 loss_att 5.672795 loss_ctc 3.815236 loss_rnnt 2.945047 hw_loss 0.486818 history loss 9.502580 rank 5
2023-02-21 04:02:41,304 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 2
2023-02-21 04:02:42,161 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 6
2023-02-21 04:02:42,455 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 3
2023-02-21 04:02:49,135 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 0
2023-02-21 04:02:50,213 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 7
2023-02-21 04:02:50,290 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 4
2023-02-21 04:02:50,995 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 1
2023-02-21 04:02:51,814 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 2
2023-02-21 04:02:52,059 DEBUG CV Batch 12/2700 loss 17.918495 loss_att 16.440964 loss_ctc 26.223043 loss_rnnt 16.845884 hw_loss 0.489082 history loss 9.558764 rank 5
2023-02-21 04:02:52,610 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 6
2023-02-21 04:02:53,115 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 3
2023-02-21 04:03:00,273 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 0
2023-02-21 04:03:01,108 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 7
2023-02-21 04:03:01,388 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 4
2023-02-21 04:03:01,726 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 1
2023-02-21 04:03:03,081 DEBUG CV Batch 12/2800 loss 21.177238 loss_att 15.847621 loss_ctc 25.016270 loss_rnnt 21.574335 hw_loss 0.294289 history loss 9.812610 rank 5
2023-02-21 04:03:03,137 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 2
2023-02-21 04:03:03,834 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 6
2023-02-21 04:03:04,280 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 3
2023-02-21 04:03:11,543 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 0
2023-02-21 04:03:12,524 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 7
2023-02-21 04:03:13,097 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 4
2023-02-21 04:03:13,493 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 1
2023-02-21 04:03:14,530 DEBUG CV Batch 12/2900 loss 24.890041 loss_att 25.067268 loss_ctc 30.957306 loss_rnnt 23.845959 hw_loss 0.374386 history loss 10.086942 rank 5
2023-02-21 04:03:14,603 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 2
2023-02-21 04:03:15,204 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 6
2023-02-21 04:03:15,553 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 3
2023-02-21 04:03:23,855 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 0
2023-02-21 04:03:23,952 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 7
2023-02-21 04:03:24,832 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 4
2023-02-21 04:03:25,107 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 1
2023-02-21 04:03:26,736 DEBUG CV Batch 12/3000 loss 4.808513 loss_att 5.441480 loss_ctc 6.554460 loss_rnnt 4.157268 hw_loss 0.547235 history loss 10.235934 rank 5
2023-02-21 04:03:27,059 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 2
2023-02-21 04:03:27,735 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 6
2023-02-21 04:03:27,876 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 3
2023-02-21 04:03:36,418 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 7
2023-02-21 04:03:36,555 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 0
2023-02-21 04:03:37,594 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 4
2023-02-21 04:03:37,614 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 1
2023-02-21 04:03:39,386 DEBUG CV Batch 12/3100 loss 7.982773 loss_att 7.373926 loss_ctc 13.158717 loss_rnnt 7.232080 hw_loss 0.341879 history loss 10.237162 rank 5
2023-02-21 04:03:39,818 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 2
2023-02-21 04:03:40,324 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 6
2023-02-21 04:03:40,662 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 3
2023-02-21 04:03:46,930 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 0
2023-02-21 04:03:49,651 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 7
2023-02-21 04:03:50,563 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 4
2023-02-21 04:03:50,624 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 2
2023-02-21 04:03:50,727 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 1
2023-02-21 04:03:51,045 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 6
2023-02-21 04:03:51,420 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 3
2023-02-21 04:03:52,445 DEBUG CV Batch 12/3200 loss 2.322072 loss_att 2.514811 loss_ctc 1.259597 loss_rnnt 2.242154 hw_loss 0.343187 history loss 10.173423 rank 5
2023-02-21 04:03:59,500 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 0
2023-02-21 04:04:00,632 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 7
2023-02-21 04:04:01,518 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 1
2023-02-21 04:04:02,251 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 4
2023-02-21 04:04:03,171 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 2
2023-02-21 04:04:03,517 DEBUG CV Batch 12/3300 loss 5.417719 loss_att 10.668929 loss_ctc 6.647630 loss_rnnt 4.157021 hw_loss 0.087129 history loss 10.097855 rank 5
2023-02-21 04:04:03,927 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 6
2023-02-21 04:04:04,168 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 3
2023-02-21 04:04:14,497 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 1
2023-02-21 04:04:14,700 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 7
2023-02-21 04:04:15,035 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 0
2023-02-21 04:04:15,283 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 4
2023-02-21 04:04:16,198 DEBUG CV Batch 12/3400 loss 7.972264 loss_att 7.828246 loss_ctc 8.413713 loss_rnnt 7.831792 hw_loss 0.207029 history loss 9.986679 rank 5
2023-02-21 04:04:18,562 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 2
2023-02-21 04:04:19,476 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 6
2023-02-21 04:04:19,906 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 3
2023-02-21 04:04:25,587 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 0
2023-02-21 04:04:29,222 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 2
2023-02-21 04:04:30,092 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 6
2023-02-21 04:04:30,680 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 3
2023-02-21 04:04:30,757 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 1
2023-02-21 04:04:30,773 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 7
2023-02-21 04:04:31,265 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 4
2023-02-21 04:04:32,059 DEBUG CV Batch 12/3500 loss 75.494644 loss_att 122.522537 loss_ctc 88.744644 loss_rnnt 64.181549 hw_loss 0.264098 history loss 10.011540 rank 5
2023-02-21 04:04:39,404 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 0
2023-02-21 04:04:41,700 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 1
2023-02-21 04:04:42,121 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 4
2023-02-21 04:04:42,603 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 7
2023-02-21 04:04:42,852 DEBUG CV Batch 12/3600 loss 6.614385 loss_att 18.310844 loss_ctc 4.472626 loss_rnnt 4.396227 hw_loss 0.308310 history loss 9.899165 rank 5
2023-02-21 04:04:42,996 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 2
2023-02-21 04:04:43,985 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 6
2023-02-21 04:04:44,769 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 3
2023-02-21 04:04:50,591 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 0
2023-02-21 04:04:54,258 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 2
2023-02-21 04:04:55,178 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 6
2023-02-21 04:04:55,953 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 3
2023-02-21 04:04:56,025 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 1
2023-02-21 04:04:56,310 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 4
2023-02-21 04:04:56,774 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 7
2023-02-21 04:04:57,054 DEBUG CV Batch 12/3700 loss 8.774673 loss_att 8.503014 loss_ctc 11.044429 loss_rnnt 8.333601 hw_loss 0.361443 history loss 9.846790 rank 5
2023-02-21 04:04:59,345 INFO Epoch 12 CV info cv_loss 9.790112218608366
2023-02-21 04:04:59,345 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt
2023-02-21 04:05:03,066 INFO Epoch 12 CV info cv_loss 9.790112219202776
2023-02-21 04:05:03,066 INFO Epoch 13 TRAIN info lr 0.00047248090341652513
2023-02-21 04:05:03,069 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:04,460 INFO Epoch 12 CV info cv_loss 9.79011222020638
2023-02-21 04:05:04,460 INFO Epoch 13 TRAIN info lr 0.0004724640281945108
2023-02-21 04:05:04,464 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:04,496 INFO Epoch 13 TRAIN info lr 0.000472561085421889
2023-02-21 04:05:04,499 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:04,548 INFO Epoch 12 CV info cv_loss 9.790112219947941
2023-02-21 04:05:04,549 INFO Epoch 13 TRAIN info lr 0.00047244715478052376
2023-02-21 04:05:04,551 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:07,649 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 1
2023-02-21 04:05:07,733 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 4
2023-02-21 04:05:08,365 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 7
2023-02-21 04:05:08,557 DEBUG CV Batch 12/3800 loss 3.724884 loss_att 4.799893 loss_ctc 3.883984 loss_rnnt 3.278374 hw_loss 0.394303 history loss 9.799736 rank 5
2023-02-21 04:05:16,623 INFO Epoch 12 CV info cv_loss 9.790112219047712
2023-02-21 04:05:16,624 INFO Epoch 13 TRAIN info lr 0.0004725125493321245
2023-02-21 04:05:16,628 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:16,954 INFO Epoch 12 CV info cv_loss 9.79011221858683
2023-02-21 04:05:16,955 INFO Epoch 13 TRAIN info lr 0.00047252731960234127
2023-02-21 04:05:16,960 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:17,476 INFO Epoch 12 CV info cv_loss 9.79011221890557
2023-02-21 04:05:17,477 INFO Epoch 13 TRAIN info lr 0.0004724513729645356
2023-02-21 04:05:17,481 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:05:17,584 INFO Epoch 12 CV info cv_loss 9.790112218759122
2023-02-21 04:05:17,584 INFO Epoch 13 TRAIN info lr 0.00047243028317424103
2023-02-21 04:05:17,589 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 04:06:38,465 DEBUG TRAIN Batch 13/0 loss 11.456985 loss_att 11.389501 loss_ctc 13.748978 loss_rnnt 10.954193 hw_loss 0.395043 lr 0.00047245 rank 3
2023-02-21 04:06:38,473 DEBUG TRAIN Batch 13/0 loss 11.533974 loss_att 10.720987 loss_ctc 12.632805 loss_rnnt 11.317082 hw_loss 0.436830 lr 0.00047245 rank 7
2023-02-21 04:06:38,473 DEBUG TRAIN Batch 13/0 loss 10.194885 loss_att 9.960636 loss_ctc 11.616414 loss_rnnt 9.830174 hw_loss 0.416294 lr 0.00047246 rank 6
2023-02-21 04:06:38,476 DEBUG TRAIN Batch 13/0 loss 11.380892 loss_att 10.385787 loss_ctc 12.252028 loss_rnnt 11.254189 hw_loss 0.392950 lr 0.00047256 rank 0
2023-02-21 04:06:38,483 DEBUG TRAIN Batch 13/0 loss 15.693950 loss_att 14.583914 loss_ctc 17.970041 loss_rnnt 15.373849 hw_loss 0.447429 lr 0.00047248 rank 2
2023-02-21 04:06:38,511 DEBUG TRAIN Batch 13/0 loss 11.977064 loss_att 10.644021 loss_ctc 12.863403 loss_rnnt 11.858879 hw_loss 0.499902 lr 0.00047243 rank 5
2023-02-21 04:06:38,517 DEBUG TRAIN Batch 13/0 loss 10.003601 loss_att 10.072613 loss_ctc 11.070436 loss_rnnt 9.653180 hw_loss 0.364452 lr 0.00047253 rank 1
2023-02-21 04:06:38,530 DEBUG TRAIN Batch 13/0 loss 16.318459 loss_att 14.971928 loss_ctc 17.818224 loss_rnnt 16.143341 hw_loss 0.458357 lr 0.00047251 rank 4
2023-02-21 04:07:35,751 DEBUG TRAIN Batch 13/100 loss 8.046227 loss_att 9.919061 loss_ctc 7.799102 loss_rnnt 7.704561 hw_loss 0.000094 lr 0.00047245 rank 0
2023-02-21 04:07:35,756 DEBUG TRAIN Batch 13/100 loss 9.163392 loss_att 11.066354 loss_ctc 8.337478 loss_rnnt 8.765226 hw_loss 0.239428 lr 0.00047236 rank 6
2023-02-21 04:07:35,761 DEBUG TRAIN Batch 13/100 loss 17.205574 loss_att 18.939947 loss_ctc 19.709688 loss_rnnt 16.328930 hw_loss 0.367286 lr 0.00047237 rank 2
2023-02-21 04:07:35,766 DEBUG TRAIN Batch 13/100 loss 4.523389 loss_att 7.255868 loss_ctc 4.602578 loss_rnnt 3.767047 hw_loss 0.373666 lr 0.00047232 rank 5
2023-02-21 04:07:35,766 DEBUG TRAIN Batch 13/100 loss 24.980927 loss_att 28.815979 loss_ctc 35.779137 loss_rnnt 22.641417 hw_loss 0.248887 lr 0.00047234 rank 3
2023-02-21 04:07:35,767 DEBUG TRAIN Batch 13/100 loss 15.831126 loss_att 23.726458 loss_ctc 28.339153 loss_rnnt 12.433454 hw_loss 0.282878 lr 0.00047240 rank 4
2023-02-21 04:07:35,771 DEBUG TRAIN Batch 13/100 loss 16.488293 loss_att 21.055330 loss_ctc 19.587612 loss_rnnt 14.974466 hw_loss 0.350953 lr 0.00047242 rank 1
2023-02-21 04:07:35,773 DEBUG TRAIN Batch 13/100 loss 8.497625 loss_att 13.639814 loss_ctc 9.918576 loss_rnnt 7.181119 hw_loss 0.184888 lr 0.00047234 rank 7
2023-02-21 04:08:34,467 DEBUG TRAIN Batch 13/200 loss 18.517832 loss_att 26.747118 loss_ctc 22.249601 loss_rnnt 16.269089 hw_loss 0.197467 lr 0.00047235 rank 0
2023-02-21 04:08:34,475 DEBUG TRAIN Batch 13/200 loss 10.715865 loss_att 15.630514 loss_ctc 9.520322 loss_rnnt 9.791437 hw_loss 0.189192 lr 0.00047227 rank 2
2023-02-21 04:08:34,476 DEBUG TRAIN Batch 13/200 loss 2.245740 loss_att 8.549547 loss_ctc 1.012778 loss_rnnt 1.134096 hw_loss 0.028647 lr 0.00047223 rank 3
2023-02-21 04:08:34,476 DEBUG TRAIN Batch 13/200 loss 15.554957 loss_att 21.374306 loss_ctc 22.684027 loss_rnnt 13.288726 hw_loss 0.284659 lr 0.00047222 rank 5
2023-02-21 04:08:34,483 DEBUG TRAIN Batch 13/200 loss 9.868223 loss_att 9.403218 loss_ctc 16.548618 loss_rnnt 8.813155 hw_loss 0.482532 lr 0.00047231 rank 1
2023-02-21 04:08:34,485 DEBUG TRAIN Batch 13/200 loss 17.659460 loss_att 14.731220 loss_ctc 20.264463 loss_rnnt 17.755167 hw_loss 0.267387 lr 0.00047224 rank 7
2023-02-21 04:08:34,531 DEBUG TRAIN Batch 13/200 loss 13.478392 loss_att 13.468286 loss_ctc 10.657870 loss_rnnt 13.711285 hw_loss 0.272246 lr 0.00047230 rank 4
2023-02-21 04:08:34,536 DEBUG TRAIN Batch 13/200 loss 23.467419 loss_att 21.919800 loss_ctc 43.167522 loss_rnnt 20.879032 hw_loss 0.508558 lr 0.00047225 rank 6
2023-02-21 04:09:31,994 DEBUG TRAIN Batch 13/300 loss 18.129728 loss_att 19.544006 loss_ctc 21.925247 loss_rnnt 17.176550 hw_loss 0.307975 lr 0.00047224 rank 0
2023-02-21 04:09:32,002 DEBUG TRAIN Batch 13/300 loss 7.568647 loss_att 7.523252 loss_ctc 10.120966 loss_rnnt 6.953677 hw_loss 0.532014 lr 0.00047216 rank 2
2023-02-21 04:09:32,004 DEBUG TRAIN Batch 13/300 loss 12.034551 loss_att 13.743180 loss_ctc 12.155830 loss_rnnt 11.456627 hw_loss 0.412553 lr 0.00047219 rank 4
2023-02-21 04:09:32,005 DEBUG TRAIN Batch 13/300 loss 12.734105 loss_att 15.855186 loss_ctc 18.759676 loss_rnnt 11.078035 hw_loss 0.428331 lr 0.00047215 rank 6
2023-02-21 04:09:32,006 DEBUG TRAIN Batch 13/300 loss 9.632811 loss_att 10.307114 loss_ctc 11.909357 loss_rnnt 9.063842 hw_loss 0.244817 lr 0.00047213 rank 7
2023-02-21 04:09:32,006 DEBUG TRAIN Batch 13/300 loss 11.518559 loss_att 10.713474 loss_ctc 11.771617 loss_rnnt 11.458523 hw_loss 0.351211 lr 0.00047211 rank 5
2023-02-21 04:09:32,008 DEBUG TRAIN Batch 13/300 loss 16.022585 loss_att 14.190529 loss_ctc 21.226368 loss_rnnt 15.436557 hw_loss 0.484880 lr 0.00047213 rank 3
2023-02-21 04:09:32,015 DEBUG TRAIN Batch 13/300 loss 15.407655 loss_att 15.549131 loss_ctc 18.595200 loss_rnnt 14.676517 hw_loss 0.520944 lr 0.00047221 rank 1
2023-02-21 04:10:30,381 DEBUG TRAIN Batch 13/400 loss 8.970708 loss_att 14.514850 loss_ctc 13.838484 loss_rnnt 7.014142 hw_loss 0.372566 lr 0.00047210 rank 1
2023-02-21 04:10:30,381 DEBUG TRAIN Batch 13/400 loss 22.457340 loss_att 26.825211 loss_ctc 25.994370 loss_rnnt 20.919468 hw_loss 0.361301 lr 0.00047206 rank 2
2023-02-21 04:10:30,382 DEBUG TRAIN Batch 13/400 loss 17.183899 loss_att 20.598015 loss_ctc 24.292740 loss_rnnt 15.504918 hw_loss 0.090582 lr 0.00047214 rank 0
2023-02-21 04:10:30,384 DEBUG TRAIN Batch 13/400 loss 7.038893 loss_att 12.818218 loss_ctc 10.195614 loss_rnnt 5.353011 hw_loss 0.204602 lr 0.00047202 rank 3
2023-02-21 04:10:30,389 DEBUG TRAIN Batch 13/400 loss 16.168629 loss_att 15.500937 loss_ctc 16.907612 loss_rnnt 16.093857 hw_loss 0.205837 lr 0.00047201 rank 5
2023-02-21 04:10:30,390 DEBUG TRAIN Batch 13/400 loss 20.864317 loss_att 26.439146 loss_ctc 22.580418 loss_rnnt 19.432688 hw_loss 0.164720 lr 0.00047209 rank 4
2023-02-21 04:10:30,392 DEBUG TRAIN Batch 13/400 loss 13.636197 loss_att 19.862988 loss_ctc 19.595291 loss_rnnt 11.475263 hw_loss 0.226932 lr 0.00047203 rank 7
2023-02-21 04:10:30,393 DEBUG TRAIN Batch 13/400 loss 17.705212 loss_att 23.755733 loss_ctc 18.430290 loss_rnnt 16.234722 hw_loss 0.306951 lr 0.00047204 rank 6
2023-02-21 04:11:29,136 DEBUG TRAIN Batch 13/500 loss 14.699768 loss_att 17.960934 loss_ctc 18.334667 loss_rnnt 13.354486 hw_loss 0.390742 lr 0.00047203 rank 0
2023-02-21 04:11:29,140 DEBUG TRAIN Batch 13/500 loss 7.854400 loss_att 11.606302 loss_ctc 12.667431 loss_rnnt 6.359795 hw_loss 0.192163 lr 0.00047194 rank 6
2023-02-21 04:11:29,142 DEBUG TRAIN Batch 13/500 loss 43.650707 loss_att 39.631748 loss_ctc 42.821651 loss_rnnt 44.362679 hw_loss 0.379417 lr 0.00047190 rank 5
2023-02-21 04:11:29,145 DEBUG TRAIN Batch 13/500 loss 48.436890 loss_att 49.744164 loss_ctc 66.800751 loss_rnnt 45.600784 hw_loss 0.236506 lr 0.00047200 rank 1
2023-02-21 04:11:29,145 DEBUG TRAIN Batch 13/500 loss 15.121364 loss_att 20.051651 loss_ctc 14.307841 loss_rnnt 14.243615 hw_loss 0.000300 lr 0.00047195 rank 2
2023-02-21 04:11:29,146 DEBUG TRAIN Batch 13/500 loss 19.028442 loss_att 25.801510 loss_ctc 15.508518 loss_rnnt 18.053379 hw_loss 0.168323 lr 0.00047198 rank 4
2023-02-21 04:11:29,147 DEBUG TRAIN Batch 13/500 loss 24.810995 loss_att 25.557306 loss_ctc 29.240894 loss_rnnt 23.940075 hw_loss 0.245639 lr 0.00047192 rank 7
2023-02-21 04:11:29,161 DEBUG TRAIN Batch 13/500 loss 22.319729 loss_att 39.664627 loss_ctc 30.086790 loss_rnnt 17.716148 hw_loss 0.185613 lr 0.00047192 rank 3
2023-02-21 04:12:27,036 DEBUG TRAIN Batch 13/600 loss 16.677320 loss_att 21.232157 loss_ctc 18.801525 loss_rnnt 15.337245 hw_loss 0.273526 lr 0.00047193 rank 0
2023-02-21 04:12:27,042 DEBUG TRAIN Batch 13/600 loss 22.603651 loss_att 22.899586 loss_ctc 27.832142 loss_rnnt 21.682821 hw_loss 0.308457 lr 0.00047185 rank 2
2023-02-21 04:12:27,045 DEBUG TRAIN Batch 13/600 loss 14.516187 loss_att 21.681971 loss_ctc 23.294626 loss_rnnt 11.731817 hw_loss 0.338913 lr 0.00047188 rank 4
2023-02-21 04:12:27,046 DEBUG TRAIN Batch 13/600 loss 12.311541 loss_att 15.828315 loss_ctc 14.795742 loss_rnnt 11.158660 hw_loss 0.221811 lr 0.00047189 rank 1
2023-02-21 04:12:27,047 DEBUG TRAIN Batch 13/600 loss 21.570328 loss_att 22.197266 loss_ctc 26.406622 loss_rnnt 20.542747 hw_loss 0.482540 lr 0.00047180 rank 5
2023-02-21 04:12:27,049 DEBUG TRAIN Batch 13/600 loss 19.146856 loss_att 18.668983 loss_ctc 21.136297 loss_rnnt 18.875105 hw_loss 0.191374 lr 0.00047181 rank 3
2023-02-21 04:12:27,056 DEBUG TRAIN Batch 13/600 loss 18.360821 loss_att 20.139271 loss_ctc 19.933712 loss_rnnt 17.684036 hw_loss 0.208831 lr 0.00047183 rank 6
2023-02-21 04:12:27,059 DEBUG TRAIN Batch 13/600 loss 19.796185 loss_att 23.189436 loss_ctc 25.688454 loss_rnnt 18.122482 hw_loss 0.392655 lr 0.00047182 rank 7
2023-02-21 04:13:25,974 DEBUG TRAIN Batch 13/700 loss 8.519945 loss_att 11.736465 loss_ctc 8.583312 loss_rnnt 7.718087 hw_loss 0.281446 lr 0.00047182 rank 0
2023-02-21 04:13:25,985 DEBUG TRAIN Batch 13/700 loss 7.285683 loss_att 11.522336 loss_ctc 8.359153 loss_rnnt 6.170774 hw_loss 0.233341 lr 0.00047177 rank 4
2023-02-21 04:13:25,986 DEBUG TRAIN Batch 13/700 loss 16.338753 loss_att 18.123878 loss_ctc 17.534248 loss_rnnt 15.740648 hw_loss 0.153151 lr 0.00047171 rank 3
2023-02-21 04:13:25,988 DEBUG TRAIN Batch 13/700 loss 15.554922 loss_att 24.104206 loss_ctc 23.430828 loss_rnnt 12.628471 hw_loss 0.312139 lr 0.00047174 rank 2
2023-02-21 04:13:25,988 DEBUG TRAIN Batch 13/700 loss 12.088017 loss_att 17.543013 loss_ctc 13.964085 loss_rnnt 10.624144 hw_loss 0.230124 lr 0.00047169 rank 5
2023-02-21 04:13:25,990 DEBUG TRAIN Batch 13/700 loss 22.570156 loss_att 30.016882 loss_ctc 29.704542 loss_rnnt 20.044468 hw_loss 0.159545 lr 0.00047173 rank 6
2023-02-21 04:13:25,990 DEBUG TRAIN Batch 13/700 loss 14.281877 loss_att 16.583824 loss_ctc 15.510609 loss_rnnt 13.509796 hw_loss 0.277237 lr 0.00047179 rank 1
2023-02-21 04:13:26,052 DEBUG TRAIN Batch 13/700 loss 11.620986 loss_att 11.459414 loss_ctc 15.134912 loss_rnnt 11.080123 hw_loss 0.196225 lr 0.00047171 rank 7
2023-02-21 04:14:25,810 DEBUG TRAIN Batch 13/800 loss 15.214019 loss_att 19.014885 loss_ctc 20.652866 loss_rnnt 13.491363 hw_loss 0.444943 lr 0.00047172 rank 0
2023-02-21 04:14:25,813 DEBUG TRAIN Batch 13/800 loss 14.506363 loss_att 16.288815 loss_ctc 18.251041 loss_rnnt 13.587293 hw_loss 0.118669 lr 0.00047168 rank 1
2023-02-21 04:14:25,814 DEBUG TRAIN Batch 13/800 loss 2.481952 loss_att 12.298600 loss_ctc 1.302876 loss_rnnt 0.675757 hw_loss 0.000141 lr 0.00047159 rank 5
2023-02-21 04:14:25,818 DEBUG TRAIN Batch 13/800 loss 1.854942 loss_att 5.674627 loss_ctc 3.044039 loss_rnnt 0.738905 hw_loss 0.362912 lr 0.00047164 rank 2
2023-02-21 04:14:25,822 DEBUG TRAIN Batch 13/800 loss 20.656429 loss_att 30.143757 loss_ctc 22.688452 loss_rnnt 18.487789 hw_loss 0.000450 lr 0.00047160 rank 3
2023-02-21 04:14:25,825 DEBUG TRAIN Batch 13/800 loss 9.620224 loss_att 11.574840 loss_ctc 13.463384 loss_rnnt 8.514556 hw_loss 0.379358 lr 0.00047167 rank 4
2023-02-21 04:14:25,833 DEBUG TRAIN Batch 13/800 loss 10.236870 loss_att 7.583039 loss_ctc 12.432254 loss_rnnt 10.244621 hw_loss 0.431808 lr 0.00047161 rank 7
2023-02-21 04:14:25,849 DEBUG TRAIN Batch 13/800 loss 2.895515 loss_att 9.268574 loss_ctc 3.807731 loss_rnnt 1.412315 hw_loss 0.163048 lr 0.00047162 rank 6
2023-02-21 04:15:44,710 DEBUG TRAIN Batch 13/900 loss 29.331718 loss_att 27.525616 loss_ctc 31.486801 loss_rnnt 29.173622 hw_loss 0.434954 lr 0.00047153 rank 2
2023-02-21 04:15:44,711 DEBUG TRAIN Batch 13/900 loss 7.514488 loss_att 10.885673 loss_ctc 7.926524 loss_rnnt 6.615091 hw_loss 0.319164 lr 0.00047150 rank 3
2023-02-21 04:15:44,710 DEBUG TRAIN Batch 13/900 loss 14.426621 loss_att 18.718105 loss_ctc 18.107101 loss_rnnt 12.917842 hw_loss 0.299537 lr 0.00047161 rank 0
2023-02-21 04:15:44,714 DEBUG TRAIN Batch 13/900 loss 14.180787 loss_att 17.494198 loss_ctc 19.626240 loss_rnnt 12.670095 hw_loss 0.228656 lr 0.00047152 rank 6
2023-02-21 04:15:44,716 DEBUG TRAIN Batch 13/900 loss 13.422731 loss_att 11.291670 loss_ctc 12.509192 loss_rnnt 13.804201 hw_loss 0.312277 lr 0.00047158 rank 1
2023-02-21 04:15:44,718 DEBUG TRAIN Batch 13/900 loss 8.853184 loss_att 10.891462 loss_ctc 10.742487 loss_rnnt 8.079598 hw_loss 0.213791 lr 0.00047156 rank 4
2023-02-21 04:15:44,722 DEBUG TRAIN Batch 13/900 loss 9.909039 loss_att 10.055745 loss_ctc 11.432480 loss_rnnt 9.460613 hw_loss 0.404922 lr 0.00047148 rank 5
2023-02-21 04:15:44,725 DEBUG TRAIN Batch 13/900 loss 21.711761 loss_att 25.295298 loss_ctc 25.275295 loss_rnnt 20.360363 hw_loss 0.299164 lr 0.00047150 rank 7
2023-02-21 04:16:44,376 DEBUG TRAIN Batch 13/1000 loss 19.284176 loss_att 20.497866 loss_ctc 18.474764 loss_rnnt 18.929531 hw_loss 0.412177 lr 0.00047151 rank 0
2023-02-21 04:16:44,383 DEBUG TRAIN Batch 13/1000 loss 5.762529 loss_att 9.340651 loss_ctc 6.216194 loss_rnnt 4.766013 hw_loss 0.413257 lr 0.00047141 rank 6
2023-02-21 04:16:44,392 DEBUG TRAIN Batch 13/1000 loss 19.140638 loss_att 25.755911 loss_ctc 21.309071 loss_rnnt 17.486046 hw_loss 0.079526 lr 0.00047139 rank 3
2023-02-21 04:16:44,395 DEBUG TRAIN Batch 13/1000 loss 7.029054 loss_att 15.454258 loss_ctc 9.107578 loss_rnnt 4.921268 hw_loss 0.273017 lr 0.00047143 rank 2
2023-02-21 04:16:44,398 DEBUG TRAIN Batch 13/1000 loss 13.380017 loss_att 18.562338 loss_ctc 18.469955 loss_rnnt 11.470232 hw_loss 0.364992 lr 0.00047146 rank 4
2023-02-21 04:16:44,399 DEBUG TRAIN Batch 13/1000 loss 5.365003 loss_att 8.392900 loss_ctc 4.324500 loss_rnnt 4.766522 hw_loss 0.246817 lr 0.00047140 rank 7
2023-02-21 04:16:44,413 DEBUG TRAIN Batch 13/1000 loss 5.412969 loss_att 8.291037 loss_ctc 10.707890 loss_rnnt 4.067877 hw_loss 0.119042 lr 0.00047147 rank 1
2023-02-21 04:16:44,458 DEBUG TRAIN Batch 13/1000 loss 6.349823 loss_att 10.222318 loss_ctc 8.284815 loss_rnnt 5.244257 hw_loss 0.137002 lr 0.00047138 rank 5
2023-02-21 04:17:42,678 DEBUG TRAIN Batch 13/1100 loss 22.664843 loss_att 22.672510 loss_ctc 24.504538 loss_rnnt 22.230698 hw_loss 0.351217 lr 0.00047140 rank 0
2023-02-21 04:17:42,679 DEBUG TRAIN Batch 13/1100 loss 1.278736 loss_att 3.465542 loss_ctc 0.885649 loss_rnnt 0.893593 hw_loss 0.000362 lr 0.00047131 rank 6
2023-02-21 04:17:42,685 DEBUG TRAIN Batch 13/1100 loss 1.722764 loss_att 4.973892 loss_ctc 2.221090 loss_rnnt 0.719875 hw_loss 0.536661 lr 0.00047132 rank 2
2023-02-21 04:17:42,687 DEBUG TRAIN Batch 13/1100 loss 18.195286 loss_att 16.600143 loss_ctc 18.601759 loss_rnnt 18.460028 hw_loss 0.000174 lr 0.00047127 rank 5
2023-02-21 04:17:42,689 DEBUG TRAIN Batch 13/1100 loss 6.149867 loss_att 12.995945 loss_ctc 13.185541 loss_rnnt 3.558055 hw_loss 0.533449 lr 0.00047129 rank 7
2023-02-21 04:17:42,695 DEBUG TRAIN Batch 13/1100 loss 17.669052 loss_att 25.814884 loss_ctc 25.134907 loss_rnnt 14.967428 hw_loss 0.144397 lr 0.00047129 rank 3
2023-02-21 04:17:42,701 DEBUG TRAIN Batch 13/1100 loss 14.604464 loss_att 19.793228 loss_ctc 19.497396 loss_rnnt 12.652135 hw_loss 0.491595 lr 0.00047135 rank 4
2023-02-21 04:17:42,749 DEBUG TRAIN Batch 13/1100 loss 29.086231 loss_att 30.672365 loss_ctc 41.565826 loss_rnnt 26.989334 hw_loss 0.216983 lr 0.00047137 rank 1
2023-02-21 04:18:40,881 DEBUG TRAIN Batch 13/1200 loss 1.495491 loss_att 3.313242 loss_ctc 0.955332 loss_rnnt 1.028482 hw_loss 0.329025 lr 0.00047130 rank 0
2023-02-21 04:18:40,882 DEBUG TRAIN Batch 13/1200 loss 19.492964 loss_att 23.560246 loss_ctc 21.092957 loss_rnnt 18.336840 hw_loss 0.242501 lr 0.00047117 rank 5
2023-02-21 04:18:40,887 DEBUG TRAIN Batch 13/1200 loss 8.209686 loss_att 14.048281 loss_ctc 12.460604 loss_rnnt 6.319206 hw_loss 0.292450 lr 0.00047126 rank 1
2023-02-21 04:18:40,888 DEBUG TRAIN Batch 13/1200 loss 7.435297 loss_att 9.272885 loss_ctc 10.240987 loss_rnnt 6.578342 hw_loss 0.216273 lr 0.00047120 rank 6
2023-02-21 04:18:40,894 DEBUG TRAIN Batch 13/1200 loss 21.502094 loss_att 29.533405 loss_ctc 33.076969 loss_rnnt 18.136457 hw_loss 0.405110 lr 0.00047118 rank 3
2023-02-21 04:18:40,895 DEBUG TRAIN Batch 13/1200 loss 2.080704 loss_att 4.340411 loss_ctc 3.456039 loss_rnnt 1.444371 hw_loss 0.001901 lr 0.00047122 rank 2
2023-02-21 04:18:40,895 DEBUG TRAIN Batch 13/1200 loss 24.130934 loss_att 24.569439 loss_ctc 30.436659 loss_rnnt 23.000160 hw_loss 0.379332 lr 0.00047119 rank 7
2023-02-21 04:18:40,896 DEBUG TRAIN Batch 13/1200 loss 16.097029 loss_att 22.998653 loss_ctc 22.996040 loss_rnnt 13.644423 hw_loss 0.285774 lr 0.00047125 rank 4
2023-02-21 04:19:41,350 DEBUG TRAIN Batch 13/1300 loss 8.171518 loss_att 16.789612 loss_ctc 14.812709 loss_rnnt 5.483110 hw_loss 0.148682 lr 0.00047119 rank 0
2023-02-21 04:19:41,359 DEBUG TRAIN Batch 13/1300 loss 8.196621 loss_att 11.774962 loss_ctc 6.729345 loss_rnnt 7.642178 hw_loss 0.064521 lr 0.00047108 rank 3
2023-02-21 04:19:41,359 DEBUG TRAIN Batch 13/1300 loss 8.250160 loss_att 12.218949 loss_ctc 11.758912 loss_rnnt 6.866983 hw_loss 0.227970 lr 0.00047111 rank 2
2023-02-21 04:19:41,365 DEBUG TRAIN Batch 13/1300 loss 5.572305 loss_att 9.532132 loss_ctc 9.786196 loss_rnnt 4.023724 hw_loss 0.365182 lr 0.00047116 rank 1
2023-02-21 04:19:41,366 DEBUG TRAIN Batch 13/1300 loss 31.652905 loss_att 28.258820 loss_ctc 41.240696 loss_rnnt 31.053215 hw_loss 0.000251 lr 0.00047110 rank 6
2023-02-21 04:19:41,369 DEBUG TRAIN Batch 13/1300 loss 10.076876 loss_att 16.130928 loss_ctc 12.547160 loss_rnnt 8.507260 hw_loss 0.055187 lr 0.00047106 rank 5
2023-02-21 04:19:41,372 DEBUG TRAIN Batch 13/1300 loss 17.310169 loss_att 18.454548 loss_ctc 15.544836 loss_rnnt 17.115025 hw_loss 0.378087 lr 0.00047108 rank 7
2023-02-21 04:19:41,430 DEBUG TRAIN Batch 13/1300 loss 21.403400 loss_att 28.407928 loss_ctc 22.832541 loss_rnnt 19.638237 hw_loss 0.325697 lr 0.00047114 rank 4
2023-02-21 04:20:38,638 DEBUG TRAIN Batch 13/1400 loss 8.032861 loss_att 13.092840 loss_ctc 10.669704 loss_rnnt 6.474542 hw_loss 0.365144 lr 0.00047098 rank 3
2023-02-21 04:20:38,638 DEBUG TRAIN Batch 13/1400 loss 17.133080 loss_att 22.500742 loss_ctc 17.203287 loss_rnnt 15.860036 hw_loss 0.356533 lr 0.00047109 rank 0
2023-02-21 04:20:38,640 DEBUG TRAIN Batch 13/1400 loss 1.850253 loss_att 7.092393 loss_ctc 2.369598 loss_rnnt 0.465066 hw_loss 0.501586 lr 0.00047096 rank 5
2023-02-21 04:20:38,644 DEBUG TRAIN Batch 13/1400 loss 12.038247 loss_att 16.465492 loss_ctc 18.432407 loss_rnnt 10.037548 hw_loss 0.492555 lr 0.00047099 rank 6
2023-02-21 04:20:38,644 DEBUG TRAIN Batch 13/1400 loss 4.829905 loss_att 9.397850 loss_ctc 1.883712 loss_rnnt 4.309055 hw_loss 0.000162 lr 0.00047104 rank 4
2023-02-21 04:20:38,645 DEBUG TRAIN Batch 13/1400 loss 3.677719 loss_att 6.545154 loss_ctc 6.181862 loss_rnnt 2.466004 hw_loss 0.570643 lr 0.00047101 rank 2
2023-02-21 04:20:38,648 DEBUG TRAIN Batch 13/1400 loss 15.588788 loss_att 20.637339 loss_ctc 18.961742 loss_rnnt 14.129189 hw_loss 0.000302 lr 0.00047106 rank 1
2023-02-21 04:20:38,659 DEBUG TRAIN Batch 13/1400 loss 4.623822 loss_att 7.957137 loss_ctc 3.566079 loss_rnnt 3.862164 hw_loss 0.442549 lr 0.00047098 rank 7
2023-02-21 04:21:37,164 DEBUG TRAIN Batch 13/1500 loss 16.980322 loss_att 21.670158 loss_ctc 20.569641 loss_rnnt 15.446943 hw_loss 0.219066 lr 0.00047098 rank 0
2023-02-21 04:21:37,170 DEBUG TRAIN Batch 13/1500 loss 12.499072 loss_att 14.917082 loss_ctc 19.695234 loss_rnnt 11.055838 hw_loss 0.000271 lr 0.00047089 rank 6
2023-02-21 04:21:37,172 DEBUG TRAIN Batch 13/1500 loss 32.698227 loss_att 37.050304 loss_ctc 33.896561 loss_rnnt 31.462772 hw_loss 0.384867 lr 0.00047087 rank 3
2023-02-21 04:21:37,174 DEBUG TRAIN Batch 13/1500 loss 16.571791 loss_att 19.124668 loss_ctc 24.702402 loss_rnnt 14.767073 hw_loss 0.393861 lr 0.00047090 rank 2
2023-02-21 04:21:37,176 DEBUG TRAIN Batch 13/1500 loss 4.047400 loss_att 7.177266 loss_ctc 7.337334 loss_rnnt 2.874128 hw_loss 0.203701 lr 0.00047085 rank 5
2023-02-21 04:21:37,176 DEBUG TRAIN Batch 13/1500 loss 17.153843 loss_att 19.205750 loss_ctc 22.072037 loss_rnnt 16.007156 hw_loss 0.151025 lr 0.00047094 rank 4
2023-02-21 04:21:37,210 DEBUG TRAIN Batch 13/1500 loss 4.865225 loss_att 7.185979 loss_ctc 6.128516 loss_rnnt 4.010066 hw_loss 0.417319 lr 0.00047095 rank 1
2023-02-21 04:21:37,221 DEBUG TRAIN Batch 13/1500 loss 14.818136 loss_att 21.132086 loss_ctc 25.875160 loss_rnnt 11.830483 hw_loss 0.469860 lr 0.00047088 rank 7
2023-02-21 04:22:37,310 DEBUG TRAIN Batch 13/1600 loss 37.646057 loss_att 34.138042 loss_ctc 36.022419 loss_rnnt 38.562370 hw_loss 0.003322 lr 0.00047088 rank 0
2023-02-21 04:22:37,310 DEBUG TRAIN Batch 13/1600 loss 14.443985 loss_att 10.767260 loss_ctc 12.526938 loss_rnnt 15.290506 hw_loss 0.270803 lr 0.00047080 rank 2
2023-02-21 04:22:37,320 DEBUG TRAIN Batch 13/1600 loss 15.990969 loss_att 20.416065 loss_ctc 23.323729 loss_rnnt 14.126671 hw_loss 0.002957 lr 0.00047075 rank 5
2023-02-21 04:22:37,325 DEBUG TRAIN Batch 13/1600 loss 11.902113 loss_att 19.873480 loss_ctc 17.464005 loss_rnnt 9.435116 hw_loss 0.245882 lr 0.00047085 rank 1
2023-02-21 04:22:37,326 DEBUG TRAIN Batch 13/1600 loss 1.090074 loss_att 2.587216 loss_ctc 0.410415 loss_rnnt 0.665925 hw_loss 0.403767 lr 0.00047083 rank 4
2023-02-21 04:22:37,328 DEBUG TRAIN Batch 13/1600 loss 1.250281 loss_att 3.407843 loss_ctc 1.033394 loss_rnnt 0.842742 hw_loss 0.009272 lr 0.00047077 rank 3
2023-02-21 04:22:37,329 DEBUG TRAIN Batch 13/1600 loss 2.698257 loss_att 5.090334 loss_ctc 4.723319 loss_rnnt 1.842323 hw_loss 0.201582 lr 0.00047078 rank 6
2023-02-21 04:22:37,338 DEBUG TRAIN Batch 13/1600 loss 2.555552 loss_att 4.924314 loss_ctc 1.618909 loss_rnnt 2.015534 hw_loss 0.358407 lr 0.00047077 rank 7
2023-02-21 04:23:36,654 DEBUG TRAIN Batch 13/1700 loss 2.662276 loss_att 5.048779 loss_ctc 2.346424 loss_rnnt 2.227024 hw_loss 0.000121 lr 0.00047078 rank 0
2023-02-21 04:23:36,656 DEBUG TRAIN Batch 13/1700 loss 1.984675 loss_att 7.430823 loss_ctc 1.307243 loss_rnnt 0.894367 hw_loss 0.171380 lr 0.00047066 rank 3
2023-02-21 04:23:36,656 DEBUG TRAIN Batch 13/1700 loss 19.397644 loss_att 19.501450 loss_ctc 23.312193 loss_rnnt 18.810282 hw_loss 0.083740 lr 0.00047068 rank 6
2023-02-21 04:23:36,660 DEBUG TRAIN Batch 13/1700 loss 3.505362 loss_att 8.979855 loss_ctc 5.295817 loss_rnnt 1.946941 hw_loss 0.421491 lr 0.00047070 rank 2
2023-02-21 04:23:36,660 DEBUG TRAIN Batch 13/1700 loss 25.309870 loss_att 40.654385 loss_ctc 28.630239 loss_rnnt 21.566780 hw_loss 0.434006 lr 0.00047067 rank 7
2023-02-21 04:23:36,663 DEBUG TRAIN Batch 13/1700 loss 3.801342 loss_att 4.903143 loss_ctc 3.944504 loss_rnnt 3.561798 hw_loss 0.000180 lr 0.00047073 rank 4
2023-02-21 04:23:36,704 DEBUG TRAIN Batch 13/1700 loss 13.906964 loss_att 14.275293 loss_ctc 19.817007 loss_rnnt 13.045227 hw_loss 0.000121 lr 0.00047074 rank 1
2023-02-21 04:23:36,749 DEBUG TRAIN Batch 13/1700 loss 25.020704 loss_att 26.095772 loss_ctc 30.657022 loss_rnnt 24.054115 hw_loss 0.000126 lr 0.00047065 rank 5
2023-02-21 04:24:56,679 DEBUG TRAIN Batch 13/1800 loss 5.625965 loss_att 9.539548 loss_ctc 9.955235 loss_rnnt 4.212968 hw_loss 0.099458 lr 0.00047059 rank 2
2023-02-21 04:24:56,681 DEBUG TRAIN Batch 13/1800 loss 9.223648 loss_att 16.487257 loss_ctc 12.664520 loss_rnnt 7.220970 hw_loss 0.170952 lr 0.00047067 rank 0
2023-02-21 04:24:56,685 DEBUG TRAIN Batch 13/1800 loss 6.370536 loss_att 11.503175 loss_ctc 8.901158 loss_rnnt 4.925363 hw_loss 0.152304 lr 0.00047064 rank 1
2023-02-21 04:24:56,691 DEBUG TRAIN Batch 13/1800 loss 5.570907 loss_att 9.989833 loss_ctc 5.833632 loss_rnnt 4.467884 hw_loss 0.345388 lr 0.00047054 rank 5
2023-02-21 04:24:56,692 DEBUG TRAIN Batch 13/1800 loss 8.846730 loss_att 15.701197 loss_ctc 13.422781 loss_rnnt 6.698474 hw_loss 0.313545 lr 0.00047056 rank 3
2023-02-21 04:24:56,693 DEBUG TRAIN Batch 13/1800 loss 11.105308 loss_att 16.001131 loss_ctc 14.858251 loss_rnnt 9.425983 hw_loss 0.374563 lr 0.00047057 rank 6
2023-02-21 04:24:56,696 DEBUG TRAIN Batch 13/1800 loss 5.979288 loss_att 9.280888 loss_ctc 6.409040 loss_rnnt 5.158235 hw_loss 0.193935 lr 0.00047056 rank 7
2023-02-21 04:24:56,753 DEBUG TRAIN Batch 13/1800 loss 11.815236 loss_att 11.364733 loss_ctc 13.502975 loss_rnnt 11.559517 hw_loss 0.226476 lr 0.00047062 rank 4
2023-02-21 04:25:57,099 DEBUG TRAIN Batch 13/1900 loss 19.092627 loss_att 22.589111 loss_ctc 25.343594 loss_rnnt 17.559565 hw_loss 0.000566 lr 0.00047057 rank 0
2023-02-21 04:25:57,109 DEBUG TRAIN Batch 13/1900 loss 12.570236 loss_att 15.288177 loss_ctc 12.191674 loss_rnnt 11.939041 hw_loss 0.258903 lr 0.00047049 rank 2
2023-02-21 04:25:57,109 DEBUG TRAIN Batch 13/1900 loss 13.682280 loss_att 21.123100 loss_ctc 24.308270 loss_rnnt 10.601548 hw_loss 0.329566 lr 0.00047047 rank 6
2023-02-21 04:25:57,112 DEBUG TRAIN Batch 13/1900 loss 5.332031 loss_att 9.752733 loss_ctc 6.548134 loss_rnnt 4.140917 hw_loss 0.271549 lr 0.00047045 rank 3
2023-02-21 04:25:57,113 DEBUG TRAIN Batch 13/1900 loss 13.955394 loss_att 18.457607 loss_ctc 20.402969 loss_rnnt 12.138626 hw_loss 0.106216 lr 0.00047046 rank 7
2023-02-21 04:25:57,113 DEBUG TRAIN Batch 13/1900 loss 17.211761 loss_att 20.122627 loss_ctc 21.030771 loss_rnnt 15.924473 hw_loss 0.367342 lr 0.00047052 rank 4
2023-02-21 04:25:57,114 DEBUG TRAIN Batch 13/1900 loss 9.482534 loss_att 12.256741 loss_ctc 14.666000 loss_rnnt 8.236319 hw_loss 0.000460 lr 0.00047044 rank 5
2023-02-21 04:25:57,172 DEBUG TRAIN Batch 13/1900 loss 10.422315 loss_att 15.877604 loss_ctc 14.516616 loss_rnnt 8.552164 hw_loss 0.437224 lr 0.00047053 rank 1
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 10.76 GiB total capacity; 5.96 GiB already allocated; 1.35 GiB free; 8.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:47027
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:6045
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:57504
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:31153
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:12442
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:384
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:26518
