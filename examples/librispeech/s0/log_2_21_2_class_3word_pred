/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_21_rnnt_bias_0-3word_finetune.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_21_rnnt_bias_loss_2_class_3word_finetune/ddp_init
2023-02-22 22:44:05,740 INFO training on multiple gpus, this gpu 1
2023-02-22 22:44:05,741 INFO training on multiple gpus, this gpu 5
2023-02-22 22:44:05,748 INFO training on multiple gpus, this gpu 6
2023-02-22 22:44:05,748 INFO training on multiple gpus, this gpu 2
2023-02-22 22:44:05,748 INFO training on multiple gpus, this gpu 3
2023-02-22 22:44:05,748 INFO training on multiple gpus, this gpu 0
2023-02-22 22:44:05,750 INFO training on multiple gpus, this gpu 4
2023-02-22 22:44:05,750 INFO training on multiple gpus, this gpu 7
2023-02-22 22:44:05,838 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 22:44:05,849 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 22:44:05,858 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-22 22:44:05,871 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-22 22:44:05,877 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-22 22:44:05,878 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-22 22:44:05,887 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-22 22:44:05,890 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-22 22:44:05,890 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,891 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,892 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,894 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,896 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,898 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,899 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:05,903 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:44:18,058 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,079 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,106 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,134 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,161 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,191 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,223 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:18,248 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/10.pt for GPU
2023-02-22 22:44:40,251 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,253 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,293 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,295 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,319 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,321 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,367 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,369 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,424 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,426 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,454 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,456 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,473 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,475 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:44:40,692 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:44:40,694 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-22 22:45:53,865 DEBUG TRAIN Batch 11/0 loss 440.801727 loss_att 84.904732 loss_ctc 686.363098 loss_rnnt 478.858734 hw_loss 0.714111 lr 0.00052195 rank 3
2023-02-22 22:45:53,883 DEBUG TRAIN Batch 11/0 loss 579.872437 loss_att 80.237221 loss_ctc 909.482300 loss_rnnt 635.330811 hw_loss 0.976216 lr 0.00052195 rank 2
2023-02-22 22:45:53,890 DEBUG TRAIN Batch 11/0 loss 343.252594 loss_att 81.302795 loss_ctc 908.389526 loss_rnnt 319.941620 hw_loss 0.654985 lr 0.00052195 rank 1
2023-02-22 22:45:53,894 DEBUG TRAIN Batch 11/0 loss 416.339569 loss_att 75.911789 loss_ctc 924.325623 loss_rnnt 416.278961 hw_loss 0.777543 lr 0.00052195 rank 4
2023-02-22 22:45:53,896 DEBUG TRAIN Batch 11/0 loss 365.050812 loss_att 76.180733 loss_ctc 895.510986 loss_rnnt 351.731873 hw_loss 0.684253 lr 0.00052195 rank 0
2023-02-22 22:45:53,896 DEBUG TRAIN Batch 11/0 loss 497.215912 loss_att 75.246277 loss_ctc 839.556702 loss_rnnt 535.539917 hw_loss 0.795984 lr 0.00052195 rank 6
2023-02-22 22:45:53,901 DEBUG TRAIN Batch 11/0 loss 398.476807 loss_att 82.909569 loss_ctc 1097.637451 loss_rnnt 367.968109 hw_loss 0.751442 lr 0.00052195 rank 5
2023-02-22 22:45:53,938 DEBUG TRAIN Batch 11/0 loss 629.655212 loss_att 81.368439 loss_ctc 1074.003906 loss_rnnt 679.675598 hw_loss 0.732148 lr 0.00052195 rank 7
2023-02-22 22:47:12,981 DEBUG TRAIN Batch 11/100 loss 237.972427 loss_att 266.953552 loss_ctc 389.209229 loss_rnnt 211.791016 hw_loss 0.413008 lr 0.00052167 rank 2
2023-02-22 22:47:12,981 DEBUG TRAIN Batch 11/100 loss 270.804993 loss_att 298.084534 loss_ctc 424.772949 loss_rnnt 244.598846 hw_loss 0.414715 lr 0.00052167 rank 4
2023-02-22 22:47:12,983 DEBUG TRAIN Batch 11/100 loss 275.213074 loss_att 327.706238 loss_ctc 450.818939 loss_rnnt 241.065918 hw_loss 0.439563 lr 0.00052167 rank 5
2023-02-22 22:47:12,985 DEBUG TRAIN Batch 11/100 loss 210.665039 loss_att 264.702881 loss_ctc 339.211487 loss_rnnt 182.495972 hw_loss 0.416229 lr 0.00052167 rank 7
2023-02-22 22:47:12,987 DEBUG TRAIN Batch 11/100 loss 307.108368 loss_att 325.147400 loss_ctc 482.118347 loss_rnnt 279.898315 hw_loss 0.501704 lr 0.00052167 rank 0
2023-02-22 22:47:12,989 DEBUG TRAIN Batch 11/100 loss 244.308838 loss_att 286.935791 loss_ctc 401.896576 loss_rnnt 214.524445 hw_loss 0.463644 lr 0.00052167 rank 1
2023-02-22 22:47:12,989 DEBUG TRAIN Batch 11/100 loss 257.312988 loss_att 271.707123 loss_ctc 382.289429 loss_rnnt 237.544678 hw_loss 0.423683 lr 0.00052167 rank 6
2023-02-22 22:47:13,034 DEBUG TRAIN Batch 11/100 loss 246.781601 loss_att 260.676941 loss_ctc 366.874146 loss_rnnt 227.722992 hw_loss 0.501040 lr 0.00052167 rank 3
2023-02-22 22:48:30,579 DEBUG TRAIN Batch 11/200 loss 142.657837 loss_att 241.223083 loss_ctc 143.325027 loss_rnnt 122.639275 hw_loss 0.406044 lr 0.00052139 rank 4
2023-02-22 22:48:30,579 DEBUG TRAIN Batch 11/200 loss 148.805893 loss_att 251.701508 loss_ctc 135.724518 loss_rnnt 129.735138 hw_loss 0.442150 lr 0.00052139 rank 7
2023-02-22 22:48:30,582 DEBUG TRAIN Batch 11/200 loss 165.359558 loss_att 256.107300 loss_ctc 169.861206 loss_rnnt 146.435699 hw_loss 0.326419 lr 0.00052139 rank 2
2023-02-22 22:48:30,585 DEBUG TRAIN Batch 11/200 loss 182.109680 loss_att 279.902954 loss_ctc 185.619934 loss_rnnt 161.814667 hw_loss 0.503135 lr 0.00052139 rank 0
2023-02-22 22:48:30,585 DEBUG TRAIN Batch 11/200 loss 167.538132 loss_att 285.851715 loss_ctc 154.372421 loss_rnnt 145.450195 hw_loss 0.338707 lr 0.00052139 rank 5
2023-02-22 22:48:30,592 DEBUG TRAIN Batch 11/200 loss 150.968628 loss_att 263.325653 loss_ctc 145.785126 loss_rnnt 129.032990 hw_loss 0.291275 lr 0.00052139 rank 6
2023-02-22 22:48:30,592 DEBUG TRAIN Batch 11/200 loss 163.957367 loss_att 276.835236 loss_ctc 160.140564 loss_rnnt 141.673309 hw_loss 0.407618 lr 0.00052139 rank 1
2023-02-22 22:48:30,634 DEBUG TRAIN Batch 11/200 loss 176.170914 loss_att 282.993958 loss_ctc 162.985596 loss_rnnt 156.371353 hw_loss 0.361865 lr 0.00052139 rank 3
2023-02-22 22:49:49,043 DEBUG TRAIN Batch 11/300 loss 178.672226 loss_att 316.393707 loss_ctc 171.056870 loss_rnnt 151.922318 hw_loss 0.414334 lr 0.00052110 rank 0
2023-02-22 22:49:49,047 DEBUG TRAIN Batch 11/300 loss 146.108917 loss_att 293.168823 loss_ctc 136.557434 loss_rnnt 117.775452 hw_loss 0.365643 lr 0.00052110 rank 3
2023-02-22 22:49:49,050 DEBUG TRAIN Batch 11/300 loss 136.643692 loss_att 268.028412 loss_ctc 128.989227 loss_rnnt 111.177673 hw_loss 0.393100 lr 0.00052110 rank 6
2023-02-22 22:49:49,051 DEBUG TRAIN Batch 11/300 loss 130.469910 loss_att 281.617920 loss_ctc 117.805740 loss_rnnt 101.747940 hw_loss 0.339206 lr 0.00052110 rank 7
2023-02-22 22:49:49,052 DEBUG TRAIN Batch 11/300 loss 117.454491 loss_att 265.365021 loss_ctc 96.580696 loss_rnnt 90.441666 hw_loss 0.401037 lr 0.00052110 rank 4
2023-02-22 22:49:49,052 DEBUG TRAIN Batch 11/300 loss 148.846008 loss_att 273.956970 loss_ctc 142.917755 loss_rnnt 124.416557 hw_loss 0.370659 lr 0.00052110 rank 2
2023-02-22 22:49:49,058 DEBUG TRAIN Batch 11/300 loss 123.916046 loss_att 244.940384 loss_ctc 115.663673 loss_rnnt 100.629868 hw_loss 0.340558 lr 0.00052110 rank 5
2023-02-22 22:49:49,065 DEBUG TRAIN Batch 11/300 loss 131.392044 loss_att 256.610504 loss_ctc 121.770493 loss_rnnt 107.396545 hw_loss 0.440010 lr 0.00052110 rank 1
2023-02-22 22:51:07,541 DEBUG TRAIN Batch 11/400 loss 112.403542 loss_att 228.641296 loss_ctc 104.606483 loss_rnnt 89.984238 hw_loss 0.396310 lr 0.00052082 rank 0
2023-02-22 22:51:07,542 DEBUG TRAIN Batch 11/400 loss 73.141190 loss_att 215.402771 loss_ctc 55.621887 loss_rnnt 46.796047 hw_loss 0.428889 lr 0.00052082 rank 7
2023-02-22 22:51:07,543 DEBUG TRAIN Batch 11/400 loss 89.709656 loss_att 222.072601 loss_ctc 79.139450 loss_rnnt 64.390419 hw_loss 0.480004 lr 0.00052082 rank 6
2023-02-22 22:51:07,544 DEBUG TRAIN Batch 11/400 loss 130.590439 loss_att 268.823395 loss_ctc 123.681259 loss_rnnt 103.687874 hw_loss 0.332237 lr 0.00052082 rank 2
2023-02-22 22:51:07,546 DEBUG TRAIN Batch 11/400 loss 114.474037 loss_att 258.029907 loss_ctc 110.123886 loss_rnnt 86.136688 hw_loss 0.386603 lr 0.00052082 rank 1
2023-02-22 22:51:07,547 DEBUG TRAIN Batch 11/400 loss 111.283241 loss_att 232.533142 loss_ctc 105.538429 loss_rnnt 87.590485 hw_loss 0.391408 lr 0.00052082 rank 4
2023-02-22 22:51:07,548 DEBUG TRAIN Batch 11/400 loss 124.826317 loss_att 264.468628 loss_ctc 112.969780 loss_rnnt 98.256989 hw_loss 0.415771 lr 0.00052082 rank 3
2023-02-22 22:51:07,575 DEBUG TRAIN Batch 11/400 loss 118.212578 loss_att 245.263580 loss_ctc 118.069244 loss_rnnt 92.596588 hw_loss 0.421704 lr 0.00052082 rank 5
2023-02-22 22:52:24,781 DEBUG TRAIN Batch 11/500 loss 74.498306 loss_att 190.750732 loss_ctc 63.189522 loss_rnnt 52.564022 hw_loss 0.359317 lr 0.00052054 rank 3
2023-02-22 22:52:24,783 DEBUG TRAIN Batch 11/500 loss 68.796295 loss_att 187.111145 loss_ctc 53.990421 loss_rnnt 46.902115 hw_loss 0.384972 lr 0.00052054 rank 4
2023-02-22 22:52:24,784 DEBUG TRAIN Batch 11/500 loss 99.470009 loss_att 217.929047 loss_ctc 86.336746 loss_rnnt 77.282173 hw_loss 0.463349 lr 0.00052054 rank 5
2023-02-22 22:52:24,785 DEBUG TRAIN Batch 11/500 loss 81.637573 loss_att 180.221832 loss_ctc 74.153542 loss_rnnt 62.736244 hw_loss 0.341887 lr 0.00052054 rank 2
2023-02-22 22:52:24,786 DEBUG TRAIN Batch 11/500 loss 154.156235 loss_att 273.509644 loss_ctc 156.860641 loss_rnnt 129.678253 hw_loss 0.462555 lr 0.00052054 rank 0
2023-02-22 22:52:24,788 DEBUG TRAIN Batch 11/500 loss 61.130318 loss_att 155.485214 loss_ctc 50.787857 loss_rnnt 43.422821 hw_loss 0.404091 lr 0.00052054 rank 1
2023-02-22 22:52:24,788 DEBUG TRAIN Batch 11/500 loss 109.129402 loss_att 233.492310 loss_ctc 104.300377 loss_rnnt 84.680756 hw_loss 0.412365 lr 0.00052054 rank 7
2023-02-22 22:52:24,789 DEBUG TRAIN Batch 11/500 loss 78.352722 loss_att 200.159302 loss_ctc 70.664833 loss_rnnt 54.824158 hw_loss 0.360559 lr 0.00052054 rank 6
2023-02-22 22:53:41,454 DEBUG TRAIN Batch 11/600 loss 54.300201 loss_att 123.560661 loss_ctc 52.353436 loss_rnnt 40.511662 hw_loss 0.367523 lr 0.00052026 rank 4
2023-02-22 22:53:41,455 DEBUG TRAIN Batch 11/600 loss 56.359295 loss_att 115.121658 loss_ctc 53.294575 loss_rnnt 44.824196 hw_loss 0.358597 lr 0.00052026 rank 3
2023-02-22 22:53:41,455 DEBUG TRAIN Batch 11/600 loss 106.602859 loss_att 214.573959 loss_ctc 106.474953 loss_rnnt 84.862450 hw_loss 0.306085 lr 0.00052026 rank 7
2023-02-22 22:53:41,458 DEBUG TRAIN Batch 11/600 loss 51.211903 loss_att 100.997002 loss_ctc 48.084675 loss_rnnt 41.432129 hw_loss 0.449464 lr 0.00052026 rank 2
2023-02-22 22:53:41,459 DEBUG TRAIN Batch 11/600 loss 44.202808 loss_att 127.946136 loss_ctc 32.880589 loss_rnnt 28.787365 hw_loss 0.330765 lr 0.00052026 rank 1
2023-02-22 22:53:41,460 DEBUG TRAIN Batch 11/600 loss 44.557194 loss_att 96.194016 loss_ctc 39.635857 loss_rnnt 34.707970 hw_loss 0.333820 lr 0.00052026 rank 0
2023-02-22 22:53:41,462 DEBUG TRAIN Batch 11/600 loss 83.997116 loss_att 180.133575 loss_ctc 81.359451 loss_rnnt 64.912468 hw_loss 0.391965 lr 0.00052026 rank 6
2023-02-22 22:53:41,463 DEBUG TRAIN Batch 11/600 loss 41.782776 loss_att 96.457474 loss_ctc 39.101833 loss_rnnt 30.988548 hw_loss 0.406396 lr 0.00052026 rank 5
2023-02-22 22:55:00,976 DEBUG TRAIN Batch 11/700 loss 62.410095 loss_att 179.261597 loss_ctc 49.766098 loss_rnnt 40.538612 hw_loss 0.350716 lr 0.00051997 rank 5
2023-02-22 22:55:00,978 DEBUG TRAIN Batch 11/700 loss 58.787720 loss_att 159.712769 loss_ctc 46.677643 loss_rnnt 40.003479 hw_loss 0.401082 lr 0.00051997 rank 2
2023-02-22 22:55:00,980 DEBUG TRAIN Batch 11/700 loss 99.206696 loss_att 231.448456 loss_ctc 87.450485 loss_rnnt 74.141121 hw_loss 0.346332 lr 0.00051997 rank 1
2023-02-22 22:55:00,982 DEBUG TRAIN Batch 11/700 loss 46.629913 loss_att 140.442963 loss_ctc 43.250076 loss_rnnt 28.078604 hw_loss 0.448771 lr 0.00051997 rank 0
2023-02-22 22:55:00,984 DEBUG TRAIN Batch 11/700 loss 91.987282 loss_att 216.744812 loss_ctc 87.896057 loss_rnnt 67.384781 hw_loss 0.368401 lr 0.00051997 rank 4
2023-02-22 22:55:00,986 DEBUG TRAIN Batch 11/700 loss 63.463127 loss_att 180.080658 loss_ctc 52.809776 loss_rnnt 41.357491 hw_loss 0.379828 lr 0.00051997 rank 3
2023-02-22 22:55:00,995 DEBUG TRAIN Batch 11/700 loss 58.899391 loss_att 177.238281 loss_ctc 45.638489 loss_rnnt 36.828957 hw_loss 0.320204 lr 0.00051997 rank 7
2023-02-22 22:55:01,025 DEBUG TRAIN Batch 11/700 loss 113.835236 loss_att 255.629196 loss_ctc 114.972069 loss_rnnt 85.142616 hw_loss 0.341720 lr 0.00051997 rank 6
2023-02-22 22:56:18,182 DEBUG TRAIN Batch 11/800 loss 74.882988 loss_att 128.941360 loss_ctc 69.104218 loss_rnnt 64.663170 hw_loss 0.334950 lr 0.00051969 rank 0
2023-02-22 22:56:18,187 DEBUG TRAIN Batch 11/800 loss 95.504921 loss_att 206.749023 loss_ctc 86.833855 loss_rnnt 74.256569 hw_loss 0.291872 lr 0.00051969 rank 2
2023-02-22 22:56:18,188 DEBUG TRAIN Batch 11/800 loss 104.864250 loss_att 202.679062 loss_ctc 104.769150 loss_rnnt 85.171936 hw_loss 0.266305 lr 0.00051969 rank 5
2023-02-22 22:56:18,188 DEBUG TRAIN Batch 11/800 loss 65.047432 loss_att 166.439621 loss_ctc 55.528740 loss_rnnt 45.843197 hw_loss 0.365535 lr 0.00051969 rank 3
2023-02-22 22:56:18,193 DEBUG TRAIN Batch 11/800 loss 72.355064 loss_att 167.852127 loss_ctc 62.828049 loss_rnnt 54.312653 hw_loss 0.399878 lr 0.00051969 rank 1
2023-02-22 22:56:18,195 DEBUG TRAIN Batch 11/800 loss 63.546635 loss_att 148.483765 loss_ctc 57.243439 loss_rnnt 47.186745 hw_loss 0.399156 lr 0.00051969 rank 4
2023-02-22 22:56:18,196 DEBUG TRAIN Batch 11/800 loss 76.591927 loss_att 179.825119 loss_ctc 63.455048 loss_rnnt 57.561462 hw_loss 0.253885 lr 0.00051969 rank 6
2023-02-22 22:56:18,199 DEBUG TRAIN Batch 11/800 loss 71.111565 loss_att 180.807831 loss_ctc 59.291649 loss_rnnt 50.572826 hw_loss 0.329020 lr 0.00051969 rank 7
2023-02-22 22:57:34,858 DEBUG TRAIN Batch 11/900 loss 49.176033 loss_att 97.240654 loss_ctc 43.866570 loss_rnnt 40.080883 hw_loss 0.356539 lr 0.00051941 rank 4
2023-02-22 22:57:34,864 DEBUG TRAIN Batch 11/900 loss 37.203579 loss_att 96.371910 loss_ctc 34.675697 loss_rnnt 25.506439 hw_loss 0.375987 lr 0.00051941 rank 3
2023-02-22 22:57:34,865 DEBUG TRAIN Batch 11/900 loss 45.324192 loss_att 96.997879 loss_ctc 41.268993 loss_rnnt 35.315319 hw_loss 0.402804 lr 0.00051941 rank 5
2023-02-22 22:57:34,865 DEBUG TRAIN Batch 11/900 loss 58.030941 loss_att 125.240517 loss_ctc 56.459587 loss_rnnt 44.576290 hw_loss 0.416731 lr 0.00051941 rank 0
2023-02-22 22:57:34,866 DEBUG TRAIN Batch 11/900 loss 102.000786 loss_att 201.844086 loss_ctc 100.411423 loss_rnnt 82.041885 hw_loss 0.379050 lr 0.00051941 rank 2
2023-02-22 22:57:34,872 DEBUG TRAIN Batch 11/900 loss 46.723240 loss_att 104.468483 loss_ctc 46.132961 loss_rnnt 35.014153 hw_loss 0.447646 lr 0.00051941 rank 6
2023-02-22 22:57:34,873 DEBUG TRAIN Batch 11/900 loss 45.439915 loss_att 98.247528 loss_ctc 47.335472 loss_rnnt 34.376541 hw_loss 0.467081 lr 0.00051941 rank 1
2023-02-22 22:57:34,918 DEBUG TRAIN Batch 11/900 loss 65.645187 loss_att 123.898254 loss_ctc 66.732590 loss_rnnt 53.659904 hw_loss 0.355647 lr 0.00051941 rank 7
2023-02-22 22:58:53,506 DEBUG TRAIN Batch 11/1000 loss 57.135662 loss_att 118.151947 loss_ctc 54.637058 loss_rnnt 45.035271 hw_loss 0.431775 lr 0.00051913 rank 4
2023-02-22 22:58:53,506 DEBUG TRAIN Batch 11/1000 loss 50.227665 loss_att 113.362213 loss_ctc 46.573494 loss_rnnt 37.907684 hw_loss 0.338050 lr 0.00051913 rank 5
2023-02-22 22:58:53,511 DEBUG TRAIN Batch 11/1000 loss 46.171295 loss_att 102.166725 loss_ctc 44.339249 loss_rnnt 34.996304 hw_loss 0.412835 lr 0.00051913 rank 2
2023-02-22 22:58:53,513 DEBUG TRAIN Batch 11/1000 loss 79.555130 loss_att 143.990723 loss_ctc 81.668915 loss_rnnt 66.238632 hw_loss 0.276646 lr 0.00051913 rank 0
2023-02-22 22:58:53,514 DEBUG TRAIN Batch 11/1000 loss 51.146770 loss_att 121.671661 loss_ctc 46.771362 loss_rnnt 37.451347 hw_loss 0.325937 lr 0.00051913 rank 1
2023-02-22 22:58:53,522 DEBUG TRAIN Batch 11/1000 loss 47.925819 loss_att 105.330765 loss_ctc 52.810001 loss_rnnt 35.579285 hw_loss 0.401854 lr 0.00051913 rank 3
2023-02-22 22:58:53,534 DEBUG TRAIN Batch 11/1000 loss 57.595974 loss_att 119.966965 loss_ctc 57.798088 loss_rnnt 44.886074 hw_loss 0.391416 lr 0.00051913 rank 7
2023-02-22 22:58:53,561 DEBUG TRAIN Batch 11/1000 loss 62.152935 loss_att 117.925171 loss_ctc 67.202271 loss_rnnt 50.124878 hw_loss 0.375681 lr 0.00051913 rank 6
2023-02-22 23:00:12,522 DEBUG TRAIN Batch 11/1100 loss 38.995205 loss_att 59.225456 loss_ctc 35.518322 loss_rnnt 35.240582 hw_loss 0.322798 lr 0.00051885 rank 0
2023-02-22 23:00:12,526 DEBUG TRAIN Batch 11/1100 loss 28.047520 loss_att 61.754135 loss_ctc 25.852095 loss_rnnt 21.365576 hw_loss 0.437522 lr 0.00051885 rank 2
2023-02-22 23:00:12,526 DEBUG TRAIN Batch 11/1100 loss 53.278004 loss_att 97.439636 loss_ctc 47.000183 loss_rnnt 45.128414 hw_loss 0.289314 lr 0.00051885 rank 7
2023-02-22 23:00:12,527 DEBUG TRAIN Batch 11/1100 loss 39.864613 loss_att 74.108795 loss_ctc 37.771355 loss_rnnt 33.104515 hw_loss 0.356933 lr 0.00051885 rank 5
2023-02-22 23:00:12,529 DEBUG TRAIN Batch 11/1100 loss 44.875538 loss_att 79.644287 loss_ctc 48.636978 loss_rnnt 37.246597 hw_loss 0.325624 lr 0.00051885 rank 1
2023-02-22 23:00:12,530 DEBUG TRAIN Batch 11/1100 loss 60.324871 loss_att 93.351250 loss_ctc 67.125923 loss_rnnt 52.584160 hw_loss 0.428680 lr 0.00051885 rank 4
2023-02-22 23:00:12,541 DEBUG TRAIN Batch 11/1100 loss 41.936508 loss_att 85.331390 loss_ctc 37.561741 loss_rnnt 33.672249 hw_loss 0.316098 lr 0.00051885 rank 3
2023-02-22 23:00:12,577 DEBUG TRAIN Batch 11/1100 loss 64.250656 loss_att 100.730469 loss_ctc 60.575886 loss_rnnt 57.254150 hw_loss 0.357199 lr 0.00051885 rank 6
2023-02-22 23:01:30,318 DEBUG TRAIN Batch 11/1200 loss 39.308781 loss_att 58.763367 loss_ctc 45.329151 loss_rnnt 34.421722 hw_loss 0.362674 lr 0.00051857 rank 2
2023-02-22 23:01:30,321 DEBUG TRAIN Batch 11/1200 loss 44.249104 loss_att 66.200363 loss_ctc 49.336346 loss_rnnt 38.992382 hw_loss 0.352817 lr 0.00051857 rank 3
2023-02-22 23:01:30,322 DEBUG TRAIN Batch 11/1200 loss 39.396603 loss_att 69.595032 loss_ctc 39.737835 loss_rnnt 33.101269 hw_loss 0.394031 lr 0.00051857 rank 4
2023-02-22 23:01:30,322 DEBUG TRAIN Batch 11/1200 loss 24.530769 loss_att 41.217743 loss_ctc 31.373926 loss_rnnt 20.048595 hw_loss 0.435674 lr 0.00051857 rank 5
2023-02-22 23:01:30,326 DEBUG TRAIN Batch 11/1200 loss 43.127983 loss_att 68.260689 loss_ctc 46.927471 loss_rnnt 37.345440 hw_loss 0.467622 lr 0.00051857 rank 6
2023-02-22 23:01:30,325 DEBUG TRAIN Batch 11/1200 loss 30.571991 loss_att 55.960064 loss_ctc 36.733879 loss_rnnt 24.481907 hw_loss 0.357904 lr 0.00051857 rank 7
2023-02-22 23:01:30,326 DEBUG TRAIN Batch 11/1200 loss 44.462437 loss_att 62.366730 loss_ctc 51.467007 loss_rnnt 39.705769 hw_loss 0.453489 lr 0.00051857 rank 0
2023-02-22 23:01:30,328 DEBUG TRAIN Batch 11/1200 loss 36.649525 loss_att 58.528698 loss_ctc 41.339108 loss_rnnt 31.464911 hw_loss 0.344064 lr 0.00051857 rank 1
2023-02-22 23:02:48,107 DEBUG TRAIN Batch 11/1300 loss 41.399246 loss_att 83.817986 loss_ctc 41.627235 loss_rnnt 32.703499 hw_loss 0.340501 lr 0.00051829 rank 5
2023-02-22 23:02:48,108 DEBUG TRAIN Batch 11/1300 loss 46.166691 loss_att 94.655075 loss_ctc 51.141846 loss_rnnt 35.609241 hw_loss 0.368277 lr 0.00051829 rank 0
2023-02-22 23:02:48,110 DEBUG TRAIN Batch 11/1300 loss 34.914444 loss_att 74.179909 loss_ctc 30.712070 loss_rnnt 27.402142 hw_loss 0.411611 lr 0.00051829 rank 2
2023-02-22 23:02:48,113 DEBUG TRAIN Batch 11/1300 loss 22.358309 loss_att 27.765541 loss_ctc 23.085693 loss_rnnt 20.923828 hw_loss 0.480094 lr 0.00051829 rank 6
2023-02-22 23:02:48,114 DEBUG TRAIN Batch 11/1300 loss 50.056313 loss_att 94.882858 loss_ctc 50.310051 loss_rnnt 40.901196 hw_loss 0.292456 lr 0.00051829 rank 4
2023-02-22 23:02:48,114 DEBUG TRAIN Batch 11/1300 loss 52.195812 loss_att 106.399132 loss_ctc 51.970757 loss_rnnt 41.184196 hw_loss 0.376795 lr 0.00051829 rank 3
2023-02-22 23:02:48,116 DEBUG TRAIN Batch 11/1300 loss 58.731133 loss_att 103.404305 loss_ctc 57.111126 loss_rnnt 49.842697 hw_loss 0.318375 lr 0.00051829 rank 1
2023-02-22 23:02:48,119 DEBUG TRAIN Batch 11/1300 loss 26.119505 loss_att 35.147629 loss_ctc 28.164835 loss_rnnt 23.721807 hw_loss 0.598808 lr 0.00051829 rank 7
2023-02-22 23:04:06,759 DEBUG TRAIN Batch 11/1400 loss 29.638477 loss_att 55.186573 loss_ctc 37.878365 loss_rnnt 23.254017 hw_loss 0.330351 lr 0.00051802 rank 2
2023-02-22 23:04:06,763 DEBUG TRAIN Batch 11/1400 loss 63.005756 loss_att 114.643494 loss_ctc 64.966736 loss_rnnt 52.204567 hw_loss 0.397821 lr 0.00051802 rank 4
2023-02-22 23:04:06,769 DEBUG TRAIN Batch 11/1400 loss 34.074509 loss_att 59.892555 loss_ctc 34.758430 loss_rnnt 28.618896 hw_loss 0.376517 lr 0.00051802 rank 0
2023-02-22 23:04:06,769 DEBUG TRAIN Batch 11/1400 loss 80.045723 loss_att 128.775177 loss_ctc 74.596855 loss_rnnt 70.824615 hw_loss 0.378237 lr 0.00051802 rank 3
2023-02-22 23:04:06,769 DEBUG TRAIN Batch 11/1400 loss 35.678463 loss_att 65.911186 loss_ctc 32.060970 loss_rnnt 29.946594 hw_loss 0.314361 lr 0.00051802 rank 5
2023-02-22 23:04:06,777 DEBUG TRAIN Batch 11/1400 loss 35.228100 loss_att 65.746559 loss_ctc 40.569946 loss_rnnt 28.236012 hw_loss 0.330273 lr 0.00051802 rank 6
2023-02-22 23:04:06,779 DEBUG TRAIN Batch 11/1400 loss 54.474045 loss_att 81.989288 loss_ctc 61.089996 loss_rnnt 47.898754 hw_loss 0.356463 lr 0.00051802 rank 1
2023-02-22 23:04:06,818 DEBUG TRAIN Batch 11/1400 loss 27.865353 loss_att 68.183243 loss_ctc 25.571890 loss_rnnt 19.936764 hw_loss 0.320261 lr 0.00051802 rank 7
2023-02-22 23:05:25,502 DEBUG TRAIN Batch 11/1500 loss 36.556091 loss_att 54.526154 loss_ctc 31.768936 loss_rnnt 33.391747 hw_loss 0.391162 lr 0.00051774 rank 7
2023-02-22 23:05:25,503 DEBUG TRAIN Batch 11/1500 loss 28.582500 loss_att 57.765881 loss_ctc 26.260918 loss_rnnt 22.825821 hw_loss 0.430404 lr 0.00051774 rank 5
2023-02-22 23:05:25,505 DEBUG TRAIN Batch 11/1500 loss 38.078079 loss_att 59.962219 loss_ctc 38.290176 loss_rnnt 33.466621 hw_loss 0.386905 lr 0.00051774 rank 0
2023-02-22 23:05:25,508 DEBUG TRAIN Batch 11/1500 loss 20.996986 loss_att 37.761211 loss_ctc 19.289331 loss_rnnt 17.661774 hw_loss 0.393855 lr 0.00051774 rank 4
2023-02-22 23:05:25,511 DEBUG TRAIN Batch 11/1500 loss 43.760189 loss_att 67.077065 loss_ctc 56.043083 loss_rnnt 37.261887 hw_loss 0.369765 lr 0.00051774 rank 6
2023-02-22 23:05:25,512 DEBUG TRAIN Batch 11/1500 loss 67.142853 loss_att 111.245811 loss_ctc 67.496536 loss_rnnt 58.043945 hw_loss 0.433431 lr 0.00051774 rank 2
2023-02-22 23:05:25,525 DEBUG TRAIN Batch 11/1500 loss 20.551067 loss_att 39.834080 loss_ctc 21.038767 loss_rnnt 16.473076 hw_loss 0.293180 lr 0.00051774 rank 1
2023-02-22 23:05:25,554 DEBUG TRAIN Batch 11/1500 loss 39.847656 loss_att 64.409256 loss_ctc 38.223595 loss_rnnt 34.951591 hw_loss 0.375540 lr 0.00051774 rank 3
2023-02-22 23:06:42,512 DEBUG TRAIN Batch 11/1600 loss 37.872211 loss_att 68.551254 loss_ctc 38.031681 loss_rnnt 31.567785 hw_loss 0.276291 lr 0.00051746 rank 2
2023-02-22 23:06:42,515 DEBUG TRAIN Batch 11/1600 loss 27.523510 loss_att 44.666801 loss_ctc 30.631905 loss_rnnt 23.468971 hw_loss 0.396427 lr 0.00051746 rank 4
2023-02-22 23:06:42,519 DEBUG TRAIN Batch 11/1600 loss 37.819122 loss_att 51.842327 loss_ctc 40.138283 loss_rnnt 34.537880 hw_loss 0.313838 lr 0.00051746 rank 5
2023-02-22 23:06:42,522 DEBUG TRAIN Batch 11/1600 loss 37.602562 loss_att 61.978088 loss_ctc 43.912003 loss_rnnt 31.699884 hw_loss 0.349332 lr 0.00051746 rank 0
2023-02-22 23:06:42,524 DEBUG TRAIN Batch 11/1600 loss 55.773548 loss_att 84.019707 loss_ctc 65.370956 loss_rnnt 48.693176 hw_loss 0.284034 lr 0.00051746 rank 6
2023-02-22 23:06:42,526 DEBUG TRAIN Batch 11/1600 loss 28.655582 loss_att 46.744961 loss_ctc 36.465443 loss_rnnt 23.824699 hw_loss 0.321920 lr 0.00051746 rank 3
2023-02-22 23:06:42,526 DEBUG TRAIN Batch 11/1600 loss 33.899963 loss_att 55.724010 loss_ctc 38.332027 loss_rnnt 28.740366 hw_loss 0.382211 lr 0.00051746 rank 7
2023-02-22 23:06:42,530 DEBUG TRAIN Batch 11/1600 loss 43.192383 loss_att 70.264343 loss_ctc 44.123047 loss_rnnt 37.487858 hw_loss 0.311333 lr 0.00051746 rank 1
2023-02-22 23:07:59,001 DEBUG TRAIN Batch 11/1700 loss 39.431072 loss_att 51.653214 loss_ctc 40.213516 loss_rnnt 36.686790 hw_loss 0.366614 lr 0.00051718 rank 5
2023-02-22 23:07:59,001 DEBUG TRAIN Batch 11/1700 loss 16.318811 loss_att 34.282207 loss_ctc 17.190996 loss_rnnt 12.411621 hw_loss 0.371663 lr 0.00051718 rank 7
2023-02-22 23:07:59,002 DEBUG TRAIN Batch 11/1700 loss 20.339790 loss_att 35.234364 loss_ctc 20.970459 loss_rnnt 17.072649 hw_loss 0.382760 lr 0.00051718 rank 0
2023-02-22 23:07:59,003 DEBUG TRAIN Batch 11/1700 loss 26.885231 loss_att 42.035011 loss_ctc 24.370701 loss_rnnt 24.020088 hw_loss 0.319603 lr 0.00051718 rank 6
2023-02-22 23:07:59,005 DEBUG TRAIN Batch 11/1700 loss 38.871967 loss_att 57.882809 loss_ctc 42.113621 loss_rnnt 34.464119 hw_loss 0.325228 lr 0.00051718 rank 4
2023-02-22 23:07:59,005 DEBUG TRAIN Batch 11/1700 loss 34.158043 loss_att 50.859909 loss_ctc 30.339096 loss_rnnt 31.162785 hw_loss 0.307648 lr 0.00051718 rank 1
2023-02-22 23:07:59,005 DEBUG TRAIN Batch 11/1700 loss 49.011799 loss_att 75.271713 loss_ctc 48.678696 loss_rnnt 43.585590 hw_loss 0.409949 lr 0.00051718 rank 3
2023-02-22 23:07:59,009 DEBUG TRAIN Batch 11/1700 loss 31.983028 loss_att 53.149517 loss_ctc 30.429249 loss_rnnt 27.797827 hw_loss 0.298269 lr 0.00051718 rank 2
2023-02-22 23:09:18,934 DEBUG TRAIN Batch 11/1800 loss 43.335800 loss_att 69.038689 loss_ctc 47.977749 loss_rnnt 37.408676 hw_loss 0.314284 lr 0.00051691 rank 2
2023-02-22 23:09:18,938 DEBUG TRAIN Batch 11/1800 loss 37.950966 loss_att 54.797840 loss_ctc 41.896278 loss_rnnt 33.831676 hw_loss 0.419763 lr 0.00051691 rank 3
2023-02-22 23:09:18,938 DEBUG TRAIN Batch 11/1800 loss 31.030338 loss_att 43.852562 loss_ctc 35.053902 loss_rnnt 27.691189 hw_loss 0.446680 lr 0.00051691 rank 5
2023-02-22 23:09:18,939 DEBUG TRAIN Batch 11/1800 loss 45.372841 loss_att 60.909714 loss_ctc 44.981739 loss_rnnt 42.127754 hw_loss 0.355983 lr 0.00051691 rank 0
2023-02-22 23:09:18,938 DEBUG TRAIN Batch 11/1800 loss 44.558640 loss_att 62.000034 loss_ctc 51.021133 loss_rnnt 40.058289 hw_loss 0.282012 lr 0.00051691 rank 6
2023-02-22 23:09:18,939 DEBUG TRAIN Batch 11/1800 loss 32.468727 loss_att 44.699005 loss_ctc 36.360756 loss_rnnt 29.380026 hw_loss 0.231950 lr 0.00051691 rank 4
2023-02-22 23:09:18,939 DEBUG TRAIN Batch 11/1800 loss 37.578247 loss_att 48.037659 loss_ctc 45.463936 loss_rnnt 34.184280 hw_loss 0.469980 lr 0.00051691 rank 7
2023-02-22 23:09:18,944 DEBUG TRAIN Batch 11/1800 loss 29.229633 loss_att 46.365879 loss_ctc 30.733826 loss_rnnt 25.398872 hw_loss 0.380536 lr 0.00051691 rank 1
2023-02-22 23:10:36,678 DEBUG TRAIN Batch 11/1900 loss 23.795120 loss_att 30.301031 loss_ctc 27.504072 loss_rnnt 21.773005 hw_loss 0.424508 lr 0.00051663 rank 4
2023-02-22 23:10:36,680 DEBUG TRAIN Batch 11/1900 loss 15.177003 loss_att 21.229717 loss_ctc 13.974909 loss_rnnt 13.900002 hw_loss 0.425133 lr 0.00051663 rank 6
2023-02-22 23:10:36,683 DEBUG TRAIN Batch 11/1900 loss 23.908470 loss_att 24.046543 loss_ctc 26.043238 loss_rnnt 23.360855 hw_loss 0.441306 lr 0.00051663 rank 0
2023-02-22 23:10:36,684 DEBUG TRAIN Batch 11/1900 loss 20.397026 loss_att 27.117226 loss_ctc 21.989443 loss_rnnt 18.674679 hw_loss 0.311220 lr 0.00051663 rank 2
2023-02-22 23:10:36,684 DEBUG TRAIN Batch 11/1900 loss 16.296129 loss_att 22.227135 loss_ctc 21.546801 loss_rnnt 14.194291 hw_loss 0.404149 lr 0.00051663 rank 5
2023-02-22 23:10:36,686 DEBUG TRAIN Batch 11/1900 loss 17.311882 loss_att 18.665409 loss_ctc 17.700802 loss_rnnt 16.728851 hw_loss 0.488375 lr 0.00051663 rank 3
2023-02-22 23:10:36,688 DEBUG TRAIN Batch 11/1900 loss 17.255346 loss_att 22.771982 loss_ctc 17.534431 loss_rnnt 15.889472 hw_loss 0.422506 lr 0.00051663 rank 7
2023-02-22 23:10:36,691 DEBUG TRAIN Batch 11/1900 loss 34.623600 loss_att 41.291546 loss_ctc 33.764652 loss_rnnt 33.208229 hw_loss 0.368081 lr 0.00051663 rank 1
2023-02-22 23:11:52,016 DEBUG TRAIN Batch 11/2000 loss 67.325752 loss_att 89.650589 loss_ctc 69.814957 loss_rnnt 62.351250 hw_loss 0.333089 lr 0.00051636 rank 4
2023-02-22 23:11:52,021 DEBUG TRAIN Batch 11/2000 loss 23.432911 loss_att 36.438122 loss_ctc 21.792343 loss_rnnt 20.841331 hw_loss 0.392398 lr 0.00051636 rank 5
2023-02-22 23:11:52,022 DEBUG TRAIN Batch 11/2000 loss 44.265274 loss_att 70.989853 loss_ctc 45.890087 loss_rnnt 38.556778 hw_loss 0.275506 lr 0.00051636 rank 3
2023-02-22 23:11:52,022 DEBUG TRAIN Batch 11/2000 loss 51.850082 loss_att 52.919098 loss_ctc 54.137917 loss_rnnt 51.158947 hw_loss 0.323041 lr 0.00051636 rank 7
2023-02-22 23:11:52,024 DEBUG TRAIN Batch 11/2000 loss 53.414234 loss_att 70.921341 loss_ctc 62.714317 loss_rnnt 48.540031 hw_loss 0.248947 lr 0.00051636 rank 2
2023-02-22 23:11:52,025 DEBUG TRAIN Batch 11/2000 loss 19.305281 loss_att 31.574747 loss_ctc 22.484549 loss_rnnt 16.270298 hw_loss 0.294723 lr 0.00051636 rank 1
2023-02-22 23:11:52,025 DEBUG TRAIN Batch 11/2000 loss 33.291946 loss_att 46.615616 loss_ctc 37.643394 loss_rnnt 29.810337 hw_loss 0.443775 lr 0.00051636 rank 0
2023-02-22 23:11:52,032 DEBUG TRAIN Batch 11/2000 loss 29.213226 loss_att 46.548836 loss_ctc 35.989269 loss_rnnt 24.694000 hw_loss 0.278685 lr 0.00051636 rank 6
2023-02-22 23:13:09,591 DEBUG TRAIN Batch 11/2100 loss 60.245728 loss_att 79.861084 loss_ctc 64.098404 loss_rnnt 55.646877 hw_loss 0.303919 lr 0.00051608 rank 6
2023-02-22 23:13:09,592 DEBUG TRAIN Batch 11/2100 loss 22.189997 loss_att 27.205330 loss_ctc 20.482016 loss_rnnt 21.255943 hw_loss 0.297601 lr 0.00051608 rank 5
2023-02-22 23:13:09,592 DEBUG TRAIN Batch 11/2100 loss 57.588078 loss_att 100.523224 loss_ctc 59.190384 loss_rnnt 48.631409 hw_loss 0.292499 lr 0.00051608 rank 2
2023-02-22 23:13:09,597 DEBUG TRAIN Batch 11/2100 loss 33.999836 loss_att 53.426361 loss_ctc 36.697514 loss_rnnt 29.602596 hw_loss 0.285467 lr 0.00051608 rank 0
2023-02-22 23:13:09,597 DEBUG TRAIN Batch 11/2100 loss 49.524738 loss_att 70.782959 loss_ctc 47.591156 loss_rnnt 45.357891 hw_loss 0.324398 lr 0.00051608 rank 7
2023-02-22 23:13:09,597 DEBUG TRAIN Batch 11/2100 loss 30.977858 loss_att 40.037254 loss_ctc 37.485184 loss_rnnt 28.112850 hw_loss 0.347785 lr 0.00051608 rank 4
2023-02-22 23:13:09,599 DEBUG TRAIN Batch 11/2100 loss 35.622936 loss_att 50.527702 loss_ctc 38.269440 loss_rnnt 32.096340 hw_loss 0.361448 lr 0.00051608 rank 3
2023-02-22 23:13:09,602 DEBUG TRAIN Batch 11/2100 loss 34.168266 loss_att 43.652622 loss_ctc 40.770409 loss_rnnt 31.145306 hw_loss 0.460879 lr 0.00051608 rank 1
2023-02-22 23:14:27,258 DEBUG TRAIN Batch 11/2200 loss 15.605316 loss_att 28.017769 loss_ctc 15.847408 loss_rnnt 12.919937 hw_loss 0.319893 lr 0.00051581 rank 2
2023-02-22 23:14:27,260 DEBUG TRAIN Batch 11/2200 loss 33.484299 loss_att 46.361855 loss_ctc 26.926880 loss_rnnt 31.581308 hw_loss 0.378382 lr 0.00051581 rank 5
2023-02-22 23:14:27,260 DEBUG TRAIN Batch 11/2200 loss 23.087631 loss_att 33.968937 loss_ctc 28.125525 loss_rnnt 20.085173 hw_loss 0.289651 lr 0.00051581 rank 0
2023-02-22 23:14:27,263 DEBUG TRAIN Batch 11/2200 loss 21.748758 loss_att 34.580399 loss_ctc 24.731411 loss_rnnt 18.639301 hw_loss 0.272704 lr 0.00051581 rank 4
2023-02-22 23:14:27,264 DEBUG TRAIN Batch 11/2200 loss 42.150822 loss_att 49.694695 loss_ctc 56.113300 loss_rnnt 38.592953 hw_loss 0.351431 lr 0.00051581 rank 7
2023-02-22 23:14:27,267 DEBUG TRAIN Batch 11/2200 loss 22.437193 loss_att 29.920931 loss_ctc 30.204185 loss_rnnt 19.715794 hw_loss 0.354471 lr 0.00051581 rank 3
2023-02-22 23:14:27,268 DEBUG TRAIN Batch 11/2200 loss 35.132309 loss_att 59.005173 loss_ctc 36.409843 loss_rnnt 30.038610 hw_loss 0.278974 lr 0.00051581 rank 6
2023-02-22 23:14:27,269 DEBUG TRAIN Batch 11/2200 loss 35.772308 loss_att 41.300858 loss_ctc 42.600189 loss_rnnt 33.583473 hw_loss 0.323883 lr 0.00051581 rank 1
2023-02-22 23:15:42,259 DEBUG TRAIN Batch 11/2300 loss 27.948490 loss_att 40.716415 loss_ctc 33.012138 loss_rnnt 24.515783 hw_loss 0.382441 lr 0.00051553 rank 5
2023-02-22 23:15:42,264 DEBUG TRAIN Batch 11/2300 loss 38.405331 loss_att 58.348743 loss_ctc 47.364952 loss_rnnt 33.074745 hw_loss 0.276155 lr 0.00051553 rank 4
2023-02-22 23:15:42,266 DEBUG TRAIN Batch 11/2300 loss 25.299675 loss_att 40.441780 loss_ctc 25.445301 loss_rnnt 22.068277 hw_loss 0.344174 lr 0.00051553 rank 0
2023-02-22 23:15:42,267 DEBUG TRAIN Batch 11/2300 loss 15.713061 loss_att 22.049076 loss_ctc 15.906836 loss_rnnt 14.206093 hw_loss 0.401115 lr 0.00051553 rank 7
2023-02-22 23:15:42,267 DEBUG TRAIN Batch 11/2300 loss 19.713377 loss_att 33.603325 loss_ctc 23.092190 loss_rnnt 16.247181 hw_loss 0.445683 lr 0.00051553 rank 1
2023-02-22 23:15:42,268 DEBUG TRAIN Batch 11/2300 loss 36.917953 loss_att 56.114864 loss_ctc 43.090073 loss_rnnt 32.095779 hw_loss 0.299705 lr 0.00051553 rank 3
2023-02-22 23:15:42,269 DEBUG TRAIN Batch 11/2300 loss 22.442242 loss_att 33.568207 loss_ctc 27.283533 loss_rnnt 19.335770 hw_loss 0.442077 lr 0.00051553 rank 6
2023-02-22 23:15:42,270 DEBUG TRAIN Batch 11/2300 loss 22.120623 loss_att 35.758896 loss_ctc 27.212561 loss_rnnt 18.561026 hw_loss 0.286906 lr 0.00051553 rank 2
2023-02-22 23:16:58,530 DEBUG TRAIN Batch 11/2400 loss 19.379820 loss_att 29.225002 loss_ctc 20.391794 loss_rnnt 17.045315 hw_loss 0.432257 lr 0.00051526 rank 3
2023-02-22 23:16:58,535 DEBUG TRAIN Batch 11/2400 loss 27.031006 loss_att 40.931480 loss_ctc 32.181622 loss_rnnt 23.386124 hw_loss 0.333818 lr 0.00051526 rank 0
2023-02-22 23:16:58,536 DEBUG TRAIN Batch 11/2400 loss 13.351719 loss_att 27.564167 loss_ctc 12.262165 loss_rnnt 10.445201 hw_loss 0.392442 lr 0.00051526 rank 5
2023-02-22 23:16:58,538 DEBUG TRAIN Batch 11/2400 loss 31.301785 loss_att 36.680115 loss_ctc 34.407085 loss_rnnt 29.653976 hw_loss 0.296439 lr 0.00051526 rank 6
2023-02-22 23:16:58,538 DEBUG TRAIN Batch 11/2400 loss 33.775356 loss_att 52.406830 loss_ctc 40.072685 loss_rnnt 29.024088 hw_loss 0.347490 lr 0.00051526 rank 2
2023-02-22 23:16:58,538 DEBUG TRAIN Batch 11/2400 loss 26.251095 loss_att 31.573837 loss_ctc 32.424347 loss_rnnt 24.145430 hw_loss 0.408778 lr 0.00051526 rank 4
2023-02-22 23:16:58,545 DEBUG TRAIN Batch 11/2400 loss 18.943300 loss_att 29.774174 loss_ctc 20.624289 loss_rnnt 16.416965 hw_loss 0.255057 lr 0.00051526 rank 7
2023-02-22 23:16:58,590 DEBUG TRAIN Batch 11/2400 loss 29.326263 loss_att 42.509617 loss_ctc 29.911282 loss_rnnt 26.429794 hw_loss 0.340865 lr 0.00051526 rank 1
2023-02-22 23:18:18,015 DEBUG TRAIN Batch 11/2500 loss 22.441948 loss_att 24.349621 loss_ctc 24.614090 loss_rnnt 21.544394 hw_loss 0.424503 lr 0.00051499 rank 5
2023-02-22 23:18:18,020 DEBUG TRAIN Batch 11/2500 loss 16.472055 loss_att 21.763315 loss_ctc 18.159050 loss_rnnt 14.974673 hw_loss 0.401620 lr 0.00051499 rank 3
2023-02-22 23:18:18,020 DEBUG TRAIN Batch 11/2500 loss 27.617294 loss_att 33.344818 loss_ctc 28.401457 loss_rnnt 26.109751 hw_loss 0.482783 lr 0.00051499 rank 4
2023-02-22 23:18:18,021 DEBUG TRAIN Batch 11/2500 loss 19.656176 loss_att 27.940201 loss_ctc 23.505074 loss_rnnt 17.317612 hw_loss 0.316070 lr 0.00051499 rank 1
2023-02-22 23:18:18,021 DEBUG TRAIN Batch 11/2500 loss 24.704821 loss_att 29.958511 loss_ctc 26.912830 loss_rnnt 23.185305 hw_loss 0.326956 lr 0.00051499 rank 2
2023-02-22 23:18:18,021 DEBUG TRAIN Batch 11/2500 loss 21.233864 loss_att 28.450653 loss_ctc 23.808388 loss_rnnt 19.280987 hw_loss 0.311719 lr 0.00051499 rank 7
2023-02-22 23:18:18,023 DEBUG TRAIN Batch 11/2500 loss 20.867939 loss_att 26.354713 loss_ctc 23.286821 loss_rnnt 19.211880 hw_loss 0.442850 lr 0.00051499 rank 0
2023-02-22 23:18:18,024 DEBUG TRAIN Batch 11/2500 loss 23.524582 loss_att 30.494152 loss_ctc 24.809422 loss_rnnt 21.745335 hw_loss 0.401288 lr 0.00051499 rank 6
2023-02-22 23:19:31,381 DEBUG TRAIN Batch 11/2600 loss 26.782459 loss_att 35.358971 loss_ctc 31.114605 loss_rnnt 24.263103 hw_loss 0.424566 lr 0.00051471 rank 2
2023-02-22 23:19:31,381 DEBUG TRAIN Batch 11/2600 loss 42.577946 loss_att 50.703903 loss_ctc 42.584194 loss_rnnt 40.760921 hw_loss 0.358120 lr 0.00051471 rank 5
2023-02-22 23:19:31,384 DEBUG TRAIN Batch 11/2600 loss 18.453745 loss_att 30.087265 loss_ctc 11.877432 loss_rnnt 16.866446 hw_loss 0.257696 lr 0.00051471 rank 6
2023-02-22 23:19:31,385 DEBUG TRAIN Batch 11/2600 loss 52.135017 loss_att 61.484554 loss_ctc 65.697960 loss_rnnt 48.325005 hw_loss 0.246965 lr 0.00051471 rank 4
2023-02-22 23:19:31,386 DEBUG TRAIN Batch 11/2600 loss 28.632517 loss_att 36.235588 loss_ctc 28.280727 loss_rnnt 26.934391 hw_loss 0.420775 lr 0.00051471 rank 0
2023-02-22 23:19:31,389 DEBUG TRAIN Batch 11/2600 loss 17.413122 loss_att 37.748970 loss_ctc 16.018612 loss_rnnt 13.360634 hw_loss 0.321099 lr 0.00051471 rank 3
2023-02-22 23:19:31,390 DEBUG TRAIN Batch 11/2600 loss 23.259693 loss_att 30.744934 loss_ctc 32.409149 loss_rnnt 20.372267 hw_loss 0.319598 lr 0.00051471 rank 7
2023-02-22 23:19:31,391 DEBUG TRAIN Batch 11/2600 loss 6.174541 loss_att 13.849514 loss_ctc 6.991752 loss_rnnt 4.296354 hw_loss 0.439182 lr 0.00051471 rank 1
2023-02-22 23:20:46,718 DEBUG TRAIN Batch 11/2700 loss 34.511948 loss_att 51.274281 loss_ctc 37.102058 loss_rnnt 30.592270 hw_loss 0.415993 lr 0.00051444 rank 3
2023-02-22 23:20:46,718 DEBUG TRAIN Batch 11/2700 loss 14.138725 loss_att 19.933609 loss_ctc 18.024973 loss_rnnt 12.250092 hw_loss 0.396544 lr 0.00051444 rank 5
2023-02-22 23:20:46,719 DEBUG TRAIN Batch 11/2700 loss 58.402786 loss_att 78.368820 loss_ctc 61.393776 loss_rnnt 53.831497 hw_loss 0.336148 lr 0.00051444 rank 4
2023-02-22 23:20:46,723 DEBUG TRAIN Batch 11/2700 loss 21.831640 loss_att 25.601364 loss_ctc 26.462299 loss_rnnt 20.259140 hw_loss 0.377127 lr 0.00051444 rank 1
2023-02-22 23:20:46,725 DEBUG TRAIN Batch 11/2700 loss 18.849319 loss_att 37.119255 loss_ctc 19.843689 loss_rnnt 14.899429 hw_loss 0.306224 lr 0.00051444 rank 6
2023-02-22 23:20:46,726 DEBUG TRAIN Batch 11/2700 loss 36.191120 loss_att 47.388252 loss_ctc 37.728085 loss_rnnt 33.577286 hw_loss 0.317775 lr 0.00051444 rank 0
2023-02-22 23:20:46,726 DEBUG TRAIN Batch 11/2700 loss 29.112389 loss_att 40.988377 loss_ctc 36.005077 loss_rnnt 25.656363 hw_loss 0.303382 lr 0.00051444 rank 2
2023-02-22 23:20:46,729 DEBUG TRAIN Batch 11/2700 loss 26.303997 loss_att 35.210659 loss_ctc 25.341160 loss_rnnt 24.518410 hw_loss 0.248686 lr 0.00051444 rank 7
2023-02-22 23:22:04,233 DEBUG TRAIN Batch 11/2800 loss 18.683769 loss_att 32.597549 loss_ctc 23.923084 loss_rnnt 15.046437 hw_loss 0.292502 lr 0.00051417 rank 7
2023-02-22 23:22:04,233 DEBUG TRAIN Batch 11/2800 loss 20.052416 loss_att 36.120403 loss_ctc 19.089556 loss_rnnt 16.776978 hw_loss 0.356664 lr 0.00051417 rank 3
2023-02-22 23:22:04,235 DEBUG TRAIN Batch 11/2800 loss 46.088531 loss_att 58.624962 loss_ctc 56.237286 loss_rnnt 42.028744 hw_loss 0.373745 lr 0.00051417 rank 0
2023-02-22 23:22:04,238 DEBUG TRAIN Batch 11/2800 loss 24.594492 loss_att 33.358116 loss_ctc 26.381325 loss_rnnt 22.409628 hw_loss 0.363555 lr 0.00051417 rank 1
2023-02-22 23:22:04,239 DEBUG TRAIN Batch 11/2800 loss 21.735733 loss_att 33.223728 loss_ctc 27.164921 loss_rnnt 18.543344 hw_loss 0.320435 lr 0.00051417 rank 4
2023-02-22 23:22:04,241 DEBUG TRAIN Batch 11/2800 loss 20.943565 loss_att 32.975666 loss_ctc 22.565441 loss_rnnt 18.162865 hw_loss 0.296306 lr 0.00051417 rank 6
2023-02-22 23:22:04,255 DEBUG TRAIN Batch 11/2800 loss 16.140444 loss_att 28.394917 loss_ctc 19.372650 loss_rnnt 13.096054 hw_loss 0.304750 lr 0.00051417 rank 2
2023-02-22 23:22:04,267 DEBUG TRAIN Batch 11/2800 loss 30.585548 loss_att 40.571732 loss_ctc 32.304161 loss_rnnt 28.164940 hw_loss 0.364171 lr 0.00051417 rank 5
2023-02-22 23:23:21,518 DEBUG TRAIN Batch 11/2900 loss 10.338498 loss_att 19.426535 loss_ctc 10.028154 loss_rnnt 8.372570 hw_loss 0.355688 lr 0.00051390 rank 5
2023-02-22 23:23:21,519 DEBUG TRAIN Batch 11/2900 loss 29.555670 loss_att 45.654991 loss_ctc 30.601114 loss_rnnt 26.040276 hw_loss 0.292755 lr 0.00051390 rank 2
2023-02-22 23:23:21,520 DEBUG TRAIN Batch 11/2900 loss 35.517059 loss_att 50.394958 loss_ctc 38.133652 loss_rnnt 32.001045 hw_loss 0.359161 lr 0.00051390 rank 0
2023-02-22 23:23:21,521 DEBUG TRAIN Batch 11/2900 loss 19.328089 loss_att 33.166527 loss_ctc 19.283379 loss_rnnt 16.387447 hw_loss 0.335463 lr 0.00051390 rank 6
2023-02-22 23:23:21,523 DEBUG TRAIN Batch 11/2900 loss 25.124008 loss_att 34.179977 loss_ctc 26.717999 loss_rnnt 22.920994 hw_loss 0.336158 lr 0.00051390 rank 7
2023-02-22 23:23:21,527 DEBUG TRAIN Batch 11/2900 loss 37.319584 loss_att 53.261269 loss_ctc 35.972923 loss_rnnt 34.112850 hw_loss 0.371158 lr 0.00051390 rank 4
2023-02-22 23:23:21,528 DEBUG TRAIN Batch 11/2900 loss 37.745499 loss_att 58.959423 loss_ctc 42.018631 loss_rnnt 32.800510 hw_loss 0.248349 lr 0.00051390 rank 1
2023-02-22 23:23:21,569 DEBUG TRAIN Batch 11/2900 loss 48.928776 loss_att 56.440945 loss_ctc 60.863174 loss_rnnt 45.668125 hw_loss 0.313055 lr 0.00051390 rank 3
2023-02-22 23:24:37,837 DEBUG TRAIN Batch 11/3000 loss 17.186010 loss_att 29.110655 loss_ctc 16.665207 loss_rnnt 14.689077 hw_loss 0.340209 lr 0.00051362 rank 4
2023-02-22 23:24:37,843 DEBUG TRAIN Batch 11/3000 loss 18.158659 loss_att 26.496906 loss_ctc 18.132080 loss_rnnt 16.301434 hw_loss 0.362100 lr 0.00051362 rank 3
2023-02-22 23:24:37,844 DEBUG TRAIN Batch 11/3000 loss 40.596149 loss_att 54.928104 loss_ctc 47.988731 loss_rnnt 36.591675 hw_loss 0.285755 lr 0.00051362 rank 0
2023-02-22 23:24:37,845 DEBUG TRAIN Batch 11/3000 loss 28.823229 loss_att 43.964787 loss_ctc 23.749641 loss_rnnt 26.296768 hw_loss 0.327423 lr 0.00051362 rank 2
2023-02-22 23:24:37,845 DEBUG TRAIN Batch 11/3000 loss 28.710648 loss_att 37.427689 loss_ctc 35.761162 loss_rnnt 25.800632 hw_loss 0.424756 lr 0.00051362 rank 7
2023-02-22 23:24:37,847 DEBUG TRAIN Batch 11/3000 loss 28.250031 loss_att 38.498688 loss_ctc 27.086924 loss_rnnt 26.160423 hw_loss 0.365542 lr 0.00051362 rank 5
2023-02-22 23:24:37,854 DEBUG TRAIN Batch 11/3000 loss 19.761322 loss_att 25.289913 loss_ctc 21.086941 loss_rnnt 18.248932 hw_loss 0.431104 lr 0.00051362 rank 1
2023-02-22 23:24:37,887 DEBUG TRAIN Batch 11/3000 loss 21.215872 loss_att 25.748478 loss_ctc 25.843605 loss_rnnt 19.502888 hw_loss 0.355182 lr 0.00051362 rank 6
2023-02-22 23:25:53,211 DEBUG TRAIN Batch 11/3100 loss 33.092125 loss_att 38.638885 loss_ctc 35.196289 loss_rnnt 31.525549 hw_loss 0.331258 lr 0.00051335 rank 6
2023-02-22 23:25:53,212 DEBUG TRAIN Batch 11/3100 loss 15.794736 loss_att 22.666582 loss_ctc 17.217884 loss_rnnt 14.048839 hw_loss 0.340830 lr 0.00051335 rank 5
2023-02-22 23:25:53,212 DEBUG TRAIN Batch 11/3100 loss 13.346813 loss_att 20.642393 loss_ctc 12.699824 loss_rnnt 11.780879 hw_loss 0.362031 lr 0.00051335 rank 4
2023-02-22 23:25:53,213 DEBUG TRAIN Batch 11/3100 loss 17.114815 loss_att 24.318048 loss_ctc 16.759384 loss_rnnt 15.565248 hw_loss 0.293080 lr 0.00051335 rank 2
2023-02-22 23:25:53,215 DEBUG TRAIN Batch 11/3100 loss 23.217426 loss_att 33.113037 loss_ctc 24.828012 loss_rnnt 20.896231 hw_loss 0.238744 lr 0.00051335 rank 0
2023-02-22 23:25:53,215 DEBUG TRAIN Batch 11/3100 loss 12.552824 loss_att 17.859978 loss_ctc 14.928215 loss_rnnt 10.988563 hw_loss 0.348958 lr 0.00051335 rank 1
2023-02-22 23:25:53,216 DEBUG TRAIN Batch 11/3100 loss 22.046972 loss_att 29.308271 loss_ctc 26.484320 loss_rnnt 19.811777 hw_loss 0.358669 lr 0.00051335 rank 7
2023-02-22 23:25:53,220 DEBUG TRAIN Batch 11/3100 loss 21.813272 loss_att 34.609993 loss_ctc 24.147120 loss_rnnt 18.737686 hw_loss 0.384494 lr 0.00051335 rank 3
2023-02-22 23:27:12,303 DEBUG TRAIN Batch 11/3200 loss 26.135988 loss_att 30.752354 loss_ctc 33.793846 loss_rnnt 24.051785 hw_loss 0.262286 lr 0.00051308 rank 4
2023-02-22 23:27:12,308 DEBUG TRAIN Batch 11/3200 loss 21.154533 loss_att 23.827198 loss_ctc 24.442909 loss_rnnt 19.957293 hw_loss 0.420484 lr 0.00051308 rank 0
2023-02-22 23:27:12,318 DEBUG TRAIN Batch 11/3200 loss 31.834526 loss_att 44.190388 loss_ctc 35.772709 loss_rnnt 28.683027 hw_loss 0.291068 lr 0.00051308 rank 7
2023-02-22 23:27:12,341 DEBUG TRAIN Batch 11/3200 loss 50.915489 loss_att 67.274635 loss_ctc 50.361248 loss_rnnt 47.527710 hw_loss 0.355973 lr 0.00051308 rank 5
2023-02-22 23:27:12,342 DEBUG TRAIN Batch 11/3200 loss 11.068237 loss_att 11.773404 loss_ctc 14.445334 loss_rnnt 10.304650 hw_loss 0.323015 lr 0.00051308 rank 2
2023-02-22 23:27:12,350 DEBUG TRAIN Batch 11/3200 loss 16.196020 loss_att 28.702894 loss_ctc 14.025389 loss_rnnt 13.820564 hw_loss 0.306559 lr 0.00051308 rank 1
2023-02-22 23:27:12,359 DEBUG TRAIN Batch 11/3200 loss 17.627649 loss_att 21.423117 loss_ctc 18.029415 loss_rnnt 16.638416 hw_loss 0.331072 lr 0.00051308 rank 3
2023-02-22 23:27:12,372 DEBUG TRAIN Batch 11/3200 loss 19.498402 loss_att 26.413494 loss_ctc 23.374006 loss_rnnt 17.447247 hw_loss 0.283858 lr 0.00051308 rank 6
2023-02-22 23:28:26,939 DEBUG TRAIN Batch 11/3300 loss 24.188745 loss_att 31.012365 loss_ctc 19.376827 loss_rnnt 23.316826 hw_loss 0.278969 lr 0.00051281 rank 4
2023-02-22 23:28:26,946 DEBUG TRAIN Batch 11/3300 loss 13.126861 loss_att 30.384018 loss_ctc 12.114225 loss_rnnt 9.596395 hw_loss 0.401346 lr 0.00051281 rank 2
2023-02-22 23:28:26,947 DEBUG TRAIN Batch 11/3300 loss 9.343802 loss_att 14.043750 loss_ctc 9.368536 loss_rnnt 8.171696 hw_loss 0.429038 lr 0.00051281 rank 0
2023-02-22 23:28:26,948 DEBUG TRAIN Batch 11/3300 loss 21.846973 loss_att 32.847424 loss_ctc 18.939201 loss_rnnt 19.897385 hw_loss 0.257253 lr 0.00051281 rank 1
2023-02-22 23:28:26,951 DEBUG TRAIN Batch 11/3300 loss 11.168345 loss_att 20.716881 loss_ctc 8.681019 loss_rnnt 9.391779 hw_loss 0.372194 lr 0.00051281 rank 3
2023-02-22 23:28:26,952 DEBUG TRAIN Batch 11/3300 loss 25.549845 loss_att 37.912949 loss_ctc 33.297222 loss_rnnt 21.847281 hw_loss 0.369300 lr 0.00051281 rank 6
2023-02-22 23:28:26,952 DEBUG TRAIN Batch 11/3300 loss 15.784819 loss_att 23.379908 loss_ctc 11.988403 loss_rnnt 14.594769 hw_loss 0.332288 lr 0.00051281 rank 5
2023-02-22 23:28:26,953 DEBUG TRAIN Batch 11/3300 loss 15.993987 loss_att 26.414299 loss_ctc 20.823139 loss_rnnt 13.136851 hw_loss 0.242226 lr 0.00051281 rank 7
2023-02-22 23:29:43,171 DEBUG TRAIN Batch 11/3400 loss 26.818249 loss_att 37.497841 loss_ctc 34.240490 loss_rnnt 23.556501 hw_loss 0.255368 lr 0.00051254 rank 4
2023-02-22 23:29:43,172 DEBUG TRAIN Batch 11/3400 loss 16.736835 loss_att 24.649078 loss_ctc 19.404716 loss_rnnt 14.582146 hw_loss 0.405981 lr 0.00051254 rank 2
2023-02-22 23:29:43,173 DEBUG TRAIN Batch 11/3400 loss 24.369083 loss_att 37.514530 loss_ctc 26.363241 loss_rnnt 21.291115 hw_loss 0.343109 lr 0.00051254 rank 5
2023-02-22 23:29:43,175 DEBUG TRAIN Batch 11/3400 loss 30.738337 loss_att 37.390282 loss_ctc 33.614357 loss_rnnt 28.777401 hw_loss 0.463273 lr 0.00051254 rank 1
2023-02-22 23:29:43,177 DEBUG TRAIN Batch 11/3400 loss 25.632219 loss_att 37.478905 loss_ctc 26.123562 loss_rnnt 23.041519 hw_loss 0.292221 lr 0.00051254 rank 6
2023-02-22 23:29:43,179 DEBUG TRAIN Batch 11/3400 loss 24.021122 loss_att 33.656818 loss_ctc 36.038506 loss_rnnt 20.316761 hw_loss 0.327944 lr 0.00051254 rank 0
2023-02-22 23:29:43,179 DEBUG TRAIN Batch 11/3400 loss 12.862337 loss_att 19.870636 loss_ctc 13.438609 loss_rnnt 11.193054 hw_loss 0.357725 lr 0.00051254 rank 7
2023-02-22 23:29:43,181 DEBUG TRAIN Batch 11/3400 loss 30.896511 loss_att 37.447468 loss_ctc 28.737640 loss_rnnt 29.745687 hw_loss 0.240904 lr 0.00051254 rank 3
2023-02-22 23:30:59,714 DEBUG TRAIN Batch 11/3500 loss 10.054624 loss_att 21.628216 loss_ctc 11.852671 loss_rnnt 7.358585 hw_loss 0.265462 lr 0.00051228 rank 2
2023-02-22 23:30:59,717 DEBUG TRAIN Batch 11/3500 loss 30.042690 loss_att 38.887138 loss_ctc 34.112331 loss_rnnt 27.568304 hw_loss 0.305394 lr 0.00051228 rank 0
2023-02-22 23:30:59,719 DEBUG TRAIN Batch 11/3500 loss 15.771844 loss_att 24.585018 loss_ctc 15.295459 loss_rnnt 13.899055 hw_loss 0.325635 lr 0.00051228 rank 4
2023-02-22 23:30:59,720 DEBUG TRAIN Batch 11/3500 loss 20.935310 loss_att 30.841118 loss_ctc 22.104246 loss_rnnt 18.643131 hw_loss 0.290923 lr 0.00051228 rank 5
2023-02-22 23:30:59,719 DEBUG TRAIN Batch 11/3500 loss 34.124123 loss_att 55.658001 loss_ctc 42.534725 loss_rnnt 28.533430 hw_loss 0.304690 lr 0.00051228 rank 3
2023-02-22 23:30:59,722 DEBUG TRAIN Batch 11/3500 loss 24.917446 loss_att 36.680988 loss_ctc 28.229084 loss_rnnt 21.957289 hw_loss 0.311061 lr 0.00051228 rank 7
2023-02-22 23:30:59,725 DEBUG TRAIN Batch 11/3500 loss 23.400238 loss_att 28.859024 loss_ctc 22.268139 loss_rnnt 22.288774 hw_loss 0.319974 lr 0.00051228 rank 1
2023-02-22 23:30:59,728 DEBUG TRAIN Batch 11/3500 loss 13.529488 loss_att 29.238621 loss_ctc 14.471976 loss_rnnt 10.087905 hw_loss 0.326419 lr 0.00051228 rank 6
2023-02-22 23:32:16,993 DEBUG TRAIN Batch 11/3600 loss 23.169140 loss_att 27.504929 loss_ctc 21.194963 loss_rnnt 22.359608 hw_loss 0.385495 lr 0.00051201 rank 5
2023-02-22 23:32:16,997 DEBUG TRAIN Batch 11/3600 loss 27.638937 loss_att 33.687447 loss_ctc 35.691135 loss_rnnt 25.182987 hw_loss 0.323662 lr 0.00051201 rank 2
2023-02-22 23:32:16,998 DEBUG TRAIN Batch 11/3600 loss 22.921158 loss_att 35.108437 loss_ctc 21.458714 loss_rnnt 20.509398 hw_loss 0.317431 lr 0.00051201 rank 0
2023-02-22 23:32:17,002 DEBUG TRAIN Batch 11/3600 loss 22.935015 loss_att 26.190083 loss_ctc 27.862507 loss_rnnt 21.457750 hw_loss 0.317345 lr 0.00051201 rank 1
2023-02-22 23:32:17,003 DEBUG TRAIN Batch 11/3600 loss 27.163214 loss_att 35.585732 loss_ctc 33.751602 loss_rnnt 24.452423 hw_loss 0.277188 lr 0.00051201 rank 4
2023-02-22 23:32:17,003 DEBUG TRAIN Batch 11/3600 loss 14.925702 loss_att 18.981134 loss_ctc 14.050177 loss_rnnt 14.080375 hw_loss 0.283082 lr 0.00051201 rank 6
2023-02-22 23:32:17,003 DEBUG TRAIN Batch 11/3600 loss 9.303302 loss_att 17.345163 loss_ctc 6.552589 loss_rnnt 7.915823 hw_loss 0.273502 lr 0.00051201 rank 3
2023-02-22 23:32:17,006 DEBUG TRAIN Batch 11/3600 loss 17.900082 loss_att 22.184135 loss_ctc 20.259548 loss_rnnt 16.557245 hw_loss 0.321428 lr 0.00051201 rank 7
2023-02-22 23:33:34,644 DEBUG TRAIN Batch 11/3700 loss 34.589275 loss_att 39.419727 loss_ctc 37.523720 loss_rnnt 33.026989 hw_loss 0.384258 lr 0.00051174 rank 5
2023-02-22 23:33:34,644 DEBUG TRAIN Batch 11/3700 loss 11.863908 loss_att 18.345064 loss_ctc 13.568163 loss_rnnt 10.172802 hw_loss 0.314327 lr 0.00051174 rank 1
2023-02-22 23:33:34,646 DEBUG TRAIN Batch 11/3700 loss 31.374838 loss_att 46.817162 loss_ctc 34.523411 loss_rnnt 27.708349 hw_loss 0.296653 lr 0.00051174 rank 3
2023-02-22 23:33:34,647 DEBUG TRAIN Batch 11/3700 loss 24.168674 loss_att 26.329525 loss_ctc 29.122980 loss_rnnt 22.910639 hw_loss 0.309921 lr 0.00051174 rank 4
2023-02-22 23:33:34,649 DEBUG TRAIN Batch 11/3700 loss 14.461655 loss_att 18.690693 loss_ctc 15.709465 loss_rnnt 13.302948 hw_loss 0.274732 lr 0.00051174 rank 0
2023-02-22 23:33:34,651 DEBUG TRAIN Batch 11/3700 loss 30.666504 loss_att 33.888840 loss_ctc 33.115475 loss_rnnt 29.560801 hw_loss 0.252571 lr 0.00051174 rank 6
2023-02-22 23:33:34,654 DEBUG TRAIN Batch 11/3700 loss 15.971566 loss_att 25.338289 loss_ctc 20.271135 loss_rnnt 13.367069 hw_loss 0.296019 lr 0.00051174 rank 2
2023-02-22 23:33:34,656 DEBUG TRAIN Batch 11/3700 loss 21.259623 loss_att 24.433786 loss_ctc 22.463669 loss_rnnt 20.281578 hw_loss 0.342511 lr 0.00051174 rank 7
2023-02-22 23:34:50,160 DEBUG TRAIN Batch 11/3800 loss 10.163279 loss_att 13.751282 loss_ctc 11.362164 loss_rnnt 9.015451 hw_loss 0.506952 lr 0.00051147 rank 0
2023-02-22 23:34:50,161 DEBUG TRAIN Batch 11/3800 loss 18.364260 loss_att 20.253588 loss_ctc 18.156219 loss_rnnt 17.784859 hw_loss 0.429890 lr 0.00051147 rank 4
2023-02-22 23:34:50,163 DEBUG TRAIN Batch 11/3800 loss 30.076811 loss_att 45.489433 loss_ctc 26.522818 loss_rnnt 27.340057 hw_loss 0.240173 lr 0.00051147 rank 5
2023-02-22 23:34:50,166 DEBUG TRAIN Batch 11/3800 loss 13.391298 loss_att 13.966721 loss_ctc 16.479511 loss_rnnt 12.676810 hw_loss 0.351829 lr 0.00051147 rank 2
2023-02-22 23:34:50,165 DEBUG TRAIN Batch 11/3800 loss 20.866266 loss_att 24.567232 loss_ctc 20.537670 loss_rnnt 19.927183 hw_loss 0.455069 lr 0.00051147 rank 1
2023-02-22 23:34:50,169 DEBUG TRAIN Batch 11/3800 loss 22.509619 loss_att 22.864347 loss_ctc 26.181463 loss_rnnt 21.747917 hw_loss 0.377200 lr 0.00051147 rank 7
2023-02-22 23:34:50,171 DEBUG TRAIN Batch 11/3800 loss 15.788931 loss_att 21.598305 loss_ctc 18.864113 loss_rnnt 13.984526 hw_loss 0.435949 lr 0.00051147 rank 3
2023-02-22 23:34:50,171 DEBUG TRAIN Batch 11/3800 loss 12.412251 loss_att 18.466864 loss_ctc 13.978393 loss_rnnt 10.850799 hw_loss 0.265710 lr 0.00051147 rank 6
2023-02-22 23:36:08,738 DEBUG TRAIN Batch 11/3900 loss 17.328812 loss_att 20.699905 loss_ctc 20.090080 loss_rnnt 16.097525 hw_loss 0.354184 lr 0.00051120 rank 5
2023-02-22 23:36:08,741 DEBUG TRAIN Batch 11/3900 loss 22.953218 loss_att 36.799805 loss_ctc 32.133648 loss_rnnt 18.801607 hw_loss 0.296693 lr 0.00051120 rank 4
2023-02-22 23:36:08,742 DEBUG TRAIN Batch 11/3900 loss 17.308994 loss_att 23.247744 loss_ctc 14.207436 loss_rnnt 16.360760 hw_loss 0.326299 lr 0.00051120 rank 2
2023-02-22 23:36:08,744 DEBUG TRAIN Batch 11/3900 loss 14.157287 loss_att 30.352135 loss_ctc 19.680182 loss_rnnt 10.075356 hw_loss 0.199828 lr 0.00051120 rank 3
2023-02-22 23:36:08,748 DEBUG TRAIN Batch 11/3900 loss 22.411303 loss_att 31.409748 loss_ctc 23.981749 loss_rnnt 20.303699 hw_loss 0.184728 lr 0.00051120 rank 0
2023-02-22 23:36:08,753 DEBUG TRAIN Batch 11/3900 loss 15.643252 loss_att 23.693033 loss_ctc 20.354607 loss_rnnt 13.195396 hw_loss 0.393225 lr 0.00051120 rank 6
2023-02-22 23:36:08,753 DEBUG TRAIN Batch 11/3900 loss 25.725285 loss_att 37.146950 loss_ctc 29.462383 loss_rnnt 22.749571 hw_loss 0.362068 lr 0.00051120 rank 7
2023-02-22 23:36:08,770 DEBUG TRAIN Batch 11/3900 loss 29.322357 loss_att 32.880939 loss_ctc 31.747276 loss_rnnt 28.135847 hw_loss 0.284002 lr 0.00051120 rank 1
2023-02-22 23:37:24,573 DEBUG TRAIN Batch 11/4000 loss 19.369635 loss_att 31.304836 loss_ctc 22.325333 loss_rnnt 16.471352 hw_loss 0.219655 lr 0.00051094 rank 5
2023-02-22 23:37:24,577 DEBUG TRAIN Batch 11/4000 loss 20.406239 loss_att 29.375759 loss_ctc 21.013922 loss_rnnt 18.285133 hw_loss 0.461578 lr 0.00051094 rank 4
2023-02-22 23:37:24,577 DEBUG TRAIN Batch 11/4000 loss 18.273184 loss_att 24.624569 loss_ctc 29.090622 loss_rnnt 15.354426 hw_loss 0.386539 lr 0.00051094 rank 2
2023-02-22 23:37:24,578 DEBUG TRAIN Batch 11/4000 loss 13.220217 loss_att 22.457073 loss_ctc 14.437325 loss_rnnt 10.990259 hw_loss 0.413071 lr 0.00051094 rank 0
2023-02-22 23:37:24,579 DEBUG TRAIN Batch 11/4000 loss 19.590683 loss_att 27.914974 loss_ctc 16.804258 loss_rnnt 18.152710 hw_loss 0.271197 lr 0.00051094 rank 1
2023-02-22 23:37:24,581 DEBUG TRAIN Batch 11/4000 loss 12.494498 loss_att 17.604208 loss_ctc 12.085035 loss_rnnt 11.317593 hw_loss 0.392920 lr 0.00051094 rank 6
2023-02-22 23:37:24,585 DEBUG TRAIN Batch 11/4000 loss 20.335600 loss_att 23.318922 loss_ctc 20.004883 loss_rnnt 19.602995 hw_loss 0.337568 lr 0.00051094 rank 3
2023-02-22 23:37:24,585 DEBUG TRAIN Batch 11/4000 loss 26.068632 loss_att 36.619591 loss_ctc 26.487328 loss_rnnt 23.703014 hw_loss 0.374249 lr 0.00051094 rank 7
2023-02-22 23:38:39,909 DEBUG TRAIN Batch 11/4100 loss 12.158937 loss_att 20.232685 loss_ctc 13.991354 loss_rnnt 10.139112 hw_loss 0.301414 lr 0.00051067 rank 5
2023-02-22 23:38:39,912 DEBUG TRAIN Batch 11/4100 loss 24.405052 loss_att 34.368145 loss_ctc 29.355602 loss_rnnt 21.576084 hw_loss 0.330514 lr 0.00051067 rank 1
2023-02-22 23:38:39,912 DEBUG TRAIN Batch 11/4100 loss 17.934643 loss_att 27.600391 loss_ctc 19.152599 loss_rnnt 15.611648 hw_loss 0.426469 lr 0.00051067 rank 6
2023-02-22 23:38:39,914 DEBUG TRAIN Batch 11/4100 loss 16.385241 loss_att 21.721067 loss_ctc 15.907569 loss_rnnt 15.200142 hw_loss 0.340541 lr 0.00051067 rank 4
2023-02-22 23:38:39,915 DEBUG TRAIN Batch 11/4100 loss 26.090424 loss_att 33.853523 loss_ctc 35.855438 loss_rnnt 23.051994 hw_loss 0.344637 lr 0.00051067 rank 0
2023-02-22 23:38:39,916 DEBUG TRAIN Batch 11/4100 loss 37.881733 loss_att 47.044857 loss_ctc 42.284103 loss_rnnt 35.312351 hw_loss 0.280816 lr 0.00051067 rank 2
2023-02-22 23:38:39,916 DEBUG TRAIN Batch 11/4100 loss 14.651460 loss_att 19.443729 loss_ctc 17.696457 loss_rnnt 13.083118 hw_loss 0.382291 lr 0.00051067 rank 3
2023-02-22 23:38:39,919 DEBUG TRAIN Batch 11/4100 loss 7.943708 loss_att 18.660370 loss_ctc 9.645535 loss_rnnt 5.385900 hw_loss 0.351685 lr 0.00051067 rank 7
2023-02-22 23:39:56,151 DEBUG TRAIN Batch 11/4200 loss 36.317295 loss_att 36.836029 loss_ctc 43.998856 loss_rnnt 34.998680 hw_loss 0.357487 lr 0.00051040 rank 6
2023-02-22 23:39:56,152 DEBUG TRAIN Batch 11/4200 loss 23.609419 loss_att 35.524971 loss_ctc 26.545868 loss_rnnt 20.649757 hw_loss 0.346925 lr 0.00051040 rank 5
2023-02-22 23:39:56,154 DEBUG TRAIN Batch 11/4200 loss 25.937170 loss_att 32.869701 loss_ctc 37.892357 loss_rnnt 22.788626 hw_loss 0.315022 lr 0.00051040 rank 0
2023-02-22 23:39:56,154 DEBUG TRAIN Batch 11/4200 loss 30.584316 loss_att 47.395123 loss_ctc 35.355980 loss_rnnt 26.420458 hw_loss 0.310263 lr 0.00051040 rank 4
2023-02-22 23:39:56,157 DEBUG TRAIN Batch 11/4200 loss 11.212632 loss_att 16.809341 loss_ctc 11.109926 loss_rnnt 9.964188 hw_loss 0.267745 lr 0.00051040 rank 3
2023-02-22 23:39:56,162 DEBUG TRAIN Batch 11/4200 loss 39.459454 loss_att 45.074474 loss_ctc 46.786247 loss_rnnt 37.206165 hw_loss 0.287580 lr 0.00051040 rank 2
2023-02-22 23:39:56,163 DEBUG TRAIN Batch 11/4200 loss 24.010555 loss_att 36.537991 loss_ctc 23.125484 loss_rnnt 21.460447 hw_loss 0.304937 lr 0.00051040 rank 1
2023-02-22 23:39:56,164 DEBUG TRAIN Batch 11/4200 loss 17.407139 loss_att 25.725933 loss_ctc 20.020897 loss_rnnt 15.248978 hw_loss 0.273566 lr 0.00051040 rank 7
2023-02-22 23:41:14,677 DEBUG TRAIN Batch 11/4300 loss 20.250204 loss_att 24.766718 loss_ctc 32.393295 loss_rnnt 17.562599 hw_loss 0.309791 lr 0.00051014 rank 6
2023-02-22 23:41:14,678 DEBUG TRAIN Batch 11/4300 loss 21.238560 loss_att 23.932774 loss_ctc 22.269981 loss_rnnt 20.354771 hw_loss 0.388919 lr 0.00051014 rank 1
2023-02-22 23:41:14,678 DEBUG TRAIN Batch 11/4300 loss 19.655212 loss_att 28.626001 loss_ctc 23.718040 loss_rnnt 17.129736 hw_loss 0.355517 lr 0.00051014 rank 5
2023-02-22 23:41:14,680 DEBUG TRAIN Batch 11/4300 loss 16.981459 loss_att 25.705767 loss_ctc 21.937519 loss_rnnt 14.389228 hw_loss 0.349805 lr 0.00051014 rank 0
2023-02-22 23:41:14,681 DEBUG TRAIN Batch 11/4300 loss 26.151327 loss_att 33.928459 loss_ctc 32.376118 loss_rnnt 23.577488 hw_loss 0.353323 lr 0.00051014 rank 4
2023-02-22 23:41:14,681 DEBUG TRAIN Batch 11/4300 loss 21.336887 loss_att 25.161335 loss_ctc 24.113987 loss_rnnt 20.007118 hw_loss 0.364874 lr 0.00051014 rank 2
2023-02-22 23:41:14,683 DEBUG TRAIN Batch 11/4300 loss 22.978685 loss_att 33.202049 loss_ctc 28.570541 loss_rnnt 19.999050 hw_loss 0.355088 lr 0.00051014 rank 3
2023-02-22 23:41:14,685 DEBUG TRAIN Batch 11/4300 loss 35.063572 loss_att 38.410805 loss_ctc 43.171173 loss_rnnt 33.130157 hw_loss 0.343034 lr 0.00051014 rank 7
2023-02-22 23:42:30,120 DEBUG TRAIN Batch 11/4400 loss 15.469030 loss_att 18.783752 loss_ctc 18.801588 loss_rnnt 14.150851 hw_loss 0.395426 lr 0.00050987 rank 5
2023-02-22 23:42:30,121 DEBUG TRAIN Batch 11/4400 loss 37.188046 loss_att 38.370495 loss_ctc 42.010765 loss_rnnt 36.030674 hw_loss 0.520971 lr 0.00050987 rank 2
2023-02-22 23:42:30,121 DEBUG TRAIN Batch 11/4400 loss 28.773191 loss_att 30.949833 loss_ctc 34.239326 loss_rnnt 27.451075 hw_loss 0.296192 lr 0.00050987 rank 3
2023-02-22 23:42:30,123 DEBUG TRAIN Batch 11/4400 loss 27.513960 loss_att 33.858795 loss_ctc 29.688419 loss_rnnt 25.830799 hw_loss 0.232999 lr 0.00050987 rank 0
2023-02-22 23:42:30,123 DEBUG TRAIN Batch 11/4400 loss 21.081503 loss_att 28.026501 loss_ctc 21.316481 loss_rnnt 19.528198 hw_loss 0.249326 lr 0.00050987 rank 4
2023-02-22 23:42:30,126 DEBUG TRAIN Batch 11/4400 loss 13.093712 loss_att 16.632423 loss_ctc 15.702738 loss_rnnt 11.823766 hw_loss 0.401875 lr 0.00050987 rank 6
2023-02-22 23:42:30,128 DEBUG TRAIN Batch 11/4400 loss 11.154058 loss_att 15.720512 loss_ctc 13.543265 loss_rnnt 9.767832 hw_loss 0.289450 lr 0.00050987 rank 1
2023-02-22 23:42:30,129 DEBUG TRAIN Batch 11/4400 loss 20.855743 loss_att 22.610195 loss_ctc 27.406296 loss_rnnt 19.424736 hw_loss 0.387585 lr 0.00050987 rank 7
2023-02-22 23:43:46,218 DEBUG TRAIN Batch 11/4500 loss 36.174667 loss_att 48.100479 loss_ctc 40.756851 loss_rnnt 32.975319 hw_loss 0.381054 lr 0.00050961 rank 5
2023-02-22 23:43:46,222 DEBUG TRAIN Batch 11/4500 loss 34.225563 loss_att 44.313744 loss_ctc 48.469276 loss_rnnt 30.132055 hw_loss 0.331334 lr 0.00050961 rank 4
2023-02-22 23:43:46,224 DEBUG TRAIN Batch 11/4500 loss 43.948967 loss_att 49.757809 loss_ctc 51.011242 loss_rnnt 41.718079 hw_loss 0.239029 lr 0.00050961 rank 2
2023-02-22 23:43:46,227 DEBUG TRAIN Batch 11/4500 loss 21.826906 loss_att 32.790062 loss_ctc 28.717169 loss_rnnt 18.585777 hw_loss 0.243364 lr 0.00050961 rank 6
2023-02-22 23:43:46,227 DEBUG TRAIN Batch 11/4500 loss 17.316492 loss_att 16.984720 loss_ctc 18.438997 loss_rnnt 16.997087 hw_loss 0.442674 lr 0.00050961 rank 0
2023-02-22 23:43:46,229 DEBUG TRAIN Batch 11/4500 loss 33.136971 loss_att 45.849689 loss_ctc 40.578728 loss_rnnt 29.430092 hw_loss 0.322690 lr 0.00050961 rank 1
2023-02-22 23:43:46,230 DEBUG TRAIN Batch 11/4500 loss 30.879471 loss_att 40.240730 loss_ctc 35.469414 loss_rnnt 28.204086 hw_loss 0.358385 lr 0.00050961 rank 7
2023-02-22 23:43:46,275 DEBUG TRAIN Batch 11/4500 loss 20.210344 loss_att 22.991135 loss_ctc 22.695351 loss_rnnt 19.098049 hw_loss 0.421508 lr 0.00050961 rank 3
2023-02-22 23:45:04,188 DEBUG TRAIN Batch 11/4600 loss 16.819553 loss_att 23.152512 loss_ctc 20.593967 loss_rnnt 14.876759 hw_loss 0.324280 lr 0.00050934 rank 5
2023-02-22 23:45:04,189 DEBUG TRAIN Batch 11/4600 loss 32.672001 loss_att 43.499454 loss_ctc 40.316849 loss_rnnt 29.353378 hw_loss 0.250916 lr 0.00050934 rank 1
2023-02-22 23:45:04,190 DEBUG TRAIN Batch 11/4600 loss 18.883781 loss_att 23.530199 loss_ctc 24.323477 loss_rnnt 16.888227 hw_loss 0.639333 lr 0.00050934 rank 3
2023-02-22 23:45:04,190 DEBUG TRAIN Batch 11/4600 loss 29.375240 loss_att 37.347969 loss_ctc 30.085865 loss_rnnt 27.499733 hw_loss 0.349143 lr 0.00050934 rank 4
2023-02-22 23:45:04,190 DEBUG TRAIN Batch 11/4600 loss 19.398712 loss_att 28.892855 loss_ctc 27.713345 loss_rnnt 16.207920 hw_loss 0.343772 lr 0.00050934 rank 2
2023-02-22 23:45:04,192 DEBUG TRAIN Batch 11/4600 loss 26.180122 loss_att 30.519470 loss_ctc 32.356232 loss_rnnt 24.334896 hw_loss 0.288518 lr 0.00050934 rank 6
2023-02-22 23:45:04,193 DEBUG TRAIN Batch 11/4600 loss 17.414490 loss_att 24.507351 loss_ctc 19.662186 loss_rnnt 15.523582 hw_loss 0.323705 lr 0.00050934 rank 7
2023-02-22 23:45:04,199 DEBUG TRAIN Batch 11/4600 loss 22.980888 loss_att 33.239182 loss_ctc 25.430389 loss_rnnt 20.477829 hw_loss 0.234000 lr 0.00050934 rank 0
2023-02-22 23:46:20,599 DEBUG TRAIN Batch 11/4700 loss 15.291240 loss_att 21.095257 loss_ctc 18.987835 loss_rnnt 13.425596 hw_loss 0.397427 lr 0.00050908 rank 1
2023-02-22 23:46:20,599 DEBUG TRAIN Batch 11/4700 loss 15.381275 loss_att 23.584927 loss_ctc 16.189011 loss_rnnt 13.513310 hw_loss 0.224131 lr 0.00050908 rank 7
2023-02-22 23:46:20,600 DEBUG TRAIN Batch 11/4700 loss 11.330956 loss_att 17.795374 loss_ctc 15.426485 loss_rnnt 9.278082 hw_loss 0.401098 lr 0.00050908 rank 5
2023-02-22 23:46:20,601 DEBUG TRAIN Batch 11/4700 loss 27.900581 loss_att 39.943752 loss_ctc 29.770924 loss_rnnt 25.130543 hw_loss 0.210047 lr 0.00050908 rank 2
2023-02-22 23:46:20,602 DEBUG TRAIN Batch 11/4700 loss 24.455662 loss_att 28.443333 loss_ctc 30.034124 loss_rnnt 22.734299 hw_loss 0.337562 lr 0.00050908 rank 4
2023-02-22 23:46:20,603 DEBUG TRAIN Batch 11/4700 loss 11.656767 loss_att 24.263914 loss_ctc 14.090621 loss_rnnt 8.636073 hw_loss 0.327657 lr 0.00050908 rank 3
2023-02-22 23:46:20,604 DEBUG TRAIN Batch 11/4700 loss 13.534884 loss_att 21.725933 loss_ctc 15.056067 loss_rnnt 11.511226 hw_loss 0.342422 lr 0.00050908 rank 0
2023-02-22 23:46:20,605 DEBUG TRAIN Batch 11/4700 loss 23.290565 loss_att 28.060076 loss_ctc 24.007915 loss_rnnt 22.097490 hw_loss 0.269112 lr 0.00050908 rank 6
2023-02-22 23:47:35,707 DEBUG TRAIN Batch 11/4800 loss 22.000139 loss_att 36.797310 loss_ctc 21.103813 loss_rnnt 18.975956 hw_loss 0.345487 lr 0.00050882 rank 0
2023-02-22 23:47:35,708 DEBUG TRAIN Batch 11/4800 loss 14.950872 loss_att 21.383907 loss_ctc 13.758947 loss_rnnt 13.696299 hw_loss 0.237918 lr 0.00050882 rank 6
2023-02-22 23:47:35,708 DEBUG TRAIN Batch 11/4800 loss 26.508087 loss_att 33.298508 loss_ctc 25.944992 loss_rnnt 25.048756 hw_loss 0.330608 lr 0.00050882 rank 5
2023-02-22 23:47:35,709 DEBUG TRAIN Batch 11/4800 loss 18.566439 loss_att 24.244873 loss_ctc 19.222668 loss_rnnt 17.187433 hw_loss 0.292161 lr 0.00050882 rank 7
2023-02-22 23:47:35,710 DEBUG TRAIN Batch 11/4800 loss 37.720901 loss_att 38.413132 loss_ctc 40.553551 loss_rnnt 37.022408 hw_loss 0.341929 lr 0.00050882 rank 2
2023-02-22 23:47:35,710 DEBUG TRAIN Batch 11/4800 loss 35.955044 loss_att 42.353065 loss_ctc 35.358673 loss_rnnt 34.593460 hw_loss 0.302800 lr 0.00050882 rank 4
2023-02-22 23:47:35,727 DEBUG TRAIN Batch 11/4800 loss 26.565338 loss_att 35.229904 loss_ctc 29.077965 loss_rnnt 24.362808 hw_loss 0.252371 lr 0.00050882 rank 3
2023-02-22 23:47:35,737 DEBUG TRAIN Batch 11/4800 loss 10.479153 loss_att 17.164829 loss_ctc 11.514430 loss_rnnt 8.847564 hw_loss 0.293280 lr 0.00050882 rank 1
2023-02-22 23:48:53,064 DEBUG TRAIN Batch 11/4900 loss 12.716825 loss_att 17.902327 loss_ctc 16.129906 loss_rnnt 11.002197 hw_loss 0.417094 lr 0.00050855 rank 5
2023-02-22 23:48:53,066 DEBUG TRAIN Batch 11/4900 loss 22.850105 loss_att 21.083870 loss_ctc 25.911301 loss_rnnt 22.609051 hw_loss 0.349019 lr 0.00050855 rank 3
2023-02-22 23:48:53,068 DEBUG TRAIN Batch 11/4900 loss 23.685957 loss_att 32.315369 loss_ctc 25.379993 loss_rnnt 21.514244 hw_loss 0.412420 lr 0.00050855 rank 7
2023-02-22 23:48:53,071 DEBUG TRAIN Batch 11/4900 loss 21.639364 loss_att 29.672384 loss_ctc 26.069128 loss_rnnt 19.256752 hw_loss 0.347575 lr 0.00050855 rank 4
2023-02-22 23:48:53,073 DEBUG TRAIN Batch 11/4900 loss 8.646383 loss_att 15.286089 loss_ctc 8.598069 loss_rnnt 7.159610 hw_loss 0.309890 lr 0.00050855 rank 0
2023-02-22 23:48:53,077 DEBUG TRAIN Batch 11/4900 loss 20.763441 loss_att 21.751493 loss_ctc 20.647100 loss_rnnt 20.458897 hw_loss 0.229590 lr 0.00050855 rank 2
2023-02-22 23:48:53,113 DEBUG TRAIN Batch 11/4900 loss 18.282742 loss_att 29.655434 loss_ctc 18.589777 loss_rnnt 15.809229 hw_loss 0.296314 lr 0.00050855 rank 1
2023-02-22 23:48:53,115 DEBUG TRAIN Batch 11/4900 loss 22.665760 loss_att 30.911457 loss_ctc 24.103109 loss_rnnt 20.659840 hw_loss 0.309624 lr 0.00050855 rank 6
2023-02-22 23:50:11,883 DEBUG TRAIN Batch 11/5000 loss 30.076954 loss_att 34.606663 loss_ctc 35.650219 loss_rnnt 28.220617 hw_loss 0.388676 lr 0.00050829 rank 2
2023-02-22 23:50:11,893 DEBUG TRAIN Batch 11/5000 loss 11.036660 loss_att 18.168127 loss_ctc 13.089121 loss_rnnt 9.121618 hw_loss 0.403287 lr 0.00050829 rank 3
2023-02-22 23:50:11,893 DEBUG TRAIN Batch 11/5000 loss 37.621346 loss_att 40.664726 loss_ctc 43.860111 loss_rnnt 35.991718 hw_loss 0.354586 lr 0.00050829 rank 0
2023-02-22 23:50:11,894 DEBUG TRAIN Batch 11/5000 loss 24.407656 loss_att 25.943270 loss_ctc 27.999022 loss_rnnt 23.405487 hw_loss 0.405371 lr 0.00050829 rank 4
2023-02-22 23:50:11,895 DEBUG TRAIN Batch 11/5000 loss 29.867760 loss_att 43.843357 loss_ctc 33.321457 loss_rnnt 26.401865 hw_loss 0.394279 lr 0.00050829 rank 5
2023-02-22 23:50:11,898 DEBUG TRAIN Batch 11/5000 loss 14.795947 loss_att 23.786480 loss_ctc 15.065310 loss_rnnt 12.803793 hw_loss 0.296500 lr 0.00050829 rank 1
2023-02-22 23:50:11,900 DEBUG TRAIN Batch 11/5000 loss 18.104784 loss_att 19.306005 loss_ctc 19.436008 loss_rnnt 17.411865 hw_loss 0.515957 lr 0.00050829 rank 6
2023-02-22 23:50:11,903 DEBUG TRAIN Batch 11/5000 loss 19.794058 loss_att 22.744337 loss_ctc 22.399580 loss_rnnt 18.661484 hw_loss 0.365846 lr 0.00050829 rank 7
2023-02-22 23:51:28,341 DEBUG TRAIN Batch 11/5100 loss 21.934668 loss_att 26.197197 loss_ctc 23.487997 loss_rnnt 20.653006 hw_loss 0.416331 lr 0.00050803 rank 0
2023-02-22 23:51:28,342 DEBUG TRAIN Batch 11/5100 loss 9.117925 loss_att 17.733891 loss_ctc 9.914339 loss_rnnt 7.163776 hw_loss 0.233937 lr 0.00050803 rank 5
2023-02-22 23:51:28,342 DEBUG TRAIN Batch 11/5100 loss 23.060246 loss_att 36.235443 loss_ctc 28.947765 loss_rnnt 19.550972 hw_loss 0.167308 lr 0.00050803 rank 4
2023-02-22 23:51:28,343 DEBUG TRAIN Batch 11/5100 loss 18.070118 loss_att 26.936367 loss_ctc 17.642956 loss_rnnt 16.233818 hw_loss 0.225009 lr 0.00050803 rank 2
2023-02-22 23:51:28,347 DEBUG TRAIN Batch 11/5100 loss 11.813494 loss_att 13.446290 loss_ctc 12.190555 loss_rnnt 11.186998 hw_loss 0.468114 lr 0.00050803 rank 3
2023-02-22 23:51:28,349 DEBUG TRAIN Batch 11/5100 loss 13.347727 loss_att 25.183832 loss_ctc 15.746628 loss_rnnt 10.537657 hw_loss 0.230617 lr 0.00050803 rank 7
2023-02-22 23:51:28,350 DEBUG TRAIN Batch 11/5100 loss 12.381860 loss_att 14.035466 loss_ctc 14.624596 loss_rnnt 11.537116 hw_loss 0.403107 lr 0.00050803 rank 1
2023-02-22 23:51:28,395 DEBUG TRAIN Batch 11/5100 loss 15.550396 loss_att 25.220798 loss_ctc 22.403494 loss_rnnt 12.548156 hw_loss 0.289524 lr 0.00050803 rank 6
2023-02-22 23:52:44,437 DEBUG TRAIN Batch 11/5200 loss 18.528973 loss_att 22.272129 loss_ctc 15.456953 loss_rnnt 18.049541 hw_loss 0.263254 lr 0.00050776 rank 4
2023-02-22 23:52:44,440 DEBUG TRAIN Batch 11/5200 loss 21.988270 loss_att 27.883657 loss_ctc 26.422150 loss_rnnt 20.029350 hw_loss 0.353733 lr 0.00050776 rank 1
2023-02-22 23:52:44,442 DEBUG TRAIN Batch 11/5200 loss 17.441980 loss_att 20.955843 loss_ctc 14.080005 loss_rnnt 17.008690 hw_loss 0.335210 lr 0.00050776 rank 5
2023-02-22 23:52:44,443 DEBUG TRAIN Batch 11/5200 loss 14.800919 loss_att 22.039724 loss_ctc 17.591156 loss_rnnt 12.757841 hw_loss 0.418661 lr 0.00050776 rank 0
2023-02-22 23:52:44,446 DEBUG TRAIN Batch 11/5200 loss 14.635387 loss_att 17.925255 loss_ctc 14.412265 loss_rnnt 13.851076 hw_loss 0.292661 lr 0.00050776 rank 3
2023-02-22 23:52:44,446 DEBUG TRAIN Batch 11/5200 loss 23.729155 loss_att 29.362495 loss_ctc 21.589476 loss_rnnt 22.720320 hw_loss 0.313984 lr 0.00050776 rank 6
2023-02-22 23:52:44,448 DEBUG TRAIN Batch 11/5200 loss 21.613312 loss_att 28.635256 loss_ctc 22.949535 loss_rnnt 19.865349 hw_loss 0.310144 lr 0.00050776 rank 7
2023-02-22 23:52:44,447 DEBUG TRAIN Batch 11/5200 loss 10.720855 loss_att 14.265756 loss_ctc 9.728942 loss_rnnt 10.050045 hw_loss 0.176409 lr 0.00050776 rank 2
2023-02-22 23:54:01,889 DEBUG TRAIN Batch 11/5300 loss 20.904783 loss_att 33.170494 loss_ctc 32.192154 loss_rnnt 16.802656 hw_loss 0.270005 lr 0.00050750 rank 5
2023-02-22 23:54:01,891 DEBUG TRAIN Batch 11/5300 loss 16.485302 loss_att 22.837715 loss_ctc 18.469860 loss_rnnt 14.821411 hw_loss 0.241497 lr 0.00050750 rank 0
2023-02-22 23:54:01,892 DEBUG TRAIN Batch 11/5300 loss 14.161540 loss_att 20.553440 loss_ctc 18.278152 loss_rnnt 12.215631 hw_loss 0.222461 lr 0.00050750 rank 7
2023-02-22 23:54:01,893 DEBUG TRAIN Batch 11/5300 loss 11.823166 loss_att 19.276136 loss_ctc 9.862806 loss_rnnt 10.453160 hw_loss 0.263987 lr 0.00050750 rank 1
2023-02-22 23:54:01,893 DEBUG TRAIN Batch 11/5300 loss 8.738100 loss_att 16.874809 loss_ctc 7.166443 loss_rnnt 7.151671 hw_loss 0.316203 lr 0.00050750 rank 3
2023-02-22 23:54:01,894 DEBUG TRAIN Batch 11/5300 loss 16.417130 loss_att 25.824268 loss_ctc 18.333042 loss_rnnt 14.113853 hw_loss 0.311991 lr 0.00050750 rank 2
2023-02-22 23:54:01,895 DEBUG TRAIN Batch 11/5300 loss 32.977081 loss_att 38.949741 loss_ctc 39.496582 loss_rnnt 30.795834 hw_loss 0.220224 lr 0.00050750 rank 4
2023-02-22 23:54:01,897 DEBUG TRAIN Batch 11/5300 loss 11.592981 loss_att 16.317884 loss_ctc 14.012722 loss_rnnt 10.155817 hw_loss 0.317906 lr 0.00050750 rank 6
2023-02-22 23:55:18,640 DEBUG TRAIN Batch 11/5400 loss 13.475363 loss_att 18.879948 loss_ctc 13.187773 loss_rnnt 12.259548 hw_loss 0.324831 lr 0.00050724 rank 0
2023-02-22 23:55:18,641 DEBUG TRAIN Batch 11/5400 loss 18.629953 loss_att 19.700485 loss_ctc 22.578911 loss_rnnt 17.734365 hw_loss 0.290543 lr 0.00050724 rank 5
2023-02-22 23:55:18,642 DEBUG TRAIN Batch 11/5400 loss 14.808432 loss_att 16.265291 loss_ctc 19.582550 loss_rnnt 13.745955 hw_loss 0.252293 lr 0.00050724 rank 3
2023-02-22 23:55:18,642 DEBUG TRAIN Batch 11/5400 loss 25.438921 loss_att 31.216747 loss_ctc 32.674797 loss_rnnt 23.104858 hw_loss 0.400715 lr 0.00050724 rank 7
2023-02-22 23:55:18,645 DEBUG TRAIN Batch 11/5400 loss 28.659559 loss_att 37.755531 loss_ctc 33.313290 loss_rnnt 26.039869 hw_loss 0.337495 lr 0.00050724 rank 2
2023-02-22 23:55:18,649 DEBUG TRAIN Batch 11/5400 loss 14.578133 loss_att 22.032024 loss_ctc 14.095623 loss_rnnt 13.016856 hw_loss 0.252811 lr 0.00050724 rank 4
2023-02-22 23:55:18,649 DEBUG TRAIN Batch 11/5400 loss 23.109144 loss_att 27.876469 loss_ctc 29.494556 loss_rnnt 21.133530 hw_loss 0.320177 lr 0.00050724 rank 6
2023-02-22 23:55:18,650 DEBUG TRAIN Batch 11/5400 loss 30.913401 loss_att 37.256554 loss_ctc 37.995728 loss_rnnt 28.538378 hw_loss 0.303905 lr 0.00050724 rank 1
2023-02-22 23:56:33,377 DEBUG TRAIN Batch 11/5500 loss 14.254041 loss_att 18.968691 loss_ctc 15.432234 loss_rnnt 12.999801 hw_loss 0.289159 lr 0.00050698 rank 0
2023-02-22 23:56:33,378 DEBUG TRAIN Batch 11/5500 loss 27.424919 loss_att 34.219780 loss_ctc 33.155277 loss_rnnt 25.127483 hw_loss 0.327033 lr 0.00050698 rank 3
2023-02-22 23:56:33,380 DEBUG TRAIN Batch 11/5500 loss 19.843977 loss_att 22.883785 loss_ctc 22.228100 loss_rnnt 18.749676 hw_loss 0.315857 lr 0.00050698 rank 5
2023-02-22 23:56:33,380 DEBUG TRAIN Batch 11/5500 loss 24.459190 loss_att 36.097710 loss_ctc 27.166836 loss_rnnt 21.595257 hw_loss 0.328521 lr 0.00050698 rank 4
2023-02-22 23:56:33,380 DEBUG TRAIN Batch 11/5500 loss 16.857506 loss_att 17.390263 loss_ctc 19.026545 loss_rnnt 16.288544 hw_loss 0.324759 lr 0.00050698 rank 2
2023-02-22 23:56:33,385 DEBUG TRAIN Batch 11/5500 loss 27.612442 loss_att 36.932533 loss_ctc 29.630159 loss_rnnt 25.328995 hw_loss 0.281998 lr 0.00050698 rank 1
2023-02-22 23:56:33,385 DEBUG TRAIN Batch 11/5500 loss 30.564444 loss_att 34.620667 loss_ctc 33.578690 loss_rnnt 29.179081 hw_loss 0.322907 lr 0.00050698 rank 7
2023-02-22 23:56:33,389 DEBUG TRAIN Batch 11/5500 loss 17.602261 loss_att 22.502560 loss_ctc 20.070105 loss_rnnt 16.072847 hw_loss 0.413072 lr 0.00050698 rank 6
2023-02-22 23:57:48,891 DEBUG TRAIN Batch 11/5600 loss 16.547144 loss_att 20.161160 loss_ctc 17.000580 loss_rnnt 15.604174 hw_loss 0.299452 lr 0.00050672 rank 0
2023-02-22 23:57:48,895 DEBUG TRAIN Batch 11/5600 loss 27.521870 loss_att 32.379532 loss_ctc 26.760059 loss_rnnt 26.485508 hw_loss 0.312005 lr 0.00050672 rank 1
2023-02-22 23:57:48,895 DEBUG TRAIN Batch 11/5600 loss 14.676435 loss_att 21.230091 loss_ctc 16.325672 loss_rnnt 12.969586 hw_loss 0.330409 lr 0.00050672 rank 5
2023-02-22 23:57:48,896 DEBUG TRAIN Batch 11/5600 loss 12.758033 loss_att 14.633004 loss_ctc 15.031298 loss_rnnt 11.871015 hw_loss 0.391727 lr 0.00050672 rank 6
2023-02-22 23:57:48,896 DEBUG TRAIN Batch 11/5600 loss 30.124445 loss_att 38.533875 loss_ctc 33.825104 loss_rnnt 27.772060 hw_loss 0.332022 lr 0.00050672 rank 4
2023-02-22 23:57:48,897 DEBUG TRAIN Batch 11/5600 loss 21.304264 loss_att 31.844410 loss_ctc 20.606531 loss_rnnt 19.156940 hw_loss 0.248110 lr 0.00050672 rank 3
2023-02-22 23:57:48,899 DEBUG TRAIN Batch 11/5600 loss 25.569756 loss_att 28.859066 loss_ctc 30.141373 loss_rnnt 24.148611 hw_loss 0.288250 lr 0.00050672 rank 7
2023-02-22 23:57:48,900 DEBUG TRAIN Batch 11/5600 loss 15.996824 loss_att 17.824495 loss_ctc 16.659294 loss_rnnt 15.361115 hw_loss 0.340961 lr 0.00050672 rank 2
2023-02-22 23:59:08,338 DEBUG TRAIN Batch 11/5700 loss 8.198207 loss_att 11.398542 loss_ctc 10.277409 loss_rnnt 7.079443 hw_loss 0.377754 lr 0.00050646 rank 5
2023-02-22 23:59:08,339 DEBUG TRAIN Batch 11/5700 loss 14.561845 loss_att 16.675776 loss_ctc 17.550718 loss_rnnt 13.541655 hw_loss 0.372915 lr 0.00050646 rank 2
2023-02-22 23:59:08,340 DEBUG TRAIN Batch 11/5700 loss 15.113851 loss_att 17.131020 loss_ctc 18.888664 loss_rnnt 13.964374 hw_loss 0.455129 lr 0.00050646 rank 4
2023-02-22 23:59:08,340 DEBUG TRAIN Batch 11/5700 loss 20.705444 loss_att 24.402420 loss_ctc 22.399008 loss_rnnt 19.600056 hw_loss 0.262842 lr 0.00050646 rank 1
2023-02-22 23:59:08,342 DEBUG TRAIN Batch 11/5700 loss 9.001876 loss_att 13.118080 loss_ctc 10.724358 loss_rnnt 7.763156 hw_loss 0.348402 lr 0.00050646 rank 3
2023-02-22 23:59:08,344 DEBUG TRAIN Batch 11/5700 loss 10.771067 loss_att 15.069790 loss_ctc 11.985629 loss_rnnt 9.582328 hw_loss 0.313224 lr 0.00050646 rank 0
2023-02-22 23:59:08,374 DEBUG TRAIN Batch 11/5700 loss 27.499187 loss_att 44.668785 loss_ctc 28.815575 loss_rnnt 23.772711 hw_loss 0.219448 lr 0.00050646 rank 7
2023-02-22 23:59:08,379 DEBUG TRAIN Batch 11/5700 loss 26.982628 loss_att 38.087193 loss_ctc 34.426216 loss_rnnt 23.566639 hw_loss 0.379870 lr 0.00050646 rank 6
2023-02-23 00:00:23,662 DEBUG TRAIN Batch 11/5800 loss 14.400196 loss_att 22.152885 loss_ctc 14.687682 loss_rnnt 12.668545 hw_loss 0.267716 lr 0.00050620 rank 0
2023-02-23 00:00:23,664 DEBUG TRAIN Batch 11/5800 loss 24.319096 loss_att 22.649237 loss_ctc 27.197992 loss_rnnt 24.022715 hw_loss 0.462189 lr 0.00050620 rank 1
2023-02-23 00:00:23,664 DEBUG TRAIN Batch 11/5800 loss 40.595360 loss_att 49.823338 loss_ctc 43.299187 loss_rnnt 38.244545 hw_loss 0.271327 lr 0.00050620 rank 2
2023-02-23 00:00:23,668 DEBUG TRAIN Batch 11/5800 loss 15.910645 loss_att 24.146191 loss_ctc 12.842995 loss_rnnt 14.529001 hw_loss 0.269163 lr 0.00050620 rank 5
2023-02-23 00:00:23,669 DEBUG TRAIN Batch 11/5800 loss 18.360416 loss_att 23.195829 loss_ctc 17.286940 loss_rnnt 17.418205 hw_loss 0.221735 lr 0.00050620 rank 4
2023-02-23 00:00:23,670 DEBUG TRAIN Batch 11/5800 loss 16.213892 loss_att 20.681231 loss_ctc 17.520031 loss_rnnt 14.992239 hw_loss 0.288813 lr 0.00050620 rank 6
2023-02-23 00:00:23,671 DEBUG TRAIN Batch 11/5800 loss 14.642219 loss_att 24.419287 loss_ctc 18.339619 loss_rnnt 12.055617 hw_loss 0.259128 lr 0.00050620 rank 3
2023-02-23 00:00:23,673 DEBUG TRAIN Batch 11/5800 loss 22.883839 loss_att 27.896847 loss_ctc 27.579361 loss_rnnt 21.105915 hw_loss 0.279846 lr 0.00050620 rank 7
2023-02-23 00:01:40,056 DEBUG TRAIN Batch 11/5900 loss 19.765951 loss_att 30.223671 loss_ctc 25.770367 loss_rnnt 16.726620 hw_loss 0.275996 lr 0.00050594 rank 6
2023-02-23 00:01:40,060 DEBUG TRAIN Batch 11/5900 loss 16.839193 loss_att 22.492512 loss_ctc 21.179611 loss_rnnt 14.960311 hw_loss 0.317808 lr 0.00050594 rank 3
2023-02-23 00:01:40,061 DEBUG TRAIN Batch 11/5900 loss 17.147461 loss_att 20.684818 loss_ctc 17.333311 loss_rnnt 16.273666 hw_loss 0.265396 lr 0.00050594 rank 2
2023-02-23 00:01:40,061 DEBUG TRAIN Batch 11/5900 loss 19.483444 loss_att 28.557259 loss_ctc 23.599140 loss_rnnt 17.004692 hw_loss 0.216053 lr 0.00050594 rank 4
2023-02-23 00:01:40,061 DEBUG TRAIN Batch 11/5900 loss 12.459097 loss_att 20.164776 loss_ctc 13.423685 loss_rnnt 10.632753 hw_loss 0.293617 lr 0.00050594 rank 5
2023-02-23 00:01:40,062 DEBUG TRAIN Batch 11/5900 loss 14.211908 loss_att 22.227564 loss_ctc 13.242334 loss_rnnt 12.621531 hw_loss 0.218480 lr 0.00050594 rank 0
2023-02-23 00:01:40,067 DEBUG TRAIN Batch 11/5900 loss 12.586443 loss_att 20.931862 loss_ctc 15.487361 loss_rnnt 10.397621 hw_loss 0.249279 lr 0.00050594 rank 1
2023-02-23 00:01:40,110 DEBUG TRAIN Batch 11/5900 loss 18.415638 loss_att 21.083803 loss_ctc 20.006886 loss_rnnt 17.542423 hw_loss 0.238904 lr 0.00050594 rank 7
2023-02-23 00:02:57,801 DEBUG TRAIN Batch 11/6000 loss 17.907816 loss_att 23.787048 loss_ctc 17.132063 loss_rnnt 16.651863 hw_loss 0.344140 lr 0.00050568 rank 0
2023-02-23 00:02:57,801 DEBUG TRAIN Batch 11/6000 loss 17.979313 loss_att 30.015591 loss_ctc 24.946815 loss_rnnt 14.459126 hw_loss 0.344867 lr 0.00050568 rank 2
2023-02-23 00:02:57,804 DEBUG TRAIN Batch 11/6000 loss 35.376286 loss_att 38.061966 loss_ctc 32.916470 loss_rnnt 34.958801 hw_loss 0.390602 lr 0.00050568 rank 3
2023-02-23 00:02:57,804 DEBUG TRAIN Batch 11/6000 loss 26.046780 loss_att 33.078823 loss_ctc 34.858490 loss_rnnt 23.319998 hw_loss 0.272769 lr 0.00050568 rank 6
2023-02-23 00:02:57,805 DEBUG TRAIN Batch 11/6000 loss 21.717663 loss_att 26.150053 loss_ctc 27.065426 loss_rnnt 19.975620 hw_loss 0.267241 lr 0.00050568 rank 4
2023-02-23 00:02:57,806 DEBUG TRAIN Batch 11/6000 loss 14.644897 loss_att 22.534241 loss_ctc 20.668888 loss_rnnt 12.052386 hw_loss 0.396455 lr 0.00050568 rank 5
2023-02-23 00:02:57,809 DEBUG TRAIN Batch 11/6000 loss 16.373514 loss_att 21.575272 loss_ctc 17.168282 loss_rnnt 15.027994 hw_loss 0.373497 lr 0.00050568 rank 7
2023-02-23 00:02:57,813 DEBUG TRAIN Batch 11/6000 loss 14.968291 loss_att 20.920574 loss_ctc 21.676062 loss_rnnt 12.726128 hw_loss 0.295007 lr 0.00050568 rank 1
2023-02-23 00:04:15,109 DEBUG TRAIN Batch 11/6100 loss 23.860502 loss_att 33.133286 loss_ctc 26.439217 loss_rnnt 21.486320 hw_loss 0.329619 lr 0.00050542 rank 0
2023-02-23 00:04:15,111 DEBUG TRAIN Batch 11/6100 loss 13.404250 loss_att 20.058533 loss_ctc 17.645117 loss_rnnt 11.353314 hw_loss 0.289929 lr 0.00050542 rank 4
2023-02-23 00:04:15,112 DEBUG TRAIN Batch 11/6100 loss 11.657933 loss_att 14.907933 loss_ctc 13.177052 loss_rnnt 10.692743 hw_loss 0.211201 lr 0.00050542 rank 2
2023-02-23 00:04:15,113 DEBUG TRAIN Batch 11/6100 loss 10.136984 loss_att 14.568928 loss_ctc 11.382790 loss_rnnt 8.926748 hw_loss 0.295760 lr 0.00050542 rank 5
2023-02-23 00:04:15,117 DEBUG TRAIN Batch 11/6100 loss 13.588715 loss_att 18.164360 loss_ctc 12.691347 loss_rnnt 12.653584 hw_loss 0.261844 lr 0.00050542 rank 7
2023-02-23 00:04:15,117 DEBUG TRAIN Batch 11/6100 loss 22.286724 loss_att 27.328318 loss_ctc 20.117479 loss_rnnt 21.405470 hw_loss 0.304067 lr 0.00050542 rank 6
2023-02-23 00:04:15,117 DEBUG TRAIN Batch 11/6100 loss 15.898687 loss_att 21.565210 loss_ctc 18.635260 loss_rnnt 14.246702 hw_loss 0.288381 lr 0.00050542 rank 3
2023-02-23 00:04:15,122 DEBUG TRAIN Batch 11/6100 loss 32.053383 loss_att 38.550804 loss_ctc 31.682472 loss_rnnt 30.656855 hw_loss 0.274687 lr 0.00050542 rank 1
2023-02-23 00:05:30,456 DEBUG TRAIN Batch 11/6200 loss 16.830103 loss_att 23.250256 loss_ctc 18.508516 loss_rnnt 15.165161 hw_loss 0.294602 lr 0.00050517 rank 5
2023-02-23 00:05:30,461 DEBUG TRAIN Batch 11/6200 loss 21.605026 loss_att 26.561546 loss_ctc 25.381292 loss_rnnt 19.991016 hw_loss 0.223506 lr 0.00050517 rank 1
2023-02-23 00:05:30,461 DEBUG TRAIN Batch 11/6200 loss 23.370045 loss_att 29.867970 loss_ctc 32.896343 loss_rnnt 20.636057 hw_loss 0.307925 lr 0.00050517 rank 0
2023-02-23 00:05:30,462 DEBUG TRAIN Batch 11/6200 loss 26.795614 loss_att 32.063622 loss_ctc 36.395859 loss_rnnt 24.314346 hw_loss 0.276815 lr 0.00050517 rank 2
2023-02-23 00:05:30,463 DEBUG TRAIN Batch 11/6200 loss 13.191741 loss_att 16.093132 loss_ctc 14.244131 loss_rnnt 12.272324 hw_loss 0.372789 lr 0.00050517 rank 4
2023-02-23 00:05:30,463 DEBUG TRAIN Batch 11/6200 loss 23.216850 loss_att 26.582443 loss_ctc 24.236210 loss_rnnt 22.270142 hw_loss 0.258142 lr 0.00050517 rank 7
2023-02-23 00:05:30,468 DEBUG TRAIN Batch 11/6200 loss 23.527828 loss_att 26.963520 loss_ctc 21.928394 loss_rnnt 22.913639 hw_loss 0.263081 lr 0.00050517 rank 6
2023-02-23 00:05:30,470 DEBUG TRAIN Batch 11/6200 loss 22.046577 loss_att 24.341236 loss_ctc 24.639919 loss_rnnt 21.052906 hw_loss 0.354297 lr 0.00050517 rank 3
2023-02-23 00:06:47,830 DEBUG TRAIN Batch 11/6300 loss 14.096118 loss_att 19.708885 loss_ctc 18.351166 loss_rnnt 12.235689 hw_loss 0.319753 lr 0.00050491 rank 5
2023-02-23 00:06:47,831 DEBUG TRAIN Batch 11/6300 loss 24.322941 loss_att 31.346331 loss_ctc 29.882114 loss_rnnt 22.021717 hw_loss 0.291230 lr 0.00050491 rank 0
2023-02-23 00:06:47,833 DEBUG TRAIN Batch 11/6300 loss 20.612787 loss_att 26.959282 loss_ctc 20.527584 loss_rnnt 19.239841 hw_loss 0.215641 lr 0.00050491 rank 6
2023-02-23 00:06:47,833 DEBUG TRAIN Batch 11/6300 loss 14.103993 loss_att 17.918726 loss_ctc 18.009069 loss_rnnt 12.638793 hw_loss 0.340457 lr 0.00050491 rank 4
2023-02-23 00:06:47,834 DEBUG TRAIN Batch 11/6300 loss 20.411398 loss_att 24.765732 loss_ctc 26.957817 loss_rnnt 18.485704 hw_loss 0.341190 lr 0.00050491 rank 2
2023-02-23 00:06:47,836 DEBUG TRAIN Batch 11/6300 loss 13.230351 loss_att 21.634596 loss_ctc 17.139339 loss_rnnt 10.865446 hw_loss 0.305360 lr 0.00050491 rank 7
2023-02-23 00:06:47,838 DEBUG TRAIN Batch 11/6300 loss 12.707221 loss_att 15.322308 loss_ctc 13.075565 loss_rnnt 12.024568 hw_loss 0.207231 lr 0.00050491 rank 1
2023-02-23 00:06:47,884 DEBUG TRAIN Batch 11/6300 loss 21.036749 loss_att 22.301369 loss_ctc 22.481428 loss_rnnt 20.447327 hw_loss 0.269767 lr 0.00050491 rank 3
2023-02-23 00:08:07,156 DEBUG TRAIN Batch 11/6400 loss 16.754772 loss_att 19.234787 loss_ctc 18.109245 loss_rnnt 15.869802 hw_loss 0.390699 lr 0.00050465 rank 0
2023-02-23 00:08:07,161 DEBUG TRAIN Batch 11/6400 loss 14.934768 loss_att 19.461079 loss_ctc 14.660979 loss_rnnt 13.905867 hw_loss 0.300272 lr 0.00050465 rank 5
2023-02-23 00:08:07,162 DEBUG TRAIN Batch 11/6400 loss 8.787303 loss_att 16.007296 loss_ctc 10.969031 loss_rnnt 6.888336 hw_loss 0.307633 lr 0.00050465 rank 6
2023-02-23 00:08:07,167 DEBUG TRAIN Batch 11/6400 loss 24.264627 loss_att 30.243099 loss_ctc 26.752285 loss_rnnt 22.573196 hw_loss 0.307593 lr 0.00050465 rank 7
2023-02-23 00:08:07,168 DEBUG TRAIN Batch 11/6400 loss 28.225616 loss_att 32.499535 loss_ctc 27.174774 loss_rnnt 27.402451 hw_loss 0.203429 lr 0.00050465 rank 2
2023-02-23 00:08:07,170 DEBUG TRAIN Batch 11/6400 loss 11.560552 loss_att 13.297390 loss_ctc 13.634883 loss_rnnt 10.732600 hw_loss 0.382511 lr 0.00050465 rank 3
2023-02-23 00:08:07,172 DEBUG TRAIN Batch 11/6400 loss 14.361827 loss_att 15.413637 loss_ctc 17.664436 loss_rnnt 13.523764 hw_loss 0.351289 lr 0.00050465 rank 1
2023-02-23 00:08:07,172 DEBUG TRAIN Batch 11/6400 loss 34.035339 loss_att 42.849350 loss_ctc 35.236885 loss_rnnt 32.007927 hw_loss 0.195747 lr 0.00050465 rank 4
2023-02-23 00:09:22,919 DEBUG TRAIN Batch 11/6500 loss 27.318058 loss_att 21.375183 loss_ctc 32.638229 loss_rnnt 27.689674 hw_loss 0.201753 lr 0.00050439 rank 0
2023-02-23 00:09:22,920 DEBUG TRAIN Batch 11/6500 loss 14.722111 loss_att 19.850227 loss_ctc 17.890682 loss_rnnt 13.102989 hw_loss 0.320667 lr 0.00050439 rank 5
2023-02-23 00:09:22,921 DEBUG TRAIN Batch 11/6500 loss 17.190641 loss_att 26.841660 loss_ctc 19.153877 loss_rnnt 14.854467 hw_loss 0.270386 lr 0.00050439 rank 4
2023-02-23 00:09:22,922 DEBUG TRAIN Batch 11/6500 loss 8.444598 loss_att 15.058882 loss_ctc 5.249408 loss_rnnt 7.368057 hw_loss 0.336954 lr 0.00050439 rank 6
2023-02-23 00:09:22,923 DEBUG TRAIN Batch 11/6500 loss 22.233816 loss_att 28.995712 loss_ctc 29.770981 loss_rnnt 19.767754 hw_loss 0.203861 lr 0.00050439 rank 2
2023-02-23 00:09:22,924 DEBUG TRAIN Batch 11/6500 loss 19.045675 loss_att 22.289291 loss_ctc 19.426464 loss_rnnt 18.248150 hw_loss 0.183806 lr 0.00050439 rank 3
2023-02-23 00:09:22,925 DEBUG TRAIN Batch 11/6500 loss 19.743696 loss_att 26.911423 loss_ctc 24.675146 loss_rnnt 17.522291 hw_loss 0.244372 lr 0.00050439 rank 1
2023-02-23 00:09:22,930 DEBUG TRAIN Batch 11/6500 loss 13.725208 loss_att 18.744528 loss_ctc 14.470850 loss_rnnt 12.443876 hw_loss 0.333843 lr 0.00050439 rank 7
2023-02-23 00:10:38,766 DEBUG TRAIN Batch 11/6600 loss 14.023114 loss_att 22.744343 loss_ctc 21.558722 loss_rnnt 11.177871 hw_loss 0.180468 lr 0.00050414 rank 2
2023-02-23 00:10:38,769 DEBUG TRAIN Batch 11/6600 loss 15.208615 loss_att 15.688927 loss_ctc 22.585155 loss_rnnt 13.949276 hw_loss 0.337011 lr 0.00050414 rank 3
2023-02-23 00:10:38,769 DEBUG TRAIN Batch 11/6600 loss 25.187391 loss_att 31.779598 loss_ctc 26.199804 loss_rnnt 23.523817 hw_loss 0.394027 lr 0.00050414 rank 4
2023-02-23 00:10:38,772 DEBUG TRAIN Batch 11/6600 loss 21.322500 loss_att 23.008905 loss_ctc 27.466991 loss_rnnt 20.060123 hw_loss 0.198430 lr 0.00050414 rank 5
2023-02-23 00:10:38,773 DEBUG TRAIN Batch 11/6600 loss 23.009430 loss_att 27.803074 loss_ctc 30.488136 loss_rnnt 20.892765 hw_loss 0.301452 lr 0.00050414 rank 0
2023-02-23 00:10:38,779 DEBUG TRAIN Batch 11/6600 loss 14.333037 loss_att 23.210009 loss_ctc 17.268589 loss_rnnt 12.026843 hw_loss 0.261362 lr 0.00050414 rank 1
2023-02-23 00:10:38,779 DEBUG TRAIN Batch 11/6600 loss 25.177870 loss_att 28.608841 loss_ctc 27.027756 loss_rnnt 24.113892 hw_loss 0.245872 lr 0.00050414 rank 6
2023-02-23 00:10:38,782 DEBUG TRAIN Batch 11/6600 loss 19.600868 loss_att 22.172722 loss_ctc 23.816126 loss_rnnt 18.405949 hw_loss 0.222213 lr 0.00050414 rank 7
2023-02-23 00:11:55,992 DEBUG TRAIN Batch 11/6700 loss 13.304044 loss_att 18.414833 loss_ctc 13.716757 loss_rnnt 12.066792 hw_loss 0.300124 lr 0.00050388 rank 3
2023-02-23 00:11:55,995 DEBUG TRAIN Batch 11/6700 loss 24.926453 loss_att 32.729298 loss_ctc 28.983793 loss_rnnt 22.642727 hw_loss 0.341580 lr 0.00050388 rank 5
2023-02-23 00:11:55,995 DEBUG TRAIN Batch 11/6700 loss 19.701384 loss_att 27.146559 loss_ctc 21.756638 loss_rnnt 17.805824 hw_loss 0.248416 lr 0.00050388 rank 2
2023-02-23 00:11:55,996 DEBUG TRAIN Batch 11/6700 loss 24.275120 loss_att 28.368061 loss_ctc 32.681664 loss_rnnt 22.173407 hw_loss 0.304227 lr 0.00050388 rank 0
2023-02-23 00:11:55,997 DEBUG TRAIN Batch 11/6700 loss 24.640638 loss_att 32.420563 loss_ctc 29.254557 loss_rnnt 22.303265 hw_loss 0.311630 lr 0.00050388 rank 7
2023-02-23 00:11:56,000 DEBUG TRAIN Batch 11/6700 loss 18.392662 loss_att 21.225409 loss_ctc 21.355072 loss_rnnt 17.266756 hw_loss 0.308191 lr 0.00050388 rank 4
2023-02-23 00:11:56,001 DEBUG TRAIN Batch 11/6700 loss 19.073315 loss_att 21.448690 loss_ctc 28.633427 loss_rnnt 17.204012 hw_loss 0.224148 lr 0.00050388 rank 1
2023-02-23 00:11:56,002 DEBUG TRAIN Batch 11/6700 loss 7.535284 loss_att 9.756433 loss_ctc 6.653435 loss_rnnt 7.034664 hw_loss 0.326193 lr 0.00050388 rank 6
2023-02-23 00:13:14,537 DEBUG TRAIN Batch 11/6800 loss 23.514078 loss_att 26.378647 loss_ctc 28.615047 loss_rnnt 22.133217 hw_loss 0.239660 lr 0.00050363 rank 4
2023-02-23 00:13:14,542 DEBUG TRAIN Batch 11/6800 loss 12.911476 loss_att 18.873178 loss_ctc 14.643594 loss_rnnt 11.378614 hw_loss 0.205450 lr 0.00050363 rank 2
2023-02-23 00:13:14,543 DEBUG TRAIN Batch 11/6800 loss 9.369654 loss_att 14.102806 loss_ctc 11.950985 loss_rnnt 7.935933 hw_loss 0.267960 lr 0.00050363 rank 5
2023-02-23 00:13:14,547 DEBUG TRAIN Batch 11/6800 loss 20.121815 loss_att 25.603247 loss_ctc 24.887939 loss_rnnt 18.218353 hw_loss 0.321923 lr 0.00050363 rank 0
2023-02-23 00:13:14,548 DEBUG TRAIN Batch 11/6800 loss 15.675490 loss_att 18.088877 loss_ctc 19.629133 loss_rnnt 14.475177 hw_loss 0.357159 lr 0.00050363 rank 6
2023-02-23 00:13:14,550 DEBUG TRAIN Batch 11/6800 loss 23.629568 loss_att 29.068104 loss_ctc 28.918201 loss_rnnt 21.654034 hw_loss 0.342517 lr 0.00050363 rank 1
2023-02-23 00:13:14,552 DEBUG TRAIN Batch 11/6800 loss 15.878909 loss_att 16.132889 loss_ctc 14.019493 loss_rnnt 15.915381 hw_loss 0.301228 lr 0.00050363 rank 7
2023-02-23 00:13:14,552 DEBUG TRAIN Batch 11/6800 loss 7.048366 loss_att 12.725584 loss_ctc 7.695363 loss_rnnt 5.598928 hw_loss 0.426989 lr 0.00050363 rank 3
2023-02-23 00:14:31,616 DEBUG TRAIN Batch 11/6900 loss 9.275955 loss_att 10.820305 loss_ctc 9.489515 loss_rnnt 8.857926 hw_loss 0.151281 lr 0.00050337 rank 2
2023-02-23 00:14:31,619 DEBUG TRAIN Batch 11/6900 loss 37.337982 loss_att 41.962467 loss_ctc 52.059822 loss_rnnt 34.308945 hw_loss 0.264810 lr 0.00050337 rank 5
2023-02-23 00:14:31,621 DEBUG TRAIN Batch 11/6900 loss 10.856329 loss_att 14.829037 loss_ctc 14.169319 loss_rnnt 9.453453 hw_loss 0.312378 lr 0.00050337 rank 0
2023-02-23 00:14:31,623 DEBUG TRAIN Batch 11/6900 loss 9.992534 loss_att 9.957147 loss_ctc 11.183124 loss_rnnt 9.662980 hw_loss 0.333535 lr 0.00050337 rank 6
2023-02-23 00:14:31,624 DEBUG TRAIN Batch 11/6900 loss 7.726215 loss_att 14.852613 loss_ctc 8.438902 loss_rnnt 5.955595 hw_loss 0.469341 lr 0.00050337 rank 4
2023-02-23 00:14:31,628 DEBUG TRAIN Batch 11/6900 loss 37.437115 loss_att 44.653595 loss_ctc 42.784336 loss_rnnt 35.109344 hw_loss 0.321579 lr 0.00050337 rank 1
2023-02-23 00:14:31,630 DEBUG TRAIN Batch 11/6900 loss 18.127451 loss_att 19.306625 loss_ctc 20.104778 loss_rnnt 17.422741 hw_loss 0.384806 lr 0.00050337 rank 7
2023-02-23 00:14:31,669 DEBUG TRAIN Batch 11/6900 loss 30.217388 loss_att 30.016979 loss_ctc 35.019497 loss_rnnt 29.465233 hw_loss 0.284920 lr 0.00050337 rank 3
2023-02-23 00:15:47,583 DEBUG TRAIN Batch 11/7000 loss 13.891529 loss_att 22.125151 loss_ctc 17.869329 loss_rnnt 11.553370 hw_loss 0.301989 lr 0.00050312 rank 5
2023-02-23 00:15:47,586 DEBUG TRAIN Batch 11/7000 loss 17.776804 loss_att 25.642040 loss_ctc 22.109631 loss_rnnt 15.423452 hw_loss 0.379862 lr 0.00050312 rank 7
2023-02-23 00:15:47,586 DEBUG TRAIN Batch 11/7000 loss 19.657286 loss_att 20.851059 loss_ctc 20.941669 loss_rnnt 19.077570 hw_loss 0.318204 lr 0.00050312 rank 3
2023-02-23 00:15:47,586 DEBUG TRAIN Batch 11/7000 loss 14.434764 loss_att 18.171719 loss_ctc 14.903284 loss_rnnt 13.451732 hw_loss 0.324697 lr 0.00050312 rank 0
2023-02-23 00:15:47,586 DEBUG TRAIN Batch 11/7000 loss 10.592023 loss_att 10.804628 loss_ctc 12.787743 loss_rnnt 10.021940 hw_loss 0.440248 lr 0.00050312 rank 2
2023-02-23 00:15:47,587 DEBUG TRAIN Batch 11/7000 loss 15.580832 loss_att 22.885530 loss_ctc 18.767660 loss_rnnt 13.478095 hw_loss 0.406664 lr 0.00050312 rank 1
2023-02-23 00:15:47,587 DEBUG TRAIN Batch 11/7000 loss 12.445375 loss_att 14.727201 loss_ctc 15.463466 loss_rnnt 11.377743 hw_loss 0.391603 lr 0.00050312 rank 4
2023-02-23 00:15:47,588 DEBUG TRAIN Batch 11/7000 loss 22.704006 loss_att 32.265739 loss_ctc 29.056829 loss_rnnt 19.785055 hw_loss 0.299178 lr 0.00050312 rank 6
2023-02-23 00:17:07,035 DEBUG TRAIN Batch 11/7100 loss 19.360308 loss_att 26.246693 loss_ctc 20.670650 loss_rnnt 17.624235 hw_loss 0.345158 lr 0.00050286 rank 7
2023-02-23 00:17:07,033 DEBUG TRAIN Batch 11/7100 loss 17.499794 loss_att 19.357662 loss_ctc 23.503338 loss_rnnt 16.096230 hw_loss 0.434096 lr 0.00050286 rank 0
2023-02-23 00:17:07,035 DEBUG TRAIN Batch 11/7100 loss 16.821434 loss_att 25.312256 loss_ctc 21.152454 loss_rnnt 14.397840 hw_loss 0.277422 lr 0.00050286 rank 5
2023-02-23 00:17:07,036 DEBUG TRAIN Batch 11/7100 loss 27.977852 loss_att 34.263199 loss_ctc 29.317059 loss_rnnt 26.418167 hw_loss 0.232599 lr 0.00050286 rank 3
2023-02-23 00:17:07,038 DEBUG TRAIN Batch 11/7100 loss 16.636421 loss_att 27.213121 loss_ctc 25.658360 loss_rnnt 13.135511 hw_loss 0.342459 lr 0.00050286 rank 4
2023-02-23 00:17:07,040 DEBUG TRAIN Batch 11/7100 loss 21.688814 loss_att 21.326260 loss_ctc 24.181805 loss_rnnt 21.259930 hw_loss 0.316868 lr 0.00050286 rank 2
2023-02-23 00:17:07,042 DEBUG TRAIN Batch 11/7100 loss 13.216343 loss_att 20.487686 loss_ctc 16.771061 loss_rnnt 11.148253 hw_loss 0.262238 lr 0.00050286 rank 6
2023-02-23 00:17:07,087 DEBUG TRAIN Batch 11/7100 loss 17.364237 loss_att 26.240978 loss_ctc 18.982702 loss_rnnt 15.185751 hw_loss 0.351265 lr 0.00050286 rank 1
2023-02-23 00:18:23,413 DEBUG TRAIN Batch 11/7200 loss 26.879961 loss_att 32.417179 loss_ctc 34.111095 loss_rnnt 24.677628 hw_loss 0.245132 lr 0.00050261 rank 4
2023-02-23 00:18:23,413 DEBUG TRAIN Batch 11/7200 loss 23.535967 loss_att 34.479225 loss_ctc 25.647488 loss_rnnt 20.928644 hw_loss 0.257123 lr 0.00050261 rank 5
2023-02-23 00:18:23,414 DEBUG TRAIN Batch 11/7200 loss 27.474453 loss_att 30.034830 loss_ctc 36.432705 loss_rnnt 25.597965 hw_loss 0.318709 lr 0.00050261 rank 0
2023-02-23 00:18:23,414 DEBUG TRAIN Batch 11/7200 loss 22.062754 loss_att 25.076056 loss_ctc 27.804653 loss_rnnt 20.622990 hw_loss 0.134094 lr 0.00050261 rank 2
2023-02-23 00:18:23,416 DEBUG TRAIN Batch 11/7200 loss 9.600975 loss_att 15.817991 loss_ctc 10.214111 loss_rnnt 8.074986 hw_loss 0.376562 lr 0.00050261 rank 3
2023-02-23 00:18:23,416 DEBUG TRAIN Batch 11/7200 loss 18.042683 loss_att 18.004063 loss_ctc 21.671267 loss_rnnt 17.402456 hw_loss 0.307760 lr 0.00050261 rank 6
2023-02-23 00:18:23,421 DEBUG TRAIN Batch 11/7200 loss 19.634052 loss_att 22.786541 loss_ctc 23.082973 loss_rnnt 18.361307 hw_loss 0.341976 lr 0.00050261 rank 7
2023-02-23 00:18:23,421 DEBUG TRAIN Batch 11/7200 loss 30.925735 loss_att 39.102608 loss_ctc 35.911186 loss_rnnt 28.488989 hw_loss 0.256212 lr 0.00050261 rank 1
2023-02-23 00:19:39,263 DEBUG TRAIN Batch 11/7300 loss 7.903746 loss_att 14.582920 loss_ctc 9.582888 loss_rnnt 6.176496 hw_loss 0.314119 lr 0.00050235 rank 2
2023-02-23 00:19:39,264 DEBUG TRAIN Batch 11/7300 loss 18.512421 loss_att 22.107422 loss_ctc 21.436373 loss_rnnt 17.224983 hw_loss 0.334829 lr 0.00050235 rank 5
2023-02-23 00:19:39,267 DEBUG TRAIN Batch 11/7300 loss 12.864982 loss_att 16.418602 loss_ctc 16.057360 loss_rnnt 11.544870 hw_loss 0.344504 lr 0.00050235 rank 7
2023-02-23 00:19:39,267 DEBUG TRAIN Batch 11/7300 loss 10.778825 loss_att 12.252482 loss_ctc 11.180544 loss_rnnt 10.280535 hw_loss 0.281243 lr 0.00050235 rank 1
2023-02-23 00:19:39,267 DEBUG TRAIN Batch 11/7300 loss 11.547931 loss_att 12.786883 loss_ctc 12.351208 loss_rnnt 11.071727 hw_loss 0.227456 lr 0.00050235 rank 4
2023-02-23 00:19:39,268 DEBUG TRAIN Batch 11/7300 loss 9.118231 loss_att 13.526937 loss_ctc 10.624198 loss_rnnt 7.869562 hw_loss 0.311499 lr 0.00050235 rank 0
2023-02-23 00:19:39,273 DEBUG TRAIN Batch 11/7300 loss 16.796444 loss_att 20.344505 loss_ctc 19.187525 loss_rnnt 15.648276 hw_loss 0.224519 lr 0.00050235 rank 3
2023-02-23 00:19:39,316 DEBUG TRAIN Batch 11/7300 loss 11.811373 loss_att 15.273491 loss_ctc 15.124188 loss_rnnt 10.555058 hw_loss 0.229091 lr 0.00050235 rank 6
2023-02-23 00:20:56,104 DEBUG TRAIN Batch 11/7400 loss 15.827824 loss_att 22.276043 loss_ctc 19.612490 loss_rnnt 13.918560 hw_loss 0.215620 lr 0.00050210 rank 4
2023-02-23 00:20:56,106 DEBUG TRAIN Batch 11/7400 loss 16.791859 loss_att 22.113802 loss_ctc 19.804207 loss_rnnt 15.233774 hw_loss 0.172593 lr 0.00050210 rank 5
2023-02-23 00:20:56,108 DEBUG TRAIN Batch 11/7400 loss 20.350471 loss_att 29.101078 loss_ctc 26.801996 loss_rnnt 17.583241 hw_loss 0.294198 lr 0.00050210 rank 1
2023-02-23 00:20:56,108 DEBUG TRAIN Batch 11/7400 loss 23.534315 loss_att 30.027435 loss_ctc 27.815788 loss_rnnt 21.485481 hw_loss 0.336274 lr 0.00050210 rank 0
2023-02-23 00:20:56,110 DEBUG TRAIN Batch 11/7400 loss 24.079777 loss_att 28.466759 loss_ctc 28.418190 loss_rnnt 22.502922 hw_loss 0.226880 lr 0.00050210 rank 6
2023-02-23 00:20:56,111 DEBUG TRAIN Batch 11/7400 loss 13.907538 loss_att 18.425259 loss_ctc 16.192999 loss_rnnt 12.536383 hw_loss 0.305407 lr 0.00050210 rank 3
2023-02-23 00:20:56,112 DEBUG TRAIN Batch 11/7400 loss 21.554203 loss_att 26.173532 loss_ctc 27.105347 loss_rnnt 19.768196 hw_loss 0.228730 lr 0.00050210 rank 2
2023-02-23 00:20:56,114 DEBUG TRAIN Batch 11/7400 loss 15.197519 loss_att 19.343842 loss_ctc 20.167538 loss_rnnt 13.608655 hw_loss 0.181743 lr 0.00050210 rank 7
2023-02-23 00:22:15,912 DEBUG TRAIN Batch 11/7500 loss 19.407444 loss_att 23.054583 loss_ctc 23.490051 loss_rnnt 17.950134 hw_loss 0.344128 lr 0.00050185 rank 0
2023-02-23 00:22:15,916 DEBUG TRAIN Batch 11/7500 loss 18.833487 loss_att 26.423080 loss_ctc 21.053877 loss_rnnt 16.852545 hw_loss 0.313074 lr 0.00050185 rank 5
2023-02-23 00:22:15,917 DEBUG TRAIN Batch 11/7500 loss 18.428120 loss_att 20.712118 loss_ctc 20.569675 loss_rnnt 17.564247 hw_loss 0.227872 lr 0.00050185 rank 3
2023-02-23 00:22:15,917 DEBUG TRAIN Batch 11/7500 loss 25.812754 loss_att 27.260178 loss_ctc 28.420538 loss_rnnt 25.027967 hw_loss 0.276741 lr 0.00050185 rank 2
2023-02-23 00:22:15,917 DEBUG TRAIN Batch 11/7500 loss 22.871305 loss_att 23.243595 loss_ctc 26.889675 loss_rnnt 22.111652 hw_loss 0.280147 lr 0.00050185 rank 4
2023-02-23 00:22:15,917 DEBUG TRAIN Batch 11/7500 loss 13.073799 loss_att 17.510401 loss_ctc 15.812678 loss_rnnt 11.643031 hw_loss 0.334243 lr 0.00050185 rank 6
2023-02-23 00:22:15,925 DEBUG TRAIN Batch 11/7500 loss 12.460582 loss_att 15.085350 loss_ctc 17.712936 loss_rnnt 11.107239 hw_loss 0.240142 lr 0.00050185 rank 7
2023-02-23 00:22:15,969 DEBUG TRAIN Batch 11/7500 loss 14.296507 loss_att 23.182220 loss_ctc 19.088793 loss_rnnt 11.641245 hw_loss 0.448401 lr 0.00050185 rank 1
2023-02-23 00:23:33,581 DEBUG TRAIN Batch 11/7600 loss 15.827019 loss_att 19.833143 loss_ctc 15.331164 loss_rnnt 14.901725 hw_loss 0.356593 lr 0.00050160 rank 3
2023-02-23 00:23:33,585 DEBUG TRAIN Batch 11/7600 loss 18.694485 loss_att 19.479822 loss_ctc 20.649639 loss_rnnt 18.087202 hw_loss 0.355363 lr 0.00050160 rank 2
2023-02-23 00:23:33,585 DEBUG TRAIN Batch 11/7600 loss 14.394196 loss_att 20.306093 loss_ctc 17.860172 loss_rnnt 12.574404 hw_loss 0.328651 lr 0.00050160 rank 0
2023-02-23 00:23:33,585 DEBUG TRAIN Batch 11/7600 loss 19.553764 loss_att 22.561474 loss_ctc 22.231392 loss_rnnt 18.409027 hw_loss 0.349082 lr 0.00050160 rank 5
2023-02-23 00:23:33,587 DEBUG TRAIN Batch 11/7600 loss 17.530785 loss_att 20.600777 loss_ctc 20.625837 loss_rnnt 16.324965 hw_loss 0.335902 lr 0.00050160 rank 4
2023-02-23 00:23:33,588 DEBUG TRAIN Batch 11/7600 loss 26.395617 loss_att 45.291042 loss_ctc 32.739868 loss_rnnt 21.597830 hw_loss 0.323999 lr 0.00050160 rank 6
2023-02-23 00:23:33,593 DEBUG TRAIN Batch 11/7600 loss 15.125590 loss_att 24.430258 loss_ctc 17.206968 loss_rnnt 12.852955 hw_loss 0.251595 lr 0.00050160 rank 7
2023-02-23 00:23:33,899 DEBUG TRAIN Batch 11/7600 loss 19.964630 loss_att 22.659214 loss_ctc 24.746931 loss_rnnt 18.596918 hw_loss 0.358417 lr 0.00050160 rank 1
2023-02-23 00:24:49,279 DEBUG TRAIN Batch 11/7700 loss 15.207273 loss_att 14.009272 loss_ctc 17.861933 loss_rnnt 14.861852 hw_loss 0.433252 lr 0.00050134 rank 1
2023-02-23 00:24:49,279 DEBUG TRAIN Batch 11/7700 loss 16.102585 loss_att 18.858393 loss_ctc 16.922939 loss_rnnt 15.269748 hw_loss 0.323051 lr 0.00050134 rank 0
2023-02-23 00:24:49,281 DEBUG TRAIN Batch 11/7700 loss 37.138794 loss_att 44.760399 loss_ctc 44.189617 loss_rnnt 34.567101 hw_loss 0.201115 lr 0.00050134 rank 4
2023-02-23 00:24:49,282 DEBUG TRAIN Batch 11/7700 loss 19.742193 loss_att 25.634945 loss_ctc 20.383804 loss_rnnt 18.366848 hw_loss 0.208587 lr 0.00050134 rank 7
2023-02-23 00:24:49,284 DEBUG TRAIN Batch 11/7700 loss 13.298778 loss_att 19.569363 loss_ctc 14.248919 loss_rnnt 11.721626 hw_loss 0.368151 lr 0.00050134 rank 2
2023-02-23 00:24:49,284 DEBUG TRAIN Batch 11/7700 loss 18.951649 loss_att 25.097624 loss_ctc 22.229530 loss_rnnt 17.087826 hw_loss 0.370453 lr 0.00050134 rank 5
2023-02-23 00:24:49,286 DEBUG TRAIN Batch 11/7700 loss 14.407127 loss_att 22.485249 loss_ctc 19.065578 loss_rnnt 12.033839 hw_loss 0.256007 lr 0.00050134 rank 6
2023-02-23 00:24:49,332 DEBUG TRAIN Batch 11/7700 loss 19.166897 loss_att 27.784031 loss_ctc 24.177914 loss_rnnt 16.642696 hw_loss 0.248696 lr 0.00050134 rank 3
2023-02-23 00:26:07,657 DEBUG TRAIN Batch 11/7800 loss 18.085169 loss_att 21.552807 loss_ctc 21.837751 loss_rnnt 16.724262 hw_loss 0.313193 lr 0.00050109 rank 4
2023-02-23 00:26:07,660 DEBUG TRAIN Batch 11/7800 loss 16.177292 loss_att 23.075298 loss_ctc 19.391151 loss_rnnt 14.200076 hw_loss 0.317061 lr 0.00050109 rank 3
2023-02-23 00:26:07,663 DEBUG TRAIN Batch 11/7800 loss 15.710430 loss_att 17.632812 loss_ctc 22.294167 loss_rnnt 14.329894 hw_loss 0.221676 lr 0.00050109 rank 1
2023-02-23 00:26:07,664 DEBUG TRAIN Batch 11/7800 loss 23.977810 loss_att 29.867373 loss_ctc 30.404980 loss_rnnt 21.789822 hw_loss 0.287099 lr 0.00050109 rank 6
2023-02-23 00:26:07,669 DEBUG TRAIN Batch 11/7800 loss 26.417610 loss_att 27.447414 loss_ctc 36.087566 loss_rnnt 24.787518 hw_loss 0.252760 lr 0.00050109 rank 7
2023-02-23 00:26:07,684 DEBUG TRAIN Batch 11/7800 loss 9.224789 loss_att 15.993958 loss_ctc 13.259940 loss_rnnt 7.215391 hw_loss 0.220395 lr 0.00050109 rank 5
2023-02-23 00:26:07,684 DEBUG TRAIN Batch 11/7800 loss 18.033226 loss_att 23.424641 loss_ctc 25.471708 loss_rnnt 15.836020 hw_loss 0.238359 lr 0.00050109 rank 2
2023-02-23 00:26:07,719 DEBUG TRAIN Batch 11/7800 loss 12.826919 loss_att 19.528175 loss_ctc 15.208060 loss_rnnt 11.023601 hw_loss 0.272963 lr 0.00050109 rank 0
2023-02-23 00:27:24,993 DEBUG TRAIN Batch 11/7900 loss 9.032343 loss_att 12.036394 loss_ctc 10.661713 loss_rnnt 8.040591 hw_loss 0.325672 lr 0.00050084 rank 1
2023-02-23 00:27:24,993 DEBUG TRAIN Batch 11/7900 loss 31.488440 loss_att 37.076611 loss_ctc 35.805523 loss_rnnt 29.676283 hw_loss 0.222953 lr 0.00050084 rank 2
2023-02-23 00:27:24,993 DEBUG TRAIN Batch 11/7900 loss 23.657469 loss_att 25.108910 loss_ctc 29.895742 loss_rnnt 22.387810 hw_loss 0.276750 lr 0.00050084 rank 0
2023-02-23 00:27:24,995 DEBUG TRAIN Batch 11/7900 loss 14.451920 loss_att 21.432037 loss_ctc 14.652456 loss_rnnt 12.845486 hw_loss 0.344387 lr 0.00050084 rank 4
2023-02-23 00:27:24,996 DEBUG TRAIN Batch 11/7900 loss 14.320180 loss_att 17.618576 loss_ctc 14.810759 loss_rnnt 13.442590 hw_loss 0.285937 lr 0.00050084 rank 7
2023-02-23 00:27:24,997 DEBUG TRAIN Batch 11/7900 loss 21.243570 loss_att 29.741087 loss_ctc 27.584070 loss_rnnt 18.566099 hw_loss 0.248567 lr 0.00050084 rank 5
2023-02-23 00:27:25,000 DEBUG TRAIN Batch 11/7900 loss 38.307076 loss_att 34.867767 loss_ctc 46.184593 loss_rnnt 37.766144 hw_loss 0.334609 lr 0.00050084 rank 6
2023-02-23 00:27:25,047 DEBUG TRAIN Batch 11/7900 loss 11.067276 loss_att 15.942724 loss_ctc 10.820675 loss_rnnt 9.968977 hw_loss 0.292668 lr 0.00050084 rank 3
2023-02-23 00:28:40,897 DEBUG TRAIN Batch 11/8000 loss 19.004827 loss_att 22.882582 loss_ctc 24.668871 loss_rnnt 17.334455 hw_loss 0.261779 lr 0.00050059 rank 2
2023-02-23 00:28:40,899 DEBUG TRAIN Batch 11/8000 loss 24.356882 loss_att 39.127842 loss_ctc 28.583290 loss_rnnt 20.691494 hw_loss 0.276889 lr 0.00050059 rank 4
2023-02-23 00:28:40,902 DEBUG TRAIN Batch 11/8000 loss 21.172285 loss_att 23.408144 loss_ctc 25.883259 loss_rnnt 19.980515 hw_loss 0.218380 lr 0.00050059 rank 7
2023-02-23 00:28:40,901 DEBUG TRAIN Batch 11/8000 loss 20.682852 loss_att 25.210606 loss_ctc 27.171799 loss_rnnt 18.763763 hw_loss 0.278144 lr 0.00050059 rank 5
2023-02-23 00:28:40,904 DEBUG TRAIN Batch 11/8000 loss 14.816083 loss_att 19.896912 loss_ctc 19.527767 loss_rnnt 13.021069 hw_loss 0.282418 lr 0.00050059 rank 6
2023-02-23 00:28:40,904 DEBUG TRAIN Batch 11/8000 loss 11.623655 loss_att 19.934416 loss_ctc 16.530130 loss_rnnt 9.218380 hw_loss 0.166736 lr 0.00050059 rank 0
2023-02-23 00:28:40,906 DEBUG TRAIN Batch 11/8000 loss 20.194582 loss_att 21.945326 loss_ctc 22.001772 loss_rnnt 19.402695 hw_loss 0.376461 lr 0.00050059 rank 3
2023-02-23 00:28:40,952 DEBUG TRAIN Batch 11/8000 loss 15.939034 loss_att 21.010372 loss_ctc 16.901423 loss_rnnt 14.678135 hw_loss 0.221834 lr 0.00050059 rank 1
2023-02-23 00:29:57,750 DEBUG TRAIN Batch 11/8100 loss 16.623499 loss_att 23.961033 loss_ctc 18.680292 loss_rnnt 14.757380 hw_loss 0.233199 lr 0.00050034 rank 5
2023-02-23 00:29:57,754 DEBUG TRAIN Batch 11/8100 loss 16.036505 loss_att 22.919502 loss_ctc 16.187635 loss_rnnt 14.481377 hw_loss 0.296957 lr 0.00050034 rank 3
2023-02-23 00:29:57,755 DEBUG TRAIN Batch 11/8100 loss 19.172426 loss_att 22.361553 loss_ctc 25.248308 loss_rnnt 17.564798 hw_loss 0.299406 lr 0.00050034 rank 2
2023-02-23 00:29:57,757 DEBUG TRAIN Batch 11/8100 loss 17.916758 loss_att 25.149399 loss_ctc 18.292734 loss_rnnt 16.273424 hw_loss 0.275015 lr 0.00050034 rank 1
2023-02-23 00:29:57,758 DEBUG TRAIN Batch 11/8100 loss 20.211985 loss_att 28.428192 loss_ctc 23.855062 loss_rnnt 17.912556 hw_loss 0.319581 lr 0.00050034 rank 0
2023-02-23 00:29:57,758 DEBUG TRAIN Batch 11/8100 loss 15.723130 loss_att 21.241003 loss_ctc 18.295506 loss_rnnt 14.100504 hw_loss 0.330126 lr 0.00050034 rank 4
2023-02-23 00:29:57,761 DEBUG TRAIN Batch 11/8100 loss 21.819239 loss_att 23.885796 loss_ctc 27.290350 loss_rnnt 20.460155 hw_loss 0.405543 lr 0.00050034 rank 6
2023-02-23 00:29:57,765 DEBUG TRAIN Batch 11/8100 loss 25.713100 loss_att 29.501328 loss_ctc 31.903339 loss_rnnt 23.946501 hw_loss 0.344228 lr 0.00050034 rank 7
2023-02-23 00:31:13,936 DEBUG TRAIN Batch 11/8200 loss 10.678501 loss_att 11.872422 loss_ctc 12.392563 loss_rnnt 10.008696 hw_loss 0.379648 lr 0.00050009 rank 3
2023-02-23 00:31:13,937 DEBUG TRAIN Batch 11/8200 loss 19.361998 loss_att 23.886681 loss_ctc 25.427950 loss_rnnt 17.460224 hw_loss 0.352577 lr 0.00050009 rank 4
2023-02-23 00:31:13,939 DEBUG TRAIN Batch 11/8200 loss 19.365536 loss_att 26.172104 loss_ctc 26.207996 loss_rnnt 16.921030 hw_loss 0.320369 lr 0.00050009 rank 5
2023-02-23 00:31:13,942 DEBUG TRAIN Batch 11/8200 loss 16.903732 loss_att 29.928513 loss_ctc 20.825096 loss_rnnt 13.567512 hw_loss 0.390777 lr 0.00050009 rank 7
2023-02-23 00:31:13,943 DEBUG TRAIN Batch 11/8200 loss 12.218127 loss_att 15.292219 loss_ctc 12.428732 loss_rnnt 11.415806 hw_loss 0.298917 lr 0.00050009 rank 0
2023-02-23 00:31:13,943 DEBUG TRAIN Batch 11/8200 loss 19.753078 loss_att 19.069221 loss_ctc 23.374853 loss_rnnt 19.131310 hw_loss 0.516819 lr 0.00050009 rank 6
2023-02-23 00:31:13,944 DEBUG TRAIN Batch 11/8200 loss 13.503542 loss_att 13.685088 loss_ctc 14.406700 loss_rnnt 13.143251 hw_loss 0.381674 lr 0.00050009 rank 2
2023-02-23 00:31:13,947 DEBUG TRAIN Batch 11/8200 loss 26.373318 loss_att 29.361788 loss_ctc 34.918392 loss_rnnt 24.492569 hw_loss 0.269463 lr 0.00050009 rank 1
2023-02-23 00:32:28,929 DEBUG TRAIN Batch 11/8300 loss 11.020511 loss_att 11.756992 loss_ctc 13.427737 loss_rnnt 10.349413 hw_loss 0.380321 lr 0.00049984 rank 4
2023-02-23 00:32:28,930 DEBUG TRAIN Batch 11/8300 loss 30.160727 loss_att 31.300659 loss_ctc 34.378750 loss_rnnt 29.166088 hw_loss 0.382969 lr 0.00049984 rank 3
2023-02-23 00:32:28,933 DEBUG TRAIN Batch 11/8300 loss 18.384279 loss_att 20.599472 loss_ctc 21.287104 loss_rnnt 17.387186 hw_loss 0.313146 lr 0.00049984 rank 1
2023-02-23 00:32:28,933 DEBUG TRAIN Batch 11/8300 loss 14.453418 loss_att 20.455666 loss_ctc 18.161880 loss_rnnt 12.611425 hw_loss 0.275779 lr 0.00049984 rank 5
2023-02-23 00:32:28,933 DEBUG TRAIN Batch 11/8300 loss 19.573109 loss_att 22.120039 loss_ctc 25.759861 loss_rnnt 18.124577 hw_loss 0.214212 lr 0.00049984 rank 0
2023-02-23 00:32:28,935 DEBUG TRAIN Batch 11/8300 loss 19.076321 loss_att 23.804989 loss_ctc 25.414852 loss_rnnt 17.147720 hw_loss 0.258240 lr 0.00049984 rank 2
2023-02-23 00:32:28,938 DEBUG TRAIN Batch 11/8300 loss 19.126358 loss_att 26.643749 loss_ctc 24.904732 loss_rnnt 16.735146 hw_loss 0.219909 lr 0.00049984 rank 6
2023-02-23 00:32:28,941 DEBUG TRAIN Batch 11/8300 loss 12.953549 loss_att 22.092609 loss_ctc 18.263273 loss_rnnt 10.236332 hw_loss 0.340203 lr 0.00049984 rank 7
2023-02-23 00:33:20,976 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 5
2023-02-23 00:33:20,976 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 1
2023-02-23 00:33:20,980 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 6
2023-02-23 00:33:20,981 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 0
2023-02-23 00:33:20,985 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 3
2023-02-23 00:33:20,995 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 7
2023-02-23 00:33:21,005 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 4
2023-02-23 00:33:21,010 DEBUG CV Batch 11/0 loss 3.128786 loss_att 3.478750 loss_ctc 3.524439 loss_rnnt 2.791758 hw_loss 0.401778 history loss 3.012905 rank 2
2023-02-23 00:33:32,062 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 5
2023-02-23 00:33:32,164 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 3
2023-02-23 00:33:32,315 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 6
2023-02-23 00:33:32,602 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 4
2023-02-23 00:33:32,634 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 2
2023-02-23 00:33:32,636 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 7
2023-02-23 00:33:32,669 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 0
2023-02-23 00:33:33,307 DEBUG CV Batch 11/100 loss 11.756503 loss_att 13.219294 loss_ctc 13.052855 loss_rnnt 11.162436 hw_loss 0.241241 history loss 5.571238 rank 1
2023-02-23 00:33:45,983 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 5
2023-02-23 00:33:46,075 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 0
2023-02-23 00:33:46,253 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 3
2023-02-23 00:33:46,426 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 4
2023-02-23 00:33:46,452 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 6
2023-02-23 00:33:46,505 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 2
2023-02-23 00:33:46,757 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 1
2023-02-23 00:33:47,500 DEBUG CV Batch 11/200 loss 12.919263 loss_att 32.246174 loss_ctc 12.970490 loss_rnnt 8.929920 hw_loss 0.219619 history loss 6.314532 rank 7
2023-02-23 00:33:58,234 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 0
2023-02-23 00:33:58,387 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 5
2023-02-23 00:33:58,495 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 4
2023-02-23 00:33:58,561 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 2
2023-02-23 00:33:58,882 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 3
2023-02-23 00:33:59,021 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 1
2023-02-23 00:33:59,164 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 6
2023-02-23 00:34:00,111 DEBUG CV Batch 11/300 loss 8.317227 loss_att 8.931946 loss_ctc 9.211855 loss_rnnt 7.896314 hw_loss 0.335035 history loss 6.490626 rank 7
2023-02-23 00:34:10,277 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 0
2023-02-23 00:34:10,287 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 5
2023-02-23 00:34:10,431 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 2
2023-02-23 00:34:10,472 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 4
2023-02-23 00:34:10,844 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 3
2023-02-23 00:34:11,182 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 1
2023-02-23 00:34:12,352 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 7
2023-02-23 00:34:12,372 DEBUG CV Batch 11/400 loss 30.396719 loss_att 125.080338 loss_ctc 16.791584 loss_rnnt 13.133312 hw_loss 0.263817 history loss 7.729588 rank 6
2023-02-23 00:34:20,636 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 5
2023-02-23 00:34:20,785 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 0
2023-02-23 00:34:20,891 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 2
2023-02-23 00:34:21,064 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 4
2023-02-23 00:34:21,356 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 3
2023-02-23 00:34:21,797 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 1
2023-02-23 00:34:23,079 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 7
2023-02-23 00:34:23,663 DEBUG CV Batch 11/500 loss 8.391816 loss_att 9.547140 loss_ctc 10.813599 loss_rnnt 7.704226 hw_loss 0.250539 history loss 8.867378 rank 6
2023-02-23 00:34:32,791 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 5
2023-02-23 00:34:32,912 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 0
2023-02-23 00:34:33,094 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 2
2023-02-23 00:34:33,097 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 4
2023-02-23 00:34:33,506 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 3
2023-02-23 00:34:33,988 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 1
2023-02-23 00:34:35,406 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 7
2023-02-23 00:34:35,853 DEBUG CV Batch 11/600 loss 11.873155 loss_att 11.901749 loss_ctc 14.100367 loss_rnnt 11.364360 hw_loss 0.386465 history loss 10.031512 rank 6
2023-02-23 00:34:44,713 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 0
2023-02-23 00:34:45,115 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 5
2023-02-23 00:34:45,379 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 4
2023-02-23 00:34:45,422 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 1
2023-02-23 00:34:45,537 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 2
2023-02-23 00:34:46,154 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 3
2023-02-23 00:34:47,359 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 6
2023-02-23 00:34:47,532 DEBUG CV Batch 11/700 loss 37.853081 loss_att 86.433945 loss_ctc 28.258221 loss_rnnt 29.263420 hw_loss 0.286502 history loss 10.990725 rank 7
2023-02-23 00:34:56,685 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 0
2023-02-23 00:34:57,400 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 5
2023-02-23 00:34:57,465 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 1
2023-02-23 00:34:57,471 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 2
2023-02-23 00:34:57,504 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 4
2023-02-23 00:34:57,971 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 3
2023-02-23 00:34:58,751 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 6
2023-02-23 00:35:00,026 DEBUG CV Batch 11/800 loss 17.750866 loss_att 16.587532 loss_ctc 17.837288 loss_rnnt 17.754225 hw_loss 0.408347 history loss 10.262087 rank 7
2023-02-23 00:35:10,753 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 0
2023-02-23 00:35:11,133 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 5
2023-02-23 00:35:11,406 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 4
2023-02-23 00:35:11,431 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 2
2023-02-23 00:35:11,726 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 1
2023-02-23 00:35:12,190 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 3
2023-02-23 00:35:12,243 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 6
2023-02-23 00:35:13,974 DEBUG CV Batch 11/900 loss 16.620083 loss_att 39.967060 loss_ctc 24.548241 loss_rnnt 10.762026 hw_loss 0.246697 history loss 9.991067 rank 7
2023-02-23 00:35:22,975 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 0
2023-02-23 00:35:23,243 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 5
2023-02-23 00:35:23,591 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 4
2023-02-23 00:35:23,606 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 2
2023-02-23 00:35:24,288 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 1
2023-02-23 00:35:24,396 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 3
2023-02-23 00:35:24,706 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 6
2023-02-23 00:35:26,517 DEBUG CV Batch 11/1000 loss 5.130908 loss_att 6.552398 loss_ctc 6.596751 loss_rnnt 4.492386 hw_loss 0.297710 history loss 9.667486 rank 7
2023-02-23 00:35:34,925 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 0
2023-02-23 00:35:34,995 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 5
2023-02-23 00:35:35,435 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 4
2023-02-23 00:35:35,437 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 2
2023-02-23 00:35:36,255 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 3
2023-02-23 00:35:36,298 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 1
2023-02-23 00:35:36,818 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 6
2023-02-23 00:35:38,630 DEBUG CV Batch 11/1100 loss 9.105987 loss_att 7.446632 loss_ctc 10.353132 loss_rnnt 9.063322 hw_loss 0.390468 history loss 9.633352 rank 7
2023-02-23 00:35:45,381 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 5
2023-02-23 00:35:45,386 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 0
2023-02-23 00:35:45,853 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 4
2023-02-23 00:35:45,884 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 2
2023-02-23 00:35:46,819 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 1
2023-02-23 00:35:47,110 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 3
2023-02-23 00:35:47,413 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 6
2023-02-23 00:35:49,414 DEBUG CV Batch 11/1200 loss 13.134251 loss_att 14.386707 loss_ctc 13.974327 loss_rnnt 12.585180 hw_loss 0.349817 history loss 10.106879 rank 7
2023-02-23 00:35:57,251 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 5
2023-02-23 00:35:57,381 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 0
2023-02-23 00:35:57,835 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 2
2023-02-23 00:35:57,864 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 4
2023-02-23 00:35:58,875 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 1
2023-02-23 00:35:59,607 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 6
2023-02-23 00:35:59,844 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 3
2023-02-23 00:36:01,635 DEBUG CV Batch 11/1300 loss 10.337246 loss_att 9.080402 loss_ctc 11.931880 loss_rnnt 10.179849 hw_loss 0.367778 history loss 10.470187 rank 7
2023-02-23 00:36:08,766 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 5
2023-02-23 00:36:08,865 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 0
2023-02-23 00:36:09,322 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 2
2023-02-23 00:36:09,378 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 4
2023-02-23 00:36:10,621 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 1
2023-02-23 00:36:10,954 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 6
2023-02-23 00:36:11,799 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 3
2023-02-23 00:36:13,150 DEBUG CV Batch 11/1400 loss 18.455950 loss_att 71.973221 loss_ctc 8.596819 loss_rnnt 8.942059 hw_loss 0.234351 history loss 10.945703 rank 7
2023-02-23 00:36:21,247 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 0
2023-02-23 00:36:21,378 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 5
2023-02-23 00:36:21,749 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 4
2023-02-23 00:36:21,762 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 2
2023-02-23 00:36:23,131 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 1
2023-02-23 00:36:23,381 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 6
2023-02-23 00:36:24,757 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 3
2023-02-23 00:36:25,831 DEBUG CV Batch 11/1500 loss 10.796935 loss_att 11.281665 loss_ctc 9.168476 loss_rnnt 10.755219 hw_loss 0.303562 history loss 10.684542 rank 7
2023-02-23 00:36:35,088 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 5
2023-02-23 00:36:35,093 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 0
2023-02-23 00:36:35,284 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 4
2023-02-23 00:36:35,367 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 2
2023-02-23 00:36:36,825 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 1
2023-02-23 00:36:37,574 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 6
2023-02-23 00:36:39,083 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 3
2023-02-23 00:36:39,500 DEBUG CV Batch 11/1600 loss 15.812107 loss_att 27.823486 loss_ctc 15.810527 loss_rnnt 13.247128 hw_loss 0.305463 history loss 10.568750 rank 7
2023-02-23 00:36:47,546 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 5
2023-02-23 00:36:47,712 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 0
2023-02-23 00:36:47,827 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 4
2023-02-23 00:36:47,841 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 2
2023-02-23 00:36:49,468 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 1
2023-02-23 00:36:50,145 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 6
2023-02-23 00:36:51,603 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 3
2023-02-23 00:36:52,121 DEBUG CV Batch 11/1700 loss 12.378910 loss_att 11.624945 loss_ctc 17.551767 loss_rnnt 11.654285 hw_loss 0.348194 history loss 10.418112 rank 7
2023-02-23 00:36:56,733 INFO Epoch 11 CV info cv_loss 10.350269775219022
2023-02-23 00:36:56,734 INFO Epoch 12 TRAIN info lr 0.000499732714477516
2023-02-23 00:36:56,739 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:36:56,978 INFO Epoch 11 CV info cv_loss 10.350269775443001
2023-02-23 00:36:56,979 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/11.pt
2023-02-23 00:36:57,077 INFO Epoch 11 CV info cv_loss 10.350269774392016
2023-02-23 00:36:57,077 INFO Epoch 11 CV info cv_loss 10.350269773461637
2023-02-23 00:36:57,079 INFO Epoch 12 TRAIN info lr 0.0004997102520563573
2023-02-23 00:36:57,079 INFO Epoch 12 TRAIN info lr 0.0004997551799280648
2023-02-23 00:36:57,084 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:36:57,084 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:36:58,727 INFO Epoch 11 CV info cv_loss 10.350269773134281
2023-02-23 00:36:58,728 INFO Epoch 12 TRAIN info lr 0.0004997701585784279
2023-02-23 00:36:58,731 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:36:59,078 INFO Epoch 12 TRAIN info lr 0.0004998026169417659
2023-02-23 00:36:59,084 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:36:59,479 INFO Epoch 11 CV info cv_loss 10.350269775115645
2023-02-23 00:36:59,480 INFO Epoch 12 TRAIN info lr 0.0004997077564187539
2023-02-23 00:36:59,484 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:37:00,775 INFO Epoch 11 CV info cv_loss 10.350269775184563
2023-02-23 00:37:00,777 INFO Epoch 12 TRAIN info lr 0.0004997102520563573
2023-02-23 00:37:00,780 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:37:01,949 INFO Epoch 11 CV info cv_loss 10.350269775184563
2023-02-23 00:37:01,950 INFO Epoch 12 TRAIN info lr 0.0004996927833783022
2023-02-23 00:37:01,954 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:38:12,986 DEBUG TRAIN Batch 12/0 loss 11.403854 loss_att 10.365146 loss_ctc 12.467945 loss_rnnt 11.219362 hw_loss 0.469412 lr 0.00049980 rank 0
2023-02-23 00:38:12,986 DEBUG TRAIN Batch 12/0 loss 14.207307 loss_att 15.283161 loss_ctc 17.062580 loss_rnnt 13.374591 hw_loss 0.444080 lr 0.00049973 rank 5
2023-02-23 00:38:12,994 DEBUG TRAIN Batch 12/0 loss 10.791258 loss_att 9.934102 loss_ctc 11.430202 loss_rnnt 10.652876 hw_loss 0.421165 lr 0.00049971 rank 2
2023-02-23 00:38:12,994 DEBUG TRAIN Batch 12/0 loss 9.491864 loss_att 9.360957 loss_ctc 10.698602 loss_rnnt 9.152026 hw_loss 0.384604 lr 0.00049975 rank 4
2023-02-23 00:38:12,998 DEBUG TRAIN Batch 12/0 loss 11.490951 loss_att 10.683279 loss_ctc 12.969007 loss_rnnt 11.220629 hw_loss 0.440214 lr 0.00049971 rank 6
2023-02-23 00:38:13,009 DEBUG TRAIN Batch 12/0 loss 20.167244 loss_att 17.395527 loss_ctc 23.673712 loss_rnnt 20.076031 hw_loss 0.333801 lr 0.00049969 rank 7
2023-02-23 00:38:13,010 DEBUG TRAIN Batch 12/0 loss 18.125170 loss_att 18.375080 loss_ctc 21.993912 loss_rnnt 17.328156 hw_loss 0.433499 lr 0.00049977 rank 1
2023-02-23 00:38:13,012 DEBUG TRAIN Batch 12/0 loss 16.590515 loss_att 15.181261 loss_ctc 17.302221 loss_rnnt 16.588251 hw_loss 0.354784 lr 0.00049971 rank 3
2023-02-23 00:39:28,110 DEBUG TRAIN Batch 12/100 loss 12.680893 loss_att 15.495852 loss_ctc 13.828804 loss_rnnt 11.789816 hw_loss 0.328183 lr 0.00049955 rank 0
2023-02-23 00:39:28,110 DEBUG TRAIN Batch 12/100 loss 6.774143 loss_att 13.395622 loss_ctc 7.685892 loss_rnnt 5.140391 hw_loss 0.352294 lr 0.00049946 rank 3
2023-02-23 00:39:28,112 DEBUG TRAIN Batch 12/100 loss 26.081821 loss_att 23.610775 loss_ctc 25.414579 loss_rnnt 26.506016 hw_loss 0.298085 lr 0.00049948 rank 5
2023-02-23 00:39:28,117 DEBUG TRAIN Batch 12/100 loss 8.470617 loss_att 13.885645 loss_ctc 12.361717 loss_rnnt 6.707809 hw_loss 0.301855 lr 0.00049950 rank 4
2023-02-23 00:39:28,117 DEBUG TRAIN Batch 12/100 loss 19.918697 loss_att 29.345947 loss_ctc 24.116652 loss_rnnt 17.328522 hw_loss 0.271876 lr 0.00049946 rank 2
2023-02-23 00:39:28,118 DEBUG TRAIN Batch 12/100 loss 25.673655 loss_att 29.766659 loss_ctc 28.822960 loss_rnnt 24.278858 hw_loss 0.293038 lr 0.00049946 rank 6
2023-02-23 00:39:28,119 DEBUG TRAIN Batch 12/100 loss 12.954006 loss_att 17.136190 loss_ctc 15.359874 loss_rnnt 11.626287 hw_loss 0.319685 lr 0.00049952 rank 1
2023-02-23 00:39:28,120 DEBUG TRAIN Batch 12/100 loss 28.632799 loss_att 37.013329 loss_ctc 38.451454 loss_rnnt 25.512503 hw_loss 0.253189 lr 0.00049944 rank 7
2023-02-23 00:40:42,810 DEBUG TRAIN Batch 12/200 loss 12.592396 loss_att 19.439863 loss_ctc 14.547222 loss_rnnt 10.789633 hw_loss 0.323674 lr 0.00049925 rank 4
2023-02-23 00:40:42,810 DEBUG TRAIN Batch 12/200 loss 24.868462 loss_att 31.447510 loss_ctc 26.870993 loss_rnnt 23.058109 hw_loss 0.426629 lr 0.00049927 rank 1
2023-02-23 00:40:42,813 DEBUG TRAIN Batch 12/200 loss 21.279457 loss_att 25.100756 loss_ctc 20.188961 loss_rnnt 20.531776 hw_loss 0.241535 lr 0.00049930 rank 0
2023-02-23 00:40:42,814 DEBUG TRAIN Batch 12/200 loss 33.600346 loss_att 36.579727 loss_ctc 46.626080 loss_rnnt 31.168158 hw_loss 0.186646 lr 0.00049923 rank 5
2023-02-23 00:40:42,815 DEBUG TRAIN Batch 12/200 loss 21.570421 loss_att 28.723885 loss_ctc 24.993599 loss_rnnt 19.458012 hw_loss 0.422425 lr 0.00049921 rank 3
2023-02-23 00:40:42,816 DEBUG TRAIN Batch 12/200 loss 6.079718 loss_att 11.223942 loss_ctc 6.810923 loss_rnnt 4.840159 hw_loss 0.212287 lr 0.00049921 rank 6
2023-02-23 00:40:42,816 DEBUG TRAIN Batch 12/200 loss 18.255043 loss_att 20.006716 loss_ctc 22.517935 loss_rnnt 17.193909 hw_loss 0.267026 lr 0.00049919 rank 7
2023-02-23 00:40:42,819 DEBUG TRAIN Batch 12/200 loss 20.071049 loss_att 25.778885 loss_ctc 23.717993 loss_rnnt 18.309116 hw_loss 0.251448 lr 0.00049921 rank 2
2023-02-23 00:42:00,155 DEBUG TRAIN Batch 12/300 loss 18.299259 loss_att 24.218590 loss_ctc 18.631641 loss_rnnt 16.903761 hw_loss 0.313715 lr 0.00049894 rank 7
2023-02-23 00:42:00,155 DEBUG TRAIN Batch 12/300 loss 12.560347 loss_att 18.072300 loss_ctc 16.303879 loss_rnnt 10.815460 hw_loss 0.268796 lr 0.00049902 rank 1
2023-02-23 00:42:00,156 DEBUG TRAIN Batch 12/300 loss 15.748854 loss_att 18.889858 loss_ctc 17.181576 loss_rnnt 14.791877 hw_loss 0.258273 lr 0.00049898 rank 5
2023-02-23 00:42:00,157 DEBUG TRAIN Batch 12/300 loss 6.404349 loss_att 11.186680 loss_ctc 6.693056 loss_rnnt 5.284740 hw_loss 0.233715 lr 0.00049896 rank 3
2023-02-23 00:42:00,158 DEBUG TRAIN Batch 12/300 loss 26.983759 loss_att 35.631065 loss_ctc 30.575779 loss_rnnt 24.645267 hw_loss 0.243928 lr 0.00049901 rank 4
2023-02-23 00:42:00,157 DEBUG TRAIN Batch 12/300 loss 20.318947 loss_att 28.379307 loss_ctc 24.499144 loss_rnnt 18.017471 hw_loss 0.247583 lr 0.00049896 rank 2
2023-02-23 00:42:00,159 DEBUG TRAIN Batch 12/300 loss 15.670123 loss_att 17.609192 loss_ctc 17.552090 loss_rnnt 14.878222 hw_loss 0.287172 lr 0.00049905 rank 0
2023-02-23 00:42:00,163 DEBUG TRAIN Batch 12/300 loss 14.561646 loss_att 22.786154 loss_ctc 18.164957 loss_rnnt 12.261292 hw_loss 0.328148 lr 0.00049896 rank 6
2023-02-23 00:43:18,425 DEBUG TRAIN Batch 12/400 loss 17.592270 loss_att 23.384953 loss_ctc 21.136501 loss_rnnt 15.786168 hw_loss 0.328124 lr 0.00049873 rank 5
2023-02-23 00:43:18,428 DEBUG TRAIN Batch 12/400 loss 14.932085 loss_att 26.121386 loss_ctc 14.594505 loss_rnnt 12.595549 hw_loss 0.269413 lr 0.00049880 rank 0
2023-02-23 00:43:18,428 DEBUG TRAIN Batch 12/400 loss 14.689534 loss_att 19.460194 loss_ctc 15.447909 loss_rnnt 13.444675 hw_loss 0.355518 lr 0.00049871 rank 3
2023-02-23 00:43:18,432 DEBUG TRAIN Batch 12/400 loss 18.314051 loss_att 22.015779 loss_ctc 21.099201 loss_rnnt 17.069242 hw_loss 0.249580 lr 0.00049871 rank 6
2023-02-23 00:43:18,432 DEBUG TRAIN Batch 12/400 loss 8.634933 loss_att 11.787488 loss_ctc 12.213730 loss_rnnt 7.329596 hw_loss 0.370598 lr 0.00049876 rank 4
2023-02-23 00:43:18,434 DEBUG TRAIN Batch 12/400 loss 27.279741 loss_att 31.474606 loss_ctc 30.998142 loss_rnnt 25.801365 hw_loss 0.269278 lr 0.00049871 rank 2
2023-02-23 00:43:18,438 DEBUG TRAIN Batch 12/400 loss 13.939461 loss_att 19.369850 loss_ctc 14.508543 loss_rnnt 12.670029 hw_loss 0.201518 lr 0.00049870 rank 7
2023-02-23 00:43:18,438 DEBUG TRAIN Batch 12/400 loss 10.787959 loss_att 17.993296 loss_ctc 14.535563 loss_rnnt 8.716851 hw_loss 0.244425 lr 0.00049877 rank 1
2023-02-23 00:44:35,707 DEBUG TRAIN Batch 12/500 loss 23.466303 loss_att 26.958469 loss_ctc 24.303045 loss_rnnt 22.545879 hw_loss 0.207049 lr 0.00049851 rank 4
2023-02-23 00:44:35,713 DEBUG TRAIN Batch 12/500 loss 15.755565 loss_att 19.681437 loss_ctc 16.897078 loss_rnnt 14.671030 hw_loss 0.275919 lr 0.00049846 rank 2
2023-02-23 00:44:35,715 DEBUG TRAIN Batch 12/500 loss 44.413139 loss_att 48.843647 loss_ctc 49.367355 loss_rnnt 42.625732 hw_loss 0.451397 lr 0.00049849 rank 5
2023-02-23 00:44:35,715 DEBUG TRAIN Batch 12/500 loss 24.640110 loss_att 30.319437 loss_ctc 24.258003 loss_rnnt 23.378254 hw_loss 0.331757 lr 0.00049846 rank 3
2023-02-23 00:44:35,718 DEBUG TRAIN Batch 12/500 loss 13.034963 loss_att 18.115814 loss_ctc 15.943901 loss_rnnt 11.444129 hw_loss 0.350261 lr 0.00049846 rank 6
2023-02-23 00:44:35,717 DEBUG TRAIN Batch 12/500 loss 17.156994 loss_att 22.913647 loss_ctc 17.841284 loss_rnnt 15.725304 hw_loss 0.354602 lr 0.00049845 rank 7
2023-02-23 00:44:35,717 DEBUG TRAIN Batch 12/500 loss 8.728280 loss_att 10.296609 loss_ctc 8.552994 loss_rnnt 8.292667 hw_loss 0.272472 lr 0.00049856 rank 0
2023-02-23 00:44:35,719 DEBUG TRAIN Batch 12/500 loss 18.889179 loss_att 22.506016 loss_ctc 21.720768 loss_rnnt 17.664257 hw_loss 0.232516 lr 0.00049852 rank 1
2023-02-23 00:45:51,200 DEBUG TRAIN Batch 12/600 loss 11.793517 loss_att 14.622940 loss_ctc 12.089124 loss_rnnt 10.982891 hw_loss 0.384985 lr 0.00049831 rank 0
2023-02-23 00:45:51,200 DEBUG TRAIN Batch 12/600 loss 19.637794 loss_att 20.980560 loss_ctc 22.067392 loss_rnnt 18.871054 hw_loss 0.326699 lr 0.00049821 rank 6
2023-02-23 00:45:51,202 DEBUG TRAIN Batch 12/600 loss 14.288111 loss_att 12.443693 loss_ctc 15.493695 loss_rnnt 14.312370 hw_loss 0.344771 lr 0.00049824 rank 5
2023-02-23 00:45:51,204 DEBUG TRAIN Batch 12/600 loss 20.648245 loss_att 22.334541 loss_ctc 23.835903 loss_rnnt 19.682728 hw_loss 0.381068 lr 0.00049820 rank 7
2023-02-23 00:45:51,207 DEBUG TRAIN Batch 12/600 loss 25.173643 loss_att 23.901024 loss_ctc 32.263172 loss_rnnt 24.272873 hw_loss 0.393794 lr 0.00049822 rank 2
2023-02-23 00:45:51,207 DEBUG TRAIN Batch 12/600 loss 11.381835 loss_att 13.141618 loss_ctc 12.951794 loss_rnnt 10.601692 hw_loss 0.410358 lr 0.00049822 rank 3
2023-02-23 00:45:51,211 DEBUG TRAIN Batch 12/600 loss 12.827207 loss_att 12.329491 loss_ctc 14.577857 loss_rnnt 12.510019 hw_loss 0.343708 lr 0.00049826 rank 4
2023-02-23 00:45:51,215 DEBUG TRAIN Batch 12/600 loss 15.446419 loss_att 20.400860 loss_ctc 18.331398 loss_rnnt 13.864421 hw_loss 0.387086 lr 0.00049828 rank 1
2023-02-23 00:47:10,133 DEBUG TRAIN Batch 12/700 loss 30.605965 loss_att 46.229401 loss_ctc 38.918457 loss_rnnt 26.219177 hw_loss 0.288314 lr 0.00049797 rank 2
2023-02-23 00:47:10,133 DEBUG TRAIN Batch 12/700 loss 20.351009 loss_att 26.837631 loss_ctc 21.772217 loss_rnnt 18.669722 hw_loss 0.364632 lr 0.00049801 rank 4
2023-02-23 00:47:10,136 DEBUG TRAIN Batch 12/700 loss 17.438681 loss_att 23.387865 loss_ctc 20.363024 loss_rnnt 15.734640 hw_loss 0.233046 lr 0.00049806 rank 0
2023-02-23 00:47:10,137 DEBUG TRAIN Batch 12/700 loss 31.405382 loss_att 29.326731 loss_ctc 32.659714 loss_rnnt 31.493282 hw_loss 0.301097 lr 0.00049797 rank 6
2023-02-23 00:47:10,139 DEBUG TRAIN Batch 12/700 loss 13.332685 loss_att 15.946612 loss_ctc 16.317886 loss_rnnt 12.266279 hw_loss 0.272988 lr 0.00049799 rank 5
2023-02-23 00:47:10,141 DEBUG TRAIN Batch 12/700 loss 10.817898 loss_att 17.788340 loss_ctc 12.829253 loss_rnnt 9.061559 hw_loss 0.176381 lr 0.00049797 rank 3
2023-02-23 00:47:10,148 DEBUG TRAIN Batch 12/700 loss 14.777424 loss_att 25.622438 loss_ctc 20.712353 loss_rnnt 11.719707 hw_loss 0.182607 lr 0.00049795 rank 7
2023-02-23 00:47:10,199 DEBUG TRAIN Batch 12/700 loss 25.147312 loss_att 31.844625 loss_ctc 34.921356 loss_rnnt 22.413960 hw_loss 0.170033 lr 0.00049803 rank 1
2023-02-23 00:48:27,286 DEBUG TRAIN Batch 12/800 loss 21.309650 loss_att 32.760204 loss_ctc 23.388662 loss_rnnt 18.629124 hw_loss 0.212279 lr 0.00049772 rank 2
2023-02-23 00:48:27,289 DEBUG TRAIN Batch 12/800 loss 16.186304 loss_att 18.912882 loss_ctc 24.071438 loss_rnnt 14.460146 hw_loss 0.242797 lr 0.00049781 rank 0
2023-02-23 00:48:27,289 DEBUG TRAIN Batch 12/800 loss 18.326361 loss_att 23.983620 loss_ctc 24.069977 loss_rnnt 16.312496 hw_loss 0.218619 lr 0.00049775 rank 5
2023-02-23 00:48:27,289 DEBUG TRAIN Batch 12/800 loss 12.799480 loss_att 17.546444 loss_ctc 18.422340 loss_rnnt 10.950781 hw_loss 0.280488 lr 0.00049777 rank 4
2023-02-23 00:48:27,291 DEBUG TRAIN Batch 12/800 loss 12.230474 loss_att 16.671076 loss_ctc 13.385086 loss_rnnt 11.011572 hw_loss 0.331564 lr 0.00049772 rank 3
2023-02-23 00:48:27,295 DEBUG TRAIN Batch 12/800 loss 11.743702 loss_att 18.522705 loss_ctc 11.734241 loss_rnnt 10.266116 hw_loss 0.230713 lr 0.00049778 rank 1
2023-02-23 00:48:27,295 DEBUG TRAIN Batch 12/800 loss 16.881662 loss_att 19.323078 loss_ctc 16.701233 loss_rnnt 16.256001 hw_loss 0.302693 lr 0.00049771 rank 7
2023-02-23 00:48:27,295 DEBUG TRAIN Batch 12/800 loss 12.847397 loss_att 20.305504 loss_ctc 14.527645 loss_rnnt 10.973265 hw_loss 0.297145 lr 0.00049772 rank 6
2023-02-23 00:49:42,773 DEBUG TRAIN Batch 12/900 loss 24.043615 loss_att 27.550711 loss_ctc 31.036919 loss_rnnt 22.264624 hw_loss 0.272125 lr 0.00049750 rank 5
2023-02-23 00:49:42,777 DEBUG TRAIN Batch 12/900 loss 21.163095 loss_att 23.206470 loss_ctc 22.800228 loss_rnnt 20.402100 hw_loss 0.251319 lr 0.00049747 rank 6
2023-02-23 00:49:42,776 DEBUG TRAIN Batch 12/900 loss 13.313940 loss_att 19.233885 loss_ctc 17.004612 loss_rnnt 11.486290 hw_loss 0.284195 lr 0.00049748 rank 3
2023-02-23 00:49:42,777 DEBUG TRAIN Batch 12/900 loss 25.598030 loss_att 36.661579 loss_ctc 32.063618 loss_rnnt 22.399921 hw_loss 0.231225 lr 0.00049752 rank 4
2023-02-23 00:49:42,777 DEBUG TRAIN Batch 12/900 loss 12.074258 loss_att 21.044512 loss_ctc 17.708015 loss_rnnt 9.376259 hw_loss 0.286463 lr 0.00049748 rank 2
2023-02-23 00:49:42,778 DEBUG TRAIN Batch 12/900 loss 24.438635 loss_att 37.810131 loss_ctc 29.072830 loss_rnnt 21.011124 hw_loss 0.253719 lr 0.00049746 rank 7
2023-02-23 00:49:42,780 DEBUG TRAIN Batch 12/900 loss 22.385622 loss_att 23.287670 loss_ctc 25.229261 loss_rnnt 21.604763 hw_loss 0.414935 lr 0.00049754 rank 1
2023-02-23 00:49:42,781 DEBUG TRAIN Batch 12/900 loss 16.592461 loss_att 27.396431 loss_ctc 23.124926 loss_rnnt 13.453470 hw_loss 0.201002 lr 0.00049757 rank 0
2023-02-23 00:50:59,167 DEBUG TRAIN Batch 12/1000 loss 21.249159 loss_att 25.025536 loss_ctc 21.465557 loss_rnnt 20.320835 hw_loss 0.270366 lr 0.00049725 rank 5
2023-02-23 00:50:59,169 DEBUG TRAIN Batch 12/1000 loss 8.538655 loss_att 15.438465 loss_ctc 9.721132 loss_rnnt 6.862710 hw_loss 0.259349 lr 0.00049729 rank 1
2023-02-23 00:50:59,173 DEBUG TRAIN Batch 12/1000 loss 10.336944 loss_att 17.118128 loss_ctc 12.445069 loss_rnnt 8.559238 hw_loss 0.263220 lr 0.00049723 rank 6
2023-02-23 00:50:59,175 DEBUG TRAIN Batch 12/1000 loss 27.330631 loss_att 29.431173 loss_ctc 29.704773 loss_rnnt 26.433012 hw_loss 0.301796 lr 0.00049723 rank 3
2023-02-23 00:50:59,181 DEBUG TRAIN Batch 12/1000 loss 26.197121 loss_att 27.317085 loss_ctc 31.165325 loss_rnnt 25.156090 hw_loss 0.289894 lr 0.00049727 rank 4
2023-02-23 00:50:59,200 DEBUG TRAIN Batch 12/1000 loss 6.374909 loss_att 10.561963 loss_ctc 8.967766 loss_rnnt 5.097582 hw_loss 0.176626 lr 0.00049732 rank 0
2023-02-23 00:50:59,206 DEBUG TRAIN Batch 12/1000 loss 14.478607 loss_att 20.005692 loss_ctc 17.579010 loss_rnnt 12.801376 hw_loss 0.297050 lr 0.00049723 rank 2
2023-02-23 00:50:59,223 DEBUG TRAIN Batch 12/1000 loss 10.468313 loss_att 15.385962 loss_ctc 13.294605 loss_rnnt 8.993919 hw_loss 0.213799 lr 0.00049721 rank 7
2023-02-23 00:52:17,961 DEBUG TRAIN Batch 12/1100 loss 10.744508 loss_att 14.291044 loss_ctc 13.027864 loss_rnnt 9.557253 hw_loss 0.325314 lr 0.00049698 rank 2
2023-02-23 00:52:17,962 DEBUG TRAIN Batch 12/1100 loss 25.670998 loss_att 25.183365 loss_ctc 30.825529 loss_rnnt 24.894930 hw_loss 0.349362 lr 0.00049703 rank 4
2023-02-23 00:52:17,962 DEBUG TRAIN Batch 12/1100 loss 12.267482 loss_att 14.768429 loss_ctc 13.739053 loss_rnnt 11.428273 hw_loss 0.267769 lr 0.00049704 rank 1
2023-02-23 00:52:17,964 DEBUG TRAIN Batch 12/1100 loss 22.975702 loss_att 25.916145 loss_ctc 25.072206 loss_rnnt 22.019093 hw_loss 0.166844 lr 0.00049701 rank 5
2023-02-23 00:52:17,965 DEBUG TRAIN Batch 12/1100 loss 24.808784 loss_att 35.015594 loss_ctc 33.695076 loss_rnnt 21.424595 hw_loss 0.296231 lr 0.00049708 rank 0
2023-02-23 00:52:17,966 DEBUG TRAIN Batch 12/1100 loss 15.248215 loss_att 16.381464 loss_ctc 15.555372 loss_rnnt 14.776717 hw_loss 0.382300 lr 0.00049698 rank 3
2023-02-23 00:52:17,967 DEBUG TRAIN Batch 12/1100 loss 15.311633 loss_att 20.264400 loss_ctc 14.549421 loss_rnnt 14.299969 hw_loss 0.230136 lr 0.00049697 rank 7
2023-02-23 00:52:18,010 DEBUG TRAIN Batch 12/1100 loss 16.193769 loss_att 17.130175 loss_ctc 19.022522 loss_rnnt 15.482439 hw_loss 0.275400 lr 0.00049698 rank 6
2023-02-23 00:53:33,125 DEBUG TRAIN Batch 12/1200 loss 13.976786 loss_att 14.783497 loss_ctc 12.011963 loss_rnnt 13.960304 hw_loss 0.219591 lr 0.00049674 rank 2
2023-02-23 00:53:33,130 DEBUG TRAIN Batch 12/1200 loss 14.176123 loss_att 14.497999 loss_ctc 17.405821 loss_rnnt 13.540600 hw_loss 0.263476 lr 0.00049683 rank 0
2023-02-23 00:53:33,131 DEBUG TRAIN Batch 12/1200 loss 13.486128 loss_att 14.105047 loss_ctc 16.351881 loss_rnnt 12.781629 hw_loss 0.372403 lr 0.00049678 rank 4
2023-02-23 00:53:33,132 DEBUG TRAIN Batch 12/1200 loss 17.738283 loss_att 20.435236 loss_ctc 22.470034 loss_rnnt 16.376427 hw_loss 0.359186 lr 0.00049674 rank 3
2023-02-23 00:53:33,132 DEBUG TRAIN Batch 12/1200 loss 15.013393 loss_att 13.692402 loss_ctc 16.858139 loss_rnnt 14.819460 hw_loss 0.397809 lr 0.00049676 rank 5
2023-02-23 00:53:33,134 DEBUG TRAIN Batch 12/1200 loss 20.338497 loss_att 21.548075 loss_ctc 21.774937 loss_rnnt 19.725950 hw_loss 0.335821 lr 0.00049680 rank 1
2023-02-23 00:53:33,134 DEBUG TRAIN Batch 12/1200 loss 11.857589 loss_att 16.643877 loss_ctc 14.461267 loss_rnnt 10.411326 hw_loss 0.265963 lr 0.00049672 rank 7
2023-02-23 00:53:33,136 DEBUG TRAIN Batch 12/1200 loss 21.716532 loss_att 23.864407 loss_ctc 25.143936 loss_rnnt 20.658310 hw_loss 0.321866 lr 0.00049674 rank 6
2023-02-23 00:54:48,867 DEBUG TRAIN Batch 12/1300 loss 10.000078 loss_att 17.649189 loss_ctc 14.007629 loss_rnnt 7.780603 hw_loss 0.291211 lr 0.00049659 rank 0
2023-02-23 00:54:48,869 DEBUG TRAIN Batch 12/1300 loss 12.086914 loss_att 12.344963 loss_ctc 15.553602 loss_rnnt 11.361080 hw_loss 0.397499 lr 0.00049649 rank 6
2023-02-23 00:54:48,870 DEBUG TRAIN Batch 12/1300 loss 30.425982 loss_att 43.733482 loss_ctc 38.538345 loss_rnnt 26.517204 hw_loss 0.310557 lr 0.00049648 rank 7
2023-02-23 00:54:48,870 DEBUG TRAIN Batch 12/1300 loss 5.852886 loss_att 9.083618 loss_ctc 6.984132 loss_rnnt 4.925448 hw_loss 0.244609 lr 0.00049649 rank 3
2023-02-23 00:54:48,872 DEBUG TRAIN Batch 12/1300 loss 5.865355 loss_att 13.572151 loss_ctc 4.948577 loss_rnnt 4.301446 hw_loss 0.271476 lr 0.00049652 rank 5
2023-02-23 00:54:48,872 DEBUG TRAIN Batch 12/1300 loss 23.275251 loss_att 29.680367 loss_ctc 36.599091 loss_rnnt 20.154461 hw_loss 0.118603 lr 0.00049654 rank 4
2023-02-23 00:54:48,872 DEBUG TRAIN Batch 12/1300 loss 23.976074 loss_att 26.676464 loss_ctc 30.043301 loss_rnnt 22.460751 hw_loss 0.311786 lr 0.00049649 rank 2
2023-02-23 00:54:48,878 DEBUG TRAIN Batch 12/1300 loss 11.660604 loss_att 21.964596 loss_ctc 14.392783 loss_rnnt 9.088604 hw_loss 0.275459 lr 0.00049655 rank 1
2023-02-23 00:56:06,448 DEBUG TRAIN Batch 12/1400 loss 20.820120 loss_att 28.417257 loss_ctc 22.009306 loss_rnnt 18.993650 hw_loss 0.278410 lr 0.00049627 rank 5
2023-02-23 00:56:06,448 DEBUG TRAIN Batch 12/1400 loss 14.203717 loss_att 19.590353 loss_ctc 19.300802 loss_rnnt 12.327560 hw_loss 0.223535 lr 0.00049625 rank 2
2023-02-23 00:56:06,451 DEBUG TRAIN Batch 12/1400 loss 12.690810 loss_att 13.997068 loss_ctc 16.340057 loss_rnnt 11.806484 hw_loss 0.255955 lr 0.00049625 rank 3
2023-02-23 00:56:06,453 DEBUG TRAIN Batch 12/1400 loss 4.584238 loss_att 8.088482 loss_ctc 4.345824 loss_rnnt 3.724247 hw_loss 0.357995 lr 0.00049634 rank 0
2023-02-23 00:56:06,456 DEBUG TRAIN Batch 12/1400 loss 16.798246 loss_att 21.270611 loss_ctc 15.923901 loss_rnnt 15.859030 hw_loss 0.302482 lr 0.00049623 rank 7
2023-02-23 00:56:06,458 DEBUG TRAIN Batch 12/1400 loss 16.847118 loss_att 22.928335 loss_ctc 19.477787 loss_rnnt 15.147581 hw_loss 0.248508 lr 0.00049629 rank 4
2023-02-23 00:56:06,462 DEBUG TRAIN Batch 12/1400 loss 6.077329 loss_att 9.770876 loss_ctc 6.251676 loss_rnnt 5.134322 hw_loss 0.339471 lr 0.00049625 rank 6
2023-02-23 00:56:06,474 DEBUG TRAIN Batch 12/1400 loss 8.761812 loss_att 14.017765 loss_ctc 11.495937 loss_rnnt 7.246185 hw_loss 0.187288 lr 0.00049631 rank 1
2023-02-23 00:57:23,721 DEBUG TRAIN Batch 12/1500 loss 16.768044 loss_att 22.444349 loss_ctc 23.441818 loss_rnnt 14.572368 hw_loss 0.319834 lr 0.00049599 rank 7
2023-02-23 00:57:23,721 DEBUG TRAIN Batch 12/1500 loss 33.258961 loss_att 30.207388 loss_ctc 34.934868 loss_rnnt 33.474220 hw_loss 0.321748 lr 0.00049601 rank 2
2023-02-23 00:57:23,725 DEBUG TRAIN Batch 12/1500 loss 4.269159 loss_att 9.903198 loss_ctc 5.917176 loss_rnnt 2.746731 hw_loss 0.329784 lr 0.00049603 rank 5
2023-02-23 00:57:23,726 DEBUG TRAIN Batch 12/1500 loss 6.473788 loss_att 11.663424 loss_ctc 8.367184 loss_rnnt 5.022783 hw_loss 0.301171 lr 0.00049605 rank 4
2023-02-23 00:57:23,727 DEBUG TRAIN Batch 12/1500 loss 11.697267 loss_att 19.339249 loss_ctc 13.708054 loss_rnnt 9.710756 hw_loss 0.356265 lr 0.00049610 rank 0
2023-02-23 00:57:23,728 DEBUG TRAIN Batch 12/1500 loss 11.302716 loss_att 15.987757 loss_ctc 10.179549 loss_rnnt 10.301634 hw_loss 0.400930 lr 0.00049600 rank 6
2023-02-23 00:57:23,729 DEBUG TRAIN Batch 12/1500 loss 10.013118 loss_att 13.763802 loss_ctc 17.368893 loss_rnnt 8.097086 hw_loss 0.347109 lr 0.00049606 rank 1
2023-02-23 00:57:23,773 DEBUG TRAIN Batch 12/1500 loss 5.908714 loss_att 9.399122 loss_ctc 7.579918 loss_rnnt 4.838709 hw_loss 0.279555 lr 0.00049601 rank 3
2023-02-23 00:58:39,478 DEBUG TRAIN Batch 12/1600 loss 4.281947 loss_att 12.112183 loss_ctc 3.645885 loss_rnnt 2.627997 hw_loss 0.323834 lr 0.00049578 rank 5
2023-02-23 00:58:39,479 DEBUG TRAIN Batch 12/1600 loss 10.787463 loss_att 14.158482 loss_ctc 12.998735 loss_rnnt 9.612129 hw_loss 0.386800 lr 0.00049576 rank 3
2023-02-23 00:58:39,479 DEBUG TRAIN Batch 12/1600 loss 19.361000 loss_att 22.039585 loss_ctc 26.805590 loss_rnnt 17.675125 hw_loss 0.295398 lr 0.00049581 rank 4
2023-02-23 00:58:39,482 DEBUG TRAIN Batch 12/1600 loss 13.898347 loss_att 16.602320 loss_ctc 16.181828 loss_rnnt 12.906370 hw_loss 0.275098 lr 0.00049585 rank 0
2023-02-23 00:58:39,482 DEBUG TRAIN Batch 12/1600 loss 13.155939 loss_att 16.697939 loss_ctc 15.827782 loss_rnnt 11.931672 hw_loss 0.299288 lr 0.00049576 rank 2
2023-02-23 00:58:39,483 DEBUG TRAIN Batch 12/1600 loss 15.389075 loss_att 21.786127 loss_ctc 18.444809 loss_rnnt 13.560667 hw_loss 0.265438 lr 0.00049582 rank 1
2023-02-23 00:58:39,484 DEBUG TRAIN Batch 12/1600 loss 9.068810 loss_att 15.216860 loss_ctc 11.279796 loss_rnnt 7.385492 hw_loss 0.297955 lr 0.00049576 rank 6
2023-02-23 00:58:39,488 DEBUG TRAIN Batch 12/1600 loss 18.773315 loss_att 30.227703 loss_ctc 18.957626 loss_rnnt 16.357933 hw_loss 0.187368 lr 0.00049574 rank 7
2023-02-23 00:59:56,523 DEBUG TRAIN Batch 12/1700 loss 23.968472 loss_att 29.921143 loss_ctc 28.496748 loss_rnnt 22.041922 hw_loss 0.247963 lr 0.00049561 rank 0
2023-02-23 00:59:56,526 DEBUG TRAIN Batch 12/1700 loss 23.772621 loss_att 26.477394 loss_ctc 30.300659 loss_rnnt 22.185741 hw_loss 0.329103 lr 0.00049554 rank 5
2023-02-23 00:59:56,529 DEBUG TRAIN Batch 12/1700 loss 13.379228 loss_att 16.662119 loss_ctc 12.595623 loss_rnnt 12.694881 hw_loss 0.247965 lr 0.00049552 rank 2
2023-02-23 00:59:56,529 DEBUG TRAIN Batch 12/1700 loss 18.645535 loss_att 21.918522 loss_ctc 21.945366 loss_rnnt 17.386158 hw_loss 0.308999 lr 0.00049552 rank 6
2023-02-23 00:59:56,529 DEBUG TRAIN Batch 12/1700 loss 12.129434 loss_att 17.567043 loss_ctc 17.030441 loss_rnnt 10.265577 hw_loss 0.230374 lr 0.00049558 rank 1
2023-02-23 00:59:56,531 DEBUG TRAIN Batch 12/1700 loss 11.709994 loss_att 14.827301 loss_ctc 15.270296 loss_rnnt 10.468373 hw_loss 0.268973 lr 0.00049550 rank 7
2023-02-23 00:59:56,531 DEBUG TRAIN Batch 12/1700 loss 17.354483 loss_att 20.336010 loss_ctc 22.752075 loss_rnnt 15.854293 hw_loss 0.345389 lr 0.00049556 rank 4
2023-02-23 00:59:56,535 DEBUG TRAIN Batch 12/1700 loss 24.014389 loss_att 28.948742 loss_ctc 27.013363 loss_rnnt 22.505295 hw_loss 0.229425 lr 0.00049552 rank 3
2023-02-23 01:01:15,591 DEBUG TRAIN Batch 12/1800 loss 5.852041 loss_att 9.309799 loss_ctc 6.227033 loss_rnnt 4.966678 hw_loss 0.269649 lr 0.00049527 rank 6
2023-02-23 01:01:15,594 DEBUG TRAIN Batch 12/1800 loss 17.403666 loss_att 19.827156 loss_ctc 20.374592 loss_rnnt 16.385761 hw_loss 0.257030 lr 0.00049528 rank 2
2023-02-23 01:01:15,598 DEBUG TRAIN Batch 12/1800 loss 11.297815 loss_att 15.469090 loss_ctc 14.960364 loss_rnnt 9.826276 hw_loss 0.279270 lr 0.00049530 rank 5
2023-02-23 01:01:15,598 DEBUG TRAIN Batch 12/1800 loss 16.215469 loss_att 17.235174 loss_ctc 18.971687 loss_rnnt 15.473869 hw_loss 0.319056 lr 0.00049537 rank 0
2023-02-23 01:01:15,598 DEBUG TRAIN Batch 12/1800 loss 17.455946 loss_att 18.954933 loss_ctc 20.011478 loss_rnnt 16.676926 hw_loss 0.259657 lr 0.00049532 rank 4
2023-02-23 01:01:15,599 DEBUG TRAIN Batch 12/1800 loss 10.690561 loss_att 11.208799 loss_ctc 13.092705 loss_rnnt 10.075523 hw_loss 0.358320 lr 0.00049528 rank 3
2023-02-23 01:01:15,600 DEBUG TRAIN Batch 12/1800 loss 14.135772 loss_att 18.520853 loss_ctc 17.449718 loss_rnnt 12.650940 hw_loss 0.311167 lr 0.00049526 rank 7
2023-02-23 01:01:15,600 DEBUG TRAIN Batch 12/1800 loss 24.218504 loss_att 28.421089 loss_ctc 27.515915 loss_rnnt 22.778000 hw_loss 0.300621 lr 0.00049533 rank 1
2023-02-23 01:02:32,262 DEBUG TRAIN Batch 12/1900 loss 17.434408 loss_att 28.316582 loss_ctc 17.320578 loss_rnnt 15.174795 hw_loss 0.184418 lr 0.00049508 rank 4
2023-02-23 01:02:32,266 DEBUG TRAIN Batch 12/1900 loss 14.109487 loss_att 20.691963 loss_ctc 18.189358 loss_rnnt 12.139456 hw_loss 0.205415 lr 0.00049512 rank 0
2023-02-23 01:02:32,270 DEBUG TRAIN Batch 12/1900 loss 16.121719 loss_att 20.338747 loss_ctc 18.425110 loss_rnnt 14.795433 hw_loss 0.329550 lr 0.00049505 rank 5
2023-02-23 01:02:32,271 DEBUG TRAIN Batch 12/1900 loss 18.568150 loss_att 20.468565 loss_ctc 24.097324 loss_rnnt 17.289753 hw_loss 0.302041 lr 0.00049503 rank 2
2023-02-23 01:02:32,272 DEBUG TRAIN Batch 12/1900 loss 13.684564 loss_att 12.466845 loss_ctc 15.244519 loss_rnnt 13.456783 hw_loss 0.493745 lr 0.00049509 rank 1
2023-02-23 01:02:32,273 DEBUG TRAIN Batch 12/1900 loss 33.288574 loss_att 38.167992 loss_ctc 37.576992 loss_rnnt 31.581882 hw_loss 0.298161 lr 0.00049503 rank 3
2023-02-23 01:02:32,275 DEBUG TRAIN Batch 12/1900 loss 16.028246 loss_att 21.234470 loss_ctc 23.004307 loss_rnnt 13.873364 hw_loss 0.344051 lr 0.00049503 rank 6
2023-02-23 01:02:32,280 DEBUG TRAIN Batch 12/1900 loss 13.717058 loss_att 15.770433 loss_ctc 16.891245 loss_rnnt 12.674674 hw_loss 0.390908 lr 0.00049502 rank 7
2023-02-23 01:03:48,301 DEBUG TRAIN Batch 12/2000 loss 23.120842 loss_att 28.026953 loss_ctc 27.164642 loss_rnnt 21.441427 hw_loss 0.298160 lr 0.00049479 rank 3
2023-02-23 01:03:48,302 DEBUG TRAIN Batch 12/2000 loss 25.431438 loss_att 27.900799 loss_ctc 28.520359 loss_rnnt 24.381329 hw_loss 0.270718 lr 0.00049479 rank 2
2023-02-23 01:03:48,303 DEBUG TRAIN Batch 12/2000 loss 6.228992 loss_att 12.874971 loss_ctc 6.224315 loss_rnnt 4.793661 hw_loss 0.200174 lr 0.00049481 rank 5
2023-02-23 01:03:48,305 DEBUG TRAIN Batch 12/2000 loss 15.727353 loss_att 19.850887 loss_ctc 15.799640 loss_rnnt 14.764686 hw_loss 0.240606 lr 0.00049485 rank 1
2023-02-23 01:03:48,305 DEBUG TRAIN Batch 12/2000 loss 21.108698 loss_att 23.605122 loss_ctc 27.040123 loss_rnnt 19.700628 hw_loss 0.221119 lr 0.00049483 rank 4
2023-02-23 01:03:48,306 DEBUG TRAIN Batch 12/2000 loss 19.502903 loss_att 26.578945 loss_ctc 20.637688 loss_rnnt 17.805307 hw_loss 0.245781 lr 0.00049477 rank 7
2023-02-23 01:03:48,310 DEBUG TRAIN Batch 12/2000 loss 20.507996 loss_att 21.080097 loss_ctc 29.559162 loss_rnnt 19.031296 hw_loss 0.291485 lr 0.00049488 rank 0
2023-02-23 01:03:48,312 DEBUG TRAIN Batch 12/2000 loss 10.790251 loss_att 17.037115 loss_ctc 13.841907 loss_rnnt 8.962720 hw_loss 0.321130 lr 0.00049479 rank 6
2023-02-23 01:05:06,812 DEBUG TRAIN Batch 12/2100 loss 9.530344 loss_att 14.166330 loss_ctc 10.884376 loss_rnnt 8.243329 hw_loss 0.336150 lr 0.00049455 rank 3
2023-02-23 01:05:06,814 DEBUG TRAIN Batch 12/2100 loss 13.987751 loss_att 22.601074 loss_ctc 15.860110 loss_rnnt 11.876277 hw_loss 0.260927 lr 0.00049457 rank 5
2023-02-23 01:05:06,815 DEBUG TRAIN Batch 12/2100 loss 6.572661 loss_att 12.283562 loss_ctc 6.270027 loss_rnnt 5.328359 hw_loss 0.267138 lr 0.00049459 rank 4
2023-02-23 01:05:06,816 DEBUG TRAIN Batch 12/2100 loss 9.904876 loss_att 15.414387 loss_ctc 12.225307 loss_rnnt 8.396090 hw_loss 0.182799 lr 0.00049455 rank 2
2023-02-23 01:05:06,817 DEBUG TRAIN Batch 12/2100 loss 29.747236 loss_att 33.371071 loss_ctc 33.248035 loss_rnnt 28.409145 hw_loss 0.274784 lr 0.00049464 rank 0
2023-02-23 01:05:06,818 DEBUG TRAIN Batch 12/2100 loss 14.020250 loss_att 18.072441 loss_ctc 18.336058 loss_rnnt 12.544483 hw_loss 0.168541 lr 0.00049453 rank 7
2023-02-23 01:05:06,819 DEBUG TRAIN Batch 12/2100 loss 8.904074 loss_att 14.625804 loss_ctc 10.644784 loss_rnnt 7.387744 hw_loss 0.262292 lr 0.00049461 rank 1
2023-02-23 01:05:06,820 DEBUG TRAIN Batch 12/2100 loss 19.191452 loss_att 25.142544 loss_ctc 25.949966 loss_rnnt 16.975101 hw_loss 0.234369 lr 0.00049455 rank 6
2023-02-23 01:06:23,482 DEBUG TRAIN Batch 12/2200 loss 23.518250 loss_att 25.297503 loss_ctc 21.789436 loss_rnnt 23.285759 hw_loss 0.200902 lr 0.00049431 rank 2
2023-02-23 01:06:23,486 DEBUG TRAIN Batch 12/2200 loss 12.271809 loss_att 15.377180 loss_ctc 15.020366 loss_rnnt 11.066805 hw_loss 0.407724 lr 0.00049430 rank 6
2023-02-23 01:06:23,486 DEBUG TRAIN Batch 12/2200 loss 9.423431 loss_att 13.536905 loss_ctc 10.104955 loss_rnnt 8.363151 hw_loss 0.275091 lr 0.00049431 rank 3
2023-02-23 01:06:23,487 DEBUG TRAIN Batch 12/2200 loss 24.455101 loss_att 27.314480 loss_ctc 30.465294 loss_rnnt 22.901669 hw_loss 0.337871 lr 0.00049436 rank 1
2023-02-23 01:06:23,487 DEBUG TRAIN Batch 12/2200 loss 23.673880 loss_att 25.552063 loss_ctc 29.199921 loss_rnnt 22.392773 hw_loss 0.316247 lr 0.00049440 rank 0
2023-02-23 01:06:23,489 DEBUG TRAIN Batch 12/2200 loss 19.653442 loss_att 24.713905 loss_ctc 29.048458 loss_rnnt 17.244349 hw_loss 0.270618 lr 0.00049433 rank 5
2023-02-23 01:06:23,491 DEBUG TRAIN Batch 12/2200 loss 13.750449 loss_att 19.530830 loss_ctc 14.694966 loss_rnnt 12.343037 hw_loss 0.235126 lr 0.00049435 rank 4
2023-02-23 01:06:23,497 DEBUG TRAIN Batch 12/2200 loss 22.173170 loss_att 26.628668 loss_ctc 25.523504 loss_rnnt 20.731327 hw_loss 0.195060 lr 0.00049429 rank 7
2023-02-23 01:07:38,946 DEBUG TRAIN Batch 12/2300 loss 19.016182 loss_att 25.264765 loss_ctc 21.328659 loss_rnnt 17.289448 hw_loss 0.316288 lr 0.00049411 rank 4
2023-02-23 01:07:38,949 DEBUG TRAIN Batch 12/2300 loss 33.540531 loss_att 35.186256 loss_ctc 37.867245 loss_rnnt 32.480740 hw_loss 0.288284 lr 0.00049409 rank 5
2023-02-23 01:07:38,952 DEBUG TRAIN Batch 12/2300 loss 8.085151 loss_att 10.072903 loss_ctc 8.782323 loss_rnnt 7.425380 hw_loss 0.317372 lr 0.00049406 rank 6
2023-02-23 01:07:38,951 DEBUG TRAIN Batch 12/2300 loss 26.573940 loss_att 27.938145 loss_ctc 27.784660 loss_rnnt 26.005308 hw_loss 0.251934 lr 0.00049415 rank 0
2023-02-23 01:07:38,955 DEBUG TRAIN Batch 12/2300 loss 16.440916 loss_att 22.701221 loss_ctc 23.247276 loss_rnnt 14.151271 hw_loss 0.243878 lr 0.00049406 rank 2
2023-02-23 01:07:38,956 DEBUG TRAIN Batch 12/2300 loss 17.398106 loss_att 21.641569 loss_ctc 23.765051 loss_rnnt 15.542845 hw_loss 0.295583 lr 0.00049412 rank 1
2023-02-23 01:07:38,956 DEBUG TRAIN Batch 12/2300 loss 21.314877 loss_att 27.817251 loss_ctc 28.075672 loss_rnnt 18.951456 hw_loss 0.302818 lr 0.00049406 rank 3
2023-02-23 01:07:38,960 DEBUG TRAIN Batch 12/2300 loss 22.750566 loss_att 29.443859 loss_ctc 28.615406 loss_rnnt 20.466362 hw_loss 0.306688 lr 0.00049405 rank 7
2023-02-23 01:08:56,916 DEBUG TRAIN Batch 12/2400 loss 13.544475 loss_att 17.117935 loss_ctc 16.436581 loss_rnnt 12.222197 hw_loss 0.416200 lr 0.00049382 rank 2
2023-02-23 01:08:56,921 DEBUG TRAIN Batch 12/2400 loss 15.763198 loss_att 20.841614 loss_ctc 19.110914 loss_rnnt 14.158271 hw_loss 0.267903 lr 0.00049385 rank 5
2023-02-23 01:08:56,921 DEBUG TRAIN Batch 12/2400 loss 18.722908 loss_att 25.537249 loss_ctc 18.202805 loss_rnnt 17.257965 hw_loss 0.321418 lr 0.00049391 rank 0
2023-02-23 01:08:56,923 DEBUG TRAIN Batch 12/2400 loss 10.421159 loss_att 10.598049 loss_ctc 11.422380 loss_rnnt 10.114496 hw_loss 0.258352 lr 0.00049382 rank 3
2023-02-23 01:08:56,927 DEBUG TRAIN Batch 12/2400 loss 14.881164 loss_att 22.057402 loss_ctc 17.075968 loss_rnnt 12.980229 hw_loss 0.324463 lr 0.00049387 rank 4
2023-02-23 01:08:56,930 DEBUG TRAIN Batch 12/2400 loss 11.163147 loss_att 13.867658 loss_ctc 12.349136 loss_rnnt 10.290558 hw_loss 0.325416 lr 0.00049382 rank 6
2023-02-23 01:08:56,930 DEBUG TRAIN Batch 12/2400 loss 15.598154 loss_att 18.634865 loss_ctc 16.091309 loss_rnnt 14.763391 hw_loss 0.303122 lr 0.00049388 rank 1
2023-02-23 01:08:56,932 DEBUG TRAIN Batch 12/2400 loss 13.039454 loss_att 20.038282 loss_ctc 15.210258 loss_rnnt 11.210607 hw_loss 0.261827 lr 0.00049381 rank 7
2023-02-23 01:10:15,788 DEBUG TRAIN Batch 12/2500 loss 7.883605 loss_att 9.376792 loss_ctc 10.150522 loss_rnnt 7.122725 hw_loss 0.299976 lr 0.00049358 rank 2
2023-02-23 01:10:15,788 DEBUG TRAIN Batch 12/2500 loss 7.897188 loss_att 8.892004 loss_ctc 8.380452 loss_rnnt 7.421798 hw_loss 0.397484 lr 0.00049360 rank 5
2023-02-23 01:10:15,794 DEBUG TRAIN Batch 12/2500 loss 18.737614 loss_att 26.385796 loss_ctc 29.074543 loss_rnnt 15.620918 hw_loss 0.391504 lr 0.00049363 rank 4
2023-02-23 01:10:15,797 DEBUG TRAIN Batch 12/2500 loss 14.912255 loss_att 17.817745 loss_ctc 18.565044 loss_rnnt 13.648550 hw_loss 0.366693 lr 0.00049364 rank 1
2023-02-23 01:10:15,797 DEBUG TRAIN Batch 12/2500 loss 34.761955 loss_att 41.467316 loss_ctc 44.065029 loss_rnnt 31.998753 hw_loss 0.340718 lr 0.00049358 rank 3
2023-02-23 01:10:15,802 DEBUG TRAIN Batch 12/2500 loss 24.164473 loss_att 30.807560 loss_ctc 29.408686 loss_rnnt 22.021231 hw_loss 0.216371 lr 0.00049367 rank 0
2023-02-23 01:10:15,802 DEBUG TRAIN Batch 12/2500 loss 21.130056 loss_att 24.760704 loss_ctc 26.367926 loss_rnnt 19.518328 hw_loss 0.351030 lr 0.00049358 rank 6
2023-02-23 01:10:15,825 DEBUG TRAIN Batch 12/2500 loss 15.547656 loss_att 15.949474 loss_ctc 16.062717 loss_rnnt 15.176347 hw_loss 0.416758 lr 0.00049357 rank 7
2023-02-23 01:11:31,822 DEBUG TRAIN Batch 12/2600 loss 25.644320 loss_att 31.095867 loss_ctc 32.109085 loss_rnnt 23.540737 hw_loss 0.283696 lr 0.00049336 rank 5
2023-02-23 01:11:31,828 DEBUG TRAIN Batch 12/2600 loss 12.433022 loss_att 17.692253 loss_ctc 16.527866 loss_rnnt 10.707248 hw_loss 0.239905 lr 0.00049334 rank 3
2023-02-23 01:11:31,830 DEBUG TRAIN Batch 12/2600 loss 25.135818 loss_att 31.505363 loss_ctc 29.505560 loss_rnnt 23.140167 hw_loss 0.260830 lr 0.00049334 rank 2
2023-02-23 01:11:31,831 DEBUG TRAIN Batch 12/2600 loss 17.297279 loss_att 18.578268 loss_ctc 21.439072 loss_rnnt 16.291502 hw_loss 0.370016 lr 0.00049334 rank 6
2023-02-23 01:11:31,832 DEBUG TRAIN Batch 12/2600 loss 21.942877 loss_att 24.404640 loss_ctc 23.625715 loss_rnnt 21.095478 hw_loss 0.245004 lr 0.00049343 rank 0
2023-02-23 01:11:31,833 DEBUG TRAIN Batch 12/2600 loss 8.742300 loss_att 16.419273 loss_ctc 10.254591 loss_rnnt 6.855348 hw_loss 0.281095 lr 0.00049339 rank 4
2023-02-23 01:11:31,835 DEBUG TRAIN Batch 12/2600 loss 28.122772 loss_att 32.910427 loss_ctc 32.238586 loss_rnnt 26.471548 hw_loss 0.271723 lr 0.00049333 rank 7
2023-02-23 01:11:31,878 DEBUG TRAIN Batch 12/2600 loss 12.222140 loss_att 18.300356 loss_ctc 14.552475 loss_rnnt 10.580785 hw_loss 0.215627 lr 0.00049340 rank 1
2023-02-23 01:12:46,877 DEBUG TRAIN Batch 12/2700 loss 12.240243 loss_att 20.994856 loss_ctc 13.308299 loss_rnnt 10.230141 hw_loss 0.218947 lr 0.00049310 rank 2
2023-02-23 01:12:46,879 DEBUG TRAIN Batch 12/2700 loss 20.373901 loss_att 28.812382 loss_ctc 25.994949 loss_rnnt 17.788498 hw_loss 0.277938 lr 0.00049310 rank 3
2023-02-23 01:12:46,881 DEBUG TRAIN Batch 12/2700 loss 9.108787 loss_att 13.436438 loss_ctc 8.132029 loss_rnnt 8.233109 hw_loss 0.263217 lr 0.00049316 rank 1
2023-02-23 01:12:46,887 DEBUG TRAIN Batch 12/2700 loss 18.322416 loss_att 27.643093 loss_ctc 26.377144 loss_rnnt 15.178431 hw_loss 0.386035 lr 0.00049312 rank 5
2023-02-23 01:12:46,886 DEBUG TRAIN Batch 12/2700 loss 21.301661 loss_att 31.202438 loss_ctc 25.417601 loss_rnnt 18.636023 hw_loss 0.256296 lr 0.00049315 rank 4
2023-02-23 01:12:46,886 DEBUG TRAIN Batch 12/2700 loss 9.211681 loss_att 13.716243 loss_ctc 8.623620 loss_rnnt 8.240063 hw_loss 0.279590 lr 0.00049310 rank 6
2023-02-23 01:12:46,888 DEBUG TRAIN Batch 12/2700 loss 14.445627 loss_att 22.665743 loss_ctc 19.040712 loss_rnnt 12.001770 hw_loss 0.350919 lr 0.00049309 rank 7
2023-02-23 01:12:46,888 DEBUG TRAIN Batch 12/2700 loss 15.139939 loss_att 20.367435 loss_ctc 18.851851 loss_rnnt 13.464206 hw_loss 0.253710 lr 0.00049319 rank 0
2023-02-23 01:14:05,179 DEBUG TRAIN Batch 12/2800 loss 13.262948 loss_att 15.535496 loss_ctc 14.782260 loss_rnnt 12.476349 hw_loss 0.242841 lr 0.00049286 rank 2
2023-02-23 01:14:05,179 DEBUG TRAIN Batch 12/2800 loss 10.508187 loss_att 15.052155 loss_ctc 14.186987 loss_rnnt 8.950824 hw_loss 0.296372 lr 0.00049288 rank 5
2023-02-23 01:14:05,180 DEBUG TRAIN Batch 12/2800 loss 18.379210 loss_att 23.641384 loss_ctc 24.556049 loss_rnnt 16.398542 hw_loss 0.196229 lr 0.00049291 rank 4
2023-02-23 01:14:05,186 DEBUG TRAIN Batch 12/2800 loss 7.636981 loss_att 11.746732 loss_ctc 10.605146 loss_rnnt 6.213390 hw_loss 0.386037 lr 0.00049295 rank 0
2023-02-23 01:14:05,186 DEBUG TRAIN Batch 12/2800 loss 11.964351 loss_att 18.211878 loss_ctc 17.371346 loss_rnnt 9.840868 hw_loss 0.286959 lr 0.00049285 rank 7
2023-02-23 01:14:05,186 DEBUG TRAIN Batch 12/2800 loss 8.340307 loss_att 13.704336 loss_ctc 8.665968 loss_rnnt 7.050939 hw_loss 0.324640 lr 0.00049286 rank 3
2023-02-23 01:14:05,189 DEBUG TRAIN Batch 12/2800 loss 10.586856 loss_att 15.594570 loss_ctc 9.602013 loss_rnnt 9.531417 hw_loss 0.347267 lr 0.00049292 rank 1
2023-02-23 01:14:05,233 DEBUG TRAIN Batch 12/2800 loss 23.461603 loss_att 27.591621 loss_ctc 24.733465 loss_rnnt 22.353125 hw_loss 0.211673 lr 0.00049286 rank 6
2023-02-23 01:15:23,402 DEBUG TRAIN Batch 12/2900 loss 20.612303 loss_att 26.385216 loss_ctc 22.344099 loss_rnnt 19.011482 hw_loss 0.403750 lr 0.00049262 rank 3
2023-02-23 01:15:23,403 DEBUG TRAIN Batch 12/2900 loss 17.152798 loss_att 22.535122 loss_ctc 21.268078 loss_rnnt 15.336220 hw_loss 0.358891 lr 0.00049262 rank 2
2023-02-23 01:15:23,404 DEBUG TRAIN Batch 12/2900 loss 11.272875 loss_att 15.620261 loss_ctc 13.742665 loss_rnnt 9.939130 hw_loss 0.253054 lr 0.00049267 rank 4
2023-02-23 01:15:23,406 DEBUG TRAIN Batch 12/2900 loss 14.955279 loss_att 19.017630 loss_ctc 21.994530 loss_rnnt 13.074625 hw_loss 0.243031 lr 0.00049265 rank 5
2023-02-23 01:15:23,407 DEBUG TRAIN Batch 12/2900 loss 15.853892 loss_att 22.086765 loss_ctc 19.069757 loss_rnnt 13.999630 hw_loss 0.335447 lr 0.00049261 rank 7
2023-02-23 01:15:23,408 DEBUG TRAIN Batch 12/2900 loss 14.003994 loss_att 18.946907 loss_ctc 15.767174 loss_rnnt 12.665865 hw_loss 0.214602 lr 0.00049271 rank 0
2023-02-23 01:15:23,409 DEBUG TRAIN Batch 12/2900 loss 10.303109 loss_att 13.277737 loss_ctc 11.695736 loss_rnnt 9.347254 hw_loss 0.328586 lr 0.00049268 rank 1
2023-02-23 01:15:23,458 DEBUG TRAIN Batch 12/2900 loss 9.539700 loss_att 12.470144 loss_ctc 8.810890 loss_rnnt 8.894682 hw_loss 0.292692 lr 0.00049262 rank 6
2023-02-23 01:16:39,734 DEBUG TRAIN Batch 12/3000 loss 18.746077 loss_att 22.908283 loss_ctc 18.629444 loss_rnnt 17.790440 hw_loss 0.260150 lr 0.00049239 rank 2
2023-02-23 01:16:39,739 DEBUG TRAIN Batch 12/3000 loss 8.155039 loss_att 10.752942 loss_ctc 10.971361 loss_rnnt 7.134001 hw_loss 0.236151 lr 0.00049241 rank 5
2023-02-23 01:16:39,739 DEBUG TRAIN Batch 12/3000 loss 7.176965 loss_att 16.499611 loss_ctc 11.207754 loss_rnnt 4.616579 hw_loss 0.297033 lr 0.00049247 rank 0
2023-02-23 01:16:39,740 DEBUG TRAIN Batch 12/3000 loss 15.308617 loss_att 17.763180 loss_ctc 19.657478 loss_rnnt 14.096476 hw_loss 0.265085 lr 0.00049238 rank 6
2023-02-23 01:16:39,742 DEBUG TRAIN Batch 12/3000 loss 20.178680 loss_att 21.961105 loss_ctc 23.829273 loss_rnnt 19.155527 hw_loss 0.337353 lr 0.00049239 rank 3
2023-02-23 01:16:39,743 DEBUG TRAIN Batch 12/3000 loss 25.959730 loss_att 26.435631 loss_ctc 32.480019 loss_rnnt 24.866985 hw_loss 0.240363 lr 0.00049243 rank 4
2023-02-23 01:16:39,744 DEBUG TRAIN Batch 12/3000 loss 16.684315 loss_att 20.348482 loss_ctc 20.360348 loss_rnnt 15.260368 hw_loss 0.376831 lr 0.00049244 rank 1
2023-02-23 01:16:39,790 DEBUG TRAIN Batch 12/3000 loss 23.183992 loss_att 28.120319 loss_ctc 26.508072 loss_rnnt 21.586363 hw_loss 0.313414 lr 0.00049237 rank 7
2023-02-23 01:17:56,498 DEBUG TRAIN Batch 12/3100 loss 8.209875 loss_att 10.228303 loss_ctc 9.649672 loss_rnnt 7.432610 hw_loss 0.340514 lr 0.00049217 rank 5
2023-02-23 01:17:56,503 DEBUG TRAIN Batch 12/3100 loss 13.678830 loss_att 16.140394 loss_ctc 13.831913 loss_rnnt 12.979358 hw_loss 0.350153 lr 0.00049214 rank 6
2023-02-23 01:17:56,506 DEBUG TRAIN Batch 12/3100 loss 14.457738 loss_att 17.939411 loss_ctc 19.066298 loss_rnnt 13.000977 hw_loss 0.273660 lr 0.00049213 rank 7
2023-02-23 01:17:56,505 DEBUG TRAIN Batch 12/3100 loss 19.941961 loss_att 20.514374 loss_ctc 23.901535 loss_rnnt 19.044460 hw_loss 0.478266 lr 0.00049219 rank 4
2023-02-23 01:17:56,506 DEBUG TRAIN Batch 12/3100 loss 6.967191 loss_att 9.732411 loss_ctc 6.980929 loss_rnnt 6.226455 hw_loss 0.348488 lr 0.00049220 rank 1
2023-02-23 01:17:56,507 DEBUG TRAIN Batch 12/3100 loss 27.432627 loss_att 31.230392 loss_ctc 39.098461 loss_rnnt 24.981647 hw_loss 0.254963 lr 0.00049215 rank 2
2023-02-23 01:17:56,510 DEBUG TRAIN Batch 12/3100 loss 14.982911 loss_att 15.230289 loss_ctc 13.983109 loss_rnnt 14.924398 hw_loss 0.266897 lr 0.00049215 rank 3
2023-02-23 01:17:56,521 DEBUG TRAIN Batch 12/3100 loss 19.716276 loss_att 23.016171 loss_ctc 24.530939 loss_rnnt 18.272133 hw_loss 0.266641 lr 0.00049223 rank 0
2023-02-23 01:19:18,006 DEBUG TRAIN Batch 12/3200 loss 12.717102 loss_att 18.298162 loss_ctc 10.643481 loss_rnnt 11.736792 hw_loss 0.263590 lr 0.00049195 rank 4
2023-02-23 01:19:18,012 DEBUG TRAIN Batch 12/3200 loss 14.324094 loss_att 14.258207 loss_ctc 15.779427 loss_rnnt 13.925872 hw_loss 0.407541 lr 0.00049191 rank 2
2023-02-23 01:19:18,012 DEBUG TRAIN Batch 12/3200 loss 24.912775 loss_att 28.054546 loss_ctc 27.449127 loss_rnnt 23.782867 hw_loss 0.306324 lr 0.00049191 rank 6
2023-02-23 01:19:18,017 DEBUG TRAIN Batch 12/3200 loss 11.331711 loss_att 16.526489 loss_ctc 14.638560 loss_rnnt 9.737625 hw_loss 0.214157 lr 0.00049191 rank 3
2023-02-23 01:19:18,017 DEBUG TRAIN Batch 12/3200 loss 10.362754 loss_att 11.390184 loss_ctc 12.049680 loss_rnnt 9.747238 hw_loss 0.347075 lr 0.00049200 rank 0
2023-02-23 01:19:18,019 DEBUG TRAIN Batch 12/3200 loss 25.278620 loss_att 32.637634 loss_ctc 35.193718 loss_rnnt 22.413214 hw_loss 0.134233 lr 0.00049189 rank 7
2023-02-23 01:19:18,020 DEBUG TRAIN Batch 12/3200 loss 19.333929 loss_att 24.029705 loss_ctc 24.187155 loss_rnnt 17.651148 hw_loss 0.180993 lr 0.00049197 rank 1
2023-02-23 01:19:18,029 DEBUG TRAIN Batch 12/3200 loss 17.093851 loss_att 22.967617 loss_ctc 19.339699 loss_rnnt 15.529609 hw_loss 0.168835 lr 0.00049193 rank 5
2023-02-23 01:20:32,973 DEBUG TRAIN Batch 12/3300 loss 10.439687 loss_att 16.270655 loss_ctc 12.779064 loss_rnnt 8.832147 hw_loss 0.242680 lr 0.00049169 rank 5
2023-02-23 01:20:32,976 DEBUG TRAIN Batch 12/3300 loss 6.956604 loss_att 10.524904 loss_ctc 9.797127 loss_rnnt 5.742272 hw_loss 0.228628 lr 0.00049176 rank 0
2023-02-23 01:20:32,976 DEBUG TRAIN Batch 12/3300 loss 15.799819 loss_att 17.757553 loss_ctc 19.842260 loss_rnnt 14.749163 hw_loss 0.225218 lr 0.00049173 rank 1
2023-02-23 01:20:32,980 DEBUG TRAIN Batch 12/3300 loss 20.654461 loss_att 25.495565 loss_ctc 28.119537 loss_rnnt 18.587593 hw_loss 0.193691 lr 0.00049167 rank 6
2023-02-23 01:20:32,982 DEBUG TRAIN Batch 12/3300 loss 16.838589 loss_att 19.066004 loss_ctc 18.412420 loss_rnnt 16.000582 hw_loss 0.342522 lr 0.00049167 rank 3
2023-02-23 01:20:32,984 DEBUG TRAIN Batch 12/3300 loss 12.611903 loss_att 16.884893 loss_ctc 13.214836 loss_rnnt 11.526016 hw_loss 0.282934 lr 0.00049171 rank 4
2023-02-23 01:20:32,985 DEBUG TRAIN Batch 12/3300 loss 16.601078 loss_att 22.702282 loss_ctc 18.594915 loss_rnnt 14.892082 hw_loss 0.417955 lr 0.00049167 rank 2
2023-02-23 01:20:32,988 DEBUG TRAIN Batch 12/3300 loss 11.908783 loss_att 17.954824 loss_ctc 14.766643 loss_rnnt 10.192698 hw_loss 0.235926 lr 0.00049165 rank 7
2023-02-23 01:21:49,643 DEBUG TRAIN Batch 12/3400 loss 11.000814 loss_att 19.910160 loss_ctc 12.736678 loss_rnnt 8.853693 hw_loss 0.250883 lr 0.00049149 rank 1
2023-02-23 01:21:49,644 DEBUG TRAIN Batch 12/3400 loss 22.103580 loss_att 22.970943 loss_ctc 26.744461 loss_rnnt 21.127592 hw_loss 0.344496 lr 0.00049143 rank 2
2023-02-23 01:21:49,645 DEBUG TRAIN Batch 12/3400 loss 22.181585 loss_att 23.224270 loss_ctc 23.787354 loss_rnnt 21.588713 hw_loss 0.319191 lr 0.00049152 rank 0
2023-02-23 01:21:49,645 DEBUG TRAIN Batch 12/3400 loss 16.790878 loss_att 18.188187 loss_ctc 18.542627 loss_rnnt 16.154007 hw_loss 0.232207 lr 0.00049148 rank 4
2023-02-23 01:21:49,647 DEBUG TRAIN Batch 12/3400 loss 12.477910 loss_att 18.085381 loss_ctc 13.598595 loss_rnnt 11.062068 hw_loss 0.271732 lr 0.00049145 rank 5
2023-02-23 01:21:49,651 DEBUG TRAIN Batch 12/3400 loss 23.758198 loss_att 26.173309 loss_ctc 26.957571 loss_rnnt 22.726492 hw_loss 0.228940 lr 0.00049142 rank 7
2023-02-23 01:21:49,652 DEBUG TRAIN Batch 12/3400 loss 17.372969 loss_att 22.602240 loss_ctc 22.785952 loss_rnnt 15.442301 hw_loss 0.305781 lr 0.00049143 rank 3
2023-02-23 01:21:49,696 DEBUG TRAIN Batch 12/3400 loss 11.009661 loss_att 15.623929 loss_ctc 16.039478 loss_rnnt 9.289223 hw_loss 0.238015 lr 0.00049143 rank 6
2023-02-23 01:23:07,304 DEBUG TRAIN Batch 12/3500 loss 15.726067 loss_att 17.337107 loss_ctc 17.238087 loss_rnnt 15.002500 hw_loss 0.374543 lr 0.00049122 rank 5
2023-02-23 01:23:07,309 DEBUG TRAIN Batch 12/3500 loss 23.189602 loss_att 21.301538 loss_ctc 28.966431 loss_rnnt 22.657413 hw_loss 0.261671 lr 0.00049120 rank 3
2023-02-23 01:23:07,310 DEBUG TRAIN Batch 12/3500 loss 16.993309 loss_att 25.863956 loss_ctc 20.183531 loss_rnnt 14.672539 hw_loss 0.227393 lr 0.00049118 rank 7
2023-02-23 01:23:07,312 DEBUG TRAIN Batch 12/3500 loss 20.995947 loss_att 23.839546 loss_ctc 29.332542 loss_rnnt 19.152264 hw_loss 0.306411 lr 0.00049128 rank 0
2023-02-23 01:23:07,312 DEBUG TRAIN Batch 12/3500 loss 17.470213 loss_att 19.559380 loss_ctc 18.266857 loss_rnnt 16.805025 hw_loss 0.264624 lr 0.00049124 rank 4
2023-02-23 01:23:07,313 DEBUG TRAIN Batch 12/3500 loss 13.864899 loss_att 20.463844 loss_ctc 15.603433 loss_rnnt 12.154212 hw_loss 0.298300 lr 0.00049120 rank 2
2023-02-23 01:23:07,323 DEBUG TRAIN Batch 12/3500 loss 17.216295 loss_att 19.044102 loss_ctc 24.688351 loss_rnnt 15.715187 hw_loss 0.261135 lr 0.00049119 rank 6
2023-02-23 01:23:07,332 DEBUG TRAIN Batch 12/3500 loss 24.675537 loss_att 29.933359 loss_ctc 29.601919 loss_rnnt 22.842453 hw_loss 0.233755 lr 0.00049125 rank 1
2023-02-23 01:24:25,169 DEBUG TRAIN Batch 12/3600 loss 17.576551 loss_att 18.259081 loss_ctc 18.638264 loss_rnnt 17.176437 hw_loss 0.228832 lr 0.00049100 rank 4
2023-02-23 01:24:25,171 DEBUG TRAIN Batch 12/3600 loss 19.678999 loss_att 22.860193 loss_ctc 24.127621 loss_rnnt 18.308086 hw_loss 0.265356 lr 0.00049098 rank 5
2023-02-23 01:24:25,171 DEBUG TRAIN Batch 12/3600 loss 12.535555 loss_att 16.971771 loss_ctc 11.929316 loss_rnnt 11.600149 hw_loss 0.241866 lr 0.00049102 rank 1
2023-02-23 01:24:25,171 DEBUG TRAIN Batch 12/3600 loss 23.617125 loss_att 25.341461 loss_ctc 28.171413 loss_rnnt 22.521824 hw_loss 0.268490 lr 0.00049105 rank 0
2023-02-23 01:24:25,172 DEBUG TRAIN Batch 12/3600 loss 20.627270 loss_att 21.063229 loss_ctc 23.116165 loss_rnnt 20.087883 hw_loss 0.225639 lr 0.00049096 rank 2
2023-02-23 01:24:25,174 DEBUG TRAIN Batch 12/3600 loss 30.714018 loss_att 29.648462 loss_ctc 38.544353 loss_rnnt 29.760283 hw_loss 0.230257 lr 0.00049096 rank 3
2023-02-23 01:24:25,174 DEBUG TRAIN Batch 12/3600 loss 22.355433 loss_att 28.161236 loss_ctc 24.615280 loss_rnnt 20.767109 hw_loss 0.235966 lr 0.00049094 rank 7
2023-02-23 01:24:25,175 DEBUG TRAIN Batch 12/3600 loss 17.792595 loss_att 25.147999 loss_ctc 23.529634 loss_rnnt 15.374377 hw_loss 0.341623 lr 0.00049096 rank 6
2023-02-23 01:25:40,891 DEBUG TRAIN Batch 12/3700 loss 26.403229 loss_att 25.371796 loss_ctc 29.207336 loss_rnnt 26.007868 hw_loss 0.427059 lr 0.00049081 rank 0
2023-02-23 01:25:40,891 DEBUG TRAIN Batch 12/3700 loss 14.923795 loss_att 15.790125 loss_ctc 17.965189 loss_rnnt 14.222259 hw_loss 0.230157 lr 0.00049072 rank 2
2023-02-23 01:25:40,892 DEBUG TRAIN Batch 12/3700 loss 22.117298 loss_att 32.182190 loss_ctc 28.681808 loss_rnnt 19.144325 hw_loss 0.158864 lr 0.00049072 rank 3
2023-02-23 01:25:40,896 DEBUG TRAIN Batch 12/3700 loss 12.308686 loss_att 13.867038 loss_ctc 10.993765 loss_rnnt 12.006766 hw_loss 0.310448 lr 0.00049072 rank 6
2023-02-23 01:25:40,898 DEBUG TRAIN Batch 12/3700 loss 27.390465 loss_att 29.710079 loss_ctc 37.693390 loss_rnnt 25.372520 hw_loss 0.338060 lr 0.00049074 rank 5
2023-02-23 01:25:40,900 DEBUG TRAIN Batch 12/3700 loss 10.526305 loss_att 13.327902 loss_ctc 12.424968 loss_rnnt 9.565140 hw_loss 0.276922 lr 0.00049076 rank 4
2023-02-23 01:25:40,901 DEBUG TRAIN Batch 12/3700 loss 16.780333 loss_att 20.057779 loss_ctc 20.569351 loss_rnnt 15.502956 hw_loss 0.218781 lr 0.00049071 rank 7
2023-02-23 01:25:40,943 DEBUG TRAIN Batch 12/3700 loss 25.971867 loss_att 30.131655 loss_ctc 28.159592 loss_rnnt 24.712280 hw_loss 0.254869 lr 0.00049078 rank 1
2023-02-23 01:26:58,472 DEBUG TRAIN Batch 12/3800 loss 14.791736 loss_att 16.968044 loss_ctc 18.584946 loss_rnnt 13.652812 hw_loss 0.371063 lr 0.00049049 rank 2
2023-02-23 01:26:58,472 DEBUG TRAIN Batch 12/3800 loss 7.621085 loss_att 8.436866 loss_ctc 8.039820 loss_rnnt 7.150622 hw_loss 0.471518 lr 0.00049057 rank 0
2023-02-23 01:26:58,473 DEBUG TRAIN Batch 12/3800 loss 16.644314 loss_att 18.795160 loss_ctc 21.256609 loss_rnnt 15.509928 hw_loss 0.167331 lr 0.00049048 rank 6
2023-02-23 01:26:58,473 DEBUG TRAIN Batch 12/3800 loss 14.096046 loss_att 14.411804 loss_ctc 16.430740 loss_rnnt 13.477420 hw_loss 0.457843 lr 0.00049051 rank 5
2023-02-23 01:26:58,474 DEBUG TRAIN Batch 12/3800 loss 22.871611 loss_att 29.778660 loss_ctc 25.925329 loss_rnnt 20.994652 hw_loss 0.165723 lr 0.00049053 rank 4
2023-02-23 01:26:58,477 DEBUG TRAIN Batch 12/3800 loss 9.824921 loss_att 13.943311 loss_ctc 11.347450 loss_rnnt 8.635769 hw_loss 0.304629 lr 0.00049047 rank 7
2023-02-23 01:26:58,479 DEBUG TRAIN Batch 12/3800 loss 18.472609 loss_att 21.345951 loss_ctc 23.773298 loss_rnnt 17.018097 hw_loss 0.324530 lr 0.00049054 rank 1
2023-02-23 01:26:58,518 DEBUG TRAIN Batch 12/3800 loss 11.932987 loss_att 18.486788 loss_ctc 16.006414 loss_rnnt 9.899254 hw_loss 0.337220 lr 0.00049049 rank 3
2023-02-23 01:28:17,876 DEBUG TRAIN Batch 12/3900 loss 8.105257 loss_att 11.946857 loss_ctc 11.875618 loss_rnnt 6.649372 hw_loss 0.346595 lr 0.00049025 rank 2
2023-02-23 01:28:17,877 DEBUG TRAIN Batch 12/3900 loss 7.880255 loss_att 9.615185 loss_ctc 9.406974 loss_rnnt 7.166615 hw_loss 0.305796 lr 0.00049025 rank 6
2023-02-23 01:28:17,877 DEBUG TRAIN Batch 12/3900 loss 11.109256 loss_att 14.997948 loss_ctc 11.464000 loss_rnnt 10.163482 hw_loss 0.226382 lr 0.00049034 rank 0
2023-02-23 01:28:17,878 DEBUG TRAIN Batch 12/3900 loss 13.486635 loss_att 22.675524 loss_ctc 18.526367 loss_rnnt 10.852713 hw_loss 0.232839 lr 0.00049025 rank 3
2023-02-23 01:28:17,880 DEBUG TRAIN Batch 12/3900 loss 12.447767 loss_att 16.620422 loss_ctc 11.201168 loss_rnnt 11.597109 hw_loss 0.341888 lr 0.00049031 rank 1
2023-02-23 01:28:17,880 DEBUG TRAIN Batch 12/3900 loss 6.430560 loss_att 11.606305 loss_ctc 9.059303 loss_rnnt 4.921690 hw_loss 0.231041 lr 0.00049029 rank 4
2023-02-23 01:28:17,884 DEBUG TRAIN Batch 12/3900 loss 54.965939 loss_att 57.518776 loss_ctc 61.453457 loss_rnnt 53.483562 hw_loss 0.200264 lr 0.00049027 rank 5
2023-02-23 01:28:17,899 DEBUG TRAIN Batch 12/3900 loss 19.120440 loss_att 23.130001 loss_ctc 23.377289 loss_rnnt 17.658562 hw_loss 0.173226 lr 0.00049023 rank 7
2023-02-23 01:29:35,185 DEBUG TRAIN Batch 12/4000 loss 24.854555 loss_att 31.058479 loss_ctc 30.364904 loss_rnnt 22.712461 hw_loss 0.312370 lr 0.00049004 rank 5
2023-02-23 01:29:35,185 DEBUG TRAIN Batch 12/4000 loss 8.680131 loss_att 11.011047 loss_ctc 12.233726 loss_rnnt 7.620233 hw_loss 0.224817 lr 0.00049001 rank 2
2023-02-23 01:29:35,195 DEBUG TRAIN Batch 12/4000 loss 10.554666 loss_att 11.784791 loss_ctc 11.595451 loss_rnnt 10.026833 hw_loss 0.268194 lr 0.00049007 rank 1
2023-02-23 01:29:35,197 DEBUG TRAIN Batch 12/4000 loss 28.373507 loss_att 30.223637 loss_ctc 37.790154 loss_rnnt 26.587936 hw_loss 0.299981 lr 0.00049010 rank 0
2023-02-23 01:29:35,197 DEBUG TRAIN Batch 12/4000 loss 9.823769 loss_att 15.896712 loss_ctc 11.546111 loss_rnnt 8.211376 hw_loss 0.315296 lr 0.00049001 rank 6
2023-02-23 01:29:35,200 DEBUG TRAIN Batch 12/4000 loss 12.942678 loss_att 20.167385 loss_ctc 13.821643 loss_rnnt 11.228037 hw_loss 0.285944 lr 0.00049000 rank 7
2023-02-23 01:29:35,200 DEBUG TRAIN Batch 12/4000 loss 15.711550 loss_att 18.614712 loss_ctc 15.341286 loss_rnnt 15.070552 hw_loss 0.205754 lr 0.00049006 rank 4
2023-02-23 01:29:35,237 DEBUG TRAIN Batch 12/4000 loss 20.969822 loss_att 25.928234 loss_ctc 26.617775 loss_rnnt 19.056744 hw_loss 0.315631 lr 0.00049001 rank 3
2023-02-23 01:30:51,567 DEBUG TRAIN Batch 12/4100 loss 13.286586 loss_att 17.795692 loss_ctc 16.015108 loss_rnnt 11.855702 hw_loss 0.309860 lr 0.00048982 rank 4
2023-02-23 01:30:51,567 DEBUG TRAIN Batch 12/4100 loss 8.177277 loss_att 8.284840 loss_ctc 7.505122 loss_rnnt 8.086735 hw_loss 0.297468 lr 0.00048987 rank 0
2023-02-23 01:30:51,568 DEBUG TRAIN Batch 12/4100 loss 17.271219 loss_att 23.537537 loss_ctc 22.354748 loss_rnnt 15.231772 hw_loss 0.203213 lr 0.00048980 rank 5
2023-02-23 01:30:51,572 DEBUG TRAIN Batch 12/4100 loss 9.239418 loss_att 15.031185 loss_ctc 13.282677 loss_rnnt 7.297866 hw_loss 0.457684 lr 0.00048978 rank 3
2023-02-23 01:30:51,574 DEBUG TRAIN Batch 12/4100 loss 16.922173 loss_att 21.555016 loss_ctc 19.809748 loss_rnnt 15.458452 hw_loss 0.285264 lr 0.00048978 rank 2
2023-02-23 01:30:51,576 DEBUG TRAIN Batch 12/4100 loss 11.417952 loss_att 14.126774 loss_ctc 14.519461 loss_rnnt 10.362185 hw_loss 0.188376 lr 0.00048984 rank 1
2023-02-23 01:30:51,577 DEBUG TRAIN Batch 12/4100 loss 15.338778 loss_att 16.706783 loss_ctc 15.251970 loss_rnnt 14.946430 hw_loss 0.244352 lr 0.00048978 rank 6
2023-02-23 01:30:51,577 DEBUG TRAIN Batch 12/4100 loss 16.153328 loss_att 23.010189 loss_ctc 18.279913 loss_rnnt 14.325160 hw_loss 0.324845 lr 0.00048976 rank 7
2023-02-23 01:32:08,521 DEBUG TRAIN Batch 12/4200 loss 19.067240 loss_att 23.026184 loss_ctc 23.757154 loss_rnnt 17.488876 hw_loss 0.302347 lr 0.00048963 rank 0
2023-02-23 01:32:08,525 DEBUG TRAIN Batch 12/4200 loss 12.301435 loss_att 13.143301 loss_ctc 14.150121 loss_rnnt 11.751989 hw_loss 0.252339 lr 0.00048954 rank 2
2023-02-23 01:32:08,524 DEBUG TRAIN Batch 12/4200 loss 14.283820 loss_att 20.445585 loss_ctc 14.606962 loss_rnnt 12.866285 hw_loss 0.266430 lr 0.00048960 rank 1
2023-02-23 01:32:08,530 DEBUG TRAIN Batch 12/4200 loss 23.575853 loss_att 26.129532 loss_ctc 27.727579 loss_rnnt 22.377386 hw_loss 0.251565 lr 0.00048957 rank 5
2023-02-23 01:32:08,531 DEBUG TRAIN Batch 12/4200 loss 15.170318 loss_att 19.086756 loss_ctc 19.181740 loss_rnnt 13.694465 hw_loss 0.295705 lr 0.00048959 rank 4
2023-02-23 01:32:08,533 DEBUG TRAIN Batch 12/4200 loss 7.922008 loss_att 12.859815 loss_ctc 9.018315 loss_rnnt 6.657384 hw_loss 0.245414 lr 0.00048953 rank 7
2023-02-23 01:32:08,533 DEBUG TRAIN Batch 12/4200 loss 20.234129 loss_att 23.879673 loss_ctc 25.125378 loss_rnnt 18.696676 hw_loss 0.292836 lr 0.00048954 rank 3
2023-02-23 01:32:08,575 DEBUG TRAIN Batch 12/4200 loss 10.131320 loss_att 17.038076 loss_ctc 9.297541 loss_rnnt 8.699068 hw_loss 0.303884 lr 0.00048954 rank 6
2023-02-23 01:33:26,357 DEBUG TRAIN Batch 12/4300 loss 23.346706 loss_att 28.792166 loss_ctc 29.140385 loss_rnnt 21.374741 hw_loss 0.206969 lr 0.00048933 rank 5
2023-02-23 01:33:26,361 DEBUG TRAIN Batch 12/4300 loss 38.596233 loss_att 38.205822 loss_ctc 40.624733 loss_rnnt 38.223316 hw_loss 0.338498 lr 0.00048935 rank 4
2023-02-23 01:33:26,363 DEBUG TRAIN Batch 12/4300 loss 6.739436 loss_att 10.054836 loss_ctc 11.998798 loss_rnnt 5.176734 hw_loss 0.371949 lr 0.00048937 rank 1
2023-02-23 01:33:26,364 DEBUG TRAIN Batch 12/4300 loss 3.076058 loss_att 5.969056 loss_ctc 3.483546 loss_rnnt 2.264664 hw_loss 0.334619 lr 0.00048940 rank 0
2023-02-23 01:33:26,365 DEBUG TRAIN Batch 12/4300 loss 8.781075 loss_att 14.963197 loss_ctc 13.126979 loss_rnnt 6.860524 hw_loss 0.196264 lr 0.00048929 rank 7
2023-02-23 01:33:26,367 DEBUG TRAIN Batch 12/4300 loss 22.068773 loss_att 24.067415 loss_ctc 21.637634 loss_rnnt 21.618359 hw_loss 0.202822 lr 0.00048931 rank 6
2023-02-23 01:33:26,369 DEBUG TRAIN Batch 12/4300 loss 11.059514 loss_att 11.333361 loss_ctc 10.579772 loss_rnnt 10.851213 hw_loss 0.407808 lr 0.00048931 rank 3
2023-02-23 01:33:26,371 DEBUG TRAIN Batch 12/4300 loss 8.984753 loss_att 13.901415 loss_ctc 12.617887 loss_rnnt 7.429814 hw_loss 0.163476 lr 0.00048931 rank 2
2023-02-23 01:34:40,896 DEBUG TRAIN Batch 12/4400 loss 9.683324 loss_att 10.631181 loss_ctc 11.064213 loss_rnnt 9.167796 hw_loss 0.265945 lr 0.00048910 rank 5
2023-02-23 01:34:40,899 DEBUG TRAIN Batch 12/4400 loss 15.106534 loss_att 21.740921 loss_ctc 18.565224 loss_rnnt 13.177542 hw_loss 0.264295 lr 0.00048907 rank 6
2023-02-23 01:34:40,901 DEBUG TRAIN Batch 12/4400 loss 10.443850 loss_att 11.954321 loss_ctc 11.509005 loss_rnnt 9.826476 hw_loss 0.324859 lr 0.00048912 rank 4
2023-02-23 01:34:40,903 DEBUG TRAIN Batch 12/4400 loss 15.039451 loss_att 16.770414 loss_ctc 17.959192 loss_rnnt 14.190795 hw_loss 0.212183 lr 0.00048906 rank 7
2023-02-23 01:34:40,905 DEBUG TRAIN Batch 12/4400 loss 16.085825 loss_att 20.221943 loss_ctc 21.428663 loss_rnnt 14.385887 hw_loss 0.300627 lr 0.00048908 rank 2
2023-02-23 01:34:40,907 DEBUG TRAIN Batch 12/4400 loss 17.254339 loss_att 17.066219 loss_ctc 18.968801 loss_rnnt 16.875956 hw_loss 0.351395 lr 0.00048916 rank 0
2023-02-23 01:34:40,910 DEBUG TRAIN Batch 12/4400 loss 19.380402 loss_att 21.346949 loss_ctc 24.961845 loss_rnnt 18.135815 hw_loss 0.200787 lr 0.00048913 rank 1
2023-02-23 01:34:40,953 DEBUG TRAIN Batch 12/4400 loss 28.704475 loss_att 28.165451 loss_ctc 37.431400 loss_rnnt 27.496723 hw_loss 0.284938 lr 0.00048908 rank 3
2023-02-23 01:35:57,058 DEBUG TRAIN Batch 12/4500 loss 17.149540 loss_att 19.512547 loss_ctc 20.556746 loss_rnnt 16.030664 hw_loss 0.359959 lr 0.00048884 rank 2
2023-02-23 01:35:57,061 DEBUG TRAIN Batch 12/4500 loss 13.513197 loss_att 18.207314 loss_ctc 17.629393 loss_rnnt 11.871860 hw_loss 0.288165 lr 0.00048886 rank 5
2023-02-23 01:35:57,064 DEBUG TRAIN Batch 12/4500 loss 12.172897 loss_att 16.154913 loss_ctc 14.156920 loss_rnnt 11.024452 hw_loss 0.164072 lr 0.00048884 rank 3
2023-02-23 01:35:57,065 DEBUG TRAIN Batch 12/4500 loss 14.986824 loss_att 19.444452 loss_ctc 18.566456 loss_rnnt 13.499586 hw_loss 0.222054 lr 0.00048893 rank 0
2023-02-23 01:35:57,067 DEBUG TRAIN Batch 12/4500 loss 6.827648 loss_att 13.320257 loss_ctc 9.657582 loss_rnnt 5.050064 hw_loss 0.190757 lr 0.00048888 rank 4
2023-02-23 01:35:57,070 DEBUG TRAIN Batch 12/4500 loss 8.490066 loss_att 10.032773 loss_ctc 12.974508 loss_rnnt 7.469739 hw_loss 0.213484 lr 0.00048884 rank 6
2023-02-23 01:35:57,072 DEBUG TRAIN Batch 12/4500 loss 15.474780 loss_att 16.368561 loss_ctc 19.463081 loss_rnnt 14.565759 hw_loss 0.372173 lr 0.00048890 rank 1
2023-02-23 01:35:57,073 DEBUG TRAIN Batch 12/4500 loss 9.137831 loss_att 18.021122 loss_ctc 12.568356 loss_rnnt 6.821035 hw_loss 0.155127 lr 0.00048883 rank 7
2023-02-23 01:37:15,417 DEBUG TRAIN Batch 12/4600 loss 8.646038 loss_att 10.467708 loss_ctc 7.472824 loss_rnnt 8.332491 hw_loss 0.198079 lr 0.00048861 rank 3
2023-02-23 01:37:15,418 DEBUG TRAIN Batch 12/4600 loss 10.257203 loss_att 15.463840 loss_ctc 12.368773 loss_rnnt 8.768232 hw_loss 0.311439 lr 0.00048870 rank 0
2023-02-23 01:37:15,418 DEBUG TRAIN Batch 12/4600 loss 19.812504 loss_att 23.404240 loss_ctc 34.395695 loss_rnnt 16.983418 hw_loss 0.311843 lr 0.00048865 rank 4
2023-02-23 01:37:15,419 DEBUG TRAIN Batch 12/4600 loss 12.371463 loss_att 15.257804 loss_ctc 13.226344 loss_rnnt 11.580630 hw_loss 0.186713 lr 0.00048859 rank 7
2023-02-23 01:37:15,421 DEBUG TRAIN Batch 12/4600 loss 15.649139 loss_att 20.989414 loss_ctc 20.023224 loss_rnnt 13.809535 hw_loss 0.353137 lr 0.00048863 rank 5
2023-02-23 01:37:15,423 DEBUG TRAIN Batch 12/4600 loss 16.748625 loss_att 16.812157 loss_ctc 17.758186 loss_rnnt 16.451729 hw_loss 0.280465 lr 0.00048861 rank 2
2023-02-23 01:37:15,428 DEBUG TRAIN Batch 12/4600 loss 18.323620 loss_att 25.434124 loss_ctc 18.152704 loss_rnnt 16.783020 hw_loss 0.264914 lr 0.00048866 rank 1
2023-02-23 01:37:15,431 DEBUG TRAIN Batch 12/4600 loss 14.926325 loss_att 19.778133 loss_ctc 14.403355 loss_rnnt 13.851051 hw_loss 0.327454 lr 0.00048861 rank 6
2023-02-23 01:38:31,155 DEBUG TRAIN Batch 12/4700 loss 7.112012 loss_att 13.519789 loss_ctc 8.723488 loss_rnnt 5.444117 hw_loss 0.321520 lr 0.00048838 rank 2
2023-02-23 01:38:31,158 DEBUG TRAIN Batch 12/4700 loss 10.640930 loss_att 13.677118 loss_ctc 13.623789 loss_rnnt 9.492214 hw_loss 0.269556 lr 0.00048843 rank 1
2023-02-23 01:38:31,159 DEBUG TRAIN Batch 12/4700 loss 10.770828 loss_att 16.454784 loss_ctc 11.414333 loss_rnnt 9.383575 hw_loss 0.308737 lr 0.00048837 rank 6
2023-02-23 01:38:31,159 DEBUG TRAIN Batch 12/4700 loss 11.836077 loss_att 15.376211 loss_ctc 14.367685 loss_rnnt 10.618982 hw_loss 0.321601 lr 0.00048840 rank 5
2023-02-23 01:38:31,160 DEBUG TRAIN Batch 12/4700 loss 16.407793 loss_att 22.967373 loss_ctc 18.091566 loss_rnnt 14.660509 hw_loss 0.395368 lr 0.00048846 rank 0
2023-02-23 01:38:31,162 DEBUG TRAIN Batch 12/4700 loss 20.278343 loss_att 27.079384 loss_ctc 25.027939 loss_rnnt 18.135754 hw_loss 0.279569 lr 0.00048842 rank 4
2023-02-23 01:38:31,163 DEBUG TRAIN Batch 12/4700 loss 17.888227 loss_att 23.949120 loss_ctc 21.107634 loss_rnnt 16.106300 hw_loss 0.263431 lr 0.00048838 rank 3
2023-02-23 01:38:31,171 DEBUG TRAIN Batch 12/4700 loss 8.600915 loss_att 13.384966 loss_ctc 9.781936 loss_rnnt 7.344821 hw_loss 0.265900 lr 0.00048836 rank 7
2023-02-23 01:39:46,388 DEBUG TRAIN Batch 12/4800 loss 5.081778 loss_att 9.395466 loss_ctc 8.716218 loss_rnnt 3.548146 hw_loss 0.349316 lr 0.00048823 rank 0
2023-02-23 01:39:46,391 DEBUG TRAIN Batch 12/4800 loss 19.625824 loss_att 26.680222 loss_ctc 26.328033 loss_rnnt 17.183109 hw_loss 0.259138 lr 0.00048813 rank 7
2023-02-23 01:39:46,391 DEBUG TRAIN Batch 12/4800 loss 11.457410 loss_att 18.313770 loss_ctc 15.723033 loss_rnnt 9.355629 hw_loss 0.303296 lr 0.00048818 rank 4
2023-02-23 01:39:46,393 DEBUG TRAIN Batch 12/4800 loss 21.601301 loss_att 22.871109 loss_ctc 25.425785 loss_rnnt 20.715723 hw_loss 0.228160 lr 0.00048816 rank 5
2023-02-23 01:39:46,394 DEBUG TRAIN Batch 12/4800 loss 27.048273 loss_att 29.173763 loss_ctc 33.416000 loss_rnnt 25.609367 hw_loss 0.308959 lr 0.00048814 rank 3
2023-02-23 01:39:46,395 DEBUG TRAIN Batch 12/4800 loss 20.408106 loss_att 24.251762 loss_ctc 29.460463 loss_rnnt 18.327959 hw_loss 0.195812 lr 0.00048814 rank 2
2023-02-23 01:39:46,398 DEBUG TRAIN Batch 12/4800 loss 9.571223 loss_att 13.804244 loss_ctc 12.952002 loss_rnnt 8.216835 hw_loss 0.106899 lr 0.00048820 rank 1
2023-02-23 01:39:46,401 DEBUG TRAIN Batch 12/4800 loss 6.737643 loss_att 11.128469 loss_ctc 9.602095 loss_rnnt 5.354610 hw_loss 0.230514 lr 0.00048814 rank 6
2023-02-23 01:41:02,708 DEBUG TRAIN Batch 12/4900 loss 17.906038 loss_att 22.938095 loss_ctc 21.938801 loss_rnnt 16.232574 hw_loss 0.242534 lr 0.00048791 rank 2
2023-02-23 01:41:02,715 DEBUG TRAIN Batch 12/4900 loss 12.255692 loss_att 17.532814 loss_ctc 17.377211 loss_rnnt 10.351382 hw_loss 0.311279 lr 0.00048791 rank 3
2023-02-23 01:41:02,716 DEBUG TRAIN Batch 12/4900 loss 10.213812 loss_att 16.344528 loss_ctc 10.973358 loss_rnnt 8.751436 hw_loss 0.253049 lr 0.00048789 rank 7
2023-02-23 01:41:02,716 DEBUG TRAIN Batch 12/4900 loss 17.276485 loss_att 22.412472 loss_ctc 24.477631 loss_rnnt 15.133827 hw_loss 0.291202 lr 0.00048800 rank 0
2023-02-23 01:41:02,723 DEBUG TRAIN Batch 12/4900 loss 13.431812 loss_att 17.956993 loss_ctc 15.236693 loss_rnnt 12.152085 hw_loss 0.251324 lr 0.00048791 rank 6
2023-02-23 01:41:02,737 DEBUG TRAIN Batch 12/4900 loss 11.701364 loss_att 13.409721 loss_ctc 12.086842 loss_rnnt 11.163922 hw_loss 0.270698 lr 0.00048793 rank 5
2023-02-23 01:41:02,748 DEBUG TRAIN Batch 12/4900 loss 10.110474 loss_att 15.468512 loss_ctc 13.624599 loss_rnnt 8.483936 hw_loss 0.161962 lr 0.00048795 rank 4
2023-02-23 01:41:02,761 DEBUG TRAIN Batch 12/4900 loss 20.620703 loss_att 25.550867 loss_ctc 24.033741 loss_rnnt 19.044254 hw_loss 0.253766 lr 0.00048797 rank 1
2023-02-23 01:42:22,695 DEBUG TRAIN Batch 12/5000 loss 10.186716 loss_att 10.694293 loss_ctc 9.684316 loss_rnnt 9.983936 hw_loss 0.315472 lr 0.00048770 rank 5
2023-02-23 01:42:22,701 DEBUG TRAIN Batch 12/5000 loss 12.169227 loss_att 12.742006 loss_ctc 14.879028 loss_rnnt 11.534503 hw_loss 0.297866 lr 0.00048768 rank 2
2023-02-23 01:42:22,702 DEBUG TRAIN Batch 12/5000 loss 18.373549 loss_att 19.483843 loss_ctc 22.337536 loss_rnnt 17.451572 hw_loss 0.321348 lr 0.00048772 rank 4
2023-02-23 01:42:22,703 DEBUG TRAIN Batch 12/5000 loss 24.217110 loss_att 27.258125 loss_ctc 28.633951 loss_rnnt 22.940870 hw_loss 0.148358 lr 0.00048776 rank 0
2023-02-23 01:42:22,704 DEBUG TRAIN Batch 12/5000 loss 11.730472 loss_att 12.624517 loss_ctc 16.429062 loss_rnnt 10.752227 hw_loss 0.324294 lr 0.00048768 rank 6
2023-02-23 01:42:22,704 DEBUG TRAIN Batch 12/5000 loss 10.631038 loss_att 10.288475 loss_ctc 12.011786 loss_rnnt 10.320042 hw_loss 0.366393 lr 0.00048768 rank 3
2023-02-23 01:42:22,704 DEBUG TRAIN Batch 12/5000 loss 21.369761 loss_att 26.819988 loss_ctc 26.247246 loss_rnnt 19.460873 hw_loss 0.315958 lr 0.00048773 rank 1
2023-02-23 01:42:22,705 DEBUG TRAIN Batch 12/5000 loss 10.184628 loss_att 11.325841 loss_ctc 10.355467 loss_rnnt 9.761220 hw_loss 0.323225 lr 0.00048766 rank 7
2023-02-23 01:43:39,662 DEBUG TRAIN Batch 12/5100 loss 8.653232 loss_att 8.229036 loss_ctc 10.339744 loss_rnnt 8.306913 hw_loss 0.386794 lr 0.00048753 rank 0
2023-02-23 01:43:39,664 DEBUG TRAIN Batch 12/5100 loss 7.305842 loss_att 17.882175 loss_ctc 8.487753 loss_rnnt 4.840989 hw_loss 0.359998 lr 0.00048745 rank 2
2023-02-23 01:43:39,668 DEBUG TRAIN Batch 12/5100 loss 14.519332 loss_att 18.219559 loss_ctc 12.274867 loss_rnnt 13.916949 hw_loss 0.302998 lr 0.00048749 rank 4
2023-02-23 01:43:39,669 DEBUG TRAIN Batch 12/5100 loss 16.227116 loss_att 23.430092 loss_ctc 21.006737 loss_rnnt 14.013629 hw_loss 0.254268 lr 0.00048747 rank 5
2023-02-23 01:43:39,671 DEBUG TRAIN Batch 12/5100 loss 13.515197 loss_att 15.484715 loss_ctc 16.451056 loss_rnnt 12.620132 hw_loss 0.205712 lr 0.00048744 rank 6
2023-02-23 01:43:39,672 DEBUG TRAIN Batch 12/5100 loss 19.701538 loss_att 21.547466 loss_ctc 26.949331 loss_rnnt 18.195183 hw_loss 0.320244 lr 0.00048750 rank 1
2023-02-23 01:43:39,674 DEBUG TRAIN Batch 12/5100 loss 14.483451 loss_att 17.100512 loss_ctc 18.076239 loss_rnnt 13.349157 hw_loss 0.247206 lr 0.00048743 rank 7
2023-02-23 01:43:39,712 DEBUG TRAIN Batch 12/5100 loss 21.282307 loss_att 27.398842 loss_ctc 33.758415 loss_rnnt 18.256170 hw_loss 0.261278 lr 0.00048745 rank 3
2023-02-23 01:44:54,910 DEBUG TRAIN Batch 12/5200 loss 14.341421 loss_att 13.102453 loss_ctc 14.395575 loss_rnnt 14.370085 hw_loss 0.397332 lr 0.00048721 rank 6
2023-02-23 01:44:54,910 DEBUG TRAIN Batch 12/5200 loss 11.384894 loss_att 16.075224 loss_ctc 12.641428 loss_rnnt 10.158569 hw_loss 0.226353 lr 0.00048730 rank 0
2023-02-23 01:44:54,911 DEBUG TRAIN Batch 12/5200 loss 24.929544 loss_att 30.325294 loss_ctc 30.116575 loss_rnnt 22.998854 hw_loss 0.299881 lr 0.00048727 rank 1
2023-02-23 01:44:54,912 DEBUG TRAIN Batch 12/5200 loss 19.003899 loss_att 19.286144 loss_ctc 22.673244 loss_rnnt 18.278824 hw_loss 0.336335 lr 0.00048722 rank 2
2023-02-23 01:44:54,914 DEBUG TRAIN Batch 12/5200 loss 13.524119 loss_att 17.104221 loss_ctc 13.390558 loss_rnnt 12.688845 hw_loss 0.256989 lr 0.00048726 rank 4
2023-02-23 01:44:54,915 DEBUG TRAIN Batch 12/5200 loss 9.450004 loss_att 12.535569 loss_ctc 11.716650 loss_rnnt 8.425432 hw_loss 0.197323 lr 0.00048724 rank 5
2023-02-23 01:44:54,916 DEBUG TRAIN Batch 12/5200 loss 13.278241 loss_att 19.934990 loss_ctc 16.750607 loss_rnnt 11.356981 hw_loss 0.237988 lr 0.00048722 rank 3
2023-02-23 01:44:54,917 DEBUG TRAIN Batch 12/5200 loss 13.901320 loss_att 19.316261 loss_ctc 16.739050 loss_rnnt 12.273041 hw_loss 0.312988 lr 0.00048720 rank 7
2023-02-23 01:46:12,392 DEBUG TRAIN Batch 12/5300 loss 9.585029 loss_att 11.987494 loss_ctc 11.289456 loss_rnnt 8.752709 hw_loss 0.233566 lr 0.00048698 rank 2
2023-02-23 01:46:12,393 DEBUG TRAIN Batch 12/5300 loss 13.828212 loss_att 15.882087 loss_ctc 19.680378 loss_rnnt 12.417484 hw_loss 0.411871 lr 0.00048698 rank 3
2023-02-23 01:46:12,393 DEBUG TRAIN Batch 12/5300 loss 9.875133 loss_att 10.531337 loss_ctc 8.227472 loss_rnnt 9.727066 hw_loss 0.443463 lr 0.00048700 rank 5
2023-02-23 01:46:12,397 DEBUG TRAIN Batch 12/5300 loss 15.029444 loss_att 17.563591 loss_ctc 18.772736 loss_rnnt 13.890745 hw_loss 0.248933 lr 0.00048703 rank 4
2023-02-23 01:46:12,401 DEBUG TRAIN Batch 12/5300 loss 8.854464 loss_att 12.586357 loss_ctc 8.919436 loss_rnnt 7.984145 hw_loss 0.216147 lr 0.00048707 rank 0
2023-02-23 01:46:12,404 DEBUG TRAIN Batch 12/5300 loss 11.435206 loss_att 15.849859 loss_ctc 12.758430 loss_rnnt 10.288160 hw_loss 0.164412 lr 0.00048697 rank 7
2023-02-23 01:46:12,404 DEBUG TRAIN Batch 12/5300 loss 41.278816 loss_att 42.959839 loss_ctc 61.127041 loss_rnnt 38.133244 hw_loss 0.305501 lr 0.00048704 rank 1
2023-02-23 01:46:12,447 DEBUG TRAIN Batch 12/5300 loss 23.473795 loss_att 31.159306 loss_ctc 27.242626 loss_rnnt 21.358719 hw_loss 0.141493 lr 0.00048698 rank 6
2023-02-23 01:47:30,103 DEBUG TRAIN Batch 12/5400 loss 15.082426 loss_att 16.424358 loss_ctc 14.401077 loss_rnnt 14.790904 hw_loss 0.213717 lr 0.00048684 rank 0
2023-02-23 01:47:30,103 DEBUG TRAIN Batch 12/5400 loss 12.567161 loss_att 16.868668 loss_ctc 15.064770 loss_rnnt 11.234962 hw_loss 0.260402 lr 0.00048679 rank 4
2023-02-23 01:47:30,104 DEBUG TRAIN Batch 12/5400 loss 5.283281 loss_att 8.463718 loss_ctc 8.865593 loss_rnnt 3.990892 hw_loss 0.334987 lr 0.00048675 rank 2
2023-02-23 01:47:30,106 DEBUG TRAIN Batch 12/5400 loss 23.085463 loss_att 25.532431 loss_ctc 28.348053 loss_rnnt 21.722408 hw_loss 0.322470 lr 0.00048675 rank 3
2023-02-23 01:47:30,109 DEBUG TRAIN Batch 12/5400 loss 15.653469 loss_att 19.534901 loss_ctc 21.534641 loss_rnnt 13.994129 hw_loss 0.185432 lr 0.00048677 rank 5
2023-02-23 01:47:30,111 DEBUG TRAIN Batch 12/5400 loss 32.602886 loss_att 37.853764 loss_ctc 40.711430 loss_rnnt 30.333656 hw_loss 0.258593 lr 0.00048681 rank 1
2023-02-23 01:47:30,112 DEBUG TRAIN Batch 12/5400 loss 10.854290 loss_att 15.469530 loss_ctc 15.666820 loss_rnnt 9.102852 hw_loss 0.350100 lr 0.00048675 rank 6
2023-02-23 01:47:30,113 DEBUG TRAIN Batch 12/5400 loss 9.562508 loss_att 12.100033 loss_ctc 11.555614 loss_rnnt 8.624248 hw_loss 0.309388 lr 0.00048674 rank 7
2023-02-23 01:48:45,145 DEBUG TRAIN Batch 12/5500 loss 12.871659 loss_att 16.245220 loss_ctc 17.412136 loss_rnnt 11.482447 hw_loss 0.204568 lr 0.00048654 rank 5
2023-02-23 01:48:45,146 DEBUG TRAIN Batch 12/5500 loss 6.762717 loss_att 12.089947 loss_ctc 7.478698 loss_rnnt 5.444566 hw_loss 0.294825 lr 0.00048652 rank 3
2023-02-23 01:48:45,148 DEBUG TRAIN Batch 12/5500 loss 5.238670 loss_att 8.812901 loss_ctc 8.557760 loss_rnnt 3.984348 hw_loss 0.181745 lr 0.00048652 rank 2
2023-02-23 01:48:45,150 DEBUG TRAIN Batch 12/5500 loss 15.631736 loss_att 16.280525 loss_ctc 17.408081 loss_rnnt 15.109632 hw_loss 0.291563 lr 0.00048656 rank 4
2023-02-23 01:48:45,150 DEBUG TRAIN Batch 12/5500 loss 9.308658 loss_att 10.684079 loss_ctc 5.455453 loss_rnnt 9.405132 hw_loss 0.266628 lr 0.00048658 rank 1
2023-02-23 01:48:45,152 DEBUG TRAIN Batch 12/5500 loss 14.616305 loss_att 16.657763 loss_ctc 16.359074 loss_rnnt 13.821322 hw_loss 0.289357 lr 0.00048651 rank 7
2023-02-23 01:48:45,152 DEBUG TRAIN Batch 12/5500 loss 11.863020 loss_att 17.484480 loss_ctc 16.839310 loss_rnnt 9.908992 hw_loss 0.311682 lr 0.00048661 rank 0
2023-02-23 01:48:45,189 DEBUG TRAIN Batch 12/5500 loss 18.916569 loss_att 24.163624 loss_ctc 25.807293 loss_rnnt 16.844139 hw_loss 0.195481 lr 0.00048652 rank 6
2023-02-23 01:50:01,727 DEBUG TRAIN Batch 12/5600 loss 11.766839 loss_att 12.102524 loss_ctc 14.595701 loss_rnnt 11.164734 hw_loss 0.295850 lr 0.00048638 rank 0
2023-02-23 01:50:01,727 DEBUG TRAIN Batch 12/5600 loss 12.318769 loss_att 15.014173 loss_ctc 19.739677 loss_rnnt 10.641340 hw_loss 0.279177 lr 0.00048633 rank 4
2023-02-23 01:50:01,729 DEBUG TRAIN Batch 12/5600 loss 25.098940 loss_att 25.364777 loss_ctc 31.830078 loss_rnnt 24.034077 hw_loss 0.214144 lr 0.00048629 rank 3
2023-02-23 01:50:01,730 DEBUG TRAIN Batch 12/5600 loss 16.625164 loss_att 19.175758 loss_ctc 20.294113 loss_rnnt 15.431934 hw_loss 0.363597 lr 0.00048629 rank 2
2023-02-23 01:50:01,731 DEBUG TRAIN Batch 12/5600 loss 12.361659 loss_att 15.881527 loss_ctc 16.857433 loss_rnnt 10.919844 hw_loss 0.259511 lr 0.00048631 rank 5
2023-02-23 01:50:01,732 DEBUG TRAIN Batch 12/5600 loss 13.028095 loss_att 15.420596 loss_ctc 10.707239 loss_rnnt 12.742970 hw_loss 0.217638 lr 0.00048629 rank 6
2023-02-23 01:50:01,736 DEBUG TRAIN Batch 12/5600 loss 19.489792 loss_att 22.895723 loss_ctc 24.172260 loss_rnnt 18.027554 hw_loss 0.293857 lr 0.00048628 rank 7
2023-02-23 01:50:01,737 DEBUG TRAIN Batch 12/5600 loss 6.303728 loss_att 10.684526 loss_ctc 8.180110 loss_rnnt 5.076602 hw_loss 0.188966 lr 0.00048635 rank 1
2023-02-23 01:51:21,366 DEBUG TRAIN Batch 12/5700 loss 14.037327 loss_att 16.623077 loss_ctc 19.668926 loss_rnnt 12.589773 hw_loss 0.336610 lr 0.00048606 rank 6
2023-02-23 01:51:21,367 DEBUG TRAIN Batch 12/5700 loss 34.590694 loss_att 38.738613 loss_ctc 40.706738 loss_rnnt 32.763294 hw_loss 0.341891 lr 0.00048606 rank 3
2023-02-23 01:51:21,371 DEBUG TRAIN Batch 12/5700 loss 14.204690 loss_att 16.307524 loss_ctc 15.067175 loss_rnnt 13.503401 hw_loss 0.310733 lr 0.00048615 rank 0
2023-02-23 01:51:21,372 DEBUG TRAIN Batch 12/5700 loss 15.620644 loss_att 15.392805 loss_ctc 17.609711 loss_rnnt 15.154447 hw_loss 0.462291 lr 0.00048606 rank 2
2023-02-23 01:51:21,372 DEBUG TRAIN Batch 12/5700 loss 9.091661 loss_att 9.202239 loss_ctc 11.118239 loss_rnnt 8.642659 hw_loss 0.293767 lr 0.00048605 rank 7
2023-02-23 01:51:21,375 DEBUG TRAIN Batch 12/5700 loss 15.171658 loss_att 16.224781 loss_ctc 19.423340 loss_rnnt 14.271827 hw_loss 0.229339 lr 0.00048612 rank 1
2023-02-23 01:51:21,390 DEBUG TRAIN Batch 12/5700 loss 12.849127 loss_att 14.932720 loss_ctc 15.248934 loss_rnnt 11.952189 hw_loss 0.300457 lr 0.00048608 rank 5
2023-02-23 01:51:21,405 DEBUG TRAIN Batch 12/5700 loss 13.431293 loss_att 20.579382 loss_ctc 13.942947 loss_rnnt 11.746206 hw_loss 0.351091 lr 0.00048610 rank 4
2023-02-23 01:52:36,109 DEBUG TRAIN Batch 12/5800 loss 17.135654 loss_att 21.207489 loss_ctc 15.533003 loss_rnnt 16.310019 hw_loss 0.421794 lr 0.00048583 rank 2
2023-02-23 01:52:36,115 DEBUG TRAIN Batch 12/5800 loss 18.133133 loss_att 22.247723 loss_ctc 23.252144 loss_rnnt 16.477448 hw_loss 0.281686 lr 0.00048582 rank 7
2023-02-23 01:52:36,116 DEBUG TRAIN Batch 12/5800 loss 20.001562 loss_att 22.160103 loss_ctc 24.589062 loss_rnnt 18.810547 hw_loss 0.276827 lr 0.00048585 rank 5
2023-02-23 01:52:36,116 DEBUG TRAIN Batch 12/5800 loss 6.178470 loss_att 10.567500 loss_ctc 5.539685 loss_rnnt 5.252168 hw_loss 0.250626 lr 0.00048587 rank 4
2023-02-23 01:52:36,118 DEBUG TRAIN Batch 12/5800 loss 18.129606 loss_att 27.137283 loss_ctc 17.715044 loss_rnnt 16.258684 hw_loss 0.233739 lr 0.00048583 rank 6
2023-02-23 01:52:36,118 DEBUG TRAIN Batch 12/5800 loss 12.121136 loss_att 19.760338 loss_ctc 14.035167 loss_rnnt 10.173419 hw_loss 0.308762 lr 0.00048583 rank 3
2023-02-23 01:52:36,118 DEBUG TRAIN Batch 12/5800 loss 12.999684 loss_att 18.031361 loss_ctc 14.756458 loss_rnnt 11.636770 hw_loss 0.229392 lr 0.00048589 rank 1
2023-02-23 01:52:36,119 DEBUG TRAIN Batch 12/5800 loss 10.320064 loss_att 12.772245 loss_ctc 12.058346 loss_rnnt 9.454453 hw_loss 0.268882 lr 0.00048592 rank 0
2023-02-23 01:53:52,157 DEBUG TRAIN Batch 12/5900 loss 22.091547 loss_att 27.493181 loss_ctc 25.839674 loss_rnnt 20.326214 hw_loss 0.347353 lr 0.00048560 rank 2
2023-02-23 01:53:52,158 DEBUG TRAIN Batch 12/5900 loss 16.658983 loss_att 16.743746 loss_ctc 19.088913 loss_rnnt 16.177399 hw_loss 0.263700 lr 0.00048562 rank 5
2023-02-23 01:53:52,161 DEBUG TRAIN Batch 12/5900 loss 12.288057 loss_att 20.202333 loss_ctc 16.874640 loss_rnnt 9.934571 hw_loss 0.298285 lr 0.00048569 rank 0
2023-02-23 01:53:52,163 DEBUG TRAIN Batch 12/5900 loss 5.170032 loss_att 10.242050 loss_ctc 5.131886 loss_rnnt 4.012569 hw_loss 0.277771 lr 0.00048560 rank 3
2023-02-23 01:53:52,166 DEBUG TRAIN Batch 12/5900 loss 16.258755 loss_att 17.982632 loss_ctc 19.210800 loss_rnnt 15.393603 hw_loss 0.237693 lr 0.00048566 rank 1
2023-02-23 01:53:52,166 DEBUG TRAIN Batch 12/5900 loss 9.998341 loss_att 16.470985 loss_ctc 14.701483 loss_rnnt 7.941337 hw_loss 0.253853 lr 0.00048565 rank 4
2023-02-23 01:53:52,168 DEBUG TRAIN Batch 12/5900 loss 20.650549 loss_att 22.051483 loss_ctc 25.290577 loss_rnnt 19.584377 hw_loss 0.313715 lr 0.00048559 rank 7
2023-02-23 01:53:52,213 DEBUG TRAIN Batch 12/5900 loss 10.363310 loss_att 16.004940 loss_ctc 14.989698 loss_rnnt 8.502202 hw_loss 0.217365 lr 0.00048560 rank 6
2023-02-23 01:55:09,624 DEBUG TRAIN Batch 12/6000 loss 14.337158 loss_att 21.644999 loss_ctc 17.283039 loss_rnnt 12.338243 hw_loss 0.271054 lr 0.00048542 rank 4
2023-02-23 01:55:09,630 DEBUG TRAIN Batch 12/6000 loss 27.638985 loss_att 31.399281 loss_ctc 33.751217 loss_rnnt 25.931665 hw_loss 0.263051 lr 0.00048540 rank 5
2023-02-23 01:55:09,633 DEBUG TRAIN Batch 12/6000 loss 15.060798 loss_att 24.499088 loss_ctc 21.089823 loss_rnnt 12.182647 hw_loss 0.349920 lr 0.00048537 rank 6
2023-02-23 01:55:09,633 DEBUG TRAIN Batch 12/6000 loss 25.734215 loss_att 31.879896 loss_ctc 34.782112 loss_rnnt 23.190609 hw_loss 0.202659 lr 0.00048538 rank 2
2023-02-23 01:55:09,636 DEBUG TRAIN Batch 12/6000 loss 9.862902 loss_att 17.577988 loss_ctc 10.859310 loss_rnnt 8.052952 hw_loss 0.251399 lr 0.00048536 rank 7
2023-02-23 01:55:09,637 DEBUG TRAIN Batch 12/6000 loss 20.361443 loss_att 23.261187 loss_ctc 24.194210 loss_rnnt 19.094427 hw_loss 0.330062 lr 0.00048546 rank 0
2023-02-23 01:55:09,640 DEBUG TRAIN Batch 12/6000 loss 23.933504 loss_att 27.473717 loss_ctc 29.709274 loss_rnnt 22.297146 hw_loss 0.296648 lr 0.00048543 rank 1
2023-02-23 01:55:09,660 DEBUG TRAIN Batch 12/6000 loss 3.807801 loss_att 7.804297 loss_ctc 5.571924 loss_rnnt 2.618969 hw_loss 0.289342 lr 0.00048538 rank 3
2023-02-23 01:56:26,228 DEBUG TRAIN Batch 12/6100 loss 4.386565 loss_att 7.510183 loss_ctc 4.913101 loss_rnnt 3.543667 hw_loss 0.277441 lr 0.00048515 rank 2
2023-02-23 01:56:26,229 DEBUG TRAIN Batch 12/6100 loss 33.676785 loss_att 34.825455 loss_ctc 38.478535 loss_rnnt 32.594715 hw_loss 0.397695 lr 0.00048513 rank 7
2023-02-23 01:56:26,229 DEBUG TRAIN Batch 12/6100 loss 17.455154 loss_att 23.892532 loss_ctc 23.654713 loss_rnnt 15.216030 hw_loss 0.234450 lr 0.00048515 rank 3
2023-02-23 01:56:26,230 DEBUG TRAIN Batch 12/6100 loss 13.560833 loss_att 22.186306 loss_ctc 17.086027 loss_rnnt 11.150642 hw_loss 0.403257 lr 0.00048514 rank 6
2023-02-23 01:56:26,231 DEBUG TRAIN Batch 12/6100 loss 23.670807 loss_att 24.290775 loss_ctc 25.072456 loss_rnnt 23.201559 hw_loss 0.296940 lr 0.00048519 rank 4
2023-02-23 01:56:26,231 DEBUG TRAIN Batch 12/6100 loss 21.088287 loss_att 27.934284 loss_ctc 30.343910 loss_rnnt 18.372057 hw_loss 0.211772 lr 0.00048520 rank 1
2023-02-23 01:56:26,239 DEBUG TRAIN Batch 12/6100 loss 13.421532 loss_att 16.476900 loss_ctc 17.684673 loss_rnnt 12.083202 hw_loss 0.297818 lr 0.00048517 rank 5
2023-02-23 01:56:26,245 DEBUG TRAIN Batch 12/6100 loss 43.182186 loss_att 45.681328 loss_ctc 48.420166 loss_rnnt 41.790344 hw_loss 0.363027 lr 0.00048523 rank 0
2023-02-23 01:57:42,567 DEBUG TRAIN Batch 12/6200 loss 12.816000 loss_att 12.854105 loss_ctc 14.817382 loss_rnnt 12.387263 hw_loss 0.289246 lr 0.00048492 rank 2
2023-02-23 01:57:42,567 DEBUG TRAIN Batch 12/6200 loss 8.180401 loss_att 11.918896 loss_ctc 12.293360 loss_rnnt 6.791621 hw_loss 0.173788 lr 0.00048497 rank 1
2023-02-23 01:57:42,568 DEBUG TRAIN Batch 12/6200 loss 15.185015 loss_att 17.234650 loss_ctc 17.172363 loss_rnnt 14.313749 hw_loss 0.368172 lr 0.00048490 rank 7
2023-02-23 01:57:42,569 DEBUG TRAIN Batch 12/6200 loss 12.989242 loss_att 19.025427 loss_ctc 16.531315 loss_rnnt 11.219930 hw_loss 0.168371 lr 0.00048494 rank 5
2023-02-23 01:57:42,569 DEBUG TRAIN Batch 12/6200 loss 11.466948 loss_att 13.341381 loss_ctc 16.057491 loss_rnnt 10.356063 hw_loss 0.232361 lr 0.00048496 rank 4
2023-02-23 01:57:42,572 DEBUG TRAIN Batch 12/6200 loss 15.238060 loss_att 19.965611 loss_ctc 18.031574 loss_rnnt 13.785532 hw_loss 0.252279 lr 0.00048500 rank 0
2023-02-23 01:57:42,574 DEBUG TRAIN Batch 12/6200 loss 8.204268 loss_att 10.886189 loss_ctc 9.665474 loss_rnnt 7.345103 hw_loss 0.239915 lr 0.00048492 rank 6
2023-02-23 01:57:42,616 DEBUG TRAIN Batch 12/6200 loss 3.856997 loss_att 7.023804 loss_ctc 4.864995 loss_rnnt 3.003200 hw_loss 0.161318 lr 0.00048492 rank 3
2023-02-23 01:59:00,070 DEBUG TRAIN Batch 12/6300 loss 13.349006 loss_att 17.272272 loss_ctc 18.469337 loss_rnnt 11.804241 hw_loss 0.145128 lr 0.00048473 rank 4
2023-02-23 01:59:00,071 DEBUG TRAIN Batch 12/6300 loss 18.255180 loss_att 20.129658 loss_ctc 21.054157 loss_rnnt 17.301329 hw_loss 0.385798 lr 0.00048475 rank 1
2023-02-23 01:59:00,071 DEBUG TRAIN Batch 12/6300 loss 9.762214 loss_att 9.675581 loss_ctc 10.242952 loss_rnnt 9.535610 hw_loss 0.337185 lr 0.00048469 rank 3
2023-02-23 01:59:00,074 DEBUG TRAIN Batch 12/6300 loss 22.127220 loss_att 24.209347 loss_ctc 29.542669 loss_rnnt 20.554045 hw_loss 0.315040 lr 0.00048471 rank 5
2023-02-23 01:59:00,074 DEBUG TRAIN Batch 12/6300 loss 11.668886 loss_att 10.982382 loss_ctc 12.069302 loss_rnnt 11.536338 hw_loss 0.405864 lr 0.00048469 rank 2
2023-02-23 01:59:00,078 DEBUG TRAIN Batch 12/6300 loss 19.829634 loss_att 24.130842 loss_ctc 26.768011 loss_rnnt 17.865179 hw_loss 0.335802 lr 0.00048469 rank 6
2023-02-23 01:59:00,077 DEBUG TRAIN Batch 12/6300 loss 21.888342 loss_att 23.898399 loss_ctc 28.196507 loss_rnnt 20.477369 hw_loss 0.314761 lr 0.00048477 rank 0
2023-02-23 01:59:00,079 DEBUG TRAIN Batch 12/6300 loss 11.547163 loss_att 12.766156 loss_ctc 14.854612 loss_rnnt 10.672615 hw_loss 0.355793 lr 0.00048467 rank 7
2023-02-23 02:00:20,335 DEBUG TRAIN Batch 12/6400 loss 20.638391 loss_att 28.691294 loss_ctc 30.356525 loss_rnnt 17.534122 hw_loss 0.371130 lr 0.00048450 rank 4
2023-02-23 02:00:20,338 DEBUG TRAIN Batch 12/6400 loss 7.354284 loss_att 8.080456 loss_ctc 7.638692 loss_rnnt 7.007464 hw_loss 0.306871 lr 0.00048448 rank 5
2023-02-23 02:00:20,345 DEBUG TRAIN Batch 12/6400 loss 7.597750 loss_att 9.047768 loss_ctc 9.073413 loss_rnnt 6.954970 hw_loss 0.292539 lr 0.00048446 rank 6
2023-02-23 02:00:20,347 DEBUG TRAIN Batch 12/6400 loss 13.683298 loss_att 16.536470 loss_ctc 17.545671 loss_rnnt 12.441958 hw_loss 0.291979 lr 0.00048445 rank 7
2023-02-23 02:00:20,352 DEBUG TRAIN Batch 12/6400 loss 29.653135 loss_att 32.093548 loss_ctc 38.452381 loss_rnnt 27.911505 hw_loss 0.150590 lr 0.00048446 rank 2
2023-02-23 02:00:20,355 DEBUG TRAIN Batch 12/6400 loss 27.173403 loss_att 33.431168 loss_ctc 36.381416 loss_rnnt 24.546545 hw_loss 0.276694 lr 0.00048452 rank 1
2023-02-23 02:00:20,362 DEBUG TRAIN Batch 12/6400 loss 15.508883 loss_att 17.218370 loss_ctc 15.159597 loss_rnnt 15.057799 hw_loss 0.292046 lr 0.00048455 rank 0
2023-02-23 02:00:20,396 DEBUG TRAIN Batch 12/6400 loss 10.172169 loss_att 16.233383 loss_ctc 12.804995 loss_rnnt 8.465449 hw_loss 0.268936 lr 0.00048446 rank 3
2023-02-23 02:01:36,372 DEBUG TRAIN Batch 12/6500 loss 33.075176 loss_att 37.469444 loss_ctc 42.371529 loss_rnnt 30.774105 hw_loss 0.342574 lr 0.00048424 rank 2
2023-02-23 02:01:36,376 DEBUG TRAIN Batch 12/6500 loss 16.975985 loss_att 20.726440 loss_ctc 20.058670 loss_rnnt 15.664022 hw_loss 0.282834 lr 0.00048432 rank 0
2023-02-23 02:01:36,376 DEBUG TRAIN Batch 12/6500 loss 24.967785 loss_att 34.906860 loss_ctc 29.986319 loss_rnnt 22.208685 hw_loss 0.191525 lr 0.00048426 rank 5
2023-02-23 02:01:36,379 DEBUG TRAIN Batch 12/6500 loss 16.230791 loss_att 20.474155 loss_ctc 17.597958 loss_rnnt 15.065712 hw_loss 0.251467 lr 0.00048424 rank 3
2023-02-23 02:01:36,380 DEBUG TRAIN Batch 12/6500 loss 16.450188 loss_att 18.721565 loss_ctc 22.463402 loss_rnnt 15.038555 hw_loss 0.291740 lr 0.00048429 rank 1
2023-02-23 02:01:36,381 DEBUG TRAIN Batch 12/6500 loss 18.739553 loss_att 22.087942 loss_ctc 20.498901 loss_rnnt 17.679518 hw_loss 0.292088 lr 0.00048428 rank 4
2023-02-23 02:01:36,383 DEBUG TRAIN Batch 12/6500 loss 32.941372 loss_att 40.282211 loss_ctc 49.540398 loss_rnnt 29.080938 hw_loss 0.335737 lr 0.00048423 rank 6
2023-02-23 02:01:36,428 DEBUG TRAIN Batch 12/6500 loss 17.292847 loss_att 22.173029 loss_ctc 20.242119 loss_rnnt 15.782738 hw_loss 0.264069 lr 0.00048422 rank 7
2023-02-23 02:02:51,175 DEBUG TRAIN Batch 12/6600 loss 8.786991 loss_att 11.997947 loss_ctc 9.169924 loss_rnnt 7.894337 hw_loss 0.373885 lr 0.00048409 rank 0
2023-02-23 02:02:51,176 DEBUG TRAIN Batch 12/6600 loss 14.276088 loss_att 16.661110 loss_ctc 20.952934 loss_rnnt 12.773757 hw_loss 0.253276 lr 0.00048406 rank 1
2023-02-23 02:02:51,176 DEBUG TRAIN Batch 12/6600 loss 17.596485 loss_att 20.347734 loss_ctc 18.881790 loss_rnnt 16.697056 hw_loss 0.333386 lr 0.00048399 rank 7
2023-02-23 02:02:51,179 DEBUG TRAIN Batch 12/6600 loss 10.984808 loss_att 18.620680 loss_ctc 16.370045 loss_rnnt 8.601141 hw_loss 0.259614 lr 0.00048401 rank 3
2023-02-23 02:02:51,180 DEBUG TRAIN Batch 12/6600 loss 10.485702 loss_att 14.980837 loss_ctc 14.599558 loss_rnnt 8.909850 hw_loss 0.240582 lr 0.00048401 rank 6
2023-02-23 02:02:51,180 DEBUG TRAIN Batch 12/6600 loss 12.929904 loss_att 15.931718 loss_ctc 16.619419 loss_rnnt 11.738534 hw_loss 0.185758 lr 0.00048405 rank 4
2023-02-23 02:02:51,180 DEBUG TRAIN Batch 12/6600 loss 13.087031 loss_att 14.334871 loss_ctc 17.267403 loss_rnnt 12.125441 hw_loss 0.289952 lr 0.00048403 rank 5
2023-02-23 02:02:51,181 DEBUG TRAIN Batch 12/6600 loss 18.015408 loss_att 21.530457 loss_ctc 26.178925 loss_rnnt 16.043516 hw_loss 0.338272 lr 0.00048401 rank 2
2023-02-23 02:04:08,139 DEBUG TRAIN Batch 12/6700 loss 12.778545 loss_att 17.948544 loss_ctc 13.735219 loss_rnnt 11.499866 hw_loss 0.219606 lr 0.00048380 rank 5
2023-02-23 02:04:08,144 DEBUG TRAIN Batch 12/6700 loss 13.630747 loss_att 18.193104 loss_ctc 16.906887 loss_rnnt 12.184656 hw_loss 0.181498 lr 0.00048382 rank 4
2023-02-23 02:04:08,149 DEBUG TRAIN Batch 12/6700 loss 13.949755 loss_att 14.611024 loss_ctc 17.975231 loss_rnnt 13.132775 hw_loss 0.277491 lr 0.00048378 rank 3
2023-02-23 02:04:08,151 DEBUG TRAIN Batch 12/6700 loss 6.620793 loss_att 12.625670 loss_ctc 11.188300 loss_rnnt 4.675993 hw_loss 0.252794 lr 0.00048384 rank 1
2023-02-23 02:04:08,151 DEBUG TRAIN Batch 12/6700 loss 20.090052 loss_att 26.272543 loss_ctc 29.481731 loss_rnnt 17.444086 hw_loss 0.294828 lr 0.00048387 rank 0
2023-02-23 02:04:08,154 DEBUG TRAIN Batch 12/6700 loss 12.745023 loss_att 14.341223 loss_ctc 12.372149 loss_rnnt 12.331241 hw_loss 0.270483 lr 0.00048378 rank 6
2023-02-23 02:04:08,173 DEBUG TRAIN Batch 12/6700 loss 21.457418 loss_att 26.363705 loss_ctc 24.667482 loss_rnnt 19.850485 hw_loss 0.370634 lr 0.00048378 rank 2
2023-02-23 02:04:08,204 DEBUG TRAIN Batch 12/6700 loss 13.469445 loss_att 17.539942 loss_ctc 14.987756 loss_rnnt 12.282673 hw_loss 0.319184 lr 0.00048377 rank 7
2023-02-23 02:05:26,081 DEBUG TRAIN Batch 12/6800 loss 8.791993 loss_att 13.102620 loss_ctc 9.679864 loss_rnnt 7.626167 hw_loss 0.347468 lr 0.00048356 rank 2
2023-02-23 02:05:26,082 DEBUG TRAIN Batch 12/6800 loss 13.516616 loss_att 18.116379 loss_ctc 17.057133 loss_rnnt 11.976673 hw_loss 0.277353 lr 0.00048356 rank 3
2023-02-23 02:05:26,087 DEBUG TRAIN Batch 12/6800 loss 11.730567 loss_att 16.736212 loss_ctc 15.708330 loss_rnnt 10.091887 hw_loss 0.200970 lr 0.00048361 rank 1
2023-02-23 02:05:26,088 DEBUG TRAIN Batch 12/6800 loss 15.646381 loss_att 17.501842 loss_ctc 16.618727 loss_rnnt 14.994224 hw_loss 0.283911 lr 0.00048360 rank 4
2023-02-23 02:05:26,089 DEBUG TRAIN Batch 12/6800 loss 6.341620 loss_att 8.884579 loss_ctc 6.100387 loss_rnnt 5.675000 hw_loss 0.356612 lr 0.00048358 rank 5
2023-02-23 02:05:26,089 DEBUG TRAIN Batch 12/6800 loss 6.804013 loss_att 11.249346 loss_ctc 8.761417 loss_rnnt 5.438578 hw_loss 0.403840 lr 0.00048364 rank 0
2023-02-23 02:05:26,092 DEBUG TRAIN Batch 12/6800 loss 13.515622 loss_att 14.836632 loss_ctc 13.758680 loss_rnnt 13.074339 hw_loss 0.271265 lr 0.00048354 rank 7
2023-02-23 02:05:26,135 DEBUG TRAIN Batch 12/6800 loss 20.768908 loss_att 24.929161 loss_ctc 24.334627 loss_rnnt 19.337652 hw_loss 0.232083 lr 0.00048355 rank 6
2023-02-23 02:06:43,090 DEBUG TRAIN Batch 12/6900 loss 18.574530 loss_att 20.522604 loss_ctc 22.659367 loss_rnnt 17.436001 hw_loss 0.383004 lr 0.00048333 rank 3
2023-02-23 02:06:43,094 DEBUG TRAIN Batch 12/6900 loss 16.344412 loss_att 17.744907 loss_ctc 19.168709 loss_rnnt 15.519125 hw_loss 0.316152 lr 0.00048333 rank 2
2023-02-23 02:06:43,096 DEBUG TRAIN Batch 12/6900 loss 22.483391 loss_att 24.719810 loss_ctc 27.577646 loss_rnnt 21.210005 hw_loss 0.275376 lr 0.00048333 rank 6
2023-02-23 02:06:43,097 DEBUG TRAIN Batch 12/6900 loss 15.740294 loss_att 15.906418 loss_ctc 19.087826 loss_rnnt 15.137502 hw_loss 0.231053 lr 0.00048331 rank 7
2023-02-23 02:06:43,097 DEBUG TRAIN Batch 12/6900 loss 7.849283 loss_att 11.943748 loss_ctc 12.241341 loss_rnnt 6.313157 hw_loss 0.246797 lr 0.00048335 rank 5
2023-02-23 02:06:43,097 DEBUG TRAIN Batch 12/6900 loss 10.850308 loss_att 15.180111 loss_ctc 13.126239 loss_rnnt 9.571815 hw_loss 0.204518 lr 0.00048341 rank 0
2023-02-23 02:06:43,098 DEBUG TRAIN Batch 12/6900 loss 15.155139 loss_att 18.014942 loss_ctc 22.757610 loss_rnnt 13.375293 hw_loss 0.364169 lr 0.00048337 rank 4
2023-02-23 02:06:43,140 DEBUG TRAIN Batch 12/6900 loss 12.112096 loss_att 16.665304 loss_ctc 16.085133 loss_rnnt 10.566840 hw_loss 0.196643 lr 0.00048338 rank 1
2023-02-23 02:07:59,130 DEBUG TRAIN Batch 12/7000 loss 24.410906 loss_att 31.998360 loss_ctc 28.855225 loss_rnnt 22.200424 hw_loss 0.188279 lr 0.00048314 rank 4
2023-02-23 02:07:59,131 DEBUG TRAIN Batch 12/7000 loss 13.584521 loss_att 16.862526 loss_ctc 15.978561 loss_rnnt 12.468989 hw_loss 0.263861 lr 0.00048310 rank 2
2023-02-23 02:07:59,131 DEBUG TRAIN Batch 12/7000 loss 13.837867 loss_att 15.889151 loss_ctc 17.639687 loss_rnnt 12.797874 hw_loss 0.230300 lr 0.00048312 rank 5
2023-02-23 02:07:59,130 DEBUG TRAIN Batch 12/7000 loss 17.957548 loss_att 19.019541 loss_ctc 21.659319 loss_rnnt 17.069994 hw_loss 0.340471 lr 0.00048319 rank 0
2023-02-23 02:07:59,135 DEBUG TRAIN Batch 12/7000 loss 16.354832 loss_att 20.536880 loss_ctc 17.547857 loss_rnnt 15.154432 hw_loss 0.384227 lr 0.00048309 rank 7
2023-02-23 02:07:59,137 DEBUG TRAIN Batch 12/7000 loss 15.739697 loss_att 14.023459 loss_ctc 18.198570 loss_rnnt 15.561696 hw_loss 0.362621 lr 0.00048316 rank 1
2023-02-23 02:07:59,138 DEBUG TRAIN Batch 12/7000 loss 5.777381 loss_att 8.430703 loss_ctc 6.662645 loss_rnnt 5.054259 hw_loss 0.139541 lr 0.00048310 rank 3
2023-02-23 02:07:59,140 DEBUG TRAIN Batch 12/7000 loss 11.974644 loss_att 15.085791 loss_ctc 14.830504 loss_rnnt 10.796867 hw_loss 0.327686 lr 0.00048310 rank 6
2023-02-23 02:09:18,920 DEBUG TRAIN Batch 12/7100 loss 12.046618 loss_att 15.551147 loss_ctc 14.889933 loss_rnnt 10.828082 hw_loss 0.259727 lr 0.00048290 rank 5
2023-02-23 02:09:18,927 DEBUG TRAIN Batch 12/7100 loss 16.840322 loss_att 23.779709 loss_ctc 20.620296 loss_rnnt 14.815738 hw_loss 0.248833 lr 0.00048292 rank 4
2023-02-23 02:09:18,928 DEBUG TRAIN Batch 12/7100 loss 16.519524 loss_att 20.143011 loss_ctc 21.768913 loss_rnnt 14.957541 hw_loss 0.257562 lr 0.00048296 rank 0
2023-02-23 02:09:18,928 DEBUG TRAIN Batch 12/7100 loss 9.678825 loss_att 16.214281 loss_ctc 9.030293 loss_rnnt 8.321840 hw_loss 0.255686 lr 0.00048286 rank 7
2023-02-23 02:09:18,930 DEBUG TRAIN Batch 12/7100 loss 25.760365 loss_att 32.661072 loss_ctc 31.525784 loss_rnnt 23.490685 hw_loss 0.226531 lr 0.00048288 rank 2
2023-02-23 02:09:18,930 DEBUG TRAIN Batch 12/7100 loss 12.855458 loss_att 13.645121 loss_ctc 15.688785 loss_rnnt 12.162115 hw_loss 0.295565 lr 0.00048288 rank 6
2023-02-23 02:09:18,932 DEBUG TRAIN Batch 12/7100 loss 13.278891 loss_att 17.453873 loss_ctc 13.257578 loss_rnnt 12.298460 hw_loss 0.278016 lr 0.00048293 rank 1
2023-02-23 02:09:18,932 DEBUG TRAIN Batch 12/7100 loss 5.859732 loss_att 8.503263 loss_ctc 6.964000 loss_rnnt 5.019054 hw_loss 0.308880 lr 0.00048288 rank 3
2023-02-23 02:10:34,796 DEBUG TRAIN Batch 12/7200 loss 10.839730 loss_att 15.387215 loss_ctc 11.149631 loss_rnnt 9.823315 hw_loss 0.122998 lr 0.00048274 rank 0
2023-02-23 02:10:34,798 DEBUG TRAIN Batch 12/7200 loss 4.741426 loss_att 8.755875 loss_ctc 5.926947 loss_rnnt 3.596372 hw_loss 0.345180 lr 0.00048267 rank 5
2023-02-23 02:10:34,800 DEBUG TRAIN Batch 12/7200 loss 16.965691 loss_att 22.072386 loss_ctc 22.897795 loss_rnnt 15.002628 hw_loss 0.282705 lr 0.00048269 rank 4
2023-02-23 02:10:34,801 DEBUG TRAIN Batch 12/7200 loss 22.018250 loss_att 24.228584 loss_ctc 29.716839 loss_rnnt 20.400160 hw_loss 0.280401 lr 0.00048265 rank 2
2023-02-23 02:10:34,801 DEBUG TRAIN Batch 12/7200 loss 13.283422 loss_att 20.001564 loss_ctc 16.352058 loss_rnnt 11.355938 hw_loss 0.327569 lr 0.00048264 rank 7
2023-02-23 02:10:34,803 DEBUG TRAIN Batch 12/7200 loss 13.257344 loss_att 15.176218 loss_ctc 16.270893 loss_rnnt 12.312677 hw_loss 0.298283 lr 0.00048265 rank 6
2023-02-23 02:10:34,806 DEBUG TRAIN Batch 12/7200 loss 13.098208 loss_att 17.064486 loss_ctc 16.464901 loss_rnnt 11.747892 hw_loss 0.202816 lr 0.00048265 rank 3
2023-02-23 02:10:34,853 DEBUG TRAIN Batch 12/7200 loss 15.458935 loss_att 21.193356 loss_ctc 26.494446 loss_rnnt 12.729170 hw_loss 0.209025 lr 0.00048271 rank 1
2023-02-23 02:11:49,927 DEBUG TRAIN Batch 12/7300 loss 35.927814 loss_att 38.439835 loss_ctc 44.521008 loss_rnnt 34.178352 hw_loss 0.189929 lr 0.00048245 rank 5
2023-02-23 02:11:49,929 DEBUG TRAIN Batch 12/7300 loss 17.639938 loss_att 20.223843 loss_ctc 21.323999 loss_rnnt 16.551750 hw_loss 0.150371 lr 0.00048247 rank 4
2023-02-23 02:11:49,930 DEBUG TRAIN Batch 12/7300 loss 23.137264 loss_att 27.016823 loss_ctc 26.907810 loss_rnnt 21.738882 hw_loss 0.224497 lr 0.00048251 rank 0
2023-02-23 02:11:49,931 DEBUG TRAIN Batch 12/7300 loss 16.661278 loss_att 21.397989 loss_ctc 19.536133 loss_rnnt 15.229696 hw_loss 0.189237 lr 0.00048248 rank 1
2023-02-23 02:11:49,933 DEBUG TRAIN Batch 12/7300 loss 12.128343 loss_att 15.657345 loss_ctc 20.051619 loss_rnnt 10.174927 hw_loss 0.358459 lr 0.00048241 rank 7
2023-02-23 02:11:49,933 DEBUG TRAIN Batch 12/7300 loss 12.109599 loss_att 19.562656 loss_ctc 13.695486 loss_rnnt 10.298744 hw_loss 0.203983 lr 0.00048243 rank 6
2023-02-23 02:11:49,936 DEBUG TRAIN Batch 12/7300 loss 16.557444 loss_att 21.060358 loss_ctc 24.294464 loss_rnnt 14.459602 hw_loss 0.310607 lr 0.00048243 rank 2
2023-02-23 02:11:49,936 DEBUG TRAIN Batch 12/7300 loss 5.907889 loss_att 9.721971 loss_ctc 6.605947 loss_rnnt 4.860681 hw_loss 0.358723 lr 0.00048243 rank 3
2023-02-23 02:13:05,535 DEBUG TRAIN Batch 12/7400 loss 9.044623 loss_att 15.555560 loss_ctc 12.283596 loss_rnnt 7.185692 hw_loss 0.234150 lr 0.00048229 rank 0
2023-02-23 02:13:05,540 DEBUG TRAIN Batch 12/7400 loss 9.869141 loss_att 10.536500 loss_ctc 10.097203 loss_rnnt 9.550508 hw_loss 0.290160 lr 0.00048220 rank 2
2023-02-23 02:13:05,539 DEBUG TRAIN Batch 12/7400 loss 17.567642 loss_att 21.251358 loss_ctc 19.702248 loss_rnnt 16.337608 hw_loss 0.391270 lr 0.00048222 rank 5
2023-02-23 02:13:05,539 DEBUG TRAIN Batch 12/7400 loss 24.497341 loss_att 28.665842 loss_ctc 25.994864 loss_rnnt 23.336136 hw_loss 0.239686 lr 0.00048219 rank 7
2023-02-23 02:13:05,544 DEBUG TRAIN Batch 12/7400 loss 18.143703 loss_att 21.621641 loss_ctc 20.729042 loss_rnnt 16.958168 hw_loss 0.272319 lr 0.00048225 rank 4
2023-02-23 02:13:05,545 DEBUG TRAIN Batch 12/7400 loss 15.849482 loss_att 18.623592 loss_ctc 18.077394 loss_rnnt 14.828011 hw_loss 0.317988 lr 0.00048220 rank 6
2023-02-23 02:13:05,547 DEBUG TRAIN Batch 12/7400 loss 18.391264 loss_att 21.401796 loss_ctc 20.542999 loss_rnnt 17.384357 hw_loss 0.221069 lr 0.00048220 rank 3
2023-02-23 02:13:05,550 DEBUG TRAIN Batch 12/7400 loss 15.233008 loss_att 18.053614 loss_ctc 21.572414 loss_rnnt 13.666153 hw_loss 0.295275 lr 0.00048226 rank 1
2023-02-23 02:14:23,583 DEBUG TRAIN Batch 12/7500 loss 17.817556 loss_att 21.021278 loss_ctc 20.407143 loss_rnnt 16.684036 hw_loss 0.276557 lr 0.00048200 rank 5
2023-02-23 02:14:23,587 DEBUG TRAIN Batch 12/7500 loss 11.001929 loss_att 15.721622 loss_ctc 15.794448 loss_rnnt 9.242042 hw_loss 0.331777 lr 0.00048198 rank 2
2023-02-23 02:14:23,587 DEBUG TRAIN Batch 12/7500 loss 15.551913 loss_att 19.361206 loss_ctc 21.235365 loss_rnnt 13.896155 hw_loss 0.255202 lr 0.00048198 rank 3
2023-02-23 02:14:23,589 DEBUG TRAIN Batch 12/7500 loss 19.237667 loss_att 21.768925 loss_ctc 21.457029 loss_rnnt 18.291033 hw_loss 0.270873 lr 0.00048202 rank 4
2023-02-23 02:14:23,589 DEBUG TRAIN Batch 12/7500 loss 23.633646 loss_att 25.258873 loss_ctc 27.798328 loss_rnnt 22.614166 hw_loss 0.260890 lr 0.00048198 rank 6
2023-02-23 02:14:23,589 DEBUG TRAIN Batch 12/7500 loss 18.179087 loss_att 18.581268 loss_ctc 19.060493 loss_rnnt 17.824810 hw_loss 0.293097 lr 0.00048206 rank 0
2023-02-23 02:14:23,593 DEBUG TRAIN Batch 12/7500 loss 10.622994 loss_att 14.077661 loss_ctc 15.033794 loss_rnnt 9.189439 hw_loss 0.289715 lr 0.00048203 rank 1
2023-02-23 02:14:23,603 DEBUG TRAIN Batch 12/7500 loss 15.168818 loss_att 20.312042 loss_ctc 19.262220 loss_rnnt 13.410724 hw_loss 0.344367 lr 0.00048196 rank 7
2023-02-23 02:15:38,589 DEBUG TRAIN Batch 12/7600 loss 11.797480 loss_att 12.963530 loss_ctc 15.979750 loss_rnnt 10.788940 hw_loss 0.408175 lr 0.00048176 rank 2
2023-02-23 02:15:38,591 DEBUG TRAIN Batch 12/7600 loss 8.694470 loss_att 10.414421 loss_ctc 10.848617 loss_rnnt 7.819931 hw_loss 0.456246 lr 0.00048176 rank 3
2023-02-23 02:15:38,590 DEBUG TRAIN Batch 12/7600 loss 17.540562 loss_att 19.276484 loss_ctc 21.550919 loss_rnnt 16.502810 hw_loss 0.292223 lr 0.00048181 rank 1
2023-02-23 02:15:38,591 DEBUG TRAIN Batch 12/7600 loss 9.213946 loss_att 10.392887 loss_ctc 12.395202 loss_rnnt 8.415220 hw_loss 0.260196 lr 0.00048184 rank 0
2023-02-23 02:15:38,595 DEBUG TRAIN Batch 12/7600 loss 14.289410 loss_att 16.938587 loss_ctc 17.740850 loss_rnnt 13.129910 hw_loss 0.317760 lr 0.00048175 rank 6
2023-02-23 02:15:38,595 DEBUG TRAIN Batch 12/7600 loss 20.704531 loss_att 26.558197 loss_ctc 23.148855 loss_rnnt 19.069721 hw_loss 0.259061 lr 0.00048180 rank 4
2023-02-23 02:15:38,594 DEBUG TRAIN Batch 12/7600 loss 26.737499 loss_att 32.005520 loss_ctc 35.699657 loss_rnnt 24.344406 hw_loss 0.271007 lr 0.00048178 rank 5
2023-02-23 02:15:38,598 DEBUG TRAIN Batch 12/7600 loss 11.410085 loss_att 12.382492 loss_ctc 13.193633 loss_rnnt 10.801522 hw_loss 0.330512 lr 0.00048174 rank 7
2023-02-23 02:16:53,764 DEBUG TRAIN Batch 12/7700 loss 8.788052 loss_att 8.782582 loss_ctc 9.475372 loss_rnnt 8.559947 hw_loss 0.257917 lr 0.00048155 rank 5
2023-02-23 02:16:53,767 DEBUG TRAIN Batch 12/7700 loss 7.304714 loss_att 10.618818 loss_ctc 9.197140 loss_rnnt 6.227977 hw_loss 0.302987 lr 0.00048162 rank 0
2023-02-23 02:16:53,769 DEBUG TRAIN Batch 12/7700 loss 20.110140 loss_att 27.892559 loss_ctc 24.607082 loss_rnnt 17.802244 hw_loss 0.284663 lr 0.00048153 rank 3
2023-02-23 02:16:53,770 DEBUG TRAIN Batch 12/7700 loss 18.803436 loss_att 22.253136 loss_ctc 17.306929 loss_rnnt 18.164375 hw_loss 0.278731 lr 0.00048153 rank 2
2023-02-23 02:16:53,769 DEBUG TRAIN Batch 12/7700 loss 14.716297 loss_att 20.521948 loss_ctc 23.033697 loss_rnnt 12.374886 hw_loss 0.133677 lr 0.00048157 rank 4
2023-02-23 02:16:53,773 DEBUG TRAIN Batch 12/7700 loss 10.789140 loss_att 19.175787 loss_ctc 15.259251 loss_rnnt 8.367560 hw_loss 0.277942 lr 0.00048153 rank 6
2023-02-23 02:16:53,773 DEBUG TRAIN Batch 12/7700 loss 25.142000 loss_att 31.226490 loss_ctc 25.883495 loss_rnnt 23.709415 hw_loss 0.219037 lr 0.00048152 rank 7
2023-02-23 02:16:53,815 DEBUG TRAIN Batch 12/7700 loss 7.179330 loss_att 12.296968 loss_ctc 10.981174 loss_rnnt 5.527687 hw_loss 0.227255 lr 0.00048159 rank 1
2023-02-23 02:18:12,347 DEBUG TRAIN Batch 12/7800 loss 14.383663 loss_att 17.904034 loss_ctc 14.955989 loss_rnnt 13.470009 hw_loss 0.249881 lr 0.00048135 rank 4
2023-02-23 02:18:12,347 DEBUG TRAIN Batch 12/7800 loss 9.553381 loss_att 15.510478 loss_ctc 14.187477 loss_rnnt 7.640653 hw_loss 0.193929 lr 0.00048139 rank 0
2023-02-23 02:18:12,349 DEBUG TRAIN Batch 12/7800 loss 17.975657 loss_att 25.516670 loss_ctc 22.798971 loss_rnnt 15.724029 hw_loss 0.188093 lr 0.00048131 rank 3
2023-02-23 02:18:12,349 DEBUG TRAIN Batch 12/7800 loss 12.739108 loss_att 17.847712 loss_ctc 18.236336 loss_rnnt 10.825149 hw_loss 0.298641 lr 0.00048133 rank 5
2023-02-23 02:18:12,352 DEBUG TRAIN Batch 12/7800 loss 6.975494 loss_att 12.346168 loss_ctc 9.029815 loss_rnnt 5.484926 hw_loss 0.267231 lr 0.00048136 rank 1
2023-02-23 02:18:12,353 DEBUG TRAIN Batch 12/7800 loss 30.658619 loss_att 37.711174 loss_ctc 32.798168 loss_rnnt 28.855064 hw_loss 0.202065 lr 0.00048131 rank 2
2023-02-23 02:18:12,356 DEBUG TRAIN Batch 12/7800 loss 15.301604 loss_att 20.156944 loss_ctc 19.677341 loss_rnnt 13.537140 hw_loss 0.393684 lr 0.00048131 rank 6
2023-02-23 02:18:12,357 DEBUG TRAIN Batch 12/7800 loss 24.108587 loss_att 27.758724 loss_ctc 31.164501 loss_rnnt 22.275818 hw_loss 0.303664 lr 0.00048129 rank 7
2023-02-23 02:19:28,260 DEBUG TRAIN Batch 12/7900 loss 5.038662 loss_att 10.442327 loss_ctc 5.966561 loss_rnnt 3.656717 hw_loss 0.332798 lr 0.00048109 rank 3
2023-02-23 02:19:28,260 DEBUG TRAIN Batch 12/7900 loss 9.373999 loss_att 11.050814 loss_ctc 9.370303 loss_rnnt 8.909580 hw_loss 0.242903 lr 0.00048117 rank 0
2023-02-23 02:19:28,260 DEBUG TRAIN Batch 12/7900 loss 15.318112 loss_att 15.724876 loss_ctc 14.460043 loss_rnnt 15.211514 hw_loss 0.261852 lr 0.00048109 rank 2
2023-02-23 02:19:28,260 DEBUG TRAIN Batch 12/7900 loss 18.856524 loss_att 22.570807 loss_ctc 24.597715 loss_rnnt 17.230949 hw_loss 0.219797 lr 0.00048107 rank 7
2023-02-23 02:19:28,260 DEBUG TRAIN Batch 12/7900 loss 21.213152 loss_att 26.269192 loss_ctc 22.335197 loss_rnnt 19.900244 hw_loss 0.285179 lr 0.00048111 rank 5
2023-02-23 02:19:28,261 DEBUG TRAIN Batch 12/7900 loss 18.391066 loss_att 23.390900 loss_ctc 24.448936 loss_rnnt 16.461994 hw_loss 0.227605 lr 0.00048113 rank 4
2023-02-23 02:19:28,265 DEBUG TRAIN Batch 12/7900 loss 7.392248 loss_att 10.585812 loss_ctc 9.530649 loss_rnnt 6.305273 hw_loss 0.305890 lr 0.00048109 rank 6
2023-02-23 02:19:28,264 DEBUG TRAIN Batch 12/7900 loss 30.298542 loss_att 40.825245 loss_ctc 42.419674 loss_rnnt 26.413267 hw_loss 0.307092 lr 0.00048114 rank 1
2023-02-23 02:20:44,559 DEBUG TRAIN Batch 12/8000 loss 8.830005 loss_att 15.295880 loss_ctc 8.542115 loss_rnnt 7.399771 hw_loss 0.328959 lr 0.00048090 rank 4
2023-02-23 02:20:44,561 DEBUG TRAIN Batch 12/8000 loss 10.358527 loss_att 15.351084 loss_ctc 13.929869 loss_rnnt 8.712213 hw_loss 0.321795 lr 0.00048086 rank 2
2023-02-23 02:20:44,560 DEBUG TRAIN Batch 12/8000 loss 18.001554 loss_att 21.483358 loss_ctc 18.338688 loss_rnnt 17.084776 hw_loss 0.328998 lr 0.00048095 rank 0
2023-02-23 02:20:44,561 DEBUG TRAIN Batch 12/8000 loss 19.945242 loss_att 22.952404 loss_ctc 21.879860 loss_rnnt 18.962296 hw_loss 0.231680 lr 0.00048088 rank 5
2023-02-23 02:20:44,565 DEBUG TRAIN Batch 12/8000 loss 17.050556 loss_att 18.502964 loss_ctc 21.546312 loss_rnnt 16.047754 hw_loss 0.211657 lr 0.00048086 rank 3
2023-02-23 02:20:44,566 DEBUG TRAIN Batch 12/8000 loss 20.218700 loss_att 23.320419 loss_ctc 31.286491 loss_rnnt 17.952942 hw_loss 0.318201 lr 0.00048092 rank 1
2023-02-23 02:20:44,570 DEBUG TRAIN Batch 12/8000 loss 14.675091 loss_att 18.923981 loss_ctc 16.539288 loss_rnnt 13.442152 hw_loss 0.252378 lr 0.00048085 rank 7
2023-02-23 02:20:44,614 DEBUG TRAIN Batch 12/8000 loss 23.244802 loss_att 26.784859 loss_ctc 24.541824 loss_rnnt 22.226971 hw_loss 0.256660 lr 0.00048086 rank 6
2023-02-23 02:22:00,626 DEBUG TRAIN Batch 12/8100 loss 24.475178 loss_att 28.562578 loss_ctc 28.539280 loss_rnnt 22.978706 hw_loss 0.257083 lr 0.00048066 rank 5
2023-02-23 02:22:00,631 DEBUG TRAIN Batch 12/8100 loss 13.337627 loss_att 15.328449 loss_ctc 17.547346 loss_rnnt 12.242816 hw_loss 0.253781 lr 0.00048068 rank 4
2023-02-23 02:22:00,632 DEBUG TRAIN Batch 12/8100 loss 10.401071 loss_att 14.331007 loss_ctc 15.280825 loss_rnnt 8.837229 hw_loss 0.238541 lr 0.00048064 rank 6
2023-02-23 02:22:00,632 DEBUG TRAIN Batch 12/8100 loss 12.265006 loss_att 18.689747 loss_ctc 16.153217 loss_rnnt 10.309357 hw_loss 0.285511 lr 0.00048072 rank 0
2023-02-23 02:22:00,635 DEBUG TRAIN Batch 12/8100 loss 12.996025 loss_att 16.047192 loss_ctc 19.283703 loss_rnnt 11.423923 hw_loss 0.231586 lr 0.00048064 rank 2
2023-02-23 02:22:00,636 DEBUG TRAIN Batch 12/8100 loss 14.730444 loss_att 18.744566 loss_ctc 18.821871 loss_rnnt 13.269709 hw_loss 0.210724 lr 0.00048063 rank 7
2023-02-23 02:22:00,641 DEBUG TRAIN Batch 12/8100 loss 8.615124 loss_att 12.809577 loss_ctc 9.868099 loss_rnnt 7.490829 hw_loss 0.221888 lr 0.00048064 rank 3
2023-02-23 02:22:00,642 DEBUG TRAIN Batch 12/8100 loss 17.840858 loss_att 19.131468 loss_ctc 20.520899 loss_rnnt 17.028734 hw_loss 0.368744 lr 0.00048070 rank 1
2023-02-23 02:23:17,876 DEBUG TRAIN Batch 12/8200 loss 12.706882 loss_att 14.332277 loss_ctc 13.640079 loss_rnnt 12.105954 hw_loss 0.283919 lr 0.00048042 rank 2
2023-02-23 02:23:17,877 DEBUG TRAIN Batch 12/8200 loss 25.272970 loss_att 24.872353 loss_ctc 30.978626 loss_rnnt 24.466013 hw_loss 0.236862 lr 0.00048050 rank 0
2023-02-23 02:23:17,877 DEBUG TRAIN Batch 12/8200 loss 9.038717 loss_att 12.446699 loss_ctc 12.636337 loss_rnnt 7.712891 hw_loss 0.308527 lr 0.00048042 rank 6
2023-02-23 02:23:17,879 DEBUG TRAIN Batch 12/8200 loss 16.723829 loss_att 22.376045 loss_ctc 23.924246 loss_rnnt 14.540104 hw_loss 0.174801 lr 0.00048044 rank 5
2023-02-23 02:23:17,883 DEBUG TRAIN Batch 12/8200 loss 7.214678 loss_att 8.864526 loss_ctc 13.685522 loss_rnnt 5.901811 hw_loss 0.225222 lr 0.00048041 rank 7
2023-02-23 02:23:17,884 DEBUG TRAIN Batch 12/8200 loss 8.795327 loss_att 9.288697 loss_ctc 11.592690 loss_rnnt 8.175891 hw_loss 0.277089 lr 0.00048046 rank 4
2023-02-23 02:23:17,885 DEBUG TRAIN Batch 12/8200 loss 10.094213 loss_att 11.914133 loss_ctc 13.245757 loss_rnnt 9.179893 hw_loss 0.243995 lr 0.00048042 rank 3
2023-02-23 02:23:17,886 DEBUG TRAIN Batch 12/8200 loss 17.228607 loss_att 16.949944 loss_ctc 21.659172 loss_rnnt 16.530592 hw_loss 0.305637 lr 0.00048047 rank 1
2023-02-23 02:24:32,874 DEBUG TRAIN Batch 12/8300 loss 19.309912 loss_att 26.389286 loss_ctc 24.127104 loss_rnnt 17.092176 hw_loss 0.299192 lr 0.00048020 rank 2
2023-02-23 02:24:32,881 DEBUG TRAIN Batch 12/8300 loss 10.242314 loss_att 11.486503 loss_ctc 11.905292 loss_rnnt 9.570754 hw_loss 0.376862 lr 0.00048028 rank 0
2023-02-23 02:24:32,883 DEBUG TRAIN Batch 12/8300 loss 27.374390 loss_att 33.442863 loss_ctc 43.486450 loss_rnnt 23.924191 hw_loss 0.165435 lr 0.00048018 rank 7
2023-02-23 02:24:32,883 DEBUG TRAIN Batch 12/8300 loss 15.520045 loss_att 21.119249 loss_ctc 20.001554 loss_rnnt 13.632708 hw_loss 0.318677 lr 0.00048022 rank 5
2023-02-23 02:24:32,883 DEBUG TRAIN Batch 12/8300 loss 29.975466 loss_att 33.819263 loss_ctc 43.271820 loss_rnnt 27.355370 hw_loss 0.147165 lr 0.00048024 rank 4
2023-02-23 02:24:32,884 DEBUG TRAIN Batch 12/8300 loss 6.240404 loss_att 11.878399 loss_ctc 6.426765 loss_rnnt 4.969064 hw_loss 0.222925 lr 0.00048020 rank 3
2023-02-23 02:24:32,884 DEBUG TRAIN Batch 12/8300 loss 17.598545 loss_att 24.331640 loss_ctc 21.490620 loss_rnnt 15.549872 hw_loss 0.343330 lr 0.00048025 rank 1
2023-02-23 02:24:32,930 DEBUG TRAIN Batch 12/8300 loss 12.963919 loss_att 18.543079 loss_ctc 20.916759 loss_rnnt 10.680507 hw_loss 0.201003 lr 0.00048020 rank 6
2023-02-23 02:25:20,014 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 4
2023-02-23 02:25:20,019 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 7
2023-02-23 02:25:20,020 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 3
2023-02-23 02:25:20,022 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 5
2023-02-23 02:25:20,025 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 6
2023-02-23 02:25:20,047 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 2
2023-02-23 02:25:20,048 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 0
2023-02-23 02:25:20,050 DEBUG CV Batch 12/0 loss 3.019027 loss_att 2.999797 loss_ctc 3.537432 loss_rnnt 2.742389 hw_loss 0.396306 history loss 2.907211 rank 1
2023-02-23 02:25:31,067 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 2
2023-02-23 02:25:31,131 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 3
2023-02-23 02:25:31,215 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 5
2023-02-23 02:25:31,235 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 4
2023-02-23 02:25:31,346 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 0
2023-02-23 02:25:31,409 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 1
2023-02-23 02:25:31,546 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 6
2023-02-23 02:25:31,705 DEBUG CV Batch 12/100 loss 9.208975 loss_att 10.225531 loss_ctc 12.210989 loss_rnnt 8.443348 hw_loss 0.303839 history loss 4.713729 rank 7
2023-02-23 02:25:45,175 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 0
2023-02-23 02:25:45,301 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 4
2023-02-23 02:25:45,316 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 2
2023-02-23 02:25:45,366 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 1
2023-02-23 02:25:45,413 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 5
2023-02-23 02:25:45,454 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 3
2023-02-23 02:25:45,861 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 6
2023-02-23 02:25:45,923 DEBUG CV Batch 12/200 loss 7.685596 loss_att 18.430662 loss_ctc 7.379117 loss_rnnt 5.466919 hw_loss 0.207238 history loss 5.331727 rank 7
2023-02-23 02:25:57,281 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 0
2023-02-23 02:25:57,290 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 2
2023-02-23 02:25:57,313 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 4
2023-02-23 02:25:57,366 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 5
2023-02-23 02:25:57,489 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 3
2023-02-23 02:25:57,578 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 1
2023-02-23 02:25:58,305 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 7
2023-02-23 02:25:58,525 DEBUG CV Batch 12/300 loss 6.752774 loss_att 7.109005 loss_ctc 9.312174 loss_rnnt 6.115908 hw_loss 0.420687 history loss 5.574477 rank 6
2023-02-23 02:26:09,142 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 2
2023-02-23 02:26:09,241 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 4
2023-02-23 02:26:09,250 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 5
2023-02-23 02:26:09,310 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 0
2023-02-23 02:26:09,542 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 3
2023-02-23 02:26:10,387 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 1
2023-02-23 02:26:10,582 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 7
2023-02-23 02:26:11,048 DEBUG CV Batch 12/400 loss 26.003036 loss_att 135.616669 loss_ctc 10.638269 loss_rnnt 6.029869 hw_loss 0.185766 history loss 6.642460 rank 6
2023-02-23 02:26:19,501 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 2
2023-02-23 02:26:19,596 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 5
2023-02-23 02:26:19,711 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 4
2023-02-23 02:26:19,736 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 0
2023-02-23 02:26:20,146 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 3
2023-02-23 02:26:20,953 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 1
2023-02-23 02:26:21,454 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 7
2023-02-23 02:26:21,620 DEBUG CV Batch 12/500 loss 6.299170 loss_att 7.728208 loss_ctc 9.232688 loss_rnnt 5.465107 hw_loss 0.294599 history loss 7.680170 rank 6
2023-02-23 02:26:31,478 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 2
2023-02-23 02:26:31,592 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 5
2023-02-23 02:26:31,761 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 0
2023-02-23 02:26:31,811 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 4
2023-02-23 02:26:32,638 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 3
2023-02-23 02:26:33,240 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 1
2023-02-23 02:26:33,819 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 7
2023-02-23 02:26:33,888 DEBUG CV Batch 12/600 loss 11.608094 loss_att 11.035817 loss_ctc 14.678152 loss_rnnt 11.084383 hw_loss 0.429045 history loss 8.826555 rank 6
2023-02-23 02:26:43,290 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 0
2023-02-23 02:26:43,567 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 4
2023-02-23 02:26:43,643 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 2
2023-02-23 02:26:43,831 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 5
2023-02-23 02:26:44,730 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 1
2023-02-23 02:26:45,036 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 3
2023-02-23 02:26:45,396 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 6
2023-02-23 02:26:45,709 DEBUG CV Batch 12/700 loss 31.155350 loss_att 79.509285 loss_ctc 25.855499 loss_rnnt 22.074379 hw_loss 0.219055 history loss 9.650865 rank 7
2023-02-23 02:26:55,152 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 0
2023-02-23 02:26:55,749 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 2
2023-02-23 02:26:55,880 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 5
2023-02-23 02:26:55,881 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 4
2023-02-23 02:26:57,305 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 6
2023-02-23 02:26:57,387 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 1
2023-02-23 02:26:57,460 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 3
2023-02-23 02:26:58,545 DEBUG CV Batch 12/800 loss 15.897588 loss_att 14.215635 loss_ctc 19.475300 loss_rnnt 15.606239 hw_loss 0.282584 history loss 8.995247 rank 7
2023-02-23 02:27:09,074 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 0
2023-02-23 02:27:09,652 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 2
2023-02-23 02:27:09,773 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 5
2023-02-23 02:27:10,004 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 4
2023-02-23 02:27:11,304 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 6
2023-02-23 02:27:11,337 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 3
2023-02-23 02:27:11,341 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 1
2023-02-23 02:27:13,472 DEBUG CV Batch 12/900 loss 17.077713 loss_att 24.375462 loss_ctc 26.344385 loss_rnnt 14.291099 hw_loss 0.171580 history loss 8.730581 rank 7
2023-02-23 02:27:21,282 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 0
2023-02-23 02:27:21,781 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 2
2023-02-23 02:27:21,857 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 5
2023-02-23 02:27:22,236 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 4
2023-02-23 02:27:23,618 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 3
2023-02-23 02:27:23,666 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 6
2023-02-23 02:27:24,255 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 1
2023-02-23 02:27:25,928 DEBUG CV Batch 12/1000 loss 5.671241 loss_att 5.588969 loss_ctc 5.305601 loss_rnnt 5.553353 hw_loss 0.343300 history loss 8.450926 rank 7
2023-02-23 02:27:33,264 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 0
2023-02-23 02:27:33,544 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 2
2023-02-23 02:27:33,708 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 5
2023-02-23 02:27:34,144 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 4
2023-02-23 02:27:35,546 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 3
2023-02-23 02:27:35,710 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 6
2023-02-23 02:27:36,579 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 1
2023-02-23 02:27:38,065 DEBUG CV Batch 12/1100 loss 8.350470 loss_att 6.873324 loss_ctc 10.323240 loss_rnnt 8.195669 hw_loss 0.350985 history loss 8.406387 rank 7
2023-02-23 02:27:43,737 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 0
2023-02-23 02:27:43,922 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 2
2023-02-23 02:27:44,009 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 5
2023-02-23 02:27:44,731 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 4
2023-02-23 02:27:46,104 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 3
2023-02-23 02:27:46,351 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 6
2023-02-23 02:27:47,205 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 1
2023-02-23 02:27:48,874 DEBUG CV Batch 12/1200 loss 10.894470 loss_att 11.837404 loss_ctc 12.356400 loss_rnnt 10.392306 hw_loss 0.222476 history loss 8.831881 rank 7
2023-02-23 02:27:55,685 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 0
2023-02-23 02:27:55,803 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 2
2023-02-23 02:27:55,920 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 5
2023-02-23 02:27:56,727 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 4
2023-02-23 02:27:58,160 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 3
2023-02-23 02:27:58,445 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 6
2023-02-23 02:27:59,247 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 1
2023-02-23 02:28:01,294 DEBUG CV Batch 12/1300 loss 8.290308 loss_att 7.563841 loss_ctc 9.855988 loss_rnnt 8.058426 hw_loss 0.315785 history loss 9.204959 rank 7
2023-02-23 02:28:06,829 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 0
2023-02-23 02:28:07,725 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 5
2023-02-23 02:28:07,756 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 2
2023-02-23 02:28:08,655 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 4
2023-02-23 02:28:09,594 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 3
2023-02-23 02:28:09,758 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 6
2023-02-23 02:28:10,521 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 1
2023-02-23 02:28:12,983 DEBUG CV Batch 12/1400 loss 13.107467 loss_att 49.730541 loss_ctc 9.150311 loss_rnnt 6.196668 hw_loss 0.213382 history loss 9.605569 rank 7
2023-02-23 02:28:18,834 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 0
2023-02-23 02:28:20,067 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 5
2023-02-23 02:28:20,158 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 2
2023-02-23 02:28:20,939 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 4
2023-02-23 02:28:21,933 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 6
2023-02-23 02:28:22,640 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 3
2023-02-23 02:28:22,712 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 1
2023-02-23 02:28:24,641 DEBUG CV Batch 12/1500 loss 7.811830 loss_att 8.606473 loss_ctc 7.228394 loss_rnnt 7.575176 hw_loss 0.291595 history loss 9.391254 rank 7
2023-02-23 02:28:32,435 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 0
2023-02-23 02:28:33,842 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 5
2023-02-23 02:28:34,076 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 2
2023-02-23 02:28:34,556 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 4
2023-02-23 02:28:35,756 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 6
2023-02-23 02:28:36,550 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 1
2023-02-23 02:28:37,390 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 3
2023-02-23 02:28:37,985 DEBUG CV Batch 12/1600 loss 11.594534 loss_att 18.064686 loss_ctc 15.373026 loss_rnnt 9.636932 hw_loss 0.299573 history loss 9.284567 rank 7
2023-02-23 02:28:45,013 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 0
2023-02-23 02:28:46,244 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 5
2023-02-23 02:28:46,585 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 2
2023-02-23 02:28:47,083 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 4
2023-02-23 02:28:48,471 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 6
2023-02-23 02:28:49,186 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 1
2023-02-23 02:28:50,620 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 3
2023-02-23 02:28:50,801 DEBUG CV Batch 12/1700 loss 14.247410 loss_att 11.955255 loss_ctc 19.402206 loss_rnnt 13.876348 hw_loss 0.266599 history loss 9.164931 rank 7
2023-02-23 02:28:54,380 INFO Epoch 12 CV info cv_loss 9.10331929600735
2023-02-23 02:28:54,381 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/12.pt
2023-02-23 02:28:54,998 INFO Epoch 13 TRAIN info lr 0.00048020349573474536
2023-02-23 02:28:55,002 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:55,453 INFO Epoch 12 CV info cv_loss 9.103319297545061
2023-02-23 02:28:55,454 INFO Epoch 13 TRAIN info lr 0.00048014813895038046
2023-02-23 02:28:55,457 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:55,741 INFO Epoch 12 CV info cv_loss 9.103319296317476
2023-02-23 02:28:55,742 INFO Epoch 13 TRAIN info lr 0.00048011493406757423
2023-02-23 02:28:55,744 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:56,251 INFO Epoch 12 CV info cv_loss 9.103319296963575
2023-02-23 02:28:56,252 INFO Epoch 13 TRAIN info lr 0.00048012157449302343
2023-02-23 02:28:56,257 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:57,778 INFO Epoch 12 CV info cv_loss 9.103319297562292
2023-02-23 02:28:57,779 INFO Epoch 13 TRAIN info lr 0.00048010608059555546
2023-02-23 02:28:57,783 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:58,424 INFO Epoch 12 CV info cv_loss 9.103319298044712
2023-02-23 02:28:58,425 INFO Epoch 13 TRAIN info lr 0.00048018356508677356
2023-02-23 02:28:58,428 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:28:59,637 INFO Epoch 12 CV info cv_loss 9.103319296214101
2023-02-23 02:28:59,638 INFO Epoch 13 TRAIN info lr 0.0004800972276133011
2023-02-23 02:28:59,642 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:29:00,031 INFO Epoch 12 CV info cv_loss 9.10331929763767
2023-02-23 02:29:00,033 INFO Epoch 13 TRAIN info lr 0.0004800773101936961
2023-02-23 02:29:00,036 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:30:12,705 DEBUG TRAIN Batch 13/0 loss 11.684168 loss_att 11.020643 loss_ctc 14.002007 loss_rnnt 11.339723 hw_loss 0.315196 lr 0.00048020 rank 0
2023-02-23 02:30:12,706 DEBUG TRAIN Batch 13/0 loss 9.117366 loss_att 8.651875 loss_ctc 11.463208 loss_rnnt 8.695526 hw_loss 0.379048 lr 0.00048012 rank 4
2023-02-23 02:30:12,707 DEBUG TRAIN Batch 13/0 loss 11.718540 loss_att 10.977201 loss_ctc 14.389908 loss_rnnt 11.343276 hw_loss 0.313780 lr 0.00048015 rank 5
2023-02-23 02:30:12,709 DEBUG TRAIN Batch 13/0 loss 15.183072 loss_att 12.732126 loss_ctc 16.002239 loss_rnnt 15.456816 hw_loss 0.201046 lr 0.00048010 rank 6
2023-02-23 02:30:12,711 DEBUG TRAIN Batch 13/0 loss 13.700752 loss_att 12.481133 loss_ctc 15.732378 loss_rnnt 13.465600 hw_loss 0.390363 lr 0.00048018 rank 1
2023-02-23 02:30:12,712 DEBUG TRAIN Batch 13/0 loss 7.576482 loss_att 7.944746 loss_ctc 8.549844 loss_rnnt 7.189226 hw_loss 0.344665 lr 0.00048011 rank 2
2023-02-23 02:30:12,743 DEBUG TRAIN Batch 13/0 loss 15.616180 loss_att 13.456461 loss_ctc 18.058365 loss_rnnt 15.508464 hw_loss 0.401316 lr 0.00048008 rank 7
2023-02-23 02:30:12,761 DEBUG TRAIN Batch 13/0 loss 9.278467 loss_att 9.517533 loss_ctc 10.731033 loss_rnnt 8.775485 hw_loss 0.490301 lr 0.00048010 rank 3
2023-02-23 02:31:29,016 DEBUG TRAIN Batch 13/100 loss 2.878046 loss_att 6.969701 loss_ctc 2.457322 loss_rnnt 2.015527 hw_loss 0.188033 lr 0.00047989 rank 2
2023-02-23 02:31:29,019 DEBUG TRAIN Batch 13/100 loss 4.148807 loss_att 8.317509 loss_ctc 4.454434 loss_rnnt 3.197720 hw_loss 0.143616 lr 0.00047992 rank 5
2023-02-23 02:31:29,022 DEBUG TRAIN Batch 13/100 loss 19.511431 loss_att 26.126574 loss_ctc 23.360958 loss_rnnt 17.501823 hw_loss 0.324954 lr 0.00047998 rank 0
2023-02-23 02:31:29,026 DEBUG TRAIN Batch 13/100 loss 13.095242 loss_att 22.428200 loss_ctc 12.844490 loss_rnnt 11.126204 hw_loss 0.254777 lr 0.00047985 rank 7
2023-02-23 02:31:29,027 DEBUG TRAIN Batch 13/100 loss 10.989627 loss_att 18.120922 loss_ctc 13.937491 loss_rnnt 9.046152 hw_loss 0.232814 lr 0.00047987 rank 3
2023-02-23 02:31:29,030 DEBUG TRAIN Batch 13/100 loss 15.750482 loss_att 19.028055 loss_ctc 11.808815 loss_rnnt 15.517721 hw_loss 0.192755 lr 0.00047990 rank 4
2023-02-23 02:31:29,033 DEBUG TRAIN Batch 13/100 loss 17.415699 loss_att 20.920034 loss_ctc 24.524527 loss_rnnt 15.639113 hw_loss 0.239765 lr 0.00047988 rank 6
2023-02-23 02:31:29,069 DEBUG TRAIN Batch 13/100 loss 6.279212 loss_att 9.676225 loss_ctc 5.760318 loss_rnnt 5.509998 hw_loss 0.298120 lr 0.00047996 rank 1
2023-02-23 02:32:43,918 DEBUG TRAIN Batch 13/200 loss 8.015983 loss_att 12.701536 loss_ctc 8.764023 loss_rnnt 6.847383 hw_loss 0.247031 lr 0.00047976 rank 0
2023-02-23 02:32:43,921 DEBUG TRAIN Batch 13/200 loss 6.142483 loss_att 8.605597 loss_ctc 7.310194 loss_rnnt 5.348831 hw_loss 0.272501 lr 0.00047963 rank 7
2023-02-23 02:32:43,921 DEBUG TRAIN Batch 13/200 loss 8.391677 loss_att 13.304684 loss_ctc 10.251713 loss_rnnt 7.057418 hw_loss 0.194349 lr 0.00047970 rank 5
2023-02-23 02:32:43,921 DEBUG TRAIN Batch 13/200 loss 19.372066 loss_att 23.795868 loss_ctc 28.304861 loss_rnnt 17.189083 hw_loss 0.200969 lr 0.00047967 rank 2
2023-02-23 02:32:43,922 DEBUG TRAIN Batch 13/200 loss 9.367693 loss_att 11.687614 loss_ctc 12.425081 loss_rnnt 8.372684 hw_loss 0.231324 lr 0.00047965 rank 3
2023-02-23 02:32:43,923 DEBUG TRAIN Batch 13/200 loss 8.841433 loss_att 11.656898 loss_ctc 12.869335 loss_rnnt 7.636237 hw_loss 0.196968 lr 0.00047968 rank 4
2023-02-23 02:32:43,926 DEBUG TRAIN Batch 13/200 loss 27.249182 loss_att 31.344032 loss_ctc 34.923347 loss_rnnt 25.248596 hw_loss 0.296988 lr 0.00047966 rank 6
2023-02-23 02:32:43,926 DEBUG TRAIN Batch 13/200 loss 18.863249 loss_att 21.538607 loss_ctc 22.405766 loss_rnnt 17.778343 hw_loss 0.145307 lr 0.00047974 rank 1
2023-02-23 02:34:00,531 DEBUG TRAIN Batch 13/300 loss 8.200345 loss_att 12.857533 loss_ctc 10.636330 loss_rnnt 6.805496 hw_loss 0.259901 lr 0.00047944 rank 6
2023-02-23 02:34:00,533 DEBUG TRAIN Batch 13/300 loss 17.233433 loss_att 22.821356 loss_ctc 27.466408 loss_rnnt 14.621250 hw_loss 0.244125 lr 0.00047945 rank 2
2023-02-23 02:34:00,533 DEBUG TRAIN Batch 13/300 loss 7.985799 loss_att 13.640617 loss_ctc 9.585552 loss_rnnt 6.492901 hw_loss 0.278689 lr 0.00047946 rank 4
2023-02-23 02:34:00,533 DEBUG TRAIN Batch 13/300 loss 20.315706 loss_att 21.550564 loss_ctc 21.730637 loss_rnnt 19.738735 hw_loss 0.265016 lr 0.00047948 rank 5
2023-02-23 02:34:00,536 DEBUG TRAIN Batch 13/300 loss 11.061603 loss_att 13.982222 loss_ctc 11.581081 loss_rnnt 10.269394 hw_loss 0.260290 lr 0.00047952 rank 1
2023-02-23 02:34:00,538 DEBUG TRAIN Batch 13/300 loss 21.144932 loss_att 23.337469 loss_ctc 30.817450 loss_rnnt 19.270372 hw_loss 0.274468 lr 0.00047954 rank 0
2023-02-23 02:34:00,538 DEBUG TRAIN Batch 13/300 loss 18.313726 loss_att 27.054583 loss_ctc 23.233219 loss_rnnt 15.786041 hw_loss 0.231716 lr 0.00047943 rank 3
2023-02-23 02:34:00,541 DEBUG TRAIN Batch 13/300 loss 16.868279 loss_att 23.552797 loss_ctc 21.916513 loss_rnnt 14.708605 hw_loss 0.280632 lr 0.00047941 rank 7
2023-02-23 02:35:17,503 DEBUG TRAIN Batch 13/400 loss 31.743521 loss_att 36.147881 loss_ctc 38.506710 loss_rnnt 29.757616 hw_loss 0.381134 lr 0.00047924 rank 4
2023-02-23 02:35:17,503 DEBUG TRAIN Batch 13/400 loss 13.899509 loss_att 17.376328 loss_ctc 16.009510 loss_rnnt 12.834065 hw_loss 0.166400 lr 0.00047923 rank 2
2023-02-23 02:35:17,509 DEBUG TRAIN Batch 13/400 loss 25.090309 loss_att 30.192205 loss_ctc 35.773766 loss_rnnt 22.523638 hw_loss 0.228428 lr 0.00047919 rank 7
2023-02-23 02:35:17,510 DEBUG TRAIN Batch 13/400 loss 11.860416 loss_att 17.865841 loss_ctc 15.514748 loss_rnnt 10.055413 hw_loss 0.218763 lr 0.00047926 rank 5
2023-02-23 02:35:17,511 DEBUG TRAIN Batch 13/400 loss 4.422809 loss_att 8.522965 loss_ctc 6.915153 loss_rnnt 3.172873 hw_loss 0.182985 lr 0.00047932 rank 0
2023-02-23 02:35:17,511 DEBUG TRAIN Batch 13/400 loss 16.522806 loss_att 19.843201 loss_ctc 22.768908 loss_rnnt 14.862787 hw_loss 0.305864 lr 0.00047930 rank 1
2023-02-23 02:35:17,514 DEBUG TRAIN Batch 13/400 loss 20.013199 loss_att 20.339457 loss_ctc 28.782990 loss_rnnt 18.654673 hw_loss 0.232439 lr 0.00047922 rank 6
2023-02-23 02:35:17,517 DEBUG TRAIN Batch 13/400 loss 15.724413 loss_att 19.532949 loss_ctc 14.685750 loss_rnnt 14.925411 hw_loss 0.329590 lr 0.00047921 rank 3
2023-02-23 02:36:33,647 DEBUG TRAIN Batch 13/500 loss 15.106346 loss_att 16.000839 loss_ctc 19.389692 loss_rnnt 14.188005 hw_loss 0.315614 lr 0.00047904 rank 5
2023-02-23 02:36:33,650 DEBUG TRAIN Batch 13/500 loss 15.778454 loss_att 17.477402 loss_ctc 22.554951 loss_rnnt 14.392273 hw_loss 0.267858 lr 0.00047897 rank 7
2023-02-23 02:36:33,654 DEBUG TRAIN Batch 13/500 loss 11.915092 loss_att 13.931242 loss_ctc 13.271846 loss_rnnt 11.249657 hw_loss 0.152448 lr 0.00047910 rank 0
2023-02-23 02:36:33,655 DEBUG TRAIN Batch 13/500 loss 10.388021 loss_att 13.649927 loss_ctc 14.398883 loss_rnnt 8.994764 hw_loss 0.386426 lr 0.00047902 rank 4
2023-02-23 02:36:33,659 DEBUG TRAIN Batch 13/500 loss 7.011653 loss_att 11.273162 loss_ctc 7.508897 loss_rnnt 5.946448 hw_loss 0.274883 lr 0.00047901 rank 2
2023-02-23 02:36:33,659 DEBUG TRAIN Batch 13/500 loss 10.898291 loss_att 11.183011 loss_ctc 11.753500 loss_rnnt 10.582672 hw_loss 0.271210 lr 0.00047908 rank 1
2023-02-23 02:36:33,662 DEBUG TRAIN Batch 13/500 loss 11.869473 loss_att 14.964485 loss_ctc 15.279781 loss_rnnt 10.663604 hw_loss 0.247799 lr 0.00047900 rank 6
2023-02-23 02:36:33,697 DEBUG TRAIN Batch 13/500 loss 19.964073 loss_att 23.378988 loss_ctc 26.495173 loss_rnnt 18.253996 hw_loss 0.293025 lr 0.00047899 rank 3
2023-02-23 02:37:49,671 DEBUG TRAIN Batch 13/600 loss 9.942083 loss_att 11.555666 loss_ctc 12.469998 loss_rnnt 9.164107 hw_loss 0.221635 lr 0.00047875 rank 7
2023-02-23 02:37:49,674 DEBUG TRAIN Batch 13/600 loss 13.625855 loss_att 15.154066 loss_ctc 15.734790 loss_rnnt 12.832656 hw_loss 0.386936 lr 0.00047880 rank 4
2023-02-23 02:37:49,674 DEBUG TRAIN Batch 13/600 loss 10.987989 loss_att 12.999146 loss_ctc 13.776093 loss_rnnt 9.992673 hw_loss 0.415009 lr 0.00047879 rank 2
2023-02-23 02:37:49,676 DEBUG TRAIN Batch 13/600 loss 11.135693 loss_att 12.785461 loss_ctc 13.097956 loss_rnnt 10.413905 hw_loss 0.244123 lr 0.00047878 rank 6
2023-02-23 02:37:49,676 DEBUG TRAIN Batch 13/600 loss 18.139137 loss_att 25.299355 loss_ctc 24.091080 loss_rnnt 15.764544 hw_loss 0.279294 lr 0.00047882 rank 5
2023-02-23 02:37:49,681 DEBUG TRAIN Batch 13/600 loss 17.611803 loss_att 16.937132 loss_ctc 21.283094 loss_rnnt 17.065090 hw_loss 0.360264 lr 0.00047886 rank 1
2023-02-23 02:37:49,682 DEBUG TRAIN Batch 13/600 loss 12.719857 loss_att 12.494430 loss_ctc 15.083072 loss_rnnt 12.291853 hw_loss 0.296241 lr 0.00047877 rank 3
2023-02-23 02:37:49,682 DEBUG TRAIN Batch 13/600 loss 12.545797 loss_att 14.703316 loss_ctc 17.268833 loss_rnnt 11.242900 hw_loss 0.453105 lr 0.00047888 rank 0
2023-02-23 02:39:08,857 DEBUG TRAIN Batch 13/700 loss 10.215645 loss_att 15.930283 loss_ctc 11.690097 loss_rnnt 8.753112 hw_loss 0.230647 lr 0.00047857 rank 2
2023-02-23 02:39:08,864 DEBUG TRAIN Batch 13/700 loss 7.936967 loss_att 12.522002 loss_ctc 11.982668 loss_rnnt 6.312481 hw_loss 0.315096 lr 0.00047860 rank 5
2023-02-23 02:39:08,868 DEBUG TRAIN Batch 13/700 loss 7.402347 loss_att 9.115947 loss_ctc 10.486697 loss_rnnt 6.520020 hw_loss 0.240675 lr 0.00047864 rank 1
2023-02-23 02:39:08,869 DEBUG TRAIN Batch 13/700 loss 7.682196 loss_att 12.612814 loss_ctc 11.957024 loss_rnnt 5.980864 hw_loss 0.272310 lr 0.00047866 rank 0
2023-02-23 02:39:08,876 DEBUG TRAIN Batch 13/700 loss 6.473549 loss_att 12.790148 loss_ctc 9.221336 loss_rnnt 4.702087 hw_loss 0.265820 lr 0.00047853 rank 7
2023-02-23 02:39:08,884 DEBUG TRAIN Batch 13/700 loss 20.984564 loss_att 27.841236 loss_ctc 27.898609 loss_rnnt 18.596207 hw_loss 0.178404 lr 0.00047858 rank 4
2023-02-23 02:39:08,907 DEBUG TRAIN Batch 13/700 loss 14.284101 loss_att 28.862095 loss_ctc 21.419357 loss_rnnt 10.294903 hw_loss 0.229183 lr 0.00047856 rank 6
2023-02-23 02:39:08,925 DEBUG TRAIN Batch 13/700 loss 11.626748 loss_att 15.503652 loss_ctc 14.884750 loss_rnnt 10.291525 hw_loss 0.235203 lr 0.00047855 rank 3
2023-02-23 02:40:24,097 DEBUG TRAIN Batch 13/800 loss 11.071392 loss_att 18.536598 loss_ctc 13.152775 loss_rnnt 9.169048 hw_loss 0.247097 lr 0.00047835 rank 2
2023-02-23 02:40:24,100 DEBUG TRAIN Batch 13/800 loss 11.561932 loss_att 15.820501 loss_ctc 16.812519 loss_rnnt 9.864154 hw_loss 0.273723 lr 0.00047838 rank 5
2023-02-23 02:40:24,102 DEBUG TRAIN Batch 13/800 loss 15.569257 loss_att 19.286606 loss_ctc 20.463585 loss_rnnt 14.071037 hw_loss 0.191574 lr 0.00047844 rank 0
2023-02-23 02:40:24,104 DEBUG TRAIN Batch 13/800 loss 19.408304 loss_att 20.161497 loss_ctc 24.939762 loss_rnnt 18.363117 hw_loss 0.294412 lr 0.00047842 rank 1
2023-02-23 02:40:24,104 DEBUG TRAIN Batch 13/800 loss 10.510736 loss_att 13.312520 loss_ctc 10.961357 loss_rnnt 9.734071 hw_loss 0.292924 lr 0.00047836 rank 4
2023-02-23 02:40:24,107 DEBUG TRAIN Batch 13/800 loss 16.804173 loss_att 18.241405 loss_ctc 18.761044 loss_rnnt 16.075455 hw_loss 0.338165 lr 0.00047833 rank 3
2023-02-23 02:40:24,108 DEBUG TRAIN Batch 13/800 loss 4.802399 loss_att 8.462114 loss_ctc 6.809838 loss_rnnt 3.610539 hw_loss 0.360484 lr 0.00047834 rank 6
2023-02-23 02:40:24,109 DEBUG TRAIN Batch 13/800 loss 12.047658 loss_att 16.121765 loss_ctc 17.877737 loss_rnnt 10.313917 hw_loss 0.265452 lr 0.00047831 rank 7
2023-02-23 02:41:39,901 DEBUG TRAIN Batch 13/900 loss 7.263697 loss_att 12.058694 loss_ctc 9.107277 loss_rnnt 5.955167 hw_loss 0.194475 lr 0.00047814 rank 4
2023-02-23 02:41:39,903 DEBUG TRAIN Batch 13/900 loss 10.029816 loss_att 16.709349 loss_ctc 15.040461 loss_rnnt 7.868698 hw_loss 0.294609 lr 0.00047822 rank 0
2023-02-23 02:41:39,905 DEBUG TRAIN Batch 13/900 loss 5.148542 loss_att 9.599518 loss_ctc 7.177174 loss_rnnt 3.820191 hw_loss 0.314384 lr 0.00047813 rank 2
2023-02-23 02:41:39,907 DEBUG TRAIN Batch 13/900 loss 8.379234 loss_att 12.492001 loss_ctc 15.514281 loss_rnnt 6.462627 hw_loss 0.267588 lr 0.00047812 rank 3
2023-02-23 02:41:39,909 DEBUG TRAIN Batch 13/900 loss 34.433571 loss_att 38.103455 loss_ctc 42.020668 loss_rnnt 32.558441 hw_loss 0.242893 lr 0.00047812 rank 6
2023-02-23 02:41:39,909 DEBUG TRAIN Batch 13/900 loss 22.317816 loss_att 27.478609 loss_ctc 28.459763 loss_rnnt 20.304119 hw_loss 0.304895 lr 0.00047817 rank 5
2023-02-23 02:41:39,909 DEBUG TRAIN Batch 13/900 loss 12.976167 loss_att 17.881481 loss_ctc 14.979885 loss_rnnt 11.599844 hw_loss 0.240182 lr 0.00047820 rank 1
2023-02-23 02:41:39,913 DEBUG TRAIN Batch 13/900 loss 11.650861 loss_att 13.482247 loss_ctc 12.124097 loss_rnnt 11.068668 hw_loss 0.286530 lr 0.00047810 rank 7
2023-02-23 02:42:55,700 DEBUG TRAIN Batch 13/1000 loss 14.976527 loss_att 19.344412 loss_ctc 13.795573 loss_rnnt 14.092993 hw_loss 0.313908 lr 0.00047795 rank 5
2023-02-23 02:42:55,702 DEBUG TRAIN Batch 13/1000 loss 15.694980 loss_att 19.082993 loss_ctc 17.525560 loss_rnnt 14.634354 hw_loss 0.260524 lr 0.00047790 rank 3
2023-02-23 02:42:55,702 DEBUG TRAIN Batch 13/1000 loss 7.951176 loss_att 12.018276 loss_ctc 9.627461 loss_rnnt 6.740092 hw_loss 0.326549 lr 0.00047798 rank 1
2023-02-23 02:42:55,705 DEBUG TRAIN Batch 13/1000 loss 9.595901 loss_att 13.456892 loss_ctc 10.024988 loss_rnnt 8.632751 hw_loss 0.250764 lr 0.00047791 rank 6
2023-02-23 02:42:55,705 DEBUG TRAIN Batch 13/1000 loss 9.368944 loss_att 13.973750 loss_ctc 10.977911 loss_rnnt 8.155341 hw_loss 0.146460 lr 0.00047791 rank 2
2023-02-23 02:42:55,705 DEBUG TRAIN Batch 13/1000 loss 9.466964 loss_att 9.590841 loss_ctc 10.507048 loss_rnnt 9.145469 hw_loss 0.296327 lr 0.00047800 rank 0
2023-02-23 02:42:55,711 DEBUG TRAIN Batch 13/1000 loss 14.463486 loss_att 17.404428 loss_ctc 16.292130 loss_rnnt 13.512238 hw_loss 0.223574 lr 0.00047788 rank 7
2023-02-23 02:42:55,730 DEBUG TRAIN Batch 13/1000 loss 16.704729 loss_att 22.251049 loss_ctc 17.815239 loss_rnnt 15.327492 hw_loss 0.224825 lr 0.00047792 rank 4
2023-02-23 02:44:13,605 DEBUG TRAIN Batch 13/1100 loss 11.781351 loss_att 16.413683 loss_ctc 13.475286 loss_rnnt 10.485137 hw_loss 0.269796 lr 0.00047770 rank 4
2023-02-23 02:44:13,605 DEBUG TRAIN Batch 13/1100 loss 8.878117 loss_att 13.442983 loss_ctc 10.896507 loss_rnnt 7.483833 hw_loss 0.397858 lr 0.00047773 rank 5
2023-02-23 02:44:13,609 DEBUG TRAIN Batch 13/1100 loss 23.120859 loss_att 23.326771 loss_ctc 28.551149 loss_rnnt 22.221252 hw_loss 0.251977 lr 0.00047770 rank 2
2023-02-23 02:44:13,609 DEBUG TRAIN Batch 13/1100 loss 10.184680 loss_att 12.631395 loss_ctc 12.097525 loss_rnnt 9.295678 hw_loss 0.271150 lr 0.00047778 rank 0
2023-02-23 02:44:13,611 DEBUG TRAIN Batch 13/1100 loss 17.312328 loss_att 20.898567 loss_ctc 20.152010 loss_rnnt 16.089117 hw_loss 0.238763 lr 0.00047769 rank 6
2023-02-23 02:44:13,615 DEBUG TRAIN Batch 13/1100 loss 6.108602 loss_att 9.478960 loss_ctc 7.131104 loss_rnnt 5.165574 hw_loss 0.248667 lr 0.00047776 rank 1
2023-02-23 02:44:13,615 DEBUG TRAIN Batch 13/1100 loss 13.153519 loss_att 18.263618 loss_ctc 19.251352 loss_rnnt 11.132300 hw_loss 0.349038 lr 0.00047768 rank 3
2023-02-23 02:44:13,659 DEBUG TRAIN Batch 13/1100 loss 13.650367 loss_att 13.069687 loss_ctc 16.678583 loss_rnnt 13.241978 hw_loss 0.226431 lr 0.00047766 rank 7
2023-02-23 02:45:31,469 DEBUG TRAIN Batch 13/1200 loss 11.215062 loss_att 14.969169 loss_ctc 13.012876 loss_rnnt 10.085608 hw_loss 0.260483 lr 0.00047746 rank 3
2023-02-23 02:45:31,471 DEBUG TRAIN Batch 13/1200 loss 19.975874 loss_att 23.073652 loss_ctc 23.298620 loss_rnnt 18.768082 hw_loss 0.272255 lr 0.00047757 rank 0
2023-02-23 02:45:31,473 DEBUG TRAIN Batch 13/1200 loss 13.446117 loss_att 14.489956 loss_ctc 16.753565 loss_rnnt 12.568496 hw_loss 0.427240 lr 0.00047748 rank 2
2023-02-23 02:45:31,474 DEBUG TRAIN Batch 13/1200 loss 29.087593 loss_att 28.833109 loss_ctc 33.115814 loss_rnnt 28.421247 hw_loss 0.337778 lr 0.00047755 rank 1
2023-02-23 02:45:31,474 DEBUG TRAIN Batch 13/1200 loss 9.463261 loss_att 10.251541 loss_ctc 10.699374 loss_rnnt 8.966694 hw_loss 0.326429 lr 0.00047751 rank 5
2023-02-23 02:45:31,475 DEBUG TRAIN Batch 13/1200 loss 28.829859 loss_att 32.118958 loss_ctc 37.110363 loss_rnnt 26.936823 hw_loss 0.245900 lr 0.00047749 rank 4
2023-02-23 02:45:31,476 DEBUG TRAIN Batch 13/1200 loss 17.469301 loss_att 22.119923 loss_ctc 21.486092 loss_rnnt 15.814548 hw_loss 0.354481 lr 0.00047747 rank 6
2023-02-23 02:45:31,522 DEBUG TRAIN Batch 13/1200 loss 17.543701 loss_att 22.164070 loss_ctc 23.166321 loss_rnnt 15.693557 hw_loss 0.330723 lr 0.00047744 rank 7
2023-02-23 02:46:48,005 DEBUG TRAIN Batch 13/1300 loss 10.719330 loss_att 14.567194 loss_ctc 16.348469 loss_rnnt 9.074540 hw_loss 0.233747 lr 0.00047729 rank 5
2023-02-23 02:46:48,015 DEBUG TRAIN Batch 13/1300 loss 13.061465 loss_att 17.784597 loss_ctc 13.457661 loss_rnnt 11.897699 hw_loss 0.311834 lr 0.00047726 rank 2
2023-02-23 02:46:48,016 DEBUG TRAIN Batch 13/1300 loss 10.825089 loss_att 12.833425 loss_ctc 14.579308 loss_rnnt 9.771214 hw_loss 0.284333 lr 0.00047735 rank 0
2023-02-23 02:46:48,015 DEBUG TRAIN Batch 13/1300 loss 14.195433 loss_att 13.694042 loss_ctc 16.148861 loss_rnnt 13.874393 hw_loss 0.301611 lr 0.00047727 rank 4
2023-02-23 02:46:48,017 DEBUG TRAIN Batch 13/1300 loss 13.244584 loss_att 15.156937 loss_ctc 14.399471 loss_rnnt 12.565776 hw_loss 0.266913 lr 0.00047722 rank 7
2023-02-23 02:46:48,017 DEBUG TRAIN Batch 13/1300 loss 22.448566 loss_att 25.992382 loss_ctc 32.436958 loss_rnnt 20.308443 hw_loss 0.186701 lr 0.00047724 rank 3
2023-02-23 02:46:48,020 DEBUG TRAIN Batch 13/1300 loss 24.613453 loss_att 26.031710 loss_ctc 38.325539 loss_rnnt 22.346813 hw_loss 0.290085 lr 0.00047733 rank 1
2023-02-23 02:46:48,020 DEBUG TRAIN Batch 13/1300 loss 17.994974 loss_att 18.664854 loss_ctc 24.004889 loss_rnnt 16.899033 hw_loss 0.301209 lr 0.00047725 rank 6
2023-02-23 02:48:05,711 DEBUG TRAIN Batch 13/1400 loss 5.129672 loss_att 10.422658 loss_ctc 6.678463 loss_rnnt 3.803005 hw_loss 0.115434 lr 0.00047705 rank 4
2023-02-23 02:48:05,712 DEBUG TRAIN Batch 13/1400 loss 7.142456 loss_att 9.941555 loss_ctc 9.647510 loss_rnnt 6.102748 hw_loss 0.273528 lr 0.00047703 rank 6
2023-02-23 02:48:05,714 DEBUG TRAIN Batch 13/1400 loss 15.177454 loss_att 22.303171 loss_ctc 18.145792 loss_rnnt 13.219406 hw_loss 0.257111 lr 0.00047704 rank 2
2023-02-23 02:48:05,714 DEBUG TRAIN Batch 13/1400 loss 19.602619 loss_att 26.291134 loss_ctc 20.257168 loss_rnnt 18.019369 hw_loss 0.296763 lr 0.00047711 rank 1
2023-02-23 02:48:05,716 DEBUG TRAIN Batch 13/1400 loss 15.832521 loss_att 24.179081 loss_ctc 29.480200 loss_rnnt 12.224818 hw_loss 0.222563 lr 0.00047703 rank 3
2023-02-23 02:48:05,718 DEBUG TRAIN Batch 13/1400 loss 11.993043 loss_att 17.064472 loss_ctc 13.625242 loss_rnnt 10.606537 hw_loss 0.289862 lr 0.00047713 rank 0
2023-02-23 02:48:05,719 DEBUG TRAIN Batch 13/1400 loss 12.178228 loss_att 17.363918 loss_ctc 14.342613 loss_rnnt 10.685936 hw_loss 0.312319 lr 0.00047701 rank 7
2023-02-23 02:48:05,722 DEBUG TRAIN Batch 13/1400 loss 7.803708 loss_att 11.121555 loss_ctc 9.152585 loss_rnnt 6.775758 hw_loss 0.345994 lr 0.00047708 rank 5
2023-02-23 02:49:22,599 DEBUG TRAIN Batch 13/1500 loss 7.681155 loss_att 11.861584 loss_ctc 11.673254 loss_rnnt 6.143303 hw_loss 0.317785 lr 0.00047683 rank 4
2023-02-23 02:49:22,602 DEBUG TRAIN Batch 13/1500 loss 11.197368 loss_att 12.860430 loss_ctc 14.190868 loss_rnnt 10.336821 hw_loss 0.241500 lr 0.00047686 rank 5
2023-02-23 02:49:22,602 DEBUG TRAIN Batch 13/1500 loss 9.089213 loss_att 14.349841 loss_ctc 11.359368 loss_rnnt 7.574977 hw_loss 0.298920 lr 0.00047691 rank 0
2023-02-23 02:49:22,603 DEBUG TRAIN Batch 13/1500 loss 14.226844 loss_att 20.387413 loss_ctc 16.401676 loss_rnnt 12.579471 hw_loss 0.234905 lr 0.00047681 rank 3
2023-02-23 02:49:22,606 DEBUG TRAIN Batch 13/1500 loss 10.032216 loss_att 19.404617 loss_ctc 18.774582 loss_rnnt 6.855875 hw_loss 0.255398 lr 0.00047683 rank 2
2023-02-23 02:49:22,610 DEBUG TRAIN Batch 13/1500 loss 19.929802 loss_att 22.028141 loss_ctc 24.498327 loss_rnnt 18.735895 hw_loss 0.309561 lr 0.00047679 rank 7
2023-02-23 02:49:22,611 DEBUG TRAIN Batch 13/1500 loss 18.178898 loss_att 20.478477 loss_ctc 19.741940 loss_rnnt 17.422653 hw_loss 0.164859 lr 0.00047682 rank 6
2023-02-23 02:49:22,658 DEBUG TRAIN Batch 13/1500 loss 13.155977 loss_att 18.309507 loss_ctc 15.018980 loss_rnnt 11.755110 hw_loss 0.228306 lr 0.00047689 rank 1
2023-02-23 02:50:38,627 DEBUG TRAIN Batch 13/1600 loss 23.971678 loss_att 26.873079 loss_ctc 25.038841 loss_rnnt 23.136999 hw_loss 0.210210 lr 0.00047664 rank 5
2023-02-23 02:50:38,628 DEBUG TRAIN Batch 13/1600 loss 19.194637 loss_att 23.160366 loss_ctc 26.393608 loss_rnnt 17.264828 hw_loss 0.331501 lr 0.00047661 rank 2
2023-02-23 02:50:38,633 DEBUG TRAIN Batch 13/1600 loss 9.174187 loss_att 13.650108 loss_ctc 13.098345 loss_rnnt 7.661731 hw_loss 0.176345 lr 0.00047662 rank 4
2023-02-23 02:50:38,636 DEBUG TRAIN Batch 13/1600 loss 11.697726 loss_att 16.226976 loss_ctc 17.787762 loss_rnnt 9.812336 hw_loss 0.314128 lr 0.00047670 rank 0
2023-02-23 02:50:38,637 DEBUG TRAIN Batch 13/1600 loss 17.811594 loss_att 22.759026 loss_ctc 22.821121 loss_rnnt 15.997941 hw_loss 0.292931 lr 0.00047668 rank 1
2023-02-23 02:50:38,638 DEBUG TRAIN Batch 13/1600 loss 12.504822 loss_att 15.019976 loss_ctc 16.476849 loss_rnnt 11.372174 hw_loss 0.187524 lr 0.00047659 rank 3
2023-02-23 02:50:38,640 DEBUG TRAIN Batch 13/1600 loss 13.543127 loss_att 20.794558 loss_ctc 17.284170 loss_rnnt 11.487071 hw_loss 0.200557 lr 0.00047660 rank 6
2023-02-23 02:50:38,641 DEBUG TRAIN Batch 13/1600 loss 19.766167 loss_att 23.071808 loss_ctc 24.252823 loss_rnnt 18.404058 hw_loss 0.192673 lr 0.00047657 rank 7
2023-02-23 02:51:54,386 DEBUG TRAIN Batch 13/1700 loss 15.934463 loss_att 19.242538 loss_ctc 22.159866 loss_rnnt 14.233814 hw_loss 0.391833 lr 0.00047643 rank 5
2023-02-23 02:51:54,388 DEBUG TRAIN Batch 13/1700 loss 22.649668 loss_att 27.059381 loss_ctc 37.992882 loss_rnnt 19.619850 hw_loss 0.191462 lr 0.00047638 rank 6
2023-02-23 02:51:54,387 DEBUG TRAIN Batch 13/1700 loss 10.770228 loss_att 15.965396 loss_ctc 12.345116 loss_rnnt 9.416519 hw_loss 0.196296 lr 0.00047640 rank 4
2023-02-23 02:51:54,389 DEBUG TRAIN Batch 13/1700 loss 18.307581 loss_att 22.105679 loss_ctc 22.170048 loss_rnnt 16.889791 hw_loss 0.268453 lr 0.00047636 rank 7
2023-02-23 02:51:54,393 DEBUG TRAIN Batch 13/1700 loss 12.811273 loss_att 16.818890 loss_ctc 16.032747 loss_rnnt 11.429838 hw_loss 0.281965 lr 0.00047639 rank 2
2023-02-23 02:51:54,394 DEBUG TRAIN Batch 13/1700 loss 13.955262 loss_att 18.709553 loss_ctc 14.615452 loss_rnnt 12.753111 hw_loss 0.306129 lr 0.00047648 rank 0
2023-02-23 02:51:54,396 DEBUG TRAIN Batch 13/1700 loss 17.762413 loss_att 21.304583 loss_ctc 22.386969 loss_rnnt 16.307474 hw_loss 0.243555 lr 0.00047646 rank 1
2023-02-23 02:51:54,439 DEBUG TRAIN Batch 13/1700 loss 17.727337 loss_att 22.874298 loss_ctc 25.869991 loss_rnnt 15.476752 hw_loss 0.254075 lr 0.00047638 rank 3
2023-02-23 02:53:13,966 DEBUG TRAIN Batch 13/1800 loss 8.787900 loss_att 12.980928 loss_ctc 11.085176 loss_rnnt 7.509291 hw_loss 0.250686 lr 0.00047626 rank 0
2023-02-23 02:53:13,967 DEBUG TRAIN Batch 13/1800 loss 13.519857 loss_att 15.711182 loss_ctc 18.445358 loss_rnnt 12.314123 hw_loss 0.207631 lr 0.00047618 rank 4
2023-02-23 02:53:13,967 DEBUG TRAIN Batch 13/1800 loss 17.646349 loss_att 19.883213 loss_ctc 20.997786 loss_rnnt 16.571346 hw_loss 0.338952 lr 0.00047618 rank 2
2023-02-23 02:53:13,970 DEBUG TRAIN Batch 13/1800 loss 11.102257 loss_att 15.147345 loss_ctc 12.471064 loss_rnnt 9.975060 hw_loss 0.254382 lr 0.00047616 rank 3
2023-02-23 02:53:13,970 DEBUG TRAIN Batch 13/1800 loss 6.098822 loss_att 8.214055 loss_ctc 6.627720 loss_rnnt 5.489792 hw_loss 0.216495 lr 0.00047621 rank 5
2023-02-23 02:53:13,973 DEBUG TRAIN Batch 13/1800 loss 14.106454 loss_att 18.324425 loss_ctc 20.929245 loss_rnnt 12.197956 hw_loss 0.290994 lr 0.00047624 rank 1
2023-02-23 02:53:13,974 DEBUG TRAIN Batch 13/1800 loss 14.202976 loss_att 16.923056 loss_ctc 14.487123 loss_rnnt 13.566767 hw_loss 0.101824 lr 0.00047617 rank 6
2023-02-23 02:53:13,976 DEBUG TRAIN Batch 13/1800 loss 12.535482 loss_att 16.015680 loss_ctc 15.656906 loss_rnnt 11.290011 hw_loss 0.249828 lr 0.00047614 rank 7
2023-02-23 02:54:29,930 DEBUG TRAIN Batch 13/1900 loss 6.449349 loss_att 11.329987 loss_ctc 8.529173 loss_rnnt 5.108724 hw_loss 0.163476 lr 0.00047599 rank 5
2023-02-23 02:54:29,930 DEBUG TRAIN Batch 13/1900 loss 14.341880 loss_att 13.880926 loss_ctc 16.692621 loss_rnnt 14.010594 hw_loss 0.206332 lr 0.00047594 rank 3
2023-02-23 02:54:29,934 DEBUG TRAIN Batch 13/1900 loss 12.022038 loss_att 11.316317 loss_ctc 14.434546 loss_rnnt 11.646962 hw_loss 0.364782 lr 0.00047596 rank 2
2023-02-23 02:54:29,935 DEBUG TRAIN Batch 13/1900 loss 11.656497 loss_att 14.000410 loss_ctc 17.488884 loss_rnnt 10.256755 hw_loss 0.287454 lr 0.00047595 rank 6
2023-02-23 02:54:29,935 DEBUG TRAIN Batch 13/1900 loss 12.496945 loss_att 15.702440 loss_ctc 15.618515 loss_rnnt 11.261806 hw_loss 0.333434 lr 0.00047597 rank 4
2023-02-23 02:54:29,939 DEBUG TRAIN Batch 13/1900 loss 10.825999 loss_att 12.862046 loss_ctc 12.725230 loss_rnnt 10.034694 hw_loss 0.245373 lr 0.00047605 rank 0
2023-02-23 02:54:29,940 DEBUG TRAIN Batch 13/1900 loss 12.108958 loss_att 16.268871 loss_ctc 15.540636 loss_rnnt 10.674454 hw_loss 0.271809 lr 0.00047603 rank 1
2023-02-23 02:54:29,941 DEBUG TRAIN Batch 13/1900 loss 25.643206 loss_att 27.933987 loss_ctc 34.786484 loss_rnnt 23.847450 hw_loss 0.222176 lr 0.00047593 rank 7
2023-02-23 02:55:46,568 DEBUG TRAIN Batch 13/2000 loss 13.680097 loss_att 17.072441 loss_ctc 21.539091 loss_rnnt 11.826553 hw_loss 0.238515 lr 0.00047573 rank 3
2023-02-23 02:55:46,569 DEBUG TRAIN Batch 13/2000 loss 12.362921 loss_att 15.290167 loss_ctc 17.018475 loss_rnnt 11.059464 hw_loss 0.182377 lr 0.00047578 rank 5
2023-02-23 02:55:46,570 DEBUG TRAIN Batch 13/2000 loss 10.990380 loss_att 12.829457 loss_ctc 12.640040 loss_rnnt 10.243611 hw_loss 0.298124 lr 0.00047575 rank 4
2023-02-23 02:55:46,571 DEBUG TRAIN Batch 13/2000 loss 16.004776 loss_att 22.401470 loss_ctc 18.253220 loss_rnnt 14.305632 hw_loss 0.225026 lr 0.00047575 rank 2
2023-02-23 02:55:46,571 DEBUG TRAIN Batch 13/2000 loss 17.226723 loss_att 18.818914 loss_ctc 21.399488 loss_rnnt 16.208122 hw_loss 0.269611 lr 0.00047583 rank 0
2023-02-23 02:55:46,572 DEBUG TRAIN Batch 13/2000 loss 11.732239 loss_att 16.279100 loss_ctc 12.408606 loss_rnnt 10.603539 hw_loss 0.242144 lr 0.00047581 rank 1
2023-02-23 02:55:46,575 DEBUG TRAIN Batch 13/2000 loss 4.931281 loss_att 9.879580 loss_ctc 6.606045 loss_rnnt 3.593789 hw_loss 0.233495 lr 0.00047571 rank 7
2023-02-23 02:55:46,576 DEBUG TRAIN Batch 13/2000 loss 14.588543 loss_att 19.457977 loss_ctc 21.347570 loss_rnnt 12.630871 hw_loss 0.154840 lr 0.00047574 rank 6
2023-02-23 02:57:04,337 DEBUG TRAIN Batch 13/2100 loss 7.283793 loss_att 10.783909 loss_ctc 7.348918 loss_rnnt 6.439563 hw_loss 0.254105 lr 0.00047562 rank 0
2023-02-23 02:57:04,339 DEBUG TRAIN Batch 13/2100 loss 11.314717 loss_att 16.972752 loss_ctc 15.284235 loss_rnnt 9.514408 hw_loss 0.261438 lr 0.00047552 rank 6
2023-02-23 02:57:04,341 DEBUG TRAIN Batch 13/2100 loss 10.957108 loss_att 13.158337 loss_ctc 12.724174 loss_rnnt 10.153987 hw_loss 0.238627 lr 0.00047556 rank 5
2023-02-23 02:57:04,342 DEBUG TRAIN Batch 13/2100 loss 26.939625 loss_att 35.304325 loss_ctc 34.456432 loss_rnnt 24.158138 hw_loss 0.199328 lr 0.00047554 rank 4
2023-02-23 02:57:04,342 DEBUG TRAIN Batch 13/2100 loss 9.581966 loss_att 15.319752 loss_ctc 11.475565 loss_rnnt 8.062099 hw_loss 0.224680 lr 0.00047553 rank 2
2023-02-23 02:57:04,348 DEBUG TRAIN Batch 13/2100 loss 16.089357 loss_att 23.758558 loss_ctc 25.696384 loss_rnnt 13.135889 hw_loss 0.260044 lr 0.00047560 rank 1
2023-02-23 02:57:04,373 DEBUG TRAIN Batch 13/2100 loss 17.040915 loss_att 17.977459 loss_ctc 18.080479 loss_rnnt 16.572552 hw_loss 0.267081 lr 0.00047551 rank 3
2023-02-23 02:57:04,388 DEBUG TRAIN Batch 13/2100 loss 13.268167 loss_att 15.214678 loss_ctc 16.469906 loss_rnnt 12.315325 hw_loss 0.256202 lr 0.00047549 rank 7
2023-02-23 02:58:21,794 DEBUG TRAIN Batch 13/2200 loss 8.646549 loss_att 11.021254 loss_ctc 10.789438 loss_rnnt 7.728301 hw_loss 0.295480 lr 0.00047532 rank 2
2023-02-23 02:58:21,798 DEBUG TRAIN Batch 13/2200 loss 6.135580 loss_att 11.722273 loss_ctc 9.700452 loss_rnnt 4.378917 hw_loss 0.307516 lr 0.00047531 rank 6
2023-02-23 02:58:21,800 DEBUG TRAIN Batch 13/2200 loss 7.702404 loss_att 14.591011 loss_ctc 6.824914 loss_rnnt 6.290012 hw_loss 0.284380 lr 0.00047532 rank 4
2023-02-23 02:58:21,801 DEBUG TRAIN Batch 13/2200 loss 27.343874 loss_att 33.271317 loss_ctc 34.781494 loss_rnnt 25.056007 hw_loss 0.207553 lr 0.00047540 rank 0
2023-02-23 02:58:21,802 DEBUG TRAIN Batch 13/2200 loss 18.425110 loss_att 18.759575 loss_ctc 21.257717 loss_rnnt 17.804798 hw_loss 0.329507 lr 0.00047535 rank 5
2023-02-23 02:58:21,805 DEBUG TRAIN Batch 13/2200 loss 13.159787 loss_att 20.193165 loss_ctc 16.634270 loss_rnnt 11.180712 hw_loss 0.204628 lr 0.00047528 rank 7
2023-02-23 02:58:21,809 DEBUG TRAIN Batch 13/2200 loss 10.974002 loss_att 16.019873 loss_ctc 12.704130 loss_rnnt 9.586338 hw_loss 0.277135 lr 0.00047530 rank 3
2023-02-23 02:58:21,810 DEBUG TRAIN Batch 13/2200 loss 13.122380 loss_att 17.073151 loss_ctc 19.469940 loss_rnnt 11.315841 hw_loss 0.318836 lr 0.00047538 rank 1
2023-02-23 02:59:37,837 DEBUG TRAIN Batch 13/2300 loss 9.854124 loss_att 12.563963 loss_ctc 9.163008 loss_rnnt 9.289799 hw_loss 0.214698 lr 0.00047513 rank 5
2023-02-23 02:59:37,837 DEBUG TRAIN Batch 13/2300 loss 16.546394 loss_att 18.971029 loss_ctc 19.238010 loss_rnnt 15.598361 hw_loss 0.195419 lr 0.00047510 rank 2
2023-02-23 02:59:37,838 DEBUG TRAIN Batch 13/2300 loss 7.334199 loss_att 10.304599 loss_ctc 8.239955 loss_rnnt 6.441751 hw_loss 0.333001 lr 0.00047511 rank 4
2023-02-23 02:59:37,840 DEBUG TRAIN Batch 13/2300 loss 12.260943 loss_att 15.357809 loss_ctc 15.747351 loss_rnnt 11.019792 hw_loss 0.294234 lr 0.00047517 rank 1
2023-02-23 02:59:37,842 DEBUG TRAIN Batch 13/2300 loss 19.074202 loss_att 23.892372 loss_ctc 25.210060 loss_rnnt 17.188393 hw_loss 0.195113 lr 0.00047519 rank 0
2023-02-23 02:59:37,844 DEBUG TRAIN Batch 13/2300 loss 12.199993 loss_att 15.733864 loss_ctc 15.787079 loss_rnnt 10.844228 hw_loss 0.320085 lr 0.00047508 rank 3
2023-02-23 02:59:37,846 DEBUG TRAIN Batch 13/2300 loss 17.770422 loss_att 19.213745 loss_ctc 19.179050 loss_rnnt 17.177267 hw_loss 0.218762 lr 0.00047507 rank 7
2023-02-23 02:59:37,889 DEBUG TRAIN Batch 13/2300 loss 9.264177 loss_att 13.731296 loss_ctc 11.060547 loss_rnnt 8.050993 hw_loss 0.150460 lr 0.00047509 rank 6
2023-02-23 03:00:54,711 DEBUG TRAIN Batch 13/2400 loss 10.631323 loss_att 14.741737 loss_ctc 19.387278 loss_rnnt 8.530216 hw_loss 0.209181 lr 0.00047489 rank 4
2023-02-23 03:00:54,711 DEBUG TRAIN Batch 13/2400 loss 14.182784 loss_att 17.138559 loss_ctc 18.200842 loss_rnnt 12.911452 hw_loss 0.270817 lr 0.00047497 rank 0
2023-02-23 03:00:54,715 DEBUG TRAIN Batch 13/2400 loss 16.575336 loss_att 18.173788 loss_ctc 19.105215 loss_rnnt 15.743641 hw_loss 0.327537 lr 0.00047485 rank 7
2023-02-23 03:00:54,716 DEBUG TRAIN Batch 13/2400 loss 8.692260 loss_att 12.482882 loss_ctc 13.582288 loss_rnnt 7.126804 hw_loss 0.291239 lr 0.00047489 rank 2
2023-02-23 03:00:54,719 DEBUG TRAIN Batch 13/2400 loss 15.853941 loss_att 20.190254 loss_ctc 18.154119 loss_rnnt 14.519420 hw_loss 0.301065 lr 0.00047492 rank 5
2023-02-23 03:00:54,726 DEBUG TRAIN Batch 13/2400 loss 10.392135 loss_att 14.849188 loss_ctc 14.361156 loss_rnnt 8.870490 hw_loss 0.189436 lr 0.00047488 rank 6
2023-02-23 03:00:54,729 DEBUG TRAIN Batch 13/2400 loss 6.308743 loss_att 10.783779 loss_ctc 6.222480 loss_rnnt 5.275019 hw_loss 0.281658 lr 0.00047495 rank 1
2023-02-23 03:00:54,760 DEBUG TRAIN Batch 13/2400 loss 17.202463 loss_att 22.195286 loss_ctc 23.658209 loss_rnnt 15.186002 hw_loss 0.294619 lr 0.00047487 rank 3
2023-02-23 03:02:15,215 DEBUG TRAIN Batch 13/2500 loss 11.918189 loss_att 11.808474 loss_ctc 12.310833 loss_rnnt 11.710698 hw_loss 0.332029 lr 0.00047476 rank 0
2023-02-23 03:02:15,221 DEBUG TRAIN Batch 13/2500 loss 13.437192 loss_att 16.375359 loss_ctc 16.937592 loss_rnnt 12.268091 hw_loss 0.215151 lr 0.00047471 rank 5
2023-02-23 03:02:15,223 DEBUG TRAIN Batch 13/2500 loss 10.829654 loss_att 14.378727 loss_ctc 16.825884 loss_rnnt 9.184877 hw_loss 0.253996 lr 0.00047467 rank 2
2023-02-23 03:02:15,224 DEBUG TRAIN Batch 13/2500 loss 17.948233 loss_att 20.656502 loss_ctc 21.799002 loss_rnnt 16.783535 hw_loss 0.205513 lr 0.00047466 rank 6
2023-02-23 03:02:15,225 DEBUG TRAIN Batch 13/2500 loss 11.218343 loss_att 15.204021 loss_ctc 16.164892 loss_rnnt 9.649470 hw_loss 0.210369 lr 0.00047468 rank 4
2023-02-23 03:02:15,226 DEBUG TRAIN Batch 13/2500 loss 16.182001 loss_att 16.123604 loss_ctc 19.815586 loss_rnnt 15.540283 hw_loss 0.316725 lr 0.00047466 rank 3
2023-02-23 03:02:15,227 DEBUG TRAIN Batch 13/2500 loss 15.906899 loss_att 16.606430 loss_ctc 17.685644 loss_rnnt 15.342659 hw_loss 0.350940 lr 0.00047474 rank 1
2023-02-23 03:02:15,272 DEBUG TRAIN Batch 13/2500 loss 13.280636 loss_att 14.228731 loss_ctc 14.499998 loss_rnnt 12.807419 hw_loss 0.226904 lr 0.00047464 rank 7
2023-02-23 03:03:33,321 DEBUG TRAIN Batch 13/2600 loss 13.182207 loss_att 13.355268 loss_ctc 14.995390 loss_rnnt 12.688331 hw_loss 0.407824 lr 0.00047447 rank 4
2023-02-23 03:03:33,327 DEBUG TRAIN Batch 13/2600 loss 13.534980 loss_att 20.906883 loss_ctc 17.961506 loss_rnnt 11.326010 hw_loss 0.270727 lr 0.00047446 rank 2
2023-02-23 03:03:33,328 DEBUG TRAIN Batch 13/2600 loss 14.017857 loss_att 20.656872 loss_ctc 21.532272 loss_rnnt 11.570926 hw_loss 0.219762 lr 0.00047442 rank 7
2023-02-23 03:03:33,328 DEBUG TRAIN Batch 13/2600 loss 13.919306 loss_att 21.042912 loss_ctc 17.874062 loss_rnnt 11.834377 hw_loss 0.249200 lr 0.00047449 rank 5
2023-02-23 03:03:33,329 DEBUG TRAIN Batch 13/2600 loss 17.611261 loss_att 22.523838 loss_ctc 28.531307 loss_rnnt 14.986537 hw_loss 0.349129 lr 0.00047444 rank 3
2023-02-23 03:03:33,329 DEBUG TRAIN Batch 13/2600 loss 22.672876 loss_att 29.563240 loss_ctc 25.122206 loss_rnnt 20.823471 hw_loss 0.271417 lr 0.00047454 rank 0
2023-02-23 03:03:33,336 DEBUG TRAIN Batch 13/2600 loss 20.218225 loss_att 22.213078 loss_ctc 29.716970 loss_rnnt 18.392292 hw_loss 0.300863 lr 0.00047453 rank 1
2023-02-23 03:03:33,384 DEBUG TRAIN Batch 13/2600 loss 14.129052 loss_att 13.749972 loss_ctc 16.804203 loss_rnnt 13.658188 hw_loss 0.356238 lr 0.00047445 rank 6
2023-02-23 03:04:49,260 DEBUG TRAIN Batch 13/2700 loss 12.393734 loss_att 16.210474 loss_ctc 20.556301 loss_rnnt 10.466522 hw_loss 0.141604 lr 0.00047425 rank 4
2023-02-23 03:04:49,261 DEBUG TRAIN Batch 13/2700 loss 11.519730 loss_att 16.065071 loss_ctc 18.143162 loss_rnnt 9.588202 hw_loss 0.261254 lr 0.00047428 rank 5
2023-02-23 03:04:49,264 DEBUG TRAIN Batch 13/2700 loss 12.566841 loss_att 16.913376 loss_ctc 10.717251 loss_rnnt 11.857899 hw_loss 0.161715 lr 0.00047425 rank 2
2023-02-23 03:04:49,266 DEBUG TRAIN Batch 13/2700 loss 25.509588 loss_att 25.119957 loss_ctc 33.073872 loss_rnnt 24.425350 hw_loss 0.287992 lr 0.00047421 rank 7
2023-02-23 03:04:49,266 DEBUG TRAIN Batch 13/2700 loss 3.828870 loss_att 7.599405 loss_ctc 3.499573 loss_rnnt 2.939437 hw_loss 0.336061 lr 0.00047433 rank 0
2023-02-23 03:04:49,269 DEBUG TRAIN Batch 13/2700 loss 26.649710 loss_att 25.875008 loss_ctc 24.965582 loss_rnnt 26.892065 hw_loss 0.257123 lr 0.00047423 rank 3
2023-02-23 03:04:49,269 DEBUG TRAIN Batch 13/2700 loss 7.386620 loss_att 10.526590 loss_ctc 8.178297 loss_rnnt 6.483553 hw_loss 0.317843 lr 0.00047431 rank 1
2023-02-23 03:04:49,312 DEBUG TRAIN Batch 13/2700 loss 5.103086 loss_att 9.777656 loss_ctc 6.069139 loss_rnnt 3.888764 hw_loss 0.282376 lr 0.00047424 rank 6
2023-02-23 03:06:07,394 DEBUG TRAIN Batch 13/2800 loss 6.967741 loss_att 10.055590 loss_ctc 8.333344 loss_rnnt 6.004431 hw_loss 0.306860 lr 0.00047406 rank 5
2023-02-23 03:06:07,396 DEBUG TRAIN Batch 13/2800 loss 12.076173 loss_att 18.205210 loss_ctc 16.211344 loss_rnnt 10.110384 hw_loss 0.353673 lr 0.00047404 rank 4
2023-02-23 03:06:07,397 DEBUG TRAIN Batch 13/2800 loss 9.418316 loss_att 14.505440 loss_ctc 11.024863 loss_rnnt 8.006182 hw_loss 0.338444 lr 0.00047402 rank 3
2023-02-23 03:06:07,401 DEBUG TRAIN Batch 13/2800 loss 17.066998 loss_att 22.647696 loss_ctc 20.967615 loss_rnnt 15.317536 hw_loss 0.212323 lr 0.00047412 rank 0
2023-02-23 03:06:07,402 DEBUG TRAIN Batch 13/2800 loss 15.369529 loss_att 17.908720 loss_ctc 17.645588 loss_rnnt 14.407301 hw_loss 0.282967 lr 0.00047403 rank 2
2023-02-23 03:06:07,407 DEBUG TRAIN Batch 13/2800 loss 7.388331 loss_att 9.121836 loss_ctc 5.970018 loss_rnnt 7.157978 hw_loss 0.136426 lr 0.00047400 rank 7
2023-02-23 03:06:07,408 DEBUG TRAIN Batch 13/2800 loss 15.593657 loss_att 18.094830 loss_ctc 27.203892 loss_rnnt 13.455658 hw_loss 0.168247 lr 0.00047410 rank 1
2023-02-23 03:06:07,410 DEBUG TRAIN Batch 13/2800 loss 8.646379 loss_att 13.809113 loss_ctc 12.431831 loss_rnnt 6.930578 hw_loss 0.334738 lr 0.00047402 rank 6
2023-02-23 03:07:25,162 DEBUG TRAIN Batch 13/2900 loss 15.766608 loss_att 21.852819 loss_ctc 21.803867 loss_rnnt 13.562819 hw_loss 0.340460 lr 0.00047382 rank 2
2023-02-23 03:07:25,163 DEBUG TRAIN Batch 13/2900 loss 12.399268 loss_att 13.681463 loss_ctc 16.778061 loss_rnnt 11.343843 hw_loss 0.403399 lr 0.00047385 rank 5
2023-02-23 03:07:25,166 DEBUG TRAIN Batch 13/2900 loss 13.631507 loss_att 19.606318 loss_ctc 16.566883 loss_rnnt 11.924004 hw_loss 0.227171 lr 0.00047380 rank 3
2023-02-23 03:07:25,167 DEBUG TRAIN Batch 13/2900 loss 11.288982 loss_att 13.367186 loss_ctc 13.547232 loss_rnnt 10.467086 hw_loss 0.197167 lr 0.00047383 rank 4
2023-02-23 03:07:25,167 DEBUG TRAIN Batch 13/2900 loss 12.627275 loss_att 17.381941 loss_ctc 15.870382 loss_rnnt 11.157523 hw_loss 0.162007 lr 0.00047389 rank 1
2023-02-23 03:07:25,167 DEBUG TRAIN Batch 13/2900 loss 14.255282 loss_att 18.539284 loss_ctc 20.684790 loss_rnnt 12.439603 hw_loss 0.190519 lr 0.00047390 rank 0
2023-02-23 03:07:25,170 DEBUG TRAIN Batch 13/2900 loss 8.042036 loss_att 11.709876 loss_ctc 9.713822 loss_rnnt 6.983899 hw_loss 0.190619 lr 0.00047378 rank 7
2023-02-23 03:07:25,213 DEBUG TRAIN Batch 13/2900 loss 23.526100 loss_att 26.799500 loss_ctc 29.905281 loss_rnnt 21.824461 hw_loss 0.368254 lr 0.00047381 rank 6
2023-02-23 03:08:41,493 DEBUG TRAIN Batch 13/3000 loss 13.316791 loss_att 16.344252 loss_ctc 17.049467 loss_rnnt 12.094330 hw_loss 0.223649 lr 0.00047364 rank 5
2023-02-23 03:08:41,501 DEBUG TRAIN Batch 13/3000 loss 17.464399 loss_att 22.296978 loss_ctc 20.319313 loss_rnnt 15.953745 hw_loss 0.306528 lr 0.00047361 rank 4
2023-02-23 03:08:41,501 DEBUG TRAIN Batch 13/3000 loss 27.815723 loss_att 29.104015 loss_ctc 35.369148 loss_rnnt 26.404522 hw_loss 0.274540 lr 0.00047359 rank 3
2023-02-23 03:08:41,502 DEBUG TRAIN Batch 13/3000 loss 12.756557 loss_att 21.392838 loss_ctc 17.391050 loss_rnnt 10.240290 hw_loss 0.320774 lr 0.00047361 rank 2
2023-02-23 03:08:41,502 DEBUG TRAIN Batch 13/3000 loss 15.270919 loss_att 21.170206 loss_ctc 21.579521 loss_rnnt 13.126024 hw_loss 0.232292 lr 0.00047360 rank 6
2023-02-23 03:08:41,503 DEBUG TRAIN Batch 13/3000 loss 15.399933 loss_att 19.571171 loss_ctc 18.690262 loss_rnnt 14.007904 hw_loss 0.223258 lr 0.00047367 rank 1
2023-02-23 03:08:41,503 DEBUG TRAIN Batch 13/3000 loss 21.770676 loss_att 29.561668 loss_ctc 23.740110 loss_rnnt 19.835955 hw_loss 0.213622 lr 0.00047369 rank 0
2023-02-23 03:08:41,505 DEBUG TRAIN Batch 13/3000 loss 13.747565 loss_att 17.074511 loss_ctc 15.587818 loss_rnnt 12.733994 hw_loss 0.192778 lr 0.00047357 rank 7
2023-02-23 03:09:58,321 DEBUG TRAIN Batch 13/3100 loss 13.057859 loss_att 12.259214 loss_ctc 16.304146 loss_rnnt 12.590917 hw_loss 0.363436 lr 0.00047348 rank 0
2023-02-23 03:09:58,321 DEBUG TRAIN Batch 13/3100 loss 20.349669 loss_att 21.963150 loss_ctc 25.353151 loss_rnnt 19.257397 hw_loss 0.192085 lr 0.00047339 rank 2
2023-02-23 03:09:58,323 DEBUG TRAIN Batch 13/3100 loss 15.384014 loss_att 17.845648 loss_ctc 24.949938 loss_rnnt 13.437216 hw_loss 0.335654 lr 0.00047340 rank 4
2023-02-23 03:09:58,327 DEBUG TRAIN Batch 13/3100 loss 21.407797 loss_att 24.093170 loss_ctc 22.768452 loss_rnnt 20.577549 hw_loss 0.209538 lr 0.00047343 rank 5
2023-02-23 03:09:58,328 DEBUG TRAIN Batch 13/3100 loss 31.636433 loss_att 35.400665 loss_ctc 44.289349 loss_rnnt 29.021267 hw_loss 0.328615 lr 0.00047346 rank 1
2023-02-23 03:09:58,329 DEBUG TRAIN Batch 13/3100 loss 8.917526 loss_att 13.118262 loss_ctc 12.640686 loss_rnnt 7.434963 hw_loss 0.273741 lr 0.00047336 rank 7
2023-02-23 03:09:58,334 DEBUG TRAIN Batch 13/3100 loss 13.287976 loss_att 19.154490 loss_ctc 19.387547 loss_rnnt 11.133680 hw_loss 0.314469 lr 0.00047339 rank 6
2023-02-23 03:09:58,336 DEBUG TRAIN Batch 13/3100 loss 10.172674 loss_att 11.793377 loss_ctc 10.180003 loss_rnnt 9.705032 hw_loss 0.267232 lr 0.00047338 rank 3
2023-02-23 03:11:17,785 DEBUG TRAIN Batch 13/3200 loss 12.210781 loss_att 12.191209 loss_ctc 12.996135 loss_rnnt 11.936274 hw_loss 0.325701 lr 0.00047317 rank 6
2023-02-23 03:11:17,786 DEBUG TRAIN Batch 13/3200 loss 16.660860 loss_att 16.370945 loss_ctc 18.922634 loss_rnnt 16.268757 hw_loss 0.278468 lr 0.00047319 rank 4
2023-02-23 03:11:17,787 DEBUG TRAIN Batch 13/3200 loss 7.807529 loss_att 10.699821 loss_ctc 10.523599 loss_rnnt 6.742973 hw_loss 0.232416 lr 0.00047321 rank 5
2023-02-23 03:11:17,787 DEBUG TRAIN Batch 13/3200 loss 10.493488 loss_att 15.504107 loss_ctc 11.012652 loss_rnnt 9.294138 hw_loss 0.240009 lr 0.00047325 rank 1
2023-02-23 03:11:17,790 DEBUG TRAIN Batch 13/3200 loss 11.173609 loss_att 10.862820 loss_ctc 13.565564 loss_rnnt 10.691916 hw_loss 0.421730 lr 0.00047318 rank 2
2023-02-23 03:11:17,790 DEBUG TRAIN Batch 13/3200 loss 7.520404 loss_att 11.942070 loss_ctc 9.212488 loss_rnnt 6.281466 hw_loss 0.241862 lr 0.00047327 rank 0
2023-02-23 03:11:17,790 DEBUG TRAIN Batch 13/3200 loss 12.940191 loss_att 13.249512 loss_ctc 17.142622 loss_rnnt 12.123783 hw_loss 0.364164 lr 0.00047315 rank 7
2023-02-23 03:11:17,792 DEBUG TRAIN Batch 13/3200 loss 10.145216 loss_att 12.567223 loss_ctc 11.738761 loss_rnnt 9.269223 hw_loss 0.335849 lr 0.00047317 rank 3
2023-02-23 03:12:33,156 DEBUG TRAIN Batch 13/3300 loss 16.313133 loss_att 21.380407 loss_ctc 21.891319 loss_rnnt 14.390265 hw_loss 0.310600 lr 0.00047300 rank 5
2023-02-23 03:12:33,158 DEBUG TRAIN Batch 13/3300 loss 8.889803 loss_att 13.310646 loss_ctc 11.750250 loss_rnnt 7.480213 hw_loss 0.270051 lr 0.00047297 rank 2
2023-02-23 03:12:33,163 DEBUG TRAIN Batch 13/3300 loss 17.857988 loss_att 26.562286 loss_ctc 23.514624 loss_rnnt 15.262131 hw_loss 0.188962 lr 0.00047306 rank 0
2023-02-23 03:12:33,164 DEBUG TRAIN Batch 13/3300 loss 6.595014 loss_att 8.091636 loss_ctc 7.077816 loss_rnnt 6.077833 hw_loss 0.287782 lr 0.00047298 rank 4
2023-02-23 03:12:33,165 DEBUG TRAIN Batch 13/3300 loss 20.678934 loss_att 27.934727 loss_ctc 27.672638 loss_rnnt 18.198795 hw_loss 0.180906 lr 0.00047296 rank 6
2023-02-23 03:12:33,166 DEBUG TRAIN Batch 13/3300 loss 6.030984 loss_att 15.421074 loss_ctc 7.626086 loss_rnnt 3.779171 hw_loss 0.302092 lr 0.00047294 rank 7
2023-02-23 03:12:33,167 DEBUG TRAIN Batch 13/3300 loss 10.835738 loss_att 12.835810 loss_ctc 12.733130 loss_rnnt 10.064238 hw_loss 0.222190 lr 0.00047295 rank 3
2023-02-23 03:12:33,168 DEBUG TRAIN Batch 13/3300 loss 34.190044 loss_att 42.118958 loss_ctc 42.824478 loss_rnnt 31.332867 hw_loss 0.225261 lr 0.00047304 rank 1
2023-02-23 03:13:48,997 DEBUG TRAIN Batch 13/3400 loss 15.533384 loss_att 19.816484 loss_ctc 18.704895 loss_rnnt 14.148599 hw_loss 0.197433 lr 0.00047279 rank 5
2023-02-23 03:13:48,998 DEBUG TRAIN Batch 13/3400 loss 16.297102 loss_att 19.658415 loss_ctc 24.142965 loss_rnnt 14.470913 hw_loss 0.202145 lr 0.00047272 rank 7
2023-02-23 03:13:48,998 DEBUG TRAIN Batch 13/3400 loss 11.468443 loss_att 14.693237 loss_ctc 13.277587 loss_rnnt 10.447598 hw_loss 0.252500 lr 0.00047276 rank 2
2023-02-23 03:13:49,000 DEBUG TRAIN Batch 13/3400 loss 8.755599 loss_att 14.192511 loss_ctc 12.373566 loss_rnnt 7.060631 hw_loss 0.234731 lr 0.00047275 rank 6
2023-02-23 03:13:49,002 DEBUG TRAIN Batch 13/3400 loss 18.913237 loss_att 23.009655 loss_ctc 27.204142 loss_rnnt 16.887001 hw_loss 0.190309 lr 0.00047284 rank 0
2023-02-23 03:13:49,005 DEBUG TRAIN Batch 13/3400 loss 7.992249 loss_att 13.360327 loss_ctc 10.527351 loss_rnnt 6.432961 hw_loss 0.276861 lr 0.00047277 rank 4
2023-02-23 03:13:49,006 DEBUG TRAIN Batch 13/3400 loss 6.532665 loss_att 10.844982 loss_ctc 8.720436 loss_rnnt 5.280365 hw_loss 0.183998 lr 0.00047283 rank 1
2023-02-23 03:13:49,007 DEBUG TRAIN Batch 13/3400 loss 9.849785 loss_att 13.813787 loss_ctc 10.075615 loss_rnnt 8.846053 hw_loss 0.339037 lr 0.00047274 rank 3
2023-02-23 03:15:06,733 DEBUG TRAIN Batch 13/3500 loss 26.286495 loss_att 27.075230 loss_ctc 37.335732 loss_rnnt 24.492092 hw_loss 0.306421 lr 0.00047258 rank 5
2023-02-23 03:15:06,734 DEBUG TRAIN Batch 13/3500 loss 8.206746 loss_att 10.825264 loss_ctc 12.171925 loss_rnnt 7.018895 hw_loss 0.253980 lr 0.00047255 rank 2
2023-02-23 03:15:06,736 DEBUG TRAIN Batch 13/3500 loss 13.344044 loss_att 16.588175 loss_ctc 16.161816 loss_rnnt 12.167093 hw_loss 0.285789 lr 0.00047251 rank 7
2023-02-23 03:15:06,736 DEBUG TRAIN Batch 13/3500 loss 12.855062 loss_att 15.868840 loss_ctc 15.625549 loss_rnnt 11.751594 hw_loss 0.246214 lr 0.00047261 rank 1
2023-02-23 03:15:06,736 DEBUG TRAIN Batch 13/3500 loss 24.495289 loss_att 22.882278 loss_ctc 27.432461 loss_rnnt 24.261454 hw_loss 0.309025 lr 0.00047253 rank 3
2023-02-23 03:15:06,737 DEBUG TRAIN Batch 13/3500 loss 14.894856 loss_att 18.598131 loss_ctc 12.330542 loss_rnnt 14.330166 hw_loss 0.311147 lr 0.00047255 rank 4
2023-02-23 03:15:06,742 DEBUG TRAIN Batch 13/3500 loss 8.393284 loss_att 11.660110 loss_ctc 10.273112 loss_rnnt 7.358463 hw_loss 0.245273 lr 0.00047263 rank 0
2023-02-23 03:15:06,757 DEBUG TRAIN Batch 13/3500 loss 31.219063 loss_att 34.643166 loss_ctc 38.694130 loss_rnnt 29.393822 hw_loss 0.269517 lr 0.00047254 rank 6
2023-02-23 03:16:25,125 DEBUG TRAIN Batch 13/3600 loss 14.620690 loss_att 15.928261 loss_ctc 14.586721 loss_rnnt 14.250237 hw_loss 0.212753 lr 0.00047234 rank 2
2023-02-23 03:16:25,126 DEBUG TRAIN Batch 13/3600 loss 11.785761 loss_att 15.246033 loss_ctc 12.030630 loss_rnnt 10.971269 hw_loss 0.168352 lr 0.00047240 rank 1
2023-02-23 03:16:25,128 DEBUG TRAIN Batch 13/3600 loss 16.275307 loss_att 17.474642 loss_ctc 21.415819 loss_rnnt 15.223944 hw_loss 0.236427 lr 0.00047242 rank 0
2023-02-23 03:16:25,128 DEBUG TRAIN Batch 13/3600 loss 12.620015 loss_att 15.214640 loss_ctc 17.719521 loss_rnnt 11.242743 hw_loss 0.334525 lr 0.00047234 rank 4
2023-02-23 03:16:25,130 DEBUG TRAIN Batch 13/3600 loss 6.223115 loss_att 11.596801 loss_ctc 8.727083 loss_rnnt 4.729456 hw_loss 0.159485 lr 0.00047230 rank 7
2023-02-23 03:16:25,134 DEBUG TRAIN Batch 13/3600 loss 11.120645 loss_att 11.936184 loss_ctc 13.018177 loss_rnnt 10.524866 hw_loss 0.336873 lr 0.00047237 rank 5
2023-02-23 03:16:25,134 DEBUG TRAIN Batch 13/3600 loss 13.895647 loss_att 19.348597 loss_ctc 13.963362 loss_rnnt 12.664850 hw_loss 0.245957 lr 0.00047232 rank 3
2023-02-23 03:16:25,134 DEBUG TRAIN Batch 13/3600 loss 15.172364 loss_att 18.665310 loss_ctc 18.592289 loss_rnnt 13.836979 hw_loss 0.339009 lr 0.00047233 rank 6
2023-02-23 03:17:40,750 DEBUG TRAIN Batch 13/3700 loss 12.516566 loss_att 13.760592 loss_ctc 12.691671 loss_rnnt 12.082594 hw_loss 0.303413 lr 0.00047211 rank 3
2023-02-23 03:17:40,752 DEBUG TRAIN Batch 13/3700 loss 33.207024 loss_att 41.453289 loss_ctc 41.544868 loss_rnnt 30.348307 hw_loss 0.183275 lr 0.00047213 rank 4
2023-02-23 03:17:40,753 DEBUG TRAIN Batch 13/3700 loss 16.048317 loss_att 18.275700 loss_ctc 15.579863 loss_rnnt 15.449878 hw_loss 0.403918 lr 0.00047216 rank 5
2023-02-23 03:17:40,757 DEBUG TRAIN Batch 13/3700 loss 10.929607 loss_att 12.448545 loss_ctc 13.074533 loss_rnnt 10.196125 hw_loss 0.269447 lr 0.00047213 rank 2
2023-02-23 03:17:40,758 DEBUG TRAIN Batch 13/3700 loss 17.397917 loss_att 19.292122 loss_ctc 23.150160 loss_rnnt 16.084866 hw_loss 0.313583 lr 0.00047212 rank 6
2023-02-23 03:17:40,758 DEBUG TRAIN Batch 13/3700 loss 8.692963 loss_att 9.107296 loss_ctc 10.857912 loss_rnnt 8.133781 hw_loss 0.351853 lr 0.00047221 rank 0
2023-02-23 03:17:40,763 DEBUG TRAIN Batch 13/3700 loss 16.536024 loss_att 21.025866 loss_ctc 25.917297 loss_rnnt 14.235987 hw_loss 0.283563 lr 0.00047219 rank 1
2023-02-23 03:17:40,764 DEBUG TRAIN Batch 13/3700 loss 8.915027 loss_att 12.131905 loss_ctc 13.157032 loss_rnnt 7.580865 hw_loss 0.234721 lr 0.00047209 rank 7
2023-02-23 03:18:56,281 DEBUG TRAIN Batch 13/3800 loss 13.923013 loss_att 18.632000 loss_ctc 21.111942 loss_rnnt 11.910197 hw_loss 0.210927 lr 0.00047195 rank 5
2023-02-23 03:18:56,283 DEBUG TRAIN Batch 13/3800 loss 10.208128 loss_att 11.535961 loss_ctc 11.556386 loss_rnnt 9.600579 hw_loss 0.304150 lr 0.00047192 rank 4
2023-02-23 03:18:56,284 DEBUG TRAIN Batch 13/3800 loss 12.525124 loss_att 13.644739 loss_ctc 14.947678 loss_rnnt 11.850836 hw_loss 0.238796 lr 0.00047188 rank 7
2023-02-23 03:18:56,284 DEBUG TRAIN Batch 13/3800 loss 11.007837 loss_att 11.480542 loss_ctc 13.900299 loss_rnnt 10.384102 hw_loss 0.269124 lr 0.00047190 rank 3
2023-02-23 03:18:56,284 DEBUG TRAIN Batch 13/3800 loss 9.420057 loss_att 9.607723 loss_ctc 10.440206 loss_rnnt 9.084919 hw_loss 0.302972 lr 0.00047192 rank 2
2023-02-23 03:18:56,285 DEBUG TRAIN Batch 13/3800 loss 14.623298 loss_att 16.009134 loss_ctc 19.338890 loss_rnnt 13.523559 hw_loss 0.363423 lr 0.00047198 rank 1
2023-02-23 03:18:56,288 DEBUG TRAIN Batch 13/3800 loss 14.972623 loss_att 18.181244 loss_ctc 17.599257 loss_rnnt 13.783378 hw_loss 0.369943 lr 0.00047191 rank 6
2023-02-23 03:18:56,289 DEBUG TRAIN Batch 13/3800 loss 13.891340 loss_att 14.979294 loss_ctc 14.895136 loss_rnnt 13.381674 hw_loss 0.296694 lr 0.00047200 rank 0
2023-02-23 03:20:15,549 DEBUG TRAIN Batch 13/3900 loss 13.895153 loss_att 21.550175 loss_ctc 22.044493 loss_rnnt 11.141311 hw_loss 0.255484 lr 0.00047179 rank 0
2023-02-23 03:20:15,551 DEBUG TRAIN Batch 13/3900 loss 11.859017 loss_att 11.694119 loss_ctc 15.075670 loss_rnnt 11.292563 hw_loss 0.319774 lr 0.00047171 rank 4
2023-02-23 03:20:15,552 DEBUG TRAIN Batch 13/3900 loss 9.365709 loss_att 13.999392 loss_ctc 11.306725 loss_rnnt 8.017115 hw_loss 0.305729 lr 0.00047169 rank 3
2023-02-23 03:20:15,552 DEBUG TRAIN Batch 13/3900 loss 12.948473 loss_att 16.089163 loss_ctc 17.231750 loss_rnnt 11.611013 hw_loss 0.259161 lr 0.00047171 rank 2
2023-02-23 03:20:15,553 DEBUG TRAIN Batch 13/3900 loss 15.780814 loss_att 21.702168 loss_ctc 20.819298 loss_rnnt 13.805450 hw_loss 0.223677 lr 0.00047174 rank 5
2023-02-23 03:20:15,559 DEBUG TRAIN Batch 13/3900 loss 16.226358 loss_att 18.763226 loss_ctc 15.874712 loss_rnnt 15.646150 hw_loss 0.224475 lr 0.00047170 rank 6
2023-02-23 03:20:15,561 DEBUG TRAIN Batch 13/3900 loss 11.641954 loss_att 18.500843 loss_ctc 10.751201 loss_rnnt 10.250964 hw_loss 0.258712 lr 0.00047177 rank 1
2023-02-23 03:20:15,663 DEBUG TRAIN Batch 13/3900 loss 26.532343 loss_att 37.358574 loss_ctc 30.584143 loss_rnnt 23.711639 hw_loss 0.216033 lr 0.00047167 rank 7
2023-02-23 03:21:32,016 DEBUG TRAIN Batch 13/4000 loss 10.360462 loss_att 20.922583 loss_ctc 15.096407 loss_rnnt 7.442853 hw_loss 0.325734 lr 0.00047153 rank 5
2023-02-23 03:21:32,017 DEBUG TRAIN Batch 13/4000 loss 7.865793 loss_att 13.765766 loss_ctc 11.099669 loss_rnnt 6.137431 hw_loss 0.219719 lr 0.00047150 rank 2
2023-02-23 03:21:32,018 DEBUG TRAIN Batch 13/4000 loss 10.523004 loss_att 14.493134 loss_ctc 13.226376 loss_rnnt 9.282988 hw_loss 0.160388 lr 0.00047158 rank 0
2023-02-23 03:21:32,019 DEBUG TRAIN Batch 13/4000 loss 18.522261 loss_att 22.445711 loss_ctc 21.438396 loss_rnnt 17.227272 hw_loss 0.227776 lr 0.00047146 rank 7
2023-02-23 03:21:32,020 DEBUG TRAIN Batch 13/4000 loss 14.619930 loss_att 19.523827 loss_ctc 16.524014 loss_rnnt 13.246524 hw_loss 0.260158 lr 0.00047156 rank 1
2023-02-23 03:21:32,025 DEBUG TRAIN Batch 13/4000 loss 16.620550 loss_att 26.758095 loss_ctc 26.537344 loss_rnnt 13.190093 hw_loss 0.151331 lr 0.00047148 rank 3
2023-02-23 03:21:32,026 DEBUG TRAIN Batch 13/4000 loss 6.554030 loss_att 12.262496 loss_ctc 9.043127 loss_rnnt 4.928808 hw_loss 0.284343 lr 0.00047150 rank 4
2023-02-23 03:21:32,031 DEBUG TRAIN Batch 13/4000 loss 12.188638 loss_att 16.879362 loss_ctc 15.589178 loss_rnnt 10.676757 hw_loss 0.225621 lr 0.00047149 rank 6
2023-02-23 03:22:47,934 DEBUG TRAIN Batch 13/4100 loss 16.587555 loss_att 21.692415 loss_ctc 17.068430 loss_rnnt 15.393763 hw_loss 0.203819 lr 0.00047129 rank 2
2023-02-23 03:22:47,940 DEBUG TRAIN Batch 13/4100 loss 14.744472 loss_att 17.074760 loss_ctc 15.406776 loss_rnnt 14.058186 hw_loss 0.247350 lr 0.00047127 rank 3
2023-02-23 03:22:47,942 DEBUG TRAIN Batch 13/4100 loss 11.015857 loss_att 14.276250 loss_ctc 15.336679 loss_rnnt 9.645443 hw_loss 0.266672 lr 0.00047132 rank 5
2023-02-23 03:22:47,941 DEBUG TRAIN Batch 13/4100 loss 20.283970 loss_att 22.386791 loss_ctc 23.925688 loss_rnnt 19.273533 hw_loss 0.195582 lr 0.00047125 rank 7
2023-02-23 03:22:47,942 DEBUG TRAIN Batch 13/4100 loss 27.021803 loss_att 25.863401 loss_ctc 33.706413 loss_rnnt 26.226551 hw_loss 0.254339 lr 0.00047135 rank 1
2023-02-23 03:22:47,943 DEBUG TRAIN Batch 13/4100 loss 11.905455 loss_att 15.717811 loss_ctc 19.914495 loss_rnnt 9.877668 hw_loss 0.370204 lr 0.00047137 rank 0
2023-02-23 03:22:47,943 DEBUG TRAIN Batch 13/4100 loss 5.940853 loss_att 7.765027 loss_ctc 7.095002 loss_rnnt 5.338777 hw_loss 0.156291 lr 0.00047129 rank 4
2023-02-23 03:22:47,994 DEBUG TRAIN Batch 13/4100 loss 15.646894 loss_att 18.097630 loss_ctc 19.369040 loss_rnnt 14.534941 hw_loss 0.235350 lr 0.00047128 rank 6
2023-02-23 03:24:04,023 DEBUG TRAIN Batch 13/4200 loss 8.963286 loss_att 12.500428 loss_ctc 9.891197 loss_rnnt 7.933122 hw_loss 0.373154 lr 0.00047111 rank 5
2023-02-23 03:24:04,026 DEBUG TRAIN Batch 13/4200 loss 18.477703 loss_att 21.446281 loss_ctc 28.036154 loss_rnnt 16.483486 hw_loss 0.236328 lr 0.00047108 rank 2
2023-02-23 03:24:04,027 DEBUG TRAIN Batch 13/4200 loss 17.778332 loss_att 19.015228 loss_ctc 20.567068 loss_rnnt 17.055805 hw_loss 0.193718 lr 0.00047107 rank 6
2023-02-23 03:24:04,027 DEBUG TRAIN Batch 13/4200 loss 9.074258 loss_att 12.528379 loss_ctc 11.871178 loss_rnnt 7.808938 hw_loss 0.377948 lr 0.00047106 rank 3
2023-02-23 03:24:04,029 DEBUG TRAIN Batch 13/4200 loss 14.472960 loss_att 18.202942 loss_ctc 18.128935 loss_rnnt 13.119711 hw_loss 0.224605 lr 0.00047116 rank 0
2023-02-23 03:24:04,028 DEBUG TRAIN Batch 13/4200 loss 13.599010 loss_att 17.081610 loss_ctc 15.903872 loss_rnnt 12.422250 hw_loss 0.324232 lr 0.00047114 rank 1
2023-02-23 03:24:04,030 DEBUG TRAIN Batch 13/4200 loss 13.713127 loss_att 16.821648 loss_ctc 15.714007 loss_rnnt 12.663475 hw_loss 0.302181 lr 0.00047108 rank 4
2023-02-23 03:24:04,032 DEBUG TRAIN Batch 13/4200 loss 31.688349 loss_att 32.735451 loss_ctc 38.430862 loss_rnnt 30.414398 hw_loss 0.310362 lr 0.00047104 rank 7
2023-02-23 03:25:21,935 DEBUG TRAIN Batch 13/4300 loss 19.200542 loss_att 22.389107 loss_ctc 22.790627 loss_rnnt 17.922646 hw_loss 0.302824 lr 0.00047086 rank 6
2023-02-23 03:25:21,936 DEBUG TRAIN Batch 13/4300 loss 9.576323 loss_att 14.490627 loss_ctc 14.073953 loss_rnnt 7.834455 hw_loss 0.298729 lr 0.00047088 rank 4
2023-02-23 03:25:21,940 DEBUG TRAIN Batch 13/4300 loss 8.313935 loss_att 8.797264 loss_ctc 10.119788 loss_rnnt 7.783446 hw_loss 0.361954 lr 0.00047090 rank 5
2023-02-23 03:25:21,943 DEBUG TRAIN Batch 13/4300 loss 10.120855 loss_att 11.578590 loss_ctc 13.530652 loss_rnnt 9.261475 hw_loss 0.212237 lr 0.00047095 rank 0
2023-02-23 03:25:21,943 DEBUG TRAIN Batch 13/4300 loss 6.855775 loss_att 7.794193 loss_ctc 8.659362 loss_rnnt 6.265644 hw_loss 0.303692 lr 0.00047087 rank 2
2023-02-23 03:25:21,942 DEBUG TRAIN Batch 13/4300 loss 14.943630 loss_att 19.065168 loss_ctc 22.185352 loss_rnnt 13.017966 hw_loss 0.254612 lr 0.00047085 rank 3
2023-02-23 03:25:21,943 DEBUG TRAIN Batch 13/4300 loss 11.864118 loss_att 14.954309 loss_ctc 15.091825 loss_rnnt 10.654215 hw_loss 0.302818 lr 0.00047083 rank 7
2023-02-23 03:25:21,946 DEBUG TRAIN Batch 13/4300 loss 12.783245 loss_att 14.565992 loss_ctc 15.660376 loss_rnnt 11.885654 hw_loss 0.295171 lr 0.00047093 rank 1
2023-02-23 03:26:39,007 DEBUG TRAIN Batch 13/4400 loss 20.984917 loss_att 23.500143 loss_ctc 26.199554 loss_rnnt 19.622652 hw_loss 0.307377 lr 0.00047067 rank 4
2023-02-23 03:26:39,010 DEBUG TRAIN Batch 13/4400 loss 14.418296 loss_att 20.049566 loss_ctc 23.794647 loss_rnnt 11.973019 hw_loss 0.129081 lr 0.00047069 rank 5
2023-02-23 03:26:39,010 DEBUG TRAIN Batch 13/4400 loss 20.763906 loss_att 22.778151 loss_ctc 23.586836 loss_rnnt 19.842497 hw_loss 0.266569 lr 0.00047066 rank 2
2023-02-23 03:26:39,011 DEBUG TRAIN Batch 13/4400 loss 13.631289 loss_att 14.537617 loss_ctc 18.057943 loss_rnnt 12.766720 hw_loss 0.174526 lr 0.00047074 rank 0
2023-02-23 03:26:39,013 DEBUG TRAIN Batch 13/4400 loss 12.171313 loss_att 14.442442 loss_ctc 15.195273 loss_rnnt 11.189818 hw_loss 0.232639 lr 0.00047073 rank 1
2023-02-23 03:26:39,015 DEBUG TRAIN Batch 13/4400 loss 12.805851 loss_att 13.311588 loss_ctc 15.077193 loss_rnnt 12.234340 hw_loss 0.314098 lr 0.00047064 rank 3
2023-02-23 03:26:39,024 DEBUG TRAIN Batch 13/4400 loss 9.712469 loss_att 13.848045 loss_ctc 14.028864 loss_rnnt 8.182135 hw_loss 0.239436 lr 0.00047065 rank 6
2023-02-23 03:26:39,026 DEBUG TRAIN Batch 13/4400 loss 21.945768 loss_att 21.573803 loss_ctc 26.913214 loss_rnnt 21.247412 hw_loss 0.207045 lr 0.00047062 rank 7
2023-02-23 03:27:55,326 DEBUG TRAIN Batch 13/4500 loss 12.255807 loss_att 14.485304 loss_ctc 16.501930 loss_rnnt 11.093791 hw_loss 0.281188 lr 0.00047046 rank 4
2023-02-23 03:27:55,327 DEBUG TRAIN Batch 13/4500 loss 5.357468 loss_att 10.577195 loss_ctc 6.953369 loss_rnnt 4.038446 hw_loss 0.116794 lr 0.00047054 rank 0
2023-02-23 03:27:55,326 DEBUG TRAIN Batch 13/4500 loss 25.005350 loss_att 30.597088 loss_ctc 25.095373 loss_rnnt 23.790800 hw_loss 0.157874 lr 0.00047042 rank 7
2023-02-23 03:27:55,327 DEBUG TRAIN Batch 13/4500 loss 5.961759 loss_att 12.064278 loss_ctc 10.109692 loss_rnnt 4.079124 hw_loss 0.204513 lr 0.00047048 rank 5
2023-02-23 03:27:55,329 DEBUG TRAIN Batch 13/4500 loss 20.030931 loss_att 22.456385 loss_ctc 26.871847 loss_rnnt 18.519756 hw_loss 0.213682 lr 0.00047045 rank 2
2023-02-23 03:27:55,329 DEBUG TRAIN Batch 13/4500 loss 13.436635 loss_att 14.751049 loss_ctc 16.817036 loss_rnnt 12.564618 hw_loss 0.297025 lr 0.00047044 rank 6
2023-02-23 03:27:55,329 DEBUG TRAIN Batch 13/4500 loss 12.929174 loss_att 15.144602 loss_ctc 13.108376 loss_rnnt 12.345042 hw_loss 0.219663 lr 0.00047052 rank 1
2023-02-23 03:27:55,374 DEBUG TRAIN Batch 13/4500 loss 12.685139 loss_att 12.432075 loss_ctc 15.586887 loss_rnnt 12.149961 hw_loss 0.372919 lr 0.00047044 rank 3
2023-02-23 03:29:12,935 DEBUG TRAIN Batch 13/4600 loss 9.319184 loss_att 13.497950 loss_ctc 13.205852 loss_rnnt 7.887072 hw_loss 0.146505 lr 0.00047028 rank 5
2023-02-23 03:29:12,939 DEBUG TRAIN Batch 13/4600 loss 13.703337 loss_att 18.354548 loss_ctc 14.431408 loss_rnnt 12.560616 hw_loss 0.216380 lr 0.00047025 rank 4
2023-02-23 03:29:12,941 DEBUG TRAIN Batch 13/4600 loss 21.553171 loss_att 27.513950 loss_ctc 27.446426 loss_rnnt 19.414534 hw_loss 0.301343 lr 0.00047033 rank 0
2023-02-23 03:29:12,944 DEBUG TRAIN Batch 13/4600 loss 17.388458 loss_att 20.497227 loss_ctc 18.609970 loss_rnnt 16.475971 hw_loss 0.239744 lr 0.00047021 rank 7
2023-02-23 03:29:12,944 DEBUG TRAIN Batch 13/4600 loss 21.067495 loss_att 22.833931 loss_ctc 24.334423 loss_rnnt 20.164009 hw_loss 0.214887 lr 0.00047023 rank 3
2023-02-23 03:29:12,944 DEBUG TRAIN Batch 13/4600 loss 12.480283 loss_att 14.084110 loss_ctc 13.403809 loss_rnnt 11.930377 hw_loss 0.198759 lr 0.00047031 rank 1
2023-02-23 03:29:12,946 DEBUG TRAIN Batch 13/4600 loss 6.112320 loss_att 11.326200 loss_ctc 10.394259 loss_rnnt 4.329435 hw_loss 0.317219 lr 0.00047024 rank 2
2023-02-23 03:29:12,945 DEBUG TRAIN Batch 13/4600 loss 8.677808 loss_att 10.408069 loss_ctc 13.821020 loss_rnnt 7.514010 hw_loss 0.247468 lr 0.00047024 rank 6
2023-02-23 03:30:33,711 DEBUG TRAIN Batch 13/4700 loss 18.279146 loss_att 24.793833 loss_ctc 24.481987 loss_rnnt 16.042862 hw_loss 0.199316 lr 0.00047012 rank 0
2023-02-23 03:30:33,712 DEBUG TRAIN Batch 13/4700 loss 13.314763 loss_att 14.941623 loss_ctc 14.683226 loss_rnnt 12.680736 hw_loss 0.236613 lr 0.00047004 rank 2
2023-02-23 03:30:33,713 DEBUG TRAIN Batch 13/4700 loss 7.975102 loss_att 14.058645 loss_ctc 9.512768 loss_rnnt 6.387876 hw_loss 0.310303 lr 0.00047007 rank 5
2023-02-23 03:30:33,714 DEBUG TRAIN Batch 13/4700 loss 25.131811 loss_att 30.840504 loss_ctc 25.042278 loss_rnnt 23.853756 hw_loss 0.277978 lr 0.00047003 rank 6
2023-02-23 03:30:33,714 DEBUG TRAIN Batch 13/4700 loss 10.064539 loss_att 14.334117 loss_ctc 13.190806 loss_rnnt 8.672807 hw_loss 0.226840 lr 0.00047004 rank 4
2023-02-23 03:30:33,719 DEBUG TRAIN Batch 13/4700 loss 21.961853 loss_att 25.641235 loss_ctc 24.565701 loss_rnnt 20.750082 hw_loss 0.241341 lr 0.00047000 rank 7
2023-02-23 03:30:33,720 DEBUG TRAIN Batch 13/4700 loss 7.291832 loss_att 10.786119 loss_ctc 9.618182 loss_rnnt 6.217442 hw_loss 0.122536 lr 0.00047002 rank 3
2023-02-23 03:30:33,725 DEBUG TRAIN Batch 13/4700 loss 11.163358 loss_att 13.678445 loss_ctc 11.771060 loss_rnnt 10.454678 hw_loss 0.233691 lr 0.00047010 rank 1
2023-02-23 03:31:49,556 DEBUG TRAIN Batch 13/4800 loss 23.981216 loss_att 29.969090 loss_ctc 29.723475 loss_rnnt 21.877487 hw_loss 0.263475 lr 0.00046983 rank 2
2023-02-23 03:31:49,556 DEBUG TRAIN Batch 13/4800 loss 10.813337 loss_att 14.748026 loss_ctc 14.432187 loss_rnnt 9.415005 hw_loss 0.241651 lr 0.00046986 rank 5
2023-02-23 03:31:49,556 DEBUG TRAIN Batch 13/4800 loss 14.743097 loss_att 21.274780 loss_ctc 19.958679 loss_rnnt 12.620312 hw_loss 0.226945 lr 0.00046983 rank 4
2023-02-23 03:31:49,559 DEBUG TRAIN Batch 13/4800 loss 25.425247 loss_att 27.006538 loss_ctc 35.388992 loss_rnnt 23.638077 hw_loss 0.267023 lr 0.00046979 rank 7
2023-02-23 03:31:49,559 DEBUG TRAIN Batch 13/4800 loss 14.329207 loss_att 15.429707 loss_ctc 17.032164 loss_rnnt 13.589071 hw_loss 0.299329 lr 0.00046991 rank 0
2023-02-23 03:31:49,561 DEBUG TRAIN Batch 13/4800 loss 28.764683 loss_att 35.081818 loss_ctc 36.174175 loss_rnnt 26.391220 hw_loss 0.228940 lr 0.00046981 rank 3
2023-02-23 03:31:49,562 DEBUG TRAIN Batch 13/4800 loss 20.103783 loss_att 20.607063 loss_ctc 24.336723 loss_rnnt 19.337902 hw_loss 0.189058 lr 0.00046982 rank 6
2023-02-23 03:31:49,564 DEBUG TRAIN Batch 13/4800 loss 22.667044 loss_att 26.037287 loss_ctc 30.265999 loss_rnnt 20.880932 hw_loss 0.185381 lr 0.00046989 rank 1
2023-02-23 03:33:05,871 DEBUG TRAIN Batch 13/4900 loss 9.387152 loss_att 11.674656 loss_ctc 11.682164 loss_rnnt 8.562095 hw_loss 0.115414 lr 0.00046962 rank 2
2023-02-23 03:33:05,874 DEBUG TRAIN Batch 13/4900 loss 8.885443 loss_att 13.020798 loss_ctc 12.689651 loss_rnnt 7.388522 hw_loss 0.304915 lr 0.00046961 rank 6
2023-02-23 03:33:05,874 DEBUG TRAIN Batch 13/4900 loss 11.266936 loss_att 14.227568 loss_ctc 14.851535 loss_rnnt 10.060534 hw_loss 0.255619 lr 0.00046959 rank 7
2023-02-23 03:33:05,876 DEBUG TRAIN Batch 13/4900 loss 16.654671 loss_att 21.022871 loss_ctc 19.162342 loss_rnnt 15.327260 hw_loss 0.223906 lr 0.00046963 rank 4
2023-02-23 03:33:05,878 DEBUG TRAIN Batch 13/4900 loss 11.334928 loss_att 13.479006 loss_ctc 12.589318 loss_rnnt 10.624846 hw_loss 0.213779 lr 0.00046970 rank 0
2023-02-23 03:33:05,879 DEBUG TRAIN Batch 13/4900 loss 17.144552 loss_att 17.965729 loss_ctc 19.433519 loss_rnnt 16.516014 hw_loss 0.298325 lr 0.00046969 rank 1
2023-02-23 03:33:05,880 DEBUG TRAIN Batch 13/4900 loss 8.511308 loss_att 9.613846 loss_ctc 10.637909 loss_rnnt 7.872662 hw_loss 0.252359 lr 0.00046960 rank 3
2023-02-23 03:33:05,881 DEBUG TRAIN Batch 13/4900 loss 6.644540 loss_att 10.051214 loss_ctc 8.247336 loss_rnnt 5.580970 hw_loss 0.315993 lr 0.00046965 rank 5
2023-02-23 03:34:24,777 DEBUG TRAIN Batch 13/5000 loss 16.647520 loss_att 17.813000 loss_ctc 18.619701 loss_rnnt 15.961220 hw_loss 0.356711 lr 0.00046945 rank 5
2023-02-23 03:34:24,777 DEBUG TRAIN Batch 13/5000 loss 10.995748 loss_att 10.631776 loss_ctc 13.694684 loss_rnnt 10.485685 hw_loss 0.418122 lr 0.00046950 rank 0
2023-02-23 03:34:24,778 DEBUG TRAIN Batch 13/5000 loss 20.481758 loss_att 25.958488 loss_ctc 26.130177 loss_rnnt 18.478378 hw_loss 0.290458 lr 0.00046948 rank 1
2023-02-23 03:34:24,779 DEBUG TRAIN Batch 13/5000 loss 13.820485 loss_att 15.283289 loss_ctc 17.297380 loss_rnnt 12.926623 hw_loss 0.258215 lr 0.00046941 rank 2
2023-02-23 03:34:24,781 DEBUG TRAIN Batch 13/5000 loss 17.137941 loss_att 21.484568 loss_ctc 20.995804 loss_rnnt 15.589338 hw_loss 0.309180 lr 0.00046941 rank 6
2023-02-23 03:34:24,781 DEBUG TRAIN Batch 13/5000 loss 11.588558 loss_att 13.436049 loss_ctc 16.239853 loss_rnnt 10.475381 hw_loss 0.231574 lr 0.00046942 rank 4
2023-02-23 03:34:24,787 DEBUG TRAIN Batch 13/5000 loss 12.679903 loss_att 18.412579 loss_ctc 19.065790 loss_rnnt 10.573531 hw_loss 0.203224 lr 0.00046938 rank 7
2023-02-23 03:34:24,832 DEBUG TRAIN Batch 13/5000 loss 16.946917 loss_att 19.607208 loss_ctc 19.527588 loss_rnnt 15.913714 hw_loss 0.294476 lr 0.00046940 rank 3
2023-02-23 03:35:40,075 DEBUG TRAIN Batch 13/5100 loss 23.179386 loss_att 23.735235 loss_ctc 31.555252 loss_rnnt 21.844397 hw_loss 0.200694 lr 0.00046924 rank 5
2023-02-23 03:35:40,080 DEBUG TRAIN Batch 13/5100 loss 14.407566 loss_att 15.050285 loss_ctc 16.701824 loss_rnnt 13.764071 hw_loss 0.391970 lr 0.00046921 rank 4
2023-02-23 03:35:40,081 DEBUG TRAIN Batch 13/5100 loss 7.120141 loss_att 8.096806 loss_ctc 10.187677 loss_rnnt 6.296521 hw_loss 0.411151 lr 0.00046917 rank 7
2023-02-23 03:35:40,087 DEBUG TRAIN Batch 13/5100 loss 9.287173 loss_att 9.377956 loss_ctc 11.316936 loss_rnnt 8.807228 hw_loss 0.358412 lr 0.00046919 rank 3
2023-02-23 03:35:40,087 DEBUG TRAIN Batch 13/5100 loss 16.618416 loss_att 17.820694 loss_ctc 23.080420 loss_rnnt 15.355546 hw_loss 0.301522 lr 0.00046921 rank 2
2023-02-23 03:35:40,088 DEBUG TRAIN Batch 13/5100 loss 8.124484 loss_att 12.242495 loss_ctc 10.187838 loss_rnnt 6.919678 hw_loss 0.198920 lr 0.00046929 rank 0
2023-02-23 03:35:40,088 DEBUG TRAIN Batch 13/5100 loss 8.942450 loss_att 13.593527 loss_ctc 10.488276 loss_rnnt 7.710388 hw_loss 0.179506 lr 0.00046927 rank 1
2023-02-23 03:35:40,135 DEBUG TRAIN Batch 13/5100 loss 18.356863 loss_att 16.770107 loss_ctc 20.332539 loss_rnnt 18.313757 hw_loss 0.181937 lr 0.00046920 rank 6
2023-02-23 03:36:54,345 DEBUG TRAIN Batch 13/5200 loss 13.445313 loss_att 19.469429 loss_ctc 17.987106 loss_rnnt 11.545650 hw_loss 0.167377 lr 0.00046903 rank 5
2023-02-23 03:36:54,359 DEBUG TRAIN Batch 13/5200 loss 7.348719 loss_att 10.640323 loss_ctc 11.428093 loss_rnnt 5.958691 hw_loss 0.352109 lr 0.00046901 rank 4
2023-02-23 03:36:54,359 DEBUG TRAIN Batch 13/5200 loss 11.385389 loss_att 17.141727 loss_ctc 12.061255 loss_rnnt 10.040064 hw_loss 0.194892 lr 0.00046908 rank 0
2023-02-23 03:36:54,362 DEBUG TRAIN Batch 13/5200 loss 15.082557 loss_att 21.897163 loss_ctc 21.984524 loss_rnnt 12.641983 hw_loss 0.295107 lr 0.00046907 rank 1
2023-02-23 03:36:54,363 DEBUG TRAIN Batch 13/5200 loss 30.663523 loss_att 37.460266 loss_ctc 40.194359 loss_rnnt 27.944729 hw_loss 0.166253 lr 0.00046898 rank 3
2023-02-23 03:36:54,362 DEBUG TRAIN Batch 13/5200 loss 11.318549 loss_att 15.647600 loss_ctc 16.512613 loss_rnnt 9.670498 hw_loss 0.168185 lr 0.00046900 rank 2
2023-02-23 03:36:54,363 DEBUG TRAIN Batch 13/5200 loss 19.182417 loss_att 21.511120 loss_ctc 18.467159 loss_rnnt 18.647919 hw_loss 0.307735 lr 0.00046899 rank 6
2023-02-23 03:36:54,366 DEBUG TRAIN Batch 13/5200 loss 16.506901 loss_att 22.841400 loss_ctc 22.842428 loss_rnnt 14.287520 hw_loss 0.202022 lr 0.00046897 rank 7
2023-02-23 03:38:12,127 DEBUG TRAIN Batch 13/5300 loss 11.892578 loss_att 12.820604 loss_ctc 21.292471 loss_rnnt 10.325300 hw_loss 0.240662 lr 0.00046879 rank 2
2023-02-23 03:38:12,131 DEBUG TRAIN Batch 13/5300 loss 27.587389 loss_att 32.061073 loss_ctc 32.694660 loss_rnnt 25.852303 hw_loss 0.298839 lr 0.00046888 rank 0
2023-02-23 03:38:12,133 DEBUG TRAIN Batch 13/5300 loss 11.932334 loss_att 13.733427 loss_ctc 16.357925 loss_rnnt 10.850079 hw_loss 0.247419 lr 0.00046883 rank 5
2023-02-23 03:38:12,135 DEBUG TRAIN Batch 13/5300 loss 14.399733 loss_att 17.051222 loss_ctc 17.200668 loss_rnnt 13.329857 hw_loss 0.311475 lr 0.00046880 rank 4
2023-02-23 03:38:12,139 DEBUG TRAIN Batch 13/5300 loss 17.616325 loss_att 16.564842 loss_ctc 26.889343 loss_rnnt 16.450642 hw_loss 0.261707 lr 0.00046879 rank 6
2023-02-23 03:38:12,140 DEBUG TRAIN Batch 13/5300 loss 14.379286 loss_att 17.082714 loss_ctc 20.533688 loss_rnnt 12.833484 hw_loss 0.345993 lr 0.00046876 rank 7
2023-02-23 03:38:12,141 DEBUG TRAIN Batch 13/5300 loss 3.403945 loss_att 7.281234 loss_ctc 3.571177 loss_rnnt 2.450600 hw_loss 0.291731 lr 0.00046878 rank 3
2023-02-23 03:38:12,141 DEBUG TRAIN Batch 13/5300 loss 19.445780 loss_att 23.672775 loss_ctc 23.736790 loss_rnnt 17.891874 hw_loss 0.255697 lr 0.00046886 rank 1
2023-02-23 03:39:29,387 DEBUG TRAIN Batch 13/5400 loss 12.842566 loss_att 16.853937 loss_ctc 21.841690 loss_rnnt 10.709047 hw_loss 0.246301 lr 0.00046860 rank 4
2023-02-23 03:39:29,390 DEBUG TRAIN Batch 13/5400 loss 9.198847 loss_att 12.084977 loss_ctc 12.495174 loss_rnnt 8.041353 hw_loss 0.263921 lr 0.00046867 rank 0
2023-02-23 03:39:29,393 DEBUG TRAIN Batch 13/5400 loss 12.975542 loss_att 12.799932 loss_ctc 14.317761 loss_rnnt 12.657339 hw_loss 0.326932 lr 0.00046862 rank 5
2023-02-23 03:39:29,394 DEBUG TRAIN Batch 13/5400 loss 18.709778 loss_att 22.266809 loss_ctc 22.902822 loss_rnnt 17.313469 hw_loss 0.235927 lr 0.00046855 rank 7
2023-02-23 03:39:29,396 DEBUG TRAIN Batch 13/5400 loss 16.585653 loss_att 17.896061 loss_ctc 22.400257 loss_rnnt 15.440351 hw_loss 0.202388 lr 0.00046857 rank 3
2023-02-23 03:39:29,397 DEBUG TRAIN Batch 13/5400 loss 8.154073 loss_att 12.277962 loss_ctc 11.345461 loss_rnnt 6.855181 hw_loss 0.091115 lr 0.00046859 rank 2
2023-02-23 03:39:29,401 DEBUG TRAIN Batch 13/5400 loss 21.933151 loss_att 26.642601 loss_ctc 29.124668 loss_rnnt 19.895969 hw_loss 0.255796 lr 0.00046865 rank 1
2023-02-23 03:39:29,402 DEBUG TRAIN Batch 13/5400 loss 8.210711 loss_att 12.757026 loss_ctc 12.173373 loss_rnnt 6.602160 hw_loss 0.320499 lr 0.00046858 rank 6
2023-02-23 03:40:44,776 DEBUG TRAIN Batch 13/5500 loss 7.331635 loss_att 9.554182 loss_ctc 8.003596 loss_rnnt 6.607458 hw_loss 0.356387 lr 0.00046839 rank 4
2023-02-23 03:40:44,778 DEBUG TRAIN Batch 13/5500 loss 28.764868 loss_att 33.018559 loss_ctc 36.964169 loss_rnnt 26.728855 hw_loss 0.172564 lr 0.00046837 rank 3
2023-02-23 03:40:44,781 DEBUG TRAIN Batch 13/5500 loss 20.766388 loss_att 22.922642 loss_ctc 23.261845 loss_rnnt 19.847507 hw_loss 0.290440 lr 0.00046838 rank 2
2023-02-23 03:40:44,781 DEBUG TRAIN Batch 13/5500 loss 13.070796 loss_att 17.894447 loss_ctc 14.511370 loss_rnnt 11.737091 hw_loss 0.331686 lr 0.00046841 rank 5
2023-02-23 03:40:44,781 DEBUG TRAIN Batch 13/5500 loss 18.249317 loss_att 23.647203 loss_ctc 22.674278 loss_rnnt 16.451311 hw_loss 0.240813 lr 0.00046835 rank 7
2023-02-23 03:40:44,781 DEBUG TRAIN Batch 13/5500 loss 10.420853 loss_att 14.333977 loss_ctc 18.333527 loss_rnnt 8.471258 hw_loss 0.209899 lr 0.00046847 rank 0
2023-02-23 03:40:44,784 DEBUG TRAIN Batch 13/5500 loss 13.200494 loss_att 15.401839 loss_ctc 15.712643 loss_rnnt 12.321974 hw_loss 0.193685 lr 0.00046838 rank 6
2023-02-23 03:40:44,787 DEBUG TRAIN Batch 13/5500 loss 9.096277 loss_att 10.646447 loss_ctc 11.618660 loss_rnnt 8.254912 hw_loss 0.365650 lr 0.00046845 rank 1
2023-02-23 03:42:00,862 DEBUG TRAIN Batch 13/5600 loss 17.720110 loss_att 24.259029 loss_ctc 23.647308 loss_rnnt 15.491728 hw_loss 0.244319 lr 0.00046818 rank 2
2023-02-23 03:42:00,867 DEBUG TRAIN Batch 13/5600 loss 15.736247 loss_att 17.988497 loss_ctc 19.993757 loss_rnnt 14.624577 hw_loss 0.175412 lr 0.00046821 rank 5
2023-02-23 03:42:00,869 DEBUG TRAIN Batch 13/5600 loss 14.317210 loss_att 15.675135 loss_ctc 18.971354 loss_rnnt 13.304697 hw_loss 0.225704 lr 0.00046816 rank 3
2023-02-23 03:42:00,869 DEBUG TRAIN Batch 13/5600 loss 8.544373 loss_att 12.047076 loss_ctc 8.104970 loss_rnnt 7.785306 hw_loss 0.219585 lr 0.00046818 rank 4
2023-02-23 03:42:00,869 DEBUG TRAIN Batch 13/5600 loss 17.197107 loss_att 22.085354 loss_ctc 22.874575 loss_rnnt 15.355167 hw_loss 0.201175 lr 0.00046814 rank 7
2023-02-23 03:42:00,870 DEBUG TRAIN Batch 13/5600 loss 18.827240 loss_att 22.104900 loss_ctc 23.168640 loss_rnnt 17.408928 hw_loss 0.344865 lr 0.00046824 rank 1
2023-02-23 03:42:00,871 DEBUG TRAIN Batch 13/5600 loss 12.566732 loss_att 14.143593 loss_ctc 16.445984 loss_rnnt 11.548607 hw_loss 0.347852 lr 0.00046826 rank 0
2023-02-23 03:42:00,874 DEBUG TRAIN Batch 13/5600 loss 14.203368 loss_att 16.040144 loss_ctc 17.189255 loss_rnnt 13.310557 hw_loss 0.238758 lr 0.00046817 rank 6
2023-02-23 03:43:20,158 DEBUG TRAIN Batch 13/5700 loss 18.015072 loss_att 25.682529 loss_ctc 25.044460 loss_rnnt 15.436034 hw_loss 0.203054 lr 0.00046800 rank 5
2023-02-23 03:43:20,162 DEBUG TRAIN Batch 13/5700 loss 7.504164 loss_att 9.339252 loss_ctc 10.687093 loss_rnnt 6.533523 hw_loss 0.336063 lr 0.00046796 rank 3
2023-02-23 03:43:20,164 DEBUG TRAIN Batch 13/5700 loss 8.507802 loss_att 13.517759 loss_ctc 13.447505 loss_rnnt 6.715568 hw_loss 0.246781 lr 0.00046805 rank 0
2023-02-23 03:43:20,166 DEBUG TRAIN Batch 13/5700 loss 10.388744 loss_att 15.958035 loss_ctc 14.486930 loss_rnnt 8.583221 hw_loss 0.272324 lr 0.00046797 rank 2
2023-02-23 03:43:20,165 DEBUG TRAIN Batch 13/5700 loss 14.093364 loss_att 17.529507 loss_ctc 17.873821 loss_rnnt 12.773497 hw_loss 0.241083 lr 0.00046798 rank 4
2023-02-23 03:43:20,165 DEBUG TRAIN Batch 13/5700 loss 14.111647 loss_att 17.837189 loss_ctc 20.548033 loss_rnnt 12.312335 hw_loss 0.367534 lr 0.00046794 rank 7
2023-02-23 03:43:20,168 DEBUG TRAIN Batch 13/5700 loss 5.188826 loss_att 8.577093 loss_ctc 5.417176 loss_rnnt 4.359880 hw_loss 0.226587 lr 0.00046796 rank 6
2023-02-23 03:43:20,169 DEBUG TRAIN Batch 13/5700 loss 13.379029 loss_att 14.567106 loss_ctc 15.608006 loss_rnnt 12.699409 hw_loss 0.271515 lr 0.00046804 rank 1
2023-02-23 03:44:36,268 DEBUG TRAIN Batch 13/5800 loss 10.451943 loss_att 11.907263 loss_ctc 10.944186 loss_rnnt 9.949178 hw_loss 0.273879 lr 0.00046780 rank 5
2023-02-23 03:44:36,272 DEBUG TRAIN Batch 13/5800 loss 12.562072 loss_att 15.279434 loss_ctc 15.566422 loss_rnnt 11.455390 hw_loss 0.304930 lr 0.00046776 rank 6
2023-02-23 03:44:36,273 DEBUG TRAIN Batch 13/5800 loss 14.508341 loss_att 14.912365 loss_ctc 18.055916 loss_rnnt 13.744297 hw_loss 0.394180 lr 0.00046777 rank 2
2023-02-23 03:44:36,274 DEBUG TRAIN Batch 13/5800 loss 25.152683 loss_att 26.275482 loss_ctc 26.047438 loss_rnnt 24.704990 hw_loss 0.194681 lr 0.00046777 rank 4
2023-02-23 03:44:36,274 DEBUG TRAIN Batch 13/5800 loss 4.042251 loss_att 7.623903 loss_ctc 5.242692 loss_rnnt 3.064116 hw_loss 0.190772 lr 0.00046785 rank 0
2023-02-23 03:44:36,276 DEBUG TRAIN Batch 13/5800 loss 22.866171 loss_att 33.662888 loss_ctc 28.008049 loss_rnnt 19.853588 hw_loss 0.314353 lr 0.00046773 rank 7
2023-02-23 03:44:36,276 DEBUG TRAIN Batch 13/5800 loss 24.775433 loss_att 26.103733 loss_ctc 35.603985 loss_rnnt 22.953991 hw_loss 0.209958 lr 0.00046783 rank 1
2023-02-23 03:44:36,280 DEBUG TRAIN Batch 13/5800 loss 14.407209 loss_att 15.736206 loss_ctc 14.569338 loss_rnnt 13.922850 hw_loss 0.369269 lr 0.00046775 rank 3
2023-02-23 03:45:51,633 DEBUG TRAIN Batch 13/5900 loss 10.352246 loss_att 15.432964 loss_ctc 14.123968 loss_rnnt 8.708769 hw_loss 0.233318 lr 0.00046757 rank 4
2023-02-23 03:45:51,635 DEBUG TRAIN Batch 13/5900 loss 18.144320 loss_att 23.213919 loss_ctc 24.019001 loss_rnnt 16.198936 hw_loss 0.277825 lr 0.00046765 rank 0
2023-02-23 03:45:51,636 DEBUG TRAIN Batch 13/5900 loss 12.860384 loss_att 13.297156 loss_ctc 13.435946 loss_rnnt 12.589442 hw_loss 0.200336 lr 0.00046759 rank 5
2023-02-23 03:45:51,636 DEBUG TRAIN Batch 13/5900 loss 20.434408 loss_att 19.558678 loss_ctc 21.008238 loss_rnnt 20.350677 hw_loss 0.341934 lr 0.00046756 rank 2
2023-02-23 03:45:51,638 DEBUG TRAIN Batch 13/5900 loss 16.125278 loss_att 15.521161 loss_ctc 17.148710 loss_rnnt 15.936488 hw_loss 0.324669 lr 0.00046753 rank 7
2023-02-23 03:45:51,639 DEBUG TRAIN Batch 13/5900 loss 20.273314 loss_att 25.787336 loss_ctc 22.851379 loss_rnnt 18.630024 hw_loss 0.368891 lr 0.00046755 rank 3
2023-02-23 03:45:51,639 DEBUG TRAIN Batch 13/5900 loss 12.372954 loss_att 13.959588 loss_ctc 12.565363 loss_rnnt 11.950653 hw_loss 0.148723 lr 0.00046756 rank 6
2023-02-23 03:45:51,639 DEBUG TRAIN Batch 13/5900 loss 9.963610 loss_att 12.032510 loss_ctc 15.492550 loss_rnnt 8.709012 hw_loss 0.194297 lr 0.00046763 rank 1
2023-02-23 03:47:09,435 DEBUG TRAIN Batch 13/6000 loss 13.953021 loss_att 16.153683 loss_ctc 18.690691 loss_rnnt 12.718174 hw_loss 0.305672 lr 0.00046744 rank 0
2023-02-23 03:47:09,437 DEBUG TRAIN Batch 13/6000 loss 13.048200 loss_att 15.414437 loss_ctc 13.391180 loss_rnnt 12.425364 hw_loss 0.194733 lr 0.00046739 rank 5
2023-02-23 03:47:09,438 DEBUG TRAIN Batch 13/6000 loss 8.881956 loss_att 10.355618 loss_ctc 9.432684 loss_rnnt 8.422148 hw_loss 0.171835 lr 0.00046742 rank 1
2023-02-23 03:47:09,439 DEBUG TRAIN Batch 13/6000 loss 10.961171 loss_att 15.179010 loss_ctc 12.864679 loss_rnnt 9.663542 hw_loss 0.375488 lr 0.00046735 rank 6
2023-02-23 03:47:09,441 DEBUG TRAIN Batch 13/6000 loss 7.432163 loss_att 11.876370 loss_ctc 8.166788 loss_rnnt 6.333248 hw_loss 0.210230 lr 0.00046737 rank 4
2023-02-23 03:47:09,442 DEBUG TRAIN Batch 13/6000 loss 7.976635 loss_att 9.924568 loss_ctc 7.621826 loss_rnnt 7.444250 hw_loss 0.356449 lr 0.00046736 rank 2
2023-02-23 03:47:09,441 DEBUG TRAIN Batch 13/6000 loss 18.975153 loss_att 19.541832 loss_ctc 21.775202 loss_rnnt 18.330267 hw_loss 0.296643 lr 0.00046734 rank 3
2023-02-23 03:47:09,445 DEBUG TRAIN Batch 13/6000 loss 11.884417 loss_att 15.575507 loss_ctc 17.175999 loss_rnnt 10.317940 hw_loss 0.230089 lr 0.00046732 rank 7
2023-02-23 03:48:27,101 DEBUG TRAIN Batch 13/6100 loss 13.613206 loss_att 19.422298 loss_ctc 19.985970 loss_rnnt 11.458227 hw_loss 0.268985 lr 0.00046724 rank 0
2023-02-23 03:48:27,106 DEBUG TRAIN Batch 13/6100 loss 10.292093 loss_att 13.086323 loss_ctc 12.985493 loss_rnnt 9.235743 hw_loss 0.259472 lr 0.00046719 rank 5
2023-02-23 03:48:27,109 DEBUG TRAIN Batch 13/6100 loss 30.247143 loss_att 31.686159 loss_ctc 39.394531 loss_rnnt 28.598770 hw_loss 0.264223 lr 0.00046716 rank 2
2023-02-23 03:48:27,109 DEBUG TRAIN Batch 13/6100 loss 13.223763 loss_att 16.133314 loss_ctc 19.708847 loss_rnnt 11.664506 hw_loss 0.211253 lr 0.00046712 rank 7
2023-02-23 03:48:27,109 DEBUG TRAIN Batch 13/6100 loss 10.634109 loss_att 13.589499 loss_ctc 14.890072 loss_rnnt 9.339519 hw_loss 0.255099 lr 0.00046715 rank 6
2023-02-23 03:48:27,109 DEBUG TRAIN Batch 13/6100 loss 14.249909 loss_att 17.964664 loss_ctc 18.891483 loss_rnnt 12.758462 hw_loss 0.243036 lr 0.00046714 rank 3
2023-02-23 03:48:27,110 DEBUG TRAIN Batch 13/6100 loss 21.222204 loss_att 26.661133 loss_ctc 25.943266 loss_rnnt 19.412052 hw_loss 0.174168 lr 0.00046716 rank 4
2023-02-23 03:48:27,116 DEBUG TRAIN Batch 13/6100 loss 9.025698 loss_att 12.343564 loss_ctc 15.921936 loss_rnnt 7.251550 hw_loss 0.358268 lr 0.00046722 rank 1
2023-02-23 03:49:42,965 DEBUG TRAIN Batch 13/6200 loss 12.367415 loss_att 13.912971 loss_ctc 15.223108 loss_rnnt 11.540085 hw_loss 0.257737 lr 0.00046694 rank 3
2023-02-23 03:49:42,967 DEBUG TRAIN Batch 13/6200 loss 13.094532 loss_att 14.817801 loss_ctc 16.947462 loss_rnnt 12.109716 hw_loss 0.237074 lr 0.00046695 rank 2
2023-02-23 03:49:42,970 DEBUG TRAIN Batch 13/6200 loss 10.530039 loss_att 12.212078 loss_ctc 12.484581 loss_rnnt 9.793654 hw_loss 0.261322 lr 0.00046698 rank 5
2023-02-23 03:49:42,973 DEBUG TRAIN Batch 13/6200 loss 16.330278 loss_att 16.420267 loss_ctc 23.632736 loss_rnnt 15.264380 hw_loss 0.139202 lr 0.00046701 rank 1
2023-02-23 03:49:42,974 DEBUG TRAIN Batch 13/6200 loss 5.936610 loss_att 7.633339 loss_ctc 8.658121 loss_rnnt 5.098605 hw_loss 0.254609 lr 0.00046703 rank 0
2023-02-23 03:49:42,976 DEBUG TRAIN Batch 13/6200 loss 9.110660 loss_att 13.319337 loss_ctc 9.648669 loss_rnnt 8.062059 hw_loss 0.253368 lr 0.00046696 rank 4
2023-02-23 03:49:42,976 DEBUG TRAIN Batch 13/6200 loss 8.839000 loss_att 15.008978 loss_ctc 12.766396 loss_rnnt 6.948616 hw_loss 0.248877 lr 0.00046694 rank 6
2023-02-23 03:49:42,977 DEBUG TRAIN Batch 13/6200 loss 7.008874 loss_att 9.524240 loss_ctc 9.353312 loss_rnnt 6.061202 hw_loss 0.247513 lr 0.00046692 rank 7
2023-02-23 03:50:58,121 DEBUG TRAIN Batch 13/6300 loss 8.240305 loss_att 12.389022 loss_ctc 8.104635 loss_rnnt 7.303529 hw_loss 0.234604 lr 0.00046683 rank 0
2023-02-23 03:50:58,121 DEBUG TRAIN Batch 13/6300 loss 10.878992 loss_att 15.440205 loss_ctc 11.940576 loss_rnnt 9.712387 hw_loss 0.211535 lr 0.00046675 rank 2
2023-02-23 03:50:58,125 DEBUG TRAIN Batch 13/6300 loss 31.746115 loss_att 31.731178 loss_ctc 33.176605 loss_rnnt 31.390738 hw_loss 0.314305 lr 0.00046674 rank 6
2023-02-23 03:50:58,126 DEBUG TRAIN Batch 13/6300 loss 11.314080 loss_att 16.044830 loss_ctc 11.853321 loss_rnnt 10.194410 hw_loss 0.190540 lr 0.00046678 rank 5
2023-02-23 03:50:58,128 DEBUG TRAIN Batch 13/6300 loss 18.848940 loss_att 21.843021 loss_ctc 27.591919 loss_rnnt 16.947540 hw_loss 0.256599 lr 0.00046673 rank 3
2023-02-23 03:50:58,130 DEBUG TRAIN Batch 13/6300 loss 9.598382 loss_att 11.175110 loss_ctc 13.310033 loss_rnnt 8.603943 hw_loss 0.345387 lr 0.00046675 rank 4
2023-02-23 03:50:58,133 DEBUG TRAIN Batch 13/6300 loss 24.307274 loss_att 25.129520 loss_ctc 32.801327 loss_rnnt 22.882944 hw_loss 0.238763 lr 0.00046671 rank 7
2023-02-23 03:50:58,135 DEBUG TRAIN Batch 13/6300 loss 33.646942 loss_att 37.641983 loss_ctc 38.425522 loss_rnnt 32.099442 hw_loss 0.208781 lr 0.00046681 rank 1
2023-02-23 03:52:18,014 DEBUG TRAIN Batch 13/6400 loss 4.093378 loss_att 8.668386 loss_ctc 6.075437 loss_rnnt 2.746374 hw_loss 0.314491 lr 0.00046663 rank 0
2023-02-23 03:52:18,013 DEBUG TRAIN Batch 13/6400 loss 11.263141 loss_att 20.147778 loss_ctc 12.337883 loss_rnnt 9.192749 hw_loss 0.281560 lr 0.00046655 rank 4
2023-02-23 03:52:18,013 DEBUG TRAIN Batch 13/6400 loss 9.722460 loss_att 12.002396 loss_ctc 11.924530 loss_rnnt 8.750414 hw_loss 0.417092 lr 0.00046654 rank 2
2023-02-23 03:52:18,015 DEBUG TRAIN Batch 13/6400 loss 10.373398 loss_att 10.446864 loss_ctc 15.336584 loss_rnnt 9.495403 hw_loss 0.377894 lr 0.00046651 rank 7
2023-02-23 03:52:18,016 DEBUG TRAIN Batch 13/6400 loss 16.327421 loss_att 18.673206 loss_ctc 21.590986 loss_rnnt 15.033001 hw_loss 0.231475 lr 0.00046661 rank 1
2023-02-23 03:52:18,020 DEBUG TRAIN Batch 13/6400 loss 16.843815 loss_att 21.698483 loss_ctc 22.438099 loss_rnnt 14.988563 hw_loss 0.259528 lr 0.00046654 rank 6
2023-02-23 03:52:18,020 DEBUG TRAIN Batch 13/6400 loss 9.309770 loss_att 15.334326 loss_ctc 10.889174 loss_rnnt 7.719484 hw_loss 0.327726 lr 0.00046653 rank 3
2023-02-23 03:52:18,065 DEBUG TRAIN Batch 13/6400 loss 11.575352 loss_att 15.549496 loss_ctc 16.526930 loss_rnnt 9.987035 hw_loss 0.249895 lr 0.00046658 rank 5
2023-02-23 03:53:34,318 DEBUG TRAIN Batch 13/6500 loss 15.876519 loss_att 17.134373 loss_ctc 17.766716 loss_rnnt 15.237389 hw_loss 0.254125 lr 0.00046637 rank 5
2023-02-23 03:53:34,322 DEBUG TRAIN Batch 13/6500 loss 8.075416 loss_att 9.903269 loss_ctc 11.226450 loss_rnnt 7.188457 hw_loss 0.189843 lr 0.00046633 rank 3
2023-02-23 03:53:34,324 DEBUG TRAIN Batch 13/6500 loss 5.254296 loss_att 6.826451 loss_ctc 8.874481 loss_rnnt 4.357769 hw_loss 0.186385 lr 0.00046635 rank 4
2023-02-23 03:53:34,326 DEBUG TRAIN Batch 13/6500 loss 13.462296 loss_att 16.519302 loss_ctc 13.833240 loss_rnnt 12.654909 hw_loss 0.274733 lr 0.00046640 rank 1
2023-02-23 03:53:34,326 DEBUG TRAIN Batch 13/6500 loss 25.029501 loss_att 30.778191 loss_ctc 27.088814 loss_rnnt 23.479637 hw_loss 0.235409 lr 0.00046634 rank 2
2023-02-23 03:53:34,327 DEBUG TRAIN Batch 13/6500 loss 6.765990 loss_att 10.239410 loss_ctc 7.998443 loss_rnnt 5.722448 hw_loss 0.345996 lr 0.00046642 rank 0
2023-02-23 03:53:34,333 DEBUG TRAIN Batch 13/6500 loss 13.990358 loss_att 16.861914 loss_ctc 17.710316 loss_rnnt 12.784670 hw_loss 0.253842 lr 0.00046633 rank 6
2023-02-23 03:53:34,647 DEBUG TRAIN Batch 13/6500 loss 25.502813 loss_att 27.060368 loss_ctc 31.370564 loss_rnnt 24.300766 hw_loss 0.202819 lr 0.00046631 rank 7
2023-02-23 03:54:51,082 DEBUG TRAIN Batch 13/6600 loss 17.848459 loss_att 21.580547 loss_ctc 25.469305 loss_rnnt 15.958063 hw_loss 0.239750 lr 0.00046612 rank 3
2023-02-23 03:54:51,083 DEBUG TRAIN Batch 13/6600 loss 27.425606 loss_att 29.825891 loss_ctc 34.838253 loss_rnnt 25.736898 hw_loss 0.413054 lr 0.00046613 rank 6
2023-02-23 03:54:51,087 DEBUG TRAIN Batch 13/6600 loss 24.845831 loss_att 25.710999 loss_ctc 34.121532 loss_rnnt 23.266529 hw_loss 0.317827 lr 0.00046617 rank 5
2023-02-23 03:54:51,088 DEBUG TRAIN Batch 13/6600 loss 7.566367 loss_att 12.304836 loss_ctc 10.696434 loss_rnnt 6.077192 hw_loss 0.232760 lr 0.00046614 rank 2
2023-02-23 03:54:51,089 DEBUG TRAIN Batch 13/6600 loss 8.944123 loss_att 15.051958 loss_ctc 10.897337 loss_rnnt 7.323165 hw_loss 0.260555 lr 0.00046622 rank 0
2023-02-23 03:54:51,090 DEBUG TRAIN Batch 13/6600 loss 30.584671 loss_att 34.064857 loss_ctc 41.201401 loss_rnnt 28.296604 hw_loss 0.330868 lr 0.00046615 rank 4
2023-02-23 03:54:51,091 DEBUG TRAIN Batch 13/6600 loss 6.466137 loss_att 11.477859 loss_ctc 9.042857 loss_rnnt 4.923858 hw_loss 0.368198 lr 0.00046620 rank 1
2023-02-23 03:54:51,091 DEBUG TRAIN Batch 13/6600 loss 10.828430 loss_att 13.921230 loss_ctc 10.038552 loss_rnnt 10.219629 hw_loss 0.179171 lr 0.00046610 rank 7
2023-02-23 03:56:08,276 DEBUG TRAIN Batch 13/6700 loss 12.750506 loss_att 16.614124 loss_ctc 12.362997 loss_rnnt 11.906286 hw_loss 0.230936 lr 0.00046594 rank 4
2023-02-23 03:56:08,276 DEBUG TRAIN Batch 13/6700 loss 19.732674 loss_att 23.623758 loss_ctc 25.214157 loss_rnnt 18.109474 hw_loss 0.213971 lr 0.00046602 rank 0
2023-02-23 03:56:08,278 DEBUG TRAIN Batch 13/6700 loss 20.341457 loss_att 22.153179 loss_ctc 24.329336 loss_rnnt 19.364761 hw_loss 0.154941 lr 0.00046593 rank 6
2023-02-23 03:56:08,280 DEBUG TRAIN Batch 13/6700 loss 13.140285 loss_att 17.690611 loss_ctc 19.768127 loss_rnnt 11.215335 hw_loss 0.245953 lr 0.00046600 rank 1
2023-02-23 03:56:08,280 DEBUG TRAIN Batch 13/6700 loss 7.648363 loss_att 12.249444 loss_ctc 10.064393 loss_rnnt 6.302502 hw_loss 0.194076 lr 0.00046592 rank 3
2023-02-23 03:56:08,282 DEBUG TRAIN Batch 13/6700 loss 11.160748 loss_att 14.263531 loss_ctc 14.459996 loss_rnnt 9.982496 hw_loss 0.220867 lr 0.00046597 rank 5
2023-02-23 03:56:08,284 DEBUG TRAIN Batch 13/6700 loss 5.576385 loss_att 9.266150 loss_ctc 7.139539 loss_rnnt 4.558860 hw_loss 0.133408 lr 0.00046594 rank 2
2023-02-23 03:56:08,284 DEBUG TRAIN Batch 13/6700 loss 13.261649 loss_att 15.138953 loss_ctc 15.589658 loss_rnnt 12.424755 hw_loss 0.283187 lr 0.00046590 rank 7
2023-02-23 03:57:25,651 DEBUG TRAIN Batch 13/6800 loss 9.091522 loss_att 14.316631 loss_ctc 10.770287 loss_rnnt 7.695274 hw_loss 0.238857 lr 0.00046573 rank 2
2023-02-23 03:57:25,652 DEBUG TRAIN Batch 13/6800 loss 10.354070 loss_att 12.527290 loss_ctc 11.928616 loss_rnnt 9.571777 hw_loss 0.258205 lr 0.00046582 rank 0
2023-02-23 03:57:25,656 DEBUG TRAIN Batch 13/6800 loss 16.047714 loss_att 19.854076 loss_ctc 22.707752 loss_rnnt 14.256465 hw_loss 0.266201 lr 0.00046573 rank 6
2023-02-23 03:57:25,656 DEBUG TRAIN Batch 13/6800 loss 7.359859 loss_att 10.863327 loss_ctc 9.663906 loss_rnnt 6.209427 hw_loss 0.267248 lr 0.00046574 rank 4
2023-02-23 03:57:25,658 DEBUG TRAIN Batch 13/6800 loss 9.556656 loss_att 12.061722 loss_ctc 12.045277 loss_rnnt 8.650736 hw_loss 0.137046 lr 0.00046572 rank 3
2023-02-23 03:57:25,659 DEBUG TRAIN Batch 13/6800 loss 15.719846 loss_att 20.128963 loss_ctc 19.302687 loss_rnnt 14.207908 hw_loss 0.285755 lr 0.00046576 rank 5
2023-02-23 03:57:25,661 DEBUG TRAIN Batch 13/6800 loss 9.001116 loss_att 10.119362 loss_ctc 11.266387 loss_rnnt 8.341421 hw_loss 0.251266 lr 0.00046580 rank 1
2023-02-23 03:57:25,661 DEBUG TRAIN Batch 13/6800 loss 12.614736 loss_att 18.345169 loss_ctc 17.005457 loss_rnnt 10.704124 hw_loss 0.335802 lr 0.00046570 rank 7
2023-02-23 03:58:42,055 DEBUG TRAIN Batch 13/6900 loss 9.367407 loss_att 12.277064 loss_ctc 14.475575 loss_rnnt 7.980461 hw_loss 0.232360 lr 0.00046553 rank 2
2023-02-23 03:58:42,057 DEBUG TRAIN Batch 13/6900 loss 13.503225 loss_att 14.516907 loss_ctc 18.274336 loss_rnnt 12.498273 hw_loss 0.311377 lr 0.00046561 rank 0
2023-02-23 03:58:42,059 DEBUG TRAIN Batch 13/6900 loss 21.134893 loss_att 26.106922 loss_ctc 27.562803 loss_rnnt 19.167128 hw_loss 0.218072 lr 0.00046554 rank 4
2023-02-23 03:58:42,059 DEBUG TRAIN Batch 13/6900 loss 13.861250 loss_att 13.818589 loss_ctc 13.663074 loss_rnnt 13.826515 hw_loss 0.130669 lr 0.00046556 rank 5
2023-02-23 03:58:42,061 DEBUG TRAIN Batch 13/6900 loss 9.633554 loss_att 13.865422 loss_ctc 10.120147 loss_rnnt 8.626043 hw_loss 0.180482 lr 0.00046559 rank 1
2023-02-23 03:58:42,063 DEBUG TRAIN Batch 13/6900 loss 16.968481 loss_att 17.866722 loss_ctc 19.807323 loss_rnnt 16.286001 hw_loss 0.233101 lr 0.00046550 rank 7
2023-02-23 03:58:42,064 DEBUG TRAIN Batch 13/6900 loss 16.454601 loss_att 17.942987 loss_ctc 15.470133 loss_rnnt 16.185080 hw_loss 0.193327 lr 0.00046552 rank 6
2023-02-23 03:58:42,106 DEBUG TRAIN Batch 13/6900 loss 17.169649 loss_att 17.619080 loss_ctc 19.977154 loss_rnnt 16.563770 hw_loss 0.265610 lr 0.00046552 rank 3
2023-02-23 03:59:59,796 DEBUG TRAIN Batch 13/7000 loss 20.651262 loss_att 20.713032 loss_ctc 23.522356 loss_rnnt 20.122704 hw_loss 0.250109 lr 0.00046534 rank 4
2023-02-23 03:59:59,796 DEBUG TRAIN Batch 13/7000 loss 12.270441 loss_att 18.428921 loss_ctc 16.812071 loss_rnnt 10.374988 hw_loss 0.109138 lr 0.00046536 rank 5
2023-02-23 03:59:59,797 DEBUG TRAIN Batch 13/7000 loss 12.169607 loss_att 13.483892 loss_ctc 15.708883 loss_rnnt 11.282405 hw_loss 0.285827 lr 0.00046531 rank 3
2023-02-23 03:59:59,797 DEBUG TRAIN Batch 13/7000 loss 12.625853 loss_att 16.645782 loss_ctc 14.552073 loss_rnnt 11.452713 hw_loss 0.210607 lr 0.00046541 rank 0
2023-02-23 03:59:59,797 DEBUG TRAIN Batch 13/7000 loss 14.273505 loss_att 17.783947 loss_ctc 18.112272 loss_rnnt 12.865651 hw_loss 0.363618 lr 0.00046539 rank 1
2023-02-23 03:59:59,798 DEBUG TRAIN Batch 13/7000 loss 13.445722 loss_att 15.749229 loss_ctc 18.530251 loss_rnnt 12.198034 hw_loss 0.204467 lr 0.00046533 rank 2
2023-02-23 03:59:59,801 DEBUG TRAIN Batch 13/7000 loss 11.272593 loss_att 15.834135 loss_ctc 14.261036 loss_rnnt 9.782446 hw_loss 0.336339 lr 0.00046532 rank 6
2023-02-23 03:59:59,807 DEBUG TRAIN Batch 13/7000 loss 9.605996 loss_att 12.277489 loss_ctc 13.537431 loss_rnnt 8.316555 hw_loss 0.433032 lr 0.00046530 rank 7
2023-02-23 04:01:18,540 DEBUG TRAIN Batch 13/7100 loss 23.388641 loss_att 31.200275 loss_ctc 31.551132 loss_rnnt 20.634871 hw_loss 0.193338 lr 0.00046513 rank 2
2023-02-23 04:01:18,540 DEBUG TRAIN Batch 13/7100 loss 6.016028 loss_att 10.712464 loss_ctc 6.931493 loss_rnnt 4.863816 hw_loss 0.170366 lr 0.00046521 rank 0
2023-02-23 04:01:18,541 DEBUG TRAIN Batch 13/7100 loss 10.118042 loss_att 11.644788 loss_ctc 11.390239 loss_rnnt 9.479755 hw_loss 0.306208 lr 0.00046512 rank 6
2023-02-23 04:01:18,542 DEBUG TRAIN Batch 13/7100 loss 10.039986 loss_att 15.552184 loss_ctc 14.757159 loss_rnnt 8.152384 hw_loss 0.292885 lr 0.00046516 rank 5
2023-02-23 04:01:18,542 DEBUG TRAIN Batch 13/7100 loss 12.412462 loss_att 16.571209 loss_ctc 20.214520 loss_rnnt 10.413222 hw_loss 0.238530 lr 0.00046510 rank 7
2023-02-23 04:01:18,543 DEBUG TRAIN Batch 13/7100 loss 7.436122 loss_att 10.860542 loss_ctc 10.690608 loss_rnnt 6.176820 hw_loss 0.263413 lr 0.00046514 rank 4
2023-02-23 04:01:18,545 DEBUG TRAIN Batch 13/7100 loss 16.808573 loss_att 17.955750 loss_ctc 26.542633 loss_rnnt 15.139304 hw_loss 0.266170 lr 0.00046511 rank 3
2023-02-23 04:01:18,557 DEBUG TRAIN Batch 13/7100 loss 11.085172 loss_att 17.119865 loss_ctc 17.641846 loss_rnnt 8.898145 hw_loss 0.198495 lr 0.00046519 rank 1
2023-02-23 04:02:35,568 DEBUG TRAIN Batch 13/7200 loss 16.147747 loss_att 17.923958 loss_ctc 19.134483 loss_rnnt 15.226337 hw_loss 0.314877 lr 0.00046496 rank 5
2023-02-23 04:02:35,570 DEBUG TRAIN Batch 13/7200 loss 11.925421 loss_att 15.851114 loss_ctc 18.750523 loss_rnnt 10.055273 hw_loss 0.328116 lr 0.00046493 rank 4
2023-02-23 04:02:35,570 DEBUG TRAIN Batch 13/7200 loss 10.085607 loss_att 14.245892 loss_ctc 21.150640 loss_rnnt 7.579767 hw_loss 0.372083 lr 0.00046493 rank 2
2023-02-23 04:02:35,579 DEBUG TRAIN Batch 13/7200 loss 14.317893 loss_att 22.234791 loss_ctc 23.977230 loss_rnnt 11.350496 hw_loss 0.180196 lr 0.00046499 rank 1
2023-02-23 04:02:35,580 DEBUG TRAIN Batch 13/7200 loss 14.619231 loss_att 20.970417 loss_ctc 25.707563 loss_rnnt 11.717728 hw_loss 0.286539 lr 0.00046501 rank 0
2023-02-23 04:02:35,581 DEBUG TRAIN Batch 13/7200 loss 8.878839 loss_att 11.986698 loss_ctc 8.589500 loss_rnnt 8.124251 hw_loss 0.321742 lr 0.00046491 rank 3
2023-02-23 04:02:35,583 DEBUG TRAIN Batch 13/7200 loss 12.475940 loss_att 15.892687 loss_ctc 13.816969 loss_rnnt 11.484533 hw_loss 0.242351 lr 0.00046489 rank 7
2023-02-23 04:02:35,622 DEBUG TRAIN Batch 13/7200 loss 14.719841 loss_att 18.853292 loss_ctc 19.362659 loss_rnnt 13.176513 hw_loss 0.182990 lr 0.00046492 rank 6
2023-02-23 04:03:52,830 DEBUG TRAIN Batch 13/7300 loss 16.340668 loss_att 21.952278 loss_ctc 24.042778 loss_rnnt 14.092006 hw_loss 0.186360 lr 0.00046473 rank 2
2023-02-23 04:03:52,831 DEBUG TRAIN Batch 13/7300 loss 6.814116 loss_att 11.891003 loss_ctc 7.966722 loss_rnnt 5.468407 hw_loss 0.331222 lr 0.00046479 rank 1
2023-02-23 04:03:52,833 DEBUG TRAIN Batch 13/7300 loss 15.178806 loss_att 19.647537 loss_ctc 19.469517 loss_rnnt 13.537893 hw_loss 0.328260 lr 0.00046469 rank 7
2023-02-23 04:03:52,833 DEBUG TRAIN Batch 13/7300 loss 22.657366 loss_att 22.169085 loss_ctc 29.488480 loss_rnnt 21.739384 hw_loss 0.196539 lr 0.00046476 rank 5
2023-02-23 04:03:52,838 DEBUG TRAIN Batch 13/7300 loss 10.947793 loss_att 15.099159 loss_ctc 16.201328 loss_rnnt 9.350475 hw_loss 0.124825 lr 0.00046473 rank 4
2023-02-23 04:03:52,838 DEBUG TRAIN Batch 13/7300 loss 15.276589 loss_att 16.077133 loss_ctc 18.919073 loss_rnnt 14.485054 hw_loss 0.273302 lr 0.00046471 rank 3
2023-02-23 04:03:52,838 DEBUG TRAIN Batch 13/7300 loss 19.231661 loss_att 18.810158 loss_ctc 21.216045 loss_rnnt 18.893509 hw_loss 0.296002 lr 0.00046472 rank 6
2023-02-23 04:03:52,839 DEBUG TRAIN Batch 13/7300 loss 12.219332 loss_att 15.783123 loss_ctc 14.246902 loss_rnnt 11.110344 hw_loss 0.236036 lr 0.00046481 rank 0
2023-02-23 04:05:11,350 DEBUG TRAIN Batch 13/7400 loss 5.727881 loss_att 8.981883 loss_ctc 7.753888 loss_rnnt 4.685902 hw_loss 0.226958 lr 0.00046453 rank 2
2023-02-23 04:05:11,353 DEBUG TRAIN Batch 13/7400 loss 5.021026 loss_att 6.538357 loss_ctc 6.922178 loss_rnnt 4.341248 hw_loss 0.230298 lr 0.00046451 rank 3
2023-02-23 04:05:11,358 DEBUG TRAIN Batch 13/7400 loss 7.198505 loss_att 11.202006 loss_ctc 11.799557 loss_rnnt 5.626591 hw_loss 0.295762 lr 0.00046461 rank 0
2023-02-23 04:05:11,359 DEBUG TRAIN Batch 13/7400 loss 5.601280 loss_att 11.128031 loss_ctc 11.268634 loss_rnnt 3.559018 hw_loss 0.339870 lr 0.00046452 rank 6
2023-02-23 04:05:11,359 DEBUG TRAIN Batch 13/7400 loss 11.673483 loss_att 16.040691 loss_ctc 15.160288 loss_rnnt 10.184877 hw_loss 0.281730 lr 0.00046456 rank 5
2023-02-23 04:05:11,367 DEBUG TRAIN Batch 13/7400 loss 3.339323 loss_att 8.236115 loss_ctc 5.688370 loss_rnnt 1.936395 hw_loss 0.206930 lr 0.00046449 rank 7
2023-02-23 04:05:11,372 DEBUG TRAIN Batch 13/7400 loss 12.053250 loss_att 16.731781 loss_ctc 15.092480 loss_rnnt 10.571415 hw_loss 0.264183 lr 0.00046453 rank 4
2023-02-23 04:05:11,408 DEBUG TRAIN Batch 13/7400 loss 13.788294 loss_att 18.021381 loss_ctc 20.122288 loss_rnnt 11.984359 hw_loss 0.211473 lr 0.00046459 rank 1
2023-02-23 04:06:31,046 DEBUG TRAIN Batch 13/7500 loss 7.044302 loss_att 10.516267 loss_ctc 9.464951 loss_rnnt 5.863118 hw_loss 0.307571 lr 0.00046431 rank 3
2023-02-23 04:06:31,048 DEBUG TRAIN Batch 13/7500 loss 15.938551 loss_att 16.724571 loss_ctc 19.774641 loss_rnnt 15.102977 hw_loss 0.312922 lr 0.00046441 rank 0
2023-02-23 04:06:31,048 DEBUG TRAIN Batch 13/7500 loss 13.817408 loss_att 17.425745 loss_ctc 17.595364 loss_rnnt 12.507154 hw_loss 0.159109 lr 0.00046433 rank 2
2023-02-23 04:06:31,048 DEBUG TRAIN Batch 13/7500 loss 11.509171 loss_att 12.906527 loss_ctc 12.471301 loss_rnnt 10.955192 hw_loss 0.274170 lr 0.00046436 rank 5
2023-02-23 04:06:31,055 DEBUG TRAIN Batch 13/7500 loss 25.259865 loss_att 26.727489 loss_ctc 38.222603 loss_rnnt 23.065960 hw_loss 0.322531 lr 0.00046433 rank 4
2023-02-23 04:06:31,056 DEBUG TRAIN Batch 13/7500 loss 11.108286 loss_att 14.325588 loss_ctc 14.805016 loss_rnnt 9.840920 hw_loss 0.245639 lr 0.00046439 rank 1
2023-02-23 04:06:31,056 DEBUG TRAIN Batch 13/7500 loss 17.753384 loss_att 21.256441 loss_ctc 32.021927 loss_rnnt 14.991529 hw_loss 0.297695 lr 0.00046432 rank 6
2023-02-23 04:06:31,058 DEBUG TRAIN Batch 13/7500 loss 13.620666 loss_att 14.510118 loss_ctc 18.229116 loss_rnnt 12.696845 hw_loss 0.246509 lr 0.00046429 rank 7
2023-02-23 04:07:47,774 DEBUG TRAIN Batch 13/7600 loss 17.631777 loss_att 21.800985 loss_ctc 23.497953 loss_rnnt 15.907369 hw_loss 0.203266 lr 0.00046413 rank 2
2023-02-23 04:07:47,779 DEBUG TRAIN Batch 13/7600 loss 13.659563 loss_att 16.862133 loss_ctc 17.188606 loss_rnnt 12.429149 hw_loss 0.223803 lr 0.00046421 rank 0
2023-02-23 04:07:47,780 DEBUG TRAIN Batch 13/7600 loss 7.720867 loss_att 11.120714 loss_ctc 12.545485 loss_rnnt 6.241446 hw_loss 0.292817 lr 0.00046416 rank 5
2023-02-23 04:07:47,781 DEBUG TRAIN Batch 13/7600 loss 9.590683 loss_att 12.691187 loss_ctc 13.784876 loss_rnnt 8.302027 hw_loss 0.204992 lr 0.00046419 rank 1
2023-02-23 04:07:47,781 DEBUG TRAIN Batch 13/7600 loss 11.265043 loss_att 14.589963 loss_ctc 15.448080 loss_rnnt 9.886177 hw_loss 0.292770 lr 0.00046413 rank 4
2023-02-23 04:07:47,782 DEBUG TRAIN Batch 13/7600 loss 10.246887 loss_att 10.674937 loss_ctc 13.995803 loss_rnnt 9.508921 hw_loss 0.285939 lr 0.00046409 rank 7
2023-02-23 04:07:47,783 DEBUG TRAIN Batch 13/7600 loss 12.302520 loss_att 15.568304 loss_ctc 17.615349 loss_rnnt 10.777189 hw_loss 0.307118 lr 0.00046412 rank 6
2023-02-23 04:07:47,789 DEBUG TRAIN Batch 13/7600 loss 19.326218 loss_att 20.608482 loss_ctc 26.620655 loss_rnnt 17.971189 hw_loss 0.236222 lr 0.00046411 rank 3
2023-02-23 04:09:04,384 DEBUG TRAIN Batch 13/7700 loss 15.129901 loss_att 19.976067 loss_ctc 19.140612 loss_rnnt 13.507105 hw_loss 0.222751 lr 0.00046393 rank 4
2023-02-23 04:09:04,387 DEBUG TRAIN Batch 13/7700 loss 13.183238 loss_att 17.236233 loss_ctc 19.512423 loss_rnnt 11.378870 hw_loss 0.281018 lr 0.00046392 rank 6
2023-02-23 04:09:04,389 DEBUG TRAIN Batch 13/7700 loss 6.708461 loss_att 15.698467 loss_ctc 11.725574 loss_rnnt 4.129334 hw_loss 0.210332 lr 0.00046389 rank 7
2023-02-23 04:09:04,390 DEBUG TRAIN Batch 13/7700 loss 24.475733 loss_att 25.339066 loss_ctc 25.998196 loss_rnnt 23.988689 hw_loss 0.208844 lr 0.00046396 rank 5
2023-02-23 04:09:04,391 DEBUG TRAIN Batch 13/7700 loss 15.064191 loss_att 14.737409 loss_ctc 18.511753 loss_rnnt 14.493675 hw_loss 0.330368 lr 0.00046393 rank 2
2023-02-23 04:09:04,396 DEBUG TRAIN Batch 13/7700 loss 16.715689 loss_att 17.682463 loss_ctc 20.599678 loss_rnnt 15.886909 hw_loss 0.220426 lr 0.00046401 rank 0
2023-02-23 04:09:04,397 DEBUG TRAIN Batch 13/7700 loss 10.811773 loss_att 11.138052 loss_ctc 12.971426 loss_rnnt 10.251108 hw_loss 0.388981 lr 0.00046399 rank 1
2023-02-23 04:09:04,440 DEBUG TRAIN Batch 13/7700 loss 19.135778 loss_att 20.632452 loss_ctc 19.673481 loss_rnnt 18.633331 hw_loss 0.246410 lr 0.00046391 rank 3
2023-02-23 04:10:22,533 DEBUG TRAIN Batch 13/7800 loss 10.309756 loss_att 14.287947 loss_ctc 9.935950 loss_rnnt 9.431288 hw_loss 0.248759 lr 0.00046369 rank 7
2023-02-23 04:10:22,534 DEBUG TRAIN Batch 13/7800 loss 9.502392 loss_att 13.286692 loss_ctc 9.865826 loss_rnnt 8.626583 hw_loss 0.132171 lr 0.00046373 rank 2
2023-02-23 04:10:22,536 DEBUG TRAIN Batch 13/7800 loss 13.649140 loss_att 18.729078 loss_ctc 17.255789 loss_rnnt 12.022216 hw_loss 0.243843 lr 0.00046376 rank 5
2023-02-23 04:10:22,542 DEBUG TRAIN Batch 13/7800 loss 36.031822 loss_att 39.622559 loss_ctc 49.466881 loss_rnnt 33.407333 hw_loss 0.215628 lr 0.00046373 rank 4
2023-02-23 04:10:22,543 DEBUG TRAIN Batch 13/7800 loss 8.025590 loss_att 11.767773 loss_ctc 9.281269 loss_rnnt 6.938693 hw_loss 0.320695 lr 0.00046381 rank 0
2023-02-23 04:10:22,574 DEBUG TRAIN Batch 13/7800 loss 18.852182 loss_att 23.644825 loss_ctc 24.284962 loss_rnnt 17.042887 hw_loss 0.236991 lr 0.00046371 rank 3
2023-02-23 04:10:22,582 DEBUG TRAIN Batch 13/7800 loss 9.860692 loss_att 11.410969 loss_ctc 9.307299 loss_rnnt 9.555645 hw_loss 0.128958 lr 0.00046372 rank 6
2023-02-23 04:10:22,586 DEBUG TRAIN Batch 13/7800 loss 11.403453 loss_att 16.885992 loss_ctc 19.601429 loss_rnnt 9.078261 hw_loss 0.254289 lr 0.00046379 rank 1
2023-02-23 04:11:40,668 DEBUG TRAIN Batch 13/7900 loss 19.321516 loss_att 18.269480 loss_ctc 28.176388 loss_rnnt 18.265472 hw_loss 0.160881 lr 0.00046359 rank 1
2023-02-23 04:11:40,670 DEBUG TRAIN Batch 13/7900 loss 25.085630 loss_att 30.770546 loss_ctc 33.340393 loss_rnnt 22.657982 hw_loss 0.356305 lr 0.00046353 rank 4
2023-02-23 04:11:40,670 DEBUG TRAIN Batch 13/7900 loss 14.512054 loss_att 18.773724 loss_ctc 23.319851 loss_rnnt 12.366203 hw_loss 0.223393 lr 0.00046353 rank 2
2023-02-23 04:11:40,671 DEBUG TRAIN Batch 13/7900 loss 12.753727 loss_att 18.454409 loss_ctc 16.195084 loss_rnnt 10.989761 hw_loss 0.309338 lr 0.00046356 rank 5
2023-02-23 04:11:40,672 DEBUG TRAIN Batch 13/7900 loss 17.516251 loss_att 24.582380 loss_ctc 27.439949 loss_rnnt 14.628952 hw_loss 0.282962 lr 0.00046351 rank 3
2023-02-23 04:11:40,672 DEBUG TRAIN Batch 13/7900 loss 14.880282 loss_att 18.909786 loss_ctc 19.411983 loss_rnnt 13.262434 hw_loss 0.389478 lr 0.00046361 rank 0
2023-02-23 04:11:40,676 DEBUG TRAIN Batch 13/7900 loss 11.537848 loss_att 14.712137 loss_ctc 15.497091 loss_rnnt 10.267664 hw_loss 0.201429 lr 0.00046352 rank 6
2023-02-23 04:11:40,717 DEBUG TRAIN Batch 13/7900 loss 18.224470 loss_att 20.290304 loss_ctc 20.143818 loss_rnnt 17.419487 hw_loss 0.254817 lr 0.00046349 rank 7
2023-02-23 04:12:56,424 DEBUG TRAIN Batch 13/8000 loss 14.218079 loss_att 18.536930 loss_ctc 19.657719 loss_rnnt 12.502230 hw_loss 0.237736 lr 0.00046333 rank 2
2023-02-23 04:12:56,427 DEBUG TRAIN Batch 13/8000 loss 11.795385 loss_att 14.400873 loss_ctc 15.908117 loss_rnnt 10.562346 hw_loss 0.306707 lr 0.00046336 rank 5
2023-02-23 04:12:56,428 DEBUG TRAIN Batch 13/8000 loss 21.229952 loss_att 24.270790 loss_ctc 24.852486 loss_rnnt 20.006851 hw_loss 0.247364 lr 0.00046341 rank 0
2023-02-23 04:12:56,432 DEBUG TRAIN Batch 13/8000 loss 15.488874 loss_att 17.831955 loss_ctc 17.337143 loss_rnnt 14.618699 hw_loss 0.290856 lr 0.00046331 rank 3
2023-02-23 04:12:56,432 DEBUG TRAIN Batch 13/8000 loss 14.960322 loss_att 17.282726 loss_ctc 17.274551 loss_rnnt 14.046545 hw_loss 0.263874 lr 0.00046339 rank 1
2023-02-23 04:12:56,433 DEBUG TRAIN Batch 13/8000 loss 15.355281 loss_att 17.945694 loss_ctc 16.803383 loss_rnnt 14.538749 hw_loss 0.197567 lr 0.00046333 rank 4
2023-02-23 04:12:56,436 DEBUG TRAIN Batch 13/8000 loss 7.945970 loss_att 11.236229 loss_ctc 9.055320 loss_rnnt 7.006936 hw_loss 0.249503 lr 0.00046329 rank 7
2023-02-23 04:12:56,437 DEBUG TRAIN Batch 13/8000 loss 9.835771 loss_att 12.791892 loss_ctc 13.830737 loss_rnnt 8.571331 hw_loss 0.263536 lr 0.00046332 rank 6
2023-02-23 04:14:12,716 DEBUG TRAIN Batch 13/8100 loss 23.252432 loss_att 24.091614 loss_ctc 26.956841 loss_rnnt 22.398844 hw_loss 0.359679 lr 0.00046314 rank 4
2023-02-23 04:14:12,716 DEBUG TRAIN Batch 13/8100 loss 15.172692 loss_att 17.554960 loss_ctc 19.727314 loss_rnnt 13.982738 hw_loss 0.199158 lr 0.00046316 rank 5
2023-02-23 04:14:12,718 DEBUG TRAIN Batch 13/8100 loss 11.415421 loss_att 12.615238 loss_ctc 14.527960 loss_rnnt 10.577759 hw_loss 0.342553 lr 0.00046313 rank 2
2023-02-23 04:14:12,719 DEBUG TRAIN Batch 13/8100 loss 26.774895 loss_att 29.108393 loss_ctc 34.193466 loss_rnnt 25.199814 hw_loss 0.223574 lr 0.00046311 rank 3
2023-02-23 04:14:12,725 DEBUG TRAIN Batch 13/8100 loss 10.436337 loss_att 14.067445 loss_ctc 12.360754 loss_rnnt 9.326254 hw_loss 0.238636 lr 0.00046310 rank 7
2023-02-23 04:14:12,724 DEBUG TRAIN Batch 13/8100 loss 5.895648 loss_att 9.022943 loss_ctc 7.818564 loss_rnnt 4.892751 hw_loss 0.226968 lr 0.00046321 rank 0
2023-02-23 04:14:12,727 DEBUG TRAIN Batch 13/8100 loss 15.046828 loss_att 18.795780 loss_ctc 16.341970 loss_rnnt 13.924371 hw_loss 0.374963 lr 0.00046312 rank 6
2023-02-23 04:14:12,727 DEBUG TRAIN Batch 13/8100 loss 14.489666 loss_att 19.725113 loss_ctc 18.807590 loss_rnnt 12.711697 hw_loss 0.290920 lr 0.00046319 rank 1
2023-02-23 04:15:29,670 DEBUG TRAIN Batch 13/8200 loss 15.797773 loss_att 18.341198 loss_ctc 21.691448 loss_rnnt 14.325188 hw_loss 0.333896 lr 0.00046293 rank 2
2023-02-23 04:15:29,670 DEBUG TRAIN Batch 13/8200 loss 29.044008 loss_att 34.395081 loss_ctc 35.580864 loss_rnnt 26.995087 hw_loss 0.200861 lr 0.00046301 rank 0
2023-02-23 04:15:29,677 DEBUG TRAIN Batch 13/8200 loss 16.208027 loss_att 18.458200 loss_ctc 20.302486 loss_rnnt 15.020774 hw_loss 0.358669 lr 0.00046292 rank 3
2023-02-23 04:15:29,678 DEBUG TRAIN Batch 13/8200 loss 19.666048 loss_att 21.512741 loss_ctc 23.998083 loss_rnnt 18.561890 hw_loss 0.294774 lr 0.00046299 rank 1
2023-02-23 04:15:29,679 DEBUG TRAIN Batch 13/8200 loss 12.400498 loss_att 17.111309 loss_ctc 15.745185 loss_rnnt 10.912187 hw_loss 0.187860 lr 0.00046290 rank 7
2023-02-23 04:15:29,679 DEBUG TRAIN Batch 13/8200 loss 10.952898 loss_att 15.041105 loss_ctc 13.028543 loss_rnnt 9.738040 hw_loss 0.225867 lr 0.00046296 rank 5
2023-02-23 04:15:29,679 DEBUG TRAIN Batch 13/8200 loss 10.128306 loss_att 12.567926 loss_ctc 12.123785 loss_rnnt 9.214308 hw_loss 0.300019 lr 0.00046294 rank 4
2023-02-23 04:15:29,725 DEBUG TRAIN Batch 13/8200 loss 11.820095 loss_att 13.602937 loss_ctc 16.789640 loss_rnnt 10.669587 hw_loss 0.246250 lr 0.00046292 rank 6
2023-02-23 04:16:45,757 DEBUG TRAIN Batch 13/8300 loss 7.891540 loss_att 15.575237 loss_ctc 13.587683 loss_rnnt 5.422729 hw_loss 0.323597 lr 0.00046272 rank 3
2023-02-23 04:16:45,758 DEBUG TRAIN Batch 13/8300 loss 9.777193 loss_att 12.444441 loss_ctc 15.164042 loss_rnnt 8.381327 hw_loss 0.270320 lr 0.00046273 rank 2
2023-02-23 04:16:45,759 DEBUG TRAIN Batch 13/8300 loss 9.334118 loss_att 9.709105 loss_ctc 11.726966 loss_rnnt 8.815287 hw_loss 0.233977 lr 0.00046274 rank 4
2023-02-23 04:16:45,759 DEBUG TRAIN Batch 13/8300 loss 10.659253 loss_att 14.101000 loss_ctc 16.711903 loss_rnnt 9.021072 hw_loss 0.267771 lr 0.00046281 rank 0
2023-02-23 04:16:45,759 DEBUG TRAIN Batch 13/8300 loss 8.275648 loss_att 11.339931 loss_ctc 9.126822 loss_rnnt 7.448757 hw_loss 0.188522 lr 0.00046276 rank 5
2023-02-23 04:16:45,763 DEBUG TRAIN Batch 13/8300 loss 7.499870 loss_att 9.643456 loss_ctc 9.841521 loss_rnnt 6.629024 hw_loss 0.243579 lr 0.00046272 rank 6
2023-02-23 04:16:45,764 DEBUG TRAIN Batch 13/8300 loss 9.560447 loss_att 11.322597 loss_ctc 10.756172 loss_rnnt 8.917439 hw_loss 0.245904 lr 0.00046279 rank 1
2023-02-23 04:16:45,811 DEBUG TRAIN Batch 13/8300 loss 6.232977 loss_att 7.058161 loss_ctc 7.290138 loss_rnnt 5.767166 hw_loss 0.299662 lr 0.00046270 rank 7
2023-02-23 04:17:44,763 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 6
2023-02-23 04:17:44,764 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 4
2023-02-23 04:17:44,766 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 5
2023-02-23 04:17:44,767 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 3
2023-02-23 04:17:44,782 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 1
2023-02-23 04:17:44,786 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 0
2023-02-23 04:17:44,788 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 7
2023-02-23 04:17:44,791 DEBUG CV Batch 13/0 loss 3.038507 loss_att 2.914591 loss_ctc 3.800717 loss_rnnt 2.753007 hw_loss 0.391228 history loss 2.925969 rank 2
2023-02-23 04:17:55,866 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 5
2023-02-23 04:17:55,884 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 2
2023-02-23 04:17:55,915 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 4
2023-02-23 04:17:56,050 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 3
2023-02-23 04:17:56,139 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 6
2023-02-23 04:17:56,189 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 1
2023-02-23 04:17:56,211 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 0
2023-02-23 04:17:56,299 DEBUG CV Batch 13/100 loss 9.974556 loss_att 10.406703 loss_ctc 12.006350 loss_rnnt 9.509734 hw_loss 0.201539 history loss 4.440798 rank 7
2023-02-23 04:18:09,859 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 2
2023-02-23 04:18:09,866 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 4
2023-02-23 04:18:09,877 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 5
2023-02-23 04:18:09,948 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 3
2023-02-23 04:18:10,244 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 7
2023-02-23 04:18:10,279 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 0
2023-02-23 04:18:10,357 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 1
2023-02-23 04:18:10,941 DEBUG CV Batch 13/200 loss 7.407497 loss_att 17.252560 loss_ctc 8.401232 loss_rnnt 5.209085 hw_loss 0.181690 history loss 4.982003 rank 6
2023-02-23 04:18:21,752 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 5
2023-02-23 04:18:21,840 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 4
2023-02-23 04:18:21,841 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 2
2023-02-23 04:18:22,043 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 3
2023-02-23 04:18:22,383 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 0
2023-02-23 04:18:22,514 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 7
2023-02-23 04:18:22,571 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 1
2023-02-23 04:18:23,232 DEBUG CV Batch 13/300 loss 7.428362 loss_att 7.636539 loss_ctc 8.405572 loss_rnnt 7.129192 hw_loss 0.238574 history loss 5.128805 rank 6
2023-02-23 04:18:33,630 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 5
2023-02-23 04:18:33,743 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 2
2023-02-23 04:18:33,745 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 4
2023-02-23 04:18:34,456 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 0
2023-02-23 04:18:34,733 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 1
2023-02-23 04:18:34,750 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 7
2023-02-23 04:18:34,918 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 3
2023-02-23 04:18:35,363 DEBUG CV Batch 13/400 loss 27.374834 loss_att 111.370010 loss_ctc 12.903504 loss_rnnt 12.411309 hw_loss 0.176248 history loss 6.248887 rank 6
2023-02-23 04:18:44,043 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 5
2023-02-23 04:18:44,248 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 2
2023-02-23 04:18:44,329 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 4
2023-02-23 04:18:45,121 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 0
2023-02-23 04:18:45,399 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 1
2023-02-23 04:18:45,538 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 7
2023-02-23 04:18:45,941 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 6
2023-02-23 04:18:46,207 DEBUG CV Batch 13/500 loss 6.543020 loss_att 8.081411 loss_ctc 8.465477 loss_rnnt 5.831760 hw_loss 0.276103 history loss 7.218590 rank 3
2023-02-23 04:18:56,970 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 2
2023-02-23 04:18:56,998 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 5
2023-02-23 04:18:57,118 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 4
2023-02-23 04:18:57,587 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 1
2023-02-23 04:18:57,786 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 0
2023-02-23 04:18:58,128 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 6
2023-02-23 04:18:58,328 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 7
2023-02-23 04:18:58,352 DEBUG CV Batch 13/600 loss 9.818732 loss_att 9.531565 loss_ctc 13.088858 loss_rnnt 9.213712 hw_loss 0.424569 history loss 8.234742 rank 3
2023-02-23 04:19:09,779 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 4
2023-02-23 04:19:09,806 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 5
2023-02-23 04:19:09,905 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 2
2023-02-23 04:19:09,982 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 6
2023-02-23 04:19:10,009 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 1
2023-02-23 04:19:10,077 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 3
2023-02-23 04:19:10,472 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 0
2023-02-23 04:19:10,582 DEBUG CV Batch 13/700 loss 26.032719 loss_att 71.915512 loss_ctc 22.586988 loss_rnnt 17.278198 hw_loss 0.070110 history loss 9.035625 rank 7
2023-02-23 04:19:21,948 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 4
2023-02-23 04:19:22,004 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 2
2023-02-23 04:19:22,029 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 6
2023-02-23 04:19:22,031 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 5
2023-02-23 04:19:22,054 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 3
2023-02-23 04:19:22,262 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 7
2023-02-23 04:19:22,393 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 1
2023-02-23 04:19:22,562 DEBUG CV Batch 13/800 loss 15.760004 loss_att 13.838870 loss_ctc 19.208931 loss_rnnt 15.535353 hw_loss 0.279417 history loss 8.421219 rank 0
2023-02-23 04:19:35,780 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 2
2023-02-23 04:19:35,802 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 4
2023-02-23 04:19:35,927 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 5
2023-02-23 04:19:36,020 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 3
2023-02-23 04:19:36,020 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 1
2023-02-23 04:19:36,452 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 6
2023-02-23 04:19:36,582 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 0
2023-02-23 04:19:36,594 DEBUG CV Batch 13/900 loss 18.098757 loss_att 23.220219 loss_ctc 26.418039 loss_rnnt 15.823213 hw_loss 0.266277 history loss 8.179183 rank 7
2023-02-23 04:19:47,925 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 4
2023-02-23 04:19:48,018 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 2
2023-02-23 04:19:48,089 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 5
2023-02-23 04:19:48,219 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 3
2023-02-23 04:19:48,883 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 6
2023-02-23 04:19:48,992 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 0
2023-02-23 04:19:49,117 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 7
2023-02-23 04:19:49,207 DEBUG CV Batch 13/1000 loss 4.707838 loss_att 5.607745 loss_ctc 4.543357 loss_rnnt 4.384221 hw_loss 0.310437 history loss 7.882593 rank 1
2023-02-23 04:19:59,815 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 4
2023-02-23 04:19:59,833 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 2
2023-02-23 04:19:59,898 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 5
2023-02-23 04:20:00,193 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 3
2023-02-23 04:20:00,918 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 6
2023-02-23 04:20:01,049 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 0
2023-02-23 04:20:01,201 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 1
2023-02-23 04:20:01,277 DEBUG CV Batch 13/1100 loss 7.965670 loss_att 6.847012 loss_ctc 10.130075 loss_rnnt 7.707448 hw_loss 0.362561 history loss 7.875242 rank 7
2023-02-23 04:20:10,223 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 4
2023-02-23 04:20:10,329 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 2
2023-02-23 04:20:10,346 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 5
2023-02-23 04:20:10,699 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 3
2023-02-23 04:20:11,499 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 6
2023-02-23 04:20:11,580 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 0
2023-02-23 04:20:11,789 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 1
2023-02-23 04:20:12,651 DEBUG CV Batch 13/1200 loss 6.686072 loss_att 9.850798 loss_ctc 7.876885 loss_rnnt 5.733098 hw_loss 0.302352 history loss 8.276556 rank 7
2023-02-23 04:20:22,152 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 4
2023-02-23 04:20:22,223 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 5
2023-02-23 04:20:22,243 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 2
2023-02-23 04:20:22,647 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 3
2023-02-23 04:20:23,586 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 0
2023-02-23 04:20:23,609 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 6
2023-02-23 04:20:23,804 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 1
2023-02-23 04:20:25,100 DEBUG CV Batch 13/1300 loss 7.306869 loss_att 7.052790 loss_ctc 8.831095 loss_rnnt 6.983947 hw_loss 0.319701 history loss 8.601460 rank 7
2023-02-23 04:20:33,756 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 4
2023-02-23 04:20:33,957 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 5
2023-02-23 04:20:34,126 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 2
2023-02-23 04:20:34,427 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 3
2023-02-23 04:20:34,925 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 6
2023-02-23 04:20:35,071 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 1
2023-02-23 04:20:36,253 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 0
2023-02-23 04:20:36,473 DEBUG CV Batch 13/1400 loss 11.892476 loss_att 37.707954 loss_ctc 5.756351 loss_rnnt 7.429756 hw_loss 0.220825 history loss 8.994628 rank 7
2023-02-23 04:20:46,281 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 4
2023-02-23 04:20:46,310 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 2
2023-02-23 04:20:46,395 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 5
2023-02-23 04:20:46,833 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 3
2023-02-23 04:20:46,987 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 1
2023-02-23 04:20:47,435 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 6
2023-02-23 04:20:48,226 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 7
2023-02-23 04:20:48,956 DEBUG CV Batch 13/1500 loss 5.868076 loss_att 7.393575 loss_ctc 4.716772 loss_rnnt 5.570850 hw_loss 0.273062 history loss 8.787366 rank 0
2023-02-23 04:20:59,883 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 4
2023-02-23 04:20:59,999 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 2
2023-02-23 04:21:00,005 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 5
2023-02-23 04:21:00,640 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 3
2023-02-23 04:21:00,844 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 1
2023-02-23 04:21:00,983 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 6
2023-02-23 04:21:02,369 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 7
2023-02-23 04:21:03,003 DEBUG CV Batch 13/1600 loss 13.278621 loss_att 18.856604 loss_ctc 14.119697 loss_rnnt 11.943366 hw_loss 0.201589 history loss 8.692185 rank 0
2023-02-23 04:21:12,407 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 4
2023-02-23 04:21:12,409 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 5
2023-02-23 04:21:12,459 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 2
2023-02-23 04:21:13,214 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 3
2023-02-23 04:21:13,573 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 1
2023-02-23 04:21:13,698 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 6
2023-02-23 04:21:15,575 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 0
2023-02-23 04:21:16,108 DEBUG CV Batch 13/1700 loss 11.872235 loss_att 10.506327 loss_ctc 16.905804 loss_rnnt 11.319180 hw_loss 0.290803 history loss 8.557819 rank 7
2023-02-23 04:21:21,563 INFO Epoch 13 CV info cv_loss 8.514816236579534
2023-02-23 04:21:21,564 INFO Epoch 14 TRAIN info lr 0.00046264377588212645
2023-02-23 04:21:21,567 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:21,645 INFO Epoch 13 CV info cv_loss 8.514816236631221
2023-02-23 04:21:21,646 INFO Epoch 14 TRAIN info lr 0.0004626695242540769
2023-02-23 04:21:21,650 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:21,657 INFO Epoch 13 CV info cv_loss 8.514816236122957
2023-02-23 04:21:21,659 INFO Epoch 14 TRAIN info lr 0.00046266754345744945
2023-02-23 04:21:21,663 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:22,463 INFO Epoch 13 CV info cv_loss 8.514816236915504
2023-02-23 04:21:22,464 INFO Epoch 14 TRAIN info lr 0.00046263981497567417
2023-02-23 04:21:22,467 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:22,855 INFO Epoch 13 CV info cv_loss 8.514816238052635
2023-02-23 04:21:22,856 INFO Epoch 14 TRAIN info lr 0.0004627428316189756
2023-02-23 04:21:22,859 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:23,076 INFO Epoch 13 CV info cv_loss 8.514816237906187
2023-02-23 04:21:23,077 INFO Epoch 14 TRAIN info lr 0.0004627012204604753
2023-02-23 04:21:23,080 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:24,860 INFO Epoch 13 CV info cv_loss 8.514816237509914
2023-02-23 04:21:24,861 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/13.pt
2023-02-23 04:21:25,360 INFO Epoch 13 CV info cv_loss 8.514816236467544
2023-02-23 04:21:25,361 INFO Epoch 14 TRAIN info lr 0.00046262595260419977
2023-02-23 04:21:25,365 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:21:25,558 INFO Epoch 14 TRAIN info lr 0.00046267546679660805
2023-02-23 04:21:25,562 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 04:22:38,561 DEBUG TRAIN Batch 14/0 loss 12.610961 loss_att 12.031202 loss_ctc 16.107521 loss_rnnt 12.071025 hw_loss 0.355648 lr 0.00046264 rank 5
2023-02-23 04:22:38,564 DEBUG TRAIN Batch 14/0 loss 9.748449 loss_att 9.539070 loss_ctc 12.027291 loss_rnnt 9.310356 hw_loss 0.330232 lr 0.00046274 rank 1
2023-02-23 04:22:38,564 DEBUG TRAIN Batch 14/0 loss 12.207490 loss_att 11.600504 loss_ctc 13.103280 loss_rnnt 12.019751 hw_loss 0.355685 lr 0.00046267 rank 2
2023-02-23 04:22:38,566 DEBUG TRAIN Batch 14/0 loss 10.649692 loss_att 10.316902 loss_ctc 12.043189 loss_rnnt 10.309633 hw_loss 0.414030 lr 0.00046267 rank 4
2023-02-23 04:22:38,571 DEBUG TRAIN Batch 14/0 loss 7.837661 loss_att 7.240918 loss_ctc 9.282732 loss_rnnt 7.560177 hw_loss 0.382793 lr 0.00046264 rank 3
2023-02-23 04:22:38,574 DEBUG TRAIN Batch 14/0 loss 15.139101 loss_att 13.912552 loss_ctc 17.288122 loss_rnnt 14.871839 hw_loss 0.423818 lr 0.00046270 rank 6
2023-02-23 04:22:38,584 DEBUG TRAIN Batch 14/0 loss 13.046421 loss_att 12.625483 loss_ctc 15.247221 loss_rnnt 12.674078 hw_loss 0.305796 lr 0.00046262 rank 7
2023-02-23 04:22:38,587 DEBUG TRAIN Batch 14/0 loss 10.386751 loss_att 8.673835 loss_ctc 11.425107 loss_rnnt 10.388540 hw_loss 0.379401 lr 0.00046267 rank 0
2023-02-23 04:23:54,931 DEBUG TRAIN Batch 14/100 loss 31.491568 loss_att 32.019108 loss_ctc 34.816467 loss_rnnt 30.765371 hw_loss 0.332567 lr 0.00046248 rank 0
2023-02-23 04:23:54,935 DEBUG TRAIN Batch 14/100 loss 7.584331 loss_att 11.945896 loss_ctc 14.998915 loss_rnnt 5.575855 hw_loss 0.276659 lr 0.00046247 rank 4
2023-02-23 04:23:54,938 DEBUG TRAIN Batch 14/100 loss 9.965693 loss_att 13.622990 loss_ctc 11.184745 loss_rnnt 8.944857 hw_loss 0.237818 lr 0.00046247 rank 2
2023-02-23 04:23:54,938 DEBUG TRAIN Batch 14/100 loss 6.064192 loss_att 10.460526 loss_ctc 7.887796 loss_rnnt 4.850330 hw_loss 0.171465 lr 0.00046250 rank 6
2023-02-23 04:23:54,939 DEBUG TRAIN Batch 14/100 loss 7.685346 loss_att 13.205368 loss_ctc 8.905013 loss_rnnt 6.284080 hw_loss 0.252448 lr 0.00046244 rank 3
2023-02-23 04:23:54,939 DEBUG TRAIN Batch 14/100 loss 20.872917 loss_att 22.843231 loss_ctc 24.664135 loss_rnnt 19.869205 hw_loss 0.195287 lr 0.00046244 rank 5
2023-02-23 04:23:54,944 DEBUG TRAIN Batch 14/100 loss 10.416796 loss_att 11.616413 loss_ctc 15.079421 loss_rnnt 9.486571 hw_loss 0.128658 lr 0.00046254 rank 1
2023-02-23 04:23:54,945 DEBUG TRAIN Batch 14/100 loss 33.330151 loss_att 41.264404 loss_ctc 44.137146 loss_rnnt 30.200897 hw_loss 0.190249 lr 0.00046243 rank 7
2023-02-23 04:25:11,045 DEBUG TRAIN Batch 14/200 loss 14.870476 loss_att 17.266272 loss_ctc 19.513271 loss_rnnt 13.615311 hw_loss 0.294311 lr 0.00046227 rank 4
2023-02-23 04:25:11,047 DEBUG TRAIN Batch 14/200 loss 13.999644 loss_att 20.094593 loss_ctc 20.057457 loss_rnnt 11.894259 hw_loss 0.147538 lr 0.00046227 rank 2
2023-02-23 04:25:11,048 DEBUG TRAIN Batch 14/200 loss 22.770016 loss_att 22.544315 loss_ctc 22.956402 loss_rnnt 22.689987 hw_loss 0.188093 lr 0.00046235 rank 1
2023-02-23 04:25:11,049 DEBUG TRAIN Batch 14/200 loss 7.628737 loss_att 10.327671 loss_ctc 7.825633 loss_rnnt 6.939119 hw_loss 0.231712 lr 0.00046224 rank 3
2023-02-23 04:25:11,051 DEBUG TRAIN Batch 14/200 loss 6.418484 loss_att 11.333170 loss_ctc 10.208485 loss_rnnt 4.782168 hw_loss 0.277585 lr 0.00046230 rank 6
2023-02-23 04:25:11,051 DEBUG TRAIN Batch 14/200 loss 11.851402 loss_att 14.847667 loss_ctc 14.293356 loss_rnnt 10.819381 hw_loss 0.200954 lr 0.00046225 rank 5
2023-02-23 04:25:11,051 DEBUG TRAIN Batch 14/200 loss 14.395549 loss_att 19.417242 loss_ctc 18.686531 loss_rnnt 12.724170 hw_loss 0.177956 lr 0.00046228 rank 0
2023-02-23 04:25:11,054 DEBUG TRAIN Batch 14/200 loss 9.560249 loss_att 14.123259 loss_ctc 15.424094 loss_rnnt 7.676332 hw_loss 0.355255 lr 0.00046223 rank 7
2023-02-23 04:26:28,316 DEBUG TRAIN Batch 14/300 loss 9.255941 loss_att 12.443563 loss_ctc 14.414552 loss_rnnt 7.781412 hw_loss 0.279732 lr 0.00046207 rank 4
2023-02-23 04:26:28,318 DEBUG TRAIN Batch 14/300 loss 6.879227 loss_att 10.003653 loss_ctc 7.663955 loss_rnnt 6.000133 hw_loss 0.280460 lr 0.00046205 rank 5
2023-02-23 04:26:28,323 DEBUG TRAIN Batch 14/300 loss 10.497714 loss_att 14.694228 loss_ctc 13.332221 loss_rnnt 9.169286 hw_loss 0.208482 lr 0.00046204 rank 3
2023-02-23 04:26:28,324 DEBUG TRAIN Batch 14/300 loss 8.342349 loss_att 10.902117 loss_ctc 13.859669 loss_rnnt 6.992260 hw_loss 0.192172 lr 0.00046203 rank 7
2023-02-23 04:26:28,325 DEBUG TRAIN Batch 14/300 loss 8.800344 loss_att 14.376921 loss_ctc 10.502044 loss_rnnt 7.342175 hw_loss 0.217423 lr 0.00046211 rank 6
2023-02-23 04:26:28,327 DEBUG TRAIN Batch 14/300 loss 10.733709 loss_att 15.302046 loss_ctc 15.065416 loss_rnnt 9.140454 hw_loss 0.191301 lr 0.00046215 rank 1
2023-02-23 04:26:28,351 DEBUG TRAIN Batch 14/300 loss 12.245502 loss_att 14.896099 loss_ctc 15.268250 loss_rnnt 11.197460 hw_loss 0.215415 lr 0.00046208 rank 0
2023-02-23 04:26:28,360 DEBUG TRAIN Batch 14/300 loss 12.607001 loss_att 14.221440 loss_ctc 13.911407 loss_rnnt 12.011377 hw_loss 0.185277 lr 0.00046207 rank 2
2023-02-23 04:27:46,672 DEBUG TRAIN Batch 14/400 loss 8.959785 loss_att 11.215972 loss_ctc 12.377400 loss_rnnt 7.936376 hw_loss 0.218420 lr 0.00046188 rank 0
2023-02-23 04:27:46,673 DEBUG TRAIN Batch 14/400 loss 13.989461 loss_att 16.429083 loss_ctc 15.458509 loss_rnnt 13.203024 hw_loss 0.192449 lr 0.00046185 rank 5
2023-02-23 04:27:46,673 DEBUG TRAIN Batch 14/400 loss 7.861913 loss_att 9.378930 loss_ctc 8.630172 loss_rnnt 7.327549 hw_loss 0.240987 lr 0.00046188 rank 2
2023-02-23 04:27:46,676 DEBUG TRAIN Batch 14/400 loss 9.597943 loss_att 13.939255 loss_ctc 9.571480 loss_rnnt 8.604904 hw_loss 0.240572 lr 0.00046188 rank 4
2023-02-23 04:27:46,676 DEBUG TRAIN Batch 14/400 loss 9.147629 loss_att 13.429603 loss_ctc 13.961773 loss_rnnt 7.492960 hw_loss 0.293226 lr 0.00046195 rank 1
2023-02-23 04:27:46,676 DEBUG TRAIN Batch 14/400 loss 3.933946 loss_att 7.883036 loss_ctc 5.707775 loss_rnnt 2.760681 hw_loss 0.275506 lr 0.00046183 rank 7
2023-02-23 04:27:46,676 DEBUG TRAIN Batch 14/400 loss 20.326902 loss_att 22.690817 loss_ctc 26.556591 loss_rnnt 18.887096 hw_loss 0.255744 lr 0.00046191 rank 6
2023-02-23 04:27:46,715 DEBUG TRAIN Batch 14/400 loss 18.008856 loss_att 22.330526 loss_ctc 22.244766 loss_rnnt 16.489868 hw_loss 0.168495 lr 0.00046185 rank 3
2023-02-23 04:29:03,290 DEBUG TRAIN Batch 14/500 loss 13.095670 loss_att 15.876878 loss_ctc 16.928663 loss_rnnt 11.833044 hw_loss 0.366224 lr 0.00046165 rank 5
2023-02-23 04:29:03,292 DEBUG TRAIN Batch 14/500 loss 17.309422 loss_att 19.995562 loss_ctc 22.864302 loss_rnnt 15.947797 hw_loss 0.157024 lr 0.00046168 rank 4
2023-02-23 04:29:03,295 DEBUG TRAIN Batch 14/500 loss 7.992326 loss_att 12.869953 loss_ctc 13.078838 loss_rnnt 6.227592 hw_loss 0.208140 lr 0.00046164 rank 7
2023-02-23 04:29:03,296 DEBUG TRAIN Batch 14/500 loss 13.064542 loss_att 14.741471 loss_ctc 14.812156 loss_rnnt 12.364666 hw_loss 0.246515 lr 0.00046171 rank 6
2023-02-23 04:29:03,299 DEBUG TRAIN Batch 14/500 loss 13.237619 loss_att 13.765884 loss_ctc 19.198774 loss_rnnt 12.159313 hw_loss 0.333438 lr 0.00046168 rank 2
2023-02-23 04:29:03,302 DEBUG TRAIN Batch 14/500 loss 21.763174 loss_att 24.468796 loss_ctc 26.835634 loss_rnnt 20.442915 hw_loss 0.192763 lr 0.00046165 rank 3
2023-02-23 04:29:03,303 DEBUG TRAIN Batch 14/500 loss 21.200285 loss_att 22.784950 loss_ctc 22.344818 loss_rnnt 20.616617 hw_loss 0.213991 lr 0.00046175 rank 1
2023-02-23 04:29:03,309 DEBUG TRAIN Batch 14/500 loss 12.363093 loss_att 14.957102 loss_ctc 14.261955 loss_rnnt 11.426003 hw_loss 0.309576 lr 0.00046169 rank 0
2023-02-23 04:30:20,051 DEBUG TRAIN Batch 14/600 loss 8.661726 loss_att 8.014029 loss_ctc 10.936031 loss_rnnt 8.325711 hw_loss 0.304336 lr 0.00046146 rank 5
2023-02-23 04:30:20,052 DEBUG TRAIN Batch 14/600 loss 19.317272 loss_att 17.516186 loss_ctc 21.784334 loss_rnnt 19.136158 hw_loss 0.398231 lr 0.00046148 rank 4
2023-02-23 04:30:20,052 DEBUG TRAIN Batch 14/600 loss 10.729037 loss_att 12.045004 loss_ctc 12.873710 loss_rnnt 10.007322 hw_loss 0.323560 lr 0.00046149 rank 0
2023-02-23 04:30:20,053 DEBUG TRAIN Batch 14/600 loss 13.555376 loss_att 12.201535 loss_ctc 14.753028 loss_rnnt 13.504591 hw_loss 0.303499 lr 0.00046148 rank 2
2023-02-23 04:30:20,056 DEBUG TRAIN Batch 14/600 loss 12.960657 loss_att 12.529558 loss_ctc 15.127819 loss_rnnt 12.553725 hw_loss 0.382869 lr 0.00046152 rank 6
2023-02-23 04:30:20,059 DEBUG TRAIN Batch 14/600 loss 13.188088 loss_att 15.211376 loss_ctc 17.702953 loss_rnnt 12.008371 hw_loss 0.324518 lr 0.00046144 rank 7
2023-02-23 04:30:20,061 DEBUG TRAIN Batch 14/600 loss 7.442665 loss_att 9.015444 loss_ctc 12.499344 loss_rnnt 6.302080 hw_loss 0.284636 lr 0.00046156 rank 1
2023-02-23 04:30:20,061 DEBUG TRAIN Batch 14/600 loss 17.923214 loss_att 19.430857 loss_ctc 24.110224 loss_rnnt 16.636084 hw_loss 0.301249 lr 0.00046145 rank 3
2023-02-23 04:31:38,181 DEBUG TRAIN Batch 14/700 loss 7.525573 loss_att 9.628628 loss_ctc 10.001373 loss_rnnt 6.634902 hw_loss 0.262412 lr 0.00046129 rank 2
2023-02-23 04:31:38,182 DEBUG TRAIN Batch 14/700 loss 10.936831 loss_att 10.906702 loss_ctc 12.753506 loss_rnnt 10.505572 hw_loss 0.365742 lr 0.00046126 rank 3
2023-02-23 04:31:38,183 DEBUG TRAIN Batch 14/700 loss 23.073364 loss_att 30.136475 loss_ctc 27.212074 loss_rnnt 20.953630 hw_loss 0.291158 lr 0.00046126 rank 5
2023-02-23 04:31:38,183 DEBUG TRAIN Batch 14/700 loss 12.089680 loss_att 13.973983 loss_ctc 12.164138 loss_rnnt 11.616451 hw_loss 0.162078 lr 0.00046129 rank 0
2023-02-23 04:31:38,190 DEBUG TRAIN Batch 14/700 loss 6.156060 loss_att 10.376660 loss_ctc 8.443636 loss_rnnt 4.902519 hw_loss 0.195770 lr 0.00046132 rank 6
2023-02-23 04:31:38,190 DEBUG TRAIN Batch 14/700 loss 37.410610 loss_att 39.227066 loss_ctc 38.636440 loss_rnnt 36.740852 hw_loss 0.268166 lr 0.00046124 rank 7
2023-02-23 04:31:38,197 DEBUG TRAIN Batch 14/700 loss 4.059051 loss_att 5.501413 loss_ctc 3.206734 loss_rnnt 3.662716 hw_loss 0.415323 lr 0.00046129 rank 4
2023-02-23 04:31:38,229 DEBUG TRAIN Batch 14/700 loss 8.345920 loss_att 13.400784 loss_ctc 12.577116 loss_rnnt 6.668905 hw_loss 0.191031 lr 0.00046136 rank 1
2023-02-23 04:32:54,088 DEBUG TRAIN Batch 14/800 loss 5.707891 loss_att 8.704823 loss_ctc 9.891292 loss_rnnt 4.384691 hw_loss 0.311302 lr 0.00046107 rank 5
2023-02-23 04:32:54,089 DEBUG TRAIN Batch 14/800 loss 16.052826 loss_att 21.835281 loss_ctc 22.070015 loss_rnnt 13.994789 hw_loss 0.186099 lr 0.00046109 rank 2
2023-02-23 04:32:54,089 DEBUG TRAIN Batch 14/800 loss 17.829700 loss_att 18.757204 loss_ctc 20.025089 loss_rnnt 17.223557 hw_loss 0.239857 lr 0.00046110 rank 0
2023-02-23 04:32:54,092 DEBUG TRAIN Batch 14/800 loss 18.177225 loss_att 25.307322 loss_ctc 20.103260 loss_rnnt 16.350780 hw_loss 0.269291 lr 0.00046106 rank 3
2023-02-23 04:32:54,094 DEBUG TRAIN Batch 14/800 loss 15.703853 loss_att 18.331415 loss_ctc 17.931356 loss_rnnt 14.762877 hw_loss 0.222119 lr 0.00046105 rank 7
2023-02-23 04:32:54,095 DEBUG TRAIN Batch 14/800 loss 10.369164 loss_att 15.573423 loss_ctc 11.132658 loss_rnnt 9.123327 hw_loss 0.193472 lr 0.00046109 rank 4
2023-02-23 04:32:54,097 DEBUG TRAIN Batch 14/800 loss 21.520178 loss_att 28.482355 loss_ctc 30.483404 loss_rnnt 18.833158 hw_loss 0.186539 lr 0.00046112 rank 6
2023-02-23 04:32:54,097 DEBUG TRAIN Batch 14/800 loss 7.688929 loss_att 11.078531 loss_ctc 9.606986 loss_rnnt 6.586044 hw_loss 0.317293 lr 0.00046116 rank 1
2023-02-23 04:34:09,522 DEBUG TRAIN Batch 14/900 loss 9.444110 loss_att 10.792526 loss_ctc 13.621646 loss_rnnt 8.465157 hw_loss 0.285498 lr 0.00046090 rank 4
2023-02-23 04:34:09,523 DEBUG TRAIN Batch 14/900 loss 5.653333 loss_att 9.629140 loss_ctc 6.599401 loss_rnnt 4.555352 hw_loss 0.331269 lr 0.00046089 rank 2
2023-02-23 04:34:09,528 DEBUG TRAIN Batch 14/900 loss 18.155872 loss_att 15.375554 loss_ctc 16.321609 loss_rnnt 18.808655 hw_loss 0.277219 lr 0.00046090 rank 0
2023-02-23 04:34:09,532 DEBUG TRAIN Batch 14/900 loss 17.439745 loss_att 23.930923 loss_ctc 23.177847 loss_rnnt 15.243043 hw_loss 0.250095 lr 0.00046093 rank 6
2023-02-23 04:34:09,531 DEBUG TRAIN Batch 14/900 loss 13.778193 loss_att 16.112730 loss_ctc 15.940057 loss_rnnt 12.876637 hw_loss 0.274496 lr 0.00046085 rank 7
2023-02-23 04:34:09,532 DEBUG TRAIN Batch 14/900 loss 14.218691 loss_att 17.884321 loss_ctc 20.572168 loss_rnnt 12.514715 hw_loss 0.231977 lr 0.00046087 rank 5
2023-02-23 04:34:09,534 DEBUG TRAIN Batch 14/900 loss 12.601771 loss_att 13.763168 loss_ctc 19.688923 loss_rnnt 11.341495 hw_loss 0.155709 lr 0.00046097 rank 1
2023-02-23 04:34:09,577 DEBUG TRAIN Batch 14/900 loss 12.115084 loss_att 14.598318 loss_ctc 18.568352 loss_rnnt 10.671599 hw_loss 0.162003 lr 0.00046087 rank 3
2023-02-23 04:35:28,094 DEBUG TRAIN Batch 14/1000 loss 10.209189 loss_att 10.832655 loss_ctc 11.803808 loss_rnnt 9.752449 hw_loss 0.223934 lr 0.00046070 rank 2
2023-02-23 04:35:28,098 DEBUG TRAIN Batch 14/1000 loss 9.799105 loss_att 14.036880 loss_ctc 14.221079 loss_rnnt 8.237796 hw_loss 0.232797 lr 0.00046073 rank 6
2023-02-23 04:35:28,097 DEBUG TRAIN Batch 14/1000 loss 12.383841 loss_att 14.926085 loss_ctc 17.658215 loss_rnnt 11.050493 hw_loss 0.228089 lr 0.00046067 rank 3
2023-02-23 04:35:28,100 DEBUG TRAIN Batch 14/1000 loss 10.208842 loss_att 13.799637 loss_ctc 12.039001 loss_rnnt 9.140724 hw_loss 0.198634 lr 0.00046077 rank 1
2023-02-23 04:35:28,101 DEBUG TRAIN Batch 14/1000 loss 16.352467 loss_att 18.448608 loss_ctc 18.467918 loss_rnnt 15.564611 hw_loss 0.162310 lr 0.00046071 rank 0
2023-02-23 04:35:28,103 DEBUG TRAIN Batch 14/1000 loss 8.589797 loss_att 12.867495 loss_ctc 10.865488 loss_rnnt 7.339351 hw_loss 0.171527 lr 0.00046070 rank 4
2023-02-23 04:35:28,104 DEBUG TRAIN Batch 14/1000 loss 12.093598 loss_att 15.676167 loss_ctc 16.824663 loss_rnnt 10.607405 hw_loss 0.260385 lr 0.00046067 rank 5
2023-02-23 04:35:28,148 DEBUG TRAIN Batch 14/1000 loss 3.226313 loss_att 5.294692 loss_ctc 3.972108 loss_rnnt 2.602969 hw_loss 0.206679 lr 0.00046066 rank 7
2023-02-23 04:36:45,899 DEBUG TRAIN Batch 14/1100 loss 12.356539 loss_att 14.818724 loss_ctc 17.942125 loss_rnnt 10.956045 hw_loss 0.306210 lr 0.00046050 rank 2
2023-02-23 04:36:45,902 DEBUG TRAIN Batch 14/1100 loss 9.239924 loss_att 11.903688 loss_ctc 10.712920 loss_rnnt 8.379257 hw_loss 0.246592 lr 0.00046048 rank 5
2023-02-23 04:36:45,902 DEBUG TRAIN Batch 14/1100 loss 8.926824 loss_att 13.591230 loss_ctc 14.487282 loss_rnnt 7.100350 hw_loss 0.285370 lr 0.00046051 rank 0
2023-02-23 04:36:45,904 DEBUG TRAIN Batch 14/1100 loss 9.039032 loss_att 12.677313 loss_ctc 12.486759 loss_rnnt 7.688625 hw_loss 0.305723 lr 0.00046058 rank 1
2023-02-23 04:36:45,904 DEBUG TRAIN Batch 14/1100 loss 10.566709 loss_att 12.824532 loss_ctc 22.550142 loss_rnnt 8.360415 hw_loss 0.294258 lr 0.00046050 rank 4
2023-02-23 04:36:45,905 DEBUG TRAIN Batch 14/1100 loss 13.226602 loss_att 19.817982 loss_ctc 19.132900 loss_rnnt 10.955824 hw_loss 0.309367 lr 0.00046047 rank 3
2023-02-23 04:36:45,906 DEBUG TRAIN Batch 14/1100 loss 15.688473 loss_att 16.006327 loss_ctc 16.808531 loss_rnnt 15.343507 hw_loss 0.247601 lr 0.00046054 rank 6
2023-02-23 04:36:45,909 DEBUG TRAIN Batch 14/1100 loss 14.539363 loss_att 18.103338 loss_ctc 19.317053 loss_rnnt 13.039788 hw_loss 0.280788 lr 0.00046046 rank 7
2023-02-23 04:38:03,509 DEBUG TRAIN Batch 14/1200 loss 10.968213 loss_att 11.688210 loss_ctc 13.820327 loss_rnnt 10.251422 hw_loss 0.360956 lr 0.00046031 rank 0
2023-02-23 04:38:03,509 DEBUG TRAIN Batch 14/1200 loss 10.970466 loss_att 10.602634 loss_ctc 11.619232 loss_rnnt 10.804034 hw_loss 0.287804 lr 0.00046031 rank 2
2023-02-23 04:38:03,511 DEBUG TRAIN Batch 14/1200 loss 11.794124 loss_att 13.624142 loss_ctc 16.099689 loss_rnnt 10.702995 hw_loss 0.283217 lr 0.00046027 rank 7
2023-02-23 04:38:03,510 DEBUG TRAIN Batch 14/1200 loss 15.749815 loss_att 15.473804 loss_ctc 21.398474 loss_rnnt 14.871092 hw_loss 0.338946 lr 0.00046028 rank 5
2023-02-23 04:38:03,512 DEBUG TRAIN Batch 14/1200 loss 8.530813 loss_att 15.929864 loss_ctc 10.593944 loss_rnnt 6.638295 hw_loss 0.258044 lr 0.00046031 rank 4
2023-02-23 04:38:03,514 DEBUG TRAIN Batch 14/1200 loss 12.765046 loss_att 15.643163 loss_ctc 16.639614 loss_rnnt 11.539524 hw_loss 0.249918 lr 0.00046028 rank 3
2023-02-23 04:38:03,514 DEBUG TRAIN Batch 14/1200 loss 14.648979 loss_att 12.986385 loss_ctc 16.055542 loss_rnnt 14.610408 hw_loss 0.344152 lr 0.00046034 rank 6
2023-02-23 04:38:03,522 DEBUG TRAIN Batch 14/1200 loss 23.686193 loss_att 24.893208 loss_ctc 32.789509 loss_rnnt 22.056683 hw_loss 0.326874 lr 0.00046038 rank 1
2023-02-23 04:39:18,377 DEBUG TRAIN Batch 14/1300 loss 9.490532 loss_att 15.561941 loss_ctc 13.998674 loss_rnnt 7.576608 hw_loss 0.184793 lr 0.00046011 rank 4
2023-02-23 04:39:18,380 DEBUG TRAIN Batch 14/1300 loss 30.649000 loss_att 33.674358 loss_ctc 33.334190 loss_rnnt 29.531078 hw_loss 0.290292 lr 0.00046014 rank 6
2023-02-23 04:39:18,380 DEBUG TRAIN Batch 14/1300 loss 9.871649 loss_att 13.857379 loss_ctc 13.934253 loss_rnnt 8.350163 hw_loss 0.342486 lr 0.00046009 rank 5
2023-02-23 04:39:18,380 DEBUG TRAIN Batch 14/1300 loss 12.273899 loss_att 16.604050 loss_ctc 17.644947 loss_rnnt 10.574280 hw_loss 0.220219 lr 0.00046011 rank 2
2023-02-23 04:39:18,382 DEBUG TRAIN Batch 14/1300 loss 9.798530 loss_att 13.996080 loss_ctc 16.989012 loss_rnnt 7.838844 hw_loss 0.302709 lr 0.00046012 rank 0
2023-02-23 04:39:18,384 DEBUG TRAIN Batch 14/1300 loss 15.373564 loss_att 17.873648 loss_ctc 24.186165 loss_rnnt 13.510252 hw_loss 0.353029 lr 0.00046019 rank 1
2023-02-23 04:39:18,385 DEBUG TRAIN Batch 14/1300 loss 6.008395 loss_att 8.358961 loss_ctc 7.391696 loss_rnnt 5.240223 hw_loss 0.213035 lr 0.00046008 rank 3
2023-02-23 04:39:18,386 DEBUG TRAIN Batch 14/1300 loss 6.063219 loss_att 10.908511 loss_ctc 9.820151 loss_rnnt 4.465983 hw_loss 0.238599 lr 0.00046007 rank 7
2023-02-23 04:40:35,736 DEBUG TRAIN Batch 14/1400 loss 23.347176 loss_att 27.126278 loss_ctc 28.463507 loss_rnnt 21.767212 hw_loss 0.266179 lr 0.00045992 rank 2
2023-02-23 04:40:35,741 DEBUG TRAIN Batch 14/1400 loss 10.000635 loss_att 15.293746 loss_ctc 10.127447 loss_rnnt 8.805433 hw_loss 0.224382 lr 0.00045995 rank 6
2023-02-23 04:40:35,743 DEBUG TRAIN Batch 14/1400 loss 1.814368 loss_att 5.064600 loss_ctc 2.037095 loss_rnnt 1.018196 hw_loss 0.218303 lr 0.00045989 rank 3
2023-02-23 04:40:35,744 DEBUG TRAIN Batch 14/1400 loss 8.073253 loss_att 10.713755 loss_ctc 8.637470 loss_rnnt 7.333534 hw_loss 0.255728 lr 0.00045992 rank 0
2023-02-23 04:40:35,745 DEBUG TRAIN Batch 14/1400 loss 2.835498 loss_att 6.427980 loss_ctc 3.653790 loss_rnnt 1.870439 hw_loss 0.257731 lr 0.00045992 rank 4
2023-02-23 04:40:35,749 DEBUG TRAIN Batch 14/1400 loss 11.433108 loss_att 17.223003 loss_ctc 17.925680 loss_rnnt 9.243481 hw_loss 0.311199 lr 0.00045989 rank 5
2023-02-23 04:40:35,749 DEBUG TRAIN Batch 14/1400 loss 17.500238 loss_att 17.828184 loss_ctc 21.272255 loss_rnnt 16.879442 hw_loss 0.098006 lr 0.00045988 rank 7
2023-02-23 04:40:35,788 DEBUG TRAIN Batch 14/1400 loss 19.223022 loss_att 25.640537 loss_ctc 27.356441 loss_rnnt 16.760084 hw_loss 0.178089 lr 0.00045999 rank 1
2023-02-23 04:41:52,482 DEBUG TRAIN Batch 14/1500 loss 4.563708 loss_att 8.316509 loss_ctc 8.705820 loss_rnnt 3.124050 hw_loss 0.256529 lr 0.00045972 rank 4
2023-02-23 04:41:52,487 DEBUG TRAIN Batch 14/1500 loss 15.697353 loss_att 20.283915 loss_ctc 18.436392 loss_rnnt 14.323017 hw_loss 0.172159 lr 0.00045970 rank 5
2023-02-23 04:41:52,488 DEBUG TRAIN Batch 14/1500 loss 8.626804 loss_att 13.564569 loss_ctc 11.537227 loss_rnnt 7.133190 hw_loss 0.221259 lr 0.00045973 rank 0
2023-02-23 04:41:52,489 DEBUG TRAIN Batch 14/1500 loss 8.108046 loss_att 11.814792 loss_ctc 10.087255 loss_rnnt 6.940631 hw_loss 0.304071 lr 0.00045980 rank 1
2023-02-23 04:41:52,490 DEBUG TRAIN Batch 14/1500 loss 14.100616 loss_att 17.089645 loss_ctc 19.422710 loss_rnnt 12.702096 hw_loss 0.170813 lr 0.00045972 rank 2
2023-02-23 04:41:52,491 DEBUG TRAIN Batch 14/1500 loss 6.370636 loss_att 9.038536 loss_ctc 9.072803 loss_rnnt 5.378570 hw_loss 0.184120 lr 0.00045968 rank 7
2023-02-23 04:41:52,493 DEBUG TRAIN Batch 14/1500 loss 20.855692 loss_att 23.901091 loss_ctc 26.423147 loss_rnnt 19.398636 hw_loss 0.198090 lr 0.00045970 rank 3
2023-02-23 04:41:52,493 DEBUG TRAIN Batch 14/1500 loss 9.127391 loss_att 14.337686 loss_ctc 11.435429 loss_rnnt 7.614020 hw_loss 0.306700 lr 0.00045976 rank 6
2023-02-23 04:43:08,527 DEBUG TRAIN Batch 14/1600 loss 21.200914 loss_att 23.998980 loss_ctc 25.742916 loss_rnnt 19.939816 hw_loss 0.179783 lr 0.00045951 rank 5
2023-02-23 04:43:08,531 DEBUG TRAIN Batch 14/1600 loss 17.724005 loss_att 20.400595 loss_ctc 22.287485 loss_rnnt 16.420130 hw_loss 0.300174 lr 0.00045956 rank 6
2023-02-23 04:43:08,535 DEBUG TRAIN Batch 14/1600 loss 5.159587 loss_att 7.937738 loss_ctc 5.702192 loss_rnnt 4.421125 hw_loss 0.207159 lr 0.00045953 rank 2
2023-02-23 04:43:08,535 DEBUG TRAIN Batch 14/1600 loss 16.829260 loss_att 18.531677 loss_ctc 21.855370 loss_rnnt 15.642165 hw_loss 0.330869 lr 0.00045954 rank 0
2023-02-23 04:43:08,536 DEBUG TRAIN Batch 14/1600 loss 15.122851 loss_att 17.665287 loss_ctc 19.623056 loss_rnnt 13.831470 hw_loss 0.342871 lr 0.00045960 rank 1
2023-02-23 04:43:08,537 DEBUG TRAIN Batch 14/1600 loss 12.547266 loss_att 16.885464 loss_ctc 18.641808 loss_rnnt 10.788796 hw_loss 0.146670 lr 0.00045953 rank 4
2023-02-23 04:43:08,539 DEBUG TRAIN Batch 14/1600 loss 9.727032 loss_att 14.297518 loss_ctc 14.003965 loss_rnnt 8.141733 hw_loss 0.189269 lr 0.00045950 rank 3
2023-02-23 04:43:08,541 DEBUG TRAIN Batch 14/1600 loss 13.199956 loss_att 17.696560 loss_ctc 13.803209 loss_rnnt 12.094718 hw_loss 0.235284 lr 0.00045949 rank 7
2023-02-23 04:44:24,640 DEBUG TRAIN Batch 14/1700 loss 8.504763 loss_att 10.039386 loss_ctc 13.763168 loss_rnnt 7.383993 hw_loss 0.211358 lr 0.00045934 rank 4
2023-02-23 04:44:24,640 DEBUG TRAIN Batch 14/1700 loss 12.971157 loss_att 13.979116 loss_ctc 13.304257 loss_rnnt 12.605054 hw_loss 0.225183 lr 0.00045934 rank 0
2023-02-23 04:44:24,642 DEBUG TRAIN Batch 14/1700 loss 6.910418 loss_att 11.868782 loss_ctc 9.857036 loss_rnnt 5.402783 hw_loss 0.230774 lr 0.00045929 rank 7
2023-02-23 04:44:24,644 DEBUG TRAIN Batch 14/1700 loss 10.123243 loss_att 13.981972 loss_ctc 11.463799 loss_rnnt 8.996315 hw_loss 0.330829 lr 0.00045931 rank 5
2023-02-23 04:44:24,643 DEBUG TRAIN Batch 14/1700 loss 18.765612 loss_att 19.444693 loss_ctc 19.013601 loss_rnnt 18.444366 hw_loss 0.285684 lr 0.00045937 rank 6
2023-02-23 04:44:24,644 DEBUG TRAIN Batch 14/1700 loss 9.986043 loss_att 11.746164 loss_ctc 14.658617 loss_rnnt 8.842536 hw_loss 0.315889 lr 0.00045933 rank 2
2023-02-23 04:44:24,644 DEBUG TRAIN Batch 14/1700 loss 14.369785 loss_att 18.632687 loss_ctc 16.438957 loss_rnnt 13.099979 hw_loss 0.265005 lr 0.00045941 rank 1
2023-02-23 04:44:24,655 DEBUG TRAIN Batch 14/1700 loss 7.026171 loss_att 11.788073 loss_ctc 9.628294 loss_rnnt 5.608076 hw_loss 0.222684 lr 0.00045931 rank 3
2023-02-23 04:45:43,338 DEBUG TRAIN Batch 14/1800 loss 13.461991 loss_att 13.045088 loss_ctc 15.940738 loss_rnnt 13.097633 hw_loss 0.219823 lr 0.00045912 rank 5
2023-02-23 04:45:43,345 DEBUG TRAIN Batch 14/1800 loss 8.035968 loss_att 11.502201 loss_ctc 10.367691 loss_rnnt 6.963257 hw_loss 0.128562 lr 0.00045911 rank 3
2023-02-23 04:45:43,346 DEBUG TRAIN Batch 14/1800 loss 11.796034 loss_att 11.822923 loss_ctc 15.320216 loss_rnnt 11.185511 hw_loss 0.253601 lr 0.00045914 rank 2
2023-02-23 04:45:43,346 DEBUG TRAIN Batch 14/1800 loss 11.732412 loss_att 16.026360 loss_ctc 15.386329 loss_rnnt 10.279174 hw_loss 0.201116 lr 0.00045914 rank 4
2023-02-23 04:45:43,347 DEBUG TRAIN Batch 14/1800 loss 12.097773 loss_att 15.641312 loss_ctc 17.334080 loss_rnnt 10.581431 hw_loss 0.205235 lr 0.00045915 rank 0
2023-02-23 04:45:43,348 DEBUG TRAIN Batch 14/1800 loss 12.727147 loss_att 14.779467 loss_ctc 16.155012 loss_rnnt 11.716613 hw_loss 0.268167 lr 0.00045917 rank 6
2023-02-23 04:45:43,350 DEBUG TRAIN Batch 14/1800 loss 18.486467 loss_att 19.141094 loss_ctc 21.587585 loss_rnnt 17.838148 hw_loss 0.194835 lr 0.00045910 rank 7
2023-02-23 04:45:43,398 DEBUG TRAIN Batch 14/1800 loss 22.498848 loss_att 21.335985 loss_ctc 28.045527 loss_rnnt 21.897871 hw_loss 0.176238 lr 0.00045921 rank 1
2023-02-23 04:47:00,086 DEBUG TRAIN Batch 14/1900 loss 7.227465 loss_att 12.486863 loss_ctc 8.733459 loss_rnnt 5.854014 hw_loss 0.226447 lr 0.00045895 rank 2
2023-02-23 04:47:00,086 DEBUG TRAIN Batch 14/1900 loss 9.430618 loss_att 9.371065 loss_ctc 11.764492 loss_rnnt 8.909735 hw_loss 0.415521 lr 0.00045896 rank 0
2023-02-23 04:47:00,091 DEBUG TRAIN Batch 14/1900 loss 11.496979 loss_att 12.558208 loss_ctc 15.273203 loss_rnnt 10.630557 hw_loss 0.282526 lr 0.00045892 rank 3
2023-02-23 04:47:00,093 DEBUG TRAIN Batch 14/1900 loss 14.629875 loss_att 13.227541 loss_ctc 17.706026 loss_rnnt 14.292908 hw_loss 0.388654 lr 0.00045891 rank 7
2023-02-23 04:47:00,095 DEBUG TRAIN Batch 14/1900 loss 13.993402 loss_att 19.114553 loss_ctc 16.269115 loss_rnnt 12.494295 hw_loss 0.321462 lr 0.00045898 rank 6
2023-02-23 04:47:00,119 DEBUG TRAIN Batch 14/1900 loss 15.641761 loss_att 15.817223 loss_ctc 18.023491 loss_rnnt 15.114050 hw_loss 0.328228 lr 0.00045895 rank 4
2023-02-23 04:47:00,127 DEBUG TRAIN Batch 14/1900 loss 5.635988 loss_att 7.923823 loss_ctc 5.283229 loss_rnnt 5.103199 hw_loss 0.229232 lr 0.00045892 rank 5
2023-02-23 04:47:00,138 DEBUG TRAIN Batch 14/1900 loss 17.035830 loss_att 21.113937 loss_ctc 18.648014 loss_rnnt 15.866447 hw_loss 0.260252 lr 0.00045902 rank 1
2023-02-23 04:48:18,175 DEBUG TRAIN Batch 14/2000 loss 14.211267 loss_att 16.557156 loss_ctc 18.960300 loss_rnnt 13.013239 hw_loss 0.179335 lr 0.00045873 rank 5
2023-02-23 04:48:18,185 DEBUG TRAIN Batch 14/2000 loss 12.421314 loss_att 16.297554 loss_ctc 14.059916 loss_rnnt 11.322413 hw_loss 0.197199 lr 0.00045873 rank 3
2023-02-23 04:48:18,185 DEBUG TRAIN Batch 14/2000 loss 7.581562 loss_att 9.250094 loss_ctc 6.345097 loss_rnnt 7.236734 hw_loss 0.329968 lr 0.00045875 rank 2
2023-02-23 04:48:18,186 DEBUG TRAIN Batch 14/2000 loss 7.995473 loss_att 11.253242 loss_ctc 11.445183 loss_rnnt 6.709173 hw_loss 0.327722 lr 0.00045876 rank 4
2023-02-23 04:48:18,188 DEBUG TRAIN Batch 14/2000 loss 12.524381 loss_att 15.451950 loss_ctc 17.036892 loss_rnnt 11.215660 hw_loss 0.227882 lr 0.00045876 rank 0
2023-02-23 04:48:18,192 DEBUG TRAIN Batch 14/2000 loss 12.088585 loss_att 16.187042 loss_ctc 14.612430 loss_rnnt 10.805151 hw_loss 0.238558 lr 0.00045883 rank 1
2023-02-23 04:48:18,194 DEBUG TRAIN Batch 14/2000 loss 11.887539 loss_att 15.401615 loss_ctc 11.875922 loss_rnnt 11.071963 hw_loss 0.214330 lr 0.00045879 rank 6
2023-02-23 04:48:18,232 DEBUG TRAIN Batch 14/2000 loss 18.699486 loss_att 24.693710 loss_ctc 22.097839 loss_rnnt 16.937708 hw_loss 0.205906 lr 0.00045871 rank 7
2023-02-23 04:49:38,068 DEBUG TRAIN Batch 14/2100 loss 6.818870 loss_att 11.830279 loss_ctc 7.626095 loss_rnnt 5.562973 hw_loss 0.273721 lr 0.00045853 rank 3
2023-02-23 04:49:38,069 DEBUG TRAIN Batch 14/2100 loss 11.544931 loss_att 17.558987 loss_ctc 15.762443 loss_rnnt 9.657270 hw_loss 0.229717 lr 0.00045857 rank 0
2023-02-23 04:49:38,073 DEBUG TRAIN Batch 14/2100 loss 12.552793 loss_att 14.300335 loss_ctc 17.351828 loss_rnnt 11.421911 hw_loss 0.265316 lr 0.00045859 rank 6
2023-02-23 04:49:38,072 DEBUG TRAIN Batch 14/2100 loss 10.649708 loss_att 16.844429 loss_ctc 15.373945 loss_rnnt 8.619651 hw_loss 0.302277 lr 0.00045856 rank 2
2023-02-23 04:49:38,076 DEBUG TRAIN Batch 14/2100 loss 10.510241 loss_att 11.390486 loss_ctc 11.631763 loss_rnnt 10.104437 hw_loss 0.150409 lr 0.00045856 rank 4
2023-02-23 04:49:38,078 DEBUG TRAIN Batch 14/2100 loss 22.938795 loss_att 24.758364 loss_ctc 30.562639 loss_rnnt 21.460390 hw_loss 0.183706 lr 0.00045854 rank 5
2023-02-23 04:49:38,078 DEBUG TRAIN Batch 14/2100 loss 11.038001 loss_att 14.698385 loss_ctc 14.448402 loss_rnnt 9.751018 hw_loss 0.187849 lr 0.00045852 rank 7
2023-02-23 04:49:38,114 DEBUG TRAIN Batch 14/2100 loss 17.453213 loss_att 20.053076 loss_ctc 20.753164 loss_rnnt 16.361216 hw_loss 0.247555 lr 0.00045863 rank 1
2023-02-23 04:50:54,647 DEBUG TRAIN Batch 14/2200 loss 8.182528 loss_att 13.907450 loss_ctc 12.271014 loss_rnnt 6.371016 hw_loss 0.227621 lr 0.00045838 rank 0
2023-02-23 04:50:54,649 DEBUG TRAIN Batch 14/2200 loss 10.970052 loss_att 13.202869 loss_ctc 15.231985 loss_rnnt 9.808924 hw_loss 0.274325 lr 0.00045833 rank 7
2023-02-23 04:50:54,652 DEBUG TRAIN Batch 14/2200 loss 6.071681 loss_att 10.731167 loss_ctc 8.478619 loss_rnnt 4.688246 hw_loss 0.244902 lr 0.00045835 rank 5
2023-02-23 04:50:54,653 DEBUG TRAIN Batch 14/2200 loss 12.483014 loss_att 17.456913 loss_ctc 19.163010 loss_rnnt 10.473150 hw_loss 0.233282 lr 0.00045840 rank 6
2023-02-23 04:50:54,654 DEBUG TRAIN Batch 14/2200 loss 14.750523 loss_att 20.992168 loss_ctc 20.505028 loss_rnnt 12.583269 hw_loss 0.284355 lr 0.00045837 rank 2
2023-02-23 04:50:54,655 DEBUG TRAIN Batch 14/2200 loss 4.955569 loss_att 8.646477 loss_ctc 6.024942 loss_rnnt 3.884854 hw_loss 0.356156 lr 0.00045837 rank 4
2023-02-23 04:50:54,656 DEBUG TRAIN Batch 14/2200 loss 13.227907 loss_att 14.868832 loss_ctc 14.065718 loss_rnnt 12.618423 hw_loss 0.317987 lr 0.00045844 rank 1
2023-02-23 04:50:54,656 DEBUG TRAIN Batch 14/2200 loss 10.580237 loss_att 12.486600 loss_ctc 13.408825 loss_rnnt 9.732643 hw_loss 0.167207 lr 0.00045834 rank 3
2023-02-23 04:52:10,616 DEBUG TRAIN Batch 14/2300 loss 36.629074 loss_att 36.890503 loss_ctc 45.710140 loss_rnnt 35.227180 hw_loss 0.260245 lr 0.00045818 rank 2
2023-02-23 04:52:10,622 DEBUG TRAIN Batch 14/2300 loss 4.618189 loss_att 7.329336 loss_ctc 6.855975 loss_rnnt 3.690726 hw_loss 0.162867 lr 0.00045815 rank 3
2023-02-23 04:52:10,623 DEBUG TRAIN Batch 14/2300 loss 15.370351 loss_att 16.266476 loss_ctc 16.408844 loss_rnnt 14.913406 hw_loss 0.261099 lr 0.00045818 rank 0
2023-02-23 04:52:10,625 DEBUG TRAIN Batch 14/2300 loss 9.132331 loss_att 11.355200 loss_ctc 12.494508 loss_rnnt 8.081933 hw_loss 0.295375 lr 0.00045815 rank 5
2023-02-23 04:52:10,625 DEBUG TRAIN Batch 14/2300 loss 10.413500 loss_att 13.118464 loss_ctc 11.872448 loss_rnnt 9.529310 hw_loss 0.278757 lr 0.00045818 rank 4
2023-02-23 04:52:10,627 DEBUG TRAIN Batch 14/2300 loss 17.330233 loss_att 22.774979 loss_ctc 27.161762 loss_rnnt 14.753214 hw_loss 0.332247 lr 0.00045814 rank 7
2023-02-23 04:52:10,628 DEBUG TRAIN Batch 14/2300 loss 4.960251 loss_att 8.608897 loss_ctc 6.351858 loss_rnnt 3.882443 hw_loss 0.304746 lr 0.00045821 rank 6
2023-02-23 04:52:10,632 DEBUG TRAIN Batch 14/2300 loss 17.794294 loss_att 20.536036 loss_ctc 23.681293 loss_rnnt 16.339428 hw_loss 0.227970 lr 0.00045825 rank 1
2023-02-23 04:53:26,058 DEBUG TRAIN Batch 14/2400 loss 17.430786 loss_att 19.100838 loss_ctc 20.272465 loss_rnnt 16.560333 hw_loss 0.295409 lr 0.00045796 rank 5
2023-02-23 04:53:26,059 DEBUG TRAIN Batch 14/2400 loss 11.883956 loss_att 17.904583 loss_ctc 15.456724 loss_rnnt 10.113385 hw_loss 0.168894 lr 0.00045799 rank 4
2023-02-23 04:53:26,060 DEBUG TRAIN Batch 14/2400 loss 11.679578 loss_att 16.134995 loss_ctc 16.012564 loss_rnnt 10.060134 hw_loss 0.282431 lr 0.00045799 rank 0
2023-02-23 04:53:26,060 DEBUG TRAIN Batch 14/2400 loss 9.193558 loss_att 11.651368 loss_ctc 13.071044 loss_rnnt 8.059953 hw_loss 0.234459 lr 0.00045796 rank 3
2023-02-23 04:53:26,063 DEBUG TRAIN Batch 14/2400 loss 11.949754 loss_att 16.411716 loss_ctc 14.535032 loss_rnnt 10.606031 hw_loss 0.199923 lr 0.00045802 rank 6
2023-02-23 04:53:26,064 DEBUG TRAIN Batch 14/2400 loss 10.332998 loss_att 12.788523 loss_ctc 13.748807 loss_rnnt 9.226367 hw_loss 0.300159 lr 0.00045798 rank 2
2023-02-23 04:53:26,065 DEBUG TRAIN Batch 14/2400 loss 10.043661 loss_att 12.477554 loss_ctc 12.928115 loss_rnnt 9.044724 hw_loss 0.239183 lr 0.00045794 rank 7
2023-02-23 04:53:26,112 DEBUG TRAIN Batch 14/2400 loss 15.263831 loss_att 18.352585 loss_ctc 23.867443 loss_rnnt 13.389408 hw_loss 0.205357 lr 0.00045806 rank 1
2023-02-23 04:54:45,999 DEBUG TRAIN Batch 14/2500 loss 24.883665 loss_att 31.745428 loss_ctc 34.050625 loss_rnnt 22.154922 hw_loss 0.251491 lr 0.00045779 rank 2
2023-02-23 04:54:46,001 DEBUG TRAIN Batch 14/2500 loss 10.842531 loss_att 11.356303 loss_ctc 12.845549 loss_rnnt 10.315433 hw_loss 0.294892 lr 0.00045779 rank 4
2023-02-23 04:54:46,002 DEBUG TRAIN Batch 14/2500 loss 34.221489 loss_att 35.961868 loss_ctc 38.400330 loss_rnnt 33.208858 hw_loss 0.201325 lr 0.00045777 rank 5
2023-02-23 04:54:46,002 DEBUG TRAIN Batch 14/2500 loss 10.633597 loss_att 11.956861 loss_ctc 12.952879 loss_rnnt 9.886869 hw_loss 0.324070 lr 0.00045775 rank 7
2023-02-23 04:54:46,003 DEBUG TRAIN Batch 14/2500 loss 9.854774 loss_att 9.741798 loss_ctc 14.053234 loss_rnnt 9.157216 hw_loss 0.300673 lr 0.00045782 rank 6
2023-02-23 04:54:46,003 DEBUG TRAIN Batch 14/2500 loss 17.411163 loss_att 23.908636 loss_ctc 21.983624 loss_rnnt 15.365026 hw_loss 0.256839 lr 0.00045776 rank 3
2023-02-23 04:54:46,004 DEBUG TRAIN Batch 14/2500 loss 10.883026 loss_att 10.716810 loss_ctc 13.840288 loss_rnnt 10.372637 hw_loss 0.279994 lr 0.00045780 rank 0
2023-02-23 04:54:46,008 DEBUG TRAIN Batch 14/2500 loss 7.792045 loss_att 8.253009 loss_ctc 9.761889 loss_rnnt 7.261709 hw_loss 0.329058 lr 0.00045786 rank 1
2023-02-23 04:56:01,837 DEBUG TRAIN Batch 14/2600 loss 7.876682 loss_att 11.924184 loss_ctc 14.035281 loss_rnnt 6.114589 hw_loss 0.246461 lr 0.00045758 rank 5
2023-02-23 04:56:01,840 DEBUG TRAIN Batch 14/2600 loss 12.736338 loss_att 18.418211 loss_ctc 19.720127 loss_rnnt 10.541721 hw_loss 0.238257 lr 0.00045760 rank 4
2023-02-23 04:56:01,842 DEBUG TRAIN Batch 14/2600 loss 6.313995 loss_att 10.469166 loss_ctc 6.733186 loss_rnnt 5.264588 hw_loss 0.304650 lr 0.00045760 rank 2
2023-02-23 04:56:01,842 DEBUG TRAIN Batch 14/2600 loss 6.004930 loss_att 7.082146 loss_ctc 6.530115 loss_rnnt 5.609044 hw_loss 0.207034 lr 0.00045757 rank 3
2023-02-23 04:56:01,845 DEBUG TRAIN Batch 14/2600 loss 25.437056 loss_att 33.577366 loss_ctc 34.624416 loss_rnnt 22.509222 hw_loss 0.140233 lr 0.00045761 rank 0
2023-02-23 04:56:01,847 DEBUG TRAIN Batch 14/2600 loss 14.896972 loss_att 18.652172 loss_ctc 17.605959 loss_rnnt 13.671360 hw_loss 0.212573 lr 0.00045767 rank 1
2023-02-23 04:56:01,847 DEBUG TRAIN Batch 14/2600 loss 18.631872 loss_att 21.462938 loss_ctc 21.663788 loss_rnnt 17.514244 hw_loss 0.275924 lr 0.00045763 rank 6
2023-02-23 04:56:01,853 DEBUG TRAIN Batch 14/2600 loss 8.814538 loss_att 10.296031 loss_ctc 13.082041 loss_rnnt 7.851093 hw_loss 0.184024 lr 0.00045756 rank 7
2023-02-23 04:57:17,088 DEBUG TRAIN Batch 14/2700 loss 9.211331 loss_att 11.977308 loss_ctc 11.494802 loss_rnnt 8.229338 hw_loss 0.233129 lr 0.00045744 rank 6
2023-02-23 04:57:17,090 DEBUG TRAIN Batch 14/2700 loss 9.937456 loss_att 12.276142 loss_ctc 12.361763 loss_rnnt 9.026699 hw_loss 0.224584 lr 0.00045739 rank 5
2023-02-23 04:57:17,091 DEBUG TRAIN Batch 14/2700 loss 10.089943 loss_att 10.609787 loss_ctc 10.343012 loss_rnnt 9.835321 hw_loss 0.219208 lr 0.00045737 rank 7
2023-02-23 04:57:17,091 DEBUG TRAIN Batch 14/2700 loss 24.251392 loss_att 23.454107 loss_ctc 27.481644 loss_rnnt 23.870634 hw_loss 0.205343 lr 0.00045741 rank 2
2023-02-23 04:57:17,090 DEBUG TRAIN Batch 14/2700 loss 3.459643 loss_att 7.414773 loss_ctc 5.192977 loss_rnnt 2.357833 hw_loss 0.149387 lr 0.00045741 rank 4
2023-02-23 04:57:17,094 DEBUG TRAIN Batch 14/2700 loss 10.938960 loss_att 12.440466 loss_ctc 15.652304 loss_rnnt 9.889744 hw_loss 0.225880 lr 0.00045742 rank 0
2023-02-23 04:57:17,095 DEBUG TRAIN Batch 14/2700 loss 8.321935 loss_att 13.803753 loss_ctc 9.710723 loss_rnnt 6.977688 hw_loss 0.117583 lr 0.00045748 rank 1
2023-02-23 04:57:17,098 DEBUG TRAIN Batch 14/2700 loss 6.110812 loss_att 10.993016 loss_ctc 7.200363 loss_rnnt 4.839956 hw_loss 0.279639 lr 0.00045738 rank 3
2023-02-23 04:58:34,583 DEBUG TRAIN Batch 14/2800 loss 14.645771 loss_att 18.882048 loss_ctc 19.998381 loss_rnnt 12.979671 hw_loss 0.197181 lr 0.00045722 rank 4
2023-02-23 04:58:34,584 DEBUG TRAIN Batch 14/2800 loss 9.249804 loss_att 10.298906 loss_ctc 11.789557 loss_rnnt 8.580656 hw_loss 0.226299 lr 0.00045722 rank 2
2023-02-23 04:58:34,588 DEBUG TRAIN Batch 14/2800 loss 18.985056 loss_att 23.020172 loss_ctc 24.907160 loss_rnnt 17.229397 hw_loss 0.298167 lr 0.00045719 rank 5
2023-02-23 04:58:34,589 DEBUG TRAIN Batch 14/2800 loss 14.259990 loss_att 16.228012 loss_ctc 18.666977 loss_rnnt 13.148978 hw_loss 0.243391 lr 0.00045719 rank 3
2023-02-23 04:58:34,590 DEBUG TRAIN Batch 14/2800 loss 14.188891 loss_att 16.187550 loss_ctc 15.577273 loss_rnnt 13.410873 hw_loss 0.362190 lr 0.00045722 rank 0
2023-02-23 04:58:34,607 DEBUG TRAIN Batch 14/2800 loss 7.722925 loss_att 13.012010 loss_ctc 9.922084 loss_rnnt 6.200041 hw_loss 0.322210 lr 0.00045729 rank 1
2023-02-23 04:58:34,612 DEBUG TRAIN Batch 14/2800 loss 8.649145 loss_att 12.201239 loss_ctc 11.672538 loss_rnnt 7.401371 hw_loss 0.251694 lr 0.00045718 rank 7
2023-02-23 04:58:34,633 DEBUG TRAIN Batch 14/2800 loss 10.918169 loss_att 14.826003 loss_ctc 13.977114 loss_rnnt 9.544795 hw_loss 0.344902 lr 0.00045725 rank 6
2023-02-23 04:59:52,171 DEBUG TRAIN Batch 14/2900 loss 4.415875 loss_att 7.704619 loss_ctc 5.788664 loss_rnnt 3.509421 hw_loss 0.123125 lr 0.00045700 rank 5
2023-02-23 04:59:52,172 DEBUG TRAIN Batch 14/2900 loss 6.429104 loss_att 10.729784 loss_ctc 8.975429 loss_rnnt 5.095826 hw_loss 0.250559 lr 0.00045700 rank 3
2023-02-23 04:59:52,174 DEBUG TRAIN Batch 14/2900 loss 10.817807 loss_att 11.055420 loss_ctc 10.766629 loss_rnnt 10.618479 hw_loss 0.297432 lr 0.00045703 rank 4
2023-02-23 04:59:52,175 DEBUG TRAIN Batch 14/2900 loss 12.295322 loss_att 12.897560 loss_ctc 12.761038 loss_rnnt 11.959603 hw_loss 0.287203 lr 0.00045703 rank 2
2023-02-23 04:59:52,175 DEBUG TRAIN Batch 14/2900 loss 9.221404 loss_att 12.842722 loss_ctc 13.374907 loss_rnnt 7.877159 hw_loss 0.124087 lr 0.00045703 rank 0
2023-02-23 04:59:52,177 DEBUG TRAIN Batch 14/2900 loss 22.808298 loss_att 28.988949 loss_ctc 31.859211 loss_rnnt 20.244507 hw_loss 0.226636 lr 0.00045706 rank 6
2023-02-23 04:59:52,179 DEBUG TRAIN Batch 14/2900 loss 9.823225 loss_att 12.486986 loss_ctc 12.147102 loss_rnnt 8.841075 hw_loss 0.261653 lr 0.00045710 rank 1
2023-02-23 04:59:52,187 DEBUG TRAIN Batch 14/2900 loss 9.959684 loss_att 16.098366 loss_ctc 14.295394 loss_rnnt 8.032293 hw_loss 0.227926 lr 0.00045699 rank 7
2023-02-23 05:01:08,625 DEBUG TRAIN Batch 14/3000 loss 17.128687 loss_att 19.528881 loss_ctc 24.238892 loss_rnnt 15.537411 hw_loss 0.306020 lr 0.00045687 rank 6
2023-02-23 05:01:08,627 DEBUG TRAIN Batch 14/3000 loss 13.151639 loss_att 17.189234 loss_ctc 18.093468 loss_rnnt 11.560457 hw_loss 0.233907 lr 0.00045681 rank 5
2023-02-23 05:01:08,628 DEBUG TRAIN Batch 14/3000 loss 19.771273 loss_att 22.463394 loss_ctc 22.587055 loss_rnnt 18.751562 hw_loss 0.198468 lr 0.00045684 rank 0
2023-02-23 05:01:08,628 DEBUG TRAIN Batch 14/3000 loss 10.769617 loss_att 12.772984 loss_ctc 16.459604 loss_rnnt 9.489854 hw_loss 0.225794 lr 0.00045684 rank 2
2023-02-23 05:01:08,629 DEBUG TRAIN Batch 14/3000 loss 7.846316 loss_att 9.325180 loss_ctc 10.490971 loss_rnnt 7.078907 hw_loss 0.223154 lr 0.00045684 rank 4
2023-02-23 05:01:08,631 DEBUG TRAIN Batch 14/3000 loss 4.724378 loss_att 8.111138 loss_ctc 6.356650 loss_rnnt 3.703769 hw_loss 0.235540 lr 0.00045691 rank 1
2023-02-23 05:01:08,632 DEBUG TRAIN Batch 14/3000 loss 17.715822 loss_att 23.039635 loss_ctc 27.928724 loss_rnnt 15.190674 hw_loss 0.184997 lr 0.00045681 rank 3
2023-02-23 05:01:08,677 DEBUG TRAIN Batch 14/3000 loss 14.561749 loss_att 18.294613 loss_ctc 21.746008 loss_rnnt 12.704895 hw_loss 0.285715 lr 0.00045680 rank 7
2023-02-23 05:02:24,979 DEBUG TRAIN Batch 14/3100 loss 7.628751 loss_att 9.165526 loss_ctc 9.601732 loss_rnnt 6.949094 hw_loss 0.204822 lr 0.00045665 rank 4
2023-02-23 05:02:24,985 DEBUG TRAIN Batch 14/3100 loss 10.651799 loss_att 10.523354 loss_ctc 13.318308 loss_rnnt 10.125061 hw_loss 0.369175 lr 0.00045662 rank 5
2023-02-23 05:02:24,989 DEBUG TRAIN Batch 14/3100 loss 10.283262 loss_att 13.375546 loss_ctc 13.378135 loss_rnnt 9.123492 hw_loss 0.241243 lr 0.00045660 rank 7
2023-02-23 05:02:24,992 DEBUG TRAIN Batch 14/3100 loss 8.406361 loss_att 7.962339 loss_ctc 9.650986 loss_rnnt 8.129308 hw_loss 0.374825 lr 0.00045668 rank 6
2023-02-23 05:02:24,992 DEBUG TRAIN Batch 14/3100 loss 14.037773 loss_att 15.010475 loss_ctc 17.455101 loss_rnnt 13.215467 hw_loss 0.322728 lr 0.00045664 rank 2
2023-02-23 05:02:24,992 DEBUG TRAIN Batch 14/3100 loss 11.248326 loss_att 13.379837 loss_ctc 15.410128 loss_rnnt 10.104637 hw_loss 0.304649 lr 0.00045665 rank 0
2023-02-23 05:02:24,992 DEBUG TRAIN Batch 14/3100 loss 10.980537 loss_att 11.769654 loss_ctc 13.692688 loss_rnnt 10.322350 hw_loss 0.260145 lr 0.00045662 rank 3
2023-02-23 05:02:25,001 DEBUG TRAIN Batch 14/3100 loss 6.375007 loss_att 7.915200 loss_ctc 8.311963 loss_rnnt 5.683677 hw_loss 0.234432 lr 0.00045672 rank 1
2023-02-23 05:03:44,690 DEBUG TRAIN Batch 14/3200 loss 24.193800 loss_att 26.403265 loss_ctc 30.049393 loss_rnnt 22.800785 hw_loss 0.319450 lr 0.00045643 rank 3
2023-02-23 05:03:44,691 DEBUG TRAIN Batch 14/3200 loss 11.642459 loss_att 12.294525 loss_ctc 15.142004 loss_rnnt 10.875395 hw_loss 0.318834 lr 0.00045641 rank 7
2023-02-23 05:03:44,692 DEBUG TRAIN Batch 14/3200 loss 11.345357 loss_att 14.745625 loss_ctc 12.089786 loss_rnnt 10.424755 hw_loss 0.264922 lr 0.00045643 rank 5
2023-02-23 05:03:44,693 DEBUG TRAIN Batch 14/3200 loss 28.353245 loss_att 35.957672 loss_ctc 47.711884 loss_rnnt 24.115965 hw_loss 0.253582 lr 0.00045646 rank 0
2023-02-23 05:03:44,694 DEBUG TRAIN Batch 14/3200 loss 12.764275 loss_att 18.200600 loss_ctc 18.414539 loss_rnnt 10.779045 hw_loss 0.271117 lr 0.00045649 rank 6
2023-02-23 05:03:44,695 DEBUG TRAIN Batch 14/3200 loss 24.391640 loss_att 24.789078 loss_ctc 29.053692 loss_rnnt 23.614187 hw_loss 0.143176 lr 0.00045646 rank 4
2023-02-23 05:03:44,703 DEBUG TRAIN Batch 14/3200 loss 10.969927 loss_att 16.702635 loss_ctc 17.752098 loss_rnnt 8.775496 hw_loss 0.269250 lr 0.00045645 rank 2
2023-02-23 05:03:44,738 DEBUG TRAIN Batch 14/3200 loss 31.620468 loss_att 33.604496 loss_ctc 35.443607 loss_rnnt 30.644299 hw_loss 0.130523 lr 0.00045653 rank 1
2023-02-23 05:04:59,796 DEBUG TRAIN Batch 14/3300 loss 20.350622 loss_att 27.174679 loss_ctc 23.697266 loss_rnnt 18.393888 hw_loss 0.273192 lr 0.00045626 rank 2
2023-02-23 05:04:59,799 DEBUG TRAIN Batch 14/3300 loss 9.083364 loss_att 13.909870 loss_ctc 12.228476 loss_rnnt 7.607295 hw_loss 0.171411 lr 0.00045624 rank 5
2023-02-23 05:04:59,802 DEBUG TRAIN Batch 14/3300 loss 9.885208 loss_att 13.695765 loss_ctc 15.424845 loss_rnnt 8.214872 hw_loss 0.318012 lr 0.00045627 rank 4
2023-02-23 05:04:59,803 DEBUG TRAIN Batch 14/3300 loss 17.742926 loss_att 19.644600 loss_ctc 19.045532 loss_rnnt 17.045174 hw_loss 0.269503 lr 0.00045627 rank 0
2023-02-23 05:04:59,805 DEBUG TRAIN Batch 14/3300 loss 16.560751 loss_att 22.741613 loss_ctc 24.453991 loss_rnnt 14.130095 hw_loss 0.266345 lr 0.00045630 rank 6
2023-02-23 05:04:59,806 DEBUG TRAIN Batch 14/3300 loss 19.383799 loss_att 26.674128 loss_ctc 29.506756 loss_rnnt 16.452034 hw_loss 0.232442 lr 0.00045622 rank 7
2023-02-23 05:04:59,809 DEBUG TRAIN Batch 14/3300 loss 14.115354 loss_att 13.342539 loss_ctc 16.054447 loss_rnnt 13.838847 hw_loss 0.323482 lr 0.00045624 rank 3
2023-02-23 05:04:59,810 DEBUG TRAIN Batch 14/3300 loss 8.105227 loss_att 9.815624 loss_ctc 12.769056 loss_rnnt 7.045039 hw_loss 0.180496 lr 0.00045634 rank 1
2023-02-23 05:06:15,482 DEBUG TRAIN Batch 14/3400 loss 9.020784 loss_att 11.411487 loss_ctc 11.141014 loss_rnnt 8.141380 hw_loss 0.222312 lr 0.00045605 rank 5
2023-02-23 05:06:15,483 DEBUG TRAIN Batch 14/3400 loss 11.501847 loss_att 12.905146 loss_ctc 14.777965 loss_rnnt 10.680828 hw_loss 0.194145 lr 0.00045608 rank 0
2023-02-23 05:06:15,488 DEBUG TRAIN Batch 14/3400 loss 45.821545 loss_att 48.162201 loss_ctc 56.943291 loss_rnnt 43.759918 hw_loss 0.207371 lr 0.00045607 rank 2
2023-02-23 05:06:15,488 DEBUG TRAIN Batch 14/3400 loss 7.318212 loss_att 8.785856 loss_ctc 8.304813 loss_rnnt 6.797894 hw_loss 0.178579 lr 0.00045608 rank 4
2023-02-23 05:06:15,490 DEBUG TRAIN Batch 14/3400 loss 20.992744 loss_att 22.702127 loss_ctc 25.333067 loss_rnnt 19.933838 hw_loss 0.259352 lr 0.00045615 rank 1
2023-02-23 05:06:15,491 DEBUG TRAIN Batch 14/3400 loss 8.186590 loss_att 11.459682 loss_ctc 10.052551 loss_rnnt 7.180675 hw_loss 0.192192 lr 0.00045603 rank 7
2023-02-23 05:06:15,493 DEBUG TRAIN Batch 14/3400 loss 13.316113 loss_att 15.980831 loss_ctc 14.963266 loss_rnnt 12.482376 hw_loss 0.152197 lr 0.00045611 rank 6
2023-02-23 05:06:15,492 DEBUG TRAIN Batch 14/3400 loss 15.705073 loss_att 18.181227 loss_ctc 20.096508 loss_rnnt 14.541687 hw_loss 0.154931 lr 0.00045605 rank 3
2023-02-23 05:07:31,787 DEBUG TRAIN Batch 14/3500 loss 13.869879 loss_att 14.807349 loss_ctc 14.755472 loss_rnnt 13.438046 hw_loss 0.236738 lr 0.00045589 rank 0
2023-02-23 05:07:31,791 DEBUG TRAIN Batch 14/3500 loss 9.849922 loss_att 12.646356 loss_ctc 12.577991 loss_rnnt 8.794802 hw_loss 0.247670 lr 0.00045596 rank 1
2023-02-23 05:07:31,791 DEBUG TRAIN Batch 14/3500 loss 14.933848 loss_att 18.757078 loss_ctc 21.418358 loss_rnnt 13.166428 hw_loss 0.259073 lr 0.00045589 rank 4
2023-02-23 05:07:31,792 DEBUG TRAIN Batch 14/3500 loss 14.474982 loss_att 16.994293 loss_ctc 17.342388 loss_rnnt 13.415974 hw_loss 0.324050 lr 0.00045588 rank 2
2023-02-23 05:07:31,794 DEBUG TRAIN Batch 14/3500 loss 24.363516 loss_att 26.092346 loss_ctc 28.960747 loss_rnnt 23.224024 hw_loss 0.338927 lr 0.00045585 rank 7
2023-02-23 05:07:31,795 DEBUG TRAIN Batch 14/3500 loss 13.950376 loss_att 18.397638 loss_ctc 19.680853 loss_rnnt 12.223422 hw_loss 0.137694 lr 0.00045586 rank 3
2023-02-23 05:07:31,797 DEBUG TRAIN Batch 14/3500 loss 6.158602 loss_att 11.199156 loss_ctc 10.332125 loss_rnnt 4.470557 hw_loss 0.231495 lr 0.00045592 rank 6
2023-02-23 05:07:31,805 DEBUG TRAIN Batch 14/3500 loss 27.071005 loss_att 26.607763 loss_ctc 34.205734 loss_rnnt 26.119617 hw_loss 0.173880 lr 0.00045586 rank 5
2023-02-23 05:08:49,914 DEBUG TRAIN Batch 14/3600 loss 13.118892 loss_att 13.956408 loss_ctc 16.896893 loss_rnnt 12.328001 hw_loss 0.224353 lr 0.00045570 rank 2
2023-02-23 05:08:49,915 DEBUG TRAIN Batch 14/3600 loss 10.409143 loss_att 14.218325 loss_ctc 12.765822 loss_rnnt 9.203028 hw_loss 0.243855 lr 0.00045567 rank 3
2023-02-23 05:08:49,917 DEBUG TRAIN Batch 14/3600 loss 13.749001 loss_att 16.333628 loss_ctc 19.546761 loss_rnnt 12.305153 hw_loss 0.288540 lr 0.00045570 rank 4
2023-02-23 05:08:49,917 DEBUG TRAIN Batch 14/3600 loss 21.098898 loss_att 23.995930 loss_ctc 29.631422 loss_rnnt 19.208649 hw_loss 0.324699 lr 0.00045567 rank 5
2023-02-23 05:08:49,919 DEBUG TRAIN Batch 14/3600 loss 7.032618 loss_att 10.454943 loss_ctc 9.344939 loss_rnnt 5.929958 hw_loss 0.206033 lr 0.00045573 rank 6
2023-02-23 05:08:49,921 DEBUG TRAIN Batch 14/3600 loss 10.762738 loss_att 13.880518 loss_ctc 15.001097 loss_rnnt 9.391264 hw_loss 0.342755 lr 0.00045570 rank 0
2023-02-23 05:08:49,921 DEBUG TRAIN Batch 14/3600 loss 21.936382 loss_att 23.774448 loss_ctc 27.240261 loss_rnnt 20.773195 hw_loss 0.165729 lr 0.00045577 rank 1
2023-02-23 05:08:49,927 DEBUG TRAIN Batch 14/3600 loss 7.359299 loss_att 10.120301 loss_ctc 10.475110 loss_rnnt 6.255944 hw_loss 0.254459 lr 0.00045566 rank 7
2023-02-23 05:10:05,844 DEBUG TRAIN Batch 14/3700 loss 11.682535 loss_att 11.310647 loss_ctc 16.071615 loss_rnnt 10.963591 hw_loss 0.390210 lr 0.00045548 rank 5
2023-02-23 05:10:05,847 DEBUG TRAIN Batch 14/3700 loss 13.834449 loss_att 15.077421 loss_ctc 19.074081 loss_rnnt 12.731727 hw_loss 0.291583 lr 0.00045558 rank 1
2023-02-23 05:10:05,848 DEBUG TRAIN Batch 14/3700 loss 14.370735 loss_att 17.959297 loss_ctc 19.363657 loss_rnnt 12.855843 hw_loss 0.246481 lr 0.00045551 rank 4
2023-02-23 05:10:05,849 DEBUG TRAIN Batch 14/3700 loss 13.579917 loss_att 16.040546 loss_ctc 15.761050 loss_rnnt 12.665172 hw_loss 0.247129 lr 0.00045551 rank 0
2023-02-23 05:10:05,850 DEBUG TRAIN Batch 14/3700 loss 18.843681 loss_att 23.098846 loss_ctc 25.136465 loss_rnnt 17.044697 hw_loss 0.204213 lr 0.00045551 rank 2
2023-02-23 05:10:05,851 DEBUG TRAIN Batch 14/3700 loss 14.204219 loss_att 15.931173 loss_ctc 22.618530 loss_rnnt 12.595338 hw_loss 0.265465 lr 0.00045548 rank 3
2023-02-23 05:10:05,851 DEBUG TRAIN Batch 14/3700 loss 15.805836 loss_att 17.491856 loss_ctc 20.523682 loss_rnnt 14.655247 hw_loss 0.345636 lr 0.00045554 rank 6
2023-02-23 05:10:05,854 DEBUG TRAIN Batch 14/3700 loss 10.918762 loss_att 12.514221 loss_ctc 17.552362 loss_rnnt 9.588288 hw_loss 0.237939 lr 0.00045547 rank 7
2023-02-23 05:11:21,420 DEBUG TRAIN Batch 14/3800 loss 12.446801 loss_att 15.084772 loss_ctc 14.986438 loss_rnnt 11.443634 hw_loss 0.256788 lr 0.00045533 rank 0
2023-02-23 05:11:21,427 DEBUG TRAIN Batch 14/3800 loss 20.648001 loss_att 21.425014 loss_ctc 27.237118 loss_rnnt 19.500898 hw_loss 0.212160 lr 0.00045532 rank 4
2023-02-23 05:11:21,428 DEBUG TRAIN Batch 14/3800 loss 18.249239 loss_att 22.733156 loss_ctc 21.708715 loss_rnnt 16.748539 hw_loss 0.267472 lr 0.00045529 rank 5
2023-02-23 05:11:21,432 DEBUG TRAIN Batch 14/3800 loss 10.331564 loss_att 10.125386 loss_ctc 12.895457 loss_rnnt 9.901760 hw_loss 0.242224 lr 0.00045532 rank 2
2023-02-23 05:11:21,433 DEBUG TRAIN Batch 14/3800 loss 8.814809 loss_att 14.067589 loss_ctc 12.449503 loss_rnnt 7.150012 hw_loss 0.243028 lr 0.00045529 rank 3
2023-02-23 05:11:21,437 DEBUG TRAIN Batch 14/3800 loss 11.107165 loss_att 19.104218 loss_ctc 13.875224 loss_rnnt 9.057879 hw_loss 0.151502 lr 0.00045535 rank 6
2023-02-23 05:11:21,439 DEBUG TRAIN Batch 14/3800 loss 11.403191 loss_att 11.986482 loss_ctc 14.917327 loss_rnnt 10.702080 hw_loss 0.217313 lr 0.00045528 rank 7
2023-02-23 05:11:21,441 DEBUG TRAIN Batch 14/3800 loss 14.392543 loss_att 15.932394 loss_ctc 17.066839 loss_rnnt 13.555299 hw_loss 0.323812 lr 0.00045539 rank 1
2023-02-23 05:12:39,918 DEBUG TRAIN Batch 14/3900 loss 8.504323 loss_att 11.208456 loss_ctc 12.553421 loss_rnnt 7.289594 hw_loss 0.251293 lr 0.00045513 rank 4
2023-02-23 05:12:39,920 DEBUG TRAIN Batch 14/3900 loss 12.555020 loss_att 15.476873 loss_ctc 15.380946 loss_rnnt 11.472316 hw_loss 0.227895 lr 0.00045511 rank 5
2023-02-23 05:12:39,924 DEBUG TRAIN Batch 14/3900 loss 19.382856 loss_att 24.638969 loss_ctc 29.463409 loss_rnnt 16.830099 hw_loss 0.295244 lr 0.00045513 rank 2
2023-02-23 05:12:39,924 DEBUG TRAIN Batch 14/3900 loss 16.077379 loss_att 19.991287 loss_ctc 21.728531 loss_rnnt 14.419632 hw_loss 0.227770 lr 0.00045514 rank 0
2023-02-23 05:12:39,926 DEBUG TRAIN Batch 14/3900 loss 15.898754 loss_att 19.915319 loss_ctc 23.811466 loss_rnnt 13.892577 hw_loss 0.277192 lr 0.00045509 rank 7
2023-02-23 05:12:39,931 DEBUG TRAIN Batch 14/3900 loss 15.061710 loss_att 22.936150 loss_ctc 17.874393 loss_rnnt 12.988674 hw_loss 0.230857 lr 0.00045516 rank 6
2023-02-23 05:12:39,935 DEBUG TRAIN Batch 14/3900 loss 8.570235 loss_att 9.260580 loss_ctc 10.605392 loss_rnnt 7.977773 hw_loss 0.343199 lr 0.00045510 rank 3
2023-02-23 05:12:39,937 DEBUG TRAIN Batch 14/3900 loss 19.858238 loss_att 25.287674 loss_ctc 28.056969 loss_rnnt 17.614454 hw_loss 0.121376 lr 0.00045520 rank 1
2023-02-23 05:13:56,122 DEBUG TRAIN Batch 14/4000 loss 14.971841 loss_att 16.004087 loss_ctc 17.830002 loss_rnnt 14.223282 hw_loss 0.301917 lr 0.00045494 rank 4
2023-02-23 05:13:56,125 DEBUG TRAIN Batch 14/4000 loss 17.970514 loss_att 21.262110 loss_ctc 25.230480 loss_rnnt 16.193562 hw_loss 0.282445 lr 0.00045492 rank 5
2023-02-23 05:13:56,127 DEBUG TRAIN Batch 14/4000 loss 10.072035 loss_att 13.574535 loss_ctc 11.178785 loss_rnnt 9.127923 hw_loss 0.180082 lr 0.00045491 rank 3
2023-02-23 05:13:56,128 DEBUG TRAIN Batch 14/4000 loss 14.185516 loss_att 17.382792 loss_ctc 17.121250 loss_rnnt 13.043988 hw_loss 0.207452 lr 0.00045497 rank 6
2023-02-23 05:13:56,129 DEBUG TRAIN Batch 14/4000 loss 7.384694 loss_att 10.891439 loss_ctc 11.164593 loss_rnnt 6.064203 hw_loss 0.215917 lr 0.00045494 rank 2
2023-02-23 05:13:56,131 DEBUG TRAIN Batch 14/4000 loss 22.319593 loss_att 21.146683 loss_ctc 24.976231 loss_rnnt 22.044731 hw_loss 0.291046 lr 0.00045495 rank 0
2023-02-23 05:13:56,133 DEBUG TRAIN Batch 14/4000 loss 4.168362 loss_att 7.940348 loss_ctc 5.507509 loss_rnnt 3.092475 hw_loss 0.268007 lr 0.00045501 rank 1
2023-02-23 05:13:56,133 DEBUG TRAIN Batch 14/4000 loss 4.249467 loss_att 8.810457 loss_ctc 9.739758 loss_rnnt 2.460051 hw_loss 0.272212 lr 0.00045490 rank 7
2023-02-23 05:15:11,532 DEBUG TRAIN Batch 14/4100 loss 12.012328 loss_att 15.705158 loss_ctc 17.770950 loss_rnnt 10.361452 hw_loss 0.270925 lr 0.00045475 rank 2
2023-02-23 05:15:11,532 DEBUG TRAIN Batch 14/4100 loss 11.654298 loss_att 18.515507 loss_ctc 14.857815 loss_rnnt 9.737724 hw_loss 0.219744 lr 0.00045482 rank 1
2023-02-23 05:15:11,534 DEBUG TRAIN Batch 14/4100 loss 6.914496 loss_att 9.909156 loss_ctc 7.742157 loss_rnnt 6.108995 hw_loss 0.180402 lr 0.00045473 rank 3
2023-02-23 05:15:11,536 DEBUG TRAIN Batch 14/4100 loss 14.077170 loss_att 16.264414 loss_ctc 18.166138 loss_rnnt 12.946405 hw_loss 0.277725 lr 0.00045473 rank 5
2023-02-23 05:15:11,537 DEBUG TRAIN Batch 14/4100 loss 16.665075 loss_att 20.646334 loss_ctc 18.958288 loss_rnnt 15.459056 hw_loss 0.195014 lr 0.00045471 rank 7
2023-02-23 05:15:11,537 DEBUG TRAIN Batch 14/4100 loss 7.878024 loss_att 11.893885 loss_ctc 13.418005 loss_rnnt 6.205822 hw_loss 0.244434 lr 0.00045476 rank 0
2023-02-23 05:15:11,539 DEBUG TRAIN Batch 14/4100 loss 11.134725 loss_att 18.426918 loss_ctc 14.944920 loss_rnnt 9.010937 hw_loss 0.294980 lr 0.00045475 rank 4
2023-02-23 05:15:11,539 DEBUG TRAIN Batch 14/4100 loss 14.183517 loss_att 15.990209 loss_ctc 20.180273 loss_rnnt 12.932407 hw_loss 0.169135 lr 0.00045478 rank 6
2023-02-23 05:16:28,009 DEBUG TRAIN Batch 14/4200 loss 15.497072 loss_att 19.778467 loss_ctc 17.894135 loss_rnnt 14.175322 hw_loss 0.273496 lr 0.00045454 rank 5
2023-02-23 05:16:28,012 DEBUG TRAIN Batch 14/4200 loss 8.280652 loss_att 10.281038 loss_ctc 9.070330 loss_rnnt 7.650297 hw_loss 0.234352 lr 0.00045456 rank 2
2023-02-23 05:16:28,012 DEBUG TRAIN Batch 14/4200 loss 7.354316 loss_att 10.215771 loss_ctc 10.050118 loss_rnnt 6.286241 hw_loss 0.255643 lr 0.00045460 rank 6
2023-02-23 05:16:28,014 DEBUG TRAIN Batch 14/4200 loss 12.473201 loss_att 13.227568 loss_ctc 14.840857 loss_rnnt 11.905647 hw_loss 0.189358 lr 0.00045454 rank 3
2023-02-23 05:16:28,014 DEBUG TRAIN Batch 14/4200 loss 15.192092 loss_att 17.454384 loss_ctc 24.874886 loss_rnnt 13.400846 hw_loss 0.089528 lr 0.00045457 rank 0
2023-02-23 05:16:28,016 DEBUG TRAIN Batch 14/4200 loss 17.096239 loss_att 19.159986 loss_ctc 22.195047 loss_rnnt 15.886345 hw_loss 0.219942 lr 0.00045457 rank 4
2023-02-23 05:16:28,018 DEBUG TRAIN Batch 14/4200 loss 18.056093 loss_att 26.994457 loss_ctc 23.981983 loss_rnnt 15.322501 hw_loss 0.292126 lr 0.00045464 rank 1
2023-02-23 05:16:28,072 DEBUG TRAIN Batch 14/4200 loss 11.386068 loss_att 11.832069 loss_ctc 12.555326 loss_rnnt 11.000302 hw_loss 0.263745 lr 0.00045452 rank 7
2023-02-23 05:17:46,513 DEBUG TRAIN Batch 14/4300 loss 9.219243 loss_att 12.031721 loss_ctc 13.510201 loss_rnnt 7.955547 hw_loss 0.242011 lr 0.00045435 rank 5
2023-02-23 05:17:46,513 DEBUG TRAIN Batch 14/4300 loss 9.717204 loss_att 13.532292 loss_ctc 13.434134 loss_rnnt 8.356848 hw_loss 0.190778 lr 0.00045438 rank 4
2023-02-23 05:17:46,519 DEBUG TRAIN Batch 14/4300 loss 8.353777 loss_att 11.202357 loss_ctc 12.373537 loss_rnnt 7.040843 hw_loss 0.388594 lr 0.00045438 rank 2
2023-02-23 05:17:46,522 DEBUG TRAIN Batch 14/4300 loss 10.578954 loss_att 12.372234 loss_ctc 14.437299 loss_rnnt 9.551303 hw_loss 0.289780 lr 0.00045435 rank 3
2023-02-23 05:17:46,523 DEBUG TRAIN Batch 14/4300 loss 16.626417 loss_att 20.435070 loss_ctc 21.804420 loss_rnnt 15.087667 hw_loss 0.162411 lr 0.00045445 rank 1
2023-02-23 05:17:46,527 DEBUG TRAIN Batch 14/4300 loss 11.901613 loss_att 14.986898 loss_ctc 15.145950 loss_rnnt 10.694395 hw_loss 0.295468 lr 0.00045438 rank 0
2023-02-23 05:17:46,553 DEBUG TRAIN Batch 14/4300 loss 24.670017 loss_att 23.193350 loss_ctc 29.700623 loss_rnnt 24.133163 hw_loss 0.302703 lr 0.00045441 rank 6
2023-02-23 05:17:46,558 DEBUG TRAIN Batch 14/4300 loss 8.109468 loss_att 12.790860 loss_ctc 11.657333 loss_rnnt 6.576406 hw_loss 0.232002 lr 0.00045434 rank 7
2023-02-23 05:19:03,607 DEBUG TRAIN Batch 14/4400 loss 32.225853 loss_att 34.202778 loss_ctc 31.443375 loss_rnnt 31.847548 hw_loss 0.163591 lr 0.00045422 rank 6
2023-02-23 05:19:03,610 DEBUG TRAIN Batch 14/4400 loss 11.941650 loss_att 14.107710 loss_ctc 16.336773 loss_rnnt 10.728231 hw_loss 0.364108 lr 0.00045420 rank 0
2023-02-23 05:19:03,613 DEBUG TRAIN Batch 14/4400 loss 16.791122 loss_att 18.422302 loss_ctc 24.422508 loss_rnnt 15.272102 hw_loss 0.328622 lr 0.00045419 rank 4
2023-02-23 05:19:03,614 DEBUG TRAIN Batch 14/4400 loss 11.938942 loss_att 17.397022 loss_ctc 11.921186 loss_rnnt 10.678874 hw_loss 0.320288 lr 0.00045417 rank 5
2023-02-23 05:19:03,615 DEBUG TRAIN Batch 14/4400 loss 14.851768 loss_att 19.310953 loss_ctc 24.700912 loss_rnnt 12.484093 hw_loss 0.304912 lr 0.00045426 rank 1
2023-02-23 05:19:03,618 DEBUG TRAIN Batch 14/4400 loss 8.570102 loss_att 10.788247 loss_ctc 12.641951 loss_rnnt 7.408686 hw_loss 0.327888 lr 0.00045419 rank 2
2023-02-23 05:19:03,621 DEBUG TRAIN Batch 14/4400 loss 7.984141 loss_att 9.992174 loss_ctc 8.656031 loss_rnnt 7.347929 hw_loss 0.271914 lr 0.00045415 rank 7
2023-02-23 05:19:03,657 DEBUG TRAIN Batch 14/4400 loss 14.164334 loss_att 14.518517 loss_ctc 17.083683 loss_rnnt 13.597849 hw_loss 0.199503 lr 0.00045416 rank 3
2023-02-23 05:20:20,143 DEBUG TRAIN Batch 14/4500 loss 10.861272 loss_att 14.582363 loss_ctc 21.908918 loss_rnnt 8.522326 hw_loss 0.228203 lr 0.00045400 rank 4
2023-02-23 05:20:20,146 DEBUG TRAIN Batch 14/4500 loss 23.961514 loss_att 24.061825 loss_ctc 25.534492 loss_rnnt 23.639166 hw_loss 0.173541 lr 0.00045400 rank 2
2023-02-23 05:20:20,147 DEBUG TRAIN Batch 14/4500 loss 7.886182 loss_att 12.887683 loss_ctc 8.715863 loss_rnnt 6.699924 hw_loss 0.141252 lr 0.00045396 rank 7
2023-02-23 05:20:20,147 DEBUG TRAIN Batch 14/4500 loss 4.697586 loss_att 7.652285 loss_ctc 7.283712 loss_rnnt 3.616099 hw_loss 0.273245 lr 0.00045401 rank 0
2023-02-23 05:20:20,147 DEBUG TRAIN Batch 14/4500 loss 10.810070 loss_att 15.201636 loss_ctc 21.750305 loss_rnnt 8.320284 hw_loss 0.286452 lr 0.00045398 rank 5
2023-02-23 05:20:20,150 DEBUG TRAIN Batch 14/4500 loss 2.604172 loss_att 5.311587 loss_ctc 5.570646 loss_rnnt 1.570426 hw_loss 0.181373 lr 0.00045403 rank 6
2023-02-23 05:20:20,154 DEBUG TRAIN Batch 14/4500 loss 7.708615 loss_att 11.892822 loss_ctc 12.273283 loss_rnnt 6.135735 hw_loss 0.238906 lr 0.00045407 rank 1
2023-02-23 05:20:20,202 DEBUG TRAIN Batch 14/4500 loss 10.618276 loss_att 13.449099 loss_ctc 12.652069 loss_rnnt 9.639088 hw_loss 0.265967 lr 0.00045398 rank 3
2023-02-23 05:21:38,741 DEBUG TRAIN Batch 14/4600 loss 18.562183 loss_att 22.449627 loss_ctc 24.744205 loss_rnnt 16.871998 hw_loss 0.165802 lr 0.00045379 rank 5
2023-02-23 05:21:38,742 DEBUG TRAIN Batch 14/4600 loss 9.802592 loss_att 15.820676 loss_ctc 14.530252 loss_rnnt 7.850824 hw_loss 0.220868 lr 0.00045382 rank 4
2023-02-23 05:21:38,743 DEBUG TRAIN Batch 14/4600 loss 8.930686 loss_att 10.929264 loss_ctc 9.074547 loss_rnnt 8.432924 hw_loss 0.147873 lr 0.00045381 rank 2
2023-02-23 05:21:38,747 DEBUG TRAIN Batch 14/4600 loss 5.851551 loss_att 6.879368 loss_ctc 6.756641 loss_rnnt 5.385211 hw_loss 0.262684 lr 0.00045379 rank 3
2023-02-23 05:21:38,748 DEBUG TRAIN Batch 14/4600 loss 5.818261 loss_att 10.027040 loss_ctc 9.274662 loss_rnnt 4.430019 hw_loss 0.160560 lr 0.00045389 rank 1
2023-02-23 05:21:38,748 DEBUG TRAIN Batch 14/4600 loss 14.867470 loss_att 15.057613 loss_ctc 18.851120 loss_rnnt 14.150683 hw_loss 0.276760 lr 0.00045382 rank 0
2023-02-23 05:21:38,753 DEBUG TRAIN Batch 14/4600 loss 12.773871 loss_att 11.710558 loss_ctc 17.354481 loss_rnnt 12.261791 hw_loss 0.213740 lr 0.00045385 rank 6
2023-02-23 05:21:38,756 DEBUG TRAIN Batch 14/4600 loss 9.641118 loss_att 15.018526 loss_ctc 15.635440 loss_rnnt 7.650252 hw_loss 0.217765 lr 0.00045378 rank 7
2023-02-23 05:22:54,252 DEBUG TRAIN Batch 14/4700 loss 12.302891 loss_att 15.294999 loss_ctc 17.907934 loss_rnnt 10.836149 hw_loss 0.226840 lr 0.00045366 rank 6
2023-02-23 05:22:54,252 DEBUG TRAIN Batch 14/4700 loss 13.017642 loss_att 14.445749 loss_ctc 14.144492 loss_rnnt 12.481462 hw_loss 0.188087 lr 0.00045359 rank 7
2023-02-23 05:22:54,256 DEBUG TRAIN Batch 14/4700 loss 26.618298 loss_att 30.216442 loss_ctc 36.559902 loss_rnnt 24.446463 hw_loss 0.237483 lr 0.00045364 rank 0
2023-02-23 05:22:54,256 DEBUG TRAIN Batch 14/4700 loss 15.706203 loss_att 21.415340 loss_ctc 18.234844 loss_rnnt 14.091644 hw_loss 0.254211 lr 0.00045361 rank 5
2023-02-23 05:22:54,257 DEBUG TRAIN Batch 14/4700 loss 21.445572 loss_att 22.545044 loss_ctc 24.940985 loss_rnnt 20.647305 hw_loss 0.210600 lr 0.00045363 rank 2
2023-02-23 05:22:54,257 DEBUG TRAIN Batch 14/4700 loss 17.029490 loss_att 20.286964 loss_ctc 29.843628 loss_rnnt 14.544238 hw_loss 0.234761 lr 0.00045363 rank 4
2023-02-23 05:22:54,258 DEBUG TRAIN Batch 14/4700 loss 11.332880 loss_att 15.042355 loss_ctc 15.743551 loss_rnnt 9.906539 hw_loss 0.180669 lr 0.00045360 rank 3
2023-02-23 05:22:54,258 DEBUG TRAIN Batch 14/4700 loss 7.588978 loss_att 10.313204 loss_ctc 8.247786 loss_rnnt 6.830973 hw_loss 0.234971 lr 0.00045370 rank 1
2023-02-23 05:24:08,726 DEBUG TRAIN Batch 14/4800 loss 12.684089 loss_att 14.868490 loss_ctc 16.433243 loss_rnnt 11.558016 hw_loss 0.354945 lr 0.00045342 rank 5
2023-02-23 05:24:08,729 DEBUG TRAIN Batch 14/4800 loss 5.828946 loss_att 10.690515 loss_ctc 8.378464 loss_rnnt 4.366158 hw_loss 0.282257 lr 0.00045351 rank 1
2023-02-23 05:24:08,730 DEBUG TRAIN Batch 14/4800 loss 7.639807 loss_att 11.212471 loss_ctc 11.870357 loss_rnnt 6.255796 hw_loss 0.197634 lr 0.00045344 rank 4
2023-02-23 05:24:08,733 DEBUG TRAIN Batch 14/4800 loss 9.548188 loss_att 12.389753 loss_ctc 13.228237 loss_rnnt 8.350719 hw_loss 0.259654 lr 0.00045344 rank 2
2023-02-23 05:24:08,735 DEBUG TRAIN Batch 14/4800 loss 11.008541 loss_att 16.134598 loss_ctc 13.967213 loss_rnnt 9.413414 hw_loss 0.328925 lr 0.00045345 rank 0
2023-02-23 05:24:08,737 DEBUG TRAIN Batch 14/4800 loss 15.782101 loss_att 16.171391 loss_ctc 17.855190 loss_rnnt 15.286756 hw_loss 0.264516 lr 0.00045342 rank 3
2023-02-23 05:24:08,741 DEBUG TRAIN Batch 14/4800 loss 15.694584 loss_att 19.100641 loss_ctc 19.181015 loss_rnnt 14.448614 hw_loss 0.187315 lr 0.00045340 rank 7
2023-02-23 05:24:08,742 DEBUG TRAIN Batch 14/4800 loss 13.067469 loss_att 16.808685 loss_ctc 17.183578 loss_rnnt 11.612579 hw_loss 0.295933 lr 0.00045347 rank 6
2023-02-23 05:25:24,883 DEBUG TRAIN Batch 14/4900 loss 11.992770 loss_att 14.284399 loss_ctc 13.612374 loss_rnnt 11.150249 hw_loss 0.315465 lr 0.00045333 rank 1
2023-02-23 05:25:24,883 DEBUG TRAIN Batch 14/4900 loss 4.441316 loss_att 6.504366 loss_ctc 6.490883 loss_rnnt 3.650591 hw_loss 0.196572 lr 0.00045326 rank 0
2023-02-23 05:25:24,883 DEBUG TRAIN Batch 14/4900 loss 22.790800 loss_att 23.689707 loss_ctc 25.920452 loss_rnnt 22.011311 hw_loss 0.342040 lr 0.00045325 rank 2
2023-02-23 05:25:24,886 DEBUG TRAIN Batch 14/4900 loss 11.983378 loss_att 14.385722 loss_ctc 13.650444 loss_rnnt 11.108467 hw_loss 0.322810 lr 0.00045323 rank 5
2023-02-23 05:25:24,888 DEBUG TRAIN Batch 14/4900 loss 12.693851 loss_att 14.666908 loss_ctc 17.613613 loss_rnnt 11.480976 hw_loss 0.304305 lr 0.00045329 rank 6
2023-02-23 05:25:24,887 DEBUG TRAIN Batch 14/4900 loss 13.096848 loss_att 15.620203 loss_ctc 18.929426 loss_rnnt 11.703143 hw_loss 0.208794 lr 0.00045323 rank 3
2023-02-23 05:25:24,889 DEBUG TRAIN Batch 14/4900 loss 14.003199 loss_att 17.548555 loss_ctc 18.585476 loss_rnnt 12.572015 hw_loss 0.208391 lr 0.00045326 rank 4
2023-02-23 05:25:24,894 DEBUG TRAIN Batch 14/4900 loss 9.966050 loss_att 13.701647 loss_ctc 13.830700 loss_rnnt 8.521813 hw_loss 0.340932 lr 0.00045322 rank 7
2023-02-23 05:26:44,030 DEBUG TRAIN Batch 14/5000 loss 6.711576 loss_att 10.841692 loss_ctc 9.926250 loss_rnnt 5.285879 hw_loss 0.320718 lr 0.00045305 rank 5
2023-02-23 05:26:44,033 DEBUG TRAIN Batch 14/5000 loss 8.214515 loss_att 10.179247 loss_ctc 10.028591 loss_rnnt 7.478904 hw_loss 0.188976 lr 0.00045314 rank 1
2023-02-23 05:26:44,034 DEBUG TRAIN Batch 14/5000 loss 10.200840 loss_att 14.336821 loss_ctc 13.342582 loss_rnnt 8.794477 hw_loss 0.300503 lr 0.00045307 rank 2
2023-02-23 05:26:44,035 DEBUG TRAIN Batch 14/5000 loss 11.418996 loss_att 15.987228 loss_ctc 18.731224 loss_rnnt 9.356386 hw_loss 0.326247 lr 0.00045307 rank 4
2023-02-23 05:26:44,037 DEBUG TRAIN Batch 14/5000 loss 10.093005 loss_att 11.121696 loss_ctc 13.380721 loss_rnnt 9.263779 hw_loss 0.347112 lr 0.00045310 rank 6
2023-02-23 05:26:44,036 DEBUG TRAIN Batch 14/5000 loss 14.629063 loss_att 14.450914 loss_ctc 19.708731 loss_rnnt 13.800424 hw_loss 0.350584 lr 0.00045303 rank 7
2023-02-23 05:26:44,038 DEBUG TRAIN Batch 14/5000 loss 5.963405 loss_att 8.465884 loss_ctc 10.411556 loss_rnnt 4.717026 hw_loss 0.286492 lr 0.00045304 rank 3
2023-02-23 05:26:44,039 DEBUG TRAIN Batch 14/5000 loss 5.626002 loss_att 7.991155 loss_ctc 7.069688 loss_rnnt 4.792673 hw_loss 0.314637 lr 0.00045308 rank 0
2023-02-23 05:28:00,310 DEBUG TRAIN Batch 14/5100 loss 7.287964 loss_att 7.601242 loss_ctc 8.191409 loss_rnnt 6.869054 hw_loss 0.442116 lr 0.00045288 rank 4
2023-02-23 05:28:00,314 DEBUG TRAIN Batch 14/5100 loss 29.615267 loss_att 29.949657 loss_ctc 51.339153 loss_rnnt 26.558186 hw_loss 0.175656 lr 0.00045288 rank 2
2023-02-23 05:28:00,314 DEBUG TRAIN Batch 14/5100 loss 12.292357 loss_att 11.687293 loss_ctc 14.323541 loss_rnnt 11.981088 hw_loss 0.302734 lr 0.00045289 rank 0
2023-02-23 05:28:00,315 DEBUG TRAIN Batch 14/5100 loss 16.258259 loss_att 18.559074 loss_ctc 16.173073 loss_rnnt 15.642890 hw_loss 0.312310 lr 0.00045295 rank 1
2023-02-23 05:28:00,316 DEBUG TRAIN Batch 14/5100 loss 7.515747 loss_att 11.393353 loss_ctc 12.416899 loss_rnnt 5.943853 hw_loss 0.267912 lr 0.00045284 rank 7
2023-02-23 05:28:00,318 DEBUG TRAIN Batch 14/5100 loss 15.537399 loss_att 21.897610 loss_ctc 21.366070 loss_rnnt 13.354457 hw_loss 0.250771 lr 0.00045291 rank 6
2023-02-23 05:28:00,320 DEBUG TRAIN Batch 14/5100 loss 10.407176 loss_att 12.812713 loss_ctc 10.324782 loss_rnnt 9.789797 hw_loss 0.276110 lr 0.00045286 rank 5
2023-02-23 05:28:00,319 DEBUG TRAIN Batch 14/5100 loss 6.449325 loss_att 7.491647 loss_ctc 7.864593 loss_rnnt 5.885380 hw_loss 0.312707 lr 0.00045286 rank 3
2023-02-23 05:29:16,230 DEBUG TRAIN Batch 14/5200 loss 23.601492 loss_att 23.585747 loss_ctc 26.473379 loss_rnnt 23.077805 hw_loss 0.269842 lr 0.00045268 rank 5
2023-02-23 05:29:16,236 DEBUG TRAIN Batch 14/5200 loss 13.157698 loss_att 16.934965 loss_ctc 15.833181 loss_rnnt 11.860778 hw_loss 0.346380 lr 0.00045270 rank 4
2023-02-23 05:29:16,238 DEBUG TRAIN Batch 14/5200 loss 12.453407 loss_att 14.069775 loss_ctc 13.097626 loss_rnnt 11.871887 hw_loss 0.323158 lr 0.00045270 rank 2
2023-02-23 05:29:16,240 DEBUG TRAIN Batch 14/5200 loss 13.662492 loss_att 22.289892 loss_ctc 20.485569 loss_rnnt 10.891228 hw_loss 0.255074 lr 0.00045267 rank 3
2023-02-23 05:29:16,240 DEBUG TRAIN Batch 14/5200 loss 2.682874 loss_att 6.814872 loss_ctc 3.240489 loss_rnnt 1.665785 hw_loss 0.218140 lr 0.00045270 rank 0
2023-02-23 05:29:16,240 DEBUG TRAIN Batch 14/5200 loss 7.494888 loss_att 12.242680 loss_ctc 9.257328 loss_rnnt 6.179977 hw_loss 0.244428 lr 0.00045273 rank 6
2023-02-23 05:29:16,241 DEBUG TRAIN Batch 14/5200 loss 11.346754 loss_att 17.049995 loss_ctc 15.419331 loss_rnnt 9.577568 hw_loss 0.160365 lr 0.00045266 rank 7
2023-02-23 05:29:16,244 DEBUG TRAIN Batch 14/5200 loss 2.061444 loss_att 6.584957 loss_ctc 1.518695 loss_rnnt 1.080285 hw_loss 0.279042 lr 0.00045277 rank 1
2023-02-23 05:30:34,150 DEBUG TRAIN Batch 14/5300 loss 33.337090 loss_att 35.701279 loss_ctc 48.789253 loss_rnnt 30.683306 hw_loss 0.226233 lr 0.00045249 rank 5
2023-02-23 05:30:34,157 DEBUG TRAIN Batch 14/5300 loss 17.999235 loss_att 21.713589 loss_ctc 25.670904 loss_rnnt 16.128073 hw_loss 0.197629 lr 0.00045247 rank 7
2023-02-23 05:30:34,157 DEBUG TRAIN Batch 14/5300 loss 10.204727 loss_att 13.394650 loss_ctc 12.015163 loss_rnnt 9.191558 hw_loss 0.250865 lr 0.00045252 rank 0
2023-02-23 05:30:34,157 DEBUG TRAIN Batch 14/5300 loss 10.250551 loss_att 13.230559 loss_ctc 11.063986 loss_rnnt 9.441095 hw_loss 0.196869 lr 0.00045251 rank 4
2023-02-23 05:30:34,158 DEBUG TRAIN Batch 14/5300 loss 8.012131 loss_att 11.094945 loss_ctc 11.661810 loss_rnnt 6.844093 hw_loss 0.121595 lr 0.00045249 rank 3
2023-02-23 05:30:34,158 DEBUG TRAIN Batch 14/5300 loss 12.213296 loss_att 13.593693 loss_ctc 16.343332 loss_rnnt 11.256824 hw_loss 0.243226 lr 0.00045251 rank 2
2023-02-23 05:30:34,164 DEBUG TRAIN Batch 14/5300 loss 10.540502 loss_att 16.485279 loss_ctc 11.661224 loss_rnnt 9.113400 hw_loss 0.166344 lr 0.00045254 rank 6
2023-02-23 05:30:34,168 DEBUG TRAIN Batch 14/5300 loss 5.638751 loss_att 9.380127 loss_ctc 8.577166 loss_rnnt 4.343217 hw_loss 0.291506 lr 0.00045258 rank 1
2023-02-23 05:31:52,303 DEBUG TRAIN Batch 14/5400 loss 12.441740 loss_att 14.975329 loss_ctc 21.267483 loss_rnnt 10.636816 hw_loss 0.227699 lr 0.00045230 rank 5
2023-02-23 05:31:52,304 DEBUG TRAIN Batch 14/5400 loss 4.728061 loss_att 10.451140 loss_ctc 7.660979 loss_rnnt 3.036702 hw_loss 0.291913 lr 0.00045233 rank 2
2023-02-23 05:31:52,306 DEBUG TRAIN Batch 14/5400 loss 14.650064 loss_att 16.575762 loss_ctc 18.938000 loss_rnnt 13.533487 hw_loss 0.299462 lr 0.00045233 rank 0
2023-02-23 05:31:52,307 DEBUG TRAIN Batch 14/5400 loss 7.954642 loss_att 12.276957 loss_ctc 9.540379 loss_rnnt 6.749145 hw_loss 0.243006 lr 0.00045233 rank 4
2023-02-23 05:31:52,311 DEBUG TRAIN Batch 14/5400 loss 8.172687 loss_att 9.370739 loss_ctc 10.570615 loss_rnnt 7.463535 hw_loss 0.280907 lr 0.00045240 rank 1
2023-02-23 05:31:52,312 DEBUG TRAIN Batch 14/5400 loss 17.794971 loss_att 22.746780 loss_ctc 26.443464 loss_rnnt 15.502593 hw_loss 0.279159 lr 0.00045229 rank 7
2023-02-23 05:31:52,312 DEBUG TRAIN Batch 14/5400 loss 13.022861 loss_att 12.862028 loss_ctc 15.485819 loss_rnnt 12.584000 hw_loss 0.267440 lr 0.00045236 rank 6
2023-02-23 05:31:52,355 DEBUG TRAIN Batch 14/5400 loss 16.452095 loss_att 19.214970 loss_ctc 20.751854 loss_rnnt 15.200710 hw_loss 0.235327 lr 0.00045230 rank 3
2023-02-23 05:33:08,275 DEBUG TRAIN Batch 14/5500 loss 21.423645 loss_att 20.811508 loss_ctc 29.052372 loss_rnnt 20.415958 hw_loss 0.211784 lr 0.00045212 rank 3
2023-02-23 05:33:08,277 DEBUG TRAIN Batch 14/5500 loss 12.946969 loss_att 17.347023 loss_ctc 19.224405 loss_rnnt 11.107685 hw_loss 0.229278 lr 0.00045214 rank 4
2023-02-23 05:33:08,281 DEBUG TRAIN Batch 14/5500 loss 7.954263 loss_att 11.518327 loss_ctc 11.777716 loss_rnnt 6.537448 hw_loss 0.364143 lr 0.00045214 rank 2
2023-02-23 05:33:08,282 DEBUG TRAIN Batch 14/5500 loss 11.463964 loss_att 14.396240 loss_ctc 16.557308 loss_rnnt 10.078897 hw_loss 0.224061 lr 0.00045212 rank 5
2023-02-23 05:33:08,281 DEBUG TRAIN Batch 14/5500 loss 13.890420 loss_att 16.893728 loss_ctc 20.896673 loss_rnnt 12.180165 hw_loss 0.328922 lr 0.00045215 rank 0
2023-02-23 05:33:08,284 DEBUG TRAIN Batch 14/5500 loss 5.883127 loss_att 8.973276 loss_ctc 6.414261 loss_rnnt 5.068232 hw_loss 0.236338 lr 0.00045221 rank 1
2023-02-23 05:33:08,285 DEBUG TRAIN Batch 14/5500 loss 15.340384 loss_att 20.802822 loss_ctc 19.734566 loss_rnnt 13.525595 hw_loss 0.255773 lr 0.00045217 rank 6
2023-02-23 05:33:08,333 DEBUG TRAIN Batch 14/5500 loss 13.474996 loss_att 14.456812 loss_ctc 14.138519 loss_rnnt 13.074785 hw_loss 0.216331 lr 0.00045210 rank 7
2023-02-23 05:34:26,194 DEBUG TRAIN Batch 14/5600 loss 11.433803 loss_att 15.298935 loss_ctc 14.409756 loss_rnnt 10.150331 hw_loss 0.213096 lr 0.00045193 rank 3
2023-02-23 05:34:26,197 DEBUG TRAIN Batch 14/5600 loss 15.027975 loss_att 19.143446 loss_ctc 19.537607 loss_rnnt 13.411923 hw_loss 0.359384 lr 0.00045199 rank 6
2023-02-23 05:34:26,199 DEBUG TRAIN Batch 14/5600 loss 20.065191 loss_att 20.505753 loss_ctc 23.698311 loss_rnnt 19.298485 hw_loss 0.364086 lr 0.00045196 rank 4
2023-02-23 05:34:26,201 DEBUG TRAIN Batch 14/5600 loss 15.205318 loss_att 17.935318 loss_ctc 21.254683 loss_rnnt 13.747795 hw_loss 0.196765 lr 0.00045203 rank 1
2023-02-23 05:34:26,202 DEBUG TRAIN Batch 14/5600 loss 6.377049 loss_att 7.853522 loss_ctc 9.280147 loss_rnnt 5.541924 hw_loss 0.286409 lr 0.00045192 rank 7
2023-02-23 05:34:26,202 DEBUG TRAIN Batch 14/5600 loss 15.993856 loss_att 17.656429 loss_ctc 22.465988 loss_rnnt 14.642693 hw_loss 0.291934 lr 0.00045196 rank 0
2023-02-23 05:34:26,203 DEBUG TRAIN Batch 14/5600 loss 18.346706 loss_att 18.848539 loss_ctc 23.399000 loss_rnnt 17.451202 hw_loss 0.227811 lr 0.00045196 rank 2
2023-02-23 05:34:26,210 DEBUG TRAIN Batch 14/5600 loss 13.139451 loss_att 14.722444 loss_ctc 13.967498 loss_rnnt 12.639753 hw_loss 0.136301 lr 0.00045193 rank 5
2023-02-23 05:35:45,469 DEBUG TRAIN Batch 14/5700 loss 6.062283 loss_att 11.237936 loss_ctc 8.743704 loss_rnnt 4.548350 hw_loss 0.227398 lr 0.00045175 rank 5
2023-02-23 05:35:45,472 DEBUG TRAIN Batch 14/5700 loss 12.703465 loss_att 14.111479 loss_ctc 18.221069 loss_rnnt 11.531986 hw_loss 0.289116 lr 0.00045180 rank 6
2023-02-23 05:35:45,473 DEBUG TRAIN Batch 14/5700 loss 9.695111 loss_att 10.612016 loss_ctc 12.961646 loss_rnnt 8.974411 hw_loss 0.190840 lr 0.00045173 rank 7
2023-02-23 05:35:45,473 DEBUG TRAIN Batch 14/5700 loss 7.085362 loss_att 7.321414 loss_ctc 9.279612 loss_rnnt 6.554969 hw_loss 0.357403 lr 0.00045184 rank 1
2023-02-23 05:35:45,473 DEBUG TRAIN Batch 14/5700 loss 16.532705 loss_att 15.520891 loss_ctc 19.710588 loss_rnnt 16.158520 hw_loss 0.286555 lr 0.00045178 rank 0
2023-02-23 05:35:45,473 DEBUG TRAIN Batch 14/5700 loss 9.312308 loss_att 11.447458 loss_ctc 12.027979 loss_rnnt 8.353689 hw_loss 0.317813 lr 0.00045177 rank 4
2023-02-23 05:35:45,474 DEBUG TRAIN Batch 14/5700 loss 7.424976 loss_att 9.190320 loss_ctc 9.642377 loss_rnnt 6.648077 hw_loss 0.240329 lr 0.00045175 rank 3
2023-02-23 05:35:45,475 DEBUG TRAIN Batch 14/5700 loss 12.101092 loss_att 11.861189 loss_ctc 15.608641 loss_rnnt 11.468188 hw_loss 0.399771 lr 0.00045177 rank 2
2023-02-23 05:37:02,733 DEBUG TRAIN Batch 14/5800 loss 11.216059 loss_att 15.298203 loss_ctc 11.137169 loss_rnnt 10.253551 hw_loss 0.293620 lr 0.00045166 rank 1
2023-02-23 05:37:02,733 DEBUG TRAIN Batch 14/5800 loss 9.550727 loss_att 14.333879 loss_ctc 15.000055 loss_rnnt 7.746120 hw_loss 0.227623 lr 0.00045157 rank 5
2023-02-23 05:37:02,735 DEBUG TRAIN Batch 14/5800 loss 4.464315 loss_att 11.090447 loss_ctc 4.681617 loss_rnnt 2.997853 hw_loss 0.210490 lr 0.00045155 rank 7
2023-02-23 05:37:02,739 DEBUG TRAIN Batch 14/5800 loss 6.100554 loss_att 10.304013 loss_ctc 8.258158 loss_rnnt 4.833931 hw_loss 0.259221 lr 0.00045159 rank 4
2023-02-23 05:37:02,740 DEBUG TRAIN Batch 14/5800 loss 7.429552 loss_att 9.696975 loss_ctc 13.388250 loss_rnnt 6.023353 hw_loss 0.296667 lr 0.00045160 rank 0
2023-02-23 05:37:02,741 DEBUG TRAIN Batch 14/5800 loss 1.465449 loss_att 5.470982 loss_ctc 0.818407 loss_rnnt 0.673829 hw_loss 0.143975 lr 0.00045159 rank 2
2023-02-23 05:37:02,742 DEBUG TRAIN Batch 14/5800 loss 9.646815 loss_att 9.963009 loss_ctc 9.913391 loss_rnnt 9.383101 hw_loss 0.309247 lr 0.00045162 rank 6
2023-02-23 05:37:02,786 DEBUG TRAIN Batch 14/5800 loss 10.053543 loss_att 9.587730 loss_ctc 11.603870 loss_rnnt 9.738077 hw_loss 0.378594 lr 0.00045156 rank 3
2023-02-23 05:38:20,814 DEBUG TRAIN Batch 14/5900 loss 9.775265 loss_att 15.768368 loss_ctc 16.094854 loss_rnnt 7.604867 hw_loss 0.242182 lr 0.00045140 rank 2
2023-02-23 05:38:20,815 DEBUG TRAIN Batch 14/5900 loss 13.894100 loss_att 16.540133 loss_ctc 17.477127 loss_rnnt 12.801120 hw_loss 0.161319 lr 0.00045141 rank 4
2023-02-23 05:38:20,818 DEBUG TRAIN Batch 14/5900 loss 17.713329 loss_att 17.373377 loss_ctc 23.443569 loss_rnnt 16.855440 hw_loss 0.303464 lr 0.00045137 rank 7
2023-02-23 05:38:20,820 DEBUG TRAIN Batch 14/5900 loss 10.154134 loss_att 12.685989 loss_ctc 13.596325 loss_rnnt 9.052142 hw_loss 0.256240 lr 0.00045138 rank 5
2023-02-23 05:38:20,821 DEBUG TRAIN Batch 14/5900 loss 5.896616 loss_att 12.799795 loss_ctc 12.531013 loss_rnnt 3.498234 hw_loss 0.249676 lr 0.00045141 rank 0
2023-02-23 05:38:20,823 DEBUG TRAIN Batch 14/5900 loss 8.341903 loss_att 13.076302 loss_ctc 9.343623 loss_rnnt 7.128867 hw_loss 0.248612 lr 0.00045138 rank 3
2023-02-23 05:38:20,823 DEBUG TRAIN Batch 14/5900 loss 7.057956 loss_att 8.439415 loss_ctc 8.625621 loss_rnnt 6.472200 hw_loss 0.188329 lr 0.00045144 rank 6
2023-02-23 05:38:20,825 DEBUG TRAIN Batch 14/5900 loss 15.959323 loss_att 18.344967 loss_ctc 18.698574 loss_rnnt 14.949016 hw_loss 0.314895 lr 0.00045147 rank 1
2023-02-23 05:39:39,379 DEBUG TRAIN Batch 14/6000 loss 12.630453 loss_att 14.838480 loss_ctc 17.001724 loss_rnnt 11.493952 hw_loss 0.210110 lr 0.00045120 rank 5
2023-02-23 05:39:39,383 DEBUG TRAIN Batch 14/6000 loss 15.479407 loss_att 20.408400 loss_ctc 26.224958 loss_rnnt 12.931192 hw_loss 0.243141 lr 0.00045125 rank 6
2023-02-23 05:39:39,383 DEBUG TRAIN Batch 14/6000 loss 17.330437 loss_att 22.505749 loss_ctc 19.802847 loss_rnnt 15.873756 hw_loss 0.172428 lr 0.00045129 rank 1
2023-02-23 05:39:39,386 DEBUG TRAIN Batch 14/6000 loss 10.733057 loss_att 13.091168 loss_ctc 11.286910 loss_rnnt 10.040627 hw_loss 0.275550 lr 0.00045122 rank 4
2023-02-23 05:39:39,387 DEBUG TRAIN Batch 14/6000 loss 13.937503 loss_att 16.458279 loss_ctc 17.401730 loss_rnnt 12.860003 hw_loss 0.208961 lr 0.00045119 rank 3
2023-02-23 05:39:39,388 DEBUG TRAIN Batch 14/6000 loss 5.846629 loss_att 11.941387 loss_ctc 13.682372 loss_rnnt 3.442544 hw_loss 0.263190 lr 0.00045123 rank 0
2023-02-23 05:39:39,389 DEBUG TRAIN Batch 14/6000 loss 18.682089 loss_att 20.969536 loss_ctc 22.397804 loss_rnnt 17.653173 hw_loss 0.142494 lr 0.00045122 rank 2
2023-02-23 05:39:39,433 DEBUG TRAIN Batch 14/6000 loss 13.466786 loss_att 19.813177 loss_ctc 18.414680 loss_rnnt 11.351068 hw_loss 0.350101 lr 0.00045118 rank 7
2023-02-23 05:40:56,847 DEBUG TRAIN Batch 14/6100 loss 11.000833 loss_att 12.247519 loss_ctc 14.016221 loss_rnnt 10.256308 hw_loss 0.174630 lr 0.00045104 rank 4
2023-02-23 05:40:56,852 DEBUG TRAIN Batch 14/6100 loss 8.670067 loss_att 10.609305 loss_ctc 11.578420 loss_rnnt 7.755807 hw_loss 0.259933 lr 0.00045101 rank 5
2023-02-23 05:40:56,854 DEBUG TRAIN Batch 14/6100 loss 11.027826 loss_att 11.732065 loss_ctc 12.518712 loss_rnnt 10.580336 hw_loss 0.202235 lr 0.00045107 rank 6
2023-02-23 05:40:56,854 DEBUG TRAIN Batch 14/6100 loss 14.171066 loss_att 16.887407 loss_ctc 20.232487 loss_rnnt 12.684937 hw_loss 0.252509 lr 0.00045104 rank 2
2023-02-23 05:40:56,855 DEBUG TRAIN Batch 14/6100 loss 10.563995 loss_att 14.051447 loss_ctc 13.485319 loss_rnnt 9.369200 hw_loss 0.202117 lr 0.00045104 rank 0
2023-02-23 05:40:56,855 DEBUG TRAIN Batch 14/6100 loss 22.997213 loss_att 28.704575 loss_ctc 29.140057 loss_rnnt 20.882627 hw_loss 0.288879 lr 0.00045100 rank 7
2023-02-23 05:40:56,857 DEBUG TRAIN Batch 14/6100 loss 14.039216 loss_att 17.941895 loss_ctc 17.404409 loss_rnnt 12.709295 hw_loss 0.188800 lr 0.00045111 rank 1
2023-02-23 05:40:56,857 DEBUG TRAIN Batch 14/6100 loss 4.888064 loss_att 8.989730 loss_ctc 7.117891 loss_rnnt 3.620681 hw_loss 0.280762 lr 0.00045101 rank 3
2023-02-23 05:42:12,757 DEBUG TRAIN Batch 14/6200 loss 8.034724 loss_att 11.362051 loss_ctc 12.035191 loss_rnnt 6.693403 hw_loss 0.267112 lr 0.00045086 rank 0
2023-02-23 05:42:12,760 DEBUG TRAIN Batch 14/6200 loss 15.428309 loss_att 15.930943 loss_ctc 19.647089 loss_rnnt 14.631226 hw_loss 0.251348 lr 0.00045083 rank 5
2023-02-23 05:42:12,761 DEBUG TRAIN Batch 14/6200 loss 9.834196 loss_att 16.666061 loss_ctc 16.578020 loss_rnnt 7.454817 hw_loss 0.213428 lr 0.00045085 rank 4
2023-02-23 05:42:12,762 DEBUG TRAIN Batch 14/6200 loss 6.007002 loss_att 8.457431 loss_ctc 7.868505 loss_rnnt 5.179523 hw_loss 0.167235 lr 0.00045081 rank 7
2023-02-23 05:42:12,764 DEBUG TRAIN Batch 14/6200 loss 16.231604 loss_att 17.350006 loss_ctc 20.206812 loss_rnnt 15.373697 hw_loss 0.195372 lr 0.00045088 rank 6
2023-02-23 05:42:12,765 DEBUG TRAIN Batch 14/6200 loss 16.627716 loss_att 18.342594 loss_ctc 21.572906 loss_rnnt 15.469195 hw_loss 0.292851 lr 0.00045085 rank 2
2023-02-23 05:42:12,766 DEBUG TRAIN Batch 14/6200 loss 12.794285 loss_att 14.161613 loss_ctc 17.712790 loss_rnnt 11.691179 hw_loss 0.325946 lr 0.00045083 rank 3
2023-02-23 05:42:12,768 DEBUG TRAIN Batch 14/6200 loss 25.664280 loss_att 27.885893 loss_ctc 31.165770 loss_rnnt 24.314318 hw_loss 0.322698 lr 0.00045092 rank 1
2023-02-23 05:43:29,676 DEBUG TRAIN Batch 14/6300 loss 16.043779 loss_att 22.554817 loss_ctc 19.902142 loss_rnnt 14.078115 hw_loss 0.279391 lr 0.00045065 rank 5
2023-02-23 05:43:29,679 DEBUG TRAIN Batch 14/6300 loss 6.283988 loss_att 7.811347 loss_ctc 8.036236 loss_rnnt 5.613218 hw_loss 0.246869 lr 0.00045074 rank 1
2023-02-23 05:43:29,679 DEBUG TRAIN Batch 14/6300 loss 10.639881 loss_att 9.622965 loss_ctc 12.149415 loss_rnnt 10.498083 hw_loss 0.269833 lr 0.00045067 rank 2
2023-02-23 05:43:29,682 DEBUG TRAIN Batch 14/6300 loss 20.200727 loss_att 23.089676 loss_ctc 27.259480 loss_rnnt 18.555727 hw_loss 0.236331 lr 0.00045070 rank 6
2023-02-23 05:43:29,683 DEBUG TRAIN Batch 14/6300 loss 6.113808 loss_att 11.262550 loss_ctc 10.226748 loss_rnnt 4.419240 hw_loss 0.218302 lr 0.00045063 rank 7
2023-02-23 05:43:29,685 DEBUG TRAIN Batch 14/6300 loss 20.300596 loss_att 19.599113 loss_ctc 25.461828 loss_rnnt 19.589344 hw_loss 0.306345 lr 0.00045068 rank 0
2023-02-23 05:43:29,685 DEBUG TRAIN Batch 14/6300 loss 9.855962 loss_att 13.782257 loss_ctc 12.240202 loss_rnnt 8.650352 hw_loss 0.192097 lr 0.00045064 rank 3
2023-02-23 05:43:29,687 DEBUG TRAIN Batch 14/6300 loss 12.028107 loss_att 14.647133 loss_ctc 14.659685 loss_rnnt 11.056348 hw_loss 0.182019 lr 0.00045067 rank 4
2023-02-23 05:44:48,134 DEBUG TRAIN Batch 14/6400 loss 9.084593 loss_att 13.438501 loss_ctc 12.520927 loss_rnnt 7.626722 hw_loss 0.241709 lr 0.00045049 rank 2
2023-02-23 05:44:48,134 DEBUG TRAIN Batch 14/6400 loss 21.083445 loss_att 22.902693 loss_ctc 32.241604 loss_rnnt 19.125528 hw_loss 0.199330 lr 0.00045047 rank 5
2023-02-23 05:44:48,137 DEBUG TRAIN Batch 14/6400 loss 7.484581 loss_att 10.009212 loss_ctc 9.955973 loss_rnnt 6.488935 hw_loss 0.302253 lr 0.00045046 rank 3
2023-02-23 05:44:48,141 DEBUG TRAIN Batch 14/6400 loss 12.830996 loss_att 22.262243 loss_ctc 19.855114 loss_rnnt 9.813266 hw_loss 0.365495 lr 0.00045052 rank 6
2023-02-23 05:44:48,147 DEBUG TRAIN Batch 14/6400 loss 6.176570 loss_att 8.260631 loss_ctc 8.683069 loss_rnnt 5.243792 hw_loss 0.340812 lr 0.00045056 rank 1
2023-02-23 05:44:48,159 DEBUG TRAIN Batch 14/6400 loss 11.710709 loss_att 20.218790 loss_ctc 18.007183 loss_rnnt 9.060812 hw_loss 0.203908 lr 0.00045049 rank 0
2023-02-23 05:44:48,163 DEBUG TRAIN Batch 14/6400 loss 7.529371 loss_att 8.027248 loss_ctc 9.930512 loss_rnnt 6.949389 hw_loss 0.300478 lr 0.00045049 rank 4
2023-02-23 05:44:48,187 DEBUG TRAIN Batch 14/6400 loss 15.913852 loss_att 17.180990 loss_ctc 21.799704 loss_rnnt 14.745077 hw_loss 0.244813 lr 0.00045045 rank 7
2023-02-23 05:46:03,395 DEBUG TRAIN Batch 14/6500 loss 25.248373 loss_att 27.574089 loss_ctc 33.113091 loss_rnnt 23.589828 hw_loss 0.271447 lr 0.00045030 rank 2
2023-02-23 05:46:03,396 DEBUG TRAIN Batch 14/6500 loss 7.925648 loss_att 12.102198 loss_ctc 14.774059 loss_rnnt 6.046918 hw_loss 0.244308 lr 0.00045034 rank 6
2023-02-23 05:46:03,396 DEBUG TRAIN Batch 14/6500 loss 7.904918 loss_att 9.327765 loss_ctc 12.181061 loss_rnnt 6.996267 hw_loss 0.101115 lr 0.00045028 rank 5
2023-02-23 05:46:03,398 DEBUG TRAIN Batch 14/6500 loss 9.056544 loss_att 12.122156 loss_ctc 10.088797 loss_rnnt 8.125710 hw_loss 0.337649 lr 0.00045031 rank 4
2023-02-23 05:46:03,401 DEBUG TRAIN Batch 14/6500 loss 7.097552 loss_att 10.947676 loss_ctc 6.691226 loss_rnnt 6.280324 hw_loss 0.190086 lr 0.00045037 rank 1
2023-02-23 05:46:03,404 DEBUG TRAIN Batch 14/6500 loss 5.310843 loss_att 7.827246 loss_ctc 8.425514 loss_rnnt 4.275543 hw_loss 0.218871 lr 0.00045031 rank 0
2023-02-23 05:46:03,404 DEBUG TRAIN Batch 14/6500 loss 7.752765 loss_att 10.259560 loss_ctc 9.684180 loss_rnnt 6.896886 hw_loss 0.181871 lr 0.00045028 rank 3
2023-02-23 05:46:03,405 DEBUG TRAIN Batch 14/6500 loss 9.836072 loss_att 12.113240 loss_ctc 10.954913 loss_rnnt 9.179969 hw_loss 0.096545 lr 0.00045027 rank 7
2023-02-23 05:47:19,718 DEBUG TRAIN Batch 14/6600 loss 9.079757 loss_att 11.191650 loss_ctc 9.498920 loss_rnnt 8.475624 hw_loss 0.235998 lr 0.00045010 rank 5
2023-02-23 05:47:19,719 DEBUG TRAIN Batch 14/6600 loss 13.608404 loss_att 15.364905 loss_ctc 24.596283 loss_rnnt 11.666808 hw_loss 0.234835 lr 0.00045013 rank 0
2023-02-23 05:47:19,721 DEBUG TRAIN Batch 14/6600 loss 19.458643 loss_att 23.226736 loss_ctc 25.419470 loss_rnnt 17.808685 hw_loss 0.190430 lr 0.00045008 rank 7
2023-02-23 05:47:19,723 DEBUG TRAIN Batch 14/6600 loss 18.507734 loss_att 21.419765 loss_ctc 23.373314 loss_rnnt 17.212959 hw_loss 0.119295 lr 0.00045012 rank 2
2023-02-23 05:47:19,724 DEBUG TRAIN Batch 14/6600 loss 19.026686 loss_att 21.489790 loss_ctc 22.930273 loss_rnnt 17.950672 hw_loss 0.117964 lr 0.00045019 rank 1
2023-02-23 05:47:19,725 DEBUG TRAIN Batch 14/6600 loss 16.886307 loss_att 19.298737 loss_ctc 22.383560 loss_rnnt 15.547520 hw_loss 0.231248 lr 0.00045010 rank 3
2023-02-23 05:47:19,726 DEBUG TRAIN Batch 14/6600 loss 17.521009 loss_att 18.347206 loss_ctc 21.367325 loss_rnnt 16.718254 hw_loss 0.233765 lr 0.00045012 rank 4
2023-02-23 05:47:19,764 DEBUG TRAIN Batch 14/6600 loss 5.995299 loss_att 11.052901 loss_ctc 7.980029 loss_rnnt 4.539803 hw_loss 0.336271 lr 0.00045015 rank 6
2023-02-23 05:48:36,108 DEBUG TRAIN Batch 14/6700 loss 11.125059 loss_att 17.967365 loss_ctc 18.595224 loss_rnnt 8.667666 hw_loss 0.174206 lr 0.00044994 rank 4
2023-02-23 05:48:36,108 DEBUG TRAIN Batch 14/6700 loss 7.472667 loss_att 10.730484 loss_ctc 9.581142 loss_rnnt 6.471228 hw_loss 0.128900 lr 0.00044992 rank 5
2023-02-23 05:48:36,109 DEBUG TRAIN Batch 14/6700 loss 16.349010 loss_att 17.538181 loss_ctc 18.513229 loss_rnnt 15.739504 hw_loss 0.155833 lr 0.00044995 rank 0
2023-02-23 05:48:36,114 DEBUG TRAIN Batch 14/6700 loss 7.109943 loss_att 12.493032 loss_ctc 9.874949 loss_rnnt 5.587714 hw_loss 0.144270 lr 0.00044994 rank 2
2023-02-23 05:48:36,115 DEBUG TRAIN Batch 14/6700 loss 6.757729 loss_att 9.285634 loss_ctc 9.056821 loss_rnnt 5.862820 hw_loss 0.155218 lr 0.00044990 rank 7
2023-02-23 05:48:36,116 DEBUG TRAIN Batch 14/6700 loss 10.863813 loss_att 12.196126 loss_ctc 12.265501 loss_rnnt 10.317047 hw_loss 0.175146 lr 0.00045001 rank 1
2023-02-23 05:48:36,116 DEBUG TRAIN Batch 14/6700 loss 6.745997 loss_att 10.683933 loss_ctc 9.836462 loss_rnnt 5.414060 hw_loss 0.248040 lr 0.00044997 rank 6
2023-02-23 05:48:36,119 DEBUG TRAIN Batch 14/6700 loss 7.805487 loss_att 11.682997 loss_ctc 10.086485 loss_rnnt 6.625192 hw_loss 0.188738 lr 0.00044991 rank 3
2023-02-23 05:49:54,715 DEBUG TRAIN Batch 14/6800 loss 13.532618 loss_att 15.554622 loss_ctc 19.314363 loss_rnnt 12.227446 hw_loss 0.243512 lr 0.00044976 rank 2
2023-02-23 05:49:54,716 DEBUG TRAIN Batch 14/6800 loss 9.825400 loss_att 12.654419 loss_ctc 13.881225 loss_rnnt 8.629025 hw_loss 0.168365 lr 0.00044976 rank 0
2023-02-23 05:49:54,722 DEBUG TRAIN Batch 14/6800 loss 11.304960 loss_att 12.332979 loss_ctc 13.411097 loss_rnnt 10.710307 hw_loss 0.202936 lr 0.00044974 rank 5
2023-02-23 05:49:54,723 DEBUG TRAIN Batch 14/6800 loss 23.614550 loss_att 26.893589 loss_ctc 32.204266 loss_rnnt 21.736706 hw_loss 0.143891 lr 0.00044973 rank 3
2023-02-23 05:49:54,725 DEBUG TRAIN Batch 14/6800 loss 11.153529 loss_att 13.973696 loss_ctc 15.677138 loss_rnnt 9.832218 hw_loss 0.288995 lr 0.00044983 rank 1
2023-02-23 05:49:54,726 DEBUG TRAIN Batch 14/6800 loss 17.092480 loss_att 22.105228 loss_ctc 22.954536 loss_rnnt 15.138933 hw_loss 0.317602 lr 0.00044976 rank 4
2023-02-23 05:49:54,728 DEBUG TRAIN Batch 14/6800 loss 6.641953 loss_att 9.200673 loss_ctc 9.353971 loss_rnnt 5.568099 hw_loss 0.375951 lr 0.00044979 rank 6
2023-02-23 05:49:54,729 DEBUG TRAIN Batch 14/6800 loss 18.764114 loss_att 19.902225 loss_ctc 27.951641 loss_rnnt 17.143307 hw_loss 0.315344 lr 0.00044972 rank 7
2023-02-23 05:51:12,090 DEBUG TRAIN Batch 14/6900 loss 11.686198 loss_att 14.268412 loss_ctc 17.390936 loss_rnnt 10.262044 hw_loss 0.275774 lr 0.00044964 rank 1
2023-02-23 05:51:12,091 DEBUG TRAIN Batch 14/6900 loss 14.383073 loss_att 15.617233 loss_ctc 18.861633 loss_rnnt 13.425638 hw_loss 0.212739 lr 0.00044958 rank 0
2023-02-23 05:51:12,091 DEBUG TRAIN Batch 14/6900 loss 11.556793 loss_att 14.373431 loss_ctc 12.476017 loss_rnnt 10.716378 hw_loss 0.289733 lr 0.00044955 rank 3
2023-02-23 05:51:12,094 DEBUG TRAIN Batch 14/6900 loss 14.932097 loss_att 15.759422 loss_ctc 16.586849 loss_rnnt 14.396670 hw_loss 0.279992 lr 0.00044958 rank 2
2023-02-23 05:51:12,095 DEBUG TRAIN Batch 14/6900 loss 15.270275 loss_att 16.051140 loss_ctc 20.340227 loss_rnnt 14.297667 hw_loss 0.263329 lr 0.00044955 rank 5
2023-02-23 05:51:12,098 DEBUG TRAIN Batch 14/6900 loss 8.340454 loss_att 10.344724 loss_ctc 12.029550 loss_rnnt 7.321061 hw_loss 0.237487 lr 0.00044958 rank 4
2023-02-23 05:51:12,098 DEBUG TRAIN Batch 14/6900 loss 6.923039 loss_att 7.809138 loss_ctc 8.777170 loss_rnnt 6.329995 hw_loss 0.316138 lr 0.00044954 rank 7
2023-02-23 05:51:12,099 DEBUG TRAIN Batch 14/6900 loss 11.523332 loss_att 11.296787 loss_ctc 14.425029 loss_rnnt 11.059212 hw_loss 0.229756 lr 0.00044961 rank 6
2023-02-23 05:52:26,449 DEBUG TRAIN Batch 14/7000 loss 11.468239 loss_att 16.219578 loss_ctc 13.063589 loss_rnnt 10.204975 hw_loss 0.188029 lr 0.00044946 rank 1
2023-02-23 05:52:26,451 DEBUG TRAIN Batch 14/7000 loss 17.216635 loss_att 21.480892 loss_ctc 22.402090 loss_rnnt 15.580086 hw_loss 0.173068 lr 0.00044937 rank 5
2023-02-23 05:52:26,453 DEBUG TRAIN Batch 14/7000 loss 8.721528 loss_att 12.967954 loss_ctc 11.822444 loss_rnnt 7.361752 hw_loss 0.181942 lr 0.00044940 rank 4
2023-02-23 05:52:26,455 DEBUG TRAIN Batch 14/7000 loss 7.209994 loss_att 8.515712 loss_ctc 9.809660 loss_rnnt 6.402623 hw_loss 0.374259 lr 0.00044940 rank 0
2023-02-23 05:52:26,458 DEBUG TRAIN Batch 14/7000 loss 10.684349 loss_att 16.924950 loss_ctc 11.943338 loss_rnnt 9.157682 hw_loss 0.207528 lr 0.00044936 rank 7
2023-02-23 05:52:26,458 DEBUG TRAIN Batch 14/7000 loss 6.935450 loss_att 12.008851 loss_ctc 6.799161 loss_rnnt 5.849574 hw_loss 0.167564 lr 0.00044939 rank 2
2023-02-23 05:52:26,461 DEBUG TRAIN Batch 14/7000 loss 4.366972 loss_att 7.267576 loss_ctc 6.676536 loss_rnnt 3.363318 hw_loss 0.216734 lr 0.00044942 rank 6
2023-02-23 05:52:26,494 DEBUG TRAIN Batch 14/7000 loss 11.507038 loss_att 15.004016 loss_ctc 18.922354 loss_rnnt 9.704163 hw_loss 0.215195 lr 0.00044937 rank 3
2023-02-23 05:53:45,256 DEBUG TRAIN Batch 14/7100 loss 5.730856 loss_att 10.044239 loss_ctc 9.151377 loss_rnnt 4.235616 hw_loss 0.330929 lr 0.00044921 rank 2
2023-02-23 05:53:45,261 DEBUG TRAIN Batch 14/7100 loss 14.082550 loss_att 14.711988 loss_ctc 18.049364 loss_rnnt 13.295512 hw_loss 0.247953 lr 0.00044928 rank 1
2023-02-23 05:53:45,264 DEBUG TRAIN Batch 14/7100 loss 18.011784 loss_att 23.177879 loss_ctc 26.250204 loss_rnnt 15.787718 hw_loss 0.173228 lr 0.00044919 rank 5
2023-02-23 05:53:45,266 DEBUG TRAIN Batch 14/7100 loss 4.349921 loss_att 6.649907 loss_ctc 3.329151 loss_rnnt 3.883738 hw_loss 0.266792 lr 0.00044922 rank 0
2023-02-23 05:53:45,266 DEBUG TRAIN Batch 14/7100 loss 11.577692 loss_att 10.459803 loss_ctc 14.884459 loss_rnnt 11.149665 hw_loss 0.395066 lr 0.00044919 rank 3
2023-02-23 05:53:45,268 DEBUG TRAIN Batch 14/7100 loss 21.801744 loss_att 25.071777 loss_ctc 34.229240 loss_rnnt 19.377777 hw_loss 0.211802 lr 0.00044921 rank 4
2023-02-23 05:53:45,268 DEBUG TRAIN Batch 14/7100 loss 23.760561 loss_att 26.141392 loss_ctc 28.469580 loss_rnnt 22.562355 hw_loss 0.176569 lr 0.00044924 rank 6
2023-02-23 05:53:45,268 DEBUG TRAIN Batch 14/7100 loss 10.814707 loss_att 13.352158 loss_ctc 14.775143 loss_rnnt 9.667030 hw_loss 0.210239 lr 0.00044917 rank 7
2023-02-23 05:55:00,569 DEBUG TRAIN Batch 14/7200 loss 4.802377 loss_att 8.930178 loss_ctc 8.065102 loss_rnnt 3.416116 hw_loss 0.235632 lr 0.00044901 rank 5
2023-02-23 05:55:00,570 DEBUG TRAIN Batch 14/7200 loss 8.636567 loss_att 9.420802 loss_ctc 7.625470 loss_rnnt 8.397513 hw_loss 0.406911 lr 0.00044903 rank 4
2023-02-23 05:55:00,572 DEBUG TRAIN Batch 14/7200 loss 8.789590 loss_att 12.226944 loss_ctc 8.722971 loss_rnnt 8.002351 hw_loss 0.203720 lr 0.00044901 rank 3
2023-02-23 05:55:00,576 DEBUG TRAIN Batch 14/7200 loss 10.940872 loss_att 15.589369 loss_ctc 19.460644 loss_rnnt 8.776735 hw_loss 0.184626 lr 0.00044899 rank 7
2023-02-23 05:55:00,576 DEBUG TRAIN Batch 14/7200 loss 12.576101 loss_att 15.485589 loss_ctc 13.944876 loss_rnnt 11.643114 hw_loss 0.316099 lr 0.00044904 rank 0
2023-02-23 05:55:00,581 DEBUG TRAIN Batch 14/7200 loss 6.976785 loss_att 10.714101 loss_ctc 12.187449 loss_rnnt 5.430555 hw_loss 0.195022 lr 0.00044903 rank 2
2023-02-23 05:55:00,581 DEBUG TRAIN Batch 14/7200 loss 11.895085 loss_att 15.110161 loss_ctc 18.470207 loss_rnnt 10.278321 hw_loss 0.181997 lr 0.00044910 rank 1
2023-02-23 05:55:00,582 DEBUG TRAIN Batch 14/7200 loss 4.865362 loss_att 8.490614 loss_ctc 9.342012 loss_rnnt 3.429528 hw_loss 0.213557 lr 0.00044906 rank 6
2023-02-23 05:56:15,962 DEBUG TRAIN Batch 14/7300 loss 9.065321 loss_att 10.792845 loss_ctc 11.739465 loss_rnnt 8.300589 hw_loss 0.117516 lr 0.00044883 rank 5
2023-02-23 05:56:15,962 DEBUG TRAIN Batch 14/7300 loss 5.324798 loss_att 11.229903 loss_ctc 7.629422 loss_rnnt 3.727761 hw_loss 0.203872 lr 0.00044883 rank 3
2023-02-23 05:56:15,965 DEBUG TRAIN Batch 14/7300 loss 14.034539 loss_att 18.351379 loss_ctc 24.176268 loss_rnnt 11.726130 hw_loss 0.174019 lr 0.00044885 rank 4
2023-02-23 05:56:15,965 DEBUG TRAIN Batch 14/7300 loss 5.896844 loss_att 9.738322 loss_ctc 9.083658 loss_rnnt 4.570377 hw_loss 0.249869 lr 0.00044881 rank 7
2023-02-23 05:56:15,965 DEBUG TRAIN Batch 14/7300 loss 21.235519 loss_att 20.417521 loss_ctc 25.317629 loss_rnnt 20.784569 hw_loss 0.131754 lr 0.00044885 rank 2
2023-02-23 05:56:15,966 DEBUG TRAIN Batch 14/7300 loss 18.333824 loss_att 20.869001 loss_ctc 23.530848 loss_rnnt 16.969660 hw_loss 0.307862 lr 0.00044886 rank 0
2023-02-23 05:56:15,970 DEBUG TRAIN Batch 14/7300 loss 9.183919 loss_att 13.338219 loss_ctc 13.165907 loss_rnnt 7.678022 hw_loss 0.270198 lr 0.00044892 rank 1
2023-02-23 05:56:15,970 DEBUG TRAIN Batch 14/7300 loss 9.721883 loss_att 12.606682 loss_ctc 13.933872 loss_rnnt 8.445139 hw_loss 0.259096 lr 0.00044888 rank 6
2023-02-23 05:57:33,795 DEBUG TRAIN Batch 14/7400 loss 17.770882 loss_att 21.173527 loss_ctc 25.776251 loss_rnnt 15.875504 hw_loss 0.276501 lr 0.00044867 rank 2
2023-02-23 05:57:33,797 DEBUG TRAIN Batch 14/7400 loss 9.955532 loss_att 12.890254 loss_ctc 12.053596 loss_rnnt 8.947507 hw_loss 0.265009 lr 0.00044865 rank 5
2023-02-23 05:57:33,797 DEBUG TRAIN Batch 14/7400 loss 9.255976 loss_att 10.605784 loss_ctc 12.583369 loss_rnnt 8.430224 hw_loss 0.210256 lr 0.00044867 rank 4
2023-02-23 05:57:33,803 DEBUG TRAIN Batch 14/7400 loss 10.601837 loss_att 14.726658 loss_ctc 12.090836 loss_rnnt 9.462391 hw_loss 0.217405 lr 0.00044864 rank 3
2023-02-23 05:57:33,803 DEBUG TRAIN Batch 14/7400 loss 11.366437 loss_att 13.820930 loss_ctc 14.520167 loss_rnnt 10.331428 hw_loss 0.231774 lr 0.00044868 rank 0
2023-02-23 05:57:33,804 DEBUG TRAIN Batch 14/7400 loss 21.890970 loss_att 23.761894 loss_ctc 30.541523 loss_rnnt 20.193588 hw_loss 0.318359 lr 0.00044874 rank 1
2023-02-23 05:57:33,807 DEBUG TRAIN Batch 14/7400 loss 7.005357 loss_att 10.441712 loss_ctc 9.370141 loss_rnnt 5.870465 hw_loss 0.248091 lr 0.00044870 rank 6
2023-02-23 05:57:33,809 DEBUG TRAIN Batch 14/7400 loss 6.690664 loss_att 9.350423 loss_ctc 8.334590 loss_rnnt 5.786584 hw_loss 0.286759 lr 0.00044863 rank 7
2023-02-23 05:58:52,419 DEBUG TRAIN Batch 14/7500 loss 9.693236 loss_att 12.963437 loss_ctc 8.524653 loss_rnnt 9.104046 hw_loss 0.170554 lr 0.00044847 rank 5
2023-02-23 05:58:52,422 DEBUG TRAIN Batch 14/7500 loss 5.118249 loss_att 10.530247 loss_ctc 8.857540 loss_rnnt 3.354249 hw_loss 0.343180 lr 0.00044846 rank 3
2023-02-23 05:58:52,423 DEBUG TRAIN Batch 14/7500 loss 18.894478 loss_att 20.488071 loss_ctc 23.796383 loss_rnnt 17.760792 hw_loss 0.302588 lr 0.00044849 rank 2
2023-02-23 05:58:52,427 DEBUG TRAIN Batch 14/7500 loss 6.877540 loss_att 8.529147 loss_ctc 9.995654 loss_rnnt 6.004304 hw_loss 0.238434 lr 0.00044845 rank 7
2023-02-23 05:58:52,429 DEBUG TRAIN Batch 14/7500 loss 17.434259 loss_att 21.219189 loss_ctc 29.546385 loss_rnnt 14.891336 hw_loss 0.320600 lr 0.00044850 rank 0
2023-02-23 05:58:52,429 DEBUG TRAIN Batch 14/7500 loss 11.068691 loss_att 16.632658 loss_ctc 11.915163 loss_rnnt 9.702740 hw_loss 0.263055 lr 0.00044849 rank 4
2023-02-23 05:58:52,430 DEBUG TRAIN Batch 14/7500 loss 5.481721 loss_att 7.405325 loss_ctc 6.004579 loss_rnnt 4.926804 hw_loss 0.188404 lr 0.00044852 rank 6
2023-02-23 05:58:52,431 DEBUG TRAIN Batch 14/7500 loss 19.991959 loss_att 22.538725 loss_ctc 23.591621 loss_rnnt 18.905201 hw_loss 0.182714 lr 0.00044856 rank 1
2023-02-23 06:00:08,495 DEBUG TRAIN Batch 14/7600 loss 24.881151 loss_att 30.218410 loss_ctc 33.190842 loss_rnnt 22.570578 hw_loss 0.253433 lr 0.00044829 rank 5
2023-02-23 06:00:08,497 DEBUG TRAIN Batch 14/7600 loss 26.664246 loss_att 38.961716 loss_ctc 40.070354 loss_rnnt 22.340702 hw_loss 0.143562 lr 0.00044827 rank 7
2023-02-23 06:00:08,498 DEBUG TRAIN Batch 14/7600 loss 11.473096 loss_att 10.958004 loss_ctc 15.335118 loss_rnnt 10.902881 hw_loss 0.296808 lr 0.00044831 rank 2
2023-02-23 06:00:08,499 DEBUG TRAIN Batch 14/7600 loss 17.176378 loss_att 17.621645 loss_ctc 20.021397 loss_rnnt 16.617298 hw_loss 0.170046 lr 0.00044831 rank 4
2023-02-23 06:00:08,499 DEBUG TRAIN Batch 14/7600 loss 8.939543 loss_att 13.333372 loss_ctc 13.101041 loss_rnnt 7.406785 hw_loss 0.185860 lr 0.00044832 rank 0
2023-02-23 06:00:08,502 DEBUG TRAIN Batch 14/7600 loss 11.711525 loss_att 12.432100 loss_ctc 12.730101 loss_rnnt 11.293735 hw_loss 0.258499 lr 0.00044834 rank 6
2023-02-23 06:00:08,503 DEBUG TRAIN Batch 14/7600 loss 9.369427 loss_att 12.017239 loss_ctc 14.970448 loss_rnnt 7.968194 hw_loss 0.234126 lr 0.00044828 rank 3
2023-02-23 06:00:08,512 DEBUG TRAIN Batch 14/7600 loss 11.158692 loss_att 10.737365 loss_ctc 13.231711 loss_rnnt 10.772169 hw_loss 0.364472 lr 0.00044838 rank 1
2023-02-23 06:01:23,423 DEBUG TRAIN Batch 14/7700 loss 12.208076 loss_att 14.538124 loss_ctc 15.897595 loss_rnnt 11.110417 hw_loss 0.261962 lr 0.00044813 rank 4
2023-02-23 06:01:23,424 DEBUG TRAIN Batch 14/7700 loss 8.503047 loss_att 13.693197 loss_ctc 11.640638 loss_rnnt 6.939884 hw_loss 0.200226 lr 0.00044820 rank 1
2023-02-23 06:01:23,425 DEBUG TRAIN Batch 14/7700 loss 10.571849 loss_att 15.066391 loss_ctc 13.456001 loss_rnnt 9.210478 hw_loss 0.146082 lr 0.00044813 rank 2
2023-02-23 06:01:23,427 DEBUG TRAIN Batch 14/7700 loss 14.703528 loss_att 16.501812 loss_ctc 18.608501 loss_rnnt 13.668612 hw_loss 0.289868 lr 0.00044811 rank 5
2023-02-23 06:01:23,429 DEBUG TRAIN Batch 14/7700 loss 24.139553 loss_att 25.235180 loss_ctc 25.113121 loss_rnnt 23.694210 hw_loss 0.180770 lr 0.00044809 rank 7
2023-02-23 06:01:23,430 DEBUG TRAIN Batch 14/7700 loss 12.176363 loss_att 17.843277 loss_ctc 17.371815 loss_rnnt 10.245559 hw_loss 0.196302 lr 0.00044814 rank 0
2023-02-23 06:01:23,432 DEBUG TRAIN Batch 14/7700 loss 32.529087 loss_att 33.193867 loss_ctc 40.972382 loss_rnnt 31.126339 hw_loss 0.270035 lr 0.00044816 rank 6
2023-02-23 06:01:23,474 DEBUG TRAIN Batch 14/7700 loss 16.925066 loss_att 18.675961 loss_ctc 25.563925 loss_rnnt 15.287575 hw_loss 0.253998 lr 0.00044810 rank 3
2023-02-23 06:02:42,338 DEBUG TRAIN Batch 14/7800 loss 5.571758 loss_att 8.026055 loss_ctc 5.700513 loss_rnnt 4.960765 hw_loss 0.193061 lr 0.00044796 rank 0
2023-02-23 06:02:42,346 DEBUG TRAIN Batch 14/7800 loss 6.180777 loss_att 11.233541 loss_ctc 5.686269 loss_rnnt 5.147673 hw_loss 0.165912 lr 0.00044795 rank 4
2023-02-23 06:02:42,349 DEBUG TRAIN Batch 14/7800 loss 13.512575 loss_att 18.883371 loss_ctc 18.523865 loss_rnnt 11.641350 hw_loss 0.241675 lr 0.00044798 rank 6
2023-02-23 06:02:42,348 DEBUG TRAIN Batch 14/7800 loss 17.443489 loss_att 18.475870 loss_ctc 24.068548 loss_rnnt 16.248110 hw_loss 0.197924 lr 0.00044791 rank 7
2023-02-23 06:02:42,350 DEBUG TRAIN Batch 14/7800 loss 10.723747 loss_att 13.172924 loss_ctc 15.898081 loss_rnnt 9.409630 hw_loss 0.251945 lr 0.00044802 rank 1
2023-02-23 06:02:42,352 DEBUG TRAIN Batch 14/7800 loss 10.445982 loss_att 15.644956 loss_ctc 17.237385 loss_rnnt 8.344718 hw_loss 0.292403 lr 0.00044792 rank 3
2023-02-23 06:02:42,370 DEBUG TRAIN Batch 14/7800 loss 9.892289 loss_att 12.245271 loss_ctc 12.996379 loss_rnnt 8.859052 hw_loss 0.278929 lr 0.00044793 rank 5
2023-02-23 06:02:42,375 DEBUG TRAIN Batch 14/7800 loss 3.507828 loss_att 7.722226 loss_ctc 5.616028 loss_rnnt 2.270023 hw_loss 0.213436 lr 0.00044795 rank 2
2023-02-23 06:03:59,644 DEBUG TRAIN Batch 14/7900 loss 5.891709 loss_att 11.623449 loss_ctc 7.908398 loss_rnnt 4.306713 hw_loss 0.318294 lr 0.00044777 rank 4
2023-02-23 06:03:59,646 DEBUG TRAIN Batch 14/7900 loss 10.771156 loss_att 14.863411 loss_ctc 15.534451 loss_rnnt 9.234344 hw_loss 0.156102 lr 0.00044775 rank 5
2023-02-23 06:03:59,647 DEBUG TRAIN Batch 14/7900 loss 15.685445 loss_att 18.670151 loss_ctc 19.616901 loss_rnnt 14.440993 hw_loss 0.231218 lr 0.00044773 rank 7
2023-02-23 06:03:59,648 DEBUG TRAIN Batch 14/7900 loss 8.896305 loss_att 13.893286 loss_ctc 11.912048 loss_rnnt 7.418412 hw_loss 0.143245 lr 0.00044777 rank 2
2023-02-23 06:03:59,648 DEBUG TRAIN Batch 14/7900 loss 19.805468 loss_att 24.392679 loss_ctc 33.595810 loss_rnnt 16.929810 hw_loss 0.224072 lr 0.00044780 rank 6
2023-02-23 06:03:59,649 DEBUG TRAIN Batch 14/7900 loss 11.444742 loss_att 12.376919 loss_ctc 13.783276 loss_rnnt 10.807882 hw_loss 0.259914 lr 0.00044774 rank 3
2023-02-23 06:03:59,650 DEBUG TRAIN Batch 14/7900 loss 6.122208 loss_att 8.015366 loss_ctc 6.183626 loss_rnnt 5.620546 hw_loss 0.215329 lr 0.00044778 rank 0
2023-02-23 06:03:59,652 DEBUG TRAIN Batch 14/7900 loss 12.369963 loss_att 15.237907 loss_ctc 15.747581 loss_rnnt 11.261621 hw_loss 0.158254 lr 0.00044784 rank 1
2023-02-23 06:05:14,400 DEBUG TRAIN Batch 14/8000 loss 15.522745 loss_att 15.769497 loss_ctc 18.912725 loss_rnnt 14.916818 hw_loss 0.196085 lr 0.00044759 rank 4
2023-02-23 06:05:14,404 DEBUG TRAIN Batch 14/8000 loss 9.717139 loss_att 12.726572 loss_ctc 14.662529 loss_rnnt 8.352372 hw_loss 0.194053 lr 0.00044766 rank 1
2023-02-23 06:05:14,404 DEBUG TRAIN Batch 14/8000 loss 15.426041 loss_att 19.708992 loss_ctc 22.832691 loss_rnnt 13.481565 hw_loss 0.188121 lr 0.00044759 rank 2
2023-02-23 06:05:14,406 DEBUG TRAIN Batch 14/8000 loss 21.357866 loss_att 21.642376 loss_ctc 30.431631 loss_rnnt 20.017450 hw_loss 0.138142 lr 0.00044762 rank 6
2023-02-23 06:05:14,407 DEBUG TRAIN Batch 14/8000 loss 12.867090 loss_att 14.229194 loss_ctc 14.132639 loss_rnnt 12.263751 hw_loss 0.304085 lr 0.00044757 rank 5
2023-02-23 06:05:14,407 DEBUG TRAIN Batch 14/8000 loss 6.254965 loss_att 8.730639 loss_ctc 8.586074 loss_rnnt 5.279456 hw_loss 0.317925 lr 0.00044756 rank 3
2023-02-23 06:05:14,407 DEBUG TRAIN Batch 14/8000 loss 7.743323 loss_att 10.112247 loss_ctc 7.894383 loss_rnnt 7.084899 hw_loss 0.308434 lr 0.00044760 rank 0
2023-02-23 06:05:14,456 DEBUG TRAIN Batch 14/8000 loss 19.178610 loss_att 21.726234 loss_ctc 23.221859 loss_rnnt 18.022760 hw_loss 0.201044 lr 0.00044755 rank 7
2023-02-23 06:06:30,972 DEBUG TRAIN Batch 14/8100 loss 17.320551 loss_att 17.620914 loss_ctc 20.970551 loss_rnnt 16.615788 hw_loss 0.296292 lr 0.00044739 rank 5
2023-02-23 06:06:30,975 DEBUG TRAIN Batch 14/8100 loss 5.886505 loss_att 8.541580 loss_ctc 7.087626 loss_rnnt 5.038049 hw_loss 0.294922 lr 0.00044741 rank 4
2023-02-23 06:06:30,976 DEBUG TRAIN Batch 14/8100 loss 8.660964 loss_att 11.352441 loss_ctc 9.263017 loss_rnnt 7.910736 hw_loss 0.246861 lr 0.00044748 rank 1
2023-02-23 06:06:30,976 DEBUG TRAIN Batch 14/8100 loss 5.663751 loss_att 9.136923 loss_ctc 8.762821 loss_rnnt 4.424690 hw_loss 0.246033 lr 0.00044741 rank 2
2023-02-23 06:06:30,978 DEBUG TRAIN Batch 14/8100 loss 12.510406 loss_att 14.206867 loss_ctc 15.512835 loss_rnnt 11.648254 hw_loss 0.229751 lr 0.00044744 rank 6
2023-02-23 06:06:30,981 DEBUG TRAIN Batch 14/8100 loss 5.185383 loss_att 7.612898 loss_ctc 5.434213 loss_rnnt 4.552899 hw_loss 0.213381 lr 0.00044737 rank 7
2023-02-23 06:06:30,980 DEBUG TRAIN Batch 14/8100 loss 19.427511 loss_att 22.973263 loss_ctc 24.043964 loss_rnnt 17.964804 hw_loss 0.258807 lr 0.00044742 rank 0
2023-02-23 06:06:30,982 DEBUG TRAIN Batch 14/8100 loss 7.657247 loss_att 13.638167 loss_ctc 13.247671 loss_rnnt 5.593789 hw_loss 0.228531 lr 0.00044739 rank 3
2023-02-23 06:07:47,871 DEBUG TRAIN Batch 14/8200 loss 12.267056 loss_att 17.433197 loss_ctc 14.571868 loss_rnnt 10.764763 hw_loss 0.303293 lr 0.00044724 rank 0
2023-02-23 06:07:47,873 DEBUG TRAIN Batch 14/8200 loss 16.652493 loss_att 15.891348 loss_ctc 17.819328 loss_rnnt 16.528959 hw_loss 0.225345 lr 0.00044721 rank 3
2023-02-23 06:07:47,875 DEBUG TRAIN Batch 14/8200 loss 14.927874 loss_att 17.195160 loss_ctc 17.381191 loss_rnnt 14.009479 hw_loss 0.258429 lr 0.00044723 rank 4
2023-02-23 06:07:47,877 DEBUG TRAIN Batch 14/8200 loss 11.408111 loss_att 17.207150 loss_ctc 15.189667 loss_rnnt 9.605496 hw_loss 0.259872 lr 0.00044721 rank 5
2023-02-23 06:07:47,877 DEBUG TRAIN Batch 14/8200 loss 12.397856 loss_att 12.844003 loss_ctc 14.862879 loss_rnnt 11.795547 hw_loss 0.345767 lr 0.00044730 rank 1
2023-02-23 06:07:47,878 DEBUG TRAIN Batch 14/8200 loss 5.707645 loss_att 7.197960 loss_ctc 8.021349 loss_rnnt 4.938675 hw_loss 0.304525 lr 0.00044723 rank 2
2023-02-23 06:07:47,881 DEBUG TRAIN Batch 14/8200 loss 1.662485 loss_att 3.265109 loss_ctc 2.313747 loss_rnnt 1.193836 hw_loss 0.114916 lr 0.00044719 rank 7
2023-02-23 06:07:47,881 DEBUG TRAIN Batch 14/8200 loss 4.202833 loss_att 5.321393 loss_ctc 4.625028 loss_rnnt 3.744826 hw_loss 0.333754 lr 0.00044726 rank 6
2023-02-23 06:09:02,663 DEBUG TRAIN Batch 14/8300 loss 10.596443 loss_att 14.483664 loss_ctc 14.078929 loss_rnnt 9.238727 hw_loss 0.217391 lr 0.00044703 rank 5
2023-02-23 06:09:02,663 DEBUG TRAIN Batch 14/8300 loss 11.281461 loss_att 13.734752 loss_ctc 13.656076 loss_rnnt 10.294641 hw_loss 0.336647 lr 0.00044705 rank 4
2023-02-23 06:09:02,667 DEBUG TRAIN Batch 14/8300 loss 12.967792 loss_att 14.188851 loss_ctc 21.791960 loss_rnnt 11.422039 hw_loss 0.234343 lr 0.00044712 rank 1
2023-02-23 06:09:02,667 DEBUG TRAIN Batch 14/8300 loss 5.529702 loss_att 9.042681 loss_ctc 10.599157 loss_rnnt 4.020217 hw_loss 0.245555 lr 0.00044706 rank 0
2023-02-23 06:09:02,669 DEBUG TRAIN Batch 14/8300 loss 11.913898 loss_att 17.092470 loss_ctc 16.463387 loss_rnnt 10.173395 hw_loss 0.184106 lr 0.00044703 rank 3
2023-02-23 06:09:02,669 DEBUG TRAIN Batch 14/8300 loss 8.026600 loss_att 12.286928 loss_ctc 8.493007 loss_rnnt 7.018556 hw_loss 0.175858 lr 0.00044705 rank 2
2023-02-23 06:09:02,672 DEBUG TRAIN Batch 14/8300 loss 7.897314 loss_att 14.543415 loss_ctc 11.072195 loss_rnnt 6.022823 hw_loss 0.228662 lr 0.00044708 rank 6
2023-02-23 06:09:02,719 DEBUG TRAIN Batch 14/8300 loss 13.673898 loss_att 20.910105 loss_ctc 19.218987 loss_rnnt 11.364458 hw_loss 0.230348 lr 0.00044702 rank 7
2023-02-23 06:10:02,484 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 3
2023-02-23 06:10:02,485 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 6
2023-02-23 06:10:02,488 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 0
2023-02-23 06:10:02,489 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 5
2023-02-23 06:10:02,499 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 7
2023-02-23 06:10:02,504 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 2
2023-02-23 06:10:02,504 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 1
2023-02-23 06:10:02,511 DEBUG CV Batch 14/0 loss 2.337120 loss_att 2.473793 loss_ctc 3.083912 loss_rnnt 2.031263 hw_loss 0.335530 history loss 2.250560 rank 4
2023-02-23 06:10:13,546 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 2
2023-02-23 06:10:13,568 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 3
2023-02-23 06:10:13,672 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 5
2023-02-23 06:10:13,718 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 4
2023-02-23 06:10:13,774 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 0
2023-02-23 06:10:13,776 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 6
2023-02-23 06:10:13,791 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 7
2023-02-23 06:10:14,198 DEBUG CV Batch 14/100 loss 9.721000 loss_att 8.312460 loss_ctc 10.441896 loss_rnnt 9.754745 hw_loss 0.284706 history loss 4.146885 rank 1
2023-02-23 06:10:27,698 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 2
2023-02-23 06:10:27,836 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 3
2023-02-23 06:10:27,851 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 4
2023-02-23 06:10:27,928 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 5
2023-02-23 06:10:27,987 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 0
2023-02-23 06:10:28,003 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 1
2023-02-23 06:10:28,094 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 6
2023-02-23 06:10:28,285 DEBUG CV Batch 14/200 loss 7.734743 loss_att 12.666193 loss_ctc 9.253404 loss_rnnt 6.463517 hw_loss 0.154589 history loss 4.820689 rank 7
2023-02-23 06:10:39,657 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 2
2023-02-23 06:10:39,868 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 5
2023-02-23 06:10:39,895 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 4
2023-02-23 06:10:39,916 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 3
2023-02-23 06:10:40,109 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 0
2023-02-23 06:10:40,320 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 6
2023-02-23 06:10:40,661 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 7
2023-02-23 06:10:41,655 DEBUG CV Batch 14/300 loss 6.105776 loss_att 6.785214 loss_ctc 9.559481 loss_rnnt 5.311199 hw_loss 0.371616 history loss 4.949165 rank 1
2023-02-23 06:10:51,537 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 2
2023-02-23 06:10:51,698 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 5
2023-02-23 06:10:51,894 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 3
2023-02-23 06:10:51,895 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 4
2023-02-23 06:10:52,121 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 0
2023-02-23 06:10:52,454 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 6
2023-02-23 06:10:52,900 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 7
2023-02-23 06:10:54,078 DEBUG CV Batch 14/400 loss 17.052057 loss_att 64.875473 loss_ctc 8.761618 loss_rnnt 8.464540 hw_loss 0.240424 history loss 6.012057 rank 1
2023-02-23 06:11:01,933 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 2
2023-02-23 06:11:02,122 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 5
2023-02-23 06:11:02,297 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 4
2023-02-23 06:11:02,440 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 3
2023-02-23 06:11:02,695 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 0
2023-02-23 06:11:03,052 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 6
2023-02-23 06:11:03,601 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 7
2023-02-23 06:11:04,635 DEBUG CV Batch 14/500 loss 6.101275 loss_att 6.958529 loss_ctc 7.810535 loss_rnnt 5.600299 hw_loss 0.190545 history loss 6.906278 rank 1
2023-02-23 06:11:14,255 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 5
2023-02-23 06:11:14,269 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 4
2023-02-23 06:11:14,309 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 2
2023-02-23 06:11:14,508 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 3
2023-02-23 06:11:14,739 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 0
2023-02-23 06:11:15,250 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 6
2023-02-23 06:11:16,476 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 7
2023-02-23 06:11:16,796 DEBUG CV Batch 14/600 loss 8.114207 loss_att 7.499031 loss_ctc 10.560219 loss_rnnt 7.734472 hw_loss 0.331192 history loss 7.946941 rank 1
2023-02-23 06:11:26,186 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 0
2023-02-23 06:11:26,288 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 5
2023-02-23 06:11:26,323 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 4
2023-02-23 06:11:26,638 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 3
2023-02-23 06:11:26,644 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 2
2023-02-23 06:11:26,759 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 6
2023-02-23 06:11:28,292 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 1
2023-02-23 06:11:28,521 DEBUG CV Batch 14/700 loss 17.040283 loss_att 48.550003 loss_ctc 21.515587 loss_rnnt 10.007847 hw_loss 0.250845 history loss 8.756935 rank 7
2023-02-23 06:11:38,357 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 5
2023-02-23 06:11:38,378 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 4
2023-02-23 06:11:38,504 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 2
2023-02-23 06:11:38,522 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 0
2023-02-23 06:11:38,858 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 6
2023-02-23 06:11:38,887 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 3
2023-02-23 06:11:40,112 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 1
2023-02-23 06:11:40,403 DEBUG CV Batch 14/800 loss 16.093435 loss_att 12.719458 loss_ctc 17.655905 loss_rnnt 16.395130 hw_loss 0.308947 history loss 8.139069 rank 7
2023-02-23 06:11:52,280 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 5
2023-02-23 06:11:52,290 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 2
2023-02-23 06:11:52,382 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 4
2023-02-23 06:11:52,655 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 0
2023-02-23 06:11:52,784 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 6
2023-02-23 06:11:53,158 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 3
2023-02-23 06:11:54,034 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 1
2023-02-23 06:11:54,800 DEBUG CV Batch 14/900 loss 15.256262 loss_att 16.148743 loss_ctc 22.720261 loss_rnnt 13.957487 hw_loss 0.234524 history loss 7.914012 rank 7
2023-02-23 06:12:04,416 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 5
2023-02-23 06:12:04,421 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 2
2023-02-23 06:12:04,606 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 4
2023-02-23 06:12:04,897 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 0
2023-02-23 06:12:05,127 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 6
2023-02-23 06:12:05,407 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 3
2023-02-23 06:12:06,408 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 1
2023-02-23 06:12:07,244 DEBUG CV Batch 14/1000 loss 6.298615 loss_att 6.641224 loss_ctc 6.984905 loss_rnnt 5.978113 hw_loss 0.300891 history loss 7.637010 rank 7
2023-02-23 06:12:16,188 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 5
2023-02-23 06:12:16,216 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 2
2023-02-23 06:12:16,521 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 4
2023-02-23 06:12:16,869 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 0
2023-02-23 06:12:17,277 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 6
2023-02-23 06:12:17,357 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 3
2023-02-23 06:12:18,416 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 1
2023-02-23 06:12:19,638 DEBUG CV Batch 14/1100 loss 7.158606 loss_att 6.072480 loss_ctc 8.607795 loss_rnnt 6.986942 hw_loss 0.366870 history loss 7.608761 rank 7
2023-02-23 06:12:26,522 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 5
2023-02-23 06:12:26,587 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 2
2023-02-23 06:12:26,979 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 4
2023-02-23 06:12:27,410 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 0
2023-02-23 06:12:27,880 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 6
2023-02-23 06:12:27,957 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 3
2023-02-23 06:12:29,009 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 1
2023-02-23 06:12:30,384 DEBUG CV Batch 14/1200 loss 9.752608 loss_att 11.236094 loss_ctc 11.029505 loss_rnnt 9.146970 hw_loss 0.260041 history loss 7.985057 rank 7
2023-02-23 06:12:38,396 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 5
2023-02-23 06:12:38,475 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 2
2023-02-23 06:12:38,944 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 4
2023-02-23 06:12:39,476 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 0
2023-02-23 06:12:39,895 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 6
2023-02-23 06:12:40,090 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 3
2023-02-23 06:12:41,026 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 1
2023-02-23 06:12:42,522 DEBUG CV Batch 14/1300 loss 7.512483 loss_att 7.216032 loss_ctc 9.060898 loss_rnnt 7.202236 hw_loss 0.305778 history loss 8.325487 rank 7
2023-02-23 06:12:50,068 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 4
2023-02-23 06:12:50,127 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 5
2023-02-23 06:12:50,262 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 2
2023-02-23 06:12:50,769 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 0
2023-02-23 06:12:51,186 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 6
2023-02-23 06:12:51,667 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 3
2023-02-23 06:12:53,364 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 1
2023-02-23 06:12:54,933 DEBUG CV Batch 14/1400 loss 14.989386 loss_att 37.551544 loss_ctc 13.585155 loss_rnnt 10.527216 hw_loss 0.256816 history loss 8.714191 rank 7
2023-02-23 06:13:02,053 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 4
2023-02-23 06:13:02,570 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 2
2023-02-23 06:13:02,616 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 5
2023-02-23 06:13:02,964 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 6
2023-02-23 06:13:03,228 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 0
2023-02-23 06:13:03,943 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 3
2023-02-23 06:13:05,882 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 1
2023-02-23 06:13:07,377 DEBUG CV Batch 14/1500 loss 7.853878 loss_att 8.297123 loss_ctc 7.057870 loss_rnnt 7.723848 hw_loss 0.276591 history loss 8.511888 rank 7
2023-02-23 06:13:15,699 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 4
2023-02-23 06:13:16,151 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 5
2023-02-23 06:13:16,361 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 2
2023-02-23 06:13:16,941 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 0
2023-02-23 06:13:17,385 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 6
2023-02-23 06:13:17,947 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 3
2023-02-23 06:13:19,629 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 1
2023-02-23 06:13:21,069 DEBUG CV Batch 14/1600 loss 12.051929 loss_att 16.232229 loss_ctc 14.830263 loss_rnnt 10.687343 hw_loss 0.296404 history loss 8.433030 rank 7
2023-02-23 06:13:28,168 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 4
2023-02-23 06:13:28,588 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 5
2023-02-23 06:13:28,803 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 2
2023-02-23 06:13:29,507 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 0
2023-02-23 06:13:30,069 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 6
2023-02-23 06:13:30,378 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 3
2023-02-23 06:13:32,093 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 1
2023-02-23 06:13:33,631 DEBUG CV Batch 14/1700 loss 12.289021 loss_att 10.701132 loss_ctc 16.569799 loss_rnnt 11.859518 hw_loss 0.330582 history loss 8.317133 rank 7
2023-02-23 06:13:37,448 INFO Epoch 14 CV info cv_loss 8.26855726933562
2023-02-23 06:13:37,449 INFO Epoch 15 TRAIN info lr 0.00044701695129360075
2023-02-23 06:13:37,454 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:37,757 INFO Epoch 14 CV info cv_loss 8.268557271523736
2023-02-23 06:13:37,758 INFO Epoch 15 TRAIN info lr 0.00044690265941713103
2023-02-23 06:13:37,761 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:37,987 INFO Epoch 14 CV info cv_loss 8.268557268956576
2023-02-23 06:13:37,989 INFO Epoch 15 TRAIN info lr 0.00044697586755723857
2023-02-23 06:13:37,993 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:38,773 INFO Epoch 14 CV info cv_loss 8.268557268956576
2023-02-23 06:13:38,774 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/14.pt
2023-02-23 06:13:39,466 INFO Epoch 14 CV info cv_loss 8.268557270016176
2023-02-23 06:13:39,467 INFO Epoch 15 TRAIN info lr 0.00044701695129360075
2023-02-23 06:13:39,472 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:39,627 INFO Epoch 14 CV info cv_loss 8.26855727017124
2023-02-23 06:13:39,629 INFO Epoch 15 TRAIN info lr 0.00044699372865940134
2023-02-23 06:13:39,634 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:39,827 INFO Epoch 15 TRAIN info lr 0.0004470026600134637
2023-02-23 06:13:39,832 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:41,346 INFO Epoch 14 CV info cv_loss 8.268557268172644
2023-02-23 06:13:41,347 INFO Epoch 15 TRAIN info lr 0.00044704375113882475
2023-02-23 06:13:41,351 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:13:43,556 INFO Epoch 14 CV info cv_loss 8.268557268603375
2023-02-23 06:13:43,557 INFO Epoch 15 TRAIN info lr 0.0004469151558214514
2023-02-23 06:13:43,560 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 06:14:57,970 DEBUG TRAIN Batch 15/0 loss 11.450691 loss_att 10.004757 loss_ctc 12.432604 loss_rnnt 11.452200 hw_loss 0.293920 lr 0.00044702 rank 4
2023-02-23 06:14:57,974 DEBUG TRAIN Batch 15/0 loss 9.881429 loss_att 9.168884 loss_ctc 11.658678 loss_rnnt 9.580290 hw_loss 0.387526 lr 0.00044702 rank 6
2023-02-23 06:14:57,975 DEBUG TRAIN Batch 15/0 loss 11.441438 loss_att 10.676044 loss_ctc 14.348578 loss_rnnt 11.039860 hw_loss 0.313196 lr 0.00044697 rank 2
2023-02-23 06:14:57,980 DEBUG TRAIN Batch 15/0 loss 11.390825 loss_att 10.909301 loss_ctc 13.064294 loss_rnnt 11.057840 hw_loss 0.386551 lr 0.00044690 rank 5
2023-02-23 06:14:57,981 DEBUG TRAIN Batch 15/0 loss 10.461354 loss_att 10.452155 loss_ctc 12.115358 loss_rnnt 10.033051 hw_loss 0.393018 lr 0.00044699 rank 3
2023-02-23 06:14:57,984 DEBUG TRAIN Batch 15/0 loss 11.663171 loss_att 10.541843 loss_ctc 14.654711 loss_rnnt 11.303720 hw_loss 0.346582 lr 0.00044704 rank 1
2023-02-23 06:14:58,006 DEBUG TRAIN Batch 15/0 loss 13.320217 loss_att 12.385787 loss_ctc 15.323331 loss_rnnt 13.071062 hw_loss 0.316796 lr 0.00044700 rank 0
2023-02-23 06:14:58,051 DEBUG TRAIN Batch 15/0 loss 7.321368 loss_att 7.530948 loss_ctc 9.249579 loss_rnnt 6.835209 hw_loss 0.350901 lr 0.00044691 rank 7
2023-02-23 06:16:13,686 DEBUG TRAIN Batch 15/100 loss 13.929341 loss_att 18.860504 loss_ctc 16.314495 loss_rnnt 12.474070 hw_loss 0.283157 lr 0.00044672 rank 5
2023-02-23 06:16:13,689 DEBUG TRAIN Batch 15/100 loss 11.729281 loss_att 17.198408 loss_ctc 15.760542 loss_rnnt 9.994213 hw_loss 0.194513 lr 0.00044680 rank 2
2023-02-23 06:16:13,690 DEBUG TRAIN Batch 15/100 loss 8.893225 loss_att 9.976268 loss_ctc 14.585722 loss_rnnt 7.816873 hw_loss 0.188895 lr 0.00044684 rank 6
2023-02-23 06:16:13,690 DEBUG TRAIN Batch 15/100 loss 7.250995 loss_att 13.909120 loss_ctc 10.799118 loss_rnnt 5.285111 hw_loss 0.302204 lr 0.00044684 rank 4
2023-02-23 06:16:13,692 DEBUG TRAIN Batch 15/100 loss 7.349799 loss_att 11.262285 loss_ctc 6.152341 loss_rnnt 6.576588 hw_loss 0.281952 lr 0.00044673 rank 7
2023-02-23 06:16:13,692 DEBUG TRAIN Batch 15/100 loss 8.482500 loss_att 12.166881 loss_ctc 11.087729 loss_rnnt 7.289146 hw_loss 0.204590 lr 0.00044682 rank 0
2023-02-23 06:16:13,697 DEBUG TRAIN Batch 15/100 loss 26.445194 loss_att 22.916986 loss_ctc 27.424644 loss_rnnt 26.943686 hw_loss 0.143545 lr 0.00044686 rank 1
2023-02-23 06:16:13,700 DEBUG TRAIN Batch 15/100 loss 10.415125 loss_att 14.548077 loss_ctc 17.615322 loss_rnnt 8.490910 hw_loss 0.257998 lr 0.00044681 rank 3
2023-02-23 06:17:30,424 DEBUG TRAIN Batch 15/200 loss 6.160597 loss_att 12.342131 loss_ctc 9.671073 loss_rnnt 4.341028 hw_loss 0.215997 lr 0.00044666 rank 4
2023-02-23 06:17:30,427 DEBUG TRAIN Batch 15/200 loss 8.344321 loss_att 11.708542 loss_ctc 14.280787 loss_rnnt 6.758941 hw_loss 0.226892 lr 0.00044664 rank 0
2023-02-23 06:17:30,428 DEBUG TRAIN Batch 15/200 loss 9.391090 loss_att 12.441169 loss_ctc 11.663084 loss_rnnt 8.359669 hw_loss 0.222137 lr 0.00044662 rank 2
2023-02-23 06:17:30,428 DEBUG TRAIN Batch 15/200 loss 9.275523 loss_att 14.365767 loss_ctc 11.656975 loss_rnnt 7.805831 hw_loss 0.251468 lr 0.00044654 rank 5
2023-02-23 06:17:30,430 DEBUG TRAIN Batch 15/200 loss 15.804207 loss_att 18.940767 loss_ctc 19.202656 loss_rnnt 14.602652 hw_loss 0.227097 lr 0.00044669 rank 1
2023-02-23 06:17:30,435 DEBUG TRAIN Batch 15/200 loss 16.481722 loss_att 21.134876 loss_ctc 21.918333 loss_rnnt 14.709150 hw_loss 0.219490 lr 0.00044664 rank 3
2023-02-23 06:17:30,437 DEBUG TRAIN Batch 15/200 loss 13.134244 loss_att 14.579687 loss_ctc 14.631905 loss_rnnt 12.521851 hw_loss 0.231784 lr 0.00044666 rank 6
2023-02-23 06:17:30,476 DEBUG TRAIN Batch 15/200 loss 15.033535 loss_att 17.088425 loss_ctc 17.311243 loss_rnnt 14.243300 hw_loss 0.141681 lr 0.00044656 rank 7
2023-02-23 06:18:46,839 DEBUG TRAIN Batch 15/300 loss 12.422578 loss_att 17.671787 loss_ctc 13.254950 loss_rnnt 11.175930 hw_loss 0.160918 lr 0.00044644 rank 2
2023-02-23 06:18:46,845 DEBUG TRAIN Batch 15/300 loss 10.785238 loss_att 15.136800 loss_ctc 15.348536 loss_rnnt 9.215320 hw_loss 0.170937 lr 0.00044648 rank 4
2023-02-23 06:18:46,845 DEBUG TRAIN Batch 15/300 loss 9.381388 loss_att 15.639456 loss_ctc 14.953556 loss_rnnt 7.268524 hw_loss 0.221802 lr 0.00044647 rank 0
2023-02-23 06:18:46,845 DEBUG TRAIN Batch 15/300 loss 9.438989 loss_att 10.153491 loss_ctc 9.444835 loss_rnnt 9.159311 hw_loss 0.254995 lr 0.00044637 rank 5
2023-02-23 06:18:46,848 DEBUG TRAIN Batch 15/300 loss 15.420954 loss_att 18.961845 loss_ctc 17.680140 loss_rnnt 14.281874 hw_loss 0.243143 lr 0.00044638 rank 7
2023-02-23 06:18:46,855 DEBUG TRAIN Batch 15/300 loss 15.017617 loss_att 17.813114 loss_ctc 23.149120 loss_rnnt 13.269238 hw_loss 0.197025 lr 0.00044646 rank 3
2023-02-23 06:18:46,867 DEBUG TRAIN Batch 15/300 loss 15.835683 loss_att 17.548626 loss_ctc 21.258936 loss_rnnt 14.687908 hw_loss 0.153912 lr 0.00044648 rank 6
2023-02-23 06:18:46,903 DEBUG TRAIN Batch 15/300 loss 13.830454 loss_att 15.465538 loss_ctc 15.813800 loss_rnnt 13.091406 hw_loss 0.276723 lr 0.00044651 rank 1
2023-02-23 06:20:04,444 DEBUG TRAIN Batch 15/400 loss 4.578913 loss_att 6.960395 loss_ctc 7.997299 loss_rnnt 3.487950 hw_loss 0.297902 lr 0.00044619 rank 5
2023-02-23 06:20:04,444 DEBUG TRAIN Batch 15/400 loss 24.596453 loss_att 27.256891 loss_ctc 30.115902 loss_rnnt 23.236605 hw_loss 0.172186 lr 0.00044626 rank 2
2023-02-23 06:20:04,446 DEBUG TRAIN Batch 15/400 loss 9.850290 loss_att 11.688381 loss_ctc 14.010786 loss_rnnt 8.785418 hw_loss 0.267229 lr 0.00044630 rank 4
2023-02-23 06:20:04,447 DEBUG TRAIN Batch 15/400 loss 16.089560 loss_att 18.230387 loss_ctc 20.019268 loss_rnnt 15.045786 hw_loss 0.171840 lr 0.00044633 rank 1
2023-02-23 06:20:04,449 DEBUG TRAIN Batch 15/400 loss 7.349605 loss_att 10.898401 loss_ctc 10.842871 loss_rnnt 6.011167 hw_loss 0.305455 lr 0.00044629 rank 0
2023-02-23 06:20:04,451 DEBUG TRAIN Batch 15/400 loss 11.300571 loss_att 13.248249 loss_ctc 13.534363 loss_rnnt 10.498127 hw_loss 0.215755 lr 0.00044628 rank 3
2023-02-23 06:20:04,454 DEBUG TRAIN Batch 15/400 loss 5.596563 loss_att 8.333801 loss_ctc 6.522376 loss_rnnt 4.812963 hw_loss 0.211332 lr 0.00044620 rank 7
2023-02-23 06:20:04,454 DEBUG TRAIN Batch 15/400 loss 9.293427 loss_att 12.148684 loss_ctc 11.087594 loss_rnnt 8.409290 hw_loss 0.138494 lr 0.00044630 rank 6
2023-02-23 06:21:19,709 DEBUG TRAIN Batch 15/500 loss 11.515835 loss_att 13.450878 loss_ctc 17.140823 loss_rnnt 10.247336 hw_loss 0.246547 lr 0.00044612 rank 6
2023-02-23 06:21:19,709 DEBUG TRAIN Batch 15/500 loss 17.980623 loss_att 17.612148 loss_ctc 26.212675 loss_rnnt 16.798817 hw_loss 0.296053 lr 0.00044612 rank 4
2023-02-23 06:21:19,712 DEBUG TRAIN Batch 15/500 loss 16.305073 loss_att 16.905563 loss_ctc 18.458778 loss_rnnt 15.801409 hw_loss 0.180758 lr 0.00044615 rank 1
2023-02-23 06:21:19,713 DEBUG TRAIN Batch 15/500 loss 9.482984 loss_att 12.600204 loss_ctc 12.641471 loss_rnnt 8.312883 hw_loss 0.235358 lr 0.00044611 rank 0
2023-02-23 06:21:19,713 DEBUG TRAIN Batch 15/500 loss 8.254313 loss_att 9.573893 loss_ctc 10.945680 loss_rnnt 7.519881 hw_loss 0.209373 lr 0.00044601 rank 5
2023-02-23 06:21:19,714 DEBUG TRAIN Batch 15/500 loss 13.978901 loss_att 16.527832 loss_ctc 19.442539 loss_rnnt 12.630301 hw_loss 0.206868 lr 0.00044610 rank 3
2023-02-23 06:21:19,717 DEBUG TRAIN Batch 15/500 loss 7.776876 loss_att 9.847462 loss_ctc 9.785730 loss_rnnt 6.931550 hw_loss 0.306303 lr 0.00044602 rank 7
2023-02-23 06:21:19,717 DEBUG TRAIN Batch 15/500 loss 11.323225 loss_att 14.577494 loss_ctc 14.483782 loss_rnnt 10.135019 hw_loss 0.217394 lr 0.00044608 rank 2
2023-02-23 06:22:35,588 DEBUG TRAIN Batch 15/600 loss 9.706867 loss_att 10.665119 loss_ctc 12.860504 loss_rnnt 8.914803 hw_loss 0.337366 lr 0.00044595 rank 4
2023-02-23 06:22:35,589 DEBUG TRAIN Batch 15/600 loss 13.403311 loss_att 14.897528 loss_ctc 16.148371 loss_rnnt 12.566302 hw_loss 0.322795 lr 0.00044591 rank 2
2023-02-23 06:22:35,591 DEBUG TRAIN Batch 15/600 loss 12.343856 loss_att 15.182711 loss_ctc 13.584052 loss_rnnt 11.490568 hw_loss 0.225296 lr 0.00044593 rank 0
2023-02-23 06:22:35,591 DEBUG TRAIN Batch 15/600 loss 16.814825 loss_att 14.677023 loss_ctc 19.191759 loss_rnnt 16.774614 hw_loss 0.282836 lr 0.00044583 rank 5
2023-02-23 06:22:35,595 DEBUG TRAIN Batch 15/600 loss 12.680791 loss_att 11.884917 loss_ctc 14.493749 loss_rnnt 12.452453 hw_loss 0.273348 lr 0.00044595 rank 6
2023-02-23 06:22:35,600 DEBUG TRAIN Batch 15/600 loss 8.193101 loss_att 8.783510 loss_ctc 10.606600 loss_rnnt 7.559327 hw_loss 0.363549 lr 0.00044597 rank 1
2023-02-23 06:22:35,613 DEBUG TRAIN Batch 15/600 loss 13.597189 loss_att 14.226430 loss_ctc 17.381796 loss_rnnt 12.828198 hw_loss 0.259741 lr 0.00044592 rank 3
2023-02-23 06:22:35,626 DEBUG TRAIN Batch 15/600 loss 11.895094 loss_att 12.533446 loss_ctc 13.461535 loss_rnnt 11.412818 hw_loss 0.273276 lr 0.00044585 rank 7
2023-02-23 06:23:54,377 DEBUG TRAIN Batch 15/700 loss 11.098870 loss_att 18.379425 loss_ctc 13.749606 loss_rnnt 9.174772 hw_loss 0.214791 lr 0.00044576 rank 0
2023-02-23 06:23:54,379 DEBUG TRAIN Batch 15/700 loss 15.992960 loss_att 17.340109 loss_ctc 21.126413 loss_rnnt 14.869062 hw_loss 0.318765 lr 0.00044573 rank 2
2023-02-23 06:23:54,380 DEBUG TRAIN Batch 15/700 loss 10.676617 loss_att 12.175001 loss_ctc 16.315300 loss_rnnt 9.459415 hw_loss 0.310687 lr 0.00044575 rank 3
2023-02-23 06:23:54,380 DEBUG TRAIN Batch 15/700 loss 12.377782 loss_att 16.016685 loss_ctc 19.575262 loss_rnnt 10.530282 hw_loss 0.300103 lr 0.00044566 rank 5
2023-02-23 06:23:54,384 DEBUG TRAIN Batch 15/700 loss 10.641096 loss_att 13.800844 loss_ctc 13.449465 loss_rnnt 9.517151 hw_loss 0.220399 lr 0.00044567 rank 7
2023-02-23 06:23:54,384 DEBUG TRAIN Batch 15/700 loss 6.988394 loss_att 8.867065 loss_ctc 10.558693 loss_rnnt 6.006803 hw_loss 0.243406 lr 0.00044580 rank 1
2023-02-23 06:23:54,385 DEBUG TRAIN Batch 15/700 loss 13.067959 loss_att 18.519276 loss_ctc 13.163168 loss_rnnt 11.872189 hw_loss 0.174024 lr 0.00044577 rank 4
2023-02-23 06:23:54,388 DEBUG TRAIN Batch 15/700 loss 13.447733 loss_att 20.547544 loss_ctc 20.307402 loss_rnnt 11.003744 hw_loss 0.205132 lr 0.00044577 rank 6
2023-02-23 06:25:12,464 DEBUG TRAIN Batch 15/800 loss 5.138397 loss_att 9.980339 loss_ctc 9.492859 loss_rnnt 3.463367 hw_loss 0.236337 lr 0.00044548 rank 5
2023-02-23 06:25:12,465 DEBUG TRAIN Batch 15/800 loss 17.416599 loss_att 22.664879 loss_ctc 30.744247 loss_rnnt 14.490709 hw_loss 0.186022 lr 0.00044559 rank 4
2023-02-23 06:25:12,467 DEBUG TRAIN Batch 15/800 loss 10.717342 loss_att 13.292856 loss_ctc 13.179207 loss_rnnt 9.746145 hw_loss 0.239712 lr 0.00044559 rank 6
2023-02-23 06:25:12,469 DEBUG TRAIN Batch 15/800 loss 9.632552 loss_att 11.890722 loss_ctc 16.710646 loss_rnnt 8.115636 hw_loss 0.227882 lr 0.00044558 rank 0
2023-02-23 06:25:12,470 DEBUG TRAIN Batch 15/800 loss 8.553256 loss_att 11.373156 loss_ctc 11.154960 loss_rnnt 7.528934 hw_loss 0.212714 lr 0.00044555 rank 2
2023-02-23 06:25:12,470 DEBUG TRAIN Batch 15/800 loss 7.420394 loss_att 11.873314 loss_ctc 10.718891 loss_rnnt 6.010910 hw_loss 0.148313 lr 0.00044562 rank 1
2023-02-23 06:25:12,473 DEBUG TRAIN Batch 15/800 loss 16.287115 loss_att 18.770378 loss_ctc 23.848846 loss_rnnt 14.653976 hw_loss 0.240479 lr 0.00044557 rank 3
2023-02-23 06:25:12,478 DEBUG TRAIN Batch 15/800 loss 12.170209 loss_att 14.590717 loss_ctc 13.879279 loss_rnnt 11.349386 hw_loss 0.204084 lr 0.00044549 rank 7
2023-02-23 06:26:27,786 DEBUG TRAIN Batch 15/900 loss 7.661520 loss_att 9.792501 loss_ctc 9.796999 loss_rnnt 6.784616 hw_loss 0.311206 lr 0.00044540 rank 0
2023-02-23 06:26:27,788 DEBUG TRAIN Batch 15/900 loss 9.952062 loss_att 16.995195 loss_ctc 11.572506 loss_rnnt 8.208611 hw_loss 0.222684 lr 0.00044542 rank 4
2023-02-23 06:26:27,789 DEBUG TRAIN Batch 15/900 loss 5.646974 loss_att 10.187476 loss_ctc 7.852913 loss_rnnt 4.300118 hw_loss 0.271181 lr 0.00044530 rank 5
2023-02-23 06:26:27,791 DEBUG TRAIN Batch 15/900 loss 9.033425 loss_att 11.536209 loss_ctc 13.548653 loss_rnnt 7.848678 hw_loss 0.154053 lr 0.00044532 rank 7
2023-02-23 06:26:27,791 DEBUG TRAIN Batch 15/900 loss 19.946232 loss_att 22.216747 loss_ctc 20.797882 loss_rnnt 19.258247 hw_loss 0.225617 lr 0.00044538 rank 2
2023-02-23 06:26:27,792 DEBUG TRAIN Batch 15/900 loss 7.690211 loss_att 10.099573 loss_ctc 9.415737 loss_rnnt 6.826895 hw_loss 0.283825 lr 0.00044539 rank 3
2023-02-23 06:26:27,793 DEBUG TRAIN Batch 15/900 loss 18.900003 loss_att 22.673386 loss_ctc 23.124212 loss_rnnt 17.407717 hw_loss 0.326965 lr 0.00044544 rank 1
2023-02-23 06:26:27,796 DEBUG TRAIN Batch 15/900 loss 4.166478 loss_att 8.499315 loss_ctc 4.552620 loss_rnnt 3.159261 hw_loss 0.167183 lr 0.00044542 rank 6
2023-02-23 06:27:44,024 DEBUG TRAIN Batch 15/1000 loss 7.309686 loss_att 13.135143 loss_ctc 15.858908 loss_rnnt 4.834190 hw_loss 0.319701 lr 0.00044513 rank 5
2023-02-23 06:27:44,024 DEBUG TRAIN Batch 15/1000 loss 10.185878 loss_att 11.724655 loss_ctc 11.342113 loss_rnnt 9.602926 hw_loss 0.226935 lr 0.00044527 rank 1
2023-02-23 06:27:44,024 DEBUG TRAIN Batch 15/1000 loss 9.668921 loss_att 14.794205 loss_ctc 12.307762 loss_rnnt 8.210793 hw_loss 0.152300 lr 0.00044524 rank 4
2023-02-23 06:27:44,024 DEBUG TRAIN Batch 15/1000 loss 15.689727 loss_att 18.038971 loss_ctc 24.933607 loss_rnnt 13.861704 hw_loss 0.235605 lr 0.00044520 rank 2
2023-02-23 06:27:44,026 DEBUG TRAIN Batch 15/1000 loss 13.521676 loss_att 14.570396 loss_ctc 17.603506 loss_rnnt 12.644971 hw_loss 0.230095 lr 0.00044522 rank 3
2023-02-23 06:27:44,028 DEBUG TRAIN Batch 15/1000 loss 10.555727 loss_att 14.759994 loss_ctc 13.598445 loss_rnnt 9.202238 hw_loss 0.200511 lr 0.00044523 rank 0
2023-02-23 06:27:44,033 DEBUG TRAIN Batch 15/1000 loss 10.650406 loss_att 14.249454 loss_ctc 16.782940 loss_rnnt 9.025780 hw_loss 0.163399 lr 0.00044514 rank 7
2023-02-23 06:27:44,034 DEBUG TRAIN Batch 15/1000 loss 12.065356 loss_att 15.534266 loss_ctc 15.376197 loss_rnnt 10.835008 hw_loss 0.178352 lr 0.00044524 rank 6
2023-02-23 06:29:01,786 DEBUG TRAIN Batch 15/1100 loss 9.160696 loss_att 13.442049 loss_ctc 12.435188 loss_rnnt 7.724772 hw_loss 0.268226 lr 0.00044502 rank 2
2023-02-23 06:29:01,788 DEBUG TRAIN Batch 15/1100 loss 10.120296 loss_att 11.126908 loss_ctc 12.868593 loss_rnnt 9.380478 hw_loss 0.322606 lr 0.00044495 rank 5
2023-02-23 06:29:01,791 DEBUG TRAIN Batch 15/1100 loss 15.577151 loss_att 18.404255 loss_ctc 22.827860 loss_rnnt 13.944500 hw_loss 0.188380 lr 0.00044506 rank 6
2023-02-23 06:29:01,792 DEBUG TRAIN Batch 15/1100 loss 24.094366 loss_att 26.313660 loss_ctc 33.043243 loss_rnnt 22.294891 hw_loss 0.304563 lr 0.00044506 rank 4
2023-02-23 06:29:01,792 DEBUG TRAIN Batch 15/1100 loss 5.896164 loss_att 9.138178 loss_ctc 8.878596 loss_rnnt 4.728466 hw_loss 0.228073 lr 0.00044509 rank 1
2023-02-23 06:29:01,792 DEBUG TRAIN Batch 15/1100 loss 6.282552 loss_att 6.949727 loss_ctc 5.546118 loss_rnnt 6.116363 hw_loss 0.245522 lr 0.00044505 rank 0
2023-02-23 06:29:01,800 DEBUG TRAIN Batch 15/1100 loss 5.930673 loss_att 10.243826 loss_ctc 10.582958 loss_rnnt 4.332302 hw_loss 0.216443 lr 0.00044504 rank 3
2023-02-23 06:29:01,839 DEBUG TRAIN Batch 15/1100 loss 13.397385 loss_att 19.836847 loss_ctc 16.047459 loss_rnnt 11.586186 hw_loss 0.318679 lr 0.00044496 rank 7
2023-02-23 06:30:19,289 DEBUG TRAIN Batch 15/1200 loss 8.341550 loss_att 11.248098 loss_ctc 12.359769 loss_rnnt 7.074538 hw_loss 0.281136 lr 0.00044489 rank 4
2023-02-23 06:30:19,289 DEBUG TRAIN Batch 15/1200 loss 9.906521 loss_att 12.113346 loss_ctc 13.187039 loss_rnnt 8.897346 hw_loss 0.244513 lr 0.00044486 rank 3
2023-02-23 06:30:19,291 DEBUG TRAIN Batch 15/1200 loss 18.794920 loss_att 17.708939 loss_ctc 26.218325 loss_rnnt 17.855766 hw_loss 0.312304 lr 0.00044491 rank 1
2023-02-23 06:30:19,294 DEBUG TRAIN Batch 15/1200 loss 18.530085 loss_att 22.217484 loss_ctc 23.120552 loss_rnnt 17.048515 hw_loss 0.247552 lr 0.00044485 rank 2
2023-02-23 06:30:19,295 DEBUG TRAIN Batch 15/1200 loss 12.796326 loss_att 14.777112 loss_ctc 18.624617 loss_rnnt 11.468945 hw_loss 0.288970 lr 0.00044477 rank 5
2023-02-23 06:30:19,296 DEBUG TRAIN Batch 15/1200 loss 15.302382 loss_att 17.467827 loss_ctc 18.815359 loss_rnnt 14.265674 hw_loss 0.253543 lr 0.00044479 rank 7
2023-02-23 06:30:19,300 DEBUG TRAIN Batch 15/1200 loss 7.877435 loss_att 8.194961 loss_ctc 9.679667 loss_rnnt 7.415047 hw_loss 0.297346 lr 0.00044489 rank 6
2023-02-23 06:30:19,298 DEBUG TRAIN Batch 15/1200 loss 9.840318 loss_att 12.203178 loss_ctc 13.843448 loss_rnnt 8.703671 hw_loss 0.244355 lr 0.00044487 rank 0
2023-02-23 06:31:34,941 DEBUG TRAIN Batch 15/1300 loss 5.635738 loss_att 9.632014 loss_ctc 7.879179 loss_rnnt 4.447148 hw_loss 0.169142 lr 0.00044460 rank 5
2023-02-23 06:31:34,948 DEBUG TRAIN Batch 15/1300 loss 21.301147 loss_att 25.232454 loss_ctc 29.484783 loss_rnnt 19.267017 hw_loss 0.293847 lr 0.00044461 rank 7
2023-02-23 06:31:34,949 DEBUG TRAIN Batch 15/1300 loss 6.905166 loss_att 11.739123 loss_ctc 8.939207 loss_rnnt 5.602527 hw_loss 0.121203 lr 0.00044467 rank 2
2023-02-23 06:31:34,950 DEBUG TRAIN Batch 15/1300 loss 3.881619 loss_att 7.846675 loss_ctc 7.850390 loss_rnnt 2.418676 hw_loss 0.263929 lr 0.00044470 rank 0
2023-02-23 06:31:34,951 DEBUG TRAIN Batch 15/1300 loss 14.752275 loss_att 13.633511 loss_ctc 18.888952 loss_rnnt 14.186410 hw_loss 0.446364 lr 0.00044471 rank 4
2023-02-23 06:31:34,952 DEBUG TRAIN Batch 15/1300 loss 9.346389 loss_att 12.791086 loss_ctc 17.449274 loss_rnnt 7.425740 hw_loss 0.283733 lr 0.00044471 rank 6
2023-02-23 06:31:34,956 DEBUG TRAIN Batch 15/1300 loss 9.483706 loss_att 15.384641 loss_ctc 14.884273 loss_rnnt 7.458490 hw_loss 0.234289 lr 0.00044474 rank 1
2023-02-23 06:31:34,999 DEBUG TRAIN Batch 15/1300 loss 30.895729 loss_att 34.815689 loss_ctc 46.610779 loss_rnnt 27.894791 hw_loss 0.228015 lr 0.00044469 rank 3
2023-02-23 06:32:53,516 DEBUG TRAIN Batch 15/1400 loss 6.893062 loss_att 10.190330 loss_ctc 7.426345 loss_rnnt 6.028507 hw_loss 0.251245 lr 0.00044453 rank 4
2023-02-23 06:32:53,517 DEBUG TRAIN Batch 15/1400 loss 11.184285 loss_att 11.969877 loss_ctc 17.597076 loss_rnnt 10.082675 hw_loss 0.167724 lr 0.00044451 rank 3
2023-02-23 06:32:53,520 DEBUG TRAIN Batch 15/1400 loss 5.907063 loss_att 8.839447 loss_ctc 6.545938 loss_rnnt 5.102247 hw_loss 0.249668 lr 0.00044452 rank 0
2023-02-23 06:32:53,520 DEBUG TRAIN Batch 15/1400 loss 8.819370 loss_att 12.542585 loss_ctc 14.643889 loss_rnnt 7.165023 hw_loss 0.249567 lr 0.00044456 rank 1
2023-02-23 06:32:53,520 DEBUG TRAIN Batch 15/1400 loss 7.300828 loss_att 9.555004 loss_ctc 9.318433 loss_rnnt 6.482795 hw_loss 0.184096 lr 0.00044442 rank 5
2023-02-23 06:32:53,521 DEBUG TRAIN Batch 15/1400 loss 12.621868 loss_att 15.539787 loss_ctc 15.052699 loss_rnnt 11.604465 hw_loss 0.205705 lr 0.00044443 rank 7
2023-02-23 06:32:53,523 DEBUG TRAIN Batch 15/1400 loss 7.559206 loss_att 11.582668 loss_ctc 8.868317 loss_rnnt 6.442231 hw_loss 0.258251 lr 0.00044449 rank 2
2023-02-23 06:32:53,529 DEBUG TRAIN Batch 15/1400 loss 13.336883 loss_att 15.240704 loss_ctc 19.380157 loss_rnnt 12.005503 hw_loss 0.271586 lr 0.00044453 rank 6
2023-02-23 06:34:09,707 DEBUG TRAIN Batch 15/1500 loss 11.214989 loss_att 13.673960 loss_ctc 12.784930 loss_rnnt 10.407433 hw_loss 0.199568 lr 0.00044435 rank 0
2023-02-23 06:34:09,707 DEBUG TRAIN Batch 15/1500 loss 9.959599 loss_att 13.001408 loss_ctc 13.213801 loss_rnnt 8.849045 hw_loss 0.128061 lr 0.00044432 rank 2
2023-02-23 06:34:09,708 DEBUG TRAIN Batch 15/1500 loss 11.208189 loss_att 11.779800 loss_ctc 12.423872 loss_rnnt 10.841215 hw_loss 0.169802 lr 0.00044436 rank 4
2023-02-23 06:34:09,709 DEBUG TRAIN Batch 15/1500 loss 2.583073 loss_att 6.438819 loss_ctc 4.894983 loss_rnnt 1.386262 hw_loss 0.220139 lr 0.00044425 rank 5
2023-02-23 06:34:09,711 DEBUG TRAIN Batch 15/1500 loss 11.620425 loss_att 15.648924 loss_ctc 14.697395 loss_rnnt 10.304120 hw_loss 0.188141 lr 0.00044436 rank 6
2023-02-23 06:34:09,716 DEBUG TRAIN Batch 15/1500 loss 2.759295 loss_att 9.147736 loss_ctc 5.555309 loss_rnnt 0.954661 hw_loss 0.289020 lr 0.00044434 rank 3
2023-02-23 06:34:09,716 DEBUG TRAIN Batch 15/1500 loss 7.295934 loss_att 10.095295 loss_ctc 8.437373 loss_rnnt 6.450150 hw_loss 0.250726 lr 0.00044439 rank 1
2023-02-23 06:34:09,754 DEBUG TRAIN Batch 15/1500 loss 6.940877 loss_att 8.910629 loss_ctc 8.794322 loss_rnnt 6.177673 hw_loss 0.228989 lr 0.00044426 rank 7
2023-02-23 06:35:26,923 DEBUG TRAIN Batch 15/1600 loss 6.660388 loss_att 8.133427 loss_ctc 9.091797 loss_rnnt 5.925636 hw_loss 0.217416 lr 0.00044418 rank 4
2023-02-23 06:35:26,926 DEBUG TRAIN Batch 15/1600 loss 12.836139 loss_att 13.731261 loss_ctc 16.115061 loss_rnnt 12.083941 hw_loss 0.254968 lr 0.00044421 rank 1
2023-02-23 06:35:26,930 DEBUG TRAIN Batch 15/1600 loss 15.894605 loss_att 17.631002 loss_ctc 17.095200 loss_rnnt 15.259453 hw_loss 0.239611 lr 0.00044414 rank 2
2023-02-23 06:35:26,930 DEBUG TRAIN Batch 15/1600 loss 7.500447 loss_att 11.010898 loss_ctc 11.021592 loss_rnnt 6.228162 hw_loss 0.188828 lr 0.00044407 rank 5
2023-02-23 06:35:26,931 DEBUG TRAIN Batch 15/1600 loss 8.379952 loss_att 10.997615 loss_ctc 16.625069 loss_rnnt 6.686965 hw_loss 0.131449 lr 0.00044417 rank 0
2023-02-23 06:35:26,931 DEBUG TRAIN Batch 15/1600 loss 5.515940 loss_att 8.856981 loss_ctc 8.514736 loss_rnnt 4.321245 hw_loss 0.237464 lr 0.00044416 rank 3
2023-02-23 06:35:26,938 DEBUG TRAIN Batch 15/1600 loss 8.106314 loss_att 9.959558 loss_ctc 9.306164 loss_rnnt 7.416979 hw_loss 0.297573 lr 0.00044418 rank 6
2023-02-23 06:35:26,974 DEBUG TRAIN Batch 15/1600 loss 10.195889 loss_att 12.846745 loss_ctc 12.234488 loss_rnnt 9.270960 hw_loss 0.230521 lr 0.00044408 rank 7
2023-02-23 06:36:43,846 DEBUG TRAIN Batch 15/1700 loss 7.155263 loss_att 8.938890 loss_ctc 7.813157 loss_rnnt 6.582600 hw_loss 0.240410 lr 0.00044399 rank 0
2023-02-23 06:36:43,849 DEBUG TRAIN Batch 15/1700 loss 8.746471 loss_att 10.429448 loss_ctc 12.656496 loss_rnnt 7.777524 hw_loss 0.208153 lr 0.00044390 rank 5
2023-02-23 06:36:43,849 DEBUG TRAIN Batch 15/1700 loss 12.860612 loss_att 14.310583 loss_ctc 19.137569 loss_rnnt 11.615688 hw_loss 0.221252 lr 0.00044397 rank 2
2023-02-23 06:36:43,852 DEBUG TRAIN Batch 15/1700 loss 7.977087 loss_att 12.389064 loss_ctc 10.403233 loss_rnnt 6.682821 hw_loss 0.165718 lr 0.00044399 rank 3
2023-02-23 06:36:43,854 DEBUG TRAIN Batch 15/1700 loss 15.130218 loss_att 15.455704 loss_ctc 21.633352 loss_rnnt 14.057236 hw_loss 0.263999 lr 0.00044401 rank 4
2023-02-23 06:36:43,856 DEBUG TRAIN Batch 15/1700 loss 8.646581 loss_att 13.261781 loss_ctc 8.604777 loss_rnnt 7.605146 hw_loss 0.232442 lr 0.00044401 rank 6
2023-02-23 06:36:43,858 DEBUG TRAIN Batch 15/1700 loss 11.914227 loss_att 12.434793 loss_ctc 15.175033 loss_rnnt 11.239639 hw_loss 0.254438 lr 0.00044391 rank 7
2023-02-23 06:36:43,903 DEBUG TRAIN Batch 15/1700 loss 13.079926 loss_att 14.469261 loss_ctc 18.477859 loss_rnnt 11.958351 hw_loss 0.232466 lr 0.00044404 rank 1
2023-02-23 06:38:02,736 DEBUG TRAIN Batch 15/1800 loss 9.897689 loss_att 12.629736 loss_ctc 15.101588 loss_rnnt 8.499970 hw_loss 0.295229 lr 0.00044383 rank 4
2023-02-23 06:38:02,740 DEBUG TRAIN Batch 15/1800 loss 16.532434 loss_att 19.004330 loss_ctc 26.206263 loss_rnnt 14.556778 hw_loss 0.358940 lr 0.00044372 rank 5
2023-02-23 06:38:02,741 DEBUG TRAIN Batch 15/1800 loss 12.716300 loss_att 13.392226 loss_ctc 19.171827 loss_rnnt 11.608428 hw_loss 0.209906 lr 0.00044386 rank 1
2023-02-23 06:38:02,742 DEBUG TRAIN Batch 15/1800 loss 12.880851 loss_att 15.840664 loss_ctc 18.104063 loss_rnnt 11.423291 hw_loss 0.317190 lr 0.00044379 rank 2
2023-02-23 06:38:02,744 DEBUG TRAIN Batch 15/1800 loss 15.357591 loss_att 20.122097 loss_ctc 19.752344 loss_rnnt 13.695872 hw_loss 0.230345 lr 0.00044383 rank 6
2023-02-23 06:38:02,745 DEBUG TRAIN Batch 15/1800 loss 14.277716 loss_att 19.073965 loss_ctc 18.669437 loss_rnnt 12.598369 hw_loss 0.252251 lr 0.00044382 rank 0
2023-02-23 06:38:02,747 DEBUG TRAIN Batch 15/1800 loss 6.409167 loss_att 9.117665 loss_ctc 6.798524 loss_rnnt 5.697449 hw_loss 0.221446 lr 0.00044373 rank 7
2023-02-23 06:38:02,786 DEBUG TRAIN Batch 15/1800 loss 14.092586 loss_att 14.713114 loss_ctc 19.965290 loss_rnnt 13.075452 hw_loss 0.206252 lr 0.00044381 rank 3
2023-02-23 06:39:19,010 DEBUG TRAIN Batch 15/1900 loss 9.572244 loss_att 14.146773 loss_ctc 9.431366 loss_rnnt 8.616503 hw_loss 0.111784 lr 0.00044365 rank 0
2023-02-23 06:39:19,010 DEBUG TRAIN Batch 15/1900 loss 6.825784 loss_att 10.256554 loss_ctc 11.320041 loss_rnnt 5.430679 hw_loss 0.205718 lr 0.00044355 rank 5
2023-02-23 06:39:19,011 DEBUG TRAIN Batch 15/1900 loss 20.032253 loss_att 22.197952 loss_ctc 26.026968 loss_rnnt 18.645081 hw_loss 0.290132 lr 0.00044362 rank 2
2023-02-23 06:39:19,012 DEBUG TRAIN Batch 15/1900 loss 13.693530 loss_att 18.014601 loss_ctc 19.158352 loss_rnnt 12.028893 hw_loss 0.134587 lr 0.00044364 rank 3
2023-02-23 06:39:19,013 DEBUG TRAIN Batch 15/1900 loss 16.206514 loss_att 23.366863 loss_ctc 22.121761 loss_rnnt 13.853262 hw_loss 0.248407 lr 0.00044366 rank 6
2023-02-23 06:39:19,013 DEBUG TRAIN Batch 15/1900 loss 11.777734 loss_att 11.015630 loss_ctc 12.932817 loss_rnnt 11.547935 hw_loss 0.427893 lr 0.00044366 rank 4
2023-02-23 06:39:19,015 DEBUG TRAIN Batch 15/1900 loss 9.758564 loss_att 10.579190 loss_ctc 11.876439 loss_rnnt 9.150234 hw_loss 0.303414 lr 0.00044356 rank 7
2023-02-23 06:39:19,019 DEBUG TRAIN Batch 15/1900 loss 9.485724 loss_att 10.050831 loss_ctc 9.231303 loss_rnnt 9.279856 hw_loss 0.237693 lr 0.00044369 rank 1
2023-02-23 06:40:33,116 DEBUG TRAIN Batch 15/2000 loss 19.554102 loss_att 19.700289 loss_ctc 26.133297 loss_rnnt 18.546497 hw_loss 0.189636 lr 0.00044337 rank 5
2023-02-23 06:40:33,119 DEBUG TRAIN Batch 15/2000 loss 6.956157 loss_att 10.100026 loss_ctc 9.782880 loss_rnnt 5.860239 hw_loss 0.169216 lr 0.00044348 rank 4
2023-02-23 06:40:33,122 DEBUG TRAIN Batch 15/2000 loss 9.991612 loss_att 12.085501 loss_ctc 17.430191 loss_rnnt 8.456980 hw_loss 0.232582 lr 0.00044344 rank 2
2023-02-23 06:40:33,123 DEBUG TRAIN Batch 15/2000 loss 27.246853 loss_att 27.920506 loss_ctc 34.965809 loss_rnnt 25.956623 hw_loss 0.236823 lr 0.00044351 rank 1
2023-02-23 06:40:33,128 DEBUG TRAIN Batch 15/2000 loss 12.035211 loss_att 14.492390 loss_ctc 15.461799 loss_rnnt 10.985634 hw_loss 0.189868 lr 0.00044347 rank 0
2023-02-23 06:40:33,128 DEBUG TRAIN Batch 15/2000 loss 7.887072 loss_att 11.833091 loss_ctc 12.258560 loss_rnnt 6.354641 hw_loss 0.300678 lr 0.00044346 rank 3
2023-02-23 06:40:33,128 DEBUG TRAIN Batch 15/2000 loss 5.739913 loss_att 7.153941 loss_ctc 5.893526 loss_rnnt 5.314915 hw_loss 0.228207 lr 0.00044348 rank 6
2023-02-23 06:40:33,130 DEBUG TRAIN Batch 15/2000 loss 7.852102 loss_att 10.907906 loss_ctc 12.397974 loss_rnnt 6.539381 hw_loss 0.178958 lr 0.00044339 rank 7
2023-02-23 06:41:51,906 DEBUG TRAIN Batch 15/2100 loss 8.494424 loss_att 10.746408 loss_ctc 10.154670 loss_rnnt 7.719587 hw_loss 0.193262 lr 0.00044331 rank 4
2023-02-23 06:41:51,910 DEBUG TRAIN Batch 15/2100 loss 7.714921 loss_att 10.204667 loss_ctc 7.452690 loss_rnnt 7.145338 hw_loss 0.199872 lr 0.00044327 rank 2
2023-02-23 06:41:51,911 DEBUG TRAIN Batch 15/2100 loss 12.851439 loss_att 14.551390 loss_ctc 13.461154 loss_rnnt 12.304338 hw_loss 0.235901 lr 0.00044331 rank 6
2023-02-23 06:41:51,912 DEBUG TRAIN Batch 15/2100 loss 7.041648 loss_att 10.164183 loss_ctc 11.469478 loss_rnnt 5.672279 hw_loss 0.289657 lr 0.00044329 rank 3
2023-02-23 06:41:51,912 DEBUG TRAIN Batch 15/2100 loss 15.723619 loss_att 19.492382 loss_ctc 23.983543 loss_rnnt 13.769011 hw_loss 0.186623 lr 0.00044320 rank 5
2023-02-23 06:41:51,914 DEBUG TRAIN Batch 15/2100 loss 7.861150 loss_att 9.387426 loss_ctc 6.921255 loss_rnnt 7.568178 hw_loss 0.211943 lr 0.00044334 rank 1
2023-02-23 06:41:51,916 DEBUG TRAIN Batch 15/2100 loss 13.244559 loss_att 16.557190 loss_ctc 17.884453 loss_rnnt 11.813265 hw_loss 0.281464 lr 0.00044330 rank 0
2023-02-23 06:41:51,918 DEBUG TRAIN Batch 15/2100 loss 19.279463 loss_att 23.824207 loss_ctc 30.997227 loss_rnnt 16.629116 hw_loss 0.335678 lr 0.00044321 rank 7
2023-02-23 06:43:09,115 DEBUG TRAIN Batch 15/2200 loss 13.301496 loss_att 17.118452 loss_ctc 17.413935 loss_rnnt 11.849280 hw_loss 0.263435 lr 0.00044302 rank 5
2023-02-23 06:43:09,118 DEBUG TRAIN Batch 15/2200 loss 11.908101 loss_att 13.339266 loss_ctc 16.687950 loss_rnnt 10.889500 hw_loss 0.178231 lr 0.00044312 rank 0
2023-02-23 06:43:09,121 DEBUG TRAIN Batch 15/2200 loss 9.607244 loss_att 14.346813 loss_ctc 11.471788 loss_rnnt 8.263154 hw_loss 0.276692 lr 0.00044311 rank 3
2023-02-23 06:43:09,121 DEBUG TRAIN Batch 15/2200 loss 12.315925 loss_att 18.562347 loss_ctc 16.947357 loss_rnnt 10.339427 hw_loss 0.205666 lr 0.00044310 rank 2
2023-02-23 06:43:09,123 DEBUG TRAIN Batch 15/2200 loss 16.122570 loss_att 18.266977 loss_ctc 19.391983 loss_rnnt 15.127623 hw_loss 0.244024 lr 0.00044314 rank 6
2023-02-23 06:43:09,122 DEBUG TRAIN Batch 15/2200 loss 17.544291 loss_att 20.614611 loss_ctc 23.597477 loss_rnnt 15.980625 hw_loss 0.267204 lr 0.00044314 rank 4
2023-02-23 06:43:09,128 DEBUG TRAIN Batch 15/2200 loss 13.026254 loss_att 16.985737 loss_ctc 16.197512 loss_rnnt 11.696419 hw_loss 0.215819 lr 0.00044316 rank 1
2023-02-23 06:43:09,171 DEBUG TRAIN Batch 15/2200 loss 3.161893 loss_att 6.511237 loss_ctc 3.073001 loss_rnnt 2.371564 hw_loss 0.248087 lr 0.00044304 rank 7
2023-02-23 06:44:23,480 DEBUG TRAIN Batch 15/2300 loss 13.002199 loss_att 14.879231 loss_ctc 18.493282 loss_rnnt 11.721458 hw_loss 0.324730 lr 0.00044285 rank 5
2023-02-23 06:44:23,481 DEBUG TRAIN Batch 15/2300 loss 7.201324 loss_att 11.067472 loss_ctc 9.684040 loss_rnnt 6.001667 hw_loss 0.178872 lr 0.00044286 rank 7
2023-02-23 06:44:23,481 DEBUG TRAIN Batch 15/2300 loss 6.679924 loss_att 10.205170 loss_ctc 8.468649 loss_rnnt 5.627810 hw_loss 0.203566 lr 0.00044295 rank 0
2023-02-23 06:44:23,486 DEBUG TRAIN Batch 15/2300 loss 9.064861 loss_att 11.693338 loss_ctc 11.188623 loss_rnnt 8.126595 hw_loss 0.242629 lr 0.00044292 rank 2
2023-02-23 06:44:23,486 DEBUG TRAIN Batch 15/2300 loss 9.138019 loss_att 13.122702 loss_ctc 14.120173 loss_rnnt 7.519583 hw_loss 0.294770 lr 0.00044299 rank 1
2023-02-23 06:44:23,487 DEBUG TRAIN Batch 15/2300 loss 6.687588 loss_att 10.404614 loss_ctc 9.554465 loss_rnnt 5.419536 hw_loss 0.266993 lr 0.00044296 rank 4
2023-02-23 06:44:23,488 DEBUG TRAIN Batch 15/2300 loss 5.033976 loss_att 6.791056 loss_ctc 6.911937 loss_rnnt 4.239951 hw_loss 0.360402 lr 0.00044294 rank 3
2023-02-23 06:44:23,489 DEBUG TRAIN Batch 15/2300 loss 14.097782 loss_att 17.956682 loss_ctc 20.454411 loss_rnnt 12.326132 hw_loss 0.285603 lr 0.00044296 rank 6
2023-02-23 06:45:39,445 DEBUG TRAIN Batch 15/2400 loss 12.213296 loss_att 14.506069 loss_ctc 15.226673 loss_rnnt 11.231507 hw_loss 0.227721 lr 0.00044275 rank 2
2023-02-23 06:45:39,446 DEBUG TRAIN Batch 15/2400 loss 15.642257 loss_att 18.672171 loss_ctc 18.655746 loss_rnnt 14.527633 hw_loss 0.200332 lr 0.00044268 rank 5
2023-02-23 06:45:39,446 DEBUG TRAIN Batch 15/2400 loss 6.662739 loss_att 8.510788 loss_ctc 9.963056 loss_rnnt 5.713205 hw_loss 0.262280 lr 0.00044277 rank 3
2023-02-23 06:45:39,448 DEBUG TRAIN Batch 15/2400 loss 10.193457 loss_att 12.419208 loss_ctc 12.184882 loss_rnnt 9.366405 hw_loss 0.218210 lr 0.00044269 rank 7
2023-02-23 06:45:39,449 DEBUG TRAIN Batch 15/2400 loss 6.624746 loss_att 10.118882 loss_ctc 8.371225 loss_rnnt 5.510858 hw_loss 0.341620 lr 0.00044279 rank 4
2023-02-23 06:45:39,449 DEBUG TRAIN Batch 15/2400 loss 11.899877 loss_att 14.391045 loss_ctc 17.892685 loss_rnnt 10.476553 hw_loss 0.236342 lr 0.00044277 rank 0
2023-02-23 06:45:39,451 DEBUG TRAIN Batch 15/2400 loss 13.529335 loss_att 14.181053 loss_ctc 18.361958 loss_rnnt 12.596079 hw_loss 0.297304 lr 0.00044281 rank 1
2023-02-23 06:45:39,454 DEBUG TRAIN Batch 15/2400 loss 16.183594 loss_att 16.260342 loss_ctc 19.952234 loss_rnnt 15.563441 hw_loss 0.191846 lr 0.00044279 rank 6
2023-02-23 06:46:58,335 DEBUG TRAIN Batch 15/2500 loss 10.164452 loss_att 13.312238 loss_ctc 13.094958 loss_rnnt 9.070313 hw_loss 0.138462 lr 0.00044261 rank 6
2023-02-23 06:46:58,337 DEBUG TRAIN Batch 15/2500 loss 16.872597 loss_att 17.012409 loss_ctc 21.163916 loss_rnnt 16.092428 hw_loss 0.337558 lr 0.00044250 rank 5
2023-02-23 06:46:58,338 DEBUG TRAIN Batch 15/2500 loss 11.442717 loss_att 12.624159 loss_ctc 16.616619 loss_rnnt 10.363650 hw_loss 0.286731 lr 0.00044257 rank 2
2023-02-23 06:46:58,338 DEBUG TRAIN Batch 15/2500 loss 12.659307 loss_att 12.660807 loss_ctc 15.977165 loss_rnnt 12.016266 hw_loss 0.375674 lr 0.00044259 rank 3
2023-02-23 06:46:58,341 DEBUG TRAIN Batch 15/2500 loss 19.345646 loss_att 18.542137 loss_ctc 23.840866 loss_rnnt 18.722260 hw_loss 0.346361 lr 0.00044260 rank 0
2023-02-23 06:46:58,341 DEBUG TRAIN Batch 15/2500 loss 9.075176 loss_att 10.531318 loss_ctc 15.228279 loss_rnnt 7.780902 hw_loss 0.342436 lr 0.00044261 rank 4
2023-02-23 06:46:58,342 DEBUG TRAIN Batch 15/2500 loss 15.292441 loss_att 14.954852 loss_ctc 19.703669 loss_rnnt 14.619734 hw_loss 0.285117 lr 0.00044264 rank 1
2023-02-23 06:46:58,350 DEBUG TRAIN Batch 15/2500 loss 7.872316 loss_att 8.758108 loss_ctc 7.568035 loss_rnnt 7.567960 hw_loss 0.314565 lr 0.00044252 rank 7
2023-02-23 06:48:12,604 DEBUG TRAIN Batch 15/2600 loss 13.337125 loss_att 16.245831 loss_ctc 15.138336 loss_rnnt 12.339554 hw_loss 0.329377 lr 0.00044233 rank 5
2023-02-23 06:48:12,604 DEBUG TRAIN Batch 15/2600 loss 3.217743 loss_att 5.082914 loss_ctc 3.620846 loss_rnnt 2.673139 hw_loss 0.220918 lr 0.00044243 rank 0
2023-02-23 06:48:12,608 DEBUG TRAIN Batch 15/2600 loss 14.079624 loss_att 16.002594 loss_ctc 17.937567 loss_rnnt 13.052763 hw_loss 0.239769 lr 0.00044242 rank 3
2023-02-23 06:48:12,610 DEBUG TRAIN Batch 15/2600 loss 8.416962 loss_att 12.560341 loss_ctc 9.112701 loss_rnnt 7.390462 hw_loss 0.196984 lr 0.00044247 rank 1
2023-02-23 06:48:12,610 DEBUG TRAIN Batch 15/2600 loss 15.229954 loss_att 18.576765 loss_ctc 23.870777 loss_rnnt 13.253899 hw_loss 0.289846 lr 0.00044234 rank 7
2023-02-23 06:48:12,610 DEBUG TRAIN Batch 15/2600 loss 10.596905 loss_att 14.991426 loss_ctc 15.043475 loss_rnnt 8.992594 hw_loss 0.248493 lr 0.00044244 rank 4
2023-02-23 06:48:12,612 DEBUG TRAIN Batch 15/2600 loss 8.993119 loss_att 11.136187 loss_ctc 11.092054 loss_rnnt 8.148462 hw_loss 0.255346 lr 0.00044240 rank 2
2023-02-23 06:48:12,658 DEBUG TRAIN Batch 15/2600 loss 10.292064 loss_att 14.388999 loss_ctc 11.514754 loss_rnnt 9.146460 hw_loss 0.305985 lr 0.00044244 rank 6
2023-02-23 06:49:28,678 DEBUG TRAIN Batch 15/2700 loss 19.201681 loss_att 26.512648 loss_ctc 26.612167 loss_rnnt 16.625456 hw_loss 0.236189 lr 0.00044216 rank 5
2023-02-23 06:49:28,680 DEBUG TRAIN Batch 15/2700 loss 4.242596 loss_att 6.264616 loss_ctc 4.606589 loss_rnnt 3.660043 hw_loss 0.243031 lr 0.00044225 rank 3
2023-02-23 06:49:28,684 DEBUG TRAIN Batch 15/2700 loss 10.025193 loss_att 14.611526 loss_ctc 13.595653 loss_rnnt 8.429064 hw_loss 0.380253 lr 0.00044227 rank 4
2023-02-23 06:49:28,686 DEBUG TRAIN Batch 15/2700 loss 7.896254 loss_att 13.431227 loss_ctc 9.675756 loss_rnnt 6.466108 hw_loss 0.161034 lr 0.00044223 rank 2
2023-02-23 06:49:28,687 DEBUG TRAIN Batch 15/2700 loss 12.805233 loss_att 16.901451 loss_ctc 19.841850 loss_rnnt 10.967918 hw_loss 0.149726 lr 0.00044225 rank 0
2023-02-23 06:49:28,688 DEBUG TRAIN Batch 15/2700 loss 3.671746 loss_att 6.848132 loss_ctc 4.344233 loss_rnnt 2.850293 hw_loss 0.180957 lr 0.00044229 rank 1
2023-02-23 06:49:28,689 DEBUG TRAIN Batch 15/2700 loss 14.544530 loss_att 19.702259 loss_ctc 22.142670 loss_rnnt 12.409622 hw_loss 0.169271 lr 0.00044227 rank 6
2023-02-23 06:49:28,690 DEBUG TRAIN Batch 15/2700 loss 7.092017 loss_att 11.323669 loss_ctc 8.655208 loss_rnnt 5.894365 hw_loss 0.267930 lr 0.00044217 rank 7
2023-02-23 06:50:47,187 DEBUG TRAIN Batch 15/2800 loss 24.726629 loss_att 29.542652 loss_ctc 44.141228 loss_rnnt 21.049404 hw_loss 0.235138 lr 0.00044206 rank 2
2023-02-23 06:50:47,188 DEBUG TRAIN Batch 15/2800 loss 8.692086 loss_att 14.299290 loss_ctc 10.566951 loss_rnnt 7.169616 hw_loss 0.283215 lr 0.00044208 rank 0
2023-02-23 06:50:47,190 DEBUG TRAIN Batch 15/2800 loss 12.554495 loss_att 16.234789 loss_ctc 15.948854 loss_rnnt 11.232581 hw_loss 0.249886 lr 0.00044210 rank 6
2023-02-23 06:50:47,190 DEBUG TRAIN Batch 15/2800 loss 16.986860 loss_att 22.752918 loss_ctc 24.920364 loss_rnnt 14.654742 hw_loss 0.227076 lr 0.00044210 rank 4
2023-02-23 06:50:47,192 DEBUG TRAIN Batch 15/2800 loss 6.742336 loss_att 10.112746 loss_ctc 6.515245 loss_rnnt 5.989301 hw_loss 0.204810 lr 0.00044198 rank 5
2023-02-23 06:50:47,193 DEBUG TRAIN Batch 15/2800 loss 3.926502 loss_att 8.216763 loss_ctc 7.913425 loss_rnnt 2.403284 hw_loss 0.250456 lr 0.00044212 rank 1
2023-02-23 06:50:47,195 DEBUG TRAIN Batch 15/2800 loss 17.081169 loss_att 20.395576 loss_ctc 18.021353 loss_rnnt 16.185946 hw_loss 0.200594 lr 0.00044207 rank 3
2023-02-23 06:50:47,238 DEBUG TRAIN Batch 15/2800 loss 7.160105 loss_att 9.402047 loss_ctc 10.884929 loss_rnnt 6.119974 hw_loss 0.178310 lr 0.00044200 rank 7
2023-02-23 06:52:03,803 DEBUG TRAIN Batch 15/2900 loss 15.289831 loss_att 18.211206 loss_ctc 20.379429 loss_rnnt 13.881365 hw_loss 0.272957 lr 0.00044192 rank 4
2023-02-23 06:52:03,804 DEBUG TRAIN Batch 15/2900 loss 14.059228 loss_att 16.193075 loss_ctc 20.150848 loss_rnnt 12.671136 hw_loss 0.279571 lr 0.00044190 rank 3
2023-02-23 06:52:03,805 DEBUG TRAIN Batch 15/2900 loss 13.000555 loss_att 14.315641 loss_ctc 16.854450 loss_rnnt 12.119305 hw_loss 0.195713 lr 0.00044182 rank 7
2023-02-23 06:52:03,807 DEBUG TRAIN Batch 15/2900 loss 15.893301 loss_att 17.960695 loss_ctc 19.078012 loss_rnnt 14.951994 hw_loss 0.193499 lr 0.00044192 rank 6
2023-02-23 06:52:03,807 DEBUG TRAIN Batch 15/2900 loss 9.729258 loss_att 13.330313 loss_ctc 12.499903 loss_rnnt 8.532012 hw_loss 0.201780 lr 0.00044188 rank 2
2023-02-23 06:52:03,808 DEBUG TRAIN Batch 15/2900 loss 7.608130 loss_att 12.898695 loss_ctc 11.755280 loss_rnnt 5.904243 hw_loss 0.174038 lr 0.00044195 rank 1
2023-02-23 06:52:03,811 DEBUG TRAIN Batch 15/2900 loss 9.011554 loss_att 10.638788 loss_ctc 12.842022 loss_rnnt 8.087914 hw_loss 0.163997 lr 0.00044191 rank 0
2023-02-23 06:52:03,815 DEBUG TRAIN Batch 15/2900 loss 12.808720 loss_att 15.623654 loss_ctc 17.059603 loss_rnnt 11.597452 hw_loss 0.152803 lr 0.00044181 rank 5
2023-02-23 06:53:19,768 DEBUG TRAIN Batch 15/3000 loss 11.834213 loss_att 15.498753 loss_ctc 18.912426 loss_rnnt 10.050879 hw_loss 0.199998 lr 0.00044164 rank 5
2023-02-23 06:53:19,769 DEBUG TRAIN Batch 15/3000 loss 5.374672 loss_att 8.680849 loss_ctc 9.304960 loss_rnnt 4.067424 hw_loss 0.228703 lr 0.00044171 rank 2
2023-02-23 06:53:19,772 DEBUG TRAIN Batch 15/3000 loss 14.178226 loss_att 16.455946 loss_ctc 18.641228 loss_rnnt 12.977737 hw_loss 0.281021 lr 0.00044174 rank 0
2023-02-23 06:53:19,775 DEBUG TRAIN Batch 15/3000 loss 15.586576 loss_att 18.009628 loss_ctc 20.074661 loss_rnnt 14.383432 hw_loss 0.225227 lr 0.00044175 rank 4
2023-02-23 06:53:19,777 DEBUG TRAIN Batch 15/3000 loss 11.367707 loss_att 17.880432 loss_ctc 18.195951 loss_rnnt 9.008715 hw_loss 0.273779 lr 0.00044175 rank 6
2023-02-23 06:53:19,777 DEBUG TRAIN Batch 15/3000 loss 16.275164 loss_att 18.110231 loss_ctc 23.915894 loss_rnnt 14.782057 hw_loss 0.201246 lr 0.00044173 rank 3
2023-02-23 06:53:19,778 DEBUG TRAIN Batch 15/3000 loss 18.275520 loss_att 19.156803 loss_ctc 18.992208 loss_rnnt 17.888809 hw_loss 0.215430 lr 0.00044165 rank 7
2023-02-23 06:53:19,777 DEBUG TRAIN Batch 15/3000 loss 9.240334 loss_att 10.679649 loss_ctc 11.666657 loss_rnnt 8.517645 hw_loss 0.208716 lr 0.00044178 rank 1
2023-02-23 06:54:35,661 DEBUG TRAIN Batch 15/3100 loss 13.658329 loss_att 15.008152 loss_ctc 15.715663 loss_rnnt 12.952633 hw_loss 0.302664 lr 0.00044147 rank 5
2023-02-23 06:54:35,664 DEBUG TRAIN Batch 15/3100 loss 18.191116 loss_att 21.343964 loss_ctc 25.537298 loss_rnnt 16.436909 hw_loss 0.270278 lr 0.00044154 rank 2
2023-02-23 06:54:35,663 DEBUG TRAIN Batch 15/3100 loss 4.762519 loss_att 6.096681 loss_ctc 6.731451 loss_rnnt 4.016171 hw_loss 0.406858 lr 0.00044158 rank 6
2023-02-23 06:54:35,664 DEBUG TRAIN Batch 15/3100 loss 11.757798 loss_att 14.082430 loss_ctc 15.222144 loss_rnnt 10.706052 hw_loss 0.234199 lr 0.00044156 rank 0
2023-02-23 06:54:35,666 DEBUG TRAIN Batch 15/3100 loss 11.288239 loss_att 12.714062 loss_ctc 13.504773 loss_rnnt 10.535819 hw_loss 0.321973 lr 0.00044148 rank 7
2023-02-23 06:54:35,665 DEBUG TRAIN Batch 15/3100 loss 16.075762 loss_att 18.292433 loss_ctc 20.276033 loss_rnnt 14.914841 hw_loss 0.295408 lr 0.00044158 rank 4
2023-02-23 06:54:35,667 DEBUG TRAIN Batch 15/3100 loss 8.986005 loss_att 11.391556 loss_ctc 10.513521 loss_rnnt 8.145541 hw_loss 0.291911 lr 0.00044160 rank 1
2023-02-23 06:54:35,712 DEBUG TRAIN Batch 15/3100 loss 24.634062 loss_att 28.153708 loss_ctc 28.902954 loss_rnnt 23.235336 hw_loss 0.235518 lr 0.00044156 rank 3
2023-02-23 06:55:54,831 DEBUG TRAIN Batch 15/3200 loss 7.479860 loss_att 7.119745 loss_ctc 9.872434 loss_rnnt 7.039669 hw_loss 0.362260 lr 0.00044137 rank 2
2023-02-23 06:55:54,833 DEBUG TRAIN Batch 15/3200 loss 8.788203 loss_att 11.910494 loss_ctc 13.675192 loss_rnnt 7.366325 hw_loss 0.273413 lr 0.00044138 rank 3
2023-02-23 06:55:54,837 DEBUG TRAIN Batch 15/3200 loss 11.967125 loss_att 11.502520 loss_ctc 14.476077 loss_rnnt 11.562385 hw_loss 0.305877 lr 0.00044141 rank 4
2023-02-23 06:55:54,842 DEBUG TRAIN Batch 15/3200 loss 11.964026 loss_att 14.050159 loss_ctc 14.200226 loss_rnnt 11.097074 hw_loss 0.284185 lr 0.00044143 rank 1
2023-02-23 06:55:54,847 DEBUG TRAIN Batch 15/3200 loss 11.233610 loss_att 15.495338 loss_ctc 13.220942 loss_rnnt 10.060656 hw_loss 0.104308 lr 0.00044141 rank 6
2023-02-23 06:55:54,846 DEBUG TRAIN Batch 15/3200 loss 17.847229 loss_att 23.058756 loss_ctc 25.312603 loss_rnnt 15.718524 hw_loss 0.170657 lr 0.00044139 rank 0
2023-02-23 06:55:54,847 DEBUG TRAIN Batch 15/3200 loss 4.231699 loss_att 7.677716 loss_ctc 5.636424 loss_rnnt 3.247850 hw_loss 0.201280 lr 0.00044131 rank 7
2023-02-23 06:55:54,849 DEBUG TRAIN Batch 15/3200 loss 10.707649 loss_att 18.553625 loss_ctc 16.560278 loss_rnnt 8.232315 hw_loss 0.235853 lr 0.00044130 rank 5
2023-02-23 06:57:12,520 DEBUG TRAIN Batch 15/3300 loss 10.353630 loss_att 13.548846 loss_ctc 12.012678 loss_rnnt 9.417600 hw_loss 0.142088 lr 0.00044122 rank 0
2023-02-23 06:57:12,522 DEBUG TRAIN Batch 15/3300 loss 7.477893 loss_att 8.986167 loss_ctc 10.869369 loss_rnnt 6.613370 hw_loss 0.207509 lr 0.00044119 rank 2
2023-02-23 06:57:12,524 DEBUG TRAIN Batch 15/3300 loss 9.028160 loss_att 12.321733 loss_ctc 12.247162 loss_rnnt 7.817323 hw_loss 0.230478 lr 0.00044126 rank 1
2023-02-23 06:57:12,527 DEBUG TRAIN Batch 15/3300 loss 3.079369 loss_att 6.177388 loss_ctc 4.729511 loss_rnnt 2.096888 hw_loss 0.267858 lr 0.00044112 rank 5
2023-02-23 06:57:12,527 DEBUG TRAIN Batch 15/3300 loss 2.761297 loss_att 4.795123 loss_ctc 4.049814 loss_rnnt 2.070872 hw_loss 0.209732 lr 0.00044123 rank 6
2023-02-23 06:57:12,530 DEBUG TRAIN Batch 15/3300 loss 8.403411 loss_att 11.076697 loss_ctc 10.663862 loss_rnnt 7.422841 hw_loss 0.270972 lr 0.00044123 rank 4
2023-02-23 06:57:12,531 DEBUG TRAIN Batch 15/3300 loss 12.649738 loss_att 15.310160 loss_ctc 20.450720 loss_rnnt 11.009746 hw_loss 0.127082 lr 0.00044121 rank 3
2023-02-23 06:57:12,579 DEBUG TRAIN Batch 15/3300 loss 15.681344 loss_att 18.482965 loss_ctc 16.858530 loss_rnnt 14.841530 hw_loss 0.229748 lr 0.00044114 rank 7
2023-02-23 06:58:27,605 DEBUG TRAIN Batch 15/3400 loss 7.490444 loss_att 10.439556 loss_ctc 11.213476 loss_rnnt 6.291269 hw_loss 0.211777 lr 0.00044102 rank 2
2023-02-23 06:58:27,606 DEBUG TRAIN Batch 15/3400 loss 10.937730 loss_att 14.173707 loss_ctc 14.452032 loss_rnnt 9.709320 hw_loss 0.211203 lr 0.00044095 rank 5
2023-02-23 06:58:27,606 DEBUG TRAIN Batch 15/3400 loss 19.305037 loss_att 19.174284 loss_ctc 21.817047 loss_rnnt 18.876797 hw_loss 0.223978 lr 0.00044104 rank 3
2023-02-23 06:58:27,609 DEBUG TRAIN Batch 15/3400 loss 17.314314 loss_att 23.124119 loss_ctc 25.389736 loss_rnnt 14.929955 hw_loss 0.273144 lr 0.00044105 rank 0
2023-02-23 06:58:27,610 DEBUG TRAIN Batch 15/3400 loss 9.304162 loss_att 10.904633 loss_ctc 10.671238 loss_rnnt 8.675659 hw_loss 0.236497 lr 0.00044106 rank 4
2023-02-23 06:58:27,612 DEBUG TRAIN Batch 15/3400 loss 12.067364 loss_att 13.814105 loss_ctc 12.635898 loss_rnnt 11.512149 hw_loss 0.243867 lr 0.00044109 rank 1
2023-02-23 06:58:27,614 DEBUG TRAIN Batch 15/3400 loss 7.384238 loss_att 11.179008 loss_ctc 10.060287 loss_rnnt 6.184395 hw_loss 0.157656 lr 0.00044106 rank 6
2023-02-23 06:58:27,614 DEBUG TRAIN Batch 15/3400 loss 7.495046 loss_att 10.006498 loss_ctc 8.249735 loss_rnnt 6.792603 hw_loss 0.186615 lr 0.00044096 rank 7
2023-02-23 06:59:45,688 DEBUG TRAIN Batch 15/3500 loss 10.970616 loss_att 13.757401 loss_ctc 15.549477 loss_rnnt 9.692605 hw_loss 0.206511 lr 0.00044088 rank 0
2023-02-23 06:59:45,691 DEBUG TRAIN Batch 15/3500 loss 7.064651 loss_att 12.962503 loss_ctc 12.242628 loss_rnnt 5.088749 hw_loss 0.198629 lr 0.00044089 rank 4
2023-02-23 06:59:45,694 DEBUG TRAIN Batch 15/3500 loss 7.842123 loss_att 11.377341 loss_ctc 13.681541 loss_rnnt 6.194435 hw_loss 0.303854 lr 0.00044089 rank 6
2023-02-23 06:59:45,694 DEBUG TRAIN Batch 15/3500 loss 14.899459 loss_att 15.240593 loss_ctc 17.322258 loss_rnnt 14.391396 hw_loss 0.218994 lr 0.00044078 rank 5
2023-02-23 06:59:45,694 DEBUG TRAIN Batch 15/3500 loss 8.947680 loss_att 11.102180 loss_ctc 14.840504 loss_rnnt 7.620224 hw_loss 0.207835 lr 0.00044087 rank 3
2023-02-23 06:59:45,698 DEBUG TRAIN Batch 15/3500 loss 7.585035 loss_att 10.756371 loss_ctc 9.575546 loss_rnnt 6.529262 hw_loss 0.292696 lr 0.00044085 rank 2
2023-02-23 06:59:45,699 DEBUG TRAIN Batch 15/3500 loss 12.231574 loss_att 14.791355 loss_ctc 12.371801 loss_rnnt 11.587812 hw_loss 0.212081 lr 0.00044079 rank 7
2023-02-23 06:59:45,703 DEBUG TRAIN Batch 15/3500 loss 11.039573 loss_att 17.869848 loss_ctc 15.209929 loss_rnnt 9.051891 hw_loss 0.122959 lr 0.00044092 rank 1
2023-02-23 07:01:02,691 DEBUG TRAIN Batch 15/3600 loss 15.103943 loss_att 15.130638 loss_ctc 16.056614 loss_rnnt 14.855762 hw_loss 0.217159 lr 0.00044061 rank 5
2023-02-23 07:01:02,694 DEBUG TRAIN Batch 15/3600 loss 11.352869 loss_att 12.546107 loss_ctc 14.384761 loss_rnnt 10.592773 hw_loss 0.219741 lr 0.00044062 rank 7
2023-02-23 07:01:02,694 DEBUG TRAIN Batch 15/3600 loss 13.017203 loss_att 12.649624 loss_ctc 15.040068 loss_rnnt 12.713669 hw_loss 0.201253 lr 0.00044072 rank 4
2023-02-23 07:01:02,694 DEBUG TRAIN Batch 15/3600 loss 14.811704 loss_att 16.604126 loss_ctc 21.681713 loss_rnnt 13.438621 hw_loss 0.184870 lr 0.00044071 rank 0
2023-02-23 07:01:02,696 DEBUG TRAIN Batch 15/3600 loss 12.342113 loss_att 17.089447 loss_ctc 16.909151 loss_rnnt 10.676142 hw_loss 0.201682 lr 0.00044070 rank 3
2023-02-23 07:01:02,699 DEBUG TRAIN Batch 15/3600 loss 19.729681 loss_att 19.835506 loss_ctc 26.518404 loss_rnnt 18.645424 hw_loss 0.296117 lr 0.00044072 rank 6
2023-02-23 07:01:02,699 DEBUG TRAIN Batch 15/3600 loss 9.620940 loss_att 13.740039 loss_ctc 15.935024 loss_rnnt 7.856079 hw_loss 0.185930 lr 0.00044075 rank 1
2023-02-23 07:01:02,699 DEBUG TRAIN Batch 15/3600 loss 16.164288 loss_att 22.564419 loss_ctc 21.005081 loss_rnnt 14.024681 hw_loss 0.401514 lr 0.00044068 rank 2
2023-02-23 07:02:18,274 DEBUG TRAIN Batch 15/3700 loss 12.782273 loss_att 16.715374 loss_ctc 16.684704 loss_rnnt 11.358536 hw_loss 0.218987 lr 0.00044053 rank 0
2023-02-23 07:02:18,275 DEBUG TRAIN Batch 15/3700 loss 15.042132 loss_att 14.939760 loss_ctc 16.539663 loss_rnnt 14.697748 hw_loss 0.309727 lr 0.00044044 rank 5
2023-02-23 07:02:18,275 DEBUG TRAIN Batch 15/3700 loss 8.424383 loss_att 11.620677 loss_ctc 9.885925 loss_rnnt 7.450297 hw_loss 0.262416 lr 0.00044051 rank 2
2023-02-23 07:02:18,279 DEBUG TRAIN Batch 15/3700 loss 12.033361 loss_att 15.471596 loss_ctc 16.086632 loss_rnnt 10.640879 hw_loss 0.308250 lr 0.00044055 rank 4
2023-02-23 07:02:18,280 DEBUG TRAIN Batch 15/3700 loss 6.162454 loss_att 9.542459 loss_ctc 11.678253 loss_rnnt 4.602346 hw_loss 0.278751 lr 0.00044055 rank 6
2023-02-23 07:02:18,280 DEBUG TRAIN Batch 15/3700 loss 13.894853 loss_att 17.387545 loss_ctc 21.677280 loss_rnnt 12.058817 hw_loss 0.187199 lr 0.00044057 rank 1
2023-02-23 07:02:18,282 DEBUG TRAIN Batch 15/3700 loss 13.071711 loss_att 18.311180 loss_ctc 21.438765 loss_rnnt 10.806925 hw_loss 0.189906 lr 0.00044053 rank 3
2023-02-23 07:02:18,283 DEBUG TRAIN Batch 15/3700 loss 15.061317 loss_att 18.304619 loss_ctc 18.374325 loss_rnnt 13.809062 hw_loss 0.303489 lr 0.00044045 rank 7
2023-02-23 07:03:35,562 DEBUG TRAIN Batch 15/3800 loss 9.650876 loss_att 9.946628 loss_ctc 12.132132 loss_rnnt 9.041284 hw_loss 0.411767 lr 0.00044027 rank 5
2023-02-23 07:03:35,561 DEBUG TRAIN Batch 15/3800 loss 10.098912 loss_att 12.534735 loss_ctc 14.693879 loss_rnnt 8.840931 hw_loss 0.296541 lr 0.00044038 rank 4
2023-02-23 07:03:35,566 DEBUG TRAIN Batch 15/3800 loss 7.976103 loss_att 9.705518 loss_ctc 11.682612 loss_rnnt 6.972506 hw_loss 0.306586 lr 0.00044028 rank 7
2023-02-23 07:03:35,566 DEBUG TRAIN Batch 15/3800 loss 10.402354 loss_att 12.973466 loss_ctc 13.780292 loss_rnnt 9.306214 hw_loss 0.246609 lr 0.00044036 rank 0
2023-02-23 07:03:35,567 DEBUG TRAIN Batch 15/3800 loss 9.559113 loss_att 11.236752 loss_ctc 13.053041 loss_rnnt 8.627924 hw_loss 0.243382 lr 0.00044034 rank 2
2023-02-23 07:03:35,569 DEBUG TRAIN Batch 15/3800 loss 8.798596 loss_att 11.886926 loss_ctc 11.744749 loss_rnnt 7.716314 hw_loss 0.134616 lr 0.00044038 rank 6
2023-02-23 07:03:35,574 DEBUG TRAIN Batch 15/3800 loss 17.545940 loss_att 18.330320 loss_ctc 21.531019 loss_rnnt 16.684525 hw_loss 0.324738 lr 0.00044040 rank 1
2023-02-23 07:03:35,614 DEBUG TRAIN Batch 15/3800 loss 12.004807 loss_att 13.276831 loss_ctc 14.854082 loss_rnnt 11.210686 hw_loss 0.299651 lr 0.00044036 rank 3
2023-02-23 07:04:54,794 DEBUG TRAIN Batch 15/3900 loss 7.506057 loss_att 12.633234 loss_ctc 12.378417 loss_rnnt 5.787682 hw_loss 0.081172 lr 0.00044010 rank 5
2023-02-23 07:04:54,794 DEBUG TRAIN Batch 15/3900 loss 8.969584 loss_att 8.767220 loss_ctc 6.983006 loss_rnnt 9.130366 hw_loss 0.271065 lr 0.00044021 rank 4
2023-02-23 07:04:54,798 DEBUG TRAIN Batch 15/3900 loss 20.665384 loss_att 26.678892 loss_ctc 32.726639 loss_rnnt 17.701242 hw_loss 0.287387 lr 0.00044018 rank 3
2023-02-23 07:04:54,800 DEBUG TRAIN Batch 15/3900 loss 10.292398 loss_att 13.451124 loss_ctc 10.560574 loss_rnnt 9.509232 hw_loss 0.216873 lr 0.00044019 rank 0
2023-02-23 07:04:54,800 DEBUG TRAIN Batch 15/3900 loss 10.294186 loss_att 9.282190 loss_ctc 12.012462 loss_rnnt 10.018682 hw_loss 0.466499 lr 0.00044017 rank 2
2023-02-23 07:04:54,805 DEBUG TRAIN Batch 15/3900 loss 9.528156 loss_att 12.189079 loss_ctc 14.451045 loss_rnnt 8.255857 hw_loss 0.156991 lr 0.00044011 rank 7
2023-02-23 07:04:54,809 DEBUG TRAIN Batch 15/3900 loss 7.972500 loss_att 12.880795 loss_ctc 11.809085 loss_rnnt 6.355862 hw_loss 0.231439 lr 0.00044021 rank 6
2023-02-23 07:04:54,818 DEBUG TRAIN Batch 15/3900 loss 12.411852 loss_att 15.845209 loss_ctc 16.490780 loss_rnnt 11.103332 hw_loss 0.146234 lr 0.00044023 rank 1
2023-02-23 07:06:10,694 DEBUG TRAIN Batch 15/4000 loss 21.194323 loss_att 20.638767 loss_ctc 28.773468 loss_rnnt 20.182480 hw_loss 0.210752 lr 0.00044006 rank 1
2023-02-23 07:06:10,696 DEBUG TRAIN Batch 15/4000 loss 10.531222 loss_att 13.328724 loss_ctc 13.517836 loss_rnnt 9.460182 hw_loss 0.212484 lr 0.00044000 rank 2
2023-02-23 07:06:10,696 DEBUG TRAIN Batch 15/4000 loss 4.067070 loss_att 5.644482 loss_ctc 7.866664 loss_rnnt 3.145628 hw_loss 0.186277 lr 0.00044001 rank 3
2023-02-23 07:06:10,697 DEBUG TRAIN Batch 15/4000 loss 3.439786 loss_att 8.279713 loss_ctc 7.632598 loss_rnnt 1.809181 hw_loss 0.194209 lr 0.00043993 rank 5
2023-02-23 07:06:10,698 DEBUG TRAIN Batch 15/4000 loss 14.699019 loss_att 19.826063 loss_ctc 15.142837 loss_rnnt 13.495784 hw_loss 0.222470 lr 0.00044004 rank 4
2023-02-23 07:06:10,701 DEBUG TRAIN Batch 15/4000 loss 11.417286 loss_att 13.032314 loss_ctc 18.808830 loss_rnnt 9.975573 hw_loss 0.249691 lr 0.00043994 rank 7
2023-02-23 07:06:10,702 DEBUG TRAIN Batch 15/4000 loss 9.285945 loss_att 11.683244 loss_ctc 14.215281 loss_rnnt 8.054438 hw_loss 0.177756 lr 0.00044002 rank 0
2023-02-23 07:06:10,704 DEBUG TRAIN Batch 15/4000 loss 21.677776 loss_att 25.830616 loss_ctc 25.399063 loss_rnnt 20.190750 hw_loss 0.300540 lr 0.00044004 rank 6
2023-02-23 07:07:26,545 DEBUG TRAIN Batch 15/4100 loss 15.083619 loss_att 19.435966 loss_ctc 20.690670 loss_rnnt 13.364041 hw_loss 0.190315 lr 0.00043987 rank 4
2023-02-23 07:07:26,548 DEBUG TRAIN Batch 15/4100 loss 10.525471 loss_att 15.389835 loss_ctc 14.258377 loss_rnnt 8.921861 hw_loss 0.249405 lr 0.00043987 rank 6
2023-02-23 07:07:26,551 DEBUG TRAIN Batch 15/4100 loss 5.904086 loss_att 10.057094 loss_ctc 7.127614 loss_rnnt 4.779730 hw_loss 0.244909 lr 0.00043976 rank 5
2023-02-23 07:07:26,554 DEBUG TRAIN Batch 15/4100 loss 17.455816 loss_att 20.208580 loss_ctc 19.818226 loss_rnnt 16.481667 hw_loss 0.203645 lr 0.00043985 rank 0
2023-02-23 07:07:26,555 DEBUG TRAIN Batch 15/4100 loss 7.068254 loss_att 10.264149 loss_ctc 9.939502 loss_rnnt 5.973318 hw_loss 0.136732 lr 0.00043983 rank 2
2023-02-23 07:07:26,557 DEBUG TRAIN Batch 15/4100 loss 6.934268 loss_att 11.926535 loss_ctc 9.886463 loss_rnnt 5.420480 hw_loss 0.228202 lr 0.00043989 rank 1
2023-02-23 07:07:26,563 DEBUG TRAIN Batch 15/4100 loss 11.240755 loss_att 18.863354 loss_ctc 17.136419 loss_rnnt 8.813720 hw_loss 0.218301 lr 0.00043984 rank 3
2023-02-23 07:07:26,608 DEBUG TRAIN Batch 15/4100 loss 16.519461 loss_att 17.244293 loss_ctc 22.773825 loss_rnnt 15.458204 hw_loss 0.154453 lr 0.00043977 rank 7
2023-02-23 07:08:44,097 DEBUG TRAIN Batch 15/4200 loss 15.092023 loss_att 17.206175 loss_ctc 16.708500 loss_rnnt 14.344124 hw_loss 0.205385 lr 0.00043968 rank 0
2023-02-23 07:08:44,098 DEBUG TRAIN Batch 15/4200 loss 10.118547 loss_att 14.945562 loss_ctc 13.829366 loss_rnnt 8.529509 hw_loss 0.241614 lr 0.00043970 rank 4
2023-02-23 07:08:44,100 DEBUG TRAIN Batch 15/4200 loss 7.115827 loss_att 11.823706 loss_ctc 11.656490 loss_rnnt 5.456572 hw_loss 0.210483 lr 0.00043966 rank 2
2023-02-23 07:08:44,102 DEBUG TRAIN Batch 15/4200 loss 13.523873 loss_att 14.943512 loss_ctc 17.386284 loss_rnnt 12.647779 hw_loss 0.144710 lr 0.00043972 rank 1
2023-02-23 07:08:44,104 DEBUG TRAIN Batch 15/4200 loss 21.342327 loss_att 24.088308 loss_ctc 32.091923 loss_rnnt 19.248451 hw_loss 0.208874 lr 0.00043959 rank 5
2023-02-23 07:08:44,105 DEBUG TRAIN Batch 15/4200 loss 8.222527 loss_att 13.446312 loss_ctc 8.609140 loss_rnnt 7.008277 hw_loss 0.221144 lr 0.00043960 rank 7
2023-02-23 07:08:44,109 DEBUG TRAIN Batch 15/4200 loss 12.124275 loss_att 12.779095 loss_ctc 13.084105 loss_rnnt 11.742252 hw_loss 0.230778 lr 0.00043970 rank 6
2023-02-23 07:08:44,150 DEBUG TRAIN Batch 15/4200 loss 13.253778 loss_att 19.698896 loss_ctc 16.977682 loss_rnnt 11.377777 hw_loss 0.169605 lr 0.00043967 rank 3
2023-02-23 07:10:02,821 DEBUG TRAIN Batch 15/4300 loss 14.733250 loss_att 19.180353 loss_ctc 16.265535 loss_rnnt 13.480911 hw_loss 0.297399 lr 0.00043950 rank 3
2023-02-23 07:10:02,825 DEBUG TRAIN Batch 15/4300 loss 12.239257 loss_att 14.088838 loss_ctc 13.365623 loss_rnnt 11.569674 hw_loss 0.280282 lr 0.00043953 rank 6
2023-02-23 07:10:02,825 DEBUG TRAIN Batch 15/4300 loss 9.230847 loss_att 11.416526 loss_ctc 10.768219 loss_rnnt 8.418805 hw_loss 0.318606 lr 0.00043953 rank 4
2023-02-23 07:10:02,825 DEBUG TRAIN Batch 15/4300 loss 11.100075 loss_att 14.723801 loss_ctc 12.277345 loss_rnnt 10.119314 hw_loss 0.185710 lr 0.00043951 rank 0
2023-02-23 07:10:02,827 DEBUG TRAIN Batch 15/4300 loss 16.512808 loss_att 19.883015 loss_ctc 22.971893 loss_rnnt 14.875175 hw_loss 0.191961 lr 0.00043949 rank 2
2023-02-23 07:10:02,829 DEBUG TRAIN Batch 15/4300 loss 14.093153 loss_att 16.445122 loss_ctc 21.626221 loss_rnnt 12.501373 hw_loss 0.219331 lr 0.00043942 rank 5
2023-02-23 07:10:02,833 DEBUG TRAIN Batch 15/4300 loss 4.998601 loss_att 8.470319 loss_ctc 5.951189 loss_rnnt 4.048417 hw_loss 0.241555 lr 0.00043943 rank 7
2023-02-23 07:10:02,877 DEBUG TRAIN Batch 15/4300 loss 14.558506 loss_att 16.507792 loss_ctc 18.872639 loss_rnnt 13.485499 hw_loss 0.202372 lr 0.00043955 rank 1
2023-02-23 07:11:18,838 DEBUG TRAIN Batch 15/4400 loss 8.236117 loss_att 10.200967 loss_ctc 11.480733 loss_rnnt 7.242221 hw_loss 0.315584 lr 0.00043936 rank 4
2023-02-23 07:11:18,838 DEBUG TRAIN Batch 15/4400 loss 5.411001 loss_att 10.692379 loss_ctc 7.366875 loss_rnnt 3.965797 hw_loss 0.240272 lr 0.00043932 rank 2
2023-02-23 07:11:18,840 DEBUG TRAIN Batch 15/4400 loss 7.213190 loss_att 8.582079 loss_ctc 12.041695 loss_rnnt 6.167109 hw_loss 0.240941 lr 0.00043933 rank 3
2023-02-23 07:11:18,844 DEBUG TRAIN Batch 15/4400 loss 16.748066 loss_att 18.886883 loss_ctc 19.730537 loss_rnnt 15.761203 hw_loss 0.302696 lr 0.00043925 rank 5
2023-02-23 07:11:18,844 DEBUG TRAIN Batch 15/4400 loss 10.866179 loss_att 13.667728 loss_ctc 14.065562 loss_rnnt 9.777832 hw_loss 0.190225 lr 0.00043934 rank 0
2023-02-23 07:11:18,849 DEBUG TRAIN Batch 15/4400 loss 9.406489 loss_att 11.817525 loss_ctc 12.794530 loss_rnnt 8.304405 hw_loss 0.315259 lr 0.00043938 rank 1
2023-02-23 07:11:18,857 DEBUG TRAIN Batch 15/4400 loss 7.194270 loss_att 7.583014 loss_ctc 10.512727 loss_rnnt 6.482159 hw_loss 0.359815 lr 0.00043936 rank 6
2023-02-23 07:11:18,897 DEBUG TRAIN Batch 15/4400 loss 8.905657 loss_att 10.596235 loss_ctc 13.817902 loss_rnnt 7.802225 hw_loss 0.206908 lr 0.00043926 rank 7
2023-02-23 07:12:35,344 DEBUG TRAIN Batch 15/4500 loss 14.435678 loss_att 17.608406 loss_ctc 20.034725 loss_rnnt 12.921926 hw_loss 0.248751 lr 0.00043908 rank 5
2023-02-23 07:12:35,347 DEBUG TRAIN Batch 15/4500 loss 17.609781 loss_att 23.089420 loss_ctc 18.742342 loss_rnnt 16.190895 hw_loss 0.322404 lr 0.00043921 rank 1
2023-02-23 07:12:35,347 DEBUG TRAIN Batch 15/4500 loss 8.009562 loss_att 11.859732 loss_ctc 12.406002 loss_rnnt 6.555691 hw_loss 0.183087 lr 0.00043917 rank 0
2023-02-23 07:12:35,348 DEBUG TRAIN Batch 15/4500 loss 11.867160 loss_att 11.607815 loss_ctc 14.131028 loss_rnnt 11.433631 hw_loss 0.344154 lr 0.00043909 rank 7
2023-02-23 07:12:35,353 DEBUG TRAIN Batch 15/4500 loss 4.389705 loss_att 8.215123 loss_ctc 3.941091 loss_rnnt 3.580984 hw_loss 0.193973 lr 0.00043916 rank 3
2023-02-23 07:12:35,354 DEBUG TRAIN Batch 15/4500 loss 15.705566 loss_att 15.821106 loss_ctc 25.086666 loss_rnnt 14.291173 hw_loss 0.263383 lr 0.00043915 rank 2
2023-02-23 07:12:35,355 DEBUG TRAIN Batch 15/4500 loss 4.780544 loss_att 8.469021 loss_ctc 4.709152 loss_rnnt 3.894113 hw_loss 0.296728 lr 0.00043919 rank 4
2023-02-23 07:12:35,359 DEBUG TRAIN Batch 15/4500 loss 8.123504 loss_att 12.055431 loss_ctc 11.096821 loss_rnnt 6.825324 hw_loss 0.216285 lr 0.00043919 rank 6
2023-02-23 07:13:53,434 DEBUG TRAIN Batch 15/4600 loss 11.500601 loss_att 13.447484 loss_ctc 13.648911 loss_rnnt 10.696895 hw_loss 0.239792 lr 0.00043902 rank 4
2023-02-23 07:13:53,437 DEBUG TRAIN Batch 15/4600 loss 9.257074 loss_att 12.802235 loss_ctc 12.607918 loss_rnnt 7.990083 hw_loss 0.208464 lr 0.00043891 rank 5
2023-02-23 07:13:53,440 DEBUG TRAIN Batch 15/4600 loss 6.349921 loss_att 11.196078 loss_ctc 7.350233 loss_rnnt 5.081285 hw_loss 0.311305 lr 0.00043900 rank 0
2023-02-23 07:13:53,442 DEBUG TRAIN Batch 15/4600 loss 9.398558 loss_att 10.739704 loss_ctc 11.089622 loss_rnnt 8.790953 hw_loss 0.213561 lr 0.00043904 rank 1
2023-02-23 07:13:53,444 DEBUG TRAIN Batch 15/4600 loss 21.607477 loss_att 24.553368 loss_ctc 23.405437 loss_rnnt 20.671211 hw_loss 0.201295 lr 0.00043902 rank 6
2023-02-23 07:13:53,446 DEBUG TRAIN Batch 15/4600 loss 3.270268 loss_att 8.597272 loss_ctc 5.076867 loss_rnnt 1.837232 hw_loss 0.237668 lr 0.00043900 rank 3
2023-02-23 07:13:53,451 DEBUG TRAIN Batch 15/4600 loss 9.670092 loss_att 13.043413 loss_ctc 10.631362 loss_rnnt 8.724553 hw_loss 0.267570 lr 0.00043892 rank 7
2023-02-23 07:13:53,454 DEBUG TRAIN Batch 15/4600 loss 6.948593 loss_att 13.131522 loss_ctc 11.252831 loss_rnnt 5.010666 hw_loss 0.238954 lr 0.00043898 rank 2
2023-02-23 07:15:10,367 DEBUG TRAIN Batch 15/4700 loss 8.801137 loss_att 12.462477 loss_ctc 14.832640 loss_rnnt 7.137012 hw_loss 0.239355 lr 0.00043883 rank 3
2023-02-23 07:15:10,369 DEBUG TRAIN Batch 15/4700 loss 15.569542 loss_att 23.657953 loss_ctc 26.716125 loss_rnnt 12.340153 hw_loss 0.235306 lr 0.00043874 rank 5
2023-02-23 07:15:10,370 DEBUG TRAIN Batch 15/4700 loss 7.737734 loss_att 10.087201 loss_ctc 10.750400 loss_rnnt 6.772780 hw_loss 0.175072 lr 0.00043881 rank 2
2023-02-23 07:15:10,373 DEBUG TRAIN Batch 15/4700 loss 7.809632 loss_att 14.575664 loss_ctc 12.263788 loss_rnnt 5.709344 hw_loss 0.287239 lr 0.00043885 rank 6
2023-02-23 07:15:10,373 DEBUG TRAIN Batch 15/4700 loss 9.247982 loss_att 12.520925 loss_ctc 16.640129 loss_rnnt 7.458697 hw_loss 0.279520 lr 0.00043885 rank 4
2023-02-23 07:15:10,375 DEBUG TRAIN Batch 15/4700 loss 11.480730 loss_att 14.182856 loss_ctc 15.818451 loss_rnnt 10.272383 hw_loss 0.167924 lr 0.00043875 rank 7
2023-02-23 07:15:10,375 DEBUG TRAIN Batch 15/4700 loss 18.028316 loss_att 19.117420 loss_ctc 22.125298 loss_rnnt 17.116955 hw_loss 0.276141 lr 0.00043883 rank 0
2023-02-23 07:15:10,377 DEBUG TRAIN Batch 15/4700 loss 7.500103 loss_att 11.468132 loss_ctc 9.317160 loss_rnnt 6.380044 hw_loss 0.157837 lr 0.00043887 rank 1
2023-02-23 07:16:26,323 DEBUG TRAIN Batch 15/4800 loss 12.962593 loss_att 15.642839 loss_ctc 12.718520 loss_rnnt 12.312632 hw_loss 0.274601 lr 0.00043857 rank 5
2023-02-23 07:16:26,324 DEBUG TRAIN Batch 15/4800 loss 16.304863 loss_att 19.226646 loss_ctc 21.447102 loss_rnnt 14.960976 hw_loss 0.138560 lr 0.00043866 rank 3
2023-02-23 07:16:26,325 DEBUG TRAIN Batch 15/4800 loss 10.664907 loss_att 13.637542 loss_ctc 11.983635 loss_rnnt 9.755899 hw_loss 0.259967 lr 0.00043864 rank 2
2023-02-23 07:16:26,326 DEBUG TRAIN Batch 15/4800 loss 17.182680 loss_att 20.610613 loss_ctc 25.256624 loss_rnnt 15.291728 hw_loss 0.241574 lr 0.00043868 rank 4
2023-02-23 07:16:26,327 DEBUG TRAIN Batch 15/4800 loss 12.867705 loss_att 13.450891 loss_ctc 12.204197 loss_rnnt 12.736488 hw_loss 0.193212 lr 0.00043870 rank 1
2023-02-23 07:16:26,327 DEBUG TRAIN Batch 15/4800 loss 7.273031 loss_att 10.732419 loss_ctc 11.320987 loss_rnnt 5.879820 hw_loss 0.303011 lr 0.00043867 rank 0
2023-02-23 07:16:26,328 DEBUG TRAIN Batch 15/4800 loss 9.185762 loss_att 11.795496 loss_ctc 10.642164 loss_rnnt 8.348192 hw_loss 0.227693 lr 0.00043858 rank 7
2023-02-23 07:16:26,340 DEBUG TRAIN Batch 15/4800 loss 10.603927 loss_att 11.448213 loss_ctc 11.313691 loss_rnnt 10.215712 hw_loss 0.233856 lr 0.00043868 rank 6
2023-02-23 07:17:41,884 DEBUG TRAIN Batch 15/4900 loss 7.784396 loss_att 13.291676 loss_ctc 12.957020 loss_rnnt 5.866406 hw_loss 0.237844 lr 0.00043850 rank 0
2023-02-23 07:17:41,886 DEBUG TRAIN Batch 15/4900 loss 14.668596 loss_att 15.765875 loss_ctc 18.745958 loss_rnnt 13.761014 hw_loss 0.270895 lr 0.00043847 rank 2
2023-02-23 07:17:41,890 DEBUG TRAIN Batch 15/4900 loss 12.114959 loss_att 14.858790 loss_ctc 20.306274 loss_rnnt 10.376476 hw_loss 0.182889 lr 0.00043851 rank 4
2023-02-23 07:17:41,891 DEBUG TRAIN Batch 15/4900 loss 17.205580 loss_att 23.427763 loss_ctc 25.383068 loss_rnnt 14.716837 hw_loss 0.288701 lr 0.00043854 rank 1
2023-02-23 07:17:41,892 DEBUG TRAIN Batch 15/4900 loss 17.665512 loss_att 17.704828 loss_ctc 27.164444 loss_rnnt 16.315470 hw_loss 0.141853 lr 0.00043840 rank 5
2023-02-23 07:17:41,896 DEBUG TRAIN Batch 15/4900 loss 12.882507 loss_att 16.739305 loss_ctc 19.986290 loss_rnnt 11.034576 hw_loss 0.242628 lr 0.00043841 rank 7
2023-02-23 07:17:41,896 DEBUG TRAIN Batch 15/4900 loss 24.943266 loss_att 24.126312 loss_ctc 37.216534 loss_rnnt 23.359135 hw_loss 0.208288 lr 0.00043851 rank 6
2023-02-23 07:17:41,898 DEBUG TRAIN Batch 15/4900 loss 6.660311 loss_att 8.780836 loss_ctc 10.033160 loss_rnnt 5.721999 hw_loss 0.120926 lr 0.00043849 rank 3
2023-02-23 07:19:00,430 DEBUG TRAIN Batch 15/5000 loss 18.251251 loss_att 19.469408 loss_ctc 24.455650 loss_rnnt 17.109756 hw_loss 0.132392 lr 0.00043830 rank 2
2023-02-23 07:19:00,433 DEBUG TRAIN Batch 15/5000 loss 8.726992 loss_att 10.953123 loss_ctc 10.520100 loss_rnnt 7.879652 hw_loss 0.305685 lr 0.00043834 rank 4
2023-02-23 07:19:00,435 DEBUG TRAIN Batch 15/5000 loss 8.453633 loss_att 10.311078 loss_ctc 11.249011 loss_rnnt 7.631315 hw_loss 0.146460 lr 0.00043823 rank 5
2023-02-23 07:19:00,436 DEBUG TRAIN Batch 15/5000 loss 6.129190 loss_att 8.354586 loss_ctc 8.948297 loss_rnnt 5.205384 hw_loss 0.192838 lr 0.00043825 rank 7
2023-02-23 07:19:00,437 DEBUG TRAIN Batch 15/5000 loss 10.972809 loss_att 12.790424 loss_ctc 13.752789 loss_rnnt 10.149614 hw_loss 0.166888 lr 0.00043832 rank 3
2023-02-23 07:19:00,439 DEBUG TRAIN Batch 15/5000 loss 7.585245 loss_att 10.382432 loss_ctc 10.789120 loss_rnnt 6.513188 hw_loss 0.160191 lr 0.00043833 rank 0
2023-02-23 07:19:00,441 DEBUG TRAIN Batch 15/5000 loss 18.725174 loss_att 20.951611 loss_ctc 27.645021 loss_rnnt 16.949911 hw_loss 0.263746 lr 0.00043837 rank 1
2023-02-23 07:19:00,486 DEBUG TRAIN Batch 15/5000 loss 10.416308 loss_att 11.034735 loss_ctc 11.398067 loss_rnnt 9.989386 hw_loss 0.323132 lr 0.00043834 rank 6
2023-02-23 07:20:15,693 DEBUG TRAIN Batch 15/5100 loss 9.203050 loss_att 10.790465 loss_ctc 13.374837 loss_rnnt 8.170213 hw_loss 0.298341 lr 0.00043807 rank 5
2023-02-23 07:20:15,694 DEBUG TRAIN Batch 15/5100 loss 20.616518 loss_att 21.787117 loss_ctc 26.922012 loss_rnnt 19.362946 hw_loss 0.335095 lr 0.00043816 rank 0
2023-02-23 07:20:15,701 DEBUG TRAIN Batch 15/5100 loss 10.910719 loss_att 12.954817 loss_ctc 13.147045 loss_rnnt 10.046280 hw_loss 0.295203 lr 0.00043820 rank 1
2023-02-23 07:20:15,700 DEBUG TRAIN Batch 15/5100 loss 13.678830 loss_att 12.782436 loss_ctc 15.320606 loss_rnnt 13.432482 hw_loss 0.387605 lr 0.00043817 rank 4
2023-02-23 07:20:15,700 DEBUG TRAIN Batch 15/5100 loss 11.427114 loss_att 13.205954 loss_ctc 14.658304 loss_rnnt 10.507215 hw_loss 0.249947 lr 0.00043813 rank 2
2023-02-23 07:20:15,703 DEBUG TRAIN Batch 15/5100 loss 10.305322 loss_att 14.759659 loss_ctc 12.962545 loss_rnnt 8.986787 hw_loss 0.137571 lr 0.00043817 rank 6
2023-02-23 07:20:15,705 DEBUG TRAIN Batch 15/5100 loss 4.432018 loss_att 11.037691 loss_ctc 7.094291 loss_rnnt 2.614650 hw_loss 0.264870 lr 0.00043815 rank 3
2023-02-23 07:20:15,709 DEBUG TRAIN Batch 15/5100 loss 10.866182 loss_att 11.825334 loss_ctc 12.763641 loss_rnnt 10.265701 hw_loss 0.291856 lr 0.00043808 rank 7
2023-02-23 07:21:31,651 DEBUG TRAIN Batch 15/5200 loss 9.500480 loss_att 15.295404 loss_ctc 12.903532 loss_rnnt 7.785610 hw_loss 0.191521 lr 0.00043790 rank 5
2023-02-23 07:21:31,653 DEBUG TRAIN Batch 15/5200 loss 17.923428 loss_att 19.654177 loss_ctc 22.688459 loss_rnnt 16.808605 hw_loss 0.250007 lr 0.00043801 rank 6
2023-02-23 07:21:31,652 DEBUG TRAIN Batch 15/5200 loss 12.729360 loss_att 12.307316 loss_ctc 16.456512 loss_rnnt 12.150964 hw_loss 0.310971 lr 0.00043797 rank 2
2023-02-23 07:21:31,655 DEBUG TRAIN Batch 15/5200 loss 22.790400 loss_att 25.838289 loss_ctc 24.186550 loss_rnnt 21.895359 hw_loss 0.186201 lr 0.00043799 rank 0
2023-02-23 07:21:31,656 DEBUG TRAIN Batch 15/5200 loss 14.985453 loss_att 17.439693 loss_ctc 16.717203 loss_rnnt 14.169962 hw_loss 0.175765 lr 0.00043801 rank 4
2023-02-23 07:21:31,660 DEBUG TRAIN Batch 15/5200 loss 17.466780 loss_att 20.447084 loss_ctc 19.675058 loss_rnnt 16.473360 hw_loss 0.192977 lr 0.00043791 rank 7
2023-02-23 07:21:31,664 DEBUG TRAIN Batch 15/5200 loss 17.674406 loss_att 19.358688 loss_ctc 23.492298 loss_rnnt 16.493114 hw_loss 0.128839 lr 0.00043798 rank 3
2023-02-23 07:21:31,704 DEBUG TRAIN Batch 15/5200 loss 10.944274 loss_att 14.992302 loss_ctc 12.240591 loss_rnnt 9.901131 hw_loss 0.113805 lr 0.00043803 rank 1
2023-02-23 07:22:49,978 DEBUG TRAIN Batch 15/5300 loss 8.915142 loss_att 12.009723 loss_ctc 12.830397 loss_rnnt 7.660310 hw_loss 0.213527 lr 0.00043773 rank 5
2023-02-23 07:22:49,980 DEBUG TRAIN Batch 15/5300 loss 2.983837 loss_att 6.148829 loss_ctc 5.278251 loss_rnnt 1.949890 hw_loss 0.178175 lr 0.00043782 rank 0
2023-02-23 07:22:49,982 DEBUG TRAIN Batch 15/5300 loss 18.666077 loss_att 20.413113 loss_ctc 22.650732 loss_rnnt 17.693344 hw_loss 0.172565 lr 0.00043774 rank 7
2023-02-23 07:22:49,984 DEBUG TRAIN Batch 15/5300 loss 6.480626 loss_att 7.702444 loss_ctc 5.652960 loss_rnnt 6.283545 hw_loss 0.118261 lr 0.00043784 rank 4
2023-02-23 07:22:49,985 DEBUG TRAIN Batch 15/5300 loss 9.538679 loss_att 11.414043 loss_ctc 9.599346 loss_rnnt 9.023571 hw_loss 0.247399 lr 0.00043780 rank 2
2023-02-23 07:22:49,985 DEBUG TRAIN Batch 15/5300 loss 11.561644 loss_att 17.732738 loss_ctc 13.728526 loss_rnnt 9.943218 hw_loss 0.178665 lr 0.00043782 rank 3
2023-02-23 07:22:49,988 DEBUG TRAIN Batch 15/5300 loss 14.282211 loss_att 15.461949 loss_ctc 18.014622 loss_rnnt 13.465313 hw_loss 0.156177 lr 0.00043784 rank 6
2023-02-23 07:22:49,994 DEBUG TRAIN Batch 15/5300 loss 4.382716 loss_att 7.771816 loss_ctc 7.780705 loss_rnnt 3.099651 hw_loss 0.285336 lr 0.00043786 rank 1
2023-02-23 07:24:07,716 DEBUG TRAIN Batch 15/5400 loss 21.028322 loss_att 26.508392 loss_ctc 26.450413 loss_rnnt 19.012207 hw_loss 0.369666 lr 0.00043767 rank 4
2023-02-23 07:24:07,718 DEBUG TRAIN Batch 15/5400 loss 9.260480 loss_att 11.191195 loss_ctc 10.418321 loss_rnnt 8.622729 hw_loss 0.182304 lr 0.00043756 rank 5
2023-02-23 07:24:07,719 DEBUG TRAIN Batch 15/5400 loss 13.036435 loss_att 15.377441 loss_ctc 18.311413 loss_rnnt 11.689857 hw_loss 0.328213 lr 0.00043763 rank 2
2023-02-23 07:24:07,723 DEBUG TRAIN Batch 15/5400 loss 9.736495 loss_att 10.549793 loss_ctc 11.409792 loss_rnnt 9.214414 hw_loss 0.255591 lr 0.00043769 rank 1
2023-02-23 07:24:07,723 DEBUG TRAIN Batch 15/5400 loss 10.697351 loss_att 14.001673 loss_ctc 16.471886 loss_rnnt 9.153693 hw_loss 0.211604 lr 0.00043766 rank 0
2023-02-23 07:24:07,726 DEBUG TRAIN Batch 15/5400 loss 15.372124 loss_att 18.679079 loss_ctc 21.230404 loss_rnnt 13.842843 hw_loss 0.162724 lr 0.00043757 rank 7
2023-02-23 07:24:07,726 DEBUG TRAIN Batch 15/5400 loss 15.291994 loss_att 19.843805 loss_ctc 24.070412 loss_rnnt 13.113400 hw_loss 0.183329 lr 0.00043767 rank 6
2023-02-23 07:24:07,727 DEBUG TRAIN Batch 15/5400 loss 15.975408 loss_att 18.276718 loss_ctc 20.071146 loss_rnnt 14.884403 hw_loss 0.158709 lr 0.00043765 rank 3
2023-02-23 07:25:23,807 DEBUG TRAIN Batch 15/5500 loss 8.386525 loss_att 11.496412 loss_ctc 11.310781 loss_rnnt 7.268928 hw_loss 0.198224 lr 0.00043746 rank 2
2023-02-23 07:25:23,813 DEBUG TRAIN Batch 15/5500 loss 12.311481 loss_att 14.072794 loss_ctc 16.311447 loss_rnnt 11.274935 hw_loss 0.283042 lr 0.00043753 rank 1
2023-02-23 07:25:23,815 DEBUG TRAIN Batch 15/5500 loss 7.753930 loss_att 11.727753 loss_ctc 10.639297 loss_rnnt 6.493199 hw_loss 0.152344 lr 0.00043739 rank 5
2023-02-23 07:25:23,815 DEBUG TRAIN Batch 15/5500 loss 10.962306 loss_att 17.924408 loss_ctc 16.549231 loss_rnnt 8.733936 hw_loss 0.170672 lr 0.00043749 rank 0
2023-02-23 07:25:23,816 DEBUG TRAIN Batch 15/5500 loss 11.576957 loss_att 15.308718 loss_ctc 14.140725 loss_rnnt 10.335601 hw_loss 0.287189 lr 0.00043750 rank 6
2023-02-23 07:25:23,816 DEBUG TRAIN Batch 15/5500 loss 14.973211 loss_att 17.348728 loss_ctc 18.647354 loss_rnnt 13.856670 hw_loss 0.284160 lr 0.00043748 rank 3
2023-02-23 07:25:23,817 DEBUG TRAIN Batch 15/5500 loss 11.418880 loss_att 13.036808 loss_ctc 17.654316 loss_rnnt 10.123118 hw_loss 0.263973 lr 0.00043750 rank 4
2023-02-23 07:25:23,818 DEBUG TRAIN Batch 15/5500 loss 10.100867 loss_att 12.538067 loss_ctc 16.266077 loss_rnnt 8.650087 hw_loss 0.264962 lr 0.00043741 rank 7
2023-02-23 07:26:38,444 DEBUG TRAIN Batch 15/5600 loss 19.671843 loss_att 25.985596 loss_ctc 32.624138 loss_rnnt 16.568405 hw_loss 0.213213 lr 0.00043733 rank 4
2023-02-23 07:26:38,446 DEBUG TRAIN Batch 15/5600 loss 11.952433 loss_att 16.225859 loss_ctc 14.519395 loss_rnnt 10.653183 hw_loss 0.191816 lr 0.00043730 rank 2
2023-02-23 07:26:38,449 DEBUG TRAIN Batch 15/5600 loss 12.990480 loss_att 16.386354 loss_ctc 20.094854 loss_rnnt 11.214903 hw_loss 0.279662 lr 0.00043723 rank 5
2023-02-23 07:26:38,453 DEBUG TRAIN Batch 15/5600 loss 13.434834 loss_att 13.508798 loss_ctc 15.465546 loss_rnnt 13.036082 hw_loss 0.212246 lr 0.00043736 rank 1
2023-02-23 07:26:38,453 DEBUG TRAIN Batch 15/5600 loss 19.200296 loss_att 22.306047 loss_ctc 21.991249 loss_rnnt 18.097794 hw_loss 0.204798 lr 0.00043732 rank 0
2023-02-23 07:26:38,456 DEBUG TRAIN Batch 15/5600 loss 14.220859 loss_att 15.605030 loss_ctc 19.717985 loss_rnnt 13.041968 hw_loss 0.317073 lr 0.00043731 rank 3
2023-02-23 07:26:38,456 DEBUG TRAIN Batch 15/5600 loss 13.901900 loss_att 16.689964 loss_ctc 17.953026 loss_rnnt 12.657551 hw_loss 0.274850 lr 0.00043733 rank 6
2023-02-23 07:26:38,459 DEBUG TRAIN Batch 15/5600 loss 4.717142 loss_att 9.410214 loss_ctc 6.605949 loss_rnnt 3.451149 hw_loss 0.141633 lr 0.00043724 rank 7
2023-02-23 07:27:58,266 DEBUG TRAIN Batch 15/5700 loss 12.298874 loss_att 13.871696 loss_ctc 17.515141 loss_rnnt 11.148331 hw_loss 0.263393 lr 0.00043715 rank 0
2023-02-23 07:27:58,266 DEBUG TRAIN Batch 15/5700 loss 10.182258 loss_att 13.240089 loss_ctc 12.834387 loss_rnnt 9.097374 hw_loss 0.224438 lr 0.00043717 rank 4
2023-02-23 07:27:58,266 DEBUG TRAIN Batch 15/5700 loss 14.223172 loss_att 15.828462 loss_ctc 19.882471 loss_rnnt 12.975986 hw_loss 0.321667 lr 0.00043707 rank 7
2023-02-23 07:27:58,268 DEBUG TRAIN Batch 15/5700 loss 14.646686 loss_att 14.034903 loss_ctc 15.056797 loss_rnnt 14.553212 hw_loss 0.302151 lr 0.00043706 rank 5
2023-02-23 07:27:58,268 DEBUG TRAIN Batch 15/5700 loss 20.686388 loss_att 22.094099 loss_ctc 26.627577 loss_rnnt 19.460369 hw_loss 0.285597 lr 0.00043717 rank 6
2023-02-23 07:27:58,272 DEBUG TRAIN Batch 15/5700 loss 6.093112 loss_att 8.972660 loss_ctc 7.430601 loss_rnnt 5.211517 hw_loss 0.238788 lr 0.00043713 rank 2
2023-02-23 07:27:58,274 DEBUG TRAIN Batch 15/5700 loss 5.949436 loss_att 10.959450 loss_ctc 7.108875 loss_rnnt 4.674284 hw_loss 0.222297 lr 0.00043719 rank 1
2023-02-23 07:27:58,315 DEBUG TRAIN Batch 15/5700 loss 8.914653 loss_att 8.637237 loss_ctc 9.798870 loss_rnnt 8.662482 hw_loss 0.355797 lr 0.00043715 rank 3
2023-02-23 07:29:13,013 DEBUG TRAIN Batch 15/5800 loss 6.230981 loss_att 8.157822 loss_ctc 7.520992 loss_rnnt 5.537018 hw_loss 0.256112 lr 0.00043689 rank 5
2023-02-23 07:29:13,015 DEBUG TRAIN Batch 15/5800 loss 8.966537 loss_att 10.400961 loss_ctc 13.074575 loss_rnnt 7.982197 hw_loss 0.280718 lr 0.00043696 rank 2
2023-02-23 07:29:13,017 DEBUG TRAIN Batch 15/5800 loss 12.281100 loss_att 17.679096 loss_ctc 14.661469 loss_rnnt 10.746280 hw_loss 0.258448 lr 0.00043691 rank 7
2023-02-23 07:29:13,017 DEBUG TRAIN Batch 15/5800 loss 6.197042 loss_att 10.905510 loss_ctc 8.558201 loss_rnnt 4.820046 hw_loss 0.225902 lr 0.00043699 rank 0
2023-02-23 07:29:13,018 DEBUG TRAIN Batch 15/5800 loss 15.560773 loss_att 18.599701 loss_ctc 17.265547 loss_rnnt 14.596606 hw_loss 0.242020 lr 0.00043698 rank 3
2023-02-23 07:29:13,018 DEBUG TRAIN Batch 15/5800 loss 9.233458 loss_att 11.924963 loss_ctc 9.855779 loss_rnnt 8.514410 hw_loss 0.183319 lr 0.00043700 rank 4
2023-02-23 07:29:13,020 DEBUG TRAIN Batch 15/5800 loss 12.596843 loss_att 15.907070 loss_ctc 16.825201 loss_rnnt 11.238734 hw_loss 0.248029 lr 0.00043700 rank 6
2023-02-23 07:29:13,024 DEBUG TRAIN Batch 15/5800 loss 7.997642 loss_att 10.990733 loss_ctc 9.213905 loss_rnnt 7.098615 hw_loss 0.259200 lr 0.00043703 rank 1
2023-02-23 07:30:27,744 DEBUG TRAIN Batch 15/5900 loss 3.493769 loss_att 8.774866 loss_ctc 5.502403 loss_rnnt 2.030125 hw_loss 0.261764 lr 0.00043673 rank 5
2023-02-23 07:30:27,746 DEBUG TRAIN Batch 15/5900 loss 5.171376 loss_att 7.653031 loss_ctc 6.788954 loss_rnnt 4.313864 hw_loss 0.272820 lr 0.00043683 rank 4
2023-02-23 07:30:27,750 DEBUG TRAIN Batch 15/5900 loss 14.815770 loss_att 15.215742 loss_ctc 21.562069 loss_rnnt 13.723169 hw_loss 0.212062 lr 0.00043686 rank 1
2023-02-23 07:30:27,751 DEBUG TRAIN Batch 15/5900 loss 12.652965 loss_att 16.675373 loss_ctc 21.306187 loss_rnnt 10.535545 hw_loss 0.298449 lr 0.00043683 rank 6
2023-02-23 07:30:27,751 DEBUG TRAIN Batch 15/5900 loss 7.624135 loss_att 9.434188 loss_ctc 11.332546 loss_rnnt 6.648952 hw_loss 0.222597 lr 0.00043674 rank 7
2023-02-23 07:30:27,752 DEBUG TRAIN Batch 15/5900 loss 6.160895 loss_att 9.604083 loss_ctc 7.239856 loss_rnnt 5.173107 hw_loss 0.291169 lr 0.00043681 rank 3
2023-02-23 07:30:27,754 DEBUG TRAIN Batch 15/5900 loss 3.490510 loss_att 6.725085 loss_ctc 4.064620 loss_rnnt 2.607457 hw_loss 0.299232 lr 0.00043680 rank 2
2023-02-23 07:30:27,756 DEBUG TRAIN Batch 15/5900 loss 15.842941 loss_att 17.441994 loss_ctc 20.921478 loss_rnnt 14.657701 hw_loss 0.353045 lr 0.00043682 rank 0
2023-02-23 07:31:44,202 DEBUG TRAIN Batch 15/6000 loss 6.230976 loss_att 11.252541 loss_ctc 10.527376 loss_rnnt 4.513495 hw_loss 0.263088 lr 0.00043667 rank 4
2023-02-23 07:31:44,202 DEBUG TRAIN Batch 15/6000 loss 6.164588 loss_att 9.661480 loss_ctc 8.324547 loss_rnnt 5.102885 hw_loss 0.139369 lr 0.00043663 rank 2
2023-02-23 07:31:44,203 DEBUG TRAIN Batch 15/6000 loss 13.667285 loss_att 19.093655 loss_ctc 17.203856 loss_rnnt 12.000168 hw_loss 0.206813 lr 0.00043665 rank 3
2023-02-23 07:31:44,205 DEBUG TRAIN Batch 15/6000 loss 13.628981 loss_att 15.530888 loss_ctc 18.339861 loss_rnnt 12.513759 hw_loss 0.200104 lr 0.00043665 rank 0
2023-02-23 07:31:44,206 DEBUG TRAIN Batch 15/6000 loss 7.315824 loss_att 10.474201 loss_ctc 9.579716 loss_rnnt 6.290762 hw_loss 0.171625 lr 0.00043669 rank 1
2023-02-23 07:31:44,206 DEBUG TRAIN Batch 15/6000 loss 7.599586 loss_att 9.818335 loss_ctc 9.435714 loss_rnnt 6.795318 hw_loss 0.216938 lr 0.00043656 rank 5
2023-02-23 07:31:44,207 DEBUG TRAIN Batch 15/6000 loss 10.735152 loss_att 13.271935 loss_ctc 14.168875 loss_rnnt 9.630096 hw_loss 0.262255 lr 0.00043657 rank 7
2023-02-23 07:31:44,209 DEBUG TRAIN Batch 15/6000 loss 6.827511 loss_att 9.704242 loss_ctc 7.221554 loss_rnnt 6.087181 hw_loss 0.210833 lr 0.00043667 rank 6
2023-02-23 07:33:01,356 DEBUG TRAIN Batch 15/6100 loss 8.634641 loss_att 12.566034 loss_ctc 13.388504 loss_rnnt 7.057969 hw_loss 0.293520 lr 0.00043653 rank 1
2023-02-23 07:33:01,356 DEBUG TRAIN Batch 15/6100 loss 21.877314 loss_att 21.193071 loss_ctc 29.630186 loss_rnnt 20.904686 hw_loss 0.142049 lr 0.00043639 rank 5
2023-02-23 07:33:01,357 DEBUG TRAIN Batch 15/6100 loss 17.021965 loss_att 20.770988 loss_ctc 20.894218 loss_rnnt 15.612516 hw_loss 0.268768 lr 0.00043650 rank 4
2023-02-23 07:33:01,357 DEBUG TRAIN Batch 15/6100 loss 11.431277 loss_att 14.705523 loss_ctc 13.572514 loss_rnnt 10.378606 hw_loss 0.210608 lr 0.00043646 rank 2
2023-02-23 07:33:01,359 DEBUG TRAIN Batch 15/6100 loss 16.686562 loss_att 19.371897 loss_ctc 26.245298 loss_rnnt 14.760048 hw_loss 0.215531 lr 0.00043649 rank 0
2023-02-23 07:33:01,361 DEBUG TRAIN Batch 15/6100 loss 4.523737 loss_att 10.371730 loss_ctc 4.875988 loss_rnnt 3.229144 hw_loss 0.146304 lr 0.00043650 rank 6
2023-02-23 07:33:01,362 DEBUG TRAIN Batch 15/6100 loss 6.016972 loss_att 9.129428 loss_ctc 8.052992 loss_rnnt 4.987361 hw_loss 0.254345 lr 0.00043648 rank 3
2023-02-23 07:33:01,362 DEBUG TRAIN Batch 15/6100 loss 5.412698 loss_att 8.058971 loss_ctc 10.045770 loss_rnnt 4.158054 hw_loss 0.201837 lr 0.00043641 rank 7
2023-02-23 07:34:15,451 DEBUG TRAIN Batch 15/6200 loss 9.549004 loss_att 13.287460 loss_ctc 12.369663 loss_rnnt 8.314282 hw_loss 0.208015 lr 0.00043630 rank 2
2023-02-23 07:34:15,452 DEBUG TRAIN Batch 15/6200 loss 5.625594 loss_att 7.228740 loss_ctc 8.072302 loss_rnnt 4.914320 hw_loss 0.120780 lr 0.00043631 rank 3
2023-02-23 07:34:15,452 DEBUG TRAIN Batch 15/6200 loss 20.565161 loss_att 21.432196 loss_ctc 27.531527 loss_rnnt 19.280054 hw_loss 0.342841 lr 0.00043623 rank 5
2023-02-23 07:34:15,453 DEBUG TRAIN Batch 15/6200 loss 3.532725 loss_att 5.704103 loss_ctc 3.506249 loss_rnnt 2.990413 hw_loss 0.209188 lr 0.00043633 rank 6
2023-02-23 07:34:15,453 DEBUG TRAIN Batch 15/6200 loss 9.726388 loss_att 13.145453 loss_ctc 12.968025 loss_rnnt 8.453085 hw_loss 0.294883 lr 0.00043633 rank 4
2023-02-23 07:34:15,458 DEBUG TRAIN Batch 15/6200 loss 8.180593 loss_att 11.696438 loss_ctc 15.674864 loss_rnnt 6.343577 hw_loss 0.252396 lr 0.00043636 rank 1
2023-02-23 07:34:15,459 DEBUG TRAIN Batch 15/6200 loss 13.572476 loss_att 15.703324 loss_ctc 17.525185 loss_rnnt 12.462593 hw_loss 0.293786 lr 0.00043624 rank 7
2023-02-23 07:34:15,459 DEBUG TRAIN Batch 15/6200 loss 7.914310 loss_att 10.742997 loss_ctc 9.834281 loss_rnnt 6.996891 hw_loss 0.179410 lr 0.00043632 rank 0
2023-02-23 07:35:31,978 DEBUG TRAIN Batch 15/6300 loss 12.573326 loss_att 16.303762 loss_ctc 16.150963 loss_rnnt 11.181311 hw_loss 0.316709 lr 0.00043613 rank 2
2023-02-23 07:35:31,983 DEBUG TRAIN Batch 15/6300 loss 9.274912 loss_att 9.846748 loss_ctc 13.296210 loss_rnnt 8.490226 hw_loss 0.251522 lr 0.00043615 rank 3
2023-02-23 07:35:31,983 DEBUG TRAIN Batch 15/6300 loss 19.962996 loss_att 22.964947 loss_ctc 25.784012 loss_rnnt 18.411991 hw_loss 0.327149 lr 0.00043606 rank 5
2023-02-23 07:35:31,983 DEBUG TRAIN Batch 15/6300 loss 9.884434 loss_att 12.280796 loss_ctc 13.663713 loss_rnnt 8.750940 hw_loss 0.281843 lr 0.00043607 rank 7
2023-02-23 07:35:31,985 DEBUG TRAIN Batch 15/6300 loss 5.067678 loss_att 6.181615 loss_ctc 7.313898 loss_rnnt 4.425731 hw_loss 0.224369 lr 0.00043619 rank 1
2023-02-23 07:35:31,986 DEBUG TRAIN Batch 15/6300 loss 12.499131 loss_att 12.723400 loss_ctc 17.810148 loss_rnnt 11.654757 hw_loss 0.171347 lr 0.00043617 rank 4
2023-02-23 07:35:31,988 DEBUG TRAIN Batch 15/6300 loss 11.676918 loss_att 15.054193 loss_ctc 18.044722 loss_rnnt 10.007471 hw_loss 0.271782 lr 0.00043616 rank 0
2023-02-23 07:35:32,033 DEBUG TRAIN Batch 15/6300 loss 10.432847 loss_att 11.218852 loss_ctc 11.792646 loss_rnnt 9.983409 hw_loss 0.207995 lr 0.00043617 rank 6
2023-02-23 07:36:51,040 DEBUG TRAIN Batch 15/6400 loss 5.238371 loss_att 12.443878 loss_ctc 7.448356 loss_rnnt 3.404719 hw_loss 0.183537 lr 0.00043600 rank 4
2023-02-23 07:36:51,041 DEBUG TRAIN Batch 15/6400 loss 12.113492 loss_att 12.452965 loss_ctc 17.203178 loss_rnnt 11.220413 hw_loss 0.274798 lr 0.00043596 rank 2
2023-02-23 07:36:51,041 DEBUG TRAIN Batch 15/6400 loss 12.301785 loss_att 14.241718 loss_ctc 16.574230 loss_rnnt 11.202721 hw_loss 0.265157 lr 0.00043599 rank 0
2023-02-23 07:36:51,044 DEBUG TRAIN Batch 15/6400 loss 6.911979 loss_att 8.664293 loss_ctc 5.834148 loss_rnnt 6.576114 hw_loss 0.242088 lr 0.00043598 rank 3
2023-02-23 07:36:51,044 DEBUG TRAIN Batch 15/6400 loss 8.497804 loss_att 13.570656 loss_ctc 15.723518 loss_rnnt 6.392162 hw_loss 0.239330 lr 0.00043590 rank 5
2023-02-23 07:36:51,055 DEBUG TRAIN Batch 15/6400 loss 12.248787 loss_att 11.885134 loss_ctc 14.337335 loss_rnnt 11.881020 hw_loss 0.303797 lr 0.00043591 rank 7
2023-02-23 07:36:51,065 DEBUG TRAIN Batch 15/6400 loss 11.824733 loss_att 13.657938 loss_ctc 19.628540 loss_rnnt 10.305949 hw_loss 0.209315 lr 0.00043600 rank 6
2023-02-23 07:36:51,087 DEBUG TRAIN Batch 15/6400 loss 12.881733 loss_att 16.089956 loss_ctc 13.926567 loss_rnnt 12.025843 hw_loss 0.140501 lr 0.00043603 rank 1
2023-02-23 07:38:06,589 DEBUG TRAIN Batch 15/6500 loss 7.006206 loss_att 12.175470 loss_ctc 10.349681 loss_rnnt 5.504383 hw_loss 0.041576 lr 0.00043573 rank 5
2023-02-23 07:38:06,591 DEBUG TRAIN Batch 15/6500 loss 12.760179 loss_att 13.274364 loss_ctc 14.882792 loss_rnnt 12.319045 hw_loss 0.103654 lr 0.00043582 rank 0
2023-02-23 07:38:06,591 DEBUG TRAIN Batch 15/6500 loss 9.973075 loss_att 12.861803 loss_ctc 15.235281 loss_rnnt 8.604777 hw_loss 0.166734 lr 0.00043580 rank 2
2023-02-23 07:38:06,593 DEBUG TRAIN Batch 15/6500 loss 6.736149 loss_att 10.586521 loss_ctc 11.823207 loss_rnnt 5.181203 hw_loss 0.199870 lr 0.00043586 rank 1
2023-02-23 07:38:06,593 DEBUG TRAIN Batch 15/6500 loss 3.263176 loss_att 7.599250 loss_ctc 6.260849 loss_rnnt 1.847868 hw_loss 0.278256 lr 0.00043584 rank 4
2023-02-23 07:38:06,595 DEBUG TRAIN Batch 15/6500 loss 10.233176 loss_att 14.140318 loss_ctc 13.355656 loss_rnnt 8.896608 hw_loss 0.260264 lr 0.00043584 rank 6
2023-02-23 07:38:06,595 DEBUG TRAIN Batch 15/6500 loss 15.117023 loss_att 18.520676 loss_ctc 17.649738 loss_rnnt 13.983620 hw_loss 0.215581 lr 0.00043582 rank 3
2023-02-23 07:38:06,597 DEBUG TRAIN Batch 15/6500 loss 3.474753 loss_att 5.889955 loss_ctc 2.066889 loss_rnnt 3.056388 hw_loss 0.230699 lr 0.00043574 rank 7
2023-02-23 07:39:22,088 DEBUG TRAIN Batch 15/6600 loss 13.692953 loss_att 16.987309 loss_ctc 14.474536 loss_rnnt 12.776233 hw_loss 0.288073 lr 0.00043557 rank 5
2023-02-23 07:39:22,097 DEBUG TRAIN Batch 15/6600 loss 6.771481 loss_att 11.505600 loss_ctc 11.218392 loss_rnnt 5.098052 hw_loss 0.250657 lr 0.00043567 rank 4
2023-02-23 07:39:22,096 DEBUG TRAIN Batch 15/6600 loss 20.926971 loss_att 22.826851 loss_ctc 22.535353 loss_rnnt 20.168379 hw_loss 0.307812 lr 0.00043563 rank 2
2023-02-23 07:39:22,097 DEBUG TRAIN Batch 15/6600 loss 6.278231 loss_att 9.269133 loss_ctc 9.596347 loss_rnnt 5.082286 hw_loss 0.291280 lr 0.00043567 rank 6
2023-02-23 07:39:22,097 DEBUG TRAIN Batch 15/6600 loss 6.852717 loss_att 10.680027 loss_ctc 8.556425 loss_rnnt 5.749505 hw_loss 0.207354 lr 0.00043558 rank 7
2023-02-23 07:39:22,097 DEBUG TRAIN Batch 15/6600 loss 4.945963 loss_att 7.197525 loss_ctc 5.245684 loss_rnnt 4.301980 hw_loss 0.288203 lr 0.00043570 rank 1
2023-02-23 07:39:22,101 DEBUG TRAIN Batch 15/6600 loss 17.646210 loss_att 22.917055 loss_ctc 25.263836 loss_rnnt 15.450314 hw_loss 0.236328 lr 0.00043566 rank 0
2023-02-23 07:39:22,105 DEBUG TRAIN Batch 15/6600 loss 8.881666 loss_att 10.386633 loss_ctc 11.590473 loss_rnnt 8.091433 hw_loss 0.240122 lr 0.00043565 rank 3
2023-02-23 07:40:38,085 DEBUG TRAIN Batch 15/6700 loss 9.028519 loss_att 10.782736 loss_ctc 10.764681 loss_rnnt 8.321122 hw_loss 0.234496 lr 0.00043551 rank 6
2023-02-23 07:40:38,086 DEBUG TRAIN Batch 15/6700 loss 12.665289 loss_att 12.318658 loss_ctc 14.811796 loss_rnnt 12.324636 hw_loss 0.232085 lr 0.00043540 rank 5
2023-02-23 07:40:38,087 DEBUG TRAIN Batch 15/6700 loss 4.013742 loss_att 6.974822 loss_ctc 5.484417 loss_rnnt 3.127874 hw_loss 0.182927 lr 0.00043553 rank 1
2023-02-23 07:40:38,089 DEBUG TRAIN Batch 15/6700 loss 12.887610 loss_att 16.114637 loss_ctc 17.434174 loss_rnnt 11.501933 hw_loss 0.251367 lr 0.00043541 rank 7
2023-02-23 07:40:38,089 DEBUG TRAIN Batch 15/6700 loss 11.706659 loss_att 12.722267 loss_ctc 15.509121 loss_rnnt 10.879103 hw_loss 0.220201 lr 0.00043547 rank 2
2023-02-23 07:40:38,090 DEBUG TRAIN Batch 15/6700 loss 10.575027 loss_att 12.359718 loss_ctc 12.999551 loss_rnnt 9.791715 hw_loss 0.193320 lr 0.00043551 rank 4
2023-02-23 07:40:38,092 DEBUG TRAIN Batch 15/6700 loss 3.887735 loss_att 8.602245 loss_ctc 3.093058 loss_rnnt 2.956136 hw_loss 0.177476 lr 0.00043549 rank 0
2023-02-23 07:40:38,098 DEBUG TRAIN Batch 15/6700 loss 10.304654 loss_att 12.242096 loss_ctc 16.052580 loss_rnnt 9.025532 hw_loss 0.234832 lr 0.00043548 rank 3
2023-02-23 07:41:55,680 DEBUG TRAIN Batch 15/6800 loss 7.428233 loss_att 10.080917 loss_ctc 8.260454 loss_rnnt 6.667660 hw_loss 0.223262 lr 0.00043525 rank 7
2023-02-23 07:41:55,683 DEBUG TRAIN Batch 15/6800 loss 5.733031 loss_att 8.756462 loss_ctc 9.211374 loss_rnnt 4.508487 hw_loss 0.292647 lr 0.00043524 rank 5
2023-02-23 07:41:55,683 DEBUG TRAIN Batch 15/6800 loss 6.800786 loss_att 9.689922 loss_ctc 8.071832 loss_rnnt 5.961172 hw_loss 0.173088 lr 0.00043537 rank 1
2023-02-23 07:41:55,684 DEBUG TRAIN Batch 15/6800 loss 10.094708 loss_att 14.075411 loss_ctc 12.847170 loss_rnnt 8.776266 hw_loss 0.291200 lr 0.00043533 rank 0
2023-02-23 07:41:55,684 DEBUG TRAIN Batch 15/6800 loss 5.220650 loss_att 7.815738 loss_ctc 6.807009 loss_rnnt 4.396134 hw_loss 0.176219 lr 0.00043530 rank 2
2023-02-23 07:41:55,686 DEBUG TRAIN Batch 15/6800 loss 11.227636 loss_att 12.127157 loss_ctc 15.637484 loss_rnnt 10.312099 hw_loss 0.276850 lr 0.00043534 rank 4
2023-02-23 07:41:55,688 DEBUG TRAIN Batch 15/6800 loss 12.803709 loss_att 14.546566 loss_ctc 16.308725 loss_rnnt 11.806211 hw_loss 0.340484 lr 0.00043534 rank 6
2023-02-23 07:41:55,733 DEBUG TRAIN Batch 15/6800 loss 5.190374 loss_att 10.218923 loss_ctc 6.581905 loss_rnnt 3.863898 hw_loss 0.253555 lr 0.00043532 rank 3
2023-02-23 07:43:11,813 DEBUG TRAIN Batch 15/6900 loss 2.983992 loss_att 7.532197 loss_ctc 7.025675 loss_rnnt 1.393258 hw_loss 0.266626 lr 0.00043508 rank 7
2023-02-23 07:43:11,814 DEBUG TRAIN Batch 15/6900 loss 21.840357 loss_att 28.082338 loss_ctc 33.745430 loss_rnnt 18.897762 hw_loss 0.200353 lr 0.00043516 rank 0
2023-02-23 07:43:11,815 DEBUG TRAIN Batch 15/6900 loss 16.729298 loss_att 20.251499 loss_ctc 23.887161 loss_rnnt 14.950134 hw_loss 0.225641 lr 0.00043518 rank 4
2023-02-23 07:43:11,816 DEBUG TRAIN Batch 15/6900 loss 14.176109 loss_att 15.193660 loss_ctc 21.095924 loss_rnnt 12.863636 hw_loss 0.349353 lr 0.00043515 rank 3
2023-02-23 07:43:11,816 DEBUG TRAIN Batch 15/6900 loss 7.977157 loss_att 9.503529 loss_ctc 9.696408 loss_rnnt 7.301646 hw_loss 0.264380 lr 0.00043518 rank 6
2023-02-23 07:43:11,817 DEBUG TRAIN Batch 15/6900 loss 11.574720 loss_att 14.706316 loss_ctc 16.068878 loss_rnnt 10.191993 hw_loss 0.294729 lr 0.00043507 rank 5
2023-02-23 07:43:11,817 DEBUG TRAIN Batch 15/6900 loss 7.528273 loss_att 9.782351 loss_ctc 11.176433 loss_rnnt 6.465929 hw_loss 0.234576 lr 0.00043520 rank 1
2023-02-23 07:43:11,818 DEBUG TRAIN Batch 15/6900 loss 12.333207 loss_att 13.196375 loss_ctc 13.964533 loss_rnnt 11.836405 hw_loss 0.199985 lr 0.00043514 rank 2
2023-02-23 07:44:27,974 DEBUG TRAIN Batch 15/7000 loss 17.602896 loss_att 20.727673 loss_ctc 21.762371 loss_rnnt 16.275177 hw_loss 0.277809 lr 0.00043492 rank 7
2023-02-23 07:44:27,975 DEBUG TRAIN Batch 15/7000 loss 13.077478 loss_att 13.539276 loss_ctc 17.909849 loss_rnnt 12.204208 hw_loss 0.256114 lr 0.00043491 rank 5
2023-02-23 07:44:27,976 DEBUG TRAIN Batch 15/7000 loss 9.404449 loss_att 11.258389 loss_ctc 12.265972 loss_rnnt 8.527232 hw_loss 0.234173 lr 0.00043501 rank 4
2023-02-23 07:44:27,979 DEBUG TRAIN Batch 15/7000 loss 13.886190 loss_att 16.677322 loss_ctc 16.471529 loss_rnnt 12.891335 hw_loss 0.172346 lr 0.00043497 rank 2
2023-02-23 07:44:27,981 DEBUG TRAIN Batch 15/7000 loss 16.842955 loss_att 19.141581 loss_ctc 27.571064 loss_rnnt 14.778650 hw_loss 0.326557 lr 0.00043500 rank 0
2023-02-23 07:44:27,982 DEBUG TRAIN Batch 15/7000 loss 18.681057 loss_att 25.101711 loss_ctc 26.968328 loss_rnnt 16.152790 hw_loss 0.260936 lr 0.00043499 rank 3
2023-02-23 07:44:27,984 DEBUG TRAIN Batch 15/7000 loss 12.343143 loss_att 12.072491 loss_ctc 18.684011 loss_rnnt 11.365048 hw_loss 0.350202 lr 0.00043504 rank 1
2023-02-23 07:44:27,984 DEBUG TRAIN Batch 15/7000 loss 22.061975 loss_att 18.897734 loss_ctc 18.068779 loss_rnnt 23.103397 hw_loss 0.232223 lr 0.00043501 rank 6
2023-02-23 07:45:47,510 DEBUG TRAIN Batch 15/7100 loss 21.978365 loss_att 21.805387 loss_ctc 25.548040 loss_rnnt 21.470236 hw_loss 0.125192 lr 0.00043474 rank 5
2023-02-23 07:45:47,510 DEBUG TRAIN Batch 15/7100 loss 6.357677 loss_att 8.668798 loss_ctc 9.153944 loss_rnnt 5.402180 hw_loss 0.225820 lr 0.00043485 rank 4
2023-02-23 07:45:47,511 DEBUG TRAIN Batch 15/7100 loss 12.355844 loss_att 13.670406 loss_ctc 16.904478 loss_rnnt 11.372426 hw_loss 0.213789 lr 0.00043481 rank 2
2023-02-23 07:45:47,512 DEBUG TRAIN Batch 15/7100 loss 18.016663 loss_att 22.346922 loss_ctc 20.810503 loss_rnnt 16.678278 hw_loss 0.187159 lr 0.00043483 rank 3
2023-02-23 07:45:47,513 DEBUG TRAIN Batch 15/7100 loss 11.389552 loss_att 18.175636 loss_ctc 22.117506 loss_rnnt 8.484329 hw_loss 0.220521 lr 0.00043483 rank 0
2023-02-23 07:45:47,517 DEBUG TRAIN Batch 15/7100 loss 11.391024 loss_att 12.735516 loss_ctc 14.026508 loss_rnnt 10.587564 hw_loss 0.343428 lr 0.00043485 rank 6
2023-02-23 07:45:47,517 DEBUG TRAIN Batch 15/7100 loss 11.558910 loss_att 14.571001 loss_ctc 13.118701 loss_rnnt 10.581404 hw_loss 0.313344 lr 0.00043475 rank 7
2023-02-23 07:45:47,517 DEBUG TRAIN Batch 15/7100 loss 10.041451 loss_att 13.693524 loss_ctc 11.677568 loss_rnnt 8.980453 hw_loss 0.210814 lr 0.00043487 rank 1
2023-02-23 07:47:03,962 DEBUG TRAIN Batch 15/7200 loss 9.322814 loss_att 12.936939 loss_ctc 13.429878 loss_rnnt 7.951023 hw_loss 0.190047 lr 0.00043458 rank 5
2023-02-23 07:47:03,963 DEBUG TRAIN Batch 15/7200 loss 17.235754 loss_att 21.612001 loss_ctc 23.716406 loss_rnnt 15.391487 hw_loss 0.196744 lr 0.00043466 rank 3
2023-02-23 07:47:03,967 DEBUG TRAIN Batch 15/7200 loss 13.744899 loss_att 19.887768 loss_ctc 16.627975 loss_rnnt 12.003202 hw_loss 0.241335 lr 0.00043467 rank 0
2023-02-23 07:47:03,969 DEBUG TRAIN Batch 15/7200 loss 9.221464 loss_att 11.386578 loss_ctc 11.291275 loss_rnnt 8.306902 hw_loss 0.385435 lr 0.00043468 rank 4
2023-02-23 07:47:03,970 DEBUG TRAIN Batch 15/7200 loss 6.848823 loss_att 11.590698 loss_ctc 10.532518 loss_rnnt 5.289998 hw_loss 0.223671 lr 0.00043464 rank 2
2023-02-23 07:47:03,971 DEBUG TRAIN Batch 15/7200 loss 16.765932 loss_att 19.114502 loss_ctc 18.440495 loss_rnnt 15.984767 hw_loss 0.165329 lr 0.00043471 rank 1
2023-02-23 07:47:03,972 DEBUG TRAIN Batch 15/7200 loss 16.272982 loss_att 18.855431 loss_ctc 21.829903 loss_rnnt 14.868216 hw_loss 0.276289 lr 0.00043459 rank 7
2023-02-23 07:47:04,012 DEBUG TRAIN Batch 15/7200 loss 14.594316 loss_att 18.279354 loss_ctc 18.945518 loss_rnnt 13.175081 hw_loss 0.191373 lr 0.00043468 rank 6
2023-02-23 07:48:19,116 DEBUG TRAIN Batch 15/7300 loss 17.442928 loss_att 19.188339 loss_ctc 21.179825 loss_rnnt 16.466396 hw_loss 0.242245 lr 0.00043441 rank 5
2023-02-23 07:48:19,120 DEBUG TRAIN Batch 15/7300 loss 10.677950 loss_att 11.566775 loss_ctc 11.345065 loss_rnnt 10.273350 hw_loss 0.258537 lr 0.00043452 rank 6
2023-02-23 07:48:19,121 DEBUG TRAIN Batch 15/7300 loss 13.715907 loss_att 15.572641 loss_ctc 20.732967 loss_rnnt 12.273239 hw_loss 0.254462 lr 0.00043448 rank 2
2023-02-23 07:48:19,123 DEBUG TRAIN Batch 15/7300 loss 11.681770 loss_att 11.926818 loss_ctc 15.656046 loss_rnnt 10.988629 hw_loss 0.214179 lr 0.00043452 rank 4
2023-02-23 07:48:19,124 DEBUG TRAIN Batch 15/7300 loss 12.302701 loss_att 20.263134 loss_ctc 23.307888 loss_rnnt 9.154347 hw_loss 0.166705 lr 0.00043442 rank 7
2023-02-23 07:48:19,126 DEBUG TRAIN Batch 15/7300 loss 8.570749 loss_att 11.953674 loss_ctc 12.396040 loss_rnnt 7.239113 hw_loss 0.271898 lr 0.00043451 rank 0
2023-02-23 07:48:19,128 DEBUG TRAIN Batch 15/7300 loss 9.224285 loss_att 15.240213 loss_ctc 12.028748 loss_rnnt 7.533554 hw_loss 0.213032 lr 0.00043454 rank 1
2023-02-23 07:48:19,166 DEBUG TRAIN Batch 15/7300 loss 13.368258 loss_att 14.885383 loss_ctc 17.436312 loss_rnnt 12.425589 hw_loss 0.181570 lr 0.00043450 rank 3
2023-02-23 07:49:35,555 DEBUG TRAIN Batch 15/7400 loss 10.415410 loss_att 11.877916 loss_ctc 9.557478 loss_rnnt 10.143921 hw_loss 0.175086 lr 0.00043435 rank 4
2023-02-23 07:49:35,555 DEBUG TRAIN Batch 15/7400 loss 8.591155 loss_att 11.889358 loss_ctc 9.777630 loss_rnnt 7.664206 hw_loss 0.204585 lr 0.00043434 rank 0
2023-02-23 07:49:35,556 DEBUG TRAIN Batch 15/7400 loss 13.030571 loss_att 16.373524 loss_ctc 16.907471 loss_rnnt 11.729342 hw_loss 0.216970 lr 0.00043435 rank 6
2023-02-23 07:49:35,558 DEBUG TRAIN Batch 15/7400 loss 6.208550 loss_att 7.879301 loss_ctc 8.397703 loss_rnnt 5.435979 hw_loss 0.274750 lr 0.00043425 rank 5
2023-02-23 07:49:35,559 DEBUG TRAIN Batch 15/7400 loss 10.402809 loss_att 14.273918 loss_ctc 12.976748 loss_rnnt 9.115738 hw_loss 0.318108 lr 0.00043433 rank 3
2023-02-23 07:49:35,560 DEBUG TRAIN Batch 15/7400 loss 11.933662 loss_att 17.803415 loss_ctc 16.254610 loss_rnnt 10.040057 hw_loss 0.269117 lr 0.00043432 rank 2
2023-02-23 07:49:35,575 DEBUG TRAIN Batch 15/7400 loss 21.042362 loss_att 22.550842 loss_ctc 26.028599 loss_rnnt 19.909452 hw_loss 0.311966 lr 0.00043438 rank 1
2023-02-23 07:49:35,613 DEBUG TRAIN Batch 15/7400 loss 15.429949 loss_att 21.327526 loss_ctc 20.127045 loss_rnnt 13.514296 hw_loss 0.205985 lr 0.00043426 rank 7
2023-02-23 07:50:54,071 DEBUG TRAIN Batch 15/7500 loss 13.595615 loss_att 13.495218 loss_ctc 16.597178 loss_rnnt 13.052188 hw_loss 0.306188 lr 0.00043417 rank 3
2023-02-23 07:50:54,072 DEBUG TRAIN Batch 15/7500 loss 7.290164 loss_att 12.460600 loss_ctc 13.400203 loss_rnnt 5.305297 hw_loss 0.255203 lr 0.00043419 rank 4
2023-02-23 07:50:54,073 DEBUG TRAIN Batch 15/7500 loss 15.875984 loss_att 17.685352 loss_ctc 20.220337 loss_rnnt 14.849753 hw_loss 0.159582 lr 0.00043409 rank 5
2023-02-23 07:50:54,073 DEBUG TRAIN Batch 15/7500 loss 7.817029 loss_att 10.417587 loss_ctc 12.491499 loss_rnnt 6.542870 hw_loss 0.245220 lr 0.00043418 rank 0
2023-02-23 07:50:54,074 DEBUG TRAIN Batch 15/7500 loss 12.346465 loss_att 15.631624 loss_ctc 16.757072 loss_rnnt 10.976287 hw_loss 0.234499 lr 0.00043415 rank 2
2023-02-23 07:50:54,080 DEBUG TRAIN Batch 15/7500 loss 13.358413 loss_att 14.818390 loss_ctc 15.662685 loss_rnnt 12.633754 hw_loss 0.235176 lr 0.00043410 rank 7
2023-02-23 07:50:54,087 DEBUG TRAIN Batch 15/7500 loss 10.395545 loss_att 11.165789 loss_ctc 13.778715 loss_rnnt 9.640896 hw_loss 0.280331 lr 0.00043419 rank 6
2023-02-23 07:50:54,088 DEBUG TRAIN Batch 15/7500 loss 15.672342 loss_att 19.146564 loss_ctc 20.589870 loss_rnnt 14.173040 hw_loss 0.278979 lr 0.00043421 rank 1
2023-02-23 07:52:09,596 DEBUG TRAIN Batch 15/7600 loss 21.987087 loss_att 23.170891 loss_ctc 27.950880 loss_rnnt 20.813093 hw_loss 0.266365 lr 0.00043392 rank 5
2023-02-23 07:52:09,599 DEBUG TRAIN Batch 15/7600 loss 11.234268 loss_att 12.074518 loss_ctc 14.723475 loss_rnnt 10.504426 hw_loss 0.181057 lr 0.00043399 rank 2
2023-02-23 07:52:09,603 DEBUG TRAIN Batch 15/7600 loss 14.172913 loss_att 16.808552 loss_ctc 21.640678 loss_rnnt 12.501540 hw_loss 0.278516 lr 0.00043403 rank 4
2023-02-23 07:52:09,606 DEBUG TRAIN Batch 15/7600 loss 16.680395 loss_att 18.259171 loss_ctc 17.369337 loss_rnnt 16.182547 hw_loss 0.169193 lr 0.00043401 rank 3
2023-02-23 07:52:09,607 DEBUG TRAIN Batch 15/7600 loss 13.204185 loss_att 14.176969 loss_ctc 18.641367 loss_rnnt 12.186996 hw_loss 0.183136 lr 0.00043401 rank 0
2023-02-23 07:52:09,609 DEBUG TRAIN Batch 15/7600 loss 7.619028 loss_att 10.141583 loss_ctc 8.601572 loss_rnnt 6.819608 hw_loss 0.307318 lr 0.00043405 rank 1
2023-02-23 07:52:09,610 DEBUG TRAIN Batch 15/7600 loss 13.659089 loss_att 18.930117 loss_ctc 20.432384 loss_rnnt 11.603389 hw_loss 0.184479 lr 0.00043403 rank 6
2023-02-23 07:52:09,655 DEBUG TRAIN Batch 15/7600 loss 8.966592 loss_att 11.409202 loss_ctc 12.587502 loss_rnnt 7.869808 hw_loss 0.235265 lr 0.00043393 rank 7
2023-02-23 07:53:25,117 DEBUG TRAIN Batch 15/7700 loss 16.585117 loss_att 21.172840 loss_ctc 24.059113 loss_rnnt 14.626478 hw_loss 0.083553 lr 0.00043384 rank 3
2023-02-23 07:53:25,120 DEBUG TRAIN Batch 15/7700 loss 12.417421 loss_att 12.384131 loss_ctc 14.209874 loss_rnnt 12.029853 hw_loss 0.291063 lr 0.00043383 rank 2
2023-02-23 07:53:25,122 DEBUG TRAIN Batch 15/7700 loss 7.327742 loss_att 10.932593 loss_ctc 10.491639 loss_rnnt 6.052310 hw_loss 0.248640 lr 0.00043386 rank 4
2023-02-23 07:53:25,123 DEBUG TRAIN Batch 15/7700 loss 19.702164 loss_att 20.749733 loss_ctc 25.927801 loss_rnnt 18.588936 hw_loss 0.138055 lr 0.00043376 rank 5
2023-02-23 07:53:25,125 DEBUG TRAIN Batch 15/7700 loss 11.825007 loss_att 13.797095 loss_ctc 18.895061 loss_rnnt 10.345970 hw_loss 0.266149 lr 0.00043385 rank 0
2023-02-23 07:53:25,126 DEBUG TRAIN Batch 15/7700 loss 8.511419 loss_att 11.483974 loss_ctc 10.398249 loss_rnnt 7.547098 hw_loss 0.221685 lr 0.00043386 rank 6
2023-02-23 07:53:25,127 DEBUG TRAIN Batch 15/7700 loss 8.917875 loss_att 8.370982 loss_ctc 11.075510 loss_rnnt 8.530464 hw_loss 0.392073 lr 0.00043377 rank 7
2023-02-23 07:53:25,131 DEBUG TRAIN Batch 15/7700 loss 17.198425 loss_att 15.550211 loss_ctc 24.538443 loss_rnnt 16.421843 hw_loss 0.239171 lr 0.00043389 rank 1
2023-02-23 07:54:42,282 DEBUG TRAIN Batch 15/7800 loss 10.976261 loss_att 12.709096 loss_ctc 15.913078 loss_rnnt 9.871431 hw_loss 0.187537 lr 0.00043370 rank 4
2023-02-23 07:54:42,283 DEBUG TRAIN Batch 15/7800 loss 8.068203 loss_att 10.813454 loss_ctc 10.365385 loss_rnnt 7.084732 hw_loss 0.240242 lr 0.00043366 rank 2
2023-02-23 07:54:42,286 DEBUG TRAIN Batch 15/7800 loss 10.742355 loss_att 13.203316 loss_ctc 14.356886 loss_rnnt 9.634562 hw_loss 0.250618 lr 0.00043361 rank 7
2023-02-23 07:54:42,287 DEBUG TRAIN Batch 15/7800 loss 5.933977 loss_att 10.670923 loss_ctc 9.921972 loss_rnnt 4.304924 hw_loss 0.281120 lr 0.00043360 rank 5
2023-02-23 07:54:42,288 DEBUG TRAIN Batch 15/7800 loss 14.449236 loss_att 17.009195 loss_ctc 23.528988 loss_rnnt 12.569209 hw_loss 0.295129 lr 0.00043368 rank 3
2023-02-23 07:54:42,288 DEBUG TRAIN Batch 15/7800 loss 13.532286 loss_att 14.053774 loss_ctc 18.999630 loss_rnnt 12.543341 hw_loss 0.291876 lr 0.00043370 rank 6
2023-02-23 07:54:42,288 DEBUG TRAIN Batch 15/7800 loss 11.243908 loss_att 14.460516 loss_ctc 16.485184 loss_rnnt 9.814125 hw_loss 0.164294 lr 0.00043369 rank 0
2023-02-23 07:54:42,288 DEBUG TRAIN Batch 15/7800 loss 11.356997 loss_att 13.497381 loss_ctc 16.679502 loss_rnnt 10.127499 hw_loss 0.172038 lr 0.00043372 rank 1
2023-02-23 07:55:59,407 DEBUG TRAIN Batch 15/7900 loss 10.116646 loss_att 12.383191 loss_ctc 11.587733 loss_rnnt 9.329399 hw_loss 0.258359 lr 0.00043350 rank 2
2023-02-23 07:55:59,413 DEBUG TRAIN Batch 15/7900 loss 5.530656 loss_att 9.543201 loss_ctc 8.323484 loss_rnnt 4.252738 hw_loss 0.193185 lr 0.00043343 rank 5
2023-02-23 07:55:59,415 DEBUG TRAIN Batch 15/7900 loss 10.389760 loss_att 14.163736 loss_ctc 13.699009 loss_rnnt 9.067596 hw_loss 0.236503 lr 0.00043352 rank 3
2023-02-23 07:55:59,416 DEBUG TRAIN Batch 15/7900 loss 11.114283 loss_att 14.807495 loss_ctc 16.509624 loss_rnnt 9.572685 hw_loss 0.156704 lr 0.00043354 rank 4
2023-02-23 07:55:59,417 DEBUG TRAIN Batch 15/7900 loss 8.067361 loss_att 12.533888 loss_ctc 11.997316 loss_rnnt 6.533630 hw_loss 0.218307 lr 0.00043352 rank 0
2023-02-23 07:55:59,418 DEBUG TRAIN Batch 15/7900 loss 5.803521 loss_att 8.663640 loss_ctc 6.078500 loss_rnnt 5.074563 hw_loss 0.225508 lr 0.00043354 rank 6
2023-02-23 07:55:59,419 DEBUG TRAIN Batch 15/7900 loss 5.855828 loss_att 9.124440 loss_ctc 7.315280 loss_rnnt 4.918064 hw_loss 0.167716 lr 0.00043356 rank 1
2023-02-23 07:55:59,467 DEBUG TRAIN Batch 15/7900 loss 10.469713 loss_att 11.410057 loss_ctc 11.943226 loss_rnnt 9.986418 hw_loss 0.185172 lr 0.00043344 rank 7
2023-02-23 07:57:14,996 DEBUG TRAIN Batch 15/8000 loss 4.395572 loss_att 6.863070 loss_ctc 5.605358 loss_rnnt 3.634267 hw_loss 0.199689 lr 0.00043337 rank 4
2023-02-23 07:57:14,999 DEBUG TRAIN Batch 15/8000 loss 12.698176 loss_att 13.704529 loss_ctc 17.697496 loss_rnnt 11.698704 hw_loss 0.246799 lr 0.00043337 rank 6
2023-02-23 07:57:15,000 DEBUG TRAIN Batch 15/8000 loss 6.380339 loss_att 8.418789 loss_ctc 9.623523 loss_rnnt 5.438185 hw_loss 0.191324 lr 0.00043327 rank 5
2023-02-23 07:57:15,004 DEBUG TRAIN Batch 15/8000 loss 9.857894 loss_att 18.411222 loss_ctc 15.996265 loss_rnnt 7.200046 hw_loss 0.241375 lr 0.00043334 rank 2
2023-02-23 07:57:15,006 DEBUG TRAIN Batch 15/8000 loss 8.268689 loss_att 10.750673 loss_ctc 13.263210 loss_rnnt 7.010894 hw_loss 0.178994 lr 0.00043340 rank 1
2023-02-23 07:57:15,006 DEBUG TRAIN Batch 15/8000 loss 16.891632 loss_att 22.373913 loss_ctc 23.001270 loss_rnnt 14.857995 hw_loss 0.229802 lr 0.00043336 rank 0
2023-02-23 07:57:15,008 DEBUG TRAIN Batch 15/8000 loss 14.821183 loss_att 16.689260 loss_ctc 22.306553 loss_rnnt 13.309523 hw_loss 0.262493 lr 0.00043328 rank 7
2023-02-23 07:57:15,053 DEBUG TRAIN Batch 15/8000 loss 13.659594 loss_att 16.454334 loss_ctc 15.529255 loss_rnnt 12.741295 hw_loss 0.206367 lr 0.00043335 rank 3
2023-02-23 07:58:30,783 DEBUG TRAIN Batch 15/8100 loss 7.704681 loss_att 10.441223 loss_ctc 10.944071 loss_rnnt 6.593690 hw_loss 0.247057 lr 0.00043311 rank 5
2023-02-23 07:58:30,782 DEBUG TRAIN Batch 15/8100 loss 8.044563 loss_att 12.346792 loss_ctc 13.763266 loss_rnnt 6.281989 hw_loss 0.261816 lr 0.00043312 rank 7
2023-02-23 07:58:30,783 DEBUG TRAIN Batch 15/8100 loss 7.712844 loss_att 10.234945 loss_ctc 20.249603 loss_rnnt 5.408484 hw_loss 0.240698 lr 0.00043317 rank 2
2023-02-23 07:58:30,786 DEBUG TRAIN Batch 15/8100 loss 12.698327 loss_att 13.592451 loss_ctc 17.618584 loss_rnnt 11.722820 hw_loss 0.263715 lr 0.00043319 rank 3
2023-02-23 07:58:30,787 DEBUG TRAIN Batch 15/8100 loss 13.763599 loss_att 16.690844 loss_ctc 17.021500 loss_rnnt 12.587664 hw_loss 0.292686 lr 0.00043324 rank 1
2023-02-23 07:58:30,789 DEBUG TRAIN Batch 15/8100 loss 12.501056 loss_att 16.660688 loss_ctc 14.877086 loss_rnnt 11.227020 hw_loss 0.234945 lr 0.00043320 rank 0
2023-02-23 07:58:30,791 DEBUG TRAIN Batch 15/8100 loss 11.075517 loss_att 12.377627 loss_ctc 13.845286 loss_rnnt 10.287551 hw_loss 0.296705 lr 0.00043321 rank 6
2023-02-23 07:58:30,820 DEBUG TRAIN Batch 15/8100 loss 14.086971 loss_att 17.807564 loss_ctc 17.662418 loss_rnnt 12.746347 hw_loss 0.224584 lr 0.00043321 rank 4
2023-02-23 07:59:48,160 DEBUG TRAIN Batch 15/8200 loss 16.298712 loss_att 18.892321 loss_ctc 20.113657 loss_rnnt 15.147071 hw_loss 0.232988 lr 0.00043295 rank 5
2023-02-23 07:59:48,160 DEBUG TRAIN Batch 15/8200 loss 10.306035 loss_att 13.328360 loss_ctc 12.890507 loss_rnnt 9.181258 hw_loss 0.329466 lr 0.00043301 rank 2
2023-02-23 07:59:48,162 DEBUG TRAIN Batch 15/8200 loss 8.858401 loss_att 9.072248 loss_ctc 11.806523 loss_rnnt 8.256569 hw_loss 0.311213 lr 0.00043305 rank 4
2023-02-23 07:59:48,166 DEBUG TRAIN Batch 15/8200 loss 14.041792 loss_att 18.283846 loss_ctc 16.902950 loss_rnnt 12.639506 hw_loss 0.323224 lr 0.00043304 rank 0
2023-02-23 07:59:48,167 DEBUG TRAIN Batch 15/8200 loss 8.759080 loss_att 9.066025 loss_ctc 12.073373 loss_rnnt 8.094863 hw_loss 0.301731 lr 0.00043303 rank 3
2023-02-23 07:59:48,168 DEBUG TRAIN Batch 15/8200 loss 8.583483 loss_att 7.843092 loss_ctc 11.254275 loss_rnnt 8.281878 hw_loss 0.175456 lr 0.00043305 rank 6
2023-02-23 07:59:48,169 DEBUG TRAIN Batch 15/8200 loss 3.913400 loss_att 7.227213 loss_ctc 7.216993 loss_rnnt 2.668943 hw_loss 0.264777 lr 0.00043296 rank 7
2023-02-23 07:59:48,170 DEBUG TRAIN Batch 15/8200 loss 10.306816 loss_att 11.458828 loss_ctc 13.299041 loss_rnnt 9.589605 hw_loss 0.164708 lr 0.00043307 rank 1
2023-02-23 08:01:05,139 DEBUG TRAIN Batch 15/8300 loss 6.405271 loss_att 8.302998 loss_ctc 8.274854 loss_rnnt 5.656936 hw_loss 0.224085 lr 0.00043285 rank 2
2023-02-23 08:01:05,139 DEBUG TRAIN Batch 15/8300 loss 16.329489 loss_att 20.810495 loss_ctc 22.354124 loss_rnnt 14.480698 hw_loss 0.279947 lr 0.00043278 rank 5
2023-02-23 08:01:05,141 DEBUG TRAIN Batch 15/8300 loss 13.960919 loss_att 16.046352 loss_ctc 25.614784 loss_rnnt 11.886414 hw_loss 0.194195 lr 0.00043287 rank 0
2023-02-23 08:01:05,144 DEBUG TRAIN Batch 15/8300 loss 12.873453 loss_att 13.417091 loss_ctc 15.878902 loss_rnnt 12.253074 hw_loss 0.207984 lr 0.00043289 rank 6
2023-02-23 08:01:05,145 DEBUG TRAIN Batch 15/8300 loss 14.750821 loss_att 16.223444 loss_ctc 29.146444 loss_rnnt 12.430817 hw_loss 0.198871 lr 0.00043287 rank 3
2023-02-23 08:01:05,145 DEBUG TRAIN Batch 15/8300 loss 17.643866 loss_att 24.395176 loss_ctc 23.573729 loss_rnnt 15.395973 hw_loss 0.200592 lr 0.00043291 rank 1
2023-02-23 08:01:05,147 DEBUG TRAIN Batch 15/8300 loss 6.091304 loss_att 8.248171 loss_ctc 6.467522 loss_rnnt 5.467737 hw_loss 0.266308 lr 0.00043289 rank 4
2023-02-23 08:01:05,194 DEBUG TRAIN Batch 15/8300 loss 11.950694 loss_att 13.934301 loss_ctc 15.317174 loss_rnnt 10.984003 hw_loss 0.227071 lr 0.00043279 rank 7
2023-02-23 08:01:55,180 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 0
2023-02-23 08:01:55,181 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 4
2023-02-23 08:01:55,181 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 2
2023-02-23 08:01:55,186 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 7
2023-02-23 08:01:55,193 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 6
2023-02-23 08:01:55,195 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 3
2023-02-23 08:01:55,205 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 1
2023-02-23 08:01:55,207 DEBUG CV Batch 15/0 loss 1.783429 loss_att 1.935635 loss_ctc 2.120915 loss_rnnt 1.549896 hw_loss 0.296425 history loss 1.717376 rank 5
2023-02-23 08:02:06,195 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 2
2023-02-23 08:02:06,311 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 5
2023-02-23 08:02:06,415 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 0
2023-02-23 08:02:06,423 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 4
2023-02-23 08:02:06,430 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 1
2023-02-23 08:02:06,650 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 7
2023-02-23 08:02:06,858 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 3
2023-02-23 08:02:07,387 DEBUG CV Batch 15/100 loss 9.211569 loss_att 8.842046 loss_ctc 12.040850 loss_rnnt 8.736221 hw_loss 0.322527 history loss 3.884249 rank 6
2023-02-23 08:02:20,149 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 2
2023-02-23 08:02:20,175 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 0
2023-02-23 08:02:20,187 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 4
2023-02-23 08:02:20,266 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 5
2023-02-23 08:02:20,464 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 1
2023-02-23 08:02:20,630 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 7
2023-02-23 08:02:20,749 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 3
2023-02-23 08:02:21,046 DEBUG CV Batch 15/200 loss 7.175136 loss_att 16.150597 loss_ctc 8.289063 loss_rnnt 5.126154 hw_loss 0.197561 history loss 4.470521 rank 6
2023-02-23 08:02:32,097 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 2
2023-02-23 08:02:32,135 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 5
2023-02-23 08:02:32,233 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 0
2023-02-23 08:02:32,306 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 4
2023-02-23 08:02:32,696 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 1
2023-02-23 08:02:32,831 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 3
2023-02-23 08:02:32,896 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 7
2023-02-23 08:02:33,283 DEBUG CV Batch 15/300 loss 5.070611 loss_att 4.895385 loss_ctc 7.093775 loss_rnnt 4.664793 hw_loss 0.320826 history loss 4.614057 rank 6
2023-02-23 08:02:43,955 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 2
2023-02-23 08:02:43,986 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 5
2023-02-23 08:02:44,242 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 0
2023-02-23 08:02:44,249 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 4
2023-02-23 08:02:44,859 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 3
2023-02-23 08:02:44,865 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 1
2023-02-23 08:02:45,133 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 7
2023-02-23 08:02:46,273 DEBUG CV Batch 15/400 loss 17.287640 loss_att 74.155205 loss_ctc 10.312945 loss_rnnt 6.770169 hw_loss 0.138594 history loss 5.661704 rank 6
2023-02-23 08:02:54,409 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 2
2023-02-23 08:02:54,444 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 5
2023-02-23 08:02:54,738 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 0
2023-02-23 08:02:54,781 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 4
2023-02-23 08:02:55,514 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 1
2023-02-23 08:02:55,806 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 7
2023-02-23 08:02:56,183 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 3
2023-02-23 08:02:56,910 DEBUG CV Batch 15/500 loss 5.832705 loss_att 6.515674 loss_ctc 8.165482 loss_rnnt 5.273675 hw_loss 0.208873 history loss 6.513762 rank 6
2023-02-23 08:03:06,477 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 2
2023-02-23 08:03:06,591 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 5
2023-02-23 08:03:06,822 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 4
2023-02-23 08:03:06,846 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 0
2023-02-23 08:03:07,638 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 1
2023-02-23 08:03:08,034 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 7
2023-02-23 08:03:08,448 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 3
2023-02-23 08:03:09,481 DEBUG CV Batch 15/600 loss 8.948638 loss_att 8.669420 loss_ctc 11.242416 loss_rnnt 8.505904 hw_loss 0.361389 history loss 7.554526 rank 6
2023-02-23 08:03:18,451 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 0
2023-02-23 08:03:18,749 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 5
2023-02-23 08:03:18,800 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 2
2023-02-23 08:03:19,039 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 4
2023-02-23 08:03:19,388 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 1
2023-02-23 08:03:19,639 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 7
2023-02-23 08:03:19,736 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 3
2023-02-23 08:03:21,469 DEBUG CV Batch 15/700 loss 16.802071 loss_att 51.107689 loss_ctc 16.005560 loss_rnnt 9.949824 hw_loss 0.182479 history loss 8.262072 rank 6
2023-02-23 08:03:30,235 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 0
2023-02-23 08:03:30,893 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 4
2023-02-23 08:03:30,944 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 2
2023-02-23 08:03:31,002 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 5
2023-02-23 08:03:31,636 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 3
2023-02-23 08:03:31,719 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 1
2023-02-23 08:03:31,766 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 7
2023-02-23 08:03:32,982 DEBUG CV Batch 15/800 loss 14.268144 loss_att 12.897032 loss_ctc 18.260513 loss_rnnt 13.811145 hw_loss 0.372946 history loss 7.671753 rank 6
2023-02-23 08:03:44,186 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 0
2023-02-23 08:03:44,754 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 2
2023-02-23 08:03:44,814 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 5
2023-02-23 08:03:44,904 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 4
2023-02-23 08:03:45,665 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 3
2023-02-23 08:03:45,853 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 7
2023-02-23 08:03:45,901 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 1
2023-02-23 08:03:46,494 DEBUG CV Batch 15/900 loss 16.008202 loss_att 17.862946 loss_ctc 24.781178 loss_rnnt 14.394032 hw_loss 0.137798 history loss 7.444654 rank 6
2023-02-23 08:03:56,394 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 0
2023-02-23 08:03:56,899 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 5
2023-02-23 08:03:56,948 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 2
2023-02-23 08:03:57,105 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 4
2023-02-23 08:03:57,894 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 3
2023-02-23 08:03:58,261 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 1
2023-02-23 08:03:58,316 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 7
2023-02-23 08:03:58,924 DEBUG CV Batch 15/1000 loss 4.555541 loss_att 5.005433 loss_ctc 5.065670 loss_rnnt 4.238347 hw_loss 0.298496 history loss 7.187467 rank 6
2023-02-23 08:04:08,379 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 0
2023-02-23 08:04:08,680 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 5
2023-02-23 08:04:08,782 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 2
2023-02-23 08:04:08,943 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 4
2023-02-23 08:04:09,771 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 3
2023-02-23 08:04:10,272 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 1
2023-02-23 08:04:10,411 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 7
2023-02-23 08:04:11,146 DEBUG CV Batch 15/1100 loss 7.010369 loss_att 6.092568 loss_ctc 8.881832 loss_rnnt 6.760667 hw_loss 0.344502 history loss 7.166055 rank 6
2023-02-23 08:04:18,820 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 0
2023-02-23 08:04:19,135 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 5
2023-02-23 08:04:19,286 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 2
2023-02-23 08:04:19,368 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 4
2023-02-23 08:04:20,350 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 3
2023-02-23 08:04:20,927 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 1
2023-02-23 08:04:21,240 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 7
2023-02-23 08:04:22,470 DEBUG CV Batch 15/1200 loss 11.793333 loss_att 11.181552 loss_ctc 14.062400 loss_rnnt 11.466357 hw_loss 0.275231 history loss 7.533387 rank 6
2023-02-23 08:04:30,776 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 0
2023-02-23 08:04:31,005 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 5
2023-02-23 08:04:31,143 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 2
2023-02-23 08:04:31,413 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 4
2023-02-23 08:04:32,379 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 3
2023-02-23 08:04:33,003 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 1
2023-02-23 08:04:33,326 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 7
2023-02-23 08:04:34,521 DEBUG CV Batch 15/1300 loss 7.093732 loss_att 6.382760 loss_ctc 8.937390 loss_rnnt 6.842217 hw_loss 0.277289 history loss 7.880868 rank 6
2023-02-23 08:04:41,997 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 0
2023-02-23 08:04:42,362 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 5
2023-02-23 08:04:42,796 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 4
2023-02-23 08:04:43,088 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 2
2023-02-23 08:04:44,255 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 3
2023-02-23 08:04:44,759 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 7
2023-02-23 08:04:44,799 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 1
2023-02-23 08:04:45,872 DEBUG CV Batch 15/1400 loss 7.196631 loss_att 33.282299 loss_ctc 3.822887 loss_rnnt 2.312952 hw_loss 0.218208 history loss 8.220349 rank 6
2023-02-23 08:04:53,929 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 0
2023-02-23 08:04:54,847 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 5
2023-02-23 08:04:55,193 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 4
2023-02-23 08:04:55,489 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 2
2023-02-23 08:04:56,477 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 3
2023-02-23 08:04:57,112 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 7
2023-02-23 08:04:57,369 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 6
2023-02-23 08:04:57,459 DEBUG CV Batch 15/1500 loss 8.716013 loss_att 9.162996 loss_ctc 7.894608 loss_rnnt 8.615546 hw_loss 0.226110 history loss 8.016023 rank 1
2023-02-23 08:05:07,659 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 0
2023-02-23 08:05:08,518 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 5
2023-02-23 08:05:08,828 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 4
2023-02-23 08:05:09,271 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 2
2023-02-23 08:05:10,145 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 3
2023-02-23 08:05:10,954 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 6
2023-02-23 08:05:10,981 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 7
2023-02-23 08:05:11,648 DEBUG CV Batch 15/1600 loss 9.948238 loss_att 13.072963 loss_ctc 9.400603 loss_rnnt 9.289506 hw_loss 0.200261 history loss 7.922578 rank 1
2023-02-23 08:05:20,201 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 0
2023-02-23 08:05:20,997 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 5
2023-02-23 08:05:21,244 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 4
2023-02-23 08:05:21,732 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 2
2023-02-23 08:05:22,642 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 3
2023-02-23 08:05:23,671 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 6
2023-02-23 08:05:23,734 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 7
2023-02-23 08:05:24,317 DEBUG CV Batch 15/1700 loss 9.242393 loss_att 9.019564 loss_ctc 13.554323 loss_rnnt 8.583574 hw_loss 0.240863 history loss 7.801665 rank 1
2023-02-23 08:05:29,515 INFO Epoch 15 CV info cv_loss 7.750860376177728
2023-02-23 08:05:29,516 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/15.pt
2023-02-23 08:05:30,088 INFO Epoch 15 CV info cv_loss 7.7508603744138815
2023-02-23 08:05:30,089 INFO Epoch 16 TRAIN info lr 0.0004327293569285361
2023-02-23 08:05:30,092 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:30,190 INFO Epoch 16 TRAIN info lr 0.0004328314916380524
2023-02-23 08:05:30,194 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:30,362 INFO Epoch 15 CV info cv_loss 7.750860374211438
2023-02-23 08:05:30,363 INFO Epoch 16 TRAIN info lr 0.00043281365338229596
2023-02-23 08:05:30,367 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:30,868 INFO Epoch 15 CV info cv_loss 7.75086037616696
2023-02-23 08:05:30,869 INFO Epoch 16 TRAIN info lr 0.00043281365338229596
2023-02-23 08:05:30,873 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:31,868 INFO Epoch 15 CV info cv_loss 7.750860375456252
2023-02-23 08:05:31,869 INFO Epoch 16 TRAIN info lr 0.00043278284704371234
2023-02-23 08:05:31,872 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:32,987 INFO Epoch 15 CV info cv_loss 7.750860374633555
2023-02-23 08:05:32,988 INFO Epoch 16 TRAIN info lr 0.0004327569099653492
2023-02-23 08:05:32,993 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:33,046 INFO Epoch 15 CV info cv_loss 7.750860376998272
2023-02-23 08:05:33,047 INFO Epoch 16 TRAIN info lr 0.00043279419598206664
2023-02-23 08:05:33,050 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:05:34,525 INFO Epoch 15 CV info cv_loss 7.750860375787916
2023-02-23 08:05:34,526 INFO Epoch 16 TRAIN info lr 0.0004328396006652773
2023-02-23 08:05:34,529 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 08:06:47,490 DEBUG TRAIN Batch 16/0 loss 9.906919 loss_att 8.275547 loss_ctc 11.218379 loss_rnnt 9.905461 hw_loss 0.286635 lr 0.00043281 rank 4
2023-02-23 08:06:47,491 DEBUG TRAIN Batch 16/0 loss 9.811644 loss_att 9.125603 loss_ctc 11.988240 loss_rnnt 9.482178 hw_loss 0.330867 lr 0.00043283 rank 0
2023-02-23 08:06:47,492 DEBUG TRAIN Batch 16/0 loss 12.339503 loss_att 10.743698 loss_ctc 14.872725 loss_rnnt 12.169135 hw_loss 0.284562 lr 0.00043278 rank 3
2023-02-23 08:06:47,495 DEBUG TRAIN Batch 16/0 loss 11.818642 loss_att 10.822775 loss_ctc 13.150334 loss_rnnt 11.666165 hw_loss 0.326421 lr 0.00043273 rank 5
2023-02-23 08:06:47,497 DEBUG TRAIN Batch 16/0 loss 10.788291 loss_att 9.801564 loss_ctc 12.139319 loss_rnnt 10.599416 hw_loss 0.386405 lr 0.00043281 rank 2
2023-02-23 08:06:47,508 DEBUG TRAIN Batch 16/0 loss 10.614483 loss_att 10.248135 loss_ctc 12.446010 loss_rnnt 10.288474 hw_loss 0.290765 lr 0.00043284 rank 1
2023-02-23 08:06:47,515 DEBUG TRAIN Batch 16/0 loss 9.382300 loss_att 8.431355 loss_ctc 10.947333 loss_rnnt 9.174035 hw_loss 0.355842 lr 0.00043276 rank 7
2023-02-23 08:06:47,529 DEBUG TRAIN Batch 16/0 loss 5.360107 loss_att 5.681190 loss_ctc 7.623323 loss_rnnt 4.822047 hw_loss 0.322653 lr 0.00043279 rank 6
2023-02-23 08:08:02,361 DEBUG TRAIN Batch 16/100 loss 10.087154 loss_att 13.575216 loss_ctc 14.800841 loss_rnnt 8.702435 hw_loss 0.109903 lr 0.00043265 rank 2
2023-02-23 08:08:02,368 DEBUG TRAIN Batch 16/100 loss 19.125977 loss_att 18.516258 loss_ctc 26.845551 loss_rnnt 18.074234 hw_loss 0.270768 lr 0.00043267 rank 0
2023-02-23 08:08:02,369 DEBUG TRAIN Batch 16/100 loss 11.591069 loss_att 16.466246 loss_ctc 13.595152 loss_rnnt 10.208102 hw_loss 0.263850 lr 0.00043262 rank 3
2023-02-23 08:08:02,369 DEBUG TRAIN Batch 16/100 loss 10.709390 loss_att 13.281260 loss_ctc 11.566683 loss_rnnt 9.943843 hw_loss 0.256624 lr 0.00043265 rank 4
2023-02-23 08:08:02,370 DEBUG TRAIN Batch 16/100 loss 7.950785 loss_att 9.454107 loss_ctc 7.759173 loss_rnnt 7.559685 hw_loss 0.217470 lr 0.00043257 rank 5
2023-02-23 08:08:02,371 DEBUG TRAIN Batch 16/100 loss 7.478036 loss_att 10.875647 loss_ctc 11.370170 loss_rnnt 6.177230 hw_loss 0.191874 lr 0.00043268 rank 1
2023-02-23 08:08:02,371 DEBUG TRAIN Batch 16/100 loss 9.034170 loss_att 15.133709 loss_ctc 10.461236 loss_rnnt 7.500142 hw_loss 0.232208 lr 0.00043263 rank 6
2023-02-23 08:08:02,420 DEBUG TRAIN Batch 16/100 loss 6.696073 loss_att 10.149390 loss_ctc 10.099937 loss_rnnt 5.438796 hw_loss 0.211435 lr 0.00043259 rank 7
2023-02-23 08:09:19,356 DEBUG TRAIN Batch 16/200 loss 9.303809 loss_att 15.184219 loss_ctc 14.142097 loss_rnnt 7.417026 hw_loss 0.122993 lr 0.00043249 rank 2
2023-02-23 08:09:19,358 DEBUG TRAIN Batch 16/200 loss 6.240119 loss_att 11.696606 loss_ctc 9.645781 loss_rnnt 4.602047 hw_loss 0.173785 lr 0.00043246 rank 3
2023-02-23 08:09:19,362 DEBUG TRAIN Batch 16/200 loss 14.027785 loss_att 17.963829 loss_ctc 18.985996 loss_rnnt 12.496341 hw_loss 0.155890 lr 0.00043249 rank 4
2023-02-23 08:09:19,362 DEBUG TRAIN Batch 16/200 loss 6.184553 loss_att 9.351191 loss_ctc 7.818208 loss_rnnt 5.207805 hw_loss 0.235499 lr 0.00043251 rank 0
2023-02-23 08:09:19,363 DEBUG TRAIN Batch 16/200 loss 8.326022 loss_att 12.772714 loss_ctc 11.057741 loss_rnnt 6.953487 hw_loss 0.223064 lr 0.00043240 rank 5
2023-02-23 08:09:19,367 DEBUG TRAIN Batch 16/200 loss 8.255262 loss_att 9.478753 loss_ctc 8.828730 loss_rnnt 7.863137 hw_loss 0.133057 lr 0.00043247 rank 6
2023-02-23 08:09:19,370 DEBUG TRAIN Batch 16/200 loss 5.059878 loss_att 9.999395 loss_ctc 8.805007 loss_rnnt 3.430707 hw_loss 0.266094 lr 0.00043243 rank 7
2023-02-23 08:09:19,369 DEBUG TRAIN Batch 16/200 loss 12.964299 loss_att 15.327821 loss_ctc 17.200377 loss_rnnt 11.815748 hw_loss 0.208193 lr 0.00043251 rank 1
2023-02-23 08:10:36,282 DEBUG TRAIN Batch 16/300 loss 10.990562 loss_att 15.387963 loss_ctc 14.157295 loss_rnnt 9.539401 hw_loss 0.280217 lr 0.00043224 rank 5
2023-02-23 08:10:36,289 DEBUG TRAIN Batch 16/300 loss 8.036677 loss_att 11.760856 loss_ctc 12.024575 loss_rnnt 6.647551 hw_loss 0.211072 lr 0.00043231 rank 6
2023-02-23 08:10:36,289 DEBUG TRAIN Batch 16/300 loss 10.796751 loss_att 12.461628 loss_ctc 13.089336 loss_rnnt 10.009268 hw_loss 0.279056 lr 0.00043234 rank 0
2023-02-23 08:10:36,292 DEBUG TRAIN Batch 16/300 loss 10.161893 loss_att 16.134951 loss_ctc 9.801807 loss_rnnt 8.930877 hw_loss 0.158279 lr 0.00043230 rank 3
2023-02-23 08:10:36,294 DEBUG TRAIN Batch 16/300 loss 18.032610 loss_att 17.827980 loss_ctc 22.603830 loss_rnnt 17.351166 hw_loss 0.211634 lr 0.00043235 rank 1
2023-02-23 08:10:36,293 DEBUG TRAIN Batch 16/300 loss 7.883846 loss_att 13.359264 loss_ctc 7.670732 loss_rnnt 6.674012 hw_loss 0.268436 lr 0.00043233 rank 4
2023-02-23 08:10:36,293 DEBUG TRAIN Batch 16/300 loss 12.066532 loss_att 14.696486 loss_ctc 15.720077 loss_rnnt 10.889221 hw_loss 0.307837 lr 0.00043233 rank 2
2023-02-23 08:10:36,298 DEBUG TRAIN Batch 16/300 loss 12.049038 loss_att 16.155281 loss_ctc 19.357662 loss_rnnt 10.122547 hw_loss 0.245173 lr 0.00043227 rank 7
2023-02-23 08:11:53,849 DEBUG TRAIN Batch 16/400 loss 11.114534 loss_att 12.059514 loss_ctc 11.583223 loss_rnnt 10.739981 hw_loss 0.230748 lr 0.00043208 rank 5
2023-02-23 08:11:53,850 DEBUG TRAIN Batch 16/400 loss 4.490716 loss_att 7.453817 loss_ctc 7.692887 loss_rnnt 3.348283 hw_loss 0.230356 lr 0.00043216 rank 2
2023-02-23 08:11:53,853 DEBUG TRAIN Batch 16/400 loss 8.931438 loss_att 11.279205 loss_ctc 10.479066 loss_rnnt 8.093163 hw_loss 0.304447 lr 0.00043219 rank 1
2023-02-23 08:11:53,854 DEBUG TRAIN Batch 16/400 loss 7.290069 loss_att 9.754373 loss_ctc 8.699162 loss_rnnt 6.471962 hw_loss 0.257564 lr 0.00043215 rank 6
2023-02-23 08:11:53,856 DEBUG TRAIN Batch 16/400 loss 8.826829 loss_att 15.713966 loss_ctc 15.013165 loss_rnnt 6.495149 hw_loss 0.242638 lr 0.00043216 rank 4
2023-02-23 08:11:53,858 DEBUG TRAIN Batch 16/400 loss 12.349523 loss_att 15.282556 loss_ctc 18.465403 loss_rnnt 10.799968 hw_loss 0.276559 lr 0.00043218 rank 0
2023-02-23 08:11:53,859 DEBUG TRAIN Batch 16/400 loss 10.471259 loss_att 10.374005 loss_ctc 11.369389 loss_rnnt 10.251917 hw_loss 0.223206 lr 0.00043211 rank 7
2023-02-23 08:11:53,905 DEBUG TRAIN Batch 16/400 loss 18.130419 loss_att 20.954233 loss_ctc 26.221794 loss_rnnt 16.356138 hw_loss 0.245005 lr 0.00043213 rank 3
2023-02-23 08:13:09,479 DEBUG TRAIN Batch 16/500 loss 16.539030 loss_att 18.939156 loss_ctc 20.061989 loss_rnnt 15.467619 hw_loss 0.228110 lr 0.00043192 rank 5
2023-02-23 08:13:09,480 DEBUG TRAIN Batch 16/500 loss 9.925392 loss_att 14.796179 loss_ctc 14.894460 loss_rnnt 8.173023 hw_loss 0.216879 lr 0.00043200 rank 2
2023-02-23 08:13:09,484 DEBUG TRAIN Batch 16/500 loss 16.852808 loss_att 17.406851 loss_ctc 19.229298 loss_rnnt 16.346596 hw_loss 0.147259 lr 0.00043203 rank 1
2023-02-23 08:13:09,485 DEBUG TRAIN Batch 16/500 loss 11.704196 loss_att 14.179016 loss_ctc 14.997268 loss_rnnt 10.677999 hw_loss 0.172794 lr 0.00043198 rank 6
2023-02-23 08:13:09,486 DEBUG TRAIN Batch 16/500 loss 17.015347 loss_att 17.839842 loss_ctc 20.270992 loss_rnnt 16.296320 hw_loss 0.225077 lr 0.00043197 rank 3
2023-02-23 08:13:09,487 DEBUG TRAIN Batch 16/500 loss 11.068536 loss_att 13.933552 loss_ctc 15.705870 loss_rnnt 9.758572 hw_loss 0.222469 lr 0.00043200 rank 4
2023-02-23 08:13:09,489 DEBUG TRAIN Batch 16/500 loss 8.816790 loss_att 11.574032 loss_ctc 13.286909 loss_rnnt 7.538011 hw_loss 0.246215 lr 0.00043202 rank 0
2023-02-23 08:13:09,491 DEBUG TRAIN Batch 16/500 loss 14.620099 loss_att 16.247417 loss_ctc 18.916224 loss_rnnt 13.557743 hw_loss 0.307641 lr 0.00043195 rank 7
2023-02-23 08:14:25,622 DEBUG TRAIN Batch 16/600 loss 12.657465 loss_att 13.769624 loss_ctc 18.323267 loss_rnnt 11.517615 hw_loss 0.303708 lr 0.00043186 rank 0
2023-02-23 08:14:25,623 DEBUG TRAIN Batch 16/600 loss 5.042937 loss_att 7.096245 loss_ctc 6.273243 loss_rnnt 4.333562 hw_loss 0.252512 lr 0.00043184 rank 2
2023-02-23 08:14:25,626 DEBUG TRAIN Batch 16/600 loss 12.350886 loss_att 13.411251 loss_ctc 14.240521 loss_rnnt 11.726074 hw_loss 0.301477 lr 0.00043181 rank 3
2023-02-23 08:14:25,626 DEBUG TRAIN Batch 16/600 loss 17.674393 loss_att 17.706850 loss_ctc 23.229898 loss_rnnt 16.771721 hw_loss 0.291464 lr 0.00043187 rank 1
2023-02-23 08:14:25,629 DEBUG TRAIN Batch 16/600 loss 10.686752 loss_att 12.570530 loss_ctc 15.841446 loss_rnnt 9.505206 hw_loss 0.220308 lr 0.00043184 rank 4
2023-02-23 08:14:25,631 DEBUG TRAIN Batch 16/600 loss 9.810829 loss_att 9.904419 loss_ctc 11.512143 loss_rnnt 9.378672 hw_loss 0.349870 lr 0.00043176 rank 5
2023-02-23 08:14:25,631 DEBUG TRAIN Batch 16/600 loss 7.302350 loss_att 7.380602 loss_ctc 10.050899 loss_rnnt 6.777009 hw_loss 0.268533 lr 0.00043179 rank 7
2023-02-23 08:14:25,676 DEBUG TRAIN Batch 16/600 loss 12.459907 loss_att 12.317309 loss_ctc 17.303680 loss_rnnt 11.648490 hw_loss 0.363938 lr 0.00043182 rank 6
2023-02-23 08:15:44,705 DEBUG TRAIN Batch 16/700 loss 16.386875 loss_att 20.333834 loss_ctc 24.184269 loss_rnnt 14.432875 hw_loss 0.234295 lr 0.00043168 rank 2
2023-02-23 08:15:44,705 DEBUG TRAIN Batch 16/700 loss 7.449326 loss_att 10.116330 loss_ctc 11.033865 loss_rnnt 6.325107 hw_loss 0.211650 lr 0.00043170 rank 0
2023-02-23 08:15:44,708 DEBUG TRAIN Batch 16/700 loss 7.714539 loss_att 12.693975 loss_ctc 12.459029 loss_rnnt 6.013916 hw_loss 0.135256 lr 0.00043160 rank 5
2023-02-23 08:15:44,709 DEBUG TRAIN Batch 16/700 loss 6.460453 loss_att 12.128222 loss_ctc 8.374100 loss_rnnt 4.901687 hw_loss 0.318861 lr 0.00043165 rank 3
2023-02-23 08:15:44,714 DEBUG TRAIN Batch 16/700 loss 7.696577 loss_att 10.174404 loss_ctc 9.631482 loss_rnnt 6.844155 hw_loss 0.185379 lr 0.00043163 rank 7
2023-02-23 08:15:44,736 DEBUG TRAIN Batch 16/700 loss 6.629776 loss_att 9.347403 loss_ctc 10.119528 loss_rnnt 5.529388 hw_loss 0.171678 lr 0.00043171 rank 1
2023-02-23 08:15:44,753 DEBUG TRAIN Batch 16/700 loss 21.625689 loss_att 26.661636 loss_ctc 27.094456 loss_rnnt 19.739790 hw_loss 0.280383 lr 0.00043168 rank 4
2023-02-23 08:15:44,779 DEBUG TRAIN Batch 16/700 loss 5.980099 loss_att 12.900856 loss_ctc 8.300887 loss_rnnt 4.200644 hw_loss 0.160998 lr 0.00043166 rank 6
2023-02-23 08:17:00,456 DEBUG TRAIN Batch 16/800 loss 16.448223 loss_att 18.592859 loss_ctc 21.824133 loss_rnnt 15.187477 hw_loss 0.215685 lr 0.00043144 rank 5
2023-02-23 08:17:00,458 DEBUG TRAIN Batch 16/800 loss 14.212506 loss_att 21.375608 loss_ctc 18.548138 loss_rnnt 12.064853 hw_loss 0.256780 lr 0.00043152 rank 4
2023-02-23 08:17:00,461 DEBUG TRAIN Batch 16/800 loss 6.403762 loss_att 10.504808 loss_ctc 10.600019 loss_rnnt 4.890607 hw_loss 0.250209 lr 0.00043152 rank 2
2023-02-23 08:17:00,461 DEBUG TRAIN Batch 16/800 loss 16.198166 loss_att 19.901590 loss_ctc 14.803755 loss_rnnt 15.502790 hw_loss 0.263650 lr 0.00043154 rank 0
2023-02-23 08:17:00,463 DEBUG TRAIN Batch 16/800 loss 11.280843 loss_att 18.100357 loss_ctc 15.213573 loss_rnnt 9.263047 hw_loss 0.242864 lr 0.00043155 rank 1
2023-02-23 08:17:00,465 DEBUG TRAIN Batch 16/800 loss 9.046023 loss_att 11.108303 loss_ctc 12.363731 loss_rnnt 8.072793 hw_loss 0.222026 lr 0.00043150 rank 6
2023-02-23 08:17:00,466 DEBUG TRAIN Batch 16/800 loss 2.318860 loss_att 5.783134 loss_ctc 3.006217 loss_rnnt 1.412438 hw_loss 0.228600 lr 0.00043149 rank 3
2023-02-23 08:17:00,512 DEBUG TRAIN Batch 16/800 loss 10.822248 loss_att 14.664572 loss_ctc 21.627804 loss_rnnt 8.459261 hw_loss 0.288339 lr 0.00043146 rank 7
2023-02-23 08:18:15,986 DEBUG TRAIN Batch 16/900 loss 7.355752 loss_att 10.433074 loss_ctc 10.767170 loss_rnnt 6.129126 hw_loss 0.293075 lr 0.00043134 rank 6
2023-02-23 08:18:15,989 DEBUG TRAIN Batch 16/900 loss 4.184261 loss_att 6.474020 loss_ctc 7.764750 loss_rnnt 3.132449 hw_loss 0.218365 lr 0.00043136 rank 4
2023-02-23 08:18:15,990 DEBUG TRAIN Batch 16/900 loss 6.118784 loss_att 9.594933 loss_ctc 7.156612 loss_rnnt 5.194250 hw_loss 0.170489 lr 0.00043133 rank 3
2023-02-23 08:18:15,992 DEBUG TRAIN Batch 16/900 loss 5.173126 loss_att 11.625867 loss_ctc 8.121815 loss_rnnt 3.372189 hw_loss 0.219806 lr 0.00043136 rank 2
2023-02-23 08:18:15,993 DEBUG TRAIN Batch 16/900 loss 8.151279 loss_att 11.134250 loss_ctc 11.452747 loss_rnnt 6.983154 hw_loss 0.246255 lr 0.00043128 rank 5
2023-02-23 08:18:15,993 DEBUG TRAIN Batch 16/900 loss 11.795122 loss_att 14.063318 loss_ctc 17.156246 loss_rnnt 10.522370 hw_loss 0.195553 lr 0.00043138 rank 0
2023-02-23 08:18:15,996 DEBUG TRAIN Batch 16/900 loss 6.484080 loss_att 11.489492 loss_ctc 9.646686 loss_rnnt 4.958714 hw_loss 0.192382 lr 0.00043130 rank 7
2023-02-23 08:18:15,999 DEBUG TRAIN Batch 16/900 loss 19.117153 loss_att 19.187065 loss_ctc 22.847855 loss_rnnt 18.478964 hw_loss 0.237715 lr 0.00043139 rank 1
2023-02-23 08:19:33,220 DEBUG TRAIN Batch 16/1000 loss 5.343956 loss_att 9.074893 loss_ctc 7.559865 loss_rnnt 4.148217 hw_loss 0.288934 lr 0.00043112 rank 5
2023-02-23 08:19:33,223 DEBUG TRAIN Batch 16/1000 loss 10.719213 loss_att 11.832302 loss_ctc 12.200052 loss_rnnt 10.167089 hw_loss 0.247616 lr 0.00043120 rank 2
2023-02-23 08:19:33,229 DEBUG TRAIN Batch 16/1000 loss 10.113534 loss_att 11.699676 loss_ctc 12.229185 loss_rnnt 9.395743 hw_loss 0.222142 lr 0.00043120 rank 4
2023-02-23 08:19:33,228 DEBUG TRAIN Batch 16/1000 loss 22.738430 loss_att 24.228014 loss_ctc 20.889179 loss_rnnt 22.558123 hw_loss 0.241790 lr 0.00043117 rank 3
2023-02-23 08:19:33,229 DEBUG TRAIN Batch 16/1000 loss 13.609331 loss_att 20.033449 loss_ctc 21.151989 loss_rnnt 11.198954 hw_loss 0.224748 lr 0.00043123 rank 1
2023-02-23 08:19:33,230 DEBUG TRAIN Batch 16/1000 loss 13.819157 loss_att 15.674202 loss_ctc 13.995893 loss_rnnt 13.281931 hw_loss 0.267470 lr 0.00043114 rank 7
2023-02-23 08:19:33,232 DEBUG TRAIN Batch 16/1000 loss 8.163867 loss_att 13.208475 loss_ctc 13.163452 loss_rnnt 6.338439 hw_loss 0.281053 lr 0.00043118 rank 6
2023-02-23 08:19:33,232 DEBUG TRAIN Batch 16/1000 loss 7.139860 loss_att 10.046935 loss_ctc 11.027966 loss_rnnt 5.972816 hw_loss 0.126027 lr 0.00043122 rank 0
2023-02-23 08:20:51,153 DEBUG TRAIN Batch 16/1100 loss 7.640466 loss_att 10.766103 loss_ctc 11.030405 loss_rnnt 6.398431 hw_loss 0.309216 lr 0.00043106 rank 0
2023-02-23 08:20:51,153 DEBUG TRAIN Batch 16/1100 loss 15.061060 loss_att 19.138218 loss_ctc 23.116671 loss_rnnt 13.036745 hw_loss 0.252753 lr 0.00043104 rank 2
2023-02-23 08:20:51,156 DEBUG TRAIN Batch 16/1100 loss 13.755900 loss_att 13.285239 loss_ctc 19.710695 loss_rnnt 12.925231 hw_loss 0.245306 lr 0.00043104 rank 4
2023-02-23 08:20:51,157 DEBUG TRAIN Batch 16/1100 loss 12.643682 loss_att 16.069902 loss_ctc 18.542536 loss_rnnt 11.026560 hw_loss 0.272556 lr 0.00043096 rank 5
2023-02-23 08:20:51,161 DEBUG TRAIN Batch 16/1100 loss 5.239330 loss_att 8.531999 loss_ctc 9.228718 loss_rnnt 3.914577 hw_loss 0.251814 lr 0.00043101 rank 3
2023-02-23 08:20:51,162 DEBUG TRAIN Batch 16/1100 loss 7.980795 loss_att 9.557585 loss_ctc 10.483438 loss_rnnt 7.221560 hw_loss 0.206609 lr 0.00043102 rank 6
2023-02-23 08:20:51,163 DEBUG TRAIN Batch 16/1100 loss 10.981506 loss_att 14.000364 loss_ctc 14.097918 loss_rnnt 9.823093 hw_loss 0.260848 lr 0.00043098 rank 7
2023-02-23 08:20:51,163 DEBUG TRAIN Batch 16/1100 loss 4.505046 loss_att 7.668760 loss_ctc 5.492243 loss_rnnt 3.632604 hw_loss 0.202637 lr 0.00043106 rank 1
2023-02-23 08:22:05,680 DEBUG TRAIN Batch 16/1200 loss 4.873382 loss_att 6.728366 loss_ctc 6.988157 loss_rnnt 4.022063 hw_loss 0.371909 lr 0.00043080 rank 5
2023-02-23 08:22:05,686 DEBUG TRAIN Batch 16/1200 loss 9.996532 loss_att 11.544483 loss_ctc 13.386093 loss_rnnt 9.050176 hw_loss 0.346545 lr 0.00043088 rank 2
2023-02-23 08:22:05,686 DEBUG TRAIN Batch 16/1200 loss 7.858107 loss_att 9.781377 loss_ctc 8.868188 loss_rnnt 7.207272 hw_loss 0.246570 lr 0.00043090 rank 0
2023-02-23 08:22:05,690 DEBUG TRAIN Batch 16/1200 loss 10.994980 loss_att 11.707741 loss_ctc 13.270334 loss_rnnt 10.349802 hw_loss 0.373583 lr 0.00043088 rank 4
2023-02-23 08:22:05,690 DEBUG TRAIN Batch 16/1200 loss 4.672535 loss_att 7.802195 loss_ctc 7.827380 loss_rnnt 3.475199 hw_loss 0.282671 lr 0.00043086 rank 6
2023-02-23 08:22:05,691 DEBUG TRAIN Batch 16/1200 loss 9.937152 loss_att 12.136147 loss_ctc 13.485713 loss_rnnt 8.811085 hw_loss 0.399610 lr 0.00043082 rank 7
2023-02-23 08:22:05,697 DEBUG TRAIN Batch 16/1200 loss 10.365644 loss_att 11.722106 loss_ctc 13.312860 loss_rnnt 9.589514 hw_loss 0.209767 lr 0.00043090 rank 1
2023-02-23 08:22:05,739 DEBUG TRAIN Batch 16/1200 loss 23.061018 loss_att 23.132900 loss_ctc 37.814880 loss_rnnt 20.927984 hw_loss 0.284019 lr 0.00043085 rank 3
2023-02-23 08:23:22,431 DEBUG TRAIN Batch 16/1300 loss 13.951756 loss_att 12.988338 loss_ctc 17.689651 loss_rnnt 13.474913 hw_loss 0.320893 lr 0.00043072 rank 2
2023-02-23 08:23:22,434 DEBUG TRAIN Batch 16/1300 loss 9.916864 loss_att 11.650236 loss_ctc 12.818604 loss_rnnt 9.072379 hw_loss 0.207960 lr 0.00043066 rank 7
2023-02-23 08:23:22,435 DEBUG TRAIN Batch 16/1300 loss 12.196263 loss_att 12.210690 loss_ctc 14.518817 loss_rnnt 11.672584 hw_loss 0.395854 lr 0.00043072 rank 4
2023-02-23 08:23:22,436 DEBUG TRAIN Batch 16/1300 loss 4.318655 loss_att 7.364902 loss_ctc 5.983992 loss_rnnt 3.370249 hw_loss 0.219587 lr 0.00043064 rank 5
2023-02-23 08:23:22,436 DEBUG TRAIN Batch 16/1300 loss 10.894984 loss_att 16.016567 loss_ctc 12.582607 loss_rnnt 9.526346 hw_loss 0.223697 lr 0.00043070 rank 6
2023-02-23 08:23:22,436 DEBUG TRAIN Batch 16/1300 loss 14.114536 loss_att 14.696684 loss_ctc 19.245951 loss_rnnt 13.117336 hw_loss 0.368591 lr 0.00043069 rank 3
2023-02-23 08:23:22,441 DEBUG TRAIN Batch 16/1300 loss 5.892134 loss_att 9.577345 loss_ctc 9.283732 loss_rnnt 4.565022 hw_loss 0.258481 lr 0.00043074 rank 0
2023-02-23 08:23:22,442 DEBUG TRAIN Batch 16/1300 loss 5.572146 loss_att 8.608811 loss_ctc 6.887993 loss_rnnt 4.679924 hw_loss 0.205205 lr 0.00043074 rank 1
2023-02-23 08:24:41,855 DEBUG TRAIN Batch 16/1400 loss 11.065637 loss_att 17.640240 loss_ctc 16.406574 loss_rnnt 8.894173 hw_loss 0.270784 lr 0.00043048 rank 5
2023-02-23 08:24:41,859 DEBUG TRAIN Batch 16/1400 loss 15.735444 loss_att 15.472692 loss_ctc 18.466269 loss_rnnt 15.276306 hw_loss 0.276709 lr 0.00043058 rank 0
2023-02-23 08:24:41,859 DEBUG TRAIN Batch 16/1400 loss 7.553111 loss_att 10.241506 loss_ctc 10.063270 loss_rnnt 6.530576 hw_loss 0.281566 lr 0.00043056 rank 2
2023-02-23 08:24:41,859 DEBUG TRAIN Batch 16/1400 loss 6.305101 loss_att 7.370847 loss_ctc 9.041992 loss_rnnt 5.609752 hw_loss 0.219903 lr 0.00043054 rank 6
2023-02-23 08:24:41,862 DEBUG TRAIN Batch 16/1400 loss 7.201218 loss_att 11.261831 loss_ctc 11.414092 loss_rnnt 5.714380 hw_loss 0.211872 lr 0.00043053 rank 3
2023-02-23 08:24:41,862 DEBUG TRAIN Batch 16/1400 loss 5.402052 loss_att 9.444395 loss_ctc 7.817461 loss_rnnt 4.117524 hw_loss 0.288760 lr 0.00043056 rank 4
2023-02-23 08:24:41,867 DEBUG TRAIN Batch 16/1400 loss 24.193808 loss_att 21.499699 loss_ctc 28.848858 loss_rnnt 23.985853 hw_loss 0.236447 lr 0.00043050 rank 7
2023-02-23 08:24:41,892 DEBUG TRAIN Batch 16/1400 loss 16.920902 loss_att 19.573290 loss_ctc 24.772495 loss_rnnt 15.254436 hw_loss 0.167082 lr 0.00043059 rank 1
2023-02-23 08:26:00,058 DEBUG TRAIN Batch 16/1500 loss 8.400412 loss_att 10.879183 loss_ctc 10.127141 loss_rnnt 7.558313 hw_loss 0.217712 lr 0.00043040 rank 2
2023-02-23 08:26:00,059 DEBUG TRAIN Batch 16/1500 loss 5.255281 loss_att 7.665688 loss_ctc 6.286911 loss_rnnt 4.537436 hw_loss 0.184149 lr 0.00043032 rank 5
2023-02-23 08:26:00,062 DEBUG TRAIN Batch 16/1500 loss 14.129605 loss_att 19.445492 loss_ctc 19.839676 loss_rnnt 12.194378 hw_loss 0.207574 lr 0.00043037 rank 3
2023-02-23 08:26:00,062 DEBUG TRAIN Batch 16/1500 loss 30.825449 loss_att 33.865395 loss_ctc 38.450840 loss_rnnt 29.088547 hw_loss 0.210366 lr 0.00043034 rank 7
2023-02-23 08:26:00,062 DEBUG TRAIN Batch 16/1500 loss 2.957227 loss_att 5.515381 loss_ctc 3.933236 loss_rnnt 2.210194 hw_loss 0.197377 lr 0.00043040 rank 4
2023-02-23 08:26:00,064 DEBUG TRAIN Batch 16/1500 loss 11.189030 loss_att 15.509492 loss_ctc 15.062813 loss_rnnt 9.681332 hw_loss 0.238316 lr 0.00043043 rank 1
2023-02-23 08:26:00,067 DEBUG TRAIN Batch 16/1500 loss 6.779496 loss_att 10.049530 loss_ctc 9.593107 loss_rnnt 5.607362 hw_loss 0.268085 lr 0.00043042 rank 0
2023-02-23 08:26:00,068 DEBUG TRAIN Batch 16/1500 loss 9.995617 loss_att 11.283852 loss_ctc 15.225143 loss_rnnt 8.939162 hw_loss 0.190382 lr 0.00043038 rank 6
2023-02-23 08:27:15,369 DEBUG TRAIN Batch 16/1600 loss 16.572317 loss_att 18.272785 loss_ctc 26.935061 loss_rnnt 14.727205 hw_loss 0.231223 lr 0.00043026 rank 0
2023-02-23 08:27:15,371 DEBUG TRAIN Batch 16/1600 loss 10.464461 loss_att 15.292533 loss_ctc 16.883234 loss_rnnt 8.503635 hw_loss 0.261328 lr 0.00043018 rank 7
2023-02-23 08:27:15,372 DEBUG TRAIN Batch 16/1600 loss 10.248905 loss_att 15.696512 loss_ctc 14.029004 loss_rnnt 8.559105 hw_loss 0.180497 lr 0.00043016 rank 5
2023-02-23 08:27:15,373 DEBUG TRAIN Batch 16/1600 loss 6.239848 loss_att 9.887121 loss_ctc 7.009417 loss_rnnt 5.301870 hw_loss 0.198587 lr 0.00043027 rank 1
2023-02-23 08:27:15,374 DEBUG TRAIN Batch 16/1600 loss 7.550841 loss_att 11.186346 loss_ctc 9.669474 loss_rnnt 6.417240 hw_loss 0.232529 lr 0.00043021 rank 3
2023-02-23 08:27:15,375 DEBUG TRAIN Batch 16/1600 loss 9.308652 loss_att 11.406438 loss_ctc 12.785839 loss_rnnt 8.272082 hw_loss 0.287602 lr 0.00043024 rank 4
2023-02-23 08:27:15,375 DEBUG TRAIN Batch 16/1600 loss 11.012700 loss_att 14.466759 loss_ctc 16.592651 loss_rnnt 9.500676 hw_loss 0.144786 lr 0.00043024 rank 2
2023-02-23 08:27:15,378 DEBUG TRAIN Batch 16/1600 loss 11.633389 loss_att 13.977861 loss_ctc 14.091999 loss_rnnt 10.705326 hw_loss 0.246288 lr 0.00043022 rank 6
2023-02-23 08:28:31,493 DEBUG TRAIN Batch 16/1700 loss 11.631936 loss_att 15.559767 loss_ctc 14.925804 loss_rnnt 10.302938 hw_loss 0.195469 lr 0.00043005 rank 3
2023-02-23 08:28:31,495 DEBUG TRAIN Batch 16/1700 loss 10.544656 loss_att 13.334529 loss_ctc 12.735542 loss_rnnt 9.561785 hw_loss 0.248958 lr 0.00043008 rank 4
2023-02-23 08:28:31,496 DEBUG TRAIN Batch 16/1700 loss 24.346167 loss_att 29.389162 loss_ctc 29.424261 loss_rnnt 22.577442 hw_loss 0.155706 lr 0.00043006 rank 6
2023-02-23 08:28:31,498 DEBUG TRAIN Batch 16/1700 loss 12.169787 loss_att 13.531994 loss_ctc 16.941017 loss_rnnt 11.151885 hw_loss 0.204932 lr 0.00043011 rank 1
2023-02-23 08:28:31,499 DEBUG TRAIN Batch 16/1700 loss 6.661118 loss_att 8.359397 loss_ctc 8.807290 loss_rnnt 5.891749 hw_loss 0.269169 lr 0.00043008 rank 2
2023-02-23 08:28:31,500 DEBUG TRAIN Batch 16/1700 loss 10.828853 loss_att 14.924146 loss_ctc 17.478535 loss_rnnt 9.028851 hw_loss 0.176849 lr 0.00043010 rank 0
2023-02-23 08:28:31,501 DEBUG TRAIN Batch 16/1700 loss 22.780815 loss_att 23.373285 loss_ctc 28.102373 loss_rnnt 21.829153 hw_loss 0.231802 lr 0.00043000 rank 5
2023-02-23 08:28:31,503 DEBUG TRAIN Batch 16/1700 loss 14.902937 loss_att 17.961765 loss_ctc 18.671391 loss_rnnt 13.701452 hw_loss 0.163613 lr 0.00043003 rank 7
2023-02-23 08:29:51,131 DEBUG TRAIN Batch 16/1800 loss 8.117141 loss_att 11.660001 loss_ctc 11.579854 loss_rnnt 6.810409 hw_loss 0.255870 lr 0.00042992 rank 2
2023-02-23 08:29:51,132 DEBUG TRAIN Batch 16/1800 loss 12.436592 loss_att 13.588041 loss_ctc 15.869363 loss_rnnt 11.606580 hw_loss 0.266288 lr 0.00042989 rank 3
2023-02-23 08:29:51,133 DEBUG TRAIN Batch 16/1800 loss 23.520300 loss_att 24.743502 loss_ctc 31.551678 loss_rnnt 22.076344 hw_loss 0.240871 lr 0.00042984 rank 5
2023-02-23 08:29:51,135 DEBUG TRAIN Batch 16/1800 loss 6.964591 loss_att 8.681070 loss_ctc 8.413442 loss_rnnt 6.289068 hw_loss 0.260710 lr 0.00042990 rank 6
2023-02-23 08:29:51,135 DEBUG TRAIN Batch 16/1800 loss 8.222565 loss_att 10.822036 loss_ctc 9.722351 loss_rnnt 7.355832 hw_loss 0.275375 lr 0.00042992 rank 4
2023-02-23 08:29:51,136 DEBUG TRAIN Batch 16/1800 loss 11.038873 loss_att 13.456620 loss_ctc 14.318112 loss_rnnt 9.983005 hw_loss 0.253290 lr 0.00042995 rank 1
2023-02-23 08:29:51,139 DEBUG TRAIN Batch 16/1800 loss 6.133740 loss_att 9.800689 loss_ctc 7.468689 loss_rnnt 5.096801 hw_loss 0.235418 lr 0.00042994 rank 0
2023-02-23 08:29:51,189 DEBUG TRAIN Batch 16/1800 loss 6.061183 loss_att 7.879496 loss_ctc 7.116661 loss_rnnt 5.384529 hw_loss 0.322991 lr 0.00042987 rank 7
2023-02-23 08:31:07,334 DEBUG TRAIN Batch 16/1900 loss 14.357795 loss_att 15.981890 loss_ctc 20.326319 loss_rnnt 13.120968 hw_loss 0.217885 lr 0.00042976 rank 2
2023-02-23 08:31:07,335 DEBUG TRAIN Batch 16/1900 loss 10.192261 loss_att 19.719221 loss_ctc 15.071438 loss_rnnt 7.519839 hw_loss 0.218388 lr 0.00042968 rank 5
2023-02-23 08:31:07,336 DEBUG TRAIN Batch 16/1900 loss 7.949400 loss_att 10.037737 loss_ctc 9.410170 loss_rnnt 7.224107 hw_loss 0.211605 lr 0.00042973 rank 3
2023-02-23 08:31:07,339 DEBUG TRAIN Batch 16/1900 loss 11.778257 loss_att 12.118828 loss_ctc 15.834116 loss_rnnt 10.994427 hw_loss 0.328004 lr 0.00042978 rank 0
2023-02-23 08:31:07,340 DEBUG TRAIN Batch 16/1900 loss 7.865335 loss_att 8.395144 loss_ctc 9.449982 loss_rnnt 7.366016 hw_loss 0.341382 lr 0.00042976 rank 4
2023-02-23 08:31:07,341 DEBUG TRAIN Batch 16/1900 loss 8.307340 loss_att 10.733371 loss_ctc 10.085071 loss_rnnt 7.458304 hw_loss 0.237746 lr 0.00042971 rank 7
2023-02-23 08:31:07,341 DEBUG TRAIN Batch 16/1900 loss 14.668087 loss_att 17.611658 loss_ctc 25.684418 loss_rnnt 12.471884 hw_loss 0.259962 lr 0.00042979 rank 1
2023-02-23 08:31:07,385 DEBUG TRAIN Batch 16/1900 loss 2.864992 loss_att 5.379676 loss_ctc 2.096749 loss_rnnt 2.409500 hw_loss 0.103101 lr 0.00042974 rank 6
2023-02-23 08:32:24,629 DEBUG TRAIN Batch 16/2000 loss 8.311457 loss_att 11.503231 loss_ctc 13.971403 loss_rnnt 6.833784 hw_loss 0.158734 lr 0.00042952 rank 5
2023-02-23 08:32:24,632 DEBUG TRAIN Batch 16/2000 loss 7.649476 loss_att 11.833800 loss_ctc 12.130424 loss_rnnt 6.051147 hw_loss 0.307507 lr 0.00042960 rank 2
2023-02-23 08:32:24,634 DEBUG TRAIN Batch 16/2000 loss 10.516745 loss_att 12.472850 loss_ctc 14.692305 loss_rnnt 9.432591 hw_loss 0.255356 lr 0.00042962 rank 0
2023-02-23 08:32:24,636 DEBUG TRAIN Batch 16/2000 loss 14.976251 loss_att 19.012970 loss_ctc 28.373421 loss_rnnt 12.250149 hw_loss 0.248378 lr 0.00042963 rank 1
2023-02-23 08:32:24,636 DEBUG TRAIN Batch 16/2000 loss 25.520391 loss_att 30.497841 loss_ctc 38.204857 loss_rnnt 22.678289 hw_loss 0.291283 lr 0.00042957 rank 3
2023-02-23 08:32:24,636 DEBUG TRAIN Batch 16/2000 loss 3.576489 loss_att 6.891022 loss_ctc 4.011292 loss_rnnt 2.793910 hw_loss 0.115686 lr 0.00042960 rank 4
2023-02-23 08:32:24,644 DEBUG TRAIN Batch 16/2000 loss 21.210468 loss_att 22.937300 loss_ctc 23.614279 loss_rnnt 20.449152 hw_loss 0.178952 lr 0.00042959 rank 6
2023-02-23 08:32:24,679 DEBUG TRAIN Batch 16/2000 loss 9.263994 loss_att 14.072866 loss_ctc 11.908652 loss_rnnt 7.817935 hw_loss 0.246868 lr 0.00042955 rank 7
2023-02-23 08:33:44,264 DEBUG TRAIN Batch 16/2100 loss 12.951380 loss_att 15.649199 loss_ctc 18.867239 loss_rnnt 11.478909 hw_loss 0.270239 lr 0.00042945 rank 4
2023-02-23 08:33:44,266 DEBUG TRAIN Batch 16/2100 loss 3.569675 loss_att 6.089369 loss_ctc 4.585920 loss_rnnt 2.788675 hw_loss 0.265427 lr 0.00042946 rank 0
2023-02-23 08:33:44,267 DEBUG TRAIN Batch 16/2100 loss 16.489723 loss_att 19.081867 loss_ctc 21.514416 loss_rnnt 15.176946 hw_loss 0.233230 lr 0.00042936 rank 5
2023-02-23 08:33:44,268 DEBUG TRAIN Batch 16/2100 loss 6.376227 loss_att 8.950907 loss_ctc 8.980465 loss_rnnt 5.405168 hw_loss 0.204171 lr 0.00042943 rank 6
2023-02-23 08:33:44,270 DEBUG TRAIN Batch 16/2100 loss 6.617131 loss_att 9.029259 loss_ctc 8.455897 loss_rnnt 5.787427 hw_loss 0.191455 lr 0.00042939 rank 7
2023-02-23 08:33:44,271 DEBUG TRAIN Batch 16/2100 loss 10.795908 loss_att 12.461901 loss_ctc 13.110805 loss_rnnt 10.016761 hw_loss 0.257430 lr 0.00042945 rank 2
2023-02-23 08:33:44,277 DEBUG TRAIN Batch 16/2100 loss 4.764709 loss_att 9.324164 loss_ctc 4.570940 loss_rnnt 3.748333 hw_loss 0.244351 lr 0.00042942 rank 3
2023-02-23 08:33:44,278 DEBUG TRAIN Batch 16/2100 loss 11.729839 loss_att 14.617655 loss_ctc 12.388316 loss_rnnt 10.934109 hw_loss 0.244447 lr 0.00042947 rank 1
2023-02-23 08:35:00,475 DEBUG TRAIN Batch 16/2200 loss 13.542611 loss_att 19.239676 loss_ctc 16.539745 loss_rnnt 11.902478 hw_loss 0.189564 lr 0.00042929 rank 2
2023-02-23 08:35:00,478 DEBUG TRAIN Batch 16/2200 loss 11.558033 loss_att 11.776563 loss_ctc 12.771831 loss_rnnt 11.206020 hw_loss 0.274626 lr 0.00042921 rank 5
2023-02-23 08:35:00,480 DEBUG TRAIN Batch 16/2200 loss 13.254893 loss_att 15.280816 loss_ctc 17.626858 loss_rnnt 12.123297 hw_loss 0.269031 lr 0.00042931 rank 0
2023-02-23 08:35:00,482 DEBUG TRAIN Batch 16/2200 loss 14.957015 loss_att 18.772896 loss_ctc 17.688826 loss_rnnt 13.690960 hw_loss 0.259945 lr 0.00042923 rank 7
2023-02-23 08:35:00,484 DEBUG TRAIN Batch 16/2200 loss 7.643394 loss_att 12.909622 loss_ctc 9.597706 loss_rnnt 6.185633 hw_loss 0.269888 lr 0.00042926 rank 3
2023-02-23 08:35:00,489 DEBUG TRAIN Batch 16/2200 loss 12.975480 loss_att 19.597668 loss_ctc 19.243570 loss_rnnt 10.681347 hw_loss 0.251154 lr 0.00042929 rank 4
2023-02-23 08:35:00,492 DEBUG TRAIN Batch 16/2200 loss 11.981802 loss_att 15.589645 loss_ctc 15.989319 loss_rnnt 10.610400 hw_loss 0.216559 lr 0.00042927 rank 6
2023-02-23 08:35:00,527 DEBUG TRAIN Batch 16/2200 loss 8.828277 loss_att 13.493483 loss_ctc 12.303755 loss_rnnt 7.307914 hw_loss 0.232361 lr 0.00042931 rank 1
2023-02-23 08:36:15,904 DEBUG TRAIN Batch 16/2300 loss 22.107382 loss_att 22.880129 loss_ctc 38.151688 loss_rnnt 19.701866 hw_loss 0.209484 lr 0.00042915 rank 0
2023-02-23 08:36:15,907 DEBUG TRAIN Batch 16/2300 loss 8.674602 loss_att 13.109236 loss_ctc 10.656271 loss_rnnt 7.430858 hw_loss 0.173615 lr 0.00042916 rank 1
2023-02-23 08:36:15,907 DEBUG TRAIN Batch 16/2300 loss 14.823830 loss_att 14.616022 loss_ctc 17.624863 loss_rnnt 14.361812 hw_loss 0.243953 lr 0.00042907 rank 7
2023-02-23 08:36:15,907 DEBUG TRAIN Batch 16/2300 loss 6.509559 loss_att 10.625443 loss_ctc 11.140488 loss_rnnt 4.943831 hw_loss 0.234550 lr 0.00042913 rank 2
2023-02-23 08:36:15,908 DEBUG TRAIN Batch 16/2300 loss 12.258093 loss_att 18.851418 loss_ctc 19.011339 loss_rnnt 9.862756 hw_loss 0.330448 lr 0.00042905 rank 5
2023-02-23 08:36:15,911 DEBUG TRAIN Batch 16/2300 loss 10.929762 loss_att 13.086759 loss_ctc 12.868544 loss_rnnt 10.128566 hw_loss 0.208675 lr 0.00042913 rank 4
2023-02-23 08:36:15,915 DEBUG TRAIN Batch 16/2300 loss 11.600816 loss_att 14.778970 loss_ctc 14.257126 loss_rnnt 10.457890 hw_loss 0.287100 lr 0.00042910 rank 3
2023-02-23 08:36:15,955 DEBUG TRAIN Batch 16/2300 loss 11.296369 loss_att 13.463985 loss_ctc 12.230419 loss_rnnt 10.591447 hw_loss 0.275359 lr 0.00042911 rank 6
2023-02-23 08:37:31,292 DEBUG TRAIN Batch 16/2400 loss 11.043021 loss_att 13.044044 loss_ctc 14.924419 loss_rnnt 9.968628 hw_loss 0.293752 lr 0.00042889 rank 5
2023-02-23 08:37:31,296 DEBUG TRAIN Batch 16/2400 loss 10.203474 loss_att 14.200122 loss_ctc 12.340908 loss_rnnt 8.997873 hw_loss 0.227399 lr 0.00042900 rank 1
2023-02-23 08:37:31,297 DEBUG TRAIN Batch 16/2400 loss 8.019317 loss_att 11.082178 loss_ctc 12.041899 loss_rnnt 6.776535 hw_loss 0.175998 lr 0.00042897 rank 4
2023-02-23 08:37:31,297 DEBUG TRAIN Batch 16/2400 loss 9.657413 loss_att 12.707277 loss_ctc 17.797985 loss_rnnt 7.773458 hw_loss 0.353574 lr 0.00042895 rank 6
2023-02-23 08:37:31,299 DEBUG TRAIN Batch 16/2400 loss 5.578768 loss_att 9.049208 loss_ctc 6.089918 loss_rnnt 4.719851 hw_loss 0.181267 lr 0.00042899 rank 0
2023-02-23 08:37:31,302 DEBUG TRAIN Batch 16/2400 loss 7.111058 loss_att 11.619448 loss_ctc 12.089040 loss_rnnt 5.433395 hw_loss 0.210474 lr 0.00042897 rank 2
2023-02-23 08:37:31,317 DEBUG TRAIN Batch 16/2400 loss 14.678392 loss_att 17.756199 loss_ctc 24.295944 loss_rnnt 12.661478 hw_loss 0.223149 lr 0.00042894 rank 3
2023-02-23 08:37:31,326 DEBUG TRAIN Batch 16/2400 loss 11.732944 loss_att 15.630282 loss_ctc 16.624382 loss_rnnt 10.143797 hw_loss 0.295290 lr 0.00042892 rank 7
2023-02-23 08:38:51,326 DEBUG TRAIN Batch 16/2500 loss 12.227279 loss_att 13.426109 loss_ctc 14.984553 loss_rnnt 11.511634 hw_loss 0.202954 lr 0.00042873 rank 5
2023-02-23 08:38:51,330 DEBUG TRAIN Batch 16/2500 loss 6.928676 loss_att 8.057063 loss_ctc 9.545665 loss_rnnt 6.236152 hw_loss 0.221091 lr 0.00042881 rank 2
2023-02-23 08:38:51,331 DEBUG TRAIN Batch 16/2500 loss 11.557752 loss_att 11.282492 loss_ctc 16.479132 loss_rnnt 10.769905 hw_loss 0.350088 lr 0.00042876 rank 7
2023-02-23 08:38:51,332 DEBUG TRAIN Batch 16/2500 loss 10.813351 loss_att 14.627474 loss_ctc 16.772293 loss_rnnt 9.067101 hw_loss 0.354188 lr 0.00042878 rank 3
2023-02-23 08:38:51,333 DEBUG TRAIN Batch 16/2500 loss 11.255578 loss_att 13.642990 loss_ctc 13.485321 loss_rnnt 10.341624 hw_loss 0.260950 lr 0.00042883 rank 0
2023-02-23 08:38:51,333 DEBUG TRAIN Batch 16/2500 loss 8.877487 loss_att 8.731556 loss_ctc 11.135501 loss_rnnt 8.456045 hw_loss 0.280422 lr 0.00042881 rank 4
2023-02-23 08:38:51,337 DEBUG TRAIN Batch 16/2500 loss 11.372764 loss_att 13.541647 loss_ctc 13.553858 loss_rnnt 10.492135 hw_loss 0.292574 lr 0.00042884 rank 1
2023-02-23 08:38:51,382 DEBUG TRAIN Batch 16/2500 loss 6.003917 loss_att 7.784738 loss_ctc 7.604770 loss_rnnt 5.242185 hw_loss 0.360226 lr 0.00042880 rank 6
2023-02-23 08:40:06,654 DEBUG TRAIN Batch 16/2600 loss 16.408321 loss_att 21.826534 loss_ctc 21.422520 loss_rnnt 14.555327 hw_loss 0.188986 lr 0.00042866 rank 4
2023-02-23 08:40:06,662 DEBUG TRAIN Batch 16/2600 loss 8.217996 loss_att 13.162052 loss_ctc 10.672491 loss_rnnt 6.740490 hw_loss 0.302677 lr 0.00042866 rank 2
2023-02-23 08:40:06,661 DEBUG TRAIN Batch 16/2600 loss 18.384392 loss_att 20.270720 loss_ctc 19.806292 loss_rnnt 17.709122 hw_loss 0.203284 lr 0.00042857 rank 5
2023-02-23 08:40:06,662 DEBUG TRAIN Batch 16/2600 loss 4.866474 loss_att 8.392773 loss_ctc 7.365861 loss_rnnt 3.746579 hw_loss 0.152594 lr 0.00042863 rank 3
2023-02-23 08:40:06,662 DEBUG TRAIN Batch 16/2600 loss 27.218803 loss_att 39.312466 loss_ctc 40.618927 loss_rnnt 22.893225 hw_loss 0.225308 lr 0.00042867 rank 0
2023-02-23 08:40:06,662 DEBUG TRAIN Batch 16/2600 loss 11.062518 loss_att 15.121403 loss_ctc 18.340984 loss_rnnt 9.154506 hw_loss 0.235826 lr 0.00042860 rank 7
2023-02-23 08:40:06,662 DEBUG TRAIN Batch 16/2600 loss 14.836752 loss_att 20.189392 loss_ctc 17.135683 loss_rnnt 13.375008 hw_loss 0.158797 lr 0.00042868 rank 1
2023-02-23 08:40:06,666 DEBUG TRAIN Batch 16/2600 loss 13.808922 loss_att 19.653605 loss_ctc 19.079117 loss_rnnt 11.834051 hw_loss 0.193579 lr 0.00042864 rank 6
2023-02-23 08:41:23,146 DEBUG TRAIN Batch 16/2700 loss 11.283429 loss_att 13.663809 loss_ctc 14.645392 loss_rnnt 10.216533 hw_loss 0.267298 lr 0.00042850 rank 2
2023-02-23 08:41:23,148 DEBUG TRAIN Batch 16/2700 loss 19.175726 loss_att 24.523270 loss_ctc 29.657560 loss_rnnt 16.578018 hw_loss 0.244914 lr 0.00042852 rank 0
2023-02-23 08:41:23,149 DEBUG TRAIN Batch 16/2700 loss 11.915834 loss_att 16.214828 loss_ctc 17.888527 loss_rnnt 10.121775 hw_loss 0.258566 lr 0.00042844 rank 7
2023-02-23 08:41:23,151 DEBUG TRAIN Batch 16/2700 loss 8.558747 loss_att 12.721474 loss_ctc 14.627303 loss_rnnt 6.819292 hw_loss 0.183317 lr 0.00042848 rank 6
2023-02-23 08:41:23,152 DEBUG TRAIN Batch 16/2700 loss 14.367523 loss_att 15.246229 loss_ctc 17.383442 loss_rnnt 13.680064 hw_loss 0.205494 lr 0.00042842 rank 5
2023-02-23 08:41:23,153 DEBUG TRAIN Batch 16/2700 loss 3.213016 loss_att 5.828799 loss_ctc 6.943736 loss_rnnt 2.071125 hw_loss 0.227446 lr 0.00042850 rank 4
2023-02-23 08:41:23,155 DEBUG TRAIN Batch 16/2700 loss 14.975022 loss_att 16.217516 loss_ctc 21.336857 loss_rnnt 13.783264 hw_loss 0.178152 lr 0.00042847 rank 3
2023-02-23 08:41:23,156 DEBUG TRAIN Batch 16/2700 loss 12.898290 loss_att 16.289497 loss_ctc 17.855268 loss_rnnt 11.452777 hw_loss 0.199387 lr 0.00042852 rank 1
2023-02-23 08:42:42,063 DEBUG TRAIN Batch 16/2800 loss 5.388953 loss_att 9.033726 loss_ctc 10.461481 loss_rnnt 3.861394 hw_loss 0.229251 lr 0.00042834 rank 4
2023-02-23 08:42:42,064 DEBUG TRAIN Batch 16/2800 loss 3.015205 loss_att 5.923446 loss_ctc 3.415032 loss_rnnt 2.299698 hw_loss 0.151029 lr 0.00042834 rank 2
2023-02-23 08:42:42,067 DEBUG TRAIN Batch 16/2800 loss 11.793454 loss_att 13.045938 loss_ctc 13.963710 loss_rnnt 11.097607 hw_loss 0.292468 lr 0.00042836 rank 0
2023-02-23 08:42:42,066 DEBUG TRAIN Batch 16/2800 loss 8.849963 loss_att 12.979630 loss_ctc 13.746765 loss_rnnt 7.138425 hw_loss 0.436308 lr 0.00042831 rank 3
2023-02-23 08:42:42,070 DEBUG TRAIN Batch 16/2800 loss 10.150165 loss_att 13.341266 loss_ctc 12.757730 loss_rnnt 9.082283 hw_loss 0.153723 lr 0.00042826 rank 5
2023-02-23 08:42:42,075 DEBUG TRAIN Batch 16/2800 loss 9.232714 loss_att 10.926280 loss_ctc 13.504142 loss_rnnt 8.196751 hw_loss 0.239486 lr 0.00042837 rank 1
2023-02-23 08:42:42,075 DEBUG TRAIN Batch 16/2800 loss 4.664163 loss_att 10.126376 loss_ctc 7.246154 loss_rnnt 3.102619 hw_loss 0.234066 lr 0.00042832 rank 6
2023-02-23 08:42:42,077 DEBUG TRAIN Batch 16/2800 loss 5.900626 loss_att 10.716166 loss_ctc 6.755437 loss_rnnt 4.729849 hw_loss 0.175676 lr 0.00042829 rank 7
2023-02-23 08:43:59,331 DEBUG TRAIN Batch 16/2900 loss 6.839646 loss_att 10.273413 loss_ctc 11.319960 loss_rnnt 5.420700 hw_loss 0.252782 lr 0.00042820 rank 0
2023-02-23 08:43:59,336 DEBUG TRAIN Batch 16/2900 loss 16.438082 loss_att 20.582590 loss_ctc 24.346256 loss_rnnt 14.463200 hw_loss 0.171670 lr 0.00042818 rank 2
2023-02-23 08:43:59,337 DEBUG TRAIN Batch 16/2900 loss 6.088335 loss_att 8.975532 loss_ctc 9.797736 loss_rnnt 4.884141 hw_loss 0.247813 lr 0.00042810 rank 5
2023-02-23 08:43:59,338 DEBUG TRAIN Batch 16/2900 loss 8.056181 loss_att 12.238692 loss_ctc 13.370230 loss_rnnt 6.431650 hw_loss 0.149040 lr 0.00042818 rank 4
2023-02-23 08:43:59,338 DEBUG TRAIN Batch 16/2900 loss 10.433676 loss_att 14.569811 loss_ctc 12.349011 loss_rnnt 9.232145 hw_loss 0.222986 lr 0.00042817 rank 6
2023-02-23 08:43:59,340 DEBUG TRAIN Batch 16/2900 loss 7.565252 loss_att 11.344523 loss_ctc 11.649248 loss_rnnt 6.130174 hw_loss 0.252547 lr 0.00042816 rank 3
2023-02-23 08:43:59,344 DEBUG TRAIN Batch 16/2900 loss 19.269974 loss_att 24.758617 loss_ctc 27.371014 loss_rnnt 16.955090 hw_loss 0.256907 lr 0.00042813 rank 7
2023-02-23 08:43:59,345 DEBUG TRAIN Batch 16/2900 loss 27.941032 loss_att 29.562645 loss_ctc 41.448170 loss_rnnt 25.684458 hw_loss 0.246187 lr 0.00042821 rank 1
2023-02-23 08:45:14,594 DEBUG TRAIN Batch 16/3000 loss 6.043284 loss_att 11.138119 loss_ctc 8.690828 loss_rnnt 4.590337 hw_loss 0.151829 lr 0.00042803 rank 4
2023-02-23 08:45:14,594 DEBUG TRAIN Batch 16/3000 loss 7.189046 loss_att 9.639669 loss_ctc 10.084032 loss_rnnt 6.189652 hw_loss 0.231134 lr 0.00042803 rank 2
2023-02-23 08:45:14,596 DEBUG TRAIN Batch 16/3000 loss 6.080542 loss_att 9.212191 loss_ctc 7.971965 loss_rnnt 5.108934 hw_loss 0.174540 lr 0.00042805 rank 0
2023-02-23 08:45:14,598 DEBUG TRAIN Batch 16/3000 loss 6.035204 loss_att 9.305649 loss_ctc 8.271291 loss_rnnt 4.905697 hw_loss 0.332386 lr 0.00042797 rank 7
2023-02-23 08:45:14,600 DEBUG TRAIN Batch 16/3000 loss 5.529611 loss_att 7.715257 loss_ctc 6.695596 loss_rnnt 4.870325 hw_loss 0.125049 lr 0.00042795 rank 5
2023-02-23 08:45:14,603 DEBUG TRAIN Batch 16/3000 loss 15.776657 loss_att 19.745842 loss_ctc 20.881756 loss_rnnt 14.166754 hw_loss 0.253849 lr 0.00042801 rank 6
2023-02-23 08:45:14,602 DEBUG TRAIN Batch 16/3000 loss 14.126779 loss_att 15.194103 loss_ctc 17.590303 loss_rnnt 13.366798 hw_loss 0.158835 lr 0.00042805 rank 1
2023-02-23 08:45:14,647 DEBUG TRAIN Batch 16/3000 loss 11.518472 loss_att 15.357250 loss_ctc 17.986027 loss_rnnt 9.766845 hw_loss 0.227870 lr 0.00042800 rank 3
2023-02-23 08:46:31,439 DEBUG TRAIN Batch 16/3100 loss 11.751091 loss_att 12.503851 loss_ctc 14.852862 loss_rnnt 11.058002 hw_loss 0.241813 lr 0.00042787 rank 2
2023-02-23 08:46:31,439 DEBUG TRAIN Batch 16/3100 loss 10.782827 loss_att 13.985676 loss_ctc 13.878453 loss_rnnt 9.612312 hw_loss 0.219742 lr 0.00042787 rank 4
2023-02-23 08:46:31,439 DEBUG TRAIN Batch 16/3100 loss 9.263154 loss_att 12.468744 loss_ctc 9.629299 loss_rnnt 8.446003 hw_loss 0.238526 lr 0.00042789 rank 0
2023-02-23 08:46:31,440 DEBUG TRAIN Batch 16/3100 loss 4.683220 loss_att 6.849536 loss_ctc 6.355824 loss_rnnt 3.884070 hw_loss 0.267886 lr 0.00042784 rank 3
2023-02-23 08:46:31,441 DEBUG TRAIN Batch 16/3100 loss 11.690087 loss_att 12.664767 loss_ctc 15.465836 loss_rnnt 10.823746 hw_loss 0.314948 lr 0.00042790 rank 1
2023-02-23 08:46:31,443 DEBUG TRAIN Batch 16/3100 loss 8.962242 loss_att 9.919782 loss_ctc 13.985029 loss_rnnt 7.970389 hw_loss 0.244951 lr 0.00042779 rank 5
2023-02-23 08:46:31,443 DEBUG TRAIN Batch 16/3100 loss 9.755934 loss_att 11.683947 loss_ctc 11.544728 loss_rnnt 9.043569 hw_loss 0.165481 lr 0.00042782 rank 7
2023-02-23 08:46:31,446 DEBUG TRAIN Batch 16/3100 loss 14.335278 loss_att 16.912781 loss_ctc 16.331869 loss_rnnt 13.420586 hw_loss 0.249333 lr 0.00042785 rank 6
2023-02-23 08:47:51,223 DEBUG TRAIN Batch 16/3200 loss 1.634450 loss_att 4.475129 loss_ctc 2.265004 loss_rnnt 0.852977 hw_loss 0.242369 lr 0.00042763 rank 5
2023-02-23 08:47:51,230 DEBUG TRAIN Batch 16/3200 loss 6.509449 loss_att 7.495217 loss_ctc 7.221642 loss_rnnt 6.066021 hw_loss 0.283716 lr 0.00042768 rank 3
2023-02-23 08:47:51,235 DEBUG TRAIN Batch 16/3200 loss 9.154190 loss_att 9.091663 loss_ctc 11.862429 loss_rnnt 8.617828 hw_loss 0.352066 lr 0.00042766 rank 7
2023-02-23 08:47:51,235 DEBUG TRAIN Batch 16/3200 loss 16.757311 loss_att 17.059395 loss_ctc 22.497948 loss_rnnt 15.714264 hw_loss 0.407270 lr 0.00042771 rank 4
2023-02-23 08:47:51,236 DEBUG TRAIN Batch 16/3200 loss 5.466311 loss_att 8.572771 loss_ctc 12.412438 loss_rnnt 3.791111 hw_loss 0.239547 lr 0.00042773 rank 0
2023-02-23 08:47:51,237 DEBUG TRAIN Batch 16/3200 loss 7.723750 loss_att 8.593415 loss_ctc 9.692953 loss_rnnt 7.161224 hw_loss 0.236309 lr 0.00042770 rank 6
2023-02-23 08:47:51,247 DEBUG TRAIN Batch 16/3200 loss 24.466122 loss_att 28.326820 loss_ctc 28.658493 loss_rnnt 22.978231 hw_loss 0.293939 lr 0.00042774 rank 1
2023-02-23 08:47:51,266 DEBUG TRAIN Batch 16/3200 loss 15.600020 loss_att 20.622177 loss_ctc 17.386026 loss_rnnt 14.256794 hw_loss 0.188740 lr 0.00042771 rank 2
2023-02-23 08:49:09,185 DEBUG TRAIN Batch 16/3300 loss 9.530604 loss_att 16.026213 loss_ctc 17.846581 loss_rnnt 7.038366 hw_loss 0.158097 lr 0.00042748 rank 5
2023-02-23 08:49:09,190 DEBUG TRAIN Batch 16/3300 loss 12.038187 loss_att 17.218979 loss_ctc 12.258713 loss_rnnt 10.859619 hw_loss 0.211887 lr 0.00042758 rank 0
2023-02-23 08:49:09,193 DEBUG TRAIN Batch 16/3300 loss 11.096816 loss_att 14.436264 loss_ctc 16.730923 loss_rnnt 9.514721 hw_loss 0.305611 lr 0.00042756 rank 4
2023-02-23 08:49:09,194 DEBUG TRAIN Batch 16/3300 loss 9.442472 loss_att 13.265648 loss_ctc 15.564514 loss_rnnt 7.707957 hw_loss 0.288015 lr 0.00042750 rank 7
2023-02-23 08:49:09,194 DEBUG TRAIN Batch 16/3300 loss 6.415329 loss_att 7.813709 loss_ctc 7.857885 loss_rnnt 5.881943 hw_loss 0.115067 lr 0.00042753 rank 3
2023-02-23 08:49:09,195 DEBUG TRAIN Batch 16/3300 loss 8.208844 loss_att 10.222546 loss_ctc 12.299648 loss_rnnt 7.150747 hw_loss 0.206094 lr 0.00042754 rank 6
2023-02-23 08:49:09,197 DEBUG TRAIN Batch 16/3300 loss 14.397147 loss_att 18.461218 loss_ctc 17.473583 loss_rnnt 13.080897 hw_loss 0.174835 lr 0.00042756 rank 2
2023-02-23 08:49:09,201 DEBUG TRAIN Batch 16/3300 loss 7.543182 loss_att 9.480515 loss_ctc 11.188324 loss_rnnt 6.555539 hw_loss 0.214045 lr 0.00042758 rank 1
2023-02-23 08:50:23,959 DEBUG TRAIN Batch 16/3400 loss 23.365423 loss_att 28.895245 loss_ctc 31.805706 loss_rnnt 21.057014 hw_loss 0.144514 lr 0.00042740 rank 4
2023-02-23 08:50:23,960 DEBUG TRAIN Batch 16/3400 loss 20.891939 loss_att 21.711281 loss_ctc 26.970257 loss_rnnt 19.821186 hw_loss 0.180827 lr 0.00042740 rank 2
2023-02-23 08:50:23,960 DEBUG TRAIN Batch 16/3400 loss 6.072740 loss_att 8.217384 loss_ctc 6.854102 loss_rnnt 5.421604 hw_loss 0.221296 lr 0.00042738 rank 6
2023-02-23 08:50:23,961 DEBUG TRAIN Batch 16/3400 loss 5.742424 loss_att 9.023345 loss_ctc 8.125536 loss_rnnt 4.686607 hw_loss 0.153534 lr 0.00042743 rank 1
2023-02-23 08:50:23,963 DEBUG TRAIN Batch 16/3400 loss 9.039651 loss_att 13.339607 loss_ctc 15.723849 loss_rnnt 7.114669 hw_loss 0.325807 lr 0.00042737 rank 3
2023-02-23 08:50:23,964 DEBUG TRAIN Batch 16/3400 loss 8.882624 loss_att 11.772043 loss_ctc 14.720220 loss_rnnt 7.396592 hw_loss 0.243379 lr 0.00042732 rank 5
2023-02-23 08:50:23,965 DEBUG TRAIN Batch 16/3400 loss 15.361178 loss_att 16.600628 loss_ctc 20.559616 loss_rnnt 14.325392 hw_loss 0.177699 lr 0.00042742 rank 0
2023-02-23 08:50:23,967 DEBUG TRAIN Batch 16/3400 loss 10.620437 loss_att 13.604550 loss_ctc 13.805038 loss_rnnt 9.498813 hw_loss 0.187853 lr 0.00042735 rank 7
2023-02-23 08:51:41,923 DEBUG TRAIN Batch 16/3500 loss 8.971030 loss_att 13.183147 loss_ctc 15.662743 loss_rnnt 7.086047 hw_loss 0.281874 lr 0.00042716 rank 5
2023-02-23 08:51:41,926 DEBUG TRAIN Batch 16/3500 loss 15.448172 loss_att 16.421608 loss_ctc 20.800316 loss_rnnt 14.391883 hw_loss 0.277467 lr 0.00042725 rank 2
2023-02-23 08:51:41,929 DEBUG TRAIN Batch 16/3500 loss 10.715699 loss_att 14.140173 loss_ctc 17.493746 loss_rnnt 9.003572 hw_loss 0.231551 lr 0.00042725 rank 4
2023-02-23 08:51:41,930 DEBUG TRAIN Batch 16/3500 loss 13.353201 loss_att 14.085712 loss_ctc 14.259681 loss_rnnt 12.996622 hw_loss 0.167273 lr 0.00042726 rank 0
2023-02-23 08:51:41,932 DEBUG TRAIN Batch 16/3500 loss 4.755071 loss_att 7.528265 loss_ctc 7.320115 loss_rnnt 3.767338 hw_loss 0.170789 lr 0.00042722 rank 3
2023-02-23 08:51:41,935 DEBUG TRAIN Batch 16/3500 loss 8.667258 loss_att 13.186846 loss_ctc 12.285345 loss_rnnt 7.163724 hw_loss 0.219757 lr 0.00042723 rank 6
2023-02-23 08:51:41,941 DEBUG TRAIN Batch 16/3500 loss 13.968132 loss_att 18.068975 loss_ctc 16.477198 loss_rnnt 12.705268 hw_loss 0.202790 lr 0.00042719 rank 7
2023-02-23 08:51:41,978 DEBUG TRAIN Batch 16/3500 loss 16.335516 loss_att 22.080545 loss_ctc 22.943895 loss_rnnt 14.131626 hw_loss 0.325813 lr 0.00042727 rank 1
2023-02-23 08:52:59,931 DEBUG TRAIN Batch 16/3600 loss 15.418153 loss_att 17.995186 loss_ctc 22.865078 loss_rnnt 13.746431 hw_loss 0.306358 lr 0.00042711 rank 0
2023-02-23 08:52:59,931 DEBUG TRAIN Batch 16/3600 loss 11.123047 loss_att 14.701955 loss_ctc 13.627344 loss_rnnt 9.922496 hw_loss 0.282867 lr 0.00042707 rank 6
2023-02-23 08:52:59,932 DEBUG TRAIN Batch 16/3600 loss 23.090654 loss_att 24.771286 loss_ctc 29.525007 loss_rnnt 21.818920 hw_loss 0.145680 lr 0.00042709 rank 2
2023-02-23 08:52:59,935 DEBUG TRAIN Batch 16/3600 loss 13.916842 loss_att 15.585502 loss_ctc 18.309729 loss_rnnt 12.821611 hw_loss 0.329585 lr 0.00042706 rank 3
2023-02-23 08:52:59,937 DEBUG TRAIN Batch 16/3600 loss 11.968770 loss_att 16.549477 loss_ctc 18.688143 loss_rnnt 10.061007 hw_loss 0.179446 lr 0.00042711 rank 1
2023-02-23 08:52:59,940 DEBUG TRAIN Batch 16/3600 loss 6.313324 loss_att 10.499808 loss_ctc 6.844258 loss_rnnt 5.231378 hw_loss 0.325985 lr 0.00042704 rank 7
2023-02-23 08:52:59,961 DEBUG TRAIN Batch 16/3600 loss 10.327736 loss_att 14.161773 loss_ctc 12.464108 loss_rnnt 9.146921 hw_loss 0.242171 lr 0.00042709 rank 4
2023-02-23 08:52:59,970 DEBUG TRAIN Batch 16/3600 loss 11.919066 loss_att 15.499822 loss_ctc 16.205719 loss_rnnt 10.488976 hw_loss 0.266971 lr 0.00042701 rank 5
2023-02-23 08:54:14,959 DEBUG TRAIN Batch 16/3700 loss 8.451003 loss_att 9.449497 loss_ctc 10.032715 loss_rnnt 7.881755 hw_loss 0.297478 lr 0.00042693 rank 2
2023-02-23 08:54:14,962 DEBUG TRAIN Batch 16/3700 loss 5.900229 loss_att 10.021763 loss_ctc 9.483709 loss_rnnt 4.440206 hw_loss 0.296097 lr 0.00042685 rank 5
2023-02-23 08:54:14,963 DEBUG TRAIN Batch 16/3700 loss 10.514254 loss_att 13.556381 loss_ctc 13.774654 loss_rnnt 9.386024 hw_loss 0.159535 lr 0.00042693 rank 4
2023-02-23 08:54:14,963 DEBUG TRAIN Batch 16/3700 loss 9.183338 loss_att 10.724402 loss_ctc 11.661285 loss_rnnt 8.457881 hw_loss 0.162847 lr 0.00042690 rank 3
2023-02-23 08:54:14,964 DEBUG TRAIN Batch 16/3700 loss 9.102763 loss_att 13.683606 loss_ctc 13.536489 loss_rnnt 7.457415 hw_loss 0.258780 lr 0.00042696 rank 1
2023-02-23 08:54:14,966 DEBUG TRAIN Batch 16/3700 loss 7.162731 loss_att 10.419284 loss_ctc 9.044461 loss_rnnt 6.083007 hw_loss 0.332843 lr 0.00042695 rank 0
2023-02-23 08:54:14,969 DEBUG TRAIN Batch 16/3700 loss 11.437166 loss_att 11.196596 loss_ctc 18.799364 loss_rnnt 10.394388 hw_loss 0.204873 lr 0.00042692 rank 6
2023-02-23 08:54:14,969 DEBUG TRAIN Batch 16/3700 loss 11.306186 loss_att 13.569759 loss_ctc 14.507303 loss_rnnt 10.275373 hw_loss 0.283654 lr 0.00042688 rank 7
2023-02-23 08:55:31,010 DEBUG TRAIN Batch 16/3800 loss 6.300788 loss_att 7.649428 loss_ctc 8.709753 loss_rnnt 5.525627 hw_loss 0.345446 lr 0.00042678 rank 4
2023-02-23 08:55:31,011 DEBUG TRAIN Batch 16/3800 loss 8.818622 loss_att 11.475339 loss_ctc 13.997939 loss_rnnt 7.472652 hw_loss 0.232594 lr 0.00042680 rank 0
2023-02-23 08:55:31,011 DEBUG TRAIN Batch 16/3800 loss 8.030986 loss_att 11.240211 loss_ctc 11.579102 loss_rnnt 6.760749 hw_loss 0.291205 lr 0.00042670 rank 5
2023-02-23 08:55:31,012 DEBUG TRAIN Batch 16/3800 loss 2.745557 loss_att 5.017235 loss_ctc 4.793500 loss_rnnt 1.842667 hw_loss 0.329053 lr 0.00042676 rank 6
2023-02-23 08:55:31,015 DEBUG TRAIN Batch 16/3800 loss 11.286434 loss_att 10.631247 loss_ctc 13.678014 loss_rnnt 10.985074 hw_loss 0.212849 lr 0.00042678 rank 2
2023-02-23 08:55:31,017 DEBUG TRAIN Batch 16/3800 loss 7.440528 loss_att 8.135586 loss_ctc 7.686400 loss_rnnt 7.081771 hw_loss 0.350556 lr 0.00042675 rank 3
2023-02-23 08:55:31,018 DEBUG TRAIN Batch 16/3800 loss 12.572577 loss_att 13.305308 loss_ctc 16.195988 loss_rnnt 11.789549 hw_loss 0.287551 lr 0.00042672 rank 7
2023-02-23 08:55:31,073 DEBUG TRAIN Batch 16/3800 loss 9.873836 loss_att 12.185485 loss_ctc 12.504712 loss_rnnt 8.886272 hw_loss 0.327093 lr 0.00042680 rank 1
2023-02-23 08:56:50,614 DEBUG TRAIN Batch 16/3900 loss 3.064366 loss_att 8.221334 loss_ctc 3.846914 loss_rnnt 1.772836 hw_loss 0.292117 lr 0.00042654 rank 5
2023-02-23 08:56:50,618 DEBUG TRAIN Batch 16/3900 loss 5.191794 loss_att 8.118832 loss_ctc 5.929174 loss_rnnt 4.419629 hw_loss 0.165826 lr 0.00042660 rank 6
2023-02-23 08:56:50,619 DEBUG TRAIN Batch 16/3900 loss 8.553574 loss_att 9.713034 loss_ctc 10.041777 loss_rnnt 7.922759 hw_loss 0.375928 lr 0.00042664 rank 0
2023-02-23 08:56:50,621 DEBUG TRAIN Batch 16/3900 loss 20.294172 loss_att 23.456493 loss_ctc 25.528282 loss_rnnt 18.895927 hw_loss 0.127313 lr 0.00042659 rank 3
2023-02-23 08:56:50,623 DEBUG TRAIN Batch 16/3900 loss 3.254561 loss_att 6.824805 loss_ctc 5.045392 loss_rnnt 2.216716 hw_loss 0.159410 lr 0.00042662 rank 2
2023-02-23 08:56:50,626 DEBUG TRAIN Batch 16/3900 loss 12.007745 loss_att 13.678602 loss_ctc 13.653748 loss_rnnt 11.330630 hw_loss 0.231519 lr 0.00042665 rank 1
2023-02-23 08:56:50,628 DEBUG TRAIN Batch 16/3900 loss 7.868306 loss_att 10.520576 loss_ctc 10.913921 loss_rnnt 6.748833 hw_loss 0.343008 lr 0.00042657 rank 7
2023-02-23 08:56:50,630 DEBUG TRAIN Batch 16/3900 loss 11.965908 loss_att 15.230318 loss_ctc 14.275142 loss_rnnt 10.866133 hw_loss 0.260618 lr 0.00042662 rank 4
2023-02-23 08:58:07,662 DEBUG TRAIN Batch 16/4000 loss 5.376101 loss_att 10.096027 loss_ctc 7.180607 loss_rnnt 4.103342 hw_loss 0.165325 lr 0.00042647 rank 2
2023-02-23 08:58:07,667 DEBUG TRAIN Batch 16/4000 loss 18.295385 loss_att 22.223322 loss_ctc 26.594732 loss_rnnt 16.311848 hw_loss 0.171319 lr 0.00042641 rank 7
2023-02-23 08:58:07,667 DEBUG TRAIN Batch 16/4000 loss 15.318482 loss_att 17.852663 loss_ctc 20.106598 loss_rnnt 14.089125 hw_loss 0.157699 lr 0.00042639 rank 5
2023-02-23 08:58:07,667 DEBUG TRAIN Batch 16/4000 loss 8.325451 loss_att 11.492395 loss_ctc 13.105624 loss_rnnt 6.890419 hw_loss 0.308036 lr 0.00042645 rank 6
2023-02-23 08:58:07,671 DEBUG TRAIN Batch 16/4000 loss 12.435880 loss_att 12.951817 loss_ctc 19.013765 loss_rnnt 11.333691 hw_loss 0.228656 lr 0.00042647 rank 4
2023-02-23 08:58:07,672 DEBUG TRAIN Batch 16/4000 loss 13.420494 loss_att 18.065409 loss_ctc 15.820459 loss_rnnt 12.055182 hw_loss 0.218130 lr 0.00042644 rank 3
2023-02-23 08:58:07,673 DEBUG TRAIN Batch 16/4000 loss 10.434490 loss_att 9.890284 loss_ctc 12.245600 loss_rnnt 10.152596 hw_loss 0.279850 lr 0.00042649 rank 0
2023-02-23 08:58:07,673 DEBUG TRAIN Batch 16/4000 loss 7.198603 loss_att 11.765127 loss_ctc 10.444072 loss_rnnt 5.711188 hw_loss 0.265088 lr 0.00042649 rank 1
2023-02-23 08:59:22,853 DEBUG TRAIN Batch 16/4100 loss 12.966175 loss_att 19.218325 loss_ctc 20.305996 loss_rnnt 10.640606 hw_loss 0.180930 lr 0.00042623 rank 5
2023-02-23 08:59:22,853 DEBUG TRAIN Batch 16/4100 loss 12.366411 loss_att 14.660078 loss_ctc 17.803169 loss_rnnt 11.022592 hw_loss 0.300346 lr 0.00042631 rank 2
2023-02-23 08:59:22,856 DEBUG TRAIN Batch 16/4100 loss 10.664515 loss_att 12.192187 loss_ctc 10.636713 loss_rnnt 10.232893 hw_loss 0.243364 lr 0.00042634 rank 1
2023-02-23 08:59:22,859 DEBUG TRAIN Batch 16/4100 loss 9.374941 loss_att 14.054131 loss_ctc 11.687222 loss_rnnt 8.050896 hw_loss 0.149817 lr 0.00042628 rank 3
2023-02-23 08:59:22,859 DEBUG TRAIN Batch 16/4100 loss 9.338869 loss_att 14.515235 loss_ctc 17.573221 loss_rnnt 7.133205 hw_loss 0.135893 lr 0.00042629 rank 6
2023-02-23 08:59:22,859 DEBUG TRAIN Batch 16/4100 loss 23.423077 loss_att 21.870445 loss_ctc 23.272163 loss_rnnt 23.586826 hw_loss 0.312929 lr 0.00042633 rank 0
2023-02-23 08:59:22,859 DEBUG TRAIN Batch 16/4100 loss 10.445800 loss_att 15.097262 loss_ctc 11.217174 loss_rnnt 9.267124 hw_loss 0.272875 lr 0.00042631 rank 4
2023-02-23 08:59:22,866 DEBUG TRAIN Batch 16/4100 loss 12.451877 loss_att 12.730793 loss_ctc 12.396387 loss_rnnt 12.278513 hw_loss 0.234334 lr 0.00042626 rank 7
2023-02-23 09:00:39,878 DEBUG TRAIN Batch 16/4200 loss 10.837510 loss_att 12.356010 loss_ctc 14.989803 loss_rnnt 9.811133 hw_loss 0.316948 lr 0.00042618 rank 0
2023-02-23 09:00:39,879 DEBUG TRAIN Batch 16/4200 loss 3.346070 loss_att 6.126439 loss_ctc 4.686318 loss_rnnt 2.454703 hw_loss 0.293612 lr 0.00042616 rank 2
2023-02-23 09:00:39,880 DEBUG TRAIN Batch 16/4200 loss 20.669724 loss_att 19.881599 loss_ctc 23.084341 loss_rnnt 20.416870 hw_loss 0.165993 lr 0.00042608 rank 5
2023-02-23 09:00:39,881 DEBUG TRAIN Batch 16/4200 loss 3.821885 loss_att 5.935438 loss_ctc 4.279293 loss_rnnt 3.215056 hw_loss 0.230870 lr 0.00042613 rank 3
2023-02-23 09:00:39,881 DEBUG TRAIN Batch 16/4200 loss 9.323199 loss_att 14.403150 loss_ctc 16.863731 loss_rnnt 7.185258 hw_loss 0.218525 lr 0.00042616 rank 4
2023-02-23 09:00:39,882 DEBUG TRAIN Batch 16/4200 loss 5.250557 loss_att 8.456074 loss_ctc 6.939562 loss_rnnt 4.254555 hw_loss 0.243186 lr 0.00042614 rank 6
2023-02-23 09:00:39,883 DEBUG TRAIN Batch 16/4200 loss 7.558646 loss_att 11.953141 loss_ctc 14.198917 loss_rnnt 5.707834 hw_loss 0.162269 lr 0.00042610 rank 7
2023-02-23 09:00:39,888 DEBUG TRAIN Batch 16/4200 loss 4.104132 loss_att 6.578525 loss_ctc 5.517867 loss_rnnt 3.200204 hw_loss 0.413533 lr 0.00042618 rank 1
2023-02-23 09:01:57,637 DEBUG TRAIN Batch 16/4300 loss 11.496183 loss_att 14.679618 loss_ctc 14.179371 loss_rnnt 10.420288 hw_loss 0.152719 lr 0.00042600 rank 4
2023-02-23 09:01:57,641 DEBUG TRAIN Batch 16/4300 loss 16.834105 loss_att 22.303009 loss_ctc 27.004564 loss_rnnt 14.234028 hw_loss 0.281686 lr 0.00042600 rank 2
2023-02-23 09:01:57,647 DEBUG TRAIN Batch 16/4300 loss 3.316099 loss_att 4.248950 loss_ctc 3.984337 loss_rnnt 2.929887 hw_loss 0.207270 lr 0.00042592 rank 5
2023-02-23 09:01:57,648 DEBUG TRAIN Batch 16/4300 loss 12.865912 loss_att 15.599174 loss_ctc 20.189690 loss_rnnt 11.272226 hw_loss 0.132245 lr 0.00042595 rank 7
2023-02-23 09:01:57,649 DEBUG TRAIN Batch 16/4300 loss 7.298097 loss_att 10.403446 loss_ctc 8.983906 loss_rnnt 6.335430 hw_loss 0.219043 lr 0.00042602 rank 0
2023-02-23 09:01:57,653 DEBUG TRAIN Batch 16/4300 loss 5.318261 loss_att 7.903437 loss_ctc 5.172075 loss_rnnt 4.668818 hw_loss 0.284813 lr 0.00042603 rank 1
2023-02-23 09:01:57,652 DEBUG TRAIN Batch 16/4300 loss 9.815765 loss_att 11.994176 loss_ctc 12.303619 loss_rnnt 8.963859 hw_loss 0.158457 lr 0.00042597 rank 3
2023-02-23 09:01:57,709 DEBUG TRAIN Batch 16/4300 loss 14.932204 loss_att 17.098587 loss_ctc 17.655428 loss_rnnt 14.000032 hw_loss 0.254622 lr 0.00042598 rank 6
2023-02-23 09:03:14,416 DEBUG TRAIN Batch 16/4400 loss 12.659198 loss_att 12.465361 loss_ctc 17.486059 loss_rnnt 11.898419 hw_loss 0.292431 lr 0.00042577 rank 5
2023-02-23 09:03:14,423 DEBUG TRAIN Batch 16/4400 loss 25.153330 loss_att 26.740387 loss_ctc 31.842793 loss_rnnt 23.812275 hw_loss 0.246964 lr 0.00042579 rank 7
2023-02-23 09:03:14,424 DEBUG TRAIN Batch 16/4400 loss 2.312947 loss_att 4.872859 loss_ctc 4.381127 loss_rnnt 1.391414 hw_loss 0.250861 lr 0.00042585 rank 4
2023-02-23 09:03:14,424 DEBUG TRAIN Batch 16/4400 loss 20.436596 loss_att 21.424118 loss_ctc 28.391575 loss_rnnt 19.062752 hw_loss 0.216894 lr 0.00042587 rank 1
2023-02-23 09:03:14,425 DEBUG TRAIN Batch 16/4400 loss 20.821703 loss_att 19.788643 loss_ctc 23.102516 loss_rnnt 20.575066 hw_loss 0.279640 lr 0.00042585 rank 2
2023-02-23 09:03:14,426 DEBUG TRAIN Batch 16/4400 loss 8.801970 loss_att 11.134294 loss_ctc 10.322693 loss_rnnt 7.975991 hw_loss 0.293905 lr 0.00042587 rank 0
2023-02-23 09:03:14,426 DEBUG TRAIN Batch 16/4400 loss 13.209745 loss_att 14.812927 loss_ctc 15.291102 loss_rnnt 12.483726 hw_loss 0.239755 lr 0.00042582 rank 3
2023-02-23 09:03:14,427 DEBUG TRAIN Batch 16/4400 loss 16.897932 loss_att 20.438492 loss_ctc 21.821264 loss_rnnt 15.393369 hw_loss 0.262511 lr 0.00042583 rank 6
2023-02-23 09:04:30,024 DEBUG TRAIN Batch 16/4500 loss 12.346131 loss_att 12.301147 loss_ctc 15.775653 loss_rnnt 11.724499 hw_loss 0.325050 lr 0.00042569 rank 4
2023-02-23 09:04:30,026 DEBUG TRAIN Batch 16/4500 loss 15.541919 loss_att 22.168171 loss_ctc 20.763493 loss_rnnt 13.368486 hw_loss 0.284947 lr 0.00042567 rank 3
2023-02-23 09:04:30,031 DEBUG TRAIN Batch 16/4500 loss 7.783121 loss_att 12.304031 loss_ctc 8.483582 loss_rnnt 6.666850 hw_loss 0.222551 lr 0.00042569 rank 2
2023-02-23 09:04:30,031 DEBUG TRAIN Batch 16/4500 loss 9.178333 loss_att 11.764788 loss_ctc 9.796013 loss_rnnt 8.439667 hw_loss 0.260659 lr 0.00042572 rank 1
2023-02-23 09:04:30,031 DEBUG TRAIN Batch 16/4500 loss 8.140409 loss_att 11.436428 loss_ctc 15.075825 loss_rnnt 6.460281 hw_loss 0.180381 lr 0.00042561 rank 5
2023-02-23 09:04:30,035 DEBUG TRAIN Batch 16/4500 loss 11.163084 loss_att 12.962368 loss_ctc 15.572117 loss_rnnt 10.080744 hw_loss 0.252399 lr 0.00042568 rank 6
2023-02-23 09:04:30,035 DEBUG TRAIN Batch 16/4500 loss 10.954791 loss_att 14.071880 loss_ctc 16.110916 loss_rnnt 9.522514 hw_loss 0.227578 lr 0.00042571 rank 0
2023-02-23 09:04:30,037 DEBUG TRAIN Batch 16/4500 loss 21.128462 loss_att 23.342239 loss_ctc 22.817144 loss_rnnt 20.312168 hw_loss 0.278210 lr 0.00042564 rank 7
2023-02-23 09:05:48,969 DEBUG TRAIN Batch 16/4600 loss 10.550688 loss_att 14.606930 loss_ctc 14.576899 loss_rnnt 9.116667 hw_loss 0.161148 lr 0.00042546 rank 5
2023-02-23 09:05:48,974 DEBUG TRAIN Batch 16/4600 loss 10.508783 loss_att 16.972265 loss_ctc 14.306919 loss_rnnt 8.566625 hw_loss 0.268207 lr 0.00042554 rank 4
2023-02-23 09:05:48,975 DEBUG TRAIN Batch 16/4600 loss 14.598001 loss_att 18.411568 loss_ctc 18.361710 loss_rnnt 13.255850 hw_loss 0.145519 lr 0.00042556 rank 0
2023-02-23 09:05:48,976 DEBUG TRAIN Batch 16/4600 loss 3.068811 loss_att 7.337346 loss_ctc 6.209623 loss_rnnt 1.694746 hw_loss 0.190468 lr 0.00042552 rank 6
2023-02-23 09:05:48,979 DEBUG TRAIN Batch 16/4600 loss 11.510450 loss_att 13.538486 loss_ctc 16.677269 loss_rnnt 10.299650 hw_loss 0.218032 lr 0.00042551 rank 3
2023-02-23 09:05:48,981 DEBUG TRAIN Batch 16/4600 loss 8.002393 loss_att 10.889614 loss_ctc 10.985888 loss_rnnt 6.920503 hw_loss 0.199961 lr 0.00042554 rank 2
2023-02-23 09:05:48,984 DEBUG TRAIN Batch 16/4600 loss 4.306267 loss_att 7.000262 loss_ctc 5.351919 loss_rnnt 3.480276 hw_loss 0.277072 lr 0.00042549 rank 7
2023-02-23 09:05:48,985 DEBUG TRAIN Batch 16/4600 loss 6.571157 loss_att 10.221917 loss_ctc 8.006117 loss_rnnt 5.541474 hw_loss 0.202880 lr 0.00042557 rank 1
2023-02-23 09:07:07,557 DEBUG TRAIN Batch 16/4700 loss 11.892168 loss_att 17.529186 loss_ctc 16.419930 loss_rnnt 10.034606 hw_loss 0.237108 lr 0.00042531 rank 5
2023-02-23 09:07:07,557 DEBUG TRAIN Batch 16/4700 loss 17.154043 loss_att 18.044512 loss_ctc 17.419071 loss_rnnt 16.754276 hw_loss 0.349379 lr 0.00042536 rank 3
2023-02-23 09:07:07,560 DEBUG TRAIN Batch 16/4700 loss 9.318075 loss_att 10.858968 loss_ctc 13.235807 loss_rnnt 8.319714 hw_loss 0.314659 lr 0.00042539 rank 4
2023-02-23 09:07:07,561 DEBUG TRAIN Batch 16/4700 loss 5.621563 loss_att 5.995489 loss_ctc 6.323110 loss_rnnt 5.310750 hw_loss 0.267165 lr 0.00042540 rank 0
2023-02-23 09:07:07,562 DEBUG TRAIN Batch 16/4700 loss 6.739406 loss_att 10.324999 loss_ctc 8.048739 loss_rnnt 5.771950 hw_loss 0.142049 lr 0.00042539 rank 2
2023-02-23 09:07:07,564 DEBUG TRAIN Batch 16/4700 loss 13.927657 loss_att 15.956148 loss_ctc 15.951370 loss_rnnt 13.111118 hw_loss 0.264398 lr 0.00042541 rank 1
2023-02-23 09:07:07,565 DEBUG TRAIN Batch 16/4700 loss 12.862752 loss_att 15.615530 loss_ctc 18.351784 loss_rnnt 11.416510 hw_loss 0.307156 lr 0.00042533 rank 7
2023-02-23 09:07:07,609 DEBUG TRAIN Batch 16/4700 loss 25.989624 loss_att 29.543344 loss_ctc 28.051308 loss_rnnt 24.866432 hw_loss 0.257921 lr 0.00042537 rank 6
2023-02-23 09:08:23,721 DEBUG TRAIN Batch 16/4800 loss 5.095071 loss_att 8.107783 loss_ctc 7.371878 loss_rnnt 4.053442 hw_loss 0.254086 lr 0.00042515 rank 5
2023-02-23 09:08:23,724 DEBUG TRAIN Batch 16/4800 loss 9.466759 loss_att 13.504835 loss_ctc 11.859365 loss_rnnt 8.195807 hw_loss 0.270606 lr 0.00042523 rank 4
2023-02-23 09:08:23,724 DEBUG TRAIN Batch 16/4800 loss 13.484194 loss_att 15.612274 loss_ctc 17.739336 loss_rnnt 12.349885 hw_loss 0.265013 lr 0.00042523 rank 2
2023-02-23 09:08:23,726 DEBUG TRAIN Batch 16/4800 loss 7.708950 loss_att 13.647978 loss_ctc 9.694040 loss_rnnt 6.129651 hw_loss 0.237777 lr 0.00042521 rank 6
2023-02-23 09:08:23,727 DEBUG TRAIN Batch 16/4800 loss 13.202353 loss_att 15.087770 loss_ctc 15.721094 loss_rnnt 12.404589 hw_loss 0.159091 lr 0.00042526 rank 1
2023-02-23 09:08:23,727 DEBUG TRAIN Batch 16/4800 loss 20.684835 loss_att 20.019960 loss_ctc 24.274685 loss_rnnt 20.224854 hw_loss 0.214329 lr 0.00042525 rank 0
2023-02-23 09:08:23,728 DEBUG TRAIN Batch 16/4800 loss 1.629635 loss_att 5.588870 loss_ctc 2.378056 loss_rnnt 0.666666 hw_loss 0.133748 lr 0.00042520 rank 3
2023-02-23 09:08:23,729 DEBUG TRAIN Batch 16/4800 loss 12.545303 loss_att 17.542175 loss_ctc 21.217148 loss_rnnt 10.288503 hw_loss 0.189713 lr 0.00042518 rank 7
2023-02-23 09:09:39,320 DEBUG TRAIN Batch 16/4900 loss 11.816676 loss_att 14.336807 loss_ctc 19.637032 loss_rnnt 10.190064 hw_loss 0.149759 lr 0.00042500 rank 5
2023-02-23 09:09:39,321 DEBUG TRAIN Batch 16/4900 loss 8.391542 loss_att 11.336977 loss_ctc 11.366199 loss_rnnt 7.278348 hw_loss 0.239036 lr 0.00042508 rank 4
2023-02-23 09:09:39,326 DEBUG TRAIN Batch 16/4900 loss 6.776461 loss_att 8.134342 loss_ctc 9.733086 loss_rnnt 5.926217 hw_loss 0.345844 lr 0.00042505 rank 3
2023-02-23 09:09:39,327 DEBUG TRAIN Batch 16/4900 loss 19.795633 loss_att 19.553642 loss_ctc 22.945156 loss_rnnt 19.361315 hw_loss 0.117714 lr 0.00042503 rank 7
2023-02-23 09:09:39,328 DEBUG TRAIN Batch 16/4900 loss 7.028296 loss_att 12.040654 loss_ctc 12.648296 loss_rnnt 5.120597 hw_loss 0.292303 lr 0.00042508 rank 2
2023-02-23 09:09:39,329 DEBUG TRAIN Batch 16/4900 loss 22.104565 loss_att 22.586567 loss_ctc 25.344822 loss_rnnt 21.426361 hw_loss 0.280815 lr 0.00042510 rank 1
2023-02-23 09:09:39,331 DEBUG TRAIN Batch 16/4900 loss 8.403095 loss_att 11.156553 loss_ctc 11.942382 loss_rnnt 7.318076 hw_loss 0.117042 lr 0.00042510 rank 0
2023-02-23 09:09:39,347 DEBUG TRAIN Batch 16/4900 loss 7.964073 loss_att 10.010843 loss_ctc 9.959261 loss_rnnt 7.169766 hw_loss 0.222988 lr 0.00042506 rank 6
2023-02-23 09:10:59,070 DEBUG TRAIN Batch 16/5000 loss 5.637751 loss_att 7.021995 loss_ctc 9.385719 loss_rnnt 4.678647 hw_loss 0.342235 lr 0.00042490 rank 3
2023-02-23 09:10:59,073 DEBUG TRAIN Batch 16/5000 loss 30.919718 loss_att 33.170959 loss_ctc 38.690483 loss_rnnt 29.295502 hw_loss 0.258498 lr 0.00042494 rank 0
2023-02-23 09:10:59,073 DEBUG TRAIN Batch 16/5000 loss 7.105031 loss_att 10.821847 loss_ctc 10.240005 loss_rnnt 5.789477 hw_loss 0.289112 lr 0.00042493 rank 2
2023-02-23 09:10:59,073 DEBUG TRAIN Batch 16/5000 loss 14.689384 loss_att 16.148594 loss_ctc 20.107721 loss_rnnt 13.527778 hw_loss 0.276228 lr 0.00042487 rank 7
2023-02-23 09:10:59,075 DEBUG TRAIN Batch 16/5000 loss 4.481392 loss_att 6.818131 loss_ctc 5.603494 loss_rnnt 3.764679 hw_loss 0.187034 lr 0.00042493 rank 4
2023-02-23 09:10:59,076 DEBUG TRAIN Batch 16/5000 loss 9.155880 loss_att 10.175631 loss_ctc 12.662253 loss_rnnt 8.366681 hw_loss 0.220746 lr 0.00042485 rank 5
2023-02-23 09:10:59,077 DEBUG TRAIN Batch 16/5000 loss 9.719421 loss_att 11.406294 loss_ctc 12.161384 loss_rnnt 8.960408 hw_loss 0.180082 lr 0.00042495 rank 1
2023-02-23 09:10:59,121 DEBUG TRAIN Batch 16/5000 loss 19.313028 loss_att 23.199255 loss_ctc 27.875065 loss_rnnt 17.256584 hw_loss 0.257986 lr 0.00042491 rank 6
2023-02-23 09:12:15,005 DEBUG TRAIN Batch 16/5100 loss 20.413433 loss_att 22.980206 loss_ctc 28.007442 loss_rnnt 18.782194 hw_loss 0.197529 lr 0.00042469 rank 5
2023-02-23 09:12:15,006 DEBUG TRAIN Batch 16/5100 loss 11.611127 loss_att 14.795039 loss_ctc 13.752520 loss_rnnt 10.557960 hw_loss 0.245373 lr 0.00042475 rank 6
2023-02-23 09:12:15,007 DEBUG TRAIN Batch 16/5100 loss 8.911620 loss_att 10.846509 loss_ctc 16.687252 loss_rnnt 7.391269 hw_loss 0.181167 lr 0.00042479 rank 0
2023-02-23 09:12:15,007 DEBUG TRAIN Batch 16/5100 loss 19.697477 loss_att 23.624691 loss_ctc 31.858471 loss_rnnt 17.156681 hw_loss 0.251042 lr 0.00042477 rank 4
2023-02-23 09:12:15,008 DEBUG TRAIN Batch 16/5100 loss 14.902212 loss_att 15.296579 loss_ctc 19.891800 loss_rnnt 14.006834 hw_loss 0.283552 lr 0.00042480 rank 1
2023-02-23 09:12:15,010 DEBUG TRAIN Batch 16/5100 loss 13.696695 loss_att 16.666155 loss_ctc 21.870922 loss_rnnt 11.924010 hw_loss 0.166680 lr 0.00042472 rank 7
2023-02-23 09:12:15,013 DEBUG TRAIN Batch 16/5100 loss 10.913586 loss_att 14.746279 loss_ctc 14.631714 loss_rnnt 9.513021 hw_loss 0.259265 lr 0.00042474 rank 3
2023-02-23 09:12:15,015 DEBUG TRAIN Batch 16/5100 loss 9.819300 loss_att 10.080727 loss_ctc 12.064850 loss_rnnt 9.311097 hw_loss 0.293457 lr 0.00042477 rank 2
2023-02-23 09:13:32,635 DEBUG TRAIN Batch 16/5200 loss 8.131440 loss_att 12.479506 loss_ctc 10.399734 loss_rnnt 6.850677 hw_loss 0.203832 lr 0.00042464 rank 0
2023-02-23 09:13:32,637 DEBUG TRAIN Batch 16/5200 loss 10.151571 loss_att 12.191193 loss_ctc 13.515403 loss_rnnt 9.062433 hw_loss 0.436319 lr 0.00042462 rank 2
2023-02-23 09:13:32,643 DEBUG TRAIN Batch 16/5200 loss 7.475180 loss_att 12.655909 loss_ctc 8.976167 loss_rnnt 6.161601 hw_loss 0.144939 lr 0.00042454 rank 5
2023-02-23 09:13:32,644 DEBUG TRAIN Batch 16/5200 loss 8.118341 loss_att 10.429180 loss_ctc 17.106342 loss_rnnt 6.301652 hw_loss 0.292728 lr 0.00042464 rank 1
2023-02-23 09:13:32,645 DEBUG TRAIN Batch 16/5200 loss 13.052794 loss_att 18.120369 loss_ctc 21.058210 loss_rnnt 10.861755 hw_loss 0.206502 lr 0.00042457 rank 7
2023-02-23 09:13:32,645 DEBUG TRAIN Batch 16/5200 loss 4.462180 loss_att 8.590210 loss_ctc 7.312385 loss_rnnt 3.083763 hw_loss 0.323970 lr 0.00042462 rank 4
2023-02-23 09:13:32,647 DEBUG TRAIN Batch 16/5200 loss 19.007343 loss_att 22.712423 loss_ctc 27.699055 loss_rnnt 16.995441 hw_loss 0.209979 lr 0.00042459 rank 3
2023-02-23 09:13:32,648 DEBUG TRAIN Batch 16/5200 loss 15.284299 loss_att 14.049202 loss_ctc 15.483869 loss_rnnt 15.327393 hw_loss 0.332468 lr 0.00042460 rank 6
2023-02-23 09:14:50,671 DEBUG TRAIN Batch 16/5300 loss 12.907042 loss_att 18.037066 loss_ctc 20.847790 loss_rnnt 10.711334 hw_loss 0.208006 lr 0.00042447 rank 2
2023-02-23 09:14:50,676 DEBUG TRAIN Batch 16/5300 loss 13.255891 loss_att 14.461109 loss_ctc 16.052555 loss_rnnt 12.479506 hw_loss 0.304600 lr 0.00042448 rank 0
2023-02-23 09:14:50,677 DEBUG TRAIN Batch 16/5300 loss 24.371822 loss_att 27.165127 loss_ctc 35.000130 loss_rnnt 22.286076 hw_loss 0.206203 lr 0.00042441 rank 7
2023-02-23 09:14:50,682 DEBUG TRAIN Batch 16/5300 loss 13.740699 loss_att 17.293911 loss_ctc 19.594725 loss_rnnt 12.115450 hw_loss 0.251380 lr 0.00042445 rank 6
2023-02-23 09:14:50,684 DEBUG TRAIN Batch 16/5300 loss 6.006673 loss_att 8.645655 loss_ctc 8.647074 loss_rnnt 5.001788 hw_loss 0.234441 lr 0.00042449 rank 1
2023-02-23 09:14:50,706 DEBUG TRAIN Batch 16/5300 loss 10.441752 loss_att 12.477539 loss_ctc 14.509768 loss_rnnt 9.336609 hw_loss 0.291723 lr 0.00042439 rank 5
2023-02-23 09:14:50,718 DEBUG TRAIN Batch 16/5300 loss 5.191695 loss_att 7.607428 loss_ctc 6.348945 loss_rnnt 4.404326 hw_loss 0.281104 lr 0.00042447 rank 4
2023-02-23 09:14:50,722 DEBUG TRAIN Batch 16/5300 loss 18.149055 loss_att 20.599337 loss_ctc 25.560257 loss_rnnt 16.577793 hw_loss 0.174462 lr 0.00042444 rank 3
2023-02-23 09:16:07,488 DEBUG TRAIN Batch 16/5400 loss 13.941542 loss_att 15.563623 loss_ctc 18.687534 loss_rnnt 12.873856 hw_loss 0.207134 lr 0.00042431 rank 4
2023-02-23 09:16:07,489 DEBUG TRAIN Batch 16/5400 loss 7.297408 loss_att 11.205802 loss_ctc 9.074339 loss_rnnt 6.140380 hw_loss 0.259547 lr 0.00042431 rank 2
2023-02-23 09:16:07,496 DEBUG TRAIN Batch 16/5400 loss 14.019300 loss_att 16.003946 loss_ctc 21.590527 loss_rnnt 12.526260 hw_loss 0.162398 lr 0.00042429 rank 6
2023-02-23 09:16:07,496 DEBUG TRAIN Batch 16/5400 loss 17.008963 loss_att 19.184397 loss_ctc 21.947739 loss_rnnt 15.823490 hw_loss 0.172281 lr 0.00042423 rank 5
2023-02-23 09:16:07,496 DEBUG TRAIN Batch 16/5400 loss 12.594692 loss_att 14.514968 loss_ctc 20.750719 loss_rnnt 11.008268 hw_loss 0.215436 lr 0.00042433 rank 0
2023-02-23 09:16:07,497 DEBUG TRAIN Batch 16/5400 loss 11.770482 loss_att 17.947929 loss_ctc 16.614340 loss_rnnt 9.798539 hw_loss 0.169885 lr 0.00042428 rank 3
2023-02-23 09:16:07,499 DEBUG TRAIN Batch 16/5400 loss 11.667265 loss_att 13.921595 loss_ctc 12.423677 loss_rnnt 10.964996 hw_loss 0.282278 lr 0.00042434 rank 1
2023-02-23 09:16:07,502 DEBUG TRAIN Batch 16/5400 loss 10.429071 loss_att 16.097363 loss_ctc 16.356804 loss_rnnt 8.438543 hw_loss 0.124697 lr 0.00042426 rank 7
2023-02-23 09:17:24,123 DEBUG TRAIN Batch 16/5500 loss 12.462228 loss_att 15.276509 loss_ctc 16.514362 loss_rnnt 11.239629 hw_loss 0.223985 lr 0.00042408 rank 5
2023-02-23 09:17:24,128 DEBUG TRAIN Batch 16/5500 loss 10.758622 loss_att 12.183500 loss_ctc 16.127352 loss_rnnt 9.646704 hw_loss 0.208335 lr 0.00042413 rank 3
2023-02-23 09:17:24,128 DEBUG TRAIN Batch 16/5500 loss 6.662759 loss_att 10.287766 loss_ctc 9.076846 loss_rnnt 5.494477 hw_loss 0.227628 lr 0.00042416 rank 4
2023-02-23 09:17:24,130 DEBUG TRAIN Batch 16/5500 loss 11.542005 loss_att 13.857489 loss_ctc 12.715998 loss_rnnt 10.770699 hw_loss 0.284393 lr 0.00042418 rank 1
2023-02-23 09:17:24,131 DEBUG TRAIN Batch 16/5500 loss 7.688282 loss_att 12.411226 loss_ctc 12.263510 loss_rnnt 6.014492 hw_loss 0.223447 lr 0.00042416 rank 2
2023-02-23 09:17:24,131 DEBUG TRAIN Batch 16/5500 loss 11.026244 loss_att 13.962521 loss_ctc 19.019138 loss_rnnt 9.276795 hw_loss 0.180887 lr 0.00042411 rank 7
2023-02-23 09:17:24,131 DEBUG TRAIN Batch 16/5500 loss 16.648005 loss_att 16.707577 loss_ctc 21.739281 loss_rnnt 15.823864 hw_loss 0.250107 lr 0.00042418 rank 0
2023-02-23 09:17:24,134 DEBUG TRAIN Batch 16/5500 loss 11.143847 loss_att 12.326468 loss_ctc 16.398922 loss_rnnt 10.115377 hw_loss 0.171131 lr 0.00042414 rank 6
2023-02-23 09:18:41,068 DEBUG TRAIN Batch 16/5600 loss 23.692858 loss_att 26.317793 loss_ctc 32.190613 loss_rnnt 21.901905 hw_loss 0.249246 lr 0.00042401 rank 4
2023-02-23 09:18:41,070 DEBUG TRAIN Batch 16/5600 loss 6.769828 loss_att 8.638485 loss_ctc 8.825586 loss_rnnt 5.986989 hw_loss 0.253136 lr 0.00042398 rank 3
2023-02-23 09:18:41,073 DEBUG TRAIN Batch 16/5600 loss 10.212084 loss_att 13.190582 loss_ctc 14.539498 loss_rnnt 8.892803 hw_loss 0.274860 lr 0.00042402 rank 0
2023-02-23 09:18:41,075 DEBUG TRAIN Batch 16/5600 loss 5.545973 loss_att 8.573056 loss_ctc 8.212239 loss_rnnt 4.452333 hw_loss 0.248852 lr 0.00042395 rank 7
2023-02-23 09:18:41,075 DEBUG TRAIN Batch 16/5600 loss 16.279230 loss_att 22.225182 loss_ctc 25.401976 loss_rnnt 13.758211 hw_loss 0.216491 lr 0.00042393 rank 5
2023-02-23 09:18:41,076 DEBUG TRAIN Batch 16/5600 loss 7.675066 loss_att 8.845031 loss_ctc 9.220405 loss_rnnt 7.139579 hw_loss 0.178965 lr 0.00042401 rank 2
2023-02-23 09:18:41,085 DEBUG TRAIN Batch 16/5600 loss 7.002380 loss_att 10.349419 loss_ctc 12.875496 loss_rnnt 5.430760 hw_loss 0.223367 lr 0.00042403 rank 1
2023-02-23 09:18:41,123 DEBUG TRAIN Batch 16/5600 loss 9.609753 loss_att 12.126677 loss_ctc 13.910930 loss_rnnt 8.418563 hw_loss 0.214340 lr 0.00042399 rank 6
2023-02-23 09:20:01,219 DEBUG TRAIN Batch 16/5700 loss 12.851221 loss_att 16.648666 loss_ctc 17.929586 loss_rnnt 11.255529 hw_loss 0.298290 lr 0.00042378 rank 5
2023-02-23 09:20:01,219 DEBUG TRAIN Batch 16/5700 loss 8.519875 loss_att 11.925772 loss_ctc 13.435678 loss_rnnt 7.035124 hw_loss 0.277744 lr 0.00042387 rank 0
2023-02-23 09:20:01,223 DEBUG TRAIN Batch 16/5700 loss 17.576344 loss_att 23.795910 loss_ctc 26.699184 loss_rnnt 15.005657 hw_loss 0.206988 lr 0.00042386 rank 2
2023-02-23 09:20:01,228 DEBUG TRAIN Batch 16/5700 loss 10.534123 loss_att 11.818616 loss_ctc 14.475574 loss_rnnt 9.628580 hw_loss 0.230845 lr 0.00042388 rank 1
2023-02-23 09:20:01,228 DEBUG TRAIN Batch 16/5700 loss 7.287846 loss_att 7.975413 loss_ctc 9.249494 loss_rnnt 6.734540 hw_loss 0.289199 lr 0.00042383 rank 3
2023-02-23 09:20:01,228 DEBUG TRAIN Batch 16/5700 loss 17.332829 loss_att 18.702089 loss_ctc 21.336361 loss_rnnt 16.388803 hw_loss 0.255689 lr 0.00042384 rank 6
2023-02-23 09:20:01,229 DEBUG TRAIN Batch 16/5700 loss 23.470163 loss_att 26.891817 loss_ctc 29.444599 loss_rnnt 21.820816 hw_loss 0.315797 lr 0.00042386 rank 4
2023-02-23 09:20:01,275 DEBUG TRAIN Batch 16/5700 loss 9.121809 loss_att 8.971460 loss_ctc 10.076469 loss_rnnt 8.833380 hw_loss 0.358519 lr 0.00042380 rank 7
2023-02-23 09:21:16,422 DEBUG TRAIN Batch 16/5800 loss 6.546341 loss_att 10.251081 loss_ctc 5.230104 loss_rnnt 5.901363 hw_loss 0.149117 lr 0.00042362 rank 5
2023-02-23 09:21:16,423 DEBUG TRAIN Batch 16/5800 loss 3.145332 loss_att 5.641193 loss_ctc 3.801679 loss_rnnt 2.512336 hw_loss 0.086832 lr 0.00042370 rank 4
2023-02-23 09:21:16,428 DEBUG TRAIN Batch 16/5800 loss 14.663699 loss_att 16.698891 loss_ctc 18.823149 loss_rnnt 13.623916 hw_loss 0.146534 lr 0.00042367 rank 3
2023-02-23 09:21:16,431 DEBUG TRAIN Batch 16/5800 loss 6.517086 loss_att 10.757091 loss_ctc 6.242002 loss_rnnt 5.566485 hw_loss 0.261145 lr 0.00042370 rank 2
2023-02-23 09:21:16,432 DEBUG TRAIN Batch 16/5800 loss 11.748815 loss_att 15.189245 loss_ctc 14.395836 loss_rnnt 10.660719 hw_loss 0.088262 lr 0.00042372 rank 0
2023-02-23 09:21:16,435 DEBUG TRAIN Batch 16/5800 loss 7.306960 loss_att 12.293386 loss_ctc 12.713888 loss_rnnt 5.458496 hw_loss 0.244226 lr 0.00042365 rank 7
2023-02-23 09:21:16,436 DEBUG TRAIN Batch 16/5800 loss 11.068336 loss_att 14.956404 loss_ctc 12.866680 loss_rnnt 9.904782 hw_loss 0.274053 lr 0.00042368 rank 6
2023-02-23 09:21:16,437 DEBUG TRAIN Batch 16/5800 loss 20.621384 loss_att 20.735867 loss_ctc 25.436522 loss_rnnt 19.829857 hw_loss 0.237398 lr 0.00042373 rank 1
2023-02-23 09:22:31,811 DEBUG TRAIN Batch 16/5900 loss 7.590518 loss_att 13.959183 loss_ctc 13.827681 loss_rnnt 5.371658 hw_loss 0.212823 lr 0.00042347 rank 5
2023-02-23 09:22:31,814 DEBUG TRAIN Batch 16/5900 loss 10.047058 loss_att 11.263308 loss_ctc 11.741527 loss_rnnt 9.493759 hw_loss 0.157726 lr 0.00042355 rank 2
2023-02-23 09:22:31,815 DEBUG TRAIN Batch 16/5900 loss 7.712493 loss_att 13.219707 loss_ctc 8.298597 loss_rnnt 6.423141 hw_loss 0.205806 lr 0.00042352 rank 3
2023-02-23 09:22:31,819 DEBUG TRAIN Batch 16/5900 loss 16.082056 loss_att 18.629070 loss_ctc 18.830757 loss_rnnt 15.085844 hw_loss 0.225593 lr 0.00042353 rank 6
2023-02-23 09:22:31,818 DEBUG TRAIN Batch 16/5900 loss 18.007858 loss_att 18.791138 loss_ctc 23.957348 loss_rnnt 16.894142 hw_loss 0.307118 lr 0.00042358 rank 1
2023-02-23 09:22:31,819 DEBUG TRAIN Batch 16/5900 loss 4.201367 loss_att 8.041913 loss_ctc 3.316636 loss_rnnt 3.430404 hw_loss 0.226535 lr 0.00042357 rank 0
2023-02-23 09:22:31,819 DEBUG TRAIN Batch 16/5900 loss 4.109728 loss_att 7.086081 loss_ctc 5.314347 loss_rnnt 3.274914 hw_loss 0.147991 lr 0.00042350 rank 7
2023-02-23 09:22:31,820 DEBUG TRAIN Batch 16/5900 loss 9.081347 loss_att 11.992200 loss_ctc 11.359167 loss_rnnt 8.075304 hw_loss 0.225308 lr 0.00042355 rank 4
2023-02-23 09:23:49,695 DEBUG TRAIN Batch 16/6000 loss 11.254577 loss_att 15.513771 loss_ctc 20.341930 loss_rnnt 9.129477 hw_loss 0.115528 lr 0.00042340 rank 4
2023-02-23 09:23:49,696 DEBUG TRAIN Batch 16/6000 loss 7.890685 loss_att 11.247869 loss_ctc 12.945127 loss_rnnt 6.381420 hw_loss 0.307316 lr 0.00042332 rank 5
2023-02-23 09:23:49,699 DEBUG TRAIN Batch 16/6000 loss 18.147306 loss_att 20.177507 loss_ctc 21.107014 loss_rnnt 17.215885 hw_loss 0.245161 lr 0.00042340 rank 2
2023-02-23 09:23:49,701 DEBUG TRAIN Batch 16/6000 loss 8.967788 loss_att 11.663195 loss_ctc 16.832804 loss_rnnt 7.226218 hw_loss 0.288412 lr 0.00042342 rank 0
2023-02-23 09:23:49,703 DEBUG TRAIN Batch 16/6000 loss 11.499097 loss_att 17.011198 loss_ctc 16.888954 loss_rnnt 9.511085 hw_loss 0.313022 lr 0.00042338 rank 6
2023-02-23 09:23:49,703 DEBUG TRAIN Batch 16/6000 loss 5.374168 loss_att 9.811430 loss_ctc 9.421232 loss_rnnt 3.860492 hw_loss 0.162403 lr 0.00042337 rank 3
2023-02-23 09:23:49,707 DEBUG TRAIN Batch 16/6000 loss 13.621808 loss_att 16.568316 loss_ctc 17.034197 loss_rnnt 12.495283 hw_loss 0.154197 lr 0.00042335 rank 7
2023-02-23 09:23:49,709 DEBUG TRAIN Batch 16/6000 loss 12.036660 loss_att 14.569617 loss_ctc 17.956909 loss_rnnt 10.643188 hw_loss 0.182841 lr 0.00042342 rank 1
2023-02-23 09:25:07,362 DEBUG TRAIN Batch 16/6100 loss 5.090837 loss_att 8.283984 loss_ctc 8.824437 loss_rnnt 3.814483 hw_loss 0.262336 lr 0.00042325 rank 2
2023-02-23 09:25:07,370 DEBUG TRAIN Batch 16/6100 loss 12.906177 loss_att 16.418217 loss_ctc 17.853546 loss_rnnt 11.400076 hw_loss 0.270082 lr 0.00042323 rank 6
2023-02-23 09:25:07,370 DEBUG TRAIN Batch 16/6100 loss 7.590365 loss_att 9.297761 loss_ctc 9.217407 loss_rnnt 6.925821 hw_loss 0.198987 lr 0.00042326 rank 0
2023-02-23 09:25:07,370 DEBUG TRAIN Batch 16/6100 loss 11.414462 loss_att 15.085640 loss_ctc 14.440767 loss_rnnt 10.180842 hw_loss 0.179770 lr 0.00042327 rank 1
2023-02-23 09:25:07,371 DEBUG TRAIN Batch 16/6100 loss 16.888409 loss_att 17.151737 loss_ctc 22.767382 loss_rnnt 15.911422 hw_loss 0.263358 lr 0.00042317 rank 5
2023-02-23 09:25:07,373 DEBUG TRAIN Batch 16/6100 loss 11.897165 loss_att 15.813135 loss_ctc 14.954917 loss_rnnt 10.578387 hw_loss 0.239782 lr 0.00042325 rank 4
2023-02-23 09:25:07,378 DEBUG TRAIN Batch 16/6100 loss 8.381834 loss_att 11.406142 loss_ctc 12.385642 loss_rnnt 7.112628 hw_loss 0.244694 lr 0.00042322 rank 3
2023-02-23 09:25:07,420 DEBUG TRAIN Batch 16/6100 loss 11.800405 loss_att 14.384508 loss_ctc 12.615717 loss_rnnt 11.066094 hw_loss 0.203964 lr 0.00042319 rank 7
2023-02-23 09:26:24,620 DEBUG TRAIN Batch 16/6200 loss 7.970556 loss_att 12.587473 loss_ctc 10.175982 loss_rnnt 6.636949 hw_loss 0.217812 lr 0.00042302 rank 5
2023-02-23 09:26:24,622 DEBUG TRAIN Batch 16/6200 loss 7.310665 loss_att 8.502430 loss_ctc 10.799511 loss_rnnt 6.496144 hw_loss 0.208102 lr 0.00042308 rank 6
2023-02-23 09:26:24,623 DEBUG TRAIN Batch 16/6200 loss 2.779349 loss_att 4.593499 loss_ctc 5.547060 loss_rnnt 1.922899 hw_loss 0.233608 lr 0.00042310 rank 2
2023-02-23 09:26:24,625 DEBUG TRAIN Batch 16/6200 loss 5.205156 loss_att 7.151938 loss_ctc 6.551186 loss_rnnt 4.474148 hw_loss 0.304091 lr 0.00042311 rank 0
2023-02-23 09:26:24,625 DEBUG TRAIN Batch 16/6200 loss 16.547304 loss_att 20.771486 loss_ctc 21.860212 loss_rnnt 14.868382 hw_loss 0.235682 lr 0.00042310 rank 4
2023-02-23 09:26:24,626 DEBUG TRAIN Batch 16/6200 loss 2.492767 loss_att 5.204542 loss_ctc 4.015825 loss_rnnt 1.623376 hw_loss 0.232428 lr 0.00042312 rank 1
2023-02-23 09:26:24,626 DEBUG TRAIN Batch 16/6200 loss 3.676055 loss_att 7.132988 loss_ctc 4.440761 loss_rnnt 2.704701 hw_loss 0.333762 lr 0.00042307 rank 3
2023-02-23 09:26:24,630 DEBUG TRAIN Batch 16/6200 loss 11.617140 loss_att 12.159100 loss_ctc 15.682503 loss_rnnt 10.861549 hw_loss 0.197156 lr 0.00042304 rank 7
2023-02-23 09:27:42,337 DEBUG TRAIN Batch 16/6300 loss 15.719872 loss_att 17.556549 loss_ctc 22.206486 loss_rnnt 14.360347 hw_loss 0.238699 lr 0.00042297 rank 1
2023-02-23 09:27:42,339 DEBUG TRAIN Batch 16/6300 loss 9.841700 loss_att 11.724972 loss_ctc 13.483619 loss_rnnt 8.827934 hw_loss 0.284103 lr 0.00042293 rank 6
2023-02-23 09:27:42,339 DEBUG TRAIN Batch 16/6300 loss 18.838673 loss_att 18.355545 loss_ctc 25.226791 loss_rnnt 17.956594 hw_loss 0.238044 lr 0.00042294 rank 4
2023-02-23 09:27:42,344 DEBUG TRAIN Batch 16/6300 loss 9.940845 loss_att 11.827645 loss_ctc 11.600277 loss_rnnt 9.167654 hw_loss 0.327324 lr 0.00042287 rank 5
2023-02-23 09:27:42,348 DEBUG TRAIN Batch 16/6300 loss 12.405009 loss_att 14.432217 loss_ctc 17.368542 loss_rnnt 11.182455 hw_loss 0.291202 lr 0.00042294 rank 2
2023-02-23 09:27:42,352 DEBUG TRAIN Batch 16/6300 loss 11.619446 loss_att 16.465698 loss_ctc 17.562588 loss_rnnt 9.761713 hw_loss 0.180121 lr 0.00042296 rank 0
2023-02-23 09:27:42,356 DEBUG TRAIN Batch 16/6300 loss 10.132888 loss_att 11.193209 loss_ctc 12.499029 loss_rnnt 9.406264 hw_loss 0.373264 lr 0.00042289 rank 7
2023-02-23 09:27:42,387 DEBUG TRAIN Batch 16/6300 loss 11.966283 loss_att 12.976604 loss_ctc 14.264780 loss_rnnt 11.314155 hw_loss 0.269244 lr 0.00042292 rank 3
2023-02-23 09:29:02,474 DEBUG TRAIN Batch 16/6400 loss 19.501595 loss_att 23.740231 loss_ctc 24.904804 loss_rnnt 17.838980 hw_loss 0.177113 lr 0.00042271 rank 5
2023-02-23 09:29:02,476 DEBUG TRAIN Batch 16/6400 loss 4.864812 loss_att 5.244665 loss_ctc 5.477157 loss_rnnt 4.546049 hw_loss 0.302150 lr 0.00042279 rank 4
2023-02-23 09:29:02,478 DEBUG TRAIN Batch 16/6400 loss 4.470507 loss_att 6.948642 loss_ctc 6.941974 loss_rnnt 3.596183 hw_loss 0.092189 lr 0.00042274 rank 7
2023-02-23 09:29:02,477 DEBUG TRAIN Batch 16/6400 loss 8.763407 loss_att 10.492826 loss_ctc 10.426925 loss_rnnt 8.038416 hw_loss 0.294945 lr 0.00042279 rank 2
2023-02-23 09:29:02,478 DEBUG TRAIN Batch 16/6400 loss 9.650768 loss_att 10.535649 loss_ctc 17.955345 loss_rnnt 8.143427 hw_loss 0.418290 lr 0.00042276 rank 3
2023-02-23 09:29:02,478 DEBUG TRAIN Batch 16/6400 loss 22.899719 loss_att 23.347126 loss_ctc 33.318405 loss_rnnt 21.256224 hw_loss 0.309107 lr 0.00042281 rank 0
2023-02-23 09:29:02,480 DEBUG TRAIN Batch 16/6400 loss 7.446106 loss_att 10.798289 loss_ctc 12.323668 loss_rnnt 5.980916 hw_loss 0.270774 lr 0.00042277 rank 6
2023-02-23 09:29:02,518 DEBUG TRAIN Batch 16/6400 loss 9.364787 loss_att 11.634752 loss_ctc 12.353218 loss_rnnt 8.368949 hw_loss 0.268852 lr 0.00042282 rank 1
2023-02-23 09:30:19,915 DEBUG TRAIN Batch 16/6500 loss 7.893172 loss_att 12.921442 loss_ctc 14.256088 loss_rnnt 5.932686 hw_loss 0.199581 lr 0.00042264 rank 4
2023-02-23 09:30:19,916 DEBUG TRAIN Batch 16/6500 loss 21.029446 loss_att 23.880930 loss_ctc 30.155788 loss_rnnt 19.136694 hw_loss 0.198017 lr 0.00042256 rank 5
2023-02-23 09:30:19,920 DEBUG TRAIN Batch 16/6500 loss 13.009809 loss_att 19.119495 loss_ctc 20.119589 loss_rnnt 10.717742 hw_loss 0.229047 lr 0.00042264 rank 2
2023-02-23 09:30:19,923 DEBUG TRAIN Batch 16/6500 loss 5.484507 loss_att 12.313866 loss_ctc 9.238338 loss_rnnt 3.464999 hw_loss 0.287109 lr 0.00042267 rank 1
2023-02-23 09:30:19,925 DEBUG TRAIN Batch 16/6500 loss 13.790509 loss_att 19.343866 loss_ctc 22.636452 loss_rnnt 11.325706 hw_loss 0.327511 lr 0.00042266 rank 0
2023-02-23 09:30:19,928 DEBUG TRAIN Batch 16/6500 loss 21.759392 loss_att 22.891922 loss_ctc 25.677055 loss_rnnt 20.938848 hw_loss 0.134406 lr 0.00042259 rank 7
2023-02-23 09:30:19,928 DEBUG TRAIN Batch 16/6500 loss 8.216413 loss_att 11.826256 loss_ctc 12.983482 loss_rnnt 6.733860 hw_loss 0.234330 lr 0.00042261 rank 3
2023-02-23 09:30:19,933 DEBUG TRAIN Batch 16/6500 loss 17.139421 loss_att 24.875883 loss_ctc 19.572348 loss_rnnt 15.161100 hw_loss 0.199946 lr 0.00042262 rank 6
2023-02-23 09:31:36,440 DEBUG TRAIN Batch 16/6600 loss 6.315626 loss_att 13.491007 loss_ctc 11.564300 loss_rnnt 4.029665 hw_loss 0.283240 lr 0.00042251 rank 0
2023-02-23 09:31:36,440 DEBUG TRAIN Batch 16/6600 loss 6.771570 loss_att 11.356369 loss_ctc 10.593407 loss_rnnt 5.259584 hw_loss 0.160213 lr 0.00042249 rank 4
2023-02-23 09:31:36,442 DEBUG TRAIN Batch 16/6600 loss 6.950377 loss_att 9.412327 loss_ctc 11.504634 loss_rnnt 5.742805 hw_loss 0.202402 lr 0.00042249 rank 2
2023-02-23 09:31:36,443 DEBUG TRAIN Batch 16/6600 loss 12.291473 loss_att 14.385695 loss_ctc 13.884247 loss_rnnt 11.524977 hw_loss 0.253656 lr 0.00042241 rank 5
2023-02-23 09:31:36,444 DEBUG TRAIN Batch 16/6600 loss 17.751392 loss_att 21.048912 loss_ctc 22.210857 loss_rnnt 16.351917 hw_loss 0.272581 lr 0.00042247 rank 6
2023-02-23 09:31:36,445 DEBUG TRAIN Batch 16/6600 loss 6.652789 loss_att 10.094139 loss_ctc 7.332203 loss_rnnt 5.760902 hw_loss 0.211928 lr 0.00042252 rank 1
2023-02-23 09:31:36,450 DEBUG TRAIN Batch 16/6600 loss 14.580459 loss_att 16.265842 loss_ctc 20.651213 loss_rnnt 13.298206 hw_loss 0.254514 lr 0.00042244 rank 7
2023-02-23 09:31:36,452 DEBUG TRAIN Batch 16/6600 loss 13.298755 loss_att 17.882433 loss_ctc 14.646194 loss_rnnt 12.112664 hw_loss 0.168178 lr 0.00042246 rank 3
2023-02-23 09:32:53,780 DEBUG TRAIN Batch 16/6700 loss 11.442527 loss_att 16.802074 loss_ctc 14.876444 loss_rnnt 9.820786 hw_loss 0.172454 lr 0.00042234 rank 2
2023-02-23 09:32:53,780 DEBUG TRAIN Batch 16/6700 loss 12.141880 loss_att 15.728481 loss_ctc 18.462606 loss_rnnt 10.445885 hw_loss 0.254835 lr 0.00042231 rank 3
2023-02-23 09:32:53,782 DEBUG TRAIN Batch 16/6700 loss 10.214828 loss_att 13.081530 loss_ctc 11.268519 loss_rnnt 9.361276 hw_loss 0.261972 lr 0.00042236 rank 0
2023-02-23 09:32:53,782 DEBUG TRAIN Batch 16/6700 loss 3.804097 loss_att 8.703576 loss_ctc 5.627429 loss_rnnt 2.420869 hw_loss 0.300415 lr 0.00042234 rank 4
2023-02-23 09:32:53,783 DEBUG TRAIN Batch 16/6700 loss 9.119680 loss_att 11.272442 loss_ctc 10.989428 loss_rnnt 8.326782 hw_loss 0.211962 lr 0.00042226 rank 5
2023-02-23 09:32:53,784 DEBUG TRAIN Batch 16/6700 loss 25.869709 loss_att 29.636681 loss_ctc 40.705189 loss_rnnt 23.051399 hw_loss 0.162851 lr 0.00042229 rank 7
2023-02-23 09:32:53,786 DEBUG TRAIN Batch 16/6700 loss 19.054007 loss_att 25.406242 loss_ctc 32.139229 loss_rnnt 15.914028 hw_loss 0.234065 lr 0.00042236 rank 1
2023-02-23 09:32:53,787 DEBUG TRAIN Batch 16/6700 loss 4.616163 loss_att 8.864309 loss_ctc 6.945091 loss_rnnt 3.275138 hw_loss 0.339137 lr 0.00042232 rank 6
2023-02-23 09:34:11,789 DEBUG TRAIN Batch 16/6800 loss 21.936369 loss_att 22.107643 loss_ctc 25.825453 loss_rnnt 21.258991 hw_loss 0.233585 lr 0.00042219 rank 4
2023-02-23 09:34:11,792 DEBUG TRAIN Batch 16/6800 loss 4.613099 loss_att 6.997475 loss_ctc 5.769937 loss_rnnt 3.885752 hw_loss 0.180425 lr 0.00042211 rank 5
2023-02-23 09:34:11,796 DEBUG TRAIN Batch 16/6800 loss 17.117558 loss_att 25.069462 loss_ctc 27.534164 loss_rnnt 13.990181 hw_loss 0.277714 lr 0.00042221 rank 0
2023-02-23 09:34:11,795 DEBUG TRAIN Batch 16/6800 loss 9.835712 loss_att 13.721107 loss_ctc 13.617693 loss_rnnt 8.445891 hw_loss 0.203397 lr 0.00042219 rank 2
2023-02-23 09:34:11,802 DEBUG TRAIN Batch 16/6800 loss 17.424620 loss_att 20.460487 loss_ctc 21.215153 loss_rnnt 16.179104 hw_loss 0.249257 lr 0.00042214 rank 7
2023-02-23 09:34:11,806 DEBUG TRAIN Batch 16/6800 loss 6.277797 loss_att 9.877059 loss_ctc 11.887351 loss_rnnt 4.658927 hw_loss 0.283269 lr 0.00042216 rank 3
2023-02-23 09:34:11,832 DEBUG TRAIN Batch 16/6800 loss 33.252796 loss_att 39.839890 loss_ctc 50.114655 loss_rnnt 29.584755 hw_loss 0.191959 lr 0.00042217 rank 6
2023-02-23 09:34:11,834 DEBUG TRAIN Batch 16/6800 loss 14.553937 loss_att 17.165802 loss_ctc 21.961332 loss_rnnt 12.930826 hw_loss 0.212032 lr 0.00042221 rank 1
2023-02-23 09:35:28,617 DEBUG TRAIN Batch 16/6900 loss 13.840396 loss_att 15.660353 loss_ctc 13.062801 loss_rnnt 13.433710 hw_loss 0.274452 lr 0.00042196 rank 5
2023-02-23 09:35:28,621 DEBUG TRAIN Batch 16/6900 loss 16.753851 loss_att 18.429974 loss_ctc 17.802902 loss_rnnt 16.161596 hw_loss 0.219670 lr 0.00042206 rank 0
2023-02-23 09:35:28,622 DEBUG TRAIN Batch 16/6900 loss 5.879505 loss_att 8.387021 loss_ctc 9.632261 loss_rnnt 4.809668 hw_loss 0.127437 lr 0.00042204 rank 4
2023-02-23 09:35:28,623 DEBUG TRAIN Batch 16/6900 loss 16.386940 loss_att 19.075970 loss_ctc 22.960875 loss_rnnt 14.818445 hw_loss 0.289060 lr 0.00042204 rank 2
2023-02-23 09:35:28,624 DEBUG TRAIN Batch 16/6900 loss 12.776137 loss_att 14.337126 loss_ctc 18.902878 loss_rnnt 11.555861 hw_loss 0.170964 lr 0.00042201 rank 3
2023-02-23 09:35:28,625 DEBUG TRAIN Batch 16/6900 loss 9.455213 loss_att 12.926618 loss_ctc 14.408190 loss_rnnt 7.968255 hw_loss 0.248024 lr 0.00042199 rank 7
2023-02-23 09:35:28,626 DEBUG TRAIN Batch 16/6900 loss 11.133236 loss_att 12.084425 loss_ctc 11.896511 loss_rnnt 10.706625 hw_loss 0.252381 lr 0.00042206 rank 1
2023-02-23 09:35:28,629 DEBUG TRAIN Batch 16/6900 loss 12.047817 loss_att 13.164804 loss_ctc 16.000265 loss_rnnt 11.169863 hw_loss 0.239180 lr 0.00042202 rank 6
2023-02-23 09:36:45,273 DEBUG TRAIN Batch 16/7000 loss 11.406065 loss_att 11.996953 loss_ctc 15.649438 loss_rnnt 10.572510 hw_loss 0.280491 lr 0.00042181 rank 5
2023-02-23 09:36:45,277 DEBUG TRAIN Batch 16/7000 loss 13.387055 loss_att 13.700920 loss_ctc 13.970326 loss_rnnt 13.099399 hw_loss 0.275838 lr 0.00042189 rank 4
2023-02-23 09:36:45,281 DEBUG TRAIN Batch 16/7000 loss 8.712837 loss_att 9.690522 loss_ctc 14.453646 loss_rnnt 7.650466 hw_loss 0.190113 lr 0.00042186 rank 3
2023-02-23 09:36:45,282 DEBUG TRAIN Batch 16/7000 loss 5.385823 loss_att 10.542731 loss_ctc 6.925980 loss_rnnt 4.000608 hw_loss 0.278400 lr 0.00042189 rank 2
2023-02-23 09:36:45,284 DEBUG TRAIN Batch 16/7000 loss 4.952559 loss_att 6.827698 loss_ctc 9.451815 loss_rnnt 3.857203 hw_loss 0.225803 lr 0.00042191 rank 1
2023-02-23 09:36:45,284 DEBUG TRAIN Batch 16/7000 loss 5.310009 loss_att 6.952538 loss_ctc 7.133879 loss_rnnt 4.599889 hw_loss 0.259560 lr 0.00042191 rank 0
2023-02-23 09:36:45,316 DEBUG TRAIN Batch 16/7000 loss 9.251236 loss_att 13.641438 loss_ctc 11.360456 loss_rnnt 7.921344 hw_loss 0.319916 lr 0.00042187 rank 6
2023-02-23 09:36:45,316 DEBUG TRAIN Batch 16/7000 loss 7.186279 loss_att 13.710947 loss_ctc 11.170269 loss_rnnt 5.192095 hw_loss 0.296348 lr 0.00042184 rank 7
2023-02-23 09:38:05,592 DEBUG TRAIN Batch 16/7100 loss 8.768562 loss_att 12.791039 loss_ctc 11.715322 loss_rnnt 7.436272 hw_loss 0.252926 lr 0.00042176 rank 0
2023-02-23 09:38:05,595 DEBUG TRAIN Batch 16/7100 loss 6.573774 loss_att 11.586256 loss_ctc 10.959805 loss_rnnt 4.890821 hw_loss 0.179348 lr 0.00042176 rank 1
2023-02-23 09:38:05,595 DEBUG TRAIN Batch 16/7100 loss 17.885508 loss_att 17.196461 loss_ctc 21.875467 loss_rnnt 17.335157 hw_loss 0.292812 lr 0.00042172 rank 6
2023-02-23 09:38:05,595 DEBUG TRAIN Batch 16/7100 loss 8.009223 loss_att 13.700472 loss_ctc 11.626110 loss_rnnt 6.191608 hw_loss 0.369587 lr 0.00042171 rank 3
2023-02-23 09:38:05,596 DEBUG TRAIN Batch 16/7100 loss 5.757962 loss_att 6.683833 loss_ctc 4.295767 loss_rnnt 5.634570 hw_loss 0.249707 lr 0.00042169 rank 7
2023-02-23 09:38:05,597 DEBUG TRAIN Batch 16/7100 loss 6.392605 loss_att 9.502956 loss_ctc 7.091436 loss_rnnt 5.554668 hw_loss 0.230042 lr 0.00042174 rank 2
2023-02-23 09:38:05,600 DEBUG TRAIN Batch 16/7100 loss 9.347017 loss_att 15.267433 loss_ctc 9.461542 loss_rnnt 8.006077 hw_loss 0.265477 lr 0.00042174 rank 4
2023-02-23 09:38:05,607 DEBUG TRAIN Batch 16/7100 loss 7.163498 loss_att 11.143645 loss_ctc 11.308998 loss_rnnt 5.739863 hw_loss 0.140385 lr 0.00042166 rank 5
2023-02-23 09:39:21,011 DEBUG TRAIN Batch 16/7200 loss 9.186893 loss_att 11.382444 loss_ctc 13.281891 loss_rnnt 8.059279 hw_loss 0.267196 lr 0.00042159 rank 2
2023-02-23 09:39:21,011 DEBUG TRAIN Batch 16/7200 loss 3.787205 loss_att 5.421074 loss_ctc 4.090569 loss_rnnt 3.348884 hw_loss 0.133312 lr 0.00042156 rank 3
2023-02-23 09:39:21,011 DEBUG TRAIN Batch 16/7200 loss 6.143233 loss_att 8.838092 loss_ctc 12.040257 loss_rnnt 4.681847 hw_loss 0.255269 lr 0.00042161 rank 0
2023-02-23 09:39:21,012 DEBUG TRAIN Batch 16/7200 loss 10.255261 loss_att 14.957613 loss_ctc 15.575958 loss_rnnt 8.462321 hw_loss 0.268206 lr 0.00042151 rank 5
2023-02-23 09:39:21,014 DEBUG TRAIN Batch 16/7200 loss 11.802068 loss_att 11.551745 loss_ctc 12.358119 loss_rnnt 11.683659 hw_loss 0.176875 lr 0.00042159 rank 4
2023-02-23 09:39:21,015 DEBUG TRAIN Batch 16/7200 loss 11.581567 loss_att 14.592636 loss_ctc 13.718147 loss_rnnt 10.580920 hw_loss 0.212918 lr 0.00042154 rank 7
2023-02-23 09:39:21,016 DEBUG TRAIN Batch 16/7200 loss 15.055098 loss_att 18.967976 loss_ctc 21.778667 loss_rnnt 13.249472 hw_loss 0.237326 lr 0.00042157 rank 6
2023-02-23 09:39:21,018 DEBUG TRAIN Batch 16/7200 loss 11.110744 loss_att 13.569308 loss_ctc 16.401333 loss_rnnt 9.790894 hw_loss 0.230111 lr 0.00042161 rank 1
2023-02-23 09:40:36,240 DEBUG TRAIN Batch 16/7300 loss 11.644788 loss_att 14.090320 loss_ctc 14.596792 loss_rnnt 10.582350 hw_loss 0.336994 lr 0.00042144 rank 4
2023-02-23 09:40:36,246 DEBUG TRAIN Batch 16/7300 loss 7.810657 loss_att 8.876789 loss_ctc 11.829018 loss_rnnt 7.000417 hw_loss 0.114808 lr 0.00042136 rank 5
2023-02-23 09:40:36,248 DEBUG TRAIN Batch 16/7300 loss 9.501612 loss_att 14.187107 loss_ctc 13.396858 loss_rnnt 7.928489 hw_loss 0.218735 lr 0.00042146 rank 1
2023-02-23 09:40:36,249 DEBUG TRAIN Batch 16/7300 loss 12.881486 loss_att 16.260775 loss_ctc 18.174532 loss_rnnt 11.413562 hw_loss 0.161863 lr 0.00042141 rank 3
2023-02-23 09:40:36,249 DEBUG TRAIN Batch 16/7300 loss 12.326296 loss_att 14.606696 loss_ctc 14.090022 loss_rnnt 11.498173 hw_loss 0.256649 lr 0.00042146 rank 0
2023-02-23 09:40:36,250 DEBUG TRAIN Batch 16/7300 loss 4.368679 loss_att 9.208986 loss_ctc 9.306938 loss_rnnt 2.604891 hw_loss 0.257423 lr 0.00042144 rank 2
2023-02-23 09:40:36,250 DEBUG TRAIN Batch 16/7300 loss 6.117249 loss_att 7.689724 loss_ctc 9.809171 loss_rnnt 5.238237 hw_loss 0.135489 lr 0.00042139 rank 7
2023-02-23 09:40:36,252 DEBUG TRAIN Batch 16/7300 loss 15.663752 loss_att 14.745655 loss_ctc 23.147940 loss_rnnt 14.679411 hw_loss 0.318877 lr 0.00042142 rank 6
2023-02-23 09:41:52,636 DEBUG TRAIN Batch 16/7400 loss 12.642786 loss_att 14.100042 loss_ctc 16.322903 loss_rnnt 11.758914 hw_loss 0.190762 lr 0.00042121 rank 5
2023-02-23 09:41:52,638 DEBUG TRAIN Batch 16/7400 loss 7.556199 loss_att 12.017964 loss_ctc 10.473795 loss_rnnt 6.164349 hw_loss 0.207158 lr 0.00042131 rank 0
2023-02-23 09:41:52,641 DEBUG TRAIN Batch 16/7400 loss 5.839880 loss_att 10.139251 loss_ctc 7.499766 loss_rnnt 4.669014 hw_loss 0.168138 lr 0.00042126 rank 3
2023-02-23 09:41:52,642 DEBUG TRAIN Batch 16/7400 loss 8.226639 loss_att 9.603927 loss_ctc 10.626759 loss_rnnt 7.467901 hw_loss 0.306119 lr 0.00042127 rank 6
2023-02-23 09:41:52,643 DEBUG TRAIN Batch 16/7400 loss 6.403393 loss_att 8.383591 loss_ctc 9.436996 loss_rnnt 5.482441 hw_loss 0.225808 lr 0.00042129 rank 4
2023-02-23 09:41:52,644 DEBUG TRAIN Batch 16/7400 loss 15.910825 loss_att 21.365107 loss_ctc 21.373966 loss_rnnt 13.966810 hw_loss 0.233885 lr 0.00042129 rank 2
2023-02-23 09:41:52,647 DEBUG TRAIN Batch 16/7400 loss 11.526492 loss_att 13.994993 loss_ctc 14.247923 loss_rnnt 10.544278 hw_loss 0.235606 lr 0.00042131 rank 1
2023-02-23 09:41:52,648 DEBUG TRAIN Batch 16/7400 loss 6.880550 loss_att 10.719432 loss_ctc 9.010864 loss_rnnt 5.726541 hw_loss 0.191608 lr 0.00042124 rank 7
2023-02-23 09:43:11,861 DEBUG TRAIN Batch 16/7500 loss 6.134932 loss_att 8.909114 loss_ctc 7.058621 loss_rnnt 5.347691 hw_loss 0.204837 lr 0.00042116 rank 0
2023-02-23 09:43:11,862 DEBUG TRAIN Batch 16/7500 loss 8.065299 loss_att 9.863607 loss_ctc 10.795740 loss_rnnt 7.213989 hw_loss 0.239230 lr 0.00042106 rank 5
2023-02-23 09:43:11,864 DEBUG TRAIN Batch 16/7500 loss 10.984295 loss_att 12.802796 loss_ctc 13.237409 loss_rnnt 10.220194 hw_loss 0.187473 lr 0.00042114 rank 2
2023-02-23 09:43:11,864 DEBUG TRAIN Batch 16/7500 loss 6.537268 loss_att 9.268630 loss_ctc 9.530738 loss_rnnt 5.456926 hw_loss 0.253014 lr 0.00042112 rank 6
2023-02-23 09:43:11,864 DEBUG TRAIN Batch 16/7500 loss 6.645294 loss_att 8.925636 loss_ctc 8.959789 loss_rnnt 5.708270 hw_loss 0.323169 lr 0.00042114 rank 4
2023-02-23 09:43:11,867 DEBUG TRAIN Batch 16/7500 loss 7.235415 loss_att 10.390823 loss_ctc 11.965735 loss_rnnt 5.839832 hw_loss 0.250861 lr 0.00042116 rank 1
2023-02-23 09:43:11,867 DEBUG TRAIN Batch 16/7500 loss 5.251996 loss_att 7.807661 loss_ctc 7.086951 loss_rnnt 4.360765 hw_loss 0.253944 lr 0.00042109 rank 7
2023-02-23 09:43:11,908 DEBUG TRAIN Batch 16/7500 loss 8.903657 loss_att 10.789580 loss_ctc 13.459928 loss_rnnt 7.790640 hw_loss 0.240617 lr 0.00042111 rank 3
2023-02-23 09:44:29,393 DEBUG TRAIN Batch 16/7600 loss 3.822047 loss_att 5.619969 loss_ctc 6.734582 loss_rnnt 2.956614 hw_loss 0.220331 lr 0.00042101 rank 0
2023-02-23 09:44:29,400 DEBUG TRAIN Batch 16/7600 loss 5.492499 loss_att 7.058512 loss_ctc 5.712722 loss_rnnt 5.007431 hw_loss 0.267193 lr 0.00042091 rank 5
2023-02-23 09:44:29,401 DEBUG TRAIN Batch 16/7600 loss 12.803951 loss_att 14.070889 loss_ctc 15.445391 loss_rnnt 12.026953 hw_loss 0.321411 lr 0.00042094 rank 7
2023-02-23 09:44:29,402 DEBUG TRAIN Batch 16/7600 loss 5.447308 loss_att 9.274919 loss_ctc 8.458780 loss_rnnt 4.151548 hw_loss 0.241327 lr 0.00042099 rank 2
2023-02-23 09:44:29,404 DEBUG TRAIN Batch 16/7600 loss 16.993586 loss_att 20.494806 loss_ctc 25.820494 loss_rnnt 14.989860 hw_loss 0.237303 lr 0.00042101 rank 1
2023-02-23 09:44:29,404 DEBUG TRAIN Batch 16/7600 loss 9.227424 loss_att 11.363331 loss_ctc 8.825736 loss_rnnt 8.781157 hw_loss 0.136207 lr 0.00042097 rank 6
2023-02-23 09:44:29,407 DEBUG TRAIN Batch 16/7600 loss 10.272989 loss_att 13.157541 loss_ctc 14.370177 loss_rnnt 9.050834 hw_loss 0.185539 lr 0.00042099 rank 4
2023-02-23 09:44:29,443 DEBUG TRAIN Batch 16/7600 loss 12.665077 loss_att 18.128704 loss_ctc 20.468578 loss_rnnt 10.391650 hw_loss 0.262941 lr 0.00042096 rank 3
2023-02-23 09:45:45,310 DEBUG TRAIN Batch 16/7700 loss 11.900475 loss_att 11.144203 loss_ctc 14.680840 loss_rnnt 11.532393 hw_loss 0.278665 lr 0.00042086 rank 0
2023-02-23 09:45:45,312 DEBUG TRAIN Batch 16/7700 loss 19.066668 loss_att 21.092583 loss_ctc 21.976240 loss_rnnt 18.102556 hw_loss 0.320596 lr 0.00042076 rank 5
2023-02-23 09:45:45,315 DEBUG TRAIN Batch 16/7700 loss 8.023412 loss_att 12.485618 loss_ctc 8.732554 loss_rnnt 6.890328 hw_loss 0.273917 lr 0.00042084 rank 4
2023-02-23 09:45:45,318 DEBUG TRAIN Batch 16/7700 loss 18.664186 loss_att 19.004526 loss_ctc 22.310278 loss_rnnt 18.010036 hw_loss 0.187383 lr 0.00042084 rank 2
2023-02-23 09:45:45,319 DEBUG TRAIN Batch 16/7700 loss 17.233814 loss_att 17.995798 loss_ctc 18.711117 loss_rnnt 16.795982 hw_loss 0.165861 lr 0.00042082 rank 6
2023-02-23 09:45:45,319 DEBUG TRAIN Batch 16/7700 loss 3.672780 loss_att 8.155371 loss_ctc 4.828272 loss_rnnt 2.501303 hw_loss 0.226675 lr 0.00042079 rank 7
2023-02-23 09:45:45,325 DEBUG TRAIN Batch 16/7700 loss 3.753401 loss_att 7.684327 loss_ctc 7.035214 loss_rnnt 2.435450 hw_loss 0.176608 lr 0.00042081 rank 3
2023-02-23 09:45:45,369 DEBUG TRAIN Batch 16/7700 loss 10.300653 loss_att 10.970646 loss_ctc 11.426100 loss_rnnt 9.831659 hw_loss 0.346755 lr 0.00042087 rank 1
2023-02-23 09:47:03,518 DEBUG TRAIN Batch 16/7800 loss 8.557388 loss_att 9.895340 loss_ctc 9.183053 loss_rnnt 8.106384 hw_loss 0.187483 lr 0.00042069 rank 2
2023-02-23 09:47:03,521 DEBUG TRAIN Batch 16/7800 loss 14.547385 loss_att 18.818668 loss_ctc 22.966469 loss_rnnt 12.459137 hw_loss 0.208963 lr 0.00042069 rank 4
2023-02-23 09:47:03,523 DEBUG TRAIN Batch 16/7800 loss 11.063547 loss_att 17.519510 loss_ctc 16.734606 loss_rnnt 8.934042 hw_loss 0.154071 lr 0.00042062 rank 5
2023-02-23 09:47:03,523 DEBUG TRAIN Batch 16/7800 loss 19.520082 loss_att 22.119045 loss_ctc 30.534950 loss_rnnt 17.397629 hw_loss 0.251267 lr 0.00042067 rank 6
2023-02-23 09:47:03,526 DEBUG TRAIN Batch 16/7800 loss 6.253865 loss_att 8.546826 loss_ctc 9.391408 loss_rnnt 5.221818 hw_loss 0.290843 lr 0.00042071 rank 0
2023-02-23 09:47:03,552 DEBUG TRAIN Batch 16/7800 loss 6.930973 loss_att 9.233661 loss_ctc 9.031522 loss_rnnt 6.050300 hw_loss 0.262617 lr 0.00042066 rank 3
2023-02-23 09:47:03,560 DEBUG TRAIN Batch 16/7800 loss 6.865927 loss_att 10.322445 loss_ctc 9.438488 loss_rnnt 5.706965 hw_loss 0.233719 lr 0.00042064 rank 7
2023-02-23 09:47:03,572 DEBUG TRAIN Batch 16/7800 loss 4.003531 loss_att 8.023918 loss_ctc 5.031182 loss_rnnt 2.962896 hw_loss 0.186632 lr 0.00042072 rank 1
2023-02-23 09:48:22,291 DEBUG TRAIN Batch 16/7900 loss 13.869510 loss_att 14.914374 loss_ctc 13.192827 loss_rnnt 13.599592 hw_loss 0.283444 lr 0.00042047 rank 5
2023-02-23 09:48:22,294 DEBUG TRAIN Batch 16/7900 loss 6.079772 loss_att 6.209531 loss_ctc 6.411565 loss_rnnt 5.906595 hw_loss 0.193099 lr 0.00042052 rank 3
2023-02-23 09:48:22,296 DEBUG TRAIN Batch 16/7900 loss 6.951816 loss_att 10.478165 loss_ctc 10.317927 loss_rnnt 5.680781 hw_loss 0.219282 lr 0.00042054 rank 2
2023-02-23 09:48:22,296 DEBUG TRAIN Batch 16/7900 loss 9.848898 loss_att 16.234180 loss_ctc 15.654594 loss_rnnt 7.679639 hw_loss 0.221456 lr 0.00042057 rank 1
2023-02-23 09:48:22,297 DEBUG TRAIN Batch 16/7900 loss 17.680811 loss_att 19.082520 loss_ctc 22.597387 loss_rnnt 16.641308 hw_loss 0.194283 lr 0.00042056 rank 0
2023-02-23 09:48:22,302 DEBUG TRAIN Batch 16/7900 loss 5.762158 loss_att 8.551492 loss_ctc 9.691546 loss_rnnt 4.586688 hw_loss 0.175659 lr 0.00042054 rank 4
2023-02-23 09:48:22,303 DEBUG TRAIN Batch 16/7900 loss 9.943241 loss_att 10.768011 loss_ctc 17.004032 loss_rnnt 8.732032 hw_loss 0.196531 lr 0.00042053 rank 6
2023-02-23 09:48:22,342 DEBUG TRAIN Batch 16/7900 loss 5.283757 loss_att 8.560371 loss_ctc 8.104138 loss_rnnt 4.186010 hw_loss 0.124449 lr 0.00042049 rank 7
2023-02-23 09:49:39,596 DEBUG TRAIN Batch 16/8000 loss 14.464484 loss_att 18.136137 loss_ctc 21.176086 loss_rnnt 12.644760 hw_loss 0.357212 lr 0.00042040 rank 2
2023-02-23 09:49:39,601 DEBUG TRAIN Batch 16/8000 loss 7.282805 loss_att 10.026908 loss_ctc 8.494160 loss_rnnt 6.407390 hw_loss 0.309528 lr 0.00042042 rank 1
2023-02-23 09:49:39,601 DEBUG TRAIN Batch 16/8000 loss 6.672896 loss_att 10.004002 loss_ctc 8.527290 loss_rnnt 5.669461 hw_loss 0.168676 lr 0.00042038 rank 6
2023-02-23 09:49:39,601 DEBUG TRAIN Batch 16/8000 loss 9.209793 loss_att 11.741531 loss_ctc 10.644212 loss_rnnt 8.424838 hw_loss 0.163785 lr 0.00042040 rank 4
2023-02-23 09:49:39,602 DEBUG TRAIN Batch 16/8000 loss 11.581267 loss_att 14.742683 loss_ctc 13.992963 loss_rnnt 10.508001 hw_loss 0.223917 lr 0.00042032 rank 5
2023-02-23 09:49:39,602 DEBUG TRAIN Batch 16/8000 loss 13.867254 loss_att 19.620462 loss_ctc 21.888071 loss_rnnt 11.498436 hw_loss 0.278876 lr 0.00042037 rank 3
2023-02-23 09:49:39,604 DEBUG TRAIN Batch 16/8000 loss 14.333108 loss_att 19.885752 loss_ctc 18.559509 loss_rnnt 12.424265 hw_loss 0.440237 lr 0.00042034 rank 7
2023-02-23 09:49:39,603 DEBUG TRAIN Batch 16/8000 loss 13.172791 loss_att 15.860292 loss_ctc 16.319813 loss_rnnt 12.077568 hw_loss 0.258977 lr 0.00042041 rank 0
2023-02-23 09:50:55,645 DEBUG TRAIN Batch 16/8100 loss 9.993849 loss_att 12.975121 loss_ctc 17.971125 loss_rnnt 8.197002 hw_loss 0.256790 lr 0.00042025 rank 4
2023-02-23 09:50:55,647 DEBUG TRAIN Batch 16/8100 loss 5.340345 loss_att 8.074955 loss_ctc 9.273137 loss_rnnt 4.125284 hw_loss 0.269563 lr 0.00042017 rank 5
2023-02-23 09:50:55,647 DEBUG TRAIN Batch 16/8100 loss 8.035400 loss_att 10.927282 loss_ctc 10.187218 loss_rnnt 7.028780 hw_loss 0.265006 lr 0.00042025 rank 2
2023-02-23 09:50:55,653 DEBUG TRAIN Batch 16/8100 loss 11.091931 loss_att 15.516012 loss_ctc 14.254357 loss_rnnt 9.648049 hw_loss 0.257639 lr 0.00042027 rank 1
2023-02-23 09:50:55,654 DEBUG TRAIN Batch 16/8100 loss 10.322044 loss_att 14.315428 loss_ctc 17.430664 loss_rnnt 8.429499 hw_loss 0.273851 lr 0.00042023 rank 6
2023-02-23 09:50:55,655 DEBUG TRAIN Batch 16/8100 loss 8.272559 loss_att 11.639349 loss_ctc 11.376280 loss_rnnt 7.083517 hw_loss 0.190978 lr 0.00042019 rank 7
2023-02-23 09:50:55,656 DEBUG TRAIN Batch 16/8100 loss 13.797368 loss_att 18.494217 loss_ctc 24.875896 loss_rnnt 11.243292 hw_loss 0.257942 lr 0.00042026 rank 0
2023-02-23 09:50:55,656 DEBUG TRAIN Batch 16/8100 loss 14.317092 loss_att 16.275415 loss_ctc 18.311670 loss_rnnt 13.277226 hw_loss 0.216729 lr 0.00042022 rank 3
2023-02-23 09:52:13,653 DEBUG TRAIN Batch 16/8200 loss 10.710825 loss_att 10.547553 loss_ctc 15.542524 loss_rnnt 9.940864 hw_loss 0.296978 lr 0.00042007 rank 3
2023-02-23 09:52:13,654 DEBUG TRAIN Batch 16/8200 loss 23.203932 loss_att 24.972002 loss_ctc 33.009491 loss_rnnt 21.397736 hw_loss 0.272203 lr 0.00042011 rank 0
2023-02-23 09:52:13,655 DEBUG TRAIN Batch 16/8200 loss 14.928339 loss_att 16.499706 loss_ctc 19.366062 loss_rnnt 13.834897 hw_loss 0.351511 lr 0.00042010 rank 2
2023-02-23 09:52:13,657 DEBUG TRAIN Batch 16/8200 loss 16.701004 loss_att 18.840229 loss_ctc 21.249889 loss_rnnt 15.505337 hw_loss 0.302445 lr 0.00042002 rank 5
2023-02-23 09:52:13,657 DEBUG TRAIN Batch 16/8200 loss 5.643914 loss_att 7.642915 loss_ctc 7.120703 loss_rnnt 4.886120 hw_loss 0.302040 lr 0.00042010 rank 4
2023-02-23 09:52:13,663 DEBUG TRAIN Batch 16/8200 loss 6.838372 loss_att 10.302473 loss_ctc 12.656303 loss_rnnt 5.276131 hw_loss 0.175682 lr 0.00042012 rank 1
2023-02-23 09:52:13,666 DEBUG TRAIN Batch 16/8200 loss 13.168101 loss_att 13.873747 loss_ctc 18.330906 loss_rnnt 12.219721 hw_loss 0.222892 lr 0.00042005 rank 7
2023-02-23 09:52:13,666 DEBUG TRAIN Batch 16/8200 loss 10.510448 loss_att 11.961437 loss_ctc 12.926741 loss_rnnt 9.815823 hw_loss 0.154226 lr 0.00042008 rank 6
2023-02-23 09:53:29,355 DEBUG TRAIN Batch 16/8300 loss 11.161566 loss_att 12.089063 loss_ctc 13.318934 loss_rnnt 10.566589 hw_loss 0.228428 lr 0.00041987 rank 5
2023-02-23 09:53:29,359 DEBUG TRAIN Batch 16/8300 loss 11.154109 loss_att 11.909358 loss_ctc 14.567355 loss_rnnt 10.398055 hw_loss 0.281071 lr 0.00041990 rank 7
2023-02-23 09:53:29,362 DEBUG TRAIN Batch 16/8300 loss 5.412476 loss_att 8.001022 loss_ctc 4.800319 loss_rnnt 4.895773 hw_loss 0.151152 lr 0.00041997 rank 0
2023-02-23 09:53:29,363 DEBUG TRAIN Batch 16/8300 loss 17.097218 loss_att 17.554134 loss_ctc 20.353367 loss_rnnt 16.397091 hw_loss 0.327358 lr 0.00041995 rank 4
2023-02-23 09:53:29,366 DEBUG TRAIN Batch 16/8300 loss 17.958008 loss_att 21.302965 loss_ctc 32.085510 loss_rnnt 15.355984 hw_loss 0.092560 lr 0.00041995 rank 2
2023-02-23 09:53:29,367 DEBUG TRAIN Batch 16/8300 loss 10.564565 loss_att 15.457973 loss_ctc 14.635296 loss_rnnt 8.899261 hw_loss 0.269733 lr 0.00041992 rank 3
2023-02-23 09:53:29,367 DEBUG TRAIN Batch 16/8300 loss 6.604855 loss_att 9.975187 loss_ctc 10.323482 loss_rnnt 5.292428 hw_loss 0.267268 lr 0.00041993 rank 6
2023-02-23 09:53:29,367 DEBUG TRAIN Batch 16/8300 loss 4.249524 loss_att 7.410566 loss_ctc 5.442081 loss_rnnt 3.323534 hw_loss 0.252701 lr 0.00041997 rank 1
2023-02-23 09:54:20,481 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340964 history loss 2.141657 rank 2
2023-02-23 09:54:20,485 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 4
2023-02-23 09:54:20,485 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 3
2023-02-23 09:54:20,487 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 1
2023-02-23 09:54:20,492 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 0
2023-02-23 09:54:20,495 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 7
2023-02-23 09:54:20,497 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 6
2023-02-23 09:54:20,509 DEBUG CV Batch 16/0 loss 2.224029 loss_att 2.214139 loss_ctc 3.184474 loss_rnnt 1.916099 hw_loss 0.340965 history loss 2.141657 rank 5
2023-02-23 09:54:31,594 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 2
2023-02-23 09:54:31,781 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 0
2023-02-23 09:54:31,864 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 5
2023-02-23 09:54:31,905 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 6
2023-02-23 09:54:31,982 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 7
2023-02-23 09:54:32,028 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 4
2023-02-23 09:54:32,183 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 3
2023-02-23 09:54:32,439 DEBUG CV Batch 16/100 loss 7.309299 loss_att 7.919297 loss_ctc 9.937850 loss_rnnt 6.706790 hw_loss 0.243817 history loss 3.734548 rank 1
2023-02-23 09:54:45,869 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 2
2023-02-23 09:54:45,947 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 0
2023-02-23 09:54:45,972 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 5
2023-02-23 09:54:46,228 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 4
2023-02-23 09:54:46,268 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 6
2023-02-23 09:54:46,459 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 7
2023-02-23 09:54:46,575 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 3
2023-02-23 09:54:47,087 DEBUG CV Batch 16/200 loss 10.098730 loss_att 14.121544 loss_ctc 15.579445 loss_rnnt 8.481649 hw_loss 0.153292 history loss 4.334213 rank 1
2023-02-23 09:54:57,849 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 2
2023-02-23 09:54:57,934 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 5
2023-02-23 09:54:58,036 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 0
2023-02-23 09:54:58,308 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 4
2023-02-23 09:54:58,435 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 6
2023-02-23 09:54:58,684 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 7
2023-02-23 09:54:58,746 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 3
2023-02-23 09:54:59,399 DEBUG CV Batch 16/300 loss 3.865914 loss_att 4.727342 loss_ctc 5.820909 loss_rnnt 3.262773 hw_loss 0.319106 history loss 4.464194 rank 1
2023-02-23 09:55:09,644 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 2
2023-02-23 09:55:09,721 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 5
2023-02-23 09:55:09,994 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 0
2023-02-23 09:55:10,164 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 4
2023-02-23 09:55:10,616 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 6
2023-02-23 09:55:10,756 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 3
2023-02-23 09:55:10,807 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 7
2023-02-23 09:55:11,550 DEBUG CV Batch 16/400 loss 24.790007 loss_att 128.855270 loss_ctc 8.166493 loss_rnnt 6.126495 hw_loss 0.125489 history loss 5.463183 rank 1
2023-02-23 09:55:20,050 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 2
2023-02-23 09:55:20,132 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 5
2023-02-23 09:55:20,441 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 0
2023-02-23 09:55:20,690 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 4
2023-02-23 09:55:21,215 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 6
2023-02-23 09:55:21,510 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 3
2023-02-23 09:55:21,567 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 7
2023-02-23 09:55:22,226 DEBUG CV Batch 16/500 loss 4.282570 loss_att 5.520686 loss_ctc 6.838468 loss_rnnt 3.595126 hw_loss 0.185691 history loss 6.243322 rank 1
2023-02-23 09:55:33,298 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 5
2023-02-23 09:55:33,317 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 2
2023-02-23 09:55:33,368 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 0
2023-02-23 09:55:33,783 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 4
2023-02-23 09:55:34,240 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 6
2023-02-23 09:55:34,603 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 1
2023-02-23 09:55:34,685 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 3
2023-02-23 09:55:34,858 DEBUG CV Batch 16/600 loss 8.411128 loss_att 7.721046 loss_ctc 10.418935 loss_rnnt 8.064644 hw_loss 0.406486 history loss 7.255360 rank 7
2023-02-23 09:55:46,008 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 0
2023-02-23 09:55:46,051 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 5
2023-02-23 09:55:46,290 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 2
2023-02-23 09:55:46,581 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 4
2023-02-23 09:55:47,384 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 6
2023-02-23 09:55:47,577 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 1
2023-02-23 09:55:47,743 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 7
2023-02-23 09:55:48,136 DEBUG CV Batch 16/700 loss 22.150534 loss_att 50.898590 loss_ctc 22.972696 loss_rnnt 16.236519 hw_loss 0.102717 history loss 7.968702 rank 3
2023-02-23 09:55:58,237 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 2
2023-02-23 09:55:58,423 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 5
2023-02-23 09:55:58,535 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 0
2023-02-23 09:55:59,374 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 4
2023-02-23 09:55:59,713 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 6
2023-02-23 09:56:00,255 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 1
2023-02-23 09:56:00,905 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 3
2023-02-23 09:56:01,666 DEBUG CV Batch 16/800 loss 11.590859 loss_att 11.044053 loss_ctc 15.600601 loss_rnnt 11.006572 hw_loss 0.298157 history loss 7.396313 rank 7
2023-02-23 09:56:13,159 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 2
2023-02-23 09:56:13,361 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 0
2023-02-23 09:56:13,477 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 5
2023-02-23 09:56:13,533 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 4
2023-02-23 09:56:13,979 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 6
2023-02-23 09:56:14,331 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 1
2023-02-23 09:56:14,441 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 3
2023-02-23 09:56:15,494 DEBUG CV Batch 16/900 loss 12.586312 loss_att 14.766001 loss_ctc 22.218134 loss_rnnt 10.769011 hw_loss 0.182102 history loss 7.181597 rank 7
2023-02-23 09:56:25,300 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 2
2023-02-23 09:56:25,579 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 5
2023-02-23 09:56:25,636 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 0
2023-02-23 09:56:25,767 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 4
2023-02-23 09:56:26,281 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 6
2023-02-23 09:56:26,627 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 1
2023-02-23 09:56:26,712 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 3
2023-02-23 09:56:28,690 DEBUG CV Batch 16/1000 loss 5.865458 loss_att 7.131359 loss_ctc 6.098889 loss_rnnt 5.484808 hw_loss 0.180648 history loss 6.931915 rank 7
2023-02-23 09:56:37,132 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 2
2023-02-23 09:56:37,404 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 5
2023-02-23 09:56:37,520 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 0
2023-02-23 09:56:37,679 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 4
2023-02-23 09:56:38,358 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 6
2023-02-23 09:56:38,625 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 1
2023-02-23 09:56:38,642 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 3
2023-02-23 09:56:40,869 DEBUG CV Batch 16/1100 loss 7.076802 loss_att 6.027405 loss_ctc 8.142113 loss_rnnt 6.962940 hw_loss 0.340685 history loss 6.909132 rank 7
2023-02-23 09:56:47,496 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 2
2023-02-23 09:56:47,701 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 5
2023-02-23 09:56:48,067 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 0
2023-02-23 09:56:48,194 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 4
2023-02-23 09:56:49,019 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 6
2023-02-23 09:56:49,087 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 3
2023-02-23 09:56:49,205 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 1
2023-02-23 09:56:51,636 DEBUG CV Batch 16/1200 loss 9.091649 loss_att 9.973090 loss_ctc 11.141870 loss_rnnt 8.474782 hw_loss 0.313528 history loss 7.245065 rank 7
2023-02-23 09:56:59,325 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 2
2023-02-23 09:56:59,514 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 5
2023-02-23 09:57:00,050 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 0
2023-02-23 09:57:00,149 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 4
2023-02-23 09:57:01,117 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 3
2023-02-23 09:57:01,155 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 6
2023-02-23 09:57:01,356 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 1
2023-02-23 09:57:03,740 DEBUG CV Batch 16/1300 loss 5.797420 loss_att 5.729664 loss_ctc 7.914284 loss_rnnt 5.366621 hw_loss 0.303937 history loss 7.574075 rank 7
2023-02-23 09:57:10,410 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 2
2023-02-23 09:57:10,694 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 5
2023-02-23 09:57:11,389 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 0
2023-02-23 09:57:11,970 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 4
2023-02-23 09:57:13,016 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 3
2023-02-23 09:57:13,609 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 1
2023-02-23 09:57:13,656 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 6
2023-02-23 09:57:15,161 DEBUG CV Batch 16/1400 loss 9.782362 loss_att 29.660118 loss_ctc 5.231235 loss_rnnt 6.386116 hw_loss 0.051585 history loss 7.927857 rank 7
2023-02-23 09:57:23,100 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 2
2023-02-23 09:57:23,109 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 5
2023-02-23 09:57:23,773 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 0
2023-02-23 09:57:24,361 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 4
2023-02-23 09:57:25,756 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 3
2023-02-23 09:57:25,776 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 6
2023-02-23 09:57:25,960 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 1
2023-02-23 09:57:27,261 DEBUG CV Batch 16/1500 loss 7.681017 loss_att 8.241766 loss_ctc 8.003295 loss_rnnt 7.336112 hw_loss 0.355848 history loss 7.750311 rank 7
2023-02-23 09:57:37,013 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 5
2023-02-23 09:57:37,170 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 2
2023-02-23 09:57:37,481 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 0
2023-02-23 09:57:37,944 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 4
2023-02-23 09:57:39,603 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 3
2023-02-23 09:57:39,893 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 1
2023-02-23 09:57:39,899 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 6
2023-02-23 09:57:40,937 DEBUG CV Batch 16/1600 loss 13.314330 loss_att 17.247749 loss_ctc 14.029457 loss_rnnt 12.326325 hw_loss 0.198696 history loss 7.678526 rank 7
2023-02-23 09:57:49,422 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 5
2023-02-23 09:57:49,739 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 2
2023-02-23 09:57:50,102 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 0
2023-02-23 09:57:50,499 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 4
2023-02-23 09:57:52,044 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 3
2023-02-23 09:57:52,595 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 6
2023-02-23 09:57:53,078 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 1
2023-02-23 09:57:53,536 DEBUG CV Batch 16/1700 loss 12.009539 loss_att 11.798404 loss_ctc 16.905224 loss_rnnt 11.200542 hw_loss 0.372123 history loss 7.571513 rank 7
2023-02-23 09:57:58,622 INFO Epoch 16 CV info cv_loss 7.533550715481792
2023-02-23 09:57:58,623 INFO Epoch 17 TRAIN info lr 0.00041982430838274925
2023-02-23 09:57:58,627 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:57:58,975 INFO Epoch 16 CV info cv_loss 7.533550715275041
2023-02-23 09:57:58,977 INFO Epoch 17 TRAIN info lr 0.0004198746340731177
2023-02-23 09:57:58,981 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:57:59,391 INFO Epoch 16 CV info cv_loss 7.533550716726607
2023-02-23 09:57:59,393 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/16.pt
2023-02-23 09:57:59,710 INFO Epoch 16 CV info cv_loss 7.533550715408568
2023-02-23 09:57:59,711 INFO Epoch 17 TRAIN info lr 0.0004199116498055242
2023-02-23 09:57:59,714 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:58:00,859 INFO Epoch 17 TRAIN info lr 0.0004199249778659335
2023-02-23 09:58:00,863 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:58:01,197 INFO Epoch 16 CV info cv_loss 7.533550715158744
2023-02-23 09:58:01,198 INFO Epoch 17 TRAIN info lr 0.0004198361480936397
2023-02-23 09:58:01,200 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:58:02,143 INFO Epoch 16 CV info cv_loss 7.533550714486802
2023-02-23 09:58:02,143 INFO Epoch 17 TRAIN info lr 0.0004199072074007273
2023-02-23 09:58:02,147 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:58:02,400 INFO Epoch 16 CV info cv_loss 7.53355071456218
2023-02-23 09:58:02,401 INFO Epoch 17 TRAIN info lr 0.00041993682609622375
2023-02-23 09:58:02,406 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:58:02,836 INFO Epoch 16 CV info cv_loss 7.533550716343256
2023-02-23 09:58:02,837 INFO Epoch 17 TRAIN info lr 0.0004198435484216397
2023-02-23 09:58:02,841 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:59:15,136 DEBUG TRAIN Batch 17/0 loss 10.708901 loss_att 9.513086 loss_ctc 12.471505 loss_rnnt 10.513348 hw_loss 0.374444 lr 0.00041992 rank 0
2023-02-23 09:59:15,136 DEBUG TRAIN Batch 17/0 loss 9.698354 loss_att 8.493553 loss_ctc 11.410954 loss_rnnt 9.467590 hw_loss 0.456330 lr 0.00041991 rank 4
2023-02-23 09:59:15,136 DEBUG TRAIN Batch 17/0 loss 11.372642 loss_att 12.130668 loss_ctc 14.497084 loss_rnnt 10.688905 hw_loss 0.216637 lr 0.00041987 rank 2
2023-02-23 09:59:15,148 DEBUG TRAIN Batch 17/0 loss 6.571365 loss_att 6.624870 loss_ctc 8.270241 loss_rnnt 6.108506 hw_loss 0.423077 lr 0.00041994 rank 1
2023-02-23 09:59:15,156 DEBUG TRAIN Batch 17/0 loss 12.156005 loss_att 10.995512 loss_ctc 17.469944 loss_rnnt 11.478931 hw_loss 0.376212 lr 0.00041982 rank 5
2023-02-23 09:59:15,165 DEBUG TRAIN Batch 17/0 loss 12.555894 loss_att 12.226485 loss_ctc 15.996122 loss_rnnt 12.004253 hw_loss 0.297797 lr 0.00041991 rank 6
2023-02-23 09:59:15,195 DEBUG TRAIN Batch 17/0 loss 12.229213 loss_att 10.727771 loss_ctc 14.894129 loss_rnnt 11.994326 hw_loss 0.337223 lr 0.00041984 rank 7
2023-02-23 09:59:15,236 DEBUG TRAIN Batch 17/0 loss 11.333841 loss_att 11.901953 loss_ctc 13.468863 loss_rnnt 10.771633 hw_loss 0.307343 lr 0.00041983 rank 3
2023-02-23 10:00:31,511 DEBUG TRAIN Batch 17/100 loss 7.145563 loss_att 9.618871 loss_ctc 8.959051 loss_rnnt 6.264972 hw_loss 0.270245 lr 0.00041973 rank 2
2023-02-23 10:00:31,513 DEBUG TRAIN Batch 17/100 loss 3.555646 loss_att 10.334897 loss_ctc 5.426328 loss_rnnt 1.855395 hw_loss 0.178079 lr 0.00041967 rank 5
2023-02-23 10:00:31,514 DEBUG TRAIN Batch 17/100 loss 6.658699 loss_att 10.111596 loss_ctc 9.905352 loss_rnnt 5.399302 hw_loss 0.254870 lr 0.00041976 rank 4
2023-02-23 10:00:31,515 DEBUG TRAIN Batch 17/100 loss 1.866475 loss_att 4.850318 loss_ctc 1.674174 loss_rnnt 1.231342 hw_loss 0.120009 lr 0.00041979 rank 1
2023-02-23 10:00:31,516 DEBUG TRAIN Batch 17/100 loss 7.910573 loss_att 11.714376 loss_ctc 10.567195 loss_rnnt 6.718679 hw_loss 0.144218 lr 0.00041978 rank 0
2023-02-23 10:00:31,519 DEBUG TRAIN Batch 17/100 loss 6.184655 loss_att 8.596844 loss_ctc 7.662389 loss_rnnt 5.419012 hw_loss 0.161577 lr 0.00041969 rank 7
2023-02-23 10:00:31,519 DEBUG TRAIN Batch 17/100 loss 16.014395 loss_att 17.357288 loss_ctc 17.071991 loss_rnnt 15.511278 hw_loss 0.175358 lr 0.00041976 rank 6
2023-02-23 10:00:31,520 DEBUG TRAIN Batch 17/100 loss 4.730423 loss_att 7.575503 loss_ctc 5.825320 loss_rnnt 3.926162 hw_loss 0.167361 lr 0.00041969 rank 3
2023-02-23 10:01:46,697 DEBUG TRAIN Batch 17/200 loss 9.083108 loss_att 13.697150 loss_ctc 13.660785 loss_rnnt 7.455253 hw_loss 0.177544 lr 0.00041955 rank 7
2023-02-23 10:01:46,697 DEBUG TRAIN Batch 17/200 loss 4.223354 loss_att 7.782807 loss_ctc 4.701691 loss_rnnt 3.330703 hw_loss 0.219340 lr 0.00041958 rank 2
2023-02-23 10:01:46,698 DEBUG TRAIN Batch 17/200 loss 17.163755 loss_att 22.317545 loss_ctc 26.560238 loss_rnnt 14.776271 hw_loss 0.194741 lr 0.00041954 rank 3
2023-02-23 10:01:46,698 DEBUG TRAIN Batch 17/200 loss 4.362869 loss_att 9.010827 loss_ctc 6.001745 loss_rnnt 3.091801 hw_loss 0.230550 lr 0.00041953 rank 5
2023-02-23 10:01:46,704 DEBUG TRAIN Batch 17/200 loss 8.531281 loss_att 12.709685 loss_ctc 11.128134 loss_rnnt 7.221140 hw_loss 0.240399 lr 0.00041964 rank 1
2023-02-23 10:01:46,706 DEBUG TRAIN Batch 17/200 loss 7.016144 loss_att 11.497143 loss_ctc 8.741775 loss_rnnt 5.781715 hw_loss 0.202771 lr 0.00041961 rank 4
2023-02-23 10:01:46,708 DEBUG TRAIN Batch 17/200 loss 7.210230 loss_att 9.311188 loss_ctc 9.259488 loss_rnnt 6.422656 hw_loss 0.176529 lr 0.00041961 rank 6
2023-02-23 10:01:46,708 DEBUG TRAIN Batch 17/200 loss 3.089250 loss_att 7.412811 loss_ctc 6.021449 loss_rnnt 1.684696 hw_loss 0.279153 lr 0.00041963 rank 0
2023-02-23 10:03:04,386 DEBUG TRAIN Batch 17/300 loss 11.310844 loss_att 13.627102 loss_ctc 16.422884 loss_rnnt 10.070700 hw_loss 0.178663 lr 0.00041938 rank 5
2023-02-23 10:03:04,388 DEBUG TRAIN Batch 17/300 loss 21.553183 loss_att 25.509727 loss_ctc 36.568459 loss_rnnt 18.645323 hw_loss 0.214713 lr 0.00041947 rank 4
2023-02-23 10:03:04,388 DEBUG TRAIN Batch 17/300 loss 8.863384 loss_att 12.635057 loss_ctc 12.563896 loss_rnnt 7.512298 hw_loss 0.193783 lr 0.00041943 rank 2
2023-02-23 10:03:04,389 DEBUG TRAIN Batch 17/300 loss 6.990192 loss_att 9.511093 loss_ctc 10.342096 loss_rnnt 5.926318 hw_loss 0.211451 lr 0.00041940 rank 7
2023-02-23 10:03:04,390 DEBUG TRAIN Batch 17/300 loss 7.637811 loss_att 10.416362 loss_ctc 11.131547 loss_rnnt 6.503682 hw_loss 0.211099 lr 0.00041939 rank 3
2023-02-23 10:03:04,390 DEBUG TRAIN Batch 17/300 loss 34.102421 loss_att 38.398281 loss_ctc 40.975952 loss_rnnt 32.179688 hw_loss 0.275795 lr 0.00041948 rank 0
2023-02-23 10:03:04,391 DEBUG TRAIN Batch 17/300 loss 7.426833 loss_att 9.876762 loss_ctc 7.458218 loss_rnnt 6.831166 hw_loss 0.190306 lr 0.00041946 rank 6
2023-02-23 10:03:04,395 DEBUG TRAIN Batch 17/300 loss 17.834740 loss_att 21.035084 loss_ctc 23.139820 loss_rnnt 16.362362 hw_loss 0.234306 lr 0.00041949 rank 1
2023-02-23 10:04:22,835 DEBUG TRAIN Batch 17/400 loss 13.045923 loss_att 15.578340 loss_ctc 12.928247 loss_rnnt 12.468210 hw_loss 0.162973 lr 0.00041928 rank 2
2023-02-23 10:04:22,842 DEBUG TRAIN Batch 17/400 loss 9.085775 loss_att 12.343109 loss_ctc 15.817551 loss_rnnt 7.420647 hw_loss 0.217672 lr 0.00041932 rank 4
2023-02-23 10:04:22,843 DEBUG TRAIN Batch 17/400 loss 7.597805 loss_att 8.913673 loss_ctc 7.872344 loss_rnnt 7.157988 hw_loss 0.262572 lr 0.00041924 rank 3
2023-02-23 10:04:22,843 DEBUG TRAIN Batch 17/400 loss 15.646004 loss_att 19.076946 loss_ctc 19.500008 loss_rnnt 14.321783 hw_loss 0.232810 lr 0.00041923 rank 5
2023-02-23 10:04:22,845 DEBUG TRAIN Batch 17/400 loss 15.521179 loss_att 17.530365 loss_ctc 22.545465 loss_rnnt 14.108578 hw_loss 0.139113 lr 0.00041934 rank 1
2023-02-23 10:04:22,844 DEBUG TRAIN Batch 17/400 loss 6.647947 loss_att 8.234194 loss_ctc 9.775834 loss_rnnt 5.783530 hw_loss 0.243969 lr 0.00041933 rank 0
2023-02-23 10:04:22,845 DEBUG TRAIN Batch 17/400 loss 7.546123 loss_att 10.924366 loss_ctc 10.448813 loss_rnnt 6.321437 hw_loss 0.303773 lr 0.00041931 rank 6
2023-02-23 10:04:22,847 DEBUG TRAIN Batch 17/400 loss 9.742852 loss_att 11.863544 loss_ctc 14.344365 loss_rnnt 8.592205 hw_loss 0.211825 lr 0.00041925 rank 7
2023-02-23 10:05:40,020 DEBUG TRAIN Batch 17/500 loss 18.516668 loss_att 20.183018 loss_ctc 26.776474 loss_rnnt 16.937489 hw_loss 0.271128 lr 0.00041917 rank 4
2023-02-23 10:05:40,026 DEBUG TRAIN Batch 17/500 loss 13.760983 loss_att 17.251980 loss_ctc 18.438938 loss_rnnt 12.322701 hw_loss 0.218168 lr 0.00041913 rank 2
2023-02-23 10:05:40,027 DEBUG TRAIN Batch 17/500 loss 11.825991 loss_att 13.010783 loss_ctc 17.046928 loss_rnnt 10.796408 hw_loss 0.180938 lr 0.00041918 rank 0
2023-02-23 10:05:40,029 DEBUG TRAIN Batch 17/500 loss 4.629840 loss_att 8.076452 loss_ctc 7.747644 loss_rnnt 3.427869 hw_loss 0.181766 lr 0.00041908 rank 5
2023-02-23 10:05:40,032 DEBUG TRAIN Batch 17/500 loss 7.410565 loss_att 10.055173 loss_ctc 12.022560 loss_rnnt 6.095381 hw_loss 0.321243 lr 0.00041917 rank 6
2023-02-23 10:05:40,032 DEBUG TRAIN Batch 17/500 loss 13.588031 loss_att 17.424568 loss_ctc 22.724117 loss_rnnt 11.472149 hw_loss 0.244553 lr 0.00041910 rank 7
2023-02-23 10:05:40,032 DEBUG TRAIN Batch 17/500 loss 13.723486 loss_att 15.223495 loss_ctc 17.859146 loss_rnnt 12.690079 hw_loss 0.341221 lr 0.00041920 rank 1
2023-02-23 10:05:40,074 DEBUG TRAIN Batch 17/500 loss 9.980382 loss_att 11.099567 loss_ctc 12.139244 loss_rnnt 9.357668 hw_loss 0.208177 lr 0.00041910 rank 3
2023-02-23 10:06:56,963 DEBUG TRAIN Batch 17/600 loss 4.996490 loss_att 7.423755 loss_ctc 6.658812 loss_rnnt 4.165652 hw_loss 0.232016 lr 0.00041902 rank 4
2023-02-23 10:06:56,966 DEBUG TRAIN Batch 17/600 loss 9.269396 loss_att 10.304108 loss_ctc 11.418333 loss_rnnt 8.626545 hw_loss 0.280094 lr 0.00041904 rank 0
2023-02-23 10:06:56,967 DEBUG TRAIN Batch 17/600 loss 14.855838 loss_att 15.053514 loss_ctc 17.712421 loss_rnnt 14.285974 hw_loss 0.280221 lr 0.00041895 rank 3
2023-02-23 10:06:56,967 DEBUG TRAIN Batch 17/600 loss 15.436322 loss_att 15.522014 loss_ctc 20.818918 loss_rnnt 14.557503 hw_loss 0.270003 lr 0.00041894 rank 5
2023-02-23 10:06:56,969 DEBUG TRAIN Batch 17/600 loss 4.389634 loss_att 6.121374 loss_ctc 5.806909 loss_rnnt 3.731076 hw_loss 0.231074 lr 0.00041899 rank 2
2023-02-23 10:06:56,972 DEBUG TRAIN Batch 17/600 loss 5.435619 loss_att 6.790299 loss_ctc 9.771613 loss_rnnt 4.462918 hw_loss 0.231810 lr 0.00041896 rank 7
2023-02-23 10:06:56,973 DEBUG TRAIN Batch 17/600 loss 12.342438 loss_att 12.953334 loss_ctc 15.998215 loss_rnnt 11.553308 hw_loss 0.336587 lr 0.00041902 rank 6
2023-02-23 10:06:56,980 DEBUG TRAIN Batch 17/600 loss 24.010717 loss_att 23.984997 loss_ctc 33.650909 loss_rnnt 22.591034 hw_loss 0.261505 lr 0.00041905 rank 1
2023-02-23 10:08:16,434 DEBUG TRAIN Batch 17/700 loss 5.677879 loss_att 10.350685 loss_ctc 6.877227 loss_rnnt 4.438499 hw_loss 0.271700 lr 0.00041884 rank 2
2023-02-23 10:08:16,438 DEBUG TRAIN Batch 17/700 loss 4.608956 loss_att 6.326650 loss_ctc 4.513787 loss_rnnt 4.157208 hw_loss 0.226684 lr 0.00041880 rank 3
2023-02-23 10:08:16,442 DEBUG TRAIN Batch 17/700 loss 8.450988 loss_att 14.320295 loss_ctc 14.377928 loss_rnnt 6.349336 hw_loss 0.257871 lr 0.00041881 rank 7
2023-02-23 10:08:16,452 DEBUG TRAIN Batch 17/700 loss 7.919081 loss_att 11.835054 loss_ctc 11.426948 loss_rnnt 6.547863 hw_loss 0.225578 lr 0.00041887 rank 6
2023-02-23 10:08:16,456 DEBUG TRAIN Batch 17/700 loss 15.630825 loss_att 17.887091 loss_ctc 19.806679 loss_rnnt 14.525493 hw_loss 0.182434 lr 0.00041888 rank 4
2023-02-23 10:08:16,458 DEBUG TRAIN Batch 17/700 loss 9.806512 loss_att 11.388514 loss_ctc 12.308926 loss_rnnt 9.025854 hw_loss 0.244879 lr 0.00041879 rank 5
2023-02-23 10:08:16,463 DEBUG TRAIN Batch 17/700 loss 8.819858 loss_att 9.652382 loss_ctc 10.344637 loss_rnnt 8.340782 hw_loss 0.204875 lr 0.00041889 rank 0
2023-02-23 10:08:16,491 DEBUG TRAIN Batch 17/700 loss 6.471953 loss_att 9.303956 loss_ctc 8.833195 loss_rnnt 5.451181 hw_loss 0.261634 lr 0.00041890 rank 1
2023-02-23 10:09:31,390 DEBUG TRAIN Batch 17/800 loss 12.261938 loss_att 14.366335 loss_ctc 15.434016 loss_rnnt 11.288403 hw_loss 0.243212 lr 0.00041864 rank 5
2023-02-23 10:09:31,392 DEBUG TRAIN Batch 17/800 loss 9.903651 loss_att 15.267569 loss_ctc 12.002501 loss_rnnt 8.409434 hw_loss 0.265477 lr 0.00041869 rank 2
2023-02-23 10:09:31,394 DEBUG TRAIN Batch 17/800 loss 7.220800 loss_att 12.556184 loss_ctc 10.248022 loss_rnnt 5.616363 hw_loss 0.250746 lr 0.00041873 rank 4
2023-02-23 10:09:31,397 DEBUG TRAIN Batch 17/800 loss 11.295259 loss_att 16.157080 loss_ctc 16.983534 loss_rnnt 9.397693 hw_loss 0.312686 lr 0.00041874 rank 0
2023-02-23 10:09:31,399 DEBUG TRAIN Batch 17/800 loss 6.471127 loss_att 11.831087 loss_ctc 12.341555 loss_rnnt 4.442652 hw_loss 0.325798 lr 0.00041866 rank 3
2023-02-23 10:09:31,399 DEBUG TRAIN Batch 17/800 loss 12.396013 loss_att 15.568453 loss_ctc 18.281282 loss_rnnt 10.784821 hw_loss 0.360005 lr 0.00041873 rank 6
2023-02-23 10:09:31,401 DEBUG TRAIN Batch 17/800 loss 6.256154 loss_att 12.746574 loss_ctc 9.448154 loss_rnnt 4.453204 hw_loss 0.148623 lr 0.00041866 rank 7
2023-02-23 10:09:31,447 DEBUG TRAIN Batch 17/800 loss 10.347094 loss_att 12.955989 loss_ctc 15.809244 loss_rnnt 8.983431 hw_loss 0.212995 lr 0.00041876 rank 1
2023-02-23 10:10:47,336 DEBUG TRAIN Batch 17/900 loss 6.861120 loss_att 8.839241 loss_ctc 9.415363 loss_rnnt 6.023035 hw_loss 0.191053 lr 0.00041860 rank 0
2023-02-23 10:10:47,336 DEBUG TRAIN Batch 17/900 loss 6.307493 loss_att 7.779571 loss_ctc 8.314152 loss_rnnt 5.624547 hw_loss 0.226827 lr 0.00041858 rank 4
2023-02-23 10:10:47,337 DEBUG TRAIN Batch 17/900 loss 20.501253 loss_att 20.966618 loss_ctc 28.119017 loss_rnnt 19.261002 hw_loss 0.246520 lr 0.00041850 rank 5
2023-02-23 10:10:47,338 DEBUG TRAIN Batch 17/900 loss 6.627032 loss_att 8.703511 loss_ctc 8.509207 loss_rnnt 5.821441 hw_loss 0.261261 lr 0.00041852 rank 7
2023-02-23 10:10:47,340 DEBUG TRAIN Batch 17/900 loss 6.660168 loss_att 10.263479 loss_ctc 8.111630 loss_rnnt 5.632920 hw_loss 0.211980 lr 0.00041851 rank 3
2023-02-23 10:10:47,346 DEBUG TRAIN Batch 17/900 loss 12.431775 loss_att 13.481579 loss_ctc 16.378492 loss_rnnt 11.587317 hw_loss 0.203007 lr 0.00041861 rank 1
2023-02-23 10:10:47,347 DEBUG TRAIN Batch 17/900 loss 2.399536 loss_att 4.958028 loss_ctc 3.603716 loss_rnnt 1.648596 hw_loss 0.147532 lr 0.00041855 rank 2
2023-02-23 10:10:47,388 DEBUG TRAIN Batch 17/900 loss 14.932053 loss_att 16.925114 loss_ctc 20.285223 loss_rnnt 13.725697 hw_loss 0.176228 lr 0.00041858 rank 6
2023-02-23 10:12:03,904 DEBUG TRAIN Batch 17/1000 loss 14.749634 loss_att 18.277044 loss_ctc 20.414787 loss_rnnt 13.216990 hw_loss 0.134639 lr 0.00041835 rank 5
2023-02-23 10:12:03,905 DEBUG TRAIN Batch 17/1000 loss 17.865425 loss_att 23.204657 loss_ctc 26.033791 loss_rnnt 15.552755 hw_loss 0.291953 lr 0.00041844 rank 4
2023-02-23 10:12:03,906 DEBUG TRAIN Batch 17/1000 loss 7.783374 loss_att 9.045856 loss_ctc 8.700176 loss_rnnt 7.304265 hw_loss 0.195698 lr 0.00041836 rank 3
2023-02-23 10:12:03,907 DEBUG TRAIN Batch 17/1000 loss 9.277588 loss_att 16.614399 loss_ctc 13.280049 loss_rnnt 7.200847 hw_loss 0.141972 lr 0.00041845 rank 0
2023-02-23 10:12:03,913 DEBUG TRAIN Batch 17/1000 loss 12.026519 loss_att 18.276150 loss_ctc 19.294575 loss_rnnt 9.666128 hw_loss 0.265108 lr 0.00041843 rank 6
2023-02-23 10:12:03,916 DEBUG TRAIN Batch 17/1000 loss 10.888043 loss_att 13.147156 loss_ctc 13.047289 loss_rnnt 10.059444 hw_loss 0.166645 lr 0.00041846 rank 1
2023-02-23 10:12:03,917 DEBUG TRAIN Batch 17/1000 loss 11.944939 loss_att 13.618991 loss_ctc 18.260248 loss_rnnt 10.654456 hw_loss 0.213059 lr 0.00041840 rank 2
2023-02-23 10:12:03,919 DEBUG TRAIN Batch 17/1000 loss 7.433491 loss_att 10.460783 loss_ctc 9.311134 loss_rnnt 6.483515 hw_loss 0.176559 lr 0.00041837 rank 7
2023-02-23 10:13:21,305 DEBUG TRAIN Batch 17/1100 loss 9.326536 loss_att 13.101154 loss_ctc 12.615193 loss_rnnt 7.997946 hw_loss 0.253461 lr 0.00041832 rank 1
2023-02-23 10:13:21,307 DEBUG TRAIN Batch 17/1100 loss 8.182185 loss_att 11.449896 loss_ctc 9.505445 loss_rnnt 7.257443 hw_loss 0.177683 lr 0.00041822 rank 7
2023-02-23 10:13:21,308 DEBUG TRAIN Batch 17/1100 loss 11.470655 loss_att 16.171253 loss_ctc 13.160719 loss_rnnt 10.137852 hw_loss 0.313769 lr 0.00041825 rank 2
2023-02-23 10:13:21,309 DEBUG TRAIN Batch 17/1100 loss 13.749562 loss_att 16.481495 loss_ctc 17.358917 loss_rnnt 12.616264 hw_loss 0.198121 lr 0.00041830 rank 0
2023-02-23 10:13:21,310 DEBUG TRAIN Batch 17/1100 loss 12.691433 loss_att 15.012064 loss_ctc 12.726708 loss_rnnt 12.121542 hw_loss 0.189490 lr 0.00041829 rank 4
2023-02-23 10:13:21,311 DEBUG TRAIN Batch 17/1100 loss 11.295196 loss_att 14.161507 loss_ctc 13.254742 loss_rnnt 10.337780 hw_loss 0.230400 lr 0.00041822 rank 3
2023-02-23 10:13:21,313 DEBUG TRAIN Batch 17/1100 loss 10.019588 loss_att 14.038237 loss_ctc 12.792934 loss_rnnt 8.710297 hw_loss 0.254594 lr 0.00041820 rank 5
2023-02-23 10:13:21,360 DEBUG TRAIN Batch 17/1100 loss 6.338970 loss_att 8.997200 loss_ctc 8.760345 loss_rnnt 5.355649 hw_loss 0.241547 lr 0.00041829 rank 6
2023-02-23 10:14:37,173 DEBUG TRAIN Batch 17/1200 loss 21.434023 loss_att 26.285641 loss_ctc 32.370617 loss_rnnt 18.849615 hw_loss 0.292258 lr 0.00041807 rank 3
2023-02-23 10:14:37,176 DEBUG TRAIN Batch 17/1200 loss 11.044325 loss_att 11.835153 loss_ctc 17.337461 loss_rnnt 9.907839 hw_loss 0.261066 lr 0.00041811 rank 2
2023-02-23 10:14:37,176 DEBUG TRAIN Batch 17/1200 loss 6.626534 loss_att 8.581375 loss_ctc 8.739527 loss_rnnt 5.799667 hw_loss 0.289061 lr 0.00041816 rank 0
2023-02-23 10:14:37,177 DEBUG TRAIN Batch 17/1200 loss 16.375101 loss_att 18.876440 loss_ctc 24.949768 loss_rnnt 14.598824 hw_loss 0.248850 lr 0.00041806 rank 5
2023-02-23 10:14:37,178 DEBUG TRAIN Batch 17/1200 loss 4.091206 loss_att 6.273109 loss_ctc 7.204399 loss_rnnt 3.097036 hw_loss 0.267557 lr 0.00041808 rank 7
2023-02-23 10:14:37,178 DEBUG TRAIN Batch 17/1200 loss 3.984026 loss_att 5.850494 loss_ctc 6.712343 loss_rnnt 3.131164 hw_loss 0.217113 lr 0.00041814 rank 4
2023-02-23 10:14:37,181 DEBUG TRAIN Batch 17/1200 loss 8.460999 loss_att 10.487529 loss_ctc 10.569251 loss_rnnt 7.634696 hw_loss 0.262308 lr 0.00041817 rank 1
2023-02-23 10:14:37,183 DEBUG TRAIN Batch 17/1200 loss 15.962687 loss_att 16.988628 loss_ctc 20.524237 loss_rnnt 15.017633 hw_loss 0.246862 lr 0.00041814 rank 6
2023-02-23 10:15:52,320 DEBUG TRAIN Batch 17/1300 loss 19.571373 loss_att 23.247343 loss_ctc 21.852545 loss_rnnt 18.379141 hw_loss 0.286655 lr 0.00041796 rank 2
2023-02-23 10:15:52,324 DEBUG TRAIN Batch 17/1300 loss 9.818061 loss_att 10.809982 loss_ctc 12.913110 loss_rnnt 9.039373 hw_loss 0.314307 lr 0.00041791 rank 5
2023-02-23 10:15:52,326 DEBUG TRAIN Batch 17/1300 loss 11.804742 loss_att 12.119068 loss_ctc 15.659969 loss_rnnt 11.101501 hw_loss 0.236896 lr 0.00041792 rank 3
2023-02-23 10:15:52,328 DEBUG TRAIN Batch 17/1300 loss 8.824544 loss_att 11.712933 loss_ctc 11.406357 loss_rnnt 7.778154 hw_loss 0.233384 lr 0.00041801 rank 0
2023-02-23 10:15:52,330 DEBUG TRAIN Batch 17/1300 loss 19.535969 loss_att 25.301868 loss_ctc 30.236008 loss_rnnt 16.893957 hw_loss 0.116547 lr 0.00041800 rank 4
2023-02-23 10:15:52,330 DEBUG TRAIN Batch 17/1300 loss 9.237091 loss_att 10.603010 loss_ctc 9.723483 loss_rnnt 8.777363 hw_loss 0.228174 lr 0.00041802 rank 1
2023-02-23 10:15:52,331 DEBUG TRAIN Batch 17/1300 loss 10.877723 loss_att 13.759552 loss_ctc 11.217613 loss_rnnt 10.210756 hw_loss 0.084904 lr 0.00041799 rank 6
2023-02-23 10:15:52,332 DEBUG TRAIN Batch 17/1300 loss 10.529513 loss_att 15.035686 loss_ctc 16.682219 loss_rnnt 8.724550 hw_loss 0.156315 lr 0.00041793 rank 7
2023-02-23 10:17:09,204 DEBUG TRAIN Batch 17/1400 loss 19.858244 loss_att 23.634041 loss_ctc 26.064022 loss_rnnt 18.222130 hw_loss 0.100344 lr 0.00041777 rank 5
2023-02-23 10:17:09,209 DEBUG TRAIN Batch 17/1400 loss 4.836564 loss_att 6.635659 loss_ctc 6.082579 loss_rnnt 4.134043 hw_loss 0.331063 lr 0.00041782 rank 2
2023-02-23 10:17:09,210 DEBUG TRAIN Batch 17/1400 loss 9.403529 loss_att 12.330241 loss_ctc 10.527679 loss_rnnt 8.553003 hw_loss 0.216182 lr 0.00041785 rank 4
2023-02-23 10:17:09,213 DEBUG TRAIN Batch 17/1400 loss 14.810714 loss_att 17.707886 loss_ctc 18.169029 loss_rnnt 13.602335 hw_loss 0.339688 lr 0.00041785 rank 6
2023-02-23 10:17:09,213 DEBUG TRAIN Batch 17/1400 loss 3.017197 loss_att 6.295722 loss_ctc 3.578946 loss_rnnt 2.111394 hw_loss 0.328497 lr 0.00041778 rank 3
2023-02-23 10:17:09,218 DEBUG TRAIN Batch 17/1400 loss 15.703380 loss_att 16.999550 loss_ctc 20.232176 loss_rnnt 14.715926 hw_loss 0.233214 lr 0.00041779 rank 7
2023-02-23 10:17:09,219 DEBUG TRAIN Batch 17/1400 loss 5.497794 loss_att 10.773920 loss_ctc 7.900222 loss_rnnt 3.958434 hw_loss 0.307146 lr 0.00041788 rank 1
2023-02-23 10:17:09,234 DEBUG TRAIN Batch 17/1400 loss 5.133386 loss_att 8.781162 loss_ctc 6.726696 loss_rnnt 3.993388 hw_loss 0.371251 lr 0.00041787 rank 0
2023-02-23 10:18:26,122 DEBUG TRAIN Batch 17/1500 loss 14.237467 loss_att 15.529181 loss_ctc 19.695206 loss_rnnt 13.153311 hw_loss 0.183965 lr 0.00041762 rank 5
2023-02-23 10:18:26,125 DEBUG TRAIN Batch 17/1500 loss 9.984846 loss_att 14.014310 loss_ctc 16.249760 loss_rnnt 8.227553 hw_loss 0.217646 lr 0.00041771 rank 4
2023-02-23 10:18:26,126 DEBUG TRAIN Batch 17/1500 loss 1.962139 loss_att 4.388196 loss_ctc 2.036083 loss_rnnt 1.375320 hw_loss 0.172028 lr 0.00041767 rank 2
2023-02-23 10:18:26,126 DEBUG TRAIN Batch 17/1500 loss 11.353590 loss_att 13.550461 loss_ctc 17.270151 loss_rnnt 10.027414 hw_loss 0.183614 lr 0.00041763 rank 3
2023-02-23 10:18:26,132 DEBUG TRAIN Batch 17/1500 loss 7.222147 loss_att 8.211939 loss_ctc 9.267431 loss_rnnt 6.654532 hw_loss 0.181784 lr 0.00041772 rank 0
2023-02-23 10:18:26,132 DEBUG TRAIN Batch 17/1500 loss 17.506626 loss_att 19.319761 loss_ctc 21.531158 loss_rnnt 16.471218 hw_loss 0.255330 lr 0.00041770 rank 6
2023-02-23 10:18:26,133 DEBUG TRAIN Batch 17/1500 loss 14.622951 loss_att 15.600569 loss_ctc 23.121267 loss_rnnt 13.166506 hw_loss 0.239647 lr 0.00041764 rank 7
2023-02-23 10:18:26,134 DEBUG TRAIN Batch 17/1500 loss 14.594478 loss_att 13.800087 loss_ctc 18.289415 loss_rnnt 14.143402 hw_loss 0.219929 lr 0.00041773 rank 1
2023-02-23 10:19:41,550 DEBUG TRAIN Batch 17/1600 loss 11.484521 loss_att 16.458363 loss_ctc 15.134970 loss_rnnt 9.897421 hw_loss 0.198009 lr 0.00041756 rank 4
2023-02-23 10:19:41,551 DEBUG TRAIN Batch 17/1600 loss 15.344885 loss_att 16.114468 loss_ctc 14.504072 loss_rnnt 15.174383 hw_loss 0.241300 lr 0.00041749 rank 3
2023-02-23 10:19:41,552 DEBUG TRAIN Batch 17/1600 loss 7.714954 loss_att 10.665677 loss_ctc 10.252164 loss_rnnt 6.685213 hw_loss 0.189941 lr 0.00041757 rank 0
2023-02-23 10:19:41,555 DEBUG TRAIN Batch 17/1600 loss 7.093824 loss_att 11.343830 loss_ctc 8.847535 loss_rnnt 5.859757 hw_loss 0.281693 lr 0.00041752 rank 2
2023-02-23 10:19:41,556 DEBUG TRAIN Batch 17/1600 loss 17.092209 loss_att 19.757650 loss_ctc 23.175026 loss_rnnt 15.624448 hw_loss 0.231803 lr 0.00041747 rank 5
2023-02-23 10:19:41,557 DEBUG TRAIN Batch 17/1600 loss 8.367723 loss_att 10.249563 loss_ctc 9.396915 loss_rnnt 7.704003 hw_loss 0.281486 lr 0.00041759 rank 1
2023-02-23 10:19:41,561 DEBUG TRAIN Batch 17/1600 loss 16.471807 loss_att 17.304625 loss_ctc 19.364799 loss_rnnt 15.788518 hw_loss 0.245612 lr 0.00041756 rank 6
2023-02-23 10:19:41,561 DEBUG TRAIN Batch 17/1600 loss 7.082551 loss_att 8.206902 loss_ctc 8.994139 loss_rnnt 6.446641 hw_loss 0.292804 lr 0.00041749 rank 7
2023-02-23 10:20:58,061 DEBUG TRAIN Batch 17/1700 loss 15.267480 loss_att 17.473885 loss_ctc 21.003365 loss_rnnt 13.949914 hw_loss 0.209062 lr 0.00041742 rank 4
2023-02-23 10:20:58,064 DEBUG TRAIN Batch 17/1700 loss 17.412832 loss_att 19.026514 loss_ctc 21.660709 loss_rnnt 16.433798 hw_loss 0.168591 lr 0.00041738 rank 2
2023-02-23 10:20:58,067 DEBUG TRAIN Batch 17/1700 loss 9.096975 loss_att 12.250593 loss_ctc 9.265753 loss_rnnt 8.324903 hw_loss 0.222834 lr 0.00041734 rank 3
2023-02-23 10:20:58,069 DEBUG TRAIN Batch 17/1700 loss 6.680327 loss_att 12.572492 loss_ctc 7.825445 loss_rnnt 5.217984 hw_loss 0.246052 lr 0.00041743 rank 0
2023-02-23 10:20:58,071 DEBUG TRAIN Batch 17/1700 loss 12.371653 loss_att 14.027727 loss_ctc 14.956943 loss_rnnt 11.536209 hw_loss 0.299106 lr 0.00041735 rank 7
2023-02-23 10:20:58,071 DEBUG TRAIN Batch 17/1700 loss 9.829401 loss_att 14.914985 loss_ctc 13.436170 loss_rnnt 8.244452 hw_loss 0.162995 lr 0.00041741 rank 6
2023-02-23 10:20:58,072 DEBUG TRAIN Batch 17/1700 loss 10.956100 loss_att 16.754448 loss_ctc 16.012844 loss_rnnt 8.991068 hw_loss 0.245867 lr 0.00041744 rank 1
2023-02-23 10:20:58,073 DEBUG TRAIN Batch 17/1700 loss 9.218309 loss_att 10.948030 loss_ctc 12.454659 loss_rnnt 8.289510 hw_loss 0.283767 lr 0.00041733 rank 5
2023-02-23 10:22:16,319 DEBUG TRAIN Batch 17/1800 loss 12.501010 loss_att 15.164967 loss_ctc 16.019615 loss_rnnt 11.428330 hw_loss 0.132640 lr 0.00041723 rank 2
2023-02-23 10:22:16,320 DEBUG TRAIN Batch 17/1800 loss 18.720062 loss_att 20.033329 loss_ctc 20.300739 loss_rnnt 18.122272 hw_loss 0.233209 lr 0.00041718 rank 5
2023-02-23 10:22:16,324 DEBUG TRAIN Batch 17/1800 loss 10.707321 loss_att 12.965671 loss_ctc 12.397692 loss_rnnt 9.918535 hw_loss 0.209497 lr 0.00041720 rank 7
2023-02-23 10:22:16,325 DEBUG TRAIN Batch 17/1800 loss 11.533822 loss_att 15.620766 loss_ctc 14.229469 loss_rnnt 10.262360 hw_loss 0.177476 lr 0.00041727 rank 4
2023-02-23 10:22:16,326 DEBUG TRAIN Batch 17/1800 loss 10.059771 loss_att 11.688980 loss_ctc 13.088065 loss_rnnt 9.206084 hw_loss 0.232634 lr 0.00041728 rank 0
2023-02-23 10:22:16,328 DEBUG TRAIN Batch 17/1800 loss 6.682914 loss_att 9.536943 loss_ctc 9.154971 loss_rnnt 5.684803 hw_loss 0.183184 lr 0.00041727 rank 6
2023-02-23 10:22:16,332 DEBUG TRAIN Batch 17/1800 loss 8.638847 loss_att 11.144754 loss_ctc 9.604122 loss_rnnt 7.862737 hw_loss 0.274175 lr 0.00041729 rank 1
2023-02-23 10:22:16,369 DEBUG TRAIN Batch 17/1800 loss 12.116604 loss_att 15.324120 loss_ctc 16.379585 loss_rnnt 10.719824 hw_loss 0.350399 lr 0.00041720 rank 3
2023-02-23 10:23:33,778 DEBUG TRAIN Batch 17/1900 loss 12.520308 loss_att 13.320383 loss_ctc 14.404629 loss_rnnt 11.938025 hw_loss 0.320676 lr 0.00041704 rank 5
2023-02-23 10:23:33,783 DEBUG TRAIN Batch 17/1900 loss 10.370879 loss_att 13.828852 loss_ctc 14.777998 loss_rnnt 8.929903 hw_loss 0.303309 lr 0.00041715 rank 1
2023-02-23 10:23:33,784 DEBUG TRAIN Batch 17/1900 loss 13.269428 loss_att 11.707122 loss_ctc 15.108137 loss_rnnt 13.124269 hw_loss 0.398362 lr 0.00041714 rank 0
2023-02-23 10:23:33,786 DEBUG TRAIN Batch 17/1900 loss 9.134774 loss_att 10.688330 loss_ctc 11.809966 loss_rnnt 8.264332 hw_loss 0.380697 lr 0.00041712 rank 4
2023-02-23 10:23:33,786 DEBUG TRAIN Batch 17/1900 loss 11.283002 loss_att 13.707520 loss_ctc 13.463110 loss_rnnt 10.473421 hw_loss 0.063741 lr 0.00041709 rank 2
2023-02-23 10:23:33,790 DEBUG TRAIN Batch 17/1900 loss 6.557856 loss_att 9.660097 loss_ctc 8.207274 loss_rnnt 5.579407 hw_loss 0.258896 lr 0.00041712 rank 6
2023-02-23 10:23:33,791 DEBUG TRAIN Batch 17/1900 loss 8.780010 loss_att 10.824589 loss_ctc 9.848780 loss_rnnt 8.102622 hw_loss 0.236194 lr 0.00041705 rank 3
2023-02-23 10:23:33,793 DEBUG TRAIN Batch 17/1900 loss 9.713833 loss_att 9.312965 loss_ctc 12.984610 loss_rnnt 9.214638 hw_loss 0.268622 lr 0.00041706 rank 7
2023-02-23 10:24:48,735 DEBUG TRAIN Batch 17/2000 loss 5.368135 loss_att 8.997321 loss_ctc 6.760505 loss_rnnt 4.330251 hw_loss 0.236997 lr 0.00041694 rank 2
2023-02-23 10:24:48,736 DEBUG TRAIN Batch 17/2000 loss 11.161097 loss_att 12.041484 loss_ctc 14.418074 loss_rnnt 10.427984 hw_loss 0.230196 lr 0.00041699 rank 0
2023-02-23 10:24:48,736 DEBUG TRAIN Batch 17/2000 loss 10.330339 loss_att 12.997176 loss_ctc 13.617412 loss_rnnt 9.270403 hw_loss 0.165547 lr 0.00041689 rank 5
2023-02-23 10:24:48,738 DEBUG TRAIN Batch 17/2000 loss 5.030624 loss_att 8.774527 loss_ctc 6.435533 loss_rnnt 3.935553 hw_loss 0.298067 lr 0.00041698 rank 4
2023-02-23 10:24:48,739 DEBUG TRAIN Batch 17/2000 loss 19.343227 loss_att 24.776825 loss_ctc 29.007458 loss_rnnt 16.826870 hw_loss 0.264509 lr 0.00041691 rank 7
2023-02-23 10:24:48,740 DEBUG TRAIN Batch 17/2000 loss 12.948027 loss_att 13.197006 loss_ctc 18.202644 loss_rnnt 12.072780 hw_loss 0.234066 lr 0.00041698 rank 6
2023-02-23 10:24:48,741 DEBUG TRAIN Batch 17/2000 loss 11.582104 loss_att 14.154306 loss_ctc 16.514746 loss_rnnt 10.253323 hw_loss 0.293728 lr 0.00041700 rank 1
2023-02-23 10:24:48,783 DEBUG TRAIN Batch 17/2000 loss 12.264306 loss_att 15.974419 loss_ctc 15.800032 loss_rnnt 10.928409 hw_loss 0.229584 lr 0.00041691 rank 3
2023-02-23 10:26:06,621 DEBUG TRAIN Batch 17/2100 loss 5.448985 loss_att 8.790102 loss_ctc 7.232999 loss_rnnt 4.398216 hw_loss 0.271269 lr 0.00041683 rank 4
2023-02-23 10:26:06,624 DEBUG TRAIN Batch 17/2100 loss 6.598245 loss_att 10.047826 loss_ctc 10.025283 loss_rnnt 5.323388 hw_loss 0.240005 lr 0.00041676 rank 3
2023-02-23 10:26:06,626 DEBUG TRAIN Batch 17/2100 loss 9.693677 loss_att 14.569812 loss_ctc 12.032858 loss_rnnt 8.310041 hw_loss 0.180972 lr 0.00041680 rank 2
2023-02-23 10:26:06,626 DEBUG TRAIN Batch 17/2100 loss 8.660254 loss_att 10.540493 loss_ctc 12.131899 loss_rnnt 7.627358 hw_loss 0.363677 lr 0.00041675 rank 5
2023-02-23 10:26:06,627 DEBUG TRAIN Batch 17/2100 loss 11.203969 loss_att 16.058292 loss_ctc 13.793776 loss_rnnt 9.676980 hw_loss 0.395281 lr 0.00041685 rank 0
2023-02-23 10:26:06,631 DEBUG TRAIN Batch 17/2100 loss 12.213095 loss_att 17.300037 loss_ctc 18.112965 loss_rnnt 10.270868 hw_loss 0.259101 lr 0.00041677 rank 7
2023-02-23 10:26:06,657 DEBUG TRAIN Batch 17/2100 loss 22.534395 loss_att 24.914532 loss_ctc 23.094215 loss_rnnt 21.812141 hw_loss 0.321719 lr 0.00041686 rank 1
2023-02-23 10:26:06,666 DEBUG TRAIN Batch 17/2100 loss 11.683369 loss_att 16.042484 loss_ctc 15.045444 loss_rnnt 10.276281 hw_loss 0.163099 lr 0.00041683 rank 6
2023-02-23 10:27:24,525 DEBUG TRAIN Batch 17/2200 loss 4.422754 loss_att 6.339396 loss_ctc 5.643126 loss_rnnt 3.804480 hw_loss 0.135431 lr 0.00041669 rank 4
2023-02-23 10:27:24,525 DEBUG TRAIN Batch 17/2200 loss 8.197460 loss_att 11.154848 loss_ctc 11.869167 loss_rnnt 6.989818 hw_loss 0.237385 lr 0.00041660 rank 5
2023-02-23 10:27:24,530 DEBUG TRAIN Batch 17/2200 loss 23.812492 loss_att 28.477268 loss_ctc 35.524422 loss_rnnt 21.194828 hw_loss 0.230844 lr 0.00041669 rank 6
2023-02-23 10:27:24,529 DEBUG TRAIN Batch 17/2200 loss 6.736978 loss_att 8.450698 loss_ctc 9.688178 loss_rnnt 5.866096 hw_loss 0.252459 lr 0.00041670 rank 0
2023-02-23 10:27:24,531 DEBUG TRAIN Batch 17/2200 loss 10.201289 loss_att 12.611369 loss_ctc 13.096746 loss_rnnt 9.215788 hw_loss 0.220170 lr 0.00041665 rank 2
2023-02-23 10:27:24,534 DEBUG TRAIN Batch 17/2200 loss 8.178576 loss_att 12.032512 loss_ctc 14.105516 loss_rnnt 6.474181 hw_loss 0.268780 lr 0.00041671 rank 1
2023-02-23 10:27:24,535 DEBUG TRAIN Batch 17/2200 loss 9.159983 loss_att 10.122887 loss_ctc 9.459772 loss_rnnt 8.852427 hw_loss 0.140630 lr 0.00041662 rank 3
2023-02-23 10:27:24,586 DEBUG TRAIN Batch 17/2200 loss 6.647655 loss_att 6.836419 loss_ctc 6.651710 loss_rnnt 6.488756 hw_loss 0.226135 lr 0.00041662 rank 7
2023-02-23 10:28:38,467 DEBUG TRAIN Batch 17/2300 loss 9.743253 loss_att 14.172636 loss_ctc 10.290178 loss_rnnt 8.624285 hw_loss 0.300315 lr 0.00041651 rank 2
2023-02-23 10:28:38,469 DEBUG TRAIN Batch 17/2300 loss 6.352134 loss_att 11.134085 loss_ctc 11.279505 loss_rnnt 4.631783 hw_loss 0.200585 lr 0.00041657 rank 1
2023-02-23 10:28:38,469 DEBUG TRAIN Batch 17/2300 loss 7.901389 loss_att 12.783050 loss_ctc 14.706185 loss_rnnt 5.887329 hw_loss 0.244538 lr 0.00041655 rank 4
2023-02-23 10:28:38,472 DEBUG TRAIN Batch 17/2300 loss 7.973119 loss_att 9.654263 loss_ctc 10.956070 loss_rnnt 7.139850 hw_loss 0.186214 lr 0.00041648 rank 7
2023-02-23 10:28:38,473 DEBUG TRAIN Batch 17/2300 loss 6.502105 loss_att 12.443130 loss_ctc 9.768835 loss_rnnt 4.691021 hw_loss 0.351215 lr 0.00041656 rank 0
2023-02-23 10:28:38,474 DEBUG TRAIN Batch 17/2300 loss 8.221611 loss_att 11.177825 loss_ctc 12.281530 loss_rnnt 6.966003 hw_loss 0.230702 lr 0.00041646 rank 5
2023-02-23 10:28:38,476 DEBUG TRAIN Batch 17/2300 loss 7.259931 loss_att 11.136685 loss_ctc 10.626486 loss_rnnt 5.877317 hw_loss 0.296976 lr 0.00041647 rank 3
2023-02-23 10:28:38,482 DEBUG TRAIN Batch 17/2300 loss 11.182900 loss_att 13.442337 loss_ctc 17.146366 loss_rnnt 9.786831 hw_loss 0.279475 lr 0.00041654 rank 6
2023-02-23 10:29:53,825 DEBUG TRAIN Batch 17/2400 loss 11.819746 loss_att 17.391880 loss_ctc 18.061075 loss_rnnt 9.771960 hw_loss 0.189717 lr 0.00041640 rank 4
2023-02-23 10:29:53,828 DEBUG TRAIN Batch 17/2400 loss 9.084769 loss_att 10.879974 loss_ctc 13.210815 loss_rnnt 8.072408 hw_loss 0.193462 lr 0.00041640 rank 6
2023-02-23 10:29:53,828 DEBUG TRAIN Batch 17/2400 loss 8.609817 loss_att 12.008651 loss_ctc 13.470287 loss_rnnt 7.143124 hw_loss 0.260369 lr 0.00041636 rank 2
2023-02-23 10:29:53,830 DEBUG TRAIN Batch 17/2400 loss 6.382865 loss_att 9.652109 loss_ctc 8.795027 loss_rnnt 5.232695 hw_loss 0.327563 lr 0.00041632 rank 5
2023-02-23 10:29:53,829 DEBUG TRAIN Batch 17/2400 loss 18.201204 loss_att 20.245766 loss_ctc 24.896685 loss_rnnt 16.745771 hw_loss 0.288360 lr 0.00041641 rank 0
2023-02-23 10:29:53,832 DEBUG TRAIN Batch 17/2400 loss 5.928295 loss_att 9.345421 loss_ctc 9.040617 loss_rnnt 4.679156 hw_loss 0.282633 lr 0.00041633 rank 3
2023-02-23 10:29:53,834 DEBUG TRAIN Batch 17/2400 loss 12.916511 loss_att 15.062738 loss_ctc 16.581142 loss_rnnt 11.921059 hw_loss 0.145479 lr 0.00041633 rank 7
2023-02-23 10:29:53,836 DEBUG TRAIN Batch 17/2400 loss 14.573747 loss_att 12.357757 loss_ctc 19.902973 loss_rnnt 14.147495 hw_loss 0.297912 lr 0.00041643 rank 1
2023-02-23 10:31:13,320 DEBUG TRAIN Batch 17/2500 loss 10.282206 loss_att 11.680824 loss_ctc 13.000954 loss_rnnt 9.514632 hw_loss 0.235031 lr 0.00041617 rank 5
2023-02-23 10:31:13,323 DEBUG TRAIN Batch 17/2500 loss 8.329559 loss_att 7.804316 loss_ctc 7.677396 loss_rnnt 8.414463 hw_loss 0.200813 lr 0.00041619 rank 7
2023-02-23 10:31:13,324 DEBUG TRAIN Batch 17/2500 loss 6.482047 loss_att 8.753967 loss_ctc 7.970810 loss_rnnt 5.714137 hw_loss 0.215671 lr 0.00041626 rank 4
2023-02-23 10:31:13,326 DEBUG TRAIN Batch 17/2500 loss 11.337521 loss_att 12.975887 loss_ctc 15.687051 loss_rnnt 10.249356 hw_loss 0.338539 lr 0.00041618 rank 3
2023-02-23 10:31:13,326 DEBUG TRAIN Batch 17/2500 loss 11.484220 loss_att 12.036900 loss_ctc 16.066919 loss_rnnt 10.627057 hw_loss 0.254247 lr 0.00041627 rank 0
2023-02-23 10:31:13,332 DEBUG TRAIN Batch 17/2500 loss 11.524728 loss_att 14.540009 loss_ctc 14.382471 loss_rnnt 10.416764 hw_loss 0.232264 lr 0.00041628 rank 1
2023-02-23 10:31:13,331 DEBUG TRAIN Batch 17/2500 loss 9.511478 loss_att 10.016620 loss_ctc 12.885459 loss_rnnt 8.791251 hw_loss 0.317502 lr 0.00041622 rank 2
2023-02-23 10:31:13,335 DEBUG TRAIN Batch 17/2500 loss 10.156773 loss_att 10.176357 loss_ctc 14.101485 loss_rnnt 9.475318 hw_loss 0.284205 lr 0.00041625 rank 6
2023-02-23 10:32:29,762 DEBUG TRAIN Batch 17/2600 loss 6.778048 loss_att 10.033410 loss_ctc 7.040813 loss_rnnt 5.951683 hw_loss 0.262983 lr 0.00041604 rank 3
2023-02-23 10:32:29,766 DEBUG TRAIN Batch 17/2600 loss 11.493903 loss_att 13.596974 loss_ctc 13.931896 loss_rnnt 10.646278 hw_loss 0.191146 lr 0.00041603 rank 5
2023-02-23 10:32:29,766 DEBUG TRAIN Batch 17/2600 loss 10.109178 loss_att 16.101299 loss_ctc 14.444556 loss_rnnt 8.177899 hw_loss 0.290255 lr 0.00041613 rank 0
2023-02-23 10:32:29,766 DEBUG TRAIN Batch 17/2600 loss 5.723887 loss_att 6.754690 loss_ctc 7.146589 loss_rnnt 5.185146 hw_loss 0.267913 lr 0.00041608 rank 2
2023-02-23 10:32:29,769 DEBUG TRAIN Batch 17/2600 loss 9.763680 loss_att 10.909043 loss_ctc 12.947265 loss_rnnt 8.920845 hw_loss 0.354908 lr 0.00041605 rank 7
2023-02-23 10:32:29,771 DEBUG TRAIN Batch 17/2600 loss 13.441565 loss_att 15.746984 loss_ctc 23.403660 loss_rnnt 11.519424 hw_loss 0.248956 lr 0.00041611 rank 4
2023-02-23 10:32:29,772 DEBUG TRAIN Batch 17/2600 loss 6.576083 loss_att 8.678905 loss_ctc 7.729368 loss_rnnt 5.889972 hw_loss 0.209578 lr 0.00041611 rank 6
2023-02-23 10:32:29,776 DEBUG TRAIN Batch 17/2600 loss 19.526953 loss_att 21.211021 loss_ctc 29.710415 loss_rnnt 17.732689 hw_loss 0.186853 lr 0.00041614 rank 1
2023-02-23 10:33:44,763 DEBUG TRAIN Batch 17/2700 loss 4.289271 loss_att 6.842995 loss_ctc 6.208742 loss_rnnt 3.445037 hw_loss 0.145424 lr 0.00041598 rank 0
2023-02-23 10:33:44,768 DEBUG TRAIN Batch 17/2700 loss 13.467497 loss_att 12.374990 loss_ctc 19.172306 loss_rnnt 12.830206 hw_loss 0.178410 lr 0.00041588 rank 5
2023-02-23 10:33:44,768 DEBUG TRAIN Batch 17/2700 loss 9.439691 loss_att 12.063154 loss_ctc 13.565140 loss_rnnt 8.217382 hw_loss 0.276665 lr 0.00041593 rank 2
2023-02-23 10:33:44,769 DEBUG TRAIN Batch 17/2700 loss 4.938704 loss_att 7.898727 loss_ctc 7.902083 loss_rnnt 3.839457 hw_loss 0.210233 lr 0.00041597 rank 4
2023-02-23 10:33:44,770 DEBUG TRAIN Batch 17/2700 loss 3.067168 loss_att 4.846608 loss_ctc 3.112255 loss_rnnt 2.587963 hw_loss 0.219949 lr 0.00041599 rank 1
2023-02-23 10:33:44,770 DEBUG TRAIN Batch 17/2700 loss 9.415130 loss_att 13.595505 loss_ctc 11.923388 loss_rnnt 8.116176 hw_loss 0.240833 lr 0.00041589 rank 3
2023-02-23 10:33:44,771 DEBUG TRAIN Batch 17/2700 loss 5.456284 loss_att 10.441957 loss_ctc 8.018065 loss_rnnt 3.982228 hw_loss 0.253783 lr 0.00041596 rank 6
2023-02-23 10:33:44,772 DEBUG TRAIN Batch 17/2700 loss 8.424481 loss_att 10.984694 loss_ctc 11.038740 loss_rnnt 7.482662 hw_loss 0.152266 lr 0.00041590 rank 7
2023-02-23 10:35:01,699 DEBUG TRAIN Batch 17/2800 loss 8.543909 loss_att 11.330426 loss_ctc 16.266138 loss_rnnt 6.821685 hw_loss 0.253668 lr 0.00041579 rank 2
2023-02-23 10:35:01,702 DEBUG TRAIN Batch 17/2800 loss 12.765662 loss_att 16.349823 loss_ctc 17.029839 loss_rnnt 11.339152 hw_loss 0.264603 lr 0.00041582 rank 4
2023-02-23 10:35:01,705 DEBUG TRAIN Batch 17/2800 loss 16.637047 loss_att 17.057775 loss_ctc 20.364979 loss_rnnt 15.981079 hw_loss 0.140179 lr 0.00041574 rank 5
2023-02-23 10:35:01,708 DEBUG TRAIN Batch 17/2800 loss 10.371461 loss_att 14.112024 loss_ctc 11.212169 loss_rnnt 9.350939 hw_loss 0.300590 lr 0.00041576 rank 7
2023-02-23 10:35:01,709 DEBUG TRAIN Batch 17/2800 loss 6.913315 loss_att 7.832152 loss_ctc 8.841061 loss_rnnt 6.368518 hw_loss 0.194994 lr 0.00041575 rank 3
2023-02-23 10:35:01,710 DEBUG TRAIN Batch 17/2800 loss 6.236948 loss_att 9.575461 loss_ctc 8.558921 loss_rnnt 5.156744 hw_loss 0.192947 lr 0.00041584 rank 0
2023-02-23 10:35:01,711 DEBUG TRAIN Batch 17/2800 loss 16.378561 loss_att 19.287943 loss_ctc 22.151646 loss_rnnt 14.879293 hw_loss 0.276839 lr 0.00041585 rank 1
2023-02-23 10:35:01,711 DEBUG TRAIN Batch 17/2800 loss 7.881550 loss_att 9.465660 loss_ctc 10.168925 loss_rnnt 7.134265 hw_loss 0.235275 lr 0.00041582 rank 6
2023-02-23 10:36:18,741 DEBUG TRAIN Batch 17/2900 loss 10.481376 loss_att 14.983588 loss_ctc 14.655352 loss_rnnt 8.931346 hw_loss 0.174482 lr 0.00041568 rank 6
2023-02-23 10:36:18,742 DEBUG TRAIN Batch 17/2900 loss 9.766753 loss_att 12.685813 loss_ctc 14.535254 loss_rnnt 8.410109 hw_loss 0.256937 lr 0.00041568 rank 4
2023-02-23 10:36:18,743 DEBUG TRAIN Batch 17/2900 loss 13.436397 loss_att 14.808258 loss_ctc 17.046679 loss_rnnt 12.560429 hw_loss 0.225420 lr 0.00041564 rank 2
2023-02-23 10:36:18,745 DEBUG TRAIN Batch 17/2900 loss 8.665051 loss_att 9.820541 loss_ctc 9.022719 loss_rnnt 8.279682 hw_loss 0.199843 lr 0.00041561 rank 3
2023-02-23 10:36:18,746 DEBUG TRAIN Batch 17/2900 loss 15.891524 loss_att 18.753122 loss_ctc 20.595886 loss_rnnt 14.593246 hw_loss 0.185084 lr 0.00041569 rank 0
2023-02-23 10:36:18,747 DEBUG TRAIN Batch 17/2900 loss 11.003390 loss_att 17.358011 loss_ctc 18.986868 loss_rnnt 8.577454 hw_loss 0.169779 lr 0.00041560 rank 5
2023-02-23 10:36:18,748 DEBUG TRAIN Batch 17/2900 loss 4.128486 loss_att 7.075586 loss_ctc 6.785774 loss_rnnt 3.056792 hw_loss 0.239941 lr 0.00041571 rank 1
2023-02-23 10:36:18,753 DEBUG TRAIN Batch 17/2900 loss 18.766951 loss_att 21.262533 loss_ctc 27.430450 loss_rnnt 17.001797 hw_loss 0.207942 lr 0.00041561 rank 7
2023-02-23 10:37:34,086 DEBUG TRAIN Batch 17/3000 loss 9.973488 loss_att 13.940640 loss_ctc 12.648957 loss_rnnt 8.678295 hw_loss 0.271936 lr 0.00041546 rank 3
2023-02-23 10:37:34,086 DEBUG TRAIN Batch 17/3000 loss 14.491467 loss_att 18.780861 loss_ctc 19.198196 loss_rnnt 12.882667 hw_loss 0.231297 lr 0.00041550 rank 2
2023-02-23 10:37:34,087 DEBUG TRAIN Batch 17/3000 loss 9.818854 loss_att 12.769119 loss_ctc 12.010025 loss_rnnt 8.820374 hw_loss 0.218010 lr 0.00041554 rank 4
2023-02-23 10:37:34,088 DEBUG TRAIN Batch 17/3000 loss 4.837919 loss_att 7.253865 loss_ctc 8.939151 loss_rnnt 3.707490 hw_loss 0.188266 lr 0.00041545 rank 5
2023-02-23 10:37:34,090 DEBUG TRAIN Batch 17/3000 loss 8.998344 loss_att 11.643930 loss_ctc 14.217012 loss_rnnt 7.669469 hw_loss 0.194879 lr 0.00041556 rank 1
2023-02-23 10:37:34,091 DEBUG TRAIN Batch 17/3000 loss 8.027481 loss_att 11.297098 loss_ctc 11.724304 loss_rnnt 6.743430 hw_loss 0.257284 lr 0.00041555 rank 0
2023-02-23 10:37:34,093 DEBUG TRAIN Batch 17/3000 loss 12.925872 loss_att 14.165639 loss_ctc 13.458924 loss_rnnt 12.481300 hw_loss 0.235398 lr 0.00041547 rank 7
2023-02-23 10:37:34,139 DEBUG TRAIN Batch 17/3000 loss 7.364805 loss_att 10.728938 loss_ctc 12.190081 loss_rnnt 5.908408 hw_loss 0.262876 lr 0.00041553 rank 6
2023-02-23 10:38:50,773 DEBUG TRAIN Batch 17/3100 loss 4.708889 loss_att 7.706563 loss_ctc 7.095256 loss_rnnt 3.636099 hw_loss 0.290762 lr 0.00041531 rank 5
2023-02-23 10:38:50,774 DEBUG TRAIN Batch 17/3100 loss 10.743313 loss_att 13.298851 loss_ctc 15.512323 loss_rnnt 9.435689 hw_loss 0.301213 lr 0.00041532 rank 3
2023-02-23 10:38:50,775 DEBUG TRAIN Batch 17/3100 loss 11.632595 loss_att 13.697453 loss_ctc 15.316238 loss_rnnt 10.641315 hw_loss 0.163417 lr 0.00041539 rank 4
2023-02-23 10:38:50,776 DEBUG TRAIN Batch 17/3100 loss 11.913513 loss_att 13.103970 loss_ctc 15.480989 loss_rnnt 11.075233 hw_loss 0.233487 lr 0.00041536 rank 2
2023-02-23 10:38:50,780 DEBUG TRAIN Batch 17/3100 loss 16.820717 loss_att 17.589294 loss_ctc 20.888542 loss_rnnt 15.983654 hw_loss 0.264319 lr 0.00041541 rank 0
2023-02-23 10:38:50,784 DEBUG TRAIN Batch 17/3100 loss 5.104049 loss_att 6.294847 loss_ctc 5.266045 loss_rnnt 4.677028 hw_loss 0.313615 lr 0.00041542 rank 1
2023-02-23 10:38:50,788 DEBUG TRAIN Batch 17/3100 loss 7.431732 loss_att 10.131371 loss_ctc 8.392955 loss_rnnt 6.629843 hw_loss 0.250871 lr 0.00041533 rank 7
2023-02-23 10:38:50,825 DEBUG TRAIN Batch 17/3100 loss 11.597817 loss_att 12.323292 loss_ctc 15.710838 loss_rnnt 10.766672 hw_loss 0.258088 lr 0.00041539 rank 6
2023-02-23 10:40:09,276 DEBUG TRAIN Batch 17/3200 loss 12.201621 loss_att 15.024789 loss_ctc 13.732687 loss_rnnt 11.326739 hw_loss 0.198948 lr 0.00041521 rank 2
2023-02-23 10:40:09,279 DEBUG TRAIN Batch 17/3200 loss 6.754345 loss_att 10.036131 loss_ctc 8.277796 loss_rnnt 5.775695 hw_loss 0.223437 lr 0.00041526 rank 0
2023-02-23 10:40:09,281 DEBUG TRAIN Batch 17/3200 loss 6.477489 loss_att 7.013638 loss_ctc 9.161629 loss_rnnt 5.813755 hw_loss 0.372411 lr 0.00041518 rank 7
2023-02-23 10:40:09,282 DEBUG TRAIN Batch 17/3200 loss 15.494522 loss_att 15.983646 loss_ctc 22.765509 loss_rnnt 14.303660 hw_loss 0.231697 lr 0.00041525 rank 6
2023-02-23 10:40:09,286 DEBUG TRAIN Batch 17/3200 loss 12.981691 loss_att 12.688044 loss_ctc 18.424438 loss_rnnt 12.239922 hw_loss 0.140250 lr 0.00041518 rank 3
2023-02-23 10:40:09,298 DEBUG TRAIN Batch 17/3200 loss 3.402630 loss_att 5.642901 loss_ctc 4.361901 loss_rnnt 2.713812 hw_loss 0.211612 lr 0.00041525 rank 4
2023-02-23 10:40:09,312 DEBUG TRAIN Batch 17/3200 loss 12.463275 loss_att 15.904078 loss_ctc 16.202364 loss_rnnt 11.145209 hw_loss 0.246301 lr 0.00041527 rank 1
2023-02-23 10:40:09,346 DEBUG TRAIN Batch 17/3200 loss 7.610761 loss_att 8.818832 loss_ctc 12.604110 loss_rnnt 6.554032 hw_loss 0.280000 lr 0.00041517 rank 5
2023-02-23 10:41:24,351 DEBUG TRAIN Batch 17/3300 loss 13.555387 loss_att 17.532482 loss_ctc 23.815422 loss_rnnt 11.246452 hw_loss 0.272831 lr 0.00041502 rank 5
2023-02-23 10:41:24,352 DEBUG TRAIN Batch 17/3300 loss 23.207846 loss_att 24.252327 loss_ctc 32.823669 loss_rnnt 21.547436 hw_loss 0.317631 lr 0.00041503 rank 3
2023-02-23 10:41:24,354 DEBUG TRAIN Batch 17/3300 loss 4.940039 loss_att 7.310996 loss_ctc 6.085912 loss_rnnt 4.224303 hw_loss 0.166425 lr 0.00041510 rank 6
2023-02-23 10:41:24,354 DEBUG TRAIN Batch 17/3300 loss 6.017768 loss_att 10.670634 loss_ctc 10.524591 loss_rnnt 4.353868 hw_loss 0.248282 lr 0.00041507 rank 2
2023-02-23 10:41:24,354 DEBUG TRAIN Batch 17/3300 loss 9.492492 loss_att 9.472482 loss_ctc 12.307241 loss_rnnt 9.046275 hw_loss 0.140471 lr 0.00041512 rank 0
2023-02-23 10:41:24,357 DEBUG TRAIN Batch 17/3300 loss 17.740021 loss_att 23.166113 loss_ctc 23.577169 loss_rnnt 15.774754 hw_loss 0.190804 lr 0.00041511 rank 4
2023-02-23 10:41:24,361 DEBUG TRAIN Batch 17/3300 loss 9.616415 loss_att 11.113312 loss_ctc 12.704998 loss_rnnt 8.786634 hw_loss 0.222357 lr 0.00041513 rank 1
2023-02-23 10:41:24,361 DEBUG TRAIN Batch 17/3300 loss 14.138257 loss_att 17.272690 loss_ctc 20.772465 loss_rnnt 12.455279 hw_loss 0.321620 lr 0.00041504 rank 7
2023-02-23 10:42:39,290 DEBUG TRAIN Batch 17/3400 loss 13.370507 loss_att 14.903642 loss_ctc 26.683172 loss_rnnt 11.139227 hw_loss 0.280558 lr 0.00041488 rank 5
2023-02-23 10:42:39,290 DEBUG TRAIN Batch 17/3400 loss 4.832301 loss_att 6.616909 loss_ctc 5.591224 loss_rnnt 4.255280 hw_loss 0.222954 lr 0.00041496 rank 4
2023-02-23 10:42:39,293 DEBUG TRAIN Batch 17/3400 loss 8.973833 loss_att 12.390847 loss_ctc 12.354727 loss_rnnt 7.689324 hw_loss 0.281849 lr 0.00041490 rank 7
2023-02-23 10:42:39,295 DEBUG TRAIN Batch 17/3400 loss 8.795771 loss_att 12.032932 loss_ctc 11.384575 loss_rnnt 7.680604 hw_loss 0.229800 lr 0.00041496 rank 6
2023-02-23 10:42:39,294 DEBUG TRAIN Batch 17/3400 loss 12.469306 loss_att 14.325172 loss_ctc 18.380322 loss_rnnt 11.228178 hw_loss 0.153413 lr 0.00041493 rank 2
2023-02-23 10:42:39,295 DEBUG TRAIN Batch 17/3400 loss 7.136413 loss_att 10.671740 loss_ctc 10.737185 loss_rnnt 5.858661 hw_loss 0.169844 lr 0.00041498 rank 0
2023-02-23 10:42:39,299 DEBUG TRAIN Batch 17/3400 loss 5.258900 loss_att 7.473305 loss_ctc 9.918000 loss_rnnt 4.107175 hw_loss 0.164307 lr 0.00041489 rank 3
2023-02-23 10:42:39,300 DEBUG TRAIN Batch 17/3400 loss 18.734936 loss_att 17.629959 loss_ctc 17.353605 loss_rnnt 19.005869 hw_loss 0.251699 lr 0.00041499 rank 1
2023-02-23 10:43:56,840 DEBUG TRAIN Batch 17/3500 loss 10.675262 loss_att 12.283578 loss_ctc 15.671934 loss_rnnt 9.543101 hw_loss 0.270516 lr 0.00041474 rank 5
2023-02-23 10:43:56,842 DEBUG TRAIN Batch 17/3500 loss 4.650794 loss_att 8.333500 loss_ctc 4.981060 loss_rnnt 3.768195 hw_loss 0.191291 lr 0.00041475 rank 3
2023-02-23 10:43:56,843 DEBUG TRAIN Batch 17/3500 loss 18.772329 loss_att 23.436825 loss_ctc 21.344141 loss_rnnt 17.449682 hw_loss 0.087826 lr 0.00041476 rank 7
2023-02-23 10:43:56,843 DEBUG TRAIN Batch 17/3500 loss 8.902775 loss_att 14.650460 loss_ctc 15.054415 loss_rnnt 6.753052 hw_loss 0.337440 lr 0.00041482 rank 4
2023-02-23 10:43:56,857 DEBUG TRAIN Batch 17/3500 loss 9.329808 loss_att 12.959292 loss_ctc 13.712440 loss_rnnt 7.867606 hw_loss 0.284914 lr 0.00041483 rank 0
2023-02-23 10:43:56,862 DEBUG TRAIN Batch 17/3500 loss 9.398861 loss_att 11.790375 loss_ctc 9.063547 loss_rnnt 8.841897 hw_loss 0.231317 lr 0.00041479 rank 2
2023-02-23 10:43:56,872 DEBUG TRAIN Batch 17/3500 loss 10.504640 loss_att 14.006095 loss_ctc 13.601986 loss_rnnt 9.300526 hw_loss 0.170330 lr 0.00041485 rank 1
2023-02-23 10:43:56,903 DEBUG TRAIN Batch 17/3500 loss 11.489389 loss_att 12.990368 loss_ctc 13.958225 loss_rnnt 10.728178 hw_loss 0.247198 lr 0.00041482 rank 6
2023-02-23 10:45:14,830 DEBUG TRAIN Batch 17/3600 loss 15.343678 loss_att 18.543478 loss_ctc 20.340456 loss_rnnt 13.910737 hw_loss 0.237646 lr 0.00041464 rank 2
2023-02-23 10:45:14,835 DEBUG TRAIN Batch 17/3600 loss 5.295267 loss_att 6.376205 loss_ctc 5.574871 loss_rnnt 4.924024 hw_loss 0.220827 lr 0.00041468 rank 4
2023-02-23 10:45:14,836 DEBUG TRAIN Batch 17/3600 loss 13.566587 loss_att 16.309530 loss_ctc 16.800493 loss_rnnt 12.459101 hw_loss 0.239458 lr 0.00041461 rank 3
2023-02-23 10:45:14,835 DEBUG TRAIN Batch 17/3600 loss 13.624500 loss_att 17.430809 loss_ctc 22.122520 loss_rnnt 11.588459 hw_loss 0.265706 lr 0.00041469 rank 0
2023-02-23 10:45:14,836 DEBUG TRAIN Batch 17/3600 loss 5.262612 loss_att 8.890464 loss_ctc 7.851562 loss_rnnt 4.093447 hw_loss 0.184502 lr 0.00041459 rank 5
2023-02-23 10:45:14,836 DEBUG TRAIN Batch 17/3600 loss 7.576682 loss_att 11.748138 loss_ctc 10.408348 loss_rnnt 6.260676 hw_loss 0.195297 lr 0.00041461 rank 7
2023-02-23 10:45:14,836 DEBUG TRAIN Batch 17/3600 loss 9.707484 loss_att 12.118857 loss_ctc 11.172928 loss_rnnt 8.901671 hw_loss 0.240273 lr 0.00041470 rank 1
2023-02-23 10:45:14,844 DEBUG TRAIN Batch 17/3600 loss 7.938598 loss_att 10.371466 loss_ctc 12.765623 loss_rnnt 6.579166 hw_loss 0.429855 lr 0.00041467 rank 6
2023-02-23 10:46:29,539 DEBUG TRAIN Batch 17/3700 loss 3.035072 loss_att 6.006816 loss_ctc 3.611737 loss_rnnt 2.274817 hw_loss 0.166906 lr 0.00041445 rank 5
2023-02-23 10:46:29,541 DEBUG TRAIN Batch 17/3700 loss 11.315641 loss_att 14.203136 loss_ctc 14.750336 loss_rnnt 10.117516 hw_loss 0.305002 lr 0.00041454 rank 4
2023-02-23 10:46:29,543 DEBUG TRAIN Batch 17/3700 loss 11.527313 loss_att 14.096624 loss_ctc 13.198775 loss_rnnt 10.665893 hw_loss 0.233808 lr 0.00041446 rank 3
2023-02-23 10:46:29,544 DEBUG TRAIN Batch 17/3700 loss 9.546403 loss_att 10.738670 loss_ctc 13.991401 loss_rnnt 8.558784 hw_loss 0.293435 lr 0.00041455 rank 0
2023-02-23 10:46:29,549 DEBUG TRAIN Batch 17/3700 loss 8.473231 loss_att 11.077882 loss_ctc 11.679294 loss_rnnt 7.331789 hw_loss 0.361944 lr 0.00041447 rank 7
2023-02-23 10:46:29,550 DEBUG TRAIN Batch 17/3700 loss 5.660537 loss_att 7.785497 loss_ctc 8.934002 loss_rnnt 4.674386 hw_loss 0.233807 lr 0.00041450 rank 2
2023-02-23 10:46:29,551 DEBUG TRAIN Batch 17/3700 loss 14.943748 loss_att 15.538168 loss_ctc 18.737343 loss_rnnt 14.231091 hw_loss 0.164927 lr 0.00041456 rank 1
2023-02-23 10:46:29,599 DEBUG TRAIN Batch 17/3700 loss 12.301534 loss_att 13.364709 loss_ctc 13.862685 loss_rnnt 11.739760 hw_loss 0.264347 lr 0.00041453 rank 6
2023-02-23 10:47:44,640 DEBUG TRAIN Batch 17/3800 loss 10.498310 loss_att 11.076652 loss_ctc 14.212036 loss_rnnt 9.706317 hw_loss 0.339677 lr 0.00041432 rank 3
2023-02-23 10:47:44,641 DEBUG TRAIN Batch 17/3800 loss 24.248337 loss_att 22.854908 loss_ctc 32.804543 loss_rnnt 23.248917 hw_loss 0.257398 lr 0.00041436 rank 2
2023-02-23 10:47:44,643 DEBUG TRAIN Batch 17/3800 loss 14.839580 loss_att 16.744349 loss_ctc 22.732239 loss_rnnt 13.293693 hw_loss 0.211085 lr 0.00041431 rank 5
2023-02-23 10:47:44,644 DEBUG TRAIN Batch 17/3800 loss 8.576048 loss_att 10.648165 loss_ctc 13.034195 loss_rnnt 7.438758 hw_loss 0.240836 lr 0.00041439 rank 4
2023-02-23 10:47:44,644 DEBUG TRAIN Batch 17/3800 loss 9.297350 loss_att 9.376730 loss_ctc 11.283710 loss_rnnt 8.852634 hw_loss 0.307483 lr 0.00041433 rank 7
2023-02-23 10:47:44,644 DEBUG TRAIN Batch 17/3800 loss 11.762968 loss_att 12.429159 loss_ctc 14.359516 loss_rnnt 11.175297 hw_loss 0.202925 lr 0.00041442 rank 1
2023-02-23 10:47:44,644 DEBUG TRAIN Batch 17/3800 loss 10.768086 loss_att 11.102896 loss_ctc 14.124039 loss_rnnt 10.112350 hw_loss 0.264963 lr 0.00041441 rank 0
2023-02-23 10:47:44,645 DEBUG TRAIN Batch 17/3800 loss 12.180266 loss_att 14.843214 loss_ctc 16.156553 loss_rnnt 10.959365 hw_loss 0.296513 lr 0.00041439 rank 6
2023-02-23 10:49:03,818 DEBUG TRAIN Batch 17/3900 loss 7.960097 loss_att 7.991843 loss_ctc 11.361511 loss_rnnt 7.318783 hw_loss 0.340207 lr 0.00041417 rank 5
2023-02-23 10:49:03,819 DEBUG TRAIN Batch 17/3900 loss 6.678474 loss_att 11.657818 loss_ctc 9.838728 loss_rnnt 5.179475 hw_loss 0.153305 lr 0.00041426 rank 0
2023-02-23 10:49:03,819 DEBUG TRAIN Batch 17/3900 loss 2.470945 loss_att 4.337695 loss_ctc 2.755624 loss_rnnt 1.942661 hw_loss 0.219333 lr 0.00041425 rank 4
2023-02-23 10:49:03,821 DEBUG TRAIN Batch 17/3900 loss 13.631660 loss_att 16.250813 loss_ctc 17.456749 loss_rnnt 12.447676 hw_loss 0.281516 lr 0.00041422 rank 2
2023-02-23 10:49:03,826 DEBUG TRAIN Batch 17/3900 loss 9.350727 loss_att 12.861271 loss_ctc 14.554831 loss_rnnt 7.827646 hw_loss 0.238298 lr 0.00041419 rank 7
2023-02-23 10:49:03,833 DEBUG TRAIN Batch 17/3900 loss 6.747158 loss_att 10.812429 loss_ctc 6.894647 loss_rnnt 5.791154 hw_loss 0.231159 lr 0.00041425 rank 6
2023-02-23 10:49:03,852 DEBUG TRAIN Batch 17/3900 loss 8.139195 loss_att 12.062810 loss_ctc 9.738578 loss_rnnt 7.055252 hw_loss 0.161191 lr 0.00041418 rank 3
2023-02-23 10:49:03,885 DEBUG TRAIN Batch 17/3900 loss 8.328659 loss_att 13.412785 loss_ctc 18.317245 loss_rnnt 5.861419 hw_loss 0.222383 lr 0.00041428 rank 1
2023-02-23 10:50:20,377 DEBUG TRAIN Batch 17/4000 loss 13.501692 loss_att 18.921387 loss_ctc 14.639616 loss_rnnt 12.138939 hw_loss 0.238295 lr 0.00041403 rank 5
2023-02-23 10:50:20,381 DEBUG TRAIN Batch 17/4000 loss 6.309449 loss_att 6.613478 loss_ctc 6.221272 loss_rnnt 6.099409 hw_loss 0.301861 lr 0.00041404 rank 7
2023-02-23 10:50:20,382 DEBUG TRAIN Batch 17/4000 loss 7.734077 loss_att 9.144396 loss_ctc 14.118377 loss_rnnt 6.530082 hw_loss 0.132549 lr 0.00041411 rank 6
2023-02-23 10:50:20,383 DEBUG TRAIN Batch 17/4000 loss 13.177738 loss_att 16.771194 loss_ctc 14.463270 loss_rnnt 12.165371 hw_loss 0.229259 lr 0.00041407 rank 2
2023-02-23 10:50:20,387 DEBUG TRAIN Batch 17/4000 loss 16.457891 loss_att 19.392313 loss_ctc 27.232834 loss_rnnt 14.374388 hw_loss 0.112423 lr 0.00041404 rank 3
2023-02-23 10:50:20,387 DEBUG TRAIN Batch 17/4000 loss 11.093052 loss_att 11.892747 loss_ctc 13.757857 loss_rnnt 10.478148 hw_loss 0.186859 lr 0.00041411 rank 4
2023-02-23 10:50:20,388 DEBUG TRAIN Batch 17/4000 loss 13.349075 loss_att 17.202267 loss_ctc 21.036833 loss_rnnt 11.431808 hw_loss 0.227987 lr 0.00041412 rank 0
2023-02-23 10:50:20,392 DEBUG TRAIN Batch 17/4000 loss 6.655309 loss_att 12.791798 loss_ctc 9.367263 loss_rnnt 5.015289 hw_loss 0.095866 lr 0.00041413 rank 1
2023-02-23 10:51:36,545 DEBUG TRAIN Batch 17/4100 loss 12.551370 loss_att 14.854723 loss_ctc 17.873425 loss_rnnt 11.219823 hw_loss 0.302379 lr 0.00041397 rank 4
2023-02-23 10:51:36,545 DEBUG TRAIN Batch 17/4100 loss 16.927706 loss_att 19.437532 loss_ctc 22.620893 loss_rnnt 15.544888 hw_loss 0.228299 lr 0.00041393 rank 2
2023-02-23 10:51:36,545 DEBUG TRAIN Batch 17/4100 loss 10.948340 loss_att 15.566071 loss_ctc 14.585392 loss_rnnt 9.430760 hw_loss 0.204551 lr 0.00041398 rank 0
2023-02-23 10:51:36,548 DEBUG TRAIN Batch 17/4100 loss 18.563915 loss_att 23.378649 loss_ctc 29.415829 loss_rnnt 16.029758 hw_loss 0.233038 lr 0.00041396 rank 6
2023-02-23 10:51:36,549 DEBUG TRAIN Batch 17/4100 loss 11.220213 loss_att 12.711256 loss_ctc 17.584938 loss_rnnt 9.947063 hw_loss 0.236834 lr 0.00041390 rank 3
2023-02-23 10:51:36,552 DEBUG TRAIN Batch 17/4100 loss 4.043136 loss_att 6.283485 loss_ctc 4.401683 loss_rnnt 3.419377 hw_loss 0.239781 lr 0.00041388 rank 5
2023-02-23 10:51:36,557 DEBUG TRAIN Batch 17/4100 loss 7.790194 loss_att 10.015984 loss_ctc 10.877913 loss_rnnt 6.813576 hw_loss 0.224559 lr 0.00041390 rank 7
2023-02-23 10:51:36,561 DEBUG TRAIN Batch 17/4100 loss 14.676901 loss_att 18.005159 loss_ctc 24.969524 loss_rnnt 12.480825 hw_loss 0.296390 lr 0.00041399 rank 1
2023-02-23 10:52:55,137 DEBUG TRAIN Batch 17/4200 loss 21.174295 loss_att 21.413506 loss_ctc 23.612551 loss_rnnt 20.713436 hw_loss 0.164842 lr 0.00041383 rank 4
2023-02-23 10:52:55,137 DEBUG TRAIN Batch 17/4200 loss 9.647823 loss_att 14.721056 loss_ctc 16.462267 loss_rnnt 7.638761 hw_loss 0.160918 lr 0.00041385 rank 1
2023-02-23 10:52:55,142 DEBUG TRAIN Batch 17/4200 loss 10.464933 loss_att 13.255406 loss_ctc 11.522222 loss_rnnt 9.644835 hw_loss 0.226936 lr 0.00041382 rank 6
2023-02-23 10:52:55,145 DEBUG TRAIN Batch 17/4200 loss 9.071608 loss_att 11.714500 loss_ctc 13.838048 loss_rnnt 7.835703 hw_loss 0.134626 lr 0.00041376 rank 7
2023-02-23 10:52:55,146 DEBUG TRAIN Batch 17/4200 loss 7.727939 loss_att 11.564407 loss_ctc 11.670399 loss_rnnt 6.328458 hw_loss 0.199736 lr 0.00041384 rank 0
2023-02-23 10:52:55,147 DEBUG TRAIN Batch 17/4200 loss 14.483286 loss_att 20.328678 loss_ctc 18.756187 loss_rnnt 12.630913 hw_loss 0.212952 lr 0.00041374 rank 5
2023-02-23 10:52:55,148 DEBUG TRAIN Batch 17/4200 loss 5.002450 loss_att 6.559195 loss_ctc 5.760326 loss_rnnt 4.439481 hw_loss 0.282317 lr 0.00041379 rank 2
2023-02-23 10:52:55,186 DEBUG TRAIN Batch 17/4200 loss 8.761949 loss_att 9.218587 loss_ctc 9.712651 loss_rnnt 8.407047 hw_loss 0.256524 lr 0.00041375 rank 3
2023-02-23 10:54:13,375 DEBUG TRAIN Batch 17/4300 loss 8.279906 loss_att 13.564779 loss_ctc 12.094394 loss_rnnt 6.597209 hw_loss 0.219610 lr 0.00041370 rank 0
2023-02-23 10:54:13,376 DEBUG TRAIN Batch 17/4300 loss 9.832285 loss_att 10.776814 loss_ctc 12.197037 loss_rnnt 9.163423 hw_loss 0.308730 lr 0.00041365 rank 2
2023-02-23 10:54:13,379 DEBUG TRAIN Batch 17/4300 loss 7.098090 loss_att 9.806858 loss_ctc 8.723969 loss_rnnt 6.224657 hw_loss 0.215428 lr 0.00041361 rank 3
2023-02-23 10:54:13,382 DEBUG TRAIN Batch 17/4300 loss 9.745183 loss_att 12.945030 loss_ctc 11.916241 loss_rnnt 8.689145 hw_loss 0.237362 lr 0.00041368 rank 6
2023-02-23 10:54:13,384 DEBUG TRAIN Batch 17/4300 loss 3.164981 loss_att 7.710595 loss_ctc 6.913990 loss_rnnt 1.657334 hw_loss 0.184982 lr 0.00041360 rank 5
2023-02-23 10:54:13,385 DEBUG TRAIN Batch 17/4300 loss 16.333397 loss_att 16.259214 loss_ctc 20.204266 loss_rnnt 15.692663 hw_loss 0.261480 lr 0.00041368 rank 4
2023-02-23 10:54:13,388 DEBUG TRAIN Batch 17/4300 loss 9.747395 loss_att 12.924568 loss_ctc 17.612787 loss_rnnt 8.011266 hw_loss 0.097454 lr 0.00041362 rank 7
2023-02-23 10:54:13,388 DEBUG TRAIN Batch 17/4300 loss 12.176408 loss_att 14.890486 loss_ctc 19.103237 loss_rnnt 10.574358 hw_loss 0.254357 lr 0.00041371 rank 1
2023-02-23 10:55:30,206 DEBUG TRAIN Batch 17/4400 loss 15.304005 loss_att 17.457390 loss_ctc 21.207260 loss_rnnt 13.986500 hw_loss 0.186988 lr 0.00041356 rank 0
2023-02-23 10:55:30,209 DEBUG TRAIN Batch 17/4400 loss 14.787827 loss_att 18.521667 loss_ctc 18.338821 loss_rnnt 13.407905 hw_loss 0.299418 lr 0.00041346 rank 5
2023-02-23 10:55:30,210 DEBUG TRAIN Batch 17/4400 loss 17.833532 loss_att 20.329449 loss_ctc 23.261831 loss_rnnt 16.504692 hw_loss 0.198533 lr 0.00041351 rank 2
2023-02-23 10:55:30,210 DEBUG TRAIN Batch 17/4400 loss 9.976795 loss_att 11.750990 loss_ctc 17.341200 loss_rnnt 8.504890 hw_loss 0.253396 lr 0.00041357 rank 1
2023-02-23 10:55:30,211 DEBUG TRAIN Batch 17/4400 loss 10.022615 loss_att 10.670123 loss_ctc 12.968893 loss_rnnt 9.335779 hw_loss 0.308436 lr 0.00041354 rank 4
2023-02-23 10:55:30,212 DEBUG TRAIN Batch 17/4400 loss 9.084257 loss_att 9.549072 loss_ctc 10.970552 loss_rnnt 8.596544 hw_loss 0.268580 lr 0.00041354 rank 6
2023-02-23 10:55:30,215 DEBUG TRAIN Batch 17/4400 loss 11.030676 loss_att 11.955272 loss_ctc 16.272339 loss_rnnt 10.032775 hw_loss 0.213925 lr 0.00041348 rank 7
2023-02-23 10:55:30,257 DEBUG TRAIN Batch 17/4400 loss 6.821667 loss_att 9.011250 loss_ctc 8.254026 loss_rnnt 5.993520 hw_loss 0.373592 lr 0.00041347 rank 3
2023-02-23 10:56:46,654 DEBUG TRAIN Batch 17/4500 loss 8.471049 loss_att 10.480421 loss_ctc 9.330008 loss_rnnt 7.827976 hw_loss 0.237510 lr 0.00041340 rank 6
2023-02-23 10:56:46,655 DEBUG TRAIN Batch 17/4500 loss 9.259927 loss_att 13.490355 loss_ctc 16.696171 loss_rnnt 7.239543 hw_loss 0.342748 lr 0.00041340 rank 4
2023-02-23 10:56:46,655 DEBUG TRAIN Batch 17/4500 loss 1.514760 loss_att 3.964149 loss_ctc 2.138317 loss_rnnt 0.791598 hw_loss 0.281520 lr 0.00041337 rank 2
2023-02-23 10:56:46,657 DEBUG TRAIN Batch 17/4500 loss 12.181625 loss_att 15.441412 loss_ctc 16.344107 loss_rnnt 10.892704 hw_loss 0.153688 lr 0.00041333 rank 3
2023-02-23 10:56:46,657 DEBUG TRAIN Batch 17/4500 loss 10.178533 loss_att 13.695962 loss_ctc 18.134068 loss_rnnt 8.288207 hw_loss 0.236439 lr 0.00041341 rank 0
2023-02-23 10:56:46,658 DEBUG TRAIN Batch 17/4500 loss 7.369566 loss_att 9.371903 loss_ctc 11.139724 loss_rnnt 6.340701 hw_loss 0.235707 lr 0.00041332 rank 5
2023-02-23 10:56:46,659 DEBUG TRAIN Batch 17/4500 loss 6.934732 loss_att 11.568296 loss_ctc 7.630949 loss_rnnt 5.782459 hw_loss 0.248870 lr 0.00041343 rank 1
2023-02-23 10:56:46,697 DEBUG TRAIN Batch 17/4500 loss 5.334369 loss_att 10.739467 loss_ctc 7.151840 loss_rnnt 3.944000 hw_loss 0.125661 lr 0.00041334 rank 7
2023-02-23 10:58:04,896 DEBUG TRAIN Batch 17/4600 loss 13.865147 loss_att 18.092857 loss_ctc 23.751064 loss_rnnt 11.569147 hw_loss 0.248126 lr 0.00041322 rank 2
2023-02-23 10:58:04,896 DEBUG TRAIN Batch 17/4600 loss 10.091839 loss_att 14.601019 loss_ctc 17.515064 loss_rnnt 8.108637 hw_loss 0.171754 lr 0.00041326 rank 4
2023-02-23 10:58:04,899 DEBUG TRAIN Batch 17/4600 loss 6.479307 loss_att 11.155149 loss_ctc 9.244539 loss_rnnt 5.121836 hw_loss 0.100510 lr 0.00041318 rank 5
2023-02-23 10:58:04,901 DEBUG TRAIN Batch 17/4600 loss 9.501764 loss_att 10.179273 loss_ctc 9.415340 loss_rnnt 9.281384 hw_loss 0.180756 lr 0.00041319 rank 3
2023-02-23 10:58:04,902 DEBUG TRAIN Batch 17/4600 loss 13.773944 loss_att 16.040394 loss_ctc 17.205866 loss_rnnt 12.746881 hw_loss 0.217840 lr 0.00041326 rank 6
2023-02-23 10:58:04,906 DEBUG TRAIN Batch 17/4600 loss 7.895748 loss_att 11.271347 loss_ctc 11.225426 loss_rnnt 6.619277 hw_loss 0.295116 lr 0.00041328 rank 1
2023-02-23 10:58:04,906 DEBUG TRAIN Batch 17/4600 loss 8.284189 loss_att 13.128028 loss_ctc 13.562152 loss_rnnt 6.486637 hw_loss 0.234481 lr 0.00041327 rank 0
2023-02-23 10:58:04,908 DEBUG TRAIN Batch 17/4600 loss 10.418041 loss_att 16.405460 loss_ctc 12.581615 loss_rnnt 8.772069 hw_loss 0.300021 lr 0.00041319 rank 7
2023-02-23 10:59:20,623 DEBUG TRAIN Batch 17/4700 loss 7.582768 loss_att 12.141970 loss_ctc 10.249456 loss_rnnt 6.224586 hw_loss 0.170219 lr 0.00041304 rank 5
2023-02-23 10:59:20,626 DEBUG TRAIN Batch 17/4700 loss 16.311403 loss_att 17.677010 loss_ctc 18.524815 loss_rnnt 15.641046 hw_loss 0.191462 lr 0.00041305 rank 3
2023-02-23 10:59:20,629 DEBUG TRAIN Batch 17/4700 loss 10.571277 loss_att 13.737770 loss_ctc 15.028113 loss_rnnt 9.234793 hw_loss 0.204264 lr 0.00041312 rank 4
2023-02-23 10:59:20,630 DEBUG TRAIN Batch 17/4700 loss 12.023776 loss_att 15.236835 loss_ctc 16.564754 loss_rnnt 10.640330 hw_loss 0.253820 lr 0.00041308 rank 2
2023-02-23 10:59:20,631 DEBUG TRAIN Batch 17/4700 loss 11.704386 loss_att 16.821800 loss_ctc 16.370794 loss_rnnt 9.960199 hw_loss 0.184717 lr 0.00041313 rank 0
2023-02-23 10:59:20,632 DEBUG TRAIN Batch 17/4700 loss 7.086150 loss_att 11.646733 loss_ctc 9.545824 loss_rnnt 5.734592 hw_loss 0.209032 lr 0.00041311 rank 6
2023-02-23 10:59:20,633 DEBUG TRAIN Batch 17/4700 loss 10.497808 loss_att 13.860039 loss_ctc 12.259878 loss_rnnt 9.431208 hw_loss 0.298518 lr 0.00041314 rank 1
2023-02-23 10:59:20,680 DEBUG TRAIN Batch 17/4700 loss 3.893355 loss_att 6.310941 loss_ctc 5.057335 loss_rnnt 3.096324 hw_loss 0.296843 lr 0.00041305 rank 7
2023-02-23 11:00:36,992 DEBUG TRAIN Batch 17/4800 loss 6.133739 loss_att 9.589601 loss_ctc 8.681781 loss_rnnt 4.949568 hw_loss 0.287364 lr 0.00041299 rank 0
2023-02-23 11:00:36,997 DEBUG TRAIN Batch 17/4800 loss 8.909146 loss_att 12.956461 loss_ctc 11.379057 loss_rnnt 7.680937 hw_loss 0.167672 lr 0.00041289 rank 5
2023-02-23 11:00:36,999 DEBUG TRAIN Batch 17/4800 loss 11.424503 loss_att 14.409380 loss_ctc 16.128351 loss_rnnt 10.100471 hw_loss 0.187274 lr 0.00041298 rank 4
2023-02-23 11:00:36,999 DEBUG TRAIN Batch 17/4800 loss 10.221708 loss_att 15.996834 loss_ctc 12.949864 loss_rnnt 8.580154 hw_loss 0.230202 lr 0.00041294 rank 2
2023-02-23 11:00:37,000 DEBUG TRAIN Batch 17/4800 loss 3.403837 loss_att 6.440476 loss_ctc 5.072351 loss_rnnt 2.449871 hw_loss 0.232820 lr 0.00041291 rank 3
2023-02-23 11:00:37,001 DEBUG TRAIN Batch 17/4800 loss 5.632392 loss_att 10.196970 loss_ctc 8.882796 loss_rnnt 4.132372 hw_loss 0.288220 lr 0.00041297 rank 6
2023-02-23 11:00:37,002 DEBUG TRAIN Batch 17/4800 loss 14.955564 loss_att 17.554806 loss_ctc 28.875200 loss_rnnt 12.460430 hw_loss 0.223751 lr 0.00041300 rank 1
2023-02-23 11:00:37,044 DEBUG TRAIN Batch 17/4800 loss 9.195580 loss_att 11.535199 loss_ctc 14.513288 loss_rnnt 7.887193 hw_loss 0.246439 lr 0.00041291 rank 7
2023-02-23 11:01:54,948 DEBUG TRAIN Batch 17/4900 loss 11.756975 loss_att 16.164162 loss_ctc 15.178897 loss_rnnt 10.263574 hw_loss 0.291951 lr 0.00041280 rank 2
2023-02-23 11:01:54,951 DEBUG TRAIN Batch 17/4900 loss 7.505444 loss_att 10.367248 loss_ctc 10.537390 loss_rnnt 6.440660 hw_loss 0.165307 lr 0.00041284 rank 4
2023-02-23 11:01:54,952 DEBUG TRAIN Batch 17/4900 loss 6.894664 loss_att 9.752632 loss_ctc 8.958536 loss_rnnt 5.906781 hw_loss 0.264575 lr 0.00041285 rank 0
2023-02-23 11:01:54,952 DEBUG TRAIN Batch 17/4900 loss 16.835035 loss_att 18.108397 loss_ctc 23.527828 loss_rnnt 15.573721 hw_loss 0.214259 lr 0.00041277 rank 3
2023-02-23 11:01:54,957 DEBUG TRAIN Batch 17/4900 loss 9.669435 loss_att 10.942167 loss_ctc 9.510698 loss_rnnt 9.366622 hw_loss 0.130184 lr 0.00041277 rank 7
2023-02-23 11:01:54,957 DEBUG TRAIN Batch 17/4900 loss 13.242154 loss_att 14.174994 loss_ctc 16.133808 loss_rnnt 12.576115 hw_loss 0.176097 lr 0.00041286 rank 1
2023-02-23 11:01:54,957 DEBUG TRAIN Batch 17/4900 loss 10.780396 loss_att 13.931713 loss_ctc 18.326385 loss_rnnt 9.044176 hw_loss 0.187171 lr 0.00041275 rank 5
2023-02-23 11:01:54,960 DEBUG TRAIN Batch 17/4900 loss 10.232867 loss_att 13.503942 loss_ctc 13.743122 loss_rnnt 8.972658 hw_loss 0.258678 lr 0.00041283 rank 6
2023-02-23 11:03:13,395 DEBUG TRAIN Batch 17/5000 loss 11.145978 loss_att 12.426100 loss_ctc 16.398176 loss_rnnt 10.047145 hw_loss 0.267218 lr 0.00041270 rank 4
2023-02-23 11:03:13,397 DEBUG TRAIN Batch 17/5000 loss 8.531898 loss_att 10.854387 loss_ctc 10.042960 loss_rnnt 7.790944 hw_loss 0.140586 lr 0.00041261 rank 5
2023-02-23 11:03:13,397 DEBUG TRAIN Batch 17/5000 loss 6.321678 loss_att 7.118882 loss_ctc 9.675442 loss_rnnt 5.582925 hw_loss 0.247769 lr 0.00041269 rank 6
2023-02-23 11:03:13,398 DEBUG TRAIN Batch 17/5000 loss 9.338171 loss_att 8.585423 loss_ctc 12.217075 loss_rnnt 8.885205 hw_loss 0.411865 lr 0.00041266 rank 2
2023-02-23 11:03:13,404 DEBUG TRAIN Batch 17/5000 loss 5.611934 loss_att 7.197501 loss_ctc 6.258676 loss_rnnt 5.110607 hw_loss 0.183713 lr 0.00041271 rank 0
2023-02-23 11:03:13,408 DEBUG TRAIN Batch 17/5000 loss 6.960440 loss_att 10.018503 loss_ctc 14.549933 loss_rnnt 5.230868 hw_loss 0.198798 lr 0.00041263 rank 7
2023-02-23 11:03:13,410 DEBUG TRAIN Batch 17/5000 loss 10.705705 loss_att 11.257824 loss_ctc 13.359467 loss_rnnt 10.141166 hw_loss 0.188025 lr 0.00041272 rank 1
2023-02-23 11:03:13,412 DEBUG TRAIN Batch 17/5000 loss 12.367057 loss_att 15.424251 loss_ctc 19.375900 loss_rnnt 10.692035 hw_loss 0.242010 lr 0.00041262 rank 3
2023-02-23 11:04:28,947 DEBUG TRAIN Batch 17/5100 loss 14.279164 loss_att 19.564873 loss_ctc 22.580084 loss_rnnt 11.966253 hw_loss 0.279337 lr 0.00041248 rank 3
2023-02-23 11:04:28,950 DEBUG TRAIN Batch 17/5100 loss 4.509900 loss_att 6.114568 loss_ctc 7.720094 loss_rnnt 3.641285 hw_loss 0.224354 lr 0.00041249 rank 7
2023-02-23 11:04:28,951 DEBUG TRAIN Batch 17/5100 loss 13.914113 loss_att 15.453325 loss_ctc 16.331686 loss_rnnt 13.182963 hw_loss 0.189309 lr 0.00041247 rank 5
2023-02-23 11:04:28,952 DEBUG TRAIN Batch 17/5100 loss 12.400830 loss_att 19.651783 loss_ctc 21.227425 loss_rnnt 9.720757 hw_loss 0.099382 lr 0.00041255 rank 6
2023-02-23 11:04:28,953 DEBUG TRAIN Batch 17/5100 loss 8.970191 loss_att 11.449342 loss_ctc 13.357100 loss_rnnt 7.774699 hw_loss 0.215140 lr 0.00041256 rank 4
2023-02-23 11:04:28,953 DEBUG TRAIN Batch 17/5100 loss 9.276726 loss_att 11.763561 loss_ctc 9.603884 loss_rnnt 8.673259 hw_loss 0.117149 lr 0.00041252 rank 2
2023-02-23 11:04:28,955 DEBUG TRAIN Batch 17/5100 loss 8.790545 loss_att 9.163518 loss_ctc 10.850754 loss_rnnt 8.307876 hw_loss 0.250090 lr 0.00041257 rank 0
2023-02-23 11:04:28,959 DEBUG TRAIN Batch 17/5100 loss 14.711889 loss_att 12.544053 loss_ctc 16.637562 loss_rnnt 14.689466 hw_loss 0.373563 lr 0.00041258 rank 1
2023-02-23 11:05:44,051 DEBUG TRAIN Batch 17/5200 loss 5.531505 loss_att 9.675814 loss_ctc 5.938277 loss_rnnt 4.544777 hw_loss 0.194306 lr 0.00041235 rank 7
2023-02-23 11:05:44,054 DEBUG TRAIN Batch 17/5200 loss 14.496619 loss_att 16.778086 loss_ctc 20.076378 loss_rnnt 13.205081 hw_loss 0.171145 lr 0.00041243 rank 0
2023-02-23 11:05:44,054 DEBUG TRAIN Batch 17/5200 loss 6.975816 loss_att 8.951722 loss_ctc 13.401026 loss_rnnt 5.645834 hw_loss 0.146450 lr 0.00041241 rank 6
2023-02-23 11:05:44,055 DEBUG TRAIN Batch 17/5200 loss 7.520740 loss_att 13.636912 loss_ctc 12.826243 loss_rnnt 5.476871 hw_loss 0.212315 lr 0.00041242 rank 4
2023-02-23 11:05:44,056 DEBUG TRAIN Batch 17/5200 loss 4.478224 loss_att 8.628095 loss_ctc 7.764159 loss_rnnt 3.090098 hw_loss 0.225050 lr 0.00041238 rank 2
2023-02-23 11:05:44,056 DEBUG TRAIN Batch 17/5200 loss 10.102659 loss_att 10.398958 loss_ctc 12.989945 loss_rnnt 9.556745 hw_loss 0.190656 lr 0.00041233 rank 5
2023-02-23 11:05:44,057 DEBUG TRAIN Batch 17/5200 loss 8.083153 loss_att 12.204620 loss_ctc 11.939894 loss_rnnt 6.640538 hw_loss 0.195167 lr 0.00041244 rank 1
2023-02-23 11:05:44,059 DEBUG TRAIN Batch 17/5200 loss 5.009934 loss_att 8.228803 loss_ctc 7.092379 loss_rnnt 4.009728 hw_loss 0.147700 lr 0.00041234 rank 3
2023-02-23 11:07:01,663 DEBUG TRAIN Batch 17/5300 loss 7.005273 loss_att 14.378823 loss_ctc 7.729869 loss_rnnt 5.256338 hw_loss 0.333023 lr 0.00041219 rank 5
2023-02-23 11:07:01,663 DEBUG TRAIN Batch 17/5300 loss 11.032995 loss_att 17.668423 loss_ctc 22.449261 loss_rnnt 8.078185 hw_loss 0.197917 lr 0.00041228 rank 4
2023-02-23 11:07:01,665 DEBUG TRAIN Batch 17/5300 loss 6.966909 loss_att 9.735647 loss_ctc 12.993834 loss_rnnt 5.552024 hw_loss 0.107902 lr 0.00041229 rank 0
2023-02-23 11:07:01,669 DEBUG TRAIN Batch 17/5300 loss 9.495832 loss_att 12.001881 loss_ctc 11.957773 loss_rnnt 8.565961 hw_loss 0.188255 lr 0.00041224 rank 2
2023-02-23 11:07:01,674 DEBUG TRAIN Batch 17/5300 loss 15.604938 loss_att 18.067198 loss_ctc 20.910011 loss_rnnt 14.298499 hw_loss 0.199954 lr 0.00041221 rank 7
2023-02-23 11:07:01,676 DEBUG TRAIN Batch 17/5300 loss 6.348155 loss_att 10.508125 loss_ctc 7.536102 loss_rnnt 5.219051 hw_loss 0.260094 lr 0.00041230 rank 1
2023-02-23 11:07:01,709 DEBUG TRAIN Batch 17/5300 loss 11.464479 loss_att 15.333074 loss_ctc 19.508867 loss_rnnt 9.476500 hw_loss 0.265641 lr 0.00041227 rank 6
2023-02-23 11:07:01,744 DEBUG TRAIN Batch 17/5300 loss 8.702000 loss_att 13.162729 loss_ctc 16.447512 loss_rnnt 6.646378 hw_loss 0.245139 lr 0.00041220 rank 3
2023-02-23 11:08:19,638 DEBUG TRAIN Batch 17/5400 loss 11.074494 loss_att 14.810209 loss_ctc 14.326263 loss_rnnt 9.770762 hw_loss 0.230664 lr 0.00041210 rank 2
2023-02-23 11:08:19,640 DEBUG TRAIN Batch 17/5400 loss 8.058348 loss_att 9.528260 loss_ctc 9.248433 loss_rnnt 7.463770 hw_loss 0.266094 lr 0.00041214 rank 4
2023-02-23 11:08:19,646 DEBUG TRAIN Batch 17/5400 loss 7.635653 loss_att 12.477110 loss_ctc 10.932615 loss_rnnt 6.106586 hw_loss 0.227212 lr 0.00041206 rank 3
2023-02-23 11:08:19,647 DEBUG TRAIN Batch 17/5400 loss 6.255822 loss_att 7.969152 loss_ctc 5.551973 loss_rnnt 5.915832 hw_loss 0.170946 lr 0.00041213 rank 6
2023-02-23 11:08:19,647 DEBUG TRAIN Batch 17/5400 loss 12.590246 loss_att 16.050451 loss_ctc 16.336823 loss_rnnt 11.271084 hw_loss 0.239207 lr 0.00041215 rank 0
2023-02-23 11:08:19,647 DEBUG TRAIN Batch 17/5400 loss 8.937592 loss_att 12.032630 loss_ctc 12.815085 loss_rnnt 7.678176 hw_loss 0.231390 lr 0.00041207 rank 7
2023-02-23 11:08:19,649 DEBUG TRAIN Batch 17/5400 loss 15.617559 loss_att 20.394964 loss_ctc 24.585350 loss_rnnt 13.304686 hw_loss 0.303163 lr 0.00041216 rank 1
2023-02-23 11:08:19,651 DEBUG TRAIN Batch 17/5400 loss 6.897860 loss_att 12.578486 loss_ctc 9.567605 loss_rnnt 5.280749 hw_loss 0.234410 lr 0.00041205 rank 5
2023-02-23 11:09:35,245 DEBUG TRAIN Batch 17/5500 loss 11.300246 loss_att 12.991305 loss_ctc 15.749306 loss_rnnt 10.277483 hw_loss 0.171269 lr 0.00041191 rank 5
2023-02-23 11:09:35,249 DEBUG TRAIN Batch 17/5500 loss 18.010612 loss_att 20.210514 loss_ctc 23.923973 loss_rnnt 16.650934 hw_loss 0.246091 lr 0.00041202 rank 1
2023-02-23 11:09:35,250 DEBUG TRAIN Batch 17/5500 loss 15.504278 loss_att 16.166418 loss_ctc 17.581917 loss_rnnt 14.988905 hw_loss 0.198612 lr 0.00041201 rank 0
2023-02-23 11:09:35,249 DEBUG TRAIN Batch 17/5500 loss 9.891610 loss_att 12.316919 loss_ctc 17.086349 loss_rnnt 8.341653 hw_loss 0.197994 lr 0.00041200 rank 4
2023-02-23 11:09:35,250 DEBUG TRAIN Batch 17/5500 loss 3.921152 loss_att 9.420552 loss_ctc 7.440641 loss_rnnt 2.242374 hw_loss 0.205560 lr 0.00041192 rank 3
2023-02-23 11:09:35,253 DEBUG TRAIN Batch 17/5500 loss 12.392589 loss_att 18.227257 loss_ctc 18.637970 loss_rnnt 10.205235 hw_loss 0.351942 lr 0.00041199 rank 6
2023-02-23 11:09:35,253 DEBUG TRAIN Batch 17/5500 loss 13.422654 loss_att 15.164484 loss_ctc 20.339947 loss_rnnt 12.054432 hw_loss 0.182908 lr 0.00041196 rank 2
2023-02-23 11:09:35,304 DEBUG TRAIN Batch 17/5500 loss 10.376481 loss_att 15.353752 loss_ctc 11.954855 loss_rnnt 9.069241 hw_loss 0.190004 lr 0.00041193 rank 7
2023-02-23 11:10:52,213 DEBUG TRAIN Batch 17/5600 loss 14.194984 loss_att 17.046892 loss_ctc 21.769249 loss_rnnt 12.482520 hw_loss 0.247841 lr 0.00041182 rank 2
2023-02-23 11:10:52,214 DEBUG TRAIN Batch 17/5600 loss 23.695141 loss_att 23.693460 loss_ctc 31.535496 loss_rnnt 22.495338 hw_loss 0.290170 lr 0.00041178 rank 3
2023-02-23 11:10:52,214 DEBUG TRAIN Batch 17/5600 loss 12.443171 loss_att 15.496037 loss_ctc 19.655003 loss_rnnt 10.773306 hw_loss 0.183215 lr 0.00041179 rank 7
2023-02-23 11:10:52,218 DEBUG TRAIN Batch 17/5600 loss 12.079982 loss_att 12.963529 loss_ctc 16.491035 loss_rnnt 11.185290 hw_loss 0.243453 lr 0.00041186 rank 4
2023-02-23 11:10:52,220 DEBUG TRAIN Batch 17/5600 loss 13.622210 loss_att 15.717125 loss_ctc 15.525345 loss_rnnt 12.844957 hw_loss 0.195971 lr 0.00041177 rank 5
2023-02-23 11:10:52,221 DEBUG TRAIN Batch 17/5600 loss 4.366753 loss_att 6.317977 loss_ctc 6.351772 loss_rnnt 3.554266 hw_loss 0.295449 lr 0.00041187 rank 0
2023-02-23 11:10:52,240 DEBUG TRAIN Batch 17/5600 loss 6.701447 loss_att 7.908078 loss_ctc 7.234511 loss_rnnt 6.320289 hw_loss 0.128918 lr 0.00041188 rank 1
2023-02-23 11:10:52,248 DEBUG TRAIN Batch 17/5600 loss 12.168041 loss_att 13.930317 loss_ctc 16.326727 loss_rnnt 11.051307 hw_loss 0.393352 lr 0.00041185 rank 6
2023-02-23 11:12:11,644 DEBUG TRAIN Batch 17/5700 loss 12.905788 loss_att 15.848255 loss_ctc 19.118088 loss_rnnt 11.397734 hw_loss 0.171103 lr 0.00041173 rank 0
2023-02-23 11:12:11,646 DEBUG TRAIN Batch 17/5700 loss 6.342749 loss_att 9.581829 loss_ctc 9.949465 loss_rnnt 5.129681 hw_loss 0.158168 lr 0.00041171 rank 6
2023-02-23 11:12:11,648 DEBUG TRAIN Batch 17/5700 loss 14.043720 loss_att 17.111004 loss_ctc 21.135523 loss_rnnt 12.371966 hw_loss 0.211359 lr 0.00041172 rank 4
2023-02-23 11:12:11,650 DEBUG TRAIN Batch 17/5700 loss 8.690568 loss_att 10.814461 loss_ctc 10.521536 loss_rnnt 7.888148 hw_loss 0.250334 lr 0.00041174 rank 1
2023-02-23 11:12:11,653 DEBUG TRAIN Batch 17/5700 loss 13.131458 loss_att 13.613331 loss_ctc 16.871599 loss_rnnt 12.365725 hw_loss 0.320013 lr 0.00041164 rank 3
2023-02-23 11:12:11,655 DEBUG TRAIN Batch 17/5700 loss 4.901034 loss_att 6.722806 loss_ctc 13.384267 loss_rnnt 3.317355 hw_loss 0.165427 lr 0.00041163 rank 5
2023-02-23 11:12:11,658 DEBUG TRAIN Batch 17/5700 loss 6.446914 loss_att 7.342113 loss_ctc 9.464994 loss_rnnt 5.738407 hw_loss 0.238231 lr 0.00041165 rank 7
2023-02-23 11:12:11,666 DEBUG TRAIN Batch 17/5700 loss 21.317366 loss_att 20.053629 loss_ctc 22.522667 loss_rnnt 21.260601 hw_loss 0.279006 lr 0.00041168 rank 2
2023-02-23 11:13:27,740 DEBUG TRAIN Batch 17/5800 loss 7.093883 loss_att 8.989701 loss_ctc 9.853148 loss_rnnt 6.171222 hw_loss 0.329240 lr 0.00041149 rank 5
2023-02-23 11:13:27,741 DEBUG TRAIN Batch 17/5800 loss 2.789867 loss_att 5.260153 loss_ctc 5.111874 loss_rnnt 1.899336 hw_loss 0.162887 lr 0.00041158 rank 4
2023-02-23 11:13:27,745 DEBUG TRAIN Batch 17/5800 loss 4.697770 loss_att 8.209154 loss_ctc 6.920938 loss_rnnt 3.555290 hw_loss 0.269588 lr 0.00041154 rank 2
2023-02-23 11:13:27,747 DEBUG TRAIN Batch 17/5800 loss 11.434575 loss_att 18.249401 loss_ctc 24.854874 loss_rnnt 8.141887 hw_loss 0.263157 lr 0.00041151 rank 7
2023-02-23 11:13:27,749 DEBUG TRAIN Batch 17/5800 loss 14.772544 loss_att 18.479591 loss_ctc 19.428371 loss_rnnt 13.311599 hw_loss 0.185173 lr 0.00041159 rank 0
2023-02-23 11:13:27,751 DEBUG TRAIN Batch 17/5800 loss 13.566749 loss_att 16.909567 loss_ctc 19.267683 loss_rnnt 12.030440 hw_loss 0.201789 lr 0.00041157 rank 6
2023-02-23 11:13:27,755 DEBUG TRAIN Batch 17/5800 loss 7.643052 loss_att 10.659222 loss_ctc 10.675021 loss_rnnt 6.510981 hw_loss 0.233579 lr 0.00041160 rank 1
2023-02-23 11:13:27,792 DEBUG TRAIN Batch 17/5800 loss 5.400946 loss_att 9.847790 loss_ctc 7.499229 loss_rnnt 4.152473 hw_loss 0.148748 lr 0.00041151 rank 3
2023-02-23 11:14:44,285 DEBUG TRAIN Batch 17/5900 loss 7.706734 loss_att 9.687186 loss_ctc 8.318851 loss_rnnt 7.107479 hw_loss 0.227905 lr 0.00041146 rank 1
2023-02-23 11:14:44,286 DEBUG TRAIN Batch 17/5900 loss 6.947989 loss_att 12.320293 loss_ctc 9.738391 loss_rnnt 5.381557 hw_loss 0.224845 lr 0.00041140 rank 2
2023-02-23 11:14:44,288 DEBUG TRAIN Batch 17/5900 loss 9.395709 loss_att 10.722224 loss_ctc 13.770350 loss_rnnt 8.502164 hw_loss 0.084294 lr 0.00041135 rank 5
2023-02-23 11:14:44,289 DEBUG TRAIN Batch 17/5900 loss 8.659236 loss_att 11.419269 loss_ctc 15.060584 loss_rnnt 7.174067 hw_loss 0.149340 lr 0.00041143 rank 6
2023-02-23 11:14:44,290 DEBUG TRAIN Batch 17/5900 loss 5.720728 loss_att 8.839454 loss_ctc 7.355690 loss_rnnt 4.824678 hw_loss 0.101831 lr 0.00041137 rank 3
2023-02-23 11:14:44,291 DEBUG TRAIN Batch 17/5900 loss 6.522319 loss_att 11.438950 loss_ctc 8.351053 loss_rnnt 5.172035 hw_loss 0.230862 lr 0.00041137 rank 7
2023-02-23 11:14:44,292 DEBUG TRAIN Batch 17/5900 loss 9.222331 loss_att 14.377528 loss_ctc 15.470257 loss_rnnt 7.223545 hw_loss 0.252545 lr 0.00041145 rank 0
2023-02-23 11:14:44,345 DEBUG TRAIN Batch 17/5900 loss 18.087259 loss_att 22.115160 loss_ctc 24.910114 loss_rnnt 16.243301 hw_loss 0.241245 lr 0.00041144 rank 4
2023-02-23 11:16:01,330 DEBUG TRAIN Batch 17/6000 loss 4.206347 loss_att 7.228101 loss_ctc 4.697510 loss_rnnt 3.462412 hw_loss 0.138929 lr 0.00041130 rank 4
2023-02-23 11:16:01,332 DEBUG TRAIN Batch 17/6000 loss 6.568367 loss_att 7.887712 loss_ctc 8.364527 loss_rnnt 5.949496 hw_loss 0.216588 lr 0.00041131 rank 0
2023-02-23 11:16:01,332 DEBUG TRAIN Batch 17/6000 loss 8.795849 loss_att 11.463545 loss_ctc 12.400686 loss_rnnt 7.689057 hw_loss 0.173638 lr 0.00041126 rank 2
2023-02-23 11:16:01,333 DEBUG TRAIN Batch 17/6000 loss 12.285287 loss_att 14.414243 loss_ctc 14.020626 loss_rnnt 11.523943 hw_loss 0.195325 lr 0.00041129 rank 6
2023-02-23 11:16:01,334 DEBUG TRAIN Batch 17/6000 loss 15.756498 loss_att 20.675148 loss_ctc 19.627230 loss_rnnt 14.157215 hw_loss 0.186479 lr 0.00041123 rank 3
2023-02-23 11:16:01,336 DEBUG TRAIN Batch 17/6000 loss 9.772102 loss_att 12.449146 loss_ctc 12.516062 loss_rnnt 8.767062 hw_loss 0.194569 lr 0.00041122 rank 5
2023-02-23 11:16:01,337 DEBUG TRAIN Batch 17/6000 loss 11.495684 loss_att 15.723452 loss_ctc 13.980953 loss_rnnt 10.208555 hw_loss 0.206636 lr 0.00041132 rank 1
2023-02-23 11:16:01,340 DEBUG TRAIN Batch 17/6000 loss 17.126307 loss_att 26.243122 loss_ctc 21.357681 loss_rnnt 14.629209 hw_loss 0.205405 lr 0.00041123 rank 7
2023-02-23 11:17:19,649 DEBUG TRAIN Batch 17/6100 loss 3.776637 loss_att 5.216208 loss_ctc 4.719745 loss_rnnt 3.243169 hw_loss 0.224635 lr 0.00041117 rank 0
2023-02-23 11:17:19,652 DEBUG TRAIN Batch 17/6100 loss 9.128518 loss_att 11.726374 loss_ctc 12.268910 loss_rnnt 8.090320 hw_loss 0.187327 lr 0.00041112 rank 2
2023-02-23 11:17:19,652 DEBUG TRAIN Batch 17/6100 loss 13.472509 loss_att 21.300783 loss_ctc 20.958782 loss_rnnt 10.864181 hw_loss 0.083443 lr 0.00041108 rank 5
2023-02-23 11:17:19,653 DEBUG TRAIN Batch 17/6100 loss 10.875792 loss_att 17.321854 loss_ctc 14.591337 loss_rnnt 8.982835 hw_loss 0.203133 lr 0.00041115 rank 6
2023-02-23 11:17:19,654 DEBUG TRAIN Batch 17/6100 loss 11.524487 loss_att 14.075708 loss_ctc 16.826607 loss_rnnt 10.183937 hw_loss 0.231293 lr 0.00041109 rank 7
2023-02-23 11:17:19,654 DEBUG TRAIN Batch 17/6100 loss 8.171975 loss_att 11.916138 loss_ctc 9.658127 loss_rnnt 7.131143 hw_loss 0.175959 lr 0.00041116 rank 4
2023-02-23 11:17:19,657 DEBUG TRAIN Batch 17/6100 loss 3.532659 loss_att 7.746975 loss_ctc 4.545409 loss_rnnt 2.472400 hw_loss 0.154429 lr 0.00041118 rank 1
2023-02-23 11:17:19,702 DEBUG TRAIN Batch 17/6100 loss 12.683117 loss_att 15.972975 loss_ctc 17.474297 loss_rnnt 11.282801 hw_loss 0.194099 lr 0.00041109 rank 3
2023-02-23 11:18:36,868 DEBUG TRAIN Batch 17/6200 loss 26.387777 loss_att 34.935032 loss_ctc 38.856396 loss_rnnt 22.872274 hw_loss 0.269191 lr 0.00041098 rank 2
2023-02-23 11:18:36,871 DEBUG TRAIN Batch 17/6200 loss 7.535967 loss_att 9.957930 loss_ctc 7.443711 loss_rnnt 6.936141 hw_loss 0.239504 lr 0.00041096 rank 7
2023-02-23 11:18:36,872 DEBUG TRAIN Batch 17/6200 loss 8.061250 loss_att 11.657717 loss_ctc 8.962870 loss_rnnt 7.084979 hw_loss 0.256428 lr 0.00041095 rank 3
2023-02-23 11:18:36,872 DEBUG TRAIN Batch 17/6200 loss 7.111410 loss_att 9.842302 loss_ctc 9.421129 loss_rnnt 6.117804 hw_loss 0.261497 lr 0.00041103 rank 0
2023-02-23 11:18:36,874 DEBUG TRAIN Batch 17/6200 loss 19.269087 loss_att 16.536545 loss_ctc 19.413719 loss_rnnt 19.709629 hw_loss 0.162526 lr 0.00041104 rank 1
2023-02-23 11:18:36,874 DEBUG TRAIN Batch 17/6200 loss 12.673455 loss_att 16.591076 loss_ctc 21.423195 loss_rnnt 10.584505 hw_loss 0.260238 lr 0.00041102 rank 4
2023-02-23 11:18:36,877 DEBUG TRAIN Batch 17/6200 loss 14.608400 loss_att 14.994835 loss_ctc 19.499996 loss_rnnt 13.774824 hw_loss 0.195144 lr 0.00041102 rank 6
2023-02-23 11:18:36,909 DEBUG TRAIN Batch 17/6200 loss 6.330413 loss_att 10.101711 loss_ctc 9.498655 loss_rnnt 5.018950 hw_loss 0.252697 lr 0.00041094 rank 5
2023-02-23 11:19:53,005 DEBUG TRAIN Batch 17/6300 loss 11.529331 loss_att 12.645706 loss_ctc 19.063202 loss_rnnt 10.200127 hw_loss 0.190147 lr 0.00041085 rank 2
2023-02-23 11:19:53,007 DEBUG TRAIN Batch 17/6300 loss 6.035690 loss_att 8.106594 loss_ctc 7.424530 loss_rnnt 5.322762 hw_loss 0.212942 lr 0.00041089 rank 0
2023-02-23 11:19:53,007 DEBUG TRAIN Batch 17/6300 loss 6.826196 loss_att 7.774402 loss_ctc 8.626906 loss_rnnt 6.219466 hw_loss 0.331866 lr 0.00041088 rank 6
2023-02-23 11:19:53,008 DEBUG TRAIN Batch 17/6300 loss 6.784621 loss_att 11.985003 loss_ctc 10.122808 loss_rnnt 5.144547 hw_loss 0.290449 lr 0.00041088 rank 4
2023-02-23 11:19:53,008 DEBUG TRAIN Batch 17/6300 loss 9.432001 loss_att 12.686267 loss_ctc 14.622139 loss_rnnt 7.937138 hw_loss 0.284984 lr 0.00041082 rank 7
2023-02-23 11:19:53,011 DEBUG TRAIN Batch 17/6300 loss 9.711496 loss_att 11.424904 loss_ctc 13.346293 loss_rnnt 8.746390 hw_loss 0.258346 lr 0.00041090 rank 1
2023-02-23 11:19:53,012 DEBUG TRAIN Batch 17/6300 loss 11.884892 loss_att 14.903102 loss_ctc 17.372601 loss_rnnt 10.461593 hw_loss 0.164933 lr 0.00041080 rank 5
2023-02-23 11:19:53,013 DEBUG TRAIN Batch 17/6300 loss 10.867805 loss_att 12.745557 loss_ctc 14.421648 loss_rnnt 9.900614 hw_loss 0.220867 lr 0.00041081 rank 3
2023-02-23 11:21:12,969 DEBUG TRAIN Batch 17/6400 loss 12.832904 loss_att 13.172466 loss_ctc 19.034542 loss_rnnt 11.828323 hw_loss 0.205843 lr 0.00041071 rank 2
2023-02-23 11:21:12,970 DEBUG TRAIN Batch 17/6400 loss 16.170362 loss_att 17.490311 loss_ctc 18.016880 loss_rnnt 15.495387 hw_loss 0.308967 lr 0.00041074 rank 4
2023-02-23 11:21:12,977 DEBUG TRAIN Batch 17/6400 loss 4.917382 loss_att 5.877683 loss_ctc 5.679069 loss_rnnt 4.517329 hw_loss 0.199565 lr 0.00041066 rank 5
2023-02-23 11:21:12,979 DEBUG TRAIN Batch 17/6400 loss 6.349823 loss_att 8.818429 loss_ctc 9.068676 loss_rnnt 5.371435 hw_loss 0.229037 lr 0.00041077 rank 1
2023-02-23 11:21:12,979 DEBUG TRAIN Batch 17/6400 loss 7.257491 loss_att 11.258722 loss_ctc 9.638942 loss_rnnt 6.054273 hw_loss 0.160209 lr 0.00041074 rank 6
2023-02-23 11:21:12,981 DEBUG TRAIN Batch 17/6400 loss 12.143020 loss_att 11.709562 loss_ctc 14.315706 loss_rnnt 11.758297 hw_loss 0.340729 lr 0.00041075 rank 0
2023-02-23 11:21:12,980 DEBUG TRAIN Batch 17/6400 loss 15.575529 loss_att 19.753090 loss_ctc 24.916836 loss_rnnt 13.390013 hw_loss 0.195931 lr 0.00041067 rank 3
2023-02-23 11:21:12,981 DEBUG TRAIN Batch 17/6400 loss 4.238326 loss_att 8.607873 loss_ctc 9.633034 loss_rnnt 2.543818 hw_loss 0.189944 lr 0.00041068 rank 7
2023-02-23 11:22:29,905 DEBUG TRAIN Batch 17/6500 loss 12.458696 loss_att 11.761506 loss_ctc 14.626181 loss_rnnt 12.133512 hw_loss 0.329297 lr 0.00041052 rank 5
2023-02-23 11:22:29,909 DEBUG TRAIN Batch 17/6500 loss 9.324224 loss_att 14.290562 loss_ctc 11.470023 loss_rnnt 7.900229 hw_loss 0.271169 lr 0.00041060 rank 4
2023-02-23 11:22:29,911 DEBUG TRAIN Batch 17/6500 loss 13.266840 loss_att 15.174979 loss_ctc 18.311539 loss_rnnt 12.061502 hw_loss 0.283280 lr 0.00041060 rank 6
2023-02-23 11:22:29,912 DEBUG TRAIN Batch 17/6500 loss 14.406348 loss_att 13.882468 loss_ctc 16.656921 loss_rnnt 14.097906 hw_loss 0.212144 lr 0.00041057 rank 2
2023-02-23 11:22:29,912 DEBUG TRAIN Batch 17/6500 loss 7.529400 loss_att 9.186974 loss_ctc 11.412482 loss_rnnt 6.562277 hw_loss 0.220996 lr 0.00041053 rank 3
2023-02-23 11:22:29,912 DEBUG TRAIN Batch 17/6500 loss 10.663728 loss_att 13.993134 loss_ctc 17.280893 loss_rnnt 9.003448 hw_loss 0.210204 lr 0.00041062 rank 0
2023-02-23 11:22:29,915 DEBUG TRAIN Batch 17/6500 loss 4.962618 loss_att 9.768090 loss_ctc 7.218356 loss_rnnt 3.573881 hw_loss 0.237897 lr 0.00041054 rank 7
2023-02-23 11:22:29,918 DEBUG TRAIN Batch 17/6500 loss 1.839702 loss_att 4.176988 loss_ctc 3.485697 loss_rnnt 1.062566 hw_loss 0.169149 lr 0.00041063 rank 1
2023-02-23 11:23:46,017 DEBUG TRAIN Batch 17/6600 loss 5.216380 loss_att 7.709066 loss_ctc 7.221007 loss_rnnt 4.333624 hw_loss 0.219254 lr 0.00041047 rank 4
2023-02-23 11:23:46,018 DEBUG TRAIN Batch 17/6600 loss 9.443345 loss_att 11.903465 loss_ctc 10.893999 loss_rnnt 8.663798 hw_loss 0.176442 lr 0.00041048 rank 0
2023-02-23 11:23:46,018 DEBUG TRAIN Batch 17/6600 loss 8.592391 loss_att 10.540037 loss_ctc 12.196267 loss_rnnt 7.548289 hw_loss 0.326355 lr 0.00041043 rank 2
2023-02-23 11:23:46,023 DEBUG TRAIN Batch 17/6600 loss 6.721601 loss_att 13.550923 loss_ctc 8.274492 loss_rnnt 5.027802 hw_loss 0.226654 lr 0.00041039 rank 3
2023-02-23 11:23:46,023 DEBUG TRAIN Batch 17/6600 loss 2.258758 loss_att 4.415232 loss_ctc 2.472475 loss_rnnt 1.615499 hw_loss 0.344003 lr 0.00041038 rank 5
2023-02-23 11:23:46,023 DEBUG TRAIN Batch 17/6600 loss 6.302216 loss_att 11.056223 loss_ctc 9.352335 loss_rnnt 4.781405 hw_loss 0.306237 lr 0.00041046 rank 6
2023-02-23 11:23:46,024 DEBUG TRAIN Batch 17/6600 loss 15.459651 loss_att 19.565880 loss_ctc 22.373287 loss_rnnt 13.614124 hw_loss 0.192118 lr 0.00041049 rank 1
2023-02-23 11:23:46,075 DEBUG TRAIN Batch 17/6600 loss 7.322524 loss_att 9.799627 loss_ctc 11.895864 loss_rnnt 6.086780 hw_loss 0.244771 lr 0.00041040 rank 7
2023-02-23 11:25:03,773 DEBUG TRAIN Batch 17/6700 loss 14.672173 loss_att 19.567944 loss_ctc 21.246428 loss_rnnt 12.709421 hw_loss 0.200677 lr 0.00041033 rank 4
2023-02-23 11:25:03,774 DEBUG TRAIN Batch 17/6700 loss 7.858049 loss_att 13.614347 loss_ctc 12.346739 loss_rnnt 5.922956 hw_loss 0.347514 lr 0.00041025 rank 5
2023-02-23 11:25:03,775 DEBUG TRAIN Batch 17/6700 loss 8.293823 loss_att 10.129086 loss_ctc 8.440973 loss_rnnt 7.827508 hw_loss 0.149330 lr 0.00041032 rank 6
2023-02-23 11:25:03,776 DEBUG TRAIN Batch 17/6700 loss 9.680527 loss_att 13.843081 loss_ctc 17.547447 loss_rnnt 7.693648 hw_loss 0.197712 lr 0.00041026 rank 7
2023-02-23 11:25:03,777 DEBUG TRAIN Batch 17/6700 loss 2.477580 loss_att 3.995295 loss_ctc 3.430298 loss_rnnt 1.974406 hw_loss 0.136130 lr 0.00041029 rank 2
2023-02-23 11:25:03,778 DEBUG TRAIN Batch 17/6700 loss 16.357937 loss_att 21.419064 loss_ctc 19.676785 loss_rnnt 14.747027 hw_loss 0.292823 lr 0.00041034 rank 0
2023-02-23 11:25:03,782 DEBUG TRAIN Batch 17/6700 loss 8.571584 loss_att 10.963643 loss_ctc 12.484512 loss_rnnt 7.484708 hw_loss 0.162636 lr 0.00041035 rank 1
2023-02-23 11:25:03,822 DEBUG TRAIN Batch 17/6700 loss 7.569456 loss_att 11.164770 loss_ctc 10.493065 loss_rnnt 6.341923 hw_loss 0.222480 lr 0.00041026 rank 3
2023-02-23 11:26:22,371 DEBUG TRAIN Batch 17/6800 loss 14.470340 loss_att 15.897178 loss_ctc 17.911602 loss_rnnt 13.598436 hw_loss 0.239438 lr 0.00041011 rank 5
2023-02-23 11:26:22,372 DEBUG TRAIN Batch 17/6800 loss 10.825283 loss_att 12.449497 loss_ctc 13.743173 loss_rnnt 9.993370 hw_loss 0.221284 lr 0.00041020 rank 0
2023-02-23 11:26:22,372 DEBUG TRAIN Batch 17/6800 loss 22.479301 loss_att 25.649897 loss_ctc 28.559231 loss_rnnt 20.891373 hw_loss 0.268411 lr 0.00041012 rank 3
2023-02-23 11:26:22,372 DEBUG TRAIN Batch 17/6800 loss 9.707850 loss_att 12.307974 loss_ctc 14.839027 loss_rnnt 8.364019 hw_loss 0.261842 lr 0.00041015 rank 2
2023-02-23 11:26:22,373 DEBUG TRAIN Batch 17/6800 loss 13.871471 loss_att 15.598614 loss_ctc 15.203739 loss_rnnt 13.265503 hw_loss 0.155443 lr 0.00041019 rank 4
2023-02-23 11:26:22,376 DEBUG TRAIN Batch 17/6800 loss 14.696541 loss_att 18.001839 loss_ctc 18.552107 loss_rnnt 13.385566 hw_loss 0.254702 lr 0.00041021 rank 1
2023-02-23 11:26:22,378 DEBUG TRAIN Batch 17/6800 loss 13.666608 loss_att 15.847321 loss_ctc 14.817018 loss_rnnt 12.956020 hw_loss 0.226981 lr 0.00041013 rank 7
2023-02-23 11:26:22,383 DEBUG TRAIN Batch 17/6800 loss 13.843650 loss_att 17.362246 loss_ctc 20.875288 loss_rnnt 12.074301 hw_loss 0.240148 lr 0.00041018 rank 6
2023-02-23 11:27:36,912 DEBUG TRAIN Batch 17/6900 loss 13.057439 loss_att 13.469070 loss_ctc 15.941170 loss_rnnt 12.492373 hw_loss 0.184201 lr 0.00041002 rank 2
2023-02-23 11:27:36,914 DEBUG TRAIN Batch 17/6900 loss 9.544094 loss_att 14.139839 loss_ctc 16.244513 loss_rnnt 7.574600 hw_loss 0.294293 lr 0.00040999 rank 7
2023-02-23 11:27:36,915 DEBUG TRAIN Batch 17/6900 loss 10.079015 loss_att 14.923943 loss_ctc 17.258295 loss_rnnt 8.003828 hw_loss 0.279308 lr 0.00040997 rank 5
2023-02-23 11:27:36,915 DEBUG TRAIN Batch 17/6900 loss 9.591166 loss_att 12.709852 loss_ctc 13.410031 loss_rnnt 8.307464 hw_loss 0.282720 lr 0.00041007 rank 1
2023-02-23 11:27:36,918 DEBUG TRAIN Batch 17/6900 loss 15.330546 loss_att 19.156748 loss_ctc 20.040607 loss_rnnt 13.783072 hw_loss 0.289173 lr 0.00041006 rank 0
2023-02-23 11:27:36,917 DEBUG TRAIN Batch 17/6900 loss 10.006553 loss_att 10.199451 loss_ctc 14.298748 loss_rnnt 9.205860 hw_loss 0.355914 lr 0.00041005 rank 4
2023-02-23 11:27:36,918 DEBUG TRAIN Batch 17/6900 loss 5.465152 loss_att 7.968794 loss_ctc 7.918737 loss_rnnt 4.508489 hw_loss 0.241481 lr 0.00040998 rank 3
2023-02-23 11:27:36,920 DEBUG TRAIN Batch 17/6900 loss 10.272994 loss_att 11.816072 loss_ctc 14.037365 loss_rnnt 9.307899 hw_loss 0.289811 lr 0.00041005 rank 6
2023-02-23 11:28:52,920 DEBUG TRAIN Batch 17/7000 loss 8.164184 loss_att 11.384550 loss_ctc 13.653492 loss_rnnt 6.655016 hw_loss 0.249724 lr 0.00040983 rank 5
2023-02-23 11:28:52,921 DEBUG TRAIN Batch 17/7000 loss 13.401023 loss_att 18.280424 loss_ctc 15.805408 loss_rnnt 11.977987 hw_loss 0.237321 lr 0.00040991 rank 4
2023-02-23 11:28:52,924 DEBUG TRAIN Batch 17/7000 loss 13.732985 loss_att 13.511559 loss_ctc 15.876863 loss_rnnt 13.361717 hw_loss 0.243192 lr 0.00040984 rank 3
2023-02-23 11:28:52,928 DEBUG TRAIN Batch 17/7000 loss 7.818350 loss_att 13.316015 loss_ctc 11.377871 loss_rnnt 6.139666 hw_loss 0.196028 lr 0.00040994 rank 1
2023-02-23 11:28:52,929 DEBUG TRAIN Batch 17/7000 loss 12.585682 loss_att 13.055927 loss_ctc 14.751945 loss_rnnt 12.079709 hw_loss 0.230789 lr 0.00040993 rank 0
2023-02-23 11:28:52,930 DEBUG TRAIN Batch 17/7000 loss 9.943763 loss_att 13.510973 loss_ctc 11.481983 loss_rnnt 8.916595 hw_loss 0.203678 lr 0.00040991 rank 6
2023-02-23 11:28:52,931 DEBUG TRAIN Batch 17/7000 loss 3.140163 loss_att 5.493032 loss_ctc 3.929194 loss_rnnt 2.428072 hw_loss 0.255588 lr 0.00040988 rank 2
2023-02-23 11:28:52,978 DEBUG TRAIN Batch 17/7000 loss 12.641185 loss_att 14.440438 loss_ctc 16.820532 loss_rnnt 11.584923 hw_loss 0.260932 lr 0.00040985 rank 7
2023-02-23 11:30:11,479 DEBUG TRAIN Batch 17/7100 loss 6.907526 loss_att 8.830110 loss_ctc 8.306550 loss_rnnt 6.184881 hw_loss 0.284234 lr 0.00040974 rank 2
2023-02-23 11:30:11,482 DEBUG TRAIN Batch 17/7100 loss 7.286072 loss_att 7.907930 loss_ctc 8.108833 loss_rnnt 6.932739 hw_loss 0.223612 lr 0.00040978 rank 4
2023-02-23 11:30:11,484 DEBUG TRAIN Batch 17/7100 loss 8.707266 loss_att 11.850527 loss_ctc 14.064474 loss_rnnt 7.188229 hw_loss 0.330169 lr 0.00040980 rank 1
2023-02-23 11:30:11,484 DEBUG TRAIN Batch 17/7100 loss 7.296467 loss_att 9.450574 loss_ctc 11.298175 loss_rnnt 6.133409 hw_loss 0.372518 lr 0.00040969 rank 5
2023-02-23 11:30:11,504 DEBUG TRAIN Batch 17/7100 loss 11.563467 loss_att 19.514662 loss_ctc 14.522708 loss_rnnt 9.469116 hw_loss 0.205400 lr 0.00040971 rank 3
2023-02-23 11:30:11,516 DEBUG TRAIN Batch 17/7100 loss 4.749162 loss_att 8.222618 loss_ctc 4.982388 loss_rnnt 3.887932 hw_loss 0.253954 lr 0.00040971 rank 7
2023-02-23 11:30:11,531 DEBUG TRAIN Batch 17/7100 loss 4.055468 loss_att 5.299574 loss_ctc 4.397577 loss_rnnt 3.616088 hw_loss 0.271770 lr 0.00040977 rank 6
2023-02-23 11:30:11,534 DEBUG TRAIN Batch 17/7100 loss 10.894608 loss_att 14.017429 loss_ctc 16.863907 loss_rnnt 9.374496 hw_loss 0.186829 lr 0.00040979 rank 0
2023-02-23 11:31:29,827 DEBUG TRAIN Batch 17/7200 loss 9.006156 loss_att 16.894749 loss_ctc 13.835925 loss_rnnt 6.701347 hw_loss 0.155852 lr 0.00040956 rank 5
2023-02-23 11:31:29,831 DEBUG TRAIN Batch 17/7200 loss 20.122932 loss_att 24.042597 loss_ctc 29.260544 loss_rnnt 17.945171 hw_loss 0.329022 lr 0.00040965 rank 0
2023-02-23 11:31:29,831 DEBUG TRAIN Batch 17/7200 loss 4.928513 loss_att 7.368958 loss_ctc 9.433727 loss_rnnt 3.718795 hw_loss 0.226750 lr 0.00040966 rank 1
2023-02-23 11:31:29,834 DEBUG TRAIN Batch 17/7200 loss 8.144264 loss_att 10.870323 loss_ctc 12.417926 loss_rnnt 6.897299 hw_loss 0.247372 lr 0.00040957 rank 7
2023-02-23 11:31:29,835 DEBUG TRAIN Batch 17/7200 loss 3.730027 loss_att 6.693238 loss_ctc 5.144538 loss_rnnt 2.858564 hw_loss 0.169162 lr 0.00040960 rank 2
2023-02-23 11:31:29,835 DEBUG TRAIN Batch 17/7200 loss 13.425537 loss_att 16.670347 loss_ctc 20.632706 loss_rnnt 11.685117 hw_loss 0.244692 lr 0.00040964 rank 4
2023-02-23 11:31:29,838 DEBUG TRAIN Batch 17/7200 loss 22.643534 loss_att 27.600491 loss_ctc 30.361988 loss_rnnt 20.480402 hw_loss 0.267396 lr 0.00040963 rank 6
2023-02-23 11:31:29,841 DEBUG TRAIN Batch 17/7200 loss 23.713354 loss_att 27.577507 loss_ctc 28.417669 loss_rnnt 22.196735 hw_loss 0.218521 lr 0.00040957 rank 3
2023-02-23 11:32:46,345 DEBUG TRAIN Batch 17/7300 loss 10.538815 loss_att 13.252821 loss_ctc 12.486893 loss_rnnt 9.606015 hw_loss 0.244226 lr 0.00040950 rank 4
2023-02-23 11:32:46,347 DEBUG TRAIN Batch 17/7300 loss 6.990546 loss_att 12.324325 loss_ctc 13.839676 loss_rnnt 4.898553 hw_loss 0.210037 lr 0.00040942 rank 5
2023-02-23 11:32:46,348 DEBUG TRAIN Batch 17/7300 loss 7.439816 loss_att 9.570684 loss_ctc 10.426019 loss_rnnt 6.524303 hw_loss 0.170961 lr 0.00040943 rank 3
2023-02-23 11:32:46,350 DEBUG TRAIN Batch 17/7300 loss 25.003195 loss_att 29.907272 loss_ctc 32.437157 loss_rnnt 22.914543 hw_loss 0.218702 lr 0.00040947 rank 2
2023-02-23 11:32:46,351 DEBUG TRAIN Batch 17/7300 loss 14.581224 loss_att 16.825827 loss_ctc 19.991941 loss_rnnt 13.296637 hw_loss 0.214198 lr 0.00040951 rank 0
2023-02-23 11:32:46,353 DEBUG TRAIN Batch 17/7300 loss 7.962841 loss_att 12.408974 loss_ctc 10.288135 loss_rnnt 6.630480 hw_loss 0.249554 lr 0.00040944 rank 7
2023-02-23 11:32:46,354 DEBUG TRAIN Batch 17/7300 loss 22.723965 loss_att 27.815830 loss_ctc 29.227541 loss_rnnt 20.761269 hw_loss 0.144712 lr 0.00040950 rank 6
2023-02-23 11:32:46,355 DEBUG TRAIN Batch 17/7300 loss 5.575327 loss_att 8.715296 loss_ctc 7.687634 loss_rnnt 4.567446 hw_loss 0.184211 lr 0.00040952 rank 1
2023-02-23 11:34:03,821 DEBUG TRAIN Batch 17/7400 loss 12.671019 loss_att 16.219858 loss_ctc 19.341202 loss_rnnt 10.962333 hw_loss 0.205426 lr 0.00040928 rank 5
2023-02-23 11:34:03,825 DEBUG TRAIN Batch 17/7400 loss 12.127651 loss_att 13.222506 loss_ctc 19.552965 loss_rnnt 10.778436 hw_loss 0.262881 lr 0.00040938 rank 0
2023-02-23 11:34:03,825 DEBUG TRAIN Batch 17/7400 loss 12.135974 loss_att 13.148478 loss_ctc 15.399324 loss_rnnt 11.347450 hw_loss 0.282955 lr 0.00040936 rank 4
2023-02-23 11:34:03,825 DEBUG TRAIN Batch 17/7400 loss 12.786989 loss_att 15.643165 loss_ctc 17.423441 loss_rnnt 11.514894 hw_loss 0.154998 lr 0.00040929 rank 3
2023-02-23 11:34:03,827 DEBUG TRAIN Batch 17/7400 loss 10.202692 loss_att 13.848343 loss_ctc 15.896814 loss_rnnt 8.578621 hw_loss 0.254483 lr 0.00040933 rank 2
2023-02-23 11:34:03,829 DEBUG TRAIN Batch 17/7400 loss 23.305862 loss_att 25.834023 loss_ctc 28.972542 loss_rnnt 21.905388 hw_loss 0.261159 lr 0.00040936 rank 6
2023-02-23 11:34:03,832 DEBUG TRAIN Batch 17/7400 loss 8.387505 loss_att 12.153047 loss_ctc 11.772489 loss_rnnt 7.101208 hw_loss 0.153482 lr 0.00040930 rank 7
2023-02-23 11:34:03,880 DEBUG TRAIN Batch 17/7400 loss 12.341547 loss_att 16.712154 loss_ctc 21.232119 loss_rnnt 10.238123 hw_loss 0.082300 lr 0.00040939 rank 1
2023-02-23 11:35:22,880 DEBUG TRAIN Batch 17/7500 loss 9.585229 loss_att 11.076723 loss_ctc 13.202311 loss_rnnt 8.680810 hw_loss 0.232203 lr 0.00040924 rank 0
2023-02-23 11:35:22,886 DEBUG TRAIN Batch 17/7500 loss 21.068781 loss_att 23.048470 loss_ctc 27.961084 loss_rnnt 19.642384 hw_loss 0.209033 lr 0.00040922 rank 6
2023-02-23 11:35:22,886 DEBUG TRAIN Batch 17/7500 loss 14.564249 loss_att 16.069078 loss_ctc 18.559940 loss_rnnt 13.598564 hw_loss 0.247426 lr 0.00040919 rank 2
2023-02-23 11:35:22,886 DEBUG TRAIN Batch 17/7500 loss 8.876154 loss_att 10.608932 loss_ctc 13.610736 loss_rnnt 7.800836 hw_loss 0.182783 lr 0.00040923 rank 4
2023-02-23 11:35:22,887 DEBUG TRAIN Batch 17/7500 loss 7.774911 loss_att 9.591632 loss_ctc 10.292064 loss_rnnt 6.975386 hw_loss 0.188551 lr 0.00040915 rank 5
2023-02-23 11:35:22,889 DEBUG TRAIN Batch 17/7500 loss 13.914597 loss_att 15.017781 loss_ctc 17.210794 loss_rnnt 13.114132 hw_loss 0.263126 lr 0.00040916 rank 3
2023-02-23 11:35:22,892 DEBUG TRAIN Batch 17/7500 loss 25.105282 loss_att 25.423161 loss_ctc 29.149103 loss_rnnt 24.359711 hw_loss 0.267787 lr 0.00040925 rank 1
2023-02-23 11:35:22,895 DEBUG TRAIN Batch 17/7500 loss 17.354519 loss_att 18.642902 loss_ctc 21.104877 loss_rnnt 16.491144 hw_loss 0.198094 lr 0.00040916 rank 7
2023-02-23 11:36:38,047 DEBUG TRAIN Batch 17/7600 loss 9.166832 loss_att 8.680978 loss_ctc 11.505387 loss_rnnt 8.783303 hw_loss 0.316671 lr 0.00040905 rank 2
2023-02-23 11:36:38,047 DEBUG TRAIN Batch 17/7600 loss 12.161329 loss_att 14.128979 loss_ctc 17.624872 loss_rnnt 10.912609 hw_loss 0.237597 lr 0.00040910 rank 0
2023-02-23 11:36:38,048 DEBUG TRAIN Batch 17/7600 loss 8.054030 loss_att 9.384693 loss_ctc 10.632004 loss_rnnt 7.403156 hw_loss 0.076898 lr 0.00040909 rank 6
2023-02-23 11:36:38,049 DEBUG TRAIN Batch 17/7600 loss 11.462807 loss_att 11.649029 loss_ctc 14.867711 loss_rnnt 10.822159 hw_loss 0.280156 lr 0.00040909 rank 4
2023-02-23 11:36:38,049 DEBUG TRAIN Batch 17/7600 loss 13.030384 loss_att 14.272954 loss_ctc 17.833870 loss_rnnt 12.041016 hw_loss 0.188228 lr 0.00040901 rank 5
2023-02-23 11:36:38,052 DEBUG TRAIN Batch 17/7600 loss 12.455440 loss_att 13.804057 loss_ctc 15.180239 loss_rnnt 11.671449 hw_loss 0.283054 lr 0.00040903 rank 7
2023-02-23 11:36:38,055 DEBUG TRAIN Batch 17/7600 loss 10.386663 loss_att 14.225590 loss_ctc 13.423368 loss_rnnt 9.089981 hw_loss 0.232506 lr 0.00040902 rank 3
2023-02-23 11:36:38,098 DEBUG TRAIN Batch 17/7600 loss 5.887709 loss_att 9.671267 loss_ctc 9.055903 loss_rnnt 4.604734 hw_loss 0.194693 lr 0.00040911 rank 1
2023-02-23 11:37:54,657 DEBUG TRAIN Batch 17/7700 loss 11.166423 loss_att 14.299079 loss_ctc 15.760287 loss_rnnt 9.845139 hw_loss 0.154195 lr 0.00040896 rank 0
2023-02-23 11:37:54,659 DEBUG TRAIN Batch 17/7700 loss 7.322228 loss_att 7.610008 loss_ctc 9.336766 loss_rnnt 6.822242 hw_loss 0.325922 lr 0.00040887 rank 5
2023-02-23 11:37:54,660 DEBUG TRAIN Batch 17/7700 loss 10.930451 loss_att 9.898233 loss_ctc 12.614405 loss_rnnt 10.849201 hw_loss 0.118438 lr 0.00040888 rank 3
2023-02-23 11:37:54,660 DEBUG TRAIN Batch 17/7700 loss 2.710262 loss_att 6.650818 loss_ctc 6.926613 loss_rnnt 1.244672 hw_loss 0.216185 lr 0.00040895 rank 6
2023-02-23 11:37:54,660 DEBUG TRAIN Batch 17/7700 loss 9.348595 loss_att 16.817589 loss_ctc 15.612651 loss_rnnt 6.949808 hw_loss 0.130839 lr 0.00040889 rank 7
2023-02-23 11:37:54,661 DEBUG TRAIN Batch 17/7700 loss 9.506161 loss_att 12.336134 loss_ctc 8.318077 loss_rnnt 8.918934 hw_loss 0.336831 lr 0.00040892 rank 2
2023-02-23 11:37:54,661 DEBUG TRAIN Batch 17/7700 loss 14.469570 loss_att 16.970873 loss_ctc 19.963415 loss_rnnt 13.138621 hw_loss 0.184079 lr 0.00040895 rank 4
2023-02-23 11:37:54,661 DEBUG TRAIN Batch 17/7700 loss 5.053547 loss_att 9.135048 loss_ctc 6.025273 loss_rnnt 3.979519 hw_loss 0.240309 lr 0.00040898 rank 1
2023-02-23 11:39:14,045 DEBUG TRAIN Batch 17/7800 loss 6.021870 loss_att 8.328161 loss_ctc 8.677612 loss_rnnt 5.097222 hw_loss 0.204920 lr 0.00040882 rank 4
2023-02-23 11:39:14,048 DEBUG TRAIN Batch 17/7800 loss 5.375570 loss_att 8.217693 loss_ctc 6.250428 loss_rnnt 4.590078 hw_loss 0.188286 lr 0.00040878 rank 2
2023-02-23 11:39:14,048 DEBUG TRAIN Batch 17/7800 loss 17.803463 loss_att 22.201645 loss_ctc 24.980503 loss_rnnt 15.909613 hw_loss 0.107395 lr 0.00040873 rank 5
2023-02-23 11:39:14,049 DEBUG TRAIN Batch 17/7800 loss 3.240225 loss_att 7.717803 loss_ctc 4.527332 loss_rnnt 2.094246 hw_loss 0.147843 lr 0.00040883 rank 0
2023-02-23 11:39:14,054 DEBUG TRAIN Batch 17/7800 loss 11.474575 loss_att 17.094425 loss_ctc 12.018152 loss_rnnt 10.169730 hw_loss 0.203245 lr 0.00040884 rank 1
2023-02-23 11:39:14,057 DEBUG TRAIN Batch 17/7800 loss 7.573692 loss_att 9.847447 loss_ctc 10.939592 loss_rnnt 6.549045 hw_loss 0.227081 lr 0.00040881 rank 6
2023-02-23 11:39:14,058 DEBUG TRAIN Batch 17/7800 loss 2.847349 loss_att 5.102916 loss_ctc 5.329529 loss_rnnt 1.915169 hw_loss 0.281455 lr 0.00040875 rank 7
2023-02-23 11:39:14,095 DEBUG TRAIN Batch 17/7800 loss 7.183559 loss_att 11.695801 loss_ctc 11.028759 loss_rnnt 5.650054 hw_loss 0.221930 lr 0.00040875 rank 3
2023-02-23 11:40:31,386 DEBUG TRAIN Batch 17/7900 loss 15.269800 loss_att 21.022167 loss_ctc 22.566101 loss_rnnt 13.016536 hw_loss 0.243659 lr 0.00040860 rank 5
2023-02-23 11:40:31,390 DEBUG TRAIN Batch 17/7900 loss 8.151348 loss_att 12.199465 loss_ctc 13.454554 loss_rnnt 6.520518 hw_loss 0.213961 lr 0.00040864 rank 2
2023-02-23 11:40:31,394 DEBUG TRAIN Batch 17/7900 loss 6.546288 loss_att 9.600378 loss_ctc 6.860385 loss_rnnt 5.785787 hw_loss 0.202130 lr 0.00040869 rank 0
2023-02-23 11:40:31,393 DEBUG TRAIN Batch 17/7900 loss 8.348926 loss_att 11.713066 loss_ctc 11.491037 loss_rnnt 7.185400 hw_loss 0.134529 lr 0.00040868 rank 4
2023-02-23 11:40:31,395 DEBUG TRAIN Batch 17/7900 loss 7.013894 loss_att 9.961596 loss_ctc 7.898521 loss_rnnt 6.229182 hw_loss 0.144789 lr 0.00040861 rank 3
2023-02-23 11:40:31,397 DEBUG TRAIN Batch 17/7900 loss 5.768322 loss_att 9.233768 loss_ctc 7.013452 loss_rnnt 4.792424 hw_loss 0.218982 lr 0.00040862 rank 7
2023-02-23 11:40:31,399 DEBUG TRAIN Batch 17/7900 loss 12.421187 loss_att 16.947330 loss_ctc 18.798092 loss_rnnt 10.536065 hw_loss 0.243072 lr 0.00040870 rank 1
2023-02-23 11:40:31,439 DEBUG TRAIN Batch 17/7900 loss 17.658436 loss_att 20.558764 loss_ctc 25.097157 loss_rnnt 16.000538 hw_loss 0.161255 lr 0.00040867 rank 6
2023-02-23 11:41:47,818 DEBUG TRAIN Batch 17/8000 loss 5.191387 loss_att 10.794241 loss_ctc 6.499874 loss_rnnt 3.786337 hw_loss 0.206277 lr 0.00040847 rank 3
2023-02-23 11:41:47,819 DEBUG TRAIN Batch 17/8000 loss 7.529464 loss_att 11.875319 loss_ctc 15.474585 loss_rnnt 5.504598 hw_loss 0.180647 lr 0.00040854 rank 4
2023-02-23 11:41:47,820 DEBUG TRAIN Batch 17/8000 loss 12.781547 loss_att 13.884402 loss_ctc 15.825272 loss_rnnt 12.023345 hw_loss 0.247126 lr 0.00040846 rank 5
2023-02-23 11:41:47,821 DEBUG TRAIN Batch 17/8000 loss 8.894957 loss_att 12.025118 loss_ctc 13.496050 loss_rnnt 7.525252 hw_loss 0.244113 lr 0.00040851 rank 2
2023-02-23 11:41:47,826 DEBUG TRAIN Batch 17/8000 loss 6.478011 loss_att 10.870311 loss_ctc 9.434343 loss_rnnt 5.072895 hw_loss 0.248396 lr 0.00040855 rank 0
2023-02-23 11:41:47,827 DEBUG TRAIN Batch 17/8000 loss 12.827317 loss_att 14.442533 loss_ctc 21.569838 loss_rnnt 11.255948 hw_loss 0.154980 lr 0.00040848 rank 7
2023-02-23 11:41:47,844 DEBUG TRAIN Batch 17/8000 loss 8.184721 loss_att 10.637651 loss_ctc 13.357641 loss_rnnt 6.836529 hw_loss 0.314783 lr 0.00040854 rank 6
2023-02-23 11:41:47,859 DEBUG TRAIN Batch 17/8000 loss 11.438697 loss_att 11.834084 loss_ctc 12.791971 loss_rnnt 11.088717 hw_loss 0.169624 lr 0.00040857 rank 1
2023-02-23 11:43:06,029 DEBUG TRAIN Batch 17/8100 loss 10.843886 loss_att 13.522512 loss_ctc 16.491343 loss_rnnt 9.401493 hw_loss 0.288137 lr 0.00040842 rank 0
2023-02-23 11:43:06,029 DEBUG TRAIN Batch 17/8100 loss 16.201319 loss_att 16.286415 loss_ctc 22.037262 loss_rnnt 15.293278 hw_loss 0.211676 lr 0.00040843 rank 1
2023-02-23 11:43:06,030 DEBUG TRAIN Batch 17/8100 loss 14.949381 loss_att 19.902435 loss_ctc 22.312811 loss_rnnt 12.886559 hw_loss 0.169535 lr 0.00040833 rank 5
2023-02-23 11:43:06,031 DEBUG TRAIN Batch 17/8100 loss 4.321680 loss_att 7.509221 loss_ctc 7.304054 loss_rnnt 3.187387 hw_loss 0.185878 lr 0.00040841 rank 4
2023-02-23 11:43:06,031 DEBUG TRAIN Batch 17/8100 loss 12.605015 loss_att 16.983021 loss_ctc 14.364397 loss_rnnt 11.394795 hw_loss 0.187563 lr 0.00040840 rank 6
2023-02-23 11:43:06,033 DEBUG TRAIN Batch 17/8100 loss 4.578747 loss_att 8.872078 loss_ctc 6.286008 loss_rnnt 3.388773 hw_loss 0.194389 lr 0.00040837 rank 2
2023-02-23 11:43:06,036 DEBUG TRAIN Batch 17/8100 loss 12.926066 loss_att 16.201530 loss_ctc 19.597570 loss_rnnt 11.185957 hw_loss 0.366533 lr 0.00040834 rank 7
2023-02-23 11:43:06,038 DEBUG TRAIN Batch 17/8100 loss 12.214269 loss_att 14.904506 loss_ctc 15.441042 loss_rnnt 11.153679 hw_loss 0.173072 lr 0.00040834 rank 3
2023-02-23 11:44:23,116 DEBUG TRAIN Batch 17/8200 loss 22.233429 loss_att 24.402710 loss_ctc 29.830780 loss_rnnt 20.643257 hw_loss 0.268754 lr 0.00040819 rank 5
2023-02-23 11:44:23,118 DEBUG TRAIN Batch 17/8200 loss 5.862225 loss_att 8.086615 loss_ctc 8.706114 loss_rnnt 4.906325 hw_loss 0.247193 lr 0.00040827 rank 6
2023-02-23 11:44:23,118 DEBUG TRAIN Batch 17/8200 loss 7.417099 loss_att 9.012007 loss_ctc 10.066437 loss_rnnt 6.662787 hw_loss 0.153910 lr 0.00040827 rank 4
2023-02-23 11:44:23,120 DEBUG TRAIN Batch 17/8200 loss 17.988104 loss_att 18.806217 loss_ctc 27.046942 loss_rnnt 16.489273 hw_loss 0.238806 lr 0.00040828 rank 0
2023-02-23 11:44:23,123 DEBUG TRAIN Batch 17/8200 loss 11.906608 loss_att 13.211968 loss_ctc 14.399450 loss_rnnt 11.141616 hw_loss 0.321638 lr 0.00040824 rank 2
2023-02-23 11:44:23,123 DEBUG TRAIN Batch 17/8200 loss 5.359428 loss_att 8.166795 loss_ctc 7.806358 loss_rnnt 4.331475 hw_loss 0.262919 lr 0.00040820 rank 3
2023-02-23 11:44:23,126 DEBUG TRAIN Batch 17/8200 loss 8.672892 loss_att 11.556202 loss_ctc 11.657100 loss_rnnt 7.585186 hw_loss 0.212155 lr 0.00040821 rank 7
2023-02-23 11:44:23,171 DEBUG TRAIN Batch 17/8200 loss 17.921253 loss_att 19.584362 loss_ctc 23.729185 loss_rnnt 16.716734 hw_loss 0.182826 lr 0.00040829 rank 1
2023-02-23 11:45:37,517 DEBUG TRAIN Batch 17/8300 loss 8.029162 loss_att 9.870291 loss_ctc 10.276748 loss_rnnt 7.286010 hw_loss 0.141093 lr 0.00040810 rank 2
2023-02-23 11:45:37,520 DEBUG TRAIN Batch 17/8300 loss 8.831125 loss_att 11.019064 loss_ctc 11.758942 loss_rnnt 7.836036 hw_loss 0.313364 lr 0.00040815 rank 0
2023-02-23 11:45:37,521 DEBUG TRAIN Batch 17/8300 loss 1.118264 loss_att 3.089072 loss_ctc 1.127627 loss_rnnt 0.617786 hw_loss 0.197003 lr 0.00040805 rank 5
2023-02-23 11:45:37,522 DEBUG TRAIN Batch 17/8300 loss 6.310081 loss_att 7.924793 loss_ctc 8.404268 loss_rnnt 5.594331 hw_loss 0.212965 lr 0.00040806 rank 3
2023-02-23 11:45:37,522 DEBUG TRAIN Batch 17/8300 loss 8.921912 loss_att 12.836718 loss_ctc 10.987679 loss_rnnt 7.762717 hw_loss 0.188998 lr 0.00040813 rank 4
2023-02-23 11:45:37,525 DEBUG TRAIN Batch 17/8300 loss 7.319387 loss_att 8.057267 loss_ctc 9.488834 loss_rnnt 6.723053 hw_loss 0.299058 lr 0.00040807 rank 7
2023-02-23 11:45:37,528 DEBUG TRAIN Batch 17/8300 loss 7.431701 loss_att 9.655573 loss_ctc 6.086208 loss_rnnt 7.043025 hw_loss 0.231188 lr 0.00040813 rank 6
2023-02-23 11:45:37,576 DEBUG TRAIN Batch 17/8300 loss 20.748955 loss_att 21.792583 loss_ctc 24.260061 loss_rnnt 19.938267 hw_loss 0.250904 lr 0.00040816 rank 1
2023-02-23 11:46:24,377 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 7
2023-02-23 11:46:24,377 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 0
2023-02-23 11:46:24,382 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 1
2023-02-23 11:46:24,384 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 5
2023-02-23 11:46:24,388 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 3
2023-02-23 11:46:24,396 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 6
2023-02-23 11:46:24,410 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 4
2023-02-23 11:46:24,411 DEBUG CV Batch 17/0 loss 2.181356 loss_att 2.266192 loss_ctc 3.165822 loss_rnnt 1.862660 hw_loss 0.319625 history loss 2.100565 rank 2
2023-02-23 11:46:35,406 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 2
2023-02-23 11:46:35,496 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 5
2023-02-23 11:46:35,516 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 3
2023-02-23 11:46:35,540 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 4
2023-02-23 11:46:35,603 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 0
2023-02-23 11:46:35,619 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 1
2023-02-23 11:46:35,787 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 6
2023-02-23 11:46:35,790 DEBUG CV Batch 17/100 loss 7.230647 loss_att 6.972927 loss_ctc 8.679563 loss_rnnt 6.924591 hw_loss 0.308273 history loss 3.538810 rank 7
2023-02-23 11:46:49,286 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 5
2023-02-23 11:46:49,350 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 2
2023-02-23 11:46:49,353 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 0
2023-02-23 11:46:49,544 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 4
2023-02-23 11:46:49,636 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 1
2023-02-23 11:46:49,688 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 3
2023-02-23 11:46:49,823 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 6
2023-02-23 11:46:49,933 DEBUG CV Batch 17/200 loss 8.494834 loss_att 14.082392 loss_ctc 8.347179 loss_rnnt 7.278588 hw_loss 0.222038 history loss 4.196343 rank 7
2023-02-23 11:47:01,198 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 5
2023-02-23 11:47:01,373 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 2
2023-02-23 11:47:01,380 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 0
2023-02-23 11:47:01,664 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 4
2023-02-23 11:47:01,794 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 3
2023-02-23 11:47:01,841 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 1
2023-02-23 11:47:02,010 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 6
2023-02-23 11:47:02,173 DEBUG CV Batch 17/300 loss 4.919729 loss_att 4.803720 loss_ctc 6.882334 loss_rnnt 4.527956 hw_loss 0.287428 history loss 4.346963 rank 7
2023-02-23 11:47:12,969 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 5
2023-02-23 11:47:13,319 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 2
2023-02-23 11:47:13,350 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 0
2023-02-23 11:47:13,645 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 4
2023-02-23 11:47:13,858 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 3
2023-02-23 11:47:14,115 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 6
2023-02-23 11:47:14,331 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 7
2023-02-23 11:47:14,862 DEBUG CV Batch 17/400 loss 12.734660 loss_att 68.070511 loss_ctc 7.842792 loss_rnnt 2.288982 hw_loss 0.057668 history loss 5.388928 rank 1
2023-02-23 11:47:23,429 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 5
2023-02-23 11:47:23,731 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 2
2023-02-23 11:47:23,845 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 0
2023-02-23 11:47:24,082 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 4
2023-02-23 11:47:24,301 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 3
2023-02-23 11:47:24,747 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 6
2023-02-23 11:47:25,059 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 7
2023-02-23 11:47:25,495 DEBUG CV Batch 17/500 loss 5.190526 loss_att 6.050574 loss_ctc 8.843596 loss_rnnt 4.395506 hw_loss 0.254877 history loss 6.198557 rank 1
2023-02-23 11:47:35,452 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 5
2023-02-23 11:47:35,731 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 2
2023-02-23 11:47:35,943 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 0
2023-02-23 11:47:36,143 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 4
2023-02-23 11:47:36,531 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 3
2023-02-23 11:47:37,372 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 7
2023-02-23 11:47:37,605 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 6
2023-02-23 11:47:37,667 DEBUG CV Batch 17/600 loss 8.043514 loss_att 8.727079 loss_ctc 10.371424 loss_rnnt 7.445683 hw_loss 0.282621 history loss 7.175237 rank 1
2023-02-23 11:47:47,252 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 0
2023-02-23 11:47:47,343 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 2
2023-02-23 11:47:47,588 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 5
2023-02-23 11:47:47,638 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 4
2023-02-23 11:47:48,912 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 3
2023-02-23 11:47:49,088 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 1
2023-02-23 11:47:49,269 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 6
2023-02-23 11:47:49,537 DEBUG CV Batch 17/700 loss 13.684025 loss_att 43.880440 loss_ctc 14.595972 loss_rnnt 7.399914 hw_loss 0.231066 history loss 7.895248 rank 7
2023-02-23 11:47:59,140 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 2
2023-02-23 11:47:59,240 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 0
2023-02-23 11:47:59,568 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 5
2023-02-23 11:47:59,767 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 4
2023-02-23 11:48:01,020 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 3
2023-02-23 11:48:01,478 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 1
2023-02-23 11:48:01,734 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 7
2023-02-23 11:48:01,774 DEBUG CV Batch 17/800 loss 10.434017 loss_att 10.124893 loss_ctc 13.852343 loss_rnnt 9.914925 hw_loss 0.234640 history loss 7.306317 rank 6
2023-02-23 11:48:13,139 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 2
2023-02-23 11:48:13,141 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 0
2023-02-23 11:48:13,418 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 5
2023-02-23 11:48:13,582 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 4
2023-02-23 11:48:15,108 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 3
2023-02-23 11:48:15,200 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 1
2023-02-23 11:48:15,611 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 7
2023-02-23 11:48:16,189 DEBUG CV Batch 17/900 loss 15.119866 loss_att 19.437996 loss_ctc 25.528076 loss_rnnt 12.798979 hw_loss 0.130311 history loss 7.099449 rank 6
2023-02-23 11:48:25,264 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 2
2023-02-23 11:48:25,394 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 0
2023-02-23 11:48:25,534 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 5
2023-02-23 11:48:25,753 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 4
2023-02-23 11:48:27,332 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 3
2023-02-23 11:48:27,591 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 1
2023-02-23 11:48:27,979 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 7
2023-02-23 11:48:28,644 DEBUG CV Batch 17/1000 loss 4.086695 loss_att 4.900477 loss_ctc 4.752443 loss_rnnt 3.708592 hw_loss 0.237336 history loss 6.845032 rank 6
2023-02-23 11:48:37,069 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 2
2023-02-23 11:48:37,324 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 5
2023-02-23 11:48:37,414 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 0
2023-02-23 11:48:37,580 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 4
2023-02-23 11:48:39,268 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 3
2023-02-23 11:48:39,605 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 1
2023-02-23 11:48:39,986 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 7
2023-02-23 11:48:40,675 DEBUG CV Batch 17/1100 loss 5.615525 loss_att 5.008384 loss_ctc 7.052022 loss_rnnt 5.390419 hw_loss 0.290629 history loss 6.831689 rank 6
2023-02-23 11:48:47,452 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 2
2023-02-23 11:48:47,655 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 5
2023-02-23 11:48:47,848 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 0
2023-02-23 11:48:48,063 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 4
2023-02-23 11:48:49,760 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 3
2023-02-23 11:48:50,158 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 1
2023-02-23 11:48:50,754 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 7
2023-02-23 11:48:51,754 DEBUG CV Batch 17/1200 loss 9.713127 loss_att 10.105077 loss_ctc 12.995186 loss_rnnt 9.064692 hw_loss 0.248320 history loss 7.182273 rank 6
2023-02-23 11:48:59,371 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 2
2023-02-23 11:48:59,521 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 5
2023-02-23 11:48:59,835 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 0
2023-02-23 11:49:00,037 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 4
2023-02-23 11:49:01,795 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 3
2023-02-23 11:49:02,149 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 1
2023-02-23 11:49:02,818 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 7
2023-02-23 11:49:03,898 DEBUG CV Batch 17/1300 loss 5.579065 loss_att 6.283243 loss_ctc 7.887587 loss_rnnt 5.010961 hw_loss 0.223997 history loss 7.499706 rank 6
2023-02-23 11:49:10,658 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 2
2023-02-23 11:49:10,899 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 5
2023-02-23 11:49:11,002 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 0
2023-02-23 11:49:11,408 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 4
2023-02-23 11:49:13,815 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 3
2023-02-23 11:49:14,693 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 7
2023-02-23 11:49:15,116 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 1
2023-02-23 11:49:15,239 DEBUG CV Batch 17/1400 loss 4.696669 loss_att 15.930494 loss_ctc 4.047393 loss_rnnt 2.500676 hw_loss 0.067120 history loss 7.854535 rank 6
2023-02-23 11:49:23,075 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 0
2023-02-23 11:49:23,094 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 2
2023-02-23 11:49:23,302 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 5
2023-02-23 11:49:24,115 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 4
2023-02-23 11:49:26,365 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 3
2023-02-23 11:49:26,997 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 1
2023-02-23 11:49:27,309 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 7
2023-02-23 11:49:28,253 DEBUG CV Batch 17/1500 loss 6.291877 loss_att 7.400914 loss_ctc 6.619530 loss_rnnt 5.930709 hw_loss 0.179387 history loss 7.650926 rank 6
2023-02-23 11:49:36,610 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 2
2023-02-23 11:49:36,856 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 0
2023-02-23 11:49:36,982 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 5
2023-02-23 11:49:38,101 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 4
2023-02-23 11:49:40,284 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 3
2023-02-23 11:49:40,756 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 1
2023-02-23 11:49:40,938 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 7
2023-02-23 11:49:42,858 DEBUG CV Batch 17/1600 loss 8.864015 loss_att 11.383759 loss_ctc 8.858237 loss_rnnt 8.258577 hw_loss 0.191733 history loss 7.568468 rank 6
2023-02-23 11:49:49,038 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 2
2023-02-23 11:49:49,355 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 0
2023-02-23 11:49:49,383 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 5
2023-02-23 11:49:50,566 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 4
2023-02-23 11:49:52,811 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 3
2023-02-23 11:49:53,250 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 1
2023-02-23 11:49:53,541 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 7
2023-02-23 11:49:55,502 DEBUG CV Batch 17/1700 loss 10.539967 loss_att 9.480121 loss_ctc 16.476849 loss_rnnt 9.818430 hw_loss 0.266104 history loss 7.451622 rank 6
2023-02-23 11:49:58,300 INFO Epoch 17 CV info cv_loss 7.411524229493595
2023-02-23 11:49:58,304 INFO Epoch 18 TRAIN info lr 0.00040804024297853137
2023-02-23 11:49:58,307 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:49:58,623 INFO Epoch 17 CV info cv_loss 7.411524229017636
2023-02-23 11:49:58,624 INFO Epoch 18 TRAIN info lr 0.00040803480809423773
2023-02-23 11:49:58,628 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:49:58,705 INFO Epoch 17 CV info cv_loss 7.411524229103782
2023-02-23 11:49:58,706 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/17.pt
2023-02-23 11:49:59,726 INFO Epoch 17 CV info cv_loss 7.411524228759197
2023-02-23 11:49:59,727 INFO Epoch 18 TRAIN info lr 0.00040806606164501267
2023-02-23 11:49:59,730 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:50:01,463 INFO Epoch 18 TRAIN info lr 0.0004081000410427048
2023-02-23 11:50:01,468 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:50:02,016 INFO Epoch 17 CV info cv_loss 7.411524228373692
2023-02-23 11:50:02,018 INFO Epoch 18 TRAIN info lr 0.0004080185047442814
2023-02-23 11:50:02,022 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:50:02,497 INFO Epoch 17 CV info cv_loss 7.411524230507967
2023-02-23 11:50:02,498 INFO Epoch 18 TRAIN info lr 0.00040809188521312754
2023-02-23 11:50:02,503 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:50:02,879 INFO Epoch 17 CV info cv_loss 7.411524228604134
2023-02-23 11:50:02,880 INFO Epoch 18 TRAIN info lr 0.0004080239389771272
2023-02-23 11:50:02,883 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:50:04,749 INFO Epoch 17 CV info cv_loss 7.411524227846046
2023-02-23 11:50:04,750 INFO Epoch 18 TRAIN info lr 0.00040806062572899376
2023-02-23 11:50:04,754 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:51:16,383 DEBUG TRAIN Batch 18/0 loss 10.158658 loss_att 9.531010 loss_ctc 12.635005 loss_rnnt 9.782404 hw_loss 0.321758 lr 0.00040803 rank 5
2023-02-23 11:51:16,390 DEBUG TRAIN Batch 18/0 loss 11.160991 loss_att 10.175821 loss_ctc 12.746471 loss_rnnt 10.986774 hw_loss 0.299726 lr 0.00040806 rank 4
2023-02-23 11:51:16,390 DEBUG TRAIN Batch 18/0 loss 9.197503 loss_att 10.125139 loss_ctc 11.961508 loss_rnnt 8.484444 hw_loss 0.298121 lr 0.00040804 rank 2
2023-02-23 11:51:16,391 DEBUG TRAIN Batch 18/0 loss 12.507222 loss_att 11.926520 loss_ctc 16.881969 loss_rnnt 11.872380 hw_loss 0.314406 lr 0.00040810 rank 0
2023-02-23 11:51:16,444 DEBUG TRAIN Batch 18/0 loss 8.770829 loss_att 8.498581 loss_ctc 9.996094 loss_rnnt 8.489108 hw_loss 0.324003 lr 0.00040806 rank 6
2023-02-23 11:51:16,452 DEBUG TRAIN Batch 18/0 loss 15.064219 loss_att 13.791296 loss_ctc 18.955866 loss_rnnt 14.638996 hw_loss 0.301723 lr 0.00040809 rank 1
2023-02-23 11:51:16,467 DEBUG TRAIN Batch 18/0 loss 8.783100 loss_att 7.843461 loss_ctc 10.426326 loss_rnnt 8.576790 hw_loss 0.328390 lr 0.00040802 rank 3
2023-02-23 11:51:16,518 DEBUG TRAIN Batch 18/0 loss 11.914999 loss_att 11.167913 loss_ctc 14.846242 loss_rnnt 11.481081 hw_loss 0.360942 lr 0.00040802 rank 7
2023-02-23 11:52:31,071 DEBUG TRAIN Batch 18/100 loss 3.027046 loss_att 7.045106 loss_ctc 4.614365 loss_rnnt 1.917047 hw_loss 0.177647 lr 0.00040790 rank 5
2023-02-23 11:52:31,073 DEBUG TRAIN Batch 18/100 loss 5.181322 loss_att 7.995056 loss_ctc 8.641945 loss_rnnt 4.002851 hw_loss 0.289327 lr 0.00040790 rank 2
2023-02-23 11:52:31,076 DEBUG TRAIN Batch 18/100 loss 8.793143 loss_att 12.253636 loss_ctc 12.273079 loss_rnnt 7.584798 hw_loss 0.097978 lr 0.00040795 rank 1
2023-02-23 11:52:31,077 DEBUG TRAIN Batch 18/100 loss 6.706594 loss_att 10.868097 loss_ctc 10.239905 loss_rnnt 5.275079 hw_loss 0.240200 lr 0.00040796 rank 0
2023-02-23 11:52:31,077 DEBUG TRAIN Batch 18/100 loss 8.963033 loss_att 11.826565 loss_ctc 16.599148 loss_rnnt 7.298770 hw_loss 0.137640 lr 0.00040792 rank 6
2023-02-23 11:52:31,079 DEBUG TRAIN Batch 18/100 loss 5.291493 loss_att 8.758479 loss_ctc 8.135069 loss_rnnt 4.120757 hw_loss 0.184117 lr 0.00040793 rank 4
2023-02-23 11:52:31,079 DEBUG TRAIN Batch 18/100 loss 9.954077 loss_att 11.745458 loss_ctc 14.122005 loss_rnnt 8.959689 hw_loss 0.150725 lr 0.00040788 rank 3
2023-02-23 11:52:31,085 DEBUG TRAIN Batch 18/100 loss 9.058531 loss_att 12.913621 loss_ctc 11.803718 loss_rnnt 7.814802 hw_loss 0.200038 lr 0.00040789 rank 7
2023-02-23 11:53:46,367 DEBUG TRAIN Batch 18/200 loss 3.484054 loss_att 9.055374 loss_ctc 6.041217 loss_rnnt 1.855076 hw_loss 0.325797 lr 0.00040779 rank 4
2023-02-23 11:53:46,370 DEBUG TRAIN Batch 18/200 loss 10.998244 loss_att 14.664153 loss_ctc 11.784060 loss_rnnt 10.009628 hw_loss 0.282486 lr 0.00040779 rank 6
2023-02-23 11:53:46,371 DEBUG TRAIN Batch 18/200 loss 7.915396 loss_att 12.625414 loss_ctc 14.435035 loss_rnnt 5.991567 hw_loss 0.211012 lr 0.00040777 rank 2
2023-02-23 11:53:46,371 DEBUG TRAIN Batch 18/200 loss 5.989159 loss_att 8.292689 loss_ctc 9.273417 loss_rnnt 5.001013 hw_loss 0.167886 lr 0.00040776 rank 5
2023-02-23 11:53:46,371 DEBUG TRAIN Batch 18/200 loss 13.241990 loss_att 16.821550 loss_ctc 16.839436 loss_rnnt 11.921303 hw_loss 0.234592 lr 0.00040783 rank 0
2023-02-23 11:53:46,371 DEBUG TRAIN Batch 18/200 loss 22.674662 loss_att 26.622997 loss_ctc 34.150261 loss_rnnt 20.223721 hw_loss 0.245985 lr 0.00040775 rank 3
2023-02-23 11:53:46,375 DEBUG TRAIN Batch 18/200 loss 4.858222 loss_att 7.053488 loss_ctc 5.654936 loss_rnnt 4.251841 hw_loss 0.114562 lr 0.00040782 rank 1
2023-02-23 11:53:46,377 DEBUG TRAIN Batch 18/200 loss 12.896102 loss_att 16.015097 loss_ctc 16.338024 loss_rnnt 11.649420 hw_loss 0.307426 lr 0.00040775 rank 7
2023-02-23 11:55:03,909 DEBUG TRAIN Batch 18/300 loss 7.144125 loss_att 12.289044 loss_ctc 9.351191 loss_rnnt 5.711457 hw_loss 0.205139 lr 0.00040763 rank 5
2023-02-23 11:55:03,910 DEBUG TRAIN Batch 18/300 loss 9.807117 loss_att 11.010679 loss_ctc 12.943293 loss_rnnt 8.991120 hw_loss 0.294613 lr 0.00040763 rank 2
2023-02-23 11:55:03,911 DEBUG TRAIN Batch 18/300 loss 19.157297 loss_att 25.242895 loss_ctc 22.574326 loss_rnnt 17.374054 hw_loss 0.207223 lr 0.00040769 rank 0
2023-02-23 11:55:03,913 DEBUG TRAIN Batch 18/300 loss 2.837501 loss_att 5.308762 loss_ctc 3.637296 loss_rnnt 2.143930 hw_loss 0.173771 lr 0.00040765 rank 6
2023-02-23 11:55:03,913 DEBUG TRAIN Batch 18/300 loss 9.485419 loss_att 11.462280 loss_ctc 10.957181 loss_rnnt 8.813514 hw_loss 0.150558 lr 0.00040762 rank 7
2023-02-23 11:55:03,915 DEBUG TRAIN Batch 18/300 loss 10.324108 loss_att 12.954076 loss_ctc 11.483484 loss_rnnt 9.548025 hw_loss 0.179074 lr 0.00040761 rank 3
2023-02-23 11:55:03,919 DEBUG TRAIN Batch 18/300 loss 4.104096 loss_att 8.212239 loss_ctc 3.258996 loss_rnnt 3.234368 hw_loss 0.301463 lr 0.00040768 rank 1
2023-02-23 11:55:03,919 DEBUG TRAIN Batch 18/300 loss 6.340916 loss_att 8.417242 loss_ctc 5.704398 loss_rnnt 5.913021 hw_loss 0.182809 lr 0.00040766 rank 4
2023-02-23 11:56:20,968 DEBUG TRAIN Batch 18/400 loss 9.444112 loss_att 11.349881 loss_ctc 13.128221 loss_rnnt 8.404863 hw_loss 0.312901 lr 0.00040752 rank 6
2023-02-23 11:56:20,969 DEBUG TRAIN Batch 18/400 loss 21.338282 loss_att 23.507294 loss_ctc 27.557623 loss_rnnt 19.994511 hw_loss 0.151353 lr 0.00040752 rank 4
2023-02-23 11:56:20,971 DEBUG TRAIN Batch 18/400 loss 16.574682 loss_att 18.541267 loss_ctc 18.787334 loss_rnnt 15.764548 hw_loss 0.228369 lr 0.00040749 rank 5
2023-02-23 11:56:20,971 DEBUG TRAIN Batch 18/400 loss 5.354123 loss_att 9.222374 loss_ctc 9.161949 loss_rnnt 3.933424 hw_loss 0.261259 lr 0.00040756 rank 0
2023-02-23 11:56:20,973 DEBUG TRAIN Batch 18/400 loss 4.114307 loss_att 6.004458 loss_ctc 5.758326 loss_rnnt 3.389849 hw_loss 0.238546 lr 0.00040750 rank 2
2023-02-23 11:56:20,978 DEBUG TRAIN Batch 18/400 loss 13.559783 loss_att 15.533560 loss_ctc 15.280941 loss_rnnt 12.826469 hw_loss 0.204506 lr 0.00040747 rank 3
2023-02-23 11:56:20,978 DEBUG TRAIN Batch 18/400 loss 8.531959 loss_att 10.110824 loss_ctc 12.408609 loss_rnnt 7.530118 hw_loss 0.317214 lr 0.00040748 rank 7
2023-02-23 11:56:20,978 DEBUG TRAIN Batch 18/400 loss 11.505265 loss_att 12.916936 loss_ctc 13.170197 loss_rnnt 10.896021 hw_loss 0.196724 lr 0.00040755 rank 1
2023-02-23 11:57:35,909 DEBUG TRAIN Batch 18/500 loss 15.875044 loss_att 20.043705 loss_ctc 20.291784 loss_rnnt 14.290926 hw_loss 0.302787 lr 0.00040739 rank 4
2023-02-23 11:57:35,916 DEBUG TRAIN Batch 18/500 loss 12.457418 loss_att 15.663307 loss_ctc 15.065760 loss_rnnt 11.356762 hw_loss 0.209438 lr 0.00040742 rank 0
2023-02-23 11:57:35,916 DEBUG TRAIN Batch 18/500 loss 10.108151 loss_att 11.247898 loss_ctc 14.057384 loss_rnnt 9.214849 hw_loss 0.260227 lr 0.00040734 rank 3
2023-02-23 11:57:35,917 DEBUG TRAIN Batch 18/500 loss 13.534861 loss_att 15.664255 loss_ctc 19.257559 loss_rnnt 12.197360 hw_loss 0.278616 lr 0.00040736 rank 2
2023-02-23 11:57:35,919 DEBUG TRAIN Batch 18/500 loss 8.957662 loss_att 10.547917 loss_ctc 14.101358 loss_rnnt 7.804836 hw_loss 0.279279 lr 0.00040738 rank 6
2023-02-23 11:57:35,918 DEBUG TRAIN Batch 18/500 loss 8.933064 loss_att 12.065414 loss_ctc 10.622150 loss_rnnt 7.985370 hw_loss 0.180023 lr 0.00040736 rank 5
2023-02-23 11:57:35,921 DEBUG TRAIN Batch 18/500 loss 6.934113 loss_att 10.086536 loss_ctc 11.454578 loss_rnnt 5.642498 hw_loss 0.109504 lr 0.00040741 rank 1
2023-02-23 11:57:35,921 DEBUG TRAIN Batch 18/500 loss 15.542566 loss_att 19.438997 loss_ctc 19.806084 loss_rnnt 14.083509 hw_loss 0.208692 lr 0.00040734 rank 7
2023-02-23 11:58:52,662 DEBUG TRAIN Batch 18/600 loss 12.045367 loss_att 14.071773 loss_ctc 14.206243 loss_rnnt 11.190488 hw_loss 0.302779 lr 0.00040729 rank 0
2023-02-23 11:58:52,662 DEBUG TRAIN Batch 18/600 loss 6.865096 loss_att 7.278953 loss_ctc 8.791782 loss_rnnt 6.288970 hw_loss 0.443370 lr 0.00040725 rank 4
2023-02-23 11:58:52,670 DEBUG TRAIN Batch 18/600 loss 9.556023 loss_att 9.066378 loss_ctc 12.139274 loss_rnnt 9.142100 hw_loss 0.313910 lr 0.00040723 rank 2
2023-02-23 11:58:52,669 DEBUG TRAIN Batch 18/600 loss 7.904109 loss_att 8.355536 loss_ctc 10.168309 loss_rnnt 7.272658 hw_loss 0.448634 lr 0.00040720 rank 3
2023-02-23 11:58:52,670 DEBUG TRAIN Batch 18/600 loss 10.309550 loss_att 12.142114 loss_ctc 15.182234 loss_rnnt 9.133355 hw_loss 0.299984 lr 0.00040721 rank 7
2023-02-23 11:58:52,670 DEBUG TRAIN Batch 18/600 loss 9.730677 loss_att 11.036598 loss_ctc 13.159236 loss_rnnt 8.845683 hw_loss 0.312502 lr 0.00040722 rank 5
2023-02-23 11:58:52,672 DEBUG TRAIN Batch 18/600 loss 15.085498 loss_att 14.958149 loss_ctc 19.987453 loss_rnnt 14.312505 hw_loss 0.271629 lr 0.00040725 rank 6
2023-02-23 11:58:52,672 DEBUG TRAIN Batch 18/600 loss 12.559730 loss_att 11.205471 loss_ctc 16.945858 loss_rnnt 12.120337 hw_loss 0.235179 lr 0.00040728 rank 1
2023-02-23 12:00:11,370 DEBUG TRAIN Batch 18/700 loss 24.338118 loss_att 30.261547 loss_ctc 32.724625 loss_rnnt 21.992340 hw_loss 0.080419 lr 0.00040707 rank 3
2023-02-23 12:00:11,369 DEBUG TRAIN Batch 18/700 loss 6.187035 loss_att 9.536247 loss_ctc 11.951053 loss_rnnt 4.600060 hw_loss 0.278617 lr 0.00040709 rank 2
2023-02-23 12:00:11,370 DEBUG TRAIN Batch 18/700 loss 13.291505 loss_att 14.511456 loss_ctc 14.759933 loss_rnnt 12.754395 hw_loss 0.182493 lr 0.00040712 rank 4
2023-02-23 12:00:11,373 DEBUG TRAIN Batch 18/700 loss 9.388638 loss_att 13.187593 loss_ctc 17.297049 loss_rnnt 7.492702 hw_loss 0.153169 lr 0.00040715 rank 0
2023-02-23 12:00:11,374 DEBUG TRAIN Batch 18/700 loss 34.020969 loss_att 63.893929 loss_ctc 31.462675 loss_rnnt 28.252281 hw_loss 0.253507 lr 0.00040711 rank 6
2023-02-23 12:00:11,376 DEBUG TRAIN Batch 18/700 loss 7.792222 loss_att 11.177689 loss_ctc 8.495187 loss_rnnt 6.942904 hw_loss 0.147179 lr 0.00040707 rank 7
2023-02-23 12:00:11,378 DEBUG TRAIN Batch 18/700 loss 3.473669 loss_att 7.916216 loss_ctc 5.056085 loss_rnnt 2.262090 hw_loss 0.210152 lr 0.00040709 rank 5
2023-02-23 12:00:11,378 DEBUG TRAIN Batch 18/700 loss 4.674496 loss_att 7.914687 loss_ctc 5.466720 loss_rnnt 3.723580 hw_loss 0.369839 lr 0.00040714 rank 1
2023-02-23 12:01:26,799 DEBUG TRAIN Batch 18/800 loss 4.198749 loss_att 7.220853 loss_ctc 7.090061 loss_rnnt 3.049919 hw_loss 0.297939 lr 0.00040695 rank 5
2023-02-23 12:01:26,801 DEBUG TRAIN Batch 18/800 loss 3.889321 loss_att 5.982552 loss_ctc 5.505171 loss_rnnt 3.108053 hw_loss 0.275953 lr 0.00040693 rank 3
2023-02-23 12:01:26,803 DEBUG TRAIN Batch 18/800 loss 2.632770 loss_att 5.804503 loss_ctc 4.278591 loss_rnnt 1.704625 hw_loss 0.139416 lr 0.00040701 rank 1
2023-02-23 12:01:26,805 DEBUG TRAIN Batch 18/800 loss 12.627644 loss_att 17.934591 loss_ctc 16.328430 loss_rnnt 10.961777 hw_loss 0.208196 lr 0.00040702 rank 0
2023-02-23 12:01:26,806 DEBUG TRAIN Batch 18/800 loss 10.363498 loss_att 13.999711 loss_ctc 12.710611 loss_rnnt 9.223410 hw_loss 0.187307 lr 0.00040698 rank 6
2023-02-23 12:01:26,806 DEBUG TRAIN Batch 18/800 loss 10.336796 loss_att 11.626645 loss_ctc 12.673538 loss_rnnt 9.612764 hw_loss 0.289682 lr 0.00040698 rank 4
2023-02-23 12:01:26,806 DEBUG TRAIN Batch 18/800 loss 7.891267 loss_att 10.893508 loss_ctc 9.271306 loss_rnnt 6.975074 hw_loss 0.247011 lr 0.00040694 rank 7
2023-02-23 12:01:26,807 DEBUG TRAIN Batch 18/800 loss 3.823078 loss_att 5.718360 loss_ctc 5.274821 loss_rnnt 3.105136 hw_loss 0.272475 lr 0.00040696 rank 2
2023-02-23 12:02:42,538 DEBUG TRAIN Batch 18/900 loss 5.028502 loss_att 7.126366 loss_ctc 7.161211 loss_rnnt 4.231904 hw_loss 0.173744 lr 0.00040684 rank 6
2023-02-23 12:02:42,538 DEBUG TRAIN Batch 18/900 loss 10.001558 loss_att 12.558662 loss_ctc 10.894564 loss_rnnt 9.240534 hw_loss 0.244756 lr 0.00040682 rank 2
2023-02-23 12:02:42,539 DEBUG TRAIN Batch 18/900 loss 7.973016 loss_att 10.524966 loss_ctc 7.867787 loss_rnnt 7.370142 hw_loss 0.199714 lr 0.00040685 rank 4
2023-02-23 12:02:42,540 DEBUG TRAIN Batch 18/900 loss 13.847156 loss_att 18.636732 loss_ctc 19.949112 loss_rnnt 11.975754 hw_loss 0.187300 lr 0.00040680 rank 3
2023-02-23 12:02:42,540 DEBUG TRAIN Batch 18/900 loss 4.166355 loss_att 5.328607 loss_ctc 5.275823 loss_rnnt 3.636224 hw_loss 0.280784 lr 0.00040682 rank 5
2023-02-23 12:02:42,543 DEBUG TRAIN Batch 18/900 loss 14.880202 loss_att 18.350737 loss_ctc 21.764959 loss_rnnt 13.187185 hw_loss 0.151766 lr 0.00040688 rank 0
2023-02-23 12:02:42,545 DEBUG TRAIN Batch 18/900 loss 6.358404 loss_att 9.220736 loss_ctc 11.936852 loss_rnnt 4.900568 hw_loss 0.265456 lr 0.00040681 rank 7
2023-02-23 12:02:42,590 DEBUG TRAIN Batch 18/900 loss 6.874342 loss_att 10.039869 loss_ctc 11.403667 loss_rnnt 5.496166 hw_loss 0.264677 lr 0.00040687 rank 1
2023-02-23 12:03:59,741 DEBUG TRAIN Batch 18/1000 loss 10.143969 loss_att 12.333584 loss_ctc 12.379888 loss_rnnt 9.326517 hw_loss 0.152636 lr 0.00040667 rank 3
2023-02-23 12:03:59,742 DEBUG TRAIN Batch 18/1000 loss 7.226351 loss_att 10.780512 loss_ctc 11.783863 loss_rnnt 5.793626 hw_loss 0.214170 lr 0.00040674 rank 1
2023-02-23 12:03:59,743 DEBUG TRAIN Batch 18/1000 loss 14.295678 loss_att 14.417660 loss_ctc 14.612783 loss_rnnt 14.099652 hw_loss 0.242528 lr 0.00040668 rank 5
2023-02-23 12:03:59,744 DEBUG TRAIN Batch 18/1000 loss 8.333858 loss_att 12.534981 loss_ctc 11.089886 loss_rnnt 7.005165 hw_loss 0.226868 lr 0.00040667 rank 7
2023-02-23 12:03:59,744 DEBUG TRAIN Batch 18/1000 loss 7.633029 loss_att 13.498796 loss_ctc 18.371742 loss_rnnt 4.912406 hw_loss 0.216828 lr 0.00040671 rank 4
2023-02-23 12:03:59,749 DEBUG TRAIN Batch 18/1000 loss 10.943706 loss_att 14.289744 loss_ctc 19.286695 loss_rnnt 9.040105 hw_loss 0.228735 lr 0.00040675 rank 0
2023-02-23 12:03:59,750 DEBUG TRAIN Batch 18/1000 loss 6.175534 loss_att 7.780001 loss_ctc 8.926949 loss_rnnt 5.415093 hw_loss 0.136296 lr 0.00040669 rank 2
2023-02-23 12:03:59,788 DEBUG TRAIN Batch 18/1000 loss 9.843425 loss_att 12.469827 loss_ctc 14.288017 loss_rnnt 8.600523 hw_loss 0.234392 lr 0.00040671 rank 6
2023-02-23 12:05:18,757 DEBUG TRAIN Batch 18/1100 loss 7.240185 loss_att 8.534616 loss_ctc 9.499538 loss_rnnt 6.525747 hw_loss 0.289320 lr 0.00040655 rank 5
2023-02-23 12:05:18,758 DEBUG TRAIN Batch 18/1100 loss 13.728650 loss_att 15.239384 loss_ctc 15.453338 loss_rnnt 13.081743 hw_loss 0.215252 lr 0.00040657 rank 6
2023-02-23 12:05:18,759 DEBUG TRAIN Batch 18/1100 loss 11.041758 loss_att 14.254545 loss_ctc 13.633261 loss_rnnt 9.949596 hw_loss 0.195130 lr 0.00040658 rank 4
2023-02-23 12:05:18,759 DEBUG TRAIN Batch 18/1100 loss 9.042797 loss_att 12.079397 loss_ctc 11.326813 loss_rnnt 8.006134 hw_loss 0.234013 lr 0.00040655 rank 2
2023-02-23 12:05:18,763 DEBUG TRAIN Batch 18/1100 loss 13.105513 loss_att 15.693179 loss_ctc 20.100384 loss_rnnt 11.534779 hw_loss 0.226032 lr 0.00040661 rank 0
2023-02-23 12:05:18,765 DEBUG TRAIN Batch 18/1100 loss 12.941561 loss_att 15.188248 loss_ctc 14.931725 loss_rnnt 12.104910 hw_loss 0.228672 lr 0.00040653 rank 3
2023-02-23 12:05:18,767 DEBUG TRAIN Batch 18/1100 loss 11.001281 loss_att 12.950130 loss_ctc 16.690428 loss_rnnt 9.750865 hw_loss 0.191423 lr 0.00040654 rank 7
2023-02-23 12:05:18,767 DEBUG TRAIN Batch 18/1100 loss 9.050151 loss_att 9.826400 loss_ctc 9.957251 loss_rnnt 8.665503 hw_loss 0.203348 lr 0.00040660 rank 1
2023-02-23 12:06:35,837 DEBUG TRAIN Batch 18/1200 loss 12.436872 loss_att 14.041059 loss_ctc 12.925731 loss_rnnt 11.898465 hw_loss 0.285725 lr 0.00040648 rank 0
2023-02-23 12:06:35,837 DEBUG TRAIN Batch 18/1200 loss 5.216125 loss_att 6.347038 loss_ctc 8.478670 loss_rnnt 4.452632 hw_loss 0.191822 lr 0.00040642 rank 2
2023-02-23 12:06:35,841 DEBUG TRAIN Batch 18/1200 loss 7.507143 loss_att 10.147686 loss_ctc 8.814076 loss_rnnt 6.673707 hw_loss 0.245754 lr 0.00040644 rank 4
2023-02-23 12:06:35,841 DEBUG TRAIN Batch 18/1200 loss 12.154440 loss_att 17.467819 loss_ctc 18.946262 loss_rnnt 10.073239 hw_loss 0.211777 lr 0.00040641 rank 5
2023-02-23 12:06:35,844 DEBUG TRAIN Batch 18/1200 loss 7.494608 loss_att 9.072605 loss_ctc 11.664573 loss_rnnt 6.511513 hw_loss 0.209063 lr 0.00040640 rank 3
2023-02-23 12:06:35,844 DEBUG TRAIN Batch 18/1200 loss 9.730899 loss_att 13.677320 loss_ctc 12.916788 loss_rnnt 8.344249 hw_loss 0.323589 lr 0.00040644 rank 6
2023-02-23 12:06:35,846 DEBUG TRAIN Batch 18/1200 loss 11.068618 loss_att 12.890247 loss_ctc 15.312577 loss_rnnt 9.970663 hw_loss 0.314563 lr 0.00040640 rank 7
2023-02-23 12:06:35,891 DEBUG TRAIN Batch 18/1200 loss 6.892533 loss_att 9.787612 loss_ctc 11.515798 loss_rnnt 5.559034 hw_loss 0.258840 lr 0.00040647 rank 1
2023-02-23 12:07:51,143 DEBUG TRAIN Batch 18/1300 loss 9.237882 loss_att 8.412416 loss_ctc 10.897995 loss_rnnt 9.015152 hw_loss 0.312141 lr 0.00040628 rank 5
2023-02-23 12:07:51,144 DEBUG TRAIN Batch 18/1300 loss 17.945681 loss_att 20.319187 loss_ctc 27.793674 loss_rnnt 16.070927 hw_loss 0.163103 lr 0.00040634 rank 0
2023-02-23 12:07:51,145 DEBUG TRAIN Batch 18/1300 loss 12.052025 loss_att 13.215639 loss_ctc 15.264145 loss_rnnt 11.173620 hw_loss 0.407625 lr 0.00040631 rank 4
2023-02-23 12:07:51,147 DEBUG TRAIN Batch 18/1300 loss 3.920706 loss_att 6.446093 loss_ctc 6.055929 loss_rnnt 3.010500 hw_loss 0.225809 lr 0.00040628 rank 2
2023-02-23 12:07:51,149 DEBUG TRAIN Batch 18/1300 loss 4.175491 loss_att 7.937446 loss_ctc 5.692414 loss_rnnt 3.169966 hw_loss 0.095396 lr 0.00040627 rank 7
2023-02-23 12:07:51,151 DEBUG TRAIN Batch 18/1300 loss 12.125361 loss_att 15.739470 loss_ctc 23.994587 loss_rnnt 9.735516 hw_loss 0.158365 lr 0.00040626 rank 3
2023-02-23 12:07:51,151 DEBUG TRAIN Batch 18/1300 loss 11.354183 loss_att 12.202206 loss_ctc 16.619995 loss_rnnt 10.326698 hw_loss 0.292073 lr 0.00040630 rank 6
2023-02-23 12:07:51,156 DEBUG TRAIN Batch 18/1300 loss 10.745838 loss_att 11.673054 loss_ctc 14.969518 loss_rnnt 9.821766 hw_loss 0.329008 lr 0.00040633 rank 1
2023-02-23 12:09:09,209 DEBUG TRAIN Batch 18/1400 loss 15.404933 loss_att 16.312386 loss_ctc 20.897614 loss_rnnt 14.353178 hw_loss 0.258577 lr 0.00040614 rank 5
2023-02-23 12:09:09,214 DEBUG TRAIN Batch 18/1400 loss 8.690512 loss_att 11.069068 loss_ctc 12.174711 loss_rnnt 7.700830 hw_loss 0.092645 lr 0.00040618 rank 4
2023-02-23 12:09:09,220 DEBUG TRAIN Batch 18/1400 loss 7.518404 loss_att 12.016856 loss_ctc 11.508441 loss_rnnt 5.991472 hw_loss 0.178570 lr 0.00040613 rank 3
2023-02-23 12:09:09,220 DEBUG TRAIN Batch 18/1400 loss 3.792562 loss_att 6.452348 loss_ctc 4.905033 loss_rnnt 3.025003 hw_loss 0.163636 lr 0.00040615 rank 2
2023-02-23 12:09:09,221 DEBUG TRAIN Batch 18/1400 loss 4.487792 loss_att 8.040551 loss_ctc 6.151568 loss_rnnt 3.471067 hw_loss 0.158128 lr 0.00040621 rank 0
2023-02-23 12:09:09,222 DEBUG TRAIN Batch 18/1400 loss 10.357687 loss_att 14.395769 loss_ctc 14.410681 loss_rnnt 8.899055 hw_loss 0.207408 lr 0.00040613 rank 7
2023-02-23 12:09:09,226 DEBUG TRAIN Batch 18/1400 loss 6.600177 loss_att 8.648021 loss_ctc 7.431782 loss_rnnt 5.944332 hw_loss 0.253865 lr 0.00040620 rank 1
2023-02-23 12:09:09,267 DEBUG TRAIN Batch 18/1400 loss 5.411450 loss_att 10.108244 loss_ctc 9.134624 loss_rnnt 3.871381 hw_loss 0.195539 lr 0.00040617 rank 6
2023-02-23 12:10:25,118 DEBUG TRAIN Batch 18/1500 loss 11.168222 loss_att 11.536613 loss_ctc 10.659087 loss_rnnt 11.059927 hw_loss 0.192194 lr 0.00040604 rank 4
2023-02-23 12:10:25,118 DEBUG TRAIN Batch 18/1500 loss 18.129906 loss_att 21.627008 loss_ctc 25.772009 loss_rnnt 16.292343 hw_loss 0.223488 lr 0.00040601 rank 5
2023-02-23 12:10:25,119 DEBUG TRAIN Batch 18/1500 loss 9.221241 loss_att 12.592023 loss_ctc 13.889951 loss_rnnt 7.796109 hw_loss 0.240904 lr 0.00040602 rank 2
2023-02-23 12:10:25,122 DEBUG TRAIN Batch 18/1500 loss 13.196698 loss_att 16.169365 loss_ctc 18.247725 loss_rnnt 11.857538 hw_loss 0.133416 lr 0.00040604 rank 6
2023-02-23 12:10:25,125 DEBUG TRAIN Batch 18/1500 loss 8.843233 loss_att 12.865978 loss_ctc 14.722445 loss_rnnt 7.113914 hw_loss 0.264143 lr 0.00040607 rank 1
2023-02-23 12:10:25,126 DEBUG TRAIN Batch 18/1500 loss 10.647139 loss_att 14.164791 loss_ctc 14.712605 loss_rnnt 9.338476 hw_loss 0.118255 lr 0.00040607 rank 0
2023-02-23 12:10:25,127 DEBUG TRAIN Batch 18/1500 loss 11.679951 loss_att 14.668310 loss_ctc 14.204668 loss_rnnt 10.611940 hw_loss 0.250704 lr 0.00040599 rank 3
2023-02-23 12:10:25,129 DEBUG TRAIN Batch 18/1500 loss 13.881036 loss_att 16.523762 loss_ctc 19.083292 loss_rnnt 12.512140 hw_loss 0.275092 lr 0.00040600 rank 7
2023-02-23 12:11:41,565 DEBUG TRAIN Batch 18/1600 loss 5.221749 loss_att 10.499231 loss_ctc 8.175302 loss_rnnt 3.642493 hw_loss 0.243660 lr 0.00040588 rank 5
2023-02-23 12:11:41,567 DEBUG TRAIN Batch 18/1600 loss 9.660379 loss_att 11.708643 loss_ctc 13.215787 loss_rnnt 8.714868 hw_loss 0.115884 lr 0.00040594 rank 0
2023-02-23 12:11:41,571 DEBUG TRAIN Batch 18/1600 loss 22.518377 loss_att 25.641478 loss_ctc 30.248796 loss_rnnt 20.744965 hw_loss 0.221380 lr 0.00040593 rank 1
2023-02-23 12:11:41,571 DEBUG TRAIN Batch 18/1600 loss 17.193035 loss_att 17.634310 loss_ctc 20.569988 loss_rnnt 16.518135 hw_loss 0.255720 lr 0.00040588 rank 2
2023-02-23 12:11:41,573 DEBUG TRAIN Batch 18/1600 loss 5.746994 loss_att 9.541142 loss_ctc 10.872335 loss_rnnt 4.177854 hw_loss 0.237998 lr 0.00040586 rank 3
2023-02-23 12:11:41,576 DEBUG TRAIN Batch 18/1600 loss 6.842056 loss_att 8.832750 loss_ctc 8.184010 loss_rnnt 6.146761 hw_loss 0.221678 lr 0.00040591 rank 4
2023-02-23 12:11:41,611 DEBUG TRAIN Batch 18/1600 loss 5.704691 loss_att 10.027911 loss_ctc 8.739668 loss_rnnt 4.378848 hw_loss 0.106005 lr 0.00040590 rank 6
2023-02-23 12:11:41,645 DEBUG TRAIN Batch 18/1600 loss 7.548737 loss_att 11.461779 loss_ctc 12.219078 loss_rnnt 6.041187 hw_loss 0.191679 lr 0.00040587 rank 7
2023-02-23 12:12:58,672 DEBUG TRAIN Batch 18/1700 loss 9.382567 loss_att 11.141387 loss_ctc 11.054568 loss_rnnt 8.675845 hw_loss 0.247549 lr 0.00040581 rank 0
2023-02-23 12:12:58,672 DEBUG TRAIN Batch 18/1700 loss 15.851716 loss_att 17.082359 loss_ctc 22.431984 loss_rnnt 14.608699 hw_loss 0.224100 lr 0.00040577 rank 4
2023-02-23 12:12:58,673 DEBUG TRAIN Batch 18/1700 loss 11.529478 loss_att 14.644722 loss_ctc 12.981100 loss_rnnt 10.619615 hw_loss 0.174871 lr 0.00040573 rank 3
2023-02-23 12:12:58,676 DEBUG TRAIN Batch 18/1700 loss 6.636152 loss_att 7.826177 loss_ctc 8.913909 loss_rnnt 5.999384 hw_loss 0.178241 lr 0.00040575 rank 2
2023-02-23 12:12:58,677 DEBUG TRAIN Batch 18/1700 loss 7.458333 loss_att 9.905908 loss_ctc 14.163923 loss_rnnt 5.934919 hw_loss 0.262163 lr 0.00040574 rank 5
2023-02-23 12:12:58,679 DEBUG TRAIN Batch 18/1700 loss 16.427797 loss_att 16.660664 loss_ctc 23.279118 loss_rnnt 15.292241 hw_loss 0.329014 lr 0.00040577 rank 6
2023-02-23 12:12:58,679 DEBUG TRAIN Batch 18/1700 loss 15.143744 loss_att 14.654726 loss_ctc 15.061125 loss_rnnt 15.163101 hw_loss 0.167745 lr 0.00040580 rank 1
2023-02-23 12:12:58,684 DEBUG TRAIN Batch 18/1700 loss 5.474645 loss_att 9.426199 loss_ctc 10.182398 loss_rnnt 3.970773 hw_loss 0.160990 lr 0.00040573 rank 7
2023-02-23 12:14:18,881 DEBUG TRAIN Batch 18/1800 loss 9.573624 loss_att 12.963268 loss_ctc 17.206949 loss_rnnt 7.730080 hw_loss 0.277198 lr 0.00040567 rank 0
2023-02-23 12:14:18,883 DEBUG TRAIN Batch 18/1800 loss 12.247478 loss_att 17.606737 loss_ctc 20.547256 loss_rnnt 9.961249 hw_loss 0.202010 lr 0.00040559 rank 3
2023-02-23 12:14:18,884 DEBUG TRAIN Batch 18/1800 loss 13.953552 loss_att 15.997049 loss_ctc 19.247723 loss_rnnt 12.723308 hw_loss 0.216856 lr 0.00040561 rank 2
2023-02-23 12:14:18,888 DEBUG TRAIN Batch 18/1800 loss 5.927351 loss_att 9.419870 loss_ctc 10.921801 loss_rnnt 4.446223 hw_loss 0.218809 lr 0.00040563 rank 6
2023-02-23 12:14:18,889 DEBUG TRAIN Batch 18/1800 loss 11.204900 loss_att 12.938805 loss_ctc 17.154472 loss_rnnt 9.909561 hw_loss 0.291152 lr 0.00040560 rank 7
2023-02-23 12:14:18,891 DEBUG TRAIN Batch 18/1800 loss 13.369414 loss_att 15.733094 loss_ctc 18.807564 loss_rnnt 12.069033 hw_loss 0.192296 lr 0.00040567 rank 1
2023-02-23 12:14:18,901 DEBUG TRAIN Batch 18/1800 loss 12.111961 loss_att 16.505184 loss_ctc 22.696321 loss_rnnt 9.682131 hw_loss 0.262386 lr 0.00040564 rank 4
2023-02-23 12:14:18,908 DEBUG TRAIN Batch 18/1800 loss 5.143940 loss_att 7.927180 loss_ctc 8.697275 loss_rnnt 3.984583 hw_loss 0.241747 lr 0.00040561 rank 5
2023-02-23 12:15:34,374 DEBUG TRAIN Batch 18/1900 loss 7.704490 loss_att 11.666249 loss_ctc 13.701969 loss_rnnt 6.019428 hw_loss 0.174463 lr 0.00040551 rank 4
2023-02-23 12:15:34,377 DEBUG TRAIN Batch 18/1900 loss 12.523694 loss_att 14.035662 loss_ctc 17.543880 loss_rnnt 11.433578 hw_loss 0.221931 lr 0.00040548 rank 2
2023-02-23 12:15:34,379 DEBUG TRAIN Batch 18/1900 loss 12.978021 loss_att 19.071205 loss_ctc 20.254103 loss_rnnt 10.715425 hw_loss 0.138401 lr 0.00040554 rank 0
2023-02-23 12:15:34,381 DEBUG TRAIN Batch 18/1900 loss 14.855304 loss_att 16.662556 loss_ctc 20.471401 loss_rnnt 13.625122 hw_loss 0.224844 lr 0.00040546 rank 3
2023-02-23 12:15:34,383 DEBUG TRAIN Batch 18/1900 loss 13.103986 loss_att 14.883392 loss_ctc 16.713020 loss_rnnt 12.125232 hw_loss 0.265630 lr 0.00040548 rank 5
2023-02-23 12:15:34,387 DEBUG TRAIN Batch 18/1900 loss 12.139347 loss_att 13.162060 loss_ctc 16.985250 loss_rnnt 11.149337 hw_loss 0.261276 lr 0.00040553 rank 1
2023-02-23 12:15:34,390 DEBUG TRAIN Batch 18/1900 loss 15.719398 loss_att 21.295897 loss_ctc 19.088343 loss_rnnt 14.062117 hw_loss 0.173981 lr 0.00040547 rank 7
2023-02-23 12:15:34,426 DEBUG TRAIN Batch 18/1900 loss 13.346175 loss_att 15.728594 loss_ctc 17.475983 loss_rnnt 12.138926 hw_loss 0.337733 lr 0.00040550 rank 6
2023-02-23 12:16:50,709 DEBUG TRAIN Batch 18/2000 loss 11.714434 loss_att 12.484760 loss_ctc 14.016731 loss_rnnt 11.113265 hw_loss 0.262744 lr 0.00040537 rank 4
2023-02-23 12:16:50,710 DEBUG TRAIN Batch 18/2000 loss 2.821690 loss_att 8.088308 loss_ctc 2.812097 loss_rnnt 1.653918 hw_loss 0.216989 lr 0.00040534 rank 5
2023-02-23 12:16:50,712 DEBUG TRAIN Batch 18/2000 loss 8.067240 loss_att 15.343474 loss_ctc 17.108416 loss_rnnt 5.273240 hw_loss 0.249868 lr 0.00040537 rank 6
2023-02-23 12:16:50,715 DEBUG TRAIN Batch 18/2000 loss 4.944890 loss_att 8.655081 loss_ctc 8.164664 loss_rnnt 3.691345 hw_loss 0.154130 lr 0.00040533 rank 7
2023-02-23 12:16:50,716 DEBUG TRAIN Batch 18/2000 loss 14.157599 loss_att 17.994204 loss_ctc 22.274170 loss_rnnt 12.218834 hw_loss 0.167314 lr 0.00040541 rank 0
2023-02-23 12:16:50,718 DEBUG TRAIN Batch 18/2000 loss 11.297960 loss_att 12.469433 loss_ctc 18.943577 loss_rnnt 9.961079 hw_loss 0.155948 lr 0.00040540 rank 1
2023-02-23 12:16:50,719 DEBUG TRAIN Batch 18/2000 loss 3.293519 loss_att 7.381506 loss_ctc 6.129760 loss_rnnt 1.993896 hw_loss 0.194738 lr 0.00040535 rank 2
2023-02-23 12:16:50,761 DEBUG TRAIN Batch 18/2000 loss 11.647417 loss_att 13.623581 loss_ctc 17.283600 loss_rnnt 10.417042 hw_loss 0.156845 lr 0.00040533 rank 3
2023-02-23 12:18:09,015 DEBUG TRAIN Batch 18/2100 loss 10.551510 loss_att 13.945530 loss_ctc 11.665749 loss_rnnt 9.612148 hw_loss 0.209988 lr 0.00040519 rank 3
2023-02-23 12:18:09,015 DEBUG TRAIN Batch 18/2100 loss 5.269000 loss_att 7.708463 loss_ctc 8.230050 loss_rnnt 4.283125 hw_loss 0.193452 lr 0.00040527 rank 0
2023-02-23 12:18:09,015 DEBUG TRAIN Batch 18/2100 loss 11.706647 loss_att 15.120705 loss_ctc 19.160385 loss_rnnt 9.930237 hw_loss 0.187064 lr 0.00040524 rank 4
2023-02-23 12:18:09,018 DEBUG TRAIN Batch 18/2100 loss 14.652135 loss_att 19.787260 loss_ctc 23.136810 loss_rnnt 12.441387 hw_loss 0.098310 lr 0.00040520 rank 7
2023-02-23 12:18:09,019 DEBUG TRAIN Batch 18/2100 loss 14.126881 loss_att 15.746515 loss_ctc 17.396902 loss_rnnt 13.192489 hw_loss 0.327118 lr 0.00040521 rank 5
2023-02-23 12:18:09,022 DEBUG TRAIN Batch 18/2100 loss 7.611542 loss_att 7.749915 loss_ctc 6.413216 loss_rnnt 7.606130 hw_loss 0.257837 lr 0.00040524 rank 6
2023-02-23 12:18:09,023 DEBUG TRAIN Batch 18/2100 loss 10.388279 loss_att 13.553544 loss_ctc 12.985046 loss_rnnt 9.300274 hw_loss 0.203844 lr 0.00040522 rank 2
2023-02-23 12:18:09,069 DEBUG TRAIN Batch 18/2100 loss 11.051620 loss_att 13.832364 loss_ctc 11.728773 loss_rnnt 10.275373 hw_loss 0.243391 lr 0.00040527 rank 1
2023-02-23 12:19:25,497 DEBUG TRAIN Batch 18/2200 loss 5.419672 loss_att 8.763359 loss_ctc 9.972217 loss_rnnt 4.048707 hw_loss 0.178539 lr 0.00040510 rank 6
2023-02-23 12:19:25,500 DEBUG TRAIN Batch 18/2200 loss 4.423457 loss_att 9.448298 loss_ctc 8.785156 loss_rnnt 2.675059 hw_loss 0.303505 lr 0.00040511 rank 4
2023-02-23 12:19:25,503 DEBUG TRAIN Batch 18/2200 loss 15.352445 loss_att 18.794645 loss_ctc 18.590431 loss_rnnt 14.118410 hw_loss 0.213493 lr 0.00040508 rank 5
2023-02-23 12:19:25,505 DEBUG TRAIN Batch 18/2200 loss 6.245690 loss_att 9.349341 loss_ctc 8.059073 loss_rnnt 5.257807 hw_loss 0.235066 lr 0.00040507 rank 7
2023-02-23 12:19:25,505 DEBUG TRAIN Batch 18/2200 loss 10.311344 loss_att 15.637897 loss_ctc 14.285679 loss_rnnt 8.564760 hw_loss 0.283804 lr 0.00040514 rank 0
2023-02-23 12:19:25,506 DEBUG TRAIN Batch 18/2200 loss 8.925610 loss_att 13.370241 loss_ctc 13.041382 loss_rnnt 7.366057 hw_loss 0.228482 lr 0.00040508 rank 2
2023-02-23 12:19:25,507 DEBUG TRAIN Batch 18/2200 loss 6.837169 loss_att 13.553634 loss_ctc 8.181265 loss_rnnt 5.216462 hw_loss 0.184126 lr 0.00040506 rank 3
2023-02-23 12:19:25,509 DEBUG TRAIN Batch 18/2200 loss 3.921798 loss_att 7.121005 loss_ctc 5.620353 loss_rnnt 2.962602 hw_loss 0.174151 lr 0.00040513 rank 1
2023-02-23 12:20:40,999 DEBUG TRAIN Batch 18/2300 loss 8.047366 loss_att 11.925701 loss_ctc 9.804665 loss_rnnt 6.957087 hw_loss 0.150575 lr 0.00040501 rank 0
2023-02-23 12:20:41,000 DEBUG TRAIN Batch 18/2300 loss 15.214129 loss_att 19.326017 loss_ctc 17.557621 loss_rnnt 13.956157 hw_loss 0.230868 lr 0.00040497 rank 4
2023-02-23 12:20:41,001 DEBUG TRAIN Batch 18/2300 loss 9.262281 loss_att 10.772157 loss_ctc 11.214225 loss_rnnt 8.632427 hw_loss 0.126786 lr 0.00040494 rank 5
2023-02-23 12:20:41,004 DEBUG TRAIN Batch 18/2300 loss 9.997393 loss_att 12.679749 loss_ctc 16.062042 loss_rnnt 8.536201 hw_loss 0.217690 lr 0.00040500 rank 1
2023-02-23 12:20:41,006 DEBUG TRAIN Batch 18/2300 loss 16.455153 loss_att 18.843136 loss_ctc 23.540432 loss_rnnt 14.891017 hw_loss 0.265940 lr 0.00040493 rank 3
2023-02-23 12:20:41,008 DEBUG TRAIN Batch 18/2300 loss 8.692099 loss_att 10.804131 loss_ctc 15.196726 loss_rnnt 7.315569 hw_loss 0.162822 lr 0.00040495 rank 2
2023-02-23 12:20:41,011 DEBUG TRAIN Batch 18/2300 loss 7.885955 loss_att 8.710537 loss_ctc 10.828308 loss_rnnt 7.258142 hw_loss 0.132344 lr 0.00040493 rank 7
2023-02-23 12:20:41,012 DEBUG TRAIN Batch 18/2300 loss 13.579422 loss_att 14.532200 loss_ctc 20.762503 loss_rnnt 12.328585 hw_loss 0.192255 lr 0.00040497 rank 6
2023-02-23 12:21:56,655 DEBUG TRAIN Batch 18/2400 loss 8.990445 loss_att 12.305906 loss_ctc 13.293557 loss_rnnt 7.692614 hw_loss 0.114359 lr 0.00040484 rank 4
2023-02-23 12:21:56,662 DEBUG TRAIN Batch 18/2400 loss 18.187300 loss_att 20.031712 loss_ctc 23.120100 loss_rnnt 17.006596 hw_loss 0.288969 lr 0.00040482 rank 2
2023-02-23 12:21:56,663 DEBUG TRAIN Batch 18/2400 loss 12.903230 loss_att 15.909805 loss_ctc 18.995462 loss_rnnt 11.370966 hw_loss 0.222468 lr 0.00040481 rank 5
2023-02-23 12:21:56,665 DEBUG TRAIN Batch 18/2400 loss 5.011308 loss_att 7.249533 loss_ctc 4.769469 loss_rnnt 4.463286 hw_loss 0.248667 lr 0.00040480 rank 3
2023-02-23 12:21:56,664 DEBUG TRAIN Batch 18/2400 loss 19.725965 loss_att 21.012659 loss_ctc 24.686340 loss_rnnt 18.683014 hw_loss 0.232930 lr 0.00040487 rank 1
2023-02-23 12:21:56,665 DEBUG TRAIN Batch 18/2400 loss 9.676147 loss_att 11.908127 loss_ctc 14.569751 loss_rnnt 8.418990 hw_loss 0.296779 lr 0.00040487 rank 0
2023-02-23 12:21:56,669 DEBUG TRAIN Batch 18/2400 loss 10.909841 loss_att 12.264870 loss_ctc 14.438658 loss_rnnt 9.990925 hw_loss 0.332628 lr 0.00040480 rank 7
2023-02-23 12:21:56,670 DEBUG TRAIN Batch 18/2400 loss 8.226797 loss_att 10.177888 loss_ctc 11.565604 loss_rnnt 7.341531 hw_loss 0.093512 lr 0.00040484 rank 6
2023-02-23 12:23:16,248 DEBUG TRAIN Batch 18/2500 loss 8.683225 loss_att 9.529917 loss_ctc 11.292031 loss_rnnt 7.997499 hw_loss 0.316024 lr 0.00040468 rank 2
2023-02-23 12:23:16,249 DEBUG TRAIN Batch 18/2500 loss 5.347885 loss_att 8.521523 loss_ctc 6.930626 loss_rnnt 4.393649 hw_loss 0.203393 lr 0.00040468 rank 5
2023-02-23 12:23:16,252 DEBUG TRAIN Batch 18/2500 loss 14.885451 loss_att 17.898399 loss_ctc 19.513107 loss_rnnt 13.620425 hw_loss 0.085153 lr 0.00040474 rank 0
2023-02-23 12:23:16,253 DEBUG TRAIN Batch 18/2500 loss 5.368239 loss_att 6.999647 loss_ctc 7.861455 loss_rnnt 4.541225 hw_loss 0.315568 lr 0.00040466 rank 3
2023-02-23 12:23:16,254 DEBUG TRAIN Batch 18/2500 loss 8.966475 loss_att 10.613129 loss_ctc 10.560019 loss_rnnt 8.309563 hw_loss 0.215831 lr 0.00040471 rank 4
2023-02-23 12:23:16,257 DEBUG TRAIN Batch 18/2500 loss 10.562527 loss_att 15.168192 loss_ctc 18.533413 loss_rnnt 8.457241 hw_loss 0.227567 lr 0.00040473 rank 1
2023-02-23 12:23:16,260 DEBUG TRAIN Batch 18/2500 loss 16.599840 loss_att 17.809986 loss_ctc 20.291096 loss_rnnt 15.717526 hw_loss 0.277718 lr 0.00040470 rank 6
2023-02-23 12:23:16,305 DEBUG TRAIN Batch 18/2500 loss 8.137237 loss_att 13.834669 loss_ctc 8.189196 loss_rnnt 6.866451 hw_loss 0.233196 lr 0.00040467 rank 7
2023-02-23 12:24:33,513 DEBUG TRAIN Batch 18/2600 loss 8.180350 loss_att 9.009779 loss_ctc 11.230521 loss_rnnt 7.421483 hw_loss 0.349299 lr 0.00040455 rank 5
2023-02-23 12:24:33,513 DEBUG TRAIN Batch 18/2600 loss 6.304897 loss_att 12.693212 loss_ctc 6.750514 loss_rnnt 4.873221 hw_loss 0.177371 lr 0.00040461 rank 0
2023-02-23 12:24:33,514 DEBUG TRAIN Batch 18/2600 loss 8.703458 loss_att 13.915134 loss_ctc 14.523239 loss_rnnt 6.771802 hw_loss 0.212529 lr 0.00040458 rank 4
2023-02-23 12:24:33,516 DEBUG TRAIN Batch 18/2600 loss 9.590608 loss_att 10.928034 loss_ctc 11.969538 loss_rnnt 8.815199 hw_loss 0.357622 lr 0.00040460 rank 1
2023-02-23 12:24:33,518 DEBUG TRAIN Batch 18/2600 loss 8.623988 loss_att 12.010831 loss_ctc 12.245195 loss_rnnt 7.351496 hw_loss 0.210556 lr 0.00040457 rank 6
2023-02-23 12:24:33,519 DEBUG TRAIN Batch 18/2600 loss 12.765058 loss_att 16.682121 loss_ctc 19.485292 loss_rnnt 10.928580 hw_loss 0.294438 lr 0.00040454 rank 7
2023-02-23 12:24:33,519 DEBUG TRAIN Batch 18/2600 loss 3.443500 loss_att 7.504319 loss_ctc 6.949643 loss_rnnt 2.053731 hw_loss 0.206474 lr 0.00040455 rank 2
2023-02-23 12:24:33,563 DEBUG TRAIN Batch 18/2600 loss 6.984571 loss_att 11.625254 loss_ctc 9.120685 loss_rnnt 5.640065 hw_loss 0.246662 lr 0.00040453 rank 3
2023-02-23 12:25:49,220 DEBUG TRAIN Batch 18/2700 loss 7.487841 loss_att 11.249445 loss_ctc 8.398355 loss_rnnt 6.543186 hw_loss 0.132998 lr 0.00040448 rank 0
2023-02-23 12:25:49,222 DEBUG TRAIN Batch 18/2700 loss 15.132393 loss_att 16.087454 loss_ctc 21.517714 loss_rnnt 13.981410 hw_loss 0.203615 lr 0.00040441 rank 5
2023-02-23 12:25:49,224 DEBUG TRAIN Batch 18/2700 loss 15.726354 loss_att 17.481318 loss_ctc 19.702930 loss_rnnt 14.727176 hw_loss 0.221203 lr 0.00040442 rank 2
2023-02-23 12:25:49,225 DEBUG TRAIN Batch 18/2700 loss 8.225619 loss_att 9.498300 loss_ctc 10.459425 loss_rnnt 7.572875 hw_loss 0.188189 lr 0.00040447 rank 1
2023-02-23 12:25:49,227 DEBUG TRAIN Batch 18/2700 loss 4.618393 loss_att 8.971009 loss_ctc 6.784115 loss_rnnt 3.334207 hw_loss 0.234186 lr 0.00040440 rank 3
2023-02-23 12:25:49,226 DEBUG TRAIN Batch 18/2700 loss 10.577065 loss_att 13.159000 loss_ctc 10.617432 loss_rnnt 9.900983 hw_loss 0.289338 lr 0.00040444 rank 4
2023-02-23 12:25:49,231 DEBUG TRAIN Batch 18/2700 loss 15.603928 loss_att 16.744717 loss_ctc 23.381109 loss_rnnt 14.240717 hw_loss 0.183927 lr 0.00040440 rank 7
2023-02-23 12:25:49,235 DEBUG TRAIN Batch 18/2700 loss 8.215461 loss_att 12.892196 loss_ctc 13.532457 loss_rnnt 6.495910 hw_loss 0.141132 lr 0.00040444 rank 6
2023-02-23 12:27:07,396 DEBUG TRAIN Batch 18/2800 loss 17.574558 loss_att 20.147007 loss_ctc 26.009449 loss_rnnt 15.879549 hw_loss 0.104753 lr 0.00040431 rank 4
2023-02-23 12:27:07,397 DEBUG TRAIN Batch 18/2800 loss 6.039970 loss_att 9.737629 loss_ctc 9.184641 loss_rnnt 4.762275 hw_loss 0.222891 lr 0.00040434 rank 0
2023-02-23 12:27:07,398 DEBUG TRAIN Batch 18/2800 loss 16.933813 loss_att 20.604589 loss_ctc 29.656393 loss_rnnt 14.417644 hw_loss 0.160633 lr 0.00040428 rank 5
2023-02-23 12:27:07,400 DEBUG TRAIN Batch 18/2800 loss 12.915997 loss_att 15.581560 loss_ctc 15.985158 loss_rnnt 11.870872 hw_loss 0.192733 lr 0.00040427 rank 3
2023-02-23 12:27:07,402 DEBUG TRAIN Batch 18/2800 loss 10.246595 loss_att 14.473837 loss_ctc 13.960853 loss_rnnt 8.780849 hw_loss 0.234494 lr 0.00040429 rank 2
2023-02-23 12:27:07,402 DEBUG TRAIN Batch 18/2800 loss 10.344371 loss_att 14.251188 loss_ctc 14.611340 loss_rnnt 8.861264 hw_loss 0.249027 lr 0.00040434 rank 1
2023-02-23 12:27:07,408 DEBUG TRAIN Batch 18/2800 loss 6.888674 loss_att 9.317560 loss_ctc 9.809727 loss_rnnt 5.905217 hw_loss 0.202886 lr 0.00040427 rank 7
2023-02-23 12:27:07,444 DEBUG TRAIN Batch 18/2800 loss 11.436641 loss_att 17.272823 loss_ctc 13.639910 loss_rnnt 9.823804 hw_loss 0.284684 lr 0.00040431 rank 6
2023-02-23 12:28:25,009 DEBUG TRAIN Batch 18/2900 loss 8.488917 loss_att 11.634342 loss_ctc 14.450199 loss_rnnt 6.974716 hw_loss 0.169273 lr 0.00040413 rank 3
2023-02-23 12:28:25,009 DEBUG TRAIN Batch 18/2900 loss 19.124987 loss_att 18.747700 loss_ctc 25.131935 loss_rnnt 18.327507 hw_loss 0.135022 lr 0.00040415 rank 2
2023-02-23 12:28:25,010 DEBUG TRAIN Batch 18/2900 loss 5.501249 loss_att 8.294273 loss_ctc 7.682829 loss_rnnt 4.542527 hw_loss 0.204824 lr 0.00040415 rank 5
2023-02-23 12:28:25,013 DEBUG TRAIN Batch 18/2900 loss 8.647360 loss_att 12.190254 loss_ctc 12.763828 loss_rnnt 7.239096 hw_loss 0.282792 lr 0.00040421 rank 0
2023-02-23 12:28:25,015 DEBUG TRAIN Batch 18/2900 loss 6.708045 loss_att 10.956620 loss_ctc 10.645583 loss_rnnt 5.239308 hw_loss 0.176282 lr 0.00040418 rank 4
2023-02-23 12:28:25,016 DEBUG TRAIN Batch 18/2900 loss 6.676546 loss_att 10.337927 loss_ctc 9.013704 loss_rnnt 5.475748 hw_loss 0.294188 lr 0.00040417 rank 6
2023-02-23 12:28:25,017 DEBUG TRAIN Batch 18/2900 loss 4.884626 loss_att 7.479558 loss_ctc 7.155453 loss_rnnt 3.887105 hw_loss 0.329546 lr 0.00040414 rank 7
2023-02-23 12:28:25,017 DEBUG TRAIN Batch 18/2900 loss 16.576296 loss_att 19.983240 loss_ctc 19.283813 loss_rnnt 15.457100 hw_loss 0.144008 lr 0.00040420 rank 1
2023-02-23 12:29:39,775 DEBUG TRAIN Batch 18/3000 loss 6.408668 loss_att 9.022527 loss_ctc 8.943202 loss_rnnt 5.396314 hw_loss 0.284334 lr 0.00040402 rank 5
2023-02-23 12:29:39,777 DEBUG TRAIN Batch 18/3000 loss 6.040041 loss_att 10.047627 loss_ctc 8.080711 loss_rnnt 4.836994 hw_loss 0.242700 lr 0.00040405 rank 4
2023-02-23 12:29:39,780 DEBUG TRAIN Batch 18/3000 loss 8.167706 loss_att 11.099833 loss_ctc 10.124216 loss_rnnt 7.242712 hw_loss 0.145689 lr 0.00040407 rank 1
2023-02-23 12:29:39,781 DEBUG TRAIN Batch 18/3000 loss 16.781054 loss_att 19.601984 loss_ctc 30.493967 loss_rnnt 14.258077 hw_loss 0.244505 lr 0.00040400 rank 3
2023-02-23 12:29:39,781 DEBUG TRAIN Batch 18/3000 loss 13.196455 loss_att 15.481903 loss_ctc 19.075573 loss_rnnt 11.897821 hw_loss 0.108113 lr 0.00040404 rank 6
2023-02-23 12:29:39,782 DEBUG TRAIN Batch 18/3000 loss 6.277159 loss_att 7.779249 loss_ctc 7.861488 loss_rnnt 5.641766 hw_loss 0.231996 lr 0.00040402 rank 2
2023-02-23 12:29:39,783 DEBUG TRAIN Batch 18/3000 loss 11.023076 loss_att 12.085920 loss_ctc 17.150867 loss_rnnt 9.869408 hw_loss 0.232613 lr 0.00040408 rank 0
2023-02-23 12:29:39,785 DEBUG TRAIN Batch 18/3000 loss 8.556604 loss_att 8.976200 loss_ctc 11.413268 loss_rnnt 7.980443 hw_loss 0.208788 lr 0.00040401 rank 7
2023-02-23 12:30:55,943 DEBUG TRAIN Batch 18/3100 loss 8.895329 loss_att 9.218419 loss_ctc 13.278420 loss_rnnt 8.075382 hw_loss 0.320470 lr 0.00040389 rank 2
2023-02-23 12:30:55,947 DEBUG TRAIN Batch 18/3100 loss 5.996185 loss_att 8.814800 loss_ctc 10.426797 loss_rnnt 4.761467 hw_loss 0.150461 lr 0.00040392 rank 4
2023-02-23 12:30:55,948 DEBUG TRAIN Batch 18/3100 loss 9.154417 loss_att 10.131269 loss_ctc 13.187028 loss_rnnt 8.288930 hw_loss 0.248314 lr 0.00040395 rank 0
2023-02-23 12:30:55,948 DEBUG TRAIN Batch 18/3100 loss 11.194921 loss_att 12.147740 loss_ctc 13.167553 loss_rnnt 10.609713 hw_loss 0.246798 lr 0.00040394 rank 1
2023-02-23 12:30:55,950 DEBUG TRAIN Batch 18/3100 loss 9.316262 loss_att 11.616684 loss_ctc 10.180623 loss_rnnt 8.688293 hw_loss 0.098694 lr 0.00040389 rank 5
2023-02-23 12:30:55,953 DEBUG TRAIN Batch 18/3100 loss 16.861961 loss_att 15.922770 loss_ctc 20.082739 loss_rnnt 16.463499 hw_loss 0.294116 lr 0.00040387 rank 3
2023-02-23 12:30:55,982 DEBUG TRAIN Batch 18/3100 loss 9.685601 loss_att 13.342243 loss_ctc 13.053614 loss_rnnt 8.395350 hw_loss 0.205976 lr 0.00040391 rank 6
2023-02-23 12:30:55,988 DEBUG TRAIN Batch 18/3100 loss 7.327759 loss_att 13.890455 loss_ctc 12.208095 loss_rnnt 5.260329 hw_loss 0.195337 lr 0.00040388 rank 7
2023-02-23 12:32:14,484 DEBUG TRAIN Batch 18/3200 loss 7.415647 loss_att 8.063927 loss_ctc 8.989803 loss_rnnt 6.936168 hw_loss 0.262380 lr 0.00040375 rank 5
2023-02-23 12:32:14,486 DEBUG TRAIN Batch 18/3200 loss 9.616428 loss_att 11.167882 loss_ctc 17.052664 loss_rnnt 8.169788 hw_loss 0.271598 lr 0.00040378 rank 6
2023-02-23 12:32:14,491 DEBUG TRAIN Batch 18/3200 loss 14.920204 loss_att 14.056684 loss_ctc 18.769037 loss_rnnt 14.434801 hw_loss 0.271739 lr 0.00040378 rank 4
2023-02-23 12:32:14,493 DEBUG TRAIN Batch 18/3200 loss 7.852165 loss_att 9.475287 loss_ctc 9.853093 loss_rnnt 7.102866 hw_loss 0.296033 lr 0.00040381 rank 1
2023-02-23 12:32:14,494 DEBUG TRAIN Batch 18/3200 loss 19.311003 loss_att 24.722986 loss_ctc 31.191265 loss_rnnt 16.540432 hw_loss 0.195258 lr 0.00040382 rank 0
2023-02-23 12:32:14,495 DEBUG TRAIN Batch 18/3200 loss 9.806937 loss_att 14.311029 loss_ctc 11.021385 loss_rnnt 8.622421 hw_loss 0.228321 lr 0.00040376 rank 2
2023-02-23 12:32:14,507 DEBUG TRAIN Batch 18/3200 loss 4.616066 loss_att 6.927755 loss_ctc 6.327662 loss_rnnt 3.825233 hw_loss 0.188031 lr 0.00040374 rank 3
2023-02-23 12:32:14,507 DEBUG TRAIN Batch 18/3200 loss 9.936751 loss_att 11.680585 loss_ctc 10.196861 loss_rnnt 9.414556 hw_loss 0.260152 lr 0.00040374 rank 7
2023-02-23 12:33:30,126 DEBUG TRAIN Batch 18/3300 loss 1.972121 loss_att 5.361440 loss_ctc 3.698012 loss_rnnt 0.931938 hw_loss 0.247876 lr 0.00040362 rank 5
2023-02-23 12:33:30,126 DEBUG TRAIN Batch 18/3300 loss 5.074371 loss_att 7.514572 loss_ctc 10.394857 loss_rnnt 3.712078 hw_loss 0.309104 lr 0.00040365 rank 4
2023-02-23 12:33:30,126 DEBUG TRAIN Batch 18/3300 loss 13.619228 loss_att 19.358610 loss_ctc 18.399353 loss_rnnt 11.740559 hw_loss 0.175206 lr 0.00040369 rank 0
2023-02-23 12:33:30,127 DEBUG TRAIN Batch 18/3300 loss 9.039358 loss_att 13.354059 loss_ctc 12.515677 loss_rnnt 7.601540 hw_loss 0.208816 lr 0.00040368 rank 1
2023-02-23 12:33:30,129 DEBUG TRAIN Batch 18/3300 loss 8.338917 loss_att 12.807594 loss_ctc 12.091251 loss_rnnt 6.847796 hw_loss 0.182013 lr 0.00040363 rank 2
2023-02-23 12:33:30,130 DEBUG TRAIN Batch 18/3300 loss 14.158648 loss_att 19.934036 loss_ctc 16.391476 loss_rnnt 12.603399 hw_loss 0.192113 lr 0.00040361 rank 3
2023-02-23 12:33:30,132 DEBUG TRAIN Batch 18/3300 loss 8.399734 loss_att 13.783570 loss_ctc 11.918016 loss_rnnt 6.725062 hw_loss 0.241501 lr 0.00040365 rank 6
2023-02-23 12:33:30,132 DEBUG TRAIN Batch 18/3300 loss 5.151007 loss_att 8.522079 loss_ctc 7.881013 loss_rnnt 4.013010 hw_loss 0.187090 lr 0.00040361 rank 7
2023-02-23 12:34:46,869 DEBUG TRAIN Batch 18/3400 loss 4.228238 loss_att 8.435013 loss_ctc 6.648348 loss_rnnt 2.987283 hw_loss 0.144223 lr 0.00040348 rank 3
2023-02-23 12:34:46,870 DEBUG TRAIN Batch 18/3400 loss 9.966786 loss_att 15.064734 loss_ctc 11.642256 loss_rnnt 8.606301 hw_loss 0.220310 lr 0.00040352 rank 4
2023-02-23 12:34:46,872 DEBUG TRAIN Batch 18/3400 loss 10.821819 loss_att 13.735278 loss_ctc 16.238054 loss_rnnt 9.418767 hw_loss 0.184117 lr 0.00040348 rank 7
2023-02-23 12:34:46,872 DEBUG TRAIN Batch 18/3400 loss 15.656658 loss_att 19.408463 loss_ctc 16.813282 loss_rnnt 14.610526 hw_loss 0.265416 lr 0.00040349 rank 5
2023-02-23 12:34:46,873 DEBUG TRAIN Batch 18/3400 loss 11.452400 loss_att 12.030724 loss_ctc 10.959041 loss_rnnt 11.281917 hw_loss 0.226127 lr 0.00040350 rank 2
2023-02-23 12:34:46,875 DEBUG TRAIN Batch 18/3400 loss 7.604968 loss_att 9.622687 loss_ctc 10.903257 loss_rnnt 6.606886 hw_loss 0.290184 lr 0.00040355 rank 0
2023-02-23 12:34:46,877 DEBUG TRAIN Batch 18/3400 loss 15.370891 loss_att 16.704092 loss_ctc 19.595165 loss_rnnt 14.422018 hw_loss 0.223119 lr 0.00040355 rank 1
2023-02-23 12:34:46,878 DEBUG TRAIN Batch 18/3400 loss 7.198051 loss_att 11.467535 loss_ctc 8.764278 loss_rnnt 6.034812 hw_loss 0.188461 lr 0.00040352 rank 6
2023-02-23 12:36:04,611 DEBUG TRAIN Batch 18/3500 loss 4.842915 loss_att 7.626453 loss_ctc 6.480566 loss_rnnt 3.918647 hw_loss 0.279762 lr 0.00040336 rank 5
2023-02-23 12:36:04,611 DEBUG TRAIN Batch 18/3500 loss 11.124389 loss_att 14.424829 loss_ctc 16.210968 loss_rnnt 9.645161 hw_loss 0.264241 lr 0.00040339 rank 4
2023-02-23 12:36:04,615 DEBUG TRAIN Batch 18/3500 loss 9.511547 loss_att 12.158003 loss_ctc 11.819674 loss_rnnt 8.591475 hw_loss 0.155683 lr 0.00040342 rank 0
2023-02-23 12:36:04,615 DEBUG TRAIN Batch 18/3500 loss 17.119524 loss_att 15.308994 loss_ctc 15.134395 loss_rnnt 17.628469 hw_loss 0.220960 lr 0.00040341 rank 1
2023-02-23 12:36:04,617 DEBUG TRAIN Batch 18/3500 loss 6.358817 loss_att 8.801121 loss_ctc 6.978874 loss_rnnt 5.653275 hw_loss 0.252013 lr 0.00040336 rank 2
2023-02-23 12:36:04,617 DEBUG TRAIN Batch 18/3500 loss 7.491115 loss_att 9.825094 loss_ctc 10.926992 loss_rnnt 6.404196 hw_loss 0.303761 lr 0.00040334 rank 3
2023-02-23 12:36:04,657 DEBUG TRAIN Batch 18/3500 loss 7.341595 loss_att 10.032100 loss_ctc 12.773192 loss_rnnt 5.963502 hw_loss 0.217083 lr 0.00040338 rank 6
2023-02-23 12:36:04,690 DEBUG TRAIN Batch 18/3500 loss 13.151846 loss_att 17.287399 loss_ctc 17.415737 loss_rnnt 11.653857 hw_loss 0.191924 lr 0.00040335 rank 7
2023-02-23 12:37:21,965 DEBUG TRAIN Batch 18/3600 loss 12.977441 loss_att 17.028204 loss_ctc 17.886097 loss_rnnt 11.398893 hw_loss 0.213578 lr 0.00040325 rank 6
2023-02-23 12:37:21,966 DEBUG TRAIN Batch 18/3600 loss 9.350550 loss_att 11.230222 loss_ctc 11.017754 loss_rnnt 8.636887 hw_loss 0.216441 lr 0.00040326 rank 4
2023-02-23 12:37:21,967 DEBUG TRAIN Batch 18/3600 loss 7.915652 loss_att 10.367340 loss_ctc 10.043110 loss_rnnt 7.051207 hw_loss 0.169587 lr 0.00040329 rank 0
2023-02-23 12:37:21,968 DEBUG TRAIN Batch 18/3600 loss 13.339404 loss_att 16.612106 loss_ctc 16.565903 loss_rnnt 12.114314 hw_loss 0.263154 lr 0.00040321 rank 3
2023-02-23 12:37:21,968 DEBUG TRAIN Batch 18/3600 loss 10.647441 loss_att 14.315557 loss_ctc 13.690783 loss_rnnt 9.389579 hw_loss 0.222114 lr 0.00040323 rank 5
2023-02-23 12:37:21,969 DEBUG TRAIN Batch 18/3600 loss 9.628764 loss_att 11.118575 loss_ctc 9.973154 loss_rnnt 9.132684 hw_loss 0.285374 lr 0.00040323 rank 2
2023-02-23 12:37:21,974 DEBUG TRAIN Batch 18/3600 loss 20.002529 loss_att 25.033611 loss_ctc 32.185703 loss_rnnt 17.254559 hw_loss 0.219997 lr 0.00040322 rank 7
2023-02-23 12:37:21,974 DEBUG TRAIN Batch 18/3600 loss 6.312243 loss_att 10.213001 loss_ctc 8.332706 loss_rnnt 5.135959 hw_loss 0.237632 lr 0.00040328 rank 1
2023-02-23 12:38:37,767 DEBUG TRAIN Batch 18/3700 loss 7.457922 loss_att 11.154775 loss_ctc 8.918955 loss_rnnt 6.414333 hw_loss 0.205149 lr 0.00040312 rank 6
2023-02-23 12:38:37,767 DEBUG TRAIN Batch 18/3700 loss 10.728691 loss_att 12.882864 loss_ctc 15.773199 loss_rnnt 9.459346 hw_loss 0.311081 lr 0.00040310 rank 2
2023-02-23 12:38:37,769 DEBUG TRAIN Batch 18/3700 loss 11.723205 loss_att 12.800195 loss_ctc 17.144936 loss_rnnt 10.660427 hw_loss 0.233401 lr 0.00040310 rank 5
2023-02-23 12:38:37,770 DEBUG TRAIN Batch 18/3700 loss 9.163770 loss_att 10.308274 loss_ctc 11.169177 loss_rnnt 8.561275 hw_loss 0.199138 lr 0.00040316 rank 0
2023-02-23 12:38:37,772 DEBUG TRAIN Batch 18/3700 loss 10.343858 loss_att 13.138681 loss_ctc 17.946613 loss_rnnt 8.656253 hw_loss 0.215511 lr 0.00040313 rank 4
2023-02-23 12:38:37,774 DEBUG TRAIN Batch 18/3700 loss 8.008800 loss_att 10.393102 loss_ctc 11.705969 loss_rnnt 6.924775 hw_loss 0.214139 lr 0.00040315 rank 1
2023-02-23 12:38:37,776 DEBUG TRAIN Batch 18/3700 loss 10.061113 loss_att 11.920184 loss_ctc 15.048370 loss_rnnt 8.902667 hw_loss 0.228122 lr 0.00040308 rank 3
2023-02-23 12:38:37,780 DEBUG TRAIN Batch 18/3700 loss 11.724415 loss_att 13.065082 loss_ctc 15.239298 loss_rnnt 10.864057 hw_loss 0.231699 lr 0.00040309 rank 7
2023-02-23 12:39:53,364 DEBUG TRAIN Batch 18/3800 loss 4.814499 loss_att 6.913996 loss_ctc 5.983526 loss_rnnt 4.121720 hw_loss 0.219391 lr 0.00040303 rank 0
2023-02-23 12:39:53,364 DEBUG TRAIN Batch 18/3800 loss 15.025181 loss_att 16.933565 loss_ctc 18.857986 loss_rnnt 14.024576 hw_loss 0.202289 lr 0.00040297 rank 5
2023-02-23 12:39:53,365 DEBUG TRAIN Batch 18/3800 loss 8.865072 loss_att 9.697718 loss_ctc 13.414183 loss_rnnt 7.929307 hw_loss 0.305040 lr 0.00040295 rank 3
2023-02-23 12:39:53,369 DEBUG TRAIN Batch 18/3800 loss 7.372921 loss_att 9.801273 loss_ctc 9.381594 loss_rnnt 6.502052 hw_loss 0.220080 lr 0.00040296 rank 7
2023-02-23 12:39:53,370 DEBUG TRAIN Batch 18/3800 loss 10.389382 loss_att 13.663548 loss_ctc 13.631056 loss_rnnt 9.180889 hw_loss 0.227693 lr 0.00040297 rank 2
2023-02-23 12:39:53,371 DEBUG TRAIN Batch 18/3800 loss 5.133506 loss_att 6.572973 loss_ctc 6.911748 loss_rnnt 4.505601 hw_loss 0.192959 lr 0.00040299 rank 6
2023-02-23 12:39:53,371 DEBUG TRAIN Batch 18/3800 loss 7.784311 loss_att 10.277416 loss_ctc 8.470123 loss_rnnt 7.049887 hw_loss 0.270676 lr 0.00040300 rank 4
2023-02-23 12:39:53,374 DEBUG TRAIN Batch 18/3800 loss 9.725132 loss_att 13.413078 loss_ctc 15.222867 loss_rnnt 8.069747 hw_loss 0.346433 lr 0.00040302 rank 1
2023-02-23 12:41:12,050 DEBUG TRAIN Batch 18/3900 loss 6.997503 loss_att 12.078622 loss_ctc 9.286062 loss_rnnt 5.528036 hw_loss 0.277694 lr 0.00040290 rank 0
2023-02-23 12:41:12,059 DEBUG TRAIN Batch 18/3900 loss 5.950816 loss_att 8.193926 loss_ctc 6.847585 loss_rnnt 5.271141 hw_loss 0.209031 lr 0.00040289 rank 1
2023-02-23 12:41:12,060 DEBUG TRAIN Batch 18/3900 loss 5.417195 loss_att 9.372687 loss_ctc 8.159678 loss_rnnt 4.131608 hw_loss 0.241545 lr 0.00040282 rank 3
2023-02-23 12:41:12,060 DEBUG TRAIN Batch 18/3900 loss 3.998485 loss_att 7.311973 loss_ctc 4.562208 loss_rnnt 3.138532 hw_loss 0.228923 lr 0.00040284 rank 5
2023-02-23 12:41:12,061 DEBUG TRAIN Batch 18/3900 loss 4.545451 loss_att 9.495506 loss_ctc 5.623466 loss_rnnt 3.291971 hw_loss 0.224502 lr 0.00040284 rank 2
2023-02-23 12:41:12,063 DEBUG TRAIN Batch 18/3900 loss 9.682925 loss_att 14.562008 loss_ctc 17.571861 loss_rnnt 7.476930 hw_loss 0.334351 lr 0.00040287 rank 4
2023-02-23 12:41:12,064 DEBUG TRAIN Batch 18/3900 loss 8.278015 loss_att 11.689874 loss_ctc 9.570503 loss_rnnt 7.322706 hw_loss 0.188634 lr 0.00040283 rank 7
2023-02-23 12:41:12,066 DEBUG TRAIN Batch 18/3900 loss 8.873329 loss_att 13.392080 loss_ctc 14.758293 loss_rnnt 7.045777 hw_loss 0.260887 lr 0.00040286 rank 6
2023-02-23 12:42:27,354 DEBUG TRAIN Batch 18/4000 loss 14.850168 loss_att 20.200901 loss_ctc 21.554165 loss_rnnt 12.828518 hw_loss 0.108070 lr 0.00040270 rank 5
2023-02-23 12:42:27,356 DEBUG TRAIN Batch 18/4000 loss 10.727253 loss_att 12.780542 loss_ctc 14.992679 loss_rnnt 9.611959 hw_loss 0.254834 lr 0.00040269 rank 3
2023-02-23 12:42:27,358 DEBUG TRAIN Batch 18/4000 loss 15.230193 loss_att 18.226465 loss_ctc 21.349213 loss_rnnt 13.728119 hw_loss 0.163034 lr 0.00040271 rank 2
2023-02-23 12:42:27,359 DEBUG TRAIN Batch 18/4000 loss 8.272112 loss_att 11.520344 loss_ctc 8.613671 loss_rnnt 7.473133 hw_loss 0.194607 lr 0.00040277 rank 0
2023-02-23 12:42:27,359 DEBUG TRAIN Batch 18/4000 loss 5.212997 loss_att 10.458744 loss_ctc 9.586337 loss_rnnt 3.466313 hw_loss 0.214543 lr 0.00040273 rank 4
2023-02-23 12:42:27,360 DEBUG TRAIN Batch 18/4000 loss 12.838980 loss_att 18.681686 loss_ctc 19.589401 loss_rnnt 10.642323 hw_loss 0.240108 lr 0.00040276 rank 1
2023-02-23 12:42:27,392 DEBUG TRAIN Batch 18/4000 loss 13.992865 loss_att 14.906054 loss_ctc 19.352852 loss_rnnt 12.938620 hw_loss 0.294266 lr 0.00040269 rank 7
2023-02-23 12:42:27,401 DEBUG TRAIN Batch 18/4000 loss 13.197379 loss_att 18.135628 loss_ctc 15.385321 loss_rnnt 11.813919 hw_loss 0.195157 lr 0.00040273 rank 6
2023-02-23 12:43:44,021 DEBUG TRAIN Batch 18/4100 loss 8.974921 loss_att 10.741668 loss_ctc 16.154280 loss_rnnt 7.562833 hw_loss 0.190296 lr 0.00040264 rank 0
2023-02-23 12:43:44,022 DEBUG TRAIN Batch 18/4100 loss 7.396141 loss_att 9.651827 loss_ctc 10.899958 loss_rnnt 6.337876 hw_loss 0.262409 lr 0.00040256 rank 3
2023-02-23 12:43:44,023 DEBUG TRAIN Batch 18/4100 loss 9.517460 loss_att 10.200836 loss_ctc 12.084866 loss_rnnt 8.897425 hw_loss 0.264449 lr 0.00040258 rank 2
2023-02-23 12:43:44,024 DEBUG TRAIN Batch 18/4100 loss 7.467050 loss_att 9.603640 loss_ctc 9.125525 loss_rnnt 6.729432 hw_loss 0.167192 lr 0.00040257 rank 5
2023-02-23 12:43:44,025 DEBUG TRAIN Batch 18/4100 loss 5.827703 loss_att 8.638111 loss_ctc 7.049323 loss_rnnt 4.990147 hw_loss 0.211110 lr 0.00040260 rank 4
2023-02-23 12:43:44,026 DEBUG TRAIN Batch 18/4100 loss 10.901580 loss_att 10.089318 loss_ctc 10.170443 loss_rnnt 11.020882 hw_loss 0.263691 lr 0.00040260 rank 6
2023-02-23 12:43:44,028 DEBUG TRAIN Batch 18/4100 loss 3.213245 loss_att 6.790375 loss_ctc 3.850225 loss_rnnt 2.271992 hw_loss 0.264180 lr 0.00040263 rank 1
2023-02-23 12:43:44,045 DEBUG TRAIN Batch 18/4100 loss 6.038301 loss_att 12.069300 loss_ctc 10.259020 loss_rnnt 4.172194 hw_loss 0.182144 lr 0.00040256 rank 7
2023-02-23 12:45:01,516 DEBUG TRAIN Batch 18/4200 loss 6.239378 loss_att 9.155658 loss_ctc 10.555057 loss_rnnt 4.996793 hw_loss 0.157323 lr 0.00040244 rank 5
2023-02-23 12:45:01,522 DEBUG TRAIN Batch 18/4200 loss 9.051939 loss_att 12.097279 loss_ctc 13.035713 loss_rnnt 7.855278 hw_loss 0.105796 lr 0.00040247 rank 4
2023-02-23 12:45:01,525 DEBUG TRAIN Batch 18/4200 loss 7.748029 loss_att 10.334038 loss_ctc 11.676577 loss_rnnt 6.611542 hw_loss 0.179022 lr 0.00040243 rank 7
2023-02-23 12:45:01,527 DEBUG TRAIN Batch 18/4200 loss 19.377577 loss_att 22.904800 loss_ctc 23.613850 loss_rnnt 17.970619 hw_loss 0.256269 lr 0.00040251 rank 0
2023-02-23 12:45:01,527 DEBUG TRAIN Batch 18/4200 loss 11.410485 loss_att 11.246618 loss_ctc 14.793848 loss_rnnt 10.910547 hw_loss 0.152992 lr 0.00040250 rank 1
2023-02-23 12:45:01,528 DEBUG TRAIN Batch 18/4200 loss 12.404055 loss_att 20.826525 loss_ctc 17.510162 loss_rnnt 9.976286 hw_loss 0.117115 lr 0.00040245 rank 2
2023-02-23 12:45:01,528 DEBUG TRAIN Batch 18/4200 loss 10.636025 loss_att 16.139593 loss_ctc 18.876417 loss_rnnt 8.330370 hw_loss 0.199167 lr 0.00040247 rank 6
2023-02-23 12:45:01,534 DEBUG TRAIN Batch 18/4200 loss 9.588330 loss_att 14.561481 loss_ctc 13.049864 loss_rnnt 8.044736 hw_loss 0.163925 lr 0.00040243 rank 3
2023-02-23 12:46:20,650 DEBUG TRAIN Batch 18/4300 loss 11.702105 loss_att 12.823506 loss_ctc 9.998772 loss_rnnt 11.564711 hw_loss 0.262923 lr 0.00040238 rank 0
2023-02-23 12:46:20,653 DEBUG TRAIN Batch 18/4300 loss 20.457874 loss_att 21.385845 loss_ctc 28.485697 loss_rnnt 19.103783 hw_loss 0.183977 lr 0.00040234 rank 4
2023-02-23 12:46:20,653 DEBUG TRAIN Batch 18/4300 loss 14.736370 loss_att 19.293392 loss_ctc 23.449097 loss_rnnt 12.530935 hw_loss 0.248128 lr 0.00040231 rank 5
2023-02-23 12:46:20,655 DEBUG TRAIN Batch 18/4300 loss 14.328929 loss_att 16.062103 loss_ctc 19.724686 loss_rnnt 13.106033 hw_loss 0.294049 lr 0.00040232 rank 2
2023-02-23 12:46:20,656 DEBUG TRAIN Batch 18/4300 loss 7.572721 loss_att 9.831435 loss_ctc 12.837978 loss_rnnt 6.306713 hw_loss 0.210432 lr 0.00040230 rank 3
2023-02-23 12:46:20,657 DEBUG TRAIN Batch 18/4300 loss 6.045237 loss_att 9.027423 loss_ctc 8.386942 loss_rnnt 5.007029 hw_loss 0.242892 lr 0.00040237 rank 1
2023-02-23 12:46:20,658 DEBUG TRAIN Batch 18/4300 loss 6.227396 loss_att 8.210328 loss_ctc 6.492979 loss_rnnt 5.682420 hw_loss 0.211836 lr 0.00040234 rank 6
2023-02-23 12:46:20,659 DEBUG TRAIN Batch 18/4300 loss 10.868561 loss_att 14.405739 loss_ctc 17.446051 loss_rnnt 9.183498 hw_loss 0.188677 lr 0.00040230 rank 7
2023-02-23 12:47:38,154 DEBUG TRAIN Batch 18/4400 loss 8.196219 loss_att 12.565475 loss_ctc 15.683784 loss_rnnt 6.246415 hw_loss 0.145521 lr 0.00040225 rank 0
2023-02-23 12:47:38,157 DEBUG TRAIN Batch 18/4400 loss 7.104000 loss_att 6.978156 loss_ctc 8.455241 loss_rnnt 6.751685 hw_loss 0.369971 lr 0.00040219 rank 2
2023-02-23 12:47:38,158 DEBUG TRAIN Batch 18/4400 loss 6.989725 loss_att 9.592542 loss_ctc 10.951393 loss_rnnt 5.833265 hw_loss 0.201890 lr 0.00040218 rank 5
2023-02-23 12:47:38,159 DEBUG TRAIN Batch 18/4400 loss 14.839267 loss_att 14.983001 loss_ctc 21.547287 loss_rnnt 13.776210 hw_loss 0.262328 lr 0.00040217 rank 3
2023-02-23 12:47:38,159 DEBUG TRAIN Batch 18/4400 loss 3.439027 loss_att 9.627626 loss_ctc 5.890111 loss_rnnt 1.751983 hw_loss 0.229710 lr 0.00040217 rank 7
2023-02-23 12:47:38,159 DEBUG TRAIN Batch 18/4400 loss 6.419920 loss_att 8.868372 loss_ctc 11.195395 loss_rnnt 5.158103 hw_loss 0.253868 lr 0.00040224 rank 1
2023-02-23 12:47:38,159 DEBUG TRAIN Batch 18/4400 loss 9.195008 loss_att 11.673367 loss_ctc 12.759646 loss_rnnt 8.111808 hw_loss 0.210456 lr 0.00040221 rank 6
2023-02-23 12:47:38,164 DEBUG TRAIN Batch 18/4400 loss 10.148379 loss_att 13.121017 loss_ctc 12.702672 loss_rnnt 9.098051 hw_loss 0.216053 lr 0.00040221 rank 4
2023-02-23 12:48:54,071 DEBUG TRAIN Batch 18/4500 loss 10.131690 loss_att 10.059408 loss_ctc 14.085005 loss_rnnt 9.439643 hw_loss 0.336367 lr 0.00040208 rank 4
2023-02-23 12:48:54,072 DEBUG TRAIN Batch 18/4500 loss 7.527667 loss_att 12.095453 loss_ctc 10.237432 loss_rnnt 6.204330 hw_loss 0.090895 lr 0.00040205 rank 5
2023-02-23 12:48:54,073 DEBUG TRAIN Batch 18/4500 loss 10.960151 loss_att 10.221251 loss_ctc 14.502870 loss_rnnt 10.480488 hw_loss 0.290775 lr 0.00040211 rank 1
2023-02-23 12:48:54,074 DEBUG TRAIN Batch 18/4500 loss 6.438190 loss_att 8.825216 loss_ctc 8.951573 loss_rnnt 5.488482 hw_loss 0.257221 lr 0.00040208 rank 6
2023-02-23 12:48:54,076 DEBUG TRAIN Batch 18/4500 loss 5.764707 loss_att 10.449865 loss_ctc 9.231914 loss_rnnt 4.295018 hw_loss 0.131929 lr 0.00040204 rank 3
2023-02-23 12:48:54,076 DEBUG TRAIN Batch 18/4500 loss 6.750175 loss_att 8.233006 loss_ctc 7.599528 loss_rnnt 6.220576 hw_loss 0.224597 lr 0.00040212 rank 0
2023-02-23 12:48:54,076 DEBUG TRAIN Batch 18/4500 loss 8.939969 loss_att 13.035488 loss_ctc 13.507991 loss_rnnt 7.426265 hw_loss 0.160371 lr 0.00040204 rank 7
2023-02-23 12:48:54,127 DEBUG TRAIN Batch 18/4500 loss 10.184770 loss_att 12.112265 loss_ctc 12.956800 loss_rnnt 9.301008 hw_loss 0.241232 lr 0.00040206 rank 2
2023-02-23 12:50:12,547 DEBUG TRAIN Batch 18/4600 loss 18.405041 loss_att 20.582914 loss_ctc 23.096504 loss_rnnt 17.222467 hw_loss 0.227755 lr 0.00040195 rank 4
2023-02-23 12:50:12,550 DEBUG TRAIN Batch 18/4600 loss 11.773027 loss_att 14.768578 loss_ctc 16.320042 loss_rnnt 10.505305 hw_loss 0.116896 lr 0.00040191 rank 3
2023-02-23 12:50:12,551 DEBUG TRAIN Batch 18/4600 loss 5.087484 loss_att 7.832974 loss_ctc 8.540812 loss_rnnt 4.008002 hw_loss 0.131139 lr 0.00040198 rank 1
2023-02-23 12:50:12,552 DEBUG TRAIN Batch 18/4600 loss 9.750278 loss_att 10.686104 loss_ctc 12.672346 loss_rnnt 9.052504 hw_loss 0.226876 lr 0.00040195 rank 6
2023-02-23 12:50:12,554 DEBUG TRAIN Batch 18/4600 loss 4.837901 loss_att 6.851652 loss_ctc 5.284297 loss_rnnt 4.229931 hw_loss 0.273187 lr 0.00040199 rank 0
2023-02-23 12:50:12,560 DEBUG TRAIN Batch 18/4600 loss 8.488831 loss_att 10.115616 loss_ctc 9.074959 loss_rnnt 7.980555 hw_loss 0.196439 lr 0.00040191 rank 7
2023-02-23 12:50:12,564 DEBUG TRAIN Batch 18/4600 loss 6.027833 loss_att 9.322732 loss_ctc 13.458056 loss_rnnt 4.194771 hw_loss 0.343849 lr 0.00040193 rank 2
2023-02-23 12:50:12,567 DEBUG TRAIN Batch 18/4600 loss 14.941060 loss_att 23.080133 loss_ctc 26.836006 loss_rnnt 11.640456 hw_loss 0.162746 lr 0.00040192 rank 5
2023-02-23 12:51:29,793 DEBUG TRAIN Batch 18/4700 loss 10.367675 loss_att 13.658510 loss_ctc 13.544182 loss_rnnt 9.194634 hw_loss 0.171261 lr 0.00040180 rank 2
2023-02-23 12:51:29,795 DEBUG TRAIN Batch 18/4700 loss 13.059854 loss_att 17.475540 loss_ctc 14.831894 loss_rnnt 11.811885 hw_loss 0.241047 lr 0.00040179 rank 5
2023-02-23 12:51:29,796 DEBUG TRAIN Batch 18/4700 loss 10.270197 loss_att 11.870828 loss_ctc 15.081345 loss_rnnt 9.205943 hw_loss 0.192451 lr 0.00040178 rank 3
2023-02-23 12:51:29,797 DEBUG TRAIN Batch 18/4700 loss 10.628889 loss_att 15.119860 loss_ctc 15.349034 loss_rnnt 9.011324 hw_loss 0.168787 lr 0.00040186 rank 0
2023-02-23 12:51:29,797 DEBUG TRAIN Batch 18/4700 loss 8.314475 loss_att 9.900786 loss_ctc 10.901293 loss_rnnt 7.581390 hw_loss 0.132964 lr 0.00040182 rank 4
2023-02-23 12:51:29,801 DEBUG TRAIN Batch 18/4700 loss 9.726906 loss_att 13.699676 loss_ctc 13.328806 loss_rnnt 8.375620 hw_loss 0.143396 lr 0.00040178 rank 7
2023-02-23 12:51:29,803 DEBUG TRAIN Batch 18/4700 loss 5.474109 loss_att 7.658185 loss_ctc 8.310422 loss_rnnt 4.564223 hw_loss 0.177930 lr 0.00040185 rank 1
2023-02-23 12:51:29,804 DEBUG TRAIN Batch 18/4700 loss 7.116939 loss_att 10.380436 loss_ctc 9.642279 loss_rnnt 6.051833 hw_loss 0.141928 lr 0.00040182 rank 6
2023-02-23 12:52:44,416 DEBUG TRAIN Batch 18/4800 loss 12.443458 loss_att 15.912336 loss_ctc 19.693546 loss_rnnt 10.663261 hw_loss 0.224516 lr 0.00040166 rank 5
2023-02-23 12:52:44,417 DEBUG TRAIN Batch 18/4800 loss 10.540056 loss_att 13.531067 loss_ctc 15.219047 loss_rnnt 9.189776 hw_loss 0.240397 lr 0.00040173 rank 0
2023-02-23 12:52:44,420 DEBUG TRAIN Batch 18/4800 loss 14.448481 loss_att 15.852966 loss_ctc 18.387621 loss_rnnt 13.503426 hw_loss 0.260511 lr 0.00040165 rank 3
2023-02-23 12:52:44,421 DEBUG TRAIN Batch 18/4800 loss 4.955878 loss_att 7.631562 loss_ctc 6.331943 loss_rnnt 4.140878 hw_loss 0.180727 lr 0.00040167 rank 2
2023-02-23 12:52:44,421 DEBUG TRAIN Batch 18/4800 loss 7.962127 loss_att 10.913103 loss_ctc 13.222333 loss_rnnt 6.563049 hw_loss 0.201602 lr 0.00040169 rank 4
2023-02-23 12:52:44,425 DEBUG TRAIN Batch 18/4800 loss 9.581754 loss_att 14.944107 loss_ctc 16.687988 loss_rnnt 7.477943 hw_loss 0.157201 lr 0.00040169 rank 6
2023-02-23 12:52:44,427 DEBUG TRAIN Batch 18/4800 loss 4.839375 loss_att 6.874303 loss_ctc 4.232383 loss_rnnt 4.406228 hw_loss 0.200799 lr 0.00040165 rank 7
2023-02-23 12:52:44,428 DEBUG TRAIN Batch 18/4800 loss 15.795248 loss_att 18.010588 loss_ctc 20.999542 loss_rnnt 14.534033 hw_loss 0.232953 lr 0.00040172 rank 1
2023-02-23 12:53:59,101 DEBUG TRAIN Batch 18/4900 loss 11.808105 loss_att 16.067282 loss_ctc 14.695148 loss_rnnt 10.435112 hw_loss 0.255412 lr 0.00040152 rank 3
2023-02-23 12:53:59,102 DEBUG TRAIN Batch 18/4900 loss 9.265057 loss_att 12.271431 loss_ctc 11.485126 loss_rnnt 8.242785 hw_loss 0.234351 lr 0.00040156 rank 4
2023-02-23 12:53:59,102 DEBUG TRAIN Batch 18/4900 loss 10.589273 loss_att 13.056902 loss_ctc 14.834613 loss_rnnt 9.406847 hw_loss 0.230356 lr 0.00040154 rank 2
2023-02-23 12:53:59,104 DEBUG TRAIN Batch 18/4900 loss 13.313597 loss_att 15.494240 loss_ctc 16.668789 loss_rnnt 12.324659 hw_loss 0.197722 lr 0.00040159 rank 1
2023-02-23 12:53:59,106 DEBUG TRAIN Batch 18/4900 loss 9.505709 loss_att 13.583774 loss_ctc 18.041752 loss_rnnt 7.464153 hw_loss 0.164631 lr 0.00040160 rank 0
2023-02-23 12:53:59,109 DEBUG TRAIN Batch 18/4900 loss 10.572896 loss_att 13.177583 loss_ctc 13.241341 loss_rnnt 9.548977 hw_loss 0.275980 lr 0.00040156 rank 6
2023-02-23 12:53:59,110 DEBUG TRAIN Batch 18/4900 loss 8.700232 loss_att 11.938788 loss_ctc 11.269728 loss_rnnt 7.593457 hw_loss 0.218368 lr 0.00040153 rank 5
2023-02-23 12:53:59,155 DEBUG TRAIN Batch 18/4900 loss 16.260309 loss_att 16.873749 loss_ctc 22.525177 loss_rnnt 15.131606 hw_loss 0.320064 lr 0.00040152 rank 7
2023-02-23 12:55:18,369 DEBUG TRAIN Batch 18/5000 loss 13.316259 loss_att 15.891184 loss_ctc 20.946758 loss_rnnt 11.672909 hw_loss 0.208060 lr 0.00040139 rank 3
2023-02-23 12:55:18,370 DEBUG TRAIN Batch 18/5000 loss 3.039124 loss_att 5.458092 loss_ctc 5.244530 loss_rnnt 2.120399 hw_loss 0.264143 lr 0.00040141 rank 5
2023-02-23 12:55:18,371 DEBUG TRAIN Batch 18/5000 loss 9.108849 loss_att 9.641870 loss_ctc 11.770344 loss_rnnt 8.452244 hw_loss 0.365877 lr 0.00040141 rank 2
2023-02-23 12:55:18,375 DEBUG TRAIN Batch 18/5000 loss 11.080138 loss_att 15.033953 loss_ctc 18.013945 loss_rnnt 9.226085 hw_loss 0.260218 lr 0.00040143 rank 6
2023-02-23 12:55:18,376 DEBUG TRAIN Batch 18/5000 loss 6.949421 loss_att 8.696740 loss_ctc 9.565419 loss_rnnt 6.107874 hw_loss 0.268657 lr 0.00040143 rank 4
2023-02-23 12:55:18,377 DEBUG TRAIN Batch 18/5000 loss 8.033163 loss_att 10.158476 loss_ctc 10.852610 loss_rnnt 7.164829 hw_loss 0.126272 lr 0.00040146 rank 1
2023-02-23 12:55:18,379 DEBUG TRAIN Batch 18/5000 loss 9.413594 loss_att 10.593843 loss_ctc 11.557286 loss_rnnt 8.784816 hw_loss 0.200442 lr 0.00040147 rank 0
2023-02-23 12:55:18,406 DEBUG TRAIN Batch 18/5000 loss 6.425328 loss_att 8.739042 loss_ctc 7.999770 loss_rnnt 5.590932 hw_loss 0.303240 lr 0.00040139 rank 7
2023-02-23 12:56:34,467 DEBUG TRAIN Batch 18/5100 loss 13.555116 loss_att 16.838295 loss_ctc 22.271942 loss_rnnt 11.607580 hw_loss 0.241231 lr 0.00040134 rank 0
2023-02-23 12:56:34,468 DEBUG TRAIN Batch 18/5100 loss 11.579087 loss_att 12.201689 loss_ctc 17.836329 loss_rnnt 10.458296 hw_loss 0.303698 lr 0.00040131 rank 4
2023-02-23 12:56:34,469 DEBUG TRAIN Batch 18/5100 loss 8.453316 loss_att 12.380826 loss_ctc 12.400153 loss_rnnt 7.051158 hw_loss 0.169521 lr 0.00040127 rank 7
2023-02-23 12:56:34,470 DEBUG TRAIN Batch 18/5100 loss 11.656215 loss_att 13.777454 loss_ctc 16.413345 loss_rnnt 10.441091 hw_loss 0.293610 lr 0.00040133 rank 1
2023-02-23 12:56:34,472 DEBUG TRAIN Batch 18/5100 loss 5.579782 loss_att 8.467377 loss_ctc 8.211796 loss_rnnt 4.562527 hw_loss 0.166503 lr 0.00040128 rank 2
2023-02-23 12:56:34,472 DEBUG TRAIN Batch 18/5100 loss 12.145971 loss_att 11.110042 loss_ctc 14.066217 loss_rnnt 11.986062 hw_loss 0.208239 lr 0.00040130 rank 6
2023-02-23 12:56:34,474 DEBUG TRAIN Batch 18/5100 loss 20.155346 loss_att 18.810896 loss_ctc 25.250893 loss_rnnt 19.536066 hw_loss 0.391433 lr 0.00040128 rank 5
2023-02-23 12:56:34,518 DEBUG TRAIN Batch 18/5100 loss 18.440174 loss_att 21.865660 loss_ctc 22.167240 loss_rnnt 17.147900 hw_loss 0.206691 lr 0.00040126 rank 3
2023-02-23 12:57:49,733 DEBUG TRAIN Batch 18/5200 loss 11.041162 loss_att 12.547850 loss_ctc 12.482245 loss_rnnt 10.432605 hw_loss 0.215770 lr 0.00040121 rank 0
2023-02-23 12:57:49,733 DEBUG TRAIN Batch 18/5200 loss 3.758370 loss_att 6.602208 loss_ctc 3.873308 loss_rnnt 3.073541 hw_loss 0.188881 lr 0.00040113 rank 3
2023-02-23 12:57:49,735 DEBUG TRAIN Batch 18/5200 loss 15.664774 loss_att 16.979683 loss_ctc 20.430677 loss_rnnt 14.661474 hw_loss 0.196622 lr 0.00040115 rank 2
2023-02-23 12:57:49,735 DEBUG TRAIN Batch 18/5200 loss 6.684499 loss_att 11.557912 loss_ctc 8.520643 loss_rnnt 5.335390 hw_loss 0.243014 lr 0.00040120 rank 1
2023-02-23 12:57:49,737 DEBUG TRAIN Batch 18/5200 loss 4.219722 loss_att 7.624402 loss_ctc 7.113374 loss_rnnt 3.009697 hw_loss 0.268629 lr 0.00040115 rank 5
2023-02-23 12:57:49,737 DEBUG TRAIN Batch 18/5200 loss 3.323630 loss_att 6.157829 loss_ctc 4.285357 loss_rnnt 2.510273 hw_loss 0.221788 lr 0.00040118 rank 4
2023-02-23 12:57:49,744 DEBUG TRAIN Batch 18/5200 loss 9.341354 loss_att 10.861223 loss_ctc 11.698736 loss_rnnt 8.598799 hw_loss 0.232993 lr 0.00040117 rank 6
2023-02-23 12:57:49,745 DEBUG TRAIN Batch 18/5200 loss 5.377013 loss_att 8.392816 loss_ctc 6.169927 loss_rnnt 4.556874 hw_loss 0.208605 lr 0.00040114 rank 7
2023-02-23 12:59:08,118 DEBUG TRAIN Batch 18/5300 loss 12.034863 loss_att 13.309007 loss_ctc 14.718265 loss_rnnt 11.358825 hw_loss 0.118918 lr 0.00040102 rank 5
2023-02-23 12:59:08,122 DEBUG TRAIN Batch 18/5300 loss 10.153804 loss_att 12.774057 loss_ctc 13.895098 loss_rnnt 9.013019 hw_loss 0.221052 lr 0.00040104 rank 6
2023-02-23 12:59:08,124 DEBUG TRAIN Batch 18/5300 loss 14.354454 loss_att 15.998312 loss_ctc 24.301464 loss_rnnt 12.617325 hw_loss 0.153919 lr 0.00040102 rank 2
2023-02-23 12:59:08,124 DEBUG TRAIN Batch 18/5300 loss 7.822309 loss_att 8.948580 loss_ctc 10.635923 loss_rnnt 7.159000 hw_loss 0.117947 lr 0.00040108 rank 0
2023-02-23 12:59:08,128 DEBUG TRAIN Batch 18/5300 loss 9.777873 loss_att 15.030558 loss_ctc 14.768198 loss_rnnt 7.932883 hw_loss 0.242017 lr 0.00040101 rank 7
2023-02-23 12:59:08,129 DEBUG TRAIN Batch 18/5300 loss 2.041159 loss_att 4.995085 loss_ctc 2.597148 loss_rnnt 1.282097 hw_loss 0.176521 lr 0.00040100 rank 3
2023-02-23 12:59:08,129 DEBUG TRAIN Batch 18/5300 loss 14.715494 loss_att 16.529869 loss_ctc 18.324741 loss_rnnt 13.765437 hw_loss 0.198656 lr 0.00040105 rank 4
2023-02-23 12:59:08,174 DEBUG TRAIN Batch 18/5300 loss 2.043709 loss_att 5.203335 loss_ctc 3.197384 loss_rnnt 1.152658 hw_loss 0.197443 lr 0.00040107 rank 1
2023-02-23 13:00:24,557 DEBUG TRAIN Batch 18/5400 loss 12.578405 loss_att 16.576368 loss_ctc 16.043941 loss_rnnt 11.146346 hw_loss 0.319489 lr 0.00040089 rank 5
2023-02-23 13:00:24,558 DEBUG TRAIN Batch 18/5400 loss 6.925088 loss_att 8.896392 loss_ctc 8.073578 loss_rnnt 6.206739 hw_loss 0.320542 lr 0.00040089 rank 2
2023-02-23 13:00:24,558 DEBUG TRAIN Batch 18/5400 loss 10.344063 loss_att 12.891523 loss_ctc 11.154083 loss_rnnt 9.590173 hw_loss 0.255743 lr 0.00040088 rank 7
2023-02-23 13:00:24,559 DEBUG TRAIN Batch 18/5400 loss 14.377228 loss_att 14.850182 loss_ctc 20.406300 loss_rnnt 13.342623 hw_loss 0.255255 lr 0.00040087 rank 3
2023-02-23 13:00:24,561 DEBUG TRAIN Batch 18/5400 loss 15.502073 loss_att 16.488110 loss_ctc 16.870365 loss_rnnt 15.022635 hw_loss 0.187112 lr 0.00040091 rank 6
2023-02-23 13:00:24,561 DEBUG TRAIN Batch 18/5400 loss 12.307225 loss_att 15.523346 loss_ctc 18.592987 loss_rnnt 10.736584 hw_loss 0.167469 lr 0.00040095 rank 0
2023-02-23 13:00:24,562 DEBUG TRAIN Batch 18/5400 loss 6.085188 loss_att 9.655411 loss_ctc 9.785276 loss_rnnt 4.748903 hw_loss 0.241678 lr 0.00040094 rank 1
2023-02-23 13:00:24,565 DEBUG TRAIN Batch 18/5400 loss 8.471778 loss_att 14.030836 loss_ctc 11.071938 loss_rnnt 6.921012 hw_loss 0.172999 lr 0.00040092 rank 4
2023-02-23 13:01:40,559 DEBUG TRAIN Batch 18/5500 loss 10.970157 loss_att 13.906132 loss_ctc 13.485430 loss_rnnt 9.942333 hw_loss 0.197360 lr 0.00040076 rank 5
2023-02-23 13:01:40,559 DEBUG TRAIN Batch 18/5500 loss 9.215861 loss_att 9.576787 loss_ctc 10.539794 loss_rnnt 8.886393 hw_loss 0.151425 lr 0.00040077 rank 2
2023-02-23 13:01:40,560 DEBUG TRAIN Batch 18/5500 loss 20.999325 loss_att 23.157026 loss_ctc 39.203396 loss_rnnt 17.984465 hw_loss 0.292705 lr 0.00040079 rank 4
2023-02-23 13:01:40,561 DEBUG TRAIN Batch 18/5500 loss 17.510960 loss_att 21.565283 loss_ctc 26.153149 loss_rnnt 15.460520 hw_loss 0.163656 lr 0.00040078 rank 6
2023-02-23 13:01:40,563 DEBUG TRAIN Batch 18/5500 loss 3.142463 loss_att 5.417324 loss_ctc 4.514968 loss_rnnt 2.407153 hw_loss 0.182505 lr 0.00040082 rank 0
2023-02-23 13:01:40,563 DEBUG TRAIN Batch 18/5500 loss 7.610126 loss_att 10.500778 loss_ctc 9.301512 loss_rnnt 6.684508 hw_loss 0.228693 lr 0.00040074 rank 3
2023-02-23 13:01:40,564 DEBUG TRAIN Batch 18/5500 loss 5.546279 loss_att 9.691816 loss_ctc 6.949590 loss_rnnt 4.410827 hw_loss 0.223569 lr 0.00040081 rank 1
2023-02-23 13:01:40,565 DEBUG TRAIN Batch 18/5500 loss 11.131024 loss_att 14.012615 loss_ctc 16.391947 loss_rnnt 9.766315 hw_loss 0.163001 lr 0.00040075 rank 7
2023-02-23 13:02:57,526 DEBUG TRAIN Batch 18/5600 loss 11.454323 loss_att 14.081913 loss_ctc 14.447071 loss_rnnt 10.394350 hw_loss 0.253915 lr 0.00040066 rank 4
2023-02-23 13:02:57,527 DEBUG TRAIN Batch 18/5600 loss 16.397787 loss_att 18.247702 loss_ctc 20.867132 loss_rnnt 15.349108 hw_loss 0.155216 lr 0.00040063 rank 5
2023-02-23 13:02:57,528 DEBUG TRAIN Batch 18/5600 loss 11.493575 loss_att 11.727698 loss_ctc 13.672867 loss_rnnt 11.001826 hw_loss 0.289409 lr 0.00040062 rank 7
2023-02-23 13:02:57,531 DEBUG TRAIN Batch 18/5600 loss 9.974493 loss_att 11.415421 loss_ctc 11.415075 loss_rnnt 9.336459 hw_loss 0.295820 lr 0.00040064 rank 2
2023-02-23 13:02:57,532 DEBUG TRAIN Batch 18/5600 loss 8.617528 loss_att 11.169338 loss_ctc 13.125956 loss_rnnt 7.360224 hw_loss 0.273409 lr 0.00040066 rank 6
2023-02-23 13:02:57,533 DEBUG TRAIN Batch 18/5600 loss 11.495906 loss_att 13.254916 loss_ctc 15.427196 loss_rnnt 10.512918 hw_loss 0.200652 lr 0.00040069 rank 0
2023-02-23 13:02:57,536 DEBUG TRAIN Batch 18/5600 loss 4.417023 loss_att 6.406971 loss_ctc 5.443582 loss_rnnt 3.756738 hw_loss 0.235162 lr 0.00040069 rank 1
2023-02-23 13:02:57,580 DEBUG TRAIN Batch 18/5600 loss 10.731776 loss_att 15.305008 loss_ctc 15.662848 loss_rnnt 9.036064 hw_loss 0.231729 lr 0.00040062 rank 3
2023-02-23 13:04:17,899 DEBUG TRAIN Batch 18/5700 loss 3.798184 loss_att 5.635669 loss_ctc 5.937023 loss_rnnt 3.001884 hw_loss 0.269296 lr 0.00040053 rank 4
2023-02-23 13:04:17,900 DEBUG TRAIN Batch 18/5700 loss 11.130974 loss_att 11.012774 loss_ctc 13.997416 loss_rnnt 10.598553 hw_loss 0.326005 lr 0.00040049 rank 3
2023-02-23 13:04:17,901 DEBUG TRAIN Batch 18/5700 loss 11.081058 loss_att 13.549232 loss_ctc 15.766652 loss_rnnt 9.828626 hw_loss 0.251345 lr 0.00040050 rank 5
2023-02-23 13:04:17,903 DEBUG TRAIN Batch 18/5700 loss 12.497105 loss_att 17.203339 loss_ctc 15.402372 loss_rnnt 11.067953 hw_loss 0.188507 lr 0.00040051 rank 2
2023-02-23 13:04:17,903 DEBUG TRAIN Batch 18/5700 loss 5.251399 loss_att 7.287128 loss_ctc 6.268922 loss_rnnt 4.617807 hw_loss 0.170207 lr 0.00040049 rank 7
2023-02-23 13:04:17,905 DEBUG TRAIN Batch 18/5700 loss 10.496519 loss_att 11.137304 loss_ctc 13.572140 loss_rnnt 9.788052 hw_loss 0.319177 lr 0.00040053 rank 6
2023-02-23 13:04:17,906 DEBUG TRAIN Batch 18/5700 loss 11.099624 loss_att 10.285980 loss_ctc 13.237353 loss_rnnt 10.819625 hw_loss 0.295682 lr 0.00040056 rank 0
2023-02-23 13:04:17,909 DEBUG TRAIN Batch 18/5700 loss 15.514497 loss_att 18.113426 loss_ctc 21.565031 loss_rnnt 14.016510 hw_loss 0.321491 lr 0.00040056 rank 1
2023-02-23 13:05:33,217 DEBUG TRAIN Batch 18/5800 loss 5.708584 loss_att 10.908666 loss_ctc 10.170635 loss_rnnt 3.961217 hw_loss 0.210768 lr 0.00040040 rank 4
2023-02-23 13:05:33,219 DEBUG TRAIN Batch 18/5800 loss 7.086848 loss_att 8.912333 loss_ctc 10.296840 loss_rnnt 6.183216 hw_loss 0.207253 lr 0.00040044 rank 0
2023-02-23 13:05:33,219 DEBUG TRAIN Batch 18/5800 loss 8.405457 loss_att 11.665511 loss_ctc 13.947851 loss_rnnt 6.930555 hw_loss 0.157325 lr 0.00040037 rank 5
2023-02-23 13:05:33,221 DEBUG TRAIN Batch 18/5800 loss 4.355337 loss_att 7.692549 loss_ctc 7.238478 loss_rnnt 3.170601 hw_loss 0.249141 lr 0.00040038 rank 2
2023-02-23 13:05:33,224 DEBUG TRAIN Batch 18/5800 loss 6.693708 loss_att 10.439595 loss_ctc 8.600523 loss_rnnt 5.572965 hw_loss 0.219983 lr 0.00040036 rank 3
2023-02-23 13:05:33,225 DEBUG TRAIN Batch 18/5800 loss 16.298931 loss_att 19.236126 loss_ctc 23.028961 loss_rnnt 14.642818 hw_loss 0.321252 lr 0.00040040 rank 6
2023-02-23 13:05:33,227 DEBUG TRAIN Batch 18/5800 loss 10.593883 loss_att 13.341282 loss_ctc 14.331001 loss_rnnt 9.434967 hw_loss 0.208414 lr 0.00040036 rank 7
2023-02-23 13:05:33,268 DEBUG TRAIN Batch 18/5800 loss 5.252504 loss_att 10.348141 loss_ctc 5.297428 loss_rnnt 4.117292 hw_loss 0.206428 lr 0.00040043 rank 1
2023-02-23 13:06:48,168 DEBUG TRAIN Batch 18/5900 loss 11.769776 loss_att 13.349301 loss_ctc 16.141712 loss_rnnt 10.754210 hw_loss 0.218882 lr 0.00040025 rank 2
2023-02-23 13:06:48,168 DEBUG TRAIN Batch 18/5900 loss 5.276852 loss_att 10.356419 loss_ctc 7.889787 loss_rnnt 3.826339 hw_loss 0.161639 lr 0.00040024 rank 7
2023-02-23 13:06:48,169 DEBUG TRAIN Batch 18/5900 loss 2.474886 loss_att 5.603859 loss_ctc 5.066884 loss_rnnt 1.388254 hw_loss 0.216072 lr 0.00040028 rank 4
2023-02-23 13:06:48,171 DEBUG TRAIN Batch 18/5900 loss 6.554964 loss_att 9.140030 loss_ctc 7.360942 loss_rnnt 5.836902 hw_loss 0.175473 lr 0.00040027 rank 6
2023-02-23 13:06:48,174 DEBUG TRAIN Batch 18/5900 loss 6.032923 loss_att 9.240261 loss_ctc 8.564788 loss_rnnt 4.928572 hw_loss 0.234940 lr 0.00040031 rank 0
2023-02-23 13:06:48,174 DEBUG TRAIN Batch 18/5900 loss 9.068672 loss_att 10.744759 loss_ctc 12.786505 loss_rnnt 8.174511 hw_loss 0.118561 lr 0.00040025 rank 5
2023-02-23 13:06:48,175 DEBUG TRAIN Batch 18/5900 loss 8.407524 loss_att 13.347282 loss_ctc 13.562755 loss_rnnt 6.633249 hw_loss 0.185548 lr 0.00040023 rank 3
2023-02-23 13:06:48,177 DEBUG TRAIN Batch 18/5900 loss 13.071875 loss_att 18.939774 loss_ctc 18.099463 loss_rnnt 11.146815 hw_loss 0.152128 lr 0.00040030 rank 1
2023-02-23 13:08:05,661 DEBUG TRAIN Batch 18/6000 loss 8.658269 loss_att 12.070576 loss_ctc 10.975641 loss_rnnt 7.583961 hw_loss 0.155367 lr 0.00040012 rank 5
2023-02-23 13:08:05,664 DEBUG TRAIN Batch 18/6000 loss 11.285647 loss_att 16.488199 loss_ctc 21.877350 loss_rnnt 8.683258 hw_loss 0.280598 lr 0.00040017 rank 1
2023-02-23 13:08:05,664 DEBUG TRAIN Batch 18/6000 loss 8.590918 loss_att 14.808381 loss_ctc 11.658470 loss_rnnt 6.840448 hw_loss 0.183692 lr 0.00040012 rank 2
2023-02-23 13:08:05,666 DEBUG TRAIN Batch 18/6000 loss 4.051617 loss_att 6.973020 loss_ctc 4.532078 loss_rnnt 3.249503 hw_loss 0.288320 lr 0.00040015 rank 4
2023-02-23 13:08:05,671 DEBUG TRAIN Batch 18/6000 loss 11.956114 loss_att 13.531166 loss_ctc 12.695587 loss_rnnt 11.408689 hw_loss 0.250907 lr 0.00040010 rank 3
2023-02-23 13:08:05,673 DEBUG TRAIN Batch 18/6000 loss 15.166436 loss_att 22.948627 loss_ctc 18.275719 loss_rnnt 13.119911 hw_loss 0.141593 lr 0.00040011 rank 7
2023-02-23 13:08:05,673 DEBUG TRAIN Batch 18/6000 loss 3.392781 loss_att 6.612070 loss_ctc 5.981794 loss_rnnt 2.276398 hw_loss 0.238730 lr 0.00040014 rank 6
2023-02-23 13:08:05,675 DEBUG TRAIN Batch 18/6000 loss 7.428576 loss_att 8.188307 loss_ctc 9.406746 loss_rnnt 6.846491 hw_loss 0.311966 lr 0.00040018 rank 0
2023-02-23 13:09:23,045 DEBUG TRAIN Batch 18/6100 loss 9.368316 loss_att 10.334423 loss_ctc 10.733383 loss_rnnt 8.860658 hw_loss 0.248301 lr 0.00039999 rank 2
2023-02-23 13:09:23,049 DEBUG TRAIN Batch 18/6100 loss 16.305767 loss_att 22.056458 loss_ctc 25.815220 loss_rnnt 13.764685 hw_loss 0.230654 lr 0.00039999 rank 5
2023-02-23 13:09:23,051 DEBUG TRAIN Batch 18/6100 loss 15.640476 loss_att 20.762877 loss_ctc 22.543121 loss_rnnt 13.596293 hw_loss 0.186282 lr 0.00040005 rank 0
2023-02-23 13:09:23,051 DEBUG TRAIN Batch 18/6100 loss 12.654572 loss_att 14.790874 loss_ctc 15.058813 loss_rnnt 11.760752 hw_loss 0.273737 lr 0.00039997 rank 3
2023-02-23 13:09:23,054 DEBUG TRAIN Batch 18/6100 loss 5.365357 loss_att 8.328547 loss_ctc 6.347372 loss_rnnt 4.508099 hw_loss 0.250659 lr 0.00040002 rank 4
2023-02-23 13:09:23,056 DEBUG TRAIN Batch 18/6100 loss 6.587842 loss_att 9.168931 loss_ctc 8.004894 loss_rnnt 5.809771 hw_loss 0.136710 lr 0.00039998 rank 7
2023-02-23 13:09:23,057 DEBUG TRAIN Batch 18/6100 loss 6.079401 loss_att 9.002199 loss_ctc 9.646352 loss_rnnt 4.913985 hw_loss 0.197366 lr 0.00040004 rank 1
2023-02-23 13:09:23,057 DEBUG TRAIN Batch 18/6100 loss 11.954355 loss_att 17.236305 loss_ctc 13.552596 loss_rnnt 10.559389 hw_loss 0.235272 lr 0.00040001 rank 6
2023-02-23 13:10:38,602 DEBUG TRAIN Batch 18/6200 loss 6.744721 loss_att 8.878229 loss_ctc 9.272702 loss_rnnt 5.831734 hw_loss 0.279788 lr 0.00039987 rank 2
2023-02-23 13:10:38,603 DEBUG TRAIN Batch 18/6200 loss 8.554016 loss_att 12.161323 loss_ctc 14.949617 loss_rnnt 6.884975 hw_loss 0.177810 lr 0.00039986 rank 5
2023-02-23 13:10:38,603 DEBUG TRAIN Batch 18/6200 loss 5.610791 loss_att 8.034424 loss_ctc 6.790788 loss_rnnt 4.882557 hw_loss 0.161578 lr 0.00039992 rank 0
2023-02-23 13:10:38,604 DEBUG TRAIN Batch 18/6200 loss 17.922169 loss_att 20.964436 loss_ctc 28.185505 loss_rnnt 15.833599 hw_loss 0.209385 lr 0.00039985 rank 3
2023-02-23 13:10:38,604 DEBUG TRAIN Batch 18/6200 loss 9.665349 loss_att 11.790731 loss_ctc 11.063192 loss_rnnt 8.927586 hw_loss 0.236826 lr 0.00039989 rank 6
2023-02-23 13:10:38,604 DEBUG TRAIN Batch 18/6200 loss 5.651194 loss_att 7.685239 loss_ctc 11.913428 loss_rnnt 4.278574 hw_loss 0.245337 lr 0.00039989 rank 4
2023-02-23 13:10:38,607 DEBUG TRAIN Batch 18/6200 loss 6.505676 loss_att 9.685952 loss_ctc 10.398109 loss_rnnt 5.233537 hw_loss 0.219549 lr 0.00039992 rank 1
2023-02-23 13:10:38,607 DEBUG TRAIN Batch 18/6200 loss 7.681385 loss_att 9.298759 loss_ctc 11.198110 loss_rnnt 6.697646 hw_loss 0.358814 lr 0.00039985 rank 7
2023-02-23 13:11:54,609 DEBUG TRAIN Batch 18/6300 loss 7.024969 loss_att 8.042768 loss_ctc 10.817809 loss_rnnt 6.192227 hw_loss 0.231506 lr 0.00039974 rank 2
2023-02-23 13:11:54,612 DEBUG TRAIN Batch 18/6300 loss 11.686804 loss_att 13.667681 loss_ctc 16.576948 loss_rnnt 10.545051 hw_loss 0.175423 lr 0.00039976 rank 4
2023-02-23 13:11:54,615 DEBUG TRAIN Batch 18/6300 loss 9.213993 loss_att 9.625359 loss_ctc 12.354598 loss_rnnt 8.610424 hw_loss 0.192279 lr 0.00039980 rank 0
2023-02-23 13:11:54,615 DEBUG TRAIN Batch 18/6300 loss 9.158429 loss_att 8.999061 loss_ctc 10.407373 loss_rnnt 8.870667 hw_loss 0.287081 lr 0.00039972 rank 3
2023-02-23 13:11:54,624 DEBUG TRAIN Batch 18/6300 loss 9.577710 loss_att 12.854880 loss_ctc 11.354753 loss_rnnt 8.563964 hw_loss 0.227576 lr 0.00039973 rank 5
2023-02-23 13:11:54,624 DEBUG TRAIN Batch 18/6300 loss 6.059101 loss_att 8.863501 loss_ctc 8.686713 loss_rnnt 5.010974 hw_loss 0.256685 lr 0.00039979 rank 1
2023-02-23 13:11:54,624 DEBUG TRAIN Batch 18/6300 loss 4.865694 loss_att 10.741967 loss_ctc 5.510271 loss_rnnt 3.474381 hw_loss 0.243965 lr 0.00039972 rank 7
2023-02-23 13:11:54,669 DEBUG TRAIN Batch 18/6300 loss 8.663139 loss_att 11.126427 loss_ctc 10.614986 loss_rnnt 7.705545 hw_loss 0.383795 lr 0.00039976 rank 6
2023-02-23 13:13:14,538 DEBUG TRAIN Batch 18/6400 loss 3.744540 loss_att 5.505361 loss_ctc 4.640941 loss_rnnt 3.090650 hw_loss 0.341635 lr 0.00039964 rank 4
2023-02-23 13:13:14,542 DEBUG TRAIN Batch 18/6400 loss 5.079868 loss_att 8.364972 loss_ctc 7.712288 loss_rnnt 3.936228 hw_loss 0.254305 lr 0.00039961 rank 2
2023-02-23 13:13:14,544 DEBUG TRAIN Batch 18/6400 loss 11.840096 loss_att 12.738314 loss_ctc 14.686550 loss_rnnt 11.135335 hw_loss 0.272983 lr 0.00039963 rank 6
2023-02-23 13:13:14,546 DEBUG TRAIN Batch 18/6400 loss 10.065783 loss_att 12.649480 loss_ctc 12.545010 loss_rnnt 9.053813 hw_loss 0.308749 lr 0.00039960 rank 7
2023-02-23 13:13:14,550 DEBUG TRAIN Batch 18/6400 loss 15.492496 loss_att 18.007215 loss_ctc 24.418886 loss_rnnt 13.618521 hw_loss 0.339085 lr 0.00039966 rank 1
2023-02-23 13:13:14,550 DEBUG TRAIN Batch 18/6400 loss 7.864930 loss_att 10.864359 loss_ctc 11.992060 loss_rnnt 6.593357 hw_loss 0.227630 lr 0.00039959 rank 3
2023-02-23 13:13:14,552 DEBUG TRAIN Batch 18/6400 loss 10.997898 loss_att 15.816330 loss_ctc 12.916298 loss_rnnt 9.666296 hw_loss 0.210242 lr 0.00039967 rank 0
2023-02-23 13:13:14,562 DEBUG TRAIN Batch 18/6400 loss 6.946491 loss_att 7.154688 loss_ctc 9.731865 loss_rnnt 6.381987 hw_loss 0.284028 lr 0.00039961 rank 5
2023-02-23 13:14:31,486 DEBUG TRAIN Batch 18/6500 loss 10.431027 loss_att 15.433014 loss_ctc 14.766353 loss_rnnt 8.715006 hw_loss 0.257965 lr 0.00039950 rank 6
2023-02-23 13:14:31,487 DEBUG TRAIN Batch 18/6500 loss 8.529702 loss_att 11.107338 loss_ctc 13.542224 loss_rnnt 7.284144 hw_loss 0.115678 lr 0.00039954 rank 0
2023-02-23 13:14:31,489 DEBUG TRAIN Batch 18/6500 loss 7.929741 loss_att 12.472874 loss_ctc 7.390614 loss_rnnt 6.964943 hw_loss 0.240104 lr 0.00039948 rank 5
2023-02-23 13:14:31,490 DEBUG TRAIN Batch 18/6500 loss 11.528426 loss_att 13.069679 loss_ctc 15.087764 loss_rnnt 10.635229 hw_loss 0.206939 lr 0.00039947 rank 7
2023-02-23 13:14:31,490 DEBUG TRAIN Batch 18/6500 loss 4.541643 loss_att 6.691829 loss_ctc 8.558924 loss_rnnt 3.449806 hw_loss 0.236556 lr 0.00039948 rank 2
2023-02-23 13:14:31,490 DEBUG TRAIN Batch 18/6500 loss 7.745138 loss_att 10.819849 loss_ctc 13.482256 loss_rnnt 6.219578 hw_loss 0.273130 lr 0.00039951 rank 4
2023-02-23 13:14:31,493 DEBUG TRAIN Batch 18/6500 loss 6.563871 loss_att 8.255665 loss_ctc 11.337759 loss_rnnt 5.466669 hw_loss 0.229358 lr 0.00039946 rank 3
2023-02-23 13:14:31,498 DEBUG TRAIN Batch 18/6500 loss 2.333515 loss_att 4.370739 loss_ctc 3.826504 loss_rnnt 1.652746 hw_loss 0.139235 lr 0.00039953 rank 1
2023-02-23 13:15:46,121 DEBUG TRAIN Batch 18/6600 loss 8.885499 loss_att 13.790274 loss_ctc 11.733348 loss_rnnt 7.447620 hw_loss 0.144770 lr 0.00039941 rank 0
2023-02-23 13:15:46,124 DEBUG TRAIN Batch 18/6600 loss 5.771254 loss_att 9.371716 loss_ctc 6.926579 loss_rnnt 4.772705 hw_loss 0.233275 lr 0.00039935 rank 5
2023-02-23 13:15:46,126 DEBUG TRAIN Batch 18/6600 loss 12.770667 loss_att 13.280990 loss_ctc 15.289665 loss_rnnt 12.219011 hw_loss 0.213231 lr 0.00039938 rank 6
2023-02-23 13:15:46,127 DEBUG TRAIN Batch 18/6600 loss 3.612431 loss_att 7.127007 loss_ctc 4.762188 loss_rnnt 2.621975 hw_loss 0.251700 lr 0.00039936 rank 2
2023-02-23 13:15:46,127 DEBUG TRAIN Batch 18/6600 loss 16.817795 loss_att 18.745487 loss_ctc 20.416096 loss_rnnt 15.843752 hw_loss 0.203867 lr 0.00039940 rank 1
2023-02-23 13:15:46,127 DEBUG TRAIN Batch 18/6600 loss 5.079917 loss_att 6.125140 loss_ctc 4.914622 loss_rnnt 4.797698 hw_loss 0.178525 lr 0.00039938 rank 4
2023-02-23 13:15:46,128 DEBUG TRAIN Batch 18/6600 loss 9.806078 loss_att 11.313717 loss_ctc 13.974483 loss_rnnt 8.786714 hw_loss 0.303840 lr 0.00039934 rank 3
2023-02-23 13:15:46,137 DEBUG TRAIN Batch 18/6600 loss 13.069124 loss_att 12.316002 loss_ctc 16.346432 loss_rnnt 12.662621 hw_loss 0.225283 lr 0.00039934 rank 7
2023-02-23 13:17:03,063 DEBUG TRAIN Batch 18/6700 loss 14.431547 loss_att 15.839629 loss_ctc 17.184366 loss_rnnt 13.688787 hw_loss 0.176441 lr 0.00039922 rank 5
2023-02-23 13:17:03,070 DEBUG TRAIN Batch 18/6700 loss 5.602068 loss_att 7.981133 loss_ctc 9.080148 loss_rnnt 4.540869 hw_loss 0.228078 lr 0.00039925 rank 4
2023-02-23 13:17:03,071 DEBUG TRAIN Batch 18/6700 loss 5.121184 loss_att 8.519541 loss_ctc 9.464798 loss_rnnt 3.748058 hw_loss 0.214324 lr 0.00039928 rank 1
2023-02-23 13:17:03,071 DEBUG TRAIN Batch 18/6700 loss 6.585751 loss_att 8.972755 loss_ctc 7.837558 loss_rnnt 5.825546 hw_loss 0.217305 lr 0.00039923 rank 2
2023-02-23 13:17:03,073 DEBUG TRAIN Batch 18/6700 loss 7.977011 loss_att 9.616673 loss_ctc 9.444613 loss_rnnt 7.347878 hw_loss 0.197852 lr 0.00039921 rank 3
2023-02-23 13:17:03,074 DEBUG TRAIN Batch 18/6700 loss 7.746935 loss_att 10.772772 loss_ctc 10.857272 loss_rnnt 6.657924 hw_loss 0.129624 lr 0.00039929 rank 0
2023-02-23 13:17:03,083 DEBUG TRAIN Batch 18/6700 loss 6.384241 loss_att 10.005804 loss_ctc 8.566511 loss_rnnt 5.220587 hw_loss 0.278195 lr 0.00039921 rank 7
2023-02-23 13:17:03,083 DEBUG TRAIN Batch 18/6700 loss 11.807965 loss_att 16.643852 loss_ctc 14.898081 loss_rnnt 10.362369 hw_loss 0.124508 lr 0.00039925 rank 6
2023-02-23 13:18:20,945 DEBUG TRAIN Batch 18/6800 loss 8.584273 loss_att 10.177565 loss_ctc 11.506906 loss_rnnt 7.762363 hw_loss 0.212939 lr 0.00039908 rank 3
2023-02-23 13:18:20,948 DEBUG TRAIN Batch 18/6800 loss 11.778416 loss_att 14.549206 loss_ctc 16.747566 loss_rnnt 10.432980 hw_loss 0.241359 lr 0.00039910 rank 2
2023-02-23 13:18:20,949 DEBUG TRAIN Batch 18/6800 loss 10.207674 loss_att 11.378341 loss_ctc 11.902902 loss_rnnt 9.574861 hw_loss 0.323717 lr 0.00039916 rank 0
2023-02-23 13:18:20,949 DEBUG TRAIN Batch 18/6800 loss 24.170649 loss_att 25.490221 loss_ctc 33.432014 loss_rnnt 22.612976 hw_loss 0.110454 lr 0.00039910 rank 5
2023-02-23 13:18:20,953 DEBUG TRAIN Batch 18/6800 loss 18.688240 loss_att 21.830074 loss_ctc 22.963543 loss_rnnt 17.354939 hw_loss 0.252923 lr 0.00039913 rank 4
2023-02-23 13:18:20,954 DEBUG TRAIN Batch 18/6800 loss 8.221193 loss_att 10.573647 loss_ctc 11.849789 loss_rnnt 7.176574 hw_loss 0.169345 lr 0.00039912 rank 6
2023-02-23 13:18:20,956 DEBUG TRAIN Batch 18/6800 loss 9.565858 loss_att 10.899223 loss_ctc 11.649345 loss_rnnt 8.888605 hw_loss 0.248964 lr 0.00039915 rank 1
2023-02-23 13:18:20,957 DEBUG TRAIN Batch 18/6800 loss 7.004112 loss_att 8.726231 loss_ctc 10.993650 loss_rnnt 6.001140 hw_loss 0.237393 lr 0.00039909 rank 7
2023-02-23 13:19:37,345 DEBUG TRAIN Batch 18/6900 loss 18.145411 loss_att 20.008175 loss_ctc 24.874580 loss_rnnt 16.779242 hw_loss 0.180735 lr 0.00039903 rank 0
2023-02-23 13:19:37,346 DEBUG TRAIN Batch 18/6900 loss 16.862078 loss_att 19.159019 loss_ctc 21.374430 loss_rnnt 15.696280 hw_loss 0.196430 lr 0.00039897 rank 2
2023-02-23 13:19:37,346 DEBUG TRAIN Batch 18/6900 loss 8.660933 loss_att 12.135019 loss_ctc 10.285063 loss_rnnt 7.630791 hw_loss 0.222703 lr 0.00039900 rank 4
2023-02-23 13:19:37,346 DEBUG TRAIN Batch 18/6900 loss 9.272569 loss_att 10.731023 loss_ctc 17.849918 loss_rnnt 7.725167 hw_loss 0.210121 lr 0.00039902 rank 1
2023-02-23 13:19:37,349 DEBUG TRAIN Batch 18/6900 loss 11.364033 loss_att 15.418478 loss_ctc 16.076328 loss_rnnt 9.815890 hw_loss 0.204277 lr 0.00039897 rank 5
2023-02-23 13:19:37,349 DEBUG TRAIN Batch 18/6900 loss 2.262879 loss_att 5.588039 loss_ctc 2.762933 loss_rnnt 1.418412 hw_loss 0.211427 lr 0.00039896 rank 7
2023-02-23 13:19:37,353 DEBUG TRAIN Batch 18/6900 loss 11.831119 loss_att 13.736103 loss_ctc 16.405636 loss_rnnt 10.727656 hw_loss 0.210993 lr 0.00039899 rank 6
2023-02-23 13:19:37,397 DEBUG TRAIN Batch 18/6900 loss 6.312609 loss_att 8.367526 loss_ctc 10.414837 loss_rnnt 5.242974 hw_loss 0.209415 lr 0.00039895 rank 3
2023-02-23 13:20:53,840 DEBUG TRAIN Batch 18/7000 loss 13.149657 loss_att 14.674748 loss_ctc 16.339752 loss_rnnt 12.289509 hw_loss 0.243347 lr 0.00039887 rank 6
2023-02-23 13:20:53,842 DEBUG TRAIN Batch 18/7000 loss 9.634591 loss_att 11.365474 loss_ctc 11.564880 loss_rnnt 8.919016 hw_loss 0.210050 lr 0.00039890 rank 0
2023-02-23 13:20:53,842 DEBUG TRAIN Batch 18/7000 loss 9.139910 loss_att 12.711707 loss_ctc 11.447597 loss_rnnt 8.018244 hw_loss 0.186778 lr 0.00039885 rank 2
2023-02-23 13:20:53,842 DEBUG TRAIN Batch 18/7000 loss 8.244840 loss_att 10.805849 loss_ctc 11.744797 loss_rnnt 7.065688 hw_loss 0.375542 lr 0.00039887 rank 4
2023-02-23 13:20:53,845 DEBUG TRAIN Batch 18/7000 loss 10.434851 loss_att 12.093902 loss_ctc 14.435667 loss_rnnt 9.404598 hw_loss 0.309376 lr 0.00039884 rank 5
2023-02-23 13:20:53,849 DEBUG TRAIN Batch 18/7000 loss 10.333309 loss_att 13.954997 loss_ctc 11.258410 loss_rnnt 9.342025 hw_loss 0.269251 lr 0.00039883 rank 3
2023-02-23 13:20:53,850 DEBUG TRAIN Batch 18/7000 loss 23.425602 loss_att 31.381828 loss_ctc 29.529570 loss_rnnt 20.937757 hw_loss 0.155136 lr 0.00039883 rank 7
2023-02-23 13:20:53,852 DEBUG TRAIN Batch 18/7000 loss 15.325804 loss_att 16.042566 loss_ctc 18.621134 loss_rnnt 14.564009 hw_loss 0.335752 lr 0.00039890 rank 1
2023-02-23 13:22:11,856 DEBUG TRAIN Batch 18/7100 loss 6.994156 loss_att 9.489302 loss_ctc 8.878224 loss_rnnt 6.136793 hw_loss 0.200859 lr 0.00039870 rank 3
2023-02-23 13:22:11,859 DEBUG TRAIN Batch 18/7100 loss 15.861493 loss_att 18.228773 loss_ctc 19.792288 loss_rnnt 14.750003 hw_loss 0.213616 lr 0.00039878 rank 0
2023-02-23 13:22:11,865 DEBUG TRAIN Batch 18/7100 loss 13.165733 loss_att 12.992906 loss_ctc 18.977753 loss_rnnt 12.349940 hw_loss 0.141417 lr 0.00039872 rank 5
2023-02-23 13:22:11,872 DEBUG TRAIN Batch 18/7100 loss 5.660432 loss_att 9.598726 loss_ctc 7.476768 loss_rnnt 4.508244 hw_loss 0.229409 lr 0.00039874 rank 6
2023-02-23 13:22:11,876 DEBUG TRAIN Batch 18/7100 loss 8.654127 loss_att 10.917265 loss_ctc 11.456069 loss_rnnt 7.672511 hw_loss 0.291370 lr 0.00039872 rank 2
2023-02-23 13:22:11,880 DEBUG TRAIN Batch 18/7100 loss 10.245965 loss_att 15.549904 loss_ctc 17.605181 loss_rnnt 8.100746 hw_loss 0.193502 lr 0.00039877 rank 1
2023-02-23 13:22:11,887 DEBUG TRAIN Batch 18/7100 loss 26.828053 loss_att 31.506952 loss_ctc 40.428902 loss_rnnt 23.987671 hw_loss 0.170912 lr 0.00039875 rank 4
2023-02-23 13:22:11,909 DEBUG TRAIN Batch 18/7100 loss 4.989215 loss_att 6.183422 loss_ctc 6.998847 loss_rnnt 4.386697 hw_loss 0.179486 lr 0.00039871 rank 7
2023-02-23 13:23:29,105 DEBUG TRAIN Batch 18/7200 loss 7.578139 loss_att 10.070395 loss_ctc 8.512884 loss_rnnt 6.829286 hw_loss 0.235819 lr 0.00039865 rank 0
2023-02-23 13:23:29,108 DEBUG TRAIN Batch 18/7200 loss 11.647256 loss_att 14.254390 loss_ctc 17.720097 loss_rnnt 10.225134 hw_loss 0.170593 lr 0.00039859 rank 5
2023-02-23 13:23:29,107 DEBUG TRAIN Batch 18/7200 loss 9.898267 loss_att 15.178680 loss_ctc 19.079453 loss_rnnt 7.464044 hw_loss 0.288717 lr 0.00039859 rank 2
2023-02-23 13:23:29,107 DEBUG TRAIN Batch 18/7200 loss 4.931989 loss_att 8.385302 loss_ctc 7.263099 loss_rnnt 3.845778 hw_loss 0.158876 lr 0.00039862 rank 4
2023-02-23 13:23:29,110 DEBUG TRAIN Batch 18/7200 loss 7.042778 loss_att 10.655416 loss_ctc 9.936917 loss_rnnt 5.819261 hw_loss 0.215819 lr 0.00039857 rank 3
2023-02-23 13:23:29,112 DEBUG TRAIN Batch 18/7200 loss 10.273990 loss_att 14.130321 loss_ctc 15.041700 loss_rnnt 8.787436 hw_loss 0.149239 lr 0.00039861 rank 6
2023-02-23 13:23:29,112 DEBUG TRAIN Batch 18/7200 loss 14.898482 loss_att 20.024023 loss_ctc 20.798809 loss_rnnt 12.954350 hw_loss 0.248090 lr 0.00039858 rank 7
2023-02-23 13:23:29,114 DEBUG TRAIN Batch 18/7200 loss 6.671940 loss_att 10.454580 loss_ctc 8.295760 loss_rnnt 5.600127 hw_loss 0.185204 lr 0.00039864 rank 1
2023-02-23 13:24:45,267 DEBUG TRAIN Batch 18/7300 loss 8.129709 loss_att 10.147917 loss_ctc 12.094838 loss_rnnt 7.093757 hw_loss 0.194301 lr 0.00039847 rank 2
2023-02-23 13:24:45,270 DEBUG TRAIN Batch 18/7300 loss 7.704868 loss_att 11.296776 loss_ctc 11.448105 loss_rnnt 6.381226 hw_loss 0.199055 lr 0.00039849 rank 4
2023-02-23 13:24:45,276 DEBUG TRAIN Batch 18/7300 loss 9.069601 loss_att 11.861980 loss_ctc 14.585005 loss_rnnt 7.677678 hw_loss 0.183863 lr 0.00039852 rank 0
2023-02-23 13:24:45,276 DEBUG TRAIN Batch 18/7300 loss 6.714837 loss_att 11.875584 loss_ctc 12.754407 loss_rnnt 4.758337 hw_loss 0.223265 lr 0.00039852 rank 1
2023-02-23 13:24:45,277 DEBUG TRAIN Batch 18/7300 loss 16.564394 loss_att 17.101622 loss_ctc 24.011044 loss_rnnt 15.375172 hw_loss 0.166671 lr 0.00039846 rank 5
2023-02-23 13:24:45,283 DEBUG TRAIN Batch 18/7300 loss 9.891043 loss_att 13.165423 loss_ctc 15.094806 loss_rnnt 8.448559 hw_loss 0.175823 lr 0.00039845 rank 7
2023-02-23 13:24:45,304 DEBUG TRAIN Batch 18/7300 loss 10.002843 loss_att 13.428038 loss_ctc 15.592482 loss_rnnt 8.448751 hw_loss 0.232061 lr 0.00039845 rank 3
2023-02-23 13:24:45,340 DEBUG TRAIN Batch 18/7300 loss 9.992085 loss_att 12.986852 loss_ctc 12.375473 loss_rnnt 8.958051 hw_loss 0.219928 lr 0.00039849 rank 6
2023-02-23 13:26:02,368 DEBUG TRAIN Batch 18/7400 loss 6.516197 loss_att 8.242282 loss_ctc 9.417749 loss_rnnt 5.700234 hw_loss 0.157259 lr 0.00039832 rank 3
2023-02-23 13:26:02,369 DEBUG TRAIN Batch 18/7400 loss 5.951202 loss_att 10.953308 loss_ctc 8.528623 loss_rnnt 4.511687 hw_loss 0.178946 lr 0.00039834 rank 2
2023-02-23 13:26:02,370 DEBUG TRAIN Batch 18/7400 loss 14.757270 loss_att 14.137334 loss_ctc 15.552445 loss_rnnt 14.702139 hw_loss 0.137054 lr 0.00039840 rank 0
2023-02-23 13:26:02,373 DEBUG TRAIN Batch 18/7400 loss 5.730269 loss_att 6.989869 loss_ctc 6.165987 loss_rnnt 5.325962 hw_loss 0.176796 lr 0.00039837 rank 4
2023-02-23 13:26:02,372 DEBUG TRAIN Batch 18/7400 loss 5.964094 loss_att 8.323427 loss_ctc 8.092458 loss_rnnt 5.050943 hw_loss 0.295317 lr 0.00039836 rank 6
2023-02-23 13:26:02,376 DEBUG TRAIN Batch 18/7400 loss 16.234180 loss_att 17.383205 loss_ctc 18.775068 loss_rnnt 15.543633 hw_loss 0.228674 lr 0.00039839 rank 1
2023-02-23 13:26:02,376 DEBUG TRAIN Batch 18/7400 loss 9.663559 loss_att 10.951520 loss_ctc 11.140269 loss_rnnt 9.125160 hw_loss 0.157336 lr 0.00039833 rank 7
2023-02-23 13:26:02,377 DEBUG TRAIN Batch 18/7400 loss 16.642067 loss_att 22.004045 loss_ctc 20.581791 loss_rnnt 14.884552 hw_loss 0.299669 lr 0.00039834 rank 5
2023-02-23 13:27:20,484 DEBUG TRAIN Batch 18/7500 loss 5.760359 loss_att 7.571223 loss_ctc 6.475444 loss_rnnt 5.218046 hw_loss 0.158993 lr 0.00039826 rank 1
2023-02-23 13:27:20,485 DEBUG TRAIN Batch 18/7500 loss 6.033628 loss_att 9.302045 loss_ctc 8.578477 loss_rnnt 4.938589 hw_loss 0.191329 lr 0.00039824 rank 4
2023-02-23 13:27:20,485 DEBUG TRAIN Batch 18/7500 loss 5.762397 loss_att 6.622358 loss_ctc 7.853790 loss_rnnt 5.210683 hw_loss 0.189130 lr 0.00039821 rank 2
2023-02-23 13:27:20,489 DEBUG TRAIN Batch 18/7500 loss 14.275287 loss_att 17.554863 loss_ctc 18.920340 loss_rnnt 12.851756 hw_loss 0.278017 lr 0.00039827 rank 0
2023-02-23 13:27:20,490 DEBUG TRAIN Batch 18/7500 loss 10.328651 loss_att 12.275402 loss_ctc 14.378565 loss_rnnt 9.290222 hw_loss 0.204543 lr 0.00039821 rank 5
2023-02-23 13:27:20,492 DEBUG TRAIN Batch 18/7500 loss 13.163304 loss_att 16.509094 loss_ctc 14.247551 loss_rnnt 12.267540 hw_loss 0.153825 lr 0.00039820 rank 7
2023-02-23 13:27:20,493 DEBUG TRAIN Batch 18/7500 loss 13.770265 loss_att 15.325874 loss_ctc 19.378550 loss_rnnt 12.589378 hw_loss 0.228737 lr 0.00039823 rank 6
2023-02-23 13:27:20,531 DEBUG TRAIN Batch 18/7500 loss 11.478619 loss_att 13.730722 loss_ctc 16.581503 loss_rnnt 10.183473 hw_loss 0.308138 lr 0.00039819 rank 3
2023-02-23 13:28:36,532 DEBUG TRAIN Batch 18/7600 loss 14.829392 loss_att 16.498302 loss_ctc 16.556934 loss_rnnt 14.179903 hw_loss 0.160065 lr 0.00039808 rank 5
2023-02-23 13:28:36,535 DEBUG TRAIN Batch 18/7600 loss 5.697577 loss_att 8.913298 loss_ctc 7.732372 loss_rnnt 4.664390 hw_loss 0.222634 lr 0.00039811 rank 4
2023-02-23 13:28:36,535 DEBUG TRAIN Batch 18/7600 loss 9.116482 loss_att 9.950207 loss_ctc 12.590713 loss_rnnt 8.341136 hw_loss 0.272569 lr 0.00039809 rank 2
2023-02-23 13:28:36,536 DEBUG TRAIN Batch 18/7600 loss 6.251461 loss_att 6.504724 loss_ctc 8.582506 loss_rnnt 5.708167 hw_loss 0.340942 lr 0.00039807 rank 3
2023-02-23 13:28:36,536 DEBUG TRAIN Batch 18/7600 loss 7.557712 loss_att 8.364103 loss_ctc 8.759214 loss_rnnt 7.089398 hw_loss 0.275318 lr 0.00039814 rank 0
2023-02-23 13:28:36,540 DEBUG TRAIN Batch 18/7600 loss 7.156147 loss_att 10.274483 loss_ctc 10.947947 loss_rnnt 5.931192 hw_loss 0.179464 lr 0.00039807 rank 7
2023-02-23 13:28:36,542 DEBUG TRAIN Batch 18/7600 loss 11.702004 loss_att 13.315583 loss_ctc 16.601721 loss_rnnt 10.588954 hw_loss 0.256949 lr 0.00039811 rank 6
2023-02-23 13:28:36,589 DEBUG TRAIN Batch 18/7600 loss 7.193415 loss_att 10.675737 loss_ctc 12.447820 loss_rnnt 5.674668 hw_loss 0.228176 lr 0.00039814 rank 1
2023-02-23 13:29:53,728 DEBUG TRAIN Batch 18/7700 loss 8.058276 loss_att 11.000791 loss_ctc 11.712570 loss_rnnt 6.836842 hw_loss 0.273174 lr 0.00039799 rank 4
2023-02-23 13:29:53,730 DEBUG TRAIN Batch 18/7700 loss 7.408638 loss_att 7.710675 loss_ctc 9.093322 loss_rnnt 6.941705 hw_loss 0.341065 lr 0.00039796 rank 5
2023-02-23 13:29:53,734 DEBUG TRAIN Batch 18/7700 loss 6.499749 loss_att 12.263771 loss_ctc 12.954161 loss_rnnt 4.406882 hw_loss 0.149014 lr 0.00039802 rank 0
2023-02-23 13:29:53,734 DEBUG TRAIN Batch 18/7700 loss 11.304379 loss_att 12.274349 loss_ctc 17.025267 loss_rnnt 10.204700 hw_loss 0.267937 lr 0.00039796 rank 2
2023-02-23 13:29:53,735 DEBUG TRAIN Batch 18/7700 loss 15.626171 loss_att 18.950788 loss_ctc 21.694143 loss_rnnt 14.038524 hw_loss 0.213112 lr 0.00039795 rank 7
2023-02-23 13:29:53,737 DEBUG TRAIN Batch 18/7700 loss 5.438146 loss_att 8.811105 loss_ctc 12.155257 loss_rnnt 3.742088 hw_loss 0.235969 lr 0.00039794 rank 3
2023-02-23 13:29:53,739 DEBUG TRAIN Batch 18/7700 loss 14.713803 loss_att 15.092010 loss_ctc 17.597071 loss_rnnt 14.072445 hw_loss 0.339901 lr 0.00039798 rank 6
2023-02-23 13:29:53,782 DEBUG TRAIN Batch 18/7700 loss 8.568510 loss_att 8.352910 loss_ctc 11.807644 loss_rnnt 8.032017 hw_loss 0.276993 lr 0.00039801 rank 1
2023-02-23 13:31:13,105 DEBUG TRAIN Batch 18/7800 loss 9.148820 loss_att 12.574051 loss_ctc 13.023260 loss_rnnt 7.869556 hw_loss 0.145548 lr 0.00039789 rank 0
2023-02-23 13:31:13,107 DEBUG TRAIN Batch 18/7800 loss 5.706141 loss_att 11.349197 loss_ctc 8.194979 loss_rnnt 4.171504 hw_loss 0.139089 lr 0.00039782 rank 3
2023-02-23 13:31:13,107 DEBUG TRAIN Batch 18/7800 loss 12.859624 loss_att 13.105629 loss_ctc 14.967789 loss_rnnt 12.414000 hw_loss 0.216255 lr 0.00039783 rank 5
2023-02-23 13:31:13,113 DEBUG TRAIN Batch 18/7800 loss 13.605833 loss_att 16.016689 loss_ctc 23.476818 loss_rnnt 11.734938 hw_loss 0.136114 lr 0.00039784 rank 2
2023-02-23 13:31:13,111 DEBUG TRAIN Batch 18/7800 loss 18.723976 loss_att 16.790184 loss_ctc 19.428661 loss_rnnt 18.906897 hw_loss 0.206026 lr 0.00039782 rank 7
2023-02-23 13:31:13,124 DEBUG TRAIN Batch 18/7800 loss 8.140409 loss_att 10.209789 loss_ctc 12.689186 loss_rnnt 6.999507 hw_loss 0.225979 lr 0.00039786 rank 4
2023-02-23 13:31:13,142 DEBUG TRAIN Batch 18/7800 loss 3.422710 loss_att 9.156330 loss_ctc 3.494760 loss_rnnt 2.133669 hw_loss 0.248831 lr 0.00039788 rank 1
2023-02-23 13:31:13,161 DEBUG TRAIN Batch 18/7800 loss 2.632862 loss_att 4.196224 loss_ctc 5.562776 loss_rnnt 1.838057 hw_loss 0.171519 lr 0.00039786 rank 6
2023-02-23 13:32:30,399 DEBUG TRAIN Batch 18/7900 loss 8.827475 loss_att 11.278696 loss_ctc 11.914648 loss_rnnt 7.760997 hw_loss 0.308645 lr 0.00039771 rank 5
2023-02-23 13:32:30,407 DEBUG TRAIN Batch 18/7900 loss 10.519506 loss_att 11.365282 loss_ctc 12.354340 loss_rnnt 9.999428 hw_loss 0.199276 lr 0.00039773 rank 4
2023-02-23 13:32:30,407 DEBUG TRAIN Batch 18/7900 loss 13.974685 loss_att 14.016052 loss_ctc 18.484417 loss_rnnt 13.258875 hw_loss 0.199197 lr 0.00039777 rank 0
2023-02-23 13:32:30,408 DEBUG TRAIN Batch 18/7900 loss 5.594559 loss_att 8.229310 loss_ctc 6.505498 loss_rnnt 4.832504 hw_loss 0.213088 lr 0.00039773 rank 6
2023-02-23 13:32:30,410 DEBUG TRAIN Batch 18/7900 loss 10.382829 loss_att 13.919833 loss_ctc 20.217926 loss_rnnt 8.266684 hw_loss 0.182621 lr 0.00039771 rank 2
2023-02-23 13:32:30,411 DEBUG TRAIN Batch 18/7900 loss 12.495189 loss_att 15.084053 loss_ctc 19.745403 loss_rnnt 10.933716 hw_loss 0.144383 lr 0.00039770 rank 7
2023-02-23 13:32:30,412 DEBUG TRAIN Batch 18/7900 loss 12.886972 loss_att 13.936984 loss_ctc 16.676027 loss_rnnt 12.055242 hw_loss 0.218477 lr 0.00039769 rank 3
2023-02-23 13:32:30,415 DEBUG TRAIN Batch 18/7900 loss 14.904039 loss_att 16.358295 loss_ctc 24.338223 loss_rnnt 13.272131 hw_loss 0.155935 lr 0.00039776 rank 1
2023-02-23 13:33:46,336 DEBUG TRAIN Batch 18/8000 loss 6.441306 loss_att 7.977164 loss_ctc 9.571617 loss_rnnt 5.582618 hw_loss 0.251516 lr 0.00039760 rank 6
2023-02-23 13:33:46,338 DEBUG TRAIN Batch 18/8000 loss 7.629987 loss_att 9.288593 loss_ctc 9.843545 loss_rnnt 6.923182 hw_loss 0.149893 lr 0.00039757 rank 7
2023-02-23 13:33:46,341 DEBUG TRAIN Batch 18/8000 loss 11.918168 loss_att 17.522709 loss_ctc 20.661730 loss_rnnt 9.475124 hw_loss 0.293115 lr 0.00039756 rank 3
2023-02-23 13:33:46,341 DEBUG TRAIN Batch 18/8000 loss 13.288359 loss_att 18.388832 loss_ctc 18.342712 loss_rnnt 11.465032 hw_loss 0.242473 lr 0.00039764 rank 0
2023-02-23 13:33:46,341 DEBUG TRAIN Batch 18/8000 loss 15.919079 loss_att 20.232218 loss_ctc 24.670351 loss_rnnt 13.790345 hw_loss 0.186129 lr 0.00039758 rank 2
2023-02-23 13:33:46,342 DEBUG TRAIN Batch 18/8000 loss 10.223441 loss_att 13.159502 loss_ctc 12.713861 loss_rnnt 9.202131 hw_loss 0.191326 lr 0.00039758 rank 5
2023-02-23 13:33:46,344 DEBUG TRAIN Batch 18/8000 loss 9.512082 loss_att 10.937540 loss_ctc 12.485594 loss_rnnt 8.760250 hw_loss 0.131760 lr 0.00039761 rank 4
2023-02-23 13:33:46,393 DEBUG TRAIN Batch 18/8000 loss 4.452816 loss_att 7.819342 loss_ctc 7.838262 loss_rnnt 3.223765 hw_loss 0.195663 lr 0.00039763 rank 1
2023-02-23 13:35:03,703 DEBUG TRAIN Batch 18/8100 loss 8.399227 loss_att 11.376040 loss_ctc 11.820896 loss_rnnt 7.184742 hw_loss 0.305438 lr 0.00039746 rank 2
2023-02-23 13:35:03,704 DEBUG TRAIN Batch 18/8100 loss 6.276157 loss_att 8.583312 loss_ctc 7.366755 loss_rnnt 5.576916 hw_loss 0.173245 lr 0.00039745 rank 5
2023-02-23 13:35:03,706 DEBUG TRAIN Batch 18/8100 loss 7.203650 loss_att 9.778435 loss_ctc 10.717589 loss_rnnt 6.065340 hw_loss 0.290301 lr 0.00039744 rank 3
2023-02-23 13:35:03,706 DEBUG TRAIN Batch 18/8100 loss 10.993468 loss_att 13.053982 loss_ctc 14.217427 loss_rnnt 9.975549 hw_loss 0.329916 lr 0.00039744 rank 7
2023-02-23 13:35:03,707 DEBUG TRAIN Batch 18/8100 loss 3.373171 loss_att 6.259466 loss_ctc 4.951077 loss_rnnt 2.462187 hw_loss 0.231257 lr 0.00039748 rank 4
2023-02-23 13:35:03,710 DEBUG TRAIN Batch 18/8100 loss 9.220634 loss_att 10.759792 loss_ctc 13.160166 loss_rnnt 8.268598 hw_loss 0.222997 lr 0.00039751 rank 0
2023-02-23 13:35:03,710 DEBUG TRAIN Batch 18/8100 loss 12.977192 loss_att 16.383608 loss_ctc 23.786137 loss_rnnt 10.770871 hw_loss 0.157207 lr 0.00039748 rank 6
2023-02-23 13:35:03,718 DEBUG TRAIN Batch 18/8100 loss 7.319879 loss_att 10.296064 loss_ctc 9.107365 loss_rnnt 6.376771 hw_loss 0.205385 lr 0.00039751 rank 1
2023-02-23 13:36:20,009 DEBUG TRAIN Batch 18/8200 loss 8.630218 loss_att 10.333815 loss_ctc 11.944903 loss_rnnt 7.690292 hw_loss 0.294838 lr 0.00039739 rank 0
2023-02-23 13:36:20,009 DEBUG TRAIN Batch 18/8200 loss 5.839848 loss_att 9.404741 loss_ctc 8.374056 loss_rnnt 4.657022 hw_loss 0.247410 lr 0.00039736 rank 4
2023-02-23 13:36:20,012 DEBUG TRAIN Batch 18/8200 loss 5.458274 loss_att 7.486677 loss_ctc 6.504217 loss_rnnt 4.768964 hw_loss 0.270320 lr 0.00039731 rank 3
2023-02-23 13:36:20,012 DEBUG TRAIN Batch 18/8200 loss 14.285804 loss_att 16.285196 loss_ctc 21.805771 loss_rnnt 12.771056 hw_loss 0.210388 lr 0.00039733 rank 2
2023-02-23 13:36:20,014 DEBUG TRAIN Batch 18/8200 loss 4.649572 loss_att 11.489703 loss_ctc 7.155674 loss_rnnt 2.858924 hw_loss 0.165892 lr 0.00039732 rank 7
2023-02-23 13:36:20,015 DEBUG TRAIN Batch 18/8200 loss 10.041540 loss_att 10.761570 loss_ctc 14.955352 loss_rnnt 9.134818 hw_loss 0.201641 lr 0.00039733 rank 5
2023-02-23 13:36:20,017 DEBUG TRAIN Batch 18/8200 loss 7.090120 loss_att 8.446995 loss_ctc 10.725808 loss_rnnt 6.244666 hw_loss 0.167476 lr 0.00039735 rank 6
2023-02-23 13:36:20,019 DEBUG TRAIN Batch 18/8200 loss 5.953209 loss_att 9.407898 loss_ctc 8.434209 loss_rnnt 4.839676 hw_loss 0.172116 lr 0.00039738 rank 1
2023-02-23 13:37:37,355 DEBUG TRAIN Batch 18/8300 loss 6.751179 loss_att 9.669262 loss_ctc 10.388691 loss_rnnt 5.529728 hw_loss 0.286563 lr 0.00039720 rank 5
2023-02-23 13:37:37,357 DEBUG TRAIN Batch 18/8300 loss 4.178846 loss_att 6.874516 loss_ctc 6.090178 loss_rnnt 3.236162 hw_loss 0.278823 lr 0.00039721 rank 2
2023-02-23 13:37:37,358 DEBUG TRAIN Batch 18/8300 loss 7.227847 loss_att 10.696929 loss_ctc 12.049580 loss_rnnt 5.738026 hw_loss 0.287075 lr 0.00039726 rank 1
2023-02-23 13:37:37,360 DEBUG TRAIN Batch 18/8300 loss 9.192420 loss_att 12.399974 loss_ctc 14.929268 loss_rnnt 7.615081 hw_loss 0.320465 lr 0.00039719 rank 3
2023-02-23 13:37:37,360 DEBUG TRAIN Batch 18/8300 loss 4.596424 loss_att 8.114963 loss_ctc 9.575148 loss_rnnt 3.107274 hw_loss 0.228023 lr 0.00039723 rank 4
2023-02-23 13:37:37,361 DEBUG TRAIN Batch 18/8300 loss 9.869341 loss_att 13.281159 loss_ctc 15.751225 loss_rnnt 8.283724 hw_loss 0.223128 lr 0.00039719 rank 7
2023-02-23 13:37:37,363 DEBUG TRAIN Batch 18/8300 loss 7.479079 loss_att 8.565342 loss_ctc 10.149736 loss_rnnt 6.768244 hw_loss 0.257802 lr 0.00039726 rank 0
2023-02-23 13:37:37,363 DEBUG TRAIN Batch 18/8300 loss 6.583382 loss_att 10.123283 loss_ctc 8.731545 loss_rnnt 5.508208 hw_loss 0.151446 lr 0.00039723 rank 6
2023-02-23 13:38:34,943 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 4
2023-02-23 13:38:34,946 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 2
2023-02-23 13:38:34,949 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 6
2023-02-23 13:38:34,950 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 1
2023-02-23 13:38:34,951 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 3
2023-02-23 13:38:34,954 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 5
2023-02-23 13:38:34,954 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 0
2023-02-23 13:38:34,960 DEBUG CV Batch 18/0 loss 2.101510 loss_att 1.990019 loss_ctc 2.686376 loss_rnnt 1.884875 hw_loss 0.301783 history loss 2.023676 rank 7
2023-02-23 13:38:46,103 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 4
2023-02-23 13:38:46,128 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 3
2023-02-23 13:38:46,193 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 1
2023-02-23 13:38:46,196 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 5
2023-02-23 13:38:46,228 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 0
2023-02-23 13:38:46,282 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 2
2023-02-23 13:38:46,456 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 6
2023-02-23 13:38:46,860 DEBUG CV Batch 18/100 loss 6.661433 loss_att 7.253493 loss_ctc 9.597109 loss_rnnt 6.009161 hw_loss 0.267068 history loss 3.504682 rank 7
2023-02-23 13:39:00,074 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 5
2023-02-23 13:39:00,319 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 4
2023-02-23 13:39:00,401 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 2
2023-02-23 13:39:00,448 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 6
2023-02-23 13:39:00,494 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 0
2023-02-23 13:39:00,508 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 3
2023-02-23 13:39:00,670 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 1
2023-02-23 13:39:00,913 DEBUG CV Batch 18/200 loss 5.022972 loss_att 11.167166 loss_ctc 5.553983 loss_rnnt 3.626859 hw_loss 0.180887 history loss 4.154716 rank 7
2023-02-23 13:39:12,027 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 5
2023-02-23 13:39:12,337 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 4
2023-02-23 13:39:12,370 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 2
2023-02-23 13:39:12,541 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 3
2023-02-23 13:39:12,599 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 0
2023-02-23 13:39:12,870 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 1
2023-02-23 13:39:13,237 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 7
2023-02-23 13:39:13,403 DEBUG CV Batch 18/300 loss 5.173135 loss_att 5.341646 loss_ctc 7.651132 loss_rnnt 4.664834 hw_loss 0.270373 history loss 4.332081 rank 6
2023-02-23 13:39:23,878 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 5
2023-02-23 13:39:24,223 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 4
2023-02-23 13:39:24,259 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 2
2023-02-23 13:39:24,584 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 3
2023-02-23 13:39:24,620 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 0
2023-02-23 13:39:24,960 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 1
2023-02-23 13:39:25,446 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 7
2023-02-23 13:39:25,669 DEBUG CV Batch 18/400 loss 21.813503 loss_att 91.511017 loss_ctc 12.967809 loss_rnnt 9.013306 hw_loss 0.075224 history loss 5.324126 rank 6
2023-02-23 13:39:34,303 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 5
2023-02-23 13:39:34,668 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 2
2023-02-23 13:39:34,696 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 4
2023-02-23 13:39:35,129 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 0
2023-02-23 13:39:35,144 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 3
2023-02-23 13:39:35,517 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 1
2023-02-23 13:39:36,242 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 7
2023-02-23 13:39:37,032 DEBUG CV Batch 18/500 loss 8.175965 loss_att 7.826007 loss_ctc 10.910354 loss_rnnt 7.760395 hw_loss 0.226831 history loss 6.173997 rank 6
2023-02-23 13:39:46,324 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 5
2023-02-23 13:39:46,653 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 2
2023-02-23 13:39:46,763 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 4
2023-02-23 13:39:47,202 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 0
2023-02-23 13:39:47,735 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 1
2023-02-23 13:39:47,853 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 3
2023-02-23 13:39:48,619 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 7
2023-02-23 13:39:49,396 DEBUG CV Batch 18/600 loss 7.788564 loss_att 7.153080 loss_ctc 9.359782 loss_rnnt 7.528919 hw_loss 0.332337 history loss 7.133735 rank 6
2023-02-23 13:39:58,575 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 0
2023-02-23 13:39:58,599 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 5
2023-02-23 13:39:58,916 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 4
2023-02-23 13:39:58,984 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 2
2023-02-23 13:39:59,785 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 1
2023-02-23 13:40:00,524 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 3
2023-02-23 13:40:00,872 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 6
2023-02-23 13:40:01,043 DEBUG CV Batch 18/700 loss 12.817601 loss_att 49.573032 loss_ctc 15.258784 loss_rnnt 5.081552 hw_loss 0.111512 history loss 7.832953 rank 7
2023-02-23 13:40:10,557 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 0
2023-02-23 13:40:10,734 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 5
2023-02-23 13:40:10,828 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 2
2023-02-23 13:40:10,885 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 4
2023-02-23 13:40:12,061 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 1
2023-02-23 13:40:12,350 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 6
2023-02-23 13:40:12,905 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 3
2023-02-23 13:40:13,376 DEBUG CV Batch 18/800 loss 11.411131 loss_att 11.115723 loss_ctc 14.988602 loss_rnnt 10.823199 hw_loss 0.318782 history loss 7.251282 rank 7
2023-02-23 13:40:24,506 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 0
2023-02-23 13:40:24,750 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 2
2023-02-23 13:40:24,828 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 5
2023-02-23 13:40:24,956 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 4
2023-02-23 13:40:26,149 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 1
2023-02-23 13:40:26,303 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 6
2023-02-23 13:40:27,350 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 3
2023-02-23 13:40:28,158 DEBUG CV Batch 18/900 loss 12.037937 loss_att 13.084692 loss_ctc 17.665609 loss_rnnt 11.021239 hw_loss 0.106859 history loss 7.044234 rank 7
2023-02-23 13:40:36,713 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 0
2023-02-23 13:40:36,927 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 2
2023-02-23 13:40:36,961 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 5
2023-02-23 13:40:37,142 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 4
2023-02-23 13:40:38,495 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 1
2023-02-23 13:40:38,580 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 6
2023-02-23 13:40:39,588 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 3
2023-02-23 13:40:40,581 DEBUG CV Batch 18/1000 loss 5.517934 loss_att 5.746049 loss_ctc 7.184307 loss_rnnt 5.111870 hw_loss 0.259234 history loss 6.802875 rank 7
2023-02-23 13:40:48,602 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 0
2023-02-23 13:40:48,697 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 2
2023-02-23 13:40:48,869 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 5
2023-02-23 13:40:49,051 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 4
2023-02-23 13:40:50,581 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 1
2023-02-23 13:40:50,669 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 6
2023-02-23 13:40:51,460 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 3
2023-02-23 13:40:52,654 DEBUG CV Batch 18/1100 loss 7.157323 loss_att 5.997545 loss_ctc 8.415768 loss_rnnt 7.064996 hw_loss 0.293418 history loss 6.782204 rank 7
2023-02-23 13:40:59,081 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 2
2023-02-23 13:40:59,136 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 0
2023-02-23 13:40:59,307 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 5
2023-02-23 13:40:59,494 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 4
2023-02-23 13:41:01,205 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 1
2023-02-23 13:41:01,293 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 6
2023-02-23 13:41:02,399 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 3
2023-02-23 13:41:03,362 DEBUG CV Batch 18/1200 loss 10.352782 loss_att 10.586470 loss_ctc 12.935783 loss_rnnt 9.854980 hw_loss 0.199997 history loss 7.140244 rank 7
2023-02-23 13:41:10,941 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 2
2023-02-23 13:41:11,130 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 0
2023-02-23 13:41:11,209 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 5
2023-02-23 13:41:11,401 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 4
2023-02-23 13:41:13,341 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 1
2023-02-23 13:41:13,358 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 6
2023-02-23 13:41:15,564 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 7
2023-02-23 13:41:15,751 DEBUG CV Batch 18/1300 loss 5.811362 loss_att 6.066989 loss_ctc 7.759312 loss_rnnt 5.353205 hw_loss 0.276196 history loss 7.456096 rank 3
2023-02-23 13:41:22,302 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 0
2023-02-23 13:41:22,566 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 5
2023-02-23 13:41:22,963 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 2
2023-02-23 13:41:23,054 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 4
2023-02-23 13:41:24,720 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 6
2023-02-23 13:41:25,615 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 1
2023-02-23 13:41:26,956 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 7
2023-02-23 13:41:27,527 DEBUG CV Batch 18/1400 loss 8.365125 loss_att 25.099422 loss_ctc 7.740803 loss_rnnt 5.004086 hw_loss 0.182667 history loss 7.798770 rank 3
2023-02-23 13:41:34,411 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 0
2023-02-23 13:41:34,845 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 5
2023-02-23 13:41:35,400 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 2
2023-02-23 13:41:35,426 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 4
2023-02-23 13:41:37,480 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 6
2023-02-23 13:41:38,207 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 1
2023-02-23 13:41:38,554 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 7
2023-02-23 13:41:38,924 DEBUG CV Batch 18/1500 loss 7.221581 loss_att 8.915654 loss_ctc 8.854988 loss_rnnt 6.570653 hw_loss 0.176861 history loss 7.601847 rank 3
2023-02-23 13:41:47,946 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 0
2023-02-23 13:41:48,542 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 5
2023-02-23 13:41:48,983 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 4
2023-02-23 13:41:49,100 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 2
2023-02-23 13:41:51,027 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 6
2023-02-23 13:41:52,018 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 1
2023-02-23 13:41:52,049 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 3
2023-02-23 13:41:52,313 DEBUG CV Batch 18/1600 loss 10.523182 loss_att 13.206919 loss_ctc 11.427324 loss_rnnt 9.706125 hw_loss 0.299544 history loss 7.520782 rank 7
2023-02-23 13:42:00,501 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 0
2023-02-23 13:42:00,913 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 5
2023-02-23 13:42:01,418 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 4
2023-02-23 13:42:01,505 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 2
2023-02-23 13:42:03,661 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 6
2023-02-23 13:42:04,644 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 1
2023-02-23 13:42:04,748 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 3
2023-02-23 13:42:04,959 DEBUG CV Batch 18/1700 loss 10.316247 loss_att 9.068905 loss_ctc 15.667253 loss_rnnt 9.701037 hw_loss 0.283520 history loss 7.416539 rank 7
2023-02-23 13:42:09,912 INFO Epoch 18 CV info cv_loss 7.371141028121676
2023-02-23 13:42:09,913 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt
2023-02-23 13:42:10,059 INFO Epoch 18 CV info cv_loss 7.371141028001071
2023-02-23 13:42:10,061 INFO Epoch 19 TRAIN info lr 0.0003971657703457328
2023-02-23 13:42:10,064 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:10,634 INFO Epoch 18 CV info cv_loss 7.371141029418178
2023-02-23 13:42:10,635 INFO Epoch 19 TRAIN info lr 0.0003971507353947688
2023-02-23 13:42:10,634 INFO Epoch 18 CV info cv_loss 7.37114102841888
2023-02-23 13:42:10,635 INFO Epoch 19 TRAIN info lr 0.00039720461854039486
2023-02-23 13:42:10,638 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:10,640 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:11,257 INFO Epoch 19 TRAIN info lr 0.00039721715264806444
2023-02-23 13:42:11,262 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:13,034 INFO Epoch 18 CV info cv_loss 7.371141027604798
2023-02-23 13:42:13,036 INFO Epoch 19 TRAIN info lr 0.00039719709864530537
2023-02-23 13:42:13,041 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:13,874 INFO Epoch 18 CV info cv_loss 7.371141028705317
2023-02-23 13:42:13,876 INFO Epoch 19 TRAIN info lr 0.0003972284343595442
2023-02-23 13:42:13,880 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:13,895 INFO Epoch 18 CV info cv_loss 7.371141027643564
2023-02-23 13:42:13,896 INFO Epoch 19 TRAIN info lr 0.00039713069144929353
2023-02-23 13:42:13,900 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:42:14,960 INFO Epoch 18 CV info cv_loss 7.371141029973822
2023-02-23 13:42:14,961 INFO Epoch 19 TRAIN info lr 0.0003971068932052855
2023-02-23 13:42:14,964 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 13:43:26,867 DEBUG TRAIN Batch 19/0 loss 6.667569 loss_att 6.614873 loss_ctc 8.427257 loss_rnnt 6.272103 hw_loss 0.321337 lr 0.00039722 rank 0
2023-02-23 13:43:26,867 DEBUG TRAIN Batch 19/0 loss 10.709025 loss_att 11.083680 loss_ctc 13.342538 loss_rnnt 10.118916 hw_loss 0.307583 lr 0.00039715 rank 2
2023-02-23 13:43:26,869 DEBUG TRAIN Batch 19/0 loss 11.113746 loss_att 10.074791 loss_ctc 13.307972 loss_rnnt 10.906950 hw_loss 0.228794 lr 0.00039716 rank 5
2023-02-23 13:43:26,870 DEBUG TRAIN Batch 19/0 loss 11.048556 loss_att 9.991716 loss_ctc 12.917725 loss_rnnt 10.861273 hw_loss 0.280181 lr 0.00039720 rank 4
2023-02-23 13:43:26,904 DEBUG TRAIN Batch 19/0 loss 10.677597 loss_att 10.247189 loss_ctc 13.956863 loss_rnnt 10.158336 hw_loss 0.315204 lr 0.00039723 rank 1
2023-02-23 13:43:26,911 DEBUG TRAIN Batch 19/0 loss 9.972528 loss_att 9.581800 loss_ctc 13.840931 loss_rnnt 9.351071 hw_loss 0.344650 lr 0.00039713 rank 3
2023-02-23 13:43:26,942 DEBUG TRAIN Batch 19/0 loss 10.841571 loss_att 10.051516 loss_ctc 12.711725 loss_rnnt 10.581833 hw_loss 0.315741 lr 0.00039720 rank 6
2023-02-23 13:43:26,948 DEBUG TRAIN Batch 19/0 loss 10.572541 loss_att 9.762372 loss_ctc 12.327116 loss_rnnt 10.322259 hw_loss 0.334448 lr 0.00039711 rank 7
2023-02-23 13:44:42,220 DEBUG TRAIN Batch 19/100 loss 10.661340 loss_att 12.019723 loss_ctc 14.732965 loss_rnnt 9.715202 hw_loss 0.246709 lr 0.00039707 rank 6
2023-02-23 13:44:42,220 DEBUG TRAIN Batch 19/100 loss 12.140537 loss_att 15.797750 loss_ctc 13.631595 loss_rnnt 11.075527 hw_loss 0.252676 lr 0.00039704 rank 5
2023-02-23 13:44:42,221 DEBUG TRAIN Batch 19/100 loss 1.222484 loss_att 3.585622 loss_ctc 1.605042 loss_rnnt 0.609538 hw_loss 0.167457 lr 0.00039708 rank 4
2023-02-23 13:44:42,224 DEBUG TRAIN Batch 19/100 loss 11.819468 loss_att 15.822199 loss_ctc 17.644855 loss_rnnt 10.126904 hw_loss 0.216184 lr 0.00039710 rank 1
2023-02-23 13:44:42,224 DEBUG TRAIN Batch 19/100 loss 7.177830 loss_att 13.195653 loss_ctc 10.146935 loss_rnnt 5.454823 hw_loss 0.231677 lr 0.00039700 rank 3
2023-02-23 13:44:42,224 DEBUG TRAIN Batch 19/100 loss 7.870577 loss_att 10.895588 loss_ctc 8.794597 loss_rnnt 7.046138 hw_loss 0.180438 lr 0.00039709 rank 0
2023-02-23 13:44:42,226 DEBUG TRAIN Batch 19/100 loss 11.679375 loss_att 14.262966 loss_ctc 13.854309 loss_rnnt 10.756186 hw_loss 0.218400 lr 0.00039702 rank 2
2023-02-23 13:44:42,273 DEBUG TRAIN Batch 19/100 loss 5.967689 loss_att 8.052345 loss_ctc 7.609361 loss_rnnt 5.219545 hw_loss 0.210605 lr 0.00039698 rank 7
2023-02-23 13:45:57,050 DEBUG TRAIN Batch 19/200 loss 4.877376 loss_att 6.456793 loss_ctc 7.682903 loss_rnnt 4.059448 hw_loss 0.239952 lr 0.00039691 rank 5
2023-02-23 13:45:57,049 DEBUG TRAIN Batch 19/200 loss 9.662750 loss_att 12.689274 loss_ctc 11.905783 loss_rnnt 8.682702 hw_loss 0.141885 lr 0.00039688 rank 3
2023-02-23 13:45:57,051 DEBUG TRAIN Batch 19/200 loss 6.390252 loss_att 7.698016 loss_ctc 9.740863 loss_rnnt 5.554193 hw_loss 0.239546 lr 0.00039697 rank 0
2023-02-23 13:45:57,051 DEBUG TRAIN Batch 19/200 loss 17.409124 loss_att 19.777805 loss_ctc 20.966612 loss_rnnt 16.397209 hw_loss 0.119708 lr 0.00039695 rank 4
2023-02-23 13:45:57,053 DEBUG TRAIN Batch 19/200 loss 8.609075 loss_att 12.326950 loss_ctc 13.097025 loss_rnnt 7.180809 hw_loss 0.161808 lr 0.00039690 rank 2
2023-02-23 13:45:57,054 DEBUG TRAIN Batch 19/200 loss 4.547121 loss_att 7.560019 loss_ctc 6.857019 loss_rnnt 3.510032 hw_loss 0.237231 lr 0.00039686 rank 7
2023-02-23 13:45:57,054 DEBUG TRAIN Batch 19/200 loss 8.703410 loss_att 9.979822 loss_ctc 13.794731 loss_rnnt 7.687119 hw_loss 0.154059 lr 0.00039698 rank 1
2023-02-23 13:45:57,097 DEBUG TRAIN Batch 19/200 loss 6.412733 loss_att 9.515745 loss_ctc 10.050741 loss_rnnt 5.162865 hw_loss 0.270372 lr 0.00039695 rank 6
2023-02-23 13:47:13,986 DEBUG TRAIN Batch 19/300 loss 16.360538 loss_att 20.102364 loss_ctc 22.775600 loss_rnnt 14.709163 hw_loss 0.089379 lr 0.00039685 rank 1
2023-02-23 13:47:13,988 DEBUG TRAIN Batch 19/300 loss 12.636163 loss_att 15.746249 loss_ctc 16.826839 loss_rnnt 11.348967 hw_loss 0.199542 lr 0.00039682 rank 6
2023-02-23 13:47:13,988 DEBUG TRAIN Batch 19/300 loss 4.429176 loss_att 7.519665 loss_ctc 6.098835 loss_rnnt 3.499552 hw_loss 0.166697 lr 0.00039683 rank 4
2023-02-23 13:47:13,990 DEBUG TRAIN Batch 19/300 loss 6.135132 loss_att 8.188281 loss_ctc 7.479846 loss_rnnt 5.442287 hw_loss 0.192975 lr 0.00039684 rank 0
2023-02-23 13:47:13,990 DEBUG TRAIN Batch 19/300 loss 8.937059 loss_att 13.568036 loss_ctc 14.755651 loss_rnnt 7.108807 hw_loss 0.236709 lr 0.00039679 rank 5
2023-02-23 13:47:13,992 DEBUG TRAIN Batch 19/300 loss 9.673274 loss_att 12.166475 loss_ctc 13.873135 loss_rnnt 8.515112 hw_loss 0.186639 lr 0.00039677 rank 2
2023-02-23 13:47:13,993 DEBUG TRAIN Batch 19/300 loss 12.972305 loss_att 14.387328 loss_ctc 15.350958 loss_rnnt 12.248847 hw_loss 0.231187 lr 0.00039675 rank 3
2023-02-23 13:47:13,999 DEBUG TRAIN Batch 19/300 loss 6.773941 loss_att 9.191972 loss_ctc 11.057606 loss_rnnt 5.628417 hw_loss 0.170181 lr 0.00039673 rank 7
2023-02-23 13:48:31,918 DEBUG TRAIN Batch 19/400 loss 16.259785 loss_att 19.470432 loss_ctc 20.312096 loss_rnnt 14.908747 hw_loss 0.316124 lr 0.00039670 rank 4
2023-02-23 13:48:31,922 DEBUG TRAIN Batch 19/400 loss 18.109186 loss_att 22.318842 loss_ctc 28.935997 loss_rnnt 15.714418 hw_loss 0.204864 lr 0.00039663 rank 3
2023-02-23 13:48:31,923 DEBUG TRAIN Batch 19/400 loss 8.639836 loss_att 10.635509 loss_ctc 13.714306 loss_rnnt 7.469095 hw_loss 0.178147 lr 0.00039666 rank 5
2023-02-23 13:48:31,922 DEBUG TRAIN Batch 19/400 loss 8.661543 loss_att 12.121212 loss_ctc 11.648804 loss_rnnt 7.437491 hw_loss 0.250908 lr 0.00039672 rank 0
2023-02-23 13:48:31,927 DEBUG TRAIN Batch 19/400 loss 5.462241 loss_att 9.978099 loss_ctc 13.069536 loss_rnnt 3.413252 hw_loss 0.246583 lr 0.00039670 rank 6
2023-02-23 13:48:31,929 DEBUG TRAIN Batch 19/400 loss 14.397135 loss_att 19.610815 loss_ctc 20.554472 loss_rnnt 12.412153 hw_loss 0.227374 lr 0.00039661 rank 7
2023-02-23 13:48:31,929 DEBUG TRAIN Batch 19/400 loss 4.295785 loss_att 7.225389 loss_ctc 4.448769 loss_rnnt 3.542010 hw_loss 0.276482 lr 0.00039665 rank 2
2023-02-23 13:48:31,930 DEBUG TRAIN Batch 19/400 loss 8.216337 loss_att 10.774362 loss_ctc 9.234182 loss_rnnt 7.468943 hw_loss 0.187646 lr 0.00039673 rank 1
2023-02-23 13:49:46,630 DEBUG TRAIN Batch 19/500 loss 18.101421 loss_att 22.473917 loss_ctc 23.115273 loss_rnnt 16.472916 hw_loss 0.160298 lr 0.00039658 rank 4
2023-02-23 13:49:46,631 DEBUG TRAIN Batch 19/500 loss 7.264629 loss_att 10.042864 loss_ctc 9.615571 loss_rnnt 6.273681 hw_loss 0.228456 lr 0.00039652 rank 2
2023-02-23 13:49:46,635 DEBUG TRAIN Batch 19/500 loss 5.865875 loss_att 8.004788 loss_ctc 6.609097 loss_rnnt 5.189220 hw_loss 0.280828 lr 0.00039659 rank 0
2023-02-23 13:49:46,636 DEBUG TRAIN Batch 19/500 loss 9.982724 loss_att 13.448796 loss_ctc 16.332129 loss_rnnt 8.334922 hw_loss 0.202500 lr 0.00039654 rank 5
2023-02-23 13:49:46,636 DEBUG TRAIN Batch 19/500 loss 14.714803 loss_att 17.600462 loss_ctc 22.591345 loss_rnnt 12.984509 hw_loss 0.193040 lr 0.00039660 rank 1
2023-02-23 13:49:46,639 DEBUG TRAIN Batch 19/500 loss 11.933889 loss_att 13.082113 loss_ctc 15.699015 loss_rnnt 11.110587 hw_loss 0.171827 lr 0.00039648 rank 7
2023-02-23 13:49:46,639 DEBUG TRAIN Batch 19/500 loss 15.426497 loss_att 17.915075 loss_ctc 17.236855 loss_rnnt 14.528644 hw_loss 0.297669 lr 0.00039657 rank 6
2023-02-23 13:49:46,686 DEBUG TRAIN Batch 19/500 loss 11.392303 loss_att 14.453515 loss_ctc 16.906872 loss_rnnt 9.899065 hw_loss 0.273222 lr 0.00039650 rank 3
2023-02-23 13:51:02,225 DEBUG TRAIN Batch 19/600 loss 14.478091 loss_att 14.797030 loss_ctc 17.907684 loss_rnnt 13.838289 hw_loss 0.222626 lr 0.00039640 rank 2
2023-02-23 13:51:02,226 DEBUG TRAIN Batch 19/600 loss 9.438773 loss_att 9.378181 loss_ctc 9.740313 loss_rnnt 9.237630 hw_loss 0.324481 lr 0.00039638 rank 3
2023-02-23 13:51:02,227 DEBUG TRAIN Batch 19/600 loss 11.014893 loss_att 13.742198 loss_ctc 17.852573 loss_rnnt 9.441048 hw_loss 0.218798 lr 0.00039648 rank 1
2023-02-23 13:51:02,228 DEBUG TRAIN Batch 19/600 loss 3.754099 loss_att 5.346932 loss_ctc 5.442388 loss_rnnt 3.081523 hw_loss 0.241695 lr 0.00039641 rank 5
2023-02-23 13:51:02,230 DEBUG TRAIN Batch 19/600 loss 7.116732 loss_att 8.231864 loss_ctc 9.611942 loss_rnnt 6.403839 hw_loss 0.294699 lr 0.00039647 rank 0
2023-02-23 13:51:02,230 DEBUG TRAIN Batch 19/600 loss 7.960190 loss_att 8.975778 loss_ctc 11.307315 loss_rnnt 7.164875 hw_loss 0.273591 lr 0.00039645 rank 4
2023-02-23 13:51:02,232 DEBUG TRAIN Batch 19/600 loss 14.053155 loss_att 14.614059 loss_ctc 17.336590 loss_rnnt 13.386879 hw_loss 0.218070 lr 0.00039645 rank 6
2023-02-23 13:51:02,231 DEBUG TRAIN Batch 19/600 loss 13.238246 loss_att 16.244652 loss_ctc 17.897987 loss_rnnt 11.855284 hw_loss 0.300717 lr 0.00039636 rank 7
2023-02-23 13:52:20,891 DEBUG TRAIN Batch 19/700 loss 8.723864 loss_att 11.627127 loss_ctc 12.003268 loss_rnnt 7.568863 hw_loss 0.257052 lr 0.00039629 rank 5
2023-02-23 13:52:20,895 DEBUG TRAIN Batch 19/700 loss 6.268518 loss_att 7.170858 loss_ctc 6.626246 loss_rnnt 5.878990 hw_loss 0.302554 lr 0.00039626 rank 3
2023-02-23 13:52:20,898 DEBUG TRAIN Batch 19/700 loss 7.417788 loss_att 9.673561 loss_ctc 9.330119 loss_rnnt 6.601652 hw_loss 0.206257 lr 0.00039635 rank 1
2023-02-23 13:52:20,898 DEBUG TRAIN Batch 19/700 loss 8.081779 loss_att 12.135056 loss_ctc 10.465648 loss_rnnt 6.821600 hw_loss 0.246892 lr 0.00039632 rank 6
2023-02-23 13:52:20,903 DEBUG TRAIN Batch 19/700 loss 12.786107 loss_att 19.497974 loss_ctc 11.442818 loss_rnnt 11.494930 hw_loss 0.239829 lr 0.00039628 rank 2
2023-02-23 13:52:20,911 DEBUG TRAIN Batch 19/700 loss 8.281149 loss_att 11.840220 loss_ctc 12.675947 loss_rnnt 6.914252 hw_loss 0.129579 lr 0.00039623 rank 7
2023-02-23 13:52:20,916 DEBUG TRAIN Batch 19/700 loss 11.624933 loss_att 15.990067 loss_ctc 20.422316 loss_rnnt 9.439593 hw_loss 0.261243 lr 0.00039634 rank 0
2023-02-23 13:52:20,919 DEBUG TRAIN Batch 19/700 loss 8.547787 loss_att 12.757694 loss_ctc 13.011249 loss_rnnt 6.981799 hw_loss 0.241647 lr 0.00039633 rank 4
2023-02-23 13:53:36,170 DEBUG TRAIN Batch 19/800 loss 2.282844 loss_att 4.113358 loss_ctc 2.067145 loss_rnnt 1.845330 hw_loss 0.187822 lr 0.00039617 rank 5
2023-02-23 13:53:36,171 DEBUG TRAIN Batch 19/800 loss 5.991779 loss_att 9.950866 loss_ctc 9.663740 loss_rnnt 4.611056 hw_loss 0.186208 lr 0.00039613 rank 3
2023-02-23 13:53:36,173 DEBUG TRAIN Batch 19/800 loss 8.100200 loss_att 9.855317 loss_ctc 10.234111 loss_rnnt 7.345233 hw_loss 0.223914 lr 0.00039620 rank 6
2023-02-23 13:53:36,173 DEBUG TRAIN Batch 19/800 loss 4.990902 loss_att 6.291975 loss_ctc 5.636356 loss_rnnt 4.479814 hw_loss 0.309025 lr 0.00039620 rank 4
2023-02-23 13:53:36,174 DEBUG TRAIN Batch 19/800 loss 7.850502 loss_att 10.163856 loss_ctc 13.153370 loss_rnnt 6.559276 hw_loss 0.227822 lr 0.00039622 rank 0
2023-02-23 13:53:36,176 DEBUG TRAIN Batch 19/800 loss 5.952125 loss_att 9.059886 loss_ctc 7.180522 loss_rnnt 5.028246 hw_loss 0.259763 lr 0.00039615 rank 2
2023-02-23 13:53:36,176 DEBUG TRAIN Batch 19/800 loss 3.219432 loss_att 6.694447 loss_ctc 8.123175 loss_rnnt 1.748052 hw_loss 0.229772 lr 0.00039611 rank 7
2023-02-23 13:53:36,179 DEBUG TRAIN Batch 19/800 loss 11.808125 loss_att 16.056242 loss_ctc 17.296753 loss_rnnt 10.158703 hw_loss 0.127465 lr 0.00039623 rank 1
2023-02-23 13:54:52,524 DEBUG TRAIN Batch 19/900 loss 10.751940 loss_att 16.048643 loss_ctc 15.834181 loss_rnnt 8.892491 hw_loss 0.229639 lr 0.00039604 rank 5
2023-02-23 13:54:52,524 DEBUG TRAIN Batch 19/900 loss 7.485490 loss_att 10.282042 loss_ctc 10.954756 loss_rnnt 6.351116 hw_loss 0.210927 lr 0.00039601 rank 3
2023-02-23 13:54:52,526 DEBUG TRAIN Batch 19/900 loss 8.343001 loss_att 13.002449 loss_ctc 12.051016 loss_rnnt 6.808866 hw_loss 0.202207 lr 0.00039609 rank 0
2023-02-23 13:54:52,526 DEBUG TRAIN Batch 19/900 loss 14.000193 loss_att 16.748112 loss_ctc 19.824438 loss_rnnt 12.576198 hw_loss 0.183461 lr 0.00039603 rank 2
2023-02-23 13:54:52,526 DEBUG TRAIN Batch 19/900 loss 12.691439 loss_att 13.375675 loss_ctc 16.355640 loss_rnnt 11.945680 hw_loss 0.225658 lr 0.00039607 rank 6
2023-02-23 13:54:52,526 DEBUG TRAIN Batch 19/900 loss 9.951105 loss_att 13.314547 loss_ctc 16.699909 loss_rnnt 8.282420 hw_loss 0.180293 lr 0.00039608 rank 4
2023-02-23 13:54:52,530 DEBUG TRAIN Batch 19/900 loss 18.756330 loss_att 21.375895 loss_ctc 24.274837 loss_rnnt 17.360369 hw_loss 0.255466 lr 0.00039598 rank 7
2023-02-23 13:54:52,530 DEBUG TRAIN Batch 19/900 loss 5.823736 loss_att 7.823805 loss_ctc 7.441207 loss_rnnt 5.103147 hw_loss 0.196711 lr 0.00039610 rank 1
2023-02-23 13:56:09,166 DEBUG TRAIN Batch 19/1000 loss 10.756339 loss_att 15.242473 loss_ctc 19.151180 loss_rnnt 8.653788 hw_loss 0.161275 lr 0.00039597 rank 0
2023-02-23 13:56:09,170 DEBUG TRAIN Batch 19/1000 loss 11.567082 loss_att 14.738049 loss_ctc 14.729177 loss_rnnt 10.433887 hw_loss 0.145105 lr 0.00039590 rank 2
2023-02-23 13:56:09,171 DEBUG TRAIN Batch 19/1000 loss 6.215444 loss_att 8.221338 loss_ctc 8.624434 loss_rnnt 5.406932 hw_loss 0.161502 lr 0.00039592 rank 5
2023-02-23 13:56:09,173 DEBUG TRAIN Batch 19/1000 loss 6.633295 loss_att 9.553411 loss_ctc 10.387124 loss_rnnt 5.438607 hw_loss 0.206539 lr 0.00039595 rank 6
2023-02-23 13:56:09,173 DEBUG TRAIN Batch 19/1000 loss 7.830551 loss_att 10.760118 loss_ctc 12.210600 loss_rnnt 6.515719 hw_loss 0.271710 lr 0.00039598 rank 1
2023-02-23 13:56:09,173 DEBUG TRAIN Batch 19/1000 loss 4.930387 loss_att 9.448141 loss_ctc 7.430959 loss_rnnt 3.594550 hw_loss 0.185392 lr 0.00039596 rank 4
2023-02-23 13:56:09,210 DEBUG TRAIN Batch 19/1000 loss 11.828373 loss_att 12.586314 loss_ctc 15.656939 loss_rnnt 11.062388 hw_loss 0.194852 lr 0.00039588 rank 3
2023-02-23 13:56:09,217 DEBUG TRAIN Batch 19/1000 loss 5.679155 loss_att 8.969917 loss_ctc 6.205221 loss_rnnt 4.856445 hw_loss 0.177031 lr 0.00039586 rank 7
2023-02-23 13:57:26,978 DEBUG TRAIN Batch 19/1100 loss 8.750692 loss_att 11.654211 loss_ctc 10.948516 loss_rnnt 7.739300 hw_loss 0.258085 lr 0.00039583 rank 4
2023-02-23 13:57:26,981 DEBUG TRAIN Batch 19/1100 loss 10.865292 loss_att 16.579994 loss_ctc 15.846206 loss_rnnt 8.938345 hw_loss 0.224780 lr 0.00039579 rank 5
2023-02-23 13:57:26,981 DEBUG TRAIN Batch 19/1100 loss 7.274531 loss_att 10.380730 loss_ctc 8.133896 loss_rnnt 6.388756 hw_loss 0.281164 lr 0.00039584 rank 0
2023-02-23 13:57:26,985 DEBUG TRAIN Batch 19/1100 loss 8.259375 loss_att 10.126825 loss_ctc 9.888786 loss_rnnt 7.515673 hw_loss 0.286793 lr 0.00039586 rank 1
2023-02-23 13:57:26,987 DEBUG TRAIN Batch 19/1100 loss 11.170523 loss_att 15.070951 loss_ctc 13.587273 loss_rnnt 9.944889 hw_loss 0.231214 lr 0.00039578 rank 2
2023-02-23 13:57:26,988 DEBUG TRAIN Batch 19/1100 loss 12.458657 loss_att 13.901069 loss_ctc 12.935118 loss_rnnt 12.019503 hw_loss 0.163394 lr 0.00039582 rank 6
2023-02-23 13:57:26,988 DEBUG TRAIN Batch 19/1100 loss 6.120559 loss_att 8.089710 loss_ctc 8.520229 loss_rnnt 5.224171 hw_loss 0.342378 lr 0.00039576 rank 3
2023-02-23 13:57:26,991 DEBUG TRAIN Batch 19/1100 loss 11.661930 loss_att 13.052162 loss_ctc 19.070614 loss_rnnt 10.276531 hw_loss 0.224113 lr 0.00039574 rank 7
2023-02-23 13:58:43,015 DEBUG TRAIN Batch 19/1200 loss 10.473642 loss_att 12.454515 loss_ctc 13.278560 loss_rnnt 9.564447 hw_loss 0.260684 lr 0.00039571 rank 4
2023-02-23 13:58:43,019 DEBUG TRAIN Batch 19/1200 loss 8.202838 loss_att 9.944944 loss_ctc 11.057763 loss_rnnt 7.350994 hw_loss 0.230186 lr 0.00039567 rank 5
2023-02-23 13:58:43,018 DEBUG TRAIN Batch 19/1200 loss 12.526909 loss_att 14.760782 loss_ctc 16.466606 loss_rnnt 11.359897 hw_loss 0.365518 lr 0.00039570 rank 6
2023-02-23 13:58:43,023 DEBUG TRAIN Batch 19/1200 loss 7.624983 loss_att 12.839952 loss_ctc 10.288922 loss_rnnt 6.079432 hw_loss 0.276310 lr 0.00039573 rank 1
2023-02-23 13:58:43,022 DEBUG TRAIN Batch 19/1200 loss 5.473482 loss_att 7.135544 loss_ctc 6.782875 loss_rnnt 4.865852 hw_loss 0.188684 lr 0.00039572 rank 0
2023-02-23 13:58:43,024 DEBUG TRAIN Batch 19/1200 loss 7.338251 loss_att 10.353287 loss_ctc 13.543152 loss_rnnt 5.832565 hw_loss 0.141296 lr 0.00039565 rank 2
2023-02-23 13:58:43,028 DEBUG TRAIN Batch 19/1200 loss 9.576762 loss_att 9.081556 loss_ctc 10.675579 loss_rnnt 9.378154 hw_loss 0.283390 lr 0.00039563 rank 3
2023-02-23 13:58:43,030 DEBUG TRAIN Batch 19/1200 loss 5.747746 loss_att 7.574084 loss_ctc 6.535151 loss_rnnt 5.164829 hw_loss 0.211242 lr 0.00039561 rank 7
2023-02-23 14:00:00,250 DEBUG TRAIN Batch 19/1300 loss 8.465229 loss_att 9.503614 loss_ctc 12.108811 loss_rnnt 7.633211 hw_loss 0.259744 lr 0.00039561 rank 1
2023-02-23 14:00:00,251 DEBUG TRAIN Batch 19/1300 loss 5.781684 loss_att 10.262094 loss_ctc 10.891264 loss_rnnt 4.101123 hw_loss 0.193504 lr 0.00039558 rank 6
2023-02-23 14:00:00,252 DEBUG TRAIN Batch 19/1300 loss 6.887995 loss_att 7.253107 loss_ctc 9.726954 loss_rnnt 6.290857 hw_loss 0.272978 lr 0.00039558 rank 4
2023-02-23 14:00:00,253 DEBUG TRAIN Batch 19/1300 loss 11.974091 loss_att 15.931627 loss_ctc 16.327969 loss_rnnt 10.488764 hw_loss 0.212440 lr 0.00039555 rank 5
2023-02-23 14:00:00,252 DEBUG TRAIN Batch 19/1300 loss 19.692688 loss_att 20.746881 loss_ctc 25.842892 loss_rnnt 18.558966 hw_loss 0.192855 lr 0.00039553 rank 2
2023-02-23 14:00:00,254 DEBUG TRAIN Batch 19/1300 loss 2.956400 loss_att 7.460203 loss_ctc 3.119634 loss_rnnt 1.953056 hw_loss 0.151537 lr 0.00039551 rank 3
2023-02-23 14:00:00,256 DEBUG TRAIN Batch 19/1300 loss 5.952153 loss_att 6.908651 loss_ctc 8.945299 loss_rnnt 5.181581 hw_loss 0.337848 lr 0.00039560 rank 0
2023-02-23 14:00:00,304 DEBUG TRAIN Batch 19/1300 loss 9.877295 loss_att 14.889299 loss_ctc 12.257439 loss_rnnt 8.445328 hw_loss 0.210403 lr 0.00039549 rank 7
2023-02-23 14:01:18,565 DEBUG TRAIN Batch 19/1400 loss 7.761169 loss_att 9.629012 loss_ctc 10.765544 loss_rnnt 6.877937 hw_loss 0.204523 lr 0.00039546 rank 4
2023-02-23 14:01:18,570 DEBUG TRAIN Batch 19/1400 loss 5.134709 loss_att 8.646065 loss_ctc 7.227115 loss_rnnt 4.063787 hw_loss 0.168117 lr 0.00039539 rank 3
2023-02-23 14:01:18,572 DEBUG TRAIN Batch 19/1400 loss 11.588840 loss_att 17.448288 loss_ctc 18.781305 loss_rnnt 9.418648 hw_loss 0.073698 lr 0.00039542 rank 5
2023-02-23 14:01:18,572 DEBUG TRAIN Batch 19/1400 loss 6.856405 loss_att 9.947893 loss_ctc 10.913361 loss_rnnt 5.603209 hw_loss 0.176195 lr 0.00039547 rank 0
2023-02-23 14:01:18,574 DEBUG TRAIN Batch 19/1400 loss 10.419828 loss_att 12.747517 loss_ctc 15.529783 loss_rnnt 9.171175 hw_loss 0.190854 lr 0.00039541 rank 2
2023-02-23 14:01:18,575 DEBUG TRAIN Batch 19/1400 loss 8.336031 loss_att 11.159546 loss_ctc 11.150881 loss_rnnt 7.351380 hw_loss 0.083691 lr 0.00039545 rank 6
2023-02-23 14:01:18,575 DEBUG TRAIN Batch 19/1400 loss 11.756480 loss_att 15.363525 loss_ctc 15.437156 loss_rnnt 10.444395 hw_loss 0.187347 lr 0.00039536 rank 7
2023-02-23 14:01:18,576 DEBUG TRAIN Batch 19/1400 loss 7.518876 loss_att 11.006688 loss_ctc 10.521927 loss_rnnt 6.286617 hw_loss 0.251792 lr 0.00039548 rank 1
2023-02-23 14:02:33,686 DEBUG TRAIN Batch 19/1500 loss 17.199823 loss_att 22.748232 loss_ctc 26.414471 loss_rnnt 14.763400 hw_loss 0.183976 lr 0.00039526 rank 3
2023-02-23 14:02:33,686 DEBUG TRAIN Batch 19/1500 loss 8.370963 loss_att 12.275171 loss_ctc 12.551753 loss_rnnt 6.896787 hw_loss 0.254803 lr 0.00039535 rank 0
2023-02-23 14:02:33,687 DEBUG TRAIN Batch 19/1500 loss 15.145461 loss_att 17.590990 loss_ctc 19.141521 loss_rnnt 14.019814 hw_loss 0.194500 lr 0.00039530 rank 5
2023-02-23 14:02:33,687 DEBUG TRAIN Batch 19/1500 loss 10.739140 loss_att 17.413425 loss_ctc 14.261192 loss_rnnt 8.845963 hw_loss 0.166337 lr 0.00039528 rank 2
2023-02-23 14:02:33,689 DEBUG TRAIN Batch 19/1500 loss 20.127995 loss_att 22.676760 loss_ctc 32.253082 loss_rnnt 17.897072 hw_loss 0.195923 lr 0.00039524 rank 7
2023-02-23 14:02:33,690 DEBUG TRAIN Batch 19/1500 loss 12.518570 loss_att 15.629408 loss_ctc 16.669851 loss_rnnt 11.290321 hw_loss 0.098580 lr 0.00039534 rank 4
2023-02-23 14:02:33,693 DEBUG TRAIN Batch 19/1500 loss 8.136591 loss_att 12.188047 loss_ctc 12.364798 loss_rnnt 6.586316 hw_loss 0.330418 lr 0.00039536 rank 1
2023-02-23 14:02:33,696 DEBUG TRAIN Batch 19/1500 loss 6.139460 loss_att 8.854105 loss_ctc 9.226221 loss_rnnt 5.046293 hw_loss 0.260005 lr 0.00039533 rank 6
2023-02-23 14:03:49,446 DEBUG TRAIN Batch 19/1600 loss 3.214912 loss_att 8.498150 loss_ctc 5.082982 loss_rnnt 1.835634 hw_loss 0.137915 lr 0.00039517 rank 5
2023-02-23 14:03:49,450 DEBUG TRAIN Batch 19/1600 loss 8.374589 loss_att 11.724088 loss_ctc 9.803193 loss_rnnt 7.380133 hw_loss 0.251391 lr 0.00039516 rank 2
2023-02-23 14:03:49,451 DEBUG TRAIN Batch 19/1600 loss 7.643250 loss_att 10.480240 loss_ctc 12.183483 loss_rnnt 6.378510 hw_loss 0.172458 lr 0.00039514 rank 3
2023-02-23 14:03:49,454 DEBUG TRAIN Batch 19/1600 loss 12.674306 loss_att 15.424366 loss_ctc 15.573580 loss_rnnt 11.656521 hw_loss 0.152257 lr 0.00039512 rank 7
2023-02-23 14:03:49,454 DEBUG TRAIN Batch 19/1600 loss 9.250190 loss_att 13.886074 loss_ctc 14.551938 loss_rnnt 7.506835 hw_loss 0.204896 lr 0.00039521 rank 4
2023-02-23 14:03:49,455 DEBUG TRAIN Batch 19/1600 loss 9.795339 loss_att 12.260552 loss_ctc 11.501453 loss_rnnt 8.966398 hw_loss 0.203279 lr 0.00039523 rank 0
2023-02-23 14:03:49,457 DEBUG TRAIN Batch 19/1600 loss 11.004983 loss_att 14.063459 loss_ctc 14.143372 loss_rnnt 9.866590 hw_loss 0.202961 lr 0.00039524 rank 1
2023-02-23 14:03:49,459 DEBUG TRAIN Batch 19/1600 loss 10.009497 loss_att 12.791883 loss_ctc 14.340217 loss_rnnt 8.794547 hw_loss 0.151957 lr 0.00039521 rank 6
2023-02-23 14:05:05,683 DEBUG TRAIN Batch 19/1700 loss 3.975506 loss_att 5.479330 loss_ctc 3.637768 loss_rnnt 3.536409 hw_loss 0.343806 lr 0.00039504 rank 2
2023-02-23 14:05:05,686 DEBUG TRAIN Batch 19/1700 loss 11.087158 loss_att 12.477812 loss_ctc 16.113918 loss_rnnt 10.029259 hw_loss 0.205377 lr 0.00039505 rank 5
2023-02-23 14:05:05,686 DEBUG TRAIN Batch 19/1700 loss 4.368071 loss_att 8.016033 loss_ctc 6.708210 loss_rnnt 3.221872 hw_loss 0.196101 lr 0.00039510 rank 0
2023-02-23 14:05:05,687 DEBUG TRAIN Batch 19/1700 loss 5.756023 loss_att 10.101992 loss_ctc 6.614510 loss_rnnt 4.645557 hw_loss 0.237764 lr 0.00039499 rank 7
2023-02-23 14:05:05,687 DEBUG TRAIN Batch 19/1700 loss 2.611192 loss_att 4.283903 loss_ctc 3.224345 loss_rnnt 2.085413 hw_loss 0.205280 lr 0.00039508 rank 6
2023-02-23 14:05:05,690 DEBUG TRAIN Batch 19/1700 loss 16.019655 loss_att 17.636993 loss_ctc 24.491280 loss_rnnt 14.498636 hw_loss 0.127503 lr 0.00039509 rank 4
2023-02-23 14:05:05,694 DEBUG TRAIN Batch 19/1700 loss 7.913027 loss_att 13.665051 loss_ctc 9.201844 loss_rnnt 6.493804 hw_loss 0.181832 lr 0.00039511 rank 1
2023-02-23 14:05:05,737 DEBUG TRAIN Batch 19/1700 loss 31.134823 loss_att 29.432388 loss_ctc 39.750610 loss_rnnt 30.131769 hw_loss 0.365191 lr 0.00039502 rank 3
2023-02-23 14:06:25,207 DEBUG TRAIN Batch 19/1800 loss 9.779753 loss_att 11.511539 loss_ctc 13.087862 loss_rnnt 8.865472 hw_loss 0.237828 lr 0.00039491 rank 2
2023-02-23 14:06:25,211 DEBUG TRAIN Batch 19/1800 loss 15.531888 loss_att 17.897066 loss_ctc 21.077826 loss_rnnt 14.166550 hw_loss 0.286585 lr 0.00039497 rank 4
2023-02-23 14:06:25,212 DEBUG TRAIN Batch 19/1800 loss 6.314059 loss_att 9.308710 loss_ctc 8.043163 loss_rnnt 5.361460 hw_loss 0.230852 lr 0.00039498 rank 0
2023-02-23 14:06:25,215 DEBUG TRAIN Batch 19/1800 loss 12.707576 loss_att 16.213869 loss_ctc 17.761372 loss_rnnt 11.258068 hw_loss 0.139517 lr 0.00039499 rank 1
2023-02-23 14:06:25,217 DEBUG TRAIN Batch 19/1800 loss 5.885150 loss_att 7.185301 loss_ctc 6.138246 loss_rnnt 5.470066 hw_loss 0.227454 lr 0.00039493 rank 5
2023-02-23 14:06:25,217 DEBUG TRAIN Batch 19/1800 loss 6.870163 loss_att 7.824695 loss_ctc 9.867371 loss_rnnt 6.132576 hw_loss 0.275722 lr 0.00039487 rank 7
2023-02-23 14:06:25,219 DEBUG TRAIN Batch 19/1800 loss 9.716791 loss_att 10.115457 loss_ctc 13.087982 loss_rnnt 9.022280 hw_loss 0.309910 lr 0.00039489 rank 3
2023-02-23 14:06:25,267 DEBUG TRAIN Batch 19/1800 loss 7.680280 loss_att 9.501243 loss_ctc 9.437850 loss_rnnt 6.961195 hw_loss 0.226028 lr 0.00039496 rank 6
2023-02-23 14:07:41,463 DEBUG TRAIN Batch 19/1900 loss 7.298884 loss_att 7.242073 loss_ctc 9.771954 loss_rnnt 6.811282 hw_loss 0.317291 lr 0.00039479 rank 2
2023-02-23 14:07:41,463 DEBUG TRAIN Batch 19/1900 loss 4.546261 loss_att 7.700581 loss_ctc 9.286129 loss_rnnt 3.152845 hw_loss 0.244817 lr 0.00039477 rank 3
2023-02-23 14:07:41,465 DEBUG TRAIN Batch 19/1900 loss 7.684498 loss_att 9.103735 loss_ctc 12.109175 loss_rnnt 6.746401 hw_loss 0.120550 lr 0.00039484 rank 6
2023-02-23 14:07:41,467 DEBUG TRAIN Batch 19/1900 loss 14.344271 loss_att 15.174088 loss_ctc 21.650209 loss_rnnt 13.036519 hw_loss 0.314367 lr 0.00039486 rank 0
2023-02-23 14:07:41,468 DEBUG TRAIN Batch 19/1900 loss 8.669432 loss_att 9.705135 loss_ctc 10.699917 loss_rnnt 8.062508 hw_loss 0.241973 lr 0.00039484 rank 4
2023-02-23 14:07:41,469 DEBUG TRAIN Batch 19/1900 loss 9.357096 loss_att 12.613791 loss_ctc 10.573276 loss_rnnt 8.418463 hw_loss 0.234630 lr 0.00039481 rank 5
2023-02-23 14:07:41,488 DEBUG TRAIN Batch 19/1900 loss 11.090342 loss_att 10.517775 loss_ctc 14.448438 loss_rnnt 10.561336 hw_loss 0.367078 lr 0.00039475 rank 7
2023-02-23 14:07:41,498 DEBUG TRAIN Batch 19/1900 loss 9.675140 loss_att 10.401637 loss_ctc 13.101908 loss_rnnt 8.923613 hw_loss 0.279986 lr 0.00039487 rank 1
2023-02-23 14:08:57,769 DEBUG TRAIN Batch 19/2000 loss 11.930537 loss_att 17.537914 loss_ctc 16.373503 loss_rnnt 10.140986 hw_loss 0.141900 lr 0.00039468 rank 5
2023-02-23 14:08:57,770 DEBUG TRAIN Batch 19/2000 loss 14.037338 loss_att 18.592083 loss_ctc 20.259836 loss_rnnt 12.188858 hw_loss 0.202245 lr 0.00039467 rank 2
2023-02-23 14:08:57,770 DEBUG TRAIN Batch 19/2000 loss 6.296643 loss_att 9.685677 loss_ctc 8.911465 loss_rnnt 5.137437 hw_loss 0.248917 lr 0.00039473 rank 0
2023-02-23 14:08:57,774 DEBUG TRAIN Batch 19/2000 loss 14.357751 loss_att 14.467358 loss_ctc 15.848083 loss_rnnt 14.015887 hw_loss 0.227308 lr 0.00039471 rank 6
2023-02-23 14:08:57,773 DEBUG TRAIN Batch 19/2000 loss 5.342346 loss_att 10.128008 loss_ctc 6.829694 loss_rnnt 4.133042 hw_loss 0.100985 lr 0.00039465 rank 3
2023-02-23 14:08:57,775 DEBUG TRAIN Batch 19/2000 loss 10.826068 loss_att 16.322273 loss_ctc 18.083584 loss_rnnt 8.667089 hw_loss 0.172628 lr 0.00039462 rank 7
2023-02-23 14:08:57,778 DEBUG TRAIN Batch 19/2000 loss 12.106256 loss_att 16.695797 loss_ctc 19.464161 loss_rnnt 10.118614 hw_loss 0.166273 lr 0.00039474 rank 1
2023-02-23 14:08:57,778 DEBUG TRAIN Batch 19/2000 loss 7.945561 loss_att 11.027800 loss_ctc 11.081533 loss_rnnt 6.790220 hw_loss 0.226434 lr 0.00039472 rank 4
2023-02-23 14:10:16,744 DEBUG TRAIN Batch 19/2100 loss 11.235913 loss_att 15.819651 loss_ctc 19.395151 loss_rnnt 9.156940 hw_loss 0.139365 lr 0.00039452 rank 3
2023-02-23 14:10:16,746 DEBUG TRAIN Batch 19/2100 loss 13.194366 loss_att 17.180576 loss_ctc 16.679874 loss_rnnt 11.881313 hw_loss 0.095766 lr 0.00039456 rank 5
2023-02-23 14:10:16,746 DEBUG TRAIN Batch 19/2100 loss 5.543476 loss_att 8.705405 loss_ctc 8.495914 loss_rnnt 4.321689 hw_loss 0.367017 lr 0.00039461 rank 0
2023-02-23 14:10:16,748 DEBUG TRAIN Batch 19/2100 loss 12.762501 loss_att 16.053822 loss_ctc 16.055304 loss_rnnt 11.506958 hw_loss 0.296696 lr 0.00039460 rank 4
2023-02-23 14:10:16,750 DEBUG TRAIN Batch 19/2100 loss 3.419100 loss_att 9.206030 loss_ctc 5.694373 loss_rnnt 1.852129 hw_loss 0.199153 lr 0.00039454 rank 2
2023-02-23 14:10:16,751 DEBUG TRAIN Batch 19/2100 loss 5.236691 loss_att 9.182334 loss_ctc 8.250502 loss_rnnt 3.981022 hw_loss 0.121310 lr 0.00039450 rank 7
2023-02-23 14:10:16,753 DEBUG TRAIN Batch 19/2100 loss 8.108763 loss_att 11.777008 loss_ctc 11.574764 loss_rnnt 6.719422 hw_loss 0.362922 lr 0.00039462 rank 1
2023-02-23 14:10:16,753 DEBUG TRAIN Batch 19/2100 loss 2.040119 loss_att 4.444446 loss_ctc 2.898542 loss_rnnt 1.365036 hw_loss 0.149552 lr 0.00039459 rank 6
2023-02-23 14:11:33,733 DEBUG TRAIN Batch 19/2200 loss 11.077078 loss_att 12.343393 loss_ctc 14.042065 loss_rnnt 10.332753 hw_loss 0.179494 lr 0.00039442 rank 2
2023-02-23 14:11:33,734 DEBUG TRAIN Batch 19/2200 loss 21.614277 loss_att 22.835432 loss_ctc 31.157135 loss_rnnt 19.941765 hw_loss 0.292310 lr 0.00039444 rank 5
2023-02-23 14:11:33,735 DEBUG TRAIN Batch 19/2200 loss 5.585931 loss_att 9.416573 loss_ctc 9.968492 loss_rnnt 4.129265 hw_loss 0.199118 lr 0.00039447 rank 4
2023-02-23 14:11:33,737 DEBUG TRAIN Batch 19/2200 loss 7.575569 loss_att 12.626566 loss_ctc 12.495529 loss_rnnt 5.763925 hw_loss 0.272718 lr 0.00039449 rank 0
2023-02-23 14:11:33,738 DEBUG TRAIN Batch 19/2200 loss 6.993240 loss_att 9.814140 loss_ctc 11.457458 loss_rnnt 5.714495 hw_loss 0.223755 lr 0.00039440 rank 3
2023-02-23 14:11:33,742 DEBUG TRAIN Batch 19/2200 loss 12.520607 loss_att 15.023314 loss_ctc 17.497091 loss_rnnt 11.245970 hw_loss 0.207309 lr 0.00039438 rank 7
2023-02-23 14:11:33,747 DEBUG TRAIN Batch 19/2200 loss 10.843832 loss_att 12.857241 loss_ctc 15.968461 loss_rnnt 9.604094 hw_loss 0.288324 lr 0.00039450 rank 1
2023-02-23 14:11:33,783 DEBUG TRAIN Batch 19/2200 loss 6.959301 loss_att 9.145071 loss_ctc 10.502790 loss_rnnt 5.919646 hw_loss 0.243818 lr 0.00039447 rank 6
2023-02-23 14:12:49,716 DEBUG TRAIN Batch 19/2300 loss 7.956904 loss_att 12.386621 loss_ctc 10.942284 loss_rnnt 6.567247 hw_loss 0.198117 lr 0.00039437 rank 1
2023-02-23 14:12:49,716 DEBUG TRAIN Batch 19/2300 loss 9.436518 loss_att 11.845895 loss_ctc 14.755935 loss_rnnt 8.074357 hw_loss 0.320680 lr 0.00039431 rank 5
2023-02-23 14:12:49,716 DEBUG TRAIN Batch 19/2300 loss 11.837502 loss_att 14.507471 loss_ctc 14.037870 loss_rnnt 10.904566 hw_loss 0.197923 lr 0.00039436 rank 0
2023-02-23 14:12:49,718 DEBUG TRAIN Batch 19/2300 loss 11.688911 loss_att 15.268700 loss_ctc 14.789719 loss_rnnt 10.463413 hw_loss 0.180187 lr 0.00039435 rank 4
2023-02-23 14:12:49,718 DEBUG TRAIN Batch 19/2300 loss 8.361858 loss_att 13.554779 loss_ctc 15.186052 loss_rnnt 6.282925 hw_loss 0.244608 lr 0.00039426 rank 7
2023-02-23 14:12:49,720 DEBUG TRAIN Batch 19/2300 loss 10.208800 loss_att 12.428782 loss_ctc 16.180130 loss_rnnt 8.809145 hw_loss 0.299030 lr 0.00039434 rank 6
2023-02-23 14:12:49,720 DEBUG TRAIN Batch 19/2300 loss 15.502199 loss_att 17.326458 loss_ctc 14.654605 loss_rnnt 15.114712 hw_loss 0.254340 lr 0.00039430 rank 2
2023-02-23 14:12:49,766 DEBUG TRAIN Batch 19/2300 loss 5.493186 loss_att 7.276660 loss_ctc 7.596440 loss_rnnt 4.735709 hw_loss 0.225652 lr 0.00039428 rank 3
2023-02-23 14:14:06,980 DEBUG TRAIN Batch 19/2400 loss 8.867290 loss_att 10.075464 loss_ctc 9.637836 loss_rnnt 8.427105 hw_loss 0.179646 lr 0.00039423 rank 4
2023-02-23 14:14:06,982 DEBUG TRAIN Batch 19/2400 loss 10.763165 loss_att 14.574354 loss_ctc 15.509087 loss_rnnt 9.226348 hw_loss 0.265857 lr 0.00039419 rank 5
2023-02-23 14:14:06,985 DEBUG TRAIN Batch 19/2400 loss 20.839056 loss_att 24.251831 loss_ctc 31.184834 loss_rnnt 18.652689 hw_loss 0.233203 lr 0.00039425 rank 1
2023-02-23 14:14:06,984 DEBUG TRAIN Batch 19/2400 loss 5.844941 loss_att 8.111031 loss_ctc 8.399064 loss_rnnt 4.918723 hw_loss 0.248343 lr 0.00039422 rank 6
2023-02-23 14:14:06,985 DEBUG TRAIN Batch 19/2400 loss 8.057907 loss_att 10.401465 loss_ctc 10.379124 loss_rnnt 7.166862 hw_loss 0.211571 lr 0.00039424 rank 0
2023-02-23 14:14:06,986 DEBUG TRAIN Batch 19/2400 loss 11.290772 loss_att 11.709036 loss_ctc 17.568508 loss_rnnt 10.205354 hw_loss 0.308876 lr 0.00039413 rank 7
2023-02-23 14:14:06,989 DEBUG TRAIN Batch 19/2400 loss 12.084582 loss_att 15.366690 loss_ctc 18.282942 loss_rnnt 10.501095 hw_loss 0.188659 lr 0.00039418 rank 2
2023-02-23 14:14:06,992 DEBUG TRAIN Batch 19/2400 loss 10.578569 loss_att 14.457664 loss_ctc 15.460901 loss_rnnt 8.997318 hw_loss 0.289603 lr 0.00039416 rank 3
2023-02-23 14:15:26,528 DEBUG TRAIN Batch 19/2500 loss 10.725977 loss_att 13.655679 loss_ctc 15.062347 loss_rnnt 9.400409 hw_loss 0.302707 lr 0.00039411 rank 4
2023-02-23 14:15:26,530 DEBUG TRAIN Batch 19/2500 loss 10.302167 loss_att 11.465091 loss_ctc 13.374912 loss_rnnt 9.516745 hw_loss 0.268382 lr 0.00039407 rank 5
2023-02-23 14:15:26,533 DEBUG TRAIN Batch 19/2500 loss 12.155249 loss_att 16.532230 loss_ctc 17.827168 loss_rnnt 10.329926 hw_loss 0.363131 lr 0.00039413 rank 1
2023-02-23 14:15:26,535 DEBUG TRAIN Batch 19/2500 loss 3.491502 loss_att 5.871762 loss_ctc 4.318095 loss_rnnt 2.779325 hw_loss 0.236086 lr 0.00039412 rank 0
2023-02-23 14:15:26,537 DEBUG TRAIN Batch 19/2500 loss 11.387039 loss_att 12.680862 loss_ctc 16.940495 loss_rnnt 10.262918 hw_loss 0.234180 lr 0.00039405 rank 2
2023-02-23 14:15:26,540 DEBUG TRAIN Batch 19/2500 loss 7.134309 loss_att 8.007026 loss_ctc 9.070572 loss_rnnt 6.517081 hw_loss 0.345968 lr 0.00039410 rank 6
2023-02-23 14:15:26,541 DEBUG TRAIN Batch 19/2500 loss 6.997632 loss_att 8.907022 loss_ctc 10.373150 loss_rnnt 5.990072 hw_loss 0.329272 lr 0.00039401 rank 7
2023-02-23 14:15:26,592 DEBUG TRAIN Batch 19/2500 loss 8.995467 loss_att 10.987489 loss_ctc 13.822154 loss_rnnt 7.822930 hw_loss 0.244825 lr 0.00039403 rank 3
2023-02-23 14:16:44,199 DEBUG TRAIN Batch 19/2600 loss 11.591185 loss_att 11.085055 loss_ctc 13.972351 loss_rnnt 11.204616 hw_loss 0.319324 lr 0.00039400 rank 0
2023-02-23 14:16:44,201 DEBUG TRAIN Batch 19/2600 loss 9.840849 loss_att 11.714762 loss_ctc 17.746559 loss_rnnt 8.319611 hw_loss 0.173174 lr 0.00039395 rank 5
2023-02-23 14:16:44,204 DEBUG TRAIN Batch 19/2600 loss 8.929726 loss_att 9.157351 loss_ctc 11.767370 loss_rnnt 8.318728 hw_loss 0.350849 lr 0.00039398 rank 4
2023-02-23 14:16:44,208 DEBUG TRAIN Batch 19/2600 loss 4.749355 loss_att 8.327337 loss_ctc 6.075674 loss_rnnt 3.771849 hw_loss 0.159502 lr 0.00039398 rank 6
2023-02-23 14:16:44,208 DEBUG TRAIN Batch 19/2600 loss 9.222276 loss_att 9.354583 loss_ctc 13.292126 loss_rnnt 8.486734 hw_loss 0.312061 lr 0.00039401 rank 1
2023-02-23 14:16:44,214 DEBUG TRAIN Batch 19/2600 loss 11.500623 loss_att 16.372419 loss_ctc 15.570049 loss_rnnt 9.846272 hw_loss 0.257625 lr 0.00039393 rank 2
2023-02-23 14:16:44,215 DEBUG TRAIN Batch 19/2600 loss 5.644008 loss_att 8.347788 loss_ctc 8.345243 loss_rnnt 4.672689 hw_loss 0.131999 lr 0.00039389 rank 7
2023-02-23 14:16:44,252 DEBUG TRAIN Batch 19/2600 loss 8.259481 loss_att 11.039574 loss_ctc 10.574362 loss_rnnt 7.310243 hw_loss 0.158568 lr 0.00039391 rank 3
2023-02-23 14:17:59,524 DEBUG TRAIN Batch 19/2700 loss 11.893246 loss_att 16.100058 loss_ctc 22.370350 loss_rnnt 9.539774 hw_loss 0.215928 lr 0.00039382 rank 5
2023-02-23 14:17:59,531 DEBUG TRAIN Batch 19/2700 loss 11.658017 loss_att 17.359133 loss_ctc 20.815619 loss_rnnt 9.186940 hw_loss 0.205948 lr 0.00039386 rank 4
2023-02-23 14:17:59,533 DEBUG TRAIN Batch 19/2700 loss 6.544639 loss_att 9.811722 loss_ctc 8.504509 loss_rnnt 5.512173 hw_loss 0.220749 lr 0.00039379 rank 3
2023-02-23 14:17:59,534 DEBUG TRAIN Batch 19/2700 loss 12.244806 loss_att 15.181297 loss_ctc 18.063604 loss_rnnt 10.737468 hw_loss 0.270377 lr 0.00039385 rank 6
2023-02-23 14:17:59,535 DEBUG TRAIN Batch 19/2700 loss 8.024354 loss_att 13.893336 loss_ctc 12.107131 loss_rnnt 6.201092 hw_loss 0.197053 lr 0.00039381 rank 2
2023-02-23 14:17:59,535 DEBUG TRAIN Batch 19/2700 loss 6.390578 loss_att 8.071724 loss_ctc 8.845199 loss_rnnt 5.580797 hw_loss 0.274255 lr 0.00039387 rank 0
2023-02-23 14:17:59,536 DEBUG TRAIN Batch 19/2700 loss 6.873994 loss_att 8.918418 loss_ctc 8.551319 loss_rnnt 6.147758 hw_loss 0.175700 lr 0.00039389 rank 1
2023-02-23 14:17:59,537 DEBUG TRAIN Batch 19/2700 loss 11.571740 loss_att 12.443585 loss_ctc 14.940573 loss_rnnt 10.848616 hw_loss 0.186709 lr 0.00039377 rank 7
2023-02-23 14:19:18,140 DEBUG TRAIN Batch 19/2800 loss 9.573736 loss_att 13.637860 loss_ctc 11.863017 loss_rnnt 8.328131 hw_loss 0.239143 lr 0.00039370 rank 5
2023-02-23 14:19:18,144 DEBUG TRAIN Batch 19/2800 loss 5.931548 loss_att 11.310111 loss_ctc 6.684012 loss_rnnt 4.638975 hw_loss 0.218497 lr 0.00039374 rank 4
2023-02-23 14:19:18,146 DEBUG TRAIN Batch 19/2800 loss 15.815435 loss_att 16.959557 loss_ctc 29.356131 loss_rnnt 13.655268 hw_loss 0.236091 lr 0.00039369 rank 2
2023-02-23 14:19:18,146 DEBUG TRAIN Batch 19/2800 loss 7.948181 loss_att 11.066542 loss_ctc 10.079413 loss_rnnt 6.894247 hw_loss 0.273935 lr 0.00039367 rank 3
2023-02-23 14:19:18,149 DEBUG TRAIN Batch 19/2800 loss 13.378248 loss_att 12.531789 loss_ctc 12.708204 loss_rnnt 13.530230 hw_loss 0.199966 lr 0.00039376 rank 1
2023-02-23 14:19:18,149 DEBUG TRAIN Batch 19/2800 loss 5.568127 loss_att 10.271089 loss_ctc 8.402782 loss_rnnt 4.145260 hw_loss 0.195600 lr 0.00039375 rank 0
2023-02-23 14:19:18,150 DEBUG TRAIN Batch 19/2800 loss 10.141612 loss_att 9.579083 loss_ctc 12.505413 loss_rnnt 9.851386 hw_loss 0.164169 lr 0.00039364 rank 7
2023-02-23 14:19:18,156 DEBUG TRAIN Batch 19/2800 loss 10.094648 loss_att 14.500478 loss_ctc 16.993359 loss_rnnt 8.178293 hw_loss 0.216304 lr 0.00039373 rank 6
2023-02-23 14:20:36,583 DEBUG TRAIN Batch 19/2900 loss 6.246728 loss_att 7.580198 loss_ctc 9.348046 loss_rnnt 5.412735 hw_loss 0.288357 lr 0.00039358 rank 5
2023-02-23 14:20:36,586 DEBUG TRAIN Batch 19/2900 loss 10.156434 loss_att 12.582677 loss_ctc 15.737849 loss_rnnt 8.844146 hw_loss 0.155347 lr 0.00039362 rank 4
2023-02-23 14:20:36,587 DEBUG TRAIN Batch 19/2900 loss 9.534395 loss_att 11.829370 loss_ctc 13.936149 loss_rnnt 8.323460 hw_loss 0.309450 lr 0.00039355 rank 3
2023-02-23 14:20:36,588 DEBUG TRAIN Batch 19/2900 loss 3.317222 loss_att 6.663072 loss_ctc 6.092064 loss_rnnt 2.161261 hw_loss 0.219022 lr 0.00039364 rank 1
2023-02-23 14:20:36,592 DEBUG TRAIN Batch 19/2900 loss 7.324364 loss_att 9.182264 loss_ctc 8.903052 loss_rnnt 6.660148 hw_loss 0.154020 lr 0.00039352 rank 7
2023-02-23 14:20:36,592 DEBUG TRAIN Batch 19/2900 loss 8.607081 loss_att 9.349021 loss_ctc 13.868523 loss_rnnt 7.648790 hw_loss 0.203208 lr 0.00039357 rank 2
2023-02-23 14:20:36,593 DEBUG TRAIN Batch 19/2900 loss 10.327632 loss_att 11.940250 loss_ctc 10.992945 loss_rnnt 9.792233 hw_loss 0.232814 lr 0.00039361 rank 6
2023-02-23 14:20:36,593 DEBUG TRAIN Batch 19/2900 loss 7.206457 loss_att 9.160292 loss_ctc 9.336901 loss_rnnt 6.432999 hw_loss 0.184933 lr 0.00039363 rank 0
2023-02-23 14:21:53,734 DEBUG TRAIN Batch 19/3000 loss 5.019140 loss_att 7.914751 loss_ctc 6.455763 loss_rnnt 4.130928 hw_loss 0.220389 lr 0.00039346 rank 5
2023-02-23 14:21:53,734 DEBUG TRAIN Batch 19/3000 loss 11.784460 loss_att 13.749638 loss_ctc 13.385424 loss_rnnt 11.074000 hw_loss 0.194930 lr 0.00039351 rank 0
2023-02-23 14:21:53,735 DEBUG TRAIN Batch 19/3000 loss 5.039637 loss_att 7.876446 loss_ctc 7.474292 loss_rnnt 4.043770 hw_loss 0.194783 lr 0.00039340 rank 7
2023-02-23 14:21:53,737 DEBUG TRAIN Batch 19/3000 loss 9.655710 loss_att 12.979855 loss_ctc 14.204889 loss_rnnt 8.206770 hw_loss 0.332913 lr 0.00039352 rank 1
2023-02-23 14:21:53,738 DEBUG TRAIN Batch 19/3000 loss 6.481542 loss_att 9.057209 loss_ctc 9.919642 loss_rnnt 5.304417 hw_loss 0.381708 lr 0.00039342 rank 3
2023-02-23 14:21:53,740 DEBUG TRAIN Batch 19/3000 loss 11.163029 loss_att 16.295795 loss_ctc 15.180308 loss_rnnt 9.473059 hw_loss 0.239587 lr 0.00039350 rank 4
2023-02-23 14:21:53,742 DEBUG TRAIN Batch 19/3000 loss 6.242449 loss_att 7.931649 loss_ctc 7.835379 loss_rnnt 5.596868 hw_loss 0.178783 lr 0.00039344 rank 2
2023-02-23 14:21:53,743 DEBUG TRAIN Batch 19/3000 loss 5.720033 loss_att 8.418721 loss_ctc 9.846502 loss_rnnt 4.547695 hw_loss 0.154507 lr 0.00039349 rank 6
2023-02-23 14:23:10,000 DEBUG TRAIN Batch 19/3100 loss 7.326672 loss_att 7.827527 loss_ctc 10.302482 loss_rnnt 6.630874 hw_loss 0.372850 lr 0.00039334 rank 5
2023-02-23 14:23:10,000 DEBUG TRAIN Batch 19/3100 loss 7.127048 loss_att 11.149828 loss_ctc 10.372826 loss_rnnt 5.807752 hw_loss 0.153694 lr 0.00039337 rank 4
2023-02-23 14:23:10,007 DEBUG TRAIN Batch 19/3100 loss 9.837717 loss_att 11.397469 loss_ctc 13.643659 loss_rnnt 8.846923 hw_loss 0.321347 lr 0.00039337 rank 6
2023-02-23 14:23:10,008 DEBUG TRAIN Batch 19/3100 loss 9.206003 loss_att 10.637003 loss_ctc 11.771778 loss_rnnt 8.466578 hw_loss 0.208353 lr 0.00039340 rank 1
2023-02-23 14:23:10,009 DEBUG TRAIN Batch 19/3100 loss 12.676115 loss_att 15.232240 loss_ctc 17.993666 loss_rnnt 11.292283 hw_loss 0.306750 lr 0.00039332 rank 2
2023-02-23 14:23:10,009 DEBUG TRAIN Batch 19/3100 loss 9.490341 loss_att 13.691228 loss_ctc 12.192336 loss_rnnt 8.157516 hw_loss 0.248218 lr 0.00039339 rank 0
2023-02-23 14:23:10,010 DEBUG TRAIN Batch 19/3100 loss 9.885669 loss_att 12.083870 loss_ctc 12.946822 loss_rnnt 8.933490 hw_loss 0.195721 lr 0.00039328 rank 7
2023-02-23 14:23:10,016 DEBUG TRAIN Batch 19/3100 loss 14.864552 loss_att 15.398043 loss_ctc 17.975819 loss_rnnt 14.293100 hw_loss 0.093596 lr 0.00039330 rank 3
2023-02-23 14:24:30,136 DEBUG TRAIN Batch 19/3200 loss 11.131064 loss_att 13.243141 loss_ctc 15.188086 loss_rnnt 10.096147 hw_loss 0.134187 lr 0.00039318 rank 3
2023-02-23 14:24:30,137 DEBUG TRAIN Batch 19/3200 loss 10.331711 loss_att 10.435233 loss_ctc 13.857717 loss_rnnt 9.701152 hw_loss 0.261976 lr 0.00039326 rank 0
2023-02-23 14:24:30,140 DEBUG TRAIN Batch 19/3200 loss 5.217021 loss_att 8.421060 loss_ctc 6.829660 loss_rnnt 4.223514 hw_loss 0.258153 lr 0.00039321 rank 5
2023-02-23 14:24:30,145 DEBUG TRAIN Batch 19/3200 loss 11.594137 loss_att 15.201092 loss_ctc 15.847418 loss_rnnt 10.217886 hw_loss 0.164543 lr 0.00039325 rank 6
2023-02-23 14:24:30,146 DEBUG TRAIN Batch 19/3200 loss 13.140862 loss_att 16.185581 loss_ctc 19.967522 loss_rnnt 11.487336 hw_loss 0.251925 lr 0.00039328 rank 1
2023-02-23 14:24:30,147 DEBUG TRAIN Batch 19/3200 loss 21.447056 loss_att 24.894394 loss_ctc 25.936794 loss_rnnt 20.086632 hw_loss 0.135607 lr 0.00039316 rank 7
2023-02-23 14:24:30,165 DEBUG TRAIN Batch 19/3200 loss 6.905417 loss_att 8.093773 loss_ctc 8.287776 loss_rnnt 6.339137 hw_loss 0.270553 lr 0.00039325 rank 4
2023-02-23 14:24:30,167 DEBUG TRAIN Batch 19/3200 loss 2.922919 loss_att 3.714670 loss_ctc 3.961583 loss_rnnt 2.431438 hw_loss 0.364955 lr 0.00039320 rank 2
2023-02-23 14:25:46,684 DEBUG TRAIN Batch 19/3300 loss 9.148228 loss_att 9.983188 loss_ctc 9.383034 loss_rnnt 8.872768 hw_loss 0.144676 lr 0.00039309 rank 5
2023-02-23 14:25:46,685 DEBUG TRAIN Batch 19/3300 loss 7.430202 loss_att 8.433241 loss_ctc 9.963192 loss_rnnt 6.766583 hw_loss 0.234898 lr 0.00039314 rank 0
2023-02-23 14:25:46,687 DEBUG TRAIN Batch 19/3300 loss 2.811273 loss_att 5.607836 loss_ctc 3.142681 loss_rnnt 2.081073 hw_loss 0.237562 lr 0.00039306 rank 3
2023-02-23 14:25:46,686 DEBUG TRAIN Batch 19/3300 loss 2.205480 loss_att 6.319489 loss_ctc 4.489770 loss_rnnt 0.995449 hw_loss 0.154981 lr 0.00039308 rank 2
2023-02-23 14:25:46,688 DEBUG TRAIN Batch 19/3300 loss 15.641251 loss_att 20.426542 loss_ctc 25.227562 loss_rnnt 13.265151 hw_loss 0.264126 lr 0.00039312 rank 6
2023-02-23 14:25:46,689 DEBUG TRAIN Batch 19/3300 loss 12.771895 loss_att 11.481860 loss_ctc 15.556490 loss_rnnt 12.514825 hw_loss 0.269621 lr 0.00039315 rank 1
2023-02-23 14:25:46,689 DEBUG TRAIN Batch 19/3300 loss 8.877347 loss_att 10.323439 loss_ctc 11.174617 loss_rnnt 8.179281 hw_loss 0.192271 lr 0.00039313 rank 4
2023-02-23 14:25:46,694 DEBUG TRAIN Batch 19/3300 loss 4.664828 loss_att 7.964929 loss_ctc 6.106838 loss_rnnt 3.695899 hw_loss 0.218701 lr 0.00039304 rank 7
2023-02-23 14:27:02,179 DEBUG TRAIN Batch 19/3400 loss 5.958912 loss_att 8.413937 loss_ctc 8.246116 loss_rnnt 5.032744 hw_loss 0.244129 lr 0.00039302 rank 0
2023-02-23 14:27:02,184 DEBUG TRAIN Batch 19/3400 loss 16.472937 loss_att 17.271006 loss_ctc 21.678688 loss_rnnt 15.517967 hw_loss 0.189851 lr 0.00039297 rank 5
2023-02-23 14:27:02,186 DEBUG TRAIN Batch 19/3400 loss 2.327273 loss_att 5.203428 loss_ctc 4.820653 loss_rnnt 1.349833 hw_loss 0.130798 lr 0.00039303 rank 1
2023-02-23 14:27:02,186 DEBUG TRAIN Batch 19/3400 loss 8.883565 loss_att 10.833599 loss_ctc 13.046274 loss_rnnt 7.804183 hw_loss 0.251900 lr 0.00039294 rank 3
2023-02-23 14:27:02,186 DEBUG TRAIN Batch 19/3400 loss 6.658976 loss_att 7.062761 loss_ctc 5.556121 loss_rnnt 6.627624 hw_loss 0.183078 lr 0.00039300 rank 6
2023-02-23 14:27:02,186 DEBUG TRAIN Batch 19/3400 loss 12.144006 loss_att 15.197718 loss_ctc 20.091732 loss_rnnt 10.369643 hw_loss 0.194857 lr 0.00039301 rank 4
2023-02-23 14:27:02,189 DEBUG TRAIN Batch 19/3400 loss 12.758282 loss_att 15.784515 loss_ctc 17.997478 loss_rnnt 11.343607 hw_loss 0.207876 lr 0.00039291 rank 7
2023-02-23 14:27:02,188 DEBUG TRAIN Batch 19/3400 loss 7.048703 loss_att 10.366547 loss_ctc 8.965176 loss_rnnt 6.041441 hw_loss 0.165307 lr 0.00039296 rank 2
2023-02-23 14:28:18,255 DEBUG TRAIN Batch 19/3500 loss 11.849735 loss_att 13.549401 loss_ctc 16.423191 loss_rnnt 10.801632 hw_loss 0.184456 lr 0.00039285 rank 5
2023-02-23 14:28:18,256 DEBUG TRAIN Batch 19/3500 loss 8.912289 loss_att 9.529366 loss_ctc 10.035671 loss_rnnt 8.467158 hw_loss 0.322369 lr 0.00039279 rank 7
2023-02-23 14:28:18,255 DEBUG TRAIN Batch 19/3500 loss 5.973461 loss_att 8.773153 loss_ctc 10.198929 loss_rnnt 4.720737 hw_loss 0.242605 lr 0.00039288 rank 6
2023-02-23 14:28:18,258 DEBUG TRAIN Batch 19/3500 loss 16.657619 loss_att 19.711727 loss_ctc 24.561073 loss_rnnt 14.875090 hw_loss 0.221091 lr 0.00039284 rank 2
2023-02-23 14:28:18,258 DEBUG TRAIN Batch 19/3500 loss 7.247611 loss_att 11.284829 loss_ctc 10.332165 loss_rnnt 5.897564 hw_loss 0.246243 lr 0.00039290 rank 0
2023-02-23 14:28:18,259 DEBUG TRAIN Batch 19/3500 loss 5.555686 loss_att 9.448994 loss_ctc 8.259946 loss_rnnt 4.277336 hw_loss 0.260852 lr 0.00039291 rank 1
2023-02-23 14:28:18,259 DEBUG TRAIN Batch 19/3500 loss 7.322611 loss_att 9.948428 loss_ctc 8.784609 loss_rnnt 6.479747 hw_loss 0.230191 lr 0.00039289 rank 4
2023-02-23 14:28:18,263 DEBUG TRAIN Batch 19/3500 loss 10.482617 loss_att 14.584649 loss_ctc 14.271502 loss_rnnt 9.004581 hw_loss 0.285835 lr 0.00039282 rank 3
2023-02-23 14:29:35,487 DEBUG TRAIN Batch 19/3600 loss 8.221012 loss_att 10.824430 loss_ctc 13.768807 loss_rnnt 6.841072 hw_loss 0.224155 lr 0.00039273 rank 5
2023-02-23 14:29:35,491 DEBUG TRAIN Batch 19/3600 loss 8.453887 loss_att 12.735477 loss_ctc 14.692462 loss_rnnt 6.641778 hw_loss 0.232465 lr 0.00039278 rank 0
2023-02-23 14:29:35,492 DEBUG TRAIN Batch 19/3600 loss 7.943105 loss_att 12.743999 loss_ctc 11.018963 loss_rnnt 6.447467 hw_loss 0.235023 lr 0.00039271 rank 2
2023-02-23 14:29:35,494 DEBUG TRAIN Batch 19/3600 loss 6.967657 loss_att 10.328466 loss_ctc 9.088339 loss_rnnt 5.892944 hw_loss 0.224613 lr 0.00039277 rank 4
2023-02-23 14:29:35,495 DEBUG TRAIN Batch 19/3600 loss 2.992732 loss_att 6.187959 loss_ctc 4.014066 loss_rnnt 2.071483 hw_loss 0.273798 lr 0.00039276 rank 6
2023-02-23 14:29:35,498 DEBUG TRAIN Batch 19/3600 loss 15.651260 loss_att 20.639570 loss_ctc 27.563387 loss_rnnt 12.895964 hw_loss 0.317533 lr 0.00039267 rank 7
2023-02-23 14:29:35,502 DEBUG TRAIN Batch 19/3600 loss 8.514681 loss_att 11.807327 loss_ctc 12.740004 loss_rnnt 7.170152 hw_loss 0.229919 lr 0.00039279 rank 1
2023-02-23 14:29:35,502 DEBUG TRAIN Batch 19/3600 loss 10.101318 loss_att 10.065783 loss_ctc 10.592140 loss_rnnt 9.922397 hw_loss 0.226100 lr 0.00039270 rank 3
2023-02-23 14:30:50,731 DEBUG TRAIN Batch 19/3700 loss 12.937057 loss_att 13.799141 loss_ctc 16.227974 loss_rnnt 12.201155 hw_loss 0.233808 lr 0.00039259 rank 2
2023-02-23 14:30:50,734 DEBUG TRAIN Batch 19/3700 loss 3.732298 loss_att 5.379878 loss_ctc 5.785892 loss_rnnt 3.024830 hw_loss 0.195261 lr 0.00039266 rank 0
2023-02-23 14:30:50,736 DEBUG TRAIN Batch 19/3700 loss 8.844355 loss_att 12.965105 loss_ctc 13.545881 loss_rnnt 7.237880 hw_loss 0.291478 lr 0.00039264 rank 6
2023-02-23 14:30:50,736 DEBUG TRAIN Batch 19/3700 loss 16.906809 loss_att 20.256506 loss_ctc 22.238886 loss_rnnt 15.441669 hw_loss 0.157981 lr 0.00039267 rank 1
2023-02-23 14:30:50,737 DEBUG TRAIN Batch 19/3700 loss 6.008269 loss_att 8.192659 loss_ctc 8.866093 loss_rnnt 5.021684 hw_loss 0.316244 lr 0.00039261 rank 5
2023-02-23 14:30:50,738 DEBUG TRAIN Batch 19/3700 loss 8.190328 loss_att 13.093952 loss_ctc 8.469403 loss_rnnt 7.023515 hw_loss 0.279146 lr 0.00039265 rank 4
2023-02-23 14:30:50,746 DEBUG TRAIN Batch 19/3700 loss 8.626300 loss_att 8.924393 loss_ctc 10.987880 loss_rnnt 8.068457 hw_loss 0.343776 lr 0.00039257 rank 3
2023-02-23 14:30:50,749 DEBUG TRAIN Batch 19/3700 loss 4.859159 loss_att 7.020628 loss_ctc 5.458236 loss_rnnt 4.242809 hw_loss 0.195336 lr 0.00039255 rank 7
2023-02-23 14:32:05,751 DEBUG TRAIN Batch 19/3800 loss 11.344475 loss_att 12.019073 loss_ctc 15.450285 loss_rnnt 10.487709 hw_loss 0.327008 lr 0.00039252 rank 6
2023-02-23 14:32:05,754 DEBUG TRAIN Batch 19/3800 loss 5.778771 loss_att 7.727642 loss_ctc 11.288242 loss_rnnt 4.522176 hw_loss 0.247921 lr 0.00039254 rank 0
2023-02-23 14:32:05,757 DEBUG TRAIN Batch 19/3800 loss 4.602011 loss_att 8.358887 loss_ctc 7.301669 loss_rnnt 3.368398 hw_loss 0.229280 lr 0.00039249 rank 5
2023-02-23 14:32:05,758 DEBUG TRAIN Batch 19/3800 loss 5.775451 loss_att 8.072952 loss_ctc 9.075196 loss_rnnt 4.743881 hw_loss 0.247693 lr 0.00039255 rank 1
2023-02-23 14:32:05,759 DEBUG TRAIN Batch 19/3800 loss 9.622821 loss_att 12.675623 loss_ctc 13.158640 loss_rnnt 8.412906 hw_loss 0.239834 lr 0.00039252 rank 4
2023-02-23 14:32:05,760 DEBUG TRAIN Batch 19/3800 loss 7.546251 loss_att 9.677488 loss_ctc 11.678634 loss_rnnt 6.436302 hw_loss 0.248846 lr 0.00039247 rank 2
2023-02-23 14:32:05,760 DEBUG TRAIN Batch 19/3800 loss 11.088327 loss_att 11.507995 loss_ctc 13.933741 loss_rnnt 10.502380 hw_loss 0.229922 lr 0.00039243 rank 7
2023-02-23 14:32:05,763 DEBUG TRAIN Batch 19/3800 loss 9.067059 loss_att 11.534680 loss_ctc 14.119045 loss_rnnt 7.802260 hw_loss 0.183144 lr 0.00039245 rank 3
2023-02-23 14:33:24,796 DEBUG TRAIN Batch 19/3900 loss 14.536227 loss_att 17.121788 loss_ctc 19.789480 loss_rnnt 13.238897 hw_loss 0.149595 lr 0.00039240 rank 6
2023-02-23 14:33:24,800 DEBUG TRAIN Batch 19/3900 loss 8.625569 loss_att 11.339247 loss_ctc 9.546924 loss_rnnt 7.844866 hw_loss 0.215852 lr 0.00039237 rank 5
2023-02-23 14:33:24,800 DEBUG TRAIN Batch 19/3900 loss 8.292931 loss_att 10.555356 loss_ctc 11.129149 loss_rnnt 7.358768 hw_loss 0.194089 lr 0.00039233 rank 3
2023-02-23 14:33:24,800 DEBUG TRAIN Batch 19/3900 loss 13.101086 loss_att 14.108599 loss_ctc 20.047810 loss_rnnt 11.829893 hw_loss 0.268988 lr 0.00039243 rank 1
2023-02-23 14:33:24,800 DEBUG TRAIN Batch 19/3900 loss 17.110825 loss_att 21.188963 loss_ctc 24.436209 loss_rnnt 15.190998 hw_loss 0.239029 lr 0.00039242 rank 0
2023-02-23 14:33:24,803 DEBUG TRAIN Batch 19/3900 loss 5.716441 loss_att 6.951014 loss_ctc 9.546351 loss_rnnt 4.774032 hw_loss 0.346575 lr 0.00039240 rank 4
2023-02-23 14:33:24,809 DEBUG TRAIN Batch 19/3900 loss 3.969735 loss_att 6.114197 loss_ctc 7.514282 loss_rnnt 2.955286 hw_loss 0.211783 lr 0.00039235 rank 2
2023-02-23 14:33:24,816 DEBUG TRAIN Batch 19/3900 loss 13.366117 loss_att 20.482834 loss_ctc 18.595005 loss_rnnt 11.130110 hw_loss 0.216525 lr 0.00039231 rank 7
2023-02-23 14:34:41,095 DEBUG TRAIN Batch 19/4000 loss 15.116323 loss_att 19.915138 loss_ctc 18.529606 loss_rnnt 13.620607 hw_loss 0.151588 lr 0.00039221 rank 3
2023-02-23 14:34:41,098 DEBUG TRAIN Batch 19/4000 loss 4.944441 loss_att 9.684518 loss_ctc 8.498747 loss_rnnt 3.435519 hw_loss 0.163124 lr 0.00039228 rank 4
2023-02-23 14:34:41,099 DEBUG TRAIN Batch 19/4000 loss 6.755944 loss_att 8.925785 loss_ctc 8.825011 loss_rnnt 5.924544 hw_loss 0.227917 lr 0.00039230 rank 0
2023-02-23 14:34:41,099 DEBUG TRAIN Batch 19/4000 loss 8.970508 loss_att 10.342472 loss_ctc 9.700304 loss_rnnt 8.487848 hw_loss 0.208050 lr 0.00039231 rank 1
2023-02-23 14:34:41,100 DEBUG TRAIN Batch 19/4000 loss 7.610727 loss_att 11.551881 loss_ctc 11.877550 loss_rnnt 6.140138 hw_loss 0.212715 lr 0.00039225 rank 5
2023-02-23 14:34:41,103 DEBUG TRAIN Batch 19/4000 loss 12.509642 loss_att 15.667770 loss_ctc 21.597326 loss_rnnt 10.559813 hw_loss 0.199710 lr 0.00039223 rank 2
2023-02-23 14:34:41,107 DEBUG TRAIN Batch 19/4000 loss 6.895406 loss_att 9.369165 loss_ctc 10.189524 loss_rnnt 5.834597 hw_loss 0.237828 lr 0.00039219 rank 7
2023-02-23 14:34:41,108 DEBUG TRAIN Batch 19/4000 loss 8.549231 loss_att 13.791330 loss_ctc 11.189283 loss_rnnt 7.030328 hw_loss 0.222143 lr 0.00039228 rank 6
2023-02-23 14:35:55,900 DEBUG TRAIN Batch 19/4100 loss 14.933278 loss_att 21.274969 loss_ctc 22.409016 loss_rnnt 12.560127 hw_loss 0.202587 lr 0.00039217 rank 0
2023-02-23 14:35:55,902 DEBUG TRAIN Batch 19/4100 loss 6.604134 loss_att 8.722676 loss_ctc 9.409323 loss_rnnt 5.696372 hw_loss 0.206304 lr 0.00039212 rank 5
2023-02-23 14:35:55,903 DEBUG TRAIN Batch 19/4100 loss 13.905955 loss_att 19.797646 loss_ctc 22.270500 loss_rnnt 11.534903 hw_loss 0.145205 lr 0.00039209 rank 3
2023-02-23 14:35:55,903 DEBUG TRAIN Batch 19/4100 loss 4.103192 loss_att 7.572667 loss_ctc 6.915436 loss_rnnt 2.915319 hw_loss 0.223148 lr 0.00039211 rank 2
2023-02-23 14:35:55,905 DEBUG TRAIN Batch 19/4100 loss 10.689081 loss_att 11.964514 loss_ctc 17.307791 loss_rnnt 9.481325 hw_loss 0.131578 lr 0.00039219 rank 1
2023-02-23 14:35:55,906 DEBUG TRAIN Batch 19/4100 loss 9.293548 loss_att 10.369120 loss_ctc 12.144817 loss_rnnt 8.568405 hw_loss 0.243485 lr 0.00039216 rank 6
2023-02-23 14:35:55,908 DEBUG TRAIN Batch 19/4100 loss 1.813779 loss_att 4.559424 loss_ctc 3.096089 loss_rnnt 0.915991 hw_loss 0.333157 lr 0.00039207 rank 7
2023-02-23 14:35:55,911 DEBUG TRAIN Batch 19/4100 loss 5.235588 loss_att 10.585623 loss_ctc 12.034610 loss_rnnt 3.110821 hw_loss 0.277918 lr 0.00039216 rank 4
2023-02-23 14:37:13,144 DEBUG TRAIN Batch 19/4200 loss 12.201777 loss_att 14.315325 loss_ctc 14.117169 loss_rnnt 11.428787 hw_loss 0.177930 lr 0.00039200 rank 5
2023-02-23 14:37:13,148 DEBUG TRAIN Batch 19/4200 loss 1.187975 loss_att 2.886547 loss_ctc 1.289275 loss_rnnt 0.710503 hw_loss 0.232968 lr 0.00039197 rank 3
2023-02-23 14:37:13,149 DEBUG TRAIN Batch 19/4200 loss 14.469557 loss_att 17.067669 loss_ctc 21.671793 loss_rnnt 12.856442 hw_loss 0.249739 lr 0.00039204 rank 4
2023-02-23 14:37:13,152 DEBUG TRAIN Batch 19/4200 loss 8.599426 loss_att 11.519341 loss_ctc 9.670502 loss_rnnt 7.736339 hw_loss 0.255554 lr 0.00039203 rank 6
2023-02-23 14:37:13,152 DEBUG TRAIN Batch 19/4200 loss 8.658967 loss_att 10.541441 loss_ctc 16.590351 loss_rnnt 7.161222 hw_loss 0.119497 lr 0.00039206 rank 1
2023-02-23 14:37:13,153 DEBUG TRAIN Batch 19/4200 loss 6.387306 loss_att 10.406606 loss_ctc 10.693651 loss_rnnt 4.926371 hw_loss 0.155430 lr 0.00039205 rank 0
2023-02-23 14:37:13,154 DEBUG TRAIN Batch 19/4200 loss 11.975609 loss_att 15.341842 loss_ctc 14.037000 loss_rnnt 10.937012 hw_loss 0.169685 lr 0.00039199 rank 2
2023-02-23 14:37:13,157 DEBUG TRAIN Batch 19/4200 loss 8.000024 loss_att 10.359892 loss_ctc 12.202833 loss_rnnt 6.888703 hw_loss 0.148072 lr 0.00039195 rank 7
2023-02-23 14:38:31,718 DEBUG TRAIN Batch 19/4300 loss 5.157681 loss_att 7.607680 loss_ctc 7.444544 loss_rnnt 4.247125 hw_loss 0.216826 lr 0.00039193 rank 0
2023-02-23 14:38:31,720 DEBUG TRAIN Batch 19/4300 loss 11.984169 loss_att 16.254362 loss_ctc 19.750237 loss_rnnt 9.964310 hw_loss 0.244399 lr 0.00039192 rank 4
2023-02-23 14:38:31,722 DEBUG TRAIN Batch 19/4300 loss 15.302726 loss_att 16.855675 loss_ctc 19.399126 loss_rnnt 14.336754 hw_loss 0.204741 lr 0.00039185 rank 3
2023-02-23 14:38:31,722 DEBUG TRAIN Batch 19/4300 loss 8.988202 loss_att 11.160180 loss_ctc 12.585037 loss_rnnt 7.959298 hw_loss 0.215492 lr 0.00039187 rank 2
2023-02-23 14:38:31,723 DEBUG TRAIN Batch 19/4300 loss 11.669716 loss_att 14.076591 loss_ctc 17.934448 loss_rnnt 10.236635 hw_loss 0.218264 lr 0.00039188 rank 5
2023-02-23 14:38:31,724 DEBUG TRAIN Batch 19/4300 loss 7.588906 loss_att 10.120205 loss_ctc 8.105595 loss_rnnt 6.862866 hw_loss 0.282914 lr 0.00039194 rank 1
2023-02-23 14:38:31,733 DEBUG TRAIN Batch 19/4300 loss 9.959292 loss_att 12.638189 loss_ctc 16.580925 loss_rnnt 8.468958 hw_loss 0.134383 lr 0.00039183 rank 7
2023-02-23 14:38:31,770 DEBUG TRAIN Batch 19/4300 loss 11.088491 loss_att 15.063715 loss_ctc 15.567566 loss_rnnt 9.531428 hw_loss 0.309014 lr 0.00039191 rank 6
2023-02-23 14:39:48,383 DEBUG TRAIN Batch 19/4400 loss 11.466075 loss_att 14.067356 loss_ctc 15.166666 loss_rnnt 10.294202 hw_loss 0.296632 lr 0.00039181 rank 0
2023-02-23 14:39:48,384 DEBUG TRAIN Batch 19/4400 loss 12.180236 loss_att 13.507567 loss_ctc 15.679217 loss_rnnt 11.303096 hw_loss 0.272143 lr 0.00039175 rank 2
2023-02-23 14:39:48,387 DEBUG TRAIN Batch 19/4400 loss 7.374218 loss_att 7.653556 loss_ctc 8.448938 loss_rnnt 7.050346 hw_loss 0.233826 lr 0.00039176 rank 5
2023-02-23 14:39:48,388 DEBUG TRAIN Batch 19/4400 loss 5.856938 loss_att 5.705251 loss_ctc 8.345420 loss_rnnt 5.378492 hw_loss 0.331849 lr 0.00039173 rank 3
2023-02-23 14:39:48,389 DEBUG TRAIN Batch 19/4400 loss 5.291982 loss_att 5.934346 loss_ctc 6.867890 loss_rnnt 4.776577 hw_loss 0.331519 lr 0.00039179 rank 6
2023-02-23 14:39:48,390 DEBUG TRAIN Batch 19/4400 loss 6.617035 loss_att 8.634004 loss_ctc 8.869574 loss_rnnt 5.784614 hw_loss 0.241292 lr 0.00039180 rank 4
2023-02-23 14:39:48,391 DEBUG TRAIN Batch 19/4400 loss 13.534909 loss_att 14.833496 loss_ctc 19.621368 loss_rnnt 12.314017 hw_loss 0.280587 lr 0.00039171 rank 7
2023-02-23 14:39:48,394 DEBUG TRAIN Batch 19/4400 loss 10.112854 loss_att 11.186010 loss_ctc 12.166649 loss_rnnt 9.506380 hw_loss 0.221257 lr 0.00039182 rank 1
2023-02-23 14:41:04,914 DEBUG TRAIN Batch 19/4500 loss 7.973001 loss_att 9.046732 loss_ctc 11.008502 loss_rnnt 7.241598 hw_loss 0.209856 lr 0.00039169 rank 0
2023-02-23 14:41:04,915 DEBUG TRAIN Batch 19/4500 loss 11.813471 loss_att 13.215961 loss_ctc 15.008807 loss_rnnt 11.060028 hw_loss 0.087937 lr 0.00039164 rank 5
2023-02-23 14:41:04,916 DEBUG TRAIN Batch 19/4500 loss 11.723568 loss_att 14.664717 loss_ctc 12.260962 loss_rnnt 10.989023 hw_loss 0.139990 lr 0.00039163 rank 2
2023-02-23 14:41:04,918 DEBUG TRAIN Batch 19/4500 loss 14.085697 loss_att 14.716415 loss_ctc 17.810528 loss_rnnt 13.358239 hw_loss 0.196258 lr 0.00039170 rank 1
2023-02-23 14:41:04,921 DEBUG TRAIN Batch 19/4500 loss 12.932658 loss_att 14.065104 loss_ctc 16.602423 loss_rnnt 12.088340 hw_loss 0.240988 lr 0.00039168 rank 4
2023-02-23 14:41:04,923 DEBUG TRAIN Batch 19/4500 loss 9.393235 loss_att 13.844597 loss_ctc 13.018789 loss_rnnt 7.888914 hw_loss 0.244953 lr 0.00039167 rank 6
2023-02-23 14:41:04,924 DEBUG TRAIN Batch 19/4500 loss 10.676855 loss_att 10.635680 loss_ctc 12.790776 loss_rnnt 10.234403 hw_loss 0.316557 lr 0.00039159 rank 7
2023-02-23 14:41:04,926 DEBUG TRAIN Batch 19/4500 loss 11.133553 loss_att 17.282618 loss_ctc 18.916225 loss_rnnt 8.697456 hw_loss 0.316113 lr 0.00039161 rank 3
2023-02-23 14:42:24,182 DEBUG TRAIN Batch 19/4600 loss 11.470037 loss_att 15.160822 loss_ctc 18.174837 loss_rnnt 9.757093 hw_loss 0.151527 lr 0.00039152 rank 5
2023-02-23 14:42:24,187 DEBUG TRAIN Batch 19/4600 loss 3.971877 loss_att 10.057556 loss_ctc 6.389421 loss_rnnt 2.331611 hw_loss 0.188981 lr 0.00039155 rank 6
2023-02-23 14:42:24,190 DEBUG TRAIN Batch 19/4600 loss 3.719656 loss_att 7.551518 loss_ctc 5.258065 loss_rnnt 2.685701 hw_loss 0.117117 lr 0.00039149 rank 3
2023-02-23 14:42:24,190 DEBUG TRAIN Batch 19/4600 loss 6.210116 loss_att 8.709318 loss_ctc 7.027155 loss_rnnt 5.486085 hw_loss 0.216097 lr 0.00039157 rank 0
2023-02-23 14:42:24,190 DEBUG TRAIN Batch 19/4600 loss 9.145177 loss_att 13.282026 loss_ctc 19.282532 loss_rnnt 6.886505 hw_loss 0.149352 lr 0.00039158 rank 1
2023-02-23 14:42:24,191 DEBUG TRAIN Batch 19/4600 loss 7.387960 loss_att 10.593781 loss_ctc 11.389277 loss_rnnt 6.117210 hw_loss 0.180146 lr 0.00039156 rank 4
2023-02-23 14:42:24,193 DEBUG TRAIN Batch 19/4600 loss 18.167805 loss_att 21.168198 loss_ctc 28.180250 loss_rnnt 16.157478 hw_loss 0.141099 lr 0.00039147 rank 7
2023-02-23 14:42:24,193 DEBUG TRAIN Batch 19/4600 loss 14.241520 loss_att 15.211008 loss_ctc 19.899750 loss_rnnt 13.192065 hw_loss 0.189611 lr 0.00039151 rank 2
2023-02-23 14:43:40,063 DEBUG TRAIN Batch 19/4700 loss 10.896570 loss_att 12.284335 loss_ctc 14.879051 loss_rnnt 9.960651 hw_loss 0.238814 lr 0.00039146 rank 1
2023-02-23 14:43:40,065 DEBUG TRAIN Batch 19/4700 loss 10.907689 loss_att 13.533340 loss_ctc 14.403877 loss_rnnt 9.810892 hw_loss 0.197827 lr 0.00039135 rank 7
2023-02-23 14:43:40,068 DEBUG TRAIN Batch 19/4700 loss 8.350574 loss_att 10.313317 loss_ctc 14.920833 loss_rnnt 7.018398 hw_loss 0.119234 lr 0.00039137 rank 3
2023-02-23 14:43:40,069 DEBUG TRAIN Batch 19/4700 loss 8.075094 loss_att 12.747935 loss_ctc 14.207186 loss_rnnt 6.201778 hw_loss 0.227129 lr 0.00039140 rank 5
2023-02-23 14:43:40,069 DEBUG TRAIN Batch 19/4700 loss 10.050910 loss_att 11.176693 loss_ctc 14.616413 loss_rnnt 9.113538 hw_loss 0.194027 lr 0.00039139 rank 2
2023-02-23 14:43:40,071 DEBUG TRAIN Batch 19/4700 loss 8.142841 loss_att 10.432880 loss_ctc 14.899268 loss_rnnt 6.704884 hw_loss 0.148299 lr 0.00039143 rank 6
2023-02-23 14:43:40,093 DEBUG TRAIN Batch 19/4700 loss 4.398411 loss_att 7.023892 loss_ctc 8.084121 loss_rnnt 3.303268 hw_loss 0.147411 lr 0.00039144 rank 4
2023-02-23 14:43:40,105 DEBUG TRAIN Batch 19/4700 loss 7.311319 loss_att 10.418339 loss_ctc 11.070327 loss_rnnt 6.066098 hw_loss 0.229904 lr 0.00039145 rank 0
2023-02-23 14:44:56,289 DEBUG TRAIN Batch 19/4800 loss 7.589776 loss_att 11.005989 loss_ctc 10.953345 loss_rnnt 6.372312 hw_loss 0.160772 lr 0.00039132 rank 4
2023-02-23 14:44:56,294 DEBUG TRAIN Batch 19/4800 loss 11.888729 loss_att 16.194874 loss_ctc 17.256874 loss_rnnt 10.230335 hw_loss 0.152649 lr 0.00039128 rank 5
2023-02-23 14:44:56,295 DEBUG TRAIN Batch 19/4800 loss 8.175115 loss_att 9.816466 loss_ctc 11.090173 loss_rnnt 7.331163 hw_loss 0.238138 lr 0.00039123 rank 7
2023-02-23 14:44:56,295 DEBUG TRAIN Batch 19/4800 loss 15.070327 loss_att 20.923023 loss_ctc 25.413071 loss_rnnt 12.431578 hw_loss 0.167208 lr 0.00039134 rank 1
2023-02-23 14:44:56,296 DEBUG TRAIN Batch 19/4800 loss 10.759949 loss_att 13.522593 loss_ctc 15.221167 loss_rnnt 9.535931 hw_loss 0.143738 lr 0.00039131 rank 6
2023-02-23 14:44:56,298 DEBUG TRAIN Batch 19/4800 loss 9.724335 loss_att 11.926533 loss_ctc 12.192427 loss_rnnt 8.838692 hw_loss 0.217733 lr 0.00039127 rank 2
2023-02-23 14:44:56,298 DEBUG TRAIN Batch 19/4800 loss 6.396332 loss_att 10.519214 loss_ctc 9.739824 loss_rnnt 4.999758 hw_loss 0.236622 lr 0.00039133 rank 0
2023-02-23 14:44:56,299 DEBUG TRAIN Batch 19/4800 loss 11.837118 loss_att 14.437519 loss_ctc 16.419775 loss_rnnt 10.602818 hw_loss 0.193500 lr 0.00039125 rank 3
2023-02-23 14:46:12,279 DEBUG TRAIN Batch 19/4900 loss 11.729771 loss_att 15.937677 loss_ctc 16.082579 loss_rnnt 10.213581 hw_loss 0.176687 lr 0.00039120 rank 4
2023-02-23 14:46:12,280 DEBUG TRAIN Batch 19/4900 loss 14.558761 loss_att 16.269714 loss_ctc 18.108852 loss_rnnt 13.605278 hw_loss 0.258648 lr 0.00039116 rank 5
2023-02-23 14:46:12,279 DEBUG TRAIN Batch 19/4900 loss 14.740438 loss_att 16.427456 loss_ctc 18.254684 loss_rnnt 13.804476 hw_loss 0.243732 lr 0.00039119 rank 6
2023-02-23 14:46:12,281 DEBUG TRAIN Batch 19/4900 loss 10.204205 loss_att 13.790599 loss_ctc 17.089367 loss_rnnt 8.462017 hw_loss 0.200411 lr 0.00039115 rank 2
2023-02-23 14:46:12,285 DEBUG TRAIN Batch 19/4900 loss 15.751964 loss_att 18.546127 loss_ctc 23.776413 loss_rnnt 14.043888 hw_loss 0.148715 lr 0.00039113 rank 3
2023-02-23 14:46:12,284 DEBUG TRAIN Batch 19/4900 loss 6.082279 loss_att 7.991396 loss_ctc 9.695276 loss_rnnt 5.082949 hw_loss 0.254576 lr 0.00039121 rank 0
2023-02-23 14:46:12,284 DEBUG TRAIN Batch 19/4900 loss 9.496857 loss_att 14.002468 loss_ctc 14.727376 loss_rnnt 7.760558 hw_loss 0.258327 lr 0.00039111 rank 7
2023-02-23 14:46:12,285 DEBUG TRAIN Batch 19/4900 loss 10.225134 loss_att 13.695366 loss_ctc 14.290426 loss_rnnt 8.833967 hw_loss 0.290778 lr 0.00039122 rank 1
2023-02-23 14:47:31,026 DEBUG TRAIN Batch 19/5000 loss 14.914957 loss_att 17.456244 loss_ctc 22.967127 loss_rnnt 13.266544 hw_loss 0.124747 lr 0.00039103 rank 2
2023-02-23 14:47:31,027 DEBUG TRAIN Batch 19/5000 loss 11.102749 loss_att 14.327024 loss_ctc 16.125822 loss_rnnt 9.708652 hw_loss 0.149061 lr 0.00039104 rank 5
2023-02-23 14:47:31,029 DEBUG TRAIN Batch 19/5000 loss 9.707823 loss_att 12.161081 loss_ctc 13.017222 loss_rnnt 8.702324 hw_loss 0.137988 lr 0.00039101 rank 3
2023-02-23 14:47:31,029 DEBUG TRAIN Batch 19/5000 loss 12.956687 loss_att 14.915762 loss_ctc 13.866163 loss_rnnt 12.326221 hw_loss 0.220100 lr 0.00039108 rank 4
2023-02-23 14:47:31,030 DEBUG TRAIN Batch 19/5000 loss 11.625664 loss_att 13.130105 loss_ctc 14.316931 loss_rnnt 10.844578 hw_loss 0.227554 lr 0.00039109 rank 0
2023-02-23 14:47:31,034 DEBUG TRAIN Batch 19/5000 loss 7.362625 loss_att 8.085495 loss_ctc 7.717800 loss_rnnt 7.055453 hw_loss 0.216077 lr 0.00039099 rank 7
2023-02-23 14:47:31,036 DEBUG TRAIN Batch 19/5000 loss 5.896266 loss_att 8.214494 loss_ctc 7.349051 loss_rnnt 5.100780 hw_loss 0.259004 lr 0.00039107 rank 6
2023-02-23 14:47:31,075 DEBUG TRAIN Batch 19/5000 loss 4.973702 loss_att 7.216585 loss_ctc 9.062505 loss_rnnt 3.849109 hw_loss 0.245329 lr 0.00039110 rank 1
2023-02-23 14:48:47,022 DEBUG TRAIN Batch 19/5100 loss 10.277693 loss_att 10.764587 loss_ctc 14.263981 loss_rnnt 9.497886 hw_loss 0.282982 lr 0.00039091 rank 2
2023-02-23 14:48:47,023 DEBUG TRAIN Batch 19/5100 loss 14.982889 loss_att 14.912794 loss_ctc 20.799580 loss_rnnt 14.067889 hw_loss 0.287737 lr 0.00039097 rank 0
2023-02-23 14:48:47,026 DEBUG TRAIN Batch 19/5100 loss 7.507670 loss_att 8.131492 loss_ctc 10.628560 loss_rnnt 6.829391 hw_loss 0.257618 lr 0.00039087 rank 7
2023-02-23 14:48:47,027 DEBUG TRAIN Batch 19/5100 loss 6.992211 loss_att 12.190537 loss_ctc 10.671137 loss_rnnt 5.321809 hw_loss 0.262899 lr 0.00039092 rank 5
2023-02-23 14:48:47,027 DEBUG TRAIN Batch 19/5100 loss 9.692463 loss_att 11.676907 loss_ctc 12.698108 loss_rnnt 8.756623 hw_loss 0.259120 lr 0.00039098 rank 1
2023-02-23 14:48:47,028 DEBUG TRAIN Batch 19/5100 loss 10.856942 loss_att 14.418447 loss_ctc 16.821270 loss_rnnt 9.258600 hw_loss 0.170243 lr 0.00039096 rank 4
2023-02-23 14:48:47,028 DEBUG TRAIN Batch 19/5100 loss 7.883030 loss_att 11.625453 loss_ctc 14.220518 loss_rnnt 6.152069 hw_loss 0.257772 lr 0.00039089 rank 3
2023-02-23 14:48:47,029 DEBUG TRAIN Batch 19/5100 loss 7.699228 loss_att 11.538846 loss_ctc 8.031360 loss_rnnt 6.701397 hw_loss 0.348044 lr 0.00039095 rank 6
2023-02-23 14:50:03,483 DEBUG TRAIN Batch 19/5200 loss 4.417025 loss_att 6.754350 loss_ctc 5.359110 loss_rnnt 3.673889 hw_loss 0.281361 lr 0.00039081 rank 5
2023-02-23 14:50:03,484 DEBUG TRAIN Batch 19/5200 loss 10.404779 loss_att 11.793909 loss_ctc 11.556163 loss_rnnt 9.819762 hw_loss 0.288139 lr 0.00039077 rank 3
2023-02-23 14:50:03,488 DEBUG TRAIN Batch 19/5200 loss 10.541241 loss_att 17.072767 loss_ctc 17.084888 loss_rnnt 8.221644 hw_loss 0.264009 lr 0.00039079 rank 2
2023-02-23 14:50:03,489 DEBUG TRAIN Batch 19/5200 loss 6.898483 loss_att 7.407324 loss_ctc 9.791986 loss_rnnt 6.262223 hw_loss 0.278794 lr 0.00039084 rank 4
2023-02-23 14:50:03,492 DEBUG TRAIN Batch 19/5200 loss 6.902124 loss_att 8.718144 loss_ctc 16.906507 loss_rnnt 5.070798 hw_loss 0.251634 lr 0.00039075 rank 7
2023-02-23 14:50:03,490 DEBUG TRAIN Batch 19/5200 loss 3.767299 loss_att 6.915721 loss_ctc 5.245350 loss_rnnt 2.828030 hw_loss 0.210958 lr 0.00039085 rank 0
2023-02-23 14:50:03,492 DEBUG TRAIN Batch 19/5200 loss 9.719658 loss_att 12.758989 loss_ctc 17.212774 loss_rnnt 8.034481 hw_loss 0.146677 lr 0.00039086 rank 1
2023-02-23 14:50:03,540 DEBUG TRAIN Batch 19/5200 loss 10.273009 loss_att 13.971689 loss_ctc 13.671066 loss_rnnt 9.003836 hw_loss 0.143179 lr 0.00039083 rank 6
2023-02-23 14:51:21,583 DEBUG TRAIN Batch 19/5300 loss 9.810473 loss_att 15.882154 loss_ctc 12.990215 loss_rnnt 8.044918 hw_loss 0.238600 lr 0.00039069 rank 5
2023-02-23 14:51:21,583 DEBUG TRAIN Batch 19/5300 loss 18.916504 loss_att 21.930771 loss_ctc 27.952682 loss_rnnt 16.948460 hw_loss 0.300687 lr 0.00039072 rank 4
2023-02-23 14:51:21,589 DEBUG TRAIN Batch 19/5300 loss 6.788532 loss_att 11.455583 loss_ctc 9.427250 loss_rnnt 5.350431 hw_loss 0.286615 lr 0.00039065 rank 3
2023-02-23 14:51:21,590 DEBUG TRAIN Batch 19/5300 loss 9.454800 loss_att 12.761299 loss_ctc 12.110686 loss_rnnt 8.320182 hw_loss 0.223502 lr 0.00039067 rank 2
2023-02-23 14:51:21,592 DEBUG TRAIN Batch 19/5300 loss 5.382751 loss_att 8.635212 loss_ctc 6.198242 loss_rnnt 4.535967 hw_loss 0.164175 lr 0.00039075 rank 1
2023-02-23 14:51:21,592 DEBUG TRAIN Batch 19/5300 loss 5.770065 loss_att 8.417903 loss_ctc 8.475875 loss_rnnt 4.770677 hw_loss 0.204463 lr 0.00039063 rank 7
2023-02-23 14:51:21,593 DEBUG TRAIN Batch 19/5300 loss 16.969862 loss_att 20.788937 loss_ctc 20.512749 loss_rnnt 15.632486 hw_loss 0.189699 lr 0.00039073 rank 0
2023-02-23 14:51:21,596 DEBUG TRAIN Batch 19/5300 loss 9.936198 loss_att 12.713460 loss_ctc 18.445854 loss_rnnt 8.184401 hw_loss 0.115732 lr 0.00039072 rank 6
2023-02-23 14:52:38,559 DEBUG TRAIN Batch 19/5400 loss 9.679618 loss_att 12.286436 loss_ctc 13.078409 loss_rnnt 8.588467 hw_loss 0.218653 lr 0.00039057 rank 5
2023-02-23 14:52:38,560 DEBUG TRAIN Batch 19/5400 loss 13.125705 loss_att 15.380190 loss_ctc 21.530445 loss_rnnt 11.441704 hw_loss 0.210884 lr 0.00039060 rank 4
2023-02-23 14:52:38,563 DEBUG TRAIN Batch 19/5400 loss 7.323814 loss_att 9.524416 loss_ctc 8.887617 loss_rnnt 6.542174 hw_loss 0.249400 lr 0.00039055 rank 2
2023-02-23 14:52:38,567 DEBUG TRAIN Batch 19/5400 loss 5.541708 loss_att 8.347120 loss_ctc 6.432825 loss_rnnt 4.746891 hw_loss 0.215475 lr 0.00039063 rank 1
2023-02-23 14:52:38,569 DEBUG TRAIN Batch 19/5400 loss 10.732955 loss_att 13.322857 loss_ctc 18.621304 loss_rnnt 9.066362 hw_loss 0.181560 lr 0.00039062 rank 0
2023-02-23 14:52:38,569 DEBUG TRAIN Batch 19/5400 loss 10.397532 loss_att 13.126408 loss_ctc 12.157863 loss_rnnt 9.497194 hw_loss 0.224721 lr 0.00039051 rank 7
2023-02-23 14:52:38,571 DEBUG TRAIN Batch 19/5400 loss 13.900661 loss_att 19.873947 loss_ctc 24.253241 loss_rnnt 11.161691 hw_loss 0.307441 lr 0.00039053 rank 3
2023-02-23 14:52:38,574 DEBUG TRAIN Batch 19/5400 loss 10.386868 loss_att 11.901425 loss_ctc 14.747832 loss_rnnt 9.395203 hw_loss 0.201174 lr 0.00039060 rank 6
2023-02-23 14:53:53,921 DEBUG TRAIN Batch 19/5500 loss 12.359841 loss_att 14.037803 loss_ctc 15.944652 loss_rnnt 11.401976 hw_loss 0.270559 lr 0.00039045 rank 5
2023-02-23 14:53:53,927 DEBUG TRAIN Batch 19/5500 loss 13.859993 loss_att 17.465170 loss_ctc 21.258539 loss_rnnt 11.999630 hw_loss 0.286602 lr 0.00039041 rank 3
2023-02-23 14:53:53,927 DEBUG TRAIN Batch 19/5500 loss 9.949163 loss_att 13.332988 loss_ctc 12.073842 loss_rnnt 8.849584 hw_loss 0.261608 lr 0.00039050 rank 0
2023-02-23 14:53:53,929 DEBUG TRAIN Batch 19/5500 loss 11.251677 loss_att 15.705826 loss_ctc 15.690378 loss_rnnt 9.659563 hw_loss 0.205230 lr 0.00039043 rank 2
2023-02-23 14:53:53,929 DEBUG TRAIN Batch 19/5500 loss 12.000114 loss_att 15.094475 loss_ctc 16.733017 loss_rnnt 10.624532 hw_loss 0.235605 lr 0.00039048 rank 4
2023-02-23 14:53:53,931 DEBUG TRAIN Batch 19/5500 loss 7.730415 loss_att 10.235913 loss_ctc 10.483689 loss_rnnt 6.755745 hw_loss 0.199626 lr 0.00039051 rank 1
2023-02-23 14:53:53,934 DEBUG TRAIN Batch 19/5500 loss 7.574972 loss_att 10.839388 loss_ctc 12.682671 loss_rnnt 6.108232 hw_loss 0.249057 lr 0.00039039 rank 7
2023-02-23 14:53:53,938 DEBUG TRAIN Batch 19/5500 loss 4.968024 loss_att 7.799520 loss_ctc 7.723388 loss_rnnt 3.896018 hw_loss 0.259360 lr 0.00039048 rank 6
2023-02-23 14:55:09,628 DEBUG TRAIN Batch 19/5600 loss 8.894625 loss_att 9.258937 loss_ctc 12.728023 loss_rnnt 8.160524 hw_loss 0.281472 lr 0.00039033 rank 5
2023-02-23 14:55:09,629 DEBUG TRAIN Batch 19/5600 loss 7.340767 loss_att 12.432450 loss_ctc 12.291739 loss_rnnt 5.561800 hw_loss 0.188438 lr 0.00039030 rank 3
2023-02-23 14:55:09,632 DEBUG TRAIN Batch 19/5600 loss 7.778262 loss_att 10.015876 loss_ctc 11.955473 loss_rnnt 6.686901 hw_loss 0.162894 lr 0.00039038 rank 0
2023-02-23 14:55:09,632 DEBUG TRAIN Batch 19/5600 loss 5.819057 loss_att 7.650978 loss_ctc 6.211153 loss_rnnt 5.316915 hw_loss 0.156523 lr 0.00039036 rank 6
2023-02-23 14:55:09,636 DEBUG TRAIN Batch 19/5600 loss 7.017255 loss_att 11.694828 loss_ctc 11.367477 loss_rnnt 5.370951 hw_loss 0.245175 lr 0.00039027 rank 7
2023-02-23 14:55:09,637 DEBUG TRAIN Batch 19/5600 loss 8.560787 loss_att 11.281045 loss_ctc 14.906141 loss_rnnt 7.080014 hw_loss 0.170013 lr 0.00039039 rank 1
2023-02-23 14:55:09,662 DEBUG TRAIN Batch 19/5600 loss 17.240452 loss_att 24.280499 loss_ctc 30.268051 loss_rnnt 14.005514 hw_loss 0.168592 lr 0.00039037 rank 4
2023-02-23 14:55:09,670 DEBUG TRAIN Batch 19/5600 loss 14.170860 loss_att 15.877442 loss_ctc 16.347866 loss_rnnt 13.442835 hw_loss 0.180825 lr 0.00039031 rank 2
run_2_21_rnnt_bias_0-3word_finetune.sh: line 167: 31755 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.1.1]:16813: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.0.1]:64237: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:52103
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:43752
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:41901
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:9098
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:39810
