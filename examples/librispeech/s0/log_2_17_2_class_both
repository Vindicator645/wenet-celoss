/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_16_rnnt_bias_both_2_class.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_16_rnnt_bias_loss_2_class_both/ddp_init
2023-02-17 14:35:31,756 INFO training on multiple gpus, this gpu 1
2023-02-17 14:35:31,757 INFO training on multiple gpus, this gpu 2
2023-02-17 14:35:31,757 INFO training on multiple gpus, this gpu 0
2023-02-17 14:35:31,757 INFO training on multiple gpus, this gpu 5
2023-02-17 14:35:31,761 INFO training on multiple gpus, this gpu 4
2023-02-17 14:35:31,761 INFO training on multiple gpus, this gpu 3
2023-02-17 14:35:31,769 INFO training on multiple gpus, this gpu 7
2023-02-17 14:35:31,769 INFO training on multiple gpus, this gpu 6
2023-02-17 14:35:41,937 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-17 14:35:41,948 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-17 14:35:41,977 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-17 14:35:42,977 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-17 14:35:42,987 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-17 14:35:42,994 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-17 14:35:44,022 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-17 14:35:49,119 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-17 14:35:49,120 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:49,121 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:50,068 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:50,132 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:50,136 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:52,148 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:53,279 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:35:56,277 INFO Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-17 14:35:56,277 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-17 14:36:06,453 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,454 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:06,467 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class_both/init.pt
2023-02-17 14:36:06,471 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,472 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:36:06,475 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,476 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:06,519 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,521 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:06,557 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,560 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 14:36:06,561 INFO Epoch 0 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:06,563 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:06,636 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:06,639 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:36:07,047 INFO Epoch 0 TRAIN info lr 4e-08
2023-02-17 14:36:07,051 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58367786
2023-02-17 14:37:08,791 DEBUG TRAIN Batch 0/0 loss 557.516846 loss_att 77.514160 loss_ctc 517.883057 loss_rnnt 658.400757 hw_loss 0.752155 lr 0.00000004 rank 2
2023-02-17 14:37:08,810 DEBUG TRAIN Batch 0/0 loss 588.039612 loss_att 77.099792 loss_ctc 570.017151 loss_rnnt 692.063049 hw_loss 1.064117 lr 0.00000004 rank 1
2023-02-17 14:37:08,816 DEBUG TRAIN Batch 0/0 loss 501.412567 loss_att 74.212875 loss_ctc 478.954590 loss_rnnt 589.471802 hw_loss 0.703388 lr 0.00000004 rank 7
2023-02-17 14:37:08,819 DEBUG TRAIN Batch 0/0 loss 522.309509 loss_att 74.558525 loss_ctc 494.863617 loss_rnnt 615.152710 hw_loss 0.687106 lr 0.00000004 rank 3
2023-02-17 14:37:08,819 DEBUG TRAIN Batch 0/0 loss 542.397644 loss_att 74.409958 loss_ctc 525.720520 loss_rnnt 637.832336 hw_loss 0.724610 lr 0.00000004 rank 5
2023-02-17 14:37:08,828 DEBUG TRAIN Batch 0/0 loss 545.559509 loss_att 83.787941 loss_ctc 504.185028 loss_rnnt 642.977112 hw_loss 0.849841 lr 0.00000004 rank 0
2023-02-17 14:37:08,877 DEBUG TRAIN Batch 0/0 loss 558.248596 loss_att 75.628586 loss_ctc 536.186340 loss_rnnt 657.275757 hw_loss 0.822071 lr 0.00000004 rank 6
2023-02-17 14:37:09,075 DEBUG TRAIN Batch 0/0 loss 547.428162 loss_att 68.157715 loss_ctc 522.774719 loss_rnnt 646.329407 hw_loss 0.449995 lr 0.00000004 rank 4
2023-02-17 14:38:24,804 DEBUG TRAIN Batch 0/100 loss 2179.770508 loss_att 384.767029 loss_ctc 3163.324951 loss_rnnt 2407.484863 hw_loss 0.273461 lr 0.00000404 rank 3
2023-02-17 14:38:24,808 DEBUG TRAIN Batch 0/100 loss 2194.121094 loss_att 459.011841 loss_ctc 3069.996582 loss_rnnt 2424.202148 hw_loss 0.294650 lr 0.00000404 rank 7
2023-02-17 14:38:24,809 DEBUG TRAIN Batch 0/100 loss 2065.549805 loss_att 393.462311 loss_ctc 2928.764404 loss_rnnt 2284.667480 hw_loss 0.383604 lr 0.00000404 rank 5
2023-02-17 14:38:24,809 DEBUG TRAIN Batch 0/100 loss 2089.166748 loss_att 384.945801 loss_ctc 3123.188232 loss_rnnt 2291.926514 hw_loss 0.402527 lr 0.00000404 rank 6
2023-02-17 14:38:24,809 DEBUG TRAIN Batch 0/100 loss 2027.054565 loss_att 362.972809 loss_ctc 3119.694824 loss_rnnt 2214.016113 hw_loss 0.317709 lr 0.00000404 rank 1
2023-02-17 14:38:24,810 DEBUG TRAIN Batch 0/100 loss 2113.529053 loss_att 431.983337 loss_ctc 3013.928711 loss_rnnt 2329.587402 hw_loss 0.370099 lr 0.00000404 rank 0
2023-02-17 14:38:24,815 DEBUG TRAIN Batch 0/100 loss 2060.899414 loss_att 357.861450 loss_ctc 2983.847656 loss_rnnt 2278.170654 hw_loss 0.519096 lr 0.00000404 rank 2
2023-02-17 14:38:24,853 DEBUG TRAIN Batch 0/100 loss 2143.561768 loss_att 349.064331 loss_ctc 3065.179688 loss_rnnt 2379.378174 hw_loss 0.375924 lr 0.00000404 rank 4
2023-02-17 14:39:38,238 DEBUG TRAIN Batch 0/200 loss 888.415894 loss_att 357.441254 loss_ctc 2491.504395 loss_rnnt 780.745789 hw_loss 0.224718 lr 0.00000804 rank 4
2023-02-17 14:39:38,240 DEBUG TRAIN Batch 0/200 loss 921.176636 loss_att 421.311157 loss_ctc 2533.969727 loss_rnnt 806.037537 hw_loss 0.137198 lr 0.00000804 rank 2
2023-02-17 14:39:38,243 DEBUG TRAIN Batch 0/200 loss 972.770935 loss_att 456.082275 loss_ctc 2543.198975 loss_rnnt 866.653748 hw_loss 0.120939 lr 0.00000804 rank 7
2023-02-17 14:39:38,243 DEBUG TRAIN Batch 0/200 loss 904.062622 loss_att 349.422302 loss_ctc 2490.920654 loss_rnnt 803.324829 hw_loss 0.158991 lr 0.00000804 rank 0
2023-02-17 14:39:38,244 DEBUG TRAIN Batch 0/200 loss 915.435913 loss_att 439.074158 loss_ctc 2531.910156 loss_rnnt 795.107605 hw_loss 0.132642 lr 0.00000804 rank 3
2023-02-17 14:39:38,250 DEBUG TRAIN Batch 0/200 loss 1026.982422 loss_att 380.021118 loss_ctc 2528.508545 loss_rnnt 956.075439 hw_loss 0.179364 lr 0.00000804 rank 6
2023-02-17 14:39:38,252 DEBUG TRAIN Batch 0/200 loss 867.392639 loss_att 348.208466 loss_ctc 2542.527588 loss_rnnt 747.799316 hw_loss 0.147810 lr 0.00000804 rank 5
2023-02-17 14:39:38,286 DEBUG TRAIN Batch 0/200 loss 885.115723 loss_att 389.636200 loss_ctc 2441.352295 loss_rnnt 776.645203 hw_loss 0.127936 lr 0.00000804 rank 1
2023-02-17 14:40:53,412 DEBUG TRAIN Batch 0/300 loss 424.995850 loss_att 372.978821 loss_ctc 691.424438 loss_rnnt 399.829620 hw_loss 0.085806 lr 0.00001204 rank 2
2023-02-17 14:40:53,422 DEBUG TRAIN Batch 0/300 loss 492.439758 loss_att 392.131744 loss_ctc 1178.072754 loss_rnnt 421.008423 hw_loss 0.140956 lr 0.00001204 rank 7
2023-02-17 14:40:53,423 DEBUG TRAIN Batch 0/300 loss 419.591003 loss_att 332.554871 loss_ctc 1051.108887 loss_rnnt 352.733429 hw_loss 0.116938 lr 0.00001204 rank 1
2023-02-17 14:40:53,425 DEBUG TRAIN Batch 0/300 loss 473.080353 loss_att 420.757263 loss_ctc 740.723999 loss_rnnt 447.789124 hw_loss 0.131318 lr 0.00001204 rank 4
2023-02-17 14:40:53,425 DEBUG TRAIN Batch 0/300 loss 437.893280 loss_att 387.880066 loss_ctc 707.762939 loss_rnnt 411.879883 hw_loss 0.062672 lr 0.00001204 rank 6
2023-02-17 14:40:53,431 DEBUG TRAIN Batch 0/300 loss 404.723541 loss_att 308.141479 loss_ctc 1047.420410 loss_rnnt 338.238312 hw_loss 0.203851 lr 0.00001204 rank 0
2023-02-17 14:40:53,432 DEBUG TRAIN Batch 0/300 loss 444.862762 loss_att 352.214386 loss_ctc 1085.778442 loss_rnnt 377.825256 hw_loss 0.209471 lr 0.00001204 rank 3
2023-02-17 14:40:53,435 DEBUG TRAIN Batch 0/300 loss 491.644836 loss_att 385.226868 loss_ctc 1229.906006 loss_rnnt 414.399170 hw_loss 0.177087 lr 0.00001204 rank 5
2023-02-17 14:42:08,715 DEBUG TRAIN Batch 0/400 loss 361.884918 loss_att 288.223969 loss_ctc 863.355591 loss_rnnt 309.691406 hw_loss 0.117990 lr 0.00001604 rank 4
2023-02-17 14:42:08,716 DEBUG TRAIN Batch 0/400 loss 297.027557 loss_att 267.579407 loss_ctc 416.944031 loss_rnnt 286.857910 hw_loss 0.132035 lr 0.00001604 rank 7
2023-02-17 14:42:08,721 DEBUG TRAIN Batch 0/400 loss 321.043823 loss_att 292.159241 loss_ctc 450.483826 loss_rnnt 309.479858 hw_loss 0.154209 lr 0.00001604 rank 6
2023-02-17 14:42:08,723 DEBUG TRAIN Batch 0/400 loss 295.534424 loss_att 267.557129 loss_ctc 418.356476 loss_rnnt 284.718201 hw_loss 0.066353 lr 0.00001604 rank 1
2023-02-17 14:42:08,728 DEBUG TRAIN Batch 0/400 loss 373.804993 loss_att 299.937622 loss_ctc 923.630737 loss_rnnt 315.166107 hw_loss 0.191766 lr 0.00001604 rank 2
2023-02-17 14:42:08,728 DEBUG TRAIN Batch 0/400 loss 377.458313 loss_att 253.658539 loss_ctc 1404.100830 loss_rnnt 265.254059 hw_loss 0.147211 lr 0.00001604 rank 5
2023-02-17 14:42:08,730 DEBUG TRAIN Batch 0/400 loss 338.160950 loss_att 309.062500 loss_ctc 482.836853 loss_rnnt 324.548737 hw_loss 0.265800 lr 0.00001604 rank 3
2023-02-17 14:42:08,734 DEBUG TRAIN Batch 0/400 loss 393.840546 loss_att 319.721527 loss_ctc 907.315430 loss_rnnt 340.084534 hw_loss 0.218462 lr 0.00001604 rank 0
2023-02-17 14:43:22,490 DEBUG TRAIN Batch 0/500 loss 277.736816 loss_att 220.709686 loss_ctc 577.902588 loss_rnnt 248.927399 hw_loss 0.361411 lr 0.00002004 rank 4
2023-02-17 14:43:22,492 DEBUG TRAIN Batch 0/500 loss 292.082184 loss_att 236.151031 loss_ctc 586.199890 loss_rnnt 263.907959 hw_loss 0.271349 lr 0.00002004 rank 7
2023-02-17 14:43:22,492 DEBUG TRAIN Batch 0/500 loss 255.389786 loss_att 221.108246 loss_ctc 350.252563 loss_rnnt 249.462677 hw_loss 0.253198 lr 0.00002004 rank 6
2023-02-17 14:43:22,493 DEBUG TRAIN Batch 0/500 loss 284.381989 loss_att 235.430618 loss_ctc 511.616913 loss_rnnt 263.801819 hw_loss 0.135831 lr 0.00002004 rank 1
2023-02-17 14:43:22,494 DEBUG TRAIN Batch 0/500 loss 301.439697 loss_att 265.067322 loss_ctc 403.050690 loss_rnnt 295.089722 hw_loss 0.143038 lr 0.00002004 rank 5
2023-02-17 14:43:22,501 DEBUG TRAIN Batch 0/500 loss 261.898468 loss_att 229.308777 loss_ctc 346.638123 loss_rnnt 257.032928 hw_loss 0.159103 lr 0.00002004 rank 0
2023-02-17 14:43:22,501 DEBUG TRAIN Batch 0/500 loss 305.935516 loss_att 251.671112 loss_ctc 569.812866 loss_rnnt 281.510620 hw_loss 0.176455 lr 0.00002004 rank 3
2023-02-17 14:43:22,544 DEBUG TRAIN Batch 0/500 loss 311.255676 loss_att 240.044281 loss_ctc 744.742188 loss_rnnt 267.607086 hw_loss 0.173730 lr 0.00002004 rank 2
2023-02-17 14:44:36,616 DEBUG TRAIN Batch 0/600 loss 135.919037 loss_att 102.812019 loss_ctc 327.319092 loss_rnnt 116.930054 hw_loss 0.169459 lr 0.00002404 rank 7
2023-02-17 14:44:36,619 DEBUG TRAIN Batch 0/600 loss 257.572357 loss_att 195.713287 loss_ctc 610.301025 loss_rnnt 222.811569 hw_loss 0.191453 lr 0.00002404 rank 6
2023-02-17 14:44:36,620 DEBUG TRAIN Batch 0/600 loss 215.383545 loss_att 178.283875 loss_ctc 358.439545 loss_rnnt 203.616806 hw_loss 0.211015 lr 0.00002404 rank 3
2023-02-17 14:44:36,621 DEBUG TRAIN Batch 0/600 loss 155.877777 loss_att 122.696518 loss_ctc 327.737518 loss_rnnt 139.484818 hw_loss 0.214791 lr 0.00002404 rank 1
2023-02-17 14:44:36,626 DEBUG TRAIN Batch 0/600 loss 216.743286 loss_att 163.505249 loss_ctc 516.205750 loss_rnnt 187.381042 hw_loss 0.152837 lr 0.00002404 rank 5
2023-02-17 14:44:36,627 DEBUG TRAIN Batch 0/600 loss 98.949509 loss_att 69.912613 loss_ctc 296.567932 loss_rnnt 78.286629 hw_loss 0.227125 lr 0.00002404 rank 4
2023-02-17 14:44:36,645 DEBUG TRAIN Batch 0/600 loss 118.910728 loss_att 92.229874 loss_ctc 264.866608 loss_rnnt 104.559486 hw_loss 0.424910 lr 0.00002404 rank 2
2023-02-17 14:44:36,648 DEBUG TRAIN Batch 0/600 loss 209.036057 loss_att 179.936066 loss_ctc 277.879608 loss_rnnt 205.635162 hw_loss 0.078242 lr 0.00002404 rank 0
2023-02-17 14:45:52,805 DEBUG TRAIN Batch 0/700 loss 381.617645 loss_att 332.696808 loss_ctc 457.278564 loss_rnnt 381.216370 hw_loss 0.182498 lr 0.00002804 rank 6
2023-02-17 14:45:52,816 DEBUG TRAIN Batch 0/700 loss 367.058319 loss_att 277.871033 loss_ctc 860.110291 loss_rnnt 319.090942 hw_loss 0.121056 lr 0.00002804 rank 1
2023-02-17 14:45:52,819 DEBUG TRAIN Batch 0/700 loss 327.303253 loss_att 283.761261 loss_ctc 414.707764 loss_rnnt 324.322693 hw_loss 0.065724 lr 0.00002804 rank 4
2023-02-17 14:45:52,822 DEBUG TRAIN Batch 0/700 loss 307.509735 loss_att 264.945618 loss_ctc 382.407684 loss_rnnt 306.000092 hw_loss 0.067608 lr 0.00002804 rank 0
2023-02-17 14:45:52,822 DEBUG TRAIN Batch 0/700 loss 323.457458 loss_att 277.063385 loss_ctc 403.928314 loss_rnnt 321.999390 hw_loss 0.013953 lr 0.00002804 rank 3
2023-02-17 14:45:52,829 DEBUG TRAIN Batch 0/700 loss 364.040009 loss_att 314.483582 loss_ctc 456.763275 loss_rnnt 361.580750 hw_loss 0.013955 lr 0.00002804 rank 5
2023-02-17 14:45:52,856 DEBUG TRAIN Batch 0/700 loss 286.652496 loss_att 205.436325 loss_ctc 781.898132 loss_rnnt 236.855499 hw_loss 0.013957 lr 0.00002804 rank 7
2023-02-17 14:45:52,873 DEBUG TRAIN Batch 0/700 loss 365.993591 loss_att 317.201050 loss_ctc 436.154999 loss_rnnt 366.378174 hw_loss 0.035726 lr 0.00002804 rank 2
2023-02-17 14:47:06,796 DEBUG TRAIN Batch 0/800 loss 291.852966 loss_att 252.582199 loss_ctc 354.748901 loss_rnnt 291.224335 hw_loss 0.181295 lr 0.00003204 rank 1
2023-02-17 14:47:06,796 DEBUG TRAIN Batch 0/800 loss 280.418701 loss_att 243.439758 loss_ctc 333.556213 loss_rnnt 280.598877 hw_loss 0.244911 lr 0.00003204 rank 7
2023-02-17 14:47:06,798 DEBUG TRAIN Batch 0/800 loss 361.264038 loss_att 283.850861 loss_ctc 720.087341 loss_rnnt 328.839905 hw_loss 0.119361 lr 0.00003204 rank 4
2023-02-17 14:47:06,800 DEBUG TRAIN Batch 0/800 loss 332.473022 loss_att 254.678391 loss_ctc 735.287354 loss_rnnt 294.282166 hw_loss 0.077304 lr 0.00003204 rank 5
2023-02-17 14:47:06,801 DEBUG TRAIN Batch 0/800 loss 305.942169 loss_att 229.915039 loss_ctc 718.812500 loss_rnnt 265.993042 hw_loss 0.197203 lr 0.00003204 rank 6
2023-02-17 14:47:06,802 DEBUG TRAIN Batch 0/800 loss 395.963470 loss_att 312.420746 loss_ctc 785.200378 loss_rnnt 360.700439 hw_loss 0.137392 lr 0.00003204 rank 2
2023-02-17 14:47:06,813 DEBUG TRAIN Batch 0/800 loss 315.015106 loss_att 270.769318 loss_ctc 406.211182 loss_rnnt 311.665131 hw_loss 0.074407 lr 0.00003204 rank 0
2023-02-17 14:47:06,851 DEBUG TRAIN Batch 0/800 loss 374.947876 loss_att 299.305847 loss_ctc 730.497437 loss_rnnt 342.564636 hw_loss 0.197012 lr 0.00003204 rank 3
2023-02-17 14:48:21,010 DEBUG TRAIN Batch 0/900 loss 337.452698 loss_att 298.223694 loss_ctc 348.289185 loss_rnnt 343.761719 hw_loss 0.172336 lr 0.00003604 rank 5
2023-02-17 14:48:21,013 DEBUG TRAIN Batch 0/900 loss 296.508667 loss_att 260.611938 loss_ctc 305.349060 loss_rnnt 302.397705 hw_loss 0.209196 lr 0.00003604 rank 7
2023-02-17 14:48:21,015 DEBUG TRAIN Batch 0/900 loss 290.288116 loss_att 255.730042 loss_ctc 299.486023 loss_rnnt 295.885193 hw_loss 0.165227 lr 0.00003604 rank 1
2023-02-17 14:48:21,017 DEBUG TRAIN Batch 0/900 loss 317.979309 loss_att 279.474060 loss_ctc 327.233276 loss_rnnt 324.257294 hw_loss 0.354731 lr 0.00003604 rank 0
2023-02-17 14:48:21,018 DEBUG TRAIN Batch 0/900 loss 376.488892 loss_att 330.908325 loss_ctc 389.297852 loss_rnnt 383.834686 hw_loss 0.117074 lr 0.00003604 rank 4
2023-02-17 14:48:21,018 DEBUG TRAIN Batch 0/900 loss 339.694946 loss_att 299.588898 loss_ctc 350.465179 loss_rnnt 346.234955 hw_loss 0.084792 lr 0.00003604 rank 3
2023-02-17 14:48:21,018 DEBUG TRAIN Batch 0/900 loss 366.540833 loss_att 321.338989 loss_ctc 376.198364 loss_rnnt 374.243286 hw_loss 0.094173 lr 0.00003604 rank 2
2023-02-17 14:48:21,020 DEBUG TRAIN Batch 0/900 loss 343.988159 loss_att 301.516418 loss_ctc 351.111877 loss_rnnt 351.459229 hw_loss 0.137782 lr 0.00003604 rank 6
2023-02-17 14:49:35,999 DEBUG TRAIN Batch 0/1000 loss 350.515839 loss_att 307.620575 loss_ctc 362.121704 loss_rnnt 357.483765 hw_loss 0.119435 lr 0.00004004 rank 7
2023-02-17 14:49:36,003 DEBUG TRAIN Batch 0/1000 loss 293.592682 loss_att 259.372833 loss_ctc 302.745605 loss_rnnt 299.145813 hw_loss 0.132039 lr 0.00004004 rank 6
2023-02-17 14:49:36,003 DEBUG TRAIN Batch 0/1000 loss 318.654633 loss_att 278.539490 loss_ctc 328.809998 loss_rnnt 325.277893 hw_loss 0.085764 lr 0.00004004 rank 5
2023-02-17 14:49:36,004 DEBUG TRAIN Batch 0/1000 loss 301.293213 loss_att 264.869171 loss_ctc 309.542633 loss_rnnt 307.438293 hw_loss 0.074651 lr 0.00004004 rank 1
2023-02-17 14:49:36,007 DEBUG TRAIN Batch 0/1000 loss 358.215942 loss_att 315.982910 loss_ctc 369.221924 loss_rnnt 365.105652 hw_loss 0.167745 lr 0.00004004 rank 3
2023-02-17 14:49:36,006 DEBUG TRAIN Batch 0/1000 loss 340.272247 loss_att 298.678772 loss_ctc 351.199768 loss_rnnt 347.093445 hw_loss 0.075986 lr 0.00004004 rank 4
2023-02-17 14:49:36,007 DEBUG TRAIN Batch 0/1000 loss 297.580688 loss_att 261.893555 loss_ctc 306.082733 loss_rnnt 303.480103 hw_loss 0.195762 lr 0.00004004 rank 2
2023-02-17 14:49:36,054 DEBUG TRAIN Batch 0/1000 loss 282.295013 loss_att 250.408752 loss_ctc 291.616943 loss_rnnt 287.339539 hw_loss 0.168365 lr 0.00004004 rank 0
2023-02-17 14:50:52,310 DEBUG TRAIN Batch 0/1100 loss 282.192596 loss_att 250.674500 loss_ctc 290.971802 loss_rnnt 287.201630 hw_loss 0.232518 lr 0.00004404 rank 4
2023-02-17 14:50:52,311 DEBUG TRAIN Batch 0/1100 loss 279.186829 loss_att 247.969116 loss_ctc 288.359985 loss_rnnt 284.195190 hw_loss 0.022644 lr 0.00004404 rank 1
2023-02-17 14:50:52,313 DEBUG TRAIN Batch 0/1100 loss 302.001068 loss_att 265.412750 loss_ctc 311.833527 loss_rnnt 307.928772 hw_loss 0.147997 lr 0.00004404 rank 2
2023-02-17 14:50:52,314 DEBUG TRAIN Batch 0/1100 loss 258.912659 loss_att 227.750961 loss_ctc 268.148010 loss_rnnt 263.879089 hw_loss 0.064719 lr 0.00004404 rank 7
2023-02-17 14:50:52,317 DEBUG TRAIN Batch 0/1100 loss 286.545074 loss_att 253.274475 loss_ctc 301.294647 loss_rnnt 291.163879 hw_loss 0.128784 lr 0.00004404 rank 3
2023-02-17 14:50:52,319 DEBUG TRAIN Batch 0/1100 loss 300.598846 loss_att 266.719330 loss_ctc 314.408325 loss_rnnt 305.472992 hw_loss 0.113453 lr 0.00004404 rank 6
2023-02-17 14:50:52,320 DEBUG TRAIN Batch 0/1100 loss 280.424255 loss_att 247.800980 loss_ctc 288.518585 loss_rnnt 285.816040 hw_loss 0.100558 lr 0.00004404 rank 0
2023-02-17 14:50:52,365 DEBUG TRAIN Batch 0/1100 loss 276.784943 loss_att 246.151764 loss_ctc 287.334259 loss_rnnt 281.428619 hw_loss 0.143218 lr 0.00004404 rank 5
2023-02-17 14:52:06,370 DEBUG TRAIN Batch 0/1200 loss 194.946396 loss_att 173.534164 loss_ctc 204.157623 loss_rnnt 197.935471 hw_loss 0.122313 lr 0.00004804 rank 1
2023-02-17 14:52:06,371 DEBUG TRAIN Batch 0/1200 loss 224.848297 loss_att 200.337280 loss_ctc 236.259918 loss_rnnt 228.102295 hw_loss 0.237453 lr 0.00004804 rank 0
2023-02-17 14:52:06,378 DEBUG TRAIN Batch 0/1200 loss 172.294525 loss_att 152.918365 loss_ctc 179.386993 loss_rnnt 175.068359 hw_loss 0.291967 lr 0.00004804 rank 7
2023-02-17 14:52:06,384 DEBUG TRAIN Batch 0/1200 loss 235.340118 loss_att 207.947052 loss_ctc 244.418259 loss_rnnt 239.547714 hw_loss 0.113643 lr 0.00004804 rank 4
2023-02-17 14:52:06,386 DEBUG TRAIN Batch 0/1200 loss 151.142395 loss_att 133.911621 loss_ctc 159.636932 loss_rnnt 153.361191 hw_loss 0.177643 lr 0.00004804 rank 2
2023-02-17 14:52:06,386 DEBUG TRAIN Batch 0/1200 loss 273.224274 loss_att 242.601898 loss_ctc 287.088501 loss_rnnt 277.468811 hw_loss 0.058838 lr 0.00004804 rank 3
2023-02-17 14:52:06,386 DEBUG TRAIN Batch 0/1200 loss 236.428665 loss_att 209.898651 loss_ctc 245.911133 loss_rnnt 240.411789 hw_loss 0.109779 lr 0.00004804 rank 6
2023-02-17 14:52:06,388 DEBUG TRAIN Batch 0/1200 loss 274.898010 loss_att 242.999512 loss_ctc 288.505981 loss_rnnt 279.378723 hw_loss 0.158634 lr 0.00004804 rank 5
2023-02-17 14:53:21,428 DEBUG TRAIN Batch 0/1300 loss 374.962250 loss_att 331.776398 loss_ctc 396.203674 loss_rnnt 380.706268 hw_loss 0.114351 lr 0.00005204 rank 3
2023-02-17 14:53:21,430 DEBUG TRAIN Batch 0/1300 loss 372.373871 loss_att 328.259460 loss_ctc 393.634125 loss_rnnt 378.337524 hw_loss 0.046014 lr 0.00005204 rank 0
2023-02-17 14:53:21,436 DEBUG TRAIN Batch 0/1300 loss 316.949310 loss_att 281.131989 loss_ctc 330.201141 loss_rnnt 322.284546 hw_loss 0.115006 lr 0.00005204 rank 7
2023-02-17 14:53:21,441 DEBUG TRAIN Batch 0/1300 loss 313.145172 loss_att 278.960022 loss_ctc 327.663757 loss_rnnt 317.995728 hw_loss 0.094983 lr 0.00005204 rank 2
2023-02-17 14:53:21,443 DEBUG TRAIN Batch 0/1300 loss 303.439880 loss_att 269.980988 loss_ctc 322.934875 loss_rnnt 307.459808 hw_loss 0.135988 lr 0.00005204 rank 1
2023-02-17 14:53:21,445 DEBUG TRAIN Batch 0/1300 loss 327.043701 loss_att 289.308594 loss_ctc 341.402832 loss_rnnt 332.615784 hw_loss 0.113248 lr 0.00005204 rank 4
2023-02-17 14:53:21,448 DEBUG TRAIN Batch 0/1300 loss 77.067345 loss_att 68.580742 loss_ctc 81.377487 loss_rnnt 78.097656 hw_loss 0.173104 lr 0.00005204 rank 6
2023-02-17 14:53:21,453 DEBUG TRAIN Batch 0/1300 loss 153.977737 loss_att 136.162872 loss_ctc 163.630066 loss_rnnt 156.125809 hw_loss 0.239822 lr 0.00005204 rank 5
2023-02-17 14:54:37,667 DEBUG TRAIN Batch 0/1400 loss 353.126465 loss_att 315.444458 loss_ctc 372.619629 loss_rnnt 357.878601 hw_loss 0.347189 lr 0.00005604 rank 6
2023-02-17 14:54:37,669 DEBUG TRAIN Batch 0/1400 loss 297.567444 loss_att 265.884766 loss_ctc 313.679413 loss_rnnt 301.664429 hw_loss 0.171135 lr 0.00005604 rank 1
2023-02-17 14:54:37,670 DEBUG TRAIN Batch 0/1400 loss 344.347351 loss_att 308.013916 loss_ctc 365.799530 loss_rnnt 348.709167 hw_loss 0.083595 lr 0.00005604 rank 2
2023-02-17 14:54:37,673 DEBUG TRAIN Batch 0/1400 loss 316.666718 loss_att 279.048401 loss_ctc 334.272766 loss_rnnt 321.795288 hw_loss 0.089302 lr 0.00005604 rank 7
2023-02-17 14:54:37,674 DEBUG TRAIN Batch 0/1400 loss 342.664124 loss_att 305.788483 loss_ctc 362.418762 loss_rnnt 347.311157 hw_loss 0.176503 lr 0.00005604 rank 5
2023-02-17 14:54:37,676 DEBUG TRAIN Batch 0/1400 loss 310.588501 loss_att 276.104767 loss_ctc 326.800629 loss_rnnt 315.237030 hw_loss 0.162435 lr 0.00005604 rank 0
2023-02-17 14:54:37,717 DEBUG TRAIN Batch 0/1400 loss 302.033600 loss_att 268.587433 loss_ctc 317.959930 loss_rnnt 306.546082 hw_loss 0.099772 lr 0.00005604 rank 4
2023-02-17 14:54:37,737 DEBUG TRAIN Batch 0/1400 loss 280.736053 loss_att 249.383560 loss_ctc 294.759491 loss_rnnt 285.028748 hw_loss 0.202466 lr 0.00005604 rank 3
2023-02-17 14:55:51,846 DEBUG TRAIN Batch 0/1500 loss 288.455048 loss_att 254.978760 loss_ctc 305.420135 loss_rnnt 292.822754 hw_loss 0.122930 lr 0.00006004 rank 7
2023-02-17 14:55:51,849 DEBUG TRAIN Batch 0/1500 loss 284.824341 loss_att 255.999329 loss_ctc 304.735443 loss_rnnt 287.924255 hw_loss 0.019246 lr 0.00006004 rank 1
2023-02-17 14:55:51,848 DEBUG TRAIN Batch 0/1500 loss 275.236725 loss_att 244.490891 loss_ctc 297.817596 loss_rnnt 278.364868 hw_loss 0.019240 lr 0.00006004 rank 4
2023-02-17 14:55:51,852 DEBUG TRAIN Batch 0/1500 loss 304.435150 loss_att 271.581726 loss_ctc 324.329376 loss_rnnt 308.293823 hw_loss 0.111490 lr 0.00006004 rank 2
2023-02-17 14:55:51,855 DEBUG TRAIN Batch 0/1500 loss 330.328064 loss_att 296.398407 loss_ctc 354.360535 loss_rnnt 333.899384 hw_loss 0.019240 lr 0.00006004 rank 6
2023-02-17 14:55:51,856 DEBUG TRAIN Batch 0/1500 loss 289.297455 loss_att 256.910461 loss_ctc 306.191864 loss_rnnt 293.483978 hw_loss 0.071767 lr 0.00006004 rank 3
2023-02-17 14:55:51,857 DEBUG TRAIN Batch 0/1500 loss 323.498566 loss_att 285.609741 loss_ctc 352.933929 loss_rnnt 327.088867 hw_loss 0.117662 lr 0.00006004 rank 0
2023-02-17 14:55:51,858 DEBUG TRAIN Batch 0/1500 loss 323.879669 loss_att 287.828552 loss_ctc 344.842743 loss_rnnt 328.226654 hw_loss 0.127763 lr 0.00006004 rank 5
2023-02-17 14:57:05,983 DEBUG TRAIN Batch 0/1600 loss 315.961151 loss_att 282.707855 loss_ctc 337.839783 loss_rnnt 319.685730 hw_loss 0.016738 lr 0.00006404 rank 1
2023-02-17 14:57:05,996 DEBUG TRAIN Batch 0/1600 loss 295.642853 loss_att 264.922821 loss_ctc 318.195435 loss_rnnt 298.742310 hw_loss 0.070362 lr 0.00006404 rank 7
2023-02-17 14:57:06,003 DEBUG TRAIN Batch 0/1600 loss 279.496124 loss_att 249.085663 loss_ctc 299.602356 loss_rnnt 282.813934 hw_loss 0.156491 lr 0.00006404 rank 4
2023-02-17 14:57:06,006 DEBUG TRAIN Batch 0/1600 loss 294.383850 loss_att 260.306396 loss_ctc 315.296082 loss_rnnt 298.340393 hw_loss 0.132418 lr 0.00006404 rank 3
2023-02-17 14:57:06,007 DEBUG TRAIN Batch 0/1600 loss 300.815521 loss_att 271.067383 loss_ctc 328.966492 loss_rnnt 302.974365 hw_loss 0.069927 lr 0.00006404 rank 5
2023-02-17 14:57:06,008 DEBUG TRAIN Batch 0/1600 loss 295.624603 loss_att 263.460815 loss_ctc 312.799408 loss_rnnt 299.617950 hw_loss 0.280174 lr 0.00006404 rank 2
2023-02-17 14:57:06,008 DEBUG TRAIN Batch 0/1600 loss 264.139923 loss_att 234.204483 loss_ctc 285.684082 loss_rnnt 267.174622 hw_loss 0.149695 lr 0.00006404 rank 0
2023-02-17 14:57:06,053 DEBUG TRAIN Batch 0/1600 loss 319.005768 loss_att 281.000885 loss_ctc 342.035767 loss_rnnt 323.440613 hw_loss 0.178979 lr 0.00006404 rank 6
2023-02-17 14:58:19,613 DEBUG TRAIN Batch 0/1700 loss 264.523346 loss_att 237.022339 loss_ctc 288.981812 loss_rnnt 266.725098 hw_loss 0.069982 lr 0.00006804 rank 7
2023-02-17 14:58:19,613 DEBUG TRAIN Batch 0/1700 loss 309.466492 loss_att 277.907074 loss_ctc 337.483398 loss_rnnt 311.934723 hw_loss 0.202553 lr 0.00006804 rank 5
2023-02-17 14:58:19,614 DEBUG TRAIN Batch 0/1700 loss 271.200317 loss_att 241.197052 loss_ctc 292.206787 loss_rnnt 274.311890 hw_loss 0.165396 lr 0.00006804 rank 0
2023-02-17 14:58:19,617 DEBUG TRAIN Batch 0/1700 loss 246.602600 loss_att 219.222916 loss_ctc 261.953583 loss_rnnt 249.943405 hw_loss 0.165651 lr 0.00006804 rank 1
2023-02-17 14:58:19,618 DEBUG TRAIN Batch 0/1700 loss 292.828918 loss_att 258.567200 loss_ctc 315.959442 loss_rnnt 296.556396 hw_loss 0.076469 lr 0.00006804 rank 6
2023-02-17 14:58:19,618 DEBUG TRAIN Batch 0/1700 loss 290.569519 loss_att 257.304840 loss_ctc 309.595154 loss_rnnt 294.613159 hw_loss 0.136007 lr 0.00006804 rank 3
2023-02-17 14:58:19,624 DEBUG TRAIN Batch 0/1700 loss 270.558502 loss_att 243.194199 loss_ctc 298.194061 loss_rnnt 272.286987 hw_loss 0.111806 lr 0.00006804 rank 2
2023-02-17 14:58:19,659 DEBUG TRAIN Batch 0/1700 loss 332.146576 loss_att 297.299072 loss_ctc 354.789856 loss_rnnt 336.070923 hw_loss 0.048940 lr 0.00006804 rank 4
2023-02-17 14:59:35,812 DEBUG TRAIN Batch 0/1800 loss 141.634277 loss_att 127.506737 loss_ctc 154.285416 loss_rnnt 142.599960 hw_loss 0.324384 lr 0.00007204 rank 1
2023-02-17 14:59:35,812 DEBUG TRAIN Batch 0/1800 loss 281.788574 loss_att 252.381439 loss_ctc 303.914154 loss_rnnt 284.658386 hw_loss 0.115341 lr 0.00007204 rank 3
2023-02-17 14:59:35,819 DEBUG TRAIN Batch 0/1800 loss 215.155655 loss_att 192.116531 loss_ctc 237.283981 loss_rnnt 216.687698 hw_loss 0.235042 lr 0.00007204 rank 7
2023-02-17 14:59:35,819 DEBUG TRAIN Batch 0/1800 loss 229.760986 loss_att 206.342102 loss_ctc 245.573166 loss_rnnt 232.331223 hw_loss 0.009832 lr 0.00007204 rank 6
2023-02-17 14:59:35,820 DEBUG TRAIN Batch 0/1800 loss 266.523560 loss_att 237.460754 loss_ctc 290.741119 loss_rnnt 269.031311 hw_loss 0.142105 lr 0.00007204 rank 0
2023-02-17 14:59:35,820 DEBUG TRAIN Batch 0/1800 loss 249.466614 loss_att 219.931946 loss_ctc 265.018921 loss_rnnt 253.224472 hw_loss 0.141452 lr 0.00007204 rank 4
2023-02-17 14:59:35,821 DEBUG TRAIN Batch 0/1800 loss 258.923920 loss_att 229.021667 loss_ctc 283.910645 loss_rnnt 261.480316 hw_loss 0.173408 lr 0.00007204 rank 2
2023-02-17 14:59:35,822 DEBUG TRAIN Batch 0/1800 loss 222.799454 loss_att 198.611359 loss_ctc 242.375656 loss_rnnt 225.008347 hw_loss 0.034831 lr 0.00007204 rank 5
2023-02-17 15:00:50,122 DEBUG TRAIN Batch 0/1900 loss 352.244507 loss_att 314.038422 loss_ctc 383.317627 loss_rnnt 355.667603 hw_loss 0.140678 lr 0.00007604 rank 1
2023-02-17 15:00:50,133 DEBUG TRAIN Batch 0/1900 loss 105.467636 loss_att 95.334732 loss_ctc 113.252670 loss_rnnt 106.292099 hw_loss 0.307735 lr 0.00007604 rank 6
2023-02-17 15:00:50,139 DEBUG TRAIN Batch 0/1900 loss 159.200989 loss_att 142.321198 loss_ctc 172.056976 loss_rnnt 160.814911 hw_loss 0.089840 lr 0.00007604 rank 7
2023-02-17 15:00:50,139 DEBUG TRAIN Batch 0/1900 loss 178.605667 loss_att 159.837036 loss_ctc 195.889740 loss_rnnt 179.960983 hw_loss 0.176013 lr 0.00007604 rank 3
2023-02-17 15:00:50,141 DEBUG TRAIN Batch 0/1900 loss 119.244537 loss_att 106.117233 loss_ctc 129.253632 loss_rnnt 120.440544 hw_loss 0.177964 lr 0.00007604 rank 4
2023-02-17 15:00:50,144 DEBUG TRAIN Batch 0/1900 loss 224.584274 loss_att 201.604492 loss_ctc 241.592514 loss_rnnt 226.809433 hw_loss 0.193194 lr 0.00007604 rank 5
2023-02-17 15:00:50,145 DEBUG TRAIN Batch 0/1900 loss 133.878143 loss_att 120.048996 loss_ctc 142.523102 loss_rnnt 135.397461 hw_loss 0.175991 lr 0.00007604 rank 2
2023-02-17 15:00:50,152 DEBUG TRAIN Batch 0/1900 loss 98.313957 loss_att 88.355186 loss_ctc 108.312851 loss_rnnt 98.833221 hw_loss 0.261189 lr 0.00007604 rank 0
2023-02-17 15:02:04,116 DEBUG TRAIN Batch 0/2000 loss 242.706818 loss_att 219.065292 loss_ctc 265.520447 loss_rnnt 244.351959 hw_loss 0.077518 lr 0.00008004 rank 7
2023-02-17 15:02:04,118 DEBUG TRAIN Batch 0/2000 loss 353.336670 loss_att 316.376740 loss_ctc 374.827118 loss_rnnt 357.808594 hw_loss 0.102459 lr 0.00008004 rank 4
2023-02-17 15:02:04,120 DEBUG TRAIN Batch 0/2000 loss 332.838776 loss_att 298.812988 loss_ctc 358.988770 loss_rnnt 336.126129 hw_loss 0.058369 lr 0.00008004 rank 1
2023-02-17 15:02:04,120 DEBUG TRAIN Batch 0/2000 loss 303.284698 loss_att 277.085938 loss_ctc 339.829407 loss_rnnt 303.557220 hw_loss 0.177390 lr 0.00008004 rank 6
2023-02-17 15:02:04,122 DEBUG TRAIN Batch 0/2000 loss 345.263275 loss_att 306.234436 loss_ctc 376.726807 loss_rnnt 348.843750 hw_loss 0.056517 lr 0.00008004 rank 5
2023-02-17 15:02:04,127 DEBUG TRAIN Batch 0/2000 loss 321.524139 loss_att 287.526001 loss_ctc 356.360321 loss_rnnt 323.596619 hw_loss 0.154358 lr 0.00008004 rank 0
2023-02-17 15:02:04,131 DEBUG TRAIN Batch 0/2000 loss 319.078796 loss_att 283.932648 loss_ctc 352.713867 loss_rnnt 321.580505 hw_loss 0.080322 lr 0.00008004 rank 2
2023-02-17 15:02:04,174 DEBUG TRAIN Batch 0/2000 loss 291.677185 loss_att 262.344360 loss_ctc 321.881592 loss_rnnt 293.442780 hw_loss 0.138231 lr 0.00008004 rank 3
2023-02-17 15:03:19,403 DEBUG TRAIN Batch 0/2100 loss 284.355194 loss_att 255.879761 loss_ctc 304.016479 loss_rnnt 287.423615 hw_loss 0.009697 lr 0.00008404 rank 7
2023-02-17 15:03:19,404 DEBUG TRAIN Batch 0/2100 loss 309.042328 loss_att 278.519501 loss_ctc 338.585999 loss_rnnt 311.177124 hw_loss 0.057399 lr 0.00008404 rank 1
2023-02-17 15:03:19,406 DEBUG TRAIN Batch 0/2100 loss 307.351990 loss_att 273.086212 loss_ctc 333.481812 loss_rnnt 310.676147 hw_loss 0.084427 lr 0.00008404 rank 6
2023-02-17 15:03:19,409 DEBUG TRAIN Batch 0/2100 loss 346.479279 loss_att 309.841156 loss_ctc 383.891602 loss_rnnt 348.788635 hw_loss 0.056160 lr 0.00008404 rank 2
2023-02-17 15:03:19,410 DEBUG TRAIN Batch 0/2100 loss 281.982544 loss_att 252.736160 loss_ctc 306.475586 loss_rnnt 284.481964 hw_loss 0.157733 lr 0.00008404 rank 0
2023-02-17 15:03:19,412 DEBUG TRAIN Batch 0/2100 loss 296.974731 loss_att 263.127136 loss_ctc 321.448639 loss_rnnt 300.459961 hw_loss 0.039533 lr 0.00008404 rank 3
2023-02-17 15:03:19,412 DEBUG TRAIN Batch 0/2100 loss 316.807190 loss_att 280.617645 loss_ctc 346.554626 loss_rnnt 320.001282 hw_loss 0.145278 lr 0.00008404 rank 5
2023-02-17 15:03:19,415 DEBUG TRAIN Batch 0/2100 loss 305.378967 loss_att 274.323120 loss_ctc 336.616913 loss_rnnt 307.313446 hw_loss 0.209379 lr 0.00008404 rank 4
2023-02-17 15:04:33,617 DEBUG TRAIN Batch 0/2200 loss 293.032166 loss_att 263.946533 loss_ctc 325.707764 loss_rnnt 294.423462 hw_loss 0.129528 lr 0.00008804 rank 3
2023-02-17 15:04:33,622 DEBUG TRAIN Batch 0/2200 loss 326.757935 loss_att 292.871948 loss_ctc 358.338806 loss_rnnt 329.255310 hw_loss 0.129473 lr 0.00008804 rank 1
2023-02-17 15:04:33,623 DEBUG TRAIN Batch 0/2200 loss 321.228058 loss_att 288.825195 loss_ctc 350.007935 loss_rnnt 323.812561 hw_loss 0.110072 lr 0.00008804 rank 4
2023-02-17 15:04:33,623 DEBUG TRAIN Batch 0/2200 loss 254.386002 loss_att 226.014709 loss_ctc 280.652588 loss_rnnt 256.504395 hw_loss 0.100601 lr 0.00008804 rank 2
2023-02-17 15:04:33,623 DEBUG TRAIN Batch 0/2200 loss 283.622437 loss_att 250.550186 loss_ctc 316.341370 loss_rnnt 285.839600 hw_loss 0.065165 lr 0.00008804 rank 0
2023-02-17 15:04:33,624 DEBUG TRAIN Batch 0/2200 loss 319.439209 loss_att 290.671082 loss_ctc 357.184448 loss_rnnt 320.141357 hw_loss 0.035219 lr 0.00008804 rank 7
2023-02-17 15:04:33,625 DEBUG TRAIN Batch 0/2200 loss 250.596008 loss_att 225.889954 loss_ctc 275.953522 loss_rnnt 252.078583 hw_loss 0.145556 lr 0.00008804 rank 6
2023-02-17 15:04:33,628 DEBUG TRAIN Batch 0/2200 loss 263.575653 loss_att 236.788773 loss_ctc 290.579071 loss_rnnt 265.268738 hw_loss 0.119696 lr 0.00008804 rank 5
2023-02-17 15:05:47,368 DEBUG TRAIN Batch 0/2300 loss 301.132904 loss_att 271.072632 loss_ctc 337.595947 loss_rnnt 302.224243 hw_loss 0.110631 lr 0.00009204 rank 3
2023-02-17 15:05:47,388 DEBUG TRAIN Batch 0/2300 loss 317.325897 loss_att 278.626160 loss_ctc 347.118713 loss_rnnt 320.995728 hw_loss 0.183233 lr 0.00009204 rank 7
2023-02-17 15:05:47,390 DEBUG TRAIN Batch 0/2300 loss 351.190369 loss_att 313.234009 loss_ctc 377.329956 loss_rnnt 355.237915 hw_loss 0.109542 lr 0.00009204 rank 5
2023-02-17 15:05:47,391 DEBUG TRAIN Batch 0/2300 loss 276.952087 loss_att 247.525970 loss_ctc 308.260010 loss_rnnt 278.594788 hw_loss 0.127735 lr 0.00009204 rank 6
2023-02-17 15:05:47,394 DEBUG TRAIN Batch 0/2300 loss 198.479431 loss_att 176.164200 loss_ctc 220.720200 loss_rnnt 199.876312 hw_loss 0.188848 lr 0.00009204 rank 1
2023-02-17 15:05:47,394 DEBUG TRAIN Batch 0/2300 loss 293.133728 loss_att 262.283600 loss_ctc 316.090271 loss_rnnt 296.204041 hw_loss 0.072885 lr 0.00009204 rank 4
2023-02-17 15:05:47,395 DEBUG TRAIN Batch 0/2300 loss 223.683029 loss_att 200.888214 loss_ctc 254.331329 loss_rnnt 224.095932 hw_loss 0.111759 lr 0.00009204 rank 2
2023-02-17 15:05:47,401 DEBUG TRAIN Batch 0/2300 loss 271.411682 loss_att 242.370407 loss_ctc 296.449097 loss_rnnt 273.840210 hw_loss 0.077555 lr 0.00009204 rank 0
2023-02-17 15:07:00,970 DEBUG TRAIN Batch 0/2400 loss 275.217865 loss_att 248.048538 loss_ctc 312.453064 loss_rnnt 275.640381 hw_loss 0.087528 lr 0.00009604 rank 7
2023-02-17 15:07:00,983 DEBUG TRAIN Batch 0/2400 loss 228.950027 loss_att 201.020462 loss_ctc 248.469025 loss_rnnt 231.829056 hw_loss 0.195616 lr 0.00009604 rank 1
2023-02-17 15:07:00,987 DEBUG TRAIN Batch 0/2400 loss 289.245789 loss_att 260.007568 loss_ctc 321.149658 loss_rnnt 290.799072 hw_loss 0.076005 lr 0.00009604 rank 6
2023-02-17 15:07:00,988 DEBUG TRAIN Batch 0/2400 loss 284.333282 loss_att 252.620026 loss_ctc 313.244446 loss_rnnt 286.792725 hw_loss 0.053195 lr 0.00009604 rank 0
2023-02-17 15:07:00,993 DEBUG TRAIN Batch 0/2400 loss 259.994904 loss_att 233.310364 loss_ctc 285.171478 loss_rnnt 261.890686 hw_loss 0.157960 lr 0.00009604 rank 5
2023-02-17 15:07:01,005 DEBUG TRAIN Batch 0/2400 loss 231.398605 loss_att 207.409683 loss_ctc 257.428436 loss_rnnt 232.655746 hw_loss 0.131252 lr 0.00009604 rank 3
2023-02-17 15:07:01,052 DEBUG TRAIN Batch 0/2400 loss 290.074219 loss_att 262.526398 loss_ctc 320.508606 loss_rnnt 291.442200 hw_loss 0.156855 lr 0.00009604 rank 2
2023-02-17 15:07:01,070 DEBUG TRAIN Batch 0/2400 loss 285.986176 loss_att 255.651642 loss_ctc 314.472290 loss_rnnt 288.249512 hw_loss 0.010158 lr 0.00009604 rank 4
2023-02-17 15:08:17,004 DEBUG TRAIN Batch 0/2500 loss 218.365280 loss_att 196.982193 loss_ctc 247.559189 loss_rnnt 218.604965 hw_loss 0.270773 lr 0.00010004 rank 3
2023-02-17 15:08:17,006 DEBUG TRAIN Batch 0/2500 loss 143.984985 loss_att 128.939911 loss_ctc 154.355515 loss_rnnt 145.552185 hw_loss 0.110764 lr 0.00010004 rank 2
2023-02-17 15:08:17,006 DEBUG TRAIN Batch 0/2500 loss 318.939392 loss_att 284.489288 loss_ctc 361.521973 loss_rnnt 320.022186 hw_loss 0.242904 lr 0.00010004 rank 1
2023-02-17 15:08:17,007 DEBUG TRAIN Batch 0/2500 loss 228.382751 loss_att 204.445435 loss_ctc 252.159424 loss_rnnt 229.965210 hw_loss 0.065219 lr 0.00010004 rank 7
2023-02-17 15:08:17,008 DEBUG TRAIN Batch 0/2500 loss 223.351379 loss_att 200.186615 loss_ctc 248.595993 loss_rnnt 224.554031 hw_loss 0.120609 lr 0.00010004 rank 4
2023-02-17 15:08:17,012 DEBUG TRAIN Batch 0/2500 loss 105.387413 loss_att 95.288788 loss_ctc 116.534569 loss_rnnt 105.807083 hw_loss 0.213296 lr 0.00010004 rank 6
2023-02-17 15:08:17,016 DEBUG TRAIN Batch 0/2500 loss 245.870590 loss_att 217.663879 loss_ctc 271.981903 loss_rnnt 248.026215 hw_loss 0.007911 lr 0.00010004 rank 5
2023-02-17 15:08:17,018 DEBUG TRAIN Batch 0/2500 loss 113.426567 loss_att 102.044106 loss_ctc 125.955162 loss_rnnt 113.836578 hw_loss 0.367513 lr 0.00010004 rank 0
2023-02-17 15:09:31,157 DEBUG TRAIN Batch 0/2600 loss 359.771942 loss_att 319.876953 loss_ctc 392.791077 loss_rnnt 363.321289 hw_loss 0.050789 lr 0.00010404 rank 3
2023-02-17 15:09:31,170 DEBUG TRAIN Batch 0/2600 loss 64.908401 loss_att 59.654167 loss_ctc 72.919189 loss_rnnt 64.713310 hw_loss 0.333449 lr 0.00010404 rank 7
2023-02-17 15:09:31,171 DEBUG TRAIN Batch 0/2600 loss 256.959229 loss_att 229.923462 loss_ctc 288.133026 loss_rnnt 258.134033 hw_loss 0.142190 lr 0.00010404 rank 1
2023-02-17 15:09:31,173 DEBUG TRAIN Batch 0/2600 loss 326.318695 loss_att 292.043823 loss_ctc 358.573242 loss_rnnt 328.850067 hw_loss 0.043097 lr 0.00010404 rank 4
2023-02-17 15:09:31,174 DEBUG TRAIN Batch 0/2600 loss 349.403625 loss_att 311.587982 loss_ctc 387.049927 loss_rnnt 351.897217 hw_loss 0.093837 lr 0.00010404 rank 6
2023-02-17 15:09:31,175 DEBUG TRAIN Batch 0/2600 loss 51.212849 loss_att 48.209351 loss_ctc 55.453571 loss_rnnt 51.135227 hw_loss 0.211663 lr 0.00010404 rank 5
2023-02-17 15:09:31,179 DEBUG TRAIN Batch 0/2600 loss 303.866364 loss_att 274.645935 loss_ctc 332.641541 loss_rnnt 305.810486 hw_loss 0.118613 lr 0.00010404 rank 2
2023-02-17 15:09:31,223 DEBUG TRAIN Batch 0/2600 loss 276.706482 loss_att 245.350967 loss_ctc 298.499847 loss_rnnt 279.967255 hw_loss 0.195977 lr 0.00010404 rank 0
2023-02-17 15:10:44,769 DEBUG TRAIN Batch 0/2700 loss 300.937347 loss_att 275.543793 loss_ctc 328.856689 loss_rnnt 302.219604 hw_loss 0.138510 lr 0.00010804 rank 3
2023-02-17 15:10:44,772 DEBUG TRAIN Batch 0/2700 loss 285.311401 loss_att 255.755066 loss_ctc 317.280151 loss_rnnt 286.816101 hw_loss 0.270173 lr 0.00010804 rank 7
2023-02-17 15:10:44,775 DEBUG TRAIN Batch 0/2700 loss 325.872925 loss_att 293.588867 loss_ctc 358.316650 loss_rnnt 327.936157 hw_loss 0.127044 lr 0.00010804 rank 6
2023-02-17 15:10:44,778 DEBUG TRAIN Batch 0/2700 loss 278.398651 loss_att 250.442841 loss_ctc 309.778381 loss_rnnt 279.764587 hw_loss 0.077380 lr 0.00010804 rank 1
2023-02-17 15:10:44,780 DEBUG TRAIN Batch 0/2700 loss 291.007446 loss_att 263.168457 loss_ctc 320.431366 loss_rnnt 292.615692 hw_loss 0.068179 lr 0.00010804 rank 5
2023-02-17 15:10:44,783 DEBUG TRAIN Batch 0/2700 loss 267.977814 loss_att 240.061127 loss_ctc 296.328949 loss_rnnt 269.702850 hw_loss 0.146533 lr 0.00010804 rank 4
2023-02-17 15:10:44,786 DEBUG TRAIN Batch 0/2700 loss 271.633331 loss_att 247.502808 loss_ctc 303.248230 loss_rnnt 272.132568 hw_loss 0.209120 lr 0.00010804 rank 2
2023-02-17 15:10:44,789 DEBUG TRAIN Batch 0/2700 loss 344.176514 loss_att 311.605988 loss_ctc 381.879272 loss_rnnt 345.654358 hw_loss 0.017205 lr 0.00010804 rank 0
2023-02-17 15:12:00,590 DEBUG TRAIN Batch 0/2800 loss 260.811707 loss_att 236.250458 loss_ctc 281.573578 loss_rnnt 262.929016 hw_loss 0.050039 lr 0.00011204 rank 4
2023-02-17 15:12:00,591 DEBUG TRAIN Batch 0/2800 loss 253.612320 loss_att 232.367554 loss_ctc 277.069458 loss_rnnt 254.641342 hw_loss 0.173100 lr 0.00011204 rank 7
2023-02-17 15:12:00,592 DEBUG TRAIN Batch 0/2800 loss 256.087250 loss_att 235.343872 loss_ctc 284.532776 loss_rnnt 256.345093 hw_loss 0.183924 lr 0.00011204 rank 3
2023-02-17 15:12:00,593 DEBUG TRAIN Batch 0/2800 loss 289.315460 loss_att 258.903839 loss_ctc 311.165466 loss_rnnt 292.366669 hw_loss 0.220853 lr 0.00011204 rank 6
2023-02-17 15:12:00,597 DEBUG TRAIN Batch 0/2800 loss 274.859192 loss_att 252.914139 loss_ctc 306.434570 loss_rnnt 274.978180 hw_loss 0.112408 lr 0.00011204 rank 5
2023-02-17 15:12:00,600 DEBUG TRAIN Batch 0/2800 loss 266.143524 loss_att 242.075516 loss_ctc 297.296265 loss_rnnt 266.759460 hw_loss 0.082424 lr 0.00011204 rank 2
2023-02-17 15:12:00,602 DEBUG TRAIN Batch 0/2800 loss 314.326874 loss_att 284.153015 loss_ctc 341.770874 loss_rnnt 316.621552 hw_loss 0.151670 lr 0.00011204 rank 0
2023-02-17 15:12:00,605 DEBUG TRAIN Batch 0/2800 loss 284.627899 loss_att 258.698914 loss_ctc 324.889679 loss_rnnt 284.381775 hw_loss 0.119398 lr 0.00011204 rank 1
2023-02-17 15:13:15,064 DEBUG TRAIN Batch 0/2900 loss 253.529602 loss_att 232.655685 loss_ctc 270.636719 loss_rnnt 255.372025 hw_loss 0.096376 lr 0.00011604 rank 7
2023-02-17 15:13:15,064 DEBUG TRAIN Batch 0/2900 loss 290.738495 loss_att 264.622864 loss_ctc 315.400513 loss_rnnt 292.609589 hw_loss 0.119551 lr 0.00011604 rank 3
2023-02-17 15:13:15,066 DEBUG TRAIN Batch 0/2900 loss 289.758118 loss_att 262.416901 loss_ctc 318.673767 loss_rnnt 291.334045 hw_loss 0.069173 lr 0.00011604 rank 6
2023-02-17 15:13:15,067 DEBUG TRAIN Batch 0/2900 loss 271.598206 loss_att 246.276428 loss_ctc 295.720703 loss_rnnt 273.334625 hw_loss 0.209216 lr 0.00011604 rank 2
2023-02-17 15:13:15,071 DEBUG TRAIN Batch 0/2900 loss 242.702499 loss_att 222.103851 loss_ctc 271.438354 loss_rnnt 242.950897 hw_loss 0.074802 lr 0.00011604 rank 0
2023-02-17 15:13:15,072 DEBUG TRAIN Batch 0/2900 loss 256.544006 loss_att 231.246490 loss_ctc 280.333893 loss_rnnt 258.343018 hw_loss 0.165959 lr 0.00011604 rank 4
2023-02-17 15:13:15,070 DEBUG TRAIN Batch 0/2900 loss 230.628677 loss_att 211.873138 loss_ctc 253.604691 loss_rnnt 231.235947 hw_loss 0.150692 lr 0.00011604 rank 1
2023-02-17 15:13:15,077 DEBUG TRAIN Batch 0/2900 loss 320.039642 loss_att 286.231262 loss_ctc 350.761505 loss_rnnt 322.634521 hw_loss 0.132328 lr 0.00011604 rank 5
2023-02-17 15:14:28,923 DEBUG TRAIN Batch 0/3000 loss 265.656464 loss_att 241.694611 loss_ctc 284.965942 loss_rnnt 267.836487 hw_loss 0.070835 lr 0.00012004 rank 7
2023-02-17 15:14:28,922 DEBUG TRAIN Batch 0/3000 loss 246.858612 loss_att 223.615204 loss_ctc 268.914124 loss_rnnt 248.493958 hw_loss 0.136112 lr 0.00012004 rank 4
2023-02-17 15:14:28,927 DEBUG TRAIN Batch 0/3000 loss 244.492279 loss_att 228.655945 loss_ctc 277.686737 loss_rnnt 243.207184 hw_loss 0.049509 lr 0.00012004 rank 3
2023-02-17 15:14:28,928 DEBUG TRAIN Batch 0/3000 loss 244.339417 loss_att 224.443604 loss_ctc 267.753723 loss_rnnt 245.123657 hw_loss 0.136890 lr 0.00012004 rank 2
2023-02-17 15:14:28,930 DEBUG TRAIN Batch 0/3000 loss 249.324554 loss_att 230.520798 loss_ctc 276.588348 loss_rnnt 249.396851 hw_loss 0.099923 lr 0.00012004 rank 6
2023-02-17 15:14:28,930 DEBUG TRAIN Batch 0/3000 loss 260.662231 loss_att 241.958679 loss_ctc 292.651825 loss_rnnt 260.001526 hw_loss 0.255248 lr 0.00012004 rank 5
2023-02-17 15:14:28,936 DEBUG TRAIN Batch 0/3000 loss 244.964706 loss_att 227.687683 loss_ctc 270.991882 loss_rnnt 244.856812 hw_loss 0.174426 lr 0.00012004 rank 0
2023-02-17 15:14:28,937 DEBUG TRAIN Batch 0/3000 loss 218.850494 loss_att 200.299301 loss_ctc 244.858643 loss_rnnt 218.978546 hw_loss 0.214538 lr 0.00012004 rank 1
2023-02-17 15:15:42,554 DEBUG TRAIN Batch 0/3100 loss 226.223984 loss_att 205.923218 loss_ctc 244.043320 loss_rnnt 227.785309 hw_loss 0.230431 lr 0.00012404 rank 3
2023-02-17 15:15:42,562 DEBUG TRAIN Batch 0/3100 loss 202.221344 loss_att 187.579544 loss_ctc 224.794998 loss_rnnt 201.999741 hw_loss 0.262747 lr 0.00012404 rank 2
2023-02-17 15:15:42,563 DEBUG TRAIN Batch 0/3100 loss 218.835739 loss_att 204.207108 loss_ctc 238.052521 loss_rnnt 219.110352 hw_loss 0.166631 lr 0.00012404 rank 5
2023-02-17 15:15:42,564 DEBUG TRAIN Batch 0/3100 loss 283.009583 loss_att 263.374298 loss_ctc 302.520874 loss_rnnt 284.230103 hw_loss 0.196885 lr 0.00012404 rank 1
2023-02-17 15:15:42,565 DEBUG TRAIN Batch 0/3100 loss 230.818649 loss_att 214.873611 loss_ctc 246.345901 loss_rnnt 231.903595 hw_loss 0.063276 lr 0.00012404 rank 7
2023-02-17 15:15:42,568 DEBUG TRAIN Batch 0/3100 loss 253.429108 loss_att 229.448532 loss_ctc 280.843750 loss_rnnt 254.505646 hw_loss 0.120517 lr 0.00012404 rank 4
2023-02-17 15:15:42,566 DEBUG TRAIN Batch 0/3100 loss 166.673706 loss_att 153.726715 loss_ctc 178.903442 loss_rnnt 167.510910 hw_loss 0.227920 lr 0.00012404 rank 0
2023-02-17 15:15:42,610 DEBUG TRAIN Batch 0/3100 loss 138.642960 loss_att 131.481201 loss_ctc 150.690186 loss_rnnt 138.315155 hw_loss 0.288521 lr 0.00012404 rank 6
2023-02-17 15:16:59,468 DEBUG TRAIN Batch 0/3200 loss 104.268944 loss_att 97.756149 loss_ctc 112.685883 loss_rnnt 104.342644 hw_loss 0.199899 lr 0.00012804 rank 3
2023-02-17 15:16:59,469 DEBUG TRAIN Batch 0/3200 loss 143.552597 loss_att 135.606781 loss_ctc 155.545532 loss_rnnt 143.438049 hw_loss 0.196229 lr 0.00012804 rank 7
2023-02-17 15:16:59,472 DEBUG TRAIN Batch 0/3200 loss 256.125641 loss_att 241.830078 loss_ctc 286.845764 loss_rnnt 254.825485 hw_loss 0.118609 lr 0.00012804 rank 2
2023-02-17 15:16:59,473 DEBUG TRAIN Batch 0/3200 loss 62.678616 loss_att 60.782303 loss_ctc 69.206573 loss_rnnt 62.067993 hw_loss 0.224049 lr 0.00012804 rank 4
2023-02-17 15:16:59,473 DEBUG TRAIN Batch 0/3200 loss 267.663177 loss_att 251.432343 loss_ctc 288.502991 loss_rnnt 268.045715 hw_loss 0.159372 lr 0.00012804 rank 6
2023-02-17 15:16:59,475 DEBUG TRAIN Batch 0/3200 loss 231.584534 loss_att 221.870880 loss_ctc 251.585678 loss_rnnt 230.823593 hw_loss 0.069132 lr 0.00012804 rank 1
2023-02-17 15:16:59,477 DEBUG TRAIN Batch 0/3200 loss 109.898979 loss_att 104.936028 loss_ctc 118.227379 loss_rnnt 109.622711 hw_loss 0.297013 lr 0.00012804 rank 5
2023-02-17 15:16:59,480 DEBUG TRAIN Batch 0/3200 loss 265.010651 loss_att 250.641754 loss_ctc 285.230194 loss_rnnt 265.114960 hw_loss 0.137898 lr 0.00012804 rank 0
2023-02-17 15:18:13,371 DEBUG TRAIN Batch 0/3300 loss 297.494171 loss_att 283.037964 loss_ctc 322.483459 loss_rnnt 297.030884 hw_loss 0.042422 lr 0.00013204 rank 3
2023-02-17 15:18:13,372 DEBUG TRAIN Batch 0/3300 loss 313.886505 loss_att 294.601135 loss_ctc 335.558563 loss_rnnt 314.780273 hw_loss 0.138174 lr 0.00013204 rank 7
2023-02-17 15:18:13,381 DEBUG TRAIN Batch 0/3300 loss 315.534546 loss_att 295.981781 loss_ctc 343.238770 loss_rnnt 315.659363 hw_loss 0.172180 lr 0.00013204 rank 5
2023-02-17 15:18:13,382 DEBUG TRAIN Batch 0/3300 loss 260.795715 loss_att 248.301331 loss_ctc 275.599579 loss_rnnt 261.299011 hw_loss 0.040773 lr 0.00013204 rank 2
2023-02-17 15:18:13,383 DEBUG TRAIN Batch 0/3300 loss 299.378326 loss_att 288.381470 loss_ctc 320.221527 loss_rnnt 298.752441 hw_loss 0.086510 lr 0.00013204 rank 6
2023-02-17 15:18:13,385 DEBUG TRAIN Batch 0/3300 loss 256.034241 loss_att 251.460846 loss_ctc 273.949341 loss_rnnt 254.477097 hw_loss 0.155916 lr 0.00013204 rank 0
2023-02-17 15:18:13,386 DEBUG TRAIN Batch 0/3300 loss 250.305801 loss_att 240.834686 loss_ctc 261.755371 loss_rnnt 250.596527 hw_loss 0.144169 lr 0.00013204 rank 1
2023-02-17 15:18:13,428 DEBUG TRAIN Batch 0/3300 loss 280.288483 loss_att 265.980377 loss_ctc 309.869141 loss_rnnt 279.200562 hw_loss 0.010202 lr 0.00013204 rank 4
2023-02-17 15:19:27,455 DEBUG TRAIN Batch 0/3400 loss 248.241211 loss_att 251.879517 loss_ctc 262.342041 loss_rnnt 245.543823 hw_loss 0.168004 lr 0.00013604 rank 3
2023-02-17 15:19:27,459 DEBUG TRAIN Batch 0/3400 loss 279.462250 loss_att 278.444061 loss_ctc 299.716125 loss_rnnt 276.918060 hw_loss 0.088750 lr 0.00013604 rank 7
2023-02-17 15:19:27,460 DEBUG TRAIN Batch 0/3400 loss 236.883408 loss_att 234.378998 loss_ctc 264.418152 loss_rnnt 233.576141 hw_loss 0.256626 lr 0.00013604 rank 2
2023-02-17 15:19:27,464 DEBUG TRAIN Batch 0/3400 loss 276.515381 loss_att 270.330597 loss_ctc 290.367737 loss_rnnt 275.833374 hw_loss 0.134990 lr 0.00013604 rank 6
2023-02-17 15:19:27,466 DEBUG TRAIN Batch 0/3400 loss 284.393616 loss_att 280.157440 loss_ctc 300.609985 loss_rnnt 283.010010 hw_loss 0.128733 lr 0.00013604 rank 1
2023-02-17 15:19:27,467 DEBUG TRAIN Batch 0/3400 loss 247.914749 loss_att 242.433167 loss_ctc 256.451172 loss_rnnt 247.824997 hw_loss 0.089793 lr 0.00013604 rank 4
2023-02-17 15:19:27,484 DEBUG TRAIN Batch 0/3400 loss 252.647171 loss_att 247.018311 loss_ctc 264.064331 loss_rnnt 252.197311 hw_loss 0.100023 lr 0.00013604 rank 0
2023-02-17 15:19:27,507 DEBUG TRAIN Batch 0/3400 loss 366.514618 loss_att 352.380157 loss_ctc 401.115173 loss_rnnt 364.684692 hw_loss 0.081393 lr 0.00013604 rank 5
2023-02-17 15:20:42,474 DEBUG TRAIN Batch 0/3500 loss 245.351379 loss_att 246.117828 loss_ctc 255.807678 loss_rnnt 243.724380 hw_loss 0.149105 lr 0.00014004 rank 1
2023-02-17 15:20:42,492 DEBUG TRAIN Batch 0/3500 loss 237.167206 loss_att 236.332916 loss_ctc 243.700485 loss_rnnt 236.430588 hw_loss 0.060696 lr 0.00014004 rank 7
2023-02-17 15:20:42,492 DEBUG TRAIN Batch 0/3500 loss 216.198242 loss_att 216.692276 loss_ctc 231.900757 loss_rnnt 213.977325 hw_loss 0.053309 lr 0.00014004 rank 2
2023-02-17 15:20:42,494 DEBUG TRAIN Batch 0/3500 loss 269.601166 loss_att 269.720642 loss_ctc 283.134674 loss_rnnt 267.757202 hw_loss 0.029246 lr 0.00014004 rank 3
2023-02-17 15:20:42,497 DEBUG TRAIN Batch 0/3500 loss 222.411545 loss_att 222.761353 loss_ctc 229.802368 loss_rnnt 221.285660 hw_loss 0.132140 lr 0.00014004 rank 0
2023-02-17 15:20:42,502 DEBUG TRAIN Batch 0/3500 loss 222.810394 loss_att 224.640106 loss_ctc 227.877884 loss_rnnt 221.720367 hw_loss 0.090801 lr 0.00014004 rank 6
2023-02-17 15:20:42,509 DEBUG TRAIN Batch 0/3500 loss 264.375671 loss_att 269.485962 loss_ctc 273.095154 loss_rnnt 261.949463 hw_loss 0.452901 lr 0.00014004 rank 5
2023-02-17 15:20:42,528 DEBUG TRAIN Batch 0/3500 loss 258.203491 loss_att 251.138672 loss_ctc 272.583618 loss_rnnt 257.658997 hw_loss 0.075243 lr 0.00014004 rank 4
2023-02-17 15:21:58,307 DEBUG TRAIN Batch 0/3600 loss 283.445404 loss_att 283.483398 loss_ctc 296.923950 loss_rnnt 281.603882 hw_loss 0.068960 lr 0.00014404 rank 7
2023-02-17 15:21:58,311 DEBUG TRAIN Batch 0/3600 loss 231.939224 loss_att 239.716095 loss_ctc 235.825806 loss_rnnt 229.826874 hw_loss 0.072699 lr 0.00014404 rank 3
2023-02-17 15:21:58,313 DEBUG TRAIN Batch 0/3600 loss 191.415466 loss_att 194.270508 loss_ctc 197.781799 loss_rnnt 189.806915 hw_loss 0.353764 lr 0.00014404 rank 4
2023-02-17 15:21:58,314 DEBUG TRAIN Batch 0/3600 loss 189.391342 loss_att 199.300781 loss_ctc 193.734924 loss_rnnt 186.828003 hw_loss 0.004306 lr 0.00014404 rank 2
2023-02-17 15:21:58,316 DEBUG TRAIN Batch 0/3600 loss 180.061661 loss_att 189.082031 loss_ctc 185.629623 loss_rnnt 177.415344 hw_loss 0.187208 lr 0.00014404 rank 1
2023-02-17 15:21:58,318 DEBUG TRAIN Batch 0/3600 loss 244.450928 loss_att 254.745789 loss_ctc 258.418610 loss_rnnt 240.527313 hw_loss 0.004307 lr 0.00014404 rank 6
2023-02-17 15:21:58,319 DEBUG TRAIN Batch 0/3600 loss 261.296478 loss_att 266.366791 loss_ctc 265.733551 loss_rnnt 259.590668 hw_loss 0.187753 lr 0.00014404 rank 5
2023-02-17 15:21:58,320 DEBUG TRAIN Batch 0/3600 loss 245.640839 loss_att 243.198166 loss_ctc 253.622070 loss_rnnt 245.034836 hw_loss 0.056946 lr 0.00014404 rank 0
2023-02-17 15:23:13,199 DEBUG TRAIN Batch 0/3700 loss 65.739723 loss_att 67.964470 loss_ctc 69.866508 loss_rnnt 64.568901 hw_loss 0.329322 lr 0.00014804 rank 1
2023-02-17 15:23:13,203 DEBUG TRAIN Batch 0/3700 loss 191.873383 loss_att 204.103348 loss_ctc 198.895401 loss_rnnt 188.378448 hw_loss 0.211233 lr 0.00014804 rank 3
2023-02-17 15:23:13,203 DEBUG TRAIN Batch 0/3700 loss 146.316086 loss_att 151.146896 loss_ctc 153.101013 loss_rnnt 144.371048 hw_loss 0.139124 lr 0.00014804 rank 6
2023-02-17 15:23:13,204 DEBUG TRAIN Batch 0/3700 loss 230.606308 loss_att 239.618591 loss_ctc 243.671143 loss_rnnt 227.008209 hw_loss 0.100587 lr 0.00014804 rank 5
2023-02-17 15:23:13,204 DEBUG TRAIN Batch 0/3700 loss 201.350204 loss_att 210.815262 loss_ctc 198.012878 loss_rnnt 199.815979 hw_loss 0.161640 lr 0.00014804 rank 7
2023-02-17 15:23:13,205 DEBUG TRAIN Batch 0/3700 loss 140.279846 loss_att 149.697098 loss_ctc 145.256729 loss_rnnt 137.625488 hw_loss 0.201239 lr 0.00014804 rank 2
2023-02-17 15:23:13,212 DEBUG TRAIN Batch 0/3700 loss 203.619003 loss_att 203.666168 loss_ctc 218.535080 loss_rnnt 201.556305 hw_loss 0.120827 lr 0.00014804 rank 0
2023-02-17 15:23:13,252 DEBUG TRAIN Batch 0/3700 loss 215.036804 loss_att 229.379791 loss_ctc 217.023956 loss_rnnt 211.829773 hw_loss 0.137793 lr 0.00014804 rank 4
2023-02-17 15:24:28,305 DEBUG TRAIN Batch 0/3800 loss 142.321991 loss_att 152.487335 loss_ctc 141.738037 loss_rnnt 140.229477 hw_loss 0.257436 lr 0.00015204 rank 7
2023-02-17 15:24:28,309 DEBUG TRAIN Batch 0/3800 loss 223.167892 loss_att 237.782440 loss_ctc 218.324615 loss_rnnt 220.808395 hw_loss 0.154459 lr 0.00015204 rank 1
2023-02-17 15:24:28,314 DEBUG TRAIN Batch 0/3800 loss 151.417496 loss_att 166.425842 loss_ctc 154.283356 loss_rnnt 147.949463 hw_loss 0.157984 lr 0.00015204 rank 3
2023-02-17 15:24:28,315 DEBUG TRAIN Batch 0/3800 loss 254.594818 loss_att 269.247009 loss_ctc 258.980072 loss_rnnt 251.078491 hw_loss 0.002221 lr 0.00015204 rank 0
2023-02-17 15:24:28,316 DEBUG TRAIN Batch 0/3800 loss 276.217560 loss_att 301.737427 loss_ctc 288.068787 loss_rnnt 269.503418 hw_loss 0.056241 lr 0.00015204 rank 2
2023-02-17 15:24:28,316 DEBUG TRAIN Batch 0/3800 loss 246.155273 loss_att 265.312073 loss_ctc 242.820435 loss_rnnt 242.660645 hw_loss 0.202345 lr 0.00015204 rank 6
2023-02-17 15:24:28,317 DEBUG TRAIN Batch 0/3800 loss 147.585114 loss_att 162.130127 loss_ctc 144.192474 loss_rnnt 145.011353 hw_loss 0.219565 lr 0.00015204 rank 4
2023-02-17 15:24:28,350 DEBUG TRAIN Batch 0/3800 loss 346.834045 loss_att 367.764130 loss_ctc 355.843567 loss_rnnt 341.445587 hw_loss 0.002222 lr 0.00015204 rank 5
2023-02-17 15:25:45,744 DEBUG TRAIN Batch 0/3900 loss 200.548523 loss_att 236.634918 loss_ctc 196.910980 loss_rnnt 193.814621 hw_loss 0.003047 lr 0.00015604 rank 7
2023-02-17 15:25:45,748 DEBUG TRAIN Batch 0/3900 loss 252.246933 loss_att 283.166534 loss_ctc 254.231308 loss_rnnt 245.763062 hw_loss 0.066349 lr 0.00015604 rank 6
2023-02-17 15:25:45,750 DEBUG TRAIN Batch 0/3900 loss 228.790710 loss_att 258.504242 loss_ctc 224.373093 loss_rnnt 223.347641 hw_loss 0.167606 lr 0.00015604 rank 1
2023-02-17 15:25:45,752 DEBUG TRAIN Batch 0/3900 loss 175.643158 loss_att 206.394623 loss_ctc 182.605896 loss_rnnt 168.513611 hw_loss 0.095404 lr 0.00015604 rank 2
2023-02-17 15:25:45,752 DEBUG TRAIN Batch 0/3900 loss 251.629318 loss_att 285.723358 loss_ctc 251.484543 loss_rnnt 244.798279 hw_loss 0.059109 lr 0.00015604 rank 4
2023-02-17 15:25:45,770 DEBUG TRAIN Batch 0/3900 loss 258.126587 loss_att 284.509155 loss_ctc 256.834473 loss_rnnt 252.912323 hw_loss 0.206323 lr 0.00015604 rank 3
2023-02-17 15:25:45,780 DEBUG TRAIN Batch 0/3900 loss 256.348663 loss_att 271.982788 loss_ctc 262.722961 loss_rnnt 252.294708 hw_loss 0.144810 lr 0.00015604 rank 0
2023-02-17 15:25:45,799 DEBUG TRAIN Batch 0/3900 loss 259.611755 loss_att 278.292572 loss_ctc 264.645386 loss_rnnt 255.163025 hw_loss 0.077635 lr 0.00015604 rank 5
2023-02-17 15:27:00,242 DEBUG TRAIN Batch 0/4000 loss 200.932770 loss_att 230.505310 loss_ctc 203.834076 loss_rnnt 194.507233 hw_loss 0.232865 lr 0.00016004 rank 1
2023-02-17 15:27:00,247 DEBUG TRAIN Batch 0/4000 loss 261.194000 loss_att 292.222015 loss_ctc 248.971832 loss_rnnt 256.578369 hw_loss 0.074302 lr 0.00016004 rank 7
2023-02-17 15:27:00,247 DEBUG TRAIN Batch 0/4000 loss 203.208145 loss_att 219.552765 loss_ctc 207.740845 loss_rnnt 199.283127 hw_loss 0.097022 lr 0.00016004 rank 3
2023-02-17 15:27:00,251 DEBUG TRAIN Batch 0/4000 loss 207.036697 loss_att 244.891785 loss_ctc 207.580292 loss_rnnt 199.357330 hw_loss 0.067238 lr 0.00016004 rank 5
2023-02-17 15:27:00,253 DEBUG TRAIN Batch 0/4000 loss 300.820282 loss_att 317.903381 loss_ctc 295.999878 loss_rnnt 297.938812 hw_loss 0.201689 lr 0.00016004 rank 0
2023-02-17 15:27:00,253 DEBUG TRAIN Batch 0/4000 loss 255.234177 loss_att 295.667969 loss_ctc 249.689072 loss_rnnt 247.832870 hw_loss 0.101058 lr 0.00016004 rank 4
2023-02-17 15:27:00,256 DEBUG TRAIN Batch 0/4000 loss 232.109329 loss_att 271.814667 loss_ctc 230.586227 loss_rnnt 224.340881 hw_loss 0.057117 lr 0.00016004 rank 6
2023-02-17 15:27:00,298 DEBUG TRAIN Batch 0/4000 loss 199.749847 loss_att 233.916824 loss_ctc 192.446167 loss_rnnt 193.854919 hw_loss 0.066294 lr 0.00016004 rank 2
2023-02-17 15:28:13,540 DEBUG TRAIN Batch 0/4100 loss 179.777679 loss_att 213.089111 loss_ctc 175.798828 loss_rnnt 173.593140 hw_loss 0.098943 lr 0.00016404 rank 0
2023-02-17 15:28:13,552 DEBUG TRAIN Batch 0/4100 loss 203.228668 loss_att 239.001755 loss_ctc 200.886169 loss_rnnt 196.314941 hw_loss 0.133939 lr 0.00016404 rank 1
2023-02-17 15:28:13,555 DEBUG TRAIN Batch 0/4100 loss 192.653122 loss_att 239.707886 loss_ctc 184.521225 loss_rnnt 184.117966 hw_loss 0.390894 lr 0.00016404 rank 7
2023-02-17 15:28:13,555 DEBUG TRAIN Batch 0/4100 loss 212.470123 loss_att 236.368958 loss_ctc 206.755432 loss_rnnt 208.368958 hw_loss 0.156341 lr 0.00016404 rank 2
2023-02-17 15:28:13,556 DEBUG TRAIN Batch 0/4100 loss 234.224670 loss_att 264.758911 loss_ctc 229.819824 loss_rnnt 228.637054 hw_loss 0.127670 lr 0.00016404 rank 4
2023-02-17 15:28:13,560 DEBUG TRAIN Batch 0/4100 loss 191.525558 loss_att 226.447296 loss_ctc 185.866898 loss_rnnt 185.255951 hw_loss 0.074582 lr 0.00016404 rank 6
2023-02-17 15:28:13,561 DEBUG TRAIN Batch 0/4100 loss 194.872070 loss_att 231.869293 loss_ctc 195.859543 loss_rnnt 187.303085 hw_loss 0.071035 lr 0.00016404 rank 5
2023-02-17 15:28:13,603 DEBUG TRAIN Batch 0/4100 loss 209.574341 loss_att 255.328339 loss_ctc 206.157303 loss_rnnt 200.805969 hw_loss 0.137160 lr 0.00016404 rank 3
2023-02-17 15:29:27,947 DEBUG TRAIN Batch 0/4200 loss 238.045593 loss_att 289.656433 loss_ctc 248.345520 loss_rnnt 226.299240 hw_loss 0.095354 lr 0.00016804 rank 4
2023-02-17 15:29:27,958 DEBUG TRAIN Batch 0/4200 loss 176.965515 loss_att 221.937439 loss_ctc 167.777115 loss_rnnt 169.142822 hw_loss 0.100163 lr 0.00016804 rank 1
2023-02-17 15:29:27,959 DEBUG TRAIN Batch 0/4200 loss 211.909241 loss_att 250.788849 loss_ctc 219.529175 loss_rnnt 203.053589 hw_loss 0.119508 lr 0.00016804 rank 7
2023-02-17 15:29:27,959 DEBUG TRAIN Batch 0/4200 loss 192.732269 loss_att 249.306458 loss_ctc 191.856567 loss_rnnt 181.471848 hw_loss 0.116902 lr 0.00016804 rank 3
2023-02-17 15:29:27,963 DEBUG TRAIN Batch 0/4200 loss 219.559189 loss_att 250.553162 loss_ctc 221.171448 loss_rnnt 213.020096 hw_loss 0.234996 lr 0.00016804 rank 5
2023-02-17 15:29:27,965 DEBUG TRAIN Batch 0/4200 loss 186.218613 loss_att 211.514618 loss_ctc 184.268951 loss_rnnt 181.342072 hw_loss 0.144913 lr 0.00016804 rank 2
2023-02-17 15:29:27,971 DEBUG TRAIN Batch 0/4200 loss 233.766586 loss_att 266.254211 loss_ctc 243.182007 loss_rnnt 225.946320 hw_loss 0.126234 lr 0.00016804 rank 6
2023-02-17 15:29:27,976 DEBUG TRAIN Batch 0/4200 loss 215.491165 loss_att 262.595551 loss_ctc 209.871826 loss_rnnt 206.773102 hw_loss 0.086998 lr 0.00016804 rank 0
2023-02-17 15:30:44,144 DEBUG TRAIN Batch 0/4300 loss 195.456421 loss_att 238.962906 loss_ctc 201.157562 loss_rnnt 185.868484 hw_loss 0.237161 lr 0.00017204 rank 7
2023-02-17 15:30:44,148 DEBUG TRAIN Batch 0/4300 loss 111.182518 loss_att 136.457153 loss_ctc 116.007423 loss_rnnt 105.425423 hw_loss 0.110323 lr 0.00017204 rank 1
2023-02-17 15:30:44,148 DEBUG TRAIN Batch 0/4300 loss 162.564758 loss_att 213.561340 loss_ctc 163.337982 loss_rnnt 152.162949 hw_loss 0.186391 lr 0.00017204 rank 5
2023-02-17 15:30:44,152 DEBUG TRAIN Batch 0/4300 loss 169.616043 loss_att 195.851074 loss_ctc 169.298340 loss_rnnt 164.316254 hw_loss 0.178395 lr 0.00017204 rank 2
2023-02-17 15:30:44,153 DEBUG TRAIN Batch 0/4300 loss 154.562698 loss_att 179.022064 loss_ctc 157.108185 loss_rnnt 149.205551 hw_loss 0.235998 lr 0.00017204 rank 6
2023-02-17 15:30:44,181 DEBUG TRAIN Batch 0/4300 loss 206.428375 loss_att 258.154114 loss_ctc 204.327515 loss_rnnt 196.362457 hw_loss 0.001642 lr 0.00017204 rank 3
2023-02-17 15:30:44,190 DEBUG TRAIN Batch 0/4300 loss 143.818588 loss_att 186.490356 loss_ctc 146.414551 loss_rnnt 134.937225 hw_loss 0.001642 lr 0.00017204 rank 4
2023-02-17 15:30:44,193 DEBUG TRAIN Batch 0/4300 loss 143.652100 loss_att 177.568909 loss_ctc 146.662384 loss_rnnt 136.389328 hw_loss 0.146351 lr 0.00017204 rank 0
2023-02-17 15:31:58,424 DEBUG TRAIN Batch 0/4400 loss 183.985687 loss_att 230.889694 loss_ctc 188.904282 loss_rnnt 173.866135 hw_loss 0.155534 lr 0.00017604 rank 1
2023-02-17 15:31:58,426 DEBUG TRAIN Batch 0/4400 loss 165.891113 loss_att 194.861160 loss_ctc 173.389999 loss_rnnt 159.063873 hw_loss 0.062608 lr 0.00017604 rank 7
2023-02-17 15:31:58,432 DEBUG TRAIN Batch 0/4400 loss 68.561440 loss_att 85.452873 loss_ctc 68.534561 loss_rnnt 65.032616 hw_loss 0.288977 lr 0.00017604 rank 0
2023-02-17 15:31:58,433 DEBUG TRAIN Batch 0/4400 loss 43.559586 loss_att 52.729092 loss_ctc 45.058392 loss_rnnt 41.303513 hw_loss 0.416869 lr 0.00017604 rank 6
2023-02-17 15:31:58,436 DEBUG TRAIN Batch 0/4400 loss 174.383316 loss_att 208.570190 loss_ctc 173.839127 loss_rnnt 167.563873 hw_loss 0.102443 lr 0.00017604 rank 3
2023-02-17 15:31:58,452 DEBUG TRAIN Batch 0/4400 loss 137.924469 loss_att 193.327148 loss_ctc 134.417953 loss_rnnt 127.110939 hw_loss 0.375967 lr 0.00017604 rank 4
2023-02-17 15:31:58,456 DEBUG TRAIN Batch 0/4400 loss 38.869274 loss_att 47.430813 loss_ctc 40.210125 loss_rnnt 36.862499 hw_loss 0.216913 lr 0.00017604 rank 5
2023-02-17 15:31:58,473 DEBUG TRAIN Batch 0/4400 loss 196.620743 loss_att 272.350433 loss_ctc 190.357239 loss_rnnt 182.273743 hw_loss 0.067869 lr 0.00017604 rank 2
2023-02-17 15:33:12,518 DEBUG TRAIN Batch 0/4500 loss 86.058792 loss_att 106.943008 loss_ctc 90.411949 loss_rnnt 81.268188 hw_loss 0.062512 lr 0.00018004 rank 3
2023-02-17 15:33:12,519 DEBUG TRAIN Batch 0/4500 loss 224.278748 loss_att 279.556152 loss_ctc 229.346741 loss_rnnt 212.446945 hw_loss 0.188607 lr 0.00018004 rank 7
2023-02-17 15:33:12,520 DEBUG TRAIN Batch 0/4500 loss 207.599152 loss_att 246.763367 loss_ctc 212.281662 loss_rnnt 199.141571 hw_loss 0.000768 lr 0.00018004 rank 1
2023-02-17 15:33:12,522 DEBUG TRAIN Batch 0/4500 loss 172.270996 loss_att 226.721588 loss_ctc 166.815445 loss_rnnt 162.107880 hw_loss 0.000768 lr 0.00018004 rank 2
2023-02-17 15:33:12,524 DEBUG TRAIN Batch 0/4500 loss 150.304657 loss_att 227.076782 loss_ctc 149.952164 loss_rnnt 134.996826 hw_loss 0.000769 lr 0.00018004 rank 5
2023-02-17 15:33:12,523 DEBUG TRAIN Batch 0/4500 loss 78.753242 loss_att 103.680710 loss_ctc 82.201157 loss_rnnt 73.201271 hw_loss 0.200165 lr 0.00018004 rank 4
2023-02-17 15:33:12,529 DEBUG TRAIN Batch 0/4500 loss 170.679474 loss_att 257.712036 loss_ctc 161.975952 loss_rnnt 154.391281 hw_loss 0.079023 lr 0.00018004 rank 0
2023-02-17 15:33:12,573 DEBUG TRAIN Batch 0/4500 loss 195.426025 loss_att 254.926086 loss_ctc 198.409958 loss_rnnt 183.079773 hw_loss 0.090699 lr 0.00018004 rank 6
2023-02-17 15:34:29,583 DEBUG TRAIN Batch 0/4600 loss 155.145416 loss_att 214.011139 loss_ctc 162.169067 loss_rnnt 142.350082 hw_loss 0.160714 lr 0.00018404 rank 7
2023-02-17 15:34:29,584 DEBUG TRAIN Batch 0/4600 loss 204.890793 loss_att 254.946640 loss_ctc 207.492798 loss_rnnt 194.443665 hw_loss 0.166879 lr 0.00018404 rank 2
2023-02-17 15:34:29,588 DEBUG TRAIN Batch 0/4600 loss 174.928528 loss_att 235.442200 loss_ctc 166.928497 loss_rnnt 163.809113 hw_loss 0.156278 lr 0.00018404 rank 1
2023-02-17 15:34:29,588 DEBUG TRAIN Batch 0/4600 loss 171.071823 loss_att 245.943604 loss_ctc 157.496094 loss_rnnt 157.883774 hw_loss 0.044622 lr 0.00018404 rank 0
2023-02-17 15:34:29,598 DEBUG TRAIN Batch 0/4600 loss 221.043015 loss_att 283.622742 loss_ctc 221.246246 loss_rnnt 208.457382 hw_loss 0.079893 lr 0.00018404 rank 6
2023-02-17 15:34:29,601 DEBUG TRAIN Batch 0/4600 loss 180.126297 loss_att 243.149246 loss_ctc 179.163254 loss_rnnt 167.627258 hw_loss 0.042828 lr 0.00018404 rank 3
2023-02-17 15:34:29,626 DEBUG TRAIN Batch 0/4600 loss 191.396255 loss_att 278.541290 loss_ctc 183.117035 loss_rnnt 174.998886 hw_loss 0.135478 lr 0.00018404 rank 4
2023-02-17 15:34:29,642 DEBUG TRAIN Batch 0/4600 loss 221.121872 loss_att 283.780273 loss_ctc 233.142883 loss_rnnt 206.966858 hw_loss 0.038482 lr 0.00018404 rank 5
2023-02-17 15:35:44,902 DEBUG TRAIN Batch 0/4700 loss 182.044617 loss_att 255.595657 loss_ctc 186.130829 loss_rnnt 166.689758 hw_loss 0.187151 lr 0.00018804 rank 7
2023-02-17 15:35:44,902 DEBUG TRAIN Batch 0/4700 loss 190.338455 loss_att 269.268188 loss_ctc 192.110184 loss_rnnt 174.270233 hw_loss 0.086360 lr 0.00018804 rank 2
2023-02-17 15:35:44,906 DEBUG TRAIN Batch 0/4700 loss 166.961655 loss_att 223.523376 loss_ctc 156.688690 loss_rnnt 156.928284 hw_loss 0.170156 lr 0.00018804 rank 4
2023-02-17 15:35:44,906 DEBUG TRAIN Batch 0/4700 loss 185.918579 loss_att 240.987732 loss_ctc 193.172150 loss_rnnt 173.881195 hw_loss 0.105775 lr 0.00018804 rank 1
2023-02-17 15:35:44,908 DEBUG TRAIN Batch 0/4700 loss 148.509567 loss_att 222.341919 loss_ctc 142.652496 loss_rnnt 134.428116 hw_loss 0.179849 lr 0.00018804 rank 0
2023-02-17 15:35:44,907 DEBUG TRAIN Batch 0/4700 loss 161.874039 loss_att 220.067078 loss_ctc 166.151566 loss_rnnt 149.594330 hw_loss 0.132695 lr 0.00018804 rank 6
2023-02-17 15:35:44,911 DEBUG TRAIN Batch 0/4700 loss 236.928009 loss_att 318.816772 loss_ctc 220.494553 loss_rnnt 222.683517 hw_loss 0.108506 lr 0.00018804 rank 5
2023-02-17 15:35:44,915 DEBUG TRAIN Batch 0/4700 loss 157.551224 loss_att 229.991852 loss_ctc 147.488251 loss_rnnt 144.345108 hw_loss 0.111950 lr 0.00018804 rank 3
2023-02-17 15:36:58,749 DEBUG TRAIN Batch 0/4800 loss 162.461441 loss_att 228.029785 loss_ctc 157.265915 loss_rnnt 149.936691 hw_loss 0.194656 lr 0.00019204 rank 7
2023-02-17 15:36:58,749 DEBUG TRAIN Batch 0/4800 loss 167.289200 loss_att 218.534973 loss_ctc 172.506500 loss_rnnt 156.273254 hw_loss 0.133385 lr 0.00019204 rank 1
2023-02-17 15:36:58,749 DEBUG TRAIN Batch 0/4800 loss 182.432190 loss_att 253.278275 loss_ctc 183.319504 loss_rnnt 168.051697 hw_loss 0.174294 lr 0.00019204 rank 5
2023-02-17 15:36:58,751 DEBUG TRAIN Batch 0/4800 loss 159.993851 loss_att 231.485931 loss_ctc 155.105774 loss_rnnt 146.329575 hw_loss 0.032990 lr 0.00019204 rank 6
2023-02-17 15:36:58,758 DEBUG TRAIN Batch 0/4800 loss 178.760101 loss_att 267.879456 loss_ctc 176.789246 loss_rnnt 161.198776 hw_loss 0.000445 lr 0.00019204 rank 4
2023-02-17 15:36:58,758 DEBUG TRAIN Batch 0/4800 loss 108.054787 loss_att 183.602966 loss_ctc 100.841721 loss_rnnt 93.819366 hw_loss 0.164103 lr 0.00019204 rank 0
2023-02-17 15:36:58,760 DEBUG TRAIN Batch 0/4800 loss 175.831787 loss_att 245.433746 loss_ctc 168.641296 loss_rnnt 162.804123 hw_loss 0.123747 lr 0.00019204 rank 2
2023-02-17 15:36:58,800 DEBUG TRAIN Batch 0/4800 loss 160.839310 loss_att 234.182831 loss_ctc 152.636810 loss_rnnt 147.219086 hw_loss 0.084725 lr 0.00019204 rank 3
2023-02-17 15:38:12,798 DEBUG TRAIN Batch 0/4900 loss 105.738922 loss_att 141.929001 loss_ctc 107.021996 loss_rnnt 98.203735 hw_loss 0.236414 lr 0.00019604 rank 1
2023-02-17 15:38:12,800 DEBUG TRAIN Batch 0/4900 loss 155.062073 loss_att 222.652679 loss_ctc 153.800323 loss_rnnt 141.666504 hw_loss 0.085653 lr 0.00019604 rank 4
2023-02-17 15:38:12,803 DEBUG TRAIN Batch 0/4900 loss 152.835358 loss_att 211.579132 loss_ctc 151.880646 loss_rnnt 141.161835 hw_loss 0.097603 lr 0.00019604 rank 6
2023-02-17 15:38:12,805 DEBUG TRAIN Batch 0/4900 loss 148.097763 loss_att 214.120987 loss_ctc 151.423965 loss_rnnt 134.405563 hw_loss 0.082619 lr 0.00019604 rank 5
2023-02-17 15:38:12,808 DEBUG TRAIN Batch 0/4900 loss 154.662552 loss_att 229.281952 loss_ctc 156.238541 loss_rnnt 139.489822 hw_loss 0.072608 lr 0.00019604 rank 3
2023-02-17 15:38:12,809 DEBUG TRAIN Batch 0/4900 loss 138.876984 loss_att 200.968033 loss_ctc 136.282928 loss_rnnt 126.770111 hw_loss 0.064731 lr 0.00019604 rank 0
2023-02-17 15:38:12,812 DEBUG TRAIN Batch 0/4900 loss 166.219223 loss_att 226.464172 loss_ctc 164.818756 loss_rnnt 154.269913 hw_loss 0.163215 lr 0.00019604 rank 7
2023-02-17 15:38:12,816 DEBUG TRAIN Batch 0/4900 loss 183.700287 loss_att 245.069916 loss_ctc 187.213837 loss_rnnt 170.863861 hw_loss 0.176317 lr 0.00019604 rank 2
2023-02-17 15:39:29,863 DEBUG TRAIN Batch 0/5000 loss 67.851685 loss_att 99.997902 loss_ctc 68.288635 loss_rnnt 61.234776 hw_loss 0.242642 lr 0.00020004 rank 6
2023-02-17 15:39:29,864 DEBUG TRAIN Batch 0/5000 loss 170.615036 loss_att 250.718842 loss_ctc 161.540817 loss_rnnt 155.722183 hw_loss 0.153684 lr 0.00020004 rank 1
2023-02-17 15:39:29,867 DEBUG TRAIN Batch 0/5000 loss 122.647461 loss_att 174.802216 loss_ctc 118.887650 loss_rnnt 112.680374 hw_loss 0.070194 lr 0.00020004 rank 7
2023-02-17 15:39:29,870 DEBUG TRAIN Batch 0/5000 loss 142.888885 loss_att 208.156311 loss_ctc 143.144928 loss_rnnt 129.691605 hw_loss 0.205594 lr 0.00020004 rank 0
2023-02-17 15:39:29,871 DEBUG TRAIN Batch 0/5000 loss 138.042648 loss_att 207.639038 loss_ctc 135.309784 loss_rnnt 124.410690 hw_loss 0.144500 lr 0.00020004 rank 4
2023-02-17 15:39:29,872 DEBUG TRAIN Batch 0/5000 loss 155.570084 loss_att 226.959198 loss_ctc 154.144958 loss_rnnt 141.454559 hw_loss 0.051971 lr 0.00020004 rank 3
2023-02-17 15:39:29,875 DEBUG TRAIN Batch 0/5000 loss 81.244621 loss_att 104.979736 loss_ctc 85.789719 loss_rnnt 75.792931 hw_loss 0.184975 lr 0.00020004 rank 2
2023-02-17 15:39:29,917 DEBUG TRAIN Batch 0/5000 loss 81.079590 loss_att 105.709923 loss_ctc 78.034821 loss_rnnt 76.399521 hw_loss 0.299947 lr 0.00020004 rank 5
2023-02-17 15:40:43,372 DEBUG TRAIN Batch 0/5100 loss 156.051056 loss_att 237.975082 loss_ctc 147.794525 loss_rnnt 140.680450 hw_loss 0.162499 lr 0.00020404 rank 6
2023-02-17 15:40:43,383 DEBUG TRAIN Batch 0/5100 loss 147.845276 loss_att 188.323761 loss_ctc 151.441223 loss_rnnt 139.189682 hw_loss 0.150816 lr 0.00020404 rank 3
2023-02-17 15:40:43,389 DEBUG TRAIN Batch 0/5100 loss 180.158539 loss_att 269.046631 loss_ctc 174.335297 loss_rnnt 163.128647 hw_loss 0.053816 lr 0.00020404 rank 5
2023-02-17 15:40:43,391 DEBUG TRAIN Batch 0/5100 loss 162.536057 loss_att 255.613342 loss_ctc 158.551926 loss_rnnt 144.376938 hw_loss 0.140413 lr 0.00020404 rank 2
2023-02-17 15:40:43,391 DEBUG TRAIN Batch 0/5100 loss 164.512573 loss_att 258.492737 loss_ctc 166.448761 loss_rnnt 145.416656 hw_loss 0.078247 lr 0.00020404 rank 7
2023-02-17 15:40:43,392 DEBUG TRAIN Batch 0/5100 loss 82.894241 loss_att 122.872955 loss_ctc 81.083855 loss_rnnt 74.976685 hw_loss 0.305998 lr 0.00020404 rank 4
2023-02-17 15:40:43,396 DEBUG TRAIN Batch 0/5100 loss 201.850861 loss_att 295.402588 loss_ctc 199.711761 loss_rnnt 183.363754 hw_loss 0.116206 lr 0.00020404 rank 0
2023-02-17 15:40:43,432 DEBUG TRAIN Batch 0/5100 loss 183.875229 loss_att 271.355042 loss_ctc 186.884949 loss_rnnt 165.977661 hw_loss 0.000565 lr 0.00020404 rank 1
2023-02-17 15:41:56,655 DEBUG TRAIN Batch 0/5200 loss 183.210663 loss_att 271.716736 loss_ctc 183.811737 loss_rnnt 165.360580 hw_loss 0.128850 lr 0.00020804 rank 7
2023-02-17 15:41:56,657 DEBUG TRAIN Batch 0/5200 loss 115.480446 loss_att 192.865036 loss_ctc 103.508011 loss_rnnt 101.577171 hw_loss 0.042540 lr 0.00020804 rank 3
2023-02-17 15:41:56,660 DEBUG TRAIN Batch 0/5200 loss 155.832367 loss_att 254.106964 loss_ctc 145.062775 loss_rnnt 137.581940 hw_loss 0.058984 lr 0.00020804 rank 6
2023-02-17 15:41:56,661 DEBUG TRAIN Batch 0/5200 loss 152.557159 loss_att 233.562225 loss_ctc 152.751389 loss_rnnt 136.329514 hw_loss 0.001389 lr 0.00020804 rank 4
2023-02-17 15:41:56,666 DEBUG TRAIN Batch 0/5200 loss 166.283371 loss_att 257.174530 loss_ctc 156.971619 loss_rnnt 149.281006 hw_loss 0.123171 lr 0.00020804 rank 2
2023-02-17 15:41:56,668 DEBUG TRAIN Batch 0/5200 loss 224.195923 loss_att 308.861786 loss_ctc 225.652512 loss_rnnt 207.024872 hw_loss 0.081884 lr 0.00020804 rank 5
2023-02-17 15:41:56,672 DEBUG TRAIN Batch 0/5200 loss 143.348648 loss_att 224.575424 loss_ctc 140.529938 loss_rnnt 127.419846 hw_loss 0.111177 lr 0.00020804 rank 1
2023-02-17 15:41:56,688 DEBUG TRAIN Batch 0/5200 loss 137.872101 loss_att 217.155853 loss_ctc 128.703308 loss_rnnt 123.237122 hw_loss 0.001389 lr 0.00020804 rank 0
2023-02-17 15:43:11,652 DEBUG TRAIN Batch 0/5300 loss 163.416306 loss_att 249.911606 loss_ctc 172.223938 loss_rnnt 144.923080 hw_loss 0.037170 lr 0.00021204 rank 0
2023-02-17 15:43:11,662 DEBUG TRAIN Batch 0/5300 loss 189.655579 loss_att 285.222595 loss_ctc 196.092194 loss_rnnt 169.567429 hw_loss 0.218523 lr 0.00021204 rank 3
2023-02-17 15:43:11,664 DEBUG TRAIN Batch 0/5300 loss 152.589340 loss_att 255.875854 loss_ctc 155.008041 loss_rnnt 131.524994 hw_loss 0.158547 lr 0.00021204 rank 7
2023-02-17 15:43:11,664 DEBUG TRAIN Batch 0/5300 loss 160.745346 loss_att 247.856049 loss_ctc 159.165649 loss_rnnt 143.426697 hw_loss 0.200876 lr 0.00021204 rank 6
2023-02-17 15:43:11,664 DEBUG TRAIN Batch 0/5300 loss 136.461151 loss_att 242.788452 loss_ctc 134.825623 loss_rnnt 115.379395 hw_loss 0.064434 lr 0.00021204 rank 1
2023-02-17 15:43:11,666 DEBUG TRAIN Batch 0/5300 loss 179.810791 loss_att 254.278519 loss_ctc 196.678497 loss_rnnt 162.667114 hw_loss 0.002072 lr 0.00021204 rank 4
2023-02-17 15:43:11,667 DEBUG TRAIN Batch 0/5300 loss 148.750046 loss_att 245.630249 loss_ctc 149.017014 loss_rnnt 129.254745 hw_loss 0.156864 lr 0.00021204 rank 5
2023-02-17 15:43:11,713 DEBUG TRAIN Batch 0/5300 loss 97.969284 loss_att 207.105316 loss_ctc 84.861328 loss_rnnt 77.864983 hw_loss 0.046549 lr 0.00021204 rank 2
2023-02-17 15:44:25,607 DEBUG TRAIN Batch 0/5400 loss 142.123016 loss_att 248.015015 loss_ctc 136.361389 loss_rnnt 121.673286 hw_loss 0.074153 lr 0.00021604 rank 2
2023-02-17 15:44:25,610 DEBUG TRAIN Batch 0/5400 loss 144.526688 loss_att 230.218353 loss_ctc 144.712845 loss_rnnt 127.276428 hw_loss 0.163308 lr 0.00021604 rank 1
2023-02-17 15:44:25,617 DEBUG TRAIN Batch 0/5400 loss 133.774658 loss_att 214.486252 loss_ctc 132.104645 loss_rnnt 117.792732 hw_loss 0.116784 lr 0.00021604 rank 7
2023-02-17 15:44:25,623 DEBUG TRAIN Batch 0/5400 loss 151.728943 loss_att 248.074860 loss_ctc 151.025574 loss_rnnt 132.469193 hw_loss 0.158168 lr 0.00021604 rank 3
2023-02-17 15:44:25,625 DEBUG TRAIN Batch 0/5400 loss 132.210587 loss_att 223.013336 loss_ctc 128.327560 loss_rnnt 114.499451 hw_loss 0.128089 lr 0.00021604 rank 4
2023-02-17 15:44:25,624 DEBUG TRAIN Batch 0/5400 loss 158.392487 loss_att 246.494247 loss_ctc 158.696854 loss_rnnt 140.674286 hw_loss 0.107395 lr 0.00021604 rank 6
2023-02-17 15:44:25,627 DEBUG TRAIN Batch 0/5400 loss 167.963562 loss_att 240.594971 loss_ctc 165.137787 loss_rnnt 153.755005 hw_loss 0.110717 lr 0.00021604 rank 5
2023-02-17 15:44:25,629 DEBUG TRAIN Batch 0/5400 loss 117.600273 loss_att 211.056061 loss_ctc 118.066238 loss_rnnt 98.771301 hw_loss 0.141906 lr 0.00021604 rank 0
2023-02-17 15:45:39,244 DEBUG TRAIN Batch 0/5500 loss 136.519714 loss_att 222.797607 loss_ctc 138.071716 loss_rnnt 118.996033 hw_loss 0.114669 lr 0.00022004 rank 6
2023-02-17 15:45:39,247 DEBUG TRAIN Batch 0/5500 loss 115.763275 loss_att 190.600266 loss_ctc 107.478661 loss_rnnt 101.807396 hw_loss 0.174555 lr 0.00022004 rank 1
2023-02-17 15:45:39,250 DEBUG TRAIN Batch 0/5500 loss 147.568527 loss_att 236.677734 loss_ctc 155.856979 loss_rnnt 128.582916 hw_loss 0.109947 lr 0.00022004 rank 3
2023-02-17 15:45:39,252 DEBUG TRAIN Batch 0/5500 loss 158.075256 loss_att 244.523315 loss_ctc 156.099945 loss_rnnt 141.016891 hw_loss 0.060280 lr 0.00022004 rank 7
2023-02-17 15:45:39,253 DEBUG TRAIN Batch 0/5500 loss 132.142212 loss_att 229.937592 loss_ctc 117.960609 loss_rnnt 114.438828 hw_loss 0.066006 lr 0.00022004 rank 4
2023-02-17 15:45:39,256 DEBUG TRAIN Batch 0/5500 loss 128.693237 loss_att 210.779770 loss_ctc 129.601944 loss_rnnt 112.154625 hw_loss 0.000278 lr 0.00022004 rank 2
2023-02-17 15:45:39,262 DEBUG TRAIN Batch 0/5500 loss 120.655296 loss_att 206.243652 loss_ctc 115.489304 loss_rnnt 104.183510 hw_loss 0.080478 lr 0.00022004 rank 5
2023-02-17 15:45:39,268 DEBUG TRAIN Batch 0/5500 loss 128.028778 loss_att 211.687866 loss_ctc 131.409927 loss_rnnt 110.744690 hw_loss 0.190238 lr 0.00022004 rank 0
2023-02-17 15:46:53,725 DEBUG TRAIN Batch 0/5600 loss 144.998337 loss_att 253.819916 loss_ctc 143.789032 loss_rnnt 123.325912 hw_loss 0.130049 lr 0.00022404 rank 1
2023-02-17 15:46:53,726 DEBUG TRAIN Batch 0/5600 loss 127.753960 loss_att 205.103409 loss_ctc 128.268784 loss_rnnt 112.127823 hw_loss 0.164240 lr 0.00022404 rank 7
2023-02-17 15:46:53,726 DEBUG TRAIN Batch 0/5600 loss 101.116730 loss_att 158.820618 loss_ctc 103.001099 loss_rnnt 89.220337 hw_loss 0.195698 lr 0.00022404 rank 5
2023-02-17 15:46:53,727 DEBUG TRAIN Batch 0/5600 loss 138.556290 loss_att 226.855225 loss_ctc 136.416306 loss_rnnt 121.115662 hw_loss 0.124055 lr 0.00022404 rank 3
2023-02-17 15:46:53,729 DEBUG TRAIN Batch 0/5600 loss 118.847488 loss_att 192.626236 loss_ctc 121.902893 loss_rnnt 103.537041 hw_loss 0.276205 lr 0.00022404 rank 2
2023-02-17 15:46:53,729 DEBUG TRAIN Batch 0/5600 loss 135.920273 loss_att 208.163239 loss_ctc 136.267395 loss_rnnt 121.364151 hw_loss 0.114807 lr 0.00022404 rank 4
2023-02-17 15:46:53,729 DEBUG TRAIN Batch 0/5600 loss 89.596718 loss_att 148.716248 loss_ctc 90.881035 loss_rnnt 77.571404 hw_loss 0.056554 lr 0.00022404 rank 6
2023-02-17 15:46:53,731 DEBUG TRAIN Batch 0/5600 loss 96.944824 loss_att 164.718964 loss_ctc 94.836609 loss_rnnt 83.539055 hw_loss 0.247570 lr 0.00022404 rank 0
2023-02-17 15:48:09,920 DEBUG TRAIN Batch 0/5700 loss 131.473038 loss_att 233.487366 loss_ctc 122.910828 loss_rnnt 112.186874 hw_loss 0.046734 lr 0.00022804 rank 6
2023-02-17 15:48:09,922 DEBUG TRAIN Batch 0/5700 loss 127.724525 loss_att 212.515961 loss_ctc 128.423950 loss_rnnt 110.624756 hw_loss 0.090431 lr 0.00022804 rank 3
2023-02-17 15:48:09,923 DEBUG TRAIN Batch 0/5700 loss 62.933270 loss_att 96.740585 loss_ctc 65.073189 loss_rnnt 55.792351 hw_loss 0.176499 lr 0.00022804 rank 7
2023-02-17 15:48:09,926 DEBUG TRAIN Batch 0/5700 loss 152.838791 loss_att 278.852600 loss_ctc 152.639771 loss_rnnt 127.622864 hw_loss 0.074424 lr 0.00022804 rank 5
2023-02-17 15:48:09,926 DEBUG TRAIN Batch 0/5700 loss 157.466415 loss_att 256.234039 loss_ctc 161.899780 loss_rnnt 137.033432 hw_loss 0.165647 lr 0.00022804 rank 1
2023-02-17 15:48:09,925 DEBUG TRAIN Batch 0/5700 loss 77.917191 loss_att 154.382233 loss_ctc 74.064598 loss_rnnt 63.057861 hw_loss 0.149985 lr 0.00022804 rank 4
2023-02-17 15:48:09,930 DEBUG TRAIN Batch 0/5700 loss 174.981186 loss_att 277.978607 loss_ctc 176.999268 loss_rnnt 154.015366 hw_loss 0.182379 lr 0.00022804 rank 2
2023-02-17 15:48:09,931 DEBUG TRAIN Batch 0/5700 loss 56.635773 loss_att 85.230286 loss_ctc 59.663898 loss_rnnt 50.356735 hw_loss 0.293222 lr 0.00022804 rank 0
2023-02-17 15:49:24,388 DEBUG TRAIN Batch 0/5800 loss 141.551147 loss_att 253.198029 loss_ctc 136.166504 loss_rnnt 119.938919 hw_loss 0.001473 lr 0.00023204 rank 4
2023-02-17 15:49:24,396 DEBUG TRAIN Batch 0/5800 loss 59.666080 loss_att 87.056412 loss_ctc 60.425140 loss_rnnt 53.975189 hw_loss 0.209283 lr 0.00023204 rank 3
2023-02-17 15:49:24,397 DEBUG TRAIN Batch 0/5800 loss 148.366867 loss_att 237.048874 loss_ctc 147.819275 loss_rnnt 130.619324 hw_loss 0.157787 lr 0.00023204 rank 6
2023-02-17 15:49:24,399 DEBUG TRAIN Batch 0/5800 loss 120.279778 loss_att 218.519379 loss_ctc 119.904442 loss_rnnt 100.619499 hw_loss 0.117007 lr 0.00023204 rank 0
2023-02-17 15:49:24,402 DEBUG TRAIN Batch 0/5800 loss 159.224014 loss_att 267.877563 loss_ctc 149.234619 loss_rnnt 138.735046 hw_loss 0.169064 lr 0.00023204 rank 5
2023-02-17 15:49:24,403 DEBUG TRAIN Batch 0/5800 loss 98.740639 loss_att 215.530060 loss_ctc 92.583702 loss_rnnt 76.202896 hw_loss 0.001474 lr 0.00023204 rank 2
2023-02-17 15:49:24,406 DEBUG TRAIN Batch 0/5800 loss 97.234299 loss_att 185.146667 loss_ctc 88.704247 loss_rnnt 80.749489 hw_loss 0.074380 lr 0.00023204 rank 7
2023-02-17 15:49:24,446 DEBUG TRAIN Batch 0/5800 loss 138.034515 loss_att 240.413818 loss_ctc 137.743713 loss_rnnt 117.547546 hw_loss 0.093553 lr 0.00023204 rank 1
2023-02-17 15:50:38,185 DEBUG TRAIN Batch 0/5900 loss 127.927094 loss_att 220.026825 loss_ctc 129.251526 loss_rnnt 109.282669 hw_loss 0.089769 lr 0.00023604 rank 7
2023-02-17 15:50:38,185 DEBUG TRAIN Batch 0/5900 loss 146.463623 loss_att 255.083893 loss_ctc 144.694885 loss_rnnt 124.919327 hw_loss 0.105120 lr 0.00023604 rank 6
2023-02-17 15:50:38,187 DEBUG TRAIN Batch 0/5900 loss 135.740845 loss_att 248.562256 loss_ctc 139.479553 loss_rnnt 112.625717 hw_loss 0.098195 lr 0.00023604 rank 1
2023-02-17 15:50:38,187 DEBUG TRAIN Batch 0/5900 loss 107.061302 loss_att 227.451233 loss_ctc 109.588676 loss_rnnt 82.551819 hw_loss 0.177216 lr 0.00023604 rank 3
2023-02-17 15:50:38,190 DEBUG TRAIN Batch 0/5900 loss 121.622482 loss_att 225.182220 loss_ctc 111.506401 loss_rnnt 102.173447 hw_loss 0.161058 lr 0.00023604 rank 5
2023-02-17 15:50:38,191 DEBUG TRAIN Batch 0/5900 loss 115.506317 loss_att 214.193115 loss_ctc 113.128555 loss_rnnt 96.085617 hw_loss 0.000711 lr 0.00023604 rank 2
2023-02-17 15:50:38,192 DEBUG TRAIN Batch 0/5900 loss 168.923935 loss_att 272.021790 loss_ctc 167.001648 loss_rnnt 148.485718 hw_loss 0.140532 lr 0.00023604 rank 0
2023-02-17 15:50:38,238 DEBUG TRAIN Batch 0/5900 loss 152.436356 loss_att 258.383667 loss_ctc 148.394562 loss_rnnt 131.724899 hw_loss 0.114183 lr 0.00023604 rank 4
2023-02-17 15:51:53,756 DEBUG TRAIN Batch 0/6000 loss 122.625130 loss_att 217.638535 loss_ctc 123.084251 loss_rnnt 103.560974 hw_loss 0.000469 lr 0.00024004 rank 3
2023-02-17 15:51:53,759 DEBUG TRAIN Batch 0/6000 loss 110.125191 loss_att 217.334579 loss_ctc 109.326904 loss_rnnt 88.773216 hw_loss 0.030995 lr 0.00024004 rank 1
2023-02-17 15:51:53,766 DEBUG TRAIN Batch 0/6000 loss 122.718994 loss_att 242.283173 loss_ctc 115.293518 loss_rnnt 99.710495 hw_loss 0.160748 lr 0.00024004 rank 2
2023-02-17 15:51:53,768 DEBUG TRAIN Batch 0/6000 loss 111.605240 loss_att 226.969086 loss_ctc 111.833038 loss_rnnt 88.459953 hw_loss 0.079020 lr 0.00024004 rank 0
2023-02-17 15:51:53,768 DEBUG TRAIN Batch 0/6000 loss 110.270111 loss_att 207.826157 loss_ctc 105.448318 loss_rnnt 91.347755 hw_loss 0.101331 lr 0.00024004 rank 7
2023-02-17 15:51:53,769 DEBUG TRAIN Batch 0/6000 loss 155.189438 loss_att 266.877014 loss_ctc 155.113693 loss_rnnt 132.818344 hw_loss 0.081899 lr 0.00024004 rank 4
2023-02-17 15:51:53,771 DEBUG TRAIN Batch 0/6000 loss 134.989426 loss_att 232.253952 loss_ctc 135.283340 loss_rnnt 115.444855 hw_loss 0.098363 lr 0.00024004 rank 6
2023-02-17 15:51:53,775 DEBUG TRAIN Batch 0/6000 loss 117.452766 loss_att 204.690491 loss_ctc 115.769653 loss_rnnt 100.181976 hw_loss 0.089357 lr 0.00024004 rank 5
2023-02-17 15:53:08,322 DEBUG TRAIN Batch 0/6100 loss 136.667877 loss_att 216.322784 loss_ctc 142.936188 loss_rnnt 119.821564 hw_loss 0.149170 lr 0.00024404 rank 1
2023-02-17 15:53:08,335 DEBUG TRAIN Batch 0/6100 loss 136.743149 loss_att 256.509888 loss_ctc 135.161102 loss_rnnt 112.923691 hw_loss 0.144472 lr 0.00024404 rank 3
2023-02-17 15:53:08,339 DEBUG TRAIN Batch 0/6100 loss 107.068550 loss_att 210.927521 loss_ctc 104.135269 loss_rnnt 86.630432 hw_loss 0.107667 lr 0.00024404 rank 7
2023-02-17 15:53:08,339 DEBUG TRAIN Batch 0/6100 loss 149.876312 loss_att 249.422729 loss_ctc 152.910187 loss_rnnt 129.483765 hw_loss 0.147682 lr 0.00024404 rank 4
2023-02-17 15:53:08,340 DEBUG TRAIN Batch 0/6100 loss 116.689667 loss_att 224.160233 loss_ctc 112.758789 loss_rnnt 95.617508 hw_loss 0.191534 lr 0.00024404 rank 6
2023-02-17 15:53:08,342 DEBUG TRAIN Batch 0/6100 loss 139.366241 loss_att 217.319031 loss_ctc 142.222321 loss_rnnt 123.289490 hw_loss 0.197604 lr 0.00024404 rank 5
2023-02-17 15:53:08,345 DEBUG TRAIN Batch 0/6100 loss 114.843002 loss_att 214.419830 loss_ctc 115.513809 loss_rnnt 94.768028 hw_loss 0.131564 lr 0.00024404 rank 2
2023-02-17 15:53:08,352 DEBUG TRAIN Batch 0/6100 loss 119.302406 loss_att 226.124542 loss_ctc 111.556725 loss_rnnt 98.888962 hw_loss 0.153333 lr 0.00024404 rank 0
2023-02-17 15:54:21,811 DEBUG TRAIN Batch 0/6200 loss 98.371391 loss_att 192.522385 loss_ctc 90.840408 loss_rnnt 80.405212 hw_loss 0.262727 lr 0.00024804 rank 7
2023-02-17 15:54:21,816 DEBUG TRAIN Batch 0/6200 loss 58.231415 loss_att 102.913391 loss_ctc 57.675766 loss_rnnt 49.277603 hw_loss 0.171561 lr 0.00024804 rank 1
2023-02-17 15:54:21,818 DEBUG TRAIN Batch 0/6200 loss 100.319855 loss_att 180.673615 loss_ctc 96.783157 loss_rnnt 84.667633 hw_loss 0.099431 lr 0.00024804 rank 2
2023-02-17 15:54:21,819 DEBUG TRAIN Batch 0/6200 loss 144.825226 loss_att 254.555145 loss_ctc 148.313843 loss_rnnt 122.353096 hw_loss 0.114353 lr 0.00024804 rank 3
2023-02-17 15:54:21,820 DEBUG TRAIN Batch 0/6200 loss 127.556015 loss_att 217.948242 loss_ctc 133.761887 loss_rnnt 108.589622 hw_loss 0.113424 lr 0.00024804 rank 6
2023-02-17 15:54:21,820 DEBUG TRAIN Batch 0/6200 loss 104.735535 loss_att 198.435410 loss_ctc 101.255844 loss_rnnt 86.387344 hw_loss 0.135301 lr 0.00024804 rank 0
2023-02-17 15:54:21,822 DEBUG TRAIN Batch 0/6200 loss 120.152122 loss_att 204.178452 loss_ctc 121.278679 loss_rnnt 103.123146 hw_loss 0.137812 lr 0.00024804 rank 5
2023-02-17 15:54:21,821 DEBUG TRAIN Batch 0/6200 loss 146.715256 loss_att 237.841400 loss_ctc 157.120575 loss_rnnt 126.890541 hw_loss 0.397684 lr 0.00024804 rank 4
2023-02-17 15:55:35,315 DEBUG TRAIN Batch 0/6300 loss 150.758804 loss_att 287.485474 loss_ctc 154.092163 loss_rnnt 122.911667 hw_loss 0.107534 lr 0.00025204 rank 6
2023-02-17 15:55:35,326 DEBUG TRAIN Batch 0/6300 loss 93.316528 loss_att 167.795166 loss_ctc 87.154922 loss_rnnt 79.240746 hw_loss 0.003017 lr 0.00025204 rank 7
2023-02-17 15:55:35,329 DEBUG TRAIN Batch 0/6300 loss 100.552910 loss_att 192.646286 loss_ctc 100.232521 loss_rnnt 82.097420 hw_loss 0.149121 lr 0.00025204 rank 3
2023-02-17 15:55:35,330 DEBUG TRAIN Batch 0/6300 loss 131.973160 loss_att 238.148315 loss_ctc 128.981018 loss_rnnt 110.989777 hw_loss 0.276175 lr 0.00025204 rank 1
2023-02-17 15:55:35,332 DEBUG TRAIN Batch 0/6300 loss 74.601097 loss_att 137.295715 loss_ctc 75.536530 loss_rnnt 61.872066 hw_loss 0.122594 lr 0.00025204 rank 2
2023-02-17 15:55:35,333 DEBUG TRAIN Batch 0/6300 loss 58.116997 loss_att 103.891113 loss_ctc 62.505688 loss_rnnt 48.272106 hw_loss 0.196704 lr 0.00025204 rank 5
2023-02-17 15:55:35,333 DEBUG TRAIN Batch 0/6300 loss 106.823723 loss_att 200.652512 loss_ctc 104.144859 loss_rnnt 88.388123 hw_loss 0.050651 lr 0.00025204 rank 4
2023-02-17 15:55:35,339 DEBUG TRAIN Batch 0/6300 loss 100.915489 loss_att 145.178314 loss_ctc 105.171844 loss_rnnt 91.418015 hw_loss 0.145122 lr 0.00025204 rank 0
2023-02-17 15:56:51,575 DEBUG TRAIN Batch 0/6400 loss 133.055161 loss_att 238.583038 loss_ctc 130.717590 loss_rnnt 112.198303 hw_loss 0.118024 lr 0.00025604 rank 1
2023-02-17 15:56:51,576 DEBUG TRAIN Batch 0/6400 loss 116.001030 loss_att 243.502777 loss_ctc 106.506531 loss_rnnt 91.766441 hw_loss 0.000318 lr 0.00025604 rank 5
2023-02-17 15:56:51,578 DEBUG TRAIN Batch 0/6400 loss 109.725800 loss_att 214.397644 loss_ctc 107.744095 loss_rnnt 89.037811 hw_loss 0.033463 lr 0.00025604 rank 7
2023-02-17 15:56:51,578 DEBUG TRAIN Batch 0/6400 loss 133.166275 loss_att 231.372192 loss_ctc 128.887085 loss_rnnt 113.983231 hw_loss 0.210744 lr 0.00025604 rank 2
2023-02-17 15:56:51,580 DEBUG TRAIN Batch 0/6400 loss 114.788574 loss_att 237.457947 loss_ctc 107.489700 loss_rnnt 91.086296 hw_loss 0.265458 lr 0.00025604 rank 0
2023-02-17 15:56:51,582 DEBUG TRAIN Batch 0/6400 loss 92.223251 loss_att 149.727768 loss_ctc 94.976746 loss_rnnt 80.215439 hw_loss 0.262078 lr 0.00025604 rank 3
2023-02-17 15:56:51,582 DEBUG TRAIN Batch 0/6400 loss 74.823845 loss_att 116.214096 loss_ctc 82.596939 loss_rnnt 65.427582 hw_loss 0.153375 lr 0.00025604 rank 4
2023-02-17 15:56:51,583 DEBUG TRAIN Batch 0/6400 loss 119.563042 loss_att 227.351135 loss_ctc 119.784912 loss_rnnt 97.852524 hw_loss 0.231231 lr 0.00025604 rank 6
2023-02-17 15:58:05,017 DEBUG TRAIN Batch 0/6500 loss 134.063980 loss_att 296.125122 loss_ctc 120.453125 loss_rnnt 103.425522 hw_loss 0.076899 lr 0.00026004 rank 3
2023-02-17 15:58:05,017 DEBUG TRAIN Batch 0/6500 loss 112.159477 loss_att 242.012604 loss_ctc 109.823212 loss_rnnt 86.436783 hw_loss 0.119197 lr 0.00026004 rank 1
2023-02-17 15:58:05,018 DEBUG TRAIN Batch 0/6500 loss 97.699165 loss_att 218.396545 loss_ctc 90.036407 loss_rnnt 74.535576 hw_loss 0.085916 lr 0.00026004 rank 6
2023-02-17 15:58:05,019 DEBUG TRAIN Batch 0/6500 loss 134.034637 loss_att 233.351929 loss_ctc 131.221130 loss_rnnt 114.524979 hw_loss 0.039993 lr 0.00026004 rank 2
2023-02-17 15:58:05,019 DEBUG TRAIN Batch 0/6500 loss 113.353767 loss_att 219.212646 loss_ctc 104.495178 loss_rnnt 93.322838 hw_loss 0.075560 lr 0.00026004 rank 0
2023-02-17 15:58:05,019 DEBUG TRAIN Batch 0/6500 loss 136.873764 loss_att 253.931732 loss_ctc 140.160889 loss_rnnt 112.944260 hw_loss 0.149301 lr 0.00026004 rank 4
2023-02-17 15:58:05,029 DEBUG TRAIN Batch 0/6500 loss 108.391212 loss_att 236.042252 loss_ctc 101.772629 loss_rnnt 83.657890 hw_loss 0.160510 lr 0.00026004 rank 7
2023-02-17 15:58:05,033 DEBUG TRAIN Batch 0/6500 loss 120.531708 loss_att 225.715622 loss_ctc 118.125290 loss_rnnt 99.765961 hw_loss 0.093398 lr 0.00026004 rank 5
2023-02-17 15:59:18,791 DEBUG TRAIN Batch 0/6600 loss 127.205383 loss_att 220.242767 loss_ctc 125.833824 loss_rnnt 108.672592 hw_loss 0.202861 lr 0.00026404 rank 3
2023-02-17 15:59:18,792 DEBUG TRAIN Batch 0/6600 loss 114.350227 loss_att 214.398071 loss_ctc 114.378334 loss_rnnt 94.238312 hw_loss 0.184868 lr 0.00026404 rank 1
2023-02-17 15:59:18,793 DEBUG TRAIN Batch 0/6600 loss 148.121323 loss_att 237.438904 loss_ctc 153.934021 loss_rnnt 129.443161 hw_loss 0.074282 lr 0.00026404 rank 6
2023-02-17 15:59:18,794 DEBUG TRAIN Batch 0/6600 loss 118.305336 loss_att 232.720337 loss_ctc 117.790222 loss_rnnt 95.392120 hw_loss 0.185430 lr 0.00026404 rank 4
2023-02-17 15:59:18,795 DEBUG TRAIN Batch 0/6600 loss 80.964882 loss_att 206.334976 loss_ctc 80.298447 loss_rnnt 55.900524 hw_loss 0.148484 lr 0.00026404 rank 7
2023-02-17 15:59:18,795 DEBUG TRAIN Batch 0/6600 loss 87.274879 loss_att 199.317917 loss_ctc 81.030060 loss_rnnt 65.643936 hw_loss 0.103069 lr 0.00026404 rank 2
2023-02-17 15:59:18,801 DEBUG TRAIN Batch 0/6600 loss 105.912613 loss_att 223.261841 loss_ctc 98.587616 loss_rnnt 83.356995 hw_loss 0.117068 lr 0.00026404 rank 5
2023-02-17 15:59:18,806 DEBUG TRAIN Batch 0/6600 loss 140.588120 loss_att 231.061615 loss_ctc 149.282761 loss_rnnt 121.229843 hw_loss 0.195539 lr 0.00026404 rank 0
2023-02-17 16:00:33,008 DEBUG TRAIN Batch 0/6700 loss 129.015610 loss_att 224.880737 loss_ctc 138.458160 loss_rnnt 108.487694 hw_loss 0.179803 lr 0.00026804 rank 6
2023-02-17 16:00:33,009 DEBUG TRAIN Batch 0/6700 loss 95.959877 loss_att 203.917053 loss_ctc 94.713806 loss_rnnt 74.437492 hw_loss 0.182048 lr 0.00026804 rank 1
2023-02-17 16:00:33,011 DEBUG TRAIN Batch 0/6700 loss 92.784805 loss_att 195.827408 loss_ctc 88.148087 loss_rnnt 72.777527 hw_loss 0.031844 lr 0.00026804 rank 7
2023-02-17 16:00:33,015 DEBUG TRAIN Batch 0/6700 loss 125.785851 loss_att 247.413544 loss_ctc 117.531342 loss_rnnt 102.518341 hw_loss 0.079827 lr 0.00026804 rank 3
2023-02-17 16:00:33,016 DEBUG TRAIN Batch 0/6700 loss 115.369324 loss_att 222.087189 loss_ctc 122.979088 loss_rnnt 92.950737 hw_loss 0.113211 lr 0.00026804 rank 0
2023-02-17 16:00:33,016 DEBUG TRAIN Batch 0/6700 loss 132.807480 loss_att 274.530243 loss_ctc 131.567352 loss_rnnt 104.491356 hw_loss 0.256748 lr 0.00026804 rank 2
2023-02-17 16:00:33,019 DEBUG TRAIN Batch 0/6700 loss 123.300827 loss_att 230.516479 loss_ctc 123.646652 loss_rnnt 101.784843 hw_loss 0.050162 lr 0.00026804 rank 4
2023-02-17 16:00:33,021 DEBUG TRAIN Batch 0/6700 loss 88.867867 loss_att 208.650787 loss_ctc 77.702072 loss_rnnt 66.334198 hw_loss 0.123476 lr 0.00026804 rank 5
2023-02-17 16:01:48,037 DEBUG TRAIN Batch 0/6800 loss 172.813568 loss_att 291.214264 loss_ctc 168.893997 loss_rnnt 149.603546 hw_loss 0.098426 lr 0.00027204 rank 3
2023-02-17 16:01:48,037 DEBUG TRAIN Batch 0/6800 loss 122.690544 loss_att 223.308289 loss_ctc 120.777138 loss_rnnt 102.737823 hw_loss 0.158044 lr 0.00027204 rank 4
2023-02-17 16:01:48,041 DEBUG TRAIN Batch 0/6800 loss 93.164009 loss_att 197.266022 loss_ctc 91.274994 loss_rnnt 72.565475 hw_loss 0.056248 lr 0.00027204 rank 7
2023-02-17 16:01:48,042 DEBUG TRAIN Batch 0/6800 loss 94.736229 loss_att 205.455841 loss_ctc 94.543182 loss_rnnt 72.514900 hw_loss 0.193390 lr 0.00027204 rank 0
2023-02-17 16:01:48,043 DEBUG TRAIN Batch 0/6800 loss 91.338104 loss_att 207.265366 loss_ctc 89.029831 loss_rnnt 68.459946 hw_loss 0.000873 lr 0.00027204 rank 6
2023-02-17 16:01:48,045 DEBUG TRAIN Batch 0/6800 loss 86.517395 loss_att 188.372314 loss_ctc 82.086121 loss_rnnt 66.687187 hw_loss 0.093874 lr 0.00027204 rank 2
2023-02-17 16:01:48,046 DEBUG TRAIN Batch 0/6800 loss 74.100952 loss_att 148.225189 loss_ctc 69.464386 loss_rnnt 59.815563 hw_loss 0.147665 lr 0.00027204 rank 1
2023-02-17 16:01:48,052 DEBUG TRAIN Batch 0/6800 loss 104.060989 loss_att 215.185913 loss_ctc 100.667549 loss_rnnt 82.193069 hw_loss 0.178877 lr 0.00027204 rank 5
2023-02-17 16:03:00,697 DEBUG TRAIN Batch 0/6900 loss 95.962082 loss_att 222.710068 loss_ctc 89.194832 loss_rnnt 71.438080 hw_loss 0.143806 lr 0.00027604 rank 3
2023-02-17 16:03:00,698 DEBUG TRAIN Batch 0/6900 loss 93.351402 loss_att 191.855423 loss_ctc 89.258240 loss_rnnt 74.063782 hw_loss 0.248570 lr 0.00027604 rank 7
2023-02-17 16:03:00,702 DEBUG TRAIN Batch 0/6900 loss 118.404648 loss_att 236.530258 loss_ctc 119.077377 loss_rnnt 94.580322 hw_loss 0.205308 lr 0.00027604 rank 1
2023-02-17 16:03:00,705 DEBUG TRAIN Batch 0/6900 loss 89.586243 loss_att 157.484680 loss_ctc 91.695847 loss_rnnt 75.635315 hw_loss 0.168660 lr 0.00027604 rank 6
2023-02-17 16:03:00,707 DEBUG TRAIN Batch 0/6900 loss 91.182701 loss_att 174.070404 loss_ctc 101.572304 loss_rnnt 73.178207 hw_loss 0.078140 lr 0.00027604 rank 2
2023-02-17 16:03:00,709 DEBUG TRAIN Batch 0/6900 loss 99.511040 loss_att 183.246552 loss_ctc 102.326668 loss_rnnt 82.330681 hw_loss 0.108461 lr 0.00027604 rank 4
2023-02-17 16:03:00,710 DEBUG TRAIN Batch 0/6900 loss 73.545273 loss_att 167.215210 loss_ctc 71.038261 loss_rnnt 55.042400 hw_loss 0.193405 lr 0.00027604 rank 5
2023-02-17 16:03:00,713 DEBUG TRAIN Batch 0/6900 loss 87.948982 loss_att 188.909958 loss_ctc 89.655045 loss_rnnt 67.474693 hw_loss 0.102405 lr 0.00027604 rank 0
2023-02-17 16:04:14,382 DEBUG TRAIN Batch 0/7000 loss 83.007507 loss_att 197.794525 loss_ctc 81.393837 loss_rnnt 60.231594 hw_loss 0.063121 lr 0.00028004 rank 1
2023-02-17 16:04:14,382 DEBUG TRAIN Batch 0/7000 loss 97.808426 loss_att 173.839035 loss_ctc 100.513748 loss_rnnt 82.154274 hw_loss 0.163717 lr 0.00028004 rank 3
2023-02-17 16:04:14,385 DEBUG TRAIN Batch 0/7000 loss 90.444031 loss_att 226.945267 loss_ctc 85.910179 loss_rnnt 63.598259 hw_loss 0.281342 lr 0.00028004 rank 6
2023-02-17 16:04:14,386 DEBUG TRAIN Batch 0/7000 loss 137.060211 loss_att 292.727203 loss_ctc 133.124146 loss_rnnt 106.358810 hw_loss 0.174030 lr 0.00028004 rank 7
2023-02-17 16:04:14,387 DEBUG TRAIN Batch 0/7000 loss 161.394958 loss_att 263.815033 loss_ctc 158.988541 loss_rnnt 141.188568 hw_loss 0.081030 lr 0.00028004 rank 5
2023-02-17 16:04:14,388 DEBUG TRAIN Batch 0/7000 loss 111.691231 loss_att 255.274384 loss_ctc 97.373215 loss_rnnt 84.883606 hw_loss 0.000113 lr 0.00028004 rank 2
2023-02-17 16:04:14,391 DEBUG TRAIN Batch 0/7000 loss 72.904839 loss_att 152.194382 loss_ctc 77.167686 loss_rnnt 56.391792 hw_loss 0.162654 lr 0.00028004 rank 4
2023-02-17 16:04:14,395 DEBUG TRAIN Batch 0/7000 loss 41.671310 loss_att 68.924698 loss_ctc 41.753094 loss_rnnt 36.120808 hw_loss 0.166723 lr 0.00028004 rank 0
2023-02-17 16:05:30,425 DEBUG TRAIN Batch 0/7100 loss 112.080353 loss_att 239.405060 loss_ctc 104.752190 loss_rnnt 87.592255 hw_loss 0.000455 lr 0.00028404 rank 3
2023-02-17 16:05:30,428 DEBUG TRAIN Batch 0/7100 loss 91.013000 loss_att 201.922699 loss_ctc 93.797897 loss_rnnt 68.371887 hw_loss 0.164712 lr 0.00028404 rank 7
2023-02-17 16:05:30,428 DEBUG TRAIN Batch 0/7100 loss 110.763115 loss_att 246.300262 loss_ctc 105.674980 loss_rnnt 84.275597 hw_loss 0.109700 lr 0.00028404 rank 6
2023-02-17 16:05:30,432 DEBUG TRAIN Batch 0/7100 loss 124.862526 loss_att 248.748840 loss_ctc 127.239014 loss_rnnt 99.635521 hw_loss 0.249165 lr 0.00028404 rank 4
2023-02-17 16:05:30,434 DEBUG TRAIN Batch 0/7100 loss 112.594200 loss_att 227.860229 loss_ctc 118.417892 loss_rnnt 88.720657 hw_loss 0.082192 lr 0.00028404 rank 1
2023-02-17 16:05:30,441 DEBUG TRAIN Batch 0/7100 loss 91.486893 loss_att 216.941101 loss_ctc 85.620544 loss_rnnt 67.177986 hw_loss 0.000455 lr 0.00028404 rank 0
2023-02-17 16:05:30,481 DEBUG TRAIN Batch 0/7100 loss 65.381950 loss_att 168.953629 loss_ctc 64.560112 loss_rnnt 44.660412 hw_loss 0.218963 lr 0.00028404 rank 2
2023-02-17 16:05:30,500 DEBUG TRAIN Batch 0/7100 loss 107.035484 loss_att 238.666809 loss_ctc 96.049759 loss_rnnt 82.142761 hw_loss 0.058534 lr 0.00028404 rank 5
2023-02-17 16:06:43,856 DEBUG TRAIN Batch 0/7200 loss 82.422554 loss_att 198.323837 loss_ctc 74.623650 loss_rnnt 60.233971 hw_loss 0.090344 lr 0.00028804 rank 3
2023-02-17 16:06:43,856 DEBUG TRAIN Batch 0/7200 loss 92.813507 loss_att 221.270309 loss_ctc 83.285027 loss_rnnt 68.300751 hw_loss 0.172236 lr 0.00028804 rank 7
2023-02-17 16:06:43,856 DEBUG TRAIN Batch 0/7200 loss 79.892891 loss_att 181.148712 loss_ctc 79.371658 loss_rnnt 59.633453 hw_loss 0.145815 lr 0.00028804 rank 6
2023-02-17 16:06:43,859 DEBUG TRAIN Batch 0/7200 loss 111.111984 loss_att 220.360229 loss_ctc 105.096680 loss_rnnt 89.998520 hw_loss 0.123484 lr 0.00028804 rank 4
2023-02-17 16:06:43,863 DEBUG TRAIN Batch 0/7200 loss 99.964470 loss_att 194.812836 loss_ctc 107.477127 loss_rnnt 79.924545 hw_loss 0.128561 lr 0.00028804 rank 5
2023-02-17 16:06:43,863 DEBUG TRAIN Batch 0/7200 loss 145.460281 loss_att 271.808868 loss_ctc 150.975266 loss_rnnt 119.381042 hw_loss 0.139128 lr 0.00028804 rank 0
2023-02-17 16:06:43,865 DEBUG TRAIN Batch 0/7200 loss 90.887100 loss_att 193.756165 loss_ctc 91.829765 loss_rnnt 70.163773 hw_loss 0.044677 lr 0.00028804 rank 1
2023-02-17 16:06:43,866 DEBUG TRAIN Batch 0/7200 loss 105.895676 loss_att 218.383453 loss_ctc 101.585197 loss_rnnt 83.936630 hw_loss 0.067906 lr 0.00028804 rank 2
2023-02-17 16:07:57,173 DEBUG TRAIN Batch 0/7300 loss 109.212395 loss_att 224.985184 loss_ctc 108.630890 loss_rnnt 86.098526 hw_loss 0.069074 lr 0.00029204 rank 6
2023-02-17 16:07:57,174 DEBUG TRAIN Batch 0/7300 loss 98.718346 loss_att 201.272156 loss_ctc 106.354156 loss_rnnt 77.164993 hw_loss 0.045883 lr 0.00029204 rank 7
2023-02-17 16:07:57,178 DEBUG TRAIN Batch 0/7300 loss 91.570526 loss_att 211.088593 loss_ctc 93.132431 loss_rnnt 67.416862 hw_loss 0.078369 lr 0.00029204 rank 3
2023-02-17 16:07:57,180 DEBUG TRAIN Batch 0/7300 loss 80.834770 loss_att 187.650665 loss_ctc 78.747231 loss_rnnt 59.676369 hw_loss 0.137922 lr 0.00029204 rank 1
2023-02-17 16:07:57,180 DEBUG TRAIN Batch 0/7300 loss 102.682861 loss_att 210.458496 loss_ctc 100.951439 loss_rnnt 81.288345 hw_loss 0.131722 lr 0.00029204 rank 0
2023-02-17 16:07:57,182 DEBUG TRAIN Batch 0/7300 loss 118.329468 loss_att 238.262314 loss_ctc 119.200043 loss_rnnt 94.192322 hw_loss 0.064678 lr 0.00029204 rank 2
2023-02-17 16:07:57,184 DEBUG TRAIN Batch 0/7300 loss 108.446747 loss_att 222.130661 loss_ctc 111.204880 loss_rnnt 85.306816 hw_loss 0.066382 lr 0.00029204 rank 4
2023-02-17 16:07:57,190 DEBUG TRAIN Batch 0/7300 loss 105.651978 loss_att 217.860901 loss_ctc 115.426163 loss_rnnt 81.845245 hw_loss 0.115719 lr 0.00029204 rank 5
2023-02-17 16:09:11,120 DEBUG TRAIN Batch 0/7400 loss 96.257805 loss_att 232.043732 loss_ctc 95.127243 loss_rnnt 69.145592 hw_loss 0.198298 lr 0.00029604 rank 3
2023-02-17 16:09:11,122 DEBUG TRAIN Batch 0/7400 loss 120.697327 loss_att 201.528839 loss_ctc 121.160446 loss_rnnt 104.385437 hw_loss 0.157212 lr 0.00029604 rank 7
2023-02-17 16:09:11,122 DEBUG TRAIN Batch 0/7400 loss 114.395721 loss_att 219.374146 loss_ctc 125.348907 loss_rnnt 91.854706 hw_loss 0.159189 lr 0.00029604 rank 6
2023-02-17 16:09:11,125 DEBUG TRAIN Batch 0/7400 loss 75.842842 loss_att 162.970657 loss_ctc 70.335289 loss_rnnt 59.084694 hw_loss 0.125480 lr 0.00029604 rank 1
2023-02-17 16:09:11,127 DEBUG TRAIN Batch 0/7400 loss 114.323311 loss_att 226.855652 loss_ctc 117.392120 loss_rnnt 91.343582 hw_loss 0.120145 lr 0.00029604 rank 0
2023-02-17 16:09:11,127 DEBUG TRAIN Batch 0/7400 loss 127.094177 loss_att 258.147369 loss_ctc 124.642853 loss_rnnt 101.158707 hw_loss 0.096881 lr 0.00029604 rank 4
2023-02-17 16:09:11,128 DEBUG TRAIN Batch 0/7400 loss 124.659065 loss_att 237.643646 loss_ctc 118.507309 loss_rnnt 102.797493 hw_loss 0.159176 lr 0.00029604 rank 2
2023-02-17 16:09:11,130 DEBUG TRAIN Batch 0/7400 loss 120.213867 loss_att 218.559570 loss_ctc 124.366707 loss_rnnt 99.948593 hw_loss 0.079563 lr 0.00029604 rank 5
2023-02-17 16:10:26,487 DEBUG TRAIN Batch 0/7500 loss 78.994614 loss_att 154.677551 loss_ctc 78.554619 loss_rnnt 63.808949 hw_loss 0.202027 lr 0.00030004 rank 7
2023-02-17 16:10:26,489 DEBUG TRAIN Batch 0/7500 loss 116.686913 loss_att 244.699203 loss_ctc 125.286644 loss_rnnt 89.937439 hw_loss 0.000699 lr 0.00030004 rank 1
2023-02-17 16:10:26,489 DEBUG TRAIN Batch 0/7500 loss 73.340706 loss_att 163.188721 loss_ctc 71.157219 loss_rnnt 55.603016 hw_loss 0.111021 lr 0.00030004 rank 6
2023-02-17 16:10:26,496 DEBUG TRAIN Batch 0/7500 loss 118.662155 loss_att 237.711975 loss_ctc 113.188461 loss_rnnt 95.549507 hw_loss 0.060939 lr 0.00030004 rank 3
2023-02-17 16:10:26,499 DEBUG TRAIN Batch 0/7500 loss 90.260468 loss_att 209.605957 loss_ctc 84.673355 loss_rnnt 67.100494 hw_loss 0.067179 lr 0.00030004 rank 4
2023-02-17 16:10:26,499 DEBUG TRAIN Batch 0/7500 loss 79.775314 loss_att 183.373932 loss_ctc 78.712898 loss_rnnt 59.089588 hw_loss 0.201855 lr 0.00030004 rank 2
2023-02-17 16:10:26,502 DEBUG TRAIN Batch 0/7500 loss 110.394264 loss_att 214.168503 loss_ctc 107.560974 loss_rnnt 89.938934 hw_loss 0.146724 lr 0.00030004 rank 0
2023-02-17 16:10:26,507 DEBUG TRAIN Batch 0/7500 loss 64.354187 loss_att 137.480331 loss_ctc 65.649513 loss_rnnt 49.451763 hw_loss 0.195905 lr 0.00030004 rank 5
2023-02-17 16:11:42,408 DEBUG TRAIN Batch 0/7600 loss 68.964607 loss_att 123.488007 loss_ctc 74.564491 loss_rnnt 57.207203 hw_loss 0.198877 lr 0.00030404 rank 7
2023-02-17 16:11:42,411 DEBUG TRAIN Batch 0/7600 loss 38.416531 loss_att 56.120670 loss_ctc 42.937210 loss_rnnt 34.167809 hw_loss 0.197120 lr 0.00030404 rank 6
2023-02-17 16:11:42,413 DEBUG TRAIN Batch 0/7600 loss 62.948311 loss_att 101.337303 loss_ctc 64.389183 loss_rnnt 54.944263 hw_loss 0.251496 lr 0.00030404 rank 2
2023-02-17 16:11:42,415 DEBUG TRAIN Batch 0/7600 loss 79.981529 loss_att 187.553635 loss_ctc 73.902023 loss_rnnt 59.227341 hw_loss 0.094441 lr 0.00030404 rank 4
2023-02-17 16:11:42,416 DEBUG TRAIN Batch 0/7600 loss 106.501778 loss_att 197.499268 loss_ctc 111.575096 loss_rnnt 87.512177 hw_loss 0.213108 lr 0.00030404 rank 3
2023-02-17 16:11:42,417 DEBUG TRAIN Batch 0/7600 loss 59.857224 loss_att 160.616043 loss_ctc 60.332485 loss_rnnt 39.600121 hw_loss 0.078700 lr 0.00030404 rank 1
2023-02-17 16:11:42,417 DEBUG TRAIN Batch 0/7600 loss 82.537720 loss_att 141.487839 loss_ctc 84.570755 loss_rnnt 70.388390 hw_loss 0.165444 lr 0.00030404 rank 5
2023-02-17 16:11:42,473 DEBUG TRAIN Batch 0/7600 loss 64.539352 loss_att 121.665649 loss_ctc 64.351395 loss_rnnt 53.001072 hw_loss 0.258913 lr 0.00030404 rank 0
2023-02-17 16:12:55,979 DEBUG TRAIN Batch 0/7700 loss 120.969757 loss_att 242.796692 loss_ctc 123.366776 loss_rnnt 96.226135 hw_loss 0.109948 lr 0.00030804 rank 7
2023-02-17 16:12:55,979 DEBUG TRAIN Batch 0/7700 loss 93.365776 loss_att 199.442902 loss_ctc 91.289505 loss_rnnt 72.368942 hw_loss 0.109198 lr 0.00030804 rank 6
2023-02-17 16:12:55,980 DEBUG TRAIN Batch 0/7700 loss 50.285564 loss_att 89.619598 loss_ctc 51.659309 loss_rnnt 42.148682 hw_loss 0.162958 lr 0.00030804 rank 3
2023-02-17 16:12:55,981 DEBUG TRAIN Batch 0/7700 loss 101.325066 loss_att 211.043365 loss_ctc 100.269287 loss_rnnt 79.520248 hw_loss 0.003607 lr 0.00030804 rank 1
2023-02-17 16:12:55,984 DEBUG TRAIN Batch 0/7700 loss 126.099045 loss_att 246.080170 loss_ctc 131.976379 loss_rnnt 101.226578 hw_loss 0.173602 lr 0.00030804 rank 0
2023-02-17 16:12:55,987 DEBUG TRAIN Batch 0/7700 loss 46.402245 loss_att 88.323112 loss_ctc 46.327942 loss_rnnt 37.942333 hw_loss 0.160586 lr 0.00030804 rank 4
2023-02-17 16:12:55,988 DEBUG TRAIN Batch 0/7700 loss 77.192375 loss_att 179.887115 loss_ctc 69.290848 loss_rnnt 57.667961 hw_loss 0.073125 lr 0.00030804 rank 5
2023-02-17 16:12:55,988 DEBUG TRAIN Batch 0/7700 loss 86.109268 loss_att 200.656342 loss_ctc 83.829369 loss_rnnt 63.339508 hw_loss 0.308120 lr 0.00030804 rank 2
2023-02-17 16:14:10,994 DEBUG TRAIN Batch 0/7800 loss 77.700859 loss_att 180.691376 loss_ctc 74.737556 loss_rnnt 57.438263 hw_loss 0.111748 lr 0.00031204 rank 7
2023-02-17 16:14:11,005 DEBUG TRAIN Batch 0/7800 loss 96.133148 loss_att 215.688614 loss_ctc 87.356285 loss_rnnt 73.370941 hw_loss 0.040064 lr 0.00031204 rank 3
2023-02-17 16:14:11,009 DEBUG TRAIN Batch 0/7800 loss 121.248009 loss_att 241.791565 loss_ctc 130.466064 loss_rnnt 95.825241 hw_loss 0.159344 lr 0.00031204 rank 1
2023-02-17 16:14:11,014 DEBUG TRAIN Batch 0/7800 loss 73.898674 loss_att 180.388077 loss_ctc 78.112999 loss_rnnt 51.916794 hw_loss 0.228912 lr 0.00031204 rank 6
2023-02-17 16:14:11,014 DEBUG TRAIN Batch 0/7800 loss 88.312553 loss_att 199.729248 loss_ctc 87.448425 loss_rnnt 66.107758 hw_loss 0.068754 lr 0.00031204 rank 4
2023-02-17 16:14:11,016 DEBUG TRAIN Batch 0/7800 loss 90.878868 loss_att 180.262909 loss_ctc 96.120850 loss_rnnt 72.216118 hw_loss 0.163160 lr 0.00031204 rank 2
2023-02-17 16:14:11,017 DEBUG TRAIN Batch 0/7800 loss 99.216476 loss_att 192.052933 loss_ctc 94.180717 loss_rnnt 81.212311 hw_loss 0.203101 lr 0.00031204 rank 5
2023-02-17 16:14:11,021 DEBUG TRAIN Batch 0/7800 loss 101.246826 loss_att 246.266754 loss_ctc 93.663864 loss_rnnt 73.231171 hw_loss 0.042610 lr 0.00031204 rank 0
2023-02-17 16:15:24,209 DEBUG TRAIN Batch 0/7900 loss 77.084381 loss_att 188.680420 loss_ctc 75.017792 loss_rnnt 54.963501 hw_loss 0.144789 lr 0.00031604 rank 1
2023-02-17 16:15:24,213 DEBUG TRAIN Batch 0/7900 loss 136.397919 loss_att 254.411011 loss_ctc 140.700867 loss_rnnt 112.154755 hw_loss 0.125272 lr 0.00031604 rank 3
2023-02-17 16:15:24,215 DEBUG TRAIN Batch 0/7900 loss 90.698051 loss_att 201.762711 loss_ctc 88.818619 loss_rnnt 68.698669 hw_loss 0.069445 lr 0.00031604 rank 7
2023-02-17 16:15:24,216 DEBUG TRAIN Batch 0/7900 loss 98.177094 loss_att 212.019165 loss_ctc 98.863922 loss_rnnt 75.246307 hw_loss 0.132757 lr 0.00031604 rank 6
2023-02-17 16:15:24,217 DEBUG TRAIN Batch 0/7900 loss 74.013519 loss_att 180.711304 loss_ctc 79.421707 loss_rnnt 51.952679 hw_loss 0.000367 lr 0.00031604 rank 4
2023-02-17 16:15:24,224 DEBUG TRAIN Batch 0/7900 loss 114.191284 loss_att 229.417023 loss_ctc 123.688286 loss_rnnt 89.809929 hw_loss 0.131150 lr 0.00031604 rank 2
2023-02-17 16:15:24,226 DEBUG TRAIN Batch 0/7900 loss 83.979256 loss_att 189.799316 loss_ctc 85.750793 loss_rnnt 62.540543 hw_loss 0.072174 lr 0.00031604 rank 5
2023-02-17 16:15:24,230 DEBUG TRAIN Batch 0/7900 loss 78.259048 loss_att 190.594910 loss_ctc 80.799866 loss_rnnt 55.371952 hw_loss 0.152141 lr 0.00031604 rank 0
2023-02-17 16:16:36,440 DEBUG TRAIN Batch 0/8000 loss 62.829330 loss_att 152.142578 loss_ctc 65.192780 loss_rnnt 44.609215 hw_loss 0.079383 lr 0.00032004 rank 7
2023-02-17 16:16:36,440 DEBUG TRAIN Batch 0/8000 loss 111.885803 loss_att 223.282623 loss_ctc 114.071251 loss_rnnt 89.300583 hw_loss 0.027127 lr 0.00032004 rank 4
2023-02-17 16:16:36,442 DEBUG TRAIN Batch 0/8000 loss 94.826164 loss_att 210.527573 loss_ctc 98.032944 loss_rnnt 71.258049 hw_loss 0.000490 lr 0.00032004 rank 3
2023-02-17 16:16:36,442 DEBUG TRAIN Batch 0/8000 loss 72.034729 loss_att 137.309235 loss_ctc 79.046310 loss_rnnt 57.978687 hw_loss 0.124237 lr 0.00032004 rank 1
2023-02-17 16:16:36,444 DEBUG TRAIN Batch 0/8000 loss 99.669106 loss_att 217.866913 loss_ctc 103.103592 loss_rnnt 75.523201 hw_loss 0.090779 lr 0.00032004 rank 6
2023-02-17 16:16:36,451 DEBUG TRAIN Batch 0/8000 loss 79.431046 loss_att 179.286438 loss_ctc 79.661346 loss_rnnt 59.379246 hw_loss 0.093765 lr 0.00032004 rank 5
2023-02-17 16:16:36,454 DEBUG TRAIN Batch 0/8000 loss 104.228020 loss_att 221.374390 loss_ctc 108.232437 loss_rnnt 80.149460 hw_loss 0.216294 lr 0.00032004 rank 0
2023-02-17 16:16:36,499 DEBUG TRAIN Batch 0/8000 loss 78.475464 loss_att 188.642181 loss_ctc 75.129921 loss_rnnt 56.815315 hw_loss 0.136638 lr 0.00032004 rank 2
2023-02-17 16:17:49,942 DEBUG TRAIN Batch 0/8100 loss 79.742081 loss_att 158.124451 loss_ctc 75.835159 loss_rnnt 64.565727 hw_loss 0.039002 lr 0.00032404 rank 3
2023-02-17 16:17:49,943 DEBUG TRAIN Batch 0/8100 loss 83.237061 loss_att 180.267014 loss_ctc 86.431908 loss_rnnt 63.337048 hw_loss 0.127591 lr 0.00032404 rank 7
2023-02-17 16:17:49,948 DEBUG TRAIN Batch 0/8100 loss 99.933563 loss_att 211.349731 loss_ctc 104.361069 loss_rnnt 77.011902 hw_loss 0.090187 lr 0.00032404 rank 1
2023-02-17 16:17:49,948 DEBUG TRAIN Batch 0/8100 loss 85.880058 loss_att 178.052063 loss_ctc 87.054276 loss_rnnt 67.183762 hw_loss 0.197489 lr 0.00032404 rank 6
2023-02-17 16:17:49,950 DEBUG TRAIN Batch 0/8100 loss 68.619995 loss_att 168.402649 loss_ctc 68.072937 loss_rnnt 48.702454 hw_loss 0.063659 lr 0.00032404 rank 2
2023-02-17 16:17:49,953 DEBUG TRAIN Batch 0/8100 loss 65.851143 loss_att 145.543182 loss_ctc 70.363663 loss_rnnt 49.233963 hw_loss 0.144551 lr 0.00032404 rank 0
2023-02-17 16:17:49,954 DEBUG TRAIN Batch 0/8100 loss 78.477066 loss_att 174.947617 loss_ctc 79.093849 loss_rnnt 59.055908 hw_loss 0.084017 lr 0.00032404 rank 5
2023-02-17 16:17:49,966 DEBUG TRAIN Batch 0/8100 loss 98.799011 loss_att 212.846832 loss_ctc 110.201935 loss_rnnt 74.430412 hw_loss 0.072460 lr 0.00032404 rank 4
2023-02-17 16:19:03,189 DEBUG TRAIN Batch 0/8200 loss 88.181854 loss_att 186.334198 loss_ctc 85.136887 loss_rnnt 68.873596 hw_loss 0.157092 lr 0.00032804 rank 1
2023-02-17 16:19:03,189 DEBUG TRAIN Batch 0/8200 loss 80.463127 loss_att 143.101776 loss_ctc 83.445602 loss_rnnt 67.431900 hw_loss 0.198422 lr 0.00032804 rank 6
2023-02-17 16:19:03,189 DEBUG TRAIN Batch 0/8200 loss 89.587387 loss_att 186.542786 loss_ctc 95.692207 loss_rnnt 69.351196 hw_loss 0.058364 lr 0.00032804 rank 3
2023-02-17 16:19:03,196 DEBUG TRAIN Batch 0/8200 loss 117.786964 loss_att 217.371048 loss_ctc 117.634163 loss_rnnt 97.755249 hw_loss 0.253644 lr 0.00032804 rank 4
2023-02-17 16:19:03,198 DEBUG TRAIN Batch 0/8200 loss 84.359642 loss_att 146.274704 loss_ctc 97.030090 loss_rnnt 70.158165 hw_loss 0.242009 lr 0.00032804 rank 0
2023-02-17 16:19:03,199 DEBUG TRAIN Batch 0/8200 loss 57.054939 loss_att 111.093750 loss_ctc 60.283512 loss_rnnt 45.764812 hw_loss 0.097295 lr 0.00032804 rank 7
2023-02-17 16:19:03,200 DEBUG TRAIN Batch 0/8200 loss 59.411629 loss_att 107.347321 loss_ctc 62.587990 loss_rnnt 49.325020 hw_loss 0.142404 lr 0.00032804 rank 2
2023-02-17 16:19:03,243 DEBUG TRAIN Batch 0/8200 loss 53.278656 loss_att 134.750641 loss_ctc 51.992653 loss_rnnt 37.063938 hw_loss 0.172104 lr 0.00032804 rank 5
2023-02-17 16:20:15,813 DEBUG TRAIN Batch 0/8300 loss 85.490089 loss_att 185.225998 loss_ctc 91.204147 loss_rnnt 64.736488 hw_loss 0.083532 lr 0.00033204 rank 6
2023-02-17 16:20:15,817 DEBUG TRAIN Batch 0/8300 loss 89.866653 loss_att 190.195923 loss_ctc 84.010406 loss_rnnt 70.511154 hw_loss 0.132134 lr 0.00033204 rank 7
2023-02-17 16:20:15,818 DEBUG TRAIN Batch 0/8300 loss 110.009102 loss_att 197.611053 loss_ctc 119.031998 loss_rnnt 91.211288 hw_loss 0.139446 lr 0.00033204 rank 4
2023-02-17 16:20:15,827 DEBUG TRAIN Batch 0/8300 loss 109.205002 loss_att 222.650116 loss_ctc 111.078323 loss_rnnt 86.265892 hw_loss 0.000565 lr 0.00033204 rank 1
2023-02-17 16:20:15,834 DEBUG TRAIN Batch 0/8300 loss 82.085983 loss_att 158.691193 loss_ctc 87.125702 loss_rnnt 66.055107 hw_loss 0.071013 lr 0.00033204 rank 3
2023-02-17 16:20:15,836 DEBUG TRAIN Batch 0/8300 loss 79.158791 loss_att 183.751678 loss_ctc 80.538895 loss_rnnt 58.021690 hw_loss 0.064695 lr 0.00033204 rank 0
2023-02-17 16:20:15,835 DEBUG TRAIN Batch 0/8300 loss 82.328072 loss_att 178.857697 loss_ctc 82.818726 loss_rnnt 62.912975 hw_loss 0.082042 lr 0.00033204 rank 2
2023-02-17 16:20:15,839 DEBUG TRAIN Batch 0/8300 loss 80.381920 loss_att 180.270554 loss_ctc 79.932190 loss_rnnt 60.434246 hw_loss 0.056092 lr 0.00033204 rank 5
2023-02-17 16:21:17,378 DEBUG CV Batch 0/0 loss 15.682026 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101210 rank 1
2023-02-17 16:21:17,389 DEBUG CV Batch 0/0 loss 15.682027 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101211 rank 0
2023-02-17 16:21:17,391 DEBUG CV Batch 0/0 loss 15.682027 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101211 rank 4
2023-02-17 16:21:17,395 DEBUG CV Batch 0/0 loss 15.682027 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101211 rank 2
2023-02-17 16:21:17,397 DEBUG CV Batch 0/0 loss 15.682026 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101210 rank 6
2023-02-17 16:21:17,398 DEBUG CV Batch 0/0 loss 15.682026 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101210 rank 3
2023-02-17 16:21:17,405 DEBUG CV Batch 0/0 loss 15.682026 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101210 rank 7
2023-02-17 16:21:17,406 DEBUG CV Batch 0/0 loss 15.682026 loss_att 23.435015 loss_ctc 18.739511 loss_rnnt 13.436857 hw_loss 0.537952 history loss 15.101210 rank 5
2023-02-17 16:21:28,583 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 1
2023-02-17 16:21:28,688 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 7
2023-02-17 16:21:28,795 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 4
2023-02-17 16:21:28,811 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 0
2023-02-17 16:21:29,119 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 5
2023-02-17 16:21:29,192 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 3
2023-02-17 16:21:29,217 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 2
2023-02-17 16:21:29,300 DEBUG CV Batch 0/100 loss 66.162895 loss_att 123.607193 loss_ctc 72.046906 loss_rnnt 53.763165 hw_loss 0.236872 history loss 38.525497 rank 6
2023-02-17 16:21:41,900 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 1
2023-02-17 16:21:42,104 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 7
2023-02-17 16:21:42,194 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 4
2023-02-17 16:21:42,466 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 0
2023-02-17 16:21:42,601 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 5
2023-02-17 16:21:42,741 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 2
2023-02-17 16:21:42,758 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 6
2023-02-17 16:21:42,883 DEBUG CV Batch 0/200 loss 134.463226 loss_att 342.538849 loss_ctc 129.375488 loss_rnnt 93.493500 hw_loss 0.061794 history loss 43.605717 rank 3
2023-02-17 16:21:54,285 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 1
2023-02-17 16:21:54,352 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 4
2023-02-17 16:21:54,545 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 7
2023-02-17 16:21:54,786 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 5
2023-02-17 16:21:54,982 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 6
2023-02-17 16:21:54,985 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 0
2023-02-17 16:21:55,006 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 3
2023-02-17 16:21:55,167 DEBUG CV Batch 0/300 loss 48.367706 loss_att 92.961464 loss_ctc 52.745529 loss_rnnt 38.864174 hw_loss 0.002000 history loss 42.746900 rank 2
2023-02-17 16:22:06,255 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 1
2023-02-17 16:22:06,465 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 4
2023-02-17 16:22:06,532 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 7
2023-02-17 16:22:06,932 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 5
2023-02-17 16:22:07,027 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 6
2023-02-17 16:22:07,082 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 3
2023-02-17 16:22:07,324 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 0
2023-02-17 16:22:07,324 DEBUG CV Batch 0/400 loss 188.974304 loss_att 487.901855 loss_ctc 164.603958 loss_rnnt 132.437119 hw_loss 0.002001 history loss 45.529785 rank 2
2023-02-17 16:22:17,008 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 1
2023-02-17 16:22:17,117 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 4
2023-02-17 16:22:17,158 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 7
2023-02-17 16:22:17,697 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 5
2023-02-17 16:22:17,760 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 3
2023-02-17 16:22:17,788 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 6
2023-02-17 16:22:18,115 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 2
2023-02-17 16:22:18,434 DEBUG CV Batch 0/500 loss 67.451042 loss_att 127.221817 loss_ctc 73.023399 loss_rnnt 54.737255 hw_loss 0.031215 history loss 45.808995 rank 0
2023-02-17 16:22:29,267 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 7
2023-02-17 16:22:29,485 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 1
2023-02-17 16:22:29,491 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 4
2023-02-17 16:22:29,817 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 5
2023-02-17 16:22:29,909 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 3
2023-02-17 16:22:30,016 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 6
2023-02-17 16:22:30,210 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 2
2023-02-17 16:22:30,720 DEBUG CV Batch 0/600 loss 33.108974 loss_att 51.445137 loss_ctc 36.212036 loss_rnnt 28.805523 hw_loss 0.417144 history loss 47.415056 rank 0
2023-02-17 16:22:40,603 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 7
2023-02-17 16:22:40,792 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 1
2023-02-17 16:22:40,797 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 4
2023-02-17 16:22:41,344 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 5
2023-02-17 16:22:41,373 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 3
2023-02-17 16:22:41,444 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 6
2023-02-17 16:22:42,265 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 2
2023-02-17 16:22:42,624 DEBUG CV Batch 0/700 loss 186.962494 loss_att 417.221191 loss_ctc 201.103317 loss_rnnt 139.024246 hw_loss 0.002001 history loss 48.340685 rank 0
2023-02-17 16:22:51,915 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 7
2023-02-17 16:22:52,147 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 1
2023-02-17 16:22:52,196 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 4
2023-02-17 16:22:52,604 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 3
2023-02-17 16:22:52,693 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 6
2023-02-17 16:22:52,839 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 5
2023-02-17 16:22:53,646 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 2
2023-02-17 16:22:54,099 DEBUG CV Batch 0/800 loss 66.105774 loss_att 123.850845 loss_ctc 72.293297 loss_rnnt 53.588177 hw_loss 0.269219 history loss 46.902096 rank 0
2023-02-17 16:23:05,140 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 7
2023-02-17 16:23:05,351 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 1
2023-02-17 16:23:05,427 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 4
2023-02-17 16:23:05,958 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 3
2023-02-17 16:23:05,993 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 6
2023-02-17 16:23:06,207 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 5
2023-02-17 16:23:07,113 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 2
2023-02-17 16:23:07,794 DEBUG CV Batch 0/900 loss 120.021904 loss_att 296.577972 loss_ctc 117.698929 loss_rnnt 84.936760 hw_loss 0.156882 history loss 47.201928 rank 0
2023-02-17 16:23:17,200 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 7
2023-02-17 16:23:17,410 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 1
2023-02-17 16:23:17,516 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 4
2023-02-17 16:23:18,067 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 3
2023-02-17 16:23:18,297 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 6
2023-02-17 16:23:18,734 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 5
2023-02-17 16:23:19,419 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 2
2023-02-17 16:23:20,129 DEBUG CV Batch 0/1000 loss 41.993561 loss_att 79.636475 loss_ctc 45.312626 loss_rnnt 33.937428 hw_loss 0.159389 history loss 46.664059 rank 0
2023-02-17 16:23:29,095 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462310 history loss 46.828463 rank 7
2023-02-17 16:23:29,172 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462310 history loss 46.828463 rank 1
2023-02-17 16:23:29,407 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 4
2023-02-17 16:23:29,974 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 3
2023-02-17 16:23:30,223 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 6
2023-02-17 16:23:30,684 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 5
2023-02-17 16:23:31,459 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 2
2023-02-17 16:23:32,243 DEBUG CV Batch 0/1100 loss 19.462252 loss_att 27.114021 loss_ctc 21.973785 loss_rnnt 17.350460 hw_loss 0.462309 history loss 46.828463 rank 0
2023-02-17 16:23:39,722 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 7
2023-02-17 16:23:39,796 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 1
2023-02-17 16:23:40,101 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 4
2023-02-17 16:23:40,679 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 3
2023-02-17 16:23:40,927 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 6
2023-02-17 16:23:41,445 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 5
2023-02-17 16:23:42,258 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 2
2023-02-17 16:23:43,230 DEBUG CV Batch 0/1200 loss 79.382156 loss_att 142.302887 loss_ctc 83.713318 loss_rnnt 66.104568 hw_loss 0.217393 history loss 47.236239 rank 0
2023-02-17 16:23:51,683 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 7
2023-02-17 16:23:51,708 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 1
2023-02-17 16:23:52,043 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 4
2023-02-17 16:23:52,694 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 3
2023-02-17 16:23:53,089 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 6
2023-02-17 16:23:53,496 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 5
2023-02-17 16:23:54,318 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 2
2023-02-17 16:23:55,356 DEBUG CV Batch 0/1300 loss 33.124821 loss_att 50.045723 loss_ctc 38.560982 loss_rnnt 28.862753 hw_loss 0.287001 history loss 47.698971 rank 0
2023-02-17 16:24:02,785 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 7
2023-02-17 16:24:02,925 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 1
2023-02-17 16:24:03,249 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 4
2023-02-17 16:24:03,934 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 3
2023-02-17 16:24:04,422 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 6
2023-02-17 16:24:04,900 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 5
2023-02-17 16:24:05,615 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 2
2023-02-17 16:24:06,837 DEBUG CV Batch 0/1400 loss 144.657532 loss_att 326.404053 loss_ctc 141.054947 loss_rnnt 108.787521 hw_loss 0.002001 history loss 48.241856 rank 0
2023-02-17 16:24:14,341 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 7
2023-02-17 16:24:14,446 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 1
2023-02-17 16:24:14,803 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 4
2023-02-17 16:24:15,483 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 3
2023-02-17 16:24:15,937 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 6
2023-02-17 16:24:16,513 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 5
2023-02-17 16:24:17,301 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 2
2023-02-17 16:24:18,699 DEBUG CV Batch 0/1500 loss 65.580879 loss_att 136.780090 loss_ctc 64.996994 loss_rnnt 51.359985 hw_loss 0.110453 history loss 47.562535 rank 0
2023-02-17 16:24:27,285 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 7
2023-02-17 16:24:27,491 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 1
2023-02-17 16:24:27,837 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 4
2023-02-17 16:24:28,542 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 3
2023-02-17 16:24:29,005 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 6
2023-02-17 16:24:29,641 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 5
2023-02-17 16:24:30,680 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 2
2023-02-17 16:24:31,916 DEBUG CV Batch 0/1600 loss 116.528816 loss_att 304.393250 loss_ctc 105.714798 loss_rnnt 80.396729 hw_loss 0.002001 history loss 47.751363 rank 0
2023-02-17 16:24:39,535 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 7
2023-02-17 16:24:39,713 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 1
2023-02-17 16:24:40,087 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 4
2023-02-17 16:24:40,783 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 3
2023-02-17 16:24:41,256 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 6
2023-02-17 16:24:42,027 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 5
2023-02-17 16:24:43,004 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 2
2023-02-17 16:24:44,361 DEBUG CV Batch 0/1700 loss 56.297569 loss_att 100.801651 loss_ctc 64.254990 loss_rnnt 46.223454 hw_loss 0.210581 history loss 47.434791 rank 0
2023-02-17 16:24:48,456 INFO Epoch 0 CV info cv_loss 47.602798913872164
2023-02-17 16:24:48,457 INFO Epoch 1 TRAIN info lr 0.00033392000000000003
2023-02-17 16:24:48,458 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:48,537 INFO Epoch 0 CV info cv_loss 47.602798913872164
2023-02-17 16:24:48,537 INFO Epoch 1 TRAIN info lr 0.00033516000000000004
2023-02-17 16:24:48,542 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:49,011 INFO Epoch 0 CV info cv_loss 47.60279891249382
2023-02-17 16:24:49,011 INFO Epoch 1 TRAIN info lr 0.00033304
2023-02-17 16:24:49,013 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:49,726 INFO Epoch 0 CV info cv_loss 47.60279891690451
2023-02-17 16:24:49,727 INFO Epoch 1 TRAIN info lr 0.00033312
2023-02-17 16:24:49,731 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:50,179 INFO Epoch 0 CV info cv_loss 47.602798911942486
2023-02-17 16:24:50,180 INFO Epoch 1 TRAIN info lr 0.00033383999999999996
2023-02-17 16:24:50,185 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:50,967 INFO Epoch 0 CV info cv_loss 47.60279891070198
2023-02-17 16:24:50,968 INFO Epoch 1 TRAIN info lr 0.00033348
2023-02-17 16:24:50,969 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:52,480 INFO Epoch 0 CV info cv_loss 47.60279891869636
2023-02-17 16:24:52,481 INFO Epoch 1 TRAIN info lr 0.00033395999999999995
2023-02-17 16:24:52,486 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-17 16:24:53,424 INFO Epoch 0 CV info cv_loss 47.602798915664
2023-02-17 16:24:53,424 INFO Checkpoint: save to checkpoint exp/2_16_rnnt_bias_loss_2_class_both/0.pt
2023-02-17 16:24:54,056 INFO Epoch 1 TRAIN info lr 0.00033371999999999997
2023-02-17 16:24:54,060 INFO using accumulate grad, new batch size is 1 times larger than before
run_2_16_rnnt_bias_both_2_class.sh: line 166: 23910 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.0.1]:43678: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.0.1]:22185: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:32148
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:8092
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:50587
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:45728
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:11958
